{"id": "1549922", "url": "https://en.wikipedia.org/wiki?curid=1549922", "title": "136 (number)", "text": "136 (number)\n\n136 (one hundred [and] thirty six) is the natural number following 135 and preceding 137.\n\n136 is itself a factor of the Eddington number. With a total of 8 divisors, 8 among them, 136 is a refactorable number. It is a composite number.\n\n136 is a triangular number, a centered triangular number and a centered nonagonal number.\n\nThe sum of the ninth row of Lozanić's triangle is 136.\n\n136 is a self-descriptive number in base 4, and a repdigit in base 16. In base 10, the sum of the cubes of its digits is formula_1. The sum of the cubes of the digits of 244 is formula_2.\n\n136 is the sum of the first 16 positive integers.\n\n\n\n\n\n\n"}
{"id": "362213", "url": "https://en.wikipedia.org/wiki?curid=362213", "title": "28 (number)", "text": "28 (number)\n\n28 (twenty-eight) is the natural number following 27 and preceding 29.\n\nIt is a composite number, its proper divisors being 1, 2, 4, 7, and 14.\n\nTwenty-eight is the second perfect number. As a perfect number, it is related to the Mersenne prime 7, since 2(2 − 1) = 28. The next perfect number is 496, the previous being 6.\n\nTwenty-eight is the sum of the totient function for the first nine integers.\n\nSince the greatest prime factor of 28 + 1 = 785 is 157, which is more than 28 twice, 28 is a Størmer number.\n\nTwenty-eight is a harmonic divisor number, a happy number, a triangular number, a hexagonal number, and a centered nonagonal number.\n\nIt appears in the Padovan sequence, preceded by the terms 12, 16, 21 (it is the sum of the first two of these).\n\nIt is also a Keith number, because it recurs in a Fibonacci-like sequence started from its base 10 digits: 2, 8, 10, 18, 28...\n\nTwenty-eight is the third positive integer with a prime factorization of the form 2\"q\" where \"q\" is an odd prime.\n\nTwenty-eight is the ninth and last number in early Indian magic square of order 3.\n\nThere are twenty-eight convex uniform honeycombs.\n\nTwenty-eight is the only positive integer that has a unique Kayles nim-value.\n\nTwenty-eight is the only known number which can be expressed as a sum of the first non negative integers (1 + 2 + 3 + 4 + 5 + 6 + 7), a sum of the first primes (2 + 3 + 5 + 7 + 11) and a sum of the first non primes (1 + 4 + 6 + 8 + 9) and there is probably no other number with this property.\n\nThere are 28 oriented diffeomorphism classes of manifolds homeomorphic to the 7-sphere.\n\n\n\n\nTwenty-eight is:\n\n\n"}
{"id": "2121038", "url": "https://en.wikipedia.org/wiki?curid=2121038", "title": "313 (number)", "text": "313 (number)\n\n313 (three hundred [and] thirteen) is the natural number following 312 and preceding 314.\n\n313 is:\n\n\nIn Twelver Shia Islam mysticism, 313 is the number of soldiers in the army of the 12th \"hidden Imam\" (Mahdi).\n\n\n"}
{"id": "47950379", "url": "https://en.wikipedia.org/wiki?curid=47950379", "title": "Arditi–Ginzburg equations", "text": "Arditi–Ginzburg equations\n\nThe Arditi–Ginzburg equations describe ratio dependent predator–prey dynamics. Where \"N\" is the population of a prey species and \"P\" that of a predator, the population dynamics are described by the following two equations:\n\nHere \"f\"(\"N\") captures any change in the prey population not due to predator activity including inherent birth and death rates. The per capita effect of predators on the prey population (the harvest rate) is modeled by a function \"g\" which is a function of the ratio \"N\"/\"P\" of prey to predators. Predators receive a reproductive payoff, \"e,\" for consuming prey, and die at rate \"u\". Making predation pressure a function of the ratio of prey to predators contrasts with the prey dependent Lotka–Volterra equations, where the effect of predators on the prey population is simply a function of the magnitude of the prey population \"g\"(\"N\"). Because the number of prey harvested by each predator decreases as predators become more dense, ratio dependent predation represents an example of a trophic function. Ratio dependent predation may account for heterogeneity in large-scale natural systems in which predator efficiency decreases when prey is scarce. The merit of ratio dependent versus prey dependent models of predation has been the subject of much controversy, especially between the biologists Lev R. Ginzburg and Peter A. Abrams. Ginzburg purports that ratio dependent models more accurately depict predator-prey interactions while Abrams maintains that these models make unwarranted complicating assumptions.\n\n"}
{"id": "1777555", "url": "https://en.wikipedia.org/wiki?curid=1777555", "title": "Arithmetica", "text": "Arithmetica\n\nArithmetica () is an Ancient Greek text on mathematics written by the mathematician Diophantus in the 3rd century AD. It is a collection of 130 algebraic problems giving numerical solutions of determinate equations (those with a unique solution) and indeterminate equations.\n\nEquations in the book are presently called Diophantine equations. The method for solving these equations is known as Diophantine analysis. Most of the \"Arithmetica\" problems lead to quadratic equations. \n\nIn Book 3, Diophantus solves problems of finding values which make two linear expressions simultaneously into squares or cubes. In book 4, he finds rational powers between given numbers. He also noticed that numbers of the form formula_1 cannot be the sum of two squares. Diophantus also appears to know that every number can be written as the sum of four squares. If he did know this result (in the sense of having proved it as opposed to merely conjectured it), his doing so would be truly remarkable: even Fermat, who stated the result, failed to provide a proof of it and it was not settled until Joseph Louis Lagrange proved it using results due to Leonhard Euler.\n\n\"Arithmetica\" was originally written in thirteen books, but the Greek manuscripts that survived to the present contain no more than six books. In 1968, Fuat Sezgin found four previously unknown books of \"Arithmetica\" at the shrine of Imam Rezā in the holy Islamic city of Mashhad in northeastern Iran. The four books are thought to have been translated from Greek to Arabic by Qusta ibn Luqa (820–912). Norbert Schappacher has written:\n\n[The four missing books] resurfaced around 1971 in the Astan Quds Library in Meshed (Iran) in a copy from 1198 AD. It was not catalogued under the name of Diophantus (but under that of Qusta ibn Luqa) because the librarian was apparently not able to read the main line of the cover page where Diophantus’s name appears in geometric Kufi calligraphy.\n\"Arithmetica\" became known to mathematicians in the Islamic world in the tenth century when Abu'l-Wefa translated it into Arabic.\n\n\nDiophantus Alexandrinus, Pierre de Fermat, Claude Gaspard Bachet de Meziriac, \"Diophanti Alexandrini Arithmeticorum libri 6, et De numeris multangulis liber unus\". Cum comm. C(laude) G(aspar) Bacheti et observationibus P(ierre) de Fermat. Acc. doctrinae analyticae inventum novum, coll. ex variis eiu. Tolosae 1670, .\n"}
{"id": "9858698", "url": "https://en.wikipedia.org/wiki?curid=9858698", "title": "Association for Women in Mathematics", "text": "Association for Women in Mathematics\n\nThe Association for Women in Mathematics (AWM) is a professional society whose mission is to encourage women and girls to study and to have active careers in the mathematical sciences, and to promote equal opportunity for and the equal treatment of women and girls in the mathematical sciences. The AWM was founded in 1971 and incorporated in the state of Massachusetts. AWM has approximately 5200 members, including over 250 institutional members, such as colleges, universities, institutes, and mathematical societies. It offers numerous programs and workshops to mentor women and girls in the mathematical sciences. Much of AWM’s work is supported through federal grants.\n\nThe Association was founded in 1971 as the Association of Women Mathematicians, but the name was changed almost immediately. As reported in \"A Brief History of the Association for Women in Mathematics: The Presidents' Perspectives\", by Lenore Blum, \"As Judy Green remembers (and Chandler Davis, early AWM friend, concurs): 'The formal idea of women getting together and forming a caucus was first made publicly at a MAG [Mathematics Action Group] meeting in 1971 ... in Atlantic City. Joanne Darken, then an instructor at Temple University and now at the Community College of Philadelphia, stood up at the meeting and suggested that the women present remain and form a caucus. I have been able to document six women who remained: me (I was a graduate student at Maryland at the time), Joanne Darken, Mary [W.] Gray (she was already at American University), Diane Laison (then an instructor at Temple), Gloria Olive (a Senior Lecturer at the University of Otago, New Zealand who was visiting the U.S. at the time) and Annie Selden... It's not absolutely clear what happened next, except that I've personally always thought that Mary was responsible for getting the whole thing organized ...'\" Mary W. Gray was the early organizer, placing an advertisement in the February 1971 Notices of the AMS, and writing the first issue of the \"AWM Newsletter\" that May. Early goals of the association focused on equal pay for equal work, as well as equal consideration for admission to graduate school and support while there; for faculty appointments at all levels; for promotion and for tenure; for administrative appointments; and for government grants, positions on review and advisory panels and positions in professional organizations. The AWM holds an annual meeting at the Joint Mathematics Meetings. In 2011 the association initiated a biennial Research Symposium during its 40th anniversary celebration 40 Years and Counting.\n\nThe AWM sponsors three honorary lecture series.\n\n\nThe AWM sponsors several awards and prizes.\n\n\nThree recently created prizes for early-career women are also sponsored by the AWM.\n\n\nThe AWM Fellows program recognizes \"individuals who have demonstrated a sustained commitment to the support and advancement of women in the mathematical sciences\".\n\n\n\n"}
{"id": "1064587", "url": "https://en.wikipedia.org/wiki?curid=1064587", "title": "Bigram", "text": "Bigram\n\nA bigram or digram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A bigram is an \"n\"-gram for \"n\"=2. The frequency distribution of every bigram in a string is commonly used for simple statistical analysis of text in many applications, including in computational linguistics, cryptography, speech recognition, and so on.\n\n\"Gappy bigrams\" or \"skipping bigrams\" are word pairs which allow gaps (perhaps avoiding connecting words, or allowing some simulation of dependencies, as in a dependency grammar).\n\n\"Head word bigrams\" are gappy bigrams with an explicit dependency relationship.\n\nBigrams help provide the conditional probability of a token given the preceding token, when the relation of the conditional probability is applied:\n\nformula_1\n\nThat is, the probability formula_2 of a token formula_3 given the preceding token formula_4 is equal to the probability of their bigram, or the co-occurrence of the two tokens formula_5, divided by the probability of the preceding token.\n\nBigrams are used in most successful language models for speech recognition. They are a special case of N-gram.\n\nBigram frequency attacks can be used in cryptography to solve cryptograms. See frequency analysis.\n\nBigram frequency is one approach to statistical language identification.\n\nSome activities in logology or recreational linguistics involve bigrams. These include attempts to find English words beginning with every possible bigram, or words containing a string of repeated bigrams, such as \"logogogue\".\n\nThe frequency of the most common letter bigrams in a small English corpus is:\n\nComplete bigram frequencies for a larger corpus are available.\n"}
{"id": "3774360", "url": "https://en.wikipedia.org/wiki?curid=3774360", "title": "Bracket polynomial", "text": "Bracket polynomial\n\nIn the mathematical field of knot theory, the bracket polynomial (also known as the Kauffman bracket) is a polynomial invariant of framed links. Although it is not an invariant of knots or links (as it is not invariant under type I Reidemeister moves), a suitably \"normalized\" version yields the famous knot invariant called the Jones polynomial. The bracket polynomial plays an important role in unifying the Jones polynomial with other quantum invariants. In particular, Kauffman's interpretation of the Jones polynomial allows generalization to invariants of 3-manifolds. \n\nThe bracket polynomial was discovered by Louis Kauffman in 1987.\n\nThe bracket polynomial of any (unoriented) link diagram formula_1, denoted formula_2, is a polynomial in the variable formula_3, characterized by the three rules:\n\n\nThe pictures in the second rule represent brackets of the link diagrams which differ inside a disc as shown but are identical outside. The third rule means that adding a circle disjoint from the rest of the diagram multiplies the bracket of the remaining diagram by formula_7.\n\n"}
{"id": "175609", "url": "https://en.wikipedia.org/wiki?curid=175609", "title": "Cayley–Dickson construction", "text": "Cayley–Dickson construction\n\nIn mathematics, the Cayley–Dickson construction, named after Arthur Cayley and Leonard Eugene Dickson, produces a sequence of algebras over the field of real numbers, each with twice the dimension of the previous one. The algebras produced by this process are known as Cayley–Dickson algebras, for example complex numbers, quaternions, and octonions. These examples are useful composition algebras frequently applied in mathematical physics.\n\nThe Cayley–Dickson construction defines a new algebra similar to the direct sum of an algebra with itself, with multiplication defined in a specific way (different from the multiplication provided by the genuine direct sum) and an involution known as conjugation. The product of an element and its conjugate (or sometimes the square root of this product) is called the norm.\n\nThe symmetries of the real field disappear as the Cayley–Dickson construction is repeatedly applied: first losing order, then commutativity of multiplication, associativity of multiplication, and next alternativity.\n\nMore generally, the Cayley–Dickson construction takes any algebra with involution to another algebra with involution of twice the dimension.\n\nThe complex numbers can be written as ordered pairs (\"a\", \"b\") of real numbers \"a\" and \"b\", with the addition operator being component-by-component and with multiplication defined by\n\nA complex number whose second component is zero is associated with a real number: the complex number (\"a\", 0) is the real number \"a\".\n\nThe complex conjugate (\"a\", \"b\")* of (\"a\", \"b\") is given by\n\nThe conjugate has the property that\n\nwhich is a non-negative real number. In this way, conjugation defines a \"norm\", making the complex numbers a normed vector space over the real numbers: the norm of a complex number \"z\" is\n\nFurthermore, for any non-zero complex number \"z\", conjugation gives a multiplicative inverse,\n\nAs a complex number consists of two independent real numbers, they form a 2-dimensional vector space over the real numbers.\n\nBesides being of higher dimension, the complex numbers can be said to lack one algebraic property of the real numbers: a real number is its own conjugate.\n\nThe next step in the construction is to generalize the multiplication and conjugation operations.\n\nForm ordered pairs formula_6 of complex numbers formula_7 and formula_8, with multiplication defined by\n\nSlight variations on this formula are possible; the resulting constructions will yield structures identical up to the signs of bases.\n\nThe order of the factors seems odd now, but will be important in the next step.\n\nDefine the conjugate formula_10 of formula_6 by\n\nThese operators are direct extensions of their complex analogs: if formula_7 and formula_8 are taken from the real subset of complex numbers, the appearance of the conjugate in the formulas has no effect, so the operators are the same as those for the complex numbers.\n\nThe product of a nonzero element with its conjugate is a non-negative real number:\n\nAs before, the conjugate thus yields a norm and an inverse for any such ordered pair. So in the sense we explained above, these pairs constitute an algebra something like the real numbers. They are the quaternions, named by Hamilton in 1843.\n\nAs a quaternion consists of two independent complex numbers, they form a 4-dimensional vector space over the real numbers.\n\nThe multiplication of quaternions is not quite like the multiplication of real numbers, though. It is not commutative, that is, if formula_16 and formula_17 are quaternions, it is not always true that formula_18, but it is true that formula_19, where formula_20.\n\nAll the steps to create further algebras are the same from octonions on.\n\nThis time, form ordered pairs formula_21 of\nquaternions formula_16 and formula_17, with multiplication and conjugation defined exactly as for the quaternions:\n\nNote, however, that because the quaternions are not commutative, the order of the factors in the multiplication formula becomes important—if the last factor in the multiplication formula were formula_25 rather than\nformula_26, the formula for multiplication of an element by its conjugate would not yield a real number.\n\nFor exactly the same reasons as before, the conjugation operator yields a norm and a multiplicative inverse of any nonzero element.\n\nThis algebra was discovered by John T. Graves in 1843, and is called the octonions or the \"Cayley numbers\".\n\nAs an octonion consists of two independent quaternions, they form an 8-dimensional vector space over the real numbers.\n\nThe multiplication of octonions is even stranger than that of quaternions. Besides being non-commutative, it is not associative: that is, if formula_16, formula_17, and formula_29 are octonions, it is not always true that\n\nFor the reason of this non-associativity, octonions have no matrix representation.\n\nThe algebra immediately following the octonions is called the sedenions. It retains an algebraic property called power associativity, meaning that if formula_31 is a sedenion, formula_32, but loses the property of being an alternative algebra and hence cannot be a composition algebra.\n\nThe Cayley–Dickson construction can be carried on \"ad infinitum\", at each step producing a power-associative algebra whose dimension is double that of the algebra of the preceding step. All the algebras generated in this way over a field are \"quadratic\": that is, each element satisfies a quadratic equation with coefficients from the field.\n\nIn 1954 R. D. Schafer examined the algebras generated by the Cayley-Dickson process over a field \"F\" and showed they satisfy the flexible identity. He also proved that any derivation algebra of a Cayley-Dickson algebra is isomorphic to the derivation algebra of Cayley numbers, a 14-dimensional Lie algebra over \"F\".\n\nThe Cayley–Dickson construction, starting from the real numbers ℝ, generates division composition algebras. There are also composition algebras with isotropic quadratic forms that are obtained through a slight modification, by replacing the minus sign in the definition of the product of ordered pairs with a plus sign, as follows:\n\nWhen this modified construction is applied to ℝ, one obtains the split-complex numbers, which are ring-isomorphic to the direct sum ℝ ⊕ ℝ (also written ℝ); following that, one obtains the split-quaternions, isomorphic to M(ℝ); and the split-octonions, which are isomorphic to Zorn(ℝ). Applying the original Cayley–Dickson construction to the split-complexes also results in the split-quaternions and then the split-octonions.\n\n gave a slight generalization, defining the product and involution on \"B\"=\"A\"⊕\"A\" for \"A\" an algebra with involution (with (\"xy\") = \"y\"\"x\") to be \nfor γ an additive map that commutes with * and left and right multiplication by any element. (Over the reals all choices of γ are equivalent to −1, 0 or 1.) In this construction, \"A\" is an algebra with involution, meaning:\nThe algebra \"B\"=\"A\"⊕\"A\" produced by the Cayley–Dickson construction is also an algebra with involution.\n\n\"B\" inherits properties from \"A\" unchanged as follows. \nOther properties of \"A\" only induce weaker properties of \"B\":\n\n\n"}
{"id": "46877898", "url": "https://en.wikipedia.org/wiki?curid=46877898", "title": "Chinese Whispers (clustering method)", "text": "Chinese Whispers (clustering method)\n\nChinese Whispers is a clustering method used in network science named after the famous whispering game. Clustering methods are basically used to identify communities of nodes or links in a given network. This algorithm was designed by Chris Biemann and Sven Teresniak in 2005. The name comes from the fact that the process can be modeled as a separation of communities where the nodes send the same type of information to each other.\n\nChinese Whispers is a hard partitioning, randomized, flat clustering (no hierarchical relations between clusters) method. The random property means that running the process on the same network several times can lead to different results, while because of hard partitioning one node can only belong to one cluster at a given moment. The original algorithm is applicable to undirected, weighted and unweighted graphs. Chinese Whispers is time linear which means that it is extremely fast even if the number of nodes and links are very high in the network.\n\nThe algorithm works in the following way in an undirected unweighted graph:\n\nThe predetermined threshold for the number of the iterations is needed because it is possible that process does not converge. On the other hand in a network with approximately 10000 nodes the clusters does not change significantly after 40-50 iterations even if there is no convergence.\n\nThe main strength of Chinese Whispers lies in its time linear property. Because of the processing time increases linearly with the number of nodes, the algorithm is capable of identifying communities in a network very fast. For this reason Chinese Whispers is a good tool to analyze community structures in graph with a very high number of nodes. The effectiveness of the method increases further if the network has the small world property.\n\nOn the other hand because the algorithm is not deterministic in the case of small node number the resulting clusters often significantly differ from each other. The reason for this is that in the case of a small network it matters more from which node the iteration process starts while in large networks the relevance of starting points disappears. For this reason for small graphs other clustering methods are recommended.\n\nChinese Whispers is used in many subfield of network science. Most frequently it is mentioned in the context of Natural Language Processing problems. On the other hand the algorithm is applicable to any kind of community identification problem which is related to a network framework. Chinese Whispers is available for personal use as an extension package for Gephi which is an open source program designed for network analysis.\n"}
{"id": "31824847", "url": "https://en.wikipedia.org/wiki?curid=31824847", "title": "Chiral algebra", "text": "Chiral algebra\n\nIn mathematics, a chiral algebra is an algebraic structure introduced by as a rigorous version of the rather vague concept of a chiral algebra in physics.\n\n\n"}
{"id": "202886", "url": "https://en.wikipedia.org/wiki?curid=202886", "title": "Covariance and contravariance of vectors", "text": "Covariance and contravariance of vectors\n\nIn multilinear algebra and tensor analysis, covariance and contravariance describe how the quantitative description of certain geometric or physical entities changes with a change of basis. \n\nIn physics, a basis is sometimes thought of as a set of reference axes. A change of scale on the reference axes corresponds to a change of units in the problem. For instance, by changing scale from meters to centimeters (that is, \"dividing\" the scale of the reference axes by 100), the components of a measured velocity vector are \"multiplied\" by 100. Vectors exhibit this behavior of changing scale \"inversely\" to changes in scale to the reference axes and consequently are called \"contravariant\". As a result, vectors often have units of distance or distance with other units (as, for example, velocity has units of distance divided by time).\n\nIn contrast, covectors (also called \"dual vectors\") typically have units of the inverse of distance or the inverse of distance with other units. An example of a covector is the gradient, which has units of a spatial derivative, or distance. The components of covectors change in the \"same way\" as changes to scale of the reference axes and consequently are called \"covariant\".\n\nA third concept related to covariance and contravariance is invariance. An example of a physical observable that does not change with a change of scale on the reference axes is the mass of a particle, which has units of mass (that is, no units of distance). The single, scalar value of mass is independent of changes to the scale of the reference axes and consequently is called \"invariant\".\n\nUnder more general changes in basis:\n\nCurvilinear coordinate systems, such as cylindrical or spherical coordinates, are often used in physical and geometric problems. Associated with any coordinate system is a natural choice of coordinate basis for vectors based at each point of the space, and covariance and contravariance are particularly important for understanding how the coordinate description of a vector changes by passing from one coordinate system to another.\n\nThe terms \"covariant\" and \"contravariant\" were introduced by James Joseph Sylvester in 1851 in the context of associated algebraic forms theory. In the lexicon of category theory, covariance and contravariance are properties of functors; unfortunately, it is the lower-index objects (covectors) that generically have pullbacks, which are contravariant, while the upper-index objects (vectors) instead have pushforwards, which are covariant. This terminological conflict may be avoided by calling contravariant functors \"cofunctors\"—in accord with the \"covector\" terminology, and continuing the tradition of treating vectors as the concept and covectors as the coconcept.\n\nTensors are objects in multilinear algebra that can have aspects of both covariance \"and\" contravariance.\n\nIn physics, a vector typically arises as the outcome of a measurement or series of measurements, and is represented as a list (or tuple) of numbers such as\n\nThe numbers in the list depend on the choice of coordinate system. For instance, if the vector represents position with respect to an observer (position vector), then the coordinate system may be obtained from a system of rigid rods, or reference axes, along which the components \"v\", \"v\", and \"v\" are measured. For a vector to represent a geometric object, it must be possible to describe how it looks in any other coordinate system. That is to say, the components of the vectors will \"transform\" in a certain way in passing from one coordinate system to another.\n\nA \"contravariant vector\" has components that \"transform as the coordinates do\" under changes of coordinates (and so inversely to the transformation of the reference axes), including rotation and dilation. The vector itself does not change under these operations; instead, the components of the vector change in a way that cancels the change in the spatial axes, in the same way that coordinates change. In other words, if the reference axes were rotated in one direction, the component representation of the vector would rotate in exactly the opposite way. Similarly, if the reference axes were stretched in one direction, the components of the vector, like the coordinates, would reduce in an exactly compensating way. Mathematically, if the coordinate system undergoes a transformation described by an invertible matrix \"M\", so that a coordinate vector x is transformed to formula_4, then a contravariant vector v must be similarly transformed via formula_5. This important requirement is what distinguishes a contravariant vector from any other triple of physically meaningful quantities. For example, if \"v\" consists of the \"x\"-, \"y\"-, and \"z\"-components of velocity, then \"v\" is a contravariant vector: if the coordinates of space are stretched, rotated, or twisted, then the components of the velocity transform in the same way. Examples of contravariant vectors include displacement, velocity and acceleration. On the other hand, for instance, a triple consisting of the length, width, and height of a rectangular box could make up the three components of an abstract vector, but this vector would not be contravariant, since a change in coordinates on the space does not change the box's length, width, and height: instead these are scalars.\n\nBy contrast, a \"covariant vector\" has components that change oppositely to the coordinates or, equivalently, transform like the reference axes. For instance, the components of the gradient vector of a function\ntransform like the reference axes themselves.\n\nThe general formulation of covariance and contravariance refer to how the components of a coordinate vector transform under a change of basis (passive transformation). Thus let \"V\" be a vector space of dimension \"n\" over the field of scalars \"S\", and let each of and be a basis of \"V\". Also, let the change of basis from f to f′ be given by\n\nfor some invertible \"n\"×\"n\" matrix \"A\" with entries formula_7.\nHere, each vector \"Y\" of the f′ basis is a linear combination of the vectors \"X\" of the f basis, so that\n\nA vector formula_9 in \"V\" is expressed uniquely as a linear combination of the elements of the f basis as\n\nwhere \"v\"[f] are scalars in \"S\" known as the components of \"v\" in the f basis. Denote the column vector of components of \"v\" by v[f]:\n\nso that () can be rewritten as a matrix product\n\nThe vector \"v\" may also be expressed in terms of the f′ basis, so that\n\nHowever, since the vector \"v\" itself is invariant under the choice of basis,\n\nThe invariance of \"v\" combined with the relationship () between f and f′ implies that\n\ngiving the transformation rule\n\nIn terms of components,\n\nwhere the coefficients formula_17 are the entries of the inverse matrix of \"A\".\n\nBecause the components of the vector \"v\" transform with the \"inverse\" of the matrix \"A\", these components are said to transform contravariantly under a change of basis.\n\nThe way \"A\" relates the two pairs is depicted in the following informal diagram using an arrow. The reversal of the arrow indicates a contravariant change:\n\nA linear functional \"α\" on \"V\" is expressed uniquely in terms of its components (scalars in \"S\") in the f basis as\n\nThese components are the action of \"α\" on the basis vectors \"X\" of the f basis.\n\nUnder the change of basis from f to f′ (), the components transform so that\n\nDenote the row vector of components of \"α\" by \"α\"[f]:\n\nso that () can be rewritten as the matrix product\n\nBecause the components of the linear functional α transform with the matrix \"A\", these components are said to transform covariantly under a change of basis.\n\nThe way \"A\" relates the two pairs is depicted in the following informal diagram using an arrow. A covariant relationship is indicated since the arrows travel in the same direction:\n\nHad a column vector representation been used instead, the transformation law would be the transpose\n\nThe choice of basis f on the vector space \"V\" defines uniquely a set of coordinate functions on \"V\", by means of\nThe coordinates on \"V\" are therefore contravariant in the sense that\nConversely, a system of \"n\" quantities \"v\" that transform like the coordinates \"x\" on \"V\" defines a contravariant vector. A system of \"n\" quantities that transform oppositely to the coordinates is then a covariant vector.\n\nThis formulation of contravariance and covariance is often more natural in applications in which there is a coordinate space (a manifold) on which vectors live as tangent vectors or cotangent vectors. Given a local coordinate system \"x\" on the manifold, the reference axes for the coordinate system are the vector fields\nThis gives rise to the frame at every point of the coordinate patch.\n\nIf \"y\" is a different coordinate system and\nthen the frame f' is related to the frame f by the inverse of the Jacobian matrix of the coordinate transition:\nOr, in indices,\n\nA tangent vector is by definition a vector that is a linear combination of the coordinate partials formula_30. Thus a tangent vector is defined by\n\nSuch a vector is contravariant with respect to change of frame. Under changes in the coordinate system, one has\n\nTherefore, the components of a tangent vector transform via\n\nAccordingly, a system of \"n\" quantities \"v\" depending on the coordinates that transform in this way on passing from one coordinate system to another is called a contravariant vector.\n\nIn a finite-dimensional vector space \"V\" over a field \"K\" with a symmetric bilinear form (which may be referred to as the metric tensor), there is little distinction between covariant and contravariant vectors, because the bilinear form allows covectors to be identified with vectors. That is, a vector \"v\" uniquely determines a covector \"α\" via\nfor all vectors \"w\". Conversely, each covector \"α\" determines a unique vector \"v\" by this equation. Because of this identification of vectors with covectors, one may speak of the covariant components or contravariant components of a vector, that is, they are just representations of the same vector using the reciprocal basis.\n\nGiven a basis of \"V\", there is a unique reciprocal basis of \"V\" determined by requiring that\nthe Kronecker delta. In terms of these bases, any vector \"v\" can be written in two ways:\nThe components \"v\"[f] are the contravariant components of the vector \"v\" in the basis f, and the components \"v\"[f] are the covariant components of \"v\" in the basis f. The terminology is justified because under a change of basis,\n\nIn the Euclidean plane, the dot product allows for vectors to be identified with covectors. If formula_38 is a basis, then the dual basis formula_39 satisfies\n\nThus, e and e are perpendicular to each other, as are e and e, and the lengths of e and e normalized against e and e, respectively.\n\nFor example, suppose that we are given a basis e, e consisting of a pair of vectors making a 45° angle with one another, such that e has length 2 and e has length 1. Then the dual basis vectors are given as follows:\nApplying these rules, we find\nand\n\nThus the change of basis matrix in going from the original basis to the reciprocal basis is\nsince\n\nFor instance, the vector\nis a vector with contravariant components\n\nThe covariant components are obtained by equating the two expressions for the vector \"v\":\nso\n\nIn the three-dimensional Euclidean space, one can also determine explicitly the dual basis to a given set of basis vectors e, e, e of \"E\" that are not necessarily assumed to be orthogonal nor of unit norm. The dual basis vectors are:\n\nEven when the e and e are not orthonormal, they are still mutually reciprocal:\n\nThen the contravariant components of any vector v can be obtained by the dot product of v with the dual basis vectors:\n\nLikewise, the covariant components of v can be obtained from the dot product of v with basis vectors, viz.\n\nThen v can be expressed in two (reciprocal) ways, viz.\nor\nCombining the above relations, we have\nand we can convert between the basis and dual basis with\nand\n\nIf the basis vectors are orthonormal, then they are the same as the dual basis vectors. So there is no need to distinguish between contravariant components and covariant components which are also equal.\n\nMore generally, in an \"n\"-dimensional Euclidean space \"V\", if a basis is\nthe reciprocal basis is given by (double indices are summed over),\nwhere the coefficients \"g\" are the entries of the inverse matrix of\nIndeed, we then have\n\nThe covariant and contravariant components of any vector\n\nare related as above by\nand\n\nIn the field of physics, the adjective covariant is often used informally as a synonym for invariant. For example, the Schrödinger equation does not keep its written form under the coordinate transformations of special relativity. Thus, a physicist might say that the Schrödinger equation is \"not covariant\". In contrast, the Klein–Gordon equation and the Dirac equation do keep their written form under these coordinate transformations. Thus, a physicist might say that these equations are \"covariant\".\n\nDespite this usage of \"covariant\", it is more accurate to say that the Klein–Gordon and Dirac equations are invariant, and that the Schrödinger equation is not invariant. Additionally, to remove ambiguity, the transformation by which the invariance is evaluated should be indicated.\n\nBecause the components of vectors are contravariant and those of covectors are covariant, the vectors themselves are often referred to as being contravariant and the covectors as covariant.\n\nThe distinction between covariance and contravariance is particularly important for computations with tensors, which often have mixed variance. This means that they have both covariant and contravariant components, or both vector and covector components. The valence of a tensor is the number of variant and covariant terms, and in Einstein notation, covariant components have lower indices, while contravariant components have upper indices. The duality between covariance and contravariance intervenes whenever a vector or tensor quantity is represented by its components, although modern differential geometry uses more sophisticated index-free methods to represent tensors.\n\nIn tensor analysis, a covariant vector varies more or less reciprocally to a corresponding contravariant vector. Expressions for lengths, areas and volumes of objects in the vector space can then be given in terms of tensors with covariant and contravariant indices. Under simple expansions and contractions of the coordinates, the reciprocity is exact; under affine transformations the components of a vector intermingle on going between covariant and contravariant expression.\n\nOn a manifold, a tensor field will typically have multiple, upper and lower indices, where Einstein notation is widely used. When the manifold is equipped with a metric, covariant and contravariant indices become very closely related to one another. Contravariant indices can be turned into covariant indices by contracting with the metric tensor. The reverse is possible by contracting with the (matrix) inverse of the metric tensor. Note that in general, no such relation exists in spaces not endowed with a metric tensor. Furthermore, from a more abstract standpoint, a tensor is simply \"there\" and its components of either kind are only calculational artifacts whose values depend on the chosen coordinates.\n\nThe explanation in geometric terms is that a general tensor will have contravariant indices as well as covariant indices, because it has parts that live in the tangent bundle as well as the cotangent bundle.\n\nA contravariant vector is one which transforms like formula_65, where formula_66 are the coordinates of a particle at its proper time formula_67. A covariant vector is one which transforms like formula_68, where formula_69 is a scalar field.\n\nIn category theory, there are covariant functors and contravariant functors. The assignment of the dual space to a vector space is a standard example of a contravariant functor. Some constructions of multilinear algebra are of 'mixed' variance, which prevents them from being functors.\n\nIn differential geometry, the components of a vector relative to a basis of the tangent bundle are covariant if they change with the same linear transformation as a change of basis. They are contravariant if they change by the inverse transformation. This is sometimes a source of confusion for two distinct but related reasons. The first is that vectors whose components are covariant (called covectors or 1-forms) actually pull back under smooth functions, meaning that the operation assigning the space of covectors to a smooth manifold is actually a \"contravariant\" functor. Likewise, vectors whose components are contravariant push forward under smooth mappings, so the operation assigning the space of (contravariant) vectors to a smooth manifold is a \"covariant\" functor. Secondly, in the classical approach to differential geometry, it is not bases of the tangent bundle that are the most primitive object, but rather changes in the coordinate system. Vectors with contravariant components transform in the same way as changes in the coordinates (because these actually change oppositely to the induced change of basis). Likewise, vectors with covariant components transform in the opposite way as changes in the coordinates.\n\n\n\n"}
{"id": "30389604", "url": "https://en.wikipedia.org/wiki?curid=30389604", "title": "De Moivre's law", "text": "De Moivre's law\n\nDe Moivre's Law is a survival model applied in actuarial science, named for Abraham de Moivre. It is a simple law of mortality based on a linear survival function. \n\nDe Moivre's law has a single\nparameter formula_1 called the \"ultimate age\". Under de Moivre's\nlaw, a newborn has probability of surviving at least \"x\" years given by the\nsurvival function\nIn actuarial notation \"(x)\" denotes a status or life that has survived to age \"x\", and \"T\"(\"x\") is the future lifetime of \"(x)\" (\"T\"(\"x\") is a random variable). The conditional probability that \"(x)\" survives to age \"x+t\" is \"Pr[T(0) ≥ x+t | T(0) ≥ x] = S(x+t) / S(x),\"\nwhich is denoted by formula_3.\nUnder de Moivre's law, the conditional probability that a life aged \"x\" years survives\nat least \"t\" more years is\nand the future lifetime random variable \"T\"(\"x\") therefore follows a uniform distribution on \nformula_5. \n\nThe actuarial notation for conditional probability of failure is formula_6 \"= Pr[0 ≤ T(x) ≤ t|T(0) ≥ x]\". Under de Moivre's law, the probability that \"(x)\" fails to survive to age \"x+t\" is \n\nThe force of mortality (hazard rate or failure rate) is formula_8 where \"f(x)\" is the probability density function. Under de Moivre's law, the force of mortality for a life aged \"x\" is\nwhich has the property of an increasing failure rate with respect to age.\n\nDe Moivre's law is applied as a simple analytical law of mortality and the linear assumption is also applied as a model for interpolation for discrete survival models such as life tables.\n\nDe Moivre's law first appeared in his 1725 \"Annuities upon Lives\", the earliest known example of an actuarial textbook. Despite the name now given to it, de Moivre himself did not consider his law (he called it a \"hypothesis\") to be a true description of the pattern of human mortality. Instead, he introduced it as a useful approximation when calculating the cost of annuities. In his text, de Moivre noted that \" ... although the Notion of an equable Decrement of Life ... [does] not exactly agree with the \"Tables\", yet that Notion may successfully be employed in constructing a \"Table\" of the Values of \"Annuities\" for \"Ages\" not inferiour to \"Twelve\" ... \". Furthermore, although his text contained an algebraic demonstration that applied to the entire expected future life span, de Moivre also supplied an algebraic demonstration that applied only to a limited number of years. It was this latter result that was used in his subsequent numerical examples. These examples showed de Moivre using his hypothesis in a piecewise fashion, wherein he assumed that the overall pattern of human mortality could be approximated by several straight-line segments (see his illustration to the right). He wrote that \"since the Decrements of Life may without any sensible Error be supposed equal, for any short Interval of Time, it follows that if the whole Extent of Life be divided into several shorter Intervals, ... , the Values of \"Annuities\" for Life ... may easily be calculated ... conformably to any \"Table of Observations\", and for any \"Rate of Interest\"\". For both of the quotes, de Moivre's references to \"tables\" were to actuarial life tables. \n\nModern authors are not consistent in their treatment of de Moivre's role in the history of mortality laws. On the one hand, Dick London describes de Moivre's law as \"the first continuous probability distribution to be suggested\" for use as a model of human survival. Robert Batten takes a similar view, adding that \"[de Moivre's] hypothesis .. has of course been found unrealistic\". In contrast, the surveys of analytical human survival models by Spiegelman and Benjamin do not mention de Moivre at all (in both cases, the surveys start with the work of Benjamin Gompertz). In his essay on the history of actuarial science, Stephen Haberman does mention de Moivre, but in the section on \"Life Insurance Mathematics\" and not the one on \"Life Tables and Survival Models\". A middle ground of sorts was taken by C. W. Jordan in his \"Life Contingencies\", where he included de Moivre in his section on \"Some famous laws of mortality\", but added that \"de Moivre recognized that this was a very rough approximation [whose objective was] the practical one of simplifying the calculation of life annuity values, which in those days was an arduous task\".\n\nAnother indication that de Moivre himself did not consider his \"hypothesis\" to be a true reflection of human mortality is the fact that he offered two distinct hypotheses in the \"Annuities upon Lives\". When he turned his attention to the question of valuing annuities payable on more than one life, de Moivre found it convenient to drop his assumption of an equal number of deaths (per year) in favor of an assumption of equal probabilities of death at each year of age (i.e., what is now called the \"constant force of mortality\" assumption). Although the constant-force assumption is also recognized today as a simple analytical law of mortality, it has never been known as \"de Moivre's second law\" or any other such name.\n\n"}
{"id": "2634860", "url": "https://en.wikipedia.org/wiki?curid=2634860", "title": "Defeasible logic", "text": "Defeasible logic\n\nDefeasible logic is a non-monotonic logic proposed by Donald Nute to formalize defeasible reasoning. In defeasible logic, there are three different types of propositions:\n\n\nA priority ordering over the defeasible rules and the defeaters can be given. During the process of deduction, the strict rules are always applied, while a defeasible rule can be applied only if no defeater of a higher priority specifies that it should not.\n\n\n"}
{"id": "2084687", "url": "https://en.wikipedia.org/wiki?curid=2084687", "title": "Direct sum", "text": "Direct sum\n\nThe direct sum is an operation from abstract algebra, a branch of mathematics. For example, the direct sum formula_1, where formula_2 is real coordinate space, is the Cartesian plane, formula_3. To see how direct sum is used in abstract algebra, consider a more elementary structure in abstract algebra, the abelian group. The direct sum of two abelian groups formula_4 and formula_5 is another abelian group formula_6 consisting of the ordered pairs formula_7 where formula_8 and formula_9. (Confusingly this ordered pair is also called the cartesian product of the two groups.) To add ordered pairs, we define the sum formula_10 to be formula_11; in other words addition is defined coordinate-wise. A similar process can be used to form the direct sum of any two algebraic structures, such as rings, modules, and vector spaces.\n\nWe can also form direct sums with any number of summands, for example formula_12, provided formula_13 and formula_14 are the same kinds of algebraic structures, that is, all groups, rings, vector spaces, etc.\n\nIn the case of two summands, or any finite number of summands, the direct sum is the same as the direct product. If the arithmetic operation is written as +, as it usually is in abelian groups, then we use the direct sum. If the arithmetic operation is written as × or ⋅ or using juxtaposition (as in the expression formula_15) we use direct product.\n\nIn the case where infinitely many objects are combined, most authors make a distinction between direct sum and direct product. As an example, consider the direct sum and direct product of infinitely many real lines. An element in the direct product is an infinite sequence, such as (1,2,3...) but in the direct sum, there would be a requirement that all but finitely many coordinates be zero, so the sequence (1,2,3...) would be an element of the direct product but not of the direct sum, while (1,2,0,0,0...) would be an element of both. More generally, if a + sign is used, all but finitely many coordinates must be zero, while if some form of multiplication is used, all but finitely many coordinates must be 1. In more technical language, if the summands are formula_16, the direct sum formula_17 is defined to be the set of tuples formula_18 with formula_19 such that formula_20 for all but finitely many \"i\". The direct sum formula_17 is contained in the direct product formula_22, but is usually strictly smaller when the index set formula_23 is infinite, because direct products do not have the restriction that all but finitely many coordinates must be zero.\n\nFor example, the \"xy\"-plane, a two-dimensional vector space, can be thought of as the direct sum of two one-dimensional vector spaces, namely the \"x\" and \"y\" axes. In this direct sum, the \"x\" and \"y\" axes intersect only at the origin (the zero vector). Addition is defined coordinate-wise, that is formula_24, which is the same as vector addition.\n\nGiven two objects formula_4 and formula_5, their direct sum is written as formula_6. Given an indexed family of objects formula_28, indexed with formula_29, the direct sum may be written formula_30. Each \"A\" is called a direct summand of \"A\". If the index set is finite, the direct sum is the same as the direct product. In the case of groups, if the group operation is written as formula_31 the phrase \"direct sum\" is used, while if the group operation is written formula_32 the phrase \"direct product\" is used. When the index set is infinite, the direct sum is not the same as the direct product. In the direct sum, all but finitely many coordinates must be zero.\n\nA distinction is made between internal and external direct sums, though the two are isomorphic. If the factors are defined first, and then the direct sum is defined in terms of the factors, we have an external direct sum. For example, if we define the real numbers formula_33 and then define formula_34 the direct sum is said to be external.\n\nIf, on the other hand, we first define some algebraic object, formula_35 and then write formula_35 as the direct sum of two of its subsets, formula_37 and formula_38, then the direct sum is said to be internal. In this case, each element of formula_35 is expressible uniquely as an algebraic combination of an element of formula_37 and an element of formula_38. For an example of an internal direct sum, consider formula_42, the integers modulo six, whose elements are formula_43. This is expressible as an internal direct sum formula_44.\n\nThe direct sum of abelian groups is a prototypical example of a direct sum. Given two abelian groups formula_45 and formula_46, their direct sum formula_47 is the same as their direct product, that is the underlying set is the Cartesian product formula_48 and the group operation formula_49 is defined component-wise:\nThis definition generalizes to direct sums of finitely many abelian groups.\n\nFor an infinite family of abelian groups \"A\" for \"i\" ∈ \"I\", the direct sum\nis a proper subgroup of the direct product. It consists of the elements formula_52 such that \"a\" is the identity element of \"A\" for all but finitely many \"i\".\n\nThe \"direct sum of modules\" is a construction which combines several modules into a new module.\n\nThe most familiar examples of this construction occur when considering vector spaces, which are modules over a field. The construction may also be extended to Banach spaces and Hilbert spaces.\n\nThe direct sum of group representations generalizes the direct sum of the underlying modules, adding a group action to it. Specifically, given a group \"G\" and two representations \"V\" and \"W\" of \"G\" (or, more generally, two \"G\"-modules), the direct sum of the representations is \"V\" ⊕ \"W\" with the action of \"g\" ∈ \"G\" given component-wise, i.e.\n\nSome authors will speak of the direct sum formula_53 of two rings when they mean the direct product formula_54, but this should be avoided since formula_54 does not receive natural ring homomorphisms from \"R\" and \"S\": in particular, the map formula_56 sending \"r\" to (\"r\",0) is not a ring homomorphism since it fails to send 1 to (1,1) (assuming that 0≠1 in \"S\"). Thus formula_54 is not a coproduct in the category of rings, and should not be written as a direct sum. (The coproduct in the category of commutative rings is the tensor product of rings. In the category of rings, the coproduct is given by a construction similar to the free product of groups.)\n\nUse of direct sum terminology and notation is especially problematic when dealing with infinite families of rings: If formula_58 is an infinite collection of nontrivial rings, then the direct sum of the underlying additive groups can be equipped with termwise multiplication, but this produces a rng, i.e., a ring without a multiplicative identity.\n\nAn additive category is an abstraction of the properties of the category of modules.\nIn such a category finite products and coproducts agree and the direct sum is either of them, cf. biproduct.\n\nGeneral case : \nIn category theory the direct sum is often, but not always, the coproduct in the category of the mathematical objects in question. For example, in the category of abelian groups, direct sum is a coproduct. This is also true in the category of modules.\n\nThe direct sum formula_17 comes equipped with a \"projection\" homomorphism formula_60 for each \"j\" and a \"coprojection\" formula_61 for each \"j\". Given another algebraic object \"B\" (with the same additional structure) and homomorphisms formula_62 for every \"j\", there is a unique homomorphism formula_63 (called the sum of the \"g\") such that formula_64 for all \"j\". Thus the direct sum is the coproduct in the appropriate category.\n\n"}
{"id": "76988", "url": "https://en.wikipedia.org/wiki?curid=76988", "title": "Electrocardiography", "text": "Electrocardiography\n\nElectrocardiography (ECG or EKG) is the process of recording the electrical activity of the heart over a period of time using electrodes placed over the skin. These electrodes detect the tiny electrical changes on the skin that arise from the heart muscle's electrophysiologic pattern of depolarizing and repolarizing during each heartbeat. It is very commonly performed to detect any cardiac problems.\n\nIn a conventional 12-lead ECG, ten electrodes are placed on the patient's limbs and on the surface of the chest. The overall magnitude of the heart's electrical potential is then measured from twelve different angles (\"leads\") and is recorded over a period of time (usually ten seconds). In this way, the overall magnitude and direction of the heart's electrical depolarization is captured at each moment throughout the cardiac cycle. The graph of voltage versus time produced by this noninvasive medical procedure is an electrocardiogram.\n\nThere are three main components to an ECG: the P wave, which represents the depolarization of the atria; the QRS complex, which represents the depolarization of the ventricles; and the T wave, which represents the repolarization of the ventricles. It can also be further broken down into the following:\n\n\nDuring each heartbeat, a healthy heart has an orderly progression of depolarization that starts with pacemaker cells in the sinoatrial node, spreads throughout the atrium, passes through the atrioventricular node down into the bundle of His and into the Purkinje fibers, spreading down and to the left throughout the ventricles. This orderly pattern of depolarization gives rise to the characteristic ECG tracing. To the trained clinician, an ECG conveys a large amount of information about the structure of the heart and the function of its electrical conduction system. Among other things, an ECG can be used to measure the rate and rhythm of heartbeats, the size and position of the heart chambers, the presence of any damage to the heart's muscle cells or conduction system, the effects of heart drugs, and the function of implanted pacemakers.\n\nThe overall goal of performing an ECG is to obtain information about the structure and function of the heart. Medical uses for this information are varied and generally need knowledge of the structure and/or function of the heart to be interpreted. Some indications for performing an ECG include:\n\n\nThe United States Preventive Services Task Force does not recommend an ECG for routine screening in patients without symptoms and those at low risk for coronary artery disease. This is because an ECG may falsely indicate the existence of a problem, leading to misdiagnosis, the recommendation of invasive procedures, or overtreatment. However, persons employed in certain critical occupations, such as aircraft pilots, may be required to have an ECG as part of their routine health evaluations.\n\n\"Continuous\" ECG monitoring is used to monitor critically ill patients, patients undergoing general anesthesia, and patients who have an infrequently occurring cardiac arrhythmia that would unlikely be seen on a conventional ten-second ECG.\n\nIn the United States, a 12-lead ECG is commonly performed by specialized technicians that may be certified as electrocardiogram technicians.\nECG interpretation is a component of many healthcare fields (nurses and physicians and cardiac surgeons being the most obvious), but anyone trained to interpret an ECG is free to do so.\nHowever, \"official\" interpretation is performed by a cardiologist.\nCertain fields such as anesthesia utilize continuous ECG monitoring, and knowledge of interpreting ECGs is crucial to their jobs.\n\nOne additional form of ECG is used in clinical cardiac electrophysiology in which a catheter is used to measure the electrical activity.\nThe catheter is inserted through the femoral vein and can have several electrodes along its length to record the direction of electrical activity from within the heart.\n\nEvidence does not support the use of ECGs among those without symptoms or at low risk of cardiovascular disease as an effort for prevention.\n\nAn electrocardiograph is a machine that is used to perform electrocardiography, and produces the electrocardiogram.\nThe first electrocardiographs are discussed later and are electrically primitive compared to today's machines.\n\nThe fundamental component to an ECG is the instrumentation amplifier, which is responsible for taking the voltage difference between leads (see below) and amplifying the signal.\nECG voltages measured across the body are on the order of hundreds of microvolts up to 1 millivolt (the small square on a standard ECG is 100 microvolts).\nThis low voltage necessitates a low noise circuit and instrumentation amplifiers.\n\nEarly ECGs were constructed with analog electronics and the signal could drive a motor to print the signal on paper.\nToday, electrocardiographs use analog-to-digital converters to convert to a digital signal that can then be manipulated with digital electronics.\nThis permits digital recording of ECGs and use on computers.\n\nThere are other components to the ECG:\n\nThe typical design for a portable ECG is a combined unit that includes a screen, keyboard, and printer on a small wheeled cart.\nThe unit connects to a long cable that branches to each lead and attaches to a conductive pad on the patient.\n\nThe ECG may include a rhythm analysis algorithm that produces a computerized interpretation of the ECG.\nThe results from these algorithms are considered \"preliminary\" until verified and/or modified by someone trained in interpreting ECGs.\nIncluded in this analysis is computation of common parameters that include PR interval, QT interval, corrected QT (QTc) interval, PR axis, QRS axis, and more.\nEarlier designs recorded each lead sequentially but current designs employ circuits that can record all leads simultaneously.\nThe former introduces problems in interpretation since there may be beat-to-beat changes in the rhythm, which makes it unwise to compare across beats.\n\nMore recent advancements in electrocardiography include work in diminishing the size of the unit to make it more portable and therefore more accessible to larger groups of patients. To achieve this, these smaller devices rely on only two electrodes which together deliver \"lead I\" of the standard ECG.\n\nElectrodes are the actual conductive pads attached to the body surface. Any pair of electrodes can measure the electrical potential difference between the two corresponding locations of attachment. Such a pair forms \"a lead\". However, \"leads\" can also be formed between a physical electrode and a \"virtual electrode,\" known as \"the Wilson's central terminal\", whose potential is defined as the average potential measured by three limb electrodes that are attached to the right arm, the left arm, and the left foot, respectively. \n\nCommonly, 10 electrodes attached to the body are used to form 12 ECG leads, with each lead measuring a specific electrical potential difference (as listed in the table below).\n\nLeads are broken down into three types: limb; augmented limb; and precordial or chest. The 12-lead ECG has a total of three \"limb leads\" and three \"augmented limb leads\" arranged like spokes of a wheel in the coronal plane (vertical), and six \"precordial leads\" or \"chest leads\" that lie on the perpendicular transverse plane (horizontal).\n\nIn medical settings, the term \"leads\" is also sometimes used to refer to the electrodes themselves, although this is technically incorrect. This misuse of terminology can be the source of confusion.\n\nThe 10 electrodes in a 12-lead ECG are listed below.\n\nTwo types of electrodes in common use are a flat paper-thin sticker and a self-adhesive circular pad.\nThe former are typically used in a single ECG recording while the latter are for continuous recordings as they stick longer.\nEach electrode consists of an electrically conductive electrolyte gel and a silver/silver chloride conductor.\nThe gel typically contains potassium chloride – sometimes silver chloride as well – to permit electron conduction from the skin to the wire and to the electrocardiogram.\n\nThe common virtual electrode, known as the Wilson's central terminal (V), is produced by averaging the measurements from the electrodes RA, LA, and LL to give an average potential of the body:\nIn a 12-lead ECG, all leads except the limb leads are unipolar (aVR, aVL, aVF, V, V, V, V, V, and V).\nThe measurement of a voltage requires two contacts and so, electrically, the unipolar leads are measured from the common lead (negative) and the unipolar lead (positive).\nThis averaging for the common lead and the abstract unipolar lead concept makes for a more challenging understanding and is complicated by sloppy usage of \"lead\" and \"electrode.\"\n\nLeads I, II and III are called the \"limb leads\". The electrodes that form these signals are located on the limbs – one on each arm and one on the left leg. The limb leads form the points of what is known as Einthoven's triangle.\n\n\n\nLeads aVR, aVL, and aVF are the \"augmented limb leads\". They are derived from the same three electrodes as leads I, II, and III, but they use Goldberger's central terminal as their negative pole. Goldberger's central terminal is a combination of inputs from two limb electrodes, with a different combination for each augmented lead. It is referred to immediately below as \"the negative pole.\"\n\n\n\n\nTogether with leads I, II, and III, augmented limb leads aVR, aVL, and aVF form the basis of the hexaxial reference system, which is used to calculate the heart's electrical axis in the frontal plane.\n\nThe \"precordial leads\" lie in the transverse (horizontal) plane, perpendicular to the other six leads. The six precordial electrodes act as the positive poles for the six corresponding precordial leads: (V, V, V, V, V and V). Wilson's central terminal is used as the negative pole.\n\nAdditional electrodes may rarely be placed to generate other leads for specific diagnostic purposes. \"Right-sided\" precordial leads may be used to better study pathology of the right ventricle or for dextrocardia (and are denoted with an R (e.g., V). \"Posterior leads\" (V to V) may be used to demonstrate the presence of a posterior myocardial infarction. A \"Lewis lead\" (requiring an electrode at the right sternal border in the second intercostal space) can be used to study pathological rhythms arising in the right atrium.\n\nAn \"esophogeal lead\" can be inserted to a part of the esophagus where the distance to the posterior wall of the left atrium is only approximately 5–6 mm (remaining constant in people of different age and weight). An esophageal lead avails for a more accurate differentiation between certain cardiac arrhythmias, particularly atrial flutter, AV nodal reentrant tachycardia and orthodromic atrioventricular reentrant tachycardia. It can also evaluate the risk in people with Wolff-Parkinson-White syndrome, as well as terminate supraventricular tachycardia caused by re-entry.\n\nAn intracardiac electrogram (ICEG) is essentially an ECG with some added \"intracardiac leads\" (that is, inside the heart). The standard ECG leads (external leads) are I, II, III, aVL, V, and V. Two to four intracardiac leads are added via cardiac catheterization. The word \"electrogram\" (EGM) without further specification usually means an intracardiac electrogram.\n\nA standard 12-lead ECG report (an electrocardiograph) shows a 2.5 second tracing of each of the twelve leads. The tracings are most commonly arranged in a grid of four columns and three rows. the first column is the limb leads (I, II, and III), the second column is the augmented limb leads (aVR, aVL, and aVF), and the last two columns are the precordial leads (V to V).\nAdditionally, a rhythm strip may be included as a fourth or fifth row.\n\nThe timing across the page is continuous and not tracings of the 12 leads for the same time period.\nIn other words, if the output were traced by needles on paper, each row would switch which leads as the paper is pulled under the needle.\nFor example, the top row would first trace lead I, then switch to lead aVR, then switch to V, and then switch to V and so none of these four tracings of the leads are from the same time period as they are traced in sequence through time.\n\nEach of the 12 ECG leads records the electrical activity of the heart from a different angle, and therefore align with different anatomical areas of the heart. Two leads that look at neighboring anatomical areas are said to be \"contiguous\".\n\nIn addition, any two precordial leads next to one another are considered to be contiguous. For example, though V is an anterior lead and V is a lateral lead, they are contiguous because they are next to one another.\n\nThe formal study of the electrical conduction system of the heart is called cardiac electrophysiology (EP).\nAn electrophysiology study involves a formal study of the conduction system and can be done for various reasons.\nDuring such a study, catheters are used to access the heart and some of these catheters include electrodes that can be placed anywhere in the heart to record the electrical activity from within the heart.\nSome catheters contain several electrodes and can record the propagation of electrical activity.\n\nInterpretation of the ECG is fundamentally about understanding the electrical conduction system of the heart.\nNormal conduction starts and propagates in a predictable pattern, and deviation from this pattern can be a normal variation or be pathological.\nAn ECG does not equate with mechanical pumping activity of the heart, for example, pulseless electrical activity produces an ECG that should pump blood but no pulses are felt (and constitutes a medical emergency and CPR should be performed).\nVentricular fibrillation produces an ECG but is too dysfunctional to produce a life-sustaining cardiac output. Certain rhythms are known to have good cardiac output and some are known to have bad cardiac output.\nUltimately, an echocardiogram or other anatomical imaging modality is useful in assessing the mechanical function of the heart.\n\nLike all medical tests, what constitutes \"normal\" is based on population studies. The heartrate range of between 60 and 100 beats per minute (bpm) is considered normal since data shows this to be the usual resting heart rate.\n\nInterpretation of the ECG is ultimately that of pattern recognition.\nIn order to understand the patterns found, it is helpful to understand the theory of what ECGs represent.\nThe theory is rooted in electromagnetics and boils down to the four following points:\n\nThus, the overall direction of depolarization and repolarization produces a vector that produces positive or negative deflection on the ECG depending on which lead it points to.\nFor example, depolarizing from right to left would produce a positive deflection in lead I because the two vectors point in the same direction.\nIn contrast, that same depolarization would produce minimal deflection in V and V because the vectors are perpendicular and this phenomenon is called isoelectric.\n\nNormal rhythm produces four entities – a P wave, a QRS complex, a T wave, and a U wave – that each have a fairly unique pattern.\n\nHowever, the U wave is not typically seen and its absence is generally ignored.\nChanges in the structure of the heart and its surroundings (including blood composition) change the patterns of these four entities.\n\nECGs are normally printed on a grid.\nThe horizontal axis represents time and the vertical axis represents voltage.\nThe standard values on this grid are shown in the adjacent image:\n\nThe \"large\" box is represented by a heavier line weight than the small boxes.\n\nNot all aspects of an ECG rely on precise recordings or having a known scaling of amplitude or time.\nFor example, determining if the tracing is a sinus rhythm only requires feature recognition and matching, and not measurement of amplitudes or times (i.e., the scale of the grids are irrelevant).\nAn example to the contrary, the voltage requirements of left ventricular hypertrophy require knowing the grid scale.\n\nIn a normal heart, the heart rate is the rate in which the sinoatrial node depolarizes as it is the source of depolarization of the heart.\nHeart rate, like other vital signs like blood pressure and respiratory rate, change with age.\nIn adults, a normal heart rate is between 60 and 100 bpm (normocardic) where in children it is higher.\nA heart rate less than normal is called bradycardia (<60 in adults) and higher than normal is tachycardia (>100 in adults).\nA complication of this is when the atria and ventricles are not in synchrony and the \"heart rate\" must be specified as atrial or ventricular (e.g., the ventricular rate in ventricular fibrillation is 300–600 bpm, whereas the atrial rate can be normal [60–100] or faster [100–150]).\n\nIn normal resting hearts, the physiologic rhythm of the heart is normal sinus rhythm (NSR).\nNormal sinus rhythm produces the prototypical pattern of P wave, QRS complex, and T wave.\nGenerally, deviation from normal sinus rhythm is considered a cardiac arrhythmia.\nThus, the first question in interpreting an ECG is whether or not there is a sinus rhythm.\nA criterion for sinus rhythm is that P waves and QRS complexes appear 1-to-1, thus implying that the P wave causes the QRS complex.\n\nOnce sinus rhythm is established, or not, the second question is the rate.\nFor a sinus rhythm this is either the rate of P waves or QRS complexes since they are 1-to-1.\nIf the rate is too fast then it is sinus tachycardia and if it is too slow then it is sinus bradycardia.\n\nIf it is not a sinus rhythm, then determining the rhythm is necessary before proceeding with further interpretation.\nSome arrhythmias with characteristic findings:\n\nDetermination of rate and rhythm is necessary in order to make sense of further interpretation.\n\nThe heart has several axes, but the most common by far is the axis of the QRS complex (references to \"the axis\" imply the QRS axis).\nEach axis can be computationally determined to result in a number representing degrees of deviation from zero, or it can be categorized into a few types.\n\nThe QRS axis is the general direction of the ventricular depolarization wavefront (or mean electrical vector) in the frontal plane.\nIt is often sufficient to classify the axis as one of three types: normal, left deviated, or right deviated.\nPopulation data shows that a normal QRS axis is from −30° to 105°, with 0° being along lead I and positive being inferior and negative being superior (best understood graphically as the hexaxial reference system).\nBeyond +105° is right axis deviation and beyond −30° is left axis deviation (the third quadrant of −90° to −180° is very rare and is an indeterminate axis).\nA shortcut for determining if the QRS axis is normal is if the QRS complex is mostly positive in lead I and lead II (or lead I and aVF if +90° is the upper limit of normal).\n\nThe normal QRS axis is generally \"down and to the left\", following the anatomical orientation of the heart within the chest. An abnormal axis suggests a change in the physical shape and orientation of the heart or a defect in its conduction system that causes the ventricles to depolarize in an abnormal way.\n\nThe extent of a normal axis can be +90° or 105° depending on the source.\n\nAll of the waves on an ECG tracing and the intervals between them have a predictable time duration, a range of acceptable amplitudes (voltages), and a typical morphology. Any deviation from the normal tracing is potentially pathological and therefore of clinical significance.\n\nFor ease of measuring the amplitudes and intervals, an ECG is printed on graph paper at a standard scale: each 1 mm (one small box on the standard ECG paper) represents 40 milliseconds of time on the x-axis, and 0.1 millivolts on the y-axis.\n\nIschemia or non-ST elevation myocardial infarctions (non-STEMIs) may manifest as ST depression or inversion of T waves. It may also affect the high frequency band of the QRS.\n\nST elevation myocardial infarctions (STEMIs) have different characteristic ECG findings based on the amount of time elapsed since the MI first occurred. The earliest sign is \"hyperacute T waves,\" peaked T waves due to local hyperkalemia in ischemic myocardium. This then progresses over a period of minutes to elevations of the ST segment by at least 1 mm. Over a period of hours, a pathologic Q wave may appear and the T wave will invert. Over a period of days the ST elevation will resolve. Pathologic Q waves generally will remain permanently.\n\nThe coronary artery that has been occluded can be identified in an STEMI based on the location of ST elevation. The left anterior descending (LAD) artery supplies the anterior wall of the heart, and therefore causes ST elevations in anterior leads (V and V). The LCx supplies the lateral aspect of the heart and therefore causes ST elevations in lateral leads (I, aVL and V). The right coronary artery (RCA) usually supplies the inferior aspect of the heart, and therefore causes ST elevations in inferior leads (II, III and aVF).\n\nAn ECG tracing is affected by patient motion. Some rhythmic motions (such as shivering or tremors) can create the illusion of cardiac arrhythmia. Artifacts are distorted signals caused by a secondary internal or external sources, such as muscle movement or interference from an electrical device.\n\nDistortion poses significant challenges to healthcare providers, who employ various techniques and strategies to safely recognize these false signals. Accurately separating the ECG artifact from the true ECG signal can have a significant impact on patient outcomes and legal liabilities.\n\nImproper lead placement (for example, reversing two of the limb leads) has been estimated to occur in 0.4% to 4% of all ECG recordings, and has resulted in improper diagnosis and treatment including unnecessary use of thrombolytic therapy.\n\nNumerous diagnoses and findings can be made based upon electrocardiography, and many are discussed above. Overall, the diagnoses are made based on the patterns. For example, an \"irregularly irregular\" QRS complex without P waves is the hallmark of atrial fibrillation; however, other findings can be present as well, such as a bundle branch block that alters the shape of the QRS complexes. ECGs can be interpreted in isolation but should be applied – like all diagnostic tests – in the context of the patient. For example, an observation of peaked T waves is not sufficient to diagnose hyperkalemia; such a diagnosis should be verified by measuring the blood potassium level. Conversely, a discovery of hyperkalemia should be followed by an ECG for manifestations such as peaked T waves, widened QRS complexes, and loss of P waves. The following is an organized list of possible ECG-based diagnoses.\n\nRhythm disturbances or arrhythmias:\n\nHeart block and conduction problems:\n\nElectrolytes disturbances and intoxication:\n\nIschemia and infarction:\n\nStructural:\n\nThe etymology of the word is derived from the Greek \"electro\", because it is related to electrical activity, \"kardia\", Greek for heart, and \"graph\", a Greek root meaning \"to write\".\n\nAlexander Muirhead is reported to have attached wires to a feverish patient's wrist to obtain a record of the patient's heartbeat in 1872 at St Bartholomew's Hospital. Another early pioneer was Augustus Waller, of St Mary's Hospital in London. His electrocardiograph machine consisted of a Lippmann capillary electrometer fixed to a projector. The trace from the heartbeat was projected onto a photographic plate that was itself fixed to a toy train. This allowed a heartbeat to be recorded in real time.\n\nAn initial breakthrough came when Willem Einthoven, working in Leiden, the Netherlands, used the string galvanometer (the first practical electrocardiograph) he invented in 1901. This device was much more sensitive than both the capillary electrometer Waller used and the string galvanometer that had been invented separately in 1897 by the French engineer Clément Ader. Einthoven had previously, in 1895, assigned the letters P, Q, R, S, and T to the deflections in the theoretical waveform he created using equations which corrected the actual waveform obtained by the capillary electrometer to compensate for the imprecision of that instrument. Using letters different from A, B, C, and D (the letters used for the capillary electrometer's waveform) facilitated comparison when the uncorrected and corrected lines were drawn on the same graph. Einthoven probably chose the initial letter P to follow the example set by Descartes in geometry. When a more precise waveform was obtained using the string galvanometer, which matched the corrected capillary electrometer waveform, he continued to use the letters P, Q, R, S, and T, and these letters are still in use today. Einthoven also described the electrocardiographic features of a number of cardiovascular disorders. In 1924, he was awarded the Nobel Prize in Medicine for his discovery.\n\nBy 1927, General Electric had developed a portable apparatus that could produce electrocardiograms without the use of the string galvanometer. This device instead combined amplifier tubes similar to those used in a radio with an internal lamp and a moving mirror that directed the tracing of the electric pulses onto film.\n\nIn 1937, Taro Takemi invented a new portable electrocardiograph machine.\n\nThough the basic principles of that era are still in use today, many advances in electrocardiography have been made over the years. Instrumentation has evolved from a cumbersome laboratory apparatus to compact electronic systems that often include computerized interpretation of the electrocardiogram.\n\nIn September 2018, Apple Inc., introduced the Apple Watch Series 4, with a built-in titanium electrode in the digital crown and the sapphire crystal electronic heart sensor, which allows the watch to give a single lead electrocardiogram using only the watch interface.\n\n\n"}
{"id": "24877937", "url": "https://en.wikipedia.org/wiki?curid=24877937", "title": "Euclid's Optics", "text": "Euclid's Optics\n\nEuclid's \"Optics\" (), is a work on the geometry of vision written by the Greek mathematician Euclid around 300 BC. The earliest surviving manuscript of \"Optics\" is in Greek and dates from the 10th century AD.\nThe work deals almost entirely with the geometry of vision, with little reference to either the physical or psychological aspects of sight. No Western scientist had previously given such mathematical attention to vision. Euclid's \"Optics\" influenced the work of later Greek, Islamic, and Western European Renaissance scientists and artists.\n\nWriters before Euclid had developed theories of vision. However, their works were mostly philosophical in nature and lacked the mathematics that Euclid introduced in his \"Optics\". Efforts by the Greeks prior to Euclid were concerned primarily with the physical dimension of vision. Whereas Plato and Empedocles thought of the visual ray as \"luminous and ethereal emanation\", Euclid’s treatment of vision in a mathematical way was part of the larger Hellenistic trend to quantify a whole range of scientific fields.\n\nBecause \"Optics\" contributed a new dimension to the study of vision, it influenced later scientists. In particular, Ptolemy used Euclid's mathematical treatment of vision and his idea of a visual cone in combination with physical theories in Ptolemy's \"Optics\", which has been called \"one of the most important works on optics written before Newton\". Renaissance artists such as Brunelleschi, Alberti, and Dürer used Euclid's \"Optics\" in their own work on linear perspective.\n\nSimilar to Euclid's much more famous work on geometry, \"Elements\", \"Optics\" begins with a small number of definitions and postulates, which are then used to prove, by deductive reasoning, a body of geometric propositions (theorems in modern terminology) about vision.\n\nThe postulates in \"Optics\" are:\n\nLet it be assumed\n1. That rectilinear rays proceeding from the eye diverge indefinitely;\n2. That the figure contained by a set of visual rays is a cone of which the vertex is at the eye and the base at the surface of the objects seen;\n3. That those things are seen upon which visuals rays fall and those things are not seen upon which visual rays do not fall;\n4. That things seen under a larger angle appear larger, those under a smaller angle appear smaller, and those under equal angles appear equal;\n5. That things seen by higher visual rays appear higher, and things seen by lower visual rays appear lower;\n6. That, similarly, things seen by rays further to the right appear further to the right, and things seen by rays further to the left appear further to the left;\n7. That things seen under more angles are seen more clearly.\n\nThe geometric treatment of the subject follows the same methodology as the \"Elements\".\n\nAccording to Euclid, the eye sees objects that are within its visual cone. The visual cone is made up of straight lines, or visual rays, extending outward from the eye. These visual rays are discrete, but we perceive a continuous image because our eyes, and thus our visual rays, move very quickly. Because visual rays are discrete, however, it is possible for small objects to lie unseen between them. This accounts for the difficulty in searching for a dropped needle. Although the needle may be within one's field of view, until the eye's visual rays fall upon the needle, it will not be seen. Discrete visual rays also explain the sharp or blurred appearance of objects. According to postulate 7, the closer an object, the more visual rays fall upon it and the more detailed or sharp it appears. This is an early attempt to describe the phenomenon of optical resolution.\n\nMuch of the work considers perspective, how an object appears in space relative to the eye. For example, in proposition 8, Euclid argues that the perceived size of an object is not related to its distance from the eye by a simple proportion.\n\n"}
{"id": "20994389", "url": "https://en.wikipedia.org/wiki?curid=20994389", "title": "European Symposium on Algorithms", "text": "European Symposium on Algorithms\n\nThe European Symposium on Algorithms (ESA) is an international conference covering the field of algorithms. It has been held annually since 1993, typically in early Autumn in a different European location each year. Like most theoretical computer science conferences its contributions are strongly peer-reviewed; the articles appear in proceedings published in Springer Lecture Notes in Computer Science. Acceptance rate of ESA is 24% in 2012 in both \"Design and Analysis\" and \"Engineering and Applications\" tracks.\n\nThe first ESA was held in 1993 and contained 35 papers. The intended scope was all research in algorithms, theoretical as well as applied, carried out in the fields of computer science and discrete mathematics. An explicit aim was to intensify the exchange between these two research communities.\n\nIn 2002, ESA incorporated the conference Workshop on Algorithms Engineering (WAE). In its current format, ESA contains two distinct tracks with their own programme committees: a track on the design an analysis of algorithms, and a track on engineering and applications, together accepting around 70 contributions.\n\nSince 2001, ESA is co-located with other algorithms conferences and workshops in a combined meeting called ALGO. This is the largest European event devoted to algorithms, attracting hundreds of researchers.\n\nOther events in the ALGO conferences include the following.\n\nATMOS was co-located with the International Colloquium on Automata, Languages and Programming (ICALP) in 2001–2002.\n\n\n"}
{"id": "98316", "url": "https://en.wikipedia.org/wiki?curid=98316", "title": "Flexagon", "text": "Flexagon\n\nIn geometry, flexagons are flat models, usually constructed by folding strips of paper, that can be \"flexed\" or folded in certain ways to reveal faces besides the two that were originally on the back and front.\n\nFlexagons are usually square or rectangular (tetraflexagons) or hexagonal (hexaflexagons). A prefix can be added to the name to indicate the number of faces that the model can display, including the two faces (back and front) that are visible before flexing. For example, a hexaflexagon with a total of six faces is called a hexahexaflexagon.\n\nIn hexaflexagon theory (that is, concerning flexagons with six sides), flexagons are usually defined in terms of \"pats\".\n\nTwo flexagons are equivalent if one can be transformed to the other by a series of pinches and rotations. Flexagon equivalence is an equivalence relation.\n\nThe discovery of the first flexagon, a trihexaflexagon, is credited to the British student Arthur H. Stone, who was studying at Princeton University in the United States in 1939. His new American paper would not fit in his English binder so he cut off the ends of the paper and began folding them into different shapes. One of these formed a trihexaflexagon. Stone's colleagues Bryant Tuckerman, Richard Feynman, and John Tukey became interested in the idea and formed the Princeton Flexagon Committee. Tuckerman worked out a topological method, called the Tuckerman traverse, for revealing all the faces of a flexagon.\n\nFlexagons were introduced to the general public by the recreational mathematician Martin Gardner in 1956 in the first \"Mathematical Games\" column which he wrote for \"Scientific American\" magazine. In 1974, the magician Doug Henning included a construct-your-own hexaflexagon with the original cast recording of his Broadway show \"The Magic Show\".\n\nIn 1955, Russell Rogers and Leonard D'Andrea of Homestead Park, Pennsylvania applied for a patent, and in 1959 they were granted U.S. Patent number 2,883,195 for the hexahexaflexagon, under the title \"Changeable Amusement Devices and the Like.\"\n\nTheir patent imagined possible applications of the device \"as a toy, as an advertising display device, or as an educational geometric device.\" A few such novelties were produced by the Herbick & Held Printing Company, the printing company in Pittsburgh where Rogers worked, but the device, marketed as the \"Hexmo\", failed to catch on.\n\nThe tritetraflexagon is the simplest tetraflexagon (flexagon with square sides). The \"tri\" in the name means it has three faces, two of which are visible at any given time if the flexagon is pressed flat. The construction of the tritetraflexagon is similar to the mechanism used in the traditional Jacob's Ladder children's toy, in Rubik's Magic\nand in the magic wallet trick or the Himber wallet.\n\nA more complicated cyclic hexatetraflexagon requires no gluing. A cyclic hexatetraflexagon does not have any \"dead ends\", but the person making it can keep folding it until they reach the starting position. If the sides are colored in the process, the states can be seen more clearly.\n\nHexaflexagons come in great variety, distinguished by the number of faces that can be achieved by flexing the assembled figure. (Note that the word \"hexaflexagons\" (with no prefixes) can sometimes refer to an ordinary hexahexaflexagon, with six sides instead of other numbers.)\n\nA hexaflexagon with three faces. This is the simplest of the hexaflexagons to make and to manage, and is made from a single strip of paper, divided into nine equilateral triangles. (Some patterns provide ten triangles, two of which are glued together in the final assembly.)\n\nTo assemble, the strip is folded every third triangle, connecting back to itself after three inversions in the manner of the international recycling symbol. This makes a Möbius strip whose single edge forms a trefoil knot.\n\nThis hexaflexagon has six faces. It is made up of nineteen triangles folded from a strip of paper.\nPhotos 1-6 below show the construction of a hexaflexagon made out of cardboard triangles on a backing made from a strip of cloth. It has been decorated in six colors; orange, blue, and red in figure 1 correspond to 1, 2, and 3 in the diagram above. The opposite side, figure 2, is decorated with purple, gray, and yellow. Note the different patterns used for the colors on the two sides. Figure 3 shows the first fold, and figure 4 the result of the first nine folds, which form a spiral. Figures 5-6 show the final folding of the spiral to make a hexagon; in 5, two red faces have been hidden by a valley fold, and in 6, two red faces on the bottom side have been hidden by a mountain fold. After figure 6, the final loose triangle is folded over and attached to the other end of the original strip so that one side is all blue, and the other all orange.\n\nPhotos 7 and 8 show the process of everting the hexaflexagon to show the formerly hidden red triangles. By further manipulations, all six colors can be exposed. Faces 1, 2, and 3 are easier to find while faces 4, 5, and 6 are more difficult to find. An easy way to expose all six faces is using the Tuckerman traverse. It's named after Bryant Tuckerman, one of the first to investigate the properties of hexaflexagons. The Tuckerman traverse involves the repeated flexing by pinching one corner and flex from exactly the same corner every time. If the corner refuses to open, move to an adjacent corner and keep flexing. This procedure brings you to a 12-face cycle. During this procedure, however, 1, 2, and 3 show up three times as frequently as 4, 5, and 6. The cycle proceeds as follows:\n\n1-3-6-1-3-2-4-3-2-1-5-2\n\nAnd then back to 1 again.\n\nEach color/face can also be exposed in more than one way. In figure 6, for example, each blue triangle has at the center its corner decorated with a wedge, but it is also possible, for example, to make the ones decorated with Y's come to the center. There are 18 such possible configurations for triangles with different colors, and they can be seen by flexing the hexahexaflexagon in all possible ways in theory, but only 15 can be flexed by the ordinary hexahexaflexagon. The 3 extra configurations are impossible due to the arrangement of the 4, 5, and 6 tiles at the back flap. (The 60-degree angles in the rhombi formed by the adjacent 4, 5, or 6 tiles will only appear on the sides and never will appear at the center because it would require one to cut the strip, which is topologically forbidden.)\n\nHexahexaflexagons can be constructed from different shaped nets of eighteen equilateral triangles. One hexahexaflexagon, constructed from an irregular paper strip, is almost identical to the one shown above, except that all 18 configurations can be flexed on this version.\n\nWhile the most commonly seen hexaflexagons have either three or six faces, variations exist with four, five, seven, twelve, twenty-four, and forty-eight faces.\n\nIn these more recently discovered flexagons, each square or equilateral triangular face of a conventional flexagon is further divided into two right triangles, permitting additional flexing modes. The division of the square faces of tetraflexagons into right isosceles triangles yields the octaflexagons, and the division of the triangular faces of the hexaflexagons into 30-60-90 right triangles yields the dodecaflexagons.\n\nIn its flat state, the pentaflexagon looks much like the Chrysler logo: a regular pentagon divided from the center into five isosceles triangles, with angles 72-54-54. Because of its fivefold symmetry, the pentaflexagon cannot be folded in half. However, a complex series of flexes results in its transformation from displaying sides one and two on the front and back, to displaying its previously hidden sides three and four.\n\nBy further dividing the 72-54-54 triangles of the pentaflexagon into 36-54-90 right triangles produces one variation of the 10-sided decaflexagon.\n\nThe pentaflexagon is one of an infinite sequence of flexagons based on dividing a regular \"n\"-gon into \"n\" isosceles triangles. Other flexagons include the heptaflexagon, the isosceles octaflexagon, the enneaflexagon, and others.\n\nHarold V. McIntosh also describes \"nonplanar\" flexagons (i.e., ones which cannot be flexed so they lie flat); ones folded from pentagons called \"pentaflexagons\", and from heptagons called \"heptaflexagons\". These should be distinguished from the \"ordinary\" pentaflexagons and heptaflexagons described above, which are made out of isosceles triangles, and they \"can\" be made to lie flat.\n\nFlexagons are also a popular book structure used by artist's book creators such as Julie Chen (\"Life Cycle\") and Edward H. Hutchins (\"Album\" and \"Voces de México\"). Instructions for making tetra-tetra-flexagon and cross-flexagons are included in \"Making Handmade Books: 100+ Bindings, Structures and Forms\" by Alisa Golden.\n\nA high-order hexaflexagon was used as a plot element in Piers Anthony's novel \"\", in which a flex was analogous to the travel between alternate universes.\n\n\n\nFlexagons:\n\nTetraflexagons:\n\nHexaflexagons:\n"}
{"id": "27506488", "url": "https://en.wikipedia.org/wiki?curid=27506488", "title": "FreeFlyer", "text": "FreeFlyer\n\nFreeFlyer is a commercial off-the-shelf software application for use in satellite mission analysis, design and operations. FreeFlyer's architecture centers on its native scripting language, known as FreeForm script. As a mission planning tool, it encompasses several capabilities, including precise orbit modeling, 2D and 3D visualization, sensor modeling, maneuver modeling, maneuver estimation, plotting, orbit determination, tracking data simulation, and space environment modeling.\n\nFreeFlyer implements standard astrodynamics models such as the JGM-2, EGM-96, LP-165 gravity potential models; the Jacchia-Roberts, Harris-Priester, and NRL-MSIS atmospheric density models; the International Reference Ionosphere model; and the International Geomagnetic Reference Field magnetic field model.\n\na.i. solutions, Inc. is the owner and developer of FreeFlyer which has been in use since 1997. FreeFlyer is utilized by NASA, NOAA, and the USAF for space mission operations, mission assurance, and analysis support.\n\nFreeFlyer has been used to support many spacecraft missions, for mission planning analysis, operational analysis, or both. Specific mission examples include the International Space Station (ISS), the JSpOC Mission System, the Earth Observing System, Solar Dynamics Observatory (SDO), and Magnetospheric Multiscale Mission (MMS).\n\nFreeFlyer has also been successfully used to conduct analysis in both the high-performance computing (HPC) and service-oriented architecture (SOA) environments.\n\nFreeFlyer is one stand-alone product, with no added modules. However, it has two tiers of rising functionality.\n\nThe FreeFlyer Engineer and Mission tiers contain an integrated scripting language and development environment. The scripting language is an object-oriented script with objects and commands. Objects include properties and methods.\n\nAn example of FreeFlyer scripting is this:\n\n"}
{"id": "25021971", "url": "https://en.wikipedia.org/wiki?curid=25021971", "title": "Fréchet mean", "text": "Fréchet mean\n\nIn mathematics and statistics, the Fréchet mean is a generalization of centroids to metric spaces, giving a single representative point or central tendency for a cluster of points. It is named after Maurice Fréchet. Karcher mean is the renaming of the Riemannian Center of Mass construction developed by Karsten Grove and Hermann Karcher. On the real numbers, the arithmetic mean, median, geometric mean, and harmonic mean can all be interpreted as Fréchet means for different distance functions.\n\nLet (\"M\", \"d\") be a complete metric space. Let \"x\", \"x\", …, \"x\" be random points in \"M\". For any point \"p\" in \"M\", define the Fréchet variance to be the sum of squared distances from \"p\" to the \"x\":\n\nThe Karcher means are then those points, \"m\" of \"M\", which locally minimise Ψ:\n\nIf there is an \"m\" of \"M\" that globally minimises Ψ, then it is Fréchet mean.\n\nSometimes, the \"x\" are assigned weights \"w\", the Fréchet variance is calculated at a weighted sum,\n\nFor real numbers, the arithmetic mean is a Fréchet mean, using the usual Euclidean distance as the distance function. The median is also a Fréchet mean, using the square root of the distance.\n\nOn the positive real numbers, the (hyperbolic) distance function formula_4 can be defined. The geometric mean is the corresponding Fréchet mean. Indeed formula_5 is then an isometry from the euclidean space to this \"hyperbolic\" space and must respect the Fréchet mean: the Fréchet mean of the formula_6 is the image by formula_7 of the Fréchet mean (in the Euclidean sense) of the formula_8, i.e. it must be:\n\nOn the positive real numbers, the metric (distance function):\n\ncan be defined. The harmonic mean is the corresponding Fréchet mean.\n\nGiven a non-zero real number formula_11, the power mean can be obtained as a Fréchet mean by introducing the metric\n\nGiven an invertible function formula_7, the f-mean can be defined as the Fréchet mean obtained by using the metric:\n\nThis is sometimes called the generalised f-mean or quasi-arithmetic mean.\n\nThe general definition of the Fréchet mean that includes the possibility of weighting observations can be used to derive weighted versions for all of the above types of means.\n"}
{"id": "578327", "url": "https://en.wikipedia.org/wiki?curid=578327", "title": "Future value", "text": "Future value\n\nFuture value is the value of an asset at a specific date. It measures the nominal future sum of money that a given sum of money is \"worth\" at a specified time in the future assuming a certain interest rate, or more generally, rate of return; it is the present value multiplied by the accumulation function.\nThe value does not include corrections for inflation or other factors that affect the true value of money in the future. This is used in time value of money calculations.\n\nMoney value fluctuates over time: $100 today has a different value than $100 in five years. This is because one can invest $100 today in an interest-bearing bank account or any other investment, and that money will grow/shrink due to the rate of return. Also, if $100 today allows the purchase of an item, it is possible that $100 will not be enough to purchase the same item in five years, because of inflation (increase in purchase price).\n\nAn investor who has some money has two options: to spend it right now or to invest it. The financial compensation for saving it (and not spending it) is that the money value will accrue through the interests that he will receive from a borrower (the bank account on which he has the money deposited).\n\nTherefore, to evaluate the real worthiness of an amount of money today after a given period of time, economic agents compound the amount of money at a given interest rate. Most actuarial calculations use the risk-free interest rate which corresponds the minimum guaranteed rate provided the bank's saving account, for example. If one wants to compare their change in purchasing power, then they should use the real interest rate (nominal interest rate minus inflation rate).\n\nThe operation of evaluating a present value into the future value is called capitalization (how much will $100 today be worth in 5 years?). The reverse operation which consists in evaluating the present value of a future amount of money is called a discounting (how much $100 that will be received in 5 years- at a lottery, for example -are worth today?).\n\nIt follows that if one has to choose between receiving $100 today and $100 in one year, the rational decision is to cash the $100 today. If the money is to be received in one year and assuming the savings account interest rate is 5%, the person has to be offered at least $105 in one year so that two options are equivalent (either receiving $100 today or receiving $105 in one year). This is because if you have cash of $100 today and deposit in your savings account, you will have $105 in one year.\n\nTo determine future value (FV) using simple interest (i.e., without compounding):\n\nwhere \"PV\" is the present value or principal, \"t\" is the time in years (or a fraction of year), and \"r\" stands for the per annum interest rate. Simple interest is rarely used, as compounding is considered more meaningful . Indeed, the Future Value in this case grows linearly (it's a linear function of the initial investment): it doesn't take into account the fact that the interest earned might be compounded itself and produce further interest (which corresponds to an exponential growth of the initial investment -see below-).\nTo determine future value using compound interest:\n\nwhere \"PV\" is the present value, \"t\" is the number of compounding periods (not necessarily an integer), and \"i\" is the interest rate for that period. Thus the future value increases exponentially with time when \"i\" is positive. The growth rate is given by the period, and \"i\", the interest rate for that period. Alternatively the growth rate is expressed by the interest per unit time based on continuous compounding. For example, the following all represent the same growth rate:\n\nAlso the growth rate may be expressed in a percentage per period (nominal rate), with another period as compounding basis; for the same growth rate we have:\n\nTo convert an interest rate from one compounding basis to another compounding basis (between different periodic interest rates), the following formula applies:\n\nwhere\n\"i\" is the periodic interest rate with compounding frequency \"n\" and\n\"i\" is the periodic interest rate with compounding frequency \"n\".\n\nIf the compounding frequency is annual, \"n\" will be 1, and to get the annual interest rate (which may be referred to as the effective interest rate, or the annual percentage rate), the formula can be simplified to:\n\nwhere \"r\" is the annual rate, \"i\" the periodic rate, and \"n\" the number of compounding periods per year.\n\nProblems become more complex as you account for more variables. For example, when accounting for annuities (annual payments), there is no simple \"PV\" to plug into the equation. Either the \"PV\" must be calculated first, or a more complex annuity equation must be used. Another complication is when the interest rate is applied multiple times per period. For example, suppose the 10% interest rate in the earlier example is compounded twice a year (semi-annually). Compounding means that each successive application of the interest rate applies to all of the previously accumulated amount, so instead of getting 0.05 each 6 months, one must figure out the true annual interest rate, which in this case would be 1.1025 (one would divide the 10% by two to get 5%, then apply it twice: 1.05.) This 1.1025 represents the original amount 1.00 plus 0.05 in 6 months to make a total of 1.05, and get the same rate of interest on that 1.05 for the remaining 6 months of the year. The second six-month period returns more than the first six months because the interest rate applies to the accumulated interest as well as the original amount.\n\nThis formula gives the future value (FV) of an ordinary annuity (assuming compound interest):\n\nwhere \"r\" = interest rate; \"n\" = number of periods. The simplest way to understand the above formula is to cognitively split the right side of the equation into two parts, the payment amount, and the ratio of compounding over basic interest. The ratio of compounding is composed of the aforementioned effective interest rate over the basic (nominal) interest rate. This provides a ratio that increases the payment amount in terms present value.\n\n\n"}
{"id": "14394227", "url": "https://en.wikipedia.org/wiki?curid=14394227", "title": "Generalized Ozaki cost function", "text": "Generalized Ozaki cost function\n\nIn economics the generalized-Ozaki cost is a general description of cost described by Shuichi Nakamura.\n\nFor output \"y\", at date \"t\" and a vector of \"m\" input prices \"p\", the generalized-Ozaki cost, \"c\", is\n\nIn econometrics it is often desirable to have a model of the cost of production of a given output with given inputs—or in common terms, what it will cost to produce some number of goods at prevailing prices, or given prevailing prices and a budget, how much can be made. Generally there are two parts to a cost function, the fixed and variable costs involved in production.\n\nThe marginal cost is the change in the cost of production for a single unit. Most cost functions then take the price of the inputs and adjust for different factors of production, typically, technology, economies of scale, and elasticities of inputs.\n\nTraditional cost functions include Cobb–Douglas and the constant elasticity of substitution models. These are still used because for a wide variety of activities, effects such as varying ability to substitute materials does not change. For example, for people running a bake sale, the ability to substitute one kind of chocolate chip for another will not vary over the number of cookies they can bake. However, as economies of scale and changes in substitution become important models that handle these effects become more useful, such as the transcendental log cost function.\n\nThe traditional forms are economically homothetic. This means they can be expressed as a function, and that function can be broken into an outer part and an inner part. The inner part will appear once as a term in the outer part, and the inner part will be monotonically increasing, or to say it another way, it never goes down. However, empirically in the areas of trade and production, homoethetic and monolithic functional models do not accurately predict results. One example is in the gravity equation for trade, or how much will two countries trade with each other based on GDP and distance. This led researchers to explore non-homothetic models of production, to fit with a cross section analysis of producer behavior, for example, when producers would begin to minimize costs by switching inputs, or investing in increased production.\n"}
{"id": "9607933", "url": "https://en.wikipedia.org/wiki?curid=9607933", "title": "Handshaking lemma", "text": "Handshaking lemma\n\nIn graph theory, a branch of mathematics, the handshaking lemma is the statement that every finite undirected graph has an even number of vertices with odd degree (the number of edges touching the vertex). In more colloquial terms, in a party of people some of whom shake hands, an even number of people must have shaken an odd number of other people's hands.\n\nThe handshaking lemma is a consequence of the degree sum formula (also sometimes called the handshaking lemma),\nfor a graph with vertex set \"V\" and edge set \"E\". Both results were proven by in his famous paper on the Seven Bridges of Königsberg that began the study of graph theory.\n\nThe vertices of odd degree in a graph are sometimes called odd nodes or odd vertices; in this terminology, the handshaking lemma can be restated as the statement that every graph has an even number of odd nodes.\n\nEuler's proof of the degree sum formula uses the technique of double counting: he counts the number of incident pairs (\"v\",\"e\") where \"e\" is an edge and vertex \"v\" is one of its endpoints, in two different ways. Vertex \"v\" belongs to deg(\"v\") pairs, where deg(\"v\") (the degree of \"v\") is the number of edges incident to it. Therefore, the number of incident pairs is the sum of the degrees. However, each edge in the graph belongs to exactly two incident pairs, one for each of its endpoints; therefore, the number of incident pairs is 2|\"E\"|. Since these two formulas count the same set of objects, they must have equal values.\n\nIn a sum of integers, the parity of the sum is not affected by the even terms in the sum; the overall sum is even when there is an even number of odd terms, and odd when there is an odd number of odd terms. Since one side of the degree sum formula is the even number 2|\"E\"|, the sum on the other side must have an even number of odd terms; that is, there must be an even number of odd-degree vertices.\n\nAlternatively, it is possible to use mathematical induction to prove that the number of odd-degree vertices is even, by removing one edge at a time from a given graph and using a case analysis on the degrees of its endpoints to determine the effect of this removal on the parity of the number of odd-degree vertices.\n\nThe degree sum formula implies that every \"r\"-regular graph with \"n\" vertices has \"nr\"/2 edges. In particular, if \"r\" is odd then the number of edges must be divisible by \"r\".\n\nThe handshaking lemma does not apply to infinite graphs, even when they have only a finite number of odd-degree vertices. For instance, an infinite path graph with one endpoint has only a single odd-degree vertex rather than having an even number of such vertices.\n\nSeveral combinatorial structures listed by may be shown to be even in number by relating them to the odd vertices in an appropriate \"exchange graph\".\n\nFor instance, as C. A. B. Smith proved, in any cubic graph \"G\" there must be an even number of Hamiltonian cycles through any fixed edge \"uv\"; used a proof based on the handshaking lemma to extend this result to graphs \"G\" in which all vertices have odd degree. Thomason defines an exchange graph \"H\", the vertices of which are in one-to-one correspondence with the Hamiltonian paths beginning at \"u\" and continuing through \"v\". Two such paths \"p\" and \"p\" are connected by an edge in \"H\" if one may obtain \"p\" by adding a new edge to the end of \"p\" and removing another edge from the middle of \"p\"; this is a symmetric relation, so \"H\" is an undirected graph. If path \"p\" ends at vertex \"w\", then the vertex corresponding to \"p\" in \"H\" has degree equal to the number of ways that \"p\" may be extended by an edge that does not connect back to \"u\"; that is, the degree of this vertex in \"H\" is either deg(\"w\") − 1 (an even number) if \"p\" does not form part of a Hamiltonian cycle through \"uv\", or deg(\"w\") − 2 (an odd number) if \"p\" is part of a Hamiltonian cycle through \"uv\". Since \"H\" has an even number of odd vertices, \"G\" must have an even number of Hamiltonian cycles through \"uv\".\n\nIn connection with the exchange graph method for proving the existence of combinatorial structures, it is of interest to ask how efficiently these structures may be found. For instance, suppose one is given as input a Hamiltonian cycle in a cubic graph; it follows from Smith's theorem that there exists a second cycle. How quickly can this second cycle be found?\n\nThe handshaking lemma is also used in proofs of Sperner's lemma and of the piecewise linear case of the mountain climbing problem.\n\n"}
{"id": "240790", "url": "https://en.wikipedia.org/wiki?curid=240790", "title": "Indicator function", "text": "Indicator function\n\nIn mathematics, an indicator function or a characteristic function is a function defined on a set \"X\" that indicates membership of an element in a subset \"A\" of \"X\", having the value 1 for all elements of \"A\" and the value 0 for all elements of \"X\" not in \"A\". It is usually denoted by a symbol 1 or \"I\", sometimes in boldface or blackboard boldface, with a subscript specifying the subset.\n\nIn other contexts, such as computer science, this would more often be described as a boolean predicate function (to test set inclusion).\n\nThe indicator function of a subset \"A\" of a set \"X\" is a function\n\ndefined as\n\nThe Iverson bracket allows the equivalent notation, formula_3, to be used instead of formula_4.\n\nThe function formula_5 is sometimes denoted formula_6, formula_7, \"K\" or even just formula_8. (The Greek letter formula_9 appears because it is the initial letter of the Greek word χαρακτήρ, which is the ultimate origin of the word \"characteristic\".)\n\nThe set of all indicator functions on formula_10 can be identified with formula_11, the power set of formula_10. Consequently, both sets are sometimes denoted by formula_13. This is a special case (formula_14) of the notation formula_15 for the set of all functions formula_16.\n\n\nA related concept in statistics is that of a dummy variable. (This must not be confused with \"dummy variables\" as that term is usually used in mathematics, also called a bound variable.)\n\nThe term \"characteristic function\" has an unrelated meaning in classic probability theory. For this reason, traditional probabilists use the term indicator function for the function defined here almost exclusively, while mathematicians in other fields are more likely to use the term \"characteristic function\" to describe the function that indicates membership in a set.\n\nIn fuzzy logic and modern many-valued logic, predicates are the characteristic functions of a probability distribution. That is, the strict true/false valuation of the predicate is replaced by a quantity interpreted as the degree of truth.\n\nThe \"indicator\" or \"characteristic\" function of a subset \"A\" of some set \"X\", maps elements of \"X\" to the range {0,1}.\n\nThis mapping is surjective only when \"A\" is a non-empty proper subset of \"X\". If \"A\" ≡ \"X\", then\n1 = 1. By a similar argument, if \"A\" ≡ Ø then 1 = 0.\n\nIn the following, the dot represents multiplication, 1·1 = 1, 1·0 = 0 etc. \"+\" and \"−\" represent addition and subtraction. \"formula_19\" and \"formula_20\" is intersection and union, respectively.\n\nIf formula_8 and formula_22 are two subsets of formula_10, then\n\nwhere |\"F\"| is the cardinality of \"F\". This is one form of the principle of inclusion-exclusion.\n\nAs suggested by the previous example, the indicator function is a useful notational device in combinatorics. The notation is used in other places as well, for instance in probability theory: if formula_10 is a probability space with probability measure formula_27 and formula_8 is a measurable set, then formula_5 becomes a random variable whose expected value is equal to the probability of formula_8:\n\nThis identity is used in a simple proof of Markov's inequality.\n\nIn many cases, such as order theory, the inverse of the indicator function may be defined. This is commonly called the generalized Möbius function, as a generalization of the inverse of the indicator function in elementary number theory, the Möbius function. (See paragraph below about the use of the inverse in classical recursion theory.)\n\nGiven a probability space formula_32 with formula_33, the indicator random variable formula_34 is defined by formula_35 if formula_36 otherwise formula_37\n\n\n\n\nKurt Gödel described the \"representing function\" in his 1934 paper \"On Undecidable Propositions of Formal Mathematical Systems\". (The paper appears on pp. 41–74 in Martin Davis ed. \"The Undecidable\"):\n\nStephen Kleene (1952) (p. 227) offers up the same definition in the context of the primitive recursive functions as a function φ of a predicate P takes on values 0 if the predicate is true and 1 if the predicate is false.\n\nFor example, because the product of characteristic functions φ*φ* . . . *φ = 0 whenever any one of the functions equals 0, it plays the role of logical OR: IF φ = 0 OR φ = 0 OR . . . OR φ = 0 THEN their product is 0. What appears to the modern reader as the representing function's logical inversion, i.e. the representing function is 0 when the function R is \"true\" or satisfied\", plays a useful role in Kleene's definition of the logical functions OR, AND, and IMPLY (p. 228), the bounded- (p. 228) and unbounded- (p. 279ff) mu operators (Kleene (1952)) and the CASE function (p. 229).\n\nIn classical mathematics, characteristic functions of sets only take values 1 (members) or 0 (non-members). In fuzzy set theory, characteristic functions are generalized to take value in the real unit interval [0, 1], or more generally, in some algebra or structure (usually required to be at least a poset or lattice). Such generalized characteristic functions are more usually called membership functions, and the corresponding \"sets\" are called \"fuzzy\" sets. Fuzzy sets model the gradual change in the membership degree seen in many real-world predicates like \"tall\", \"warm\", etc.\n\nA particular indicator function is the Heaviside step function. The Heaviside step function \"H\"(\"x\") is the indicator function of the one-dimensional positive half-line, i.e. the domain [0, ∞). The distributional derivative of the Heaviside step function is equal to the Dirac delta function, i.e.\n\nwith the following property:\n\nThe derivative of the Heaviside step function can be seen as the 'inward normal derivative' at the 'boundary' of the domain given by the positive half-line. In higher dimensions, the derivative naturally generalises to the inward normal derivative, while the Heaviside step function naturally generalises to the indicator function of some domain \"D\". The surface of \"D\" will be denoted by \"S\". Proceeding, it can be derived that the inward normal derivative of the indicator gives rise to a 'surface delta function', which can be indicated by δ(x):\n\nwhere \"n\" is the outward normal of the surface \"S\". This 'surface delta function' has the following property:\n\nBy setting the function \"f\" equal to one, it follows that the inward normal derivative of the indicator integrates to the numerical value of the surface area \"S\".\n\n"}
{"id": "20938051", "url": "https://en.wikipedia.org/wiki?curid=20938051", "title": "Jiří Matoušek (mathematician)", "text": "Jiří Matoušek (mathematician)\n\nJiří (Jirka) Matoušek (10 March 1963 – 9 March 2015) was a Czech mathematician working in computational geometry and algebraic topology. He was a professor at Charles University in Prague and the author of several textbooks and research monographs.\n\nMatoušek was born in Prague. In 1986, he received his Master's degree at Charles University under Miroslav Katětov. From 1986 until his death he was employed at the Department of Applied Mathematics of Charles University in Prague, holding a professor position since 2000. He was also a visiting and later full professor at ETH Zurich.\n\nIn 1996, he won the European Mathematical Society prize and in 2000 he won the Scientist award of the Learned Society of the Czech Republic.\nHe became a fellow of the Learned Society of the Czech Republic in 2005.\n\nMatoušek's paper on computational aspects of algebraic topology won the Best Paper award at the 2012 ACM Symposium on Discrete Algorithms.\n\nAside from his own academic writing, he has translated the popularization book \"Mathematics: A Very Short Introduction\" by Timothy Gowers into Czech.\n\nHe was a supporter and signatory of the Cost of Knowledge protest. He died in 2015, aged 51.\n\n\n\n"}
{"id": "48884267", "url": "https://en.wikipedia.org/wiki?curid=48884267", "title": "Joseph R. Shoenfield", "text": "Joseph R. Shoenfield\n\nJoseph Robert Shoenfield (1927, Detroit – November 15, 2000, Durham, North Carolina) was an American mathematical logician.\n\nShoenfield obtained his PhD in 1953 with Raymond Louis Wilder at the University of Michigan (Models of formal systems).\n\nFrom 1952, he lectured at Duke University, where he remained until becoming Emeritus in 1992. From 1970 to 1973 he was President of the Mathematics Faculty. In 1956/57 he was at the Institute for Advanced Study. Shoenfield worked on recursion theory, model theory and axiomatic set theory. His textbook on mathematical logic has become a classic.\n\nFrom 1972 to 1976 he was president of the Association for Symbolic Logic. He delivered the Gödel Lecture at the 1992 meeting of the ASL.\n\nAlready in his student days, he was a passionate and strong contract bridge player.\nHe was an early member Number 694 of the American Go Association and the Memorial Tournament in North Carolina was founded in his memory. (The link includes a photograph of him.)\n\n\n"}
{"id": "7566175", "url": "https://en.wikipedia.org/wiki?curid=7566175", "title": "K-vertex-connected graph", "text": "K-vertex-connected graph\n\nIn graph theory, a connected graph \"G\" is said to be k\"-vertex-connected (or k\"-connected) if it has more than \"k\" vertices and remains connected whenever fewer than \"k\" vertices are removed.\n\nThe vertex-connectivity, or just connectivity, of a graph is the largest \"k\" for which the graph is \"k\"-vertex-connected.\n\nA graph (other than a complete graph) has connectivity \"k\" if \"k\" is the size of the smallest subset of vertices such that the graph becomes disconnected if you delete them. Complete graphs are not included in this version of the definition since they cannot be disconnected by deleting vertices. The complete graph with \"n\" vertices has connectivity \"n\" − 1, as implied by the first definition.\n\nAn equivalent definition is that a graph with at least two vertices is \"k\"-connected if, for every pair of its vertices, it is possible to find \"k\" vertex-independent paths connecting these vertices; see Menger's theorem . This definition produces the same answer, \"n\" − 1, for the connectivity of the complete graph \"K\".\n\nA 1-connected graph is called connected; a 2-connected graph is called biconnected. A 3-connected graph is called triconnected.\n\nThe 1-skeleton of any \"k\"-dimensional convex polytope forms a \"k\"-vertex-connected graph (Balinski's theorem, ). As a partial converse, Steinitz's theorem states that any 3-vertex-connected planar graph forms the skeleton of a convex polyhedron. \n\nMore generally, the \"3-sphere regular cellulation conjecture\" claims that every 2-connected graph is the \none-dimensional skeleton of a regular CW-complex on the three-dimensional sphere (http://twiki.di.uniroma1.it/pub/Users/SergioDeAgostino/DeAgostino2016.pdf).\n\nThe vertex-connectivity of an input graph \"G\" can be computed in polynomial time in the following way consider all possible pairs formula_1 of nonadjacent nodes to disconnect, using Menger's theorem to justify that the minimal-size separator for formula_1 is the number of pairwise vertex-independent paths between them, encode the input by doubling each vertex as an edge to reduce to a computation of the number of pairwise edge-independent paths, and compute the maximum number of such paths by computing the maximum flow in the graph between formula_3 and formula_4 with capacity 1 to each edge, noting that a flow of formula_5 in this graph corresponds, by the integral flow theorem, to formula_5 pairwise edge-independent paths from formula_3 to formula_4.\n\n\n"}
{"id": "588260", "url": "https://en.wikipedia.org/wiki?curid=588260", "title": "Kakeya set", "text": "Kakeya set\n\nIn mathematics, a Kakeya set, or Besicovitch set, is a set of points in Euclidean space which contains a unit line segment in every direction. For instance, a disk of radius 1/2 in the Euclidean plane, or a ball of radius 1/2 in three-dimensional space, forms a Kakeya set. Much of the research in this area has studied the problem of how small such sets can be. Besicovitch showed that there are Besicovitch sets of measure zero.\n\nA Kakeya needle set (sometimes also known as a Kakeya set) is a (Besicovitch) set in the plane with a stronger property, that a unit line segment can be rotated continuously through 180 degrees within it, returning to its original position with reversed orientation. Again, the disk of radius 1/2 is an example of a Kakeya needle set.\n\nThe Kakeya needle problem asks whether there is a minimum area of a region \"D\" in the plane, in which a needle of unit length can be turned through 360°. This question was first posed, for convex regions, by . The minimum area for convex sets is achieved by an equilateral triangle of height 1 and area 1/, as Pál showed.\n\nKakeya seems to have suggested that the Kakeya set \"D\" of minimum area, without the convexity restriction, would be a three-pointed deltoid shape. However, this is false; there are smaller non-convex Kakeya sets.\n\nBesicovitch was able to show that there is no lower bound > 0 for the area of such a region \"D\", in which a needle of unit length can be turned round. This built on earlier work of his, on plane sets which contain a unit segment in each orientation. Such a set is now called a Besicovitch set. Besicovitch's work showing such a set could have arbitrarily small measure was from 1919. The problem may have been considered by analysts before that.\n\nOne method of constructing a Besicovitch set (see figure for corresponding illustrations) is known as a \"Perron tree\" after O. Perron who was able to simplify Besicovitch's original construction: take a triangle with height 1, divide it in two, and translate both pieces over each other so that their bases overlap on some small interval. Then this new figure will have a reduced total area.\n\nNow, suppose we divide our triangle into eight subtriangles. For each consecutive pair of triangles, perform the same overlapping operation we described before to get four new shapes, each consisting of two overlapping triangles. Next, overlap consecutive pairs of these new shapes by shifting their bases over each other partially, so we're left with two shapes, and finally overlap these two in the same way. In the end, we get a shape looking somewhat like a tree, but with an area much smaller than our original triangle.\n\nTo construct an even smaller set, subdivide your triangle into, say, 2 triangles each of base length 2, and perform the same operations as we did before when we divided our triangle twice and eight times. If the amount of overlap we do on each triangle is small enough and the size \"n\" of the subdivision of our triangle is large enough, we can form a tree of area as small as we like. A Besicovitch set can be created by combining three rotations of a Perron tree created from an equilateral triangle.\n\nAdapting this method further, we can construct a sequence of sets whose intersection is a Besicovitch set of measure zero. One way of doing this is to observe that if we have any parallelogram two of whose sides are on the lines \"x\" = 0 and \"x\" = 1 then we can find a union of parallelograms also with sides on these lines, whose total area is arbitrarily small and which contain translates of all lines joining a point on \"x\" = 0 to a point on \"x\" = 1 that are in the original parallelogram. This follows from a slight variation of Besicovich's construction above. By repeating this we can find a sequence of sets\neach a finite union of parallelograms between the lines \"x\" = 0 and \"x\" = 1, whose areas tend to zero and each of which contains translates of all lines joining \"x\" = 0 and \"x\" = 1 in a unit square. The intersection of these sets is then a measure 0 set containing translates of all these lines, so a union of two copies of this intersection is a measure 0 Besicovich set.\n\nThere are other methods for constructing Besicovitch sets of measure zero aside from the 'sprouting' method. For example, Kahane uses Cantor sets to construct a Besicovitch set of measure zero in the two-dimensional plane.\n\nBy using a trick of Pál, known as Pál joins (given two parallel lines, any unit line segment can be moved continuously from one to the other on a set of arbitrary small measure), a set in which a unit line segment can be rotated continuously through 180 degrees can be created from a Besicovitch set consisting of Perron trees.\n\nIn 1941, H. J. Van Alphen showed that there are arbitrary small Kakeya needle sets inside a circle with radius 2 + ε (arbitrary ε > 0). Simply connected Kakeya needle sets with smaller area than the deltoid were found in 1965. Melvin Bloom and I. J. Schoenberg independently presented Kakeya needle sets with areas approaching to formula_2, the Bloom-Schoenberg number. Schoenberg conjectured that this number is the lower bound for the area of simply connected Kakeya needle sets. However, in 1971, F. Cunningham showed that, given ε > 0, there is a simply connected Kakeya needle set of area less than ε contained in a circle of radius 1.\n\nAlthough there are Kakeya needle sets of arbitrarily small positive measure and Besicovich sets of measure 0, there are no Kakeya needle sets of measure 0.\n\nThe same question of how small these Besicovitch sets could be was then posed in higher dimensions, giving rise to a number of conjectures known collectively as the \"Kakeya conjectures\", and have helped initiate the field of mathematics known as geometric measure theory. In particular, if there exist Besicovitch sets of measure zero, could they also have s-dimensional Hausdorff measure zero for some dimension s less than the dimension of the space in which they lie? This question gives rise to the following conjecture:\n\nThis is known to be true for \"n\" = 1, 2 but only partial results are known in higher dimensions.\n\nA modern way of approaching this problem is to consider a particular type of maximal function, which we construct as follows: Denote S ⊂ R to be the unit sphere in \"n\"-dimensional space. Define formula_3 to be the cylinder of length 1, radius δ > 0, centered at the point \"a\" ∈ R, and whose long side is parallel to the direction of the unit vector \"e\" ∈ S. Then for a locally integrable function \"f\", we define the Kakeya maximal function of \"f\" to be\n\nwhere \"m\" denotes the n-dimensional Lebesgue measure. Notice that formula_5 is defined for vectors \"e\" in the sphere S.\n\nThen there is a conjecture for these functions that, if true, will imply the Kakeya set conjecture for higher dimensions:\n\nSome results toward proving the Kakeya conjecture are the following:\n\nSomewhat surprisingly, these conjectures have been shown to be connected to a number of questions in other fields, notably in harmonic analysis. For instance, in 1971, Charles Fefferman was able to use the Besicovitch set construction to show that in dimensions greater than 1, truncated Fourier integrals taken over balls centered at the origin with radii tending to infinity need not converge in \"L\" norm when \"p\" ≠ 2 (this is in contrast to the one-dimensional case where such truncated integrals do converge).\n\nAnalogues of the Kakeya problem include considering sets containing more general shapes than lines, such as circles.\n\n\n\nA generalization of the Kakeya conjecture is to consider sets that contain, instead of segments of lines in every direction, but, say, portions of \"k\"-dimensional subspaces. Define an (\"n\", \"k\")-Besicovitch set \"K\" to be a compact set in R containing a translate of every \"k\"-dimensional unit disk which has Lebesgue measure zero. That is, if \"B\" denotes the unit ball centered at zero, for every \"k\"-dimensional subspace \"P\", there exists \"x\" ∈ R such that (\"P\" ∩ \"B\") + \"x\" ⊆ \"K\". Hence, a (\"n\", 1)-Besicovitch set is the standard Besicovitch set described earlier.\n\nIn 1979, Marstrand proved that there were no (3, 2)-Besicovitch sets. At around the same time, however, Falconer proved that there were no (\"n\", \"k\")-Besicovitch sets for 2\"k\" > \"n\". The best bound to date is by Bourgain, who proved in that no such sets exist when 2 + \"k\" > \"n\".\n\nIn 1999, Wolff posed the finite field analogue to the Kakeya problem, in hopes that the techniques for solving this conjecture could be carried over to the Euclidean case.\n\nZeev Dvir proved this conjecture in 2008, showing that the statement holds for \"c\" = 1/\"n\"!. In his proof, he observed that any polynomial in \"n\" variables of degree less than |F| vanishing on a Kakeya set must be identically zero. On the other hand, the polynomials in \"n\" variables of degree less than |F| form a vector space of dimension\n\nTherefore, there is at least one non-trivial polynomial of degree less than |F| that vanishes on any given set with less than this number of points. Combining these two observations shows that Kakeya sets must have at least |F|/\"n\"! points.\n\nIt's not clear whether the techniques will extend to proving the original Kakeya conjecture but this proof does lend credence to the original conjecture by making essentially algebraic counterexamples unlikely. Dvir has written a survey article on progress on the finite field Kakeya problem and its relationship to randomness extractors.\n\n\n\n\n"}
{"id": "26064288", "url": "https://en.wikipedia.org/wiki?curid=26064288", "title": "Lebesgue integration", "text": "Lebesgue integration\n\nIn mathematics, the integral of a non-negative function of a single variable can be regarded, in the simplest case, as the area between the graph of that function and the -axis. The Lebesgue integral extends the integral to a larger class of functions. It also extends the domains on which these functions can be defined.\n\nLong before the advent of the 20th century, mathematicians already understood that for non-negative functions with a smooth enough graph—such as continuous functions on closed bounded intervals—the \"area under the curve\" could be defined as the integral, and computed using approximation techniques on the region by polygons. However, as the need to consider more irregular functions arose—e.g., as a result of the limiting processes of mathematical analysis and the mathematical theory of probability—it became clear that more careful approximation techniques were needed to define a suitable integral. Also, one might wish to integrate on spaces more general than the real line. The Lebesgue integral provides the right abstractions needed to do this important job.\n\nThe Lebesgue integral plays an important role in probability theory, real analysis, and many other fields in the mathematical sciences. It is named after Henri Lebesgue (1875–1941), who introduced the integral . It is also a pivotal part of the axiomatic theory of probability.\n\nThe term \"Lebesgue integration\" can mean either the general theory of integration of a function with respect to a general measure, as introduced by Lebesgue, or the specific case of integration of a function defined on a sub-domain of the real line with respect to Lebesgue measure.\n\nThe integral of a positive function between limits and can be interpreted as the area under the graph of . This is easy to understand for familiar functions such as polynomials, but what does it mean for more exotic functions? In general, for which class of functions does \"area under the curve\" make sense? The answer to this question has great theoretical and practical importance.\n\nAs part of a general movement toward rigor in mathematics in the nineteenth century, mathematicians attempted to put integral calculus on a firm foundation. The Riemann integral—proposed by Bernhard Riemann (1826–1866)—is a broadly successful attempt to provide such a foundation. Riemann's definition starts with the construction of a sequence of easily calculated areas that converge to the integral of a given function. This definition is successful in the sense that it gives the expected answer for many already-solved problems, and gives useful results for many other problems.\n\nHowever, Riemann integration does not interact well with taking limits of sequences of functions, making such limiting processes difficult to analyze. This is important, for instance, in the study of Fourier series, Fourier transforms, and other topics. The Lebesgue integral is better able to describe how and when it is possible to take limits under the integral sign (via the powerful monotone convergence theorem and dominated convergence theorem).\n\nWhile the Riemann integral considers the area under a curve as made out of vertical rectangles, the Lebesgue definition considers horizontal slabs that are not necessarily just rectangles, and so it is more flexible. For this reason, the Lebesgue definition makes it possible to calculate integrals for a broader class of functions. For example, the Dirichlet function, which is 0 where its argument is irrational and 1 otherwise, has a Lebesgue integral, but does not have a Riemann integral. Furthermore, the Lebesgue integral of this function is zero, which agrees with the intuition that when picking a real number uniformly at random from the unit interval, the probability of picking a rational number should be zero.\n\nLebesgue summarized his approach to integration in a letter to Paul Montel:\nThe insight is that one should be able to rearrange the values of a function freely, while preserving the value of the integral. This process of rearrangement can convert a very pathological function into one that is \"nice\" from the point of view of integration, and thus let such pathological functions be integrated.\n\nTo get some intuition about the different approaches to integration, let us imagine that we want to find a mountain's volume (above sea level).\n\n\n\nFolland summarizes the difference between the Riemann and Lebesgue approaches thus: \"to compute the Riemann integral of , one partitions the domain into subintervals\", while in the Lebesgue integral, \"one is in effect partitioning the range of .\"\n\nTo define the Lebesgue integral requires the formal notion of a measure that, roughly, associates to each set of real numbers a nonnegative number representing the \"size\" of . This notion of \"size\" should agree with the usual length of an interval or disjoint union of intervals. Suppose that is a non-negative real-valued function. Using the \"partitioning the range of \" philosophy, the integral of should be the sum over of the elementary area contained in the thin horizontal strip between . This elementary area is just\nLet\nThe Lebesgue integral of is then defined by\nwhere the integral on the right is an ordinary improper Riemann integral. Note that is a non-negative decreasing function, and therefore has a well-defined improper Riemann integral with value in the interval . For a suitable class of functions (the measurable functions), this defines the Lebesgue integral.\n\nA general (not necessarily positive) measurable function is Lebesgue integrable if the area between the graph of and the -axis is finite:\nIn that case, as in the Riemannian case, the integral is the difference between the area above the -axis and the area below the -axis:\nwhere formula_6 is the decomposition of \"f\" into the difference of two non-negative functions given by\n\nThe theory of the Lebesgue integral requires a theory of measurable sets and measures on these sets, as well as a theory of measurable functions and integrals on these functions.\n\nMeasure theory was initially created to provide a useful abstraction of the notion of length of subsets of the real line—and, more generally, area and volume of subsets of Euclidean spaces. In particular, it provided a systematic answer to the question of which subsets of have a length. As later set theory developments showed (see non-measurable set), it is actually impossible to assign a length to all subsets of in a way that preserves some natural additivity and translation invariance properties. This suggests that picking out a suitable class of \"measurable\" subsets is an essential prerequisite.\n\nThe Riemann integral uses the notion of length explicitly. Indeed, the element of calculation for the Riemann integral is the rectangle , whose area is calculated to be . The quantity is the length of the base of the rectangle and is the height of the rectangle. Riemann could only use planar rectangles to approximate the area under the curve, because there was no adequate theory for measuring more general sets.\n\nIn the development of the theory in most modern textbooks (after 1950), the approach to measure and integration is \"axiomatic\". This means that a measure is any function μ defined on a certain class of subsets of a set , which satisfies a certain list of properties. These properties can be shown to hold in many different cases.\n\nWe start with a measure space where is a set, is a σ-algebra of subsets of , and μ is a (non-negative) measure on defined on the sets of .\n\nFor example, can be Euclidean -space or some Lebesgue measurable subset of it, is the σ-algebra of all Lebesgue measurable subsets of , and μ is the Lebesgue measure. In the mathematical theory of probability, we confine our study to a probability measure , which satisfies .\n\nLebesgue's theory defines integrals for a class of functions called measurable functions. A real-valued function on is measurable if the pre-image of every interval of the form is in :\n\nWe can show that this is equivalent to requiring that the pre-image of any Borel subset of ℝ be in . The set of measurable functions is closed under algebraic operations, but more importantly it is closed under various kinds of point-wise sequential limits:\n\nare measurable if the original sequence , where , consists of measurable functions.\n\nThere are several approaches for defining an integral:\n\nfor measurable real-valued functions defined on . \n\nOne approach to constructing the Lebesgue integral is to make use of so-called \"simple functions\": finite real-linear combinations of \"indicator functions\". Simple functions can be used to approximate a measurable function, by partitioning the range into layers. The integral of a simple function is equal to the measure of a given layer, times the height of that layer. The integral of a non-negative general measurable function is then defined as an appropriate supremum of approximations by simple functions, and the integral of a (not necessarily positive) measurable function is the difference of two integrals of non-negative measurable functions, as mentioned earlier.\n\nTo assign a value to the integral of the indicator function of a measurable set consistent with the given measure μ, the only reasonable choice is to set:\n\nNotice that the result may be equal to , unless is a \"finite\" measure.\n\nA finite linear combination of indicator functions\n\nwhere the coefficients are real numbers and the sets are measurable, is called a measurable simple function. We extend the integral by linearity to \"non-negative\" measurable simple functions. When the coefficients are non-negative, we set\n\nThe convention must be used, and the result may be infinite. Even if a simple function can be written in many ways as a linear combination of indicator functions, the integral is always the same. This can be shown using the additivity property of measures.\n\nSome care is needed when defining the integral of a \"real-valued\" simple function, to avoid the undefined expression : one assumes that the representation\n\nis such that whenever . Then the above formula for the integral of \"f\" makes sense, and the result does not depend upon the particular representation of satisfying the assumptions.\n\nIf is a measurable subset of and is a measurable simple function one defines\n\nLet be a non-negative measurable function on , which we allow to attain the value , in other words, takes non-negative values in the extended real number line. We define\n\nWe need to show this integral coincides with the preceding one, defined on the set of simple functions, when \"E\"  is a segment [\"a\", \"b\"]. There is also the question of whether this corresponds in any way to a Riemann notion of integration. It is possible to prove that the answer to both questions is yes.\n\nWe have defined the integral of \"f\" for any non-negative extended real-valued measurable function on \"E\". For some functions, this integral  ∫ \"f\" dμ  is infinite.\n\nIt is often useful to have a particular sequence of simple functions that approximates the Lebesgue integral well (analogously to a Riemann sum). For a non-negative measurable function , let formula_17 be the simple function whose value is formula_18 whenever formula_19, for \"k\" a non-negative integer less than (say) formula_20. Then it can be proven directly that\nand that the limit on the right hand side exists as an extended real number. This bridges the connection between the approach to the Lebesgue integral using simple functions, and the motivation for the Lebesgue integral using a partition of the range.\n\nTo handle signed functions, we need a few more definitions. If is a measurable function of the set to the reals (including ), then we can write\n\nwhere\n\nNote that both and are non-negative measurable functions. Also note that\n\nWe say that the Lebesgue integral of the measurable function \"exists\", or \"is defined\" if at least one of formula_26 and formula_27 is finite:\n\nIn this case we \"define\"\n\nIf\n\nwe say that is \"Lebesgue integrable\".\n\nIt turns out that this definition gives the desirable properties of the integral.\n\nComplex valued functions can be similarly integrated, by considering the real part and the imaginary part separately.\n\nIf \"h\"=\"f\"+\"ig\" for real-valued integrable functions \"f\", \"g\", then the integral of \"h\" is defined by\n\nThe function is Lebesgue integrable if and only if its absolute value is Lebesgue integrable (see Absolutely integrable function).\n\nConsider the indicator function of the rational numbers, . This function is nowhere continuous.\n\n\nA technical issue in Lebesgue integration is that the domain of integration is defined as a \"set\" (a subset of a measure space), with no notion of orientation. In elementary calculus, one defines integration with respect to an orientation:\nGeneralizing this to higher dimensions yields integration of differential forms. By contrast, Lebesgue integration provides an alternative generalization, integrating over subsets with respect to a measure; this can be notated as\nto indicate integration over a subset . For details on the relation between these generalizations, see .\n\nHere we discuss the limitations of the Riemann integral and the greater scope offered by the Lebesgue integral. This discussion presumes a working understanding of the Riemann integral.\n\nWith the advent of Fourier series, many analytical problems involving integrals came up whose satisfactory solution required interchanging limit processes and integral signs. However, the conditions under which the integrals\n\nare equal proved quite elusive in the Riemann framework. There are some other technical difficulties with the Riemann integral. These are linked with the limit-taking difficulty discussed above.\n\nFailure of monotone convergence. As shown above, the indicator function on the rationals is not Riemann integrable. In particular, the Monotone convergence theorem fails. To see why, let } be an enumeration of all the rational numbers in (they are countable so this can be done.) Then let\n\nThe function is zero everywhere, except on a finite set of points. Hence its Riemann integral is zero. Each is non-negative, and this sequence of functions is monotonically increasing, but its limit as is , which is not Riemann integrable.\n\nUnsuitability for unbounded intervals. The Riemann integral can only integrate functions on a bounded interval. It can however be extended to unbounded intervals by taking limits, so long as this doesn't yield an answer such as .\n\nIntegrating on structures other than Euclidean space. The Riemann integral is inextricably linked to the order structure of the real line.\n\nThe Lebesgue integral does not distinguish between functions that differ only on a set of μ-measure zero. To make this precise, functions and are said to be equal almost everywhere (a.e.) if\n\n\nTo wit, the integral respects the equivalence relation of almost-everywhere equality.\n\n\nThe Lebesgue integral has the following properties:\n\nLinearity: If and are Lebesgue integrable functions and and are real numbers, then is Lebesgue integrable and\n\nMonotonicity: If , then\n\nMonotone convergence theorem: Suppose is a sequence of non-negative measurable functions such that\n\nThen, the pointwise limit of is Lebesgue measurable and\n\nThe value of any of the integrals is allowed to be infinite.\n\nFatou's lemma: If is a sequence of non-negative measurable functions, then\n\nAgain, the value of any of the integrals may be infinite.\n\nDominated convergence theorem: Suppose is a sequence of complex measurable functions with pointwise limit , and there is a Lebesgue integrable function (i.e., belongs to the such that for all .\n\nThen, is Lebesgue integrable and\n\nTo illustrate some of the proof techniques used in Lebesgue integration theory, we sketch a proof of the above-mentioned Lebesgue monotone convergence theorem. Let be a non-decreasing sequence of non-negative measurable functions and put\n\nBy the monotonicity property of the integral, it is immediate that:\n\nand the limit on the right exists, because the sequence is monotonic. We now prove the inequality in the other direction. It follows from the definition of integral that there is a non-decreasing sequence of non-negative simple functions such that and\n\nTherefore, it suffices to prove that for each ,\n\nWe will show that if is a simple function and\n\nalmost everywhere, then\n\nBy breaking up the function into its constant value parts, this reduces to the case in which is the indicator function of a set. The result we have to prove is then\n\nSuppose is a measurable set and is a nondecreasing sequence of non-negative measurable functions on such that\n\nfor almost all . Then\n\nTo prove this result, fix and define the sequence of measurable sets\n\nBy monotonicity of the integral, it follows that for any ,\n\nBecause almost every is in for large enough , we have\n\nup to a set of measure . Thus by countable additivity of , and because increases with ,\n\nAs this is true for any positive the result follows.\n\nFor another Proof of the Monotone Convergence Theorem, we follow:\n\nLet be a measure space.\n\nfor all , so that\n\nNow we need to establish the reverse inequality. Fix , let be a simple function with and let\n\nThen is an increasing sequence of measurable sets with formula_62. We know that\n\nThis is true for all , including the limit:\n\nHence,\n\nThis was true for all , so it remains true for , and taking the supremum over simple by the definition of integration in ,\n\nNow we have both inequalities, so we've shown the Monotone Convergence theorem:\n\nfor , and pointwise, , the set of positive measurable functions from .\n\nIt is possible to develop the integral with respect to the Lebesgue measure without relying on the full machinery of measure theory. One such approach is provided by the Daniell integral.\n\nThere is also an alternative approach to developing the theory of integration via methods of functional analysis. The Riemann integral exists for any continuous function of compact support defined on (or a fixed open subset). Integrals of more general functions can be built starting from these integrals.\n\nLet be the space of all real-valued compactly supported continuous functions of ℝ. Define a norm on by\n\nThen is a normed vector space (and in particular, it is a metric space.) All metric spaces have Hausdorff completions, so let be its completion. This space is isomorphic to the space of Lebesgue integrable functions modulo the subspace of functions with integral zero. Furthermore, the Riemann integral is a uniformly continuous functional with respect to the norm on , which is dense in . Hence has a unique extension to all of . This integral is precisely the Lebesgue integral.\n\nMore generally, when the measure space on which the functions are defined is also a locally compact topological space (as is the case with the real numbers ℝ), measures compatible with the topology in a suitable sense (Radon measures, of which the Lebesgue measure is an example) an integral with respect to them can be defined in the same manner, starting from the integrals of continuous functions with compact support. More precisely, the compactly supported functions form a vector space that carries a natural topology, and a (Radon) measure is defined as a continuous linear functional on this space. The value of a measure at a compactly supported function is then also by definition the integral of the function. One then proceeds to expand the measure (the integral) to more general functions by continuity, and defines the measure of a set as the integral of its indicator function. This is the approach taken by and a certain number of other authors. For details see Radon measures.\n\nThe main purpose of Lebesgue integral is to provide an integral notion where limits of integrals hold under mild assumptions. There is no guarantee that every function is Lebesgue integrable. But it may happen that improper integrals exist for functions that are not Lebesgue integrable. One example would be\nover the entire real line. This function is not Lebesgue integrable, as\nOn the other hand, formula_71 exists as an improper integral and can be computed to be finite; it is twice the Dirichlet integral.\n\n\n\n\n\n\n\n\n"}
{"id": "47931235", "url": "https://en.wikipedia.org/wiki?curid=47931235", "title": "Lecture Notes in Mathematics", "text": "Lecture Notes in Mathematics\n\nLecture Notes in Mathematics (LNM, ) is a book series in the field of mathematics, including articles related to both research and teaching. It was established in 1964 and was edited by A. Dold, Heidelberg and B. Eckmann, Zürich. Its publisher is Springer Science+Business Media (formerly Springer-Verlag).\n\nThe intent of the series is to publish not only lecture notes, but results from seminars and conferences, more quickly than the several-years-long process of publishing polished journal papers in mathematics. In order to speed the publication process, early volumes of the series (before electronic publishing) were reproduced photographically from typewritten manuscripts. According to Earl Taft it has been \"enormously successful\" and \"is considered a very valuable service to the mathematical community\".\n\n"}
{"id": "15366214", "url": "https://en.wikipedia.org/wiki?curid=15366214", "title": "Leslie Fox Prize for Numerical Analysis", "text": "Leslie Fox Prize for Numerical Analysis\n\nThe Leslie Fox Prize for Numerical Analysis of the Institute of Mathematics and its Applications (IMA) is a biennial prize established in 1985 by the IMA in honour of mathematician Leslie Fox (1918-1992). The prize honours \"young numerical analysts worldwide\" (any person who is less than 31 years old), and applicants submit papers for review. A committee reviews the papers, invites shortlisted candidates to give lectures at the Leslie Fox Prize meeting, and then awards First Prize and Second Prizes based on \"mathematical and algorithmic brilliance in tandem with presentational skills.\"\n\nSource: Institute of Mathematics and its Applications\n"}
{"id": "11022628", "url": "https://en.wikipedia.org/wiki?curid=11022628", "title": "List of space groups", "text": "List of space groups\n\nThere are 230 space groups in three dimensions, given by a number index, and a full name in Hermann–Mauguin notation, and a short name (international short symbol). The long names are given with spaces for readability. The groups each have a point groups of the unit cell.\n\nIn Hermann–Mauguin notation, space groups are named by a symbol combining the point group identifier with the uppercase letters describing the lattice type. Translations within the lattice in the form of screw axes and glide planes are also noted, giving a complete crystallographic space group.\n\nThese are the Bravais lattices in three dimensions:\n\nA reflection plane m within the point groups can be replaced by a glide plane, labeled as a, b, or c depending on which axis the glide is along. There is also the n glide, which is a glide along the half of a diagonal of a face, and the d glide, which is along a quarter of either a face or space diagonal of the unit cell. The d glide is often called the diamond glide plane as it features in the diamond structure.\n\nA gyration point can be replaced by a screw axis denoted by a number, \"n\", where the angle of rotation is formula_7. The degree of translation is then added as a subscript showing how far along the axis the translation is, as a portion of the parallel lattice vector. For example, 2 is a 180° (twofold) rotation followed by a translation of ½ of the lattice vector. 3 is a 120° (threefold) rotation followed by a translation of ⅓ of the lattice vector.\n\nThe possible screw axes are: 2, 3, 3, 4, 4, 4, 6, 6, 6, 6, and 6.\n\nIn Schoenflies notation, the symbol of a space group is represented by the symbol of corresponding point group with additional superscript. The superscript doesn't give any additional information about symmetry elements of the space group, but is instead related to the order in which Schoenflies derived the space groups.\n\nIn Fedorov symbol, the type of space group is denoted as \"s\" (\"symmorphic\" ), \"h\" (\"hemisymmorphic\"), or \"a\" (\"asymmorphic\"). The number is related to the order in which Fedorov derived space groups. There are 73 symmorphic, 54 hemisymmorphic, and 103 asymmorphic space groups. Symmorphic space groups can be obtained as combination of Bravais lattices with corresponding point group. These groups contain the same symmetry elements as the corresponding point groups. Hemisymmorphic space groups contain only axial combination of symmetry elements from the corresponding point groups. All the other space groups are asymmorphic. Example for point group 4/mmm (formula_8): the symmorphic space groups are P4/mmm (formula_9, \"36s\") and I4/mmm (formula_10, \"37s\"); hemisymmorphic space groups should contain axial combination 422, these are P4/mcc (formula_11, \"35h\"), P4/nbm (formula_12, \"36h\"), P4/nnc (formula_13, \"37h\"), and I4/mcm (formula_14, \"38h\").\n\n"}
{"id": "145438", "url": "https://en.wikipedia.org/wiki?curid=145438", "title": "Meta-Object Facility", "text": "Meta-Object Facility\n\nThe Meta-Object Facility (MOF) is an Object Management Group (OMG) standard for model-driven engineering. Its purpose is to provide a type system for entities in the CORBA architecture and a set of interfaces through which those types can be created and manipulated. The official reference page may be found at OMG's website.\n\nMOF was developed to provide a type system for use in the CORBA architecture, a set of schemas by which the structure, meaning and behaviour of objects could be defined, and a set of CORBA interfaces through which these schemas could be created, stored and manipulated. \n\nMOF is designed as a four-layered architecture. It provides a meta-meta model at the top layer, called the M3 layer. This M3-model is the language used by MOF to build metamodels, called M2-models. The most prominent example of a Layer 2 MOF model is the UML metamodel, the model that describes the UML itself. These M2-models describe elements of the M1-layer, and thus M1-models. These would be, for example, models written in UML. The last layer is the M0-layer or data layer. It is used to describe real-world objects.\n\nBeyond the M3-model, MOF describes the means to create and manipulate models and metamodels by defining CORBA interfaces that describe those operations. Because of the similarities between the MOF M3-model and UML structure models, MOF metamodels are usually modeled as UML class diagrams. A supporting standard of MOF is XMI, which defines an XML-based exchange format for models on the M3-, M2-, or M1-Layer.\n\nMOF is a \"closed\" metamodeling architecture; it defines an M3-model, which conforms to itself. MOF allows a \"strict\" meta-modeling architecture; every model element on every layer is strictly in correspondence with a model element of the layer above. MOF only provides a means to define the structure, or abstract syntax of a language or of data. For defining metamodels, MOF plays exactly the role that EBNF plays for defining programming language grammars. MOF is a Domain Specific Language (DSL) used to define metamodels, just as EBNF is a DSL for defining grammars. Similarly to EBNF, MOF could be defined in MOF.\n\nIn short MOF uses the notion of MOF::Classes (not to be confused with UML::Classes), as known from object orientation, to define concepts (model elements) on a metalayer. MOF may be used to define object-oriented metamodels (as UML for example) as well as non object-oriented metamodels (as a Petri net or a Web Service metamodel). \n\nAs of May 2006, the OMG has defined two compliance points for MOF:\n\nIn June 2006, a \"request for proposal\" was issued by OMG for a third variant, SMOF (Semantic MOF).\n\nThe variant ECore that has been defined in the Eclipse Modeling Framework is more or less aligned on OMG's EMOF. \n\nAnother related standard is OCL, which describes a formal language that can be used to define model constraints in terms of predicate logic.\n\nQVT, which introduces means to query, view and transform MOF-based models, is a very important standard, approved in 2005. See Model Transformation Language for further information.\n\nMOF is an international standard:\n\nMOF can be viewed as a standard to write metamodels, for example in order to model the abstract syntax of Domain Specific Languages. Kermeta is an extension to MOF allowing executable actions to be attached to EMOF meta-models, hence making it possible to also model a DSL operational semantics and readily obtain an interpreter for it.\n\nJMI defines a Java API for manipulating MOF models.\n\nOMG's MOF is not to be confused with the Managed Object Format (MOF) defined by the Distributed Management Task Force (DMTF) in section 6 of the Common Information Model (CIM) Infrastructure Specification, version 2.5.0.\n\n\n"}
{"id": "6387477", "url": "https://en.wikipedia.org/wiki?curid=6387477", "title": "No-broadcast theorem", "text": "No-broadcast theorem\n\nIn physics, the no-broadcast theorem is a result of quantum information theory. In the case of pure quantum states, it is a corollary of the no-cloning theorem: since quantum states cannot be copied in general, they cannot be broadcast. Here, the word \"broadcast\" is used in the sense of conveying the state to two or more recipients. For multiple recipients to each receive the state, there must be, in some sense, a way of duplicating the state. The no-broadcast theorem generalizes the no-cloning theorem for mixed states.\n\nThe no-cloning theorem says that it is impossible to create two copies of an unknown state given a single copy of the state. \n\nThe no-broadcast theorem says that, given a single copy of a state drawn from a restricted non-commuting set, it is impossible to create a state such that one part of it is the same as the original state and the other part is also the same as the original state. That is, given an initial state formula_1 it is impossible to create a state formula_2 in a Hilbert space formula_3 such that the partial trace formula_4 and formula_5. Remarkably, the theorem does not hold if more than one copy of the initial state are provided: for example, broadcasting six copies starting from four copies of the original state is allowed, even if the states are drawn from a non-commuting set. The purity of the state can even be increased in the process, a phenomenon known as superbroadcasting.\n\n\n"}
{"id": "40475757", "url": "https://en.wikipedia.org/wiki?curid=40475757", "title": "PKIoverheid", "text": "PKIoverheid\n\nPKIoverheid is the Public-key infrastructure (PKI) from the Dutch government. Like any other PKI, the system issues and manages digital certificates such that they can be realized. PKIoverheid is run by .\n\n"}
{"id": "438476", "url": "https://en.wikipedia.org/wiki?curid=438476", "title": "Path integral formulation", "text": "Path integral formulation\n\nThe path integral formulation of quantum mechanics is a description of quantum theory that generalizes the action principle of classical mechanics. It replaces the classical notion of a single, unique classical trajectory for a system with a sum, or functional integral, over an infinity of quantum-mechanically possible trajectories to compute a quantum amplitude.\n\nThis formulation has proven crucial to the subsequent development of theoretical physics, because manifest Lorentz covariance (time and space components of quantities enter equations in the same way) is easier to achieve than in the operator formalism of canonical quantization. Unlike previous methods, the path integral allows a physicist to easily change coordinates between very different canonical descriptions of the same quantum system. Another advantage is that it is in practice easier to guess the correct form of the Lagrangian of a theory, which naturally enters the path integrals (for interactions of a certain type, these are \"coordinate space\" or \"Feynman path integrals\"), than the Hamiltonian. Possible downsides of the approach include that unitarity (this is related to conservation of probability; the probabilities of all physically possible outcomes must add up to one) of the S-matrix is obscure in the formulation. The path-integral approach has been proved to be equivalent to the other formalisms of quantum mechanics and quantum field theory. Thus, by \"deriving\" either approach from the other, problems associated with one or the other approach (as exemplified by Lorentz covariance or unitarity) go away.\n\nThe path integral also relates quantum and stochastic processes, and this provided the basis for the grand synthesis of the 1970s, which unified quantum field theory with the statistical field theory of a fluctuating field near a second-order phase transition. The Schrödinger equation is a diffusion equation with an imaginary diffusion constant, and the path integral is an analytic continuation of a method for summing up all possible random walks.\n\nThe basic idea of the path integral formulation can be traced back to Norbert Wiener, who introduced the Wiener integral for solving problems in diffusion and Brownian motion. This idea was extended to the use of the Lagrangian in quantum mechanics by P. A. M. Dirac in his 1933 article. The complete method was developed in 1948 by Richard Feynman. Some preliminaries were worked out earlier in his doctoral work under the supervision of John Archibald Wheeler. The original motivation stemmed from the desire to obtain a quantum-mechanical formulation for the Wheeler–Feynman absorber theory using a Lagrangian (rather than a Hamiltonian) as a starting point.\n\nIn quantum mechanics, as in classical mechanics, the Hamiltonian is the generator of time translations. This means that the state at a slightly later time differs from the state at the current time by the result of acting with the Hamiltonian operator (multiplied by the negative imaginary unit, ). For states with a definite energy, this is a statement of the de Broglie relation between frequency and energy, and the general relation is consistent with that plus the superposition principle.\n\nThe Hamiltonian in classical mechanics is derived from a Lagrangian, which is a more fundamental quantity relative to special relativity. The Hamiltonian indicates how to march forward in time, but the time is different in different reference frames. The Lagrangian is a Lorentz scalar, while the Hamiltonian is the time component of a four-vector. So the Hamiltonian is different in different frames, and this type of symmetry is not apparent in the original formulation of quantum mechanics.\n\nThe Hamiltonian is a function of the position and momentum at one time, and it determines the position and momentum a little later. The Lagrangian is a function of the position now and the position a little later (or, equivalently for infinitesimal time separations, it is a function of the position and velocity). The relation between the two is by a Legendre transformation, and the condition that determines the classical equations of motion (the Euler–Lagrange equations) is that the action has an extremum.\n\nIn quantum mechanics, the Legendre transform is hard to interpret, because the motion is not over a definite trajectory. In classical mechanics, with discretization in time, the Legendre transform becomes\n\nand\n\nwhere the partial derivative with respect to formula_3 holds fixed. The inverse Legendre transform is\n\nwhere\n\nand the partial derivative now is with respect to at fixed .\n\nIn quantum mechanics, the state is a superposition of different states with different values of , or different values of , and the quantities and can be interpreted as noncommuting operators. The operator is only definite on states that are indefinite with respect to . So consider two states separated in time and act with the operator corresponding to the Lagrangian:\n\nIf the multiplications implicit in this formula are reinterpreted as \"matrix\" multiplications, the first factor is\n\nand if this is also interpreted as a matrix multiplication, the sum over all states integrates over all , and so it takes the Fourier transform in to change basis to . That is the action on the Hilbert space – change basis to at time .\n\nNext comes\n\nor evolve an infinitesimal time into the future.\n\nFinally, the last factor in this interpretation is\n\nwhich means change basis back to at a later time.\n\nThis is not very different from just ordinary time evolution: the factor contains all the dynamical information – it pushes the state forward in time. The first part and the last part are just Fourier transforms to change to a pure basis from an intermediate basis.\nAnother way of saying this is that since the Hamiltonian is naturally a function of and , exponentiating this quantity and changing basis from to at each step allows the matrix element of to be expressed as a simple function along each path. This function is the quantum analog of the classical action. This observation is due to Paul Dirac.\n\nDirac further noted that one could square the time-evolution operator in the representation:\n\nand this gives the time-evolution operator between time and time . While in the representation the quantity that is being summed over the intermediate states is an obscure matrix element, in the representation it is reinterpreted as a quantity associated to the path. In the limit that one takes a large power of this operator, one reconstructs the full quantum evolution between two states, the early one with a fixed value of and the later one with a fixed value of . The result is a sum over paths with a phase, which is the quantum action. Crucially, Dirac identified in this article the deep quantum-mechanical reason for the principle of least action controlling the classical limit (see quotation box).\n\nDirac's work did not provide a precise prescription to calculate the sum over paths, and he did not show that one could recover the Schrödinger equation or the canonical commutation relations from this rule. This was done by Feynman. That is, the classical path arises naturally in the classical limit.\n\nFeynman showed that Dirac's quantum action was, for most cases of interest, simply equal to the classical action, appropriately discretized. This means that the classical action is the phase acquired by quantum evolution between two fixed endpoints. He proposed to recover all of quantum mechanics from the following postulates:\n\nIn order to find the overall probability amplitude for a given process, then, one adds up, or integrates, the amplitude of the 3rd postulate over the space of \"all\" possible paths of the system in between the initial and final states, including those that are absurd by classical standards. In calculating the probability amplitude for a single particle to go from one space-time coordinate to another, it is correct to include paths in which the particle describes elaborate curlicues, curves in which the particle shoots off into outer space and flies back again, and so forth. The path integral assigns to all these amplitudes \"equal weight\" but varying phase, or argument of the complex number. Contributions from paths wildly different from the classical trajectory may be suppressed by interference (see below).\n\nFeynman showed that this formulation of quantum mechanics is equivalent to the canonical approach to quantum mechanics when the Hamiltonian is at most quadratic in the momentum. An amplitude computed according to Feynman's principles will also obey the Schrödinger equation for the Hamiltonian corresponding to the given action.\n\nThe path integral formulation of quantum field theory represents the transition amplitude (corresponding to the classical correlation function) as a weighted sum of all possible histories of the system from the initial to the final state. A Feynman diagram is a graphical representation of a perturbative contribution to the transition amplitude.\n\nOne common approach to deriving the path integral formula is to divide the time interval into small pieces. Once this is done, the Trotter product formula tells us that the noncommutativity of the kinetic and potential energy operators can be ignored.\n\nFor a particle in a smooth potential, the path integral is approximated by zigzag paths, which in one dimension is a product of ordinary integrals. For the motion of the particle from position at time to at time , the time sequence\ncan be divided up into smaller segments , where , of fixed duration\n\nThis process is called \"time-slicing\".\n\nAn approximation for the path integral can be computed as proportional to\n\nwhere is the Lagrangian of the one-dimensional system with position variable and velocity considered (see below), and corresponds to the position at the th time step, if the time integral is approximated by a sum of terms.\n\nIn the limit , this becomes a functional integral, which, apart from a nonessential factor, is directly the product of the probability amplitudes (more precisely, since one must work with a continuous spectrum, the respective densities) to find the quantum mechanical particle at in the initial state and at in the final state .\n\nActually is the classical Lagrangian of the one-dimensional system considered,\nand the abovementioned \"zigzagging\" corresponds to the appearance of the terms\n\nin the Riemann sum approximating the time integral, which are finally integrated over to with the integration measure , is an arbitrary value of the interval corresponding to , e.g. its center, .\n\nThus, in contrast to classical mechanics, not only does the stationary path contribute, but actually all virtual paths between the initial and the final point also contribute.\n\nIn terms of the wave function in the position representation, the path integral formula reads as follows:\nwhere formula_17 denotes integration over all paths formula_18 with formula_19 and where formula_20 is a normalization factor. Here formula_21 is the action, given by\n\nThe path integral representation gives the quantum amplitude to go from point to point as an integral over all paths. For a free-particle action (for simplicity let , )\n\nthe integral can be evaluated explicitly.\n\nTo do this, it is convenient to start without the factor in the exponential, so that large deviations are suppressed by small numbers, not by cancelling oscillatory contributions:\n\nSplitting the integral into time slices:\n\nwhere the is interpreted as a finite collection of integrations at each integer multiple of . Each factor in the product is a Gaussian as a function of centered at with variance . The multiple integrals are a repeated convolution of this Gaussian with copies of itself at adjacent times:\n\nwhere the number of convolutions is . The result is easy to evaluate by taking the Fourier transform of both sides, so that the convolutions become multiplications:\n\nThe Fourier transform of the Gaussian is another Gaussian of reciprocal variance:\n\nand the result is\n\nThe Fourier transform gives , and it is a Gaussian again with reciprocal variance:\n\nThe proportionality constant is not really determined by the time-slicing approach, only the ratio of values for different endpoint choices is determined. The proportionality constant should be chosen to ensure that between each two time slices the time evolution is quantum-mechanically unitary, but a more illuminating way to fix the normalization is to consider the path integral as a description of a stochastic process.\n\nThe result has a probability interpretation. The sum over all paths of the exponential factor can be seen as the sum over each path of the probability of selecting that path. The probability is the product over each segment of the probability of selecting that segment, so that each segment is probabilistically independently chosen. The fact that the answer is a Gaussian spreading linearly in time is the central limit theorem, which can be interpreted as the first historical evaluation of a statistical path integral.\n\nThe probability interpretation gives a natural normalization choice. The path integral should be defined so that\n\nThis condition normalizes the Gaussian and produces a kernel that obeys the diffusion equation:\n\nFor oscillatory path integrals, ones with an in the numerator, the time slicing produces convolved Gaussians, just as before. Now, however, the convolution product is marginally singular, since it requires careful limits to evaluate the oscillating integrals. To make the factors well defined, the easiest way is to add a small imaginary part to the time increment . This is closely related to Wick rotation. Then the same convolution argument as before gives the propagation kernel:\n\nwhich, with the same normalization as before (not the sum-squares normalization – this function has a divergent norm), obeys a free Schrödinger equation:\n\nThis means that any superposition of s will also obey the same equation, by linearity. Defining\n\nthen obeys the free Schrödinger equation just as does:\n\nThe Lagrangian for the simple harmonic oscillator is\n\nWrite its trajectory as the classical trajectory plus some perturbation, and the action as . The classical trajectory can be written as\n\nThis trajectory yields the classical action\n\nNext, expand the non-classical contribution to the action as a Fourier series, which gives\n\nThis means that the propagator is\nfor some normalization\n\nUsing the infinite-product representation of the sinc function,\nthe propagator can be written as\n\nLet . One may write this propagator in terms of energy eigenstates as\n\nUsing the identities and , this amounts to\n\nOne may absorb all terms after the first into , thereby obtaining\n\nOne may finally expand in powers of : All terms in this expansion get multiplied by the factor in the front, yielding terms of the form\nComparison to the above eigenstate expansion yields the standard energy spectrum for the simple harmonic oscillator,\n\nFeynman's time-sliced approximation does not, however, exist for the most important quantum-mechanical path integrals of atoms, due to the singularity of the Coulomb potential at the origin. Only after replacing the time by another path-dependent pseudo-time parameter\nthe singularity is removed and a time-sliced approximation exists, which is exactly integrable, since it can be made harmonic by a simple coordinate transformation, as discovered in 1979 by İsmail Hakkı Duru and Hagen Kleinert. The combination of a path-dependent time transformation and a coordinate transformation is an important tool to solve many path integrals and is called generically the Duru–Kleinert transformation.\n\nThe path integral reproduces the Schrödinger equation for the initial and final state even when a potential is present. This is easiest to see by taking a path-integral over infinitesimally separated times.\n\nSince the time separation is infinitesimal and the cancelling oscillations become severe for large values of , the path integral has most weight for close to . In this case, to lowest order the potential energy is constant, and only the kinetic energy contribution is nontrivial. (This separation of the kinetic and potential energy terms in the exponent is essentially the Trotter product formula.) The exponential of the action is\n\nThe first term rotates the phase of locally by an amount proportional to the potential energy. The second term is the free particle propagator, corresponding to times a diffusion process. To lowest order in they are additive; in any case one has with (1):\n\nAs mentioned, the spread in is diffusive from the free particle propagation, with an extra infinitesimal rotation in phase which slowly varies from point to point from the potential:\n\nand this is the Schrödinger equation. Note that the normalization of the path integral needs to be fixed in exactly the same way as in the free particle case. An arbitrary continuous potential does not affect the normalization, although singular potentials require careful treatment.\n\nSince the states obey the Schrödinger equation, the path integral must reproduce the Heisenberg equations of motion for the averages of and variables, but it is instructive to see this directly. The direct approach shows that the expectation values calculated from the path integral reproduce the usual ones of quantum mechanics.\n\nStart by considering the path integral with some fixed initial state\n\nNow note that at each separate time is a separate integration variable. So it is legitimate to change variables in the integral by shifting: where is a different shift at each time but , since the endpoints are not integrated:\n\nThe change in the integral from the shift is, to first infinitesimal order in :\n\nwhich, integrating by parts in , gives:\n\nBut this was just a shift of integration variables, which doesn't change the value of the integral for any choice of . The conclusion is that this first order variation is zero for an arbitrary initial state and at any arbitrary point in time:\nthis is the Heisenberg equation of motion.\n\nIf the action contains terms which multiply and , at the same moment in time, the manipulations above are only heuristic, because the multiplication rules for these quantities is just as noncommuting in the path integral as it is in the operator formalism.\n\nIf the variation in the action exceeds by many orders of magnitude, we typically have destructive interference other than in the vicinity of those trajectories satisfying the Euler–Lagrange equation, which is now reinterpreted as the condition for constructive interference. This can be shown using the method of stationary phase applied to the propagator. As decreases, the exponential in the integral oscillates rapidly in the complex domain for any change in the action. Thus, in the limit that goes to zero, only points where the classical action does not vary contribute to the propagator.\n\nThe formulation of the path integral does not make it clear at first sight that the quantities and do not commute. In the path integral, these are just integration variables and they have no obvious ordering. Feynman discovered that the non-commutativity is still present.\n\nTo see this, consider the simplest path integral, the brownian walk. This is not yet quantum mechanics, so in the path-integral the action is not multiplied by :\n\nThe quantity is fluctuating, and the derivative is defined as the limit of a discrete difference.\n\nNote that the distance that a random walk moves is proportional to , so that:\nThis shows that the random walk is not differentiable, since the ratio that defines the derivative diverges with probability one.\n\nThe quantity is ambiguous, with two possible meanings:\n\nIn elementary calculus, the two are only different by an amount which goes to 0 as goes to 0. But in this case, the difference between the two is not 0:\n\ngive a name to the value of the difference for any one random walk:\n\nand note that is a rapidly fluctuating statistical quantity, whose average value is 1, i.e. a normalized \"Gaussian process\". The fluctuations of such a quantity can be described by a statistical Lagrangian\nand the equations of motion for derived from extremizing the action corresponding to just set it equal to 1. In physics, such a quantity is \"equal to 1 as an operator identity\". In mathematics, it \"weakly converges to 1\". In either case, it is 1 in any expectation value, or when averaged over any interval, or for all practical purpose.\n\nDefining the time order to \"be\" the operator order:\n\nThis is called the Itō lemma in stochastic calculus, and the (euclideanized) canonical commutation relations in physics.\n\nFor a general statistical action, a similar argument shows that\nand in quantum mechanics, the extra imaginary unit in the action converts this to the canonical commutation relation,\n\nFor a particle in curved space the kinetic term depends on the position, and the above time slicing cannot be applied, this being a manifestation of the notorious operator ordering problem in Schrödinger quantum mechanics. One may, however, solve this problem by transforming the time-sliced flat-space path integral to curved space using a multivalued coordinate transformation (nonholonomic mapping explained here).\n\nSometimes (e.g. a particle moving in curved space) we also have measure-theoretic factors in the functional integral:\nThis factor is needed to restore unitarity.\n\nFor instance, if\nthen it means that each spatial slice is multiplied by the measure . This measure cannot be expressed as a functional multiplying the measure because they belong to entirely different classes.\n\nIt is very common in path integrals to perform a Wick rotation from real to imaginary times. In the setting of quantum field theory, the Wick rotation changes the geometry of space-time from Lorentzian to Euclidean; as a result, Wick-rotated path integrals are often called Euclidean path integrals.\n\nIf we replace formula_73 by formula_74, the time-evolution operator formula_75 is replaced by formula_76. (This change is known as a Wick rotation.) If we repeat the derivation of the path-integral formula in this setting, we obtain\nwhere formula_78 is the Euclidean action, given by\nNote the sign change between this and the normal action, where the potential energy term is negative. (The term \"Euclidean\" is from the context of quantum field theory, where the change from real to imaginary time changes the space-time geometry from Lorentzian to Euclidean.)\n\nNow, the contribution of the kinetic energy to the path integral is as follows:\nwhere formula_81 includes all the remaining dependence of the integrand on the path. This integral has a rigorous mathematical interpretation as integration against the Wiener measure, denoted formula_82. The Wiener measure, constructed by Norbert Wiener gives a rigorous foundation to Einstein's mathematical model of Brownian motion. The subscript formula_83 indicates that the measure formula_84 is supported on paths formula_18 with formula_19.\n\nWe then have a rigorous version of the Feynman path integral, known as the Feynman–Kac formula:\n\nwhere now formula_88 satisfies the Wick-rotated version of the Schrödinger equation,\nAlthough the Wick-rotated Schrödinger equation does not have a direct physical meaning, interesting properties of the Schrödinger operator formula_90 can be extracted by studying it.\n\nMuch of the study of quantum field theories from the path-integral perspective, in both the mathematics and physics literatures, is done in the Euclidean setting, that is, after a Wick rotation. In particular, there are various results showing that if a Euclidean field theory with suitable properties can be constructed, one can then undo the Wick rotation to recover the physical, Lorentzian theory. On the other hand, it is much more difficult to give a meaning to path integrals (even Euclidean path integrals) in quantum field theory than in quantum mechanics.\n\nThe path integral is just the generalization of the integral above to all quantum mechanical problems—\nis the action of the classical problem in which one investigates the path starting at time and ending at time , and formula_17 denotes integration over all paths. In the classical limit, formula_93, the path of minimum action dominates the integral, because the phase of any path away from this fluctuates rapidly and different contributions cancel.\n\nThe connection with statistical mechanics follows. Considering only paths which begin and end in the same configuration, perform the Wick rotation , i.e., make time imaginary, and integrate over all possible beginning-ending configurations. The Wick-rotated path integral—described in the previous subsection, with the ordinary action replaced by its \"Euclidean\" counterpart—now resembles the partition function of statistical mechanics defined in a canonical ensemble with inverse temperature proportional to imaginary time, . Strictly speaking, though, this is the partition function for a statistical field theory.\n\nClearly, such a deep analogy between quantum mechanics and statistical mechanics cannot be dependent on the formulation. In the canonical formulation, one sees that the unitary evolution operator of a state is given by\n\nwhere the state is evolved from time . If one makes a Wick rotation here, and finds the amplitude to go from any state, back to the same state in (imaginary) time is given by\n\nwhich is precisely the partition function of statistical mechanics for the same system at temperature quoted earlier. One aspect of this equivalence was also known to Erwin Schrödinger who remarked that the equation named after him looked like the diffusion equation after Wick rotation. Note, however, that the Euclidean path integral is actually in the form of a \"classical\" statistical mechanics model.\n\nBoth the Schrödinger and Heisenberg approaches to quantum mechanics single out time and are not in the spirit of relativity. For example, the Heisenberg approach requires that scalar field operators obey the commutation relation\n\nfor two simultaneous spatial positions and , and this is not a relativistically invariant concept. The results of a calculation \"are\" covariant, but the symmetry is not apparent in intermediate stages. If naive field-theory calculations did not produce infinite answers in the continuum limit, this would not have been such a big problem – it would just have been a bad choice of coordinates. But the lack of symmetry means that the infinite quantities must be cut off, and the bad coordinates make it nearly impossible to cut off the theory without spoiling the symmetry. This makes it difficult to extract the physical predictions, which require a careful limiting procedure.\n\nThe problem of lost symmetry also appears in classical mechanics, where the Hamiltonian formulation also superficially singles out time. The Lagrangian formulation makes the relativistic invariance apparent. In the same way, the path integral is manifestly relativistic. It reproduces the Schrödinger equation, the Heisenberg equations of motion, and the canonical commutation relations and shows that they are compatible with relativity. It extends the Heisenberg-type operator algebra to operator product rules, which are new relations difficult to see in the old formalism.\n\nFurther, different choices of canonical variables lead to very different-seeming formulations of the same theory. The transformations between the variables can be very complicated, but the path integral makes them into reasonably straightforward changes of integration variables. For these reasons, the Feynman path integral has made earlier formalisms largely obsolete.\n\nThe price of a path integral representation is that the unitarity of a theory is no longer self-evident, but it can be proven by changing variables to some canonical representation. The path integral itself also deals with larger mathematical spaces than is usual, which requires more careful mathematics, not all of which has been fully worked out. The path integral historically was not immediately accepted, partly because it took many years to incorporate fermions properly. This required physicists to invent an entirely new mathematical object – the Grassmann variable – which also allowed changes of variables to be done naturally, as well as allowing constrained quantization.\n\nThe integration variables in the path integral are subtly non-commuting. The value of the product of two field operators at what looks like the same point depends on how the two points are ordered in space and time. This makes some naive identities fail.\n\nIn relativistic theories, there is both a particle and field representation for every theory. The field representation is a sum over all field configurations, and the particle representation is a sum over different particle paths.\n\nThe nonrelativistic formulation is traditionally given in terms of particle paths, not fields. There, the path integral in the usual variables, with fixed boundary conditions, gives the probability amplitude for a particle to go from point to point in time :\n\nThis is called the propagator. Superposing different values of the initial position with an arbitrary initial state constructs the final state:\n\nFor a spatially homogeneous system, where is only a function of , the integral is a convolution, the final state is the initial state convolved with the propagator:\n\nFor a free particle of mass , the propagator can be evaluated either explicitly from the path integral or by noting that the Schrödinger equation is a diffusion equation in imaginary time, and the solution must be a normalized Gaussian:\n\nTaking the Fourier transform in produces another Gaussian:\n\nand in -space the proportionality factor here is constant in time, as will be verified in a moment. The Fourier transform in time, extending to be zero for negative times, gives Green's function, or the frequency-space propagator:\n\nwhich is the reciprocal of the operator that annihilates the wavefunction in the Schrödinger equation, which wouldn't have come out right if the proportionality factor weren't constant in the -space representation.\n\nThe infinitesimal term in the denominator is a small positive number, which guarantees that the inverse Fourier transform in will be nonzero only for future times. For past times, the inverse Fourier transform contour closes toward values of where there is no singularity. This guarantees that propagates the particle into the future and is the reason for the subscript \"F\" on . The infinitesimal term can be interpreted as an infinitesimal rotation toward imaginary time.\n\nIt is also possible to reexpress the nonrelativistic time evolution in terms of propagators going toward the past, since the Schrödinger equation is time-reversible. The past propagator is the same as the future propagator except for the obvious difference that it vanishes in the future, and in the Gaussian is replaced by . In this case, the interpretation is that these are the quantities to convolve the final wavefunction so as to get the initial wavefunction:\nGiven the nearly identical only change is the sign of and , the parameter in Green's function can either be the energy if the paths are going toward the future, or the negative of the energy if the paths are going toward the past.\n\nFor a nonrelativistic theory, the time as measured along the path of a moving particle and the time as measured by an outside observer are the same. In relativity, this is no longer true. For a relativistic theory the propagator should be defined as the sum over all paths that travel between two points in a fixed proper time, as measured along the path (these paths describe the trajectory of a particle in space and in time):\n\nThe usual definition of the relativistic propagator only asks for the amplitude is to travel from to , after summing over all the possible proper times it could take:\nwhere is a weight factor, the relative importance of paths of different proper time. By the translation symmetry in proper time, this weight can only be an exponential factor and can be absorbed into the constant :\n\nThis is the Schwinger representation. Taking a Fourier transform over the variable can be done for each value of separately, and because each separate contribution is a Gaussian, gives whose Fourier transform is another Gaussian with reciprocal width. So in -space, the propagator can be reexpressed simply:\n\nwhich is the Euclidean propagator for a scalar particle. Rotating to be imaginary gives the usual relativistic propagator, up to a factor of and an ambiguity, which will be clarified below:\n\nThis expression can be interpreted in the nonrelativistic limit, where it is convenient to split it by partial fractions:\n\nFor states where one nonrelativistic particle is present, the initial wavefunction has a frequency distribution concentrated near . When convolving with the propagator, which in space just means multiplying by the propagator, the second term is suppressed and the first term is enhanced. For frequencies near , the dominant first term has the form\n\nThis is the expression for the nonrelativistic Green's function of a free Schrödinger particle.\n\nThe second term has a nonrelativistic limit also, but this limit is concentrated on frequencies that are negative. The second pole is dominated by contributions from paths where the proper time and the coordinate time are ticking in an opposite sense, which means that the second term is to be interpreted as the antiparticle. The nonrelativistic analysis shows that with this form the antiparticle still has positive energy.\n\nThe proper way to express this mathematically is that, adding a small suppression factor in proper time, the limit where of the first term must vanish, while the limit of the second term must vanish. In the Fourier transform, this means shifting the pole in slightly, so that the inverse Fourier transform will pick up a small decay factor in one of the time directions:\n\nWithout these terms, the pole contribution could not be unambiguously evaluated when taking the inverse Fourier transform of . The terms can be recombined:\n\nwhich when factored, produces opposite-sign infinitesimal terms in each factor. This is the mathematically precise form of the relativistic particle propagator, free of any ambiguities. The term introduces a small imaginary part to the , which in the Minkowski version is a small exponential suppression of long paths.\n\nSo in the relativistic case, the Feynman path-integral representation of the propagator includes paths going backwards in time, which describe antiparticles. The paths that contribute to the relativistic propagator go forward and backwards in time, and the interpretation of this is that the amplitude for a free particle to travel between two points includes amplitudes for the particle to fluctuate into an antiparticle, travel back in time, then forward again.\n\nUnlike the nonrelativistic case, it is impossible to produce a relativistic theory of local particle propagation without including antiparticles. All local differential operators have inverses that are nonzero outside the light cone, meaning that it is impossible to keep a particle from travelling faster than light. Such a particle cannot have a Green's function which is only nonzero in the future in a relativistically invariant theory.\n\nHowever, the path integral formulation is also extremely important in \"direct\" application to quantum field theory, in which the \"paths\" or histories being considered are not the motions of a single particle, but the possible time evolutions of a field over all space. The action is referred to technically as a functional of the field: , where the field is itself a function of space and time, and the square brackets are a reminder that the action depends on all the field's values everywhere, not just some particular value. \"One\" such given function of spacetime is called a \"field configuration\". In principle, one integrates Feynman's amplitude over the class of all possible field configurations.\n\nMuch of the formal study of QFT is devoted to the properties of the resulting functional integral, and much effort (not yet entirely successful) has been made toward making these functional integrals mathematically precise.\n\nSuch a functional integral is extremely similar to the partition function in statistical mechanics. Indeed, it is sometimes \"called\" a partition function, and the two are essentially mathematically identical except for the factor of in the exponent in Feynman's postulate 3. Analytically continuing the integral to an imaginary time variable (called a Wick rotation) makes the functional integral even more like a statistical partition function and also tames some of the mathematical difficulties of working with these integrals.\n\nIn quantum field theory, if the action is given by the functional of field configurations (which only depends locally on the fields), then the time-ordered vacuum expectation value of polynomially bounded functional , , is given by\n\nThe symbol here is a concise way to represent the infinite-dimensional integral over all possible field configurations on all of space-time. As stated above, the unadorned path integral in the denominator ensures proper normalization.\n\nStrictly speaking, the only question that can be asked in physics is: \"What fraction of states satisfying condition also satisfy condition ?\" The answer to this is a number between 0 and 1, which can be interpreted as a conditional probability, written as . In terms of path integration, since , this means\n\nwhere the functional is the superposition of all incoming states that could lead to the states we are interested in. In particular, this could be a state corresponding to the state of the Universe just after the Big Bang, although for actual calculation this can be simplified using heuristic methods. Since this expression is a quotient of path integrals, it is naturally normalised.\n\nSince this formulation of quantum mechanics is analogous to classical action principle, one might expect that identities concerning the action in classical mechanics would have quantum counterparts derivable from a functional integral. This is often the case.\n\nIn the language of functional analysis, we can write the Euler–Lagrange equations as\n(the left-hand side is a functional derivative; the equation means that the action is stationary under small changes in the field configuration). The quantum analogues of these equations are called the Schwinger–Dyson equations.\n\nIf the functional measure turns out to be translationally invariant (we'll assume this for the rest of this article, although this does not hold for, let's say nonlinear sigma models), and if we assume that after a Wick rotation\n\nwhich now becomes\nfor some , it goes to zero faster than a reciprocal of any polynomial for large values of , then we can integrate by parts (after a Wick rotation, followed by a Wick rotation back) to get the following Schwinger–Dyson equations for the expectation:\n\nfor any polynomially-bounded functional . In the deWitt notation this looks like\n\nThese equations are the analog of the on-shell EL equations. The time ordering is taken before the time derivatives inside the .\n\nIf (called the source field) is an element of the dual space of the field configurations (which has at least an affine structure because of the assumption of the translational invariance for the functional measure), then the generating functional of the source fields is defined to be\n\nNote that\n\nor\n\nwhere\n\nBasically, if is viewed as a functional distribution (this shouldn't be taken too literally as an interpretation of QFT, unlike its Wick-rotated statistical mechanics analogue, because we have time ordering complications here!), then are its moments, and is its Fourier transform.\n\nIf is a functional of , then for an operator , is defined to be the operator that substitutes for . For example, if\n\nand is a functional of , then\n\nThen, from the properties of the functional integrals\n\nwe get the \"master\" Schwinger–Dyson equation:\n\nor\n\nIf the functional measure is not translationally invariant, it might be possible to express it as the product , where is a functional and is a translationally invariant measure. This is true, for example, for nonlinear sigma models where the target space is diffeomorphic to . However, if the target manifold is some topologically nontrivial space, the concept of a translation does not even make any sense.\n\nIn that case, we would have to replace the in this equation by another functional\n\nIf we expand this equation as a Taylor series about \"J\" 0, we get the entire set of Schwinger–Dyson equations.\n\nThe path integrals are usually thought of as being the sum of all paths through an infinite space–time. However, in local quantum field theory we would restrict everything to lie within a finite \"causally complete\" region, for example inside a double light-cone. This gives a more mathematically precise and physically rigorous definition of quantum field theory.\n\nNow how about the on shell Noether's theorem for the classical case? Does it have a quantum analog as well? Yes, but with a caveat. The functional measure would have to be invariant under the one parameter group of symmetry transformation as well.\n\nLet's just assume for simplicity here that the symmetry in question is local (not local in the sense of a gauge symmetry, but in the sense that the transformed value of the field at any given point under an infinitesimal transformation would only depend on the field configuration over an arbitrarily small neighborhood of the point in question). Let's also assume that the action is local in the sense that it is the integral over spacetime of a Lagrangian, and that\nfor some function where only depends locally on (and possibly the spacetime position).\n\nIf we don't assume any special boundary conditions, this would not be a \"true\" symmetry in the true sense of the term in general unless or something. Here, is a derivation which generates the one parameter group in question. We could have antiderivations as well, such as BRST and supersymmetry.\n\nLet's also assume\nfor any polynomially-bounded functional . This property is called the invariance of the measure. And this does not hold in general. See anomaly (physics) for more details.\n\nThen,\n\nwhich implies\n\nwhere the integral is over the boundary. This is the quantum analog of Noether's theorem.\n\nNow, let's assume even further that is a local integral\n\nwhere\n\nso that\n\nwhere\n\n(this is assuming the Lagrangian only depends on and its first partial derivatives! More general Lagrangians would require a modification to this definition!). Note that we're NOT insisting that is the generator of a symmetry (i.e. we are \"not\" insisting upon the gauge principle), but just that is. And we also assume the even stronger assumption that the functional measure is locally invariant:\n\nThen, we would have\n\nAlternatively,\n\nThe above two equations are the Ward–Takahashi identities.\n\nNow for the case where , we can forget about all the boundary conditions and locality assumptions. We'd simply have\n\nAlternatively,\n\nPath integrals as they are defined here require the introduction of regulators. Changing the scale of the regulator leads to the renormalization group. In fact, renormalization is the major obstruction to making path integrals well-defined.\n\nIn one interpretation of quantum mechanics, the \"sum over histories\" interpretation, the path integral is taken to be fundamental, and reality is viewed as a single indistinguishable \"class\" of paths that all share the same events. For this interpretation, it is crucial to understand what exactly an event is. The sum-over-histories method gives identical results to canonical quantum mechanics, and Sinha and Sorkin claim the interpretation explains the Einstein–Podolsky–Rosen paradox without resorting to nonlocality.\n\nSome advocates of interpretations of quantum mechanics emphasizing decoherence have attempted to make more rigorous the notion of extracting a classical-like \"coarse-grained\" history from the space of all possible histories.\n\nWhereas in quantum mechanics the path integral formulation is fully equivalent to other formulations, it may be that it can be extended to quantum gravity, which would make it different from the Hilbert space model. Feynman had some success in this direction, and his work has been extended by Hawking and others. Approaches that use this method include causal dynamical triangulations and spinfoam models.\n\nQuantum tunnelling can be modeled by using the path integral formation to determine the action of the trajectory through a potential barrier. Using the WKB approximation, the tunneling rate () can be determined to be of the form\n\nwith the effective action and pre-exponential factor . This form is specifically useful in a dissipative system, in which the systems and surroundings must be modeled together. Using the Langevin equation to model Brownian motion, the path integral formation can be used to determine an effective action and pre-exponential model to see the effect of dissipation on tunnelling. From this model, tunneling rates of macroscopic systems (at finite temperatures) can be predicted.\n\n\n\n"}
{"id": "56424327", "url": "https://en.wikipedia.org/wiki?curid=56424327", "title": "Paul Chernoff", "text": "Paul Chernoff\n\nPaul Robert Chernoff (21 June 1942, Philadelphia – 17 January 2017) was an American mathematician, specializing in functional analysis and the mathematical foundations of quantum mechanics. He is known for Chernoff's Theorem, a mathematical result in the Feynman path integral formulation of quantum mechanics.\n\nChernoff graduated from Central High School in Philadelphia. He matriculated at Harvard University, where he received bachelor's degree \"summa cum laude\" in 1963, master's degree in 1965, and Ph.D. in 1968 under George Mackey with thesis \"Semigroup Product Formulas and Addition of Unbounded Operators\".\n\nAt the University of California, Berkeley he became in 1969 a lecturer, in 1971 an assistant professor, and in 1980 a full professor. U. C. Berkeley awarded him multiple Distinguished Teaching Awards and the Lili Fabilli and Eric Hoffer Essay Prize. In 1986 he was a visiting professor at the University of Pennsylvania.\n\nChernoff was elected in 1984 a Fellow of the American Association for the Advancement of Science and in 2012 a Fellow of the American Mathematical Society.\n\nHe gave in 1981 a simplified proof of the Groenewold-Van Hove theorem, which is a no-go theorem that relates classical mechanics to quantum mechanics.\n\n"}
{"id": "173525", "url": "https://en.wikipedia.org/wiki?curid=173525", "title": "Probabilistic method", "text": "Probabilistic method\n\nThe probabilistic method is a nonconstructive method, primarily used in combinatorics and pioneered by Paul Erdős, for proving the existence of a prescribed kind of mathematical object. It works by showing that if one randomly chooses objects from a specified class, the probability that the result is of the prescribed kind is strictly greater than zero. Although the proof uses probability, the final conclusion is determined for \"certain\", without any possible error.\n\nThis method has now been applied to other areas of mathematics such as number theory, linear algebra, and real analysis, as well as in computer science (e.g. randomized rounding), and information theory.\n\nIf every object in a collection of objects fails to have a certain property, then the probability that a random object chosen from the collection has that property is zero.\n\nSimilarly, showing that the probability is (strictly) less than 1 can be used to prove the existence of an object that does \"not\" satisfy the prescribed properties.\n\nAnother way to use the probabilistic method is by calculating the expected value of some random variable. If it can be shown that the random variable can take on a value less than the expected value, this proves that the random variable can also take on some value greater than the expected value.\n\nCommon tools used in the probabilistic method include Markov's inequality, the Chernoff bound, and the Lovász local lemma.\n\nAlthough others before him proved theorems via the probabilistic method (for example, Szele's 1943 result that there exist tournaments containing a large number of Hamiltonian cycles), many of the most well known proofs using this method are due to Erdős. Indeed, the Alon-Spencer textbook on the subject has his picture on the cover to highlight the method's association with Erdős. The first example below describes one such result from 1947 that gives a proof of a lower bound for the Ramsey number .\n\nSuppose we have a complete graph on vertices. We wish to show (for small enough values of ) that it is possible to color the edges of the graph in two colors (say red and blue) so that there is no complete subgraph on vertices which is monochromatic (every edge colored the same color).\n\nTo do so, we color the graph randomly. Color each edge independently with probability of being red and of being blue. We calculate the expected number of monochromatic subgraphs on vertices as follows:\n\nFor any set of vertices from our graph, define the variable to be if every edge amongst the vertices is the same color, and otherwise. Note that the number of monochromatic -subgraphs is the sum of over all possible subsets. For any , the expected value of is simply the probability that all of the\n\nedges in are the same color,\n\n(the factor of comes because there are two possible colors).\n\nThis holds true for any of the possible subsets we could have chosen, so we have that the sum of over all is\n\nThe sum of an expectation is the expectation of the sum (\"regardless\" of whether the variables are independent), so the expectation of the sum (the expected number of monochromatic -subgraphs) is\n\nConsider what happens if this value is less than . Since the expected number of monochromatic -subgraphs is strictly less than 1, it must be that a specific random coloring satisfies that the number of monochromatic -subgraphs is strictly less than 1. The number of monochromatic -subgraphs in this random coloring is a non-negative integer, hence it must be 0 (0 is the only non-negative integer less than 1). It follows that if :formula_5, (which holds, for example, for =5 and =4) there must exist a coloring in which there are no monochromatic -subgraphs. \n\nBy definition of the Ramsey number, this implies that must be bigger than . In particular, must grow at least exponentially with .\n\nA peculiarity of this argument is that it is entirely nonconstructive. Even though it proves (for example) that almost every coloring of the complete graph on vertices contains no monochromatic -subgraph, it gives no explicit example of such a coloring. The problem of finding such a coloring has been open for more than 50 years.\n\nA 1959 paper of Erdős (see reference cited below) addressed the following problem in graph theory: given positive integers and , does there exist a graph containing only cycles of length at least , such that the chromatic number of is at least ?\n\nIt can be shown that such a graph exists for any and , and the proof is reasonably simple. Let be very large and consider a random graph on vertices, where every edge in exists with probability . We show that with positive probability, a graph satisfies the following two properties:\n\nProof. Let be the number cycles of length less than . Number of cycles of length in the complete graph on vertices is\n\nand each of them is present in with probability . Hence by Markov's inequality we have\n\nProof. Let be the size of the largest independent set in . Clearly, we have\n\nwhen\n\nFor sufficiently large , the probability that a graph from the distribution has both properties is positive, as the events for these properties cannot be disjoint (if they were, their probabilities would sum up to more than 1).\n\nHere comes the trick: since has these two properties, we can remove at most vertices from to obtain a new graph on formula_11 vertices that contains only cycles of length at least . We can see that this new graph has no independent set of size formula_12. can only be partitioned into at least independent sets, and, hence, has chromatic number at least .\n\nThis result gives a hint as to why the computation of the chromatic number of a graph is so difficult: even when there are no local reasons (such as small cycles) for a graph to require many colors the chromatic number can still be arbitrarily large.\n\n\n"}
{"id": "2549312", "url": "https://en.wikipedia.org/wiki?curid=2549312", "title": "Provable prime", "text": "Provable prime\n\nIn number theory, a provable prime is an integer that has been calculated to be prime using a primality-proving algorithm. Contrast with probable prime, which is likely (but not certain) to be prime, based on the output of a probabilistic primality test. In principle, every prime number can be proved to be prime in polynomial time by using the AKS primality test. In practice, other methods which guarantee that their result is prime, but which do not work for all primes, are useful for the random generation of provable primes.\n\n"}
{"id": "11689633", "url": "https://en.wikipedia.org/wiki?curid=11689633", "title": "Roger W. Brockett", "text": "Roger W. Brockett\n\nRoger Ware Brockett (born October 22, 1938 in Seville, Ohio) is an American control theorist and the An Wang Professor of Computer Science and Electrical Engineering at Harvard University, who founded the Harvard Robotics Laboratory in 1983.\n\nBrockett received his B.S. in 1960, his M.S. in 1962, and his Ph.D. in 1964 (under the supervision of Mihajlo D. Mesarovic), all from Case Western Reserve University.\n\nAfter teaching at the Massachusetts Institute of Technology from 1963 to 1969, he joined the faculty at Harvard University where he became the Gordon McKay Professor of Applied Mathematics and in 1989 the An Wang Professor of Computer Science and Electrical Engineering.\n\nBrockett received several awards and honors, including:\n\n\n"}
{"id": "7252030", "url": "https://en.wikipedia.org/wiki?curid=7252030", "title": "Semi-simple operator", "text": "Semi-simple operator\n\nIn mathematics, a linear operator \"T\" on a finite-dimensional vector space is semi-simple if every \"T\"-invariant subspace has a complementary \"T\"-invariant subspace.\n\nAn important result regarding semi-simple operators is that, a linear operator on a finite dimensional vector space over an algebraically closed field is semi-simple if and only if it is diagonalizable. This is because such an operator always has an eigenvector; if it is, in addition, semi-simple, then it has a complementary invariant hyperplane, which itself has an eigenvector, and thus by induction is diagonalizable. Conversely, diagonalizable operators are easily seen to be semi-simple, as invariant subspaces are direct sums of eigenspaces, and any basis for this space can be extended to an eigenbasis.\n\n"}
{"id": "1575813", "url": "https://en.wikipedia.org/wiki?curid=1575813", "title": "Series expansion", "text": "Series expansion\n\nIn mathematics, a series expansion is a method for calculating a function that cannot be expressed by just elementary operators (addition, subtraction, multiplication and division).\n\nThe resulting so-called \"series\" often can be limited to a finite number of terms, thus yielding an approximation of the function. The fewer terms of the sequence are used, the simpler this approximation will be. Often, the resulting inaccuracy (i.e., the partial sum of the omitted terms) can be described by an equation involving Big O notation (see also asymptotic expansion). The series expansion on an open interval will also be an approximation for non-analytic functions.\n\nThere are several kinds of series expansions, such as:\n\n\nFor more details, refer to the articles mentioned.\n"}
{"id": "20768719", "url": "https://en.wikipedia.org/wiki?curid=20768719", "title": "Sharp-P-completeness of 01-permanent", "text": "Sharp-P-completeness of 01-permanent\n\nThe #P-completeness of 01-permanent, sometimes known as Valiant's theorem, is a mathematical proof about the permanent of matrices, considered a seminal result in computational complexity theory. In a 1979 scholarly paper, Leslie Valiant proved that the computational problem of computing the permanent of a matrix is #P-hard, even if the matrix is restricted to have entries that are all 0 or 1. In this restricted case, computing the permanent is even #P-complete, because it corresponds to the #P problem of counting the number of permutation matrices one can get by changing ones into zeroes.\n\nValiant's 1979 paper also introduced #P as a complexity class.\n\nOne reason for interest in the computational complexity of the permanent is that it provides an example of a problem where constructing a single solution can be done efficiently but where counting all solutions is hard. As Papadimitriou writes in his book \"Computational Complexity\":\n\nSpecifically, computing the permanent (shown to be difficult by Valiant's results) is closely connected with finding a perfect matching in a bipartite graph, which is solvable in polynomial time by the Hopcroft–Karp algorithm. For a bipartite graph with \"2n\" vertices partitioned into two parts with \"n\" vertices each, the number of perfect matchings equals the permanent of its biadjacency matrix and the square of the number of perfect matchings is equal to the permanent of its adjacency matrix. Since any 0–1 matrix is the biadjacency matrix of some bipartite graph, Valiant's theorem implies that the problem of counting the number of perfect matchings in a bipartite graph is #P-complete, and in conjunction with Toda's theorem this implies that it is hard for the entire polynomial hierarchy.\n\nThe computational complexity of the permanent also has some significance in other aspects of complexity theory: it is not known whether NC equals P (informally, whether every polynomially-solvable problem can be solved by a polylogarithmic-time parallel algorithm) and Ketan Mulmuley has suggested an approach to resolving this question that relies on writing the permanent as the determinant of a matrix.\n\nHartmann proved a generalization of Valiant's theorem concerning the complexity of computing immanants of matrices that generalize both the determinant and the permanent.\n\nBelow, the proof that computing the permanent of a 01-matrix is #P-complete is described. It mainly follows the proof by .\n\nAny square matrix formula_1 can be viewed as the adjacency matrix of a directed graph, with formula_2 representing the weight of the edge from vertex \"i\" to vertex \"j\". Then, the permanent of A is equal to the sum of the weights of all cycle-covers of the graph; this is a graph-theoretic interpretation of the permanent.\n\n\nIn order to prove that 01-Permanent is #P-hard, it is therefore sufficient to show that the number of satisfying assignments for a 3-CNF formula can be expressed succinctly as a function of the permanent of a matrix that contains only the values 0 and 1. This is usually accomplished in two steps:\n\nGiven a 3CNF-formula formula_5 with \"m\" clauses and \"n\" variables, one can construct a weighted, directed graph formula_3 such that\nThus if formula_11 is the number of satisfying assignments for formula_5, the permanent of this graph will be formula_13.\n\nThe graph construction makes use of a component that is treated as a \"black box.\" To keep the explanation simple, the properties of this component are given without actually defining the structure of the component.\n\nTo specify \"G\", one first constructs a variable node in \"G\" for each of the \"n\" variables in \"φ\". Additionally, for each of the \"m\" clauses in \"φ\", one constructs a clause component \"C\" in \"G\" that functions as a sort of \"black box.\" All that needs to be noted about \"C\" is that it has three input edges and three output edges. The input edges come either from variable nodes or from previous clause components (e.g., \"C\" for some \"o\" < \"j\") and the output edges go either to variable nodes or to later clause components (e.g., \"C\" for some formula_19). The first input and output edges correspond with the first variable of the clause \"j\", and so on. Thus far, all of the nodes that will appear in the graph \"G\" have been specified.\n\nNext, one would consider the edges. For each variable formula_20 of formula_5, one makes a true cycle (T-cycle) and a false cycle (F-cycle) in formula_3. To create the T-cycle, one starts at the variable node for formula_20 and draw an edge to the clause component formula_24 that corresponds to the first clause in which formula_20 appears. If formula_20 is the first variable in the clause of formula_5 corresponding to formula_24, this edge will be the first input edge of formula_24, and so on. Thence, draw an edge to the next clause component corresponding to the next clause of formula_5 in which formula_20 appears, connecting it from the appropriate output edge of formula_24 to the appropriate input edge of the next clause component, and so on. After the last clause in which formula_20 appears, we connect the appropriate output edge of the corresponding clause component back to formula_20's variable node. Of course, this completes the cycle. To create the F-cycle, one would follow the same procedure, but connect formula_20's variable node to those clause components in which ~formula_20 appears, and finally back to formula_20's variable node. All of these edges outside the clause components are termed \"external edges\", all of which have weight 1. Inside the clause components, the edges are termed \"internal edges\". Every external edge is part of a T-cycle or an F-cycle (but not both—that would force inconsistency).\n\nNote that the graph formula_3 is of size linear in formula_39, so the construction can be done in polytime (assuming that the clause components do not cause trouble).\n\nA useful property of formula_3 is that its cycle covers correspond to variable assignments for formula_5. For a cycle cover Z of formula_3, one can say that Z induces an assignment of values for the variables in formula_5 just in case Z contains all of the external edges in formula_20's T-cycle and none of the external edges in formula_20's F-cycle for all variables formula_20 that the assignment makes true, and vice versa for all variables formula_20 that the assignment makes false. Although any given cycle cover Z need not induce an assignment for formula_5, any one that does induces exactly one assignment, and the same assignment induced depends only on the external edges of Z. The term Z is considered an incomplete cycle cover at this stage, because one talks only about its external edges, M. In the section below, one considers M-completions to show that one has a set of cycle covers corresponding to each M that have the necessary properties.\n\nThe sort of Z's that don't induce assignments are the ones with cycles that \"jump\" inside the clause components. That is, if for every formula_24, at least one of formula_24's input edges is in Z, and every output edge of the clause components is in Z when the corresponding input edge is in Z, then Z is proper with respect to each clause component, and Z will produce a satisfying assignment for formula_5. This is because proper Z's contain either the complete T-cycle or the complete F-cycle of every variable formula_20 in formula_5 as well as each including edges going into and coming out of each clause component. Thus, these Z's assign either true or false (but never both) to each formula_20 and ensure that each clause is satisfied. Further, the sets of cycle covers corresponding to all such Z's have weight formula_9, and any other Z's have weight formula_56. The reasons for this depend on the construction of the clause components, and are outlined below.\n\nTo understand the relevant properties of the clause components formula_24, one needs the notion of an M-completion. A cycle cover Z induces a satisfying assignment just in case its external edges satisfy certain properties. For any cycle cover of formula_3, consider only its external edges, the subset M. Let M be a set of external edges. A set of internal edges L is an M-completion just in case formula_59 is a cycle cover of formula_3. Further, denote the set of all M-completions by formula_61 and the set of all resulting cycle covers of formula_3 by formula_63.\n\nRecall that construction of formula_3 was such that each external edge had weight 1, so the weight of formula_63, the cycle covers resulting from any M, depends only on the internal edges involved. We add here the premise that the construction of the clause components is such that the sum over possible M-completions of the weight of the internal edges in each clause component, where M is proper relative to the clause component, is 12. Otherwise the weight of the internal edges is 0. Since there are \"m\" clause components, and the selection of sets of internal edges, L, within each clause component is independent of the selection of sets of internal edges in other clause components, so one can multiply everything to get the weight of formula_63. So, the weight of each formula_63, where M induces a satisfying assignment, is formula_9. Further, where M does not induce a satisfying assignment, M is not proper with respect to some formula_24, so the product of the weights of internal edges in formula_63 will be formula_56.\n\nThe clause component is a weighted, directed graph with 7 nodes with edges weighted and nodes arranged to yield the properties specified above, and is given in Appendix A of Ben-Dor and Halevi (1993). Note that the internal edges here have weights drawn from the set formula_14; not all edges have 0–1 weights.\n\nFinally, since the sum of weights of all the sets of cycle covers inducing any particular satisfying assignment is 12, and the sum of weights of all other sets of cycle covers is 0, one has Perm(\"G\") = 12·(\"#φ\"). The following section reduces computing Perm(formula_3) to the permanent of a 01 matrix.\n\nThe above section has shown that Permanent is #P-hard. Through a series of reductions, any permanent can be reduced to the permanent of a matrix with entries only 0 or 1. This will prove that 01-Permanent is #P-hard as well.\n\nUsing modular arithmetic, convert an integer matrix \"A\" into an equivalent non-negative matrix formula_74 so that the permanent of formula_75 can be computed easily from the permanent of formula_74, as follows:\n\nLet formula_75 be an formula_78 integer matrix where no entry has a magnitude larger than formula_79.\n\nThe transformation of formula_75 into formula_74 is polynomial in formula_88 and formula_89, since the number of bits required to represent formula_90 is polynomial in formula_88 and formula_89\n\nAn example of the transformation and why it works is given below.\n\nHere, formula_95, formula_96, and formula_97, so formula_98. Thus\n\nNote how the elements are non-negative because of the modular arithmetic. It is simple to compute the permanent\n\nso formula_101. Then formula_84, so formula_103\n\nNote that any number can be decomposed into a sum of powers of 2. For example,\n\nThis fact is used to convert a non-negative matrix into an equivalent matrix whose entries are all powers of 2. The reduction can be expressed in terms of graphs equivalent to the matrices.\n\nLet formula_105 be a formula_88-node weighted directed graph with non-negative weights, where largest weight is formula_107. Every edge formula_108 with weight formula_109 is converted into an equivalent edge with weights in powers of 2 as follows:\n\nThis can be seen graphically in the Figure 1. The subgraph that replaces the existing edge contains formula_112 nodes and formula_113 edges.\n\nTo prove that this produces an equivalent graph formula_114 that has the same permanent as the original, one must show the correspondence between the cycle covers of formula_105 and formula_114.\n\nConsider some cycle-cover formula_117 in formula_105.\n\nNote that the size of formula_114 is polynomial in formula_88 and formula_134.\n\nThe objective here is to reduce a matrix whose entries are powers of 2 into an equivalent matrix containing only zeros and ones (i.e. a directed graph where each edge has a weight of 1).\n\nLet G be a formula_88-node directed graph where all the weights on edges are powers of two. Construct a graph, formula_114, where the weight of each edge is 1 and Perm(G) = Perm(G'). The size of this new graph, G', is polynomial in formula_88 and formula_138 where the maximal weight of any edge in graph G is formula_139.\n\nThis reduction is done locally at each edge in G that has a weight larger than 1. Let formula_140 be an edge in G with a weight formula_141. It is replaced by a subgraph formula_142 that is made up of formula_143 nodes and formula_144 edges as seen in Figure 2. Each edge in formula_142 has a weight of 1. Thus, the resulting graph G' contains only edges with a weight of 1.\n\nConsider some cycle-cover formula_117 in formula_105.\n\nQuantum computer scientist Scott Aaronson has proved #P-hardness of permanent using quantum methods.\n"}
{"id": "8853373", "url": "https://en.wikipedia.org/wiki?curid=8853373", "title": "Simplicial polytope", "text": "Simplicial polytope\n\nIn geometry, a simplicial polytope is a polytope whose facets are all simplices.\n\nFor example, a \"simplicial polyhedron\" in 3 dimensions contains only triangular faces and corresponds via Steinitz's theorem to a maximal planar graph.\n\nThey are topologically dual to simple polytopes. Polytopes which are both\nsimple and simplicial are either simplices or two-dimensional polygons.\n\nSimplicial polyhedra include:\n\nSimplicial tilings:\n\nSimplicial 4-polytopes include:\n\nSimplicial higher polytope families:\n\n"}
{"id": "15641067", "url": "https://en.wikipedia.org/wiki?curid=15641067", "title": "Super-recursive algorithm", "text": "Super-recursive algorithm\n\nIn computability theory, super-recursive algorithms are a generalization of ordinary algorithms that are more powerful, that is, compute more than Turing machines. The term was introduced by Mark Burgin, whose book \"Super-recursive algorithms\" develops their theory and presents several mathematical models. Turing machines and other mathematical models of conventional algorithms allow researchers to find properties of recursive algorithms and their computations. In a similar way, mathematical models of super-recursive algorithms, such as inductive Turing machines, allow researchers to find properties of super-recursive algorithms and their computations.\n\nBurgin, as well as other researchers (including Selim Akl, Eugene Eberbach, Peter Kugel, Jan van Leeuwen, Hava Siegelmann, Peter Wegner, and Jiří Wiedermann) who studied different kinds of super-recursive algorithms and contributed to the theory of super-recursive algorithms, have argued that super-recursive algorithms can be used to disprove the Church-Turing thesis, but this point of view has been criticized within the mathematical community and is not widely accepted.\n\nBurgin (2005: 13) uses the term recursive algorithms for algorithms that can be implemented on Turing machines, and uses the word \"algorithm\" in a more general sense. Then a super-recursive class of algorithms is \"a class of algorithms in which it is possible to compute functions not computable by any Turing machine\" (Burgin 2005: 107).\n\nSuper-recursive algorithms are closely related to hypercomputation \nin a way similar to the relationship between ordinary computation and ordinary algorithms. Computation is a process, while an algorithm is a finite constructive description of such a process. Thus a super-recursive algorithm defines a \"computational process (including processes of input and output) that cannot be realized by recursive algorithms.\" (Burgin 2005: 108). A more restricted definition demands that hypercomputation solves a supertask (see Copeland 2002; Hagar and Korolev 2007).\n\nSuper-recursive algorithms are also related to algorithmic schemes, which are more general than super-recursive algorithms. Burgin argues (2005: 115) that it is necessary to make a clear distinction between super-recursive algorithms and those algorithmic schemes that are not algorithms. Under this distinction, some types of hypercomputation are obtained by super-recursive algorithms, e.g., inductive Turing machines, while other types of hypercomputation are directed by algorithmic schemas, e.g., infinite time Turing machines. This explains how works on super-recursive algorithms are related to hypercomputation and vice versa. According to this argument, super-recursive algorithms are just one way of defining a hypercomputational process.\n\nExamples of super-recursive algorithms include (Burgin 2005: 132):\n\nExamples of algorithmic schemes include:\n\n\nFor examples of practical super-recursive algorithms, see the book of Burgin.\n\nInductive Turing machines implement an important class of super-recursive algorithms. An inductive Turing machine is a definite list of well-defined instructions for completing a task which, when given an initial state, will proceed through a well-defined series of successive states, eventually giving the final result. The difference between an inductive Turing machine and an ordinary Turing machine is that an ordinary Turing machine must stop when it has obtained its result, while in some cases an inductive Turing machine can continue to compute after obtaining the result, without stopping. Kleene called procedures that could run forever without stopping by the name \"calculation procedure or algorithm\" (Kleene 1952:137). Kleene also demanded that such an algorithm must eventually exhibit \"some object\" (Kleene 1952:137). Burgin argues that this condition is satisfied by inductive Turing machines, as their results are exhibited after a finite number of steps. The reason that inductive Turing machines cannot be instructed to halt when their final output is produced is that in some cases inductive Turing machines may not be able to tell at which step the result has been obtained.\n\nSimple inductive Turing machines are equivalent to other models of computation such as general Turing machines of Schmidhuber, trial and error predicates of Hilary Putnam, limiting partial recursive functions of Gold, and trial-and-error machines of Hintikka and Mutanen (1998). More advanced inductive Turing machines are much more powerful. There are hierarchies of inductive Turing machines that can decide membership in arbitrary sets of the arithmetical hierarchy (Burgin 2005). In comparison with other equivalent models of computation, simple inductive Turing machines and general Turing machines give direct constructions of computing automata that are thoroughly grounded in physical machines. In contrast, trial-and-error predicates, limiting recursive functions, and limiting partial recursive functions present only syntactic systems of symbols with formal rules for their manipulation. Simple inductive Turing machines and general Turing machines are related to limiting partial recursive functions and trial-and-error predicates as Turing machines are related to partial recursive functions and lambda calculus.\n\nThe non-halting computations of inductive Turing machines should not be confused with infinite-time computations (see, for example, Potgieter 2006). First, some computations of inductive Turing machines do halt. As in the case of conventional Turing machines, some halting computations give the result, while others do not. Even if it does not halt, an inductive Turing machine produces output from time to time. If this output stops changing, it is then considered the result of the computation.\n\nThere are two main distinctions between ordinary Turing machines and simple inductive Turing machines. The first distinction is that even simple inductive Turing machines can do much more than conventional Turing machines. The second distinction is that a conventional Turing machine will always determine (by coming to a final state) when the result is obtained, while a simple inductive Turing machine, in some cases (such as when \"computing\" something that cannot be computed by an ordinary Turing machine), will not be able to make this determination.\n\nA symbol sequence is computable in the limit if there is a finite, possibly non-halting program on a universal Turing machine that incrementally outputs every symbol of the sequence. This includes the dyadic expansion of π but still excludes most of the real numbers, because most cannot be described by a finite program. Traditional Turing machines with a write-only output tape cannot edit their previous outputs; generalized Turing machines, according to Jürgen Schmidhuber, can edit their output tape as well as their work tape. He defines the constructively describable symbol sequences as those that have a finite, non-halting program running on a generalized Turing machine, such that any output symbol eventually converges, that is, it does not change any more after some finite initial time interval. Schmidhuber (2000, 2002) uses this approach to define the set of formally describable or constructively computable universes or constructive theories of everything. Generalized Turing machines and simple inductive Turing machines are two classes of super-recursive algorithms that are the closest to recursive algorithms (Schmidhuber 2000).\n\nThe Church–Turing thesis in recursion theory relies on a particular definition of the term \"algorithm\". Based on definitions that are more general than the one commonly used in recursion theory, Burgin argues that super-recursive algorithms, such as inductive Turing machines disprove the Church–Turing thesis. He proves furthermore that super-recursive algorithms could theoretically provide even greater efficiency gains than using quantum algorithms.\n\nBurgin's interpretation of super-recursive algorithms has encountered opposition in the mathematical community. One critic is logician Martin Davis, who argues that Burgin's claims have been well understood \"for decades\". Davis states, \nDavis disputes Burgin's claims that sets at level formula_1 of the arithmetical hierarchy can be called computable, saying\n\n\n\n\n"}
{"id": "3234990", "url": "https://en.wikipedia.org/wiki?curid=3234990", "title": "TORQUE", "text": "TORQUE\n\nThe Terascale Open-source Resource and QUEue Manager (TORQUE) is a distributed resource manager providing control over batch jobs and distributed compute nodes. TORQUE can integrate with the non-commercial Maui Cluster Scheduler or the commercial Moab Workload Manager to improve overall utilization, scheduling and administration on a cluster. \n\nThe TORQUE community has extended the original PBS to extend scalability, fault tolerance, and functionality. Contributors include NCSA, OSC, USC, the US DOE, Sandia, PNNL, UB, TeraGrid, and other HPC organizations. As of June 2018, TORQUE is no longer open-source even though previously it was described by its developers as open-source software, using the OpenPBS version 2.3 license and as non-free software by the Debian Free Software Guidelines due to license issues.\n\nTORQUE provides enhancements over standard OpenPBS in the following areas:\n\n\n\n"}
{"id": "8244667", "url": "https://en.wikipedia.org/wiki?curid=8244667", "title": "Tarjan's strongly connected components algorithm", "text": "Tarjan's strongly connected components algorithm\n\nTarjan's algorithm is an algorithm in graph theory for finding the strongly connected components of a directed graph. It runs in linear time, matching the time bound for alternative methods including Kosaraju's algorithm and the path-based strong component algorithm. Tarjan's algorithm is named for its inventor, Robert Tarjan.\n\nThe algorithm takes a directed graph as input, and produces a partition of the graph's vertices into the graph's strongly connected components. Each vertex of the graph appears in exactly one of the strongly connected components. Any vertex that is not on a directed cycle forms a strongly connected component all by itself: for example, a vertex whose in-degree or out-degree is 0, or any vertex of an acyclic graph.\n\nThe basic idea of the algorithm is this: a depth-first search begins from an arbitrary start node (and subsequent depth-first searches are conducted on any nodes that have not yet been found). As usual with depth-first search, the search visits every node of the graph exactly once, declining to revisit any node that has already been visited. Thus, the collection of search trees is a spanning forest of the graph. The strongly connected components will be recovered as certain subtrees of this forest. The roots of these subtrees are called the \"roots\" of the strongly connected components. Any node of a strongly connected component might serve as the root, if it happens to be the first node of the component that is discovered by the search.\n\nNodes are placed on a stack in the order in which they are visited. When the depth-first search recursively visits a node v and its descendants, those nodes are not all necessarily popped from the stack when this recursive call returns. The crucial invariant property is that a node remains on the stack after it has been visited if and only if there exists a path in the input graph from it to some node earlier on the stack.\n\nAt the end of the call that visits v and its descendants, we know whether v itself has a path to any node earlier on the stack. If so, the call returns, leaving v on the stack to preserve the invariant. If not, then v must be the root of its strongly connected component, which consists of v together with any nodes later on the stack than v (such nodes all have paths back to v but not to any earlier node, because if they had paths to earlier nodes then v would also have paths to earlier nodes which is false). The connected component rooted at v is then popped from the stack and returned, again preserving the invariant.\n\nEach node v is assigned a unique integer v.index, which numbers the nodes consecutively in the order in which they are discovered. It also maintains a value v.lowlink that represents the smallest index of any node known to be reachable from v through v's DFS subtree, including v itself. Therefore v must be left on the stack if v.lowlink < v.index, whereas v must be removed as the root of a strongly connected component if v.lowlink == v.index. The value v.lowlink is computed during the depth-first search from v, as this finds the nodes that are reachable from v.\n\n algorithm tarjan is\n\nThe index variable is the depth-first search node number counter. S is the node stack, which starts out empty and stores the history of nodes explored but not yet committed to a strongly connected component. Note that this is not the normal depth-first search stack, as nodes are not popped as the search returns up the tree; they are only popped when an entire strongly connected component has been found.\n\nThe outermost loop searches each node that has not yet been visited, ensuring that nodes which are not reachable from the first node are still eventually traversed. The function strongconnect performs a single depth-first search of the graph, finding all successors from the node v, and reporting all strongly connected components of that subgraph.\n\nWhen each node finishes recursing, if its lowlink is still set to its index, then it is the root node of a strongly connected component, formed by all of the nodes above it on the stack. The algorithm pops the stack up to and including the current node, and presents all of these nodes as a strongly connected component.\n\nNote that \"v\".lowlink := min(\"v\".lowlink, \"w\".index) is the correct way to update \"v.lowlink\" if \"w\" is on stack. Because \"w\" is on the stack already, \"(v, w)\" is a back-edge in the DFS tree and therefore \"w\" is not in the subtree of \"v\". Because \"v.lowlink\" takes into account nodes reachable only through the nodes in the subtree of \"v\" we must stop at \"w\" and use \"w.index\" instead of \"w.lowlink\". \n\n\"Time Complexity\": The Tarjan procedure is called once for each node; the forall statement considers each edge at most once. The algorithm's running time is therefore linear in the number of edges and nodes in G, i.e. formula_1.\n\nIn order to achieve this complexity, the test for whether w is on the stack should be done in constant time.\nThis may be done, for example, by storing a flag on each node that indicates whether it is on the stack, and performing this test by examining the flag.\n\n\"Space Complexity\": The Tarjan procedure requires two words of supplementary data per vertex for the index and lowlink fields, along with one bit for onStack and another for determining when index is undefined. In addition, one word is required on each stack frame to hold v and another for the current position in the edge list. Finally, the worst-case size of the stack S must be formula_2 (i.e. when the graph is one giant component). This gives a final analysis of formula_3 where formula_4 is the machine word size. The variation of Nuutila and Soisalon-Soininen reduced this to formula_5 and, subsequently, that of Pearce requires only formula_6.\n\nWhile there is nothing special about the order of the nodes within each strongly connected component, one useful property of the algorithm is that no strongly connected component will be identified before any of its successors. Therefore, the order in which the strongly connected components are identified constitutes a reverse topological sort of the DAG formed by the strongly connected components.\n\nDonald Knuth described Tarjan's algorithm as one of Knuth's favorite implementations in his book \"The Stanford GraphBase\".\nHe also wrote: \n\n"}
{"id": "157178", "url": "https://en.wikipedia.org/wiki?curid=157178", "title": "Van der Waerden's theorem", "text": "Van der Waerden's theorem\n\nVan der Waerden's theorem is a theorem in the branch of mathematics called Ramsey theory. Van der Waerden's theorem states that for any given positive integers \"r\" and \"k\", there is some number \"N\" such that if the integers {1, 2, ..., \"N\"} are colored, each with one of \"r\" different colors, then there are at least \"k\" integers in arithmetic progression all of the same color. The least such \"N\" is the Van der Waerden number \"W\"(\"r\", \"k\"), named after the Dutch mathematician B. L. van der Waerden.\n\nFor example, when \"r\" = 2, you have two colors, say red and blue. \"W\"(2, 3) is bigger than 8, because you can color the integers from {1, ..., 8} like this:\nand no three integers of the same color form an arithmetic progression. But you can't add a ninth integer to the end without creating such a progression. If you add a red 9, then the red 3, 6, and 9 are in arithmetic progression. Alternatively, if you add a blue 9, then the blue 1, 5, and 9 are in arithmetic progression.\n\nIn fact, there is no way of coloring 1 through 9 without creating such a progression (it can be proved by considering examples). Therefore, \"W\"(2, 3) is 9.\n\nIt is an open problem to determine the values of \"W\"(\"r\", \"k\") for most values of \"r\" and \"k\". The proof of the theorem provides only an upper bound. For the case of \"r\" = 2 and \"k\" = 3, for example, the argument given below shows that it is sufficient to color the integers {1, ..., 325} with two colors to guarantee there will be a single-colored arithmetic progression of length 3. But in fact, the bound of 325 is very loose; the minimum required number of integers is only 9. Any coloring of the integers {1, ..., 9} will have three evenly spaced integers of one color.\n\nFor \"r\" = 3 and \"k\" = 3, the bound given by the theorem is 7(2·3 + 1)(2·3 + 1), or approximately 4.22·10. But actually, you don't need that many integers to guarantee a single-colored progression of length 3; you only need 27. (And it is possible to color {1, ..., 26} with three colors so that there is no single-colored arithmetic progression of length 3; for example:\n\nAnyone who can reduce the general upper bound to any 'reasonable' function can win a large cash prize. Ronald Graham has offered a prize of US$1000 for showing \"W\"(2,\"k\")<2. The best upper bound currently known is due to Timothy Gowers, who establishes\n\nby first establishing a similar result for Szemerédi's theorem, which is a stronger version of Van der Waerden's theorem. The previously best-known bound was due to Saharon Shelah and proceeded via first proving a result for the Hales–Jewett theorem, which is another strengthening of Van der Waerden's theorem.\n\nThe best lower bound currently known for formula_2 is that for all positive formula_3 we have formula_4, for all sufficiently large formula_5.\n\nThe following proof is due to Ron Graham and B.L. Rothschild. Khinchin gives a fairly simple proof of the theorem without estimating \"W\"(\"r\", \"k\").\n\nWe will prove the special case mentioned above, that \"W\"(2, 3) ≤ 325. Let \"c\"(\"n\") be a coloring of the integers {1, ..., 325}. We will find three elements of {1, ..., 325} in arithmetic progression that are the same color.\n\nDivide {1, ..., 325} into the 65 blocks {1, ..., 5}, {6, ..., 10}, ... {321, ..., 325}, thus each block is of the form {5\"b\" + 1, ..., 5\"b\" + 5} for some \"b\" in {0, ..., 64}. Since each integer is colored either red or blue, each block is colored in one of 32 different ways. By the pigeonhole principle, there are two blocks among the first 33 blocks that are colored identically. That is, there are two integers \"b\" and \"b\", both in {0...,32}, such that\n\nfor all \"k\" in {1, ..., 5}. Among the three integers 5\"b\" + 1, 5\"b\" + 2, 5\"b\" + 3, there must be at least two that are of the same color. (The pigeonhole principle again.) Call these 5\"b\" + \"a\" and 5\"b\" + \"a\", where the \"a\" are in {1,2,3} and \"a\" < \"a\". Suppose (without loss of generality) that these two integers are both red. (If they are both blue, just exchange 'red' and 'blue' in what follows.)\n\nLet \"a\" = 2\"a\" − \"a\". If 5\"b\" + \"a\" is red, then we have found our arithmetic progression: 5\"b\" + \"a\" are all red. \n\nOtherwise, 5\"b\" + \"a\" is blue. Since \"a\" ≤ 5, 5\"b\" + \"a\" is in the \"b\" block, and since the \"b\" block is colored identically, 5\"b\" + \"a\" is also blue.\n\nNow let \"b\" = 2\"b\" − \"b\". Then \"b\" ≤ 64. Consider the integer 5\"b\" + \"a\", which must be ≤ 325. What color is it?\n\nIf it is red, then 5\"b\" + \"a\", 5\"b\" + \"a\", and 5\"b\" + \"a\" form a red arithmetic progression. But if it is blue, then 5\"b\" + \"a\", 5\"b\" + \"a\", and 5\"b\" + \"a\" form a blue arithmetic progression. Either way, we are done.\n\nA similar argument can be advanced to show that \"W\"(3, 3) ≤ 7(2·3+1)(2·3+1). One begins by dividing the integers into 2·3 + 1 groups of 7(2·3 + 1) integers each; of the first 3 + 1 groups, two must be colored identically.\n\nDivide each of these two groups into 2·3+1 subgroups of 7 integers each; of the first 3 + 1 subgroups in each group, two of the subgroups must be colored identically. Within each of these identical subgroups, two of the first four integers must be the same color, say red; this implies either a red progression or an element of a different color, say blue, in the same subgroup.\n\nSince we have two identically-colored subgroups, there is a third subgroup, still in the same group that contains an element which, if either red or blue, would complete a red or blue progression, by a construction analogous to the one for \"W\"(2, 3). Suppose that this element is yellow. Since there is a group that is colored identically, it must contain copies of the red, blue, and yellow elements we have identified; we can now find a pair of red elements, a pair of blue elements, and a pair of yellow elements that 'focus' on the same integer, so that whatever color it is, it must complete a progression.\n\nThe proof for \"W\"(2, 3) depends essentially on proving that \"W\"(32, 2) ≤ 33. We divide the integers {1...,325} into 65 'blocks', each of which can be colored in 32 different ways, and then show that two blocks of the first 33 must be the same color, and there is a block colored the opposite way. Similarly, the proof for \"W\"(3, 3) depends on proving that\n\nBy a double induction on the number of colors and the length of the progression, the theorem is proved in general.\n\nA \"D-dimensional arithmetic progression\" (AP) consists of\nnumbers of the form:\nwhere is the basepoint, the 's are positive step-sizes, and the 's range from 0 to . A -dimensional AP is \"homogeneous\" for some coloring when it is all the same color.\n\nA \"-dimensional arithmetic progression with benefits\" is all numbers of the form above, but where you add on some of the \"boundary\" of the arithmetic progression, i.e. some of the indices 's can be equal to . The sides you tack on are ones where the first 's are equal to , and the remaining 's are less than .\n\nThe boundaries of a -dimensional AP with benefits are these additional arithmetic progressions of dimension formula_8, down to 0. The 0-dimensional arithmetic progression is the single point at index value formula_9. A -dimensional AP with benefits is \"homogeneous\" when each of the boundaries are individually homogeneous, but different boundaries do not have to necessarily have the same color.\n\nNext define the quantity to be the least integer so\nthat any assignment of colors to an interval of length or more necessarily contains a homogeneous -dimensional arithmetical progression with benefits.\n\nThe goal is to bound the size of . Note that is an upper bound for Van der Waerden's number. There are two inductions steps, as follows:\n\nBase case: , i.e. if you want a length 1 homogeneous -dimensional arithmetic sequence, with or without benefits, you have nothing to do. So this forms the base of the induction. The Van der Waerden theorem itself is the assertion that is finite, and it follows from the base case and the induction steps.\n\n\n"}
