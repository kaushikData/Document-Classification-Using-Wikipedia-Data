{"id": "454981", "url": "https://en.wikipedia.org/wiki?curid=454981", "title": "170 (number)", "text": "170 (number)\n\n170 (one hundred [and] seventy) is the natural number following 169 and preceding 171.\n\n170 is the smallest \"n\" for which φ(\"n\") and σ(\"n\") are both square (64 and 324 respectively). But 170 is never a solution for φ(\"x\"), making it a nontotient. Nor is it ever a solution to \"x\" - φ(\"x\"), making it a noncototient.\n\n170 is a repdigit in base 4 (2222) and base 16 (AA), as well as in bases 33, 84, and 169. It is also a sphenic number.\n\n170 is the largest integer for which its factorial can be stored in double-precision floating-point format. This is probably why it is also the largest factorial that Google's built-in calculator will calculate, returning the answer as 170! = 7.25741562 × 10.\n\nThere are 170 different cyclic Gilbreath permutations on 12 elements, and therefore there are 170 different real periodic points of order 12 on the Mandelbrot set.\n\n\n\n"}
{"id": "1076270", "url": "https://en.wikipedia.org/wiki?curid=1076270", "title": "AMPL", "text": "AMPL\n\nA Mathematical Programming Language (AMPL) is an algebraic modeling language to describe and solve high-complexity problems for large-scale mathematical computing (i.e., large-scale optimization and scheduling-type problems).\nIt was developed by Robert Fourer, David Gay, and Brian Kernighan at Bell Laboratories.\nAMPL supports dozens of solvers, both open source and commercial software, including CBC, CPLEX, FortMP, Gurobi, MINOS, IPOPT, SNOPT, KNITRO, and LGO. Problems are passed to solvers as nl files.\nAMPL is used by more than 100 corporate clients, and by government agencies and academic institutions.\n\nOne advantage of AMPL is the similarity of its syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain of optimization. Many modern solvers available on the NEOS Server (formerly hosted at the Argonne National Laboratory, currently hosted at the University of Wisconsin, Madison) accept AMPL input. According to the NEOS statistics AMPL is the most popular format for representing mathematical programming problems.\n\nAMPL features a mix of declarative and imperative programming styles. Formulating optimization models occurs via declarative language elements such as sets, scalar and multidimensional parameters, decision variables, objectives and constraints, which allow for concise description of most problems in the domain of mathematical optimization.\n\nProcedures and control flow statements are available in AMPL for\n\nTo support re-use and simplify construction of large-scale optimization problems, AMPL allows separation of model and data.\n\nAMPL supports a wide range of problem types, among them:\n\nAMPL invokes a solver in a separate process which has these advantages:\nInteraction with the solver is done through a well-defined nl interface.\n\nAMPL is available for many popular 32- and 64-bit operating systems including Linux, Mac OS X, some Unix, and Windows.\nThe translator is proprietary software maintained by AMPL Optimization LLC. However, several online services exist, providing free modeling and solving facilities using AMPL. A free student version with limited functionality and a free full-featured version for academic courses are also available.\n\nAMPL can be used from within Microsoft Excel via the SolverStudio Excel add-in.\n\nThe AMPL Solver Library (ASL), which allows reading nl files and provides the automatic differentiation, is open-source. It is used in many solvers to implement AMPL connection.\n\nThis table present significant steps in AMPL history.\nA transportation problem from George Dantzig is used to provide a sample AMPL model. This problem finds the least cost shipping schedule that meets requirements at markets and supplies at factories.\n\nHere is a partial list of solvers supported by AMPL:\n\n\n"}
{"id": "56463048", "url": "https://en.wikipedia.org/wiki?curid=56463048", "title": "Algorithms of Oppression", "text": "Algorithms of Oppression\n\nAlgorithms of Oppression: How Search Engines Reinforce Racism is a 2018 book by Safiya Noble in the fields of information science, machine learning, and human-computer interaction. \n\n\"Algorithms of Oppression\" is a text based on over six years of academic research on Google search algorithms. Noble argues that though algorithms themselves may not be racist, they can reflect the biases and values of the people who create them. These algorithms can then have negative biases against women of color and other marginalized populations, while also affecting Internet users in general by leading to \"racial and gender profiling, misrepresentation, and even economic redlining\".\n\nCritical reception for \"Algorithms of Oppression\" has been largely positive. In the \"Los Angeles Review of Books\", Emily Drabinski writes, \"What emerges from these pages is the sense that Google’s algorithms of oppression comprise just one of the hidden infrastructures that govern our daily lives, and that the others are likely just as hard-coded with white supremacy and misogyny as the one that Noble explores.\" In \"PopMatters,\" Hans Rollman describes writes that \"Algorithms of Oppression\" \"demonstrate[s] that search engines, and in particular Google, are not simply imperfect machines, but systems designed by humans in ways that replicate the power structures of the western countries where they are built, complete with all the sexism and racism that are built into those structures.\" In \"Booklist,\" reviewer Lesley Williams states, \"Noble’s study should prompt some soul-searching about our reliance on commercial search engines and about digital social equity.\"\n\nIn early February 2018, \"Algorithms of Oppression\" received press attention when the official Twitter account for the Institute of Electrical and Electronics Engineers expressed criticism of the book, citing that the thesis of the text, based on the text of the book's official blurb on commercial sites, could not be reproduced. IEEE's outreach historian, Alexander Magoun, later revealed that he had not read the book, and issued an apology.\n\n"}
{"id": "36553947", "url": "https://en.wikipedia.org/wiki?curid=36553947", "title": "Antonio Maria Bordoni", "text": "Antonio Maria Bordoni\n\nAntonio Maria Bordoni (19 July 1789 – 26 March 1860) was an Italian mathematician who did research on mathematical analysis, geometry, and mechanics. Joining the faculty of the University of Pavia in 1817, Bordoni is generally considered to be the founder of the mathematical school of Pavia. He was a member of various learned academies, notably the Accademia dei XL. Bordoni's famous students were Francesco Brioschi, Luigi Cremona, Eugenio Beltrami and Felice Casorati.\n\nAntonio Bordoni was born in Mezzana Corti (province of Pavia) on 19 July 1788, and graduated in Mathematics from Pavia on 7 June 1807.\n\nAfter just two months he was appointed teacher of mathematics at the military School of Pavia, established by Napoleon, and held such office until 1816 when the school was closed due to the political situation of the times.\n\nOn 1 November 1817 he became full professor of Elementary Pure mathematics at the University and in 1818 he held the chair of Infinitesimal Calculus, Geodesy and Hydrometry, a discipline he taught for 23 years.\n\nIn 1827 and 1828 he was dean of the University itself. In 1854, as the Faculty of Mathematics of the University of Pavia (it previously belonged to the one of the Philosophy) was established, he was elected Director of Mathematical Studies and held such office until his death, which occurred 26 March 1860, just a month after being appointed senator.\n\n\n"}
{"id": "40996300", "url": "https://en.wikipedia.org/wiki?curid=40996300", "title": "Central differencing scheme", "text": "Central differencing scheme\n\nIn applied mathematics, the central differencing scheme is a finite difference method. The finite difference method optimizes the approximation for the differential operator in the central node of the considered patch and provides numerical solutions to differential equations. The central differencing scheme is one of the schemes used to solve the integrated convection-diffusion equation and to calculate the transported property Φ at the e and w faces. The advantages of this method are that it is easy to understand and to implement, at least for simple material relations, and that its convergence rate is faster than some other finite differencing methods, such as forward and backward differencing. The right hand side of the convection-diffusion equation which basically highlights the diffusion terms can be represented using central difference approximation. Thus, in order to simplify the solution and analysis, linear interpolation can be used logically to compute the cell face values for the left hand side of this equation which is nothing but the convective terms. Therefore, cell face values of property for a uniform grid can be written as \n\nThe convection–diffusion equation is a collective representation of both diffusion and convection equations and describes or explains every physical phenomenon involving the two processes: convection and diffusion in transferring of particles, energy or other physical quantities inside a physical system. The convection-diffusion is as follows:\n\nhere Г is diffusion coefficient and Φ is the property\n\nFormal integration of steady-state convection–diffusion equation over a control volume gives\n\nThis equation represents flux balance in a control volume. The left hand side gives the net convective flux and the right hand side contains the net diffusive flux and the generation or destruction of the property within the control volume.\n\nIn the absence of source term equation one becomes\n\nContinuity equation:\n\nAssuming a control volume and integrating equation 2 over control volume gives:\n\nIntegration of equation 3 yields:\n\nIt is convenient to define two variables to represent the convective mass flux per unit area and diffusion conductance at cell faces which is as follows :\n\nAssuming formula_11 , we can write integrated convection–diffusion equation as:\n\nAnd integrated continuity equation as:\n\nIn central differencing scheme we try linear interpolation to compute cell face values for convection terms.\n\nFor a uniform grid we can write cell face values of property Φ as\n\nOn substituting this into integrated convection – diffusion equation we obtain,\n\nAnd on rearranging,\n\nformula_17\n\nConservation is ensured in central differencing scheme since overall flux balance is obtained by summing the net flux through each control volume taking into account the boundary fluxes for the control volumes around nodes 1 and 4. \nBoundary flux for control volume around node 1 and 4\n\nbecause formula_19\n\nCentral differencing scheme satisfies first condition of Boundedness.\nSince formula_13 from continuity equation, therefore; formula_17\n\nAnother essential requirement for Boundedness is that all coefficients of the discretised equations should have the same sign (usually all positive). But this is only satisfied when (peclet number) formula_22 because for a unidirectional flow(formula_23) formula_24 is always positive if formula_25\n\nIt requires that transportiveness changes according to magnitude of peclet number i.e. when pe is zero formula_26 is spread in all directions equally and as Pe increases (convection > diffusion) formula_26 at a point largely depends on upstream value and less on downstream value. But central differencing scheme does not possess Transportiveness at higher pe since Φ at a point is average of neighbouring nodes for all Pe.\n\nThe Taylor series truncation error of the central differencing scheme is second order. \nCentral differencing scheme will be accurate only if Pe < 2.\nOwing to this limitation central differencing is not a suitable discretisation practice for general purpose flow calculations.\n\n\n\n\n\n\n"}
{"id": "788704", "url": "https://en.wikipedia.org/wiki?curid=788704", "title": "Conjunction elimination", "text": "Conjunction elimination\n\nIn propositional logic, conjunction elimination (also called and elimination, ∧ elimination, or simplification) is a valid immediate inference, argument form and rule of inference which makes the inference that, if the conjunction \"A and B\" is true, then \"A\" is true, and \"B\" is true. The rule makes it possible to shorten longer proofs by deriving one of the conjuncts of a conjunction on a line by itself. \n\nAn example in English:\n\nThe rule consists of two separate sub-rules, which can be expressed in formal language as:\n\nand\n\nThe two sub-rules together mean that, whenever an instance of \"formula_3\" appears on a line of a proof, either \"formula_4\" or \"formula_5\" can be placed on a subsequent line by itself. The above example in English is an application of the first sub-rule.\n\nThe \"conjunction elimination\" sub-rules may be written in sequent notation:\n\nand\n\nwhere formula_8 is a metalogical symbol meaning that formula_4 is a syntactic consequence of formula_3 and formula_5 is also a syntactic consequence of formula_3 in logical system;\n\nand expressed as truth-functional tautologies or theorems of propositional logic:\n\nand\n\nwhere formula_4 and formula_5 are propositions expressed in some formal system.\n"}
{"id": "2542373", "url": "https://en.wikipedia.org/wiki?curid=2542373", "title": "Cross-sectional regression", "text": "Cross-sectional regression\n\nIn statistics and econometrics, a cross-sectional regression is a type of regression in which the explained and explanatory variables are associated with one period or point in time. This type of cross-sectional analysis is in contrast to a time-series regression or longitudinal regression in which the variables are considered to be associated with a sequence of points in time.\n\nFor example, in economics a regression to explain and predict money demand (how much people choose to hold in the form of the most liquid assets) could be conducted with either cross-sectional or time series data. A cross-sectional regression would have as each data point an observation on a particular individual's money holdings, income, and perhaps other variables at a single point in time, and different data points would reflect different individuals at the same point in time. In contrast, a regression using time series would have as each data point an entire economy's money holdings, income, etc. at one point in time, and different data points would be drawn on the same economy but at different points in time.\n\n\n\n"}
{"id": "3021207", "url": "https://en.wikipedia.org/wiki?curid=3021207", "title": "Current (mathematics)", "text": "Current (mathematics)\n\nIn mathematics, more particularly in functional analysis, differential topology, and geometric measure theory, a k-current in the sense of Georges de Rham is a functional on the space of compactly supported differential k-forms, on a smooth manifold \"M\". Formally currents behave like Schwartz distributions on a space of differential forms. In a geometric setting, they can represent integration over a submanifold, generalizing the Dirac delta function, or more generally even directional derivatives of delta functions (multipoles) spread out along subsets of \"M\".\n\nLet formula_1 denote the space of smooth \"m\"-forms with compact support on a smooth manifold formula_2. A current is a linear functional on formula_1 which is continuous in the sense of distributions. Thus a linear functional\n\nis an \"m\"-current if it is continuous in the following sense: If a sequence formula_5 of smooth forms, all supported in the same compact set, is such that all derivatives of all their coefficients tend uniformly to 0 when formula_6 tends to infinity, then formula_7 tends to 0.\n\nThe space formula_8 of \"m\"-dimensional currents on formula_2 is a real vector space with operations defined by \n\nMuch of the theory of distributions carries over to currents with minimal adjustments. For example, one may define the support of a current formula_11 as the complement of the biggest open set formula_12 such that\n\nThe linear subspace of formula_8 consisting of currents with support (in the sense above) that is a compact subset of formula_2 is denoted formula_17.\n\nIntegration over a compact rectifiable oriented submanifold \"M\" (with boundary) of dimension \"m\" defines an \"m\"-current, denoted by formula_2:\n\nIf the boundary ∂\"M\" of \"M\" is rectifiable, then it too defines a current by integration, and by virtue of Stokes' theorem one has:\n\nThis relates the exterior derivative \"d\" with the boundary operator ∂ on the homology of \"M\".\n\nIn view of this formula we can \"define\" a boundary operator on arbitrary currents\n\nvia duality with the exterior derivative by\n\nfor all compactly supported \"m\"-forms ω. \n\nCertain subclasses of currents which are closed under formula_23 can be used instead of all currents to create a homology theory, which can satisfy the Eilenberg–Steenrod axioms in certain cases. A classical example is the subclass of integral currents on Lipschitz neighborhood retracts.\n\nThe space of currents is naturally endowed with the weak-* topology, which will be further simply called \"weak convergence\". A sequence \"T\" of currents, converges to a current \"T\" if \n\nIt is possible to define several norms on subspaces of the space of all currents. One such norm is the \"mass norm\". If ω is an \"m\"-form, then define its comass by\n\nSo if ω is a simple \"m\"-form, then its mass norm is the usual L-norm of its coefficient. The mass of a current \"T\" is then defined as\n\nThe mass of a current represents the \"weighted area\" of the generalized surface. A current such that M(\"T\") < ∞ is representable by integration of a regular Borel measure by a version of the Riesz representation theorem. This is the starting point of homological integration.\n\nAn intermediate norm is Whitney's \"flat norm\", defined by \n\nTwo currents are close in the mass norm if they coincide away from a small part. On the other hand, they are close in the flat norm if they coincide up to a small deformation.\n\nRecall that\n\nso that the following defines a 0-current: \n\nIn particular every signed regular measure formula_30 is a 0-current: \n\nLet (\"x\", \"y\", \"z\") be the coordinates in ℝ. Then the following defines a 2-current (one of many): \n\n\n"}
{"id": "25646409", "url": "https://en.wikipedia.org/wiki?curid=25646409", "title": "Cycle rank", "text": "Cycle rank\n\nIn graph theory, the cycle rank of a directed graph is a digraph connectivity measure proposed first by Eggan and Büchi . Intuitively, this concept measures how close a\ndigraph is to a directed acyclic graph (DAG), in the sense that a DAG has\ncycle rank zero, while a complete digraph of order \"n\" with a self-loop at\neach vertex has cycle rank \"n\". The cycle rank of a directed graph is closely related to the tree-depth of an undirected graph and to the star height of a regular language. It has also found use\nin sparse matrix computations (see ) and logic\n\nThe cycle rank \"r\"(\"G\") of a digraph is inductively defined as follows:\n\nThe tree-depth of an undirected graph has a very similar definition, using undirected connectivity and connected components in place of strong connectivity and strongly connected components.\n\nCycle rank was introduced by in the context of star height of regular languages. It was rediscovered by as a generalization of undirected tree-depth, which had been developed beginning in the 1980s\nand applied to sparse matrix computations .\n\nThe cycle rank of a directed acyclic graph is 0, while a complete digraph of order \"n\" with a self-loop at\neach vertex has cycle rank \"n\". Apart from these, the cycle rank of a few other digraphs is known: the undirected path formula_2 of order \"n\", which possesses a symmetric edge relation and no self-loops, has cycle rank formula_3 . For the directed formula_4-torus formula_5, i.e., the cartesian product of two directed circuits of lengths \"m\" and \"n\", we have\nformula_6 and formula_7 for \"m ≠ n\" (, ).\n\nComputing the cycle rank is computationally hard: proves that the corresponding decision problem is NP-complete, even for sparse digraphs of maximum outdegree at most 2. On the positive side, the problem is solvable in time formula_8 on digraphs of maximum outdegree at most 2, and in time formula_9 on general digraphs. There is an approximation algorithm with approximation ratio formula_10.\n\nThe first application of cycle rank was in formal language theory, for studying the star height of regular languages. established a relation between the theories of regular expressions, finite automata, and of directed graphs. In subsequent years, this relation became known as \"Eggan's theorem\", cf. . \nIn automata theory, a nondeterministic finite automaton with ε-moves (ε-NFA) is defined as a 5-tuple, (\"Q\", Σ, \"δ\", \"q\", \"F\"), consisting of\nA word \"w\" ∈ Σ is accepted by the ε-NFA if there exists a directed path from the initial state \"q\" to some final state in \"F\" using edges from \"δ\", such that the concatenation of all labels visited along the path yields the word \"w\". The set of all words over Σ accepted by the automaton is the \"language\" accepted by the automaton \"A\".\n\nWhen speaking of digraph properties of a nondeterministic finite automaton \"A\" with state set \"Q\", we naturally address the digraph with vertex set \"Q\" induced by its transition relation. Now the theorem is stated as follows.\n\nProofs of this theorem are given by , and more recently by .\n\nAnother application of this concept lies in sparse matrix computations, namely for using nested dissection to compute the Cholesky factorization of a (symmetric) matrix in parallel. A given sparse formula_11-matrix \"M\" may be interpreted as the adjacency matrix of some symmetric digraph \"G\" on \"n\" vertices, in a way such that the non-zero entries of the matrix are in one-to-one correspondence with the edges of \"G\". If the cycle rank of the digraph \"G\" is at most \"k\", then the Cholesky factorization of \"M\" can be computed in at most \"k\" steps on a parallel computer with formula_12 processors .\n\n\n"}
{"id": "480010", "url": "https://en.wikipedia.org/wiki?curid=480010", "title": "Deduction theorem", "text": "Deduction theorem\n\nIn mathematical logic, the deduction theorem is a metatheorem of propositional and first-order logic. It is a formalization of the common proof technique in which an implication \"A\" → \"B\" is proved by assuming \"A\" and then deriving \"B\" from this assumption conjoined with known results. The deduction theorem explains why proofs of conditional sentences in mathematics are logically correct. Though it has seemed \"obvious\" to mathematicians literally for centuries that proving \"B\" from \"A\" conjoined with a set of theorems is tantamount to proving the implication \"A\" → \"B\" based on those theorems alone, it was left to Herbrand and Tarski to show (independently) this was logically correct in the general case. \n\nThe deduction theorem states that if a formula \"B\" is deducible from a set of assumptions formula_1, where \"A\" is a closed formula, then the implication \"A\" → \"B\" is deducible from formula_2. In symbols, formula_3\nimplies formula_4. In the special case where formula_2 is the empty set, the deduction theorem shows that formula_6 implies formula_7.\n\nThe deduction theorem holds for all first-order theories with the usual deductive systems for first-order logic. However, there are first-order systems in which new inference rules are added for which the deduction theorem fails. Most notably, the deduction theorem fails to hold in Birkhoff–von Neumann quantum logic, because the linear subspaces of a Hilbert space form a non-distributive lattice. \n\nThe deduction rule is an important property of Hilbert-style systems because the use of this metatheorem leads to much shorter proofs than would be possible without it. Although the deduction theorem could be taken as a primitive rule of inference in such systems, this approach is not generally followed; instead, the deduction theorem is obtained as an admissible rule using the other logical axioms and modus ponens. In other formal proof systems, the deduction theorem is sometimes taken as a primitive rule of inference. For example, in natural deduction, the deduction theorem is recast as an introduction rule for \"→\".\n\n\"Prove\" axiom 1:\n\n\"Prove\" axiom 2:\n\nUsing axiom 1 to show ((\"P\"→(\"Q\"→\"P\"))→\"R\")→\"R\":\n\nFrom the examples, you can see that we have added three virtual (or extra and temporary) rules of inference to our normal axiomatic logic. These are \"hypothesis\", \"reiteration\", and \"deduction\". The normal rules of inference (i.e. \"modus ponens\" and the various axioms) remain available.\n\n1. Hypothesis is a step where one adds an additional premise to those already available. So, if your previous step \"S\" was deduced as:\n\nthen one adds another premise \"H\" and gets:\n\nThis is symbolized by moving from the n-th level of indentation to the n+1-th level and saying \n\n2. Reiteration is a step where one re-uses a previous step. In practice, this is only necessary when one wants to take a hypothesis which is not the most recent hypothesis and use it as the final step before a deduction step.\n\n3. Deduction is a step where one removes the most recent hypothesis (still available) and prefixes it to the previous step. This is shown by unindenting one level as follows:\n\nIn axiomatic versions of propositional logic, one usually has among the axiom schemas (where \"P\", \"Q\", and \"R\" are replaced by any propositions):\n\nThese axiom schemas are chosen to enable one to derive the deduction theorem from them easily. So it might seem that we are begging the question. However, they can be justified by checking that they are tautologies using truth tables and that modus ponens preserves truth.\n\nFrom these axiom schemas one can quickly deduce the theorem schema \"P\"→\"P\" (reflexivity of implication) which is used below:\n\nSuppose that we have that Γ and \"H\" prove \"C\", and we wish to show that Γ proves \"H\"→\"C\". For each step \"S\" in the deduction which is a premise in Γ (a reiteration step) or an axiom, we can apply modus ponens to the axiom 1, \"S\"→(\"H\"→\"S\"), to get \"H\"→\"S\". If the step is \"H\" itself (a hypothesis step), we apply the theorem schema to get \"H\"→\"H\". If the step is the result of applying modus ponens to \"A\" and \"A\"→\"S\", we first make sure that these have been converted to \"H\"→\"A\" and \"H\"→(\"A\"→\"S\") and then we take the axiom 2, (\"H\"→(\"A\"→\"S\"))→((\"H\"→\"A\")→(\"H\"→\"S\")), and apply modus ponens to get (\"H\"→\"A\")→(\"H\"→\"S\") and then again to get \"H\"→\"S\". At the end of the proof we will have \"H\"→\"C\" as required, except that now it only depends on Γ, not on \"H\". So the deduction step will disappear, consolidated into the previous step which was the conclusion derived from \"H\".\n\nTo minimize the complexity of the resulting proof, some preprocessing should be done before the conversion. Any steps (other than the conclusion) which do not actually depend on \"H\" should be moved up before the hypothesis step and unindented one level. And any other unnecessary steps (which are not used to get the conclusion or can be bypassed), such as reiterations which are not the conclusion, should be eliminated.\n\nDuring the conversion, it may be useful to put all the applications of modus ponens to axiom 1 at the beginning of the deduction (right after the \"H\"→\"H\" step).\n\nWhen converting a modus ponens, if \"A\" is outside the scope of \"H\", then it will be necessary to apply axiom 1, \"A\"→(\"H\"→\"A\"), and modus ponens to get \"H\"→\"A\". Similarly, if \"A\"→\"S\" is outside the scope of \"H\", apply axiom 1, (\"A\"→\"S\")→(\"H\"→(\"A\"→\"S\")), and modus ponens to get \"H\"→(\"A\"→\"S\"). It should not be necessary to do both of these, unless the modus ponens step is the conclusion, because if both are outside the scope, then the modus ponens should have been moved up before \"H\" and thus be outside the scope also.\n\nUnder the Curry–Howard correspondence, the above conversion process for the deduction meta-theorem is analogous to the conversion process from lambda calculus terms to terms of combinatory logic, where axiom 1 corresponds to the K combinator, and axiom 2 corresponds to the S combinator. Note that the I combinator corresponds to the theorem schema \"P\"→\"P\".\n\nIf one intends to convert a complicated proof using the deduction theorem to a straight-line proof not using the deduction theorem, then it would probably be useful to prove these theorems once and for all at the beginning and then use them to help with the conversion:\nhelps convert the hypothesis steps.\nhelps convert modus ponens when the major premise is not dependent on the hypothesis, replaces axiom 2 while avoiding a use of axiom 1.\nhelps convert modus ponens when the minor premise is not dependent on the hypothesis, replaces axiom 2 while avoiding a use of axiom 1.\n\nThese two theorems jointly can be used in lieu of axiom 2, although the converted proof would be more complicated:\n\nPeirce's law is not a consequence of the deduction theorem, but it can be used with the deduction theorem to prove things which one might not otherwise be able to prove.\nIt can also be used to get the second of the two theorems which can used in lieu of axiom 2.\n\nThe deduction theorem is also valid in first-order logic in the following form:\n\n\nHere, the symbol formula_18 means \"is a syntactical consequence of.\" We indicate below how the proof of this deduction theorem differs from that of the deduction theorem in propositional calculus.\n\nIn the most common versions of the notion of formal proof, there are, in addition to the axiom schemes\nof propositional calculus (or the understanding that all tautologies of propositional calculus are to\nbe taken as axiom schemes in their own right), quantifier axioms, and in addition to modus ponens, one additional rule of inference, known as the rule of \"generalization\": \"From \"K\", infer ∀\"vK\".\"\n\nIn order to convert a proof of \"G\" from \"T\"∪{\"F\"} to one of \"F\"→\"G\" from \"T\", one deals \nwith steps of the proof of \"G\" which are axioms or result from application of modus ponens in the \nsame way as for proofs in propositional logic. Steps which result from application of the rule of\ngeneralization are dealt with via the following quantifier axiom (valid whenever the variable\n\"v\" is not free in formula \"H\"):\n\n\nSince in our case \"F\" is assumed to be closed, we can take \"H\" to be \"F\". This axiom allows\none to deduce \"F\"→∀\"vK\" from \"F\"→\"K\" and generalization, which is just what is needed whenever\nthe rule of generalization is applied to some \"K\" in the proof of \"G\".\n\nTo illustrate how one can convert a natural deduction to the axiomatic form of proof, we apply it to the tautology \"Q\"→((\"Q\"→\"R\")→\"R\"). In practice, it is usually enough to know that we could do this. We normally use the natural-deductive form in place of the much longer axiomatic proof.\n\nFirst, we write a proof using a natural-deduction like method:\n\nSecond, we convert the inner deduction to an axiomatic proof:\n\nThird, we convert the outer deduction to an axiomatic proof:\n\nThese three steps can be stated succinctly using the Curry–Howard correspondence:\n\nThe deduction theorem described above holds in some versions of paraconsistent logic. Usually the classical deduction theorem does not hold in paraconsistent logic. However, the following \"two-way deduction theorem\" does hold in one form of paraconsistent logic:\n\nthat requires the contrapositive inference to hold in addition to the requirement of the classical deduction theorem.\n\nThe resolution theorem is the converse of the deduction theorem. It follows immediately from modus ponens which is the elimination rule for implication.\n\n\n\n"}
{"id": "35057662", "url": "https://en.wikipedia.org/wiki?curid=35057662", "title": "Ditkin set", "text": "Ditkin set\n\nIn mathematics, a Ditkin set, introduced by , is a closed subset of the circle such that a function \"f\" vanishing on the set can be approximated by functions φ\"f\" with φ vanishing in a neighborhood of the set.\n"}
{"id": "27057803", "url": "https://en.wikipedia.org/wiki?curid=27057803", "title": "Doob–Dynkin lemma", "text": "Doob–Dynkin lemma\n\nIn probability theory, the Doob–Dynkin lemma, named after Joseph L. Doob and Eugene Dynkin, characterizes the situation when one random variable is a function of another by the inclusion of the formula_1-algebras generated by the random variables. The usual statement of the lemma is formulated in terms of one random variable being measurable with respect to the formula_1-algebra generated by the other.\n\nThe lemma plays an important role in the conditional expectation in probability theory, where it allows replacement of the conditioning on a random variable by conditioning on the formula_1-algebra that is generated by the random variable.\n\nLet formula_4 be a sample space. For a function formula_5, the formula_1-algebra generated by formula_7 is defined as the family of sets formula_8, where formula_9 are all Borel sets.\n\nLemma Let formula_10 be random elements and formula_11 be the formula_1 algebra generated by formula_13. Then formula_14 is formula_11-measurable if and only if formula_16 for some Borel measurable function formula_17.\n\nThe \"if\" part of the lemma is simply the statement that the composition of two measurable functions is measurable. The \"only if\" part is the nontrivial one.\n\nBy definition, formula_14 being formula_11-measurable is the same as formula_20 for any Borel set formula_9, which is the same as formula_22. So, the lemma can be rewritten in the following, equivalent form.\n\nLemma Let formula_10 be random elements and formula_11 and formula_25 the formula_1 algebras generated by formula_13 and formula_14, respectively. Then formula_16 for some Borel measurable function formula_17 if and only if formula_22.\n\n"}
{"id": "33989041", "url": "https://en.wikipedia.org/wiki?curid=33989041", "title": "End (graph theory)", "text": "End (graph theory)\n\nIn the mathematics of infinite graphs, an end of a graph represents, intuitively, a direction in which the graph extends to infinity. Ends may be formalized mathematically as equivalence classes of infinite paths, as havens describing strategies for pursuit-evasion games on the graph, or (in the case of locally finite graphs) as topological ends of topological spaces associated with the graph.\n\nEnds of graphs may be used (via Cayley graphs) to define ends of finitely generated groups. Finitely generated infinite groups have one, two, or infinitely many ends, and the Stallings theorem about ends of groups provides a decomposition for groups with more than one end.\n\nEnds of graphs were defined by in terms of equivalence classes of infinite paths. A in an infinite graph is a semi-infinite simple path; that is, it is an infinite sequence of vertices \"v\", \"v\", \"v\", ... in which each vertex appears at most once in the sequence and each two consecutive vertices in the sequence are the two endpoints of an edge in the graph. According to Halin's definition, two rays \"r\" and \"r\" are equivalent if there is another ray \"r\" (not necessarily different from either of the first two rays) that contains infinitely many of the vertices in each of \"r\" and \"r\". This is an equivalence relation: each ray is equivalent to itself, the definition is symmetric with regard to the ordering of the two rays, and it can be shown to be transitive. Therefore, it partitions the set of all rays into equivalence classes, and Halin defined an end as one of these equivalence classes.\n\nAn alternative definition of the same equivalence relation has also been used: two rays \"r\" and \"r\" are equivalent if there is no finite set \"X\" of vertices that separates infinitely many vertices of \"r\" from infinitely many vertices of \"r\". This is equivalent to Halin's definition: if the ray \"r\" from Halin's definition exists, then any separator must contain infinitely many points of \"r\" and therefore cannot be finite, and conversely if \"r\" does not exist then a path that alternates as many times as possible between \"r\" and \"r\" must form the desired finite separator.\n\nEnds also have a more concrete characterization in terms of havens, functions that describe evasion strategies for pursuit-evasion games on a graph \"G\". In the game in question, a robber is trying to evade a set of policemen by moving from vertex to vertex along the edges of \"G\". The police have helicopters and therefore do not need to follow the edges; however the robber can see the police coming and can choose where to move next before the helicopters land. A haven is a function β that maps each set \"X\" of police locations to one of the connected components of the subgraph formed by deleting \"X\"; a robber can evade the police by moving in each round of the game to a vertex within this component. Havens must satisfy a consistency property (corresponding to the requirement that the robber cannot move through vertices on which police have already landed): if \"X\" is a subset of \"Y\", and both \"X\" and \"Y\" are valid sets of locations for the given set of police, then β(\"X\") must be a superset of β(\"Y\"). A haven has order \"k\" if the collection of police locations for which it provides an escape strategy includes all subsets of fewer than \"k\" vertices in the graph; in particular, it has order ℵ if it maps every finite subset \"X\" of vertices to a component of \"G\" \\ \"X\". Every ray in \"G\" corresponds to a haven of order ℵ, namely, the function β that maps every finite set \"X\" to the unique component of \"G\" \\ \"X\" that contains infinitely many vertices of the ray. Conversely, every haven of order ℵ can be defined in this way by a ray. Two rays are equivalent if and only if they define the same haven, so the ends of a graph are in one-to-one correspondence with its havens of order ℵ.\n\nIf the infinite graph \"G\" is itself a ray, then it has infinitely many ray subgraphs, one starting from each vertex of \"G\". However, all of these rays are equivalent to each other, so \"G\" only has one end.\n\nIf \"G\" is a forest (that is, a graph with no finite cycles), then the intersection of any two rays is either a path or a ray; two rays are equivalent if their intersection is a ray. If a base vertex is chosen in each connected component of \"G\", then each end of \"G\" contains a unique ray starting from one of the base vertices, so the ends may be placed in one-to-one correspondence with these canonical rays. Every countable graph \"G\" has a spanning forest with the same set of ends as \"G\". However, there exist uncountably infinite graphs with only one end in which every spanning tree has infinitely many ends.\n\nIf \"G\" is an infinite grid graph, then it has many rays, and arbitrarily large sets of vertex-disjoint rays. However, it has only one end. This may be seen most easily using the characterization of ends in terms of havens: the removal of any finite set of vertices leaves exactly one infinite connected component, so there is only one haven (the one that maps each finite set to the unique infinite connected component).\n\nIn point-set topology, there is a concept of an end that is similar to, but not quite the same as, the concept of an end in graph theory, dating back much earlier to . If a topological space can be covered by a nested sequence of compact sets formula_1, then an end of the space is a sequence of components formula_2 of the complements of the compact sets. This definition does not depend on the choice of the compact sets: the ends defined by one such choice may be placed in one-to-one correspondence with the ends defined by any other choice.\n\nAn infinite graph \"G\" may be made into a topological space in two different but related ways:\nIn either case, every finite subgraph of \"G\" corresponds to a compact subspace of the topological space, and every compact subspace corresponds to a finite subgraph together with, in the Hausdorff case, finitely many compact proper subsets of edges. Thus, a graph may be covered by a nested sequence of compact sets if and only if it is locally finite, having a finite number of edges at every vertex.\n\nIf a graph \"G\" is connected and locally finite, then it has a compact cover in which the set κ is the set of vertices at distance at most \"i\" from some arbitrarily chosen starting vertex. In this case any haven β defines an end of the topological space in which formula_3. And conversely, if formula_2 is an end of the topological space defined from \"G\", it defines a haven in which β(\"X\") is the component containing \"U\", where \"i\" is any number large enough that κ contains \"X\". Thus, for connected and locally finite graphs, the topological ends are in one-to-one correspondence with the graph-theoretic ends.\n\nFor graphs that may not be locally finite, it is still possible to define a topological space from the graph and its ends. This space can be represented as a metric space if and only if the graph has a normal spanning tree, a rooted spanning tree such that each graph edge connects an ancestor-descendant pair. If a normal spanning tree exists, it has the same set of ends as the given graph: each end of the graph must contain exactly one infinite path in the tree.\n\nAn end \"E\" of a graph \"G\" is defined to be a free end if there is a finite set \"X\" of vertices with the property that \"X\" separates \"E\" from all other ends of the graph. (That is, in terms of havens, β(\"X\") is disjoint from β(\"X\") for every other end \"D\".) In a graph with finitely many ends, every end must be free. proves that, if \"G\" has infinitely many ends, then either there exists an end that is not free, or there exists an infinite family of rays that share a common starting vertex and are otherwise disjoint from each other.\n\nA thick end of a graph \"G\" is an end that contains infinitely many pairwise-disjoint rays. Halin's grid theorem characterizes the graphs that contain thick ends: they are exactly the graphs that have a subdivision of the hexagonal tiling as a subgraph.\n\n defines a connected locally finite graph to be \"almost symmetric\" if there exist a vertex \"v\" and a number \"D\" such that, for every other vertex \"w\", there is an automorphism of the graph for which the image of \"v\" is within distance \"D\" of \"w\"; equivalently, a connected locally finite graph is almost symmetric if its automorphism group has finitely many orbits. As he shows, for every connected locally finite almost-symmetric graph, the number of ends is either at most two or uncountable; if it is uncountable, the ends have the topology of a Cantor set. Additionally, Mohar shows that the number of ends controls the Cheeger constant\nwhere \"V\" ranges over all finite nonempty sets of vertices of the graph and\nwhere formula_6 denotes the set of edges with one endpoint in \"V\". For almost-symmetric graphs with uncountably many ends, \"h\" > 0; however, for almost-symmetric graphs with only two ends, \"h\" = 0.\n\nEvery group and a set of generators for the group determine a Cayley graph, a graph whose vertices are the group elements and the edges are pairs of elements (\"x\",\"gx\") where \"g\" is one of the generators. In the case of a finitely generated group, the ends of the group are defined to be the ends of the Cayley graph for the finite set of generators; this definition is invariant under the choice of generators, in the sense that if two different finite set of generators are chosen, the ends of the two Cayley graphs are in one-to-one correspondence with each other.\n\nFor instance, every free group has a Cayley graph (for its free generators) that is a tree. The free group on one generator has a doubly infinite path as its Cayley graph, with two ends. Every other free group has infinitely many ends.\n\nEvery finitely generated infinite group has either 1, 2, or infinitely many ends, and the Stallings theorem about ends of groups provides a decomposition of groups with more than one end. In particular:\n\n"}
{"id": "14771927", "url": "https://en.wikipedia.org/wiki?curid=14771927", "title": "GeneMark", "text": "GeneMark\n\nGeneMark is a generic name for a family of ab initio gene prediction programs developed at the Georgia Institute of Technology in Atlanta. Developed in 1993, original GeneMark was used in 1995 as a primary gene prediction tool for annotation of the first completely sequenced bacterial genome of \"Haemophilus influenzae\", and in 1996 for the first archaeal genome of \"Methanococcus jannaschii\". The algorithm introduced inhomogeneous three-periodic Markov chain models of protein-coding DNA sequence that became standard in gene prediction as well as Bayesian approach to gene prediction in two DNA strands simultaneously. Species specific parameters of the models were estimated from training sets of sequences of known type (protein-coding and non-coding). The major step of the algorithm computes for a given DNA fragment posterior probabilities of either being \"protein-coding\" (carrying genetic code) in each of six possible reading frames (including three frames in complementary DNA strand) or being \"non-coding\". Original GeneMark (developed before the HMM era in Bioinformatics) is an HMM-like algorithm; it can be viewed as approximation to known in the HMM theory posterior decoding algorithm for appropriately defined HMM.\n\nThe GeneMark.hmm algorithm (1998) was designed to improve gene prediction accuracy in finding short genes and gene starts. The idea was to integrate the Markov chain models used in GeneMark into a hidden Markov model framework, with transition between coding and non-coding regions formally interpreted as transitions between hidden states. Additionally, the ribosome binding site model was used to improve accuracy of gene start prediction. Next step was done with development of the self-training gene prediction tool GeneMarkS (2001). GeneMarkS has been in active use by genomics community for gene identification in new prokaryotic genomic sequences.\nGeneMarkS+, extension of GeneMarkS integrating information on homologous proteins into gene prediction is used in the NCBI pipeline for prokaryotic genomes annotation; the pipeline can annotate up to 2000 genomes daily ().\n\nAccurate identification of species specific parameters of the GeneMark and GeneMark.hmm algorithms was the key condition for making accurate gene predictions. However, the question was raised, motivated by studies of viral genomes, how to define parameters for gene prediction in a rather short sequence that has no large genomic context. In 1999 this question was addressed by development of a \"heuristic method\" computations of the parameters as functions of the sequence G+C content. Since 2004 models built by the heuristic approach have been used in finding genes in metagenomic sequences. Subsequently, analysis of several hundred prokaryotic genomes led to developing more advanced heuristic method (implemented in MetaGeneMark) in 2010. \n\nIn eukaryotic genomes modeling of exon borders with introns and intergenic regions presents a major challenge addressed by use of HMMs. The HMM architecture of eukaryotic GeneMark.hmm includes hidden states for initial, internal, and terminal exons, introns, intergenic regions and single exon genes located in both DNA strands. Initial eukaryotic GeneMark.hmm needed training sets for estimation of the algorithm parameters. In 2005 first version of self-training algorithm GeneMark-ES was developed. In 2008 the GeneMark-ES algorithm was extended to fungal genomes by developing a special intron model and more complex strategy of self-training. Then, in 2014, GeneMark-ET the algorithm that augmented self-training by information from mapped to genome unassembled RNA-Seq reads was added to the family. Gene prediction in eukaryotic transcripts can be done by the new algorithm GeneMarkS-T (2015) \n\n\n\n\n\n"}
{"id": "7392980", "url": "https://en.wikipedia.org/wiki?curid=7392980", "title": "Graduate Texts in Mathematics", "text": "Graduate Texts in Mathematics\n\nGraduate Texts in Mathematics (GTM) (ISSN 0072-5285) is a series of graduate-level textbooks in mathematics published by Springer-Verlag. The books in this series, like the other Springer-Verlag mathematics series, are yellow books of a standard size (with variable numbers of pages). The GTM series is easily identified by a white band at the top of the book.\n\nThe books in this series tend to be written at a more advanced level than the similar Undergraduate Texts in Mathematics series, although there is a fair amount of overlap between the two series in terms of material covered and difficulty level.\n\n\n\n"}
{"id": "216328", "url": "https://en.wikipedia.org/wiki?curid=216328", "title": "Harold Scott MacDonald Coxeter", "text": "Harold Scott MacDonald Coxeter\n\nHarold Scott MacDonald \"Donald\" Coxeter, FRS, FRSC, (February 9, 1907 – March 31, 2003) was a British-born Canadian geometer. Coxeter is regarded as one of the greatest geometers of the 20th century. He was born in London, received his BA (1929) and PhD (1931) from Cambridge, but lived in Canada from age 29. He was always called Donald, from his third name MacDonald. He was most noted for his work on regular polytopes and higher-dimensional geometries. He was a champion of the classical approach to geometry, in a period when the tendency was to approach geometry more and more via algebra.\n\nIn his youth, Coxeter composed music and was an accomplished pianist at the age of 10. He felt that mathematics and music were intimately related, outlining his ideas in a 1962 article on \"Mathematics and Music\" in the \"Canadian Music Journal\".\n\nCoxeter went up to Trinity College, Cambridge in 1926 to read mathematics. There he earned his BA (as Senior Wrangler) in 1928, and his doctorate in 1931. In 1932 he went to Princeton University for a year as a Rockefeller Fellow, where he worked with Hermann Weyl, Oswald Veblen, and Solomon Lefschetz. Returning to Trinity for a year, he attended Ludwig Wittgenstein's seminars on the philosophy of mathematics. In 1934 he spent a further year at Princeton as a Procter Fellow.\n\nIn 1936 Coxeter moved to the University of Toronto. In 1938 he and P. Du Val, H.T. Flather, and John Flinders Petrie published The Fifty-Nine Icosahedra with University of Toronto Press. In 1940 Coxeter edited the eleventh edition of \"Mathematical Recreations and Essays\", originally published by W. W. Rouse Ball in 1892. He was elevated to professor in 1948. Coxeter was elected a Fellow of the Royal Society of Canada in 1948 and a Fellow of the Royal Society in 1950. He met Maurits Escher in 1954 and the two became lifelong friends; his work on geometric figures helped inspire some of Escher's works, particularly the \"Circle Limit\" series based on hyperbolic tessellations. He also inspired some of the innovations of Buckminster Fuller. Coxeter, M. S. Longuet-Higgins and J. C. P. Miller were the first to publish the full list of uniform polyhedra (1954).\n\nHe worked for 60 years at the University of Toronto and published twelve books.\n\nSince 1978, the Canadian Mathematical Society have awarded the Coxeter–James Prize in his honor.\n\nHe was made a Fellow of the Royal Society in 1950 and in 1997 he was awarded their Sylvester Medal. In 1990, he became a Foreign Member of the American Academy of Arts and Sciences and in 1997 was made a Companion of the Order of Canada.\n\nIn 1973 he received the Jeffery–Williams Prize.\n\n\n\n"}
{"id": "12714757", "url": "https://en.wikipedia.org/wiki?curid=12714757", "title": "Helen G. Grundman", "text": "Helen G. Grundman\n\nHelen Giessler Grundman is an American mathematician. She is the Director of Education and Diversity at the American Mathematical Society and professor emeritus of mathematics at Bryn Mawr College. Grundman is noted for her research in number theory and efforts to increase diversity in mathematics.\n\nHelen Grundman earned her PhD in 1989 from the University of California, Berkeley, under the supervision of P. Emery Thomas.\n\nAfter receiving her PhD, Grundman spent two years as a C. L. E. Moore instructor at the Massachusetts Institute of Technology. She became a professor at Bryn Mawr College in 1991. In 2016, Grundman was named as the inaugural Director of Education and Diversity for the American Mathematical Society.\n\nIn 1994, Grundman proved that sequences of more than 2\"n\" consecutive Harshad numbers in base \"n\" do not exist.\n\nIn 2017, Grundman was selected as a fellow of the Association for Women in Mathematics in the inaugural class.\n"}
{"id": "585810", "url": "https://en.wikipedia.org/wiki?curid=585810", "title": "Heptagonal pyramidal number", "text": "Heptagonal pyramidal number\n\nIn mathematics, a heptagonal pyramidal number is a figurate number representing the number of dots in a three-dimensional pattern in the shape of a heptagonal pyramid.\n\nThe first few heptagonal pyramidal numbers are:\n\nThe \"n\"th heptagonal number can be calculated by adding up the first \"n\" heptagonal numbers, or more directly by using the formula\n"}
{"id": "23550923", "url": "https://en.wikipedia.org/wiki?curid=23550923", "title": "High-frequency trading", "text": "High-frequency trading\n\nIn financial markets, high-frequency trading (HFT) is a type of algorithmic trading characterized by high speeds, high turnover rates, and high order-to-trade ratios that leverages high-frequency financial data and electronic trading tools. While there is no single definition of HFT, among its key attributes are highly sophisticated algorithms, co-location, and very short-term investment horizons. HFT can be viewed as a primary form of algorithmic trading in finance. Specifically, it is the use of sophisticated technological tools and computer algorithms to rapidly trade securities. HFT uses proprietary trading strategies carried out by computers to move in and out of positions in seconds or fractions of a second. \n\nIn 2017, Aldridge and Krawciw estimated that in 2016 HFT on average initiated 10–40% of trading volume in equities, and 10–15% of volume in foreign exchange and commodities. Intraday, however, proportion of HFT may vary from 0% to 100% of short-term trading volume. Previous estimates reporting that HFT accounted for 60–73% of all US equity trading volume, with that number falling to approximately 50% in 2012 were highly inaccurate speculative guesses.\nHigh-frequency traders move in and out of short-term positions at high volumes and high speeds aiming to capture sometimes a fraction of a cent in profit on every trade. HFT firms do not consume significant amounts of capital, accumulate positions or hold their portfolios overnight. As a result, HFT has a potential Sharpe ratio (a measure of reward to risk) tens of times higher than traditional buy-and-hold strategies. High-frequency traders typically compete against other HFTs, rather than long-term investors. HFT firms make up the low margins with incredibly high volumes of trades, frequently numbering in the millions.\n\nA substantial body of research argues that HFT and electronic trading pose new types of challenges to the financial system. Algorithmic and high-frequency traders were both found to have contributed to volatility in the Flash Crash of May 6, 2010, when high-frequency liquidity providers rapidly withdrew from the market. Several European countries have proposed curtailing or banning HFT due to concerns about volatility.\n\nHigh-frequency trading has taken place at least since the 1930s, mostly in the form of specialists and pit traders buying and selling positions at the physical location of the exchange, with high-speed telegraph service to other exchanges.\n\nThe rapid-fire computer-based HFT developed gradually since 1983 after NASDAQ introduced a purely electronic form of trading. At the turn of the 21st century, HFT trades had an execution time of several seconds, whereas by 2010 this had decreased to milli- and even microseconds. Until recently, high-frequency trading was a little-known topic outside the financial sector, with an article published by the \"New York Times\" in July 2009 being one of the first to bring the subject to the public's attention.\n\nOn September 2, 2013, Italy became the world's first country to introduce a tax specifically targeted at HFT, charging a levy of 0.02% on equity transactions lasting less than 0.5 seconds.\n\nIn the early 2000s, high-frequency trading still accounted for fewer than 10% of equity orders, but this proportion was soon to begin rapid growth. According to data from the NYSE, trading volume grew by about 164% between 2005 and 2009 for which high-frequency trading might be accounted. As of the first quarter in 2009, total assets under management for hedge funds with high-frequency trading strategies were $141 billion, down about 21% from their peak before the worst of the crises, although most of the largest HFT's are actually LLC's owned by a small number of investors. The high-frequency strategy was first made popular by Renaissance Technologies who use both HFT and quantitative aspects in their trading. Many high-frequency firms are market makers and provide liquidity to the market which lowers volatility and helps narrow bid-offer spreads, making trading and investing cheaper for other market participants.\n\nIn the United States in 2009, high-frequency trading firms represented 2% of the approximately 20,000 firms operating today, but accounted for 73% of all equity orders volume. The major U.S. high-frequency trading firms include Virtu Financial, KCG, Tower Research Capital, IMC, Tradebot and Citadel LLC. The Bank of England estimates similar percentages for the 2010 US market share, also suggesting that in Europe HFT accounts for about 40% of equity orders volume and for Asia about 5–10%, with potential for rapid growth. By value, HFT was estimated in 2010 by consultancy \"Tabb Group\" to make up 56% of equity trades in the US and 38% in Europe.\n\nAs HFT strategies become more widely used, it can be more difficult to deploy them profitably. According to an estimate from Frederi Viens of Purdue University, profits from HFT in the U.S. has been declining from an estimated peak of $5bn in 2009, to about $1.25bn in 2012.\n\nThough the percentage of volume attributed to HFT has fallen in the equity markets, it has remained prevalent in the futures markets. According to a study in 2010 by Aite Group, about a quarter of major global futures volume came from professional high-frequency traders. In 2012, according to a study by the TABB Group, HFT accounted for more than 60 percent of all futures market volume in 2012 on U.S. exchanges.\n\nHigh-frequency trading is quantitative trading that is characterized by short portfolio holding periods All portfolio-allocation decisions are made by computerized quantitative models. The success of high-frequency trading strategies is largely driven by their ability to simultaneously process large volumes of information, something ordinary human traders cannot do. Specific algorithms are closely guarded by their owners. Many practical algorithms are in fact quite simple arbitrages which could previously have been performed at lower frequency—competition tends to occur through who can execute them the fastest rather than who can create new breakthrough algorithms.\n\nThe common types of high-frequency trading include several types of market-making, event arbitrage, statistical arbitrage, and latency arbitrage. Most high-frequency trading strategies are not fraudulent, but instead exploit minute deviations from market equilibrium.\n\nAccording to SEC:\n\nA \"market maker\" is a firm that stands ready to buy and sell a particular stock on a regular and continuous basis at a publicly quoted price. You'll most often hear about market makers in the context of the Nasdaq or other \"over the counter\" (OTC) markets. Market makers that stand ready to buy and sell stocks listed on an exchange, such as the New York Stock Exchange, are called \"third market makers.\" Many OTC stocks have more than one market-maker.\n\nMarket-makers generally must be ready to buy and sell at least 100 shares of a stock they make a market in. As a result, a large order from an investor may have to be filled by a number of market-makers at potentially different prices.\n\nThere can be a significant overlap between a 'market maker' and 'HFT firm'. HFT firms characterize their business as \"Market making – a set of high-frequency trading strategies that involve placing a limit order to sell (or offer) or a buy limit order (or bid) in order to earn the bid-ask spread. By doing so, market makers provide counterpart to incoming market orders. Although the role of market maker was traditionally fulfilled by specialist firms, this class of strategy is now implemented by a large range of investors, thanks to wide adoption of direct market access. As pointed out by empirical studies this renewed competition among liquidity providers causes reduced effective market spreads, and therefore reduced indirect costs for final investors.\" A crucial distinction is that true market makers don't exit the market at their discretion and are committed not to, where HFT firms are under no similar commitment.\n\nSome high-frequency trading firms use market making as their primary strategy. Automated Trading Desk (ATD), which was bought by Citigroup in July 2007, has been an active market maker, accounting for about 6% of total volume on both the NASDAQ and the New York Stock Exchange. In May 2016, Citadel LLC bought assets of ATD from Citigroup. Building up market making strategies typically involves precise modeling of the target market microstructure together with stochastic control techniques.\n\nThese strategies appear intimately related to the entry of new electronic venues. Academic study of Chi-X's entry into the European equity market reveals that its launch coincided with a large HFT that made markets using both the incumbent market, NYSE-Euronext, and the new market, Chi-X. The study shows that the new market provided ideal conditions for HFT market-making, low fees (i.e., rebates for quotes that led to execution) and a fast system, yet the HFT was equally active in the incumbent market to offload nonzero positions. New market entry and HFT arrival are further shown to coincide with a significant improvement in liquidity supply.\n\nThe Michael Lewis book \"\" discusses high-frequency trading, including the tactics of spoofing, layering and quote stuffing, which are all now illegal. The book details the rise of high-frequency trading in the US market.\n\nMuch information happens to be unwittingly embedded in market data, such as quotes and volumes. By observing a flow of quotes, computers are capable of extracting information that has not yet crossed the news screens. Since all quote and volume information is public, such strategies are fully compliant with all the applicable laws.\n\nFilter trading is one of the more primitive high-frequency trading strategies that involves monitoring large amounts of stocks for significant or unusual price changes or volume activity. This includes trading on announcements, news, or other event criteria. Software would then generate a buy or sell order depending on the nature of the event being looked for.\n\nTick trading often aims to recognize the beginnings of large orders being placed in the market. For example, a large order from a pension fund to buy will take place over several hours or even days, and will cause a rise in price due to increased demand. An arbitrageur can try to spot this happening then buy up the security, then profit from selling back to the pension fund. This strategy has become more difficult since the introduction of dedicated trade execution companies in the 2000s which provide optimal trading for pension and other funds, specifically designed to remove the arbitrage opportunity.\n\nCertain recurring events generate predictable short-term responses in a selected set of securities. High-frequency traders take advantage of such predictability to generate short-term profits.\n\nAnother set of high-frequency trading strategies are strategies that exploit predictable temporary deviations from stable statistical relationships among securities. Statistical arbitrage at high frequencies is actively used in all liquid securities, including equities, bonds, futures, foreign exchange, etc. Such strategies may also involve classical arbitrage strategies, such as covered interest rate parity in the foreign exchange market, which gives a relationship between the prices of a domestic bond, a bond denominated in a foreign currency, the spot price of the currency, and the price of a forward contract on the currency. High-frequency trading allows similar arbitrages using models of greater complexity involving many more than four securities.\n\nThe TABB Group estimates that annual aggregate profits of high-frequency arbitrage strategies exceeded US$21 billion in 2009, although the Purdue study estimates the profits for all high frequency trading were US$5 billion in 2009.\n\nIndex arbitrage exploits index tracker funds which are bound to buy and sell large volumes of securities in proportion to their changing weights in indices. If a HFT firm is able to access and process information which predicts these changes before the tracker funds do so, they can buy up securities in advance of the trackers and sell them on to them at a profit.\n\nCompany news in electronic text format is available from many sources including commercial providers like Bloomberg, public news websites, and Twitter feeds. Automated systems can identify company names, keywords and sometimes semantics to trade news before human traders can process it.\n\nA separate, \"naïve\" class of high-frequency trading strategies relies exclusively on ultra-low latency direct market access technology. In these strategies, computer scientists rely on speed to gain minuscule advantages in arbitraging price discrepancies in some particular security trading simultaneously on disparate markets.\n\nAnother aspect of low latency strategy has been the switch from fiber optic to microwave technology for long distance networking. Especially since 2011, there has been a trend to use microwaves to transmit data across key connections such as the one between New York City and Chicago. This is because microwaves travelling in air suffer a less than 1% speed reduction compared to light travelling in a vacuum, whereas with conventional fiber optics light travels over 30% slower.\n\nHigh-frequency trading strategies may use properties derived from market data feeds to identify orders that are posted at sub-optimal prices. Such orders may offer a profit to their counterparties that high-frequency traders can try to obtain. Examples of these features include the age of an order or the sizes of displayed orders. Tracking important order properties may also allow trading strategies to have a more accurate prediction of the future price of a security.\n\nThe effects of algorithmic and high-frequency trading are the subject of ongoing research. High frequency trading causes regulatory concerns as a contributor to market fragility.\nRegulators claim these practices contributed to volatility in the May 6, 2010 Flash Crash and find that risk controls are much less stringent for faster trades.\n\nMembers of the financial industry generally claim high-frequency trading substantially improves market liquidity, narrows bid-offer spread, lowers volatility and makes trading and investing cheaper for other market participants.\n\nAn academic study found that, for large-cap stocks and in quiescent markets during periods of \"generally rising stock prices\", high-frequency trading lowers the cost of trading and increases the informativeness of quotes; however, it found \"no significant effects for smaller-cap stocks\", and \"it remains an open question whether algorithmic trading and algorithmic liquidity supply are equally beneficial in more turbulent or declining markets. ...algorithmic liquidity suppliers may simply turn off their machines when markets spike downward.\"\n\nIn September 2011, market data vendor Nanex LLC published a report stating the contrary. They looked at the amount of quote traffic compared to the value of trade transactions over 4 and half years and saw a 10-fold decrease in efficiency. Nanex's owner is an outspoken detractor of high-frequency trading. Many discussions about HFT focus solely on the frequency aspect of the algorithms and not on their decision-making logic (which is typically kept secret by the companies that develop them). This makes it difficult for observers to pre-identify market scenarios where HFT will dampen or amplify price fluctuations. The growing quote traffic compared to trade value could indicate that more firms are trying to profit from cross-market arbitrage techniques that do not add significant value through increased liquidity when measured globally.\n\nMore fully automated markets such as NASDAQ, Direct Edge, and BATS, in the US, gained market share from less automated markets such as the NYSE. Economies of scale in electronic trading contributed to lowering commissions and trade processing fees, and contributed to international mergers and consolidation of financial exchanges.\n\nThe speeds of computer connections, measured in milliseconds or microseconds, have become important. Competition is developing among exchanges for the fastest processing times for completing trades. For example, in 2009 the London Stock Exchange bought a technology firm called MillenniumIT and announced plans to implement its Millennium Exchange platform which they claim has an average latency of 126 microseconds. This allows sub-millisecond resolution timestamping of the order book. Off-the-shelf software currently allows for nanoseconds resolution of timestamps using a GPS clock with 100 nanoseconds precision. \n\nSpending on computers and software in the financial industry increased to $26.4 billion in 2005.\n\nThe brief but dramatic stock market crash of May 6, 2010 was initially thought to have been caused by high-frequency trading. The Dow Jones Industrial Average plunged to its largest intraday point loss, but not percentage loss, in history, only to recover much of those losses within minutes.\n\nIn the aftermath of the crash, several organizations argued that high-frequency trading was not to blame, and may even have been a major factor in minimizing and partially reversing the Flash Crash. CME Group, a large futures exchange, stated that, insofar as stock index futures traded on CME Group were concerned, its investigation had found no support for the notion that high-frequency trading was related to the crash, and actually stated it had a market stabilizing effect.\n\nHowever, after almost five months of investigations, the U.S. Securities and Exchange Commission (\"SEC\") and the Commodity Futures Trading Commission (\"CFTC\") issued a joint report identifying the cause that set off the sequence of events leading to the Flash Crash and concluding that the actions of high-frequency trading firms contributed to volatility during the crash.\n\nThe report found that the cause was a single sale of $4.1 billion in futures contracts by a mutual fund, identified as Waddell & Reed Financial, in an aggressive attempt to hedge its investment position. The joint report also found that \"high-frequency traders quickly magnified the impact of the mutual fund's selling.\" The joint report \"portrayed a market so fragmented and fragile that a single large trade could send stocks into a sudden spiral,\" that a large mutual fund firm \"chose to sell a big number of futures contracts using a computer program that essentially ended up wiping out available buyers in the market,\" that as a result high-frequency firms \"were also aggressively selling the E-mini contracts,\" contributing to rapid price declines. The joint report also noted \"HFTs began to quickly buy and then resell contracts to each other — generating a 'hot-potato' volume effect as the same positions were passed rapidly back and forth.\" The combined sales by Waddell and high-frequency firms quickly drove \"the E-mini price down 3% in just four minutes.\" As prices in the futures market fell, there was a spillover into the equities markets where \"the liquidity in the market evaporated because the automated systems used by most firms to keep pace with the market paused\" and scaled back their trading or withdrew from the markets altogether. The joint report then noted that \"Automatic computerized traders on the stock market shut down as they detected the sharp rise in buying and selling.\" As computerized high-frequency traders exited the stock market, the resulting lack of liquidity \"...caused shares of some prominent companies like Procter & Gamble and Accenture to trade down as low as a penny or as high as $100,000.\" While some firms exited the market, high-frequency firms that remained in the market exacerbated price declines because they \"'escalated their aggressive selling' during the downdraft.\" In the years following the flash crash, academic researchers and experts from the CFTC pointed to high-frequency trading as just one component of the complex current U.S. market structure that led to the events of May 6, 2010.\n\nIn 2015 the Paris-based regulator of the 28-nation European Union, the European Securities and Markets Authority, proposed time standards to span the EU, that would more accurately synchronize trading clocks \"to within a nanosecond, or one-billionth of a second\" to refine regulation of gateway-to-gateway latency time— \"the speed at which trading venues acknowledge an order after receiving a trade request.\" Using these more detailed time-stamps, regulators would be better able to distinguish the order in which trade requests are received and executed, to identify market abuse and prevent potential manipulation of European securities markets by traders using advanced, powerful, fast computers and networks. The fastest technologies give traders an advantage over other \"slower\" investors as they can change prices of the securities they trade.\n\nHigh-frequency trading comprises many different types of algorithms. Various studies reported that certain types of market-making high-frequency trading reduces volatility and does not pose a systemic risk, and lowers transaction costs for retail investors, without impacting long term investors. Other studies, summarized in Aldridge, Krawciw, 2017 find that high-frequency trading strategies known as \"aggressive\" erode liquidity and cause volatility.\n\nHigh-frequency trading has been the subject of intense public focus and debate since the May 6, 2010 Flash Crash. At least one Nobel Prize–winning economist, Michael Spence, believes that HFT should be banned. A working paper found \"the presence of high frequency trading has significantly mitigated the frequency and severity of end-of-day price dislocation\".\n\nIn their joint report on the 2010 Flash Crash, the SEC and the CFTC stated that \"market makers and other liquidity providers widened their quote spreads, others reduced offered liquidity, and a significant number withdrew completely from the markets\" during the flash crash.\n\nPoliticians, regulators, scholars, journalists and market participants have all raised concerns on both sides of the Atlantic. This has led to discussion of whether high-frequency market makers should be subject to various kinds of regulations.\n\nIn a September 22, 2010 speech, SEC chairperson Mary Schapiro signaled that US authorities were considering the introduction of regulations targeted at HFT. She said, \"high frequency trading firms have a tremendous capacity to affect the stability and integrity of the equity markets. Currently, however, high frequency trading firms are subject to very little in the way of obligations either to protect that stability by promoting reasonable price continuity in tough times, or to refrain from exacerbating price volatility.\" She proposed regulation that would require high-frequency traders to stay active in volatile markets. A later SEC chair Mary Jo White pushed back against claims that high-frequency traders have an inherent benefit in the markets. SEC associate director Gregg Berman suggested that the current debate over HFT lacks perspective. In an April 2014 speech, Berman argued: \"It's much more than just the automation of quotes and cancels, in spite of the seemingly exclusive fixation on this topic by much of the media and various outspoken market pundits. (...) I worry that it may be too narrowly focused and myopic.\"\n\nThe Chicago Federal Reserve letter of October 2012, titled \"How to keep markets safe in an era of high-speed trading\", reports on the results of a survey of several dozen financial industry professionals including traders, brokers, and exchanges. It found that\n\nThe CFA Institute, a global association of investment professionals, advocated for reforms regarding high-frequency trading, including:\n\nExchanges offered a type of order called a \"Flash\" order (on NASDAQ, it was called \"Bolt\" on the Bats stock exchange) that allowed an order to lock the market (post at the same price as an order ) for a small amount of time (5 milliseconds). This order type was available to all participants but since HFT's adapted to the changes in market structure more quickly than others, they were able to use it to \"jump the queue\" and place their orders before other order types were allowed to trade at the given price. Currently, the majority of exchanges do not offer flash trading, or have discontinued it. By March 2011, the NASDAQ, BATS, and Direct Edge exchanges had all ceased offering its Competition for Price Improvement functionality (widely referred to as \"flash technology/trading\").\n\nOn September 24, 2013, the Federal Reserve revealed that some traders are under investigation for possible news leak and insider trading. An anti-HFT firm called NANEX claimed that right after the Federal Reserve announced its newest decision, trades were registered in the Chicago futures market within two milliseconds. However, the news was released to the public in Washington D.C. at exactly 2:00 pm calibrated by atomic clock, and takes 3.19 milliseconds to reach Chicago at the speed of light in straight line and ca. 7 milliseconds in practice. Most of the conspiracy revolved around using inappropriate time stamps using times from the SIP (consolidated quote that is necessarily slow) and the amount of \"jitter\" that can happen when looking at such granular timings.\n\nIn March 2012, regulators fined Octeg LLC, the equities market-making unit of high-frequency trading firm Getco LLC, for $450,000. Octeg violated Nasdaq rules and failed to maintain proper supervision over its stock trading activities. The fine resulted from a request by Nasdaq OMX for regulators to investigate the activity at Octeg LLC from the day after the May 6, 2010 Flash Crash through the following December. Nasdaq determined the Getco subsidiary lacked reasonable oversight of its algo-driven high-frequency trading.\n\nIn October 2013, regulators fined Knight Capital $12 million for the trading malfunction that led to its collapse. Knight was found to have violated the SEC's market access rule, in effect since 2010 to prevent such mistakes. Regulators stated the HFT firm ignored dozens of error messages before its computers sent millions of unintended orders to the market. Knight Capital eventually merged with Getco to form KCG Holdings. Knight lost over $460 million from its trading errors in August 2012 that caused disturbance in the U.S. stock market.\n\nIn September 2014, HFT firm Latour Trading LLC agreed to pay a SEC penalty of $16 million. Latour is a subsidiary of New York-based high-frequency trader Tower Research Capital LLC. According to the SEC's order, for at least two years Latour underestimated the amount of risk it was taking on with its trading activities. By using faulty calculations, Latour managed to buy and sell stocks without holding enough capital. At times, the Tower Research Capital subsidiary accounted for 9% of all U.S. stock trading. The SEC noted the case is the largest penalty for a violation of the net capital rule.\n\nIn response to increased regulation, some have argued that instead of promoting government intervention, it would be more efficient to focus on a solution that mitigates information asymmetries among traders and their backers.\n\nOn January 12, 2015, the SEC announced a $14 million penalty against a subsidiary of BATS Global Markets, an exchange operator that was founded by high-frequency traders. The BATS subsidiary Direct Edge failed to properly disclose order types on its two exchanges EDGA and EDGX. These exchanges offered three variations of controversial \"Hide Not Slide\" orders and failed to accurately describe their priority to other orders. The SEC found the exchanges disclosed complete and accurate information about the order types \"only to some members, including certain high-frequency trading firms that provided input about how the orders would operate\". The complaint was made in 2011 by Haim Bodek.\n\nReported in January 2015, UBS agreed to pay $14.4 million to settle charges of not disclosing an order type that allowed high-frequency traders to jump ahead of other participants. The SEC stated that UBS failed to properly disclose to all subscribers of its dark pool \"the existence of an order type that it pitched almost exclusively to market makers and high-frequency trading firms\". UBS broke the law by accepting and ranking hundreds of millions of orders priced in increments of less than one cent, which is prohibited under Regulation NMS. The order type called PrimaryPegPlus enabled HFT firms \"to place sub-penny-priced orders that jumped ahead of other orders submitted at legal, whole-penny prices\".\n\nIn June 2014, high-frequency trading firm Citadel LLC was fined $800,000 for violations that included quote stuffing. Nasdaq's disciplinary action stated that Citadel \"failed to prevent the strategy from sending millions of orders to the exchanges with few or no executions.\" It was pointed out that Citadel \"sent multiple, periodic bursts of order messages, at 10,000 orders per second, to the exchanges. This excessive messaging activity, which involved hundreds of thousands of orders for more than 19 million shares, occurred two to three times per day.\"\n\nIn July 2013, it was reported that Panther Energy Trading LLC was ordered to pay $4.5 million to U.S. and U.K. regulators on charges that the firm's high-frequency trading activities manipulated commodity markets. Panther's computer algorithms placed and quickly canceled bids and offers in futures contracts including oil, metals, interest rates and foreign currencies, the U.S. Commodity Futures Trading Commission said. In October 2014, Panther's sole owner Michael Coscia was charged with six counts of commodities fraud and six counts of \"spoofing\". The indictment stated that Coscia devised a high-frequency trading strategy to create a false impression of the available liquidity in the market, \"and to fraudulently induce other market participants to react to the deceptive market information he created\".\n\nIn October 2014, Athena Capital Research LLC was fined $1 million on price manipulation charges. The high-speed trading firm used $40 million to rig prices of thousands of stocks, including eBay Inc, according to U.S. regulators. The HFT firm Athena manipulated closing prices commonly used to track stock performance with \"high-powered computers, complex algorithms and rapid-fire trades,\" the SEC said. The regulatory action is one of the first market manipulation cases against a firm engaged in high-frequency trading. Reporting by Bloomberg noted the HFT industry is \"besieged by accusations that it cheats slower investors.\"\n\nAdvanced computerized trading platforms and market gateways are becoming standard tools of most types of traders, including high-frequency traders. Broker-dealers now compete on routing order flow directly, in the fastest and most efficient manner, to the line handler where it undergoes a strict set of risk filters before hitting the execution venue(s). Ultra-low latency direct market access (ULLDMA) is a hot topic amongst brokers and technology vendors such as Goldman Sachs, Credit Suisse, and UBS. Typically, ULLDMA systems can currently handle high amounts of volume and boast round-trip order execution speeds (from hitting \"transmit order\" to receiving an acknowledgment) of 10 milliseconds or less.\n\nSuch performance is achieved with the use of hardware acceleration or even full-hardware processing of incoming market data, in association with high-speed communication protocols, such as 10 Gigabit Ethernet or PCI Express. More specifically, some companies provide full-hardware appliances based on FPGA technology to obtain sub-microsecond end-to-end market data processing.\n\nBuy side traders made efforts to curb predatory HFT strategies. Brad Katsuyama, co-founder of the IEX, led a team that implemented THOR, a securities order-management system that splits large orders into smaller sub-orders that arrive at the same time to all the exchanges through the use of intentional delays. This largely prevents information leakage in the propagation of orders that high-speed traders can take advantage of. In 2016, after having with Intercontinental Exchange Inc. and others failed to prevent SEC approval of IEX's launch and having failed to sue as it had threatened to do over the SEC approval, Nasdaq launched a 'speed bump' product of its own to compete with IEX. According to Nasdaq CEO Robert Greifeld \"the regulator shouldn’t have approved IEX without changing the rules that required quotes to be immediately visible\". The IEX speed bump—or trading slowdown—is 350-microseconds, which the SEC ruled was within the 'immediately visible' parameter. The slowdown promises to impede HST ability \"often [to] cancel dozens of orders for every trade they make\". Due to the National Best Bid/Offer regulation (NBBO), the effect of speed bumps is highly questionable and may be disadvantaging investors by luring in orders via artificially-delayed quotes.\n\nOutside of US equities, several notable spot foreign exchange (FX) trading platforms--including ParFX, EBS Market, and Thomson Reuters Matching--have implemented their own 'speed bumps' to curb or otherwise limit HFT activity. Unlike the IEX fixed length delay that retains the temporal ordering of messages as they are received by the platform, the spot FX platforms' 'speed bumps' reorder messages so the first message received is not necessarily that processed for matching first. In short, the spot FX platforms' speed bumps seek to reduce the benefit of a participant being faster than others, as has been described in various academic papers.\n\n"}
{"id": "208732", "url": "https://en.wikipedia.org/wiki?curid=208732", "title": "Highly composite number", "text": "Highly composite number\n\nA highly composite number, also known as an anti-prime, is a positive integer with more divisors than any smaller positive integer has. The term was coined by Ramanujan (1915). However, Jean-Pierre Kahane has suggested that the concept might have been known to Plato, who set 5040 as the ideal number of citizens in a city as 5040 has more divisors than any numbers less than it.\n\nThe related concept of largely composite number refers to a positive integer which has at least as many divisors as any smaller positive integer.\n\nThe initial or smallest 38 highly composite numbers are listed in the table below . The number of divisors is given in the column labeled \"d\"(\"n\").\n\nThe table below shows all the divisors of one of these numbers.\nThe 15,000th highly composite number can be found on Achim Flammenkamp's website. It is the product of 230 primes:\n\nwhere formula_2 is the sequence of successive prime numbers, and all omitted terms (\"a\" to \"a\") are factors with exponent equal to one (i.e. the number is formula_3). More concisely, it is the product of seven distinct primorials:\n\nwhere formula_5 is the primorial formula_6.\n\nRoughly speaking, for a number to be highly composite it has to have prime factors as small as possible, but not too many of the same. By the fundamental theorem of arithmetic, every positive integer \"n\" has a unique prime factorization:\n\nwhere formula_8 are prime, and the exponents formula_9 are positive integers.\n\nAny factor of n must have the same or lesser multiplicity in each prime:\n\nSo the number of divisors of \"n\" is:\n\nHence, for a highly composite number \"n\",\n\n\nAlso, except in two special cases \"n\" = 4 and \"n\" = 36, the last exponent \"c\" must equal 1. It means that 1, 4, and 36 are the only square highly composite numbers. Saying that the sequence of exponents is non-increasing is equivalent to saying that a highly composite number is a product of primorials.\n\nNote, that although the above described conditions are necessary, they are not sufficient for a number to be highly composite. For example, 96 = 2 × 3 satisfies the above conditions and has 12 divisors but is not highly composite since there is a smaller number 60 which has the same number of divisors.\n\nIf \"Q\"(\"x\") denotes the number of highly composite numbers less than or equal to \"x\", then there are two constants \"a\" and \"b\", both greater than 1, such that\nThe first part of the inequality was proved by Paul Erdős in 1944 and the second part by Jean-Louis Nicolas in 1988. We have\n\nand\n\nHighly composite numbers higher than 6 are also abundant numbers. One need only look at the three largest proper divisors of a particular highly composite number to ascertain this fact. It is false that all highly composite numbers are also Harshad numbers in base 10. The first HCN that is not a Harshad number is 245,044,800, which has a digit sum of 27, but 27 does not divide evenly into 245,044,800.\n\n10 of the first 38 highly composite numbers are superior highly composite numbers.\nThe sequence of highly composite numbers is a subset of the sequence of smallest numbers \"k\" with exactly \"n\" divisors .\n\nHighly composite numbers whose number of divisors is also a highly composite number are for n = 1, 2, 6, 12, 60, 360, 1260, 2520, 5040, 55440, 277200, 720720, 3603600, 61261200, 2205403200, 293318625600, 6746328388800, 195643523275200 . It is extremely likely that this sequence is complete.\n\nA positive integer \"n\" is a largely composite number if \"d\"(\"n\") ≥ \"d\"(\"m\") for all \"m\" ≤ \"n\". The counting function \"Q\"(\"x\") of largely composite numbers satisfies\nfor positive \"c\",\"d\" with formula_17.\n\nBecause the prime factorization of a highly composite number uses all of the first \"k\" primes, every highly composite number must be a practical number. Many of these numbers are used in traditional systems of measurement, and tend to be used in engineering designs, due to their ease of use in calculations involving fractions.\n\n\n\n"}
{"id": "35215355", "url": "https://en.wikipedia.org/wiki?curid=35215355", "title": "Hilbert–Mumford criterion", "text": "Hilbert–Mumford criterion\n\nIn mathematics, the Hilbert–Mumford criterion, introduced by David Hilbert and David Mumford, characterizes the semistable and stable points of a group action on a vector space in terms of eigenvalues of 1-parameter subgroups .\n\n"}
{"id": "31364787", "url": "https://en.wikipedia.org/wiki?curid=31364787", "title": "Inflation-restriction exact sequence", "text": "Inflation-restriction exact sequence\n\nIn mathematics, the inflation-restriction exact sequence is an exact sequence occurring in group cohomology and is a special case of the five-term exact sequence arising from the study of spectral sequences.\n\nSpecifically, let \"G\" be a group, \"N\" a normal subgroup, and \"A\" an abelian group which is equipped with an action of \"G\", i.e., a homomorphism from \"G\" to the automorphism group of \"A\". The quotient group \"G\"/\"N\" acts on \nThen the inflation-restriction exact sequence is:\n\nIn this sequence, there are maps\n\nThe inflation and restriction are defined for general \"n\":\n\nThe transgression is defined for general \"n\" \nonly if \"H\"(\"N\", \"A\") = 0 for \"i\" ≤ \"n\" − 1.\n\nThe sequence for general \"n\" may be deduced from the case \"n\" = 1 by dimension-shifting or from the Lyndon–Hochschild–Serre spectral sequence.\n\n"}
{"id": "943917", "url": "https://en.wikipedia.org/wiki?curid=943917", "title": "Laguerre polynomials", "text": "Laguerre polynomials\n\nIn mathematics, the Laguerre polynomials, named after Edmond Laguerre (1834 - 1886), are solutions of Laguerre's equation:\n\nwhich is a second-order linear differential equation. This equation has nonsingular solutions only if \"n\" is a non-negative integer.\n\nSometimes the name Laguerre polynomials is used for solutions of\n\nwhere is still a non-negative integer.\nThen they are also named generalized Laguerre polynomials, as will be done here (alternatively associated Laguerre polynomials or, rarely, Sonine polynomials, after their inventor Nikolay Yakovlevich Sonin).\n\nMore generally, a Laguerre function is a solution when is not necessarily a non-negative integer.\n\nThe Laguerre polynomials are also used for Gaussian quadrature to numerically compute integrals of the form\n\nThese polynomials, usually denoted \"L\", \"L\", ..., are a polynomial sequence which may be defined by the Rodrigues formula,\n\nreducing to the closed form of a following section.\n\nThey are orthogonal polynomials with respect to an inner product\n\nThe sequence of Laguerre polynomials is a Sheffer sequence,\n\nThe Rook polynomials in combinatorics are more or less the same as Laguerre polynomials, up to elementary changes of variables. Further see the Tricomi–Carlitz polynomials.\n\nThe Laguerre polynomials arise in quantum mechanics, in the radial part of the solution of the Schrödinger equation for a one-electron atom. They also describe the static Wigner functions of oscillator systems in quantum mechanics in phase space. They further enter in the quantum mechanics of the Morse potential and of the .\n\nPhysicists sometimes use a definition for the Laguerre polynomials which is larger by a factor of \"n\"<nowiki>!</nowiki> than the definition used here. Likewise in quantum mechanics the definition for the associated Laguerre polynomials differs.\n\nThese are the first few Laguerre polynomials:\n\nOne can also define the Laguerre polynomials recursively, defining the first two polynomials as\n\nand then using the following recurrence relation for any \"k\" ≥ 1:\n\nIn solution of some boundary value problems, the characteristic values can be useful:\n\nThe closed form is\n\nThe generating function for them likewise follows, \n\nPolynomials of negative index can be expressed using the ones with positive index:\n\nFor arbitrary real α the polynomial solutions of the differential equation\n\nare called generalized Laguerre polynomials, or associated Laguerre polynomials.\n\nOne can also define the generalized Laguerre polynomials recursively, defining the first two polynomials as\n\nand then using the following recurrence relation for any \"k\" ≥ 1:\n\nThe simple Laguerre polynomials are the special case of the generalized Laguerre polynomials:\n\nThe Rodrigues formula for them is\n\nThe generating function for them is\n\n\n\n\n\nGiven the generating function specified above, the polynomials may be expressed in terms of a contour integral\n\nwhere the contour circles the origin once in a counterclockwise direction without enclosing the essential singularity at 1\n\nThe addition formula for Laguerre polynomials:\n\nLaguerre's polynomials satisfy the recurrence relations\n\nin particular\n\nand\n\nor\n\nmoreover\n\nThey can be used to derive the four 3-point-rules\n\ncombined they give this additional, useful recurrence relations\n\nSince formula_42 is a monic polynomial of degree formula_43 in formula_44,\nthere is the partial fraction decomposition\nThe second equality follows by the following identity, valid for integer \"i\" and and immediate from the expression of formula_42 in terms of Charlier polynomials:\nFor the third equality apply the fourth and fifth identities of this section.\n\nDifferentiating the power series representation of a generalized Laguerre polynomial \"k\" times leads to\n\nThis points to a special case () of the formula above: for integer the generalized polynomial may be written\n\nthe shift by k sometimes causing confusion with the usual parenthesis notation for a derivative.\n\nMoreover, the following equation holds:\n\nwhich generalizes with Cauchy's formula to\n\nThe derivative with respect to the second variable has the form, \nThis is evident from the contour integral representation below.\n\nThe generalized Laguerre polynomials obey the differential equation\n\nwhich may be compared with the equation obeyed by the \"k\"th derivative of the ordinary Laguerre polynomial,\n\nwhere formula_55 for this equation only.\n\nIn Sturm–Liouville form the differential equation is\n\nwhich shows that is an eigenvector for the eigenvalue .\n\nThe generalized Laguerre polynomials are orthogonal over with respect to the measure with weighting function :\n\nwhich follows from\n\nIf formula_59 denotes the Gamma distribution then the orthogonality relation can be written as\n\nThe associated, symmetric kernel polynomial has the representations (Christoffel–Darboux formula)\n\nrecursively\n\nMoreover,\n\nTurán's inequalities can be derived here, which is\n\nThe following integral is needed in the quantum mechanical treatment of the hydrogen atom,\n\nLet a function have the (formal) series expansion\n\nThen\n\nThe series converges in the associated Hilbert space if and only if\n\nMonomials are represented as\n\nwhile binomials have the parametrization\n\nThis leads directly to\n\nfor the exponential function. The incomplete gamma function has the representation\n\nIn quantum mechanics the associated Laguerre polynomial is a component of the radial part of the schrodinger equation for the hydrogen atom. The associated Laguerre polynomial is defined as\n\nformula_73\n\nErdélyi gives the following two multiplication theorems \n\nThe generalized Laguerre polynomials are related to the Hermite polynomials:\n\nwhere the \"H\"(\"x\") are the Hermite polynomials based on the weighting function exp(−\"x\"), the so-called \"physicist's version.\"\n\nBecause of this, the generalized Laguerre polynomials arise in the treatment of the quantum harmonic oscillator.\n\nThe Laguerre polynomials may be defined in terms of hypergeometric functions, specifically the confluent hypergeometric functions, as\n\nwhere formula_78 is the Pochhammer symbol (which in this case represents the rising factorial).\n\nThe generalized Laguerre polynomials satisfy the Hardy-Hille formula\nwhere the series on the left converges for formula_80 and formula_81. Using the identity\n(see generalized hypergeometric function), this can also be written as\nThis formula is a generalization of the Mehler kernel for Hermite polynomials, which can be recovered from it by using the relations between Laguerre and Hermite polynomials given above.\n\n\n\n"}
{"id": "5971835", "url": "https://en.wikipedia.org/wiki?curid=5971835", "title": "List of mathematicians (U)", "text": "List of mathematicians (U)\n\n\n"}
{"id": "17777195", "url": "https://en.wikipedia.org/wiki?curid=17777195", "title": "Lommel polynomial", "text": "Lommel polynomial\n\nA Lommel polynomial \"R\"(\"z\"), introduced by , is a polynomial in 1/\"z\" giving the recurrence relation \nwhere \"J\"(\"z\") is a Bessel function of the first kind.\n\nThey are given explicitly by\n\n\n"}
{"id": "887461", "url": "https://en.wikipedia.org/wiki?curid=887461", "title": "Marianna Csörnyei", "text": "Marianna Csörnyei\n\nMarianna Csörnyei (born October 8, 1975 in Budapest) is a Hungarian mathematician who works as a professor at the University of Chicago. She does research in real analysis, geometric measure theory, and geometric nonlinear functional analysis. She proved the equivalence of the zero measure notions of infinite dimensional Banach spaces.\n\nCsörnyei received her doctorate from Eötvös Loránd University in 1999, supervised by György Petruska. She was a professor at the Mathematics Department of University College London between 1999–2011, and spent the 2009–2010 academic year at Yale University as visiting professor. Currently, she is at the University of Chicago.\n\nShe is contributing editor of the mathematical journal \"Real Analysis Exchange\".\n\nCsörnyei won a 2002 Whitehead Prize and a Royal Society Wolfson Research Merit Award that same year.\nShe was also awarded the Philip Leverhulme Prize for Mathematics and Statistics in 2008 for her work in geometric measure theory.\n\nShe was an invited sectional speaker at the International Congress of Mathematicians, in 2010.\n\n"}
{"id": "27606754", "url": "https://en.wikipedia.org/wiki?curid=27606754", "title": "Mary L. Boas", "text": "Mary L. Boas\n\nMary Layne Boas (1917–2010) was an American mathematician and physics professor best known as the author of \"Mathematical Methods in the Physical Sciences\" (1966), an undergraduate textbook that was still widely used in college classrooms as of 1999.\n\nShe received a bachelor's degree (1938) and a master's degree (1940) in mathematics at the University of Washington, and a Ph.D. (1948) in physics at the Massachusetts Institute of Technology. She taught physics at DePaul University in Chicago for thirty years, retiring in 1987 to return to Washington. Prior to her time at DePaul University, she served as an instructor in the mathematics department at Duke University.\n\nIn 2005, at the age of 88, Boas published the third edition of her textbook. She established the Mary L. Boas Endowed Scholarship at the University of Washington in 2008 to recognize outstanding academic achievements by female students in physics.\n\nMary Boas was married to mathematician Ralph P. Boas, Jr. Her son, Harold P. Boas, is also a noted mathematician. She died on February 17, 2010, at her home near Seattle, Washington.\n"}
{"id": "53600583", "url": "https://en.wikipedia.org/wiki?curid=53600583", "title": "Maxime Crochemore", "text": "Maxime Crochemore\n\nMaxime Crochemore (born 1947) is a French computer scientist known for his numerous contributions to algorithms on strings. He is currently a professor at King's College London.\n\nCrochemore earned his doctorate (PhD) in 1978 and his Doctorat d'état (DSc) in 1983 from the University of Rouen. He was a professor at Paris 13 University in 1985–1989, and moved to a professorship at Paris Diderot University in 1989. In 2002–2007, Crochemore was a senior research fellow at King's College London, where he is a professor since 2007. Since 2007, he is also a professor emeritus at the University of Marne-la-Vallée.\n\nCrochemore holds an honorary doctorate (2014) from the University of Helsinki. A festschrift in his honour was published in 2009 as a special issue of Theoretical Computer Science.\n\nCrochemore published over 100 journal papers on string algorithms. He in particular introduced new algorithms for pattern matching, string indexing and text compression. His work received a significant number of academic citations.\n\nCrochemore has co-authored three well-known scientific monographs on the design of algorithms for string processing: \"Text Algorithms\" (1994; jointly with Wojciech Rytter), \"Jewels of Stringology\" (2002, jointly with Wojciech Rytter), and \"Algorithms on Strings\" (2007, jointly with Christophe Hancart and Thierry Lecroq).\n\n"}
{"id": "36037560", "url": "https://en.wikipedia.org/wiki?curid=36037560", "title": "Maximum common edge subgraph", "text": "Maximum common edge subgraph\n\nGiven two graphs formula_1 and formula_2, the maximum common edge subgraph problem is the problem of finding a graph formula_3 with as many edges as possible which is isomorphic to both a subgraph of formula_1 and a subgraph of formula_2.\n\nThe maximum common edge subgraph problem on general graphs is NP-complete as it is a generalization of subgraph isomorphism: a graph formula_3 is isomorphic to a subgraph of another graph formula_1 if and only if the maximum common edge subgraph of formula_1 and formula_3 has the same number of edges as formula_3. Unless the two inputs formula_1 and formula_2 to the maximum common edge subgraph problem are required to have the same number of vertices, the problem is APX-hard.\n\n"}
{"id": "663683", "url": "https://en.wikipedia.org/wiki?curid=663683", "title": "Northwest (disambiguation)", "text": "Northwest (disambiguation)\n\nNorthwest is a compass point.\n\nNorthwest or north-west or north west may also refer to:\n\n\n\n\n\n\n\n\n"}
{"id": "5929336", "url": "https://en.wikipedia.org/wiki?curid=5929336", "title": "Office of the Chief Actuary", "text": "Office of the Chief Actuary\n\nThe Office of the Chief Actuary is a government agency that has responsibility for actuarial estimates regarding social welfare programs. In Canada, the Office of the Chief Actuary works with the Canada Pension Plan and the Old Age Security Program. In the United States, both the Social Security Administration and the Centers for Medicare and Medicaid Services have an Office of the Chief Actuary that deals with Social Security and Medicare, respectively. A similar agency in the United Kingdom is called the Government Actuary's Department (GAD).\n\nIn the U.S., the Office of the Chief Actuary at the Social Security Administration plans and directs a program of actuarial estimates and analyses relating to SSA-administered retirement, survivors and disability insurance programs and to proposed changes in those programs. It evaluates operations of the Federal Old-Age and Survivors Insurance Trust Fund and the Federal Disability Insurance Trust Fund, conducts studies of program financing, performs actuarial and demographic research on social insurance and related program issues, and projects future workloads.\n\nIn addition, the Office is charged with conducting cost analyses relating to the Supplemental Security Income (SSI) program, a general-revenue financed, means-tested program for low-income aged, blind and disabled people. The Office provides technical and consultative services to the Commissioner, to the Board of Trustees of the Social Security Trust Funds, and its staff appears before Congressional Committees to provide expert testimony on the actuarial aspects of Social Security issues.\n\n, the Chief Actuary of the Social Security Administration is Stephen Goss.\n\n"}
{"id": "28183104", "url": "https://en.wikipedia.org/wiki?curid=28183104", "title": "Polygon soup", "text": "Polygon soup\n\nA polygon soup is a set of unorganized polygons, typically triangles, before the application of any structuring operation, such as e.g. octree grouping.\n\nThe term must not to be confused with the \"PolySoup\" operation available in the 3D package Houdini, whose goal is to optimize the storage space needed by some piece of geometry through the reduction of the underlying number of polygon soups used in its representation. This is accomplished by removing redundant data points (e.g. vertices with the same position) without altering the topology or assigned properties of the optimized geometry in relation to the input one. As a result of this optimization, there can be savings in the storage and processing of large polygon meshes. These savings can have a bigger impact the larger the input data is. For instance, fluid simulations, particle simulations, rigid-body simulations, environments, and character models can reach into the millions of polygons for feature films, incurring in large storage and read/write costs. In those cases, reducing the number of polygon soups required to represent such data can lead to important savings in storage use and compute time.\n"}
{"id": "15584482", "url": "https://en.wikipedia.org/wiki?curid=15584482", "title": "Pregeometry (physics)", "text": "Pregeometry (physics)\n\nIn physics, a pregeometry is a structure from which geometry develops. Some cosmological models feature a pregeometric universe before the Big Bang. The term was championed by John Archibald Wheeler in the 1960s and 1970s as a possible route to a theory of quantum gravity. Since quantum mechanics allowed a metric to fluctuate, it was argued that the merging of gravity with quantum mechanics required a set of more fundamental rules regarding connectivity that were independent of topology and dimensionality. Where geometry could describe the properties of a known surface, the physics of a hypothetical region with predefined properties, \"pregeometry\" might allow one to work with deeper underlying rules of physics that were not so strongly dependent on simplified classical assumptions about the properties of space.\n\nNo single proposal for pregeometry has gained wide consensus support in the physics community. Some notions related to pregeometry predate Wheeler, other notions depart considerably from his outline of pregeometry but are still associated with it. A 2006 paper provided a survey and critique of pregeometry or near-pregeometry proposals up to that time. A summary of these is given below:\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome additional or related pregeometry proposals are:\n\n"}
{"id": "15895901", "url": "https://en.wikipedia.org/wiki?curid=15895901", "title": "Prescribed Ricci curvature problem", "text": "Prescribed Ricci curvature problem\n\nIn Riemannian geometry, a branch of mathematics, the prescribed Ricci curvature problem is as follows: given a smooth manifold \"M\" and a symmetric 2-tensor \"h\", construct a metric on \"M\" whose Ricci curvature tensor equals \"h\".\n\n\n"}
{"id": "53242993", "url": "https://en.wikipedia.org/wiki?curid=53242993", "title": "PyMC3", "text": "PyMC3\n\nPyMC3 is a Python package for Bayesian statistical modeling and probabilistic machine learning which focuses on advanced Markov chain Monte Carlo and variational fitting algorithms. It is a rewrite from scratch of the previous version of the PyMC software. Unlike PyMC2, which had used Fortran extensions for performing computations, PyMC3 relies on Theano for automatic differentiation and also for computation optimization and dynamic C compilation. PyMC3, together with Stan, are the most popular probabilistic programming tools. PyMC3 is an open source project, developed by the community and fiscally sponsored by NumFocus.\n\nPyMC3 has been used to solve inference problems in several scientific domains, including astronomy, molecular biology, crystallography, chemistry, ecology and psychology. Previous versions of PyMC were also used widely, for example in climate science, public health, neuroscience, and parasitology.\n\nPyMC3 implements non-gradient-based and gradient-based Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference and stochastic, gradient-based variational Bayesian methods for approximate Bayesian inference.\n\n\n\n"}
{"id": "892803", "url": "https://en.wikipedia.org/wiki?curid=892803", "title": "Quantum error correction", "text": "Quantum error correction\n\nQuantum error correction (QEC) is used in quantum computing to protect quantum information from errors due to decoherence and other quantum noise. Quantum error correction is essential if one is to achieve fault-tolerant quantum computation that can deal not only with noise on stored quantum information, but also with faulty quantum gates, faulty quantum preparation, and faulty measurements.\n\nClassical error correction employs redundancy. The simplest way is to store the information multiple times, and—if these copies are later found to disagree—just take a majority vote; e.g. \nSuppose we copy a bit three times. Suppose further that a noisy error corrupts the three-bit state so that one bit is equal to zero but the other two are equal to one. If we assume that noisy errors are independent and occur with some probability p, it is most likely that the error is a single-bit error and the transmitted message is three ones. It is possible that a double-bit error occurs and the transmitted message is equal to three zeros, but this outcome is less likely than the above outcome.\n\nCopying quantum information is not possible due to the no-cloning theorem. This theorem seems to present an obstacle to formulating a theory of quantum error correction. But it is possible to \"spread\" the information of one qubit onto a highly entangled state of several (\"physical\") qubits. Peter Shor first discovered this method of formulating a \"quantum error correcting code\" by storing the information of one qubit onto a highly entangled state of nine qubits. A quantum error correcting code protects quantum information against errors of a limited form.\n\nClassical error correcting codes use a \"syndrome measurement\" to diagnose which error corrupts an encoded state. We then reverse an error by applying a corrective operation based on the syndrome. Quantum error correction also employs syndrome measurements. We perform a multi-qubit measurement that does not disturb the quantum information in the encoded state but retrieves information about the error. A syndrome measurement can determine whether a qubit has been corrupted, and if so, which one. What is more, the outcome of this operation (the \"syndrome\") tells us not only which physical qubit was affected, but also, in which of several possible ways it was affected. The latter is counter-intuitive at first sight: Since noise is arbitrary, how can the effect of noise be one of only few distinct possibilities? In most codes, the effect is either a bit flip, or a sign (of the phase) flip, or both (corresponding to the Pauli matrices \"X\", \"Z\", and \"Y\"). The reason is that the measurement of the syndrome has the projective effect of a quantum measurement. So even if the error due to the noise was arbitrary, it can be expressed as a superposition of basis operations—the \"error basis\" (which is here given by the Pauli matrices and the identity). \nThe syndrome measurement \"forces\" the qubit to \"decide\" for a certain specific \"Pauli error\" to \"have happened\", and the syndrome tells us which, so that we can let the same Pauli operator act again on the corrupted qubit to revert the effect of the error.\n\nThe syndrome measurement tells us as much as possible about the error that has happened, but \"nothing\" at all about the \"value\" that is stored in the logical qubit—as otherwise the measurement would destroy any quantum superposition of this logical qubit with other qubits in the quantum computer.\n\nThe repetition code works in a classical channel, because classical bits are easy to measure and to repeat. This stops being the case for a quantum channel in which, due to the no-cloning theorem, it is no longer possible to repeat a single qubit three times. To overcome this, a different method, such as the so-called \"three-qubit bit flip code\", has to be used. This technique uses entanglement and syndrome measurements and is comparable in performance with the repetition code.\nConsider the situation in which we want to transmit the state of a single qubit formula_1 through a noisy channel formula_2. Let us moreover assume that this channel either flips the state of the qubit, with probability formula_3, or leaves it unchanged. The action of formula_2 on a general input formula_5 can therefore be written as formula_6.\n\nLet formula_7 be the quantum state to be transmitted. With no error correcting protocol in place, the transmitted state will be correctly transmitted with probability formula_8. We can however improve on this number by \"encoding\" the state into a greater number of qubits, in such a way that errors in the corresponding \"logical qubits\" can be detected and corrected. In the case of the simple three-qubit repetition code, the encoding consists in the mappings formula_9 and formula_10. The input state formula_1 is encoded into the state formula_12. This mapping can be realized for example using two CNOT gates, entangling the system with two ancillary qubits initialized in the state formula_13. The encoded state formula_14 is what is now passed through the noisy channel.\n\nThe channel acts on formula_14 by flipping some subset (possibly empty) of its qubits. No qubit is flipped with probability formula_16, a single qubit is flipped with probability formula_17, two qubits are flipped with probability formula_18, and all three qubits are flipped with probability formula_19. Note that a further assumption about the channel is made here: we assume that formula_2 acts equally and independently on each of the three qubits in which the state is now encoded. The problem is now how to detect and correct such errors, \"without at the same time corrupting the transmitted state\".\n\nLet us assume for simplicity that formula_3 is small enough that the probability of more than a single qubit being flipped is negligible. One can then detect whether a qubit was flipped, \"without also querying for the values being transmitted\", by asking whether one of the qubits differs from the others. This amounts to performing a measurement with four different outcomes, corresponding to the following four projective measurements:formula_22This can be achieved for example by measuring formula_23 and then formula_24. This reveals which qubits are different from which others, without at the same time giving information about the state of the qubits themselves. If the outcome corresponding to formula_25 is obtained, no correction is applied, while if the outcome corresponding to formula_26 is observed, then the Pauli X gate is applied to the formula_27-th qubit. Formally, this correcting procedure corresponds to the application of the following map to the output of the channel: \nformula_28Note that, while this procedure perfectly corrects the output when zero or one flips are introduced by the channel, if more than one qubit is flipped then the output is not properly corrected. For example, if the first and second qubits are flipped, then the syndrome measurement gives the outcome formula_29, and the third qubit is flipped, instead of the first two. To assess the performance of this error correcting scheme for a general input we can study the fidelity formula_30 between the input formula_14 and the output formula_32. Being the output state formula_33 correct when no more than one qubit is flipped, which happens with probability formula_34, we can write it as formula_35, where the dots denote components of formula_33 resulting from errors not properly corrected by the protocol. It follows that formula_37This fidelity is to be compared with the corresponding fidelity obtained when no error correcting protocol is used, which was shown before to equal formula_38. A little algebra then shows that the fidelity \"after\" error correction is greater than the one without for formula_39. Note that this is consistent with the working assumption that was made while deriving the protocol (of formula_3 being small enough).\n\nFlipped bits are the only kind of error in classical computer, but there is another possibility of an error with quantum computers, the sign flip. Through the transmission in a channel the relative sign between formula_41 and formula_42 can become inverted. For instance, a qubit in the state formula_43 may have its sign flip to formula_44\n\nThe original state of the qubit\n\nformula_45\n\nwill be changed into the state\n\nformula_46\n\nIn the Hadamard basis, bit flips become sign flips and sign flips become bit flips. Let formula_47 be a quantum channel that can cause at most one phase flip. Then the bit flip code from above can recover formula_48 by transforming into the Hadamard basis before and after transmission through formula_47.\n\nThe error channel may induce either a bit flip, a sign flip, or both. It is possible to correct for both types of errors using one code, and the Shor code does just that. In fact, the Shor code corrects arbitrary single-qubit errors.\nLet formula_50 be a quantum channel that can arbitrarily corrupt a single qubit. The 1st, 4th and 7th qubits are for the sign flip code, while the three group of qubits (1,2,3), (4,5,6), and (7,8,9) are designed for the bit flip code. With the Shor code, a qubit state formula_51 will be transformed into the product of 9 qubits formula_52, where\n\nIf a bit flip error happens to a qubit, the syndrome analysis will be performed on each set of states (1,2,3), (4,5,6), and (7,8,9), then correct the error.\n\nIf the three bit flip group (1,2,3), (4,5,6), and (7,8,9) are considered as three inputs, then the Shor code circuit can be reduced as a sign flip code. This means that the Shor code can also repair sign flip error for a single qubit.\n\nThe Shor code also can correct for any arbitrary errors (both bit flip and sign flip) to a single qubit. If an error is modeled by a unitary transform U, which will act on a qubit formula_48, then formula_56 can be described in the form\n\nwhere formula_58,formula_59,formula_60, and formula_61 are complex constants, I is the identity, and the Pauli matrices are given by\n\nIf U is equal to I, then no error occurs. If formula_65, a bit flip error occurs. If formula_66, a sign flip error occurs. If formula_67 then both a bit flip error and a sign flip error occur. Due to linearity, it follows that the Shor code can correct arbitrary 1-qubit errors.\n\nSeveral proposals have been made for storing error-correctable quantum information in bosonic modes. Unlike a two-level system, an oscillator has infinitely many energy levels in a single physical system. For example, the cat code was followed shortly after by Gottesman-Kitaev-Preskill (GKP) states, and more recently by the binomial code. The insight offered by these codes is to take advantage of this redundancy within a single system, rather than to duplicate many two-level qubits.\n\nWritten in the Fock basis, the simplest binomial encoding is\n\nformula_68 \n\nwhere the subscript L indicates a \"logically encoded\" state. Then if the dominant error mechanism of the system is the stochastic application of the bosonic lowering operator formula_69 the corresponding error states are formula_70 and formula_71 respectively. Since the codewords involve only even photon number, and the error states involve only odd photon number, errors can be detected by measuring the photon number parity of the system.\n\nIn general, a \"quantum code\" for a quantum channel formula_72 is a subspace formula_73, where formula_74 is the state Hilbert space, such that there exists another quantum channel formula_75 with\n\nformula_76\n\nwhere formula_77 is the orthogonal projection onto formula_78. Here formula_75 is known as the \"correction operation\".\n\nA \"non-degenerate code\" is one for which different elements of the set of correctable errors produce linearly independent results when applied to elements of the code. If distinct of the set of correctable errors produce orthogonal results, the code is considered \"pure\".\n\nOver time, researchers have come up with several codes:\n\n\nThat these codes allow indeed for quantum computations of arbitrary length is the content of the \"threshold theorem\", found by Michael Ben-Or and Dorit Aharonov, which asserts that you can correct for all errors if you concatenate quantum codes such as the CSS codes—i.e. re-encode each logical qubit by the same code again, and so on, on logarithmically many levels—\"provided\" the error rate of individual quantum gates is below a certain threshold; as otherwise, the attempts to measure the syndrome and correct the errors would introduce more new errors than they correct for.\n\nAs of late 2004, estimates for this threshold indicate that it could be as high as 1-3%, provided that there are sufficiently many qubits available.\n\nThere have been several experimental realizations of CSS-based codes. The first demonstration was with NMR qubits. Subsequently, demonstrations have been made with linear optics, trapped ions, and superconducting (transmon) qubits.\n\nOther error correcting codes have also been implemented, such as one aimed at correcting for photon loss, the dominant error source in photonic qubit schemes.\n\n\n\n"}
{"id": "7415899", "url": "https://en.wikipedia.org/wiki?curid=7415899", "title": "Sequential quadratic programming", "text": "Sequential quadratic programming\n\nSequential quadratic programming (SQP) is an iterative method for constrained nonlinear optimization. SQP methods are used on mathematical problems for which the objective function and the constraints are twice continuously differentiable.\n\nSQP methods solve a sequence of optimization subproblems, each of which optimizes a quadratic model of the objective subject to a linearization of the constraints. If the problem is unconstrained, then the method reduces to Newton's method for finding a point where the gradient of the objective vanishes. If the problem has only equality constraints, then the method is equivalent to applying Newton's method to the first-order optimality conditions, or Karush–Kuhn–Tucker conditions, of the problem.\n\nConsider a nonlinear programming problem of the form:\n\nThe Lagrangian for this problem is\n\nwhere formula_3 and formula_4 are Lagrange multipliers. At an iterate formula_5, a basic sequential quadratic programming algorithm defines an appropriate search direction formula_6 as a solution to the quadratic programming subproblem\n\nNote that the term formula_8 in the expression above may be left out for the minimization problem, since it is constant.\n\n\nSQP methods have been implemented such well known numerical environments as MATLAB and GNU Octave. There also exist numerous software libraries, including open source\nand proprietary/commercial ones\n\n\n\n"}
{"id": "1137949", "url": "https://en.wikipedia.org/wiki?curid=1137949", "title": "Statistical arbitrage", "text": "Statistical arbitrage\n\nIn finance, statistical arbitrage (often abbreviated as \"Stat Arb\" or \"StatArb\") is a class of short-term financial trading strategies that employ mean reversion models involving broadly diversified portfolios of securities (hundreds to thousands) held for short periods of time (generally seconds to days). These strategies are supported by substantial mathematical, computational, and trading platforms.\n\nAs a trading strategy, statistical arbitrage is a heavily quantitative and computational approach to securities trading. It involves data mining and statistical methods, as well as the use of automated trading systems.\n\nHistorically, StatArb evolved out of the simpler pairs trade strategy, in which stocks are put into pairs by fundamental or market-based similarities. When one stock in a pair outperforms the other, the poorer performing stock is bought long with the expectation that it will climb towards its outperforming partner, the other is sold short. Mathematically speaking, the strategy is to find a pair of stocks with high correlation, cointegration, or other common factor characteristics. Various statistical tools have been used in the context of pairs trading ranging from simple distance-based approaches to more complex tools such as cointegration and copula concepts.\n\nStatArb considers not pairs of stocks but a portfolio of a hundred or more stocks—some long, some short—that are carefully matched by sector and region to eliminate exposure to beta and other risk factors. Portfolio construction is automated and consists of two phases. In the first or \"scoring\" phase, each stock in the market is assigned a numeric score or rank that reflects its desirability; high scores indicate stocks that should be held long and low scores indicate stocks that are candidates for shorting. The details of the scoring formula vary and are highly proprietary, but, generally (as in pairs trading), they involve a short term mean reversion principle so that, e.g., stocks that have done unusually well in the past week receive low scores and stocks that have underperformed receive high scores. In the second or \"risk reduction\" phase, the stocks are combined into a portfolio in carefully matched proportions so as to eliminate, or at least greatly reduce, market and factor risk. This phase often uses commercially available risk models like MSCI/Barra/APT/Northfield/Risk Infotech/Axioma to constrain or eliminate various risk factors.\n\nBroadly speaking, StatArb is actually any strategy that is bottom-up, beta-neutral in approach and uses statistical/econometric techniques in order to provide signals for execution. Signals are often generated through a contrarian mean reversion principle but can also be designed using such factors as lead/lag effects, corporate activity, short-term momentum, etc. This is usually referred to as a multi-factor approach to StatArb.\n\nBecause of the large number of stocks involved, the high portfolio turnover and the fairly small size of the effects one is trying to capture, the strategy is often implemented in an automated fashion and great attention is placed on reducing trading costs.\n\nStatistical arbitrage has become a major force at both hedge funds and investment banks. Many bank proprietary operations now center to varying degrees around statistical arbitrage trading.\n\nOver a finite period of time, a low probability market movement may impose heavy short-term losses. If such short-term losses are greater than the investor's funding to meet interim margin calls, its positions may need to be liquidated at a loss even when its strategy's modeled forecasts ultimately turn out to be correct. The 1998 default of Long-Term Capital Management was a widely publicized example of a fund that failed due to its inability to post collateral to cover adverse market fluctuations.\n\nStatistical arbitrage is also subject to model weakness as well as stock- or security-specific risk. The statistical relationship on which the model is based may be spurious, or may break down due to changes in the distribution of returns on the underlying assets. Factors, which the model may not be aware of having exposure to, could become the significant drivers of price action in the markets, and the inverse applies also. The existence of the investment based upon model itself may change the underlying relationship, particularly if enough entrants invest with similar principles. The exploitation of arbitrage opportunities themselves increases the efficiency of the market, thereby reducing the scope for arbitrage, so continual updating of models is necessary.\n\nOn a stock-specific level, there is risk of M&A activity or even default for an individual name. Such an event would immediately invalidate the significance of any historical relationship assumed from empirical statistical analysis of the past data.\n\nDuring July and August 2007, a number of StatArb (and other Quant type) hedge funds experienced significant losses at the same time, which is difficult to explain unless there was a common risk factor. While the reasons are not yet fully understood, several published accounts blame the emergency liquidation of a fund that experienced capital withdrawals or margin calls. By closing out its positions quickly, the fund put pressure on the prices of the stocks it was long and short. Because other StatArb funds had similar positions, due to the similarity of their alpha models and risk-reduction models, the other funds experienced adverse returns. One of the versions of the events describes how Morgan Stanley's highly successful StatArb fund, PDT, decided to reduce its positions in response to stresses in other parts of the firm, and how this contributed to several days of hectic trading.\n\nIn a sense, the fact of a stock being heavily involved in StatArb is itself a risk factor, one that is relatively new and thus was not taken into account by the StatArb models. These events showed that StatArb has developed to a point where it is a significant factor in the marketplace, that existing funds have similar positions and are in effect competing for the same returns. Simulations of simple StatArb strategies by Khandani and Lo show that the returns to such strategies have been reduced considerably from 1998 to 2007, presumably because of competition.\n\nIt has also been argued that the events during August 2007 were linked to reduction of liquidity, possibly due to risk reduction by high-frequency market makers during that time.\n\nStatistical arbitrage faces different regulatory situations in different countries or markets. In many countries where the trading security or derivatives are not fully developed, investors find it infeasible or unprofitable to implement statistical arbitrage in local markets.\n\nIn China, quantitative investment including statistical arbitrage is not the mainstream approach to investment. A set of market conditions restricts the trading behavior of funds and other financial institutions. The restriction on short selling as well as the market stabilization mechanisms (e.g. daily limit) set heavy obstacles when either individual investors or institutional investors try to implement the trading strategy implied by statistical arbitrage theory.\n\n\n\n"}
{"id": "20768125", "url": "https://en.wikipedia.org/wiki?curid=20768125", "title": "Surface-to-surface intersection problem", "text": "Surface-to-surface intersection problem\n\nThe surface-to-surface intersection (SSI) problem. is a basic problem in computer-aided geometric design: Given two intersecting surfaces in R, compute all parts of the intersection curve. If two surfaces intersect, the result will be a set of isolated points, a set of curves, a set of overlapping surfaces, or any combination of these cases. Because exact solutions can be found only for some special surface classes, approximation methods must be used for the general case.\n\n\n"}
{"id": "1468572", "url": "https://en.wikipedia.org/wiki?curid=1468572", "title": "The Man Who Counted", "text": "The Man Who Counted\n\nThe Man Who Counted (original Portuguese title: O Homem que Calculava) is a book on recreational mathematics and curious word problems by Brazilian writer Júlio César de Mello e Souza, published under the pen name Malba Tahan. Since its first publication in 1938, the book has been immensely popular in Brazil and abroad, not only among mathematics teachers but among the general public as well.\n\nThe book has been published in many other languages, including Catalan, English (in the UK and in the US), German, Italian, and Spanish, and is recommended as a paradidactic source in many countries. It earned its author a prize from the Brazilian Literary Academy.\n\nFirst published in Brazil in 1949, \"O Homem que Calculava\" is a series of tales in the style of the \"Arabian Nights\", but revolving around mathematical puzzles and curiosities. The book is ostensibly a translation by Brazilian scholar Breno de Alencar Bianco of an original manuscript by Malba Tahan, a thirteenth-century Persian scholar of the Islamic Empire – both equally fictitious.\n\nThe first two chapters tell how Hanak Tade Maia was traveling from Samarra to Baghdad when he met Beremiz Samir, a young lad with amazing mathematical abilities. The traveler then invited Beremiz to come with him to Baghdad, where a man with his abilities will certainly find profitable employment. The rest of the book tells of various incidents that befell the two men along the road and in Baghdad. In all those events, Beremiz Samir uses his abilities with calculation like a magic wand to amaze and entertain people, settle disputes, and find wise and just solutions to seemingly unsolvable problems.\n\nIn the first incident along their trip (chapter III), Beremiz settles a heated inheritance dispute between three brothers. Their father had left them 35 camels, of which 1/2 (17.5 camels) should go to his eldest son, 1/3 (11.666... camels) to the middle one, and 1/9 (3.888... camels) to the youngest. To solve the brothers dilemma, Beremiz convinces Hanak to donate his only camel to the dead man's estate. Then, with 36 camels, Beremiz gives 18, 12, and 4 animals to the three heirs, making all of them profit with the new share. Of the remaining two camels, one is returned to Hanak, and the other is claimed by Beremiz as his reward.\n\nThe translator's notes observe that a variant of this problem, with 17 camels to be divided in the same proportions, is found in hundreds of recreational mathematics books, such as those of E. Fourrey (1949) and G. Boucheny (1939). However, the 17-camel version leaves only one camel at the end, with no net profit for the estate's executor.\n\nAt the end of the book, Beremiz uses his abilities to win the hand of his student and secret love Telassim, the daughter of one of the Caliph's advisers. (The caliph mentioned is Al-Musta'sim, and the time period ends with the Abbasid dynasty's collapse.)\n\nIn the last chapter we learn that Hanak Tade Maia and Beremiz eventually moved to Constantinople, where Beremiz had three sons and Hanak visits him often.\n\nThe \"translator's note\" signed \"B. A. Bianco\" is dated from 1965. The preface signed \"Malba Tahan\" is dated \"Baghdad, 19 of the Moon of Ramadan of 1321\".\n\nThe 1993 English edition published by W.W. Norton & Co. was illustrated by Patricia Reid Baquero.\n\nThe fifty fourth printing by Editora Record (2001; in Portuguese) contains 164 pages of Malba Tahan's text, plus 60 pages of notes and historical appendices, commented solutions to all the problems, a glossary of Arabic terms, alphabetical index, and other material.\n\nThe book was translated into Arabic in 2005 by Azza Kubba, an Iraqi from Baghdad (published by Al-Jamel Publishing House, Cologne, Germany).\n\n\n"}
{"id": "31075794", "url": "https://en.wikipedia.org/wiki?curid=31075794", "title": "Thomas Kipling", "text": "Thomas Kipling\n\nThomas Kipling (1745 or 1746 – 28 January 1822) was a British churchman and academic.\n\nHe entered St John's College, Cambridge University in 1764 at age 18 and was senior wrangler in 1768. He received an M.A. in 1771, a B.D. in 1779, and a D.D. in 1784. He was Boyle Lecturer in 1792, and Master of the Temple in 1797. He was a Deputy Regius Professor of Divinity from 1787 to 1802, and Dean of Peterborough from 1798 to 1822.\n\nKipling was active as a prosecutor against William Frend for the latter's Unitarian views.\n\nBorn at Bowes, Yorkshire, he was son of William Kipling, a clock maker. He received his early education at Scroton and at Sedbergh School, and was admitted a sizar of St. John's College, Cambridge, on 28 June 1764. He graduated B.A. in 1768, was elected a fellow of his college 29 January 1770, and commenced M.A. in 1771. In 1773 he was elected one of the taxors of the university. He took the degree of B.D. in 1779. In 1782 he was elected Lady Margaret's preacher on the resignation of Richard Farmer. He was created D.D. in 1784, in which year he was presented by his college to the vicarage of Holme on Spalding Moor, Yorkshire. In 1787 he was appointed deputy regius professor of divinity, the professor, Richard Watson, being in ill-health. In 1792 he preached the Boyle lectures, but did not print the course.\n\nIn 1792 Kipling was attacked by liberals in the university for his part in promoting the prosecution of William Frend, Fellow of Jesus College who had attacked the established Church of England. On 10 February 1798 he was made Dean of Peterborough. In the summer of 1802 he resigned the deputy professorship of divinity.\n\nWhen John Lingard's \"Strictures\" on Herbert Marsh's \"Comparative View of the Churches of England and Rome\" appeared in 1815, Kipling took offence at the terminology \"modern church of England\"; and thinking that it came within the category of \"seditious words, in derogation of the established religion\", wrote to Lingard through the public papers informing him that unless within a reasonable time he published a vindication of his \"inflammatory language\" he would be indicted. Lingard merely advertised his \"Strictures\" in all the papers which had published Kipling's letter; and the controversy died away. Jeremy Bentham mentioned a letter of Kipling of 1815 on schism in his \"Church-of-Englandism\" (1817), though a fuller treatment of points he wished to make against Kipling, Gerard Andrewes and Nicholas Vansittart was omitted for reasons of length.\n\nKipling died at his parsonage, after a lingering illness, on 28 January 1822.\n\nKipling's major work was \"Codex Theodori Bezæ Cantabrigiensis, Evangelia et Apostolorum Acta complectens, quadratis literis, Græco-Latinus. Academia auspicante venerandæ has vetustatis reliquias, summa qua potuit fide, adumbravit, expressit, edidit, Codicis historiam præfixit, notasque adjecit T. Kipling\", Greek and Latin, 2 pts., Cambridge, 1793, printed at the university press. The impression was limited to 250 copies. This edition of the \"Codex Bezæ\" used types resembling the uncial characters of the original manuscript. It was criticised in the \"Monthly Review\", new ser. xii. 241–6, and by Richard Porson in two notices in the \"British Critic\", vol. iii. (1794); and the preface was attacked in a pamphlet entitled \"Remarks on Dr. Kipling's Preface to Beza. Part the first\" (London, 1793), by Thomas Edwards the vicar of Histon (no second part appeared). Errors in this edition and the bad latinity of the preface were mercilessly censured, so that in the slang of the university a \"Kiplingism\" came to be synonymous with a grammatical blunder. George Horne remarked that Kipling's work, although imperfect, was unfairly underrated. Frederick Henry Ambrose Scrivener, in the preface to his own edition of the \"Bezæ Codex Cantabrigiensis\" (Cambridge, 1864) defended Kipling's textual work, but not his adopted forms.\n\nKipling's other works were: \n\n"}
{"id": "38880232", "url": "https://en.wikipedia.org/wiki?curid=38880232", "title": "Tobler's hiking function", "text": "Tobler's hiking function\n\nTobler's hiking function is an exponential function determining the hiking speed, taking into account the slope angle. It was formulated by Waldo Tobler. This function was estimated from empirical data of Eduard Imhof.\n\nWalking velocity:\n\nwhere\n\nThe velocity on the flat terrain is 5 km / h, the maximum speed of 6 km / h is achieved roughly at -2.86°.\n\nOn flat terrain this formula works out to 5 km/h. For off-path travel, this value should be multiplied by 3/5, for horseback by 5/4.\n\nPace is the reciprocal of speed. For Tobler's hiking function it can be calculated from the following conversion:\n\nwhere\n\n"}
{"id": "390562", "url": "https://en.wikipedia.org/wiki?curid=390562", "title": "Tomasulo algorithm", "text": "Tomasulo algorithm\n\nTomasulo’s algorithm is a computer architecture hardware algorithm for dynamic scheduling of instructions that allows out-of-order execution and enables more efficient use of multiple execution units. It was developed by Robert Tomasulo at IBM in 1967 and was first implemented in the IBM System/360 Model 91’s floating point unit.\n\nThe major innovations of Tomasulo’s algorithm include register renaming in hardware, reservation stations for all execution units, and a common data bus (CDB) on which computed values broadcast to all reservation stations that may need them. These developments allow for improved parallel execution of instructions that would otherwise stall under the use of scoreboarding or other earlier algorithms.\n\nRobert Tomasulo received the Eckert–Mauchly Award in 1997 for his work on the algorithm.\n\nThe following are the concepts necessary to the implementation of Tomasulo's Algorithm:\n\nThe Common Data Bus (CDB) connects reservation stations directly to functional units. According to Tomasulo it \"preserves precedence while encouraging concurrency\". This has two important effects:\n\nInstructions are issued sequentially so that the effects of a sequence of instructions, such as exceptions raised by these instructions, occur in the same order as they would on an in-order processor, regardless of the fact that they are being executed out-of-order (i.e. non-sequentially). \n\nTomasulo's Algorithm uses register renaming to correctly perform out-of-order execution. All general-purpose and reservation station registers hold either a real value or a placeholder value. If a real value is unavailable to a destination register during the issue stage, a placeholder value is initially used. The placeholder value is a tag indicating which reservation station will produce the real value. When the unit finishes and broadcasts the result on the CDB, the placeholder will be replaced with the real value.\n\nEach functional unit has a single reservation station. Reservation stations hold information needed to execute a single instruction, including the operation and the operands. The functional unit begins processing when it is free and when all source operands needed for an instruction are real.\n\nPractically speaking, there may be exceptions for which not enough status information about an exception is available, in which case the processor may raise a special exception, called an \"imprecise\" exception. Imprecise exceptions cannot occur in non-OoOE implementations, as processor state is changed only in program order (see RISC Pipeline Exceptions).\n\nPrograms that experience \"precise\" exceptions, where the specific instruction that took the exception can be determined, can restart or re-execute at the point of the exception. However, those that experience \"imprecise\" exceptions generally cannot restart or re-execute, as the system cannot determine the specific instruction that took the exception.\n\nThe three stages listed below are the stages through which each instruction passes from the time it is issued to the time its execution is complete.\n\n\nIn the issue stage, instructions are issued for execution if all operands and reservation stations are ready or else they are stalled. Registers are renamed in this step, eliminating WAR and WAW hazards.\n\n\nIn the execute stage, the instruction operations are carried out. Instructions are delayed in this step until all of their operands are available, eliminating RAW hazards. Program correctness is maintained through effective address calculation to prevent hazards through memory.\n\n\nIn the write Result stage, ALU operations results are written back to registers and store operations are written back to memory.\n\nThe concepts of reservation stations, register renaming, and the common data bus in Tomasulo's algorithm presents significant advancements in the design of high-performance computers.\n\nReservation stations take on the responsibility of waiting for operands in the presence of data dependencies and other inconsistencies such as varying storage access time and circuit speeds, thus freeing up the functional units. This improvement overcomes long floating point delays and memory accesses. In particular the algorithm is more tolerant of cache misses. Additionally, programmers are freed from implementing optimized code. This is a result of the common data bus and reservation station working together to preserve dependencies as well as encouraging concurrency.\n\nBy tracking operands for instructions in the reservation stations and register renaming in hardware the algorithm minimizes read-after-write (RAW) and eliminates write-after-write (WAW) and Write-after-Read (WAR) computer architecture hazards. This improves performance by reducing wasted time that would otherwise be required for stalls.\n\nAn equally important improvement in the algorithm is the design is not limited to a specific pipeline structure. This improvement allows the algorithm to be more widely adopted by multiple-issue processors. Additionally, the algorithm is easily extended to enable branch speculation. \n\nTomasulo's algorithm, outside of IBM, was unused for several years after its implementation in the System/360 Model 91 architecture. However, it saw a vast increase in usage during the 1990s for 3 reasons:\n\nMany modern processors implement dynamic scheduling schemes that are derivative of Tomasulo’s original algorithm, including popular Intel x86-64 chips.\n\n\n"}
{"id": "44554214", "url": "https://en.wikipedia.org/wiki?curid=44554214", "title": "Univalent foundations", "text": "Univalent foundations\n\nUnivalent foundations are an approach to the foundations of mathematics in which mathematical structures are built out of objects called \"types\". Types in the univalent foundations do not correspond exactly to anything in set-theoretic foundations, but they may be thought of as spaces, with equal types corresponding to homotopy equivalent spaces and with equal elements of a type corresponding to points of a space connected by a path. Univalent foundations are inspired both by the old Platonic ideas of Hermann Grassmann and Georg Cantor and by the \"categorical\" mathematics in the style of Alexander Grothendieck. It departs from the use of predicate logic as the underlying formal deduction system, replacing it, at the moment, by a version of the Martin-Löf type theory. The development of the univalent foundations is closely related with the development of homotopy type theory.\n\nUnivalent foundations are compatible with structuralism, if an appropriate (i.e., categorical) notion of mathematical structure is adopted.\n\nThe main ideas of the univalent foundations were formulated by Vladimir Voevodsky in 2006/2009. The sole reference for the philosophical connections between the univalent foundations and the earlier ideas are Voevodsky's 2014 Bernays lectures. The name \"univalence\" is due to Voevodsky. A more detailed discussion of the history of some of the ideas that contribute to the current state of the univalent foundations can be found at the page on homotopy type theory.\n\nA fundamental characteristic of the univalent foundations is that they, when combined with the Martin-Löf type theory, provide a practical system for formalization of modern mathematics. A considerable amount of mathematics has been formalized using this system and modern proof assistants such as Coq and Agda. The first such library called \"Foundations\" was created by Vladimir Voevodsky in 2010. Now Foundations is a part of a larger development with several authors called UniMath. Foundations also inspired other libraries of formalized mathematics, such as the HoTT Coq library and HoTT Agda library, that developed the univalent ideas in new directions.\n\nAn important milestone for the univalent foundations was the Bourbaki Seminar talk by Thierry Coquand in June 2014.\n\nUnivalent foundations originated from the attempts to create foundations of mathematics based on higher category theory. The closest to the univalent foundations were the ideas that Michael Makkai expressed in his visionary paper known as FOLDS. The main distinction between the univalent foundations and the foundations envisioned by Makkai is the recognition that the \"higher dimensional analogs of sets\" correspond to infinity groupoids and that categories should be considered as higher-dimensional analogs of partially ordered sets.\n\nOriginally, the univalent foundations were devised by Vladimir Voevodsky with the goal of enabling those who work in classical pure mathematics to use computers to verify their theorems and constructions. The fact that the new foundations are inherently constructive was discovered in the process of writing the Foundations library (now part of the UniMath). At present, in the univalent foundations, classical mathematics is considered to be a \"retract\" of the constructive mathematics, i.e., classical mathematics is both a subset of the constructive mathematics that consists of theorems and constructions that use the excluded middle as their assumption and a \"quotient\" of the constructive mathematics by the relation of being equivalent modulo the axiom of the excluded middle.\n\nIn the formalization system for the univalent foundations that is based on the Martin-Löf type theory and its descendants such as Calculus of Inductive Constructions, the higher dimensional analogs of sets are represented by types. The world of types stratified by the concept of \"h-level\" (or \"homotopy level\").\n\nTypes of h-level 0 are those equal to the one point type. They are also called contractible types.\n\nTypes of h-level 1 are those in which any two elements are equal. Such types are called in the univalent foundations \"propositions\". The definition of propositions in terms of the h-level agrees with the definition suggested earlier by Awodey and Bauer. So while all propositions are types not all types are propositions. Being a proposition is a property of a type that requires a proof. For example, the first fundamental construction of the univalent foundations is called iscontr. It is a function from types to types. If X is a type then iscontr X is a type that has an object if and only if X is contractible. It is a theorem (which is called, in the UniMath library, isapropiscontr) that for any X the type iscontr X has h-level 1 and therefore being a contractible type is a property. This distinction between properties that are witnessed by objects of types of h-level 1 and structures that are witnessed by objects of types of higher h-levels is very important in the univalent foundations.\n\nTypes of h-level 2 are called sets. It is a theorem that the type of natural numbers has h-level 2 (isasetnat in UniMath). It is claimed by the creators of the univalent foundations that the univalent formalization of sets in the Martin-Löf type theory is the best available today environment for formal reasoning about all aspects of set-theoretical mathematics, both constructive and classical.\n\nCategories are defined (see the RezkCompletion library in UniMath) as types of h-level 3 with an additional structure that is very similar to the structure on types of h-level 2 that defines partially ordered sets. The theory of categories in the univalent foundations is somewhat different and richer than the theory of categories in set-theoretic world with the key new distinction being between pre-categories and categories.\n\nAn account of the main ideas of the univalent foundations and their connection to constructive mathematics can be found in a tutorial by Thierry Coquand (part 1, part 2). A presentation of the main ideas from the perspective of classical mathematics can be found in the review article by Alvaro Pelayo and Michael Warren, as well as in the introduction by Daniel Grayson. See also the article about the Foundations library.\n\nAn account of Voevodsky's construction of a univalent model of the Martin-Löf type theory with values in Kan simplicial sets can be found in a paper by Chris Kapulkin, Peter LeFanu Lumsdaine and Vladimir Voevodsky. Univalent models with values in the categories of inverse diagrams of simplicial sets were constructed by Michael Shulman. These models have shown that the univalence axiom is independent from the excluded middle axiom for propositions.\n\nVoevodsky's model is non-constructive since it uses in a substantial way the axiom of choice. \n\nThe problem of finding a constructive way of interpreting the rules of the Martin-Löf type theory that in addition satisfies the univalence axiom and canonicity for natural numbers remains open. A partial solution is outlined in a paper by Marc Bezem, Thierry Coquand and Simon Huber with the key remaining issue being the computational property of the eliminator for the identity types. The ideas of this paper are now being developed in several directions including the development of the cubical type theory.\n\nMost of the work on formalization of mathematics in the framework of univalent foundations is being done using various sub-systems and extensions of the Calculus of Inductive Constructions. \n\nThere are three standard problems whose solution, despite many attempts, could not be constructed using CIC:\n\n\nThese unsolved problems indicate that while CIC is a good system for the initial phase of the development of the univalent foundations, moving towards the use of computer proof assistants in the work on its more sophisticated aspects will require the development of a new generation of formal deduction and computation systems.\n\n\n\n"}
{"id": "25081608", "url": "https://en.wikipedia.org/wiki?curid=25081608", "title": "Victor Bangert", "text": "Victor Bangert\n\nVictor Bangert (born 28 November 1950, Osnabrück) is Professor of Mathematics at the Mathematisches Institut in Freiburg, Germany. His main interests are differential geometry and dynamical systems theory. He is a leading expert in the theory of closed geodesics, where one of his most celebrated result, combined with another one due to John Franks, implies that every Riemannian 2-sphere possesses infinitely many closed geodesics. He also made important contributions to Aubry–Mather theory.\n\nHe obtained his Ph.D. from Universität Dortmund in 1977 under the supervision of Rolf Wilhelm Walter, with the thesis \"Konvexität in riemannschen Mannigfaltigkeiten\".\n\nHe served in the editorial board of manuscripta mathematica from 1996 to 2017.\n\nBangert was an invited speaker at the 1994 International Congress of Mathematicians in Zürich.\n\n\n\n"}
