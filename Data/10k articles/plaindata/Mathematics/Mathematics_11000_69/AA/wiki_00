{"id": "653589", "url": "https://en.wikipedia.org/wiki?curid=653589", "title": "147 (number)", "text": "147 (number)\n\n147 (one hundred [and] forty-seven) is the natural number following 146 and preceding 148.\n\nThe digits forming 147 also form the left-hand column of a normal decimal numeric keypad.\nThe binary form of 147 contains all the two-digit binary numbers (00, 01, 10 and 11).\n"}
{"id": "186050", "url": "https://en.wikipedia.org/wiki?curid=186050", "title": "Alternativity", "text": "Alternativity\n\nIn abstract algebra, alternativity is a property of a binary operation. A magma \"G\" is said to be left alternative if (\"xx\")\"y\" = \"x\"(\"xy\") for all \"x\" and \"y\" in \"G\" and right alternative if \"y\"(\"xx\") = (\"yx\")\"x\" for all \"x\" and \"y\" in \"G\". A magma that is both left and right alternative is said to be alternative.\n\nAny associative magma (i.e., a semigroup) is alternative. More generally, a magma in which every pair of elements generates an associative submagma must be alternative. The converse, however, is not true, in contrast to the situation in alternative algebras. In fact, an alternative magma need not even be power-associative.\n"}
{"id": "16796426", "url": "https://en.wikipedia.org/wiki?curid=16796426", "title": "Area theorem (conformal mapping)", "text": "Area theorem (conformal mapping)\n\nIn the mathematical theory of conformal mappings, the area theorem\ngives an inequality satisfied by\nthe power series coefficients of certain conformal mappings.\nThe theorem is called by that name, not because of its implications, but rather because the proof uses\nthe notion of area.\n\nSuppose that formula_1 is analytic and injective in the punctured\nopen unit disk\nformula_2 and has the power series representation\nthen the coefficients formula_4 satisfy\n\nThe idea of the proof is to look at the area uncovered by the image of formula_1.\nDefine for formula_7\nThen formula_9 is a simple closed curve in the plane.\nLet formula_10 denote the unique bounded connected component of\nformula_11. The existence and\nuniqueness of formula_10 follows from Jordan's curve theorem.\n\nIf formula_13 is a domain in the plane whose boundary\nis a smooth simple closed curve formula_14,\nthen\nprovided that formula_14 is positively oriented\naround formula_13.\nThis follows easily, for example, from Green's theorem.\nAs we will soon see, formula_9 is positively oriented around\nformula_10 (and that is the reason for the minus sign in the\ndefinition of formula_9). After applying the chain rule\nand the formula for formula_9, the above expressions for\nthe area give\nTherefore, the area of formula_10 also equals to the average of the two expressions on the right\nhand side. After simplification, this yields\nwhere formula_25 denotes complex conjugation. We set formula_26 and use the power series\nexpansion for formula_1, to get\nNow note that formula_30 is formula_31 if formula_32\nand is zero otherwise. Therefore, we get\nThe area of formula_10 is clearly positive. Therefore, the right hand side\nis positive. Since formula_26, by letting formula_36, the\ntheorem now follows.\n\nIt only remains to justify the claim that formula_9 is positively oriented\naround formula_10. Let formula_39 satisfy formula_40, and set\nformula_41, say. For very small formula_42, we may write the\nexpression for the winding number of formula_43 around formula_44,\nand verify that it is equal to formula_45. Since, formula_46 does\nnot pass through formula_44 when formula_48\n(as formula_1 is injective), the invariance\nof the winding number under homotopy in the complement of formula_44\nimplies that the winding number of\nformula_9 around formula_44 is also formula_45.\nThis implies that formula_54 and that formula_9\nis positively oriented around formula_10, as required.\n\nThe inequalities satisfied by power series coefficients of conformal\nmappings were of considerable interest to mathematicians prior to\nthe solution of the Bieberbach conjecture. The area theorem\nis a central tool in this context. Moreover, the area theorem is often\nused in order to prove the Koebe 1/4 theorem, which is very\nuseful in the study of the geometry of conformal mappings.\n"}
{"id": "50909", "url": "https://en.wikipedia.org/wiki?curid=50909", "title": "Basis function", "text": "Basis function\n\nIn mathematics, a basis function is an element of a particular basis for a function space. Every continuous function in the function space can be represented as a linear combination of basis functions, just as every vector in a vector space can be represented as a linear combination of basis vectors.\n\nIn numerical analysis and approximation theory, basis functions are also called blending functions, because of their use in interpolation: In this application, a mixture of the basis functions provides an interpolating function (with the \"blend\" depending on the evaluation of the basis functions at the data points).\n\nThe base of a polynomial is the factored polynomial equation into a linear function.\n\nSines and cosines form an (orthonormal) Schauder basis for square-integrable functions. As a particular example, the collection:\nforms a basis for L(0,1).\n\n\n\n"}
{"id": "32848664", "url": "https://en.wikipedia.org/wiki?curid=32848664", "title": "Big q-Laguerre polynomials", "text": "Big q-Laguerre polynomials\n\nIn mathematics, the big \"q\"-Laguerre polynomials are a family of basic hypergeometric orthogonal polynomials in the basic Askey scheme. give a detailed list of their properties.\n\nThe polynomials are given in terms of basic hypergeometric functions and the Pochhammer symbol by \n\nformula_1\n\nBig q-Laguerre polynomials→Laguerre polynomials\n\n"}
{"id": "194129", "url": "https://en.wikipedia.org/wiki?curid=194129", "title": "Brahmagupta's formula", "text": "Brahmagupta's formula\n\nIn Euclidean geometry, Brahmagupta's formula is used to find the area of any cyclic quadrilateral (one that can be inscribed in a circle) given the lengths of the sides.\n\nBrahmagupta's formula gives the area of a cyclic quadrilateral whose sides have lengths , , , as\n\nwhere , the semiperimeter, is defined to be\n\nThis formula generalizes Heron's formula for the area of a triangle. A triangle may be regarded as a quadrilateral with one side of length zero. From this perspective, as approaches zero, a cyclic quadrilateral converges into a cyclic triangle (all triangles are cyclic), and Brahmagupta's formula simplifies to Heron's formula.\n\nIf the semiperimeter is not used, Brahmagupta's formula is\n\nAnother equivalent version is\n\nHere the notations in the figure to the right are used. The area of the cyclic quadrilateral equals the sum of the areas of and :\n\nBut since is a cyclic quadrilateral, . Hence . Therefore,\n\nSolving for common side , in and , the law of cosines gives\n\nSubstituting (since angles and are supplementary) and rearranging, we have\n\nSubstituting this in the equation for the area,\n\nThe right-hand side is of the form and hence can be written as\n\nwhich, upon rearranging the terms in the square brackets, yields\n\nIntroducing the semiperimeter ,\n\nTaking the square root, we get\n\nAn alternative, non-trigonometric proof utilizes two applications of Heron's triangle area formula on similar triangles.\n\nIn the case of non-cyclic quadrilaterals, Brahmagupta's formula can be extended by considering the measures of two opposite angles of the quadrilateral:\n\nwhere is half the sum of any two opposite angles. (The choice of which pair of opposite angles is irrelevant: if the other two angles are taken, half their sum is . Since , we have .) This more general formula is known as Bretschneider's formula.\n\nIt is a property of cyclic quadrilaterals (and ultimately of inscribed angles) that opposite angles of a quadrilateral sum to 180°. Consequently, in the case of an inscribed quadrilateral, is 90°, whence the term\n\ngiving the basic form of Brahmagupta's formula. It follows from the latter equation that the area of a cyclic quadrilateral is the maximum possible area for any quadrilateral with the given side lengths.\n\nA related formula, which was proved by Coolidge, also gives the area of a general convex quadrilateral. It is\n\nwhere and are the lengths of the diagonals of the quadrilateral. In a cyclic quadrilateral, according to Ptolemy's theorem, and the formula of Coolidge reduces to Brahmagupta's formula.\n\n"}
{"id": "56454187", "url": "https://en.wikipedia.org/wiki?curid=56454187", "title": "Bundle of principal parts", "text": "Bundle of principal parts\n\nIn algebraic geometry, given a line bundle \"L\" on a smooth variety \"X\", the bundle of \"n\"-th order principal parts of \"L\" is a vector bundle of rank \"n\" + 1 that, roughly, parametrizes \"n\"-th order Taylor expansions of sections of \"L\".\n\nPrecisely, let \"I\" be the ideal sheaf defining the diagonal embedding formula_1 and formula_2 the restrictions of projections formula_3 to formula_4. Then the bundle of \"n\"-th order principal parts is\nThen formula_6 and there is a natural exact sequence of vector bundles\nwhere formula_8 is the sheaf of differential one-forms on \"X\".\n\n\n"}
{"id": "1169985", "url": "https://en.wikipedia.org/wiki?curid=1169985", "title": "Causal Markov condition", "text": "Causal Markov condition\n\nThe Markov condition, sometimes called the Markov assumption, for a Bayesian network states that every node in a Bayesian network is conditionally independent of its nondescendents, given its parents.\n\nA node is conditionally independent of the entire network, given its Markov blanket.\n\nThe related causal Markov condition is that a is independent of its noneffects, given its direct causes. In the event that the structure of a Bayesian network accurately depicts causality, the two conditions are equivalent. However, a network may accurately embody the Markov condition without depicting causality, in which case it should not be assumed to embody the causal Markov condition.\n"}
{"id": "4009827", "url": "https://en.wikipedia.org/wiki?curid=4009827", "title": "Cheeger bound", "text": "Cheeger bound\n\nIn mathematics, the Cheeger bound is a bound of the second largest eigenvalue of the transition matrix of a finite-state, discrete-time, reversible stationary Markov chain. It can be seen as a special case of Cheeger inequalities in expander graphs.\n\nLet formula_1 be a finite set and let formula_2 be the transition probability for a reversible Markov chain on formula_1. Assume this chain has stationary distribution formula_4.\n\nDefine\n\nand for formula_6 define\n\nDefine the constant formula_8 as\n\nThe operator formula_10 acting on the space of functions from formula_11 to formula_11, defined by\n\nhas eigenvalues formula_14. It is known that formula_15. The Cheeger bound is a bound on the second largest eigenvalue formula_16.\n\n\n"}
{"id": "6026", "url": "https://en.wikipedia.org/wiki?curid=6026", "title": "Countable set", "text": "Countable set\n\nIn mathematics, a countable set is a set with the same cardinality (number of elements) as some subset of the set of natural numbers. A countable set is either a finite set or a countably infinite set. Whether finite or infinite, the elements of a countable set can always be counted one at a time and, although the counting may never finish, every element of the set is associated with a unique natural number.\n\nSome authors use countable set to mean \"countably infinite\" alone. To avoid this ambiguity, the term \"at most countable\" may be used when finite sets are included and \"countably infinite\", \"enumerable\", or \"denumerable\" otherwise.\n\nGeorg Cantor introduced the term \"countable set\", contrasting sets that are countable with those that are \"uncountable\" (i.e., \"nonenumerable\" or \"nondenumerable\"). Today, countable sets form the foundation of a branch of mathematics called \"discrete mathematics\".\n\nA set is \"countable\" if there exists an injective function from to the natural numbers }.\n\nIf such an can be found that is also surjective (and therefore bijective), then is called \"countably infinite.\"\n\nIn other words, a set is \"countably infinite\" if it has one-to-one correspondence with the natural number set, .\n\nAs noted above, this terminology is not universal. Some authors use countable to mean what is here called \"countably infinite,\" and do not include finite sets.\n\nAlternative (equivalent) formulations of the definition in terms of a bijective function or a surjective function can also be given. See below.\n\nIn 1874, in his first set theory article, Cantor proved that the set of real numbers is uncountable, thus showing that not all infinite sets are countable. In 1878, he used one-to-one correspondences to define and compare cardinalities. In 1883, he extended the natural numbers with his infinite ordinals, and used sets of ordinals to produce an infinity of sets having different infinite cardinalities.\n\nA \"set\" is a collection of \"elements\", and may be described in many ways. One way is simply to list all of its elements; for example, the set consisting of the integers 3, 4, and 5 may be denoted {3, 4, 5}. This is only effective for small sets, however; for larger sets, this would be time-consuming and error-prone. Instead of listing every single element, sometimes an ellipsis (\"...\") is used, if the writer believes that the reader can easily guess what is missing; for example, {1, 2, 3, ..., 100} presumably denotes the set of integers from 1 to 100. Even in this case, however, it is still \"possible\" to list all the elements, because the set is \"finite\".\n\nSome sets are \"infinite\"; these sets have more than \"n\" elements for any integer \"n\". For example, the set of natural numbers, denotable by {0, 1, 2, 3, 4, 5, ...}, has infinitely many elements, and we cannot use any normal number to give its size. Nonetheless, it turns out that infinite sets do have a well-defined notion of size (or more properly, of \"cardinality\", which is the technical term for the number of elements in a set), and not all infinite sets have the same cardinality.\nTo understand what this means, we first examine what it \"does not\" mean. For example, there are infinitely many odd integers, infinitely many even integers, and (hence) infinitely many integers overall. However, it turns out that the number of even integers, which is the same as the number of odd integers, is also the same as the number of integers overall. This is because we arrange things such that for every integer, there is a distinct even integer: ... −2→−4, −1→−2, 0→0, 1→2, 2→4, ...; or, more generally, \"n\"→2\"n\", see picture. What we have done here is arranged the integers and the even integers into a \"one-to-one correspondence\" (or \"bijection\"), which is a function that maps between two sets such that each element of each set corresponds to a single element in the other set.\n\nHowever, not all infinite sets have the same cardinality. For example, Georg Cantor (who introduced this concept) demonstrated that the real numbers cannot be put into one-to-one correspondence with the natural numbers (non-negative integers), and therefore that the set of real numbers has a greater cardinality than the set of natural numbers.\n\nA set is \"countable\" if: (1) it is finite, or (2) it has the same cardinality (size) as the set of natural numbers. Equivalently, a set is \"countable\" if it has the same cardinality as some subset of the set of natural numbers. Otherwise, it is \"uncountable\".\n\nBy definition a set \"S\" is \"countable\" if there exists an injective function \"f\" : \"S\" → N from \"S\" to the natural numbers N = {0, 1, 2, 3, ...}.\n\nIt might seem natural to divide the sets into different classes: put all the sets containing one element together; all the sets containing two elements together; ...; finally, put together all infinite sets and consider them as having the same size.\nThis view is not tenable, however, under the natural definition of size.\n\nTo elaborate this we need the concept of a bijection. Although a \"bijection\" seems a more advanced concept than a number, the usual development of mathematics in terms of set theory defines functions before numbers, as they are based on much simpler sets. This is where the concept of a bijection comes in: define the correspondence\n\nSince every element of {\"a\", \"b\", \"c\"} is paired with \"precisely one\" element of {1, 2, 3}, \"and\" vice versa, this defines a bijection.\n\nWe now generalize this situation and \"define\" two sets as of the same size if (and only if) there is a bijection between them. For all finite sets this gives us the usual definition of \"the same size\". What does it tell us about the size of infinite sets?\n\nConsider the sets \"A\" = {1, 2, 3, ... }, the set of positive integers and \"B\" = {2, 4, 6, ... }, the set of even positive integers. We claim that, under our definition, these sets have the same size, and that therefore \"B\" is countably infinite. Recall that to prove this we need to exhibit a bijection between them. But this is easy, using \"n\" ↔ 2\"n\", so that\n\nAs in the earlier example, every element of A has been paired off with precisely one element of B, and vice versa. Hence they have the same size. This is an example of a set of the same size as one of its proper subsets, which is impossible for finite sets.\n\nLikewise, the set of all ordered pairs of natural numbers is countably infinite, as can be seen by following a path like the one in the picture: The resulting mapping is like this:\nThis mapping covers all such ordered pairs.\n\nIf you treat each pair as being the numerator and denominator of a vulgar fraction, then for every positive fraction, we can come up with a distinct number corresponding to it. This representation includes also the natural numbers, since every natural number is also a fraction \"N\"/1. So we can conclude that there are exactly as many positive rational numbers as there are positive integers. This is true also for all rational numbers, as can be seen below.\n\nTheorem: The Cartesian product of finitely many countable sets is countable.\n\nThis form of triangular mapping recursively generalizes to vectors of finitely many natural numbers by repeatedly mapping the first two elements to a natural number. For example, (0,2,3) maps to (5,3), which maps to 39.\n\nSometimes more than one mapping is useful. This is where you map the set you want to show is countably infinite onto another set—and then map this other set to the natural numbers. For example, the positive rational numbers can easily be mapped to (a subset of) the pairs of natural numbers because \"p\"/\"q \"maps to (\"p\", \"q\").\n\nWhat about infinite subsets of countably infinite sets? Do these have fewer elements than N?\n\nTheorem: Every subset of a countable set is countable. In particular, every infinite subset of a countably infinite set is countably infinite.\n\nFor example, the set of prime numbers is countable, by mapping the \"n\"-th prime number to \"n\":\n\nWhat about sets being naturally \"larger than\" N? For instance, Z the set of all integers or Q, the set of all rational numbers, which intuitively may seem much bigger than N. But looks can be deceiving, for we assert:\n\nTheorem: Z (the set of all integers) and Q (the set of all rational numbers) are countable.\n\nIn a similar manner, the set of algebraic numbers is countable.\n\nThese facts follow easily from a result that many individuals find non-intuitive.\n\nTheorem: Any finite union of countable sets is countable. \nWith the foresight of knowing that there are uncountable sets, we can wonder whether or not this last result can be pushed any further. The answer is \"yes\" and \"no\", we can extend it, but we need to assume a new axiom to do so.\n\nTheorem: (Assuming the axiom of countable choice) The union of countably many countable sets is countable.\n\nFor example, given countable sets a, b, c, ...\nUsing a variant of the triangular enumeration we saw above:\n\n\nNote that this only works if the sets a, b, c, ... are disjoint. If not, then the union is even smaller and is therefore also countable by a previous theorem.\n\nAlso note that we need the axiom of countable choice to index \"all\" the sets a, b, c, ... simultaneously.\n\nTheorem: The set of all finite-length sequences of natural numbers is countable.\n\nThis set is the union of the length-1 sequences, the length-2 sequences, the length-3 sequences, each of which is a countable set (finite Cartesian product). So we are talking about a countable union of countable sets, which is countable by the previous theorem.\n\nTheorem: The set of all finite subsets of the natural numbers is countable.\n\nIf you have a finite subset, you can order the elements into a finite sequence. There are only countably many finite sequences, so also there are only countably many finite subsets.\n\nThe following theorem gives equivalent formulations in terms of a bijective function or a surjective function. A proof of this result can be found in Lang's text.\n\n(Basic) Theorem: Let \"S\" be a set. The following statements are equivalent:\n\nCorollary: Let \"S\" and \"T\" be sets.\n\nCantor's theorem asserts that if \"A\" is a set and \"P\"(\"A\") is its power set, i.e. the set of all subsets of \"A\", then there is no surjective function from \"A\" to \"P\"(\"A\"). A proof is given in the article Cantor's theorem. As an immediate consequence of this and the Basic Theorem above we have:\n\nProposition: The set \"P\"(N) is not countable; i.e. it is uncountable.\n\nFor an elaboration of this result see Cantor's diagonal argument.\n\nThe set of real numbers is uncountable (see Cantor's first uncountability proof), and so is the set of all infinite sequences of natural numbers.\nThe proofs of the statements in the above section rely upon the existence of functions with certain properties. This section presents functions commonly used in this role, but not the verifications that these functions have the required properties. The Basic Theorem and its Corollary are often used to simplify proofs. Observe that in that theorem can be replaced with any countably infinite set.\n\nProposition: Any finite set is countable.\n\nProof: Let be such a set. Two cases are to be considered: Either is empty or it isn't. 1.) The empty set is even itself a subset of the natural numbers, so it is countable. 2.) If is nonempty and finite, then by definition of finiteness there is a bijection between and the set {1, 2, ..., } for some positive natural number . This function is an injection from into .\n\nProposition: Any subset of a countable set is countable.\n\nProof: The restriction of an injective function to a subset of its domain is still injective.\n\nProposition: If is a countable set then } is countable.\n\nProof: If there is nothing to be shown. Otherwise let be an injection. Define by and \n\nProposition: If and are countable sets then is countable.\n\nProof: Let and be injections. Define a new injection by if is in and if is in but not in .\n\nProposition: The Cartesian product of two countable sets and is countable.\n\nProof: Observe that is countable as a consequence of the definition because the function given by is injective. It then follows from the Basic Theorem and the Corollary that the Cartesian product of any two countable sets is countable. This follows because if and are countable there are surjections and . So\nis a surjection from the countable set to the set and the Corollary implies is countable. This result generalizes to the Cartesian product of any finite collection of countable sets and the proof follows by induction on the number of sets in the collection.\n\nProposition: The integers are countable and the rational numbers are countable.\n\nProof: The integers are countable because the function given by if is non-negative and if is negative, is an injective function. The rational numbers are countable because the function given by is a surjection from the countable set to the rationals .\n\nProposition: The algebraic numbers are countable.\n\nProof: Per definition, every algebraic number (including complex numbers) is a root of a polynomial with integer coefficients. Given an algebraic number formula_1, let formula_2 be a polynomial with integer coefficients such that formula_1 is the \"k\"th root of the polynomial, where the roots are sorted by absolute value from small to big, then sorted by argument from small to big. We can define an injection (i. e. one-to-one) function given by formula_4, while formula_5 is the \"n\"-th prime.\n\nProposition: If is a countable set for each in then the union of all is also countable.\n\nProof: This is a consequence of the fact that for each there is a surjective function and hence the function\n\ngiven by is a surjection. Since is countable, the Corollary implies that the union is countable. We use the axiom of countable choice in this proof to pick for each in a surjection from the non-empty collection of surjections from to .\n\nA topological proof for the uncountability of the real numbers is described at finite intersection property.\n\nIf there is a set that is a standard model (see inner model) of ZFC set theory, then there is a minimal standard model (\"see\" Constructible universe). The Löwenheim–Skolem theorem can be used to show that this minimal model is countable. The fact that the notion of \"uncountability\" makes sense even in this model, and in particular that this model \"M\" contains elements that are:\nwas seen as paradoxical in the early days of set theory, see Skolem's paradox.\n\nThe minimal standard model includes all the algebraic numbers and all effectively computable transcendental numbers, as well as many other kinds of numbers.\n\nCountable sets can be totally ordered in various ways, e.g.:\n\nNote that in both examples of well orders here, any subset has a \"least element\"; and in both examples of non-well orders, \"some\" subsets do not have a \"least element\".\nThis is the key definition that determines whether a total order is also a well order.\n\n\n"}
{"id": "7203729", "url": "https://en.wikipedia.org/wiki?curid=7203729", "title": "D'Alembert's equation", "text": "D'Alembert's equation\n\nIn mathematics, d'Alembert's equation is a first order nonlinear ordinary differential equation, named after the French mathematician Jean le Rond d'Alembert. The equation reads as\n\nwhere formula_2. After differentiating once, and rearranging we have\n\nThe above equation is linear.\n"}
{"id": "1775329", "url": "https://en.wikipedia.org/wiki?curid=1775329", "title": "Elementary mathematics", "text": "Elementary mathematics\n\nElementary mathematics consists of mathematics topics frequently taught at the primary or secondary school levels.\n\nThere are five basic strands in Elementary Mathematics: Number Sense and Numeration, Measurement, Geometry & Spatial Sense, Patterning & Algebra, and Data Management & Probability. These five strands are the focus of Mathematics education from grade one till grade 8.\n\nIn secondary school, the main topics in elementary mathematics from grade nine until grade ten are: Number Sense and algebra, Linear Relations, Measurement and Geometry. Once students enter grade eleven and twelve students begin university and college preparation classes, which include: Functions, Calculus & Vectors, Advanced Functions, and Data Management.\n\nNumber Sense is an understanding of numbers and operations. In the number sense and numeration strand students develop an understanding of numbers by being taught various ways of representing numbers, as well as the relationships among numbers.\n\nProperties of the natural numbers such as divisibility and the distribution of prime numbers, are studied in basic number theory, another part of elementary mathematics.\n\nElementary Focus\nTo have a strong foundation in mathematics and to be able to succeed in the other strands students need to have a fundamental understanding of number sense and numeration.\n\nMeasurement skills and concepts are directly related to the world in which students live. Many of the concepts that students are taught in this strands are also used in other subjects such as science, social studies, and physical education In the measurement strand students learn about the measurable attributes of objects, in addition to the basic metric system.\n\nElementary Focus \n\nThe measurement strand consists of multiple forms of measurement as Marian Small states \" Measurement is the process of assigning a qualitative or quantitative description of size to an object based on a particular attribute.\"\n\nA formula is an entity constructed using the symbols and formation rules of a given logical language. For example, determining the volume of a sphere requires a significant amount of integral calculus or its geometrical analogue, the method of exhaustion; but, having done this once in terms of some parameter (the radius for example), mathematicians have produced a formula to describe the volume: This particular formula is:\n\nAn equation is a formula of the form \"A\" = \"B\", where \"A\" and \"B\" are expressions that may contain one or several variables called unknowns, and \"=\" denotes the equality binary relation. Although written in the form of proposition, an equation is not a statement that is either true or false, but a problem consisting of finding the values, called solutions, that, when substituted for the unknowns, yield equal values of the expressions \"A\" and \"B\". For example, 2 is the unique \"solution\" of the \"equation\" \"x\" + 2 = 4, in which the \"unknown\" is \"x\".\n\nData is a set of values of qualitative or quantitative variables; restated, pieces of data are individual pieces of information. Data in computing (or data processing) is represented in a structure that is often tabular (represented by rows and columns), a tree (a set of nodes with parent-children relationship), or a graph (a set of connected nodes). Data is typically the result of measurements and can be visualized using graphs or images.\n\nData as an abstract concept can be viewed as the lowest level of abstraction, from which information and then knowledge are derived.\n\nTwo-dimensional geometry is a branch of mathematics concerned with questions of shape, size, and relative position of two-dimensional figures. Basic topics in elementary mathematics include polygons, circles, perimeter and area.\n\nA polygon that is bounded by a finite chain of straight line segments closing in a loop to form a closed chain or \"circuit\". These segments are called its \"edges\" or \"sides\", and the points where two edges meet are the polygon's \"vertices\" (singular: vertex) or \"corners\". The interior of the polygon is sometimes called its \"body\". An \"n\"-gon is a polygon with \"n\" sides. A polygon is a 2-dimensional example of the more general polytope in any number of dimensions.\n\nA circle is a simple shape of two-dimensional geometry that is the set of all points in a plane that are at a given distance from a given point, the center.The distance between any of the points and the center is called the radius. It can also be defined as the locus of a point equidistant from a fixed point.\n\nA perimeter is a path that surrounds a two-dimensional shape. The term may be used either for the path or its length - it can be thought of as the length of the outline of a shape. The perimeter of a circle or ellipse is called its circumference.\n\nArea is the quantity that expresses the extent of a two-dimensional figure or shape. There are several well-known formulas for the areas of simple shapes such as triangles, rectangles, and circles.\n\nTwo quantities are proportional if a change in one is always accompanied by a change in the other, and if the changes are always related by use of a constant multiplier. The constant is called the coefficient of proportionality or proportionality constant.\n\n\nAnalytic geometry is the study of geometry using a coordinate system. This contrasts with synthetic geometry.\n\nUsually the Cartesian coordinate system is applied to manipulate equations for planes, straight lines, and squares, often in two and sometimes in three dimensions. Geometrically, one studies the Euclidean plane (2 dimensions) and Euclidean space (3 dimensions). As taught in school books, analytic geometry can be explained more simply: it is concerned with defining and representing geometrical shapes in a numerical way and extracting numerical information from shapes' numerical definitions and representations.\n\nTransformations are ways of shifting and scaling functions using different algebraic formulas.\n\nA negative number is a real number that is less than zero. Such numbers are often used to represent the amount of a loss or absence. For example, a debt that is owed may be thought of as a negative asset, or a decrease in some quantity may be thought of as a negative increase. Negative numbers are used to describe values on a scale that goes below zero, such as the Celsius and Fahrenheit scales for temperature.\n\nExponentiation is a mathematical operation, written as \"b\", involving two numbers, the base \"b\" and the exponent (or power) \"n\". When \"n\" is a natural number (i.e., a positive integer), exponentiation corresponds to repeated multiplication of the base: that is, \"b\" is the product of multiplying \"n\" bases:\n\nRoots are the opposite of exponents. The nth root of a number \"x\" (written formula_4) is a number \"r\" which when raised to the power \"n\" yields \"x\". That is,\nwhere \"n\" is the \"degree\" of the root. A root of degree 2 is called a \"square root\" and a root of degree 3, a \"cube root\". Roots of higher degree are referred to by using ordinal numbers, as in \"fourth root\", \"twentieth root\", etc.\n\nFor example:\n\nCompass-and-straightedge, also known as ruler-and-compass construction, is the construction of lengths, angles, and other geometric figures using only an idealized ruler and compass.\n\nThe idealized ruler, known as a straightedge, is assumed to be infinite in length, and has no markings on it and only one edge. The compass is assumed to collapse when lifted from the page, so may not be directly used to transfer distances. (This is an unimportant restriction since, using a multi-step procedure, a distance can be transferred even with collapsing compass, see compass equivalence theorem.) More formally, the only permissible constructions are those granted by Euclid's first three postulates.\n\nTwo figures or objects are congruent if they have the same shape and size, or if one has the same shape and size as the mirror image of the other. More formally, two sets of points are called congruent if, and only if, one can be transformed into the other by an isometry, i.e., a combination of rigid motions, namely a translation, a rotation, and a reflection. This means that either object can be repositioned and reflected (but not resized) so as to coincide precisely with the other object. So two distinct plane figures on a piece of paper are congruent if we can cut them out and then match them up completely. Turning the paper over is permitted.\n\nTwo geometrical objects are called similar if they both have the same shape, or one has the same shape as the mirror image of the other. More precisely, one can be obtained from the other by uniformly scaling (enlarging or shrinking), possibly with additional translation, rotation and reflection. This means that either object can be rescaled, repositioned, and reflected, so as to coincide precisely with the other object. If two objects are similar, each is congruent to the result of a uniform scaling of the other.\n\nSolid geometry was the traditional name for the geometry of three-dimensional Euclidean space. Stereometry deals with the measurements of volumes of various solid figures (three-dimensional figures) including pyramids, cylinders, cones, truncated cones, spheres, and prisms.\n\nRational number is any number that can be expressed as the quotient or fraction \"p\"/\"q\" of two integers, with the denominator \"q\" not equal to zero. Since \"q\" may be equal to 1, every integer is a rational number. The set of all rational numbers is usually denoted by a boldface Q (or blackboard bold formula_6).\n\nA pattern is a discernible regularity in the world or in a manmade design. As such, the elements of a pattern repeat in a predictable manner. A geometric pattern is a kind of pattern formed of geometric shapes and typically repeating like a wallpaper.\n\nA relation on a set \"A\" is a collection of ordered pairs of elements of \"A\". In other words, it is a subset of the Cartesian product \"A\" = . Common relations include divisibility between two numbers and inequalities.\n\nA function is a relation between a set of inputs and a set of permissible outputs with the property that each input is related to exactly one output. An example is the function that relates each real number \"x\" to its square \"x\". The output of a function \"f\" corresponding to an input \"x\" is denoted by \"f\"(\"x\") (read \"\"f\" of \"x\"\"). In this example, if the input is −3, then the output is 9, and we may write \"f\"(−3) = 9. The input variable(s) are sometimes referred to as the argument(s) of the function.\n\nThe slope of a line is a number that describes both the \"direction\" and the \"steepness\" of the line. Slope is often denoted by the letter \"m\".\n\nTrigonometry is a branch of mathematics that studies relationships involving lengths and angles of triangles. The field emerged during the 3rd century BC from applications of geometry to astronomical studies.\n\nIn the United States, there has been considerable concern about the low level of elementary mathematics skills on the part of many students, as compared to students in other developed countries. The No Child Left Behind program was one attempt to address this deficiency, requiring that all American students be tested in elementary mathematics.\n"}
{"id": "31553074", "url": "https://en.wikipedia.org/wiki?curid=31553074", "title": "Evolving networks", "text": "Evolving networks\n\nEvolving Networks are networks that change as a function of time. They are a natural extension of network science since almost all real world networks evolve over time, either by adding or removing nodes or links over time. Often all of these processes occur simultaneously, such as in social networks where people make and lose friends over time, thereby creating and destroying edges, and some people become part of new social networks or leave their networks, changing the nodes in the network. Evolving network concepts build on established network theory and are now being introduced into studying networks in many diverse fields.\n\nThe study of networks traces its foundations to the development of graph theory, which was first analyzed by Leonhard Euler in 1736 when he wrote the famous Seven Bridges of Königsberg paper. Probabilistic network theory then developed with the help of eight famous papers studying random graphs written by Paul Erdős and Alfréd Rényi. The Erdős-Rényi model (ER) supposes that a graph is composed of N labeled nodes where each pair of nodes is connected by a preset probability p.\nWhile the ER model's simplicity has helped it find many applications, it does not accurately describe many real world networks. The ER model fails to generate local clustering and triadic closures as often as they are found in real world networks. Therefore, the Watts and Strogatz model was proposed, whereby a network is constructed as a regular ring lattice, and then nodes are rewired according to some probability β. This produces a locally clustered network and dramatically reduces the average path length, creating networks which represent the small world phenomenon observed in many real world networks.\n\nDespite this achievement, both the ER and the Watts and Storgatz models fail to account for the formulation of hubs as observed in many real world networks. The degree distribution in the ER model follows a Poisson distribution, while the Watts and Strogatz model produces graphs that are homogeneous in degree. Many networks are instead scale free, meaning that their degree distribution follows a power law of the form:\n\nThis exponent turns out to be approximately 3 for many real world networks, however, it is not a universal constant and depends continuously on the network's parameters \n\nThe Barabási–Albert (BA) model was the first widely accepted model to produce scale-free networks. This was accomplished by incorporating preferential attachment and growth, where nodes are added to the network over time and are more likely to link to other nodes with high degree distributions. The BA model was first applied to degree distributions on the web, where both of these effects can be clearly seen. New web pages are added over time, and each new page is more likely to link to highly visible hubs like Google which have high degree distributions than to nodes with only a few links. Formally this preferential attachment is:\n\nThe BA model was the first model to derive the network topology from the way the network was constructed with nodes and links being added over time. However, the model makes only the simplest assumptions necessary for a scale-free network to emerge, namely that there is linear growth and linear preferential attachment. This minimal model does not capture variations in the shape of the degree distribution, variations in the degree exponent, or the size independent clustering coefficient. \nTherefore, the original model has since been modified to more fully capture the properties of evolving networks by introducing a few new properties.\n\nOne concern with the BA model is that the degree distributions of each nodes experience strong positive feedback whereby the earliest nodes with high degree distributions continue to dominate the network indefinitely. However, this can be alleviated by introducing a fitness for each node, which modifies the probability of new links being created with that node or even of links to that node being removed.\n\nIn order to preserve the preferential attachment from the BA model, this fitness is then multiplied by the preferential attachment based on degree distribution to give the true probability that a link is created which connects to node \"i\".\n\nWhere formula_4 is the fitness, which may also depend on time. A decay of fitness with respect to time may occur and can be formalized by:\n\nformula_5\n\nWhere formula_6 increases with formula_7\n\nFurther complications arise because nodes may be removed from the network with some probability. Additionally, existing links may be destroyed and new links between existing nodes may be created. The probability of these actions occurring may depend on time and may also be related to the node's fitness. Probabilities can be assigned to these events by studying the characteristics of the network in question in order to grow a model network with identical properties. This growth would take place with one of the following actions occurring at each time step:\n\nProb p: add an internal link.\n\nProb q: delete a link.\n\nProb r: delete a node.\n\nProb 1-p-q-r: add a node.\n\nIn addition to growing network models as described above, there may be times when other methods are more useful or convenient for characterizing certain properties of evolving networks.\n\nIn networked systems where competitive decision making takes place, game theory is often used to model system dynamics, and convergence towards equilibria can be considered as a driver of topological evolution. For example, Kasthurirathna and Piraveenan have shown that when individuals in a system display varying levels of rationality, improving the overall system rationality might be an evolutionary reason for the emergence of scale-free networks. They demonstrated this by applying evolutionary pressure on an initially random network which simulates a range of classic games, so that the network converges towards Nash equilibria while being allowed to re-wire. The networks become increasingly scale-free during this process.\n\nThe most common way to view evolving networks is by considering them as successive static networks. This could be conceptualized as the individual still images which compose a motion picture. Many simple parameters exist to describe a static network (number of nodes, edges, path length, connected components), or to describe specific nodes in the graph such as the number of links or the clustering coefficient. These properties can then individually be studied as a time series using signal processing notions. For example, we can track the number of links established to a server per minute by looking at the successive snapshots of the network and counting these links in each snapshot.\n\nUnfortunately, the analogy of snapshots to a motion picture also reveals the main difficulty with this approach: the time steps employed are very rarely suggested by the network and are instead arbitrary. Using extremely small time steps between each snapshot preserves resolution, but may actually obscure wider trends which only become visible over longer timescales. Conversely, using larger timescales loses the temporal order of events within each snapshot. Therefore, it may be difficult to find the appropriate timescale for dividing the evolution of a network into static snapshots.\n\nIt may be important to look at properties which cannot be directly observed by treating evolving networks as a sequence of snapshots, such as the duration of contacts between nodes Other similar properties can be defined and then it is possible to instead track these properties through the evolution of a network and visualize them directly.\n\nAnother issue with using successive snapshots is that only slight changes in network topology can have large effects on the outcome of algorithms designed to find communities. Therefore, it is necessary to use a non classical definition of communities which permits following the evolution of the community through a set of rules such as birth, death, merge, split, growth, and contraction.\n\nAlmost all real world networks are evolving networks since they are constructed over time. By varying the respective probabilities described above, it is possible to use the expanded BA model to construct a network with nearly identical properties as many observed networks. Moreover, the concept of scale free networks shows us that time evolution is a necessary part of understanding the network's properties, and that it is difficult to model an existing network as having been created instantaneously. Real evolving networks which are currently being studied include social networks, communications networks, the internet, the movie actor network, the world wide web, and transportation networks.\n\n"}
{"id": "31689303", "url": "https://en.wikipedia.org/wiki?curid=31689303", "title": "Find first set", "text": "Find first set\n\nIn software, find first set (ffs) or find first one is a bit operation that, given an unsigned machine word, identifies the least significant index or position of the bit set to one in the word. A nearly equivalent operation is count trailing zeros (ctz) or number of trailing zeros (ntz), which counts the number of zero bits following the least significant one bit. The complementary operation that finds the index or position of the most significant set bit is \"log base 2\", so called because it computes the binary logarithm formula_1. This is closely related to count leading zeros (clz) or number of leading zeros (nlz), which counts the number of zero bits preceding the most significant one bit. These four operations also have negated versions:\nThere are two common variants of find first set, the POSIX definition which starts indexing of bits at 1, herein labelled ffs, and the variant which starts indexing of bits at zero, which is equivalent to ctz and so will be called by that name.\n\nGiven the following 32-bit word:\nThe count trailing zeros operation would return 3, while the count leading zeros operation returns 16. The count leading zeros operation depends on the word size: if this 32-bit word were truncated to a 16-bit word, count leading zeros would return zero. The find first set operation would return 4, indicating the 4th position from the right. The log base 2 is 15.\n\nSimilarly, given the following 32-bit word, the bitwise negation of the above word:\nThe count trailing ones operation would return 3, the count leading ones operation would return 16, and the find first zero operation ffz would return 4.\n\nIf the word is zero (no bits set), count leading zeros and count trailing zeros both return the number of bits in the word, while ffs returns zero. Both log base 2 and zero-based implementations of find first set generally return an undefined result for the zero word.\n\nMany architectures include instructions to rapidly perform find first set and/or related operations, listed below. The most common operation is count leading zeros (clz), likely because all other operations can be implemented efficiently in terms of it (see Properties and relations).\n\nNotes: On some Alpha platforms CTLZ and CTTZ are emulated in software.\n\nA number of compiler and library vendors supply compiler intrinsics or library functions to perform find first set and/or related operations, which are frequently implemented in terms of the hardware instructions above:\nIf bits are labeled starting at 1 (which is the convention used in this article), then count trailing zeros and find first set operations are related by (except when the input is zero). If bits are labeled starting at 0, then count trailing zeros and find first set are exactly equivalent operations.\n\nGiven w bits per word, the log base 2 is easily computed from the clz and vice versa by .\n\nAs demonstrated in the example above, the find first zero, count leading ones, and count trailing ones operations can be implemented by negating the input and using find first set, count leading zeros, and count trailing zeros. The reverse is also true.\n\nOn platforms with an efficient log base 2 operation such as M68000, ctz can be computed by:\nwhere \"&\" denotes bitwise AND and \"−x\" denotes the negative of x treating x as a signed integer in two's complement arithmetic. The expression x & (−x) clears all but the least-significant 1 bit, so that the most- and least-significant 1 bit are the same.\n\nOn platforms with an efficient count leading zeros operation such as ARM and PowerPC, ffs can be computed by:\n\nConversely, clz can be computed using ctz by first rounding up to the nearest power of two using shifts and bitwise ORs, as in this 32-bit example (note that this example depends on ctz returning 32 for the zero input):\n\nOn platforms with an efficient Hamming weight (population count) operation such as SPARC's codice_1 or Blackfin's codice_2, ctz can be computed using the identity: \nffs can be computed using:\nwhere \"^\" denotes bitwise xor, and clz can be computed by:\n\nThe inverse problem (given i, produce an x such that ctz(x)=i) can be computed with a left-shift (1 « i).\n\nFind first set and related operations can be extended to arbitrarily large bit arrays in a straightforward manner by starting at one end and proceeding until a word that is not all-zero (for ffs/ctz/clz) or not all-one (for ffz/clo/cto) is encountered. A tree data structure that recursively uses bitmaps to track which words are nonzero can accelerate this.\n\nFind first set can also be implemented in software. A simple loop implementation:\n\nwhere \"«\" denotes left-shift. Similar loops can be used to implement all the related operations. On modern architectures this loop is inefficient due to a large number of conditional branches. A lookup table can eliminate most of these:\n\nThe parameter \"n\" is fixed (typically 8) and represents a time–space tradeoff. The loop may also be fully unrolled.\n\nCount Trailing Zeros (ctz) counts the number of zero bits succeeding the least significant one bit. For example, the ctz of 0x00000F00 is 8, and the ctz of 0x80000000 is 31.\n\nAn algorithm for 32-bit ctz by Leiserson, Prokop, and Randall uses de Bruijn sequences to construct a minimal perfect hash function that eliminates all branches:\nThis algorithm requires a CPU with a 32-bit multiply instruction with a 64-bit result. The 32-bit multiply instruction in the low-cost ARM Cortex-M0 / M0+ / M1 cores have a 32-bit result, though other ARM cores have another multiply instruction with a 64-bit result.\n\nThe expression (x & (-x)) again isolates the least-significant 1 bit. There are then only 32 possible words, which the unsigned multiplication and shift hash to the correct position in the table. (Note: this algorithm does not handle the zero input.) A similar algorithm works for log base 2, but rather than isolate the most-significant bit, it rounds up to the nearest integer of the form 2−1 using shifts and bitwise ORs:\n\nA binary search implementation which takes a logarithmic number of operations and branches, as in these 32-bit versions: This algorithm can be assisted by a table as well, replacing the bottom three \"if\" statements with a 256 entry lookup table using the final byte as an index.\n\nCount Leading Zeros (clz) counts the number of zero bits preceding the most significant one bit. For example, the clz of 0x00000F00 is 20, and the clz of 0x00000001 is 31.\n\nJust as count leading zeros is useful for software floating point implementations, conversely, on platforms that provide hardware conversion of integers to floating point, the exponent field can be extracted and subtracted from a constant to compute the count of leading zeros. Corrections are needed to account for rounding errors.\n\nThe following C language examples require a header file for the definition of uint8_t, uint16_t, uint32_t. It is stated here instead of repeating in each code example.\n\nThe non-optimized approach examines one bit at a time until a non-zero bit is found, as shown in this C language example, and slowest with an input value of 1 because of the many loops it has to perform to find it.\n\nAn evolution of the previous looping approach examines four bits at a time then using a lookup table for the final four bits, which is shown here. A faster looping approach would examine eight bits at a time and increasing to a 256 entry lookup table.\n\nFaster than the looping method is a binary search implementation which takes a logarithmic number of operations and branches, as in these 32-bit versions:\n\nThe binary search algorithm can be assisted by a table as well, replacing the bottom two \"if\" statements with a 16 entry lookup table using the final nibble (4-bits) as an index, which is shown here. An alternate approach replaces the bottom three \"if\" statements with a 256 entry lookup table using the final byte (8-bits) as an index. In both of these methods, the initial check for zero is removed because the final table operation takes care of it.\n\nThe fastest practical approach to simulate clz uses a precomputed 64KB lookup table, as shown in this C language example.\n\nOther than a dedicated assembly instruction that performs the CLZ type operation, the fastest method to compute CLZ is reading a pre-computed value from a lookup table. A 4-bit lookup table, clz_table_4bit[16], is used in above examples. The following are C language examples of CLZ for a 8-bit, 16-bit, 32-bit input value. The tables must be pre-computed by functions not shown here. An alternate 8-bit approach could pack two results in each table entry thus needing a 128 entry table instead of 256 entry table, because the bit count is 0 to 8 which fits in a 4-bit nibble.\n\nThe count leading zeros (clz) operation can be used to efficiently implement \"normalization\", which encodes an integer as \"m\" × 2, where \"m\" has its most significant bit in a known position (such as the highest position). This can in turn be used to implement Newton-Raphson division, perform integer to floating point conversion in software, and other applications.\n\nCount leading zeros (clz) can be used to compute the 32-bit predicate \"x = y\" (zero if true, one if false) via the identity clz(x − y) » 5, where \"»\" is unsigned right shift. It can be used to perform more sophisticated bit operations like finding the first string of \"n\" 1 bits. The expression 1 « (16 − clz(x − 1)/2) is an effective initial guess for computing the square root of a 32-bit integer using Newton's method. CLZ can efficiently implement \"null suppression\", a fast data compression technique that encodes an integer as the number of leading zero bytes together with the nonzero bytes. It can also efficiently generate exponentially distributed integers by taking the clz of uniformly random integers.\n\nThe log base 2 can be used to anticipate whether a multiplication will overflow, since formula_2.\n\nCount leading zeros and count trailing zeros can be used together to implement Gosper's loop-detection algorithm, which can find the period of a function of finite range using limited resources.\n\nThe binary GCD algorithm spends many cycles removing trailing zeros; this can be replaced by a count trailing zeros (ctz) followed by a shift. A similar loop appears in computations of the hailstone sequence.\n\nA bit array can be used to implement a priority queue. In this context, find first set (ffs) is useful in implementing the \"pop\" or \"pull highest priority element\" operation efficiently. The Linux kernel real-time scheduler internally uses codice_3 for this purpose.\n\nThe count trailing zeros operation gives a simple optimal solution to the Tower of Hanoi problem: the disks are numbered from zero, and at move \"k\", disk number ctz(\"k\") is moved the minimum possible distance to the right (circling back around to the left as needed). It can also generate a Gray code by taking an arbitrary word and flipping bit ctz(\"k\") at step \"k\".\n\n\n\n"}
{"id": "11985661", "url": "https://en.wikipedia.org/wiki?curid=11985661", "title": "Finitely generated algebra", "text": "Finitely generated algebra\n\nIn mathematics, a finitely generated algebra (also called an algebra of finite type) is an associative algebra \"A\" over a field \"K\" where there exists a finite set of elements \"a\",…,\"a\" of \"A\" such that every element of \"A\" can be expressed as a polynomial in \"a\",…,\"a\", with coefficients in \"K\". If it is necessary to emphasize the field \"K\" then the algebra is said to be finitely generated over \"K\" . Algebras that are not finitely generated are called infinitely generated. Finitely generated reduced commutative algebras are basic objects of consideration in modern algebraic geometry, where they correspond to affine algebraic varieties; for this reason, these algebras are also referred to as (commutative) affine algebras.\n\n\n\n"}
{"id": "43571557", "url": "https://en.wikipedia.org/wiki?curid=43571557", "title": "Fragment (logic)", "text": "Fragment (logic)\n\nIn mathematical logic, a fragment of a logical language or theory is a subset of this logical language obtained by imposing syntactical restrictions on the language. Hence, the well-formed formulae of the fragment are a subset of those in the original logic. However, the semantics of the formulae in the fragment and in the logic coincide, and any formula of the fragment can be expressed in the original logic.\n\nThe computational complexity of tasks such as satisfiability or model checking for the logical fragment can be no higher than the same tasks in the original logic, as there is a reduction from the first problem to the other. An important problem in computational logic is to determine fragments of well-known logics such as first-order logic that are as expressive as possible yet are decidable or more strongly have low computational complexity. The field of descriptive complexity theory aims at establishing a link between logics and computational complexity theory, by identifying logical fragments that exactly capture certain complexity classes.\n"}
{"id": "4963399", "url": "https://en.wikipedia.org/wiki?curid=4963399", "title": "Hua's lemma", "text": "Hua's lemma\n\nIn mathematics, Hua's lemma, named for Hua Loo-keng, is an estimate for exponential sums.\n\nIt states that if \"P\" is an integral-valued polynomial of degree \"k\", formula_1 is a positive real number, and \"f\" a real function defined by\n\nthen\n\nwhere formula_4 lies on a polygonal line with vertices\n"}
{"id": "41341441", "url": "https://en.wikipedia.org/wiki?curid=41341441", "title": "K-trivial set", "text": "K-trivial set\n\nIn mathematics, a set of natural numbers is called a K-trivial set if its initial segments viewed as binary strings are easy to describe: the prefix-free Kolmogorov complexity is as low as possible, close to that of a computable set. Solovay proved in 1975 that a set can be K-trivial without being computable.\n\nThe Schnorr–Levin theorem says that random sets have a high initial segment complexity. Thus the K-trivials are far from random. This is why these sets are studied in the field of algorithmic randomness, which is a subfield of Computability theory and related to algorithmic information theory in computer science.\n\nAt the same time, K-trivial sets are close to computable. For instance, they are all superlow, i.e. sets whose Turing jump is computable from the Halting problem, and form a Turing ideal, i.e. class of sets closed under Turing join and closed downward under Turing reduction.\n\nLet K be the prefix-free Kolmogorov Complexity, i.e. given a string x, K(x) outputs the least length of the input string under a prefix-free universal machine. Such a machine, intuitively, represents a universal programming language with the property that no valid program can be obtained as a proper extension of another valid program. For more background of K, see e.g. Chaitin's constant.\n\nWe say a set A of the natural numbers is K-trivial via a constant b ∈ ℕ if\n\nA set is K-trivial if it is K-trivial via some constant.\n\nIn the early days of the development of K-triviality, attention was paid to separation of K-trivial sets and computable sets.\n\nChaitin in his 1976 paper mainly studied sets such that there exists b ∈ℕ with\n\nwhere C denotes the plain Kolmogorov complexity. These sets are known as C-trivial sets. Chaitin showed they coincide with the computable sets. He also showed that the K-trivials are computable in the halting problem. This class of sets is commonly known as formula_3 sets in arithmetical hierarchy.\n\nRobert M. Solovay was the first to construct a noncomputable K-trivial set, while construction of a computably enumerable such A was attempted by Calude, Coles and other unpublished constructions by Kummer of a K-trivial, and Muchnik junior of a low for K set.\n\nIn the context of computability theory, a cost function is a computable function\n\nFor a computable approximation formula_5 of formula_3 set \"A\", such a function measures the cost \"c\"(\"n\",\"s\") of changing the approximation to A(n) at stage s. The first cost function construction was due to Kučera and Terwijn. They built a computably enumerable set that is low for Martin-Löf-randomness but not computable. Their cost function was adaptive, in that the definition of the cost function depends on the computable approximation of the formula_3 set being built.\n\nA cost function construction of a K-trivial computably enumerable noncomputable set first appeared in Downey et al.\n\nWe say a formula_3 set \"A\" obeys a cost function \"c\" if there exists a computable approximation of A, formula_9\nformula_10 where formula_11\n\nand formula_12 is the \"s\"-th step in a computable approximation of a fixed universal prefix-free machine formula_13.\n\nIn fact the set can be made promptly simple. The idea is to meet the prompt simplicity requirements,\n\nas well as to keep the costs low. We need the cost function to satisfy the limit condition\n\nnamely the supremum over stages of the cost for x goes to 0 as x increases. For instance, the standard cost function has this property. The construction essentially waits until the cost is low before putting numbers into formula_16 to meet the promptly simple requirements. We define a computable enumeration formula_17 such that\n\nformula_18. At stage \"s\"> 0 , for each \"e\" < \"s\", if formula_19 has not been met yet and there exists \"x\" ≥ 2\"e\" such that formula_20 and formula_21, then we put \"x\" into formula_22 and declare that formula_19 is met. End of construction.\n\nTo verify that the construction works, note first that \"A\" obeys the cost function since at most one number enters \"A\" for the sake of each requirement. The sum \"S\" is therefore at most\n\nSecondly, each requirement is met: if formula_25 is infinite, by the fact that the cost function satisfies the limit condition, some number will eventually be enumerated into A to meet the requirement.\n\nK-triviality turns out to coincide with some computational lowness notions, saying that a set is close to computable. The following notions capture the same class of sets.\n\nWe say that \"A\" is low for \"K\" if there is \"b\" ∈ ℕ such that\n\nHere formula_27 is prefix-free Kolmogorov complexity relative to oracle formula_28.\n\nA is low for Martin-Löf-randomness if whenever Z is Martin-Löf random, it is already Martin-Löf random relative to \"A\".\n\n\"A\" is a base for Martin-Löf-randomness if \"A\" is Turing reducible to \"Z\" for some set \"Z\" that is Martin-Löf random relative to \"A\".\n\nMore equivalent characterizations of K-triviality have been studied, such as: \n\nFrom 2009 on, concepts from analysis entered the stage. This helped solving some notorious problems.\n\nOne says that a set Y is a positive density point if every effectively closed class containing Y has positive lower Lebesgue density at Y. Bienvenu, Hölzl, Miller, and Nies showed that a ML-random is Turing incomplete iff it is a positive density point. Day and Miller used this for an affirmative answer to the ML-cupping problem: A is K-trivial iff for every Martin-Löf random set Z such that A⊕Z compute the halting problem, already Z by itself computes the halting problem.\n\nOne says that a set Y is a density-one point if every effectively closed class containing Y has Lebesgue density 1 at Y. Any Martin-Löf random set that is not a density-one point computes every K trivial set by Bienvenu, et al. Day and Miller showed that there is Martin-Löf random set which is a positive density point but not a density one point. Thus there is an incomplete such Martin-Löf random set which computes every K-trivial set. This affirmatively answered the covering problem first asked by Stephan and then published by Miller and Nies. For a summary see L. Bienvenu, A. Day, N. Greenberg, A. Kucera, J. Miller, A. Nies, and D. Turetsky\n\nVariants of K-triviality have been studied:\n"}
{"id": "37016909", "url": "https://en.wikipedia.org/wiki?curid=37016909", "title": "Koecher–Vinberg theorem", "text": "Koecher–Vinberg theorem\n\nIn operator algebra, the Koecher–Vinberg theorem is a reconstruction theorem for real Jordan algebras. It was proved independently by Max Koecher in 1957 and Ernest Vinberg in 1961. It provides a one-to-one correspondence between\nformally real Jordan algebras and so-called domains of positivity. Thus it links operator algebraic and convex order theoretic views on state spaces of physical systems.\n\nA convex cone formula_1 is called \"regular\" if formula_2 whenever both formula_3 and formula_4 are in the closure formula_5.\n\nA convex cone formula_1 in a vector space formula_7 with an inner product has a \"dual cone\" formula_8. The cone is called \"self-dual\" when formula_9. It is called \"homogeneous\" when to any two points formula_10 there is a real linear transformation formula_11 that restricts to a bijection formula_12 and satisfies formula_13.\n\nThe Koecher–Vinberg theorem now states that these properties precisely characterize the positive cones of Jordan algebras.\n\nTheorem: There is a one-to-one correspondence between formally real Jordan algebras and convex cones that are:\n\nConvex cones satisfying these four properties are called \"domains of positivity\" or \"symmetric cones\". The domain of positivity associated with a real Jordan algebra formula_7 is the interior of the 'positive' cone formula_15.\n\nFor a proof, see or .\n"}
{"id": "7335270", "url": "https://en.wikipedia.org/wiki?curid=7335270", "title": "Kähler–Einstein metric", "text": "Kähler–Einstein metric\n\nIn differential geometry, a Kähler–Einstein metric on a complex manifold is a Riemannian metric that is both a Kähler metric and an Einstein metric. A manifold is said to be Kähler–Einstein if it admits a Kähler–Einstein metric. The most important special case of these are the Calabi–Yau manifolds, which are Kähler and Ricci-flat.\n\nThe most important problem for this area is the existence of Kähler–Einstein metrics for compact Kähler manifolds.\n\nIn the case in which there is a Kähler metric, the Ricci curvature is proportional to the Kähler metric. Therefore, the first Chern class is either negative, or zero, or positive.\n\nWhen the first Chern class is negative, Aubin and Yau proved that there is always a Kähler–Einstein metric.\n\nWhen the first Chern class is zero, Yau proved the Calabi conjecture that there is always a Kähler–Einstein metric. Shing-Tung Yau was awarded with his Fields medal because of this work. That leads to the name Calabi–Yau manifolds.\n\nThe third case, the positive or Fano case, is the hardest. In this case, there is a non-trivial obstruction to existence. In 2012, Chen, Donaldson, and Sun proved that in this case existence is equivalent to an algebro-geometric criterion called K-stability. Their proof appeared in a series of articles in the Journal of the American Mathematical Society.\n\nWhen first Chern class is not definite, or we have intermediate Kodaira dimension, then finding canonical metric remained as an open problem, which is called as Algebrization conjecture via Analytical Minimal Model Program . Unifying Geometrization conjecture with algebrization conjecture and Analyzation conjecture referred as Song-Tian Program \n"}
{"id": "794872", "url": "https://en.wikipedia.org/wiki?curid=794872", "title": "List of Boolean algebra topics", "text": "List of Boolean algebra topics\n\nThis is a list of topics around Boolean algebra and propositional logic.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "708399", "url": "https://en.wikipedia.org/wiki?curid=708399", "title": "List of important publications in mathematics", "text": "List of important publications in mathematics\n\nThis is a list of important publications in mathematics, organized by field.\n\nSome reasons why a particular publication might be regarded as important:\n\nAmong published compilations of important publications in mathematics are \"Landmark writings in Western mathematics 1640–1940\" by Ivor Grattan-Guinness and \"A Source Book in Mathematics\" by David Eugene Smith.\n\nBelieved to have been written around the 8th century BC, this is one of the oldest mathematical texts. It laid the foundations of Indian mathematics and was influential in South Asia and its surrounding regions, and perhaps even Greece. Though this was primarily a geometrical text, it also contained some important algebraic developments, including the earliest list of Pythagorean triples discovered algebraically, geometric solutions of linear equations, the earliest use of quadratic equations of the forms ax = c and ax + bx = c, and integral solutions of simultaneous Diophantine equations with up to four unknowns.\n\nContains the earliest description of Gaussian elimination for solving\nsystem of linear equations, it also contains method for finding square root and cubic root.\n\n\nContains the application of right angle triangles for survey of depth or height of distant objects.\n\n\nContains the earliest description of Chinese remainder theorem.\n\n\nAryabhata introduced the method known as \"Modus Indorum\" or the method of the Indians that has become our algebra today. This algebra came along with the Hindu Number system to Arabia and then migrated to Europe. The text contains 33 verses covering mensuration (kṣetra vyāvahāra), arithmetic and geometric progressions, gnomon / shadows (shanku-chhAyA), simple, quadratic, simultaneous, and indeterminate equations. It also gave the modern standard algorithm for solving first-order diophantine equations.\n\nJigu Suanjing (626AD)\n\nThis book by Tang dynasty mathematician Wang Xiaotong Contains the world's earliest third order equation.\n\n\nContained rules for manipulating both negative and positive numbers, rules for dealing the number zero, a method for computing square roots, and general methods of solving linear and some quadratic equations, solution to Pell's equation.\n\n\nThe first book on the systematic algebraic solutions of linear and quadratic equations by the Persian scholar Muhammad ibn Mūsā al-Khwārizmī. The book is considered to be the foundation of modern algebra and Islamic mathematics. The word \"algebra\" itself is derived from the \"al-Jabr\" in the title of the book.\n\nOne of the major treatises on mathematics by Bhāskara II provides the solution for indeterminate equations of 1st and 2nd order.\n\n\nContains the earliest invention of 4th order polynomial equation.\n\n\nThis 13th century book contains the earliest complete solution of 19th century Horner's method of solving\nhigh order polynomial equations (up to 10th order). It also contains a complete solution of Chinese remainder theorem, which predates Euler and Gauss by several centuries.\n\n\nContains the application of high order polynomial equation in solving complex geometry problems.\n\n\nContains the method of establishing system of high order polynomial equations of up to four unknowns.\n\nOtherwise known as \"The Great Art\", provided the first published methods for solving cubic and quartic equations (due to Scipione del Ferro, Niccolò Fontana Tartaglia, and Lodovico Ferrari), and exhibited the first published calculations involving non-real complex numbers.\n\nAlso known as Elements of Algebra, Euler's textbook on elementary algebra is one of the first to set out algebra in the modern form we would recognize today. The first volume deals with determinate equations, while the second part deals with Diophantine equations. The last section contains a proof of Fermat's Last Theorem for the case \"n\" = 3, making some valid assumptions regarding Q() that Euler did not prove.\n\nGauss' doctoral dissertation, which contained a widely accepted (at the time) but incomplete proof of the fundamental theorem of algebra.\n\nThe title means \"Reflections on the algebraic solutions of equations\". Made the prescient observation that the roots of the Lagrange resolvent of a polynomial equation are tied to permutations of the roots of the original equation, laying a more general foundation for what had previously been an ad hoc analysis and helping motivate the later development of the theory of permutation groups, group theory, and Galois theory. The Lagrange resolvent also introduced the discrete Fourier transform of order 3.\n\nPosthumous publication of the mathematical manuscripts of Évariste Galois by Joseph Liouville. Included are Galois' papers \"Mémoire sur les conditions de résolubilité des équations par radicaux\" and \"Des équations primitives qui sont solubles par radicaux\".\n\nOnline version: Online version\n\nTraité des substitutions et des équations algébriques (Treatise on Substitutions and Algebraic Equations). The first book on group theory, giving a then-comprehensive study of permutation groups and Galois theory. In this book, Jordan introduced the notion of a simple group and epimorphism (which he called \"l'isomorphisme mériédrique\"), proved part of the Jordan–Hölder theorem, and discussed matrix groups over finite fields as well as the Jordan normal form.\n\n\nPublication data: 3 volumes, B.G. Teubner, Verlagsgesellschaft, mbH, Leipzig, 1888–1893. Volume 1, Volume 2, Volume 3.\n\nThe first comprehensive work on transformation groups, serving as the foundation for the modern theory of Lie groups.\n\nDescription: Gave a complete proof of the solvability of finite groups of odd order, establishing the long-standing Burnside conjecture that all finite non-abelian simple groups are of even order. Many of the original techniques used in this paper were used in the eventual classification of finite simple groups.\n\nProvided the first fully worked out treatment of abstract homological algebra, unifying previously disparate presentations of homology and cohomology for associative algebras, Lie algebras, and groups into a single theory.\n\nRevolutionized homological algebra by introducing abelian categories and providing a general framework for Cartan and Eilenberg's notion of derived functors.\n\nPublication data: \"Journal für die Reine und Angewandte Mathematik\"\n\nDeveloped the concept of Riemann surfaces and their topological properties beyond Riemann's 1851 thesis work, proved an index theorem for the genus (the original formulation of the Riemann–Hurwitz formula), proved the Riemann inequality for the dimension of the space of meromorphic functions with prescribed poles (the original formulation of the Riemann–Roch theorem), discussed birational transformations of a given curve and the dimension of the corresponding moduli space of inequivalent curves of a given genus, and solved more general inversion problems than those investigated by Abel and Jacobi. André Weil once wrote that this paper \"is one of the greatest pieces of mathematics that has ever been written; there is not a single word in it that is not of consequence.\" \n\nPublication data: \"Annals of Mathematics\", 1955\n\n\"FAC\", as it is usually called, was foundational for the use of sheaves in algebraic geometry, extending beyond the case of complex manifolds. Serre introduced Čech cohomology of sheaves in this paper, and, despite some technical deficiencies, revolutionized formulations of algebraic geometry. For example, the long exact sequence in sheaf cohomology allows one to show that some surjective maps of sheaves induce surjective maps on sections; specifically, these are the maps whose kernel (as a sheaf) has a vanishing first cohomology group. The dimension of a vector space of sections of a coherent sheaf is finite, in projective geometry, and such dimensions include many discrete invariants of varieties, for example Hodge numbers. While Grothendieck's derived functor cohomology has replaced Čech cohomology for technical reasons, actual calculations, such as of the cohomology of projective space, are usually carried out by Čech techniques, and for this reason Serre's paper remains important.\n\nIn mathematics, algebraic geometry and analytic geometry are closely related subjects, where \"analytic geometry\" is the theory of complex manifolds and the more general analytic spaces defined locally by the vanishing of analytic functions of several complex variables. A (mathematical) theory of the relationship between the two was put in place during the early part of the 1950s, as part of the business of laying the foundations of algebraic geometry to include, for example, techniques from Hodge theory. (\"NB\" While analytic geometry as use of Cartesian coordinates is also in a sense included in the scope of algebraic geometry, that is not the topic being discussed in this article.) The major paper consolidating the theory was \"Géometrie Algébrique et Géométrie Analytique\" by Serre, now usually referred to as \"GAGA\". A \"GAGA-style result\" would now mean any theorem of comparison, allowing passage between a category of objects from algebraic geometry, and their morphisms, and a well-defined subcategory of analytic geometry objects and holomorphic mappings.\n\nBorel and Serre's exposition of Grothendieck's version of the Riemann–Roch theorem, published after Grothendieck made it clear that he was not interested in writing up his own result. Grothendieck reinterpreted both sides of the formula that Hirzebruch proved in 1953 in the framework of morphisms between varieties, resulting in a sweeping generalization. In his proof, Grothendieck broke new ground with his concept of Grothendieck groups, which led to the development of K-theory.\n\nWritten with the assistance of Jean Dieudonné, this is Grothendieck's exposition of his reworking of the foundations of algebraic geometry. It has become the most important foundational work in modern algebraic geometry. The approach expounded in EGA, as these books are known, transformed the field and led to monumental advances.\n\nThese seminar notes on Grothendieck's reworking of the foundations of algebraic geometry report on work done at IHÉS starting in the 1960s. SGA 1 dates from the seminars of 1960–1961, and the last in the series, SGA 7, dates from 1967 to 1969. In contrast to EGA, which is intended to set foundations, SGA describes ongoing research as it unfolded in Grothendieck's seminar; as a result, it is quite difficult to read, since many of the more elementary and foundational results were relegated to EGA. One of the major results building on the results in SGA is Pierre Deligne's proof of the last of the open Weil conjectures in the early 1970s. Other authors who worked on one or several volumes of SGA include Michel Raynaud, Michael Artin, Jean-Pierre Serre, Jean-Louis Verdier, Pierre Deligne, and Nicholas Katz.\n\nBrahmagupta's Brāhmasphuṭasiddhānta is the first book that mentions zero as a number, hence Brahmagupta is considered the first to formulate the concept of zero. The current system of the four fundamental operations (addition, subtraction, multiplication and division) based on the Hindu-Arabic number system also first appeared in Brahmasphutasiddhanta. It was also one of the first texts to provide concrete ideas on positive and negative numbers.\n\nFirst presented in 1737, this paper provided the first then-comprehensive account of the properties of continued fractions. It also contains the first proof that the number e is irrational.\n\nDeveloped a general theory of binary quadratic forms to handle the general problem of when an integer is representable by the form formula_1. This included a reduction theory for binary quadratic forms, where he proved that every form is equivalent to a certain canonically chosen reduced form.\n\nThe \"Disquisitiones Arithmeticae\" is a profound and masterful book on number theory written by German mathematician Carl Friedrich Gauss and first published in 1801 when Gauss was 24. In this book Gauss brings together results in number theory obtained by mathematicians such as Fermat, Euler, Lagrange and Legendre and adds many important new results of his own. Among his contributions was the first complete proof known of the Fundamental theorem of arithmetic, the first two published proofs of the law of quadratic reciprocity, a deep investigation of binary quadratic forms going beyond Lagrange's work in Recherches d'Arithmétique, a first appearance of Gauss sums, cyclotomy, and the theory of constructible polygons with a particular application to the constructibility of the regular 17-gon. Of note, in section V, article 303 of Disquisitiones, Gauss summarized his calculations of class numbers of imaginary quadratic number fields, and in fact found all imaginary quadratic number fields of class numbers 1, 2, and 3 (confirmed in 1986) as he had conjectured. In section VII, article 358, Gauss proved what can be interpreted as the first non-trivial case of the Riemann Hypothesis for curves over finite fields (the Hasse–Weil theorem).\n\nPioneering paper in analytic number theory, which introduced Dirichlet characters and their L-functions to establish Dirichlet's theorem on arithmetic progressions. In subsequent publications, Dirichlet used these tools to determine, among other things, the class number for quadratic forms.\n\n\"Über die Anzahl der Primzahlen unter einer gegebenen Grösse\" (or \"On the Number of Primes Less Than a Given Magnitude\") is a seminal 8-page paper by Bernhard Riemann published in the November 1859 edition of the \"Monthly Reports of the Berlin Academy\". Although it is the only paper he ever published on number theory, it contains ideas which influenced dozens of researchers during the late 19th century and up to the present day. The paper consists primarily of definitions, heuristic arguments, sketches of proofs, and the application of powerful analytic methods; all of these have become essential concepts and tools of modern analytic number theory. It also contains the famous Riemann Hypothesis, one of the most important open problems in mathematics.\n\n\"Vorlesungen über Zahlentheorie\" (\"Lectures on Number Theory\") is a textbook of number theory written by German mathematicians P. G. Lejeune Dirichlet and R. Dedekind, and published in 1863.\nThe \"Vorlesungen\" can be seen as a watershed between the classical number theory of Fermat, Jacobi and Gauss, and the modern number theory of Dedekind, Riemann and Hilbert. Dirichlet does not explicitly recognise the concept of the group that is central to modern algebra, but many of his proofs show an implicit understanding of group theory\n\nUnified and made accessible many of the developments in algebraic number theory made during the nineteenth century. Although criticized by André Weil (who stated \"more than half of his famous Zahlbericht is little more than an account of Kummer's number-theoretical work, with inessential improvements\") and Emmy Noether, it was highly influential for many years following its publication.\n\nGenerally referred to simply as \"Tate's Thesis\", Tate's Princeton Ph.D. thesis, under Emil Artin, is a reworking of Erich Hecke's theory of zeta- and \"L\"-functions in terms of Fourier analysis on the adeles. The introduction of these methods into number theory made it possible to formulate extensions of Hecke's results to more general \"L\"-functions such as those arising from automorphic forms.\n\nThis publication offers evidence towards Langlands' conjectures by reworking and expanding the classical theory of modular forms and their \"L\"-functions through the introduction of representation theory.\n\nProved the Riemann hypothesis for varieties over finite fields, settling the last of the open Weil conjectures.\n\nFaltings proves a collection of important results in this paper, the most famous of which is the first proof of the Mordell conjecture (a conjecture dating back to 1922). Other theorems proved in this paper include an instance of the Tate conjecture (relating the homomorphisms between two abelian varieties over a number field to the homomorphisms between their Tate modules) and some finiteness results concerning abelian varieties over number fields with certain properties.\n\nThis article proceeds to prove a special case of the Shimura–Taniyama conjecture through the study of the deformation theory of Galois representations. This in turn implies the famed Fermat's Last Theorem. The proof's method of identification of a deformation ring with a Hecke algebra (now referred to as an \"R=T\" theorem) to prove modularity lifting theorems has been an influential development in algebraic number theory.\n\nHarris and Taylor provide the first proof of the local Langlands conjecture for GL(\"n\"). As part of the proof, this monograph also makes an in depth study of the geometry and cohomology of certain Shimura varieties at primes of bad reduction.\n\nNgô Bảo Châu proved a long-standing unsolved problem in the classical Langlands program, using methods from the Geometric Langlands program.\n\nThe eminent historian of mathematics Carl Boyer once called Euler's \"Introductio in analysin infinitorum\" the greatest modern textbook in mathematics. Published in two volumes, this book more than any other work succeeded in establishing analysis as a major branch of mathematics, with a focus and approach distinct from that used in geometry and algebra. Notably, Euler identified functions rather than curves to be the central focus in his book. Logarithmic, exponential, trigonometric, and transcendental functions were covered, as were expansions into partial fractions, evaluations of for a positive integer between 1 and 13, infinite series-infinite product formulas, continued fractions, and partitions of integers. In this work, Euler proved that every rational number can be written as a finite continued fraction, that the continued fraction of an irrational number is infinite, and derived continued fraction expansions for and formula_2. This work also contains a statement of Euler's formula and a statement of the pentagonal number theorem, which he had discovered earlier and would publish a proof for in 1751.\n\nWritten in India in 1530, this was the world's first calculus text. \"This work laid the foundation for a complete system of fluxions\"\n and served as a summary of the Kerala School's achievements in calculus, trigonometry and mathematical analysis, most of which were earlier discovered by the 14th century mathematician Madhava. It is possible that this text influenced the later development of calculus in Europe. Some of its important developments in calculus include: the fundamental ideas of differentiation and integration, the derivative, differential equations, term by term integration, numerical integration by means of infinite series, the relationship between the area of a curve and its integral, and the mean value theorem.\n\nLeibniz's first publication on differential calculus, containing the now familiar notation for differentials as well as rules for computing the derivatives of powers, products and quotients.\n\nThe Philosophiae Naturalis Principia Mathematica (Latin: \"mathematical principles of natural philosophy\", often \"Principia\" or \"Principia Mathematica\" for short) is a three-volume work by Isaac Newton published on 5 July 1687. Perhaps the most influential scientific book ever published, it contains the statement of Newton's laws of motion forming the foundation of classical mechanics as well as his law of universal gravitation, and derives Kepler's laws for the motion of the planets (which were first obtained empirically). Here was born the practice, now so standard we identify it with science, of explaining nature by postulating mathematical axioms and demonstrating that their conclusion are observable phenomena. In formulating his physical theories, Newton freely used his unpublished work on calculus. When he submitted Principia for publication, however, Newton chose to recast the majority of his proofs as geometric arguments.\n\nPublished in two books, Euler's textbook on differential calculus presented the subject in terms of the function concept, which he had introduced in his 1748 \"Introductio in analysin infinitorum\". This work opens with a study of the calculus of finite differences and makes a thorough investigation of how differentiation behaves under substitutions. Also included is a systematic study of Bernoulli polynomials and the Bernoulli numbers (naming them as such), a demonstration of how the Bernoulli numbers are related to the coefficients in the Euler–Maclaurin formula and the values of ζ(2n), a further study of Euler's constant (including its connection to the gamma function), and an application of partial fractions to differentiation.\n\nWritten in 1853, Riemann's work on trigonometric series was published posthumously. In it, he extended Cauchy's definition of the integral to that of the Riemann integral, allowing some functions with dense subsets of discontinuities on an interval to be integrated (which he demonstrated by an example). He also stated the Riemann series theorem, proved the Riemann-Lebesgue lemma for the case of bounded Riemann integrable functions, and developed the Riemann localization principle.\n\n\nLebesgue's doctoral dissertation, summarizing and extending his research to date regarding his development of measure theory and the Lebesgue integral.\n\n\nRiemann's doctoral dissertation introduced the notion of a Riemann surface, conformal mapping, simple connectivity, the Riemann sphere, the Laurent series expansion for functions having poles and branch points, and the Riemann mapping theorem.\n\n\nThe first mathematical monograph on the subject of linear metric spaces, bringing the abstract study of functional analysis to the wider mathematical community. The book introduced the ideas of a normed space and the notion of a so-called \"B\"-space, a complete normed space. The \"B\"-spaces are now called Banach spaces and are one of the basic objects of study in all areas of modern mathematical analysis. Banach also gave proofs of versions of the open mapping theorem, closed graph theorem, and Hahn–Banach theorem.\n\n\nIntroduced Fourier analysis, specifically Fourier series. Key contribution was to not simply use trigonometric series, but to model \"all\" functions by trigonometric series.\n\nWhen Fourier submitted his paper in 1807, the committee (which included Lagrange, Laplace, Malus and Legendre, among others) concluded: \"...the manner in which the author arrives at these equations is not exempt of difficulties and [...] his analysis to integrate them still leaves something to be desired on the score of generality and even rigour\". Making Fourier series rigorous, which in detail took over a century, led directly to a number of developments in analysis, notably the rigorous statement of the integral via the Dirichlet integral and later the Lebesgue integral.\n\n\nIn his habilitation thesis on Fourier series, Riemann characterized this work of Dirichlet as \"the first profound paper about the subject\". This paper gave the first rigorous proof of the convergence of Fourier series under fairly general conditions (piecewise continuity and monotonicity) by considering partial sums, which Dirichlet transformed into a particular Dirichlet integral involving what is now called the Dirichlet kernel. This paper introduced the nowhere continuous Dirichlet function and an early version of the Riemann–Lebesgue lemma.\n\n\nSettled Lusin's conjecture that the Fourier expansion of any formula_3 function converges almost everywhere.\n\nWritten around the 8th century BC, this is one of the oldest geometrical texts. It laid the foundations of Indian mathematics and was influential in South Asia and its surrounding regions, and perhaps even Greece. Among the important geometrical discoveries included in this text are: the earliest list of Pythagorean triples discovered algebraically, the earliest statement of the Pythagorean theorem, geometric solutions of linear equations, several approximations of π, the first use of irrational numbers, and an accurate computation of the square root of 2, correct to a remarkable five decimal places. Though this was primarily a geometrical text, it also contained some important algebraic developments, including the earliest use of quadratic equations of the forms ax = c and ax + bx = c, and integral solutions of simultaneous Diophantine equations with up to four unknowns.\n\nPublication data: c. 300 BC\n\nOnline version: Interactive Java version\n\nThis is often regarded as not only the most important work in geometry but one of the most important works in mathematics. It contains many important results in plane and solid geometry, algebra (books II and V), and number theory (book VII, VIII, and IX). More than any specific result in the publication, it seems that the major achievement of this publication is the promotion of an axiomatic approach as a means for proving results. Euclid's \"Elements\" has been referred to as the most successful and influential textbook ever written.\n\nThis was a Chinese mathematics book, mostly geometric, composed during the Han Dynasty, perhaps as early as 200 BC. It remained the most important textbook in China and East Asia for over a thousand years, similar to the position of Euclid's \"Elements\" in Europe. Among its contents: Linear problems solved using the principle known later in the West as the \"rule of false position\". Problems with several unknowns, solved by a principle similar to Gaussian elimination. Problems involving the principle known in the West as the Pythagorean theorem. The earliest solution of a matrix using a method equivalent to the modern method.\n\nThe Conics was written by Apollonius of Perga, a Greek mathematician. His innovative methodology and terminology, especially in the field of conics, influenced many later scholars including Ptolemy, Francesco Maurolico, Isaac Newton, and René Descartes. It was Apollonius who gave the ellipse, the parabola, and the hyperbola the names by which we know them.\n\nContains the roots of modern trigonometry. It describes the archeo-astronomy theories, principles and methods of the ancient Hindus. This siddhanta is supposed to be the knowledge that the Sun god gave to an Asura called Maya. It uses sine (jya), cosine (kojya or \"perpendicular sine\") and inverse sine (otkram jya) for the first time, and also contains the earliest use of the tangent and secant. Later Indian mathematicians such as Aryabhata made references to this text, while later Arabic and Latin translations were very influential in Europe and the Middle East.\n\nThis was a highly influential text during the Golden Age of mathematics in India. The text was highly concise and therefore elaborated upon in commentaries by later mathematicians. It made significant contributions to geometry and astronomy, including introduction of sine/ cosine, determination of the approximate value of pi and accurate calculation of the earth's circumference.\n\nLa Géométrie was published in 1637 and written by René Descartes. The book was influential in developing the Cartesian coordinate system and specifically discussed the representation of points of a plane, via real numbers; and the representation of curves, via equations.\n\nOnline version: English\n\nPublication data: \n\nHilbert's axiomatization of geometry, whose primary influence was in its pioneering approach to metamathematical questions including the use of models to prove axiom independence and the importance of establishing the consistency and completeness of an axiomatic system.\n\n\"Regular Polytopes\" is a comprehensive survey of the geometry of regular polytopes, the generalisation of regular polygons and regular polyhedra to higher dimensions. Originating with an essay entitled \"Dimensional Analogy\" written in 1923, the first edition of the book took Coxeter 24 years to complete. Originally written in 1947, the book was updated and republished in 1963 and 1973.\n\nPublication data: Mémoires de l'académie des sciences de Berlin 16 (1760) pp. 119–143; published 1767. (Full text and an English translation available from the Dartmouth Euler archive.)\n\nEstablished the theory of surfaces, and introduced the idea of principal curvatures, laying the foundation for subsequent developments in the differential geometry of surfaces.\n\nPublication data: \"Disquisitiones generales circa superficies curvas\", \"Commentationes Societatis Regiae Scientiarum Gottingesis Recentiores\" Vol. VI (1827), pp. 99–146; \"General Investigations of Curved Surfaces\" (published 1965) Raven Press, New York, translated by A.M.Hiltebeitel and J.C.Morehead.\n\nGroundbreaking work in differential geometry, introducing the notion of Gaussian curvature and Gauss' celebrated Theorema Egregium.\n\nPublication data: \"Über die Hypothesen, welche der Geometrie zu Grunde Liegen\", \"Abhandlungen der Königlichen Gesellschaft der Wissenschaften zu Göttingen\", Vol. 13, 1867. English translation\n\nRiemann's famous Habiltationsvortrag, in which he introduced the notions of a manifold, Riemannian metric, and curvature tensor.\n\nPublication data: Volume I, Volume II, Volume III, Volume IV\n\nLeçons sur la théorie génerale des surfaces et les applications géométriques du calcul infinitésimal (on the General Theory of Surfaces and the Geometric Applications of Infinitesimal Calculus). A treatise covering virtually every aspect of the 19th century differential geometry of surfaces.\n\nDescription: Poincaré's Analysis Situs and his Compléments à l'Analysis Situs laid the general foundations for algebraic topology. In these papers, Poincaré introduced the notions of homology and the fundamental group, provided an early formulation of Poincaré duality, gave the Euler–Poincaré characteristic for chain complexes, and mentioned several important conjectures including the Poincaré conjecture.\n\nThese two Comptes Rendus notes of Leray from 1946 introduced the novel concepts of sheafs, sheaf cohomology, and spectral sequences, which he had developed during his years of captivity as a prisoner of war. Leray's announcements and applications (published in other Comptes Rendus notes from 1946) drew immediate attention from other mathematicians. Subsequent clarification, development, and generalization by Henri Cartan, Jean-Louis Koszul, Armand Borel, Jean-Pierre Serre, and Leray himself allowed these concepts to be understood and applied to many other areas of mathematics. Dieudonné would later write that these notions created by Leray \"undoubtedly rank at the same level in the history of mathematics as the methods invented by Poincaré and Brouwer\".\n\nIn this paper, Thom proved the Thom transversality theorem, introduced the notions of oriented and unoriented cobordism, and demonstrated that cobordism groups could be computed as the homotopy groups of certain Thom spaces. Thom completely characterized the unoriented cobordism ring and achieved strong results for several problems, including Steenrod's problem on the realization of cycles.\n\nThe first paper on category theory. Mac Lane later wrote in \"Categories for the Working Mathematician\" that he and Eilenberg introduced categories so that they could introduce functors, and they introduced functors so that they could introduce natural equivalences. Prior to this paper, \"natural\" was used in an informal and imprecise way to designate constructions that could be made without making any choices. Afterwards, \"natural\" had a precise meaning which occurred in a wide variety of contexts and had powerful and important consequences.\n\nSaunders Mac Lane, one of the founders of category theory, wrote this exposition to bring categories to the masses. Mac Lane brings to the fore the important concepts that make category theory useful, such as adjoint functors and universal properties.\n\n\"This purpose of this book is twofold: to provide a general introduction to higher category theory (using the formalism of \"quasicategories\" or \"weak Kan complexes\"), and to apply this theory to the study of higher versions of Grothendieck topoi. A few applications to classical topology are included.\" (see arXiv.)\n\nOnline version: Online version\n\nContains the first proof that the set of all real numbers is uncountable; also contains a proof that the set of algebraic numbers is countable. (See Georg Cantor's first set theory article.)\n\nFirst published in 1914, this was the first comprehensive introduction to set theory. Besides the systematic treatment of known results in set theory, the book also contains chapters on measure theory and topology, which were then still considered parts of set theory. Here Hausdorff presents and develops highly original material which was later to become the basis for those areas.\n\nGödel proves the results of the title. Also, in the process, introduces the class L of constructible sets, a major influence in the development of axiomatic set theory.\n\nCohen's breakthrough work proved the independence of the continuum hypothesis and axiom of choice with respect to Zermelo–Fraenkel set theory. In proving this Cohen introduced the concept of \"forcing\" which led to many other major results in axiomatic set theory.\n\nPublished in 1854, The Laws of Thought was the first book to provide a mathematical foundation for logic. Its aim was a complete re-expression and extension of Aristotle's logic in the language of mathematics. Boole's work founded the discipline of algebraic logic and would later be central for Claude Shannon in the development of digital logic.\n\nPublished in 1879, the title Begriffsschrift is usually translated as \"concept writing\" or \"concept notation\"; the full title of the book identifies it as \"a formula language, modelled on that of arithmetic, of pure thought\". Frege's motivation for developing his formal logical system was similar to Leibniz's desire for a \"calculus ratiocinator\". Frege defines a logical calculus to support his research in the foundations of mathematics. Begriffsschrift is both the name of the book and the calculus defined therein. It was arguably the most significant publication in logic since Aristotle.\n\nFirst published in 1895, the Formulario mathematico was the first mathematical book written entirely in a formalized language. It contained a description of mathematical logic and many important theorems in other branches of mathematics. Many of the notations introduced in the book are now in common use.\n\nThe Principia Mathematica is a three-volume work on the foundations of mathematics, written by Bertrand Russell and Alfred North Whitehead and published in 1910–1913. It is an attempt to derive all mathematical truths from a well-defined set of axioms and inference rules in symbolic logic. The questions remained whether a contradiction could be derived from the Principia's axioms, and whether there exists a mathematical statement which could neither be proven nor disproven in the system. These questions were settled, in a rather surprising way, by Gödel's incompleteness theorem in 1931.\n\n\nOnline version: Online version\n\nIn mathematical logic, Gödel's incompleteness theorems are two celebrated theorems proved by Kurt Gödel in 1931.\nThe first incompleteness theorem states:\nFor any formal system such that (1) it is formula_4-consistent (omega-consistent), (2) it has a recursively definable set of axioms and rules of derivation, and (3) every recursive relation of natural numbers is definable in it, there exists a formula of the system such that, according to the intended interpretation of the system, it expresses a truth about natural numbers and yet it is not a theorem of the system.\n\nSettled a conjecture of Paul Erdős and Pál Turán (now known as Szemerédi's theorem) that if a sequence of natural numbers has positive upper density then it contains arbitrarily long arithmetic progressions. Szemerédi's solution has been described as a \"masterpiece of combinatorics\" and it introduced new ideas and tools to the field including a weak form of the Szemerédi regularity lemma.\n\nEuler's solution of the Königsberg bridge problem in \"Solutio problematis ad geometriam situs pertinentis\" (\"The solution of a problem relating to the geometry of position\") is considered to be the first theorem of graph theory.\n\nProvides a detailed discussion of sparse random graphs, including distribution of components, occurrence of small subgraphs, and phase transitions.\n\nPresents the Ford-Fulkerson algorithm for solving the maximum flow problem, along with many ideas on flow-based models.\n\n\"See List of important publications in theoretical computer science.\"\n\n\"See list of important publications in statistics.\"\n\nWent well beyond Émile Borel's initial investigations into strategic two-person game theory by proving the minimax theorem for two-person, zero-sum games.\n\nThis book led to the investigation of modern game theory as a prominent branch of mathematics. This work contained the method for finding optimal solutions for two-person zero-sum games.\n\nNash equilibrium\n\nThe book is in two, {0,1<nowiki>|}</nowiki>, parts. The zeroth part is about numbers, the first part about games – both the values of games and also some real games that can be played such as Nim, Hackenbush, Col and Snort amongst the many described.\n\nA compendium of information on mathematical games. It was first published in 1982 in two volumes, one focusing on Combinatorial game theory and surreal numbers, and the other concentrating on a number of specific games.\n\nA discussion of self-similar curves that have fractional dimensions between 1 and 2. These curves are examples of fractals, although Mandelbrot does not use this term in the paper, as he did not coin it until 1975.\nShows Mandelbrot's early thinking on fractals, and is an example of the linking of mathematical objects with natural forms that was a theme of much of his later work.\n\n\"Method of Fluxions\" was a book written by Isaac Newton. The book was completed in 1671, and published in 1736. Within this book, Newton describes a method (the Newton–Raphson method) for finding the real zeroes of a function.\n\nMajor early work on the calculus of variations, building upon some of Lagrange's prior investigations as well as those of Euler. Contains investigations of minimal surface determination as well as the initial appearance of Lagrange multipliers.\n\nKantorovich wrote the first paper on production planning, which used Linear Programs as the model. He received the Nobel prize for this work in 1975.\n\nDantzig's is considered the father of linear programming in the western world. He independently invented the simplex algorithm. Dantzig and Wolfe worked on decomposition algorithms for large-scale linear programs in factory and production planning.\n\nKlee and Minty gave an example showing that the simplex algorithm can take exponentially many steps to solve a linear program.\n\nKhachiyan's work on the ellipsoid method. This was the first polynomial time algorithm for linear programming.\n\nThese are publications that are not necessarily relevant to a mathematician nowadays, but are nonetheless important publications in the history of mathematics.\n\n\nOne of the oldest mathematical texts, dating to the Second Intermediate Period of ancient Egypt. It was copied by the scribe Ahmes (properly \"Ahmose\") from an older Middle Kingdom papyrus. It laid the foundations of Egyptian mathematics and in turn, later influenced Greek and Hellenistic mathematics. Besides describing how to obtain an approximation of π only missing the mark by less than one per cent, it is describes one of the earliest attempts at squaring the circle and in the process provides persuasive evidence against the theory that the Egyptians deliberately built their pyramids to enshrine the value of π in the proportions. Even though it would be a strong overstatement to suggest that the papyrus represents even rudimentary attempts at analytical geometry, Ahmes did make use of a kind of an analogue of the cotangent.\n\nAlthough the only mathematical tools at its author's disposal were what we might now consider secondary-school geometry, he used those methods with rare brilliance, explicitly using infinitesimals to solve problems that would now be treated by integral calculus. Among those problems were that of the center of gravity of a solid hemisphere, that of the center of gravity of a frustum of a circular paraboloid, and that of the area of a region bounded by a parabola and one of its secant lines. For explicit details of the method used, see Archimedes' use of infinitesimals.\n\n\nOnline version: Online version\n\nThe first known (European) system of number-naming that can be expanded beyond the needs of everyday life.\n\nContains over 6000 theorems of mathematics, assembled by George Shoobridge Carr for the purpose of training his students for the Cambridge Mathematical Tripos exams. Studied extensively by Ramanujan. (first half here)\n\nOne of the most influential books in French mathematical literature. It introduces some of the notations and definitions that are now usual (the symbol ∅ or the term bijective for example). Characterized by an extreme level of rigour, formalism and generality (up to the point of being highly criticized for that), its publication started in 1939 and is still unfinished today.\n\nWritten in 1542, it was the first really popular arithmetic book written in the English Language.\n\nTextbook of arithmetic published in 1678 by John Hawkins, who claimed to have edited manuscripts left by Edward Cocker, who had died in 1676. This influential mathematics textbook used to teach arithmetic in schools in the United Kingdom for over 150 years.\n\nAn early and popular English arithmetic textbook published in America in the 18th century. The book reached from the introductory topics to the advanced in five sections.\n\nPublication data: 1892\n\nThe most widely used and influential textbook in Russian mathematics. (See Kiselyov page and MAA review.)\n\n\nA classic textbook in introductory mathematical analysis, written by G. H. Hardy. It was first published in 1908, and went through many editions. It was intended to help reform mathematics teaching in the UK, and more specifically in the University of Cambridge, and in schools preparing pupils to study mathematics at Cambridge. As such, it was aimed directly at \"scholarship level\" students — the top 10% to 20% by ability. The book contains a large number of difficult problems. The content covers introductory calculus and the theory of infinite series.\n\nThe first introductory textbook (graduate level) expounding the abstract approach to algebra developed by Emil Artin and Emmy Noether. First published in German in 1931 by Springer Verlag. A later English translation was published in 1949 by Frederick Ungar Publishing Company.\n\nA definitive introductory text for abstract algebra using a category theoretic approach. Both a rigorous introduction from first principles, and a reasonably comprehensive survey of the field.\n\n\nThe first comprehensive introductory (graduate level) text in algebraic geometry that used the language of schemes and cohomology. Published in 1977, it lacks aspects of the scheme language which are nowadays considered central, like the functor of points.\n\nAn undergraduate introduction to not-very-naive set theory which has lasted for decades. It is still considered by many to be the best introduction to set theory for beginners. While the title states that it is naive, which is usually taken to mean without axioms, the book does introduce all the axioms of Zermelo–Fraenkel set theory and gives correct and rigorous definitions for basic objects. Where it differs from a \"true\" axiomatic set theory book is its character: There are no long-winded discussions of axiomatic minutiae, and there is next to nothing about topics like large cardinals. Instead it aims, and succeeds, in being intelligible to someone who has never thought about set theory before.\n\nThe \"nec plus ultra\" reference for basic facts about cardinal and ordinal numbers. If you have a question about the cardinality of sets occurring in everyday mathematics, the first place to look is this book, first published in the early 1950s but based on the author's lectures on the subject over the preceding 40 years.\n\nThis book is not really for beginners, but graduate students with some minimal experience in set theory and formal logic will find it a valuable self-teaching tool, particularly in regard to forcing. It is far easier to read than a true reference work such as Jech, \"Set Theory\". It may be the best textbook from which to learn forcing, though it has the disadvantage that the exposition of forcing relies somewhat on the earlier presentation of Martin's axiom.\n\nFirst published round 1935, this text was a pioneering \"reference\" text book in topology, already incorporating many modern concepts from set-theoretic topology, homological algebra and homotopy theory.\n\nFirst published in 1955, for many years the only introductory graduate level textbook in the US, teaching the basics of point set, as opposed to algebraic, topology. Prior to this the material, essential for advanced study in many fields, was only available in bits and pieces from texts on other topics or journal articles.\n\nThis short book introduces the main concepts of differential topology in Milnor's lucid and concise style. While the book does not cover very much, its topics are explained beautifully in a way that illuminates all their details.\n\nAn historical study of number theory, written by one of the 20th century's greatest researchers in the field. The book covers some thirty six centuries of arithmetical work but the bulk of it is devoted to a detailed study and exposition of the work of Fermat, Euler, Lagrange, and Legendre. The author wishes to take the reader into the workshop of his subjects to share their successes and failures. A rare opportunity to see the historical development of a subject through the mind of one of its greatest practitioners.\n\n\"An Introduction to the Theory of Numbers\" was first published in 1938, and is still in print, with the latest edition being the 6th (2008). It is likely that almost every serious student and researcher into number theory has consulted this book, and probably has it on their bookshelf. It was not intended to be a textbook, and is rather an introduction to a wide range of differing areas of number theory which would now almost certainly be covered in separate volumes. The writing style has long been regarded as exemplary, and the approach gives insight into a variety of areas without requiring much more than a good grounding in algebra, calculus and complex numbers.\n\n\n\n\"Gödel, Escher, Bach: an Eternal Golden Braid\" is a Pulitzer Prize-winning book, first published in 1979 by Basic Books.\nIt is a book about how the creative achievements of logician Kurt Gödel, artist M. C. Escher and composer Johann Sebastian Bach interweave. As the author states: \"I realized that to me, Gödel and Escher and Bach were only shadows cast in different directions by some central solid essence. I tried to reconstruct the central object, and came up with this book.\"\n\n\"The World of Mathematics\" was specially designed to make mathematics more accessible to the inexperienced. It comprises nontechnical essays on every aspect of the vast subject, including articles by and about scores of eminent mathematicians, as well as literary figures, economists, biologists, and many other eminent thinkers. Includes the work of Archimedes, Galileo, Descartes, Newton, Gregor Mendel, Edmund Halley, Jonathan Swift, John Maynard Keynes, Henri Poincaré, Lewis Carroll, George Boole, Bertrand Russell, Alfred North Whitehead, John von Neumann, and many others. In addition, an informative commentary by distinguished scholar James R. Newman precedes each essay or group of essays, explaining their relevance and context in the history and development of mathematics. Originally published in 1956, it does not include many of the exciting discoveries of the later years of the 20th century but it has no equal as a general historical survey of important topics and applications.\n"}
{"id": "16868392", "url": "https://en.wikipedia.org/wiki?curid=16868392", "title": "List of types of functions", "text": "List of types of functions\n\nFunctions can be identified according to the properties they have. These properties describe the functions behaviour under certain conditions. A parabola is a specific type of function.\n\nThese properties concern the domain, the codomain and the range of functions.\n\nThese properties concern how the function is affected by arithmetic operations on its operand.\n\nThe following are special examples of a homomorphism on a binary operation:\n\nRelative to negation:\n\nRelative to a binary operation and an order:\n\n\nRelative to topology and order:\n\n\n\n\n\nRelative to measure and topology\n\n\n\nIn general, functions are often defined by specifying the name of a dependent variable, and a way of calculating what it should map to. For this purpose, the formula_1 symbol or Church's formula_2 is often used. Also, sometimes mathematicians notate a function's domain and codomain by writing e.g. formula_3. These notions extend directly to lambda calculus and type theory, respectively.\n\nThese are functions that operate on functions or produce other functions, see Higher order function.\nExamples are:\n\n\nCategory theory is a branch of mathematics that formalizes the notion of a special function via arrows or morphisms. A category is an algebraic object that (abstractly) consists of a class of \"objects\", and for every pair of objects, a set of \"morphisms\". A partial (equiv. dependently typed) binary operation called composition is provided on morphisms, every object has one special morphism from it to itself called the identity on that object, and composition and identities are required to obey certain relations.\n\nIn a so-called concrete category, the objects are associated with mathematical structures like sets, magmas, groups, rings, topological spaces, vector spaces, metric spaces, partial orders, differentiable manifolds, uniform spaces, etc., and morphisms between two objects are associated with \"structure-preserving functions\" between them. In the examples above, these would be functions, magma homomorphisms, group homomorphisms, ring homomorphisms, continuous functions, linear transformations (or matrices), metric maps, monotonic functions, differentiable functions, and uniformly continuous functions, respectively.\n\nAs an algebraic theory, one of the advantages of category theory is to enable one to prove many general results with a minimum of assumptions. Many common notions from mathematics (e.g. surjective, injective, free object, basis, finite representation, isomorphism) are definable purely in category theoretic terms (cf. monomorphism, epimorphism).\n\nCategory theory has been suggested as a foundation for mathematics on par with set theory and type theory (cf. topos).\n\nAllegory theory provides a generalization comparable to category theory for relations instead of functions.\n\n"}
{"id": "20525930", "url": "https://en.wikipedia.org/wiki?curid=20525930", "title": "Log-space transducer", "text": "Log-space transducer\n\nA log space transducer (LST) is a type of Turing machine used for log-space reductions.\n\nA log space transducer, formula_1, has three tapes:\n\nformula_1 will be designed to compute a log-space computable function formula_4 (where formula_5 is the alphabet of both the \"input\" and \"output\" tapes). If formula_1 is executed with formula_7 on its \"input\" tape, when the machine halts, it will have formula_8 remaining on its \"output\" tape.\n\nA language formula_9 is said to be log-space reducible to a language formula_10 if there exists a log-space computable function, formula_11, which will convert an input from problem formula_12 into an input to problem formula_13. I.E. formula_14.\n\nThis seems like a rather convoluted idea, but it has two useful properties that are desirable for a reduction:\n\nTransitivity holds because it is possible to feed the output tape of one reducer (A→B) to another (B→C). At first glance, this seems incorrect because the A→C reducer needs to store the output tape from the A→B reducer onto the work tape in order to feed it into the B→C reducer, but this is not true. Each time the B→C reducer needs to access its input tape, the A→C reducer can re-run the A→B reducer, and so the output of the A→B reducer never needs to be stored entirely at once.\n\n"}
{"id": "31142581", "url": "https://en.wikipedia.org/wiki?curid=31142581", "title": "Lyapunov–Schmidt reduction", "text": "Lyapunov–Schmidt reduction\n\nIn mathematics, the Lyapunov–Schmidt reduction or Lyapunov–Schmidt construction is used to study solutions to nonlinear equations in the case when the implicit function theorem does not work. It permits the reduction of infinite-dimensional equations in Banach spaces to finite-dimensional equations. It is named after Aleksandr Lyapunov and Erhard Schmidt.\n\nLet\n\nbe the given nonlinear equation, formula_2 and formula_3 are\nBanach spaces (formula_4 is the parameter space). formula_5 is the\nformula_6-map from a neighborhood of some point formula_7 to\nformula_3 and the equation is satisfied at this point\n\nFor the case when the linear operator formula_10 is invertible, the implicit function theorem assures that there exists\na solution formula_11 satisfying the equation formula_12 at least locally close to formula_13.\n\nIn the opposite case, when the linear operator formula_10 is non-invertible, the Lyapunov–Schmidt reduction can be applied in the following\nway.\n\nOne assumes that the operator formula_10 is a Fredholm operator.\n\nformula_16 and formula_17 has finite dimension.\n\nThe range of this operator formula_18 has finite co-dimension and\nis a closed subspace in formula_3.\n\nWithout loss of generality, one can assume that formula_20\n\nLet us split formula_3 into the direct product formula_22, where formula_23.\n\nLet formula_24 be the projection operator onto formula_25.\n\nLet us consider also the direct product formula_26.\n\nApplying the operators formula_24 and formula_28 to the original equation, one obtains the equivalent system\n\nLet formula_31 and formula_32, then the first equation\n\ncan be solved with respect to formula_34 by applying the implicit function theorem to the operator\n\n(now the conditions of the implicit function theorem are fulfilled).\n\nThus, there exists a unique solution formula_36 satisfying\n\nNow substituting formula_36 into the second equation, one obtains the final finite-dimensional equation\n\nIndeed, the last equation is now finite-dimensional, since the range of formula_40 is finite-dimensional. This equation is now to be solved with respect to formula_41, which is finite-dimensional, and parameters :formula_42\n\nhomogène douée d’un mouvement de rotation, Zap. Akad. Nauk St. Petersburg (1906), 1–225.\n\n(1907), 203–474.\n\nAnnalen 65 (1908), 370–399.\n"}
{"id": "26185707", "url": "https://en.wikipedia.org/wiki?curid=26185707", "title": "Map folding", "text": "Map folding\n\nIn the mathematics of paper folding, map folding and stamp folding are two problems of counting the number of ways that a piece of paper can be folded. In the stamp folding problem, the paper is a strip of stamps with creases between them, and the folds must lie on the creases. In the map folding problem, the paper is a map, divided by creases into rectangles, and the folds must again lie only along these creases.\n\nIn the stamp folding problem, the paper to be folded is a strip of square or rectangular stamps, separated by creases, and the stamps can only be folded along those creases.\nIn one commonly considered version of the problem, each stamp is considered to be distinguishable from each other stamp, so two foldings of a strip of stamps are considered equivalent only when they have the same vertical sequence of stamps.\nFor example, there are six ways to fold a strip of three different stamps:\n\nThese include all six permutations of the stamps, but for more than three stamps not all permutations are possible. If, for a permutation , there are two numbers and with the same parity such that the four numbers , , , and appear in in that cyclic order, then cannot be folded. The parity condition implies that the creases between stamps and , and between stamps and , appear on the same side of the stack of folded stamps, but the cyclic ordering condition implies that these two creases cross each other, a physical impossibility. For instance, the four-element permutation 1324 cannot be folded, because it has this forbidden pattern with and . All remaining permutations, without this pattern, can be folded.\nThe number of different ways to fold a strip of stamps is given by the sequence\nThese numbers are always divisible by (because a cyclic permutation of a foldable stamp sequence is always itself foldable), and the quotients of this division are\nthe number of topologically distinct ways that a half-infinite curve can make crossings with a line, called \"semimeanders\".\nIn the 1960s, John E. Koehler and W. F. Lunnon implemented algorithms that, at that time, could calculate these numbers for up to 28 stamps.\nDespite additional research, the known methods for calculating these numbers take exponential time as a function of .\nThus, there is no formula or efficient algorithm known that could extend this sequence to very large values of . Nevertheless, heuristic methods from physics can be used to predict the rate of exponential growth of this sequence.\n\nThe stamp folding problem usually considers only the number of possible folded states of the strip of stamps, without considering whether it is possible to physically construct the fold by a sequence of moves starting from an unfolded strip of stamps. However, according to the solution of the carpenter's rule problem, every folded state can be constructed (or equivalently, can be unfolded).\n\nIn another variation of the stamp folding problem, the strip of stamps is considered to be blank, so that it is not possible to tell one of its ends from the other, and two foldings are considered distinct only when they have different shapes. Turning a folded strip upside-down or back-to-front is not considered to change its shape, so three stamps have only two foldings, an S-curve and a spiral. More generally, the numbers of foldings with this definition are\n\nMap folding is the question of how many ways there are to fold a rectangular map along its creases, allowing each crease to form either a mountain or a valley fold. It differs from stamp folding in that it includes both vertical and horizontal creases, rather than only creases in a single direction.\n\nThere are eight ways to fold a 2 × 2 map along its creases, counting each different vertical sequence of folded squares as a distinct way of folding the map:\n\nHowever, the general problem of counting the number of ways to fold a map remains unsolved. The numbers of ways of folding an map are known only for . They are:\n\nThe map folding and stamp folding problems are related to a problem in the mathematics of origami of whether a square with a crease pattern can be folded to a flat figure.\nIf a folding direction (either a mountain fold or a valley fold) is assigned to each crease of a strip of stamps, it is possible to test whether the result can be folded flat in polynomial time.\n\nFor the same problem on a map (divided into rectangles by creases with assigned directions) it is unknown whether a polynomial time folding algorithm exists in general,\nalthough a polynomial algorithm is known for maps.\nIn a restricted case where the map is to be folded by a sequence of \"simple\" folds, which fold the paper along a single line, the problem is polynomial.\nSome extensions of the problem, for instance to non-rectangular sheets of paper, are NP-complete.\n\nEven for a one-dimensional strip of stamps, with its creases already labeled as mountain or valley folds, it is NP-hard to find a way to fold it that minimizes the maximum number of stamps that lie between the two stamps of any crease.\n\n\n"}
{"id": "198156", "url": "https://en.wikipedia.org/wiki?curid=198156", "title": "Markov algorithm", "text": "Markov algorithm\n\nIn theoretical computer science, a Markov algorithm is a string rewriting system that uses grammar-like rules to operate on strings of symbols. Markov algorithms have been shown to be Turing-complete, which means that they are suitable as a general model of computation and can represent any mathematical expression from its simple notation. Markov algorithms are named after the Soviet mathematician Andrey Markov, Jr.\n\nRefal is a programming language based on Markov algorithms.\n\nNormal algorithms are verbal, that is intended to be applied to words in different alphabets.\n\nDefinition of any normal algorithm consists of two parts: the definition of the \"alphabet\" algorithm (the algorithm will be applied to words of these alphabet symbols), and the definition of its \"scheme\". Scheme normal algorithm is a finite ordered set of so-called \"substitution formulas\", each of which can be \"simple\" or \"final\". A simple formula substitutions are called words such as formula_1, where formula_2 and formula_3 – are two arbitrary words in the alphabet of the algorithm (called, respectively, the left and right side of the formula substitution). Similarly, the final substitution formulas are called words of the form formula_4, where formula_2 and formula_3 – are two arbitrary words in the alphabet of the algorithm. This assumes that the auxiliary characters formula_7 and formula_8 do not belong to the alphabet of the algorithm (otherwise an executable their role divider left and right sides should select the other two letters).\n\nAn example of a normal algorithm scheme in five-letter alphabet formula_9 can serve the following scheme:\n\nThe process of applying the normal algorithm to an arbitrary word formula_11 in the alphabet of this algorithm is a discrete sequence of elementary steps, consisting of the following. Let’s assume that formula_12 is the word obtained in the previous step of the algorithm (or the original word formula_11, if the current step is the first). If among formulas of substitution there is no left-hand side of which would be included in the formula_12, then the work of the algorithm is considered completed, and the result of this work is considered to be the word formula_12. Otherwise, including the substitution of the left side of which is included in the formula_12, the very first part is selected. If the substitution formula looks like formula_4, then out of all of possible representations of the word formula_12 that looks like formula_19 it is chosen one formula_20, which is the shortest. Then the work of the algorithm is considered completed with the result formula_21. However, if this substitution formula looks like formula_1, then out of all of possible representations of the word formula_12 in the form of formula_19 it is chosen one, in which formula_20 – the shortest, after which the word formula_21 is considered to be the result of the current step, subject to further processing in the next step.\n\nFor example, during the process of applying the algorithm to the scheme diagram above the word formula_27 consistently emerging the words formula_28, formula_29, formula_30, formula_31, formula_32, formula_33, formula_34, formula_35, formula_36, formula_37 and formula_38, after which the algorithm stops with the result formula_39 \n\nFor other examples, see below.\n\nAny normal algorithm is equivalent to some Turing machine, and vice versaany Turing machine is equivalent to some normal algorithm. A version of the Church-Turing thesis formulated in relation to the normal algorithm is called the \"principle of normalization.\"\n\nNormal algorithms have proved to be a convenient means for the construction of many sections of constructive mathematics. Moreover, inherent in the definition of a normal algorithm used in a number of ideas aimed at handling symbolic information programming languagesfor example, in Refal.\n\nThe \"Rules\" is a sequence of pair of strings, usually presented in the form of \"pattern\" → \"replacement\". Each rule may be either ordinary or terminating.\n\nGiven an \"input\" string: \n\n\nNote that after each rule application the search starts over from the first rule.\n\nThe following example shows the basic operation of a Markov algorithm.\n\n\n\"I bought a B of As from T S.\"\n\nIf the algorithm is applied to the above example, the Symbol string will change in the following manner.\n\n\nThe algorithm will then terminate.\n\nThese rules give a more interesting example. They rewrite binary numbers to their unary counterparts. For example: 101 will be rewritten to a string of 5 consecutive bars.\n\n\n\"101\"\n\nIf the algorithm is applied to the above example, it will terminate after the following steps.\n\n\n\n\n"}
{"id": "174475", "url": "https://en.wikipedia.org/wiki?curid=174475", "title": "Modularity theorem", "text": "Modularity theorem\n\nIn mathematics, the modularity theorem (formerly called the Taniyama–Shimura conjecture or the Taniyama–Shimura–Weil conjecture) states that elliptic curves over the field of rational numbers are related to modular forms. Andrew Wiles proved the modularity theorem for semistable elliptic curves, which was enough to imply Fermat's last theorem. Later, Christophe Breuil, Brian Conrad, Fred Diamond and Richard Taylor extended Wiles' techniques to prove the full modularity theorem in 2001. The modularity theorem is a special case of more general conjectures due to Robert Langlands. The Langlands program seeks to attach an automorphic form or automorphic representation (a suitable generalization of a modular form) to more general objects of arithmetic algebraic geometry, such as to every elliptic curve over a number field. Most cases of these extended conjectures have not yet been proved. However, proved that elliptic curves defined over real quadratic fields are modular.\n\nThe theorem states that any elliptic curve over Q can be obtained via a rational map with integer coefficients from the classical modular curve formula_1 for some integer \"N\"; this is a curve with integer coefficients with an explicit definition. This mapping is called a modular parametrization of level \"N\". If \"N\" is the smallest integer for which such a parametrization can be found (which by the modularity theorem itself is now known to be a number called the \"conductor\"), then the parametrization may be defined in terms of a mapping generated by a particular kind of modular form of weight two and level \"N\", a normalized newform with integer \"q\"-expansion, followed if need be by an isogeny.\n\nThe modularity theorem implies a closely related analytic statement: to an elliptic curve \"E\" over Q we may attach a corresponding L-series. The \"L\"-series is a Dirichlet series, commonly written\n\nThe generating function of the coefficients formula_3 is then\n\nIf we make the substitution\n\nwe see that we have written the Fourier expansion of a function formula_6 of the complex variable \"τ\", so the coefficients of the \"q\"-series are also thought of as the Fourier coefficients of formula_7. The function obtained in this way is, remarkably, a cusp form of weight two and level \"N\" and is also an eigenform (an eigenvector of all Hecke operators); this is the Hasse–Weil conjecture, which follows from the modularity theorem.\n\nSome modular forms of weight two, in turn, correspond to holomorphic differentials for an elliptic curve. The Jacobian of the modular curve can (up to isogeny) be written as a product of irreducible Abelian varieties, corresponding to Hecke eigenforms of weight 2. The 1-dimensional factors are elliptic curves (there can also be higher-dimensional factors, so not all Hecke eigenforms correspond to rational elliptic curves). The curve obtained by finding the corresponding cusp form, and then constructing a curve from it, is isogenous to the original curve (but not, in general, isomorphic to it).\n\n stated a preliminary (slightly incorrect) version of the conjecture at the 1955 international symposium on algebraic number theory in Tokyo and Nikkō. Goro Shimura and Taniyama worked on improving its rigor until 1957. rediscovered the conjecture, and showed that it would follow from the (conjectured) functional equations for some twisted L-series of the elliptic curve; this was the first serious evidence that the conjecture might be true. Weil also showed that the conductor of the elliptic curve should be the level of the corresponding modular form. The Taniyama–Shimura–Weil conjecture became a part of the Langlands program.\n\nThe conjecture attracted considerable interest when suggested that the Taniyama–Shimura–Weil conjecture implies Fermat's Last Theorem. He did this by attempting to show that any counterexample to Fermat's Last Theorem would imply the existence of at least one non-modular elliptic curve. This argument was completed when identified a missing link (now known as the epsilon conjecture or Ribet's theorem) in Frey's original work, followed two years later by 's completion of a proof of the epsilon conjecture.\n\nEven after gaining serious attention, the Taniyama–Shimura–Weil conjecture was seen by contemporary mathematicians as extraordinarily difficult to prove or perhaps even inaccessible to proof . For example, Wiles' ex-supervisor John Coates states that it seemed \"impossible to actually prove\", and Ken Ribet considered himself \"one of the vast majority of people who believed [it] was completely inaccessible\".\n\n, with some help from Richard Taylor, proved the Taniyama–Shimura–Weil conjecture for all semistable elliptic curves, which he used to prove Fermat's Last Theorem, and the full Taniyama–Shimura–Weil conjecture was finally proved by , , and who, building on Wiles' work, incrementally chipped away at the remaining cases until the full result was proved. \nOnce fully proven, the conjecture became known as the modularity theorem.\n\nSeveral theorems in number theory similar to Fermat's Last Theorem follow from the modularity theorem. For example: no cube can be written as a sum of two coprime \"n\"-th powers, \"n\" ≥ 3. (The case \"n\" = 3 was already known by Euler.)\n\n\n"}
{"id": "9119648", "url": "https://en.wikipedia.org/wiki?curid=9119648", "title": "Naum Akhiezer", "text": "Naum Akhiezer\n\nNaum Ilyich Akhiezer (; 6 March 1901 – 3 June 1980) was a Soviet mathematician of Jewish origin, known for his works in approximation theory and the theory of differential and integral operators. He is also known as the author of classical books on various subjects in analysis, and for his work on the history of mathematics. He is the brother of the theoretical physicist Aleksander Akhiezer.\n\nNaum Akhiezer was born in Cherykaw (now in Belarus). He studied in the Kiev Institute of Public Education (now Taras Shevchenko National University of Kyiv). In 1928, he defended his PhD thesis \"Aerodynamical Investigations\" under the supervision of Dmitry Grave. From 1928 to 1933, he worked at the Kiev University and at the Kiev Aviation Institute.\n\nIn 1933, Naum Akhiezer moved to Kharkiv. From 1933 to his death, except for the years of war and evacuation, he was a professor at Kharkov University and at other institutes in Kharkiv. From 1935 to 1940 and from 1947 to 1950 he was director of the Kharkov Institute of Mathematics and Mechanics. For many years he headed the Kharkov Mathematical Society.\n\nAkhiezer obtained important results in approximation theory (in particular, on extremal problems, constructive function theory, and the problem of moments), where he masterly applied the methods of the geometric theory of functions of a complex variable (especially, conformal mappings and the theory of Riemann surfaces) and of functional analysis. He found the fundamental connection between the inverse problem for important classes of differential and finite difference operators of the second order with a finite number of gaps in the spectrum, and the Jacobi inversion problem for Abelian integrals. This connection led to explicit solutions of the inverse problem for the so-called \"finite-gap operators\".\n\n\n\n"}
{"id": "6092601", "url": "https://en.wikipedia.org/wiki?curid=6092601", "title": "Neocognitron", "text": "Neocognitron\n\nThe neocognitron is a hierarchical, multilayered artificial neural network proposed by Kunihiko Fukushima in the 1980s. It has been used for handwritten character recognition and other pattern recognition tasks, and served as the inspiration for convolutional neural networks.\n\nThe neocognitron was inspired by the model proposed by Hubel & Wiesel in 1959. They found two types of cells in the visual primary cortex called \"simple cell\" and \"complex cell\", and also proposed a cascading model of these two types of cells for use in pattern recognition tasks.\n\nThe neocognitron is a natural extension of these cascading models. The neocognitron consists of multiple types of cells, the most important of which are called \"S-cells\" and \"C-cells.\" The local features are extracted by S-cells, and these features' deformation, such as local shifts, are tolerated by C-cells. Local features in the input are integrated gradually and classified in the higher layers. The idea of local feature integration is found in several other models, such as the \"LeNet\" model and the \"SIFT\" model.\n\nThere are various kinds of neocognitron. For example, some types of neocognitron can detect multiple patterns in the same input by using backward signals to achieve selective attention.\n\n\n\n"}
{"id": "32002449", "url": "https://en.wikipedia.org/wiki?curid=32002449", "title": "Ockham algebra", "text": "Ockham algebra\n\nIn mathematics, an Ockham algebra is a bounded distributive lattice with a dual endomorphism. They were introduced by , and were named after William of Ockham by . Ockham algebras form a variety.\n\nExamples of Ockham algebras include Boolean algebras, De Morgan algebras, Stone algebras, and Kleene algebras.\n\n"}
{"id": "5776985", "url": "https://en.wikipedia.org/wiki?curid=5776985", "title": "Oracle Spatial and Graph", "text": "Oracle Spatial and Graph\n\nOracle Spatial and Graph, formerly Oracle Spatial, forms a separately-licensed option component of the Oracle Database. The spatial features in Oracle Spatial and Graph aid users in managing geographic and location-data in a native type within an Oracle database, potentially supporting a wide range of applications — from automated mapping, facilities management, and geographic information systems (AM/FM/GIS), to wireless location services and location-enabled e-business. The graph features in Oracle Spatial and Graph include Oracle Network Data Model (NDM) graphs used in traditional network applications in major transportation, telcos, utilities and energy organizations and RDF semantic graphs used in social networks and social interactions and in linking disparate data sets to address requirements from the research, health sciences, finance, media and intelligence communities.\n\nThe geospatial feature of Oracle Spatial and Graph provides a SQL schema and functions that facilitate the storage, retrieval, update, and query of collections of spatial features in an Oracle database. (The spatial component of a spatial feature consists of the geometric representation of its shape in some coordinate space — referred to as its \"geometry\".)\n\nThe Oracle Spatial geospatial data features consist of:\n\n\nThe Network Data Model feature is a property graph model used to model and analyze physical and logical networks used in industries such as transportation, logistics, and utilities. Its features include:\n\nThe RDF Semantic Graph feature supports the World Wide Web Consortium (W3C) RDF standards. It provides RDF data management, querying and inferencing that are commonly used in a variety of applications ranging from semantic data integration to social network analysis and linked open data applications. Its features include:\n\nOracle Spatial and Graph is an option for Oracle Enterprise Edition, and must be licensed separately. It is not included in Oracle Standard Edition or Oracle Standard Edition One. However, the latter two editions allow the use of a subset of spatial features (called \"Oracle Locator\") at no extra cost. An appendix of the \"Oracle Spatial and Graph Developer's Guide\" specifies the functions allowed in Locator.\n\nOracle Corporation makes a companion visualization component and web map server, Oracle Fusion Middleware MapViewer, available as a feature of Oracle WebLogic Server.\n\nThe Oracle RDBMS first incorporated spatial-data capability with a modification to Oracle 4 made by scientists working with the Canadian Hydrographic Service (CHS). A joint development team of CHS and Oracle personnel subsequently redesigned the Oracle kernel, resulting in the \"Spatial Data Option\" or \"SDO\" for Oracle 7. (The SDO_ prefix continues in use within Oracle Spatial implementations.) The spatial indexing system for SDO involved an adaptation of Riemannian hypercube data-structures, invoking a helical spiral through 3-dimensional space, which allows n-size of features. This also permitted a highly efficient compression of the resulting data, suitable for the petabyte-size data repositories that CHS and other major corporate users required, and also improving search and retrieval times. The \"helical hyperspatial code\", or HHCode, as developed by CHS and implemented by Oracle Spatial, comprises a form of space-filling curve.\n\nWith Oracle 8, Oracle Corporation marketing dubbed the spatial extension simply \"Oracle Spatial\". The primary spatial indexing system no longer uses the HHCode, but a standard r-tree index.\n\nSince July, 2012, the option has been named Oracle Spatial and Graph to highlight the graph database capabilities in the product - Network Data Model graph introduced with Oracle Database 10g Release 1 and RDF Semantic Graph introduced with Oracle Database 10g Release 2.\n\n\n\nOracle Documentation Library http://www.oracle.com/pls/db121/portal.portal_db?selected=7&frame= See:\n\n"}
{"id": "4681949", "url": "https://en.wikipedia.org/wiki?curid=4681949", "title": "Pseudorandom permutation", "text": "Pseudorandom permutation\n\nIn cryptography, a pseudorandom permutation (PRP) is a function that cannot be distinguished from a random permutation (that is, a permutation selected at random with uniform probability, from the family of all permutations on the function's domain) with practical effort. An unpredictable permutation (UP) \"F\" is a permutation whose values cannot be predicted by a fast randomized algorithm. Unpredictable permutations may be used as a cryptographic primitive, a building block for cryptographic systems with more complex properties.\n\nLet \"F\" be a mapping {0,1} × {0,1} →{0,1}. \"F\" is a PRP if\n\nA pseudorandom permutation family is a collection of pseudorandom permutations, where a specific permutation may be chosen using a key.\n\nAn adversary for an unpredictable permutation is defined to be an algorithm that is given access to an oracle for both forward and inverse permutation operations. The adversary is given a challenge input \"k\" and is asked to predict the value of \"F\". It is allowed to make a series of queries to the oracle to help it make this prediction, but is not allowed to query the value of \"k\" itself.\n\nA randomized algorithm for generating permutations generates an unpredictable permutation if its outputs are permutations on a set of items (described by length-\"n\" binary strings) that cannot be predicted with accuracy significantly better than random by an adversary that makes a polynomial (in \"n\") number of queries to the oracle prior to the challenge round, whose running time is polynomial in \"n\", and whose error probability is less than 1/2 for all instances. That is, it cannot be predicted in the complexity class PP, relativized by the oracle for the permutation.\n\nThe idealized abstraction of a (keyed) block cipher is a truly random permutation on the mappings between plaintext and ciphertext. If a distinguishing algorithm exists that achieves significant advantage with less effort than specified by the block cipher's security parameter (this usually means the effort required should be about the same as a brute force search through the cipher's key space), then the cipher is considered broken at least in a certificational sense, even if such a break doesn't immediately lead to a practical security failure.\n\nModern ciphers are expected to have super pseudorandomness.\nThat is, the cipher should be indistinguishable from a randomly chosen permutation on the same message space, even if the adversary has black-box access to the forward and inverse directions of the cipher.\n\nIt can be shown that a function \"F\" is not a secure message authentication code (MAC) if it satisfies only the unpredictability requirement. It can also be shown that one cannot build an efficient variable input length MAC from a block cipher which is modelled as an UP of \"n\" bits. It has been shown that the output of a \"k\" = \"n\"/\"ω\"(log \"λ\") round Feistel construction with unpredictable round functions may leak all the intermediate round values. Even for realistic Unpredictable Functions (UF), some partial information about the intermediate round values may be leaked through the output. It was later shown that if a super-logarithmic number of rounds in the Feistel construction is used, then the resulting UP construction is secure even if the adversary gets all the intermediate round values along with the permutation output.\n\nThere is also a theorem that has been proven in this regard which states that if there exists an efficient UP adversary \"A\" that has non-negligible advantage \"ε\" in the unpredictability game against UP construction ψ and which makes a polynomial number of queries to the challenger, then there also exists a UF adversary \"A\" that has non-negligible advantage in the unpredictability game against a UF sampled from the UF family \"F\" . From this, it can be shown that the maximum advantage of the UP adversary \"A\" is \"ε\" = O (\"ε\" (\"qk\")). Here \"ε\" denotes the maximum advantage of a UF adversary running in time O(\"t\" + (\"qk\")) against a UF sampled from \"F\", where \"t\" is the running time of the PRP adversary \"A\" and \"q\" is the number of queries made by it.\n\nIn addition, a signature scheme that satisfies the property of unpredictability and not necessarily pseudo-randomness is essentially a Verifiable Unpredictable Function (VUF). A verifiable unpredictable function is defined analogously to a Verifiable Pseudorandom Function (VRF) but for pseudo-randomness being substituted with weaker unpredictability. Verifiable unpredictable permutations are the permutation analogs of VUFs or unpredictable analogs of VRPs. A VRP is also a VUP and a VUP can actually be built by building a VRP via the Feistel construction applied to a VRF. But this is not viewed useful since VUFs appear to be much easier to construct than VRFs.\n\nMichael Luby and Charles Rackoff showed that a \"strong\" pseudorandom permutation can be built from a pseudorandom function using a Luby–Rackoff construction which is built using a Feistel cipher.\n\n\n"}
{"id": "35391760", "url": "https://en.wikipedia.org/wiki?curid=35391760", "title": "Random number book", "text": "Random number book\n\nA random number book is a book whose main content is a large number of random numbers or random digits. Such books were used in early cryptography and experimental design, and were published by the Rand Corporation and others. The Rand corporation book \"A Million Random Digits with 100,000 Normal Deviates\" was first published in 1955 and was reissued in 2001. \n\nRandom number books have been rendered obsolete for most purposes by the ready availability of random number generators running on electronic computers. However they still have niche uses, particularly in the performance of experimental music pieces that call for them, such as \"Vision\" (1959) and \"Poem\" (1960) by La Monte Young.\n\n"}
{"id": "32308539", "url": "https://en.wikipedia.org/wiki?curid=32308539", "title": "Regularized canonical correlation analysis", "text": "Regularized canonical correlation analysis\n\nRegularized canonical correlation analysis is a way of using ridge regression to solve the singularity problem in the cross-covariance matrices of canonical correlation analysis. By converting formula_1 and formula_2 into formula_3 and formula_4, it ensures that the above matrices will have reliable inverses.\n\nThe idea probably dates back to Hrishikesh D. Vinod's publication in 1976 where he called it \"Canonical ridge\".\nIt has been suggested for use in the analysis of functional neuroimaging data as such data are often singular.\nIt is possible to compute the regularized canonical vectors in the lower-dimensional space.\n\n"}
{"id": "48376628", "url": "https://en.wikipedia.org/wiki?curid=48376628", "title": "Richard Baldus", "text": "Richard Baldus\n\nRichard Baldus (11 May 1885, Salonika – 28 January 1945, Munich) was a German mathematician, specializing in geometry.\n\nRichard Baldus was the son of a station chief of the Anatolian Railway. After his graduation (\"Abitur\") in 1904 at Wilhelmsgymnasium München, he studied in Munich and at the University of Erlangen, where he received his Ph.D. (\"Promotierung\") in 1910 under Max Noether with thesis \"Über Strahlensysteme, welche unendlich viele Regelflächen 2. Grades enthalten\" and where he received his \"Habilitierung\" in 1911. He became in 1919 \"Professor für Geometrie\" at the Technische Hochschule Karlsruhe and served there as rector in 1923–1924. In 1932 he became \"Professor für Geometrie\" (as successor to Sebastian Finsterwalder) at TU München, where in 1934 he also became the successor to the professorial chair of Walther von Dyck, upon the latter's retirement.\n\nIn 1933 Baldus was the president of the Deutsche Mathematiker-Vereinigung. He was an invited speaker at the International Congress of Mathematicians in 1928 at Bologna. He was elected in 1929 a member of the Heidelberger Akademie der Wissenschaften and in 1935 a member of the Bayerische Akademie der Wissenschaften.\n\n"}
{"id": "21397283", "url": "https://en.wikipedia.org/wiki?curid=21397283", "title": "Risk appetite", "text": "Risk appetite\n\nRisk appetite is a concept to help guide an organization's approach to risk and risk management.\n\nRisk appetite is the level of risk that an organization is prepared to accept in pursuit of its objectives, and before action is deemed necessary to reduce the risk. It represents a balance between the potential benefits of innovation and the threats that change inevitably brings. The ISO 31000 risk management standard refers to risk appetite as the \"Amount and type of risk that an organization is prepared to pursue, retain or take\". In a literal sense, defining your appetite means defining how \"hungry\" you are for risk.\n\nThe appropriate level will depend on the nature of the work undertaken and the objectives pursued. For example, where public safety is critical (e.g. operating a nuclear power station) appetite will tend to be low, while for an innovative project (e.g. early development on an innovative computer program) it may be very high, with the acceptance of short term failure that could pave the way to longer term success.\n\nBelow are examples of broad approaches to setting risk appetite that a business may adopt to ensure a response to risk that is proportionate given their business objectives.\n\nThe appropriate approach may vary across an organization, with different parts of the business adopting an appetite that reflects their specific role, with an overarching risk appetite framework to ensure consistency.\n\nPrecise measurement is not always possible and risk appetite will sometimes be defined by a broad statement of approach. An organization may have an appetite for some types of risk and be averse to others, depending on the context and the potential losses or gains.\n\nHowever, often measures can be developed for different categories of risk. For example, it may aid a project to know what level of delay or financial loss it is permitted to bear. Where an organization has standard measures to define the impact and likelihood of risks, this can be used to define the maximum level of risk tolerable before action should be taken to lower it.\n\nBy defining its risk appetite, an organization can arrive at an appropriate balance between uncontrolled innovation and excessive caution. It can guide people on the level of risk permitted and encourage consistency of approach across an organisation.\n\nDefined acceptable levels of risk also means that resources are not spent on further reducing risks that are already at an acceptable level.\n\nIn literature there are six main areas of risk appetite:\n\n\nThere is often a confusion between \"risk management\" and \"risk appetite\", with the rigor of the former now recovering some of its lost ground from the vagueness of the latter. Derived correctly the risk appetite is a consequence of a rigorous risk management analysis not a precursor. Simple risk management techniques deal with the impact of hazardous events, but this ignores the possibility of collateral effects of a bad outcome, such as for example becoming technically bankrupt. The quantity that can be put at risk depends on the cover available should there be a loss, and a proper analysis takes this into account. The \"appetite\" follows logically from this analysis. For example an organization should be \"hungry for risk\" if it has more than ample cover compared with its competitors and should therefore be able to gain greater returns in the market from high risk ventures.\n\n"}
{"id": "22392651", "url": "https://en.wikipedia.org/wiki?curid=22392651", "title": "Slowly varying envelope approximation", "text": "Slowly varying envelope approximation\n\nIn physics, the slowly varying envelope approximation (SVEA, sometimes also called slowly varying amplitude approximation or SVAA) is the assumption that the envelope of a forward-travelling wave pulse varies slowly in time and space compared to a period or wavelength. This requires the spectrum of the signal to be narrow-banded—hence it also referred to as the narrow-band approximation.\n\nThe slowly varying envelope approximation is often used because the resulting equations are in many cases easier to solve than the original equations, reducing the order of—all or some of—the highest-order partial derivatives. But the validity of the assumptions which are made need to be justified.\n\nFor example, consider the electromagnetic wave equation:\n\nIf k and \"ω\" are the wave number and angular frequency of the (characteristic) carrier wave for the signal \"E\"(r,\"t\"), the following representation is useful:\n\nwhere formula_3 denotes the real part of the quantity between brackets.\n\nIn the \"slowly varying envelope approximation\" (SVEA) it is assumed that the complex amplitude \"E\"(r, \"t\") only varies slowly with r and \"t\". This inherently implies that \"E\"(r, \"t\") represents waves propagating forward, predominantly in the k direction. As a result of the slow variation of \"E\"(r, \"t\"), when taking derivatives, the highest-order derivatives may be neglected:\n\nConsequently, the wave equation is approximated in the SVEA as:\n\nIt is convenient to choose k and \"ω\" such that they satisfy the dispersion relation:\n\nThis gives the following approximation to the wave equation, as a result of the slowly varying envelope approximation:\n\nThis is a hyperbolic partial differential equation, like the original wave equation, but now of first-order instead of second-order. It is valid for coherent forward-propagating waves in directions near the k-direction. The space and time scales over which \"E\" varies are generally much longer than the spatial wavelength and temporal period of the carrier wave. A numerical solution of the envelope equation thus can use much larger space and time steps, resulting in significantly less computational effort.\n\nAssume wave propagation is dominantly in the \"z\"-direction, and k is taken in this direction. The SVEA is only applied to the second-order spatial derivatives in the \"z\"-direction and time. If formula_10 is the Laplace operator in the \"x\"–\"y\" plane, the result is:\n\nThis is a parabolic partial differential equation. This equation has enhanced validity as compared to the full SVEA: it represents waves propagating in directions significantly different from the \"z\"-direction.\n\n"}
{"id": "10930578", "url": "https://en.wikipedia.org/wiki?curid=10930578", "title": "Spectral centroid", "text": "Spectral centroid\n\nThe spectral centroid is a measure used in digital signal processing to characterise a spectrum. It indicates where the \"center of mass\" of the spectrum is located. Perceptually, it has a robust connection with the impression of \"brightness\" of a sound.\n\nIt is calculated as the weighted mean of the frequencies present in the signal, determined using a Fourier transform, with their magnitudes as the weights:\n\nwhere \"x(n)\" represents the weighted frequency value, or magnitude, of bin number \"n\", and \"f(n)\" represents the center frequency of that bin.\n\nSome people use \"spectral centroid\" to refer to the median of the spectrum. This is a \"different\" statistic, the difference being essentially the same as the difference between the unweighted median and mean statistics. Since both are measures of central tendency, in some situations they will exhibit some similarity of behaviour. But since typical audio spectra are not normally distributed, the two measures will often give strongly different values. Grey and Gordon in 1978 found the mean a better fit than the median.\n\nBecause the spectral centroid is a good predictor of the \"brightness\" of a sound, it is widely used in digital audio and music processing as an automatic measure of musical timbre.\n"}
{"id": "4924488", "url": "https://en.wikipedia.org/wiki?curid=4924488", "title": "Squared triangular number", "text": "Squared triangular number\n\nIn number theory, the sum of the first cubes is the square of the th triangular number. That is,\nThe same equation may be written more compactly using the mathematical notation for summation:\n\nThis identity is sometimes called Nicomachus's theorem, after Nicomachus of Gerasa (c. 60 – c. 120 CE).\n\nMany early mathematicians have studied and provided proofs of Nicomachus's theorem. claims that \"every student of number theory surely must have marveled at this miraculous fact\". finds references to the identity not only in the works of Nicomachus in what is now Jordan in the first century CE, but also in those of Aryabhata in India in the fifth century, and in those of Al-Karaji circa 1000 in Persia. mentions several additional early mathematical works on this formula, by Alchabitius (tenth century Arabia), Gersonides (circa 1300 France), and Nilakantha Somayaji (circa 1500 India); he reproduces Nilakantha's visual proof.\n\nThe sequence of squared triangular numbers is\n\nThese numbers can be viewed as figurate numbers, a four-dimensional hyperpyramidal generalization of the triangular numbers and square pyramidal numbers.\n\nAs observes, these numbers also count the number of rectangles with horizontal and vertical sides formed in an grid. For instance, the points of a grid (or a square made up of three smaller squares on a side) can form 36 different rectangles. The number of squares in a square grid is similarly counted by the square pyramidal numbers.\n\nThe identity also admits a natural probabilistic interpretation as follows. Let be four integer numbers independently and uniformly chosen at random between and . Then, the probability that be the largest of the four numbers is equal to the probability that both is at least as large as and is at least as large as , that is, . Indeed, these probabilities are respectively the left and right sides of the Nichomacus identity, normalized to make probabilities by dividing both sides by .\n\n gives a particularly simple derivation, by expanding each cube in the sum into a set of consecutive odd numbers. Indeed, he begins by giving the identity \n\nThat identity is related to triangular numbers formula_4 in the following way:\n\nand thus the summands forming formula_6 start off just after those forming all previous values formula_7 up to formula_8.\nApplying this property, along with another well-known identity:\n\nwe obtain the following derivation:\n\nIn the more recent mathematical literature, uses the rectangle-counting interpretation of these numbers to form a geometric proof of the identity (see also ); he observes that it may also be proved easily (but uninformatively) by induction, and states that provides \"an interesting old Arabic proof\". provides a purely visual proof, provide two additional proofs, and gives seven geometric proofs.\n\nA similar result to Nicomachus's theorem holds for all power sums, namely that odd power sums (sums of odd powers) are a polynomial in triangular numbers.\nThese are called Faulhaber polynomials, of which the sum of cubes is the simplest and most elegant example.\n\n\n"}
{"id": "1288985", "url": "https://en.wikipedia.org/wiki?curid=1288985", "title": "Stueckelberg action", "text": "Stueckelberg action\n\nIn field theory, the Stueckelberg action (named after Ernst Stueckelberg (1938), \"Die Wechselwirkungskräfte in der Elektrodynamik und in der Feldtheorie der Kräfte\", \"Helv. Phys. Acta.\" 11: 225) describes a massive spin-1 field as an R (the real numbers are the Lie algebra of U(1)) Yang–Mills theory coupled to a real scalar field φ. This scalar field takes on values in a real 1D affine representation of R with\" m\" as the coupling strength.\n\nThis is a special case of the Higgs mechanism, where, in effect, and thus the mass of the Higgs scalar excitation has been taken to infinity, so the Higgs has decoupled and can be ignored, resulting in a nonlinear, affine representation of the field, instead of a linear representation — in contemporary terminology, a \"U(1)\" nonlinear -model.\n\nGauge-fixing φ=0, yields the Proca action.\n\nThis explains why, unlike the case for non-abelian vector fields, quantum electrodynamics with a massive photon is, in fact, renormalizable, even though it is not manifestly gauge invariant (after the Stückelberg scalar has been eliminated in the Proca action).\n\nThe Stueckelberg Lagrangian of the \"StSM\" (Stueckelberg extension of the Standard Model) consists of a gauge invariant kinetic term for a massive U(1) gauge field. Such a term can be implemented into the Lagrangian of the Standard Model\nwithout destroying the renormalizability of the theory and further provides a mechanism for\nmass generation that is distinct from the Higgs mechanism in the context of Abelian gauge theories.\n\nThe model involves a non-trivial\nmixing of the Stueckelberg and the Standard Model sectors by including an additional term in the effective Lagrangian of the Standard Model given by \n\nThe first term above is the Stueckelberg field strength, formula_3 and formula_4 are topological mass parameters and formula_5 is the axion.\nAfter symmetry breaking in the electroweak sector the photon remains massless. The model predicts a new type of gauge boson dubbed formula_6 which inherits a very distinct narrow decay width in this model. The St sector of the StSM decouples from the SM in limit formula_7.\n\nStueckelberg type couplings arise quite naturally in theories involving compactifications of higher-dimensional string theory, in particular, these couplings appear in the dimensional reduction of the ten-dimensional N = 1 supergravity coupled to supersymmetric Yang–Mills gauge fields in the presence of internal gauge fluxes. In the context of intersecting D-brane model building, products of U(N) gauge groups are broken to their SU(N) subgroups via the Stueckelberg couplings and thus the Abelian gauge fields become massive. Further, in a much simpler fashion one may consider a model with only one extra dimension (a type of Kaluza–Klein model) and compactify down to a four-dimensional theory. The resulting Lagrangian will contain massive vector gauge bosons that acquire masses through the Stueckelberg mechanism.\n\n\n"}
{"id": "39210398", "url": "https://en.wikipedia.org/wiki?curid=39210398", "title": "Subpaving", "text": "Subpaving\n\nIn mathematics, a subpaving is a set of nonoverlapping box of R. A subset \"X\" of R can be approximated by two subpavings \"X\" and \"X\" such that \"X\" ⊂ \"X\" ⊂ \"X\".\nThe three figures on the right show an approximation of the set \"X\" = {(\"x\", \"x\") ∈ R | \"x\" + \"x\" + \nsin(\"x\" + \"x\") ∈ [4,9]} with different accuracies. The set \"X\" corresponds to red boxes and the set \"X\" contains all red and yellow boxes.\n\nCombined with interval-based methods, subpavings are used to approximate the solution set of non-linear problems such as set inversion problems. \n\nSubpavings can also be used to prove that a set defined by nonlinear inequalities is path connected \n, to provide topological properties of such sets\n\n, to solve piano-mover's problems\n\nor to implement set computation\n"}
{"id": "32942112", "url": "https://en.wikipedia.org/wiki?curid=32942112", "title": "Sudhakara Dvivedi", "text": "Sudhakara Dvivedi\n\nSudhakara Dvivedi (1855–1910) was an Indian scholar in Sanskrit and mathematics.\n\nSudhakara Dvivedi was born in 1855 in Khajuri, a village near Varanasi. In childhood he studied mathematics under Pandit Devakrsna.\n\nIn 1883 he was appointed a librarian in the Government Sanskrit College, Varanasi where in 1890 he was appointed the teacher of mathematics and astrology after Bapudeva Sastri retired in 1889.\n\nHe was the head of mathematics department in Queen's college Benaras from where he retired in 1905 and mathematician Ganesh Prasad became the new head of department. Dvivedi wrote a number of translations, commentaries and treatises, including one on algebra which included topics such as Pellian equations, squares, and Diophantine equations.\n\n\n\n"}
{"id": "15296985", "url": "https://en.wikipedia.org/wiki?curid=15296985", "title": "T-shaped molecular geometry", "text": "T-shaped molecular geometry\n\nIn chemistry, T-shaped molecular geometry describes the structures of some molecules where a central atom has three ligands. Ordinarily, three-coordinated compounds adopt trigonal planar or pyramidal geometries. Examples of T-shaped molecules are the halogen trifluorides, such as ClF.\n\nAccording to VSEPR theory, T-shaped geometry results when three ligands and two lone pairs of electrons are bonded to the central atom, written in AXE notation as AXE. The T-shaped geometry is related to the trigonal bipyramidal molecular geometry for AX molecules with three equatorial and two axial ligands. In an AXE molecule, the two lone pairs occupy two equatorial positions, and the three ligand atoms occupy the two axial positions as well as one equatorial position. The three atoms bond at 90° angles on one side of the central atom, producing the T shape.\n\nThe trifluoroxenate(II) anion, , has been investigated as a possible first example of an AXE molecule, which might be expected by VSEPR reasoning to have six electron pairs in an octahedral arrangement with both the three lone pairs and the three ligands in a \"mer\" or T-shaped orientations. Although this anion has been detected in the gas phase, attempts at synthesis in solution and experimental structure determination were unsuccessful. A computational chemistry study showed a distorted planar Y-shaped geometry with the smallest F–Xe–F bond angle equal to 69°, rather than 90° as in a T-shaped geometry.\n\n\n"}
{"id": "7583397", "url": "https://en.wikipedia.org/wiki?curid=7583397", "title": "Tortuosity", "text": "Tortuosity\n\nTortuosity is a property of curve being tortuous (twisted; having many turns). There have been several attempts to quantify this property. Tortuosity is commonly used to describe diffusion and fluid flow in porous media, such as soils and snow.\n\nSubjective estimation (sometimes aided by optometric grading scales) is often used.\n\nThe simplest mathematical method to estimate tortuosity is the arc-chord ratio: the ratio of the length of the curve (\"L\") to the distance between the ends of it (\"C\"):\n\nArc-chord ratio equals 1 for a straight line and is infinite for a circle.\n\nAnother method, proposed in 1999, is to estimate the tortuosity as the integral of the square (or module) of the curvature. Dividing the result by length of curve or chord has also been tried.\n\nIn 2002 several Italian scientists proposed one more method. At first, the curve is divided into several (\"N\") parts with constant sign of curvature (using hysteresis to decrease sensitivity to noise). Then the arc-chord ratio for each part is found and the tortuosity is estimated by:\n\nIn this case tortuosity of both straight line and circle is estimated to be 0.\n\nIn 1993 Swiss mathematician Martin Mächler proposed an analogy: it’s relatively easy to drive a bicycle or a car in a trajectory with a constant curvature (an arc of a circle), but it’s much harder to drive where curvature changes. This would imply that roughness (or tortuosity) could be measured by relative change of curvature. In this case the proposed \"local\" measure was derivative of logarithm of curvature:\n\nHowever, in this case tortuosity of a straight line is left undefined.\n\nIn 2005 it was proposed to measure tortuosity by an integral of square of derivative of curvature, divided by the length of a curve:\n\nIn this case tortuosity of both straight line and circle is estimated to be 0.\n\nFractal dimension has been used to quantify tortuosity. The fractal dimension in 2D for a straight line is 1 (the minimal value), and ranges up to 2 for a plane-filling curve or Brownian motion.\n\nIn most of these methods digital filters and approximation by splines can be used to decrease sensitivity to noise.\n\nUsually subjective estimation is used. However, several ways to adapt methods estimating tortuosity in 2-D have also been tried. The methods include arc-chord ratio, arc-chord ratio divided by number of inflection points and integral of square of curvature, divided by length of the curve (curvature is estimated assuming that small segments of curve are planar). Another method used for quantifying tortuosity in 3D has been applied in 3D reconstructions of solid oxide fuel cell cathodes where the Euclidean distance sums of the centroids of a pore were divided by the length of the pore.\n\nTortuosity of blood vessels (for example, retinal and cerebral blood vessels) is known to be used as a medical sign.\n\nIn mathematics, cubic splines minimize the functional, equivalent to integral of square of curvature (approximating the curvature as the second derivative).\n\nIn many engineering domains dealing with mass transfer in porous materials, such as hydrogeology or heterogeneous catalysis, the tortuosity refers to the ratio of the diffusivity in the free space to the diffusivity in the porous medium (analogous to arc-chord ratio of path). Strictly speaking, however, the effective diffusivity is proportional to the reciprocal of the square of the geometrical tortuosity\n\nBecause of the porous materials found in several layers of the Fuel Cells, the tortuosity is an important variable to be analyzed. It is important to notice that there are different kind of tortuosity, i.e., gas-phase, ionic and electronic tortuosity.\n\nIn acoustics and following initial works by Maurice Anthony Biot in 1956, the tortuosity is used to describe sound propagation in fluid-saturated porous media. In such media, when frequency of the sound wave is high enough, the effect of viscous drag force between the solid and the fluid can be ignored. In this case, velocity of sound propagation in the fluid in the pores is non-dispersive and compared with the value of the velocity of sound in the free fluid is reduced by a ratio equal to the square root of the tortuosity. This has been used for a number of applications including the study of materials for acoustic isolation, and for oil prospection using acoustics means.\n\nIn analytical chemistry applied to polymers and sometimes small molecules tortuosity is applied in Gel permeation chromatography (GPC) also known as Size Exclusion Chromatography (SEC). As with any chromatography it is used to separate mixtures. In the case of GPC the separation is based on molecular size and it works by the use of stationary media with appropriately dimensioned pores. The separation occurs because larger molecules take a shorter, less tortuous path and elute more quickly and smaller molecules can pass into the pores and take a longer, more tortuous path and elute later.\n\nIn pharmaceutical sciences, tortuosity is used in relation to diffusion-controlled release from solid dosage forms. Insoluble matrix formers, such as ethyl cellulose, certain vinyl polymers, starch acetate and others control the permeation of the drug from the preparation and into the surrounding liquid. The rate of mass transfer per area unit is, among other factors, related to the shape of polymeric chains within the dosage form. Higher tortuosity or curviness retards mass transfer as it acts obstructively on the drug particles within the formulation.\n\nHVAC makes extensive use of tortuosity in evaporator and condenser coils for heat exchangers, whereas Ultra-high vacuum makes use of the inverse of tortuosity, which is conductivity, with short, straight, voluminous paths.\n\nTortuosity has been used in ecology to describe the movement paths of animals.\n"}
{"id": "2016807", "url": "https://en.wikipedia.org/wiki?curid=2016807", "title": "Uniquely inversible grammar", "text": "Uniquely inversible grammar\n\nA uniquely inversible grammar is a formal grammar where no two distinct productions give the same result. This implies the specific production can be inferred from its results.\n\nformula_1\n\n\nformula_2\n\nformula_3\n\n\nformula_4\n\nformula_5\n"}
{"id": "581175", "url": "https://en.wikipedia.org/wiki?curid=581175", "title": "Vertex-transitive graph", "text": "Vertex-transitive graph\n\nIn the mathematical field of graph theory, a vertex-transitive graph is a graph \"G\" such that, given any two vertices v and v of \"G\", there is some automorphism\n\nsuch that\n\nIn other words, a graph is vertex-transitive if its automorphism group acts transitively upon its vertices. A graph is vertex-transitive if and only if its graph complement is, since the group actions are identical.\n\nEvery symmetric graph without isolated vertices is vertex-transitive, and every vertex-transitive graph is regular. However, not all vertex-transitive graphs are symmetric (for example, the edges of the truncated tetrahedron), and not all regular graphs are vertex-transitive (for example, the Frucht graph and Tietze's graph).\n\nFinite vertex-transitive graphs include the symmetric graphs (such as the Petersen graph, the Heawood graph and the vertices and edges of the Platonic solids). The finite Cayley graphs (such as cube-connected cycles) are also vertex-transitive, as are the vertices and edges of the Archimedean solids (though only two of these are symmetric). Potočnik, Spiga and Verret have constructed a census of all connected cubic vertex-transitive graphs on at most 1280 vertices.\n\nAlthough every Cayley graph is vertex-transitive, there exist other vertex-transitive graphs that are not Cayley graphs. The most famous example is the Petersen graph, but others can be constructed including the line graphs of edge-transitive non-bipartite graphs with odd vertex degrees.\n\nThe edge-connectivity of a vertex-transitive graph is equal to the degree \"d\", while the vertex-connectivity will be at least 2(\"d\"+1)/3.\nIf the degree is 4 or less, or the graph is also edge-transitive, or the graph is a minimal Cayley graph, then the vertex-connectivity will also be equal to \"d\".\n\nInfinite vertex-transitive graphs include:\n\nTwo countable vertex-transitive graphs are called quasi-isometric if the ratio of their distance functions is bounded from below and from above. A well known conjecture stated that every infinite vertex-transitive graph is quasi-isometric to a Cayley graph. A counterexample was proposed by and Leader in 2001. In 2005, Eskin, Fisher, and Whyte confirmed the counterexample.\n\n\n"}
{"id": "690804", "url": "https://en.wikipedia.org/wiki?curid=690804", "title": "Winifred Burkle", "text": "Winifred Burkle\n\nWinifred \"Fred\" Burkle is a fictional character created by Joss Whedon and introduced by Shawn Ryan and Mere Smith on the television series \"Angel\". The character is portrayed by Amy Acker.\n\nFred was born in San Antonio, Texas to Roger and Patricia \"Trish\" Burkle. When she finished college, she moved to Los Angeles for graduate school at UCLA. Originally majoring in history, Fred took a physics class with Professor Seidel, which inspired her to take another path. Around this time, she began working at Stewart Brunell Public Library. In 1996, while shelving a demon language book, a curious Fred recited the cryptic text out loud and was accidentally sucked into a dimensional portal to Pylea (her future friend Lorne was sucked into the same portal on his side and ended up in Los Angeles). It was later discovered that the portal was actually opened by Fred's jealous college professor, Professor Seidel, who had sent every promising student to it, essentially sending them to their death. Fred was the only one of at least six to return (cf. \"Supersymmetry\"). In high school or college, Fred was a marijuana user as shown in the episode \"Spin the Bottle\". In that episode, she asks Wesley and \"Liam\" for weed and was also revealed to have had to take a personality disorder test and to be something of a conspiracy theorist.\n\nFor five years, Fred spent an arduous life as a \"cow\", the Pylean word for humans who are kept as slaves, and then as a fugitive. The harsh life of solitude and serfdom took a serious toll on her social skills, as well as her mental health; when Angel meets Fred, she is curled up in a cave, scribbling on the already-covered walls, having seemingly convinced herself that her previous life in L.A. had not been real.\n\nIt was revealed that Fred had once been forced to wear an explosive shock collar. However, Fred's salvation comes when Angel and his crew arrive in Pylea to find Cordelia Chase, who had become trapped there. It is notable that when Angel's demon came fully to the fore, it attacked just about everyone \"but\" Fred – including Gunn and Wesley. Despite this shocking display of violence, Angel never seemed to scare Fred, and even at his most demonic, he never attacked her. In fact, her presence seemed to have a calming effect on him.\n\nWhen Pylea is liberated, Fred accompanies Angel and the rest of the gang back to Los Angeles and stays in the Hyperion Hotel to re-adjust to life on Earth and regain her mental stability. Despite several traumatic instances, such as being held hostage by Gunn's old vampire-hunting crew, she adjusts quite well to \"normal\" life.\n\nHer knowledge of physics and mathematics make her an excellent asset when researching and developing strategies. Fred quickly develops a romantic relationship with Gunn, which lasts roughly one year. She is also the object of affection of Wesley Wyndam-Pryce, who attempts to step aside after Gunn and Fred started dating, but is still drawn to her. Near the end of her relationship with Gunn, Fred and Wesley share a kiss, but after discovering that Wesley had been in a relationship – albeit a rather complex one – with Lilah Morgan, her feelings for Wesley cool considerably.\n\nEventually, Fred discovers that it was actually her former professor's fault that she had been trapped in Pylea, and indeed, Professor Seidel had attempted to trap her in another world yet again. Furious, she plots to kill him with Wesley's help. Gunn, however, feels that such a brutal act, even against Seidel, will ultimately destroy her. As a result, in a battle where she is trying to trap him in a hell dimension, Gunn snaps Seidel's neck himself and drops the body into the portal. Unfortunately, this causes a rift between Fred and Gunn, and ultimately ends their relationship.\n\nHowever, everything changes for Fred when she and the rest of Angel's crew join Wolfram & Hart. (While Angel accepts a suite of rooms at Wolfram & Hart as his new home, Fred, and maybe others of the inner circle, find a home in the Los Angeles area: hers is at 511 Windward Circle. This seems to be forgotten with the advent of Illyria.) Her memory is altered by a spell cast in Season Four's last episode, \"Home\", and it is unclear how much of the events of Seasons Three and Four she remembers differently or at all (everything specific to Angel's son Connor is definitely lost). Fred receives her own laboratory and becomes the head of Wolfram & Hart's Science Division. She is a major asset to the team; Angel consistently relies on her department to quickly and efficiently solve problems. After going on a few dates with co-worker Knox, Fred begins to have feelings for Wesley again. The pair are together for about a week, but the couple's happiness is not to last.\n\nA mysterious sarcophagus, allowed through customs by a signature from Gunn, appears in the lab. As Fred examines it, a hole opens in the cover and a breath of wind blows into her face. It turns out that the sarcophagus is a holding cell for one of the original, pure-breed demons known as the Old Ones, which is predestined to rise again. The air Fred inhales is actually Illyria's essence, which immediately begins a parasitic existence in her body, eating away at it and making her a shell. Worse still, Knox had worshipped Illyria for years and worked at Wolfram & Hart for the sole purpose of bringing the demon back. Because of his affections for Fred, he chose her as the only one \"worthy\" to house his god.\n\nAs Angel and Spike travel to England to find a cure, Wesley remains in Fred's bedroom with her, comforting her as she fights bravely, but slowly begins to die. Angel learns that the only way to save Fred would be to draw Illyria back to the Deeper Well in England by using her sarcophagus as a beacon. However, thousands of others would die as Illyria's essence cut across the world back to the Well. Thus, Angel and Spike are forced to do nothing, deciding that Fred would not want this.\n\nAs she lies dying, Fred's mind begins to give way. Nearing the end, she panics, stating that Feigenbaum, a stuffed rabbit named for mathematical physicist Mitchell Feigenbaum who studied chaos theory, should be there. When Wesley asks her who Feigenbaum is, Fred replies that she doesn't know. Cradling her in his arms, Wesley stays with Fred until the moment she dies, after which her body is taken over by Illyria. \"Wesley, why can't I stay?\" are Fred's last words.\n\nAccording to Dr. Sparrow, Fred's soul is consumed by the fires of resurrection, which Wesley interprets upon overhearing as soul destruction, seemingly making it impossible for her to return from the dead or enjoy an afterlife, although the specifics of this process are not elaborated. Later, though, Illyria states that there are remnants of Fred in the form of her memories, which are a source of confusion for Illyria. On occasion, Illyria takes on the appearance of Fred in order to go about unnoticed and to deal with Fred's parents. When Wesley dies, Illyria takes on Fred's form to comfort him and because she has Fred's form, the evil warlock Cyvus Vail underestimates her and she easily kills him.\n\nHad \"Angel\" gotten a sixth season, Joss Whedon originally intended for Fred and Illyria to either be split in two as revealed by Amy Acker in an interview: \"As I’m playing this new character now, it was just some stuff that he was going to do with her and bringing Fred back and getting to work with both characters\", or for Illyria to take on more and more of Fred's memories.\n\nFred seemingly reappears in the fifth, sixth and ninth issues of \"\", manifesting as a transformation of Illyria into not just the physical appearance of Fred, but also her personality. This happens a first time upon the initial fusion of Hell and L.A. and then a second time upon a reunion of Illyria and Wesley. Issue #9 reveals that the Illyria and Fred essences have been struggling for dominance over their shared body, and that Spike has been trying to suppress Fred's manifestations (even going so far as to ask Angel for help), and admits that he would have kept Illyria away from the battle had he known Wesley was going to be present. It is later revealed in \"Spike: After the Fall\" that seeing someone Fred cared for triggers the change, while dangerous situations transform her back into Illyria. However, issue #14 explains that the Fred manifestations were just Illyria's interpretation of Fred; with these remnants lost, Illyria reverts to her true form. Later the telepath Betta George puts Wesley and Spike's memories (of Fred) into Illyria's brain to make her question her own actions. Illyria later tries to behave in such a way as Fred would have wanted, and has taken on more of her characteristics (for example, in the \"After the Fall Epilogue\" she has scrawled on the walls of her rooms as Fred did, albeit listing methods of killing).\n\nAfter the Senior Partners rewind time after Angel is killed by Gunn, Angel renames a wing of the L.A. public library the \"Burkle Wyndam-Pryce Wing\" in honor of Fred and Wesley.\n\nIn \"Buffy the Vampire Slayer\" Season Nine, Illyria is drained of her mystical energy and powers and is stuck in the physical appearance of Fred. Illyria believes she should be dead and something inside her must be keeping her alive, and is able to scientifically read graphs to determine why Dawn Summers is dying. Illyria expresses human compassion for Severin, the person who drained her and who is now about to explode with power. Illyria stays with Severin as he uses his powers to restore magic to the world which also vaporizes himself and Illyria. Scott Allie later stated it was intentionally left open to interpretation as to whether Fred was influencing Illyria.\n\nBuffy later expresses regret to her noble companion Eldre Koh on the need to destroy Illyria. Koh casts doubt on Buffy's knowledge of the elder ones.\n\nFred is later discovered alive on the streets of London by Angel. It transpires that, when magic was restored, many aspects of the supernatural were \"reset\", resulting in the resurrection of both Fred and Illyria, sharing the same body. Fred and Illyria thereafter remain with Angel for the remainder of Season 10 and throughout \"Angel\" Season 11.\n\nFred is a normal human woman with no supernatural abilities; however, her brilliant mathematical mind, immense knowledge of quantum physics and science, and a natural ability in designing inventions make her an important asset of Angel's team; Wesley once says, while addressing most of Angel's crew, \"She's smarter than all of us put together\". Due to spending so many years in Pylea, she also has a limited knowledge of the Pylean language and culture.\n\nDuring this time, Fred also acquires some reasonably good fighting skills, mainly with weapons such as stakes, guns, swords, knives, etc. Later, when Jasmine takes over Los Angeles, she's forced to face down all of Los Angeles on her own, and it is shown that she is also able to hold her own unarmed, effortlessly taking out a few armed Jasminites, including one armed SWAT member. It's seen in Season Three that she likes plants, actually talking to them during her period of mild insanity. In the episode \"Spin the Bottle,\" while she's under the effect of a magical spell, Fred is briefly fascinated with a fern. After undergoing the transformation to Illyria, she can talk to plants while at full power.\n\nFred is also portrayed as an innocent, unassuming young woman, which often leads people to underestimate her. On many occasions, she has used this to her advantage, such as shocking Connor with a stun gun and knocking out a suspicious lab assistant at Wolfram & Hart or in the third season in \"That Old Gang of Mine\" when she tricks Gunn's old gang into thinking she was going to kill Angel. Also, she shows signs of great inner strength and an innate ability to survive on her own despite overwhelming circumstances. This is shown in Season Four as she attempts to flee from Jasmine's followers, and earlier with her experiences in Pylea.\n\nFred has 64 canonical Angel appearances overall:\nThe character of Fred appeared in the final four Season Two episodes of \"Angel\" (\"Belonging\", \"Over the Rainbow\", \"Through the Looking Glass\", \"There's No Place Like Plrtz Glrb\") before becoming a regular in Season Three, until her death in \"A Hole in the World\" (episode 15 season 5), although her body continued to appear as the 'host' of Illyria. She appears in a total of 63 episodes (2001–2004).\nIn the final page of #5, it appears that Illyria turns into Fred after noticing Wesley and issue 9 shows her transforming back and forth between Fred and Illyria. However, this is later noted in issue #15 as a misguide. Illyria had been in control of the body the entire time, as well as Fred being dead the entire time.\nIn the Angel & Faith issue #10 Fred appears again. With the Return of the Magic her soul was restored. But Illyria as an Old One her grace couldn't be destroyed and she was restored in Fred's vessel too. Faith and Fred bond and become good friends, similar to Buffy and Willow.\n\nFred also appears in the \"Angel\" expanded universe. She is featured in a number of novels such as \"Sanctuary\", \"The Longest Night\", and \"Nemesis\".\n"}
{"id": "714278", "url": "https://en.wikipedia.org/wiki?curid=714278", "title": "Wolf Prize in Mathematics", "text": "Wolf Prize in Mathematics\n\nThe Wolf Prize in Mathematics is awarded almost annually by the Wolf Foundation in Israel. It is one of the six Wolf Prizes established by the Foundation and awarded since 1978; the others are in Agriculture, Chemistry, Medicine, Physics and Arts. According to a reputation survey conducted in 2013 and 2014, the Wolf Prize in Mathematics is the third most prestigious international academic award in mathematics, after the Abel Prize and the Fields Medal. Until the establishment of the Abel Prize, it was probably the closest equivalent of a \"Nobel Prize in Mathematics\", since the Fields Medal is awarded every four years only to mathematicians under the age of 40.\n\n"}
