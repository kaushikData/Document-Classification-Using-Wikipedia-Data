{"id": "854978", "url": "https://en.wikipedia.org/wiki?curid=854978", "title": "Algebra representation", "text": "Algebra representation\n\nIn abstract algebra, a representation of an associative algebra is a module for that algebra. Here an associative algebra is a (not necessarily unital) ring. If the algebra is not unital, it may be made so in a standard way (see the adjoint functors page); there is no essential difference between modules for the resulting unital ring, in which the identity acts by the identity mapping, and representations of the algebra.\n\nOne of the simplest non-trivial examples is a linear complex structure, which is a representation of the complex numbers C, thought of as an associative algebra over the real numbers R. This algebra is realized concretely as formula_1 which corresponds to . Then a representation of C is a real vector space \"V\", together with an action of C on \"V\" (a map formula_2). Concretely, this is just an action of  , as this generates the algebra, and the operator representing (the image of \"i\" in End(\"V\")) is denoted \"J\" to avoid confusion with the identity matrix \"I\").\n\nAnother important basic class of examples are representations of polynomial algebras, the free commutative algebras – these form a central object of study in commutative algebra and its geometric counterpart, algebraic geometry. A representation of a polynomial algebra in variables over the field \"K\" is concretely a \"K\"-vector space with commuting operators, and is often denoted formula_3 meaning the representation of the abstract algebra formula_4 where formula_5\n\nA basic result about such representations is that, over an algebraically closed field, the representing matrices are simultaneously triangularisable.\n\nEven the case of representations of the polynomial algebra in a single variable are of interest – this is denoted by formula_6 and is used in understanding the structure of a single linear operator on a finite-dimensional vector space. Specifically, applying the structure theorem for finitely generated modules over a principal ideal domain to this algebra yields as corollaries the various canonical forms of matrices, such as Jordan canonical form.\n\nIn some approaches to noncommutative geometry, the free noncommutative algebra (polynomials in non-commuting variables) plays a similar role, but the analysis is much more difficult.\n\nEigenvalues and eigenvectors can be generalized to algebra representations.\n\nThe generalization of an eigenvalue of an algebra representation is, rather than a single scalar, a one-dimensional representation formula_7 (i.e., an algebra homomorphism from the algebra to its underlying ring: a linear functional that is also multiplicative). This is known as a weight, and the analog of an eigenvector and eigenspace are called \"weight vector\" and \"weight space\".\n\nThe case of the eigenvalue of a single operator corresponds to the algebra formula_8 and a map of algebras formula_9 is determined by which scalar it maps the generator \"T\" to. A weight vector for an algebra representation is a vector such that any element of the algebra maps this vector to a multiple of itself – a one-dimensional submodule (subrepresentation). As the pairing formula_10 is bilinear, \"which multiple\" is an \"A\"-linear functional of \"A\" (an algebra map \"A\" → \"R\"), namely the weight. In symbols, a weight vector is a vector formula_11 such that formula_12 for all elements formula_13 for some linear functional formula_14 – note that on the left, multiplication is the algebra action, while on the right, multiplication is scalar multiplication.\n\nBecause a weight is a map to a commutative ring, the map factors through the abelianization of the algebra formula_15 – equivalently, it vanishes on the derived algebra – in terms of matrices, if formula_16 is a common eigenvector of operators formula_17 and formula_18, then formula_19 (because in both cases it is just multiplication by scalars), so common eigenvectors of an algebra must be in the set on which the algebra acts commutatively (which is annihilated by the derived algebra). Thus of central interest are the free commutative algebras, namely the polynomial algebras. In this particularly simple and important case of the polynomial algebra formula_20 in a set of commuting matrices, a weight vector of this algebra is a simultaneous eigenvector of the matrices, while a weight of this algebra is simply a formula_21-tuple of scalars formula_22 corresponding to the eigenvalue of each matrix, and hence geometrically to a point in formula_21-space. These weights – in particularly their geometry – are of central importance in understanding the representation theory of Lie algebras, specifically the finite-dimensional representations of semisimple Lie algebras.\n\nAs an application of this geometry, given an algebra that is a quotient of a polynomial algebra on formula_21 generators, it corresponds geometrically to an algebraic variety in formula_21-dimensional space, and the weight must fall on the variety – i.e., it satisfies defining equations for the variety. This generalizes the fact that eigenvalues satisfy the characteristic polynomial of a matrix in one variable.\n\n"}
{"id": "43515862", "url": "https://en.wikipedia.org/wiki?curid=43515862", "title": "Amoeba order", "text": "Amoeba order\n\nIn mathematics, the amoeba order is the partial order of open subsets of 2 of measure less than 1/2, ordered by reverse inclusion. Amoeba forcing is forcing with the amoeba order; it adds a measure 1 set of random reals. \n\nThere are several variations, where 2 is replaced by the real numbers or a real vector space or the unit interval, and the number 1/2 is replaced by some positive number \"ε\".\n\nThe name \"amoeba order\" come from the fact that a subset in the amoeba order can \"engulf\" a measure zero set by extending a \"pseudopod\" to form a larger subset in the order containing this measure zero set, which is analogous to the way an amoeba eats food.\n\nThe amoeba order satisfies the countable chain condition.\n"}
{"id": "34808785", "url": "https://en.wikipedia.org/wiki?curid=34808785", "title": "Amos Fiat", "text": "Amos Fiat\n\nAmos Fiat (born December 1, 1956 in Haifa, Israel) is an Israeli computer scientist, a professor of computer science at Tel Aviv University. He is known for his work in cryptography, online algorithms, and algorithmic game theory.\n\nFiat earned his Ph.D. in 1987 from the Weizmann Institute of Science under the supervision of Adi Shamir. After postdoctoral studies with Richard Karp and Manuel Blum at the University of California, Berkeley, he returned to Israel, taking a faculty position at Tel Aviv University.\n\nMany of Fiat's most highly cited publications concern cryptography, including his work with Adi Shamir on digital signatures (leading to the Fiat–Shamir heuristic for turning interactive identification protocols into signature schemes).\nand his work with David Chaum and Moni Naor on electronic money, used as the basis for the ecash system.\nWith Shamir and Uriel Feige in 1988, Fiat invented the Feige–Fiat–Shamir identification scheme, a method for using public-key cryptography to provide challenge-response authentication.\n\nWith Gerhard Woeginger, Fiat organized a series of Dagstuhl workshops on competitive analysis of online algorithms, and together with Woeginger he edited the book \"Online Algorithms: The State of the Art\" (Lecture Notes in Computer Science 1442, Springer-Verlag, 1998). His research papers include methods for applying competitive analysis to paging,\ncall control,\ndata management,\nand the assignment of files to servers in distributed file systems.\n\nFiat's interest in game theory stretches back to his thesis research, which included analysis of the children's game Battleship.\nHe has taken inspiration from the game Tetris in developing new job shop scheduling algorithms,\nas well as applying competitive analysis to the design of game-theoretic auctions.\n\n"}
{"id": "22912777", "url": "https://en.wikipedia.org/wiki?curid=22912777", "title": "András Frank", "text": "András Frank\n\nAndrás Frank (born 3 June 1949) is a Hungarian mathematician, working in combinatorics, especially in graph theory, and combinatorial optimisation. He is director of the Institute of Mathematics of the Eötvös Loránd University, Budapest. \n\nUsing the LLL-algorithm, Frank, and his student, Éva Tardos developed a general method, which could transform some polynomial-time algorithms into strongly polynomial. He solved the problem of finding the minimum number of edges to be added to a given undirected graph so that in the resulting graph the edge-connectivity between any two vertices \"u\" and \"v\" is at least a predetermined number \"f\"(\"u\",\"v\").\n\nHe received the Candidate of Mathematical Science degree in 1980, advisor: László Lovász, and the Doctor of Mathematical Science degree (1990) from the Hungarian Academy of Sciences. He was awarded the Tibor Szele Prize of the János Bolyai Mathematical Society in 2002 and the Albert Szent-Györgyi Prize in 2009. In June 2009 the ELTE Mathematical Institute sponsored a workshop in honor of his 60th birthday.\n\n"}
{"id": "43337378", "url": "https://en.wikipedia.org/wiki?curid=43337378", "title": "Augmentation (algebra)", "text": "Augmentation (algebra)\n\nIn algebra, an augmentation of an associative algebra \"A\" over a commutative ring \"k\" is a \"k\"-algebra homomorphism formula_1, typically denoted by ε. An algebra together with an augmentation is called an augmented algebra. The kernel of the augmentation is a two-sided ideal called the augmentation ideal of \"A\".\n\nFor example, if formula_2 is the group algebra of a group \"G\", then\nis an augmentation. \n\nIf \"A\" is a graded algebra which is connected, i.e. formula_4, then the homomorphism formula_5 which maps an element to its homogeneous component of degree 0 is an augmentation. For example,\nis an augmentation on the polynomial ring formula_7.\n"}
{"id": "47206561", "url": "https://en.wikipedia.org/wiki?curid=47206561", "title": "Awele Maduemezia", "text": "Awele Maduemezia\n\nAwele Maduemezia is a Nigerian Professor of Physics . He was born in 1934. He was former Vice Chancellor of Ambrose Alli University.\n. He also served as president of the Nigerian Association of Mathematical Physics. \nIn 1995, he was elected as President of the Nigerian Academy of Science to succeed Professor Anthony Afolabi Adegbola. He died on 20 February 2018.\n"}
{"id": "16083822", "url": "https://en.wikipedia.org/wiki?curid=16083822", "title": "Baily–Borel compactification", "text": "Baily–Borel compactification\n\nIn mathematics, the Baily–Borel compactification is a compactification of a quotient of a Hermitian symmetric space by an arithmetic group, introduced by .\n\n\n\n"}
{"id": "3988777", "url": "https://en.wikipedia.org/wiki?curid=3988777", "title": "Bochner's formula", "text": "Bochner's formula\n\nIn mathematics, Bochner's formula is a statement relating harmonic functions on a Riemannian manifold formula_1 to the Ricci curvature. The formula is named after the American mathematician Salomon Bochner.\n\nIf formula_2 is a smooth function, then\nwhere formula_4 is the gradient of formula_5 with respect to formula_6 and formula_7 is the Ricci curvature tensor. If formula_8 is harmonic (i.e., formula_9, where formula_10 is the Laplacian with respect to the metric formula_11), Bochner's formula becomes\nBochner used this formula to prove the Bochner vanishing theorem.\n\nAs a corollary, if formula_1 is a Riemannian manifold without boundary and formula_2 is a smooth, compactly supported function, then\nThis immediately follows from the first identity, observing that the integral of the left-hand side vanishes (by the divergence theorem) and integrating by parts the first term on the right-hand side.\n\n"}
{"id": "39536084", "url": "https://en.wikipedia.org/wiki?curid=39536084", "title": "Bruce Sagan", "text": "Bruce Sagan\n\nBruce E. Sagan (born March 29, 1954, Chicago, Illinois) is a Professor of Mathematics at Michigan State University. He specializes in enumerative, algebraic, and topological combinatorics. He is also known as a musician, playing music from Scandinavia and the Balkans.\n\nBruce Eli Sagan is the son of Eugene Benjamin Sagan and Arlene Kaufmann Sagan. He grew up in Berkeley, California. He started playing classical violin at a young age under the influence of his mother who was a music teacher and conductor. He received his B.S. in mathematics (1974) from California State University, East Bay (then called California State University, Hayward). He received his Ph.D. in mathematics (1979) from M.I.T. His doctoral thesis \"Partially Ordered Sets with Hooklengths - an Algorithmic Approach\" was supervised by Richard P. Stanley. He was Stanley's third doctoral student. During his graduate school years he also joined and became music director of the Mandala Folkdance Ensemble.\n\nSagan held postdoctoral positions at Université Louis Pasteur (1979–1980), the University of Michigan (1980–1983), University College of Wales, Aberystwyth, Middlebury College (1984–1985), the University of Pennsylvania, and Université du Québec à Montréal (Fall, 1985), before becoming a faculty member at MSU in the Spring of 1986. He has held visiting positions at the Institute for Mathematics and its Applications (Spring, 1988), UCSD (Spring, 1991), the Royal Institute of Technology (1993–1994), MSRI (Winter, 1997), the Isaac Newton Institute (Winter, 2001), Mittag-Leffler Institute (Spring, 2005), and DIMACS (2005–2006). He was also a rotating Program Officer at the National Science Foundation (2007–2010).\n\nSagan has published almost 100 research papers. He has given over 200 talks in North America, Europe, Asia, and Australia. These have included keynote addresses at the Conference on Formal Power Series and Algebraic Combinatorics (2006) and the British Combinatorial Conference (2011). He has graduated 9 Ph.D. students. During his time at Michigan State University, he has won two awards for teaching excellence.\n\nSagan has been an Editor-in-Chief for the Electronic Journal of Combinatorics since 2004.\n\n\n\nSagan plays music from the Scandinavian countries and the Balkans on fiddle and native instruments. These include the Swedish nyckelharpa, the Norwegian hardingfele, and the Bulgarian gadulka. In 1985 he and his then wife, Judy Barlas, founded the music and dance camp Scandinavian Week at Buffalo Gap (now known as Nordic Fiddles and Feet). He is currently a regular staff member at Northern Week at Ashokan run by Jay Ungar and Molly Mason. In 1994 he was awarded the Zorn Medal in Bronze for his playing in front of a jury of Swedish musicians. He has performed and given workshops in North America, Europe, and Australia. He plays Swedish music as a duo with Brad Battey and also with lydia ievins. His trio Veselba, with Nan Nelson and Chris Rietz, performs music from Bulgaria.\n\n\n"}
{"id": "33503735", "url": "https://en.wikipedia.org/wiki?curid=33503735", "title": "CSS code", "text": "CSS code\n\nIn quantum error correction, CSS codes, named after their inventors, Robert Calderbank, Peter Shor and Andrew Steane, are a special type of Stabilizer codes constructed from classical codes with some special properties.\n\nLet formula_1 and formula_2 be two (classical) formula_3, formula_4 codes such, that formula_5 and formula_6 both have minimal distance formula_7, where formula_8 is the code dual to formula_9. Then define formula_10, the CSS code of formula_11 over formula_9 as an formula_13 code, with formula_14 as follows: \n\nDefine for formula_15 formula_16 formula_17, where formula_18 is bitwise addition modulo 2. Then formula_19 is defined as formula_20.\n"}
{"id": "15646344", "url": "https://en.wikipedia.org/wiki?curid=15646344", "title": "Chernoff's distribution", "text": "Chernoff's distribution\n\nIn probability theory, Chernoff's distribution, named after Herman Chernoff, is the probability distribution of the random variable\n\nwhere \"W\" is a \"two-sided\" Wiener process (or two-sided \"Brownian motion\") satisfying \"W\"(0) = 0. \nIf\n\nthen \"V\"(0, \"c\") has density\n\nwhere \"g\" has Fourier transform given by\n\nand where Ai is the Airy function. Thus \"f\" is symmetric about 0 and the density \"ƒ\" = \"ƒ\". Groeneboom (1989) shows that\n\nwhere formula_6 is the largest zero of the Airy function Ai and where formula_7.\n\n"}
{"id": "6808344", "url": "https://en.wikipedia.org/wiki?curid=6808344", "title": "Circulation problem", "text": "Circulation problem\n\nThe circulation problem and its variants are a generalisation of network flow problems, with the added constraint of a lower bound on edge flows, and with flow conservation also being required for the source and sink (i.e. there are no special nodes). In variants of the problem, there are multiple commodities flowing through the network, and a cost on the flow.\n\nGiven flow network formula_1 with:\n\nand the constraints:\n\nFinding a flow assignment satisfying the constraints gives a solution to the given circulation problem. \n\nIn the minimum cost variant of the problem, minimize\n\nIn a multi-commodity circulation problem, you also need to keep track of the flow of the individual commodities:\n\nThere is also a lower bound on each flow of commodity.\n\nThe conservation constraint must be upheld individually for the commodities: \n\nFor the circulation problem, many polynomial algorithms have been developed (e.g., Edmonds and Karp algorithm, 1972; Tarjan 1987-1988). Tardos found the first strongly polynomial algorithm.\n\nFor the case of multiple commodities, the problem is NP-complete for integer flows. For fractional flows, it is solvable in polynomial time, as one can formulate the problem as a linear program.\n\nBelow are given some problems, and how to solve them with the general circulation setup given above.\n\n"}
{"id": "363523", "url": "https://en.wikipedia.org/wiki?curid=363523", "title": "Colombeau algebra", "text": "Colombeau algebra\n\nIn mathematics, a Colombeau algebra is an algebra of a certain kind containing the space of Schwartz distributions. While in classical distribution theory a general multiplication of distributions is not possible, Colombeau algebras provide a rigorous framework for this.\n\nSuch a multiplication of distributions has long been believed to be impossible because of L. Schwartz' impossibility result, which basically states that there cannot be a differential algebra containing the space of distributions and preserving the product of continuous functions. However, if one only wants to preserve the product of smooth functions instead such a construction becomes possible, as demonstrated first by Colombeau.\n\nAs a mathematical tool, Colombeau algebras can be said to combine a treatment of singularities, differentiation and nonlinear operations in one framework, lifting the limitations of distribution theory. These algebras have found numerous applications in the fields of partial differential equations, geophysics, microlocal analysis and general relativity so far.\n\nAttempting to embed the space formula_1 of distributions on formula_2 into an associative algebra formula_3, the following requirements seem to be natural:\n\n\nHowever, L. Schwartz' result implies that these requirements cannot hold simultaneously. The same is true even if, in 4., one replaces formula_14 by formula_15, the space of formula_16 times continuously differentiable functions. While this result has often been interpreted as saying that a general multiplication of distributions is not possible, in fact it only states that one cannot unrestrictedly combine differentiation, multiplication of continuous functions and the presence of singular objects like the Dirac delta.\n\nColombeau algebras are constructed to satisfy conditions 1.–3. and a condition like 4., but with formula_13 replaced by formula_18, i.e., they preserve the product of smooth (infinitely differentiable) functions only.\n\nIt is defined as a quotient algebra\n\nHere the \"moderate functions\" on formula_20 are defined as\n\nwhich are families (\"f\") of smooth functions on formula_20 such that\n\n(where R = (0,∞)) is the set of \"regularization\" indices, and for all compact subsets \"K\" of formula_20 and multiindices α we have \"N\" > 0 such that\n\nThe ideal \nformula_26\nof \"negligible functions\" is defined in the same way but with the partial derivatives instead bounded by O(\"ε\") for all \"N\" > 0.\n\nAn introduction to Colombeau Algebras is given in here \nThe space(s) of Schwartz distributions can be embedded into this \"simplified\" algebra by (component-wise) convolution with any element of the algebra having as representative a \"δ-net\", i.e. such that formula_27 in \" D' \" as ε→0.\n\nThis embedding is non-canonical, because it depends on the choice of the δ-net. However, there are versions of Colombeau algebras (so called \"full\" algebras) which allow for canonical embeddings of distributions. A well known \"full\" version is obtained by adding the mollifiers as second indexing set.\n\n\n"}
{"id": "5829819", "url": "https://en.wikipedia.org/wiki?curid=5829819", "title": "Configurable modularity", "text": "Configurable modularity\n\nConfigurable modularity is a term coined by Raoul de Campo of IBM Research and later expanded on by Nate Edwards of the same organization, denoting the ability to reuse independent components by changing their interconnections, but not their internals. In Edwards' view this characterizes all successful reuse systems, and indeed all systems which can be described as \"engineered\".\n\n"}
{"id": "9448373", "url": "https://en.wikipedia.org/wiki?curid=9448373", "title": "D-Wave Systems", "text": "D-Wave Systems\n\nD-Wave Systems, Inc. is a quantum computing company, based in Burnaby, British Columbia, Canada. D-Wave is the world's first company to sell computers which exploit quantum effects in their operation. (Whether these qualify as \"quantum computers\" in the sense generally understood is hotly disputed.)\n\nThe D-Wave One was built on early prototypes such as D-Wave's Orion Quantum Computer. The prototype was a 16-qubit quantum annealing processor, demonstrated on February 13, 2007, at the Computer History Museum in Mountain View, California. D-Wave demonstrated what they claimed to be a 28-qubit quantum annealing processor on November 12, 2007. The chip was fabricated at the NASA Jet Propulsion Laboratory Microdevices Lab in Pasadena, California.\n\nThe underlying ideas for the D-Wave approach arose from experimental results in condensed matter physics, and in particular work on quantum annealing in magnets performed by Dr. Gabriel Aeppli. These ideas were later recast in the language of quantum computation by MIT physicists Ed Farhi, Seth Lloyd, Terry Orlando and Bill Kaminsky, whose publications in 2000 and 2004 provided both a theoretical model for quantum computation that fit with the earlier work in quantum magnetism (specifically the adiabatic quantum computing model and quantum annealing, its finite temperature variant), and a specific enablement of that idea using superconducting flux qubits which is a close cousin to the designs D-Wave produced. In order to understand the origins of much of the controversy around the D-Wave approach, it is important to note that the origins of the D-Wave approach to quantum computation arose not from the conventional quantum information field, but from experimental condensed matter physics.\n\nOn May 11, 2011, D-Wave Systems announced D-Wave One, described as \"the world's first commercially available quantum computer\", operating on a 128-qubit chipset using quantum annealing (a general method for finding the global minimum of a function by a process using quantum fluctuations) to solve optimization problems. In May 2013, a collaboration between NASA, Google and the Universities Space Research Association (USRA) launched a Quantum Artificial Intelligence Lab based on the D-Wave Two 512-qubit quantum computer that would be used for research into machine learning, among other fields of study.\n\nOn August 20, 2015, D-Wave Systems announced the general availability of the D-Wave 2X system, a 1000+ qubit quantum computer. This was followed by an announcement on September 28, 2015, that it had been installed at the Quantum Artificial Intelligence Lab at NASA Ames Research Center.\n\nIn January 2017, D-Wave has released the D-Wave 2000Q and Qbsolv. \"Qbsolv\" is a piece of open-source software that solves QUBO problems on both company's quantum processors and classic hardware architectures.\n\nD-Wave was founded by Haig Farris (former chair of board), Geordie Rose (CTO and former CEO), Bob Wiens (former CFO), and Alexandre Zagoskin (former VP Research and Chief Scientist). Farris taught a business course at the University of British Columbia (UBC), where Rose obtained his Ph.D., and Zagoskin was a postdoctoral fellow. The company name refers to their first qubit designs, which used d-wave superconductors.\n\nD-Wave operated as an offshoot from UBC, while maintaining ties with the Department of Physics and Astronomy. It funded academic research in quantum computing, thus building a collaborative network of research scientists. The company collaborated with several universities and institutions, including UBC, IPHT Jena, Université de Sherbrooke, University of Toronto, University of Twente, Chalmers University of Technology, University of Erlangen, and Jet Propulsion Laboratory. These partnerships were listed on D-Wave's website until 2005. In June 2014, D-Wave announced a new quantum applications ecosystem with computational finance firm 1QB Information Technologies (1QBit) and cancer research group DNA-SEQ to focus on solving real-world problems with quantum hardware.\n\nD-Wave operated from various locations in Vancouver, British Columbia, and laboratory spaces at UBC before moving to its current location in the neighboring suburb of Burnaby. D-Wave also has offices in Palo Alto and Vienna, USA.\n\nThe first commercially produced D-Wave processor was a programmable, superconducting integrated circuit with up to 128 pair-wise coupled superconducting flux qubits. The 128-qubit processor was superseded by a 512-qubit processor in 2013. The processor is designed to implement a special-purpose quantum annealing as opposed to being operated as a universal gate-model quantum computer.\n\nD-Wave maintains a list of peer-reviewed technical publications by their own scientists and others on their website.\n\nOn February 13, 2007, D-Wave demonstrated the Orion system, running three different applications at the Computer History Museum in Mountain View, California. This marked the first public demonstration of, supposedly, a quantum computer and associated service.\n\nThe first application, an example of pattern matching, performed a search for a similar compound to a known drug within a database of molecules. The next application computed a seating arrangement for an event subject to compatibilities and incompatibilities between guests. The last involved solving a Sudoku puzzle.\n\nThe processors at the heart of D-Wave's \"Orion quantum computing system\" are designed for use as hardware accelerator processors rather than general-purpose computer microprocessors. The system is designed to solve a particular NP-complete problem related to the two dimensional Ising model in a magnetic field. D-Wave terms the device a 16-qubit superconducting adiabatic quantum computer processor.\n\nAccording to the company, a conventional front end running an application that requires the solution of an NP-complete problem, such as pattern matching, passes the problem to the Orion system.\n\nAccording to Geordie Rose, founder and Chief Technology Officer of D-Wave, NP-complete problems \"are probably not exactly solvable, no matter how big, fast or advanced computers get\"; the adiabatic quantum computer used by the Orion system is intended to quickly compute an approximate solution.\n\nOn December 8, 2009, at the Neural Information Processing Systems (NIPS) conference, a Google research team led by Hartmut Neven used D-Wave's processor to train a binary image classifier.\n\nOn May 11, 2011, D-Wave Systems announced the D-Wave One, an integrated quantum computer system running on a 128-qubit processor. The processor used in the D-Wave One code-named \"Rainier\", performs a single mathematical operation, discrete optimization. Rainier uses quantum annealing to solve optimization problems. The D-Wave One is claimed to be the world's first commercially available quantum computer system. \nThe price is approximately US$10,000,000.\n\nA research team led by Matthias Troyer and Daniel Lidar found that, while there is evidence of quantum annealing in D-Wave One, they saw no speed increase compared to classical computers. They implemented an optimized classical algorithm to solve the same particular problem as the D-Wave One.\n\nOn May 25, 2011, Lockheed Martin signed a multi-year contract with D-Wave Systems to realize the benefits based upon a quantum annealing processor applied to some of Lockheed's most challenging computation problems. The contract included purchase of the D-Wave One quantum computer, maintenance, and associated professional services.\n\nIn August 2012, a team of Harvard University researchers presented results of the largest protein-folding problem solved to date using a quantum computer. The researchers solved instances of a lattice protein folding model, known as the Miyazawa–Jernigan model, on a D-Wave One quantum computer.\n\nIn early 2012, D-Wave Systems revealed a 512-qubit quantum computer, code-named \"Vesuvius\", which was launched as a production processor in 2013.\n\nIn May 2013, Catherine McGeoch, a consultant for D-Wave, published the first comparison of the technology against regular top-end desktop computers running an optimization algorithm. Using a configuration with 439 qubits, the system performed 3,600 times as fast as CPLEX, the best algorithm on the conventional machine, solving problems with 100 or more variables in half a second compared with half an hour. The results are presented at the Computing Frontiers 2013 conference.\n\nIn March 2013 several groups of researchers at the Adiabatic Quantum Computing workshop at the Institute of Physics in London produced evidence, though only indirect, of quantum entanglement in the D-Wave chips.\n\nIn May 2013 it was announced that a collaboration between NASA, Google and the USRA launched a Quantum Artificial Intelligence Lab at the NASA Advanced Supercomputing Division at Ames Research Center in California, using a 512-qubit D-Wave Two that would be used for research into machine learning, among other fields of study.\n\nOn August 20, 2015, D-Wave released general availability of their D-Wave 2X computer, with 1,000 qubits in a Chimera graph architecture (although, due to magnetic offsets and manufacturing variability inherent in the superconductor circuit fabrication fewer than 1,152 qubits are functional and available for use. The exact number of qubits yielded will vary with each specific processor manufactured.) This was accompanied by a report comparing speeds with high-end single threaded CPUs. Unlike previous reports, this one explicitly stated that question of quantum speedup was not something they were trying to address, and focused on constant-factor performance gains over classical hardware. For general-purpose problems, a speedup of 15x was reported, but it is worth noting that these classical algorithms benefit efficiently from parallelization—so that the computer would be performing on par with, perhaps, 30 high-end single-threaded cores.\n\nThe D-Wave 2X processor is based on a 2,048-qubit chip with half of the qubits disabled; these were activated in the D-Wave 2000Q.\n\nIn 2007 Umesh Vazirani, a professor at University of California (UC) Berkeley, made the following criticism:\n\nWim van Dam, a professor at UC Santa Barbara, summarized his opinion as of 2008 in the journal \"Nature Physics\": \"At the moment it is impossible to say if D-Wave's quantum computer is intrinsically equivalent to a classical computer or not. So until more is known about their error rates, \"caveat emptor\" is the least one can say\".\n\nAn article in the May 12, 2011, edition of \"Nature\" gives details which critical academics say proves that the company's chips do have some of the quantum mechanical properties needed for quantum computing. Prior to the 2011 \"Nature\" paper, D-Wave was criticized for lacking proof that its computer was in fact a quantum computer. Nevertheless, questions were raised and later answered regarding experimental proof of quantum entanglement inside D-Wave devices.\n\nFormer MIT professor Scott Aaronson, who has described himself as \"Chief D-Wave Skeptic\", said that D-Wave's 2007 demonstration did not prove anything about the workings of the Orion computer, and that its marketing claims were deceptive. In May 2011 he said that he was \"retiring as Chief D-Wave Skeptic\", and reporting his \"skeptical but positive\" views based on a visit to D-Wave in February 2012. Aaronson said that one of the most important reasons for his new position on D-Wave was the 2011 Nature article. In May 16, 2013 he resumed his skeptic post. He criticizes D-Wave for blowing results out of proportion on press releases that claim speedups of three orders of magnitude, in light of a paper by scientists from ETH Zurich reporting a 128-qubit D-Wave computer being outperformed by a factor of 15 using regular digital computers and applying classical metaheuristics (particularly simulated annealing) to the problem that D-Wave's computer was specifically designed to solve.\n\nOn May 16, 2013, NASA and Google, together with a consortium of universities, announced a partnership with D-Wave to investigate how D-Wave's computers could be used in the creation of artificial intelligence. Prior to announcing this partnership, NASA, Google, and Universities Space Research Association put a D-Wave computer through a series of benchmark and acceptance tests, which it passed. Independent researchers found that D-Wave's computers could solve some problems as much as 3,600 times faster than particular software packages running on conventional digital computers. Other independent researchers found that different software packages running on a single core of a desktop computer can solve those same problems as fast or faster than D-Wave's computers (at least 12,000 times faster for quadratic assignment problems, and between 1 and 50 times faster for quadratic unconstrained binary optimization problems).\n\nIn January 2014, researchers at UC Berkeley and IBM published a classical model reproducing the D-Wave machine's observed behavior, suggesting that it may not be a quantum computer.\n\nIn March 2014, researchers at University College London and the University of Southern California (USC) published a paper comparing data obtained from a D-Wave Two computer with three possible explanations from classical physics and one quantum model. They found that their quantum model was a better fit to the experimental data than the Shin–Smith–Smolin–Vazirani classical model, and a much better fit than any of the other classical models. The authors conclude that \"This suggests that an open system quantum dynamical description of the D-Wave device is well-justified even in the presence of relevant thermal excitations and fast single-qubit decoherence.\"\n\nIn May 2014, researchers at D-Wave, Google, USC, Simon Fraser University, and National Research Tomsk Polytechnic University published a paper containing experimental results that demonstrated the presence of entanglement among D-Wave qubits. Qubit tunneling spectroscopy was used to measure the energy eigenspectrum of two and eight-qubit systems, demonstrating their coherence during a critical portion of the quantum annealing procedure.\n\nA study published in \"Science\" in June 2014, described as \"likely the most thorough and precise study that has been done on the performance of the D-Wave machine\" and \"the fairest comparison yet\", attempted to define and measure quantum speedup. Several definitions were put forward as some may be unverifiable by empirical tests, while others, though falsified, would nonetheless allow for the existence of performance advantages. The researchers, led by Matthias Troyer at the Swiss Federal Institute of Technology in Zurich, said that they found \"no quantum speedup\" across the entire range of their tests, and only inconclusive results when looking at subsets of the tests. Further work has advanced understanding of these test metrics and their reliance on equilibrated systems.\nThere remain open questions regarding quantum speedup. The ETH reference in the previous section is just for one class of benchmark problems. Potentially\nthere may be other classes of problems where quantum speedup might occur. Researchers at Google, NASA, LANL, USC, Texas\nA&M, and D-Wave are working to find such problem classes.\n\n\n\n"}
{"id": "48712163", "url": "https://en.wikipedia.org/wiki?curid=48712163", "title": "David Suter", "text": "David Suter\n\nDavid Suter (born 1949) is an American artist known for his many years producing editorial illustrations for clients such as \"The Washington Post\", \"Time\", and \"The New York Times\". Known as \"Suterisms\" or \"visual koans\", his illustrations are notable for their use of bistable perception, in which Suter combines multiple images and concepts into a single image. Suter is also an accomplished fine art painter and sculptor.\n\nSuter grew up in Bethesda, Maryland, the son of Richard Sturgis Suter, who worked in the CIA, and Angela Phillips Suter, an artist. He was influenced early on by the mathematically inspired work of M. C. Escher, but never had any formal art training.\n\nSuter attended a number of different colleges, not graduating from any of them. Drafted into the Army during the Vietnam War, he spent his deployment in West Germany. t\nUpon returning to the U.S., Suter got work at \"The Washington Post\" as an illustrator, for a while working as a courtroom artist during the Watergate scandal trials.\n\nSuter was awarded a Michigan Journalism Fellowship in 1977, where he spent a year studying fine art, philosophy, and history at the University of Michigan. Upon completion of the fellowship, in 1978, Suter moved to New York City to pursue editorial illustration full-time. He quickly become sought-after by such publications as \"The New York Times\" (both on the op-ed page and the book review), \"Time\" magazine (for whom he did a number of covers), \"Harper's Magazine\", and the \"Chicago Tribune\".\n\nIn a mid-1980s magazine profile, Suter described his work this way:\nFor many years, Suter has been working on creating a full-length animation of the complete text of Shakespeare's \"Hamlet\".\n\nIn the late 2000s he retired from editorial illustration to work full-time on his painting and sculpture practices.\n\nThe first exhibition of Suter's fine art — which he terms \"Constructivist Expressionist\" — was in 1996 at the Morgan Rank Gallery in East Hampton, New York. He is represented by Gallery A and Alex Gallery in Washington, D.C.\n\nIn 2011, he was arrested and detained in Serbia while transporting his paintings from France to Romania for a gallery show in Bucharest.\n\nSuter has four daughters: Valerie, Georgia, Charlotte and Olivia.\n\n\n"}
{"id": "539925", "url": "https://en.wikipedia.org/wiki?curid=539925", "title": "Duality (projective geometry)", "text": "Duality (projective geometry)\n\nIn geometry, a striking feature of projective planes is the symmetry of the roles played by points and lines in the definitions and theorems, and (plane) duality is the formalization of this concept. There are two approaches to the subject of duality, one through language () and the other a more functional approach through special mappings. These are completely equivalent and either treatment has as its starting point the axiomatic version of the geometries under consideration. In the functional approach there is a map between related geometries that is called a duality. Such a map can be constructed in many ways. The concept of plane duality readily extends to space duality and beyond that to duality in any finite-dimensional projective geometry.\n\nA projective plane may be defined axiomatically as an incidence structure, in terms of a set of \"points\", a set of \"lines\", and an incidence relation that determines which points lie on which lines. These sets can be used to define a plane dual structure.\n\nInterchange the role of \"points\" and \"lines\" in \nto obtain the \"dual structure\"\nwhere is the inverse relation of . is also a projective plane, called the dual plane of .\n\nIf and are isomorphic, then is called self-dual. The projective planes for any field (or, more generally, for every division ring(skewfield) isomorphic to its dual) are self-dual. In particular, Desarguesian planes of finite order are always self-dual. However, there are non-Desarguesian planes which are not self-dual, such as the Hall planes and some that are, such as the Hughes planes.\n\nIn a projective plane a statement involving points, lines and incidence between them that is obtained from another such statement by interchanging the words \"point\" and \"line\" and making whatever grammatical adjustments that are necessary, is called the plane dual statement of the first. The plane dual statement of \"Two points are on a unique line\" is \"Two lines meet at a unique point\". Forming the plane dual of a statement is known as \"dualizing\" the statement.\n\nIf a statement is true in a projective plane , then the plane dual of that statement must be true in the dual plane . This follows since dualizing each statement in the proof \"in \" gives a corresponding statement of the proof \"in \".\n\nThe principle of plane duality says that dualizing any theorem in a self-dual projective plane produces another theorem valid in .\n\nThe above concepts can be generalized to talk about space duality, where the terms \"points\" and \"planes\" are interchanged (and lines remain lines). This leads to the \"principle of space duality\".\n\nThese principles provide a good reason for preferring to use a \"symmetric\" term for the incidence relation. Thus instead of saying \"a point lies on a line\" one should say \"a point is incident with a line\" since dualizing the latter only involves interchanging point and line (\"a line is incident with a point\").\n\nThe validity of the principle of plane duality follows from the axiomatic definition of a projective plane. The three axioms of this definition can be written so that they are self-dual statements implying that the dual of a projective plane is also a projective plane. The dual of a true statement in a projective plane is therefore a true statement in the dual projective plane and the implication is that for self-dual planes, the dual of a true statement in that plane is also a true statement in that plane.\n\nAs the real projective plane, , is self-dual there are a number of pairs of well known results that are duals of each other. Some of these are:\n\n\nNot only statements, but also systems of points and lines can be dualized.\n\nA set of points and lines is called an \"configuration\" if of the lines pass through each point and of the points lie on each line. The dual of an configuration, is an configuration. Thus, the dual of a quadrangle, a (4, 6) configuration of four points and six lines, is a quadrilateral, a (6, 4) configuration of six points and four lines.\n\nThe set of all points on a line, called a projective range has as its dual a pencil of lines, the set of all lines on a point.\n\nA plane duality is a map from a projective plane to its \"dual plane\" (see above) which preserves incidence. That is, a plane duality will map points to lines and lines to points ( and ) in such a way that if a point is on a line (denoted by ) then . A plane duality which is an isomorphism is called a \"correlation\". The existence of a correlation means that the projective plane is \"self-dual\".\n\nThe projective plane in this definition need not be a Desarguesian plane. However, if it is, that is, with a division ring (skewfield), then a duality, as defined below for general projective spaces, gives a plane duality on that satisfies the above definition.\n\nA duality of a projective space is a permutation of the subspaces of (also denoted by with a field (or more generally a skewfield (division ring)) that reverses inclusion, that is:\nConsequently, a duality interchanges objects of dimension with objects of dimension ( = codimension ). That is, in a projective space of dimension , the points (dimension 0) correspond to hyperplanes (codimension 1), the lines joining two points (dimension 1) correspond to the intersection of two hyperplanes (codimension 2), and so on.\n\nThe \"dual\" of a finite-dimensional (right) vector space over a skewfield can be regarded as a (right) vector space of the same dimension over the opposite skewfield . There is thus an inclusion-reversing bijection between the projective spaces and . If and are isomorphic then there exists a duality on . Conversely, if admits a duality for , then and are isomorphic.\n\nLet be a duality of for . If is composed with the natural isomorphism between and , the composition is an incidence preserving bijection between and . By the Fundamental theorem of projective geometry is induced by a semilinear map with associated isomorphism , which can be viewed as an antiautomorphism of . In the classical literature, would be called a reciprocity in general, and if it would be called a correlation (and would necessarily be a field). Some authors suppress the role of the natural isomorphism and call a duality. When this is done, a duality may be thought of as a collineation between a pair of specially related projective spaces and called a reciprocity. If this collineation is a projectivity then it is called a correlation.\n\nLet denote the linear functional of associated with the vector in . Define the form by:\n\nAny duality of for is induced by a nondegenerate sesquilinear form on the underlying vector space (with a companion antiautomorphism) and conversely.\n\nHomogeneous coordinates may be used to give an algebraic description of dualities. To simplify this discussion we shall assume that is a field, but everything can be done in the same way when is a skewfield as long as attention is paid to the fact that multiplication need not be a commutative operation.\n\nThe points of can be taken to be the nonzero vectors in the ()-dimensional vector space over , where we identify two vectors which differ by a scalar factor. Another way to put it is that the points of -dimensional projective space are the 1-dimensional vector subspaces, which may be visualized as the lines through the origin in . Also the - (vector) dimensional subspaces of represent the ()- (geometric) dimensional hyperplanes of projective -space over , i.e., .\n\nA nonzero vector in also determines an - geometric dimensional subspace (hyperplane) , by \nWhen a vector is used to define a hyperplane in this way it shall be denoted by , while if it is designating a point we will use . They are referred to as \"point coordinates\" or \"hyperplane coordinates\" respectively (in the important two-dimensional case, hyperplane coordinates are called \"line coordinates\"). Some authors distinguish how a vector is to be interpreted by writing hyperplane coordinates as horizontal (row) vectors while point coordinates are written as vertical (column) vectors. Thus, if is a column vector we would have while . In terms of the usual dot product, . Since is a field, the dot product is symmetrical, meaning .\n\nA simple reciprocity (actually a correlation) can be given by between points and hyperplanes. This extends to a reciprocity between the line generated by two points and the intersection of two such hyperplanes, and so forth.\n\nSpecifically, in the projective plane, , with a field, we have the correlation given by: points in homogeneous coordinates lines with equations . In a projective space, , a correlation is given by: points in homogeneous coordinates planes with equations . This correlation would also map a line determined by two points and to the line which is the intersection of the two planes with equations and .\n\nThe associated sesquilinear form for this correlation is:\nwhere the companion antiautomorphism . This is therefore a bilinear form (note that must be a field). This can be written in matrix form (with respect to the standard basis) as:\nwhere is the identity matrix, using the convention that is a row vector and is a column vector.\n\nThe correlation is given by:\n\nThis correlation in the case of can be described geometrically using the model of the real projective plane which is a \"unit sphere with antipodes identified\", or equivalently, the model of lines and planes through the origin of the vector space . Associate to any line through the origin the unique plane through the origin which is perpendicular (orthogonal) to the line. When, in the model, these lines are considered to be the points and the planes the lines of the projective plane , this association becomes a correlation (actually a polarity) of the projective plane. The sphere model is obtained by intersecting the lines and planes through the origin with a unit sphere centered at the origin. The lines meet the sphere in antipodal points which must then be identified to obtain a point of the projective plane, and the planes meet the sphere in great circles which are thus the lines of the projective plane.\n\nThat this association \"preserves\" incidence is most easily seen from the lines and planes model. A point incident with a line in the projective plane corresponds to a line through the origin lying in a plane through the origin in the model. Applying the association, the plane becomes a line through the origin perpendicular to the plane it is associated with. This image line is perpendicular to every line of the plane which passes through the origin, in particular the original line (point of the projective plane). All lines that are perpendicular to the original line at the origin lie in the unique plane which is orthogonal to the original line, that is, the image plane under the association. Thus, the image line lies in the image plane and the association preserves incidence.\n\nAs in the above example, matrices can be used to represent dualities. Let be a duality of for and let be the associated sesquilinear form (with companion antiautomorphism ) on the underlying ()-dimensional vector space . Given a basis of , we may represent this form by:\nwhere is a nonsingular matrix over and the vectors are written as column vectors. The notation means that the antiautomorphism is applied to each coordinate of the vector .\n\nNow define the duality in terms of point coordinates by:\n\nA duality that is an involution (has order two) is called a polarity. It is necessary to distinguish between polarities of general projective spaces and those that arise from the slightly more general definition of plane duality. It is also possible to give more precise statements in the case of a finite geometry, so we shall emphasize the results in finite projective planes.\n\nIf is a duality of , with a skewfield, then a common notation is defined by for a subspace of . Hence, a polarity is a duality for which for every subspace of . It is also common to bypass mentioning the dual space and write, in terms of the associated sesquilinear form:\n\nA sesquilinear form is \"reflexive\" if implies .\n\nA duality is a polarity if and only if the (nondegenerate) sesquilinear form defining it is reflexive.\n\nPolarities have been classified, a result of that has been reproven several times. Let be a (left) vector space over the skewfield and be a reflexive nondegenerate sesquilinear form on with companion anti-automorphism . If is the sesquilinear form associated with a polarity then either:\n\nA point of is an absolute point (self-conjugate point) with respect to polarity if . Similarly, a hyperplane is an absolute hyperplane (self-conjugate hyperplane) if . Expressed in other terms, a point is an absolute point of polarity with associated sesquilinear form if and if is written in terms of matrix , .\n\nThe set of absolute points of each type of polarity can be described. We again restrict the discussion to the case that is a field.\n\nWhen composed with itself, the correlation (in any dimension) produces the identity function, so it is a polarity. The set of absolute points of this polarity would be the points whose homogeneous coordinates satisfy the equation:\nWhich points are in this point set depends on the field . If then the set is empty, there are no absolute points (and no absolute hyperplanes). On the other hand, if the set of absolute points form a nondegenerate quadric (a conic in two-dimensional space). If is a finite field of odd characteristic the absolute points also form a quadric, but if the characteristic is even the absolute points form a hyperplane (this is an example of a pseudo polarity).\n\nUnder any duality, the point is called the pole of the hyperplane , and this hyperplane is called the polar of the point . Using this terminology, the absolute points of a polarity are the points that are incident with their polars and the absolute hyperplanes are the hyperplanes that are incident with their poles.\n\nBy Wedderburn's theorem every finite skewfield is a field and an automorphism of order two (other than the identity) can only exist in a finite field whose order is a square. These facts help to simplify the general situation for finite Desarguesian planes. We have:\n\nIf is a polarity of the finite Desarguesian projective plane where for some prime , then the number of absolute points of is if is orthogonal or if is unitary. In the orthogonal case, the absolute points lie on a conic if is odd or form a line if . The unitary case can only occur if is a square; the absolute points and absolute lines form a unital.\n\nIn the general projective plane case where duality means \"plane duality\", the definitions of polarity, absolute elements, pole and polar remain the same.\n\nLet denote a projective plane of order . Counting arguments can establish that for a polarity of :\n\nThe number of non-absolute points (lines) incident with a non-absolute line (point) is even.\n\nFurthermore,\n\nThe polarity has at least absolute points and if is not a square, exactly absolute points. If has exactly absolute points then;\n\nAn upper bound on the number of absolute points in the case that is a square was given by Seib and a purely combinatorial argument can establish:\n\nA polarity in a projective plane of square order has at most absolute points. Furthermore, if the number of absolute points is , then the absolute points and absolute lines form a unital (i.e., every line of the plane meets this set of absolute points in either or points).\n\nA method that can be used to construct a polarity of the real projective plane has, as its starting point, a construction of a partial duality in the Euclidean plane.\n\nIn the Euclidean plane, fix a circle with center and radius . For each point other than define an image point so that . The mapping defined by is called inversion with respect to circle . The line through which is perpendicular to the line is called the polar of the point with respect to circle .\n\nLet be a line not passing through . Drop a perpendicular from to , meeting at the point (this is the point of that is closest to ). The image of under inversion with respect to is called the pole of . If a point is on a line (not passing through ) then the pole of lies on the polar of and vice versa. The incidence preserving process, in which points and lines are transformed into their polars and poles with respect to is called reciprocation.\n\nIn order to turn this process into a correlation, the Euclidean plane (which is not a projective plane) needs to be expanded to the extended euclidean plane by adding a line at infinity and points at infinity which lie on this line. In this expanded plane, we define the polar of the point to be the line at infinity (and is the pole of the line at infinity), and the poles of the lines through are the points of infinity where, if a line has slope its pole is the infinite point associated to the parallel class of lines with slope . The pole of the -axis is the point of infinity of the vertical lines and the pole of the -axis is the point of infinity of the horizontal lines.\n\nThe construction of a correlation based on inversion in a circle given above can be generalized by using inversion in a conic section (in the extended real plane). The correlations constructed in this manner are of order two, that is, polarities.\n\nWe shall describe this polarity algebraically by following the above construction in the case that is the unit circle (i.e., ) centered at the origin.\n\nAn affine point , other than the origin, with Cartesian coordinates has as its inverse in the unit circle the point with coordinates,\nThe line passing through that is perpendicular to the line has equation .\n\nSwitching to homogeneous coordinates using the embedding , the extension to the real projective plane is obtained by permitting the last coordinate to be 0. Recalling that point coordinates are written as column vectors and line coordinates as row vectors, we may express this polarity by:\n\nsuch that\n\nOr, using the alternate notation, . The matrix of the associated sesquilinear form (with respect to the standard basis) is:\n\nThe absolute points of this polarity are given by the solutions of:\n\nwhere . Note that restricted to the Euclidean plane (that is, set ) this is just the unit circle, the circle of inversion.\n\nThe theory of poles and polars of a conic in a projective plane can be developed without the use of coordinates and other metric concepts.\n\nLet be a conic in where is a field not of characteristic two, and let be a point of this plane not on . Two distinct secant lines to the conic, say and determine four points on the conic () that form a quadrangle. The point is a vertex of the diagonal triangle of this quadrangle. The \"polar\" of with respect to is the side of the diagonal triangle opposite .\n\nThe theory of harmonic conjugates of points on a line can also be used to define this relationship. Using the same notation as above;\n\nIf a variable line through the point is a secant of the conic , the harmonic conjugates of with respect to the two points of on the secant all lie on the \"polar\" of .\n\nThere are several properties that polarities in a projective plane have.\n\nGiven a polarity , a point lies on line , the polar of point if and only if lies on , the polar of .\n\nPoints and that are in this relation are called conjugate points with respect to . Absolute points are called self-conjugate in keeping with this definition since they are incident with their own polars. Conjugate lines are defined dually.\n\nThe line joining two self-conjugate points cannot be a self-conjugate line.\n\nA line cannot contain more than two self-conjugate points.\n\nA polarity induces an involution of conjugate points on any line that is not self-conjugate.\n\nA triangle in which each vertex is the pole of the opposite side is called a self-polar triangle.\n\nA correlation that maps the three vertices of a triangle to their opposite sides respectively is a polarity and this triangle is self-polar with respect to this polarity.\n\nThe principle of duality is due to Joseph Diaz Gergonne (1771−1859) a champion of the then emerging field of Analytic geometry and founder and editor of the first journal devoted entirely to mathematics, \"Annales de mathématiques pures et appliquées\". Gergonne and Charles Julien Brianchon (1785−1864) developed the concept of plane duality. Gergonne coined the terms \"duality\" and \"polar\" (but \"pole\" is due to F.-J. Servois) and adopted the style of writing dual statements side-by-side in his journal.\n\nJean-Victor Poncelet (1788−1867) author of the first text on projective geometry, \"Traité des propriétés projectives des figures\", was a synthetic geometer who systematically developed the theory of poles and polars with respect to a conic. Poncelet maintained that the principle of duality was a consequence of the theory of poles and polars.\n\nJulius Plücker (1801−1868) is credited with extending the concept of duality to three and higher dimensional projective spaces.\n\nPoncelet and Gergonne started out as earnest but friendly rivals presenting their different points of view and techniques in papers appearing in \"Annales de Gergonne\". Antagonism grew over the issue of priority in claiming the principle of duality as their own. A young Plücker was caught up in this feud when a paper he had submitted to Gergonne was so heavily edited by the time it was published that Poncelet was misled into believing that Plücker had plagiarized him. The vitriolic attack by Poncelet was countered by Plücker with the support of Gergonne and ultimately the onus was placed on Gergonne. Of this feud, Pierre Samuel has quipped that since both men were in the French army and Poncelet was a general while Gergonne a mere captain, Poncelet's view prevailed, at least among their French contemporaries.\n\n\n\n"}
{"id": "15374396", "url": "https://en.wikipedia.org/wiki?curid=15374396", "title": "Enveloping von Neumann algebra", "text": "Enveloping von Neumann algebra\n\nIn operator algebras, the enveloping von Neumann algebra of a C*-algebra is a von Neumann algebra that contains all the operator-algebraic information about the given C*-algebra. This may also be called the \"universal\" enveloping von Neumann algebra, since it is given by a universal property; and (as always with von Neumann algebras) the term \"W*-algebra\" may be used in place of \"von Neumann algebra\".\n\nLet \"A\" be a C*-algebra and \"π\" be its universal representation, acting on Hilbert space \"H\". The image of \"π\", \"π\"(\"A\"), is a C*-subalgebra of bounded operators on \"H\". The enveloping von Neumann algebra of \"A\" is the closure of \"π\"(\"A\") in the weak operator topology. It is sometimes denoted by \"A\"′′.\n\nThe universal representation \"π\" and \"A\"′′ satisfies the following universal property: for any representation \"π\", there is a unique *-homomorphism\n\nthat is continuous in the weak operator topology and the restriction of Φ to \"π\"(\"A\") is \"π\".\n\nAs a particular case, one can consider the continuous functional calculus, whose unique extension gives a canonical Borel functional calculus.\nBy the Sherman–Takeda theorem, the double dual of a C*-algebra \"A\", \"A\"**, can be identified with \"A\"′′, as Banach spaces.\n\nEvery representation of \"A\" uniquely determines a central projection (i.e. a projection in the center of the algebra) in \"A\"′′; it is called the central cover of that projection.\n\n"}
{"id": "28886282", "url": "https://en.wikipedia.org/wiki?curid=28886282", "title": "Equichordal point", "text": "Equichordal point\n\nIn geometry, an equichordal point is a point defined relative to a convex plane curve such that all chords passing through the point are equal in length. Two common figures with equichordal points are the circle and the limaçon. It is impossible for a curve to have more than one equichordal point.\n\nA curve is called equichordal when it has an equichordal point. Such a curve may be constructed as the pedal curve of a curve of constant width. For instance, the pedal curve of a circle is either another circle (when the center of the circle is the pedal point) or a limaçon; both are equichordal curves.\n\nIn 1916 Fujiwara proposed the question of whether a curve could have two equichordal points (offering in the same paper a proof that three or more is impossible). Independently, a year later, Blaschke, Rothe and Weitzenböck posed the same question. The problem remained unsolved until it was finally proven impossible in 1996 by Marek Rychlik. Despite its elementary formulation, the equichordal point problem was difficult to solve. Rychlik's theorem is proved by methods of advanced complex analysis and algebraic geometry and it is 72 pages long.\n"}
{"id": "20831467", "url": "https://en.wikipedia.org/wiki?curid=20831467", "title": "Fractional quantum mechanics", "text": "Fractional quantum mechanics\n\nIn physics, fractional quantum mechanics is a generalization of standard quantum mechanics, which naturally comes out when the Brownian-like quantum paths substitute with the Lévy-like ones in the Feynman path integral. It has been discovered by Nick Laskin who coined the term \"fractional quantum mechanics\".\n\nStandard quantum mechanics can be approached in three different ways: the matrix mechanics, the Schrödinger equation and the Feynman path integral.\n\nThe Feynman path integral is the path integral over Brownian-like quantum-mechanical paths. Fractional quantum mechanics has been discovered by Nick Laskin (1999) as a result of expanding the Feynman path integral, from the Brownian-like to the Lévy-like quantum mechanical paths. A path integral over the Lévy-like quantum-mechanical paths results in a generalization of quantum mechanics. If the Feynman path integral leads to the well known Schrödinger equation, then the path integral over Lévy trajectories leads to the fractional Schrödinger equation. The Lévy process is characterized\nby the Lévy index \"α\", 0 < \"α\" ≤ 2. At the special case when \"α\" = 2 the Lévy process becomes the process of Brownian motion. The fractional Schrödinger equation includes a space derivative of fractional order \"α\" instead of the second order (\"α\" = 2) space derivative in the standard Schrödinger equation. Thus, the fractional Schrödinger equation is a fractional differential equation in accordance with modern terminology. This is the key point to launch the term fractional Schrödinger equation and more general term \"fractional quantum mechanics\". As mentioned above, at \"α\" = 2 the Lévy motion becomes Brownian motion. Thus, fractional quantum mechanics includes standard quantum mechanics as a particular case at \"α\" = 2. The quantum-mechanical path integral over the Lévy paths at \"α\" = 2 becomes the well-known Feynman path integral and the fractional Schrödinger equation becomes the well-known Schrödinger equation.\n\nThe fractional Schrödinger equation discovered by Nick Laskin has the following form (see, Refs.[1,3,4])\n\nusing the standard definitions:\n\nFurther,\n\nHere, the wave functions in the position and momentum spaces; formula_3 and formula_4 are related each other by the 3-dimensional Fourier transforms:\n\nThe index \"α\" in the fractional Schrödinger equation is the Lévy index, 1 < \"α\" ≤ 2.\n\nThe effective mass of states in solid state systems can depend on the wave vector k, i.e. formally one considers m=m(k). Polariton Bose-Einstein condensate modes are examples of states in solid state systems with mass sensitive to variations and locally in k fractional quantum mechanics is experimentally feasible.\n\n\n\n\n"}
{"id": "40639448", "url": "https://en.wikipedia.org/wiki?curid=40639448", "title": "Game-theoretic rough sets", "text": "Game-theoretic rough sets\n\nThe rough sets can be used to induce three-way classification decisions. The positive negative and boundary regions can be interpreted as regions of acceptance, rejection and deferment decisions, respectively. The probabilistic rough set model extends the conventional rough sets by providing more effective way for classifying objects. A main result of probabilistic rough sets is the interpretation of three-way decisions using a pair of probabilistic thresholds. The game-theoretic rough set model determine and interprets the required thresholds by utilizing a game-theoretic environment for analyzing strategic situations between cooperative or conflicting decision making criteria. The essential idea is to implement a game for investigating how the probabilistic thresholds may change in order to improve the rough set based decision making.\n"}
{"id": "391794", "url": "https://en.wikipedia.org/wiki?curid=391794", "title": "Hassler Whitney", "text": "Hassler Whitney\n\nHassler Whitney (March 23, 1907 – May 10, 1989) was an American mathematician. He was one of the founders of singularity theory, and did foundational work in manifolds, embeddings, immersions, characteristic classes, and geometric integration theory.\n\nHassler Whitney was born on March 23, 1907, in New York City, where his father Edward Baldwin Whitney was the First District New York Supreme Court judge. His mother, Josepha (Newcomb) Whitney, was an artist and active in politics. His paternal grandfather was William Dwight Whitney, professor of Ancient Languages at Yale University, linguist, and Sanskrit scholar. Whitney was the great grandson of Connecticut Governor and US Senator Roger Sherman Baldwin, and the great-great-grandson of American founding father Roger Sherman. His maternal grandparents were astronomer and mathematician Simon Newcomb (1835-1909), a Steeves descendant, and Mary Hassler Newcomb, granddaughter of the first superintendent of the Coast Survey Ferdinand Rudolph Hassler. His great uncle Josiah Whitney was the first to survey Mount Whitney.\n\nHe married three times: his first wife was Margaret R. Howell, married on the 30 May 1930. They had three children, James Newcomb, Carol and Marian. After his first divorce, on January 16, 1955 he married Mary Barnett Garfield. He and Mary had two daughters, Sarah Newcomb and Emily Baldwin. Finally, Whitney divorced his second wife and married Barbara Floyd Osterman on 8 February 1986.\n\nWhitney and his first wife Margaret made an innovative decision in 1939 that influenced the history of modern architecture in New England, when they commissioned the architect Edwin B. Goodell, Jr. to design a new residence for their family in Weston, Massachusetts. They purchased a rocky hillside site on a historic road, next door to another International Style house by Goodell from several years earlier, designed for Richard and Caroline Field.\n\nDistinctively featuring flat roofs, flush wood siding, and corner windows—all of which were unusual architectural elements at the time—the Whitney House was also a creative response to its site, in that it placed the main living spaces one floor above ground level, with large banks of windows opening to the south sun and to views of the beautiful property. The Whitney House survives today, along with the Field House, more than 75 years following its original construction; both are contributing structures in the historic Sudbury Road Area.\nThroughout his life he pursued two particular hobbies with excitement: music and mountain-climbing. An accomplished player of the violin and the viola, Whitney played with the Princeton Musical Amateurs. He would run outside, 6 to 12 miles every other day. As an undergraduate, with his cousin Bradley Gilman, Whitney made the first ascent of the Whitney–Gilman ridge on Cannon Mountain, New Hampshire in 1929. It was the hardest and most famous rock climb in the East. He was a member of the Swiss Alpine Society and the Yale Mountaineering Society (the precursor to the Yale Outdoors Club) and climbed most of the mountain peaks in Switzerland.\n\nThree years after his third marriage, on 10 May 1989, Whitney died in Princeton, after suffering a stroke. In accordance with his wish, Hassler Whitney ashes rest atop mountain Dents Blanches in Switzerland where Oscar Burlet, another mathematician and member of the Swiss Alpine Club, placed them on August 20, 1989.\n\nWhitney attended Yale University where he received baccalaureate degrees in physics and in music, respectively in 1928 and in 1929. Later, in 1932, he earned a PhD in mathematics at Harvard University. His doctoral dissertation was \"The Coloring of Graphs\", written under the supervision of George David Birkhoff.\nAt Harvard, Birkhoff also got him a job as Instructor of Mathematics for the years 1930–31, and an Assistant Professorship for the years 1934–35. Later on he held the following working positions: NRC Fellow, Mathematics, 1931–33; Assistant Professor, 1935–40; Associate Professor, 1940–46, Professor, 1946–52; Professor Instructor, Institute for Advanced Study, Princeton University, 1952–77; Professor Emeritus, 1977–89; Chairman of the Mathematics Panel, National Science Foundation, 1953–56; Exchange Professor, Collège de France, 1957; Memorial Committee, Support of Research in Mathematical Sciences, National Research Council, 1966–67; President, International Commission of Mathematical Instruction, 1979–82; Research Mathematician, National Defense Research Committee, 1943–45; Construction of the School of Mathematics.\n\nHe was a member of the National Academy of Science; Colloquium Lecturer, American Mathematical Society, 1946; Vice President, 1948–50 and Editor, American Journal of Mathematics, 1944–49; Editor, Mathematical Reviews, 1949–54; Chairman of the Committee vis. lectureship, 1946–51; Committee Summer Instructor, 1953–54;, American Mathematical Society; American National Council Teachers of Mathematics, London Mathematical Society (Honorary), Swiss Mathematics Society (Honorary), Académie des Sciences de Paris (Foreign Associate); New York Academy of Sciences.\n\nIn 1947 he was elected member of the American Philosophical Society.\nIn 1969 he was awarded the Lester R. Ford Award for the paper in two parts \"\"The mathematics of Physical quantities\" (1968a, 1968b). In 1976 he was awarded the National Medal of Science. In 1980 he was elected honorary member of the London Mathematical Society. In 1983 he received the Wolf Prize from the Wolf Foundation, and finally, in 1985, he was awarded the Steele Prize from the American Mathematical Society.\n\nWhitney's earliest work, from 1930 to 1933, was on graph theory. Many of his contributions were to the graph-coloring, and the ultimate computer-assisted solution to the four-color problem relied on some of his results. His work in graph theory culminated in a 1933 paper, where he laid the foundations for matroids, a fundamental notion in modern combinatorics and representation theory independently introduced by him and Bartel Leendert van der Waerden in the mid 1930s. In this paper Whitney proved several theorems about the matroid of a graph : one such theorem, now called Whitney's 2-Isomorphism Theorem, states: Given and are graphs with no isolated vertices. Then and are isomorphic if and only if and are 2-isomorphic.\n\nWhitney's lifelong interest in geometric properties of functions also began around this time. His earliest work in this subject was on the possibility of extending a function defined on a closed subset of ℝ to a function on all of ℝ with certain smoothness properties. A complete solution to this problem was found only in 2005 by Charles Fefferman.\n\nIn a 1936 paper, Whitney gave a definition of a smooth manifold of class \"\" , and proved that, for high enough values of \"r\", a smooth manifold of dimension \"n\" may be embedded in ℝ, and immersed in ℝ. (In 1944 he managed to reduce the dimension of the ambient space by 1, provided that \"n\" > 2, by a technique that has come to be known as the \"Whitney trick\".) This basic result shows that manifolds may be treated intrinsically or extrinsically, as we wish. The intrinsic definition had been published only a few years earlier in the work of Oswald Veblen and J. H. C. Whitehead. These theorems opened the way for much more refined studies: of embedding, immersion and also of smoothing: that is, the possibility of having various smooth structures on a given topological manifold.\n\nHe was one of the major developers of cohomology theory, and characteristic classes, as these concepts emerged in the late 1930s, and his work on algebraic topology continued into the 40s. He also returned to the study of functions in the 1940s, continuing his work on the extension problems formulated a decade earlier, and answering a question of Laurent Schwartz in a 1948 paper \"On Ideals of Differentiable Functions\".\n\nWhitney had, throughout the 1950s, an almost unique interest in the topology of singular spaces and in singularities of smooth maps. An old idea, implicit even in the notion of a simplicial complex, was to study a singular space by decomposing it into smooth pieces (nowadays called \"strata\"). Whitney was the first to see any subtlety in this definition, and pointed out that a good \"stratification\" should satisfy conditions he termed \"A\" and \"B\", now referred to as Whitney conditions. The work of René Thom and John Mather in the 1960s showed that these conditions give a very robust definition of stratified space.\nThe singularities in low dimension of smooth mappings, later to come to prominence in the work of René Thom, were also first studied by Whitney.\n\nIn his book \"Geometric Integration Theory\" he gives a theoretical basis for Stokes' theorem applied with singularities on the boundary: later, his work on such topics inspired the researches of Jenny Harrison.\n\nThese aspects of Whitney's work have looked more unified, in retrospect and with the general development of singularity theory. Whitney's purely topological work (Stiefel–Whitney class, basic results on vector bundles) entered the mainstream more quickly.\n\nIn 1967, he became involved full-time in educational problems, especially at the elementary school level.\nHe spent many years in classrooms, both teaching mathematics and observing how it is taught. He spent four months teaching pre-algebra mathematics to a classroom of seventh graders and conducted summer courses for teachers. He traveled widely to lecture on the subject in the United States and abroad. He worked toward removing the \"mathematics anxiety,\" which he felt leads young pupils to avoid mathematics. Whitney spread the ideas of teaching mathematics to students in ways that relate the content to their own lives as opposed to teaching them rote memorization.\n\nHassler Whitney published 82 works: all his published articles, included the ones listed in this section and the preface of the book , are collected in the two volumes and .\n\n\n\n"}
{"id": "16764192", "url": "https://en.wikipedia.org/wiki?curid=16764192", "title": "Hill differential equation", "text": "Hill differential equation\n\nIn mathematics, the Hill equation or Hill differential equation is the second-order linear ordinary differential equation\nwhere \"f(t)\" is a periodic function. It is named after George William Hill, who introduced it in 1886.\n\nOne can always rescale \"t\" so that the period of \"f\"(\"t\") equals \"π\"; then the Hill equation can be rewritten using the Fourier series of \"f\"(\"t\"):\n\nImportant special cases of Hill's equation include the Mathieu equation (in which only the terms corresponding to \"n\" = 0, 1 are included) and the Meissner equation.\n\nHill's equation is an important example in the understanding of periodic differential equations. Depending on the exact shape of \"f\"(\"t\"), solutions may stay bounded for all time, or the amplitude of the oscillations in solutions may grow exponentially. The precise form of the solutions to Hill's equation is described by Floquet theory. Solutions can also be written in terms of Hill determinants.\n\nAside from its original application to lunar stability, the Hill equation appears in many settings including the modeling of a quadrupole mass spectrometer, as the one-dimensional Schrödinger equation of an electron in a crystal, quantum optics of two-level systems, and in accelerator physics.\n\n"}
{"id": "18926594", "url": "https://en.wikipedia.org/wiki?curid=18926594", "title": "Hrushovski construction", "text": "Hrushovski construction\n\nIn model theory, a branch of mathematical logic, the Hrushovski construction generalizes the Fraïssé limit by working with a notion of \"strong substructure\" formula_1 rather than formula_2. It can be thought of as a kind of \"model-theoretic forcing\", where a (usually) stable structure is created, called the \"generic\" or \"rich\" model. The specifics of formula_1 determine various properties of the generic, with its geometric properties being of particular interest. It was initially used by Ehud Hrushovski to generate a stable structure with an \"exotic\" geometry, thereby refuting Zil'ber's Conjecture.\n\nThe initial applications of the Hrushovski construction refuted two conjectures and answered a third question in the negative. Specifically, we have:\n\n\nLet \"L\" be a finite relational language. Fix C a class of \"finite\" \"L\"-structures which are closed under isomorphisms and\nsubstructures. We want to strengthen the notion of substructure; let\nformula_1 be a relation on pairs from C satisfying:\n\n\nAn embedding formula_22 is \"strong\" if formula_23.\n\nWe also want the pair (C, formula_1) to satisfy the \"amalgamation property\": if formula_25 then there is a formula_26\nso that each formula_27 embeds strongly into formula_28 with the same image for\nformula_29.\n\nFor infinite formula_28, and formula_12, we say formula_32 iff formula_33 for\nformula_34, formula_35. For any formula_36, the\n\"closure\" of formula_29 (in formula_28), formula_39 is the smallest superset of formula_29\nsatisfying formula_41.\n\nDefinition A countable structure formula_42 is a (C, formula_1)-generic if:\n\nTheorem If (C, formula_1) has the amalgamation property, then there is a unique (C, formula_1)-generic.\n\nThe existence proof proceeds in imitation of the existence proof for\nFraïssé limits. The uniqueness proof comes from an easy back and forth\nargument.\n"}
{"id": "52280151", "url": "https://en.wikipedia.org/wiki?curid=52280151", "title": "Incremental learning", "text": "Incremental learning\n\nIn computer science, incremental learning is a method of machine learning, in which input data is continuously used to extend the existing model's knowledge i.e. to further train the model. It represents a dynamic technique of supervised learning and unsupervised learning that can be applied when training data becomes available gradually over time or its size is out of system memory limits. Algorithms that can facilitate incremental learning are known as incremental machine learning algorithms.\n\nMany traditional machine learning algorithms inherently support incremental learning, other algorithms can be adapted to facilitate this. Examples of incremental algorithms include\ndecision trees\n(IDE4,\nID5R),\ndecision rules,\nartificial neural networks\n(RBF networks,\nLearn++,\nFuzzy ARTMAP,\nTopoART, and\nIGNG) or\nthe incremental SVM.\n\nThe aim of incremental learning is for the learning model to adapt to new data without forgetting its existing knowledge, it does not retrain the model. Some incremental learners have built-in some parameter or assumption that controls the relevancy of old data, while others, called stable incremental machine learning algorithms, learn representations of the training data that are not even partially forgotten over time. Fuzzy ART and TopoART are two examples for this second approach.\n\nIncremental algorithms are frequently applied to data streams or big data, addressing issues in data availability and resource scarcity respectively. Stock trend prediction and user profiling are some examples of data streams where new data becomes continuously available. Applying incremental learning to big data aims to produce faster classification or forecasting times.\n\n"}
{"id": "37329912", "url": "https://en.wikipedia.org/wiki?curid=37329912", "title": "James Eells", "text": "James Eells\n\nJames Eells (October 25, 1926, Cleveland, Ohio – February 14, 2007, Cambridge, UK) was an American mathematician, who specialized in mathematical analysis.\nEells studied mathematics at Bowdoin College in Maine and earned his undergraduate degree in 1947. After graduation he spent one year teaching mathematics at Robert College in Istanbul and starting in 1948 was for two years an instructor at Amherst College in Amherst, Massachusetts. Next he undertook graduate study at Harvard University, where in 1954 he received his Ph.D under Hassler Whitney with thesis \"Geometric Aspects of Integration Theory\". \n\nIn the academic year 1955–1956 he was at the Institute for Advanced Study (and subsequently in 1962–1963, 1972–1973 and 1977). He taught at Columbia University for several years. In 1964 he became a full professor at Cornell University. In 1963 and in 1966–1967 he was at the University of Cambridge, and after a visit to the new mathematics department developed by Erik Christopher Zeeman at the University of Warwick Eells became a professor of mathematical analysis there in 1969. Eells organized many of the University of Warwick Symposia in mathematics. \n\nIn 1986 he became the first director of the mathematics section of the Abdus Salam International Centre for Theoretical Physics in Trieste; for six years he served as director in addition to his appointment at the University of Warwick. In 1992 he retired and lived in Cambridge.\n\nEells did research on global analysis, especially, harmonic maps on Riemannian manifolds, which are important in the theory of minimal surfaces and theoretical physics. His doctoral students included John C. Wood.\n\nIn 1970 he was an invited speaker at the International Mathematical Congress in Nice (\"On Fredholm manifolds\" with K. D. Elworthy).\n\nHe was co-editor of the collected works of Hassler Whitney. Eells's doctoral students include Peter Štefan (1941 - 1978), Giorgio Valli (1960 - 1999) and . Eells was married since 1950 and had a son and three daughters.\n\n\n\n"}
{"id": "9723458", "url": "https://en.wikipedia.org/wiki?curid=9723458", "title": "Kalmanson combinatorial conditions", "text": "Kalmanson combinatorial conditions\n\nIn mathematics, the Kalmanson combinatorial conditions are a set of conditions on the distance matrix used in determining the solvability of the traveling salesman problem. These conditions apply to a special kind of cost matrix, the Kalmanson matrix, and are named after Kenneth Kalmanson.\n\n"}
{"id": "42485711", "url": "https://en.wikipedia.org/wiki?curid=42485711", "title": "Kirwan map", "text": "Kirwan map\n\nIn differential geometry, the Kirwan map, introduced by British mathematician Frances Kirwan, is the homomorphism\nwhere\n\nIt is defined as the map of equivariant cohomology induced by the inclusion formula_14 followed by the canonical isomorphism formula_15.\n\nA theorem of Kirwan says that if formula_2 is compact, then the map is surjective in rational coefficients. The analogous result holds between the K-theory of the symplectic quotient and the equivariant topological K-theory of formula_2.\n"}
{"id": "18531", "url": "https://en.wikipedia.org/wiki?curid=18531", "title": "L'Hôpital's rule", "text": "L'Hôpital's rule\n\nIn mathematics, and more specifically in calculus, L'Hôpital's rule or L'Hospital's rule () uses derivatives to help evaluate limits involving indeterminate forms. Application (or repeated application) of the rule often converts an indeterminate form to an expression that can be evaluated by substitution, allowing easier evaluation of the limit. The rule is named after the 17th-century French mathematician Guillaume de l'Hôpital. Although the contribution of the rule is often attributed to L'Hôpital, the theorem was first introduced to L'Hôpital in 1694 by the Swiss mathematician Johann Bernoulli.\n\nL'Hôpital's rule states that for functions and which are differentiable on an open interval except possibly at a point contained in , if\nformula_1 formula_2 for all in with , and formula_3 exists, then\n\nThe differentiation of the numerator and denominator often simplifies the quotient or converts it to a limit that can be evaluated directly.\n\nGuillaume de l'Hôpital (also written l'Hospital) published this rule in his 1696 book \"Analyse des Infiniment Petits pour l'Intelligence des Lignes Courbes\" (literal translation: \"Analysis of the Infinitely Small for the Understanding of Curved Lines\"), the first textbook on differential calculus. However, it is believed that the rule was discovered by the Swiss mathematician Johann Bernoulli.\n\nThe general form of L'Hôpital's rule covers many cases. Let and be extended real numbers (i.e., real numbers, positive infinity, or negative infinity). Let be an open interval containing (for a two-sided limit) or an open interval with endpoint (for a one-sided limit, or a limit at infinity if is infinite). The real valued functions and are assumed to be differentiable on except possibly at , and additionally formula_2 on except possibly at . It is also assumed that formula_6 Thus the rule applies to situations in which the ratio of the derivatives has a finite or infinite limit, but not to situations in which that ratio fluctuates permanently as gets closer and closer to .\n\nIf either\nor\nthen\nAlthough we have written \"x\" → \"c\" throughout, the limits may also be one-sided limits (\"x\" → \"c\" or \"x\" → \"c\"), when is a finite endpoint of .\n\nIn the second case, the hypothesis that diverges to infinity is not used in the proof (see note at the end of the proof section); thus, while the conditions of the rule are normally stated as above, the second sufficient condition for the rule's procedure to be valid can be more briefly stated as formula_10\n\nThe hypothesis that formula_2 appears most commonly in the literature, but some authors sidestep this hypothesis by adding other hypotheses elsewhere. One method is to define the limit of a function with the additional requirement that the limiting function is defined everywhere on the relevant interval except possibly at . Another method is to require that both and be differentiable everywhere on an interval containing .\n\nThe requirement that the limit\n\nmust exist is essential. Without this condition, formula_13 or formula_14 may exhibit undampened oscillations as formula_15 approaches formula_16, in which case L'Hôpital's rule does not apply. For example, if formula_17, formula_18 and formula_19, then\n\nthis expression does not approach a limit as formula_15 goes to formula_16, since the cosine function oscillates between and . But working with the original functions, formula_23 can be shown to exist:\n\nIn a case such as this, all that can be concluded is that\n\nso that if the limit of \"f\"/\"g\" exists, then it must lie between the inferior and superior limits of \"f\"′/\"g\"′. (In the example above, this is true, since 1 indeed lies between 0 and 2.)\n\n\n\n\n\n\n\n\nSometimes L'Hôpital's rule does not lead to an answer in a finite number of steps unless some additional steps are applied. Examples include the following:\n\n\n\nA common pitfall is using L'Hôpital's rule with some circular reasoning to compute a derivative via a difference quotient. For example, consider the task of proving the derivative formula for powers of \"x\":\n\nApplying L'Hôpital's rule and finding the derivatives with respect to of the numerator and the denominator yields \n\nThe necessity of the condition formula_2 can be seen by a counterexample due to Otto Stolz: Choosing formula_50, formula_51, there is no limit for formula_52 as formula_53. However, \nwhich tends to 0 as formula_53. Further examples of this type were found by Ralph P. Boas Jr.\n\nOther indeterminate forms, such as , , , , and , can sometimes be evaluated using L'Hôpital's rule. For example, to evaluate a limit involving , convert the difference of two functions to a quotient:\n\nwhere L'Hôpital's rule is applied when going from (1) to (2) and again when going from (3) to (4).\n\nL'Hôpital's rule can be used on indeterminate forms involving exponents by using logarithms to \"move the exponent down\". Here is an example involving the indeterminate form :\n\nIt is valid to move the limit inside the exponential function because the exponential function is continuous. Now the exponent formula_15 has been \"moved down\". The limit formula_59 is of the indeterminate form , but as shown in an example above, l'Hôpital's rule may be used to determine that\n\nThus\n\nThe Stolz–Cesàro theorem is a similar result involving limits of sequences, but it uses finite difference operators rather than derivatives.\n\nConsider the curve in the plane whose -coordinate is given by and whose -coordinate is given by , with both functions continuous, i.e., the locus of points of the form . Suppose . The limit of the ratio as is the slope of the tangent to the curve at the point . The tangent to the curve at the point is given by . L'Hôpital's rule then states that the slope of the curve when is the limit of the slope of the tangent to the curve as the curve approaches the origin, provided that this is defined.\n\nThe proof of L'Hôpital's rule is simple in the case where and are continuously differentiable at the point and where a finite limit is found after the first round of differentiation. It is not a proof of the general L'Hôpital's rule because it is stricter in its definition, requiring both differentiability and that \"c\" be a real number. Since many common functions have continuous derivatives (e.g. polynomials, sine and cosine, exponential functions), it is a special case worthy of attention.\n\nSuppose that and are continuously differentiable at a real number , that formula_62, and that formula_63. Then\n\nThis follows from the difference-quotient definition of the derivative. The last equality follows from the continuity of the derivatives at . The limit in the conclusion is not indeterminate because formula_65.\n\nThe proof of a more general version of L'Hôpital's rule is given below.\n\nThe following proof is due to , where a unified proof for the and indeterminate forms is given. Taylor notes that different proofs may be found in and .\n\nLet \"f\" and \"g\" be functions satisfying the hypotheses in the General form section. Let formula_66 be the open interval in the hypothesis with endpoint \"c\". Considering that formula_2 on this interval and \"g\" is continuous, formula_66 can be chosen smaller so that \"g\" is nonzero on formula_66.\n\nFor each \"x\" in the interval, define formula_70 and formula_71 as formula_72 ranges over all values between \"x\" and \"c\". (The symbols inf and sup denote the infimum and supremum.)\n\nFrom the differentiability of \"f\" and \"g\" on formula_66, Cauchy's mean value theorem ensures that for any two distinct points \"x\" and \"y\" in formula_66 there exists a formula_72 between \"x\" and \"y\" such that formula_76. Consequently, formula_77 for all choices of distinct \"x\" and \"y\" in the interval. The value \"g\"(\"x\")-\"g\"(\"y\") is always nonzero for distinct \"x\" and \"y\" in the interval, for if it was not, the mean value theorem would imply the existence of a \"p\" between \"x\" and \"y\" such that \"g' \"(\"p\")=0.\n\nThe definition of \"m\"(\"x\") and \"M\"(\"x\") will result in an extended real number, and so it is possible for them to take on the values ±∞. In the following two cases, \"m\"(\"x\") and \"M\"(\"x\") will establish bounds on the ratio .\n\nCase 1: formula_78\n\nFor any \"x\" in the interval formula_66, and point \"y\" between \"x\" and \"c\",\nand therefore as \"y\" approaches \"c\", formula_81 and formula_82 become zero, and so\n\nCase 2: formula_84\n\nFor every \"x\" in the interval formula_66, define formula_86. For every point \"y\" between \"x\" and \"c\",\n\nAs \"y\" approaches \"c\", both formula_88 and formula_89 become zero, and therefore\n\nThe limit superior and limit inferior are necessary since the existence of the limit of has not yet been established.\n\nIt is also the case that \n\nand\n\nIn case 1, the squeeze theorem establishes that formula_94 exists and is equal to \"L\". In the case 2, and the squeeze theorem again asserts that formula_95, and so the limit formula_94 exists and is equal to \"L\". This is the result that was to be proven.\n\nIn case 2 the assumption that \"f\"(\"x\") diverges to infinity was not used within the proof. This means that if |\"g\"(\"x\")| diverges to infinity as \"x\" approaches \"c\" and both \"f\" and \"g\" satisfy the hypotheses of L'Hôpital's rule, then no additional assumption is needed about the limit of \"f\"(\"x\"): It could even be the case that the limit of \"f\"(\"x\") does not exist. In this case, L'Hopital's theorem is actually a consequence of Cesàro–Stolz.\n\nIn the case when |\"g\"(\"x\")| diverges to infinity as \"x\" approaches \"c\" and \"f\"(\"x\") converges to a finite limit at \"c\", then L'Hôpital's rule would be applicable, but not absolutely necessary, since basic limit calculus will show that the limit of \"f\"(\"x\")/\"g\"(\"x\") as \"x\" approaches \"c\" must be zero.\n\nA simple but very useful consequence of L'Hopital's rule is a well-known criterion for differentiability. It states the following:\nsuppose that \"f\" is continuous at \"a\", and that formula_97 exists for all \"x\" in some open interval containing \"a\", except perhaps for formula_98. Suppose, moreover, that formula_99 exists. Then formula_100 also exists and\nIn particular, \"f\"' is also continuous at \"a\".\n\nConsider the functions formula_102 and formula_103. The continuity of \"f\" at \"a\" tells us that formula_104. Moreover, formula_105 since a polynomial function is always continuous everywhere. Applying L'Hopital's rule shows that formula_106.\n\n\n\n"}
{"id": "172896", "url": "https://en.wikipedia.org/wiki?curid=172896", "title": "L. E. J. Brouwer", "text": "L. E. J. Brouwer\n\nLuitzen Egbertus Jan Brouwer (; ; 27 February 1881 – 2 December 1966), usually cited as L. E. J. Brouwer but known to his friends as Bertus, was a Dutch mathematician and philosopher, who worked in topology, set theory, measure theory and complex analysis. He was the founder of the mathematical philosophy of intuitionism.\n\nEarly in his career, Brouwer proved a number of theorems in the emerging field of topology. The most important were his fixed point theorem, the topological invariance of degree, and the topological invariance of dimension. Among mathematicians generally, the best known is the first one, usually referred to now as the Brouwer Fixed Point Theorem. It is a simple corollary to the second, concerning the topological invariance of degree, which is the best known among algebraic topologists. The third theorem is perhaps the hardest.\n\nBrouwer also proved the simplicial approximation theorem in the foundations of algebraic topology, which justifies the reduction to combinatorial terms, after sufficient subdivision of simplicial complexes, of the treatment of general continuous mappings. In 1912, at age 31, he was elected a member of the Royal Netherlands Academy of Arts and Sciences. He was an Invited Speaker of the ICM in 1908 at Rome and in 1912 at Cambridge, UK.\n\nBrouwer founded intuitionism, a philosophy of mathematics that challenged the then-prevailing formalism of David Hilbert and his collaborators, who included Paul Bernays, Wilhelm Ackermann, and John von Neumann (cf. Kleene (1952), p. 46–59). A variety of constructive mathematics, intuitionism is a philosophy of the foundations of mathematics. It is sometimes and rather simplistically characterized by saying that its adherents refuse to use the law of excluded middle in mathematical reasoning.\n\nBrouwer was a member of the Significs Group. It formed part of the early history of semiotics—the study of symbols—around Victoria, Lady Welby in particular. The original meaning of his intuitionism probably can not be completely disentangled from the intellectual milieu of that group.\n\nIn 1905, at the age of 24, Brouwer expressed his philosophy of life in a short tract \"Life, Art and Mysticism\", which has been described by the mathematician Martin Davis as \"drenched in romantic pessimism\" (Davis (2002), p. 94). Arthur Schopenhauer had a formative influence on Brouwer, not least because he insisted that all concepts be fundamentally based on sense intuitions. Brouwer then \"embarked on a self-righteous campaign to reconstruct mathematical practice from the ground up so as to satisfy his philosophical convictions\"; indeed his thesis advisor refused to accept his Chapter II \"as it stands, ... all interwoven with some kind of pessimism and mystical attitude to life which is not mathematics, nor has anything to do with the foundations of mathematics\" (Davis, p. 94 quoting van Stigt, p. 41). Nevertheless, in 1908:\n\n\"After completing his dissertation, Brouwer made a conscious decision to temporarily keep his contentious ideas under wraps and to concentrate on demonstrating his mathematical prowess\" (Davis (2000), p. 95); by 1910 he had published a number of important papers, in particular the Fixed Point Theorem. Hilbert—the formalist with whom the intuitionist Brouwer would ultimately spend years in conflict—admired the young man and helped him receive a regular academic appointment (1912) at the University of Amsterdam (Davis, p. 96). It was then that \"Brouwer felt free to return to his revolutionary project which he was now calling \"intuitionism\" \" (ibid).\n\nHe was combative as a young man. He was involved in a very public and eventually demeaning controversy in the later 1920s with Hilbert over editorial policy at \"Mathematische Annalen\", at that time a leading learned journal. He became relatively isolated; the development of intuitionism at its source was taken up by his student Arend Heyting.\n\nDutch mathematician and historian of mathematics, Bartel Leendert van der Waerden attended lectures given by Brouwer in later years, and commented: \"Even though his most important research contributions were in topology, Brouwer never gave courses in topology, but always on—and only on—the foundations of his intuitionism. It seemed that he was no longer convinced of his results in topology because they were not correct from the point of view of intuitionism, and he judged everything he had done before, his greatest output, false according to his philosophy.\"\n\nAbout his last years, Davis (2002) remarks:\n\n\n\n\n"}
{"id": "1911935", "url": "https://en.wikipedia.org/wiki?curid=1911935", "title": "Libquantum", "text": "Libquantum\n\nLibquantum is a C library quantum virtual machine for the simulation of a quantum computer. It is licensed under the GNU GPL.\n\n"}
{"id": "41685609", "url": "https://en.wikipedia.org/wiki?curid=41685609", "title": "List of Greek mathematicians", "text": "List of Greek mathematicians\n\nIn historical times, Greek civilization has played one of the major roles in the history and development of mathematics. To this day, a number of Greek mathematicians are considered for their innovations and influence on mathematics.\n\n"}
{"id": "1076293", "url": "https://en.wikipedia.org/wiki?curid=1076293", "title": "List of homological algebra topics", "text": "List of homological algebra topics\n\nThis is a list of homological algebra topics, by Wikipedia page.\n\n\n"}
{"id": "5971805", "url": "https://en.wikipedia.org/wiki?curid=5971805", "title": "List of mathematicians (F)", "text": "List of mathematicians (F)\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "22583255", "url": "https://en.wikipedia.org/wiki?curid=22583255", "title": "Marcinkiewicz–Zygmund inequality", "text": "Marcinkiewicz–Zygmund inequality\n\nIn mathematics, the Marcinkiewicz–Zygmund inequality, named after Józef Marcinkiewicz and Antoni Zygmund, gives relations between moments of a collection of independent random variables. It is a generalization of the rule for the sum of variances of independent random variables to moments of arbitrary order.\n\nTheorem If formula_1, formula_2, are independent random variables such that formula_3 and formula_4, formula_5, then\n\nwhere formula_7 and formula_8 are positive constants, which depend only on formula_9.\n\nIn the case formula_10, the inequality holds with formula_11, and it reduces to the rule for the sum of variances of independent random variables with zero mean, known from elementary statistics: If formula_3 and formula_13, then\n\nSeveral similar moment inequalities are known as Khintchine inequality and Rosenthal inequalities, and there are also extensions to more general symmetric statistics of independent random variables.\n"}
{"id": "17730420", "url": "https://en.wikipedia.org/wiki?curid=17730420", "title": "Mike Paterson", "text": "Mike Paterson\n\nMichael Stewart \"Mike\" Paterson, is a British computer scientist, who was the director of the Centre for Discrete Mathematics and its Applications at the University of Warwick until 2007, and chair of the Department of Computer Science in 2005.\n\nHe received his doctorate from Cambridge University in 1967, under the supervision of David Park. He spent three years at MIT and moved to University of Warwick in 1971.\n\nPaterson is an expert on theoretical computer science with more than 100 publications, especially the design and analysis of algorithms and computational complexity. Paterson's distinguished career was recognised with the EATCS Award in 2006 and a workshop in honour of his 66th birthday in 2008, including contributions of several Turing Award and Gödel Prize laureates. For his work on distributed computing with Fischer and Lynch, he received the Dijkstra Prize in 2001, and his work with Dyer and Goldberg on counting graph homomorphisms received a best paper award at the ICALP conference in 2006. Mike Paterson received a Lester R. Ford Award in 2010. He is a Fellow of the Royal Society since 2001 and been president of the European Association for Theoretical Computer Science (EATCS). According to EATCS president Maurice Nivat, Paterson played a great role in the late 1960s in the recognition of computer science as a science, \"and that theoretical computer science, which is very close to mathematics but distinct in its motivation and inspiration, is indeed a challenging and fruitful field of research.\"\n\nHe is also an enthusiastic mountaineer.\n\n\n\n"}
{"id": "25380742", "url": "https://en.wikipedia.org/wiki?curid=25380742", "title": "Misorientation", "text": "Misorientation\n\nMisorientation is the difference in crystallographic orientation between two crystallites in a polycrystalline material.\n\nIn crystalline materials, the orientation of a crystallite is defined by a transformation from a sample reference frame (i.e. defined by the direction of a rolling or extrusion process and two orthogonal directions) to the local reference frame of the crystalline lattice, as defined by the basis of the unit cell. In the same way, misorientation is the transformation necessary to move from one local crystal frame to some other crystal frame. That is, it is the distance in orientation space between two distinct orientations. If the orientations are specified in terms of matrices of direction cosines g and g, then the misorientation operator ∆g going from A to B can be defined as follows:\n\nwhere the term g is the reverse operation of g, that is, transformation from crystal frame A back to the sample frame. This provides an alternate description of misorientation as the successive operation of transforming from the first crystal frame (A) back to the sample frame and subsequently to the new crystal frame (B).\n\nVarious methods can be used to represent this transformation operation, such as: Euler angles, Rodrigues vectors, axis/angle (where the axis is specified as a crystallographic direction), or unit quaternions.\n\nThe effect of crystal symmetry on misorientations is to reduce the fraction of the full orientation space necessary to uniquely represent all possible misorientation relationships. For example, cubic crystals (i.e. FCC) have 24 symmetrically related orientations. Each of these orientations is physically indistinguishable, though mathematically distinct. Therefore, the size of orientation space is reduced by a factor of 24. This defines the fundamental zone (FZ) for cubic symmetries. For the misorientation between two cubic crystallites, each possesses its 24 inherent symmetries. In addition, there exists a switching symmetry, defined by:\n\nwhich recognizes the invariance of misorientation to direction; A→B or B→A. The fraction of the total orientation space in the cubic-cubic fundamental zone for misorientation is then given by:\nor 1/48 the volume of the cubic fundamental zone. This also has the effect of limiting the maximum unique misorientation angle to 62.8°\n\nDisorientation describes the misorientation with the smallest possible rotation angle out of all symmetrically equivalent misorientations that fall within the FZ (usually specified as having an axis in the standard stereographic triangle for cubics). Calculation of these variants involves application of crystal symmetry operators to each of the orientations during the calculation of misorientation.\nformula_5\nwhere O denotes one of the symmetry operators for the material.\n\nThe misorientation distribution (MD) is analogous to the ODF used in characterizing texture. The MD describes the probability of the misorientation between any two grains falling into a range formula_6 around a given misorientation formula_7. While similar to a probability density, the MD is not mathematically the same due to the normalization. The intensity in a MD is given as \"multiples of random density\" (MRD) with respect to the distribution expected in a material with uniformly distributed misorientations. The MD can be calculated by either series expansion, typically using generalized spherical harmonics, or by a discrete binning scheme, where each data point is assigned to a bin and accumulated.\n\nDiscrete misorientations or the misorientation distribution can be fully described as plots in the Euler angle, axis/angle, or Rodrigues vector space. Unit quaternions, while computationally convenient, do not lend themselves to graphical representation because of their four-dimensional nature. For any of the representations, plots are usually constructed as sections through the fundamental zone; along φ in Euler angles, at increments of rotation angle for axis/angle, and at constant ρ (parallel to <001>) for Rodrigues. Due to the irregular shape of the cubic-cubic FZ, the plots are typically given as sections through the cubic FZ with the more restrictive boundaries overlaid.\n\nMackenzie plots are a one-dimensional representation of the MD plotting the relative frequency of the misorientation angle, irrespective of the axis. Mackenzie determined the misorientation distribution for a cubic sample with a random texture.\n\nThe following is an example of the algorithm for determining the axis/angle representation of misorientation between two texture components given as Euler angles:\nThe first step is converting the Euler angle representation to an orientation matrix \"g\" by:\nformula_8\n\nwhere c and s represent \"cosine\" and \"sine\", respectively. This yields the following orientation matrices:\nThe misorientation is then:\nThe axis/angle description (with the axis as a unit vector) is related to the misorientation matrix by:\n\nFor the copper—S misorientation given by Δg, the axis/angle description is 19.5° about [0.689,0.623,0.369], which is only 2.3° from <221>. This result is only one of the 1152 symmetrically related possibilities but does specify the disorientation. This can be verified by considering all possible combinations of orientation symmetry (including switching symmetry).\n\n"}
{"id": "14004780", "url": "https://en.wikipedia.org/wiki?curid=14004780", "title": "Open-loop model", "text": "Open-loop model\n\nIn game theory, an open-loop model is the one where players cannot observe the play of their opponents, as opposed to a closed-loop model, where all past play is common knowledge. The solution to an open-loop model is called \"open-loop equilibrium\".\n\nOpen loop models are more tractable, which is why they are sometimes preferred to closed-loop models even when the latter is a better description of reality.\n\n"}
{"id": "49112440", "url": "https://en.wikipedia.org/wiki?curid=49112440", "title": "Orientation sheaf", "text": "Orientation sheaf\n\nIn algebraic topology, the orientation sheaf on a manifold \"X\" of dimension \"n\" is a locally constant sheaf \"o\" on \"X\" such that the stalk of \"o\" at a point \"x\" is\n(in the integer coefficients or some other coefficients).\n\nLet formula_2 be the sheaf of differential \"k\"-forms on a manifold \"M\". If \"n\" is the dimension of \"M\", then the sheaf\nis called the sheaf of (smooth) densities on \"M\". The point of this is that, while one can integrate a differential form only if the manifold is oriented, one can always integrate a density, regardless of orientation or orientability; there is the integration map:\nIf \"M\" is oriented; i.e., the orientation sheaf of the tangent bundle of \"M\" is literally trivial, then the above reduces to the usual integration of a differential form.\n\n\n"}
{"id": "52565", "url": "https://en.wikipedia.org/wiki?curid=52565", "title": "Partial derivative", "text": "Partial derivative\n\nIn mathematics, a partial derivative of a function of several variables is its derivative with respect to one of those variables, with the others held constant (as opposed to the total derivative, in which all variables are allowed to vary). Partial derivatives are used in vector calculus and differential geometry.\n\nThe partial derivative of a function formula_1 with respect to the variable formula_2 is variously denoted by\n\nSometimes, for formula_4 the partial derivative of formula_5 with respect to formula_2 is denoted as formula_7 Since a partial derivative generally has the same arguments as the original function, its functional dependence is sometimes explicitly signified by the notation, such as in:\n\nThe symbol used to denote partial derivatives is ∂. One of the first known uses of this symbol in mathematics is by Marquis de Condorcet from 1770, who used it for partial differences. The modern partial derivative notation was created by Adrien-Marie Legendre (1786), though he later abandoned it; Carl Gustav Jacob Jacobi reintroduced the symbol again in 1841.\n\nSuppose that \"f\" is a function of more than one variable. For instance,\n\nThe graph of this function defines a surface in Euclidean space. To every point on this surface, there are an infinite number of tangent lines. Partial differentiation is the act of choosing one of these lines and finding its slope. Usually, the lines of most interest are those that are parallel to the formula_10-plane, and those that are parallel to the \"yz\"-plane (which result from holding either \"y\" or \"x\" constant, respectively.)\n\nTo find the slope of the line tangent to the function at formula_11 and parallel to the formula_10-plane, we treat formula_13 as a constant. The graph and this plane are shown on the right. Below, we see how the function looks on the plane formula_14 . By finding the derivative of the equation while assuming that formula_13 is a constant, we find that the slope of \"formula_16\" at the point formula_17 is:\n\nSo at formula_19, by substitution, the slope is 3. Therefore,\n\nat the point formula_19. That is, the partial derivative of formula_5 with respect to formula_2 at formula_19 is 3, as shown in the graph.\n\nThe function \"f\" can be reinterpreted as a family of functions of one variable indexed by the other variables:\n\nIn other words, every value of \"y\" defines a function, denoted \"f\" , which is a function of one variable \"x\". That is,\n\nNote that in this section the subscript notation \"f\" denotes a function contingent on a fixed value of \"y\", and not a partial derivative.\n\nOnce a value of \"y\" is chosen, say \"a\", then \"f\"(\"x\",\"y\") determines a function \"f\" which traces a curve \"x\" + \"ax\" + \"a\" on the formula_10-plane:\n\nIn this expression, \"a\" is a \"constant\", not a \"variable\", so \"f\" is a function of only one real variable, that being \"x\". Consequently, the definition of the derivative for a function of one variable applies:\n\nThe above procedure can be performed for any choice of \"a\". Assembling the derivatives together into a function gives a function which describes the variation of \"f\" in the \"x\" direction:\n\nThis is the partial derivative of \"f\" with respect to \"x\". Here ∂ is a rounded \"d\" called the partial derivative symbol. To distinguish it from the letter \"d\", ∂ is sometimes pronounced \"tho\" or \"partial\".\n\nIn general, the partial derivative of an n-ary function \"f\"(\"x\", ..., \"x\") in the direction \"x\" at the point (\"a\", ..., \"a\") is defined to be:\n\nIn the above difference quotient, all the variables except \"x\" are held fixed. That choice of fixed values determines a function of one variable\n\nand by definition,\n\nIn other words, the different choices of \"a\" index a family of one-variable functions just as in the example above. This expression also shows that the computation of partial derivatives reduces to the computation of one-variable derivatives.\n\nAn important example of a function of several variables is the case of a scalar-valued function \"f\"(\"x\", ..., \"x\") on a domain in Euclidean space formula_34 (e.g., on formula_35 or formula_36). In this case \"f\" has a partial derivative ∂\"f\"/∂\"x\" with respect to each variable \"x\". At the point \"a\", these partial derivatives define the vector\n\nThis vector is called the gradient of \"f\" at \"a\". If \"f\" is differentiable at every point in some domain, then the gradient is a vector-valued function ∇\"f\" which takes the point \"a\" to the vector ∇\"f\"(\"a\"). Consequently, the gradient produces a vector field.\n\nA common abuse of notation is to define the del operator (∇) as follows in three-dimensional Euclidean space formula_36 with unit vectors formula_39:\n\nOr, more generally, for \"n\"-dimensional Euclidean space formula_34 with coordinates formula_42 and unit vectors formula_43:\n\nLike ordinary derivatives, the partial derivative is defined as a limit. Let \"U\" be an open subset of formula_34 and formula_46 a function. The partial derivative of \"f\" at the point formula_47 with respect to the \"i\"-th variable \"x\" is defined as\n\nEven if all partial derivatives ∂\"f\"/∂\"x\"(\"a\") exist at a given point \"a\", the function need not be continuous there. However, if all partial derivatives exist in a neighborhood of \"a\" and are continuous there, then \"f\" is totally differentiable in that neighborhood and the total derivative is continuous. In this case, it is said that \"f\" is a C function. This can be used to generalize for vector valued functions, formula_49 by carefully using a componentwise argument.\n\nThe partial derivative formula_50 can be seen as another function defined on \"U\" and can again be partially differentiated. If all mixed second order partial derivatives are continuous at a point (or on a set), \"f\" is termed a C function at that point (or on that set); in this case, the partial derivatives can be exchanged by Clairaut's theorem:\n\nThe volume \"V\" of a cone depends on the cone's height \"h\" and its radius \"r\" according to the formula\n\nThe partial derivative of \"V\" with respect to \"r\" is\n\nwhich represents the rate with which a cone's volume changes if its radius is varied and its height is kept constant. The partial derivative with respect to formula_54 equals formula_55 which represents the rate with which the volume changes if its height is varied and its radius is kept constant.\n\nBy contrast, the \"total\" derivative of \"V\" with respect to \"r\" and \"h\" are respectively\n\nand\n\nThe difference between the total and partial derivative is the elimination of indirect dependencies between variables in partial derivatives.\n\nIf (for some arbitrary reason) the cone's proportions have to stay the same, and the height and radius are in a fixed ratio \"k\",\n\nThis gives the total derivative with respect to \"r\":\n\nwhich simplifies to:\n\nSimilarly, the total derivative with respect to \"h\" is:\n\nThe total derivative with respect to \"both\" r and h of the volume intended as scalar function of these two variables is given by the gradient vector \n\nPartial derivatives appear in any calculus-based optimization problem with more than one choice variable. For example, in economics a firm may wish to maximize profit π(\"x\", \"y\") with respect to the choice of the quantities \"x\" and \"y\" of two different types of output. The first order conditions for this optimization are π = 0 = π. Since both partial derivatives π and π will generally themselves be functions of both arguments \"x\" and \"y\", these two first order conditions form a system of two equations in two unknowns.\n\nPartial derivatives appear in thermodynamic equations like Gibbs-Duhem equation as well in other equations from mathematical physics. Here the variables being held constant in partial derivatives can be ratio of simple variables like mole fractions \"x\" in the following example involving the Gibbs energies in a ternary mixture system:\n\nExpress mole fractions of a component as functions of other components mole fraction and binary mole ratios:\n\nDifferential quotients can be formed at constant ratios like those above:\n\nRatios X, Y, Z of mole fractions can be written for ternary and multicomponent systems:\n\nwhich can be used for solving partial differential equations like:\n\nThis equality can be rearranged to have differential quotient of mole fractions on one side.\n\nPartial derivatives are key to target-aware image resizing algorithms. Widely known as seam carving, these algorithms require each pixel in an image to be assigned a numerical 'energy' to describe their dissimilarity against orthogonal adjacent pixels. The algorithm then progressively removes rows or columns with the lowest energy. The formula established to determine a pixel's energy (magnitude of gradient at a pixel) depends heavily on the constructs of partial derivatives.\n\nPartial derivatives play a prominent role in economics, in which most functions describing economic behaviour posit that the behaviour depends on more than one variable. For example, a societal consumption function may describe the amount spent on consumer goods as depending on both income and wealth; the marginal propensity to consume is then the partial derivative of the consumption function with respect to income.\n\nFor the following examples, let formula_16 be a function in formula_73 and formula_5.\n\nFirst-order partial derivatives:\n\nSecond-order partial derivatives:\n\nSecond-order mixed derivatives:\n\nHigher-order partial and mixed derivatives:\n\nWhen dealing with functions of multiple variables, some of these variables may be related to each other, thus it may be necessary to specify explicitly which variables are being held constant to avoid ambiguity. In fields such as statistical mechanics, the partial derivative of formula_16 with respect to formula_2, holding formula_13 and formula_5 constant, is often expressed as\n\nConventionally, for clarity and simplicity of notation, the partial derivative \"function\" and the \"value\" of the function at a specific point are conflated by including the function arguments when the partial derivative symbol (Leibniz notation) is used. Thus, an expression like \n\nis used for the function, while \n\nmight be used for the value of the function at the point formula_86. However, this convention breaks down when we want to evaluate the partial derivative at a point like formula_87. In such a case, evaluation of the function must be expressed in an unwieldy manner as \n\nor \n\nin order to use the Leibniz notation. Thus, in these cases, it may be preferable to use the Euler differential operator notation with formula_90 as the partial derivative symbol with respect to the \"i\"th variable. For instance, one would write formula_91 for the example described above, while the expression formula_92 represents the partial derivative \"function\" with respect to the 1st variable.\n\nFor higher order partial derivatives, the partial derivative (function) of formula_93 with respect to the \"j\"th variable is denoted formula_94. That is, formula_95, so that the variables are listed in the order in which the derivatives are taken, and thus, in reverse order of how the composition of operators is usually notated. Of course, Clairaut's theorem implies that formula_96 as long as comparatively mild regularity conditions on \"f\" are satisfied.\n\nThere is a concept for partial derivatives that is analogous to antiderivatives for regular derivatives. Given a partial derivative, it allows for the partial recovery of the original function.\n\nConsider the example of \n\nThe \"partial\" integral can be taken with respect to \"x\" (treating \"y\" as constant, in a similar manner to partial differentiation):\n\nHere, the \"constant\" of integration is no longer a constant, but instead a function of all the variables of the original function except \"x\". The reason for this is that all the other variables are treated as constant when taking the partial derivative, so any function which does not involve formula_2 will disappear when taking the partial derivative, and we have to account for this when we take the antiderivative. The most general way to represent this is to have the \"constant\" represent an unknown function of all the other variables.\n\nThus the set of functions formula_100, where \"g\" is any one-argument function, represents the entire set of functions in variables \"x\",\"y\" that could have produced the \"x\"-partial derivative formula_101.\n\nIf all the partial derivatives of a function are known (for example, with the gradient), then the antiderivatives can be matched via the above process to reconstruct the original function up to a constant. Unlike in the single-variable case, however, not every set of functions can be the set of all (first) partial derivatives of a single function. In other words, not every vector field is conservative.\n\nSecond and higher order partial derivatives are defined analogously to the higher order derivatives of univariate functions. For the function formula_102 the \"own\" second partial derivative with respect to \"x\" is simply the partial derivative of the partial derivative (both with respect to \"x\"):\n\nThe cross partial derivative with respect to \"x\" and \"y\" is obtained by taking the partial derivative of \"f\" with respect to \"x\", and then taking the partial derivative of the result with respect to \"y\", to obtain\n\nSchwarz's theorem states that if the second derivatives are continuous the expression for the cross partial derivative is unaffected by which variable the partial derivative is taken with respect to first and which is taken second. That is,\n\nor equivalently formula_106\n\nOwn and cross partial derivatives appear in the Hessian matrix which is used in the second order conditions in optimization problems.\n\n"}
{"id": "84029", "url": "https://en.wikipedia.org/wiki?curid=84029", "title": "Plane (geometry)", "text": "Plane (geometry)\n\nIn mathematics, a plane is a flat, two-dimensional surface that extends infinitely far. A plane is the two-dimensional analogue of a point (zero dimensions), a line (one dimension) and three-dimensional space. Planes can arise as subspaces of some higher-dimensional space, as with a room's walls extended infinitely far, or they may enjoy an independent existence in their own right, as in the setting of Euclidean geometry.\n\nWhen working exclusively in two-dimensional Euclidean space, the definite article is used, so, \"the\" plane refers to the whole space. Many fundamental tasks in mathematics, geometry, trigonometry, graph theory, and graphing are performed in a two-dimensional space, or, in other words, in the plane.\n\nEuclid set forth the first great landmark of mathematical thought, an axiomatic treatment of geometry. He selected a small core of undefined terms (called \"common notions\") and postulates (or axioms) which he then used to prove various geometrical statements. Although the plane in its modern sense is not directly given a definition anywhere in the \"Elements\", it may be thought of as part of the common notions. Euclid never used numbers to measure length, angle, or area. In this way the Euclidean plane is not quite the same as the Cartesian plane.\n\nA plane is a ruled surface.\n\nThis section is solely concerned with planes embedded in three dimensions: specifically, in R.\n\nIn a Euclidean space of any number of dimensions, a plane is uniquely determined by any of the following:\n\nThe following statements hold in three-dimensional Euclidean space but not in higher dimensions, though they have higher-dimensional analogues:\n\n\nIn a manner analogous to the way lines in a two-dimensional space are described using a point-slope form for their equations, planes in a three dimensional space have a natural description using a point in the plane and a vector orthogonal to it (the normal vector) to indicate its \"inclination\".\n\nSpecifically, let be the position vector of some point , and let be a nonzero vector. The plane determined by the point and the vector consists of those points , with position vector , such that the vector drawn from to is perpendicular to . Recalling that two vectors are perpendicular if and only if their dot product is zero, it follows that the desired plane can be described as the set of all points such that\nExpanded this becomes\nwhich is the \"point-normal\" form of the equation of a plane. This is just a linear equation\nwhere\nConversely, it is easily shown that if and are constants and , and are not all zero, then the graph of the equation\nis a plane having the vector as a normal. This familiar equation for a plane is called the \"general form\" of the equation of the plane.\n\nThus for example a regression equation of the form (with ) establishes a best-fit plane in three-dimensional space when there are two explanatory variables.\n\nAlternatively, a plane may be described parametrically as the set of all points of the form\nwhere \"s\" and \"t\" range over all real numbers, ' and are given linearly independent vectors defining the plane, and is the vector representing the position of an arbitrary (but fixed) point on the plane. The vectors ' and can be visualized as vectors starting at and pointing in different directions along the plane. Note that and can be perpendicular, but cannot be parallel.\n\nLet , , and be non-collinear points.\n\nThe plane passing through , , and can be described as the set of all points (x,y,z) that satisfy the following determinant equations:\n\nTo describe the plane by an equation of the form formula_8, solve the following system of equations:\n\nThis system can be solved using Cramer's rule and basic matrix manipulations. Let\n\nIf \"D\" is non-zero (so for planes not through the origin) the values for \"a\", \"b\" and \"c\" can be calculated as follows:\n\nThese equations are parametric in \"d\". Setting \"d\" equal to any non-zero number and substituting it into these equations will yield one solution set.\n\nThis plane can also be described by the \"point and a normal vector\" prescription above. A suitable normal vector is given by the cross product\nand the point can be taken to be any of the given points , or (or any other point in the plane).\n\nFor a plane formula_17 and a point formula_18 not necessarily lying on the plane, the shortest distance from formula_19 to the plane is\n\nIt follows that formula_19 lies in the plane if and only if \"D=0\".\n\nIf formula_22 meaning that \"a\", \"b\", and \"c\" are normalized then the equation becomes\n\nAnother vector form for the equation of a plane, known as the Hesse normal form relies on the parameter \"D\". This form is:\nwhere formula_25 is a unit normal vector to the plane, formula_26 a position vector of a point of the plane and \"D\" the distance of the plane from the origin.\n\nThe general formula for higher dimensions can be quickly arrived at using vector notation. Let the hyperplane have equation formula_27, where the formula_25 is a normal vector and formula_29 is a position vector to a point in the hyperplane. We desire the perpendicular distance to the point formula_30. The hyperplane may also be represented by the scalar equation formula_31, for constants formula_32. Likewise, a corresponding formula_25 may be represented as formula_34. We desire the scalar projection of the vector formula_35 in the direction of formula_25. Noting that formula_37 (as formula_38 satisfies the equation of the hyperplane) we have\n\nThe line of intersection between two planes formula_40 and formula_41 where formula_42 are normalized is given by\n\nwhere\n\nThis is found by noticing that the line must be perpendicular to both plane normals, and so parallel to their cross product formula_46 (this cross product is zero if and only if the planes are parallel, and are therefore non-intersecting or entirely coincident).\n\nThe remainder of the expression is arrived at by finding an arbitrary point on the line. To do so, consider that any point in space may be written as formula_47, since formula_48 is a basis. We wish to find a point which is on both planes (i.e. on their intersection), so insert this equation into each of the equations of the planes to get two simultaneous equations which can be solved for formula_49 and formula_50.\n\nIf we further assume that formula_51 and formula_52 are orthonormal then the closest point on the line of intersection to the origin is formula_53. If that is not the case, then a more complex procedure must be used.\n\nGiven two intersecting planes described by formula_54 and formula_55, the dihedral angle between them is defined to be the angle formula_56 between their normal directions:\n\nIn addition to its familiar geometric structure, with isomorphisms that are isometries with respect to the usual inner product, the plane may be viewed at various other levels of abstraction. Each level of abstraction corresponds to a specific category.\n\nAt one extreme, all geometrical and metric concepts may be dropped to leave the topological plane, which may be thought of as an idealized homotopically trivial infinite rubber sheet, which retains a notion of proximity, but has no distances. The topological plane has a concept of a linear path, but no concept of a straight line. The topological plane, or its equivalent the open disc, is the basic topological neighborhood used to construct surfaces (or 2-manifolds) classified in low-dimensional topology. Isomorphisms of the topological plane are all continuous bijections. The topological plane is the natural context for the branch of graph theory that deals with planar graphs, and results such as the four color theorem.\n\nThe plane may also be viewed as an affine space, whose isomorphisms are combinations of translations and non-singular linear maps. From this viewpoint there are no distances, but collinearity and ratios of distances on any line are preserved.\n\nDifferential geometry views a plane as a 2-dimensional real manifold, a topological plane which is provided with a differential structure. Again in this case, there is no notion of distance, but there is now a concept of smoothness of maps, for example a differentiable or smooth path (depending on the type of differential structure applied). The isomorphisms in this case are bijections with the chosen degree of differentiability.\n\nIn the opposite direction of abstraction, we may apply a compatible field structure to the geometric plane, giving rise to the complex plane and the major area of complex analysis. The complex field has only two isomorphisms that leave the real line fixed, the identity and conjugation.\n\nIn the same way as in the real case, the plane may also be viewed as the simplest, one-dimensional (over the complex numbers) complex manifold, sometimes called the complex line. However, this viewpoint contrasts sharply with the case of the plane as a 2-dimensional real manifold. The isomorphisms are all conformal bijections of the complex plane, but the only possibilities are maps that correspond to the composition of a multiplication by a complex number and a translation.\n\nIn addition, the Euclidean geometry (which has zero curvature everywhere) is not the only geometry that the plane may have. The plane may be given a spherical geometry by using the stereographic projection. This can be thought of as placing a sphere on the plane (just like a ball on the floor), removing the top point, and projecting the sphere onto the plane from this point). This is one of the projections that may be used in making a flat map of part of the Earth's surface. The resulting geometry has constant positive curvature.\n\nAlternatively, the plane can also be given a metric which gives it constant negative curvature giving the hyperbolic plane. The latter possibility finds an application in the theory of special relativity in the simplified case where there are two spatial dimensions and one time dimension. (The hyperbolic plane is a timelike hypersurface in three-dimensional Minkowski space.)\n\nThe one-point compactification of the plane is homeomorphic to a sphere (see stereographic projection); the open disk is homeomorphic to a sphere with the \"north pole\" missing; adding that point completes the (compact) sphere. The result of this compactification is a manifold referred to as the Riemann sphere or the complex projective line. The projection from the Euclidean plane to a sphere without a point is a diffeomorphism and even a conformal map.\n\nThe plane itself is homeomorphic (and diffeomorphic) to an open disk. For the hyperbolic plane such diffeomorphism is conformal, but for the Euclidean plane it is not.\n\n\n\n"}
{"id": "31376465", "url": "https://en.wikipedia.org/wiki?curid=31376465", "title": "PottersWheel", "text": "PottersWheel\n\nPottersWheel is a MATLAB toolbox for mathematical modeling of time-dependent dynamical systems that can be expressed as chemical reaction networks or ordinary differential equations (ODEs). It allows the automatic calibration of model parameters by fitting the model to experimental measurements. CPU-intensive functions are written or – in case of model dependent functions – dynamically generated in C. Modeling can be done interactively using graphical user interfaces or based on MATLAB scripts using the PottersWheel function library. The software is intended to support the work of a mathematical modeler as a real potter's wheel eases the modeling of pottery.\n\nThe basic use of PottersWheel covers seven phases from model creation to the prediction of new\nexperiments.\n\nThe dynamical system is formalized into a set of reactions or differential equations using a visual model designer or a text editor. The model is stored as a MATLAB *.m ASCII file. Modifications can therefore be tracked using a version control system like subversion or git. Model import and export is supported for SBML. Custom import-templates may be used to import custom model structures. Rule-based modeling is also supported, where a pattern represents a set of automatically generated reactions.\n\nExample for a simple model definition file for a reaction network A → B → C → A with observed species A and C:\n\nExternal data saved in *.xls or *.txt files can be added to a model creating a \"model-data-couple\". A mapping dialog allows to connect data column names to observed species names. Meta information in the data files comprise information about the experimental setting. Measurement errors are either stored in the data files, will be calculated using an error model, or are estimated automatically.\n\nTo fit a model to one or more data sets, the corresponding model-data-couples are combined into a \"fitting-assembly\". Parameters like initial values, rate constants, and scaling factors can be fitted in an experiment-wise or global fashion. The user may select from several numerical integrators, optimization algorithms, and calibration strategies like fitting in normal or logarithmic parameter space.\n\nThe quality of a fit is characterized by its \"chi-squared\" value. As a rule of thumb, for\n\"N\" fitted data points and \"p\" calibrated parameters, the chi-squared value should have a similar value\nas \"N\" − \"p\" or at least \"N\". Statistically, this is expressed using a chi-squared test resulting in a p-value above a significance threshold of e.g. 0.05. For lower p-values, the model is\n\n\nApart from further chi-squared based characteristics like AIC and BIC, data-model-residual analyses exist, e.g. to investigate whether the residuals follow a Gaussian distribution. Finally, parameter confidence intervals may be estimated using either the Fisher information matrix approximation or based on the profile-likelihood function, if parameters are not unambiguously identifiable.\n\nIf the fit is not acceptable, the model has to be refined and the procedure continues with step 2. Else, the dynamic model properties can be examined and predictions calculated.\n\nIf the model structure is not able to explain the experimental measurements, a set of physiologically reasonable alternative models should be created. In order to avoid redundant model paragraphs and copy-and-paste errors, this can be done using a common core-model which is the same for all variants. Then, \"daughter\"-models are created and fitted to the data, preferably using batch processing strategies based on MATLAB scripts. As a starting point to envision suitable model variants, the PottersWheel \"equalizer\" may be used to understand the dynamic behavior of the original system.\n\nA mathematical model may serve to display the concentration time-profile of unobserved species, to determine sensitive parameters representing potential targets within a clinical setting, or to calculate model characteristics like the half-life of a species.\n\nEach analysis step may be stored into a modeling report, which may be exported as a Latex-based PDF.\n\nAn experimental setting corresponds to specific characteristics of \"driving input functions\" and\ninitial concentrations. In a signal transduction pathway model the concentration of a ligand\nlike EGF may be controlled experimentally. The driving input designer allows investigating the effect of a continuous, ramp, or pulse stimulation in combination with varying initial concentrations using the equalizer. In order to discriminate competing model hypotheses, the designed experiment should have as different observable time-profiles as possible.\n\nMany dynamical systems can only be observed partially, i.e. not all system species are accessible experimentally. For biological applications the amount and quality of experimental data is often limited. In this setting parameters can be structurally or practically non-identifiable. Then, parameters may compensate each other and fitted parameter values strongly depend on initial guesses. In PottersWheel non-identifiability can be detected using the Profile Likelihood Approach. For characterizing functional relationships between the non-identifiable parameters PottersWheel applies random and systematic fit sequences.\n\n"}
{"id": "26754386", "url": "https://en.wikipedia.org/wiki?curid=26754386", "title": "Randomized rounding", "text": "Randomized rounding\n\nWithin computer science and operations research,\nmany combinatorial optimization problems are computationally intractable to solve exactly (to optimality).\nMany such problems do admit fast (polynomial time) approximation algorithms—that is, algorithms that are guaranteed to return an approximately optimal solution given any input.\n\nRandomized rounding\n\nis a widely used approach for designing and analyzing such approximation algorithms. \nThe basic idea is to use the probabilistic method\nto convert an optimal solution of a relaxation\nof the problem into an approximately optimal solution to the original problem.\n\nThe basic approach has three steps:\n\n(Although the approach is most commonly applied with linear programs,\nother kinds of relaxations are sometimes used.\nFor example, see Goeman's and Williamson's semi-definite programming-based\nMax-Cut approximation algorithm.)\n\nThe challenge in the first step is to choose a suitable integer linear program.\nFamiliarity with linear programming is required, in particular, familiarity with\nhow to model problems using linear programs and integer linear programs.\nBut, for many problems, there is a natural integer linear program that works well,\nsuch as in the Set Cover example below. (The integer linear program should have a small\nintegrality gap;\nindeed randomized rounding is often used to prove bounds on integrality gaps.)\n\nIn the second step, the optimal fractional solution can typically be computed\nin polynomial time\nusing any standard linear programming algorithm.\n\nIn the third step, the fractional solution must be converted into an integer solution\n(and thus a solution to the original problem).\nThis is called \"rounding\" the fractional solution.\nThe resulting integer solution should (provably) have cost\nnot much larger than the cost of the fractional solution.\nThis will ensure that the cost of the integer solution\nis not much larger than the cost of the optimal integer solution.\n\nThe main technique used to do the third step (rounding) is to use randomization,\nand then to use probabilistic arguments to bound the increase in cost due to the rounding\n(following the probabilistic method from combinatorics).\nThere, probabilistic arguments are used to show the existence of discrete structures with\ndesired properties. In this context, one uses such arguments to show the following:\n\nFinally, to make the third step computationally efficient,\none either shows that formula_3 approximates formula_1\nwith high probability (so that the step can remain randomized)\nor one derandomizes the rounding step,\ntypically using the method of conditional probabilities.\nThe latter method converts the randomized rounding process\ninto an efficient deterministic process that is guaranteed\nto reach a good outcome.\n\nThe randomized rounding step differs from most applications of the probabilistic method in two respects:\n\nThe following example illustrates how randomized rounding can be used to design an approximation algorithm for the Set Cover problem.\n\nFix any instance formula_14 of set cover over a universe formula_15.\n\nFor step 1, let IP be the standard integer linear program for set cover for this instance.\n\nFor step 2, let LP be the linear programming relaxation of IP,\nand compute an optimal solution formula_16 to LP\nusing any standard linear programming algorithm.\n\n(The feasible solutions to LP are the vectors formula_1\nthat assign each set formula_18\na non-negative weight formula_19,\nsuch that, for each element formula_20,\nformula_3 \"covers\" formula_22\n-- the total weight assigned to the sets containing formula_22\nis at least 1, that is,\nThe optimal solution formula_16\nis a feasible solution whose cost\nis as small as possible.)\nNote that any set cover formula_27 for formula_28\ngives a feasible solution formula_1\n(where formula_30 for formula_31,\nformula_32 otherwise).\nThe cost of this formula_27 equals the cost of formula_1, that is,\nIn other words, the linear program LP is a relaxation\nof the given set-cover problem.\n\nSince formula_16 has minimum cost among feasible solutions to the LP,\n\"the cost of formula_16 is a lower bound on the cost of the optimal set cover\".\n\nHere is a description of the third step—the rounding step,\nwhich must convert the minimum-cost fractional set cover formula_16\ninto a feasible integer solution formula_3 (corresponding to a true set cover).\n\nThe rounding step should produce an formula_3 that, with positive probability,\nhas cost within a small factor of the cost of formula_16.\nThen (since the cost of formula_16 is a lower bound on the cost of the optimal set cover),\nthe cost of formula_3 will be within a small factor of the optimal cost.\n\nAs a starting point, consider the most natural rounding scheme:\n\nWith this rounding scheme,\nthe expected cost of the chosen sets is at most formula_48,\nthe cost of the fractional cover.\nThis is good. Unfortunately the coverage is not good.\nWhen the variables formula_49 are small,\nthe probability that an element formula_22 is not covered is about\n\nSo only a constant fraction of the elements will be covered in expectation.\n\nTo make formula_3 cover every element with high probability,\nthe standard rounding scheme\nfirst \"scales up\" the rounding probabilities\nby an appropriate factor formula_53.\nHere is the standard rounding scheme:\n\nScaling the probabilities up by formula_59\nincreases the expected cost by formula_59,\nbut makes coverage of all elements likely.\nThe idea is to choose formula_59 as small\nas possible so that all elements are provably\ncovered with non-zero probability.\nHere is a detailed analysis.\n\n(Note: with care the formula_65\ncan be reduced to formula_67.)\n\nThe output formula_3 of the random rounding scheme has the desired properties\nas long as none of the following \"bad\" events occur:\n\nThe expectation of each formula_75 is at most formula_76.\nBy linearity of expectation,\nthe expectation of formula_69\nis at most formula_78.\nThus, by Markov's inequality, the probability of the first bad event\nabove is at most formula_79.\n\nFor the remaining bad events (one for each element formula_22), note that,\nsince formula_81 for any given element formula_22,\nthe probability that formula_22 is not covered is\n\n(This uses the inequality formula_85,\nwhich is strict for formula_86.)\n\nThus, for each of the formula_87 elements,\nthe probability that the element is not covered is less than formula_88.\n\nBy the naive union bound,\nthe probability that one of the formula_89 bad events happens\nis less than formula_90.\nThus, with positive probability there are no bad events\nand formula_3 is a set cover of cost at most formula_71.\nQED\n\nThe lemma above shows the \"existence\" of a set cover\nof cost formula_93).\nIn this context our goal is an efficient approximation algorithm,\nnot just an existence proof, so we are not done.\n\nOne approach would be to increase formula_59\na little bit, then show that the probability of success is at least, say, 1/4.\nWith this modification, repeating the random rounding step a few times\nis enough to ensure a successful outcome with high probability.\n\nThat approach weakens the approximation ratio.\nWe next describe a different approach that yields\na deterministic algorithm that is guaranteed to\nmatch the approximation ratio of the existence proof above.\nThe approach is called the method of conditional probabilities.\n\nThe deterministic algorithm emulates the randomized rounding scheme:\nit considers each set formula_44 in turn,\nand chooses formula_96.\nBut instead of making each choice \"randomly\" based on formula_16,\nit makes the choice \"deterministically\", so as to\n\"keep the conditional probability of failure, given the choices so far, below 1\".\n\nWe want to be able to set each variable formula_75 in turn\nso as to keep the conditional probability of failure below 1.\nTo do this, we need a good bound on the conditional probability of failure.\nThe bound will come by refining the original existence proof.\nThat proof implicitly bounds the probability of failure\nby the expectation of the random variable\nwhere\nis the set of elements left uncovered at the end.\n\nThe random variable formula_101 may appear a bit mysterious,\nbut it mirrors the probabilistic proof in a systematic way.\nThe first term in formula_101 comes from applying Markov's inequality\nto bound the probability of the first bad event (the cost is too high).\nIt contributes at least 1 to formula_101 if the cost of formula_3 is too high.\nThe second term\ncounts the number of bad events of the second kind (uncovered elements).\nIt contributes at least 1 to formula_101 if formula_3 leaves any element uncovered.\nThus, in any outcome where formula_101 is less than 1,\nformula_3 must cover all the elements\nand have cost meeting the desired bound from the lemma.\nIn short, if the rounding step fails, then formula_109.\nThis implies (by Markov's inequality) that\n\"formula_110 is an upper bound on the probability of failure.\"\nNote that the argument above is implicit already in the proof of the lemma,\nwhich also shows by calculation that formula_111.\n\nTo apply the method of conditional probabilities,\nwe need to extend the argument to bound the \"conditional\" probability of failure\nas the rounding step proceeds.\nUsually, this can be done in a systematic way,\nalthough it can be technically tedious.\n\nSo, what about the \"conditional\" probability of failure as the rounding step iterates through the sets?\nSince formula_109 in any outcome where the rounding step fails,\nby Markov's inequality, the \"conditional\" probability of failure\nis at most the \"conditional\" expectation of formula_101.\n\nNext we calculate the conditional expectation of formula_101,\nmuch as we calculated the unconditioned expectation of formula_101 in the original proof.\nConsider the state of the rounding process at the end of some iteration formula_116.\nLet formula_117 denote the sets considered so far\n(the first formula_116 sets in formula_28).\nLet formula_120 denote the (partially assigned) vector formula_3\n(so formula_122 is determined only if formula_123).\nFor each set formula_124,\nlet formula_125\ndenote the probability with which formula_75 will be set to 1.\nLet formula_127 contain the not-yet-covered elements.\nThen the conditional expectation of formula_101,\ngiven the choices made so far, that is, given formula_120, is\n\nNote that formula_131 is determined only after iteration formula_116.\n\nTo keep the conditional probability of failure below 1,\nit suffices to keep the conditional expectation of formula_101 below 1.\nTo do this, it suffices to keep the conditional expectation of formula_101 from increasing.\nThis is what the algorithm will do.\nIt will set formula_75 in each iteration to ensure that\n(where formula_137).\n\nIn the formula_116th iteration,\nhow can the algorithm set formula_139\nto ensure that formula_140?\nIt turns out that it can simply set formula_139\nso as to \"minimize\" the resulting value of formula_142.\n\nTo see why, focus on the point in time when iteration formula_116 starts.\nAt that time, formula_144 is determined,\nbut formula_142 is not yet determined\n--- it can take two possible values depending on how formula_139\nis set in iteration formula_116.\nLet formula_148 denote the value of formula_149.\nLet formula_150 and formula_151,\ndenote the two possible values of formula_142,\ndepending on whether formula_139 is set to 0, or 1, respectively.\nBy the definition of conditional expectation,\nSince a weighted average of two quantities\nis always at least the minimum of those two quantities,\nit follows that\nThus, setting formula_139\nso as to minimize the resulting value of\nformula_131\nwill guarantee that\nformula_158.\nThis is what the algorithm will do.\n\nIn detail, what does this mean?\nConsidered as a function of formula_139\nformula_131\nis a linear function of formula_139,\nand the coefficient of formula_139 in that function is\n\nThus, the algorithm should set formula_139 to 0 if this expression is positive,\nand 1 otherwise. This gives the following algorithm.\n\ninput: set system formula_28, universe formula_15, cost vector formula_167\n\noutput: set cover formula_3 (a solution to the standard integer linear program for set cover)\n\nThe algorithm ensures that the conditional expectation of formula_101,\nformula_185, does not increase at each iteration.\nSince this conditional expectation is initially less than 1 (as shown previously),\nthe algorithm ensures that the conditional expectation stays below 1.\nSince the conditional probability of failure\nis at most the conditional expectation of formula_101,\nin this way the algorithm\nensures that the conditional probability of failure stays below 1.\nThus, at the end, when all choices are determined,\nthe algorithm reaches a successful outcome.\nThat is, the algorithm above returns a set cover formula_3\nof cost at most formula_183 times\nthe minimum cost of any (fractional) set cover.\n\nIn the example above, the algorithm was guided by the conditional expectation of a random variable formula_101.\nIn some cases, instead of an exact conditional expectation,\nan \"upper bound\" (or sometimes a lower bound)\non some conditional expectation is used instead.\nThis is called a pessimistic estimator.\n\n\n\n"}
{"id": "326365", "url": "https://en.wikipedia.org/wiki?curid=326365", "title": "Reverse mathematics", "text": "Reverse mathematics\n\nReverse mathematics is a program in mathematical logic that seeks to determine which axioms are required to prove theorems of mathematics. Its defining method can briefly be described as \"going backwards from the theorems to the axioms\", in contrast to the ordinary mathematical practice of deriving theorems from axioms. It can be conceptualized as sculpting out necessary conditions from sufficient ones.\n\nThe reverse mathematics program was foreshadowed by results in set theory such as the classical theorem that the axiom of choice and Zorn's lemma are equivalent over ZF set theory. The goal of reverse mathematics, however, is to study possible axioms of ordinary theorems of mathematics rather than possible axioms for set theory.\n\nReverse mathematics is usually carried out using subsystems of second-order arithmetic, where many of its definitions and methods are inspired by previous work in constructive analysis and proof theory. The use of second-order arithmetic also allows many techniques from recursion theory to be employed; many results in reverse mathematics have corresponding results in computable analysis.\n\nThe program was founded by and brought forward by Steve Simpson. A standard reference for the subject is , while an introduction for non-specialists is .\n\nIn reverse mathematics, one starts with a framework language and a base theory—a core axiom system—that is too weak to prove most of the theorems one might be interested in, but still powerful enough to develop the definitions necessary to state these theorems. For example, to study the theorem “Every bounded sequence of real numbers has a supremum” it is necessary to use a base system which can speak of real numbers and sequences of real numbers.\n\nFor each theorem that can be stated in the base system but is not provable in the base system, the goal is to determine the particular axiom system (stronger than the base system) that is necessary to prove that theorem. To show that a system \"S\" is required to prove a theorem \"T\", two proofs are required. The first proof shows \"T\" is provable from \"S\"; this is an ordinary mathematical proof along with a justification that it can be carried out in the system \"S\". The second proof, known as a reversal, shows that \"T\" itself implies \"S\"; this proof is carried out in the base system. The reversal establishes that no axiom system \"S′\" that extends the base system can be weaker than \"S\" while still proving \"T\".\n\nMost reverse mathematics research focuses on subsystems of second-order arithmetic. The body of research in reverse mathematics has established that weak subsystems of second-order arithmetic suffice to formalize almost all undergraduate-level mathematics. In second-order arithmetic, all objects can be represented as either natural numbers or sets of natural numbers. For example, in order to prove theorems about real numbers, the real numbers can be represented as Cauchy sequences of rational numbers, each of which can be represented as a set of natural numbers.\n\nThe axiom systems most often considered in reverse mathematics are defined using axiom schemes called comprehension schemes. Such a scheme states that any set of natural numbers definable by a formula of a given complexity exists. In this context, the complexity of formulas is measured using the arithmetical hierarchy and analytical hierarchy.\n\nThe reason that reverse mathematics is not carried out using set theory as a base system is that the language of set theory is too expressive. Extremely complex sets of natural numbers can be defined by simple formulas in the language of set theory (which can quantify over arbitrary sets). In the context of second-order arithmetic, results such as Post's theorem establish a close link between the complexity of a formula and the (non)computability of the set it defines.\n\nAnother effect of using second-order arithmetic is the need to restrict general mathematical theorems to forms that can be expressed within arithmetic. For example, second-order arithmetic can express the principle \"Every countable vector space has a basis\" but it cannot express the principle \"Every vector space has a basis\". In practical terms, this means that theorems of algebra and combinatorics are restricted to countable structures, while theorems of analysis and topology are restricted to separable spaces. Many principles that imply the axiom of choice in their general form (such as \"Every vector space has a basis\") become provable in weak subsystems of second-order arithmetic when they are restricted. For example, \"every field has an algebraic closure\" is not provable in ZF set theory, but the restricted form \"every countable field has an algebraic closure\" is provable in RCA, the weakest system typically employed in reverse mathematics.\n\nA recent strand of \"higher-order\" reverse mathematics research, initiated by Ulrich Kohlenbach, focuses on subsystems of higher-order arithmetic (). \nDue to the richer language of higher-order arithmetic, the use of representations (aka 'codes') common in second-order arithmetic, is greatly reduced. \nFor example, a continuous function on the Cantor space is just a function that maps binary sequences to binary sequences, and that also satisfies the usual 'epsilon-delta'-definition of continuity. \n\nHigher-order reverse mathematics includes higher-order versions of (second-order) comprehension schemes. Such an higher-order axiom states the existence of a functional that decides the truth or falsity of formulas of a given complexity. In this context, the complexity of formulas is also measured using the arithmetical hierarchy and analytical hierarchy. The higher-order counterparts of the major subsystems of second-order arithmetic generally prove the same second-order sentences (or a large subset) as the original second-order systems (see and ). For instance, the base theory of higher-order reverse mathematics, called RCA, proves the same sentences as RCA, up to language. \n\nAs noted in the previous paragraph, second-order comprehension axioms easily generalize to the higher-order framework. However, theorems expressing the \"compactness\" of basic spaces behave quite differently in second- and higher-order arithmetic: on one hand, when restricted to countable covers/the language of second-order arithmetic, the compactness of the unit interval is provable in WKL from the next section. On the other hand, given uncountable covers/the language of higher-order arithmetic, the compactness of the unit interval is only provable from (full) second-order arithmetic (). Other covering lemmas (e.g. due to Lindelöf, Vitali, Besicovitch, etc) exhibit the same behavior, and many basic properties of the gauge integral are equivalent to the compactness of the underlying space.\n\nSecond-order arithmetic is a formal theory of the natural numbers and sets of natural numbers. Many mathematical objects, such as countable rings, groups, and fields, as well as points in effective Polish spaces, can be represented as sets of natural numbers, and modulo this representation can be studied in second-order arithmetic.\n\nReverse mathematics makes use of several subsystems of second-order arithmetic. A typical reverse mathematics theorem shows that a particular mathematical theorem \"T\" is equivalent to a particular subsystem \"S\" of second-order arithmetic over a weaker subsystem \"B\". This weaker system \"B\" is known as the base system for the result; in order for the reverse mathematics result to have\nmeaning, this system must not itself be able to prove the mathematical theorem \"T\".\n\nΠ-CA.\n\nThe following table summarizes the \"big five\" systems \nThe subscript in these names means that the induction scheme has been restricted from the full second-order induction scheme . For example, ACA includes the induction axiom (0 ∈ \"X\" ∧ ∀\"n\"(\"n\" ∈ \"X\" → \"n\" + 1 ∈ \"X\")) → ∀\"n\" \"n\" ∈ X. This together with the full comprehension axiom of second-order arithmetic implies the full second-order induction scheme given by the universal closure of (φ(0) ∧ ∀n(φ(n) → φ(n+1))) → ∀n φ(n) for any second-order formula φ. However ACA does not have the full comprehension axiom, and the subscript is a reminder that it does not have the full second-order induction scheme either. This restriction is important: systems with restricted induction have significantly lower proof-theoretical ordinals than systems with the full second-order induction scheme.\n\nRCA is the fragment of second-order arithmetic whose axioms are the axioms of Robinson arithmetic, induction for Σ formulas, and comprehension for Δ formulas.\n\nThe subsystem RCA is the one most commonly used as a base system for reverse mathematics. The initials \"RCA\" stand for \"recursive comprehension axiom\", where \"recursive\" means \"computable\", as in recursive function. This name is used because RCA corresponds informally to \"computable mathematics\". In particular, any set of natural numbers that can be proven to exist in RCA is computable, and thus any theorem which implies that noncomputable sets exist is not provable in RCA. To this extent, RCA is a constructive system, although it does not meet the requirements of the program of constructivism because it is a theory in classical logic including the law of excluded middle.\n\nDespite its seeming weakness (of not proving any noncomputable sets exist), RCA is sufficient to prove a number of classical theorems which, therefore, require only minimal logical strength. These theorems are, in a sense, below the reach of the reverse mathematics enterprise because they are already provable in the base system. The classical theorems provable in RCA include:\n\nThe first-order part of RCA (the theorems of the system that do not involve any set variables) is the set of theorems of first-order Peano arithmetic with induction limited to Σ formulas. It is provably consistent, as is RCA, in full first-order Peano arithmetic.\n\nThe subsystem WKL consists of RCA plus a weak form of Kőnig's lemma, namely the statement that every infinite subtree of the full binary tree (the tree of all finite sequences of 0's and 1's) has an infinite path. This proposition, which is known as \"weak Kőnig's lemma\", is easy to state in the language of second-order arithmetic. WKL can also be defined as the principle of Σ separation (given two Σ formulas of a free variable \"n\" which are exclusive, there is a class containing all \"n\" satisfying the one and no \"n\" satisfying the other).\n\nThe following remark on terminology is in order. The term “weak Kőnig's lemma” refers to the sentence which says that any infinite subtree of the binary tree has an infinite path. When this axiom is added to RCA, the resulting subsystem is called WKL. A similar distinction between particular axioms, on the one hand, and subsystems including the basic axioms and induction, on the other hand, is made for the stronger subsystems described below.\n\nIn a sense, weak Kőnig's lemma is a form of the axiom of choice (although, as stated, it can be proven in classical Zermelo–Fraenkel set theory without the axiom of choice). It is not constructively valid in some senses of the word constructive.\n\nTo show that WKL is actually stronger than (not provable in) RCA, it is sufficient to exhibit a theorem of WKL which implies that noncomputable sets exist. This is not difficult; WKL implies the existence of separating sets for effectively inseparable recursively enumerable sets.\n\nIt turns out that RCA and WKL have the same first-order part, meaning that they prove the same first-order sentences. WKL can prove a good number of classical mathematical results which do not follow from RCA, however. These results are not expressible as first-order statements but can be expressed as second-order statements.\n\nThe following results are equivalent to weak Kőnig's lemma and thus to WKL over RCA:\n\n\nACA is RCA plus the comprehension scheme for arithmetical formulas (which is sometimes called the \"arithmetical comprehension axiom\"). That is, ACA allows us to form the set of natural numbers satisfying an arbitrary arithmetical formula (one with no bound set variables, although possibly containing set parameters). Actually, it suffices to add to RCA the comprehension scheme for Σ formulas in order to obtain full arithmetical comprehension.\n\nThe first-order part of ACA is exactly first-order Peano arithmetic; ACA is a \"conservative\" extension of first-order Peano arithmetic. The two systems are provably (in a weak system) equiconsistent. ACA can be thought of as a framework of predicative mathematics, although there are predicatively provable theorems that are not provable in ACA. Most of the fundamental results about the natural numbers, and many other mathematical theorems, can be proven in this system.\n\nOne way of seeing that ACA is stronger than WKL is to exhibit a model of WKL that doesn't contain all arithmetical sets. In fact, it is possible to build a model of WKL consisting entirely of low sets using the low basis theorem, since low sets relative to low sets are low.\n\nThe following assertions are equivalent to ACA\nover RCA:\n\nThe system ATR adds to ACA an axiom which states, informally, that any arithmetical functional (meaning any arithmetical formula with a free number variable \"n\" and a free class variable \"X\", seen as the operator taking \"X\" to the set of \"n\" satisfying the formula) can be iterated transfinitely along any countable well ordering starting with any set. ATR is equivalent over ACA to the principle of Σ separation. ATR is impredicative, and has the proof-theoretic ordinal formula_1, the supremum of that of predicative systems.\n\nATR proves the consistency of ACA, and thus by Gödel's theorem it is strictly stronger.\n\nThe following assertions are equivalent to\nATR over RCA:\n\nΠ-CA is stronger than arithmetical transfinite recursion and is fully impredicative. It consists of RCA plus the comprehension scheme for Π formulas.\n\nIn a sense, Π-CA comprehension is to arithmetical transfinite recursion (Σ separation) as ACA is to weak Kőnig's lemma (Σ separation). It is equivalent to several statements of descriptive set theory whose proofs make use of strongly impredicative arguments; this equivalence shows that these impredicative arguments cannot be removed.\n\nThe following theorems are equivalent to Π-CA over RCA:\n\n\nThe ω in ω-model stands for the set of non-negative integers (or finite ordinals). An ω-model is a model for a fragment of second-order arithmetic whose first-order part is the standard model of Peano arithmetic, but whose second-order part may be non-standard. More precisely, an ω-model is given by a choice \"S\"⊆2 of subsets of ω. The first-order variables are interpreted in the usual way as elements of ω, and +, × have their usual meanings, while second-order variables are interpreted as elements of \"S\". There is a standard ω model where one just takes \"S\" to consist of all subsets of the integers. However, there are also other ω-models; for example, RCA has a minimal ω-model where \"S\" consists of the recursive subsets of ω.\n\nA β model is an ω model that is equivalent to the standard ω-model for Π and Σ sentences (with parameters).\n\nNon-ω models are also useful, especially in the proofs of conservation theorems.\n\n\n\n"}
{"id": "38262597", "url": "https://en.wikipedia.org/wiki?curid=38262597", "title": "SRM Engine Suite", "text": "SRM Engine Suite\n\nThe SRM Engine Suite is an engineering software tool used for simulating fuels, combustion and exhaust gas emissions in internal combustion engine (IC engine) applications. It is used worldwide by leading IC engine development organisations and fuel companies. The software is developed, maintained and supported by CMCL Innovations, Cambridge, U.K..\n\nThe software has been applied to simulate almost all engine applications and all transportation fuel combinations with many examples published in numerous leading peer-reviewed journals, a brief summary of these articles is presented here.\n\n\nThe software is based on the stochastic reactor model (SRM), which is stated in terms of a weighted stochastic particle ensemble. SRM is particular useful in the context of engine modelling \nas the dynamics of the particle ensemble includes detailed chemical kinetics whilst accounting for inhomogeneity in composition and temperature space arising from on-going fuel injection, heat transfer and turbulence mixing events. Through this coupling, heat release profiles and in particular the associated exhaust gas emissions (Particulates, NOx, Carbon monoxide, Unburned hydrocarbon \"etc\".) can be predicted more accurately than if using the more conventional approaches of standard homogenous and multi-zone reactor methods.\n\nThe software can be coupled as a plug-in into 1D engine cycle software tools, are capable of simulating the combustion and emissions during closed volume period of the cycle (combustion, TDC and negative valve overlap).\n\nAn advanced Application programming interface enables for the model to be coupled with a user-defined codes such as 3D-CFD or control software.\n\n"}
{"id": "67903", "url": "https://en.wikipedia.org/wiki?curid=67903", "title": "Squaring the square", "text": "Squaring the square\n\nSquaring the square is the problem of tiling an integral square using only other integral squares. (An integral square is a square whose sides have integer length.) The name was coined in a humorous analogy with squaring the circle. Squaring the square is an easy task unless additional conditions are set. The most studied restriction is that the squaring be perfect, meaning that the sizes of the smaller squares are all different. A related problem is squaring the plane, which can be done even with the restriction that each natural number occurs exactly once as a size of a square in the tiling. The order of a squared square is its number of constituent squares.\n\nA \"perfect\" squared square is a square such that each of the smaller squares has a different size.\n\nIt is first recorded as being studied by R. L. Brooks, C. A. B. Smith, A. H. Stone and W. T. Tutte at Cambridge University between 1936 and 1938.\nThey transformed the square tiling into an equivalent electrical circuit — they called it \"Smith diagram\" — by considering the squares as resistors that connected to their neighbors at their top and bottom edges, and then applied Kirchhoff's circuit laws and circuit decomposition techniques to that circuit. The first perfect squared squares they found were of order 69.\n\nThe first perfect squared square to be published, a compound one of side 4205 and order 55, was found by Roland Sprague in 1939.\n\nMartin Gardner published an extensive article written by W. T. Tutte about the early history of squaring the square in his mathematical games column in November 1958.\n\nA \"simple\" squared square is one where no subset of the squares forms a rectangle or square, otherwise it is \"compound\".\n\nIn 1978, A. J. W. Duijvestijn discovered a simple perfect squared square of side 112 with the smallest number of squares using a computer search. His tiling uses 21 squares, and has been proved to be minimal. This squared square forms the logo of the Trinity Mathematical Society.\n\nDuijvestijn also found 2 simple perfect squared squares of sides 110 but each comprising 22 squares. T.H. Willcocks found another. In 1999, I. Gambini proved that these 3 are the smallest perfect squared squares in terms of side length.\n\nThe perfect compound squared square with the fewest squares was discovered by T.H. Willcocks in 1946 and has 24 squares; however, it was not until 1982 that Duijvestijn, Pasquale Joseph Federico and P. Leeuw mathematically proved it to be the lowest-order example.\n\nWhen the constraint of all the squares being different sizes is relaxed, a squared square such that the side lengths of the smaller squares do not have a common divisor larger than 1 is called a \"Mrs. Perkins's quilt\". In other words, the greatest common divisor of all the smaller side lengths should be 1.\n\nThe Mrs. Perkins's quilt problem is to find a Mrs. Perkins's quilt with the fewest pieces for a given square.\n\nA cute number means a positive integer  such that some square admits a dissection into squares of no more than two different sizes, without other restrictions. It can be shown that aside from 2,3, and 5, every positive integer is cute.\n\nIn 1975, Solomon Golomb raised the question whether the whole plane can be tiled by squares, one of each integer edge-length, which he called the heterogeneous tiling conjecture. This problem was later publicized by Martin Gardner in his Scientific American column and appeared in several books, but it defied solution for over 30 years.\n\nIn \"Tilings and Patterns\", published in 1987, Branko Grünbaum and G. C. Shephard stated that in all perfect integral tilings of the plane known at that time, the sizes of the squares grew exponentially. For example, the plane can be tiled with different integral squares, but not for every integer, by recursively taking any perfect squared square and enlarging it so that the formerly smallest tile now has the size of the original squared square, then replacing this tile with a copy of the original squared square.\n\nIn 2008 James Henle and Frederick Henle proved that this, in fact, can be done. Their proof is constructive and proceeds by \"puffing up\" an L-shaped region formed by two side-by-side and horizontally flush squares of different sizes to a perfect tiling of a larger rectangular region, then adjoining the square of the smallest size not yet used to get another, larger L-shaped region. The squares added during the puffing up procedure have sizes that have not yet appeared in the construction and the procedure is set up so that the resulting rectangular regions are expanding in all four directions, which leads to a tiling of the whole plane.\n\nCubing the cube is the analogue in three dimensions of squaring the square: that is, given a cube \"C\", the problem of dividing it into finitely many smaller cubes, no two congruent.\n\nUnlike the case of squaring the square, a hard yet solvable problem, there is no perfect cubed cube and, more generally, no dissection of a rectangular cuboid \"C\" into a finite number of unequal cubes.\n\nSuppose that there is such a dissection. Make a face of \"C\" its horizontal base. The base is divided into a perfect squared rectangle \"R\" by the cubes which rest on it. Each corner square of \"R\" has a smaller adjacent edge square, and \"R\"'s smallest edge square is adjacent to smaller squares not on the edge. Therefore, the smallest square \"s\" in \"R\" is surrounded by \"larger\", and therefore \"higher\", cubes on all four sides. Hence the upper face of the cube on \"s\" is divided into a perfect squared square by the cubes which rest on it. Let \"s\" be the smallest square in this dissection. The sequence of squares \"s\", \"s\", ... is infinite and the corresponding cubes are infinite in number. This contradicts our original supposition.\n\nIf a 4-dimensional hypercube could be perfectly hypercubed then its 'faces' would be perfect cubed cubes; this is impossible. Similarly, there is no solution for all cubes of higher dimensions.\n\n\n"}
{"id": "744802", "url": "https://en.wikipedia.org/wiki?curid=744802", "title": "Stein's lemma", "text": "Stein's lemma\n\nStein's lemma, named in honor of Charles Stein, is a theorem of probability theory that is of interest primarily because of its applications to statistical inference — in particular, to James–Stein estimation and empirical Bayes methods — and its applications to portfolio choice theory. The theorem gives a formula for the covariance of one random variable with the value of a function of another, when the two random variables are jointly normally distributed.\n\nSuppose \"X\" is a normally distributed random variable with expectation μ and variance σ. Further suppose \"g\" is a function for which the two expectations E(\"g\"(\"X\") (\"X\" − μ) ) and E( \"g\" ′(\"X\") ) both exist (the existence of the expectation of any random variable is equivalent to the finiteness of the expectation of its absolute value). Then\n\nIn general, suppose \"X\" and \"Y\" are jointly normally distributed. Then\n\nIn order to prove the univariate version of this lemma, recall that the probability density function for the normal distribution with expectation 0 and variance 1 is\n\nand that for a normal distribution with expectation μ and variance σ is\n\nThen use integration by parts.\n\nSuppose \"X\" is in an exponential family, that is, \"X\" has the density\n\nSuppose this density has support formula_6 where formula_7 could be formula_8 and as formula_9,formula_10 where formula_11 is any differentiable function such that formula_12 or formula_13 if formula_7 finite. Then \n\nThe derivation is same as the special case, namely, integration by parts.\n\nIf we only know formula_16 has support formula_17, then it could be the case that formula_18 but formula_19. To see this, simply put formula_20 and formula_21 with infinitely spikes towards infinity but still integrable. One such example could be adapted from formula_22 so that formula_23 is smooth. \n\nExtensions to elliptically-contoured distributions also exist.\n\n"}
{"id": "54952", "url": "https://en.wikipedia.org/wiki?curid=54952", "title": "Technical drawing", "text": "Technical drawing\n\nTechnical drawing, drafting or drawing, is the act and discipline of composing drawings that visually communicate how something functions or is constructed.\n\nTechnical drawing is essential for communicating ideas in industry and engineering.\nTo make the drawings easier to understand, people use familiar symbols, perspectives, units of measurement, notation systems, visual styles, and page layout. Together, such conventions constitute a visual language and help to ensure that the drawing is unambiguous and relatively easy to understand. Many of the symbols and principles of technical drawing are codified in an international standard called ISO 128.\n\nThe need for precise communication in the preparation of a functional document distinguishes technical drawing from the expressive drawing of the visual arts. Artistic drawings are subjectively interpreted; their meanings are multiply determined. Technical drawings are understood to have one intended meaning.\n\nA \"drafter\", \"draftsperson\", or \"draughtsman\" is a person who makes a drawing (technical or expressive). A professional drafter who makes technical drawings is sometimes called a \"drafting technician\".\n\nA sketch is a quickly executed, freehand drawing that is usually not intended as a finished work. In general, sketching is a quick way to record an idea for later use. Architect's sketches primarily serve as a way to try out different ideas and establish a composition before a more finished work, especially when the finished work is expensive and time-consuming.\n\nArchitectural sketches, for example, are a kind of diagrams. These sketches, like metaphors, are used by architects as a means of communication in aiding design collaboration. This tool helps architects to abstract attributes of hypothetical provisional design solutions and summarize their complex patterns, hereby enhancing the design process.\n\nThe basic drafting procedure is to place a piece of paper (or other material) on a smooth surface with right-angle corners and straight sides—typically a drawing board. A sliding straightedge known as a \"T-square\" is then placed on one of the sides, allowing it to be slid across the side of the table, and over the surface of the paper.\n\n\"Parallel lines\" can be drawn simply by moving the T-square and running a pencil or technical pen along the T-square's edge. The T-square is used to hold other devices such as set squares or triangles. In this case, the drafter places one or more triangles of known angles on the T-square—which is itself at right angles to the edge of the table—and can then draw lines at any chosen angle to others on the page. Modern drafting tables come equipped with a drafting machine that is supported on both sides of the table to slide over a large piece of paper. Because it is secured on both sides, lines drawn along the edge are guaranteed to be parallel.\n\nIn addition, the drafter uses several technical drawing tools to draw curves and circles. Primary among these are the compasses, used for drawing simple arcs and circles, and the French curve, for drawing curves. A spline is a rubber coated articulated metal that can be manually bent to most curves.\n\nDrafting templates assist the drafter with creating recurring objects in a drawing without having to reproduce the object from scratch every time. This is especially useful when using common symbols; i.e. in the context of stagecraft, a lighting designer will draw from the USITT standard library of lighting fixture symbols to indicate the position of a common fixture across multiple positions. Templates are sold commercially by a number of vendors, usually customized to a specific task, but it is also not uncommon for a drafter to create his own templates.\n\nThis basic drafting system requires an accurate table and constant attention to the positioning of the tools. A common error is to allow the triangles to push the top of the T-square down slightly, thereby throwing off all angles. Even tasks as simple as drawing two angled lines meeting at a point require a number of moves of the T-square and triangles, and in general, drafting can be a time-consuming process.\n\nA solution to these problems was the introduction of the mechanical \"drafting machine\", an application of the pantograph (sometimes referred to incorrectly as a \"pentagraph\" in these situations) which allowed the drafter to have an accurate right angle at any point on the page quite quickly. These machines often included the ability to change the angle, thereby removing the need for the triangles as well.\n\nIn addition to the mastery of the mechanics of drawing lines, arcs and circles (and text) onto a piece of paper—with respect to the detailing of physical objects—the drafting effort requires a thorough understanding of geometry, trigonometry and spatial comprehension, and in all cases demands precision and accuracy, and attention to detail of high order.\n\nAlthough drafting is sometimes accomplished by a project engineer, architect, or shop personnel (such as a machinist), skilled drafters (and/or designers) usually accomplish the task, and are always in demand to some degree.\n\nToday, the mechanics of the drafting task have largely been automated and accelerated through the use of computer-aided design systems (CAD).\n\nThere are two types of computer-aided design systems used for the production\nof technical drawings\" two dimensions (\"2D\") and three dimensions (\"3D\").\n\n2D CAD systems such as AutoCAD or MicroStation replace the paper drawing discipline. The lines, circles, arcs, and curves are created within the software. It is down to the technical drawing skill of the user to produce the drawing. There is still much scope for error in the drawing when producing first and third angle orthographic projections, auxiliary projections and cross sections. A 2D CAD system is merely an electronic drawing board. Its greatest strength over direct to paper technical drawing is in the making of revisions. Whereas in a conventional hand drawn technical drawing, if a mistake is found, or a modification is required, a new drawing must be made from scratch, the 2D CAD system allows a copy of the original to be modified, saving considerable time. 2D CAD systems can be used to create plans for large projects such as buildings and aircraft but provide no way to check the various components will fit together.\nA 3D CAD system (such as KeyCreator, Autodesk Inventor, or SolidWorks) first produces the geometry of the part; the technical drawing comes from user defined views of that geometry. Any orthographic, projected or sectioned view is created by the software. There is no scope for error in the production of these views. The main scope for error comes in setting the parameter of first or third angle projection and displaying the relevant symbol on the technical drawing. 3D CAD allows individual parts to be assembled together to represent the final product. Buildings, aircraft, ships, and cars are modeled, assembled, and checked in 3D before technical drawings are released for manufacture.\n\nBoth 2D and 3D CAD systems can be used to produce technical drawings for any discipline. The various disciplines (electrical, electronic, pneumatic, hydraulic, etc.) have industry recognized symbols to represent common components.\n\nBS and ISO produce standards to show recommended practices but it is up to individuals to produce the drawings. There is no definitive standard for layout or style. The only standard across engineering workshop drawings is in the creation of orthographic projections and cross section views.\n\nDrafting can represent two dimensions (\"2D\") and three dimensions (\"3D\") although the representation itself is always created in 2D (cf. Architectural model). Drafting is the integral communication of technical or engineering drawings and is the industrial arts sub-discipline that underlies all involved technical endeavors.\n\nIn representing complex, three-dimensional objects in two-dimensional drawings, the objects can be described by at least one view plus material thickness note, 2, 3 or as many views and sections that are required to show all features of object.\n\nThe art and design that goes into making buildings is known as \"architecture\". To communicate all aspects of the shape or design, detail drawings are used. In this field, the term \"plan\" is often used when referring to the full section view of these drawings as viewed from three feet above finished floor to show the locations of doorways, windows, stairwells, etc. Architectural drawings describe and document an architect's design.\n\nEngineering can be a very broad term. It stems from the Latin \"ingenerare\", meaning \"to create\". Because this could apply to everything that humans create, it is given a narrower definition in the context of technical drawing. Engineering drawings generally deal with mechanical engineered items, such as manufactured parts and equipment.\nEngineering drawings are usually created in accordance with standardized conventions for layout, nomenclature, interpretation, appearance (such as typefaces and line styles), size, etc.\n\nIts purpose is to accurately and unambiguously capture all the geometric features of a product or a component. The end goal of an engineering drawing is to convey all the required information that will allow a manufacturer to produce that component.\n\n\"Technical illustration\" is the use of illustration to visually communicate information of a technical nature. Technical illustrations can be component technical drawings or diagrams. The aim of technical illustration is \"to generate expressive images that effectively convey certain information via the visual channel to the human observer\".\n\nThe main purpose of technical illustration is to describe or explain these items to a more or less nontechnical audience. The visual image should be accurate in terms of dimensions and proportions, and should provide \"an overall impression of what an object is or does, to enhance the viewer’s interest and understanding\".\n\nAccording to Viola (2005), \"illustrative techniques are often designed in a way that even a person with no technical understanding clearly understands the piece of art. The use of varying line widths to emphasize mass, proximity, and scale helped to make a simple line drawing more understandable to the lay person. Cross hatching, stippling, and other low abstraction techniques gave greater depth and dimension to the subject matter\".\n\nA \"cutaway drawing\" is a technical illustration, in which part of the surface of a three-dimensional model is removed in order to show some of the model's interior in relation to its exterior.\n\nThe purpose of a cutaway drawing is to \"allow the viewer to have a look into an otherwise solid opaque object. Instead of letting the inner object shine through the surrounding surface, parts of outside object are simply removed. This produces a visual appearance as if someone had cutout a piece of the object or sliced it into parts. Cutaway illustrations avoid ambiguities with respect to spatial ordering, provide a sharp contrast between foreground and background objects, and facilitate a good understanding of spatial ordering\".\n\nThe two types of technical drawings are based on graphical projection. This is used to create an image of a three-dimensional object onto a two-dimensional surface.\n\nTwo-dimensional representation uses orthographic projection to create an image where only two of the three dimensions of the object are seen.\n\nIn a three-dimensional representation, also referred to as a pictorial, all three dimensions of an object are visible.\n\nMultiview is a type of orthographic projection. There are two conventions for using multiview, first-angle and third-angle. In both cases, the front or main side of the object is the same. First-angle is drawing the object sides based on where they land. Example, looking at the front side, rotate the object 90 degrees to the right. What is seen will be drawn to the right of the front side. Third-angle is drawing the object sides based on where they are. Example, looking at the front side, rotate the object 90 degrees to the right. What is seen is actually the left side of the object and will be drawn to the left of the front side\n\nWhile multiview relates to external surfaces of an object, section views show an imaginary plane cut through an object. This is often useful to show voids in an object.\n\nAuxiliary views utilize an additional projection plane other than the common planes in a multiview. Since the features of an object need to show the true shape and size of the object, the projection plane must be parallel to the object surface. Therefore, any surface that is not in line with the three major axis needs its own projection plane to show the features correctly.\n\nPatterns, sometimes called developments, show the size and shape of a flat piece of material needed for later bending or folding into a three-dimensional shape.\n\nAn \"exploded-view drawing\" is a technical drawing of an object that shows the relationship or order of assembly of the various parts. It shows the components of an object slightly separated by distance or suspended in surrounding space in the case of a three-dimensional exploded diagram. An object is represented as if there had been a small controlled explosion emanating from the middle of the object, causing the object's parts to be separated relative distances away from their original locations.\n\nAn exploded view drawing (EVD) can show the intended assembly of mechanical or other parts. In mechanical systems usually the component closest to the center is assembled first or is the main part in which the other parts get assembled. This drawing can also help to represent disassembly of parts, where the parts on the outside normally get removed first.\n\nThere have been many standard sizes of paper at different times and in different countries, but today most of the world uses the international standard (A4 and its siblings). North America uses its own sizes.\n\nThe applicant for a patent will be required by law to furnish a drawing of the invention if or when the nature of the case requires a drawing to understand the invention with the job. This drawing must be filed with the application. This includes practically all inventions except compositions of matter or processes, but a drawing may also be useful in the case of many processes.\n\nThe drawing must show every feature of the invention specified in the claims and is required by the patent office rules to be in a particular form. The Office specifies the size of the sheet on which the drawing is made, the type of paper, the margins, and other details relating to the making of the drawing. The reason for specifying the standards in detail is that the drawings are printed and published in a uniform style when the patent issues and the drawings must also be such that they can be readily understood by persons using the patent descriptions.\n\nWorking drawings are the set of technical drawings used during the manufacturing phase of a product. In architecture, these include civil drawings, architectural drawings, structural drawings, mechanical systems drawings, electrical drawings, and plumbing drawings.\n\nAssembly drawings show how different parts go together, identify those parts by number, and have a parts list, often referred to as a bill of materials. In a technical service manual, this type of drawing may be referred to as an exploded view drawing or diagram.\nThese parts may be used in engineering.\n\nAlso called \"As-Built drawings\" or \"As-made drawings\". As-fitted drawings represent a record of the completed works, literally 'as fitted'. These are based upon the \"working drawings\" and updated to reflect any changes or alterations undertaken during construction or manufacture.\n\n\n"}
{"id": "15572814", "url": "https://en.wikipedia.org/wiki?curid=15572814", "title": "Wait/walk dilemma", "text": "Wait/walk dilemma\n\nThe wait/walk dilemma occurs when waiting for a bus at a bus stop, when the duration of the wait may exceed the time needed to arrive at a destination by another means, especially walking. \nSome work on this problem was featured in the 2008 \"Year in Ideas\" issue of \"The New York Times Magazine\".\n\nThe dilemma has been studied in an unpublished report entitled \"Walk Versus Wait: The Lazy Mathematician Wins\". Anthony B. Morton's paper \"A Note on Walking Versus Waiting\" supports and extends Chen et al.'s results. Ramnik Arora's \"A Note on Walk versus Wait: Lazy Mathematician Wins\" discusses what he believes to be some of the errors in Chen et al.'s argument; the result of Chen et al.'s paper still holds following Arora's alleged corrections. As early as 1990, writer Tom Parker had made the observation that \"walking is faster than waiting for a bus, if you're going less than a mile\".\n\nHarvard mathematics major Scott D. Kominers first began fixating on the problem while walking from MIT to Harvard, which are more than a mile apart in Cambridge, Massachusetts along MBTA bus route 1. He enlisted the help of Caltech physics major Justin G. Chen and Harvard statistics major Robert W. Sinnott to perform the analysis.\n\nTheir paper concludes that it is usually mathematically quicker to wait for the bus, at least for a little while. But once made, the decision to walk should be final, as opposed to waiting again at subsequent stops.\n\nThe corresponding problem in interstellar travel is called the wait calculation, which tries to determine the optimal time to wait for technological progress to improve spaceship speeds, before committing to the journey.\n\n\n"}
