{"id": "394781", "url": "https://en.wikipedia.org/wiki?curid=394781", "title": "36 (number)", "text": "36 (number)\n\n36 (thirty-six) is the natural number following 35 and preceding 37.\n\n36 is both the square of six and a triangular number, making it a square triangular number. It is the smallest square triangular number other than one, and it is also the only triangular number other than one whose square root is also a triangular number. It is also a circular number- a square number that ends with the same integer by itself (6×6=36).\n\nIt is the smallest number \"n\" with exactly eight solutions to the equation φ(\"x\") = \"n\". Being the smallest number with exactly nine divisors, 36 is a highly composite number. Adding up some subsets of its divisors (e.g., 6, 12, and 18) gives 36, hence 36 is a semiperfect number.\n\nThis number is the sum of a twin prime pair (17 + 19), the sum of the cubes of the first three positive integers, and also the product of the squares of the first three positive integers.\n\n36 is the number of degrees in the interior angle of each tip of a regular pentagram.\n\nThe thirty-six officers problem is a mathematical puzzle.\n\nThe number of possible outcomes (not summed) in the roll of two distinct dice.\n\n36 is the largest numeric base that some computer systems support because it exhausts the numerals, 0-9, and the letters, A-Z. See Base 36.\n\nThe truncated cube and the truncated octahedron are Archimedean solids with 36 edges.\n\nThe number of domino tilings of a 4×4 checkerboard is 36.\n\nSince it is possible to find sequences of 36 consecutive integers such that each inner member shares a factor with either the first or the last member, 36 is an Erdős–Woods number.\n\nThe sum of the integers from 1 to 36 is 666 (see number of the beast).\n\n\n\n\n\n\n"}
{"id": "363207", "url": "https://en.wikipedia.org/wiki?curid=363207", "title": "38 (number)", "text": "38 (number)\n\n38 (thirty-eight) is the natural number following 37 and preceding 39.\n\n\n\n\n\n\n\n\n\nThirty-eight is also:\n"}
{"id": "17854576", "url": "https://en.wikipedia.org/wiki?curid=17854576", "title": "Adjacent channel power ratio", "text": "Adjacent channel power ratio\n\nAdjacent Channel Power Ratio (ACPR) is ratio between the total power of adjacent channel (intermodulation signal) to the main channel's power (useful signal).\n\nThe ratio between the total power adjacent channel (intermodulation signal) to the main channel's power (useful signal). There are two ways of measuring ACPR. The first way is by finding 10*log of the ratio of the total output power to the power in adjacent channel. The second (and much more popular method) is to find the ratio of the output power in a smaller bandwidth around the center of carrier to the power in the adjacent channel. The smaller bandwidth is equal to the bandwidth of the adjacent channel signal. Second way is more popular, because it can be measured easily.\n\nACPR is desired to be as low as possible. A high ACPR indicates that significant spectral spreading has occurred.\n\n"}
{"id": "5703445", "url": "https://en.wikipedia.org/wiki?curid=5703445", "title": "Analysis Situs (paper)", "text": "Analysis Situs (paper)\n\n\"Analysis Situs\" is a seminal mathematics paper that Henri Poincaré published in 1895. Poincaré published five supplements to the paper between 1899 and 1904.\n\nThese papers provided the first systematic treatment of topology and revolutionized the subject by using algebraic structures to distinguish between non-homeomorphic topological spaces, founding the field of algebraic topology. Poincaré's papers introduced the concepts of the fundamental group and simplicial homology, provided an early formulation of the Poincaré duality theorem, introduced the Euler–Poincaré characteristic for chain complexes, and raised several important conjectures, including the celebrated Poincaré conjecture, which was later proven as a theorem. The 1895 paper coined the mathematical term \"homeomorphism\".\n\n"}
{"id": "41458095", "url": "https://en.wikipedia.org/wiki?curid=41458095", "title": "BID 610", "text": "BID 610\n\nBID 610 or Alvis was a British cipher machine used by both British and Canadian governments. It was the first fully transistorised full-duplex online cipher machine used by the British Army. It was introduced in the 1960s.\n\n"}
{"id": "54391334", "url": "https://en.wikipedia.org/wiki?curid=54391334", "title": "Banach lattice", "text": "Banach lattice\n\nIn mathematics, specifically in functional analysis and order theory, a Banach lattice formula_1 is a Riesz space with a norm formula_2 such that formula_3 is a Banach space and for all formula_4 the implication formula_5 holds, where as usual formula_6.\n\n\n"}
{"id": "3126968", "url": "https://en.wikipedia.org/wiki?curid=3126968", "title": "Boolean expression", "text": "Boolean expression\n\nIn computer science, a Boolean expression is used expression in a programming language that produces a Boolean value when evaluated, that is one of true or false. A Boolean expression may be composed of a combination of the Boolean constants true or false, Boolean-typed variables, Boolean-valued operators, and Boolean-valued functions.\n\nBoolean expressions correspond to propositional formulas in logic and are a special case of Boolean circuits.\n\nMost programming languages have the Boolean operators OR, AND and \"not\"; in C and some newer languages, these are represented by \"||\" (double pipe character), \"&&\" (double ampersand) and \"!\" (exclamation point) respectively, while the corresponding bitwise operations are represented by \"|\", \"&\" and \"~\" (tilde). In the mathematical literature the symbols used are often \"+\" (plus), \"·\" (dot) and overbar, or \"∨\" (cup), \"∧\" (cap) and \"¬\" or \"′\" (prime).\n\n\n\n"}
{"id": "456781", "url": "https://en.wikipedia.org/wiki?curid=456781", "title": "Bruck–Ryser–Chowla theorem", "text": "Bruck–Ryser–Chowla theorem\n\nThe Bruck–Ryser–Chowla theorem is a result on the combinatorics of block designs. It states that if a (\"v\", \"b\", \"r\", \"k\", λ)-design exists with \"v = b\" (a symmetric block design), then: \n\nThe theorem was proved in the case of projective planes in . It was extended to symmetric designs in .\n\nIn the special case of a symmetric design with λ = 1, that is, a projective plane, the theorem (which in this case is referred to as the Bruck–Ryser theorem) can be stated as follows: If a finite projective plane of order \"q\" exists and \"q\" is congruent to 1 or 2 (mod 4), then \"q\" must be the sum of two squares. Note that for a projective plane, the design parameters are \"v\" = \"b\" = \"q\" + \"q\" + 1, \"r\" = \"k\" = \"q\" + 1, λ = 1. Thus, \"v\" is always odd in this case.\n\nThe theorem, for example, rules out the existence of projective planes of orders 6 and 14 but allows the existence of planes of orders 10 and 12. Since a projective plane of order 10 has been shown not to exist using a combination of coding theory and large-scale computer search, the condition of the theorem is evidently not sufficient for the existence of a design. However, no stronger general non-existence criterion is known.\n\nThe existence of a symmetric (\"v\", \"b\", \"r\", \"k\", λ)-design is equivalent to the existence of a \"v\" × \"v\" incidence matrix \"R\" with elements 0 and 1 satisfying \n\nwhere is the \"v\" × \"v\" identity matrix and \"J\" is the \"v\" × \"v\" all-1 matrix. In essence, the Bruck–Ryser–Chowla theorem is a statement of the necessary conditions for the existence of a \"rational\" \"v\" × \"v\" matrix \"R\" satisfying this equation. In fact, the conditions stated in the Bruck–Ryser–Chowla theorem are not merely necessary, but also sufficient for the existence of such a rational matrix \"R\". They can be derived from the Hasse–Minkowski theorem on the rational equivalence of quadratic forms.\n\n"}
{"id": "21820039", "url": "https://en.wikipedia.org/wiki?curid=21820039", "title": "Chike Obi", "text": "Chike Obi\n\nChike Obi (April 17, 1921 – March 13, 2008) was a Nigerian politician, mathematician and professor.\n\nThe African Mathematics Union suggests that he was the first Nigerian to hold a doctorate in mathematics. Dr. Obi's early research dealt mainly with the question of the existence of periodic solutions of non-linear ordinary differential equations. He successfully used the perturbation technique, and several of his publications greatly helped to stimulate research interest in this subject throughout the world and have become classics in the literature.\n\nObi is the author of several books and journals on mathematics and Nigerian politics.\n\nObi was educated in various parts of Nigeria before reading mathematics as an external student of the University of London. Immediately after his first degree, he won a scholarship to do research study at Pembroke College, Cambridge, followed by doctoral studies at Massachusetts Institute of Technology in Cambridge, Massachusetts, United States, becoming in 1950, the first Nigerian to receive a PhD in mathematics.\n\nObi returned to lecture at the premier Nigerian University of Ibadan. He was soon diverted from this by political activities. After the war, he returned to lecture in 1970 at the University of Lagos where he quickly rose to the senior academic role of a professor.\n\nHe left Lagos to return to his root in the city of Onitsha, establishing the Nanna Institute for Scientific Studies.\n\nObi had won the Sigvard Eklund Prize for original work in differential equation from the International Centre for Theoretical Physics. He was a university teacher until his retirement as an Emeritus Professor in 1985.\n\nIn 1997, Obi claimed to be the third person to solve Fermat’s Last Theorem after Andrew Wiles and Richard Taylor in 1994.\nHe also claimed to have found an elementary proof to Fermat’s Last Theorem. This work was carried out at his Nanna Institute for Scientific Studies in Onitsha, Eastern Nigeria and published in Algebras, Groups and Geometries. However, a review of this proof published in Mathematical Reviews indicates that it was a false proof.\n\nObi helped form the Dynamic Party of Nigeria, of which he served as its first secretary-general. After the party merged with the larger National Council of Nigerian and Cameroon, Obi was elected as part of the Nigerian delegation that negotiated the country’s path to self-rule at two London conferences in 1957 and 1958.\n\nAfter Nigeria’s independence from Britain in 1960, Obi was elected a legislator in the Eastern House of Assembly in 1960, he refused to vacate his seat in the national legislature in Lagos, the Speaker of the regional house ordered that Obi be physically removed by security agents. This order was obeyed and Obi decided to commit himself to regional affairs.\n\nIn 1962, Obi was arrested and charged with treason in a closed trial organized by the then national civilian government, who accused him and others, including the main opposition leader at the time, Obafemi Awolowo, of plotting to overthrow the government. He was later released for “want of evidence.”\n\nWhen the Nigerian Civil War broke out in 1967, Obi sided with Biafra, working for the rebel leader Chukwuemeka Odumegwu Ojukwu. For a brief period in the 1970s when he served in the National Revenue Mobilization Commission.\n\nObi derided religion and ethnic extremism, and the culture of corruption pervading the Nigerian political class. He was a national newspaper columnist in the 1980s, writing under the title, \"I speak For the People.\"\n\nA visiting professor to the University of Rhode Island, USA, the University of Jos, Nigeria, and the Chinese Academy of Science, Obi was a recipient of the national honour of Commander of the Order of the Niger (CON) and a Fellow of the Nigerian Academy of Science.\n\nWhen Obi died in 2008 he was survived by his wife until 2010. Obi's wife Belinda died in early 2010 a nurse and they are survived by their four children.\n\n"}
{"id": "908548", "url": "https://en.wikipedia.org/wiki?curid=908548", "title": "Conditional entropy", "text": "Conditional entropy\n\nIn information theory, the conditional entropy (or equivocation) quantifies the amount of information needed to describe the outcome of a random variable formula_1 given that the value of another random variable formula_2 is known. Here, information is measured in shannons, nats, or hartleys. The \"entropy of formula_1 conditioned on formula_2\" is written as formula_5.\n\nLet formula_6be the entropy of the discrete random variable formula_1 conditioned on the discrete random variable formula_2 taking a certain value formula_9. Let formula_1 have probability mass function formula_11. The unconditional entropy of formula_1is calculated as formula_13, i.e.\n\nformula_14\n\nwhere formula_15 is the information content of the outcome of formula_1 taking the value formula_17. The entropy of formula_1 conditioned on formula_2 taking the value formula_9is defined analogously by conditional expectation: \n\nformula_21\n\nformula_5 is the result of averaging formula_6 over all possible values formula_9 that formula_2 may take.\n\nGiven discrete random variables formula_2 with image formula_27 and formula_1 with image formula_29, the conditional entropy of formula_1 given formula_2 is defined as the weighted sum of formula_6 for each possible value of formula_9, using formula_34 as the weights:\n\n\"Note:\" It is understood that the expressions formula_36 and formula_37 for fixed formula_38 should be treated as being equal to zero. \n\nformula_39 if and only if the value of formula_1 is completely determined by the value of formula_2.\n\nConversely, formula_42 if and only if formula_1 and formula_2 are independent random variables.\n\nAssume that the combined system determined by two random variables formula_2 and formula_1 has joint entropy formula_47, that is, we need formula_47 bits of information on average to describe its exact state. Now if we first learn the value of formula_2, we have gained formula_50 bits of information. Once formula_2 is known, we only need formula_52 bits to describe the state of the whole system. This quantity is exactly formula_5, which gives the \"chain rule\" of conditional entropy:\n\nThe chain rule follows from the above definition of conditional entropy:\n\nIn general, a chain rule for multiple random variables holds:\n\nIt has a similar form to Chain rule (probability) in probability theory, except that addition instead of multiplication is used.\n\nBayes' rule for conditional entropy states\n\n\"Proof.\" formula_58 and formula_59. Symmetry entails formula_60. Subtracting the two equations implies Bayes' rule.\n\nIf formula_1 is conditionally independent of formula_62 given formula_2 we have:\n\nFor any formula_2 and formula_1:\n\nwhere formula_68 is the mutual information between formula_2 and formula_1.\n\nFor independent formula_2 and formula_1:\n\nAlthough the specific-conditional entropy formula_75 can be either less or greater than formula_50 for a given random variate formula_77 of formula_1, formula_79 can never exceed formula_50.\n\nThe above definition is for discrete random variables and no more valid in the case of continuous random variables. The continuous version of discrete conditional entropy is called \"conditional differential (or continuous) entropy\". Let formula_2 and formula_1 be a continuous random variables with a joint probability density function formula_83. The differential conditional entropy formula_84 is defined as\n\nIn contrast to the conditional entropy for discrete random variables, the conditional differential entropy may be negative.\n\nAs in the discrete case there is a chain rule for differential entropy:\nNotice however that this rule may not be true if the involved differential entropies do not exist or are infinite.\n\nJoint differential entropy is also used in the definition of the mutual information between continuous random variables:\n\nformula_88 with equality if and only if formula_2 and formula_1 are independent.\n\nThe conditional differential entropy yields a lower bound on the expected squared error of an estimator. For any random variable formula_2, observation formula_1 and estimator formula_93 the following holds:\n\nThis is related to the uncertainty principle from quantum mechanics.\n\nIn quantum information theory, the conditional entropy is generalized to the conditional quantum entropy. The latter can take negative values, unlike its classical counterpart.\n\n"}
{"id": "17753389", "url": "https://en.wikipedia.org/wiki?curid=17753389", "title": "Contingency (philosophy)", "text": "Contingency (philosophy)\n\nIn philosophy and logic, contingency is the status of propositions that are neither true under every possible valuation (i.e. tautologies) nor false under every possible valuation (i.e. contradictions). A contingent proposition is neither necessarily true nor necessarily false. Propositions that are contingent may be so because they contain logical connectives which, along with the truth value of any of its atomic parts, determine the truth value of the proposition. This is to say that the truth value of the proposition is \"contingent\" upon the truth values of the sentences which comprise it. Contingent propositions depend on the facts, whereas analytic propositions are true without regard to any facts about which they speak.\n\nAlong with contingent propositions, there are at least three other classes of propositions, some of which overlap:\n\nAttempts in the past by philosophers and rhetoricians to allocate to rhetoric its own realm have ended with attempting to contain rhetoric within the domain of contingent and relative matters. Aristotle explained in \"Rhetoric\", \"The duty of rhetoric is to deal with such matters as we deliberate upon without arts or systems to guide us...\" Aristotle stresses the contingent because no one deliberates on the necessary or impossible. He believed that the \"unavoidable and potentially unmanageable presence of multiple possibilities\" or the complex nature of decisions creates and invites rhetoric. Aristotle's view challenges the view of Plato, who said that rhetoric had no subject matter except for deceit, and gives rhetoric its position at the pinnacle of political debate.\n\nContemporary scholars argue that if rhetoric is merely about the contingent, it automatically excludes that which is either necessary or impossible. The \"necessary\" is that which either must be done or will inevitably be done. The \"impossible\" is that which will never be done; therefore, it will not be deliberated over. For example, the United States Congress will not convene tomorrow to discuss something necessary, such as whether or not to hold elections, or something impossible, such as outlawing death. Congress convenes to discuss problems, different solutions to those problems, and the consequences of each solution.\n\nThis again raises the question of contingency because that which is deemed necessary or impossible depends almost entirely on time and perspective. In United States history, there was a time when even a congressman who opposed slavery would conclude that its retraction would be impossible. The same held true for those who favored women's suffrage. Today in the United States, slavery has been abolished and women have the right to vote. In this way, although rhetoric viewed across time is entirely contingent and includes a broader definition, rhetoric taken moment-by-moment is much more narrow and excludes both the necessary and the impossible. When faced with decisions, people will choose one option at the exclusion of the others. This inevitably produces unforeseen consequences. Because of these consequences, decision makers must deliberate and choose. Another problem arises when one asks where this knowledge of what issues are \"necessary\" and \"impossible\" originates and how the knowledge can be applied to others.\n\nRhetorician Robert L. Scott answers this problem by asserting that while rhetoric is indeed contingent and relative, it is also epistemic. Thus, for Scott, what should be debated is a matter of rhetoric, as individuals make meaning through language and determine what constitutes truth, and therefore, what is beyond question and debate. Theorist Lloyd Bitzer makes five assumptions about rhetoric in his book \"Rhetoric, Philosophy, and Literature: An Exploration\".\n\n\nThe study of contingency and relativism as it pertains to rhetoric draws from poststructuralist and postfoundationalist theory. Richard Rorty and Stanley Fish are leading theorists in this area of study at the intersection of rhetoric and contingency.\n"}
{"id": "951720", "url": "https://en.wikipedia.org/wiki?curid=951720", "title": "Cryptanalytic computer", "text": "Cryptanalytic computer\n\nA cryptanalytic computer is a computer designed to be used for cryptanalysis, which nowadays involves massive statistical analysis and multiple trial decryptions that since before World War II are possible only with automated equipment. Polish cryptanalysts designed and built automated aids in their work on Enigma traffic. Arguably, the first modern computer (digital, electronic, and somewhat programmable) was built for cryptanalytic work at Bletchley Park (the Colossus) during the war. More modern computers were important after World War II, and some machines (like the Cray-1) are reported to have had machine instructions hardwired in at the request of NSA.\n\nComputers continue to be important in cryptanalysis well into the 21st century. NSA, in fact, is said to have the largest number of installed computers on the planet. Whether or not this is true in an age of Google computer farms and such remains publicly unknown.\n\n"}
{"id": "1655440", "url": "https://en.wikipedia.org/wiki?curid=1655440", "title": "Deriving the Schwarzschild solution", "text": "Deriving the Schwarzschild solution\n\nThe Schwarzschild solution describes spacetime in the vicinity of a non-rotating massive spherically-symmetric object. Of the solutions to the Einstein field equations, it is considered by some to be one of the simplest and most useful. \n\nWorking in a coordinate chart with coordinates formula_1 labelled 1 to 4 respectively, we begin with the metric in its most general form (10 independent components, each of which is a smooth function of 4 variables). The solution is assumed to be spherically symmetric, static and vacuum. For the purposes of this article, these assumptions may be stated as follows (see the relevant links for precise definitions):\n\n\nThe first simplification to be made is to diagonalise the metric. Under the coordinate transformation, formula_9, all metric components should remain the same. The metric components formula_10 (formula_11) change under this transformation as:\n\nBut, as we expect formula_14 (metric components remain the same), this means that:\n\nSimilarly, the coordinate transformations formula_17 and formula_18 respectively give:\n\nPutting all these together gives:\n\nand hence the metric must be of the form:\n\nwhere the four metric components are independent of the time coordinate formula_2 (by the static assumption).\n\nOn each hypersurface of constant formula_2, constant formula_28 and constant formula_29 (i.e., on each radial line), formula_30 should only depend on formula_31 (by spherical symmetry). Hence formula_30 is a function of a single variable:\n\nA similar argument applied to formula_34 shows that:\n\nOn the hypersurfaces of constant formula_2 and constant formula_31, it is required that the metric be that of a 2-sphere:\n\nChoosing one of these hypersurfaces (the one with radius formula_39, say), the metric components restricted to this hypersurface (which we denote by formula_40 and formula_41) should be unchanged under rotations through formula_28 and formula_29 (again, by spherical symmetry). Comparing the forms of the metric on this hypersurface gives:\n\nwhich immediately yields:\n\nBut this is required to hold on each hypersurface; hence,\n\nAn alternative intuitive way to see that formula_49 and formula_50 must be the same as for a flat spacetime is that stretching or \ncompressing an elastic material in a spherically symmetric manner (radially), will not increase the angular distance between two points.\n\nThus, the metric can be put in the form:\n\nwith formula_52 and formula_53 as yet undetermined functions of formula_31. Note that if formula_52 or formula_53 is equal to zero at some point, the metric would be singular at that point.\n\nUsing the metric above, we find the Christoffel symbols, where the indices are formula_57. The sign formula_58 denotes a total derivative of a function.\n\nTo determine formula_52 and formula_53, the vacuum field equations are employed:\n\nHence:\n\nwhere a comma is used to set off the index that is being used for the derivative. Only three of these equations are nontrivial and upon simplification become:\n\nformula_67\n\nformula_68\n\nformula_69\n\n(the fourth equation is just formula_70 times the second equation), where the prime means the \"r\" derivative of the functions. Subtracting the first and third equations produces:\n\nformula_71\n\nwhere formula_72 is a non-zero real constant. Substituting formula_73 into the second equation and tidying up gives:\n\nformula_74\n\nwhich has general solution:\n\nformula_75\n\nfor some non-zero real constant formula_76. Hence, the metric for a static, spherically symmetric vacuum solution is now of the form:\n\nformula_77\n\nNote that the spacetime represented by the above metric is asymptotically flat, i.e. as formula_78, the metric approaches that of the Minkowski metric and the spacetime manifold resembles that of Minkowski space.\n\nThe geodesics of the metric (obtained where formula_79 is extremised) must, in some limit (e.g., toward infinite speed of light), agree with the solutions of Newtonian motion (e.g., obtained by Lagrange equations). (The metric must also limit to Minkowski space when the mass it represents vanishes.)\n\nformula_80\n\n(where formula_81 is the kinetic energy and formula_82 is the Potential Energy due to gravity) The constants formula_72 and formula_76 are fully determined by some variant of this approach; from the weak-field approximation one arrives at the result:\n\nformula_85\n\nwhere formula_86 is the gravitational constant, formula_87 is the mass of the gravitational source and formula_88 is the speed of light. It is found that:\n\nformula_89 and formula_90\n\nHence:\n\nformula_91 and formula_92\n\nSo, the Schwarzschild metric may finally be written in the form:\n\nformula_93\n\nNote that:\n\nformula_94\n\nis the definition of the Schwarzschild radius for an object of mass formula_87, so the Schwarzschild metric may be rewritten in the alternative form:\n\nformula_96\n\nwhich shows that the metric becomes singular approaching the event horizon (that is, formula_97). The metric singularity is not a physical one (although there is a real physical singularity at formula_98), as can be shown by using a suitable coordinate transformation (e.g. the Kruskal–Szekeres coordinate system).\n\nThe Schwarzschild metric can also be derived using the known physics for a circular orbit and a temporarily stationary point mass. Start with the metric with coefficients that are unknown coefficients of formula_31:\n\nNow apply the Euler-Lagrange equation to the arc length integral formula_101 Since formula_102 is constant, the integrand can be replaced with formula_103 because the E-L equation is exactly the same if the integrand is multiplied by any constant. Applying the E-L equation to formula_104 with the modified integrand yields:\n\nwhere dot denotes differentiation with respect to formula_106\n\nIn a circular orbit formula_107 so the first E-L equation above is equivalent to\n\nKepler's third law of motion is\n\nIn a circular orbit, the period formula_110 equals formula_111 implying\n\nsince the point mass formula_87 is negligible compared to the mass of the central body formula_114 So formula_115 and integrating this yields formula_116 where formula_117 is an unknown constant of integration. formula_117 can be determined by setting formula_119 in which case the space-time is flat and formula_120 So formula_121 and\n\nWhen the point mass is temporarily stationary, formula_123 and formula_124 The original metric equation becomes formula_125 and the first E-L equation above becomes formula_126 When the point mass is temporarily stationary, formula_127 is the acceleration of gravity, formula_128 So\n\nThe original formulation of the metric uses anisotropic coordinates in which the velocity of light is not the same in the radial and transverse directions. Arthur Eddington gave alternative forms in isotropic coordinates. For isotropic spherical coordinates formula_130, formula_28, formula_29, coordinates formula_28 and formula_29 are unchanged, and then (provided formula_135)\n\nThen for isotropic rectangular coordinates formula_139, formula_140, formula_141,\n\nThe metric then becomes, in isotropic rectangular coordinates:\n\nIn deriving the Schwarzschild metric, it was assumed that the metric was vacuum, spherically symmetric and static. In fact, the static assumption is stronger than required, as Birkhoff's theorem states that any spherically symmetric vacuum solution of Einstein's field equations is stationary; then one obtains the Schwarzschild solution. Birkhoff's theorem has the consequence that any pulsating star which remains spherically symmetric cannot generate gravitational waves (as the region exterior to the star must remain static).\n\n"}
{"id": "21689422", "url": "https://en.wikipedia.org/wiki?curid=21689422", "title": "Edge dominating set", "text": "Edge dominating set\n\nIn graph theory, an edge dominating set for a graph \"G\" = (\"V\", \"E\") is a subset \"D\" ⊆ \"E\" such that every edge not in \"D\" is adjacent to at least one edge in \"D\". An edge dominating set is also known as a \"line dominating set\". Figures (a)–(d) are examples of edge dominating sets (thick red lines).\n\nA minimum edge dominating set is a smallest edge dominating set. Figures (a) and (b) are examples of minimum edge dominating sets (it can be checked that there is no edge dominating set of size 2 for this graph).\n\nAn edge dominating set for \"G\" is a dominating set for its line graph \"L\"(\"G\") and vice versa.\n\nAny maximal matching is always an edge dominating set. Figures (b) and (d) are examples of maximal matchings.\n\nFurthermore, the size of a minimum edge dominating set equals the size of a minimum maximal matching. A minimum maximal matching is a minimum edge dominating set; Figure (b) is an example of a minimum maximal matching. A minimum edge dominating set is not necessarily a minimum maximal matching, as illustrated in Figure (a); however, given a minimum edge dominating set \"D\", it is easy to find a minimum maximal matching with |\"D\"| edges (see, e.g., ).\n\nDetermining whether there is an edge dominating set of a given size for a given graph is an NP-complete problem (and therefore finding a minimum edge dominating set is an NP-hard problem). show that the problem is NP-complete even in the case of a bipartite graph with maximum degree 3, and also in the case of a planar graph with maximum degree 3.\n\nThere is a simple polynomial-time approximation algorithm with approximation factor 2: find any maximal matching. A maximal matching is an edge dominating set; furthermore, a maximal matching \"M\" can be at worst 2 times as large as a smallest maximal matching, and a smallest maximal matching has the same size as the smallest edge dominating set.\n\nAlso the edge-weighted version of the problem can be approximated within factor 2, but the algorithm is considerably more complicated (; ).\n\n"}
{"id": "49024626", "url": "https://en.wikipedia.org/wiki?curid=49024626", "title": "Egorychev method", "text": "Egorychev method\n\nThe Egorychev method is a collection of techniques for finding identities among sums of binomial coefficients. The method relies on two observations. First, many identities can be proved by extracting coefficients of generating functions. Second, many generating functions are convergent power series, and coefficient extraction can be done using the Cauchy residue theorem (usually this is done by integrating over a small circular contour enclosing the origin). The sought-for identity can now be found using manipulations of integrals. Some of these manipulations are not clear from the generating function perspective. For instance, the integrand is usually a rational function, and the sum of the residues of a rational function is zero, yielding a new expression for the original sum. The residue at infinity is particularly important in these considerations.\n\nThe main integrals employed by the Egorychev method are:\n\nSuppose we seek to evaluate\n\nwhich is claimed to be :formula_7\n\nIntroduce\n\nand\n\nThis yields for the sum\n\nThis is\n\nExtracting the residue at formula_12 we get\n\nthus proving the claim.\n\nSuppose we seek to evaluate formula_14\n\nIntroduce\n\nObserve that this is zero when formula_16 so we may extend formula_17 to\ninfinity to obtain for the sum\n\nNow put formula_19 so that (observe that the image of formula_20 with formula_21 small is another closed circle-like contour which we may certainly deform to obtain another circle formula_22)\n\nand furthermore\n\nto get for the integral\n\nThis evaluates by inspection to (use the Newton binomial)\n\nHere the mapping from formula_27 to formula_12 determines the choice of\nsquare root. This example also yields to simpler methods but was included here to demonstrate the effect of substituting into the variable of integration.\n\n"}
{"id": "2070683", "url": "https://en.wikipedia.org/wiki?curid=2070683", "title": "Elliptic gamma function", "text": "Elliptic gamma function\n\nIn mathematics, the elliptic gamma function is a generalization of the q-Gamma function, which is itself the q-analog of the ordinary Gamma function. It is closely related to a function studied by , and can be expressed in terms of the triple gamma function. It is given by\n\nIt obeys several identities:\n\nand\n\nwhere θ is the q-theta function.\n\nWhen formula_5, it essentially reduces to the infinite q-Pochhammer symbol:\n\nDefine\nThen the following formula holds with formula_8 ().\n"}
{"id": "1050195", "url": "https://en.wikipedia.org/wiki?curid=1050195", "title": "Evolutionary robotics", "text": "Evolutionary robotics\n\nEvolutionary robotics (ER) is a methodology that uses evolutionary computation to develop controllers and/or hardware for autonomous robots. Algorithms in ER frequently operate on populations of candidate controllers, initially selected from some distribution. This population is then repeatedly modified according to a fitness function. In the case of genetic algorithms (or \"GAs\"), a common method in evolutionary computation, the population of candidate controllers is repeatedly grown according to crossover, mutation and other GA operators\nand then culled according to the fitness function. The candidate controllers used in ER applications may be drawn from some subset of the set of artificial neural networks, although some applications (including SAMUEL, developed at the Naval Center for Applied Research in Artificial Intelligence) use collections of \"IF THEN ELSE\" rules as the constituent parts of an individual controller. It is theoretically possible to use any set of symbolic formulations of a control law (sometimes called a policy in the machine learning community) as the space of possible candidate controllers. Artificial neural networks can also be used for robot learning outside the context of evolutionary robotics. In particular, other forms of reinforcement learning can be used for learning robot controllers.\n\nDevelopmental robotics is related to, but differs from, evolutionary robotics. ER uses populations of robots that evolve over time, whereas DevRob is interested in how the organization of a single robot's control system develops through experience, over time.\n\nThe foundation of ER was laid with work at the national research council in Rome in the 90s, but the initial idea of encoding a robot control system into a genome and have artificial evolution improve on it dates back to the late 80s.\n\nIn 1992 and 1993 three research groups, one surrounding Floreano and Mondada at the EPFL in Lausanne and a second involving Cliff, Harvey, and Husbands from COGS at the University of Sussex and a third from the University of Southern California involved M. Anthony Lewis and Andrew H Fagg reported promising results from experiments on artificial evolution of autonomous robots. The success of this early research triggered a wave of activity in labs around the world trying to harness the potential of the approach.\n\nLately, the difficulty in \"scaling up\" the complexity of the robot tasks has shifted attention somewhat towards the theoretical end of the field rather than the engineering end.\n\nEvolutionary robotics is done with many different objectives, often at the same time. These include creating useful controllers for real-world robot tasks, exploring the intricacies of evolutionary theory (such as the Baldwin effect), reproducing psychological phenomena, and finding out about biological neural networks by studying artificial ones. Creating controllers via artificial evolution requires a large number of evaluations of a large population. This is very time consuming, which is one of the reasons why controller evolution is usually done in software. Also, initial random controllers may exhibit potentially harmful behaviour, such as repeatedly crashing into a wall, which may damage the robot. Transferring controllers evolved in simulation to physical robots is very difficult and a major challenge in using the ER approach. The reason is that evolution is free to explore all possibilities to obtain a high fitness, including any inaccuracies of the simulation . This need for a large number of evaluations, requiring fast yet accurate computer simulations, is one of the limiting factors of the ER approach .\n\nIn rare cases, evolutionary computation may be used to design the physical structure of the robot, in addition to the controller. One of the most notable examples of this was Karl Sims' demo for Thinking Machines Corporation.\n\nMany of the commonly used machine learning algorithms require a set of training examples consisting of both a hypothetical input and a desired answer. In many robot learning applications the desired answer is an action for the robot to take. These actions are usually not known explicitly a priori, instead the robot can, at best, receive a value indicating the success or failure of a given action taken. Evolutionary algorithms are natural solutions to this sort of problem framework, as the fitness function need only encode the success or failure of a given controller, rather than the precise actions the controller should have taken. An alternative to the use of evolutionary computation in robot learning is the use of other forms of reinforcement learning, such as q-learning, to learn the fitness of any particular action, and then use predicted fitness values indirectly to create a controller.\n\n\n\n\n"}
{"id": "364372", "url": "https://en.wikipedia.org/wiki?curid=364372", "title": "Gelfand representation", "text": "Gelfand representation\n\nIn mathematics, the Gelfand representation in functional analysis (named after I. M. Gelfand) has two related meanings:\n\n\nIn the former case, one may regard the Gelfand representation as a far-reaching generalization of the Fourier transform of an integrable function. In the latter case, the Gelfand–Naimark representation theorem is one avenue in the development of spectral theory for normal operators, and generalizes the notion of diagonalizing a normal matrix.\n\nOne of Gelfand's original applications (and one which historically motivated much of the study of Banach algebras) was to give a much shorter and more conceptual proof of a celebrated lemma of Norbert Wiener (see the citation below), characterizing the elements of the group algebras \"L\"(R) and formula_1 whose translates span dense subspaces in the respective algebras.\n\nFor any locally compact Hausdorff topological space \"X\", the space \"C\"(\"X\") of continuous complex-valued functions on \"X\" which vanish at infinity is in a natural way a commutative C*-algebra:\n\nNote that \"C\"(\"X\") is unital if and only if \"X\" is compact, in which case \"C\"(\"X\") is equal to \"C\"(\"X\"), the algebra of all continuous complex-valued functions on \"X\".\n\nLet \"A\" be a commutative Banach algebra, defined over the field ℂ of complex numbers. A non-zero algebra homomorphism φ: \"A\" → ℂ is called a \"character\" of \"A\"; the set of all characters of \"A\" is denoted by Φ.\n\nIt can be shown that every character on \"A\" is automatically continuous, and hence Φ is a subset of the space \"A\"* of continuous linear functionals on \"A\"; moreover, when equipped with the relative weak-* topology, Φ turns out to be locally compact and Hausdorff. (This follows from the Banach–Alaoglu theorem.) The space Φ is compact (in the topology just defined) if and only if the algebra \"A\" has an identity element.\n\nGiven \"a\" ∈ \"A\", one defines the function formula_2 by formula_3. The definition of Φ and the topology on it ensure that formula_4 is continuous and vanishes at infinity, and that the map formula_5 defines a norm-decreasing, unit-preserving algebra homomorphism from \"A\" to \"C\"(Φ). This homomorphism is the \"Gelfand representation of A\", and formula_4 is the \"Gelfand transform\" of the element \"a\". In general, the representation is neither injective nor surjective.\n\nIn the case where \"A\" has an identity element, there is a bijection between Φ and the set of maximal ideals in \"A\" (this relies on the Gelfand–Mazur theorem). As a consequence, the kernel of the Gelfand representation \"A\" → \"C\"(Φ) may be identified with the Jacobson radical of \"A\". Thus the Gelfand representation is injective if and only if \"A\" is (Jacobson) semisimple.\n\nIn the case where \"A\" = \"L\"(R), the group algebra of R, then Φ is homeomorphic to R and the Gelfand transform of \"f\" ∈ \"L\"(R) is the Fourier transform formula_7.\n\nIn the case where \"A\" = \"L\"(R), the L-convolution algebra of the real half-line, then Φ is homeomorphic to {\"z\" ∈ C: Re(\"z\") ≥ 0}, and the Gelfand transform of an element \"f\" ∈ \"L\"(R) is the Laplace transform formula_8.\n\nAs motivation, consider the special case \"A\" = \"C\"(\"X\"). Given \"x\" in \"X\", let formula_9 be pointwise evaluation at \"x\", i.e. formula_10. Then formula_11 is a character on \"A\", and it can be shown that all characters of \"A\" are of this form; a more precise analysis shows that we may identify Φ with \"X\", not just as sets but as topological spaces. The Gelfand representation is then an isomorphism\n\nThe spectrum or Gelfand space of a commutative C*-algebra \"A\", denoted \"Â\", consists of the set of \"non-zero\" *-homomorphisms from \"A\" to the complex numbers. Elements of the spectrum are called characters on \"A\". (It can be shown that every algebra homomorphism from \"A\" to the complex numbers is automatically a *-homomorphism, so that this definition of the term 'character' agrees with the one above.)\n\nIn particular, the spectrum of a commutative C*-algebra is a locally compact Hausdorff space: In the unital case, i.e. where the C*-algebra has a multiplicative unit element 1, all characters \"f\" must be unital, i.e. \"f\"(1) is the complex number one. This excludes the zero homomorphism. So \"Â\" is closed under weak-* convergence and the spectrum is actually \"compact\". In the non-unital case, the weak-* closure of \"Â\" is \"Â\" ∪ {0}, where 0 is the zero homomorphism, and the removal of a single point from a compact Hausdorff space yields a locally compact Hausdorff space.\n\nNote that \"spectrum\" is an overloaded word. It also refers to the spectrum σ(\"x\") of an element \"x\" of an algebra with unit 1, that is the set of complex numbers \"r\" for which \"x\" - \"r\" 1 is not invertible in \"A\". For unital C*-algebras, the two notions are connected in the following way: σ(\"x\") is the set of complex numbers \"f\"(\"x\") where \"f\" ranges over Gelfand space of \"A\". Together with the spectral radius formula, this shows that \"Â\" is a subset of the unit ball of \"A*\" and as such can be given the relative weak-* topology. This is the topology of pointwise convergence. A net {\"f\"} of elements of the spectrum of \"A\" converges to \"f\" if and only if for each \"x\" in \"A\", the net of complex numbers {\"f\"(\"x\")} converges to \"f\"(\"x\").\n\nIf \"A\" is a separable C*-algebra, the weak-* topology is metrizable on bounded subsets. Thus the spectrum of a separable commutative C*-algebra \"A\" can be regarded as a metric space. So the topology can be characterized via convergence of sequences.\n\nEquivalently, σ(\"x\") is the range of γ(\"x\"), where γ is the Gelfand representation.\n\nLet \"A\" be a commutative C*-algebra and let \"X\" be the spectrum of \"A\". Let \n\nbe the Gelfand representation defined above.\n\nTheorem. The Gelfand map γ is an isometric *-isomorphism from \"A\" onto \"C\"(\"X\").\n\nSee the Arveson reference below.\n\nThe spectrum of a commutative C*-algebra can also be viewed as the set of all maximal ideals \"m\" of \"A\", with the hull-kernel topology. (See the earlier remarks for the general, commutative Banach algebra case.) For any such \"m\" the quotient algebra \"A/m\" is one-dimensional (by the Gelfand-Mazur theorem), and therefore any \"a\" in \"A\" gives rise to a complex-valued function on \"Y\".\n\nIn the case of C*-algebras with unit, the spectrum map gives rise to a contravariant functor from the category of C*-algebras with unit and unit-preserving continuous *-homomorphisms, to the category of compact Hausdorff spaces and continuous maps. This functor is one half of a contravariant equivalence between these two categories (its adjoint being the functor that assigns to each compact Hausdorff space \"X\" the C*-algebra \"C\"(\"X\")). In particular, given compact Hausdorff spaces \"X\" and \"Y\", then \"C\"(\"X\") is isomorphic to \"C\"(\"Y\") (as a C*-algebra) if and only if \"X\" is homeomorphic to \"Y\".\n\nThe 'full' Gelfand–Naimark theorem is a result for arbitrary (abstract) noncommutative C*-algebras \"A\", which though not quite analogous to the Gelfand representation, does provide a concrete representation of \"A\" as an algebra of operators.\n\nOne of the most significant applications is the existence of a continuous \"functional calculus\" for normal elements in C*-algebra \"A\": An element \"x\" is normal if and only if \"x\" commutes with its adjoint \"x*\", or equivalently if and only if it generates a commutative C*-algebra C*(\"x\"). By the Gelfand isomorphism applied to C*(\"x\") this is *-isomorphic to an algebra of continuous functions on a locally compact space. This observation leads almost immediately to:\n\nTheorem. Let \"A\" be a C*-algebra with identity and \"x\" an element of \"A\". Then there is a *-morphism \"f\" → \"f\"(\"x\") from the algebra of continuous functions on the spectrum σ(\"x\") into \"A\" such that\n\nThis allows us to apply continuous functions to bounded normal operators on Hilbert space.\n\n"}
{"id": "24383656", "url": "https://en.wikipedia.org/wiki?curid=24383656", "title": "George B. Purdy", "text": "George B. Purdy\n\nGeorge Barry Purdy (20 February 1944 – 30 December 2017) was a mathematician and computer scientist who specialized in cryptography, combinatorial geometry and number theory. \nPurdy received his Ph.D. from the University of Illinois at Urbana–Champaign in 1972, officially under the supervision of Paul T. Bateman, but his de facto adviser was Paul Erdős. He was on the faculty in the mathematics department at Texas A&M University for 11 years, and was appointed the Geier Professor of computer science at the University of Cincinnati in 1986.\n\nPurdy had Erdős number one and coauthored many papers with Paul Erdős, who regarded him as his own student. He is the \"P\" in G.W. Peck, a pseudonym for the group of mathematicians that also included Ronald Graham, Douglas West, Paul Erdős, Fan Chung, and Daniel Kleitman.\n\nIn 1971, Purdy was asked by Larry Roberts, the head of DARPA, to develop a secure hash function to protect passwords on ARPANET. Purdy developed the so-called Purdy polynomial, which was a polynomial of degree 2 + 17 computed modulo the 64-bit prime \"p\" = 2 - 59. The terms of the polynomial could be computed using modular exponentiation. DARPA was satisfied with the hash function, and also allowed Purdy to publish it in Communications of the ACM. It was well received around the world, and DEC eventually used it in their OpenVMS operating system. A DEC report said they chose it because it was very secure, and also because the existing standard DES could not be exported, which meant that an alternative was needed. OpenVMS uses a 64-bit version, based on a 64-bit prime, the same size as the one in the paper.\n\nWhile at Texas A&M, Purdy made an empirical observation about distances between points on two lines. Suppose that \"n\" points are to be chosen on line \"L\" and another \"n\" points on line \"M\". If \"L\" and \"M\" are perpendicular or parallel, then the points can be chosen so that the number of distinct distances determined is bounded by a constant multiply of \"n\", but otherwise the number is much larger. Erdős was very struck by this conjecture and told it to many others, and it was published in a book of unsolved problems by William Moser in 1981. It came to the attention of György Elekes, who eventually proved the conjecture as the first application of new tools from algebraic geometry that he was developing. After Elekes's untimely death, Micha Sharir collected Elekes's notes and published an organized presentation of these algebraic methods, including work of his own. This, in turn, enabled Katz and Guth to solve the Erdős distinct distances problem, a 1946 problem of Erdős. Work continues on improvements in Purdy's conjecture.\n\nIn 2015, Purdy was awarded the IEEE Joseph Desch Award for Innovation for his work on the Arpa Network and the Purdy Polynomial.\n\n"}
{"id": "22833028", "url": "https://en.wikipedia.org/wiki?curid=22833028", "title": "Gábor Tardos", "text": "Gábor Tardos\n\nGábor Tardos (born 11 July 1964) is a Hungarian mathematician, currently a professor at Central European University and previously a Canada Research Chair at Simon Fraser University. He works mainly in combinatorics and computer science. He is the younger brother of Éva Tardos.\n\nTardos started with a result in universal algebra: he exhibited a maximal clone of monotone operations which is not finitely generated. He obtained partial results concerning the Hanna Neumann conjecture. With his student, Adam Marcus, he proved a combinatorial conjecture of Zoltán Füredi and Péter Hajnal which was known to imply the Stanley–Wilf conjecture. With topological methods he proved that if formula_1 is a finite set system consisting of the unions of intervals on two disjoint lines, then formula_2 holds, where formula_3 is the least number of points covering all elements of formula_1 and formula_5 is the size of the largest disjoint subsystem of formula_1. Tardos worked out a method for optimal probabilistic fingerprint codes. Although the mathematical content is hard, the algorithm is easy to implement.\n\nHe received the European Mathematical Society prize for young researchers at the European Congress of Mathematics in 1992 and the Erdős Prize from the Hungarian Academy of Sciences in 2000. He received a Lendület Grant from the Hungarian Academy of Sciences (2009). specifically devised to keep outstanding researchers in Hungary.\n\n"}
{"id": "45031605", "url": "https://en.wikipedia.org/wiki?curid=45031605", "title": "Ida Busbridge", "text": "Ida Busbridge\n\nIda Winifred Busbridge (1908–1988) was a British mathematician who taught at the University of Oxford from 1935 until 1970. She was the first woman to be appointed to an Oxford fellowship in mathematics.\n\nIda Busbridge was educated at Christ's Hospital then studied mathematics at Royal Holloway College, London, where in 1929 she was the highest-ranked student in the final examinations for mathematics for the whole of the University of London. After obtaining a master's degree with distinction from London, she moved to Oxford in 1935 to teach mathematics to the undergraduates of five women's colleges. During the Second World War, she also helped with the education of physicists and engineers at Oxford; her workload was especially great not only because other mathematicians at the university were called up for special war service but also because women formed a higher percentage of the undergraduate population during the war years. She was appointed to a Fellowship of St Hugh's College, Oxford, in 1946 – she was the first women to be appointed to a college fellowship in mathematics. In 1962, she was awarded a Doctor of Science degree by Oxford. She was also a Fellow of the Royal Astronomical Society.\n\nBusbridge's work included integral equations and radiative transfer. She was highly regarded as a lecturer and tutor, attending to her students' educational and personal needs. She retired from Oxford in 1970, and died in 1988.\n\nShe was president of the Mathematical Association for 1964.\n"}
{"id": "205483", "url": "https://en.wikipedia.org/wiki?curid=205483", "title": "Jean-Pierre Serre", "text": "Jean-Pierre Serre\n\nJean-Pierre Serre (; born 15 September 1926) is a French mathematician who has made contributions to algebraic topology, algebraic geometry, and algebraic number theory. He was awarded the Fields Medal in 1954 and the Abel Prize in 2003.\n\nBorn in Bages, Pyrénées-Orientales, France, to pharmacist parents, Serre was educated at the Lycée de Nîmes and then from 1945 to 1948 at the École Normale Supérieure in Paris. He was awarded his doctorate from the Sorbonne in 1951. From 1948 to 1954 he held positions at the Centre National de la Recherche Scientifique in Paris. In 1956 he was elected professor at the Collège de France, a position he held until his retirement in 1994. His wife, Professor Josiane Heulot-Serre, was a chemist; she also was the director of the Ecole Normale Supérieure de Jeunes Filles. Their daughter is the former French diplomat, historian and writer Claudine Monteil. The French mathematician Denis Serre is his nephew. He practices skiing, table tennis, and rock climbing (in Fontainebleau).\n\nFrom a very young age he was an outstanding figure in the school of Henri Cartan, working on algebraic topology, several complex variables and then commutative algebra and algebraic geometry, where he introduced sheaf theory and homological algebra techniques. Serre's thesis concerned the Leray–Serre spectral sequence associated to a fibration. Together with Cartan, Serre established the technique of using Eilenberg–MacLane spaces for computing homotopy groups of spheres, which at that time was one of the major problems in topology.\n\nIn his speech at the Fields Medal award ceremony in 1954, Hermann Weyl gave high praise to Serre, and also made the point that the award was for the first time awarded to a non-analyst. Serre subsequently changed his research focus. \n\nIn the 1950s and 1960s, a fruitful collaboration between Serre and the two-years-younger Alexander Grothendieck led to important foundational work, much of it motivated by the Weil conjectures. Two major foundational papers by Serre were \"Faisceaux Algébriques Cohérents\" (FAC), on coherent cohomology, and \"Géometrie Algébrique et Géométrie Analytique\" (GAGA).\n\nEven at an early stage in his work Serre had perceived a need to construct more general and refined cohomology theories to tackle the Weil conjectures. The problem was that the cohomology of a coherent sheaf over a finite field couldn't capture as much topology as singular cohomology with integer coefficients. Amongst Serre's early candidate theories of 1954–55 was one based on Witt vector coefficients.\n\nAround 1958 Serre suggested that isotrivial principal bundles on algebraic varieties – those that become trivial after pullback by a finite étale map – are important. This acted as one important source of inspiration for Grothendieck to develop étale topology and the corresponding theory of étale cohomology. These tools, developed in full by Grothendieck and collaborators in Séminaire de géométrie algébrique (SGA) 4 and SGA 5, provided the tools for the eventual proof of the Weil conjectures by Pierre Deligne.\n\nFrom 1959 onward Serre's interests turned towards group theory, number theory, in particular Galois representations and modular forms.\n\nAmongst his most original contributions were: his \"Conjecture II\" (still open) on Galois cohomology; his use of group actions on trees (with Hyman Bass); the Borel–Serre compactification; results on the number of points of curves over finite fields; Galois representations in ℓ-adic cohomology and the proof that these representations have often a \"large\" image; the concept of \"p\"-adic modular form; and the Serre conjecture (now a theorem) on mod-\"p\" representations that made Fermat's last theorem a connected part of mainstream arithmetic geometry.\n\nIn his paper FAC, Serre asked whether a finitely generated projective module over a polynomial ring is free. This question led to a great deal of activity in commutative algebra, and was finally answered in the affirmative by Daniel Quillen and Andrei Suslin independently in 1976. This result is now known as the Quillen–Suslin theorem.\n\nSerre, at twenty-seven in 1954, is the youngest ever to be awarded the Fields Medal. He went on to win the Balzan Prize in 1985, the Steele Prize in 1995, the Wolf Prize in Mathematics in 2000, and was the first recipient of the Abel Prize in 2003. He has been awarded other prizes, such as the Gold Medal of the French National Scientific Research Centre (Centre National de la Recherche Scientifique, CNRS).\n\nHe is a foreign member of several scientific Academies (France, US, Norway, Sweden, Russia, the Royal Society, Royal Netherlands Academy of Arts and Sciences (1978)) and has received many honorary degrees (from Cambridge, Oxford, Harvard, and others). In 2012 he became a fellow of the American Mathematical Society.\n\nSerre has been awarded the highest honors in France as Grand Cross of the Legion of Honour (Grand Croix de la Légion d'Honneur) and Grand Cross of the Legion of Merit (Grand Croix de l'Ordre National du Mérite).\n\n\n\n"}
{"id": "40285963", "url": "https://en.wikipedia.org/wiki?curid=40285963", "title": "John Roe (mathematician)", "text": "John Roe (mathematician)\n\nJohn Roe (October 6, 1959 – March 9, 2018) was a British mathematician.\n\nRoe grew up in the countryside in Shropshire. He went to Rugby School, was an undergraduate at Cambridge University, and received his D.Phil. in 1985 from the University of Oxford under the supervision of Michael Atiyah. As a post-doctoral student, he was at the Mathematical Sciences Research Institute (MSRI) in Berkeley, and then a tutor at Jesus College, Oxford. From 1998 until shortly before his death he was a professor at the Pennsylvania State University.\n\nHis research interests center around index theorems, coarse geometry, operator algebras, noncommutative geometry, and the Novikov conjecture in differential topology. He was an editor of the \"Journal of Noncommutative Geometry\" and the \"Journal of Topology\".\n\nIn 1996 he was awarded the Whitehead Prize. In 2012 he became a fellow of the American Mathematical Society.\n\n\n"}
{"id": "31597396", "url": "https://en.wikipedia.org/wiki?curid=31597396", "title": "Katz centrality", "text": "Katz centrality\n\nIn graph theory, the Katz centrality of a node is a measure of centrality in a network. It was introduced by Leo Katz in 1953 and is used to measure the relative degree of influence of an actor (or node) within a social network. Unlike typical centrality measures which consider only the shortest path (the geodesic) between a pair of actors, Katz centrality measures influence by taking into account the total number of walks between a pair of actors.\n\nIt is similar to Google's PageRank and to the eigenvector centrality.\n\nKatz centrality computes the relative influence of a node within a network by measuring the number of the immediate neighbors (first degree nodes) and also all other nodes in the network that connect to the node under consideration through these immediate neighbors. Connections made with distant neighbors are, however, penalized by an attenuation factor formula_1. Each path or connection between a pair of nodes is assigned a weight determined by formula_1 and the distance between nodes as formula_3.\n\nFor example, in the figure on the right, assume that John's centrality is being measured and that formula_4. The weight assigned to each link that connects John with his immediate neighbors Jane and Bob will be formula_5. Since Jose connects to John indirectly through Bob, the weight assigned to this connection (composed of two links) will be formula_6. Similarly, the weight assigned to the connection between Agneta and John through Aziz and Jane will be formula_7 and the weight assigned to the connection between Agneta and John through Diego, Jose and Bob will be formula_8.\n\nLet \"A\" be the adjacency matrix of a network under consideration. Elements formula_9 of \"A\" are variables that take a value 1 if a node \"i\" is connected to node \"j\" and 0 otherwise. The powers of \"A\" indicate the presence (or absence) of links between two nodes through intermediaries. For instance, in matrix formula_10, if element formula_11, it indicates that node 2 and node 12 are connected through some first and second degree neighbors of node 2. If formula_12 denotes Katz centrality of a node \"i\", then mathematically:\n\nNote that the above definition uses the fact that the element at location formula_14 of formula_15 reflects the total number of formula_16 degree connections between nodes formula_17 and formula_18. The value of the attenuation factor formula_1 has to be chosen such that it is smaller than the reciprocal of the absolute value of the largest eigenvalue of \"A\". In this case the following expression can be used to calculate Katz centrality:\n\nHere formula_21 is the identity matrix, formula_22 is a vector of size \"n\" (\"n\" is the number of nodes) consisting of ones. formula_23 denotes the transposed matrix of A and formula_24 denotes matrix inversion of the term formula_25. \n\nAn extension of this framework allows for the walks to be computed in a dynamical setting. By taking a time dependent series of network adjacency snapshots of the transient edges, the dependency for walks to contribute towards a cumulative effect is presented. The arrow of time is preserved so that the contribution of activity is asymmetric in the direction of information propagation.\n\nNetwork producing data of the form: \n\nrepresenting the adjacency matrix at each time formula_27. Hence, \n\nThe time points formula_29 are ordered but not necessarily equally spaced. formula_30 for which formula_31 is a weighted count of the number of dynamic walks of length formula_32 from node formula_17 to node formula_18. The form for the dynamic communicability between participating nodes is:\n\nThis can be normalized via:\n\nTherefore centrality measures that quantify how effectively node formula_37 can 'broadcast' and 'receive' dynamic messages across the network,\n\nKatz centrality can be used to compute centrality in directed networks such as citation networks and the World Wide Web. \n\nKatz centrality is more suitable in the analysis of directed acyclic graphs where traditionally used measures like eigenvector centrality are rendered useless. \n\nKatz centrality can also be used in estimating the relative status or influence of actors in a social network. The work presented in shows the case study of applying a dynamic version of the Katz centrality to data from Twitter and focuses on particular brands which have stable discussion leaders. The application allows for a comparison of the methodology with that of human experts in the field and how the results are in agreement with a panel of social media experts.\n\nIn neuroscience, it is found that Katz centrality correlates with the relative firing rate of neurons in a neural network.. The temporal extension of the Katz centrality is applied to fMRI data obtained from a musical learning experiement in where data is collected from the subjects before and after the learning process. The results show that the changes to the network structure over the musical exposure created in each session a quantification of the cross communicability that produced clusters in line with the success of learning.\n"}
{"id": "642006", "url": "https://en.wikipedia.org/wiki?curid=642006", "title": "Laplace's method", "text": "Laplace's method\n\nIn mathematics, Laplace's method, named after Pierre-Simon Laplace, is a technique used to approximate integrals of the form\n\nwhere \"ƒ\"(\"x\") is some twice-differentiable function, \"M\" is a large number, and the endpoints \"a\" and \"b\" could possibly be infinite. This technique was originally presented in Laplace (1774, pp. 366–367).\n\nAssume that the function \"ƒ\"(\"x\") has a unique global maximum at \"x\". Then, the value \"ƒ\"(\"x\") will be larger than the other values of \"ƒ\"(\"x\"). If we multiply this function by a large number \"M\", the ratio between \"Mƒ\"(\"x\") and \"Mƒ\"(\"x\") will stay the same (since formula_2), but it will grow exponentially in the function formula_3(see figure).\n\nThus, significant contributions to the integral of this function will come only from points \"x\" in a neighbourhood of \"x\", which can then be estimated.\n\nTo state and motivate the method, we need several assumptions. We will assume that \"x\" is not an endpoint of the interval of integration, that the values \"ƒ\"(\"x\") cannot be very close to \"ƒ\"(\"x\") unless \"x\" is close to \"x\", and that the second derivative formula_4.\n\nWe can expand \"ƒ\"(\"x\") around \"x\" by Taylor's theorem,\n\nwhere formula_6 (See: big O notation.)\n\nSince \"ƒ\" has a global maximum at \"x\", and since \"x\" is not an endpoint, it is a stationary point, so the derivative of \"ƒ\" vanishes at \"x\". Therefore, the function \"ƒ\"(\"x\") may be approximated to quadratic order\n\nfor \"x\" close to \"x\" (recall that the second derivative is negative at the global maximum \"ƒ\"(\"x\")). The assumptions made ensure the accuracy of the approximation\n\n(see the picture on the right). This latter integral is a Gaussian integral if the limits of integration go from −∞ to +∞ (which can be assumed because the exponential decays very fast away from \"x\"), and thus it can be calculated. We find\n\nA generalization of this method and extension to arbitrary precision is provided by Fog (2008).\n\nFormal statement and proof:\n\nAssume that formula_10 is a twice continuously differentiable function on formula_11 with formula_12 the unique point such that formula_13. Assume additionally that formula_14.\n\nThen,\n\nLower bound:\n\nLet formula_16. Then by the continuity of formula_17 there exists formula_18 such that if formula_19 then formula_20. By Taylor's Theorem, for any formula_21, formula_22.\n\nThen we have the following lower bound:\n\nwhere the last equality was obtained by a change of variables formula_24. Remember that formula_14 so that is why we can take the square root of its negation.\n\nIf we divide both sides of the above inequality by formula_26 and take the limit we get:\n\nsince this is true for arbitrary formula_28 we get the lower bound:\n\nNote that this proof works also when formula_30 or formula_31 (or both).\n\nUpper bound:\n\nThe proof of the upper bound is similar to the proof of the lower bound but there are a few inconveniences. Again we start by picking an formula_32 but in order for the proof to work we need formula_28 small enough so that formula_34. Then, as above, by continuity of formula_17 and Taylor's Theorem we can find formula_36 so that if formula_37, then formula_38. Lastly, by our assumptions (assuming formula_39 are finite) there exists an formula_40 such that if formula_41, then formula_42.\n\nThen we can calculate the following upper bound:\n\nIf we divide both sides of the above inequality by formula_26 and take the limit we get:\n\nSince formula_28 is arbitrary we get the upper bound:\n\nAnd combining this with the lower bound gives the result.\n\nNote that the above proof obviously fails when formula_30 or formula_49 (or both). To deal with these cases, we need some extra assumptions. A sufficient (not necessary) assumption is that for formula_50, the integral formula_51 is finite, and that the number formula_52 as above exists (note that this must be an assumption in the case when the interval formula_11 is infinite). The proof proceeds otherwise as above, but the integrals\n\nmust be approximated by\n\ninstead of formula_56 as above, so that when we divide by formula_26, we get for this term\n\nwhose limit as formula_59 is formula_60. The rest of the proof (the analysis of the interesting term) proceeds as above.\n\nThe given condition in the infinite interval case is, as said above, sufficient but not necessary. However, the condition is fulfilled in many, if not in most, applications: the condition simply says that the integral we are studying must be well-defined (not infinite) and that the maximum of the function at formula_61 must be a \"true\" maximum (the number formula_62 must exist). There is no need to demand that the integral is finite for formula_63 but it is enough to demand that the integral is finite for some formula_64.\n\nThis method relies on 4 basic concepts such as\n\nFirst of all, we need to have an understanding about the so-called “approximation” in this method is related to the relative error instead of the absolute error. Therefore, if we set\n\nformula_65\n\n, this integration can be written as\n\n, where formula_67 is a small number when formula_68 is a large number obviously and the relative error will be\n\nNow, let us separate this integration into two parts: formula_70 region and the rest part.\n\nLet’s look at the Taylor expansion of formula_74 around \"x\" and translate \"x\" to \"y\" because we do the comparison in y-space, we will get\n\nNote that formula_76 because formula_77 is a stationary point.\nFrom this equation you will find that the terms higher than second derivative in this Taylor expansion is suppressed as the order of formula_78 so that formula_79 will get closer to the Gaussian function as shown in figure. Besides,\n\nformula_80\n\nBecause we do the comparison in y-space, formula_83 is fixed in formula_70which will cause formula_85; however, formula_67 is inversely proportional to formula_87, the chosen region of formula_88 will be smaller when formula_68 is increased.\n\nRelying on the 3rd concept, even if we choose a very large \"D\" , \"sD\" will finally be a very small number when formula_92 is increased to a huge number. Then, how can we guarantee the integration of the rest part will tend to 0 when formula_81 is large enough?\n\nThe basic idea is trying to find a function formula_94 which will formula_95 and the integration of formula_96 will tend to zero when formula_92 is increased. Because the exponential function of formula_98 will be always larger than zero as long as formula_99 is a real number, and this exponential function is proportional to formula_99 , the integration of formula_101 will tend to zero. For simplicity, let me choose formula_94 as a tangent through the point formula_103 as shown in the figure:\n\nIf the interval of the integration of this method is finite, we will find that no matter formula_10 is continue in the rest region, it will be always smaller than formula_99 shown above when formula_68 is large enough. By the way, it will be proved later that the integration of formula_96 will tend to zero when formula_81 is large enough.\n\nIf the interval of the integration of this method is infinite, formula_94 and formula_10 might always cross to each other. If so, we cannot guarantee that the integration of formula_101 will tend to zero finally. For example, in the case of formula_112 , formula_113 will be always diverged. Therefore, we need to require that formula_114 can converge for the infinite interval case. If so, this integration will tend to zero when formula_115 is large enough and we can choose this formula_115 as the cross of formula_94 and formula_10 .\n\nYou might ask that why not choose formula_119 as a convergent integration? Let me use an example to show you the reason. Suppose the rest part of formula_10 is formula_121 , then formula_122 and its integration will diverge; however, when formula_123 ,the integration of formula_124 converges. So, the integrations of some functions will diverge when formula_81 is not a large number, but they will converge when formula_81 is large enough.\nBased on these four concepts, we can derive the relative error of this Laplace's method.\n\nLaplace's approximation is sometimes written as\n\nwhere formula_128 is positive.\n\nImportantly, the accuracy of the approximation depends on the variable of integration, that is, on what stays in formula_129 and what goes into formula_130.\nFirst of all, let me set the global maximum is located at formula_131 which can simplify the derivation and does not lost any important information; therefore, all the derivation inside this sub-section is under this assumption. Besides, what we want is the relative error formula_132 as shown below\n\nwhere formula_134.\nSo, if we let formula_135 and formula_136, we can get\n\nsince formula_138. Now, let us find its upper bound.\n\nOwing to formula_139, we can separate this integration into 5 parts with 3 different types (a), (b) and (c), respectively. Therefore,\n\nwhere formula_141 and formula_142 are similar, let us just calculate formula_141 , and formula_144 and formula_145 are similar, too, I’ll just calculate formula_144.\n\nFor formula_147, after the translation of formula_148, we can get\n\nThis means that as long as formula_150 is large enough, it will tend to zero.\n\nFor formula_151, we can get\n\nwhere\n\nand formula_154 should have the same sign of formula_155 during this region.\nLet us choose formula_94 as the tangent across the point at formula_103 , i.e. formula_158 which is shown in the figure\n\nFrom this figure you can find that when formula_67 or formula_160 gets smaller, the region satisfies the above inequality will get larger. Therefore, if we want to find a suitable formula_99 to cover the whole formula_162 during the interval of formula_151, formula_160 will have an upper limit. Besides, because the integration of formula_165 is simple, let me use it to estimate the relative error contributed by this formula_151.\n\nBased on Taylor expansion, we can get\n\nand\n\nand then substitute them back into the calculation of formula_144; however, you can find that the remainders of these two expansions are both inversely proportional to the square root of formula_81, let me drop them out to beautify the calculation. Keeping them is better, but it will make the formula uglier.\n\nTherefore, it will tend to zero when formula_150 gets larger, but don't forget that the upper bound of formula_150 should be considered during this calculation.\n\nAbout the integration near formula_174, we can also use Taylor's Theorem to calculate it. When formula_175\n\nand you can find that it is inversely proportional to the square root of formula_81. In fact, formula_178 will have the same behave when formula_179 is a constant.\n\nConclusively, the integration near the stationary point will get smaller when formula_180 gets larger, and the rest parts will tend to zero as long as formula_150 is large enough; however, we need to remember that formula_150 has an upper limit which is decided by whether the function formula_94 is always larger than formula_184 during this rest region. However, as long as we can find one formula_94 satisfies this condition, the upper bound of formula_150 can be chosen as directly proportional to formula_87 since formula_99 is a tangent across the point of formula_189 at formula_190. So, the bigger formula_81 is, the bigger formula_150 can be.\nIn the multivariate case where formula_193 is a formula_194-dimensional vector and formula_195 is a scalar function of formula_193, Laplace's approximation is usually written as:\n\nwhere formula_198 is the Hessian matrix of formula_199 evaluated at formula_200 and where formula_201 denotes matrix determinant. Analogously to the univariate case, the Hessian is required to be negative definite.\n\nBy the way, although formula_193 denotes a formula_194-dimensional vector, the term formula_204 denotes an infinitesimal volume here, i.e. formula_205.\n\nIn extensions of Laplace's method, complex analysis, and in particular Cauchy's integral formula, is used to find a contour \"of steepest descent\" for an (asymptotically with large \"M\") equivalent integral, expressed as a line integral. In particular, if no point \"x\" where the derivative of \"ƒ\" vanishes exists on the real\nline, it may be necessary to deform the integration contour to an optimal one, where the above analysis will be possible. Again the main idea is to reduce, at least asymptotically, the calculation of the given integral to that of a simpler integral that can be explicitly evaluated. See the book of Erdelyi (1956) for a simple discussion (where the method is termed \"steepest descents\").\n\nThe appropriate formulation for the complex \"z\"-plane is\nfor a path passing through the saddle point at \"z\".\nNote the explicit appearance of a minus sign to indicate the direction of the second derivative: one must \"not\" take the modulus. Also note that if the integrand is meromorphic, one may have to add residues corresponding to poles traversed while deforming the contour (see for example section 3 of Okounkov's paper \"Symmetric functions and random partitions\").\n\nAn extension of the steepest descent method is the so-called \"nonlinear stationary phase/steepest descent method\". Here, instead of integrals, one needs to evaluate asymptotically solutions of Riemann–Hilbert factorization problems.\n\nGiven a contour \"C\" in the complex sphere, a function \"ƒ\" defined on that contour and a special point, say infinity, one seeks a function \"M\" holomorphic away from the contour \"C\", with prescribed jump across \"C\", and with a given normalization at infinity. If \"ƒ\" and hence \"M\" are matrices rather than scalars this is a problem that in general does not admit an explicit solution.\n\nAn asymptotic evaluation is then possible along the lines of the linear stationary phase/steepest descent method. The idea is to reduce asymptotically the solution of the given Riemann–Hilbert problem to that of a simpler, explicitly solvable, Riemann–Hilbert problem. Cauchy's theorem is used to justify deformations of the jump contour.\n\nThe nonlinear stationary phase was introduced by Deift and Zhou in 1993, based on earlier work of Its. A (properly speaking) nonlinear steepest descent method was introduced by Kamvissis, K. McLaughlin and P. Miller in 2003, based on previous work of Lax, Levermore, Deift, Venakides and Zhou.\nAs in the linear case, \"steepest descent contours\" solve a min-max problem. In the nonlinear case they turn out to be \"S-curves\" (defined in a different context back in the 80s by Stahl, Gonchar and Rakhmanov).\n\nThe nonlinear stationary phase/steepest descent method has applications to the theory of soliton equations\nand integrable models, random matrices and combinatorics.\n\nFor complex integrals in the form:\n\nwith \"t\" » 1, we make the substitution \"t\" = \"iu\" and the change of variable \"s\" = \"c\" + \"ix\" to get the bilateral Laplace transform:\n\nWe then split \"g\"(\"c\"+\"ix\") in its real and complex part, after which we recover \"u\" = \"t\" / \"i\". This is useful for inverse Laplace transforms, the Perron formula and complex integration.\n\nLaplace's method can be used to derive Stirling's approximation\nfor a large integer \"N\".\n\nFrom the definition of the Gamma function, we have\n\nNow we change variables, letting\nso that\nPlug these values back in to obtain\n\nThis integral has the form necessary for Laplace's method with\nwhich is twice-differentiable:\nThe maximum of \"ƒ\"(\"z\") lies at \"z\" = 1, and the second derivative of \"ƒ\"(\"z\") has the value −1 at this point. Therefore, we obtain\n\n\n"}
{"id": "11567176", "url": "https://en.wikipedia.org/wiki?curid=11567176", "title": "Linda Keen", "text": "Linda Keen\n\nLinda Jo Goldway Keen (born 9 August 1940 in New York City, New York) is a mathematician and a fellow of the American Mathematical Society. Since 1965, she has been a Professor in the Department of Mathematics and Computer Science at Lehman College of The City University of New York and a Professor of Mathematics at The Graduate Center of The City University of New York.\n\nAs a high school student she attended the Bronx High School of Science. She received her Bachelor of Science degree from the City College of New York, then studied at the Courant Institute of Mathematical Sciences, earning her Doctor of Philosophy in mathematics in 1964. She wrote her thesis on Riemann surfaces under the direction of Lipman Bers at NYU.\n\nKeen has worked at the Institute for Advanced Study, Hunter College, University of California at Berkeley, Columbia University, Boston University, Princeton University, and the Massachusetts Institute of Technology, as well as at various mathematical institutes in Europe and South America. After her initial appointment in 1965, in 1974 Keen was promoted to Full Professor at Lehman College and the CUNY Graduate Center. She is currently Executive Officer of the Mathematics Program at the Graduate Center.\n\nKeen served as president of the Association for Women in Mathematics during 1985-1986 and as vice-president of the American Mathematical Society during 1992-1995. She served on the Board of Trustees of the American Mathematical Society from 1999-2009 and as Associate Treasurer from 2009-2011. \nKeen worked with the mathematicians Paul Blanchard, Robert L. Devaney, Jane Gilman, Lisa Goldberg, Nikola Lakic and Caroline Series among many others.\n\nIn 1975, Keen presented an AMS invited address and in 1989 she presented an MAA joint invited address. In 1993 she was selected as a Noether Lecturer by the Association for Women in Mathematics.\n\nIn addition to studying Riemann surfaces, Keen has worked in hyperbolic geometry, Kleinian groups and Fuchsian groups, complex analysis, and hyperbolic dynamics. In the field of hyperbolic geometry, she is known for the Collar lemma.\n\nShe is married to Jonathan Brezin and resides in New York.\n\nShe has been honored with:\nIn 2012 she became a fellow of the American Mathematical Society.\n\nIn 2017, she was selected as a fellow of the Association for Women in Mathematics in the inaugural class.\n\n\n"}
{"id": "47147130", "url": "https://en.wikipedia.org/wiki?curid=47147130", "title": "Liouvillian function", "text": "Liouvillian function\n\nIn mathematics, a Liouvillian function is an elementary function or (recursively) the integral of a Liouvillian function.\n\nMore explicitly, it is a function of one variable which is the composition of a finite number of arithmetic operations , exponentials, constants, solutions of algebraic equations (a generalization of \"n\"th roots), and antiderivatives. The logarithm function does not need to be explicitly included since it is the integral of formula_1.\n\nIt follows directly from the definition that the set of Liouvillian functions is closed under arithmetic operations, composition, and integration. It is also closed under differentiation. It is not closed under limits and infinite sums.\n\nLiouvillian functions were introduced by Joseph Liouville in a series of papers from 1833 to 1841.\n\nAll elementary functions are Liouvillian.\n\nExamples of well-known functions which are Liouvillian but not elementary are the nonelementary integrals, for example:\n\nAll Liouvillian functions are solutions of algebraic differential equations, but not conversely. Examples of functions which are solutions of algebraic differential equations but not Liouvillian include:\n\nExamples of functions which are \"not\" solutions of algebraic differential equations and thus not Liouvillian include all transcendentally transcendental functions, such as:\n\n\n"}
{"id": "37127478", "url": "https://en.wikipedia.org/wiki?curid=37127478", "title": "List of things named after Thomas Bayes", "text": "List of things named after Thomas Bayes\n\nThomas Bayes (/beɪz/; c. 1701 – 1761) was an English statistician, philosopher, and Presbyterian minister. \n\nBayesian () refers to a range of concepts and approaches that are ultimately based on Bayes' theorem, of which the most important are:\n\nOther things named after Thomas Bayes include:\n\n\n"}
{"id": "312867", "url": "https://en.wikipedia.org/wiki?curid=312867", "title": "Local analysis", "text": "Local analysis\n\nIn mathematics, the term local analysis has at least two meanings - both derived from the idea of looking at a problem relative to each prime number \"p\" first, and then later trying to integrate the information gained at each prime into a 'global' picture. These are forms of the approach.\n\nIn group theory, local analysis was started by the Sylow theorems, which contain significant information about the structure of a finite group \"G\" for each prime number \"p\" dividing the order of \"G\". This area of study was enormously developed in the quest for the classification of finite simple groups, starting with the Feit–Thompson theorem that groups of odd order are solvable.\n\nIn number theory one may study a Diophantine equation, for example, modulo \"p\" for all primes \"p\", looking for constraints on solutions. The next step is to look modulo prime powers, and then for solutions in the \"p\"-adic field. This kind of local analysis provides conditions for solution that are \"necessary\". In cases where local analysis (plus the condition that there are real solutions) provides also \"sufficient\" conditions, one says that the \"Hasse principle\" holds: this is the best possible situation. It does for quadratic forms, but certainly not in general (for example for elliptic curves). The point of view that one would like to understand what extra conditions are needed has been very influential, for example for cubic forms.\n\nSome form of local analysis underlies both the standard applications of the Hardy-Littlewood circle method in analytic number theory, and the use of adele rings, making this one of the unifying principles across number theory.\n\n"}
{"id": "8233045", "url": "https://en.wikipedia.org/wiki?curid=8233045", "title": "Loomis–Whitney inequality", "text": "Loomis–Whitney inequality\n\nIn mathematics, the Loomis–Whitney inequality is a result in geometry, which in its simplest form, allows one to estimate the \"size\" of a formula_1-dimensional set by the sizes of its formula_2-dimensional projections. The inequality has applications in incidence geometry, the study of so-called \"lattice animals\", and other areas.\n\nThe result is named after the American mathematicians Lynn Harold Loomis and Hassler Whitney, and was published in 1949.\n\nFix a dimension formula_3 and consider the projections\n\nFor each 1 ≤ \"j\" ≤ \"d\", let\n\nThen the Loomis–Whitney inequality holds:\n\nEquivalently, taking\n\nThe Loomis–Whitney inequality can be used to relate the Lebesgue measure of a subset of Euclidean space formula_11 to its \"average widths\" in the coordinate directions. Let \"E\" be some measurable subset of formula_11 and let\n\nbe the indicator function of the projection of \"E\" onto the \"j\"th coordinate hyperplane. It follows that for any point \"x\" in \"E\",\n\nHence, by the Loomis–Whitney inequality,\n\nand hence\n\nThe quantity\n\ncan be thought of as the average width of formula_18 in the formula_19th coordinate direction. This interpretation of the Loomis–Whitney inequality also holds if we consider a finite subset of Euclidean space and replace Lebesgue measure by counting measure.\n\nThe Loomis–Whitney inequality is a special case of the Brascamp–Lieb inequality, in which the projections \"π\" above are replaced by more general linear maps, not necessarily all mapping onto spaces of the same dimension.\n"}
{"id": "31538509", "url": "https://en.wikipedia.org/wiki?curid=31538509", "title": "Margrabe's formula", "text": "Margrabe's formula\n\nIn mathematical finance, Margrabe's formula is an option pricing formula applicable to an option to exchange one risky asset for another risky asset at maturity. It was derived by William Margrabe (PhD Chicago) in 1978. Margrabe's paper has been cited by over 1500 subsequent articles.\nSuppose \"S(t)\" and \"S(t)\" are the prices of two risky assets at time \"t\", and that each has a constant continuous dividend yield \"q\". The option, \"C\", that we wish to price gives the buyer the right, but not the obligation, to exchange the second asset for the first at the time of maturity \"T\". In other words, its payoff, \"C(T)\", is max(0, \"S(T) - S(T))\".\n\nIf the volatilities of \"S\" 's are \"σ\", then formula_1, where \"ρ\" is the Pearson's correlation coefficient of the Brownian motions of the \"S\" 's.\n\nMargrabe's formula states that the fair price for the option at time 0 is:\n\nMargrabe's model of the market assumes only the existence of the two risky assets, whose prices, as usual, are assumed to follow a geometric Brownian motion. The volatilities of these Brownian motions do not need to be constant, but it is important that the volatility of \"S/S\", \"σ\", is constant. In particular, the model does not assume the existence of a riskless asset (such as a zero-coupon bond) or any kind of interest rate. The model does not require an equivalent risk-neutral probability measure, but an equivalent measure under S.\n\nThe formula is quickly proven by reducing the situation to one where we can apply the Black-Scholes formula. \n\nNotes\n\nPrimary reference\nDiscussion\n"}
{"id": "19266946", "url": "https://en.wikipedia.org/wiki?curid=19266946", "title": "Mathematical diagram", "text": "Mathematical diagram\n\nMathematical diagrams are diagrams in the field of mathematics, and diagrams using mathematics such as charts and graphs, that are mainly designed to convey mathematical relationships, for example, comparisons over time.\n\nA complex number can be visually represented as a pair of numbers forming a vector on a diagram called an Argand diagram\nThe complex plane is sometimes called the \"Argand plane\" because it is used in \"Argand diagrams\". These are named after Jean-Robert Argand (1768–1822), although they were first described by Norwegian-Danish land surveyor and mathematician Caspar Wessel (1745–1818). Argand diagrams are frequently used to plot the positions of the poles and zeroes of a function in the complex plane.\n\nThe concept of the complex plane allows a geometric interpretation of complex numbers. Under addition, they add like vectors. The multiplication of two complex numbers can be expressed most easily in polar coordinates — the magnitude or \"modulus\" of the product is the product of the two absolute values, or moduli, and the angle or \"argument\" of the product is the sum of the two angles, or arguments. In particular, multiplication by a complex number of modulus 1 acts as a rotation.\n\nIn the context of fast Fourier transform algorithms, a butterfly is a portion of the computation that combines the results of smaller discrete Fourier transforms (DFTs) into a larger DFT, or vice versa (breaking a larger DFT up into subtransforms). The name \"butterfly\" comes from the shape of the data-flow diagram in the radix-2 case, as described below. The same structure can also be found in the Viterbi algorithm, used for finding the most likely sequence of hidden states.\n\nThe butterfly diagram show a data-flow diagram connecting the inputs \"x\" (left) to the outputs \"y\" that depend on them (right) for a \"butterfly\" step of a radix-2 Cooley–Tukey FFT algorithm. This diagram resembles a butterfly as in the morpho butterfly shown for comparison), hence the name.\n\nIn mathematics, and especially in category theory, a commutative diagram is a diagram of objects, also known as vertices, and morphisms, also known as arrows or edges, such that when selecting two objects any directed path through the diagram leads to the same result by composition.\n\nCommutative diagrams play the role in category theory that equations play in algebra.\n\nA Hasse diagram is a simple picture of a finite partially ordered set, forming a drawing of the partial order's transitive reduction. Concretely, one represents each element of the set as a vertex on the page and draws a line segment or curve that goes upward from \"x\" to \"y\" precisely when \"x\" < \"y\" and there is no \"z\" such that \"x\" < \"z\" < \"y\". In this case, we say y covers x, or y is an immediate successor of x. In a Hasse diagram, it is required that the curves be drawn so that each meets exactly two vertices: its two endpoints. Any such diagram (given that the vertices are labeled) uniquely determines a partial order, and any partial order has a unique transitive reduction, but there are many possible placements of elements in the plane, resulting in different Hasse diagrams for a given order that may have widely varying appearances.\n\nIn Knot theory a useful way to visualise and manipulate knots is to project the knot onto a plane—;think of the knot casting a shadow on the wall. A small perturbation in the choice of projection will ensure that it is one-to-one except at the double points, called \"crossings\", where the \"shadow\" of the knot crosses itself once transversely\n\nAt each crossing we must indicate which section is \"over\" and which is \"under\", so as to be able to recreate the original knot. This is often done by creating a break in the strand going underneath. If by following the diagram the knot alternately crosses itself \"over\" and \"under\", then the diagram represents a particularly well-studied class of knot, alternating knots.\n\nA Venn diagram is a representation of mathematical sets: a mathematical diagram representing sets as circles, with their relationships to each other expressed through their overlapping positions, so that all possible relationships between the sets are shown.\n\nThe Venn diagram is constructed with a collection of simple closed curves drawn in the plane. The principle of these diagrams is that classes be represented by regions in such relation to one another that all the possible logical relations of these classes can be indicated in the same diagram. That is, the diagram initially leaves room for any possible relation of the classes, and the actual or given relation, can then be specified by indicating that some particular region is null or is notnull.\n\nA Voronoi diagram is a special kind of decomposition of a metric space determined by distances to a specified discrete set of objects in the space, e.g., by a discrete set of points. This diagram is named after Georgy Voronoi, also called a Voronoi tessellation, a Voronoi decomposition, or a Dirichlet tessellation after Peter Gustav Lejeune Dirichlet.\n\nIn the simplest case, we are given a set of points S in the plane, which are the Voronoi sites. Each site s has a Voronoi cell V(s) consisting of all points closer to s than to any other site. The segments of the Voronoi diagram are all the points in the plane that are equidistant to two sites. The Voronoi nodes are the points equidistant to three (or more) sites\n\nA wallpaper group or \"plane symmetry group\" or \"plane crystallographic group\" is a mathematical classification of a two-dimensional repetitive pattern, based on the symmetries in the pattern. Such patterns occur frequently in architecture and decorative art. There are 17 possible distinct groups.\n\nWallpaper groups are two-dimensional symmetry groups, intermediate in complexity between the simpler frieze groups and the three-dimensional crystallographic groups, also called space groups. Wallpaper groups categorize patterns by their symmetries. Subtle differences may place similar patterns in different groups, while patterns which are very different in style, color, scale or orientation may belong to the same group.\n\nA \"Young diagram\" or Young tableau, also called Ferrers diagram, is a finite collection of boxes, or cells, arranged in left-justified rows, with the row sizes weakly decreasing (each row has the same or shorter length than its predecessor). \n\nListing the number of boxes in each row gives a partition formula_1 of a positive integer \"n\", the total number of boxes of the diagram. The Young diagram is said to be of shape formula_1, and it carries the same information as that partition. Listing the number of boxes in each column gives another partition, the conjugate or \"transpose\" partition of formula_1; one obtains a Young diagram of that shape by reflecting the original diagram along its main diagonal.\n\nYoung tableaux were introduced by Alfred Young, a mathematician at Cambridge University, in 1900. They were then applied to the study of symmetric group by Georg Frobenius in 1903. Their theory was further developed by many mathematicians.\n\n\n\n"}
{"id": "33207538", "url": "https://en.wikipedia.org/wiki?curid=33207538", "title": "Mathematical manuscripts of Karl Marx", "text": "Mathematical manuscripts of Karl Marx\n\nThe Mathematical manuscripts of Karl Marx consist mostly of Karl Marx's attempts to understand the foundations of infinitesimal calculus, from around 1873–1883. A Russian edition edited by Sofya Yanovskaya was eventually published in 1968, and an English translation was published in 1983 . \n\nAccording to Hubert C. Kennedy, Marx \"[...] seems to have been unaware of the advances being made by continental mathematicians in the foundations of differential calculus, including the work of Cauchy.\" In the same text, Kennedy says \"While Marx's analysis of the derivative and differential had no immediate effect on the historical development of mathematics, Engels' claim that Marx made \"independent discoveries\" is certainly justified. Marx's operational definition of the differential anticipated 20th century developments in mathematics, and there is another aspect of the differential, that seems to have been seen by Marx, that has become a standard part of modern textbooks—the concept of the differential as the principal part of an increment.\", implying that Marx's apprehension and interpretation of calculus was far from short-sighted. This may have contributed to an interest in nonstandard analysis among Chinese mathematicians .\n\n"}
{"id": "12989803", "url": "https://en.wikipedia.org/wiki?curid=12989803", "title": "Non-positive curvature", "text": "Non-positive curvature\n\nIn mathematics, spaces of non-positive curvature occur in many contexts and form a generalization of hyperbolic geometry. In the category of Riemannian manifolds, one can consider the sectional curvature of the manifold and require that this curvature be everywhere less than or equal to zero. The notion of curvature extends to the category of geodesic metric spaces, where one can use comparison triangles to quantify the curvature of a space; in this context, non-positively curved spaces are known as (locally) CAT(0) spaces.\n\n\n"}
{"id": "5080727", "url": "https://en.wikipedia.org/wiki?curid=5080727", "title": "Nuclear magnetic resonance quantum computer", "text": "Nuclear magnetic resonance quantum computer\n\nNuclear magnetic resonance quantum computing (NMRQC) is one of the several proposed approaches for constructing a quantum computer, that uses the spin states of nuclei within molecules as qubits. The quantum states are probed through the nuclear magnetic resonances, allowing the system to be implemented as a variation of nuclear magnetic resonance spectroscopy. NMR differs from other implementations of quantum computers in that it uses an ensemble of systems, in this case molecules, rather than a single pure state.\n\nInitially the approach was to use the spin properties of atoms of particular molecules in a liquid sample as qubits - this is known as liquid state NMR (LSNMR). This approach has since been superseded by solid state NMR (SSNMR) as a means of quantum computation.\n\nThe ideal picture of liquid state NMR (LSNMR) quantum information processing (QIP) is based on a molecule of which some of its atom’s nuclei behave as spin-½ systems. Depending on which nuclei we are considering they will have different energy levels and different interaction with its neighbours and so we can treat them as distinguishable qubits. In this system we tend to consider the inter-atomic bonds as the source of interactions between qubits and exploit these spin-spin interactions to perform 2-qubit gates such as CNOTs that are necessary for universal quantum computation. In addition to the spin-spin interactions native to the molecule an external magnetic field can be applied (in NMR laboratories) and these impose single qubit gates. By exploiting the fact that different spins will experience different local fields we have control over the individual spins.\n\nThe picture described above is far from realistic since we are treating a single molecule. NMR is performed on an ensemble of molecules, usually with as many as 10^15 molecules. This introduces complications to the model, one of which is introduction of decoherence. In particular we have the problem of an open quantum system interacting with a macroscopic number of particles near thermal equilibrium (~mK to ~300 K). This has led the development of decoherence suppression techniques that have spread to other disciplines such as trapped ions. The other significant issue with regards to working close to thermal equilibrium is the mixedness of the state. This required the introduction of ensemble quantum processing, whose principal limitation is that as we introduce more logical qubits into our system we require larger samples in order to attain discernable signals during measurement.\n\nSolid state NMR (SSNMR) differs from LSNMR in that we have a solid state sample, for example a nitrogen vacancy diamond lattice rather than a liquid sample. This has many advantages such as lack of molecular diffusion decoherence, lower temperatures can be achieved to the point of suppressing phonon decoherence and a greater variety of control operations that allow us to overcome one of the major problems of LSNMR that is initialisation. Moreover, as in a crystal structure we can localize precisely the qubits, we can measure each qubit individually, instead of having an ensamble measurement as in LSNMR.\n\nLiquid state NMR Quantum Information Processing was first theoretically introduced independently by Cory, Fahmy and Havel and Gershenfeld and Chuang in 1997. Some early success was obtained in performing quantum algorithms in NMR systems due to the relative maturity of NMR technology. For instance, in 2001 researchers at IBM reported the successful implementation of Shor's algorithm in a 7-qubit NMR quantum computer. However, even from the early days, it was recognized that NMR quantum computers would never be very useful due to the poor scaling of the signal to noise ratio in such systems. More recent work, particularly by Caves and others, shows that all experiments in liquid state bulk ensemble NMR quantum computing to date do not possess quantum entanglement, thought to be required for quantum computation. Hence NMR quantum computing experiments are likely to have been only classical simulations of a quantum computer.\n\nThe ensemble is initialized to be the thermal equilibrium state (see quantum statistical mechanics). In mathematical parlance, this state is given by the density matrix:\nwhere \"H\" is the hamiltonian matrix of an individual molecule and\n\nwhere formula_3 is the Boltzmann constant and formula_4 the temperature. That the initial state in NMR quantum computing is in thermal equilibrium is one of the main differences compared to other quantum computing techniques, where they are initialized in a pure state. Nevertheless, suitable mixed states are capable of reflecting quantum dynamics which lead to Gershenfeld and Chuang to term them \"pseudo-pure states.\"\n\nOperations are performed on the ensemble through radio frequency (RF) pulses applied perpendicular to a strong, static magnetic field, created by a very large magnet. See nuclear magnetic resonance.\n\nConsider applying a magnetic field along the z axis, fixing this as the principal quantization axis, on a liquid sample. The Hamiltonian for a single spin would be given by the Zeeman or chemical shift term:\nwhere formula_6 is the operator for the z component of the nuclear angular momentum, and formula_7 is the resonance frequency of the spin, which is proportional to the applied magnetic field.\n\nConsidering the molecules in the liquid sample to contain two spin ½ nuclei, the system Hamiltonian will have two chemical shift terms and a dipole coupling term:\n\nControl of a spin system can be realized by means of selective RF pulses applied perpendicular to the quantization axis. In the case of a two spin system as described above, we can distinguish two types of pulses: “soft” or spin-selective pulses, whose frequency range encompasses one of the resonant frequencies only, and therefore affects only that spin; and “hard” or nonselective pulses whose frequency range is broad enough to contain both resonant frequencies and therefore these pulses couple to both spins. For detailed examples of the effects of pulses on such a spin system, the reader is referred to Section 2 of work by Cory et al.\n"}
{"id": "1355482", "url": "https://en.wikipedia.org/wiki?curid=1355482", "title": "Pell number", "text": "Pell number\n\nIn mathematics, the Pell numbers are an infinite sequence of integers, known since ancient times, that comprise the denominators of the closest rational approximations to the square root of 2. This sequence of approximations begins , , , , and , so the sequence of Pell numbers begins with 1, 2, 5, 12, and 29. The numerators of the same sequence of approximations are half the companion Pell numbers or Pell–Lucas numbers; these numbers form a second infinite sequence that begins with 2, 6, 14, 34, and 82.\n\nBoth the Pell numbers and the companion Pell numbers may be calculated by means of a recurrence relation similar to that for the Fibonacci numbers, and both sequences of numbers grow exponentially, proportionally to powers of the silver ratio 1 + . As well as being used to approximate the square root of two, Pell numbers can be used to find square triangular numbers, to construct integer approximations to the right isosceles triangle, and to solve certain combinatorial enumeration problems.\n\nAs with Pell's equation, the name of the Pell numbers stems from Leonhard Euler's mistaken attribution of the equation and the numbers derived from it to John Pell. The Pell–Lucas numbers are also named after Édouard Lucas, who studied sequences defined by recurrences of this type; the Pell and companion Pell numbers are Lucas sequences.\n\nThe Pell numbers are defined by the recurrence relation\n\nIn words, the sequence of Pell numbers starts with 0 and 1, and then each Pell number is the sum of twice the previous Pell number and the Pell number before that. The first few terms of the sequence are\n\nThe Pell numbers can also be expressed by the closed form formula\nFor large values of \"n\", the term dominates this expression, so the Pell numbers are approximately proportional to powers of the silver ratio , analogous to the growth rate of Fibonacci numbers as powers of the golden ratio.\n\nA third definition is possible, from the matrix formula\n\nMany identities can be derived or proven from these definitions; for instance an identity analogous to Cassini's identity for Fibonacci numbers,\nis an immediate consequence of the matrix formula (found by considering the determinants of the matrices on the left and right sides of the matrix formula).\n\nPell numbers arise historically and most notably in the rational approximation to . If two large integers \"x\" and \"y\" form a solution to the Pell equation\nthen their ratio \"\" provides a close approximation to . The sequence of approximations of this form is\nwhere the denominator of each fraction is a Pell number and the numerator is the sum of a Pell number and its predecessor in the sequence. That is, the solutions have the form\nThe approximation\nof this type was known to Indian mathematicians in the third or fourth century B.C. The Greek mathematicians of the fifth century B.C. also knew of this sequence of approximations: Plato refers to the numerators as rational diameters. In the 2nd century CE Theon of Smyrna used the term the side and diameter numbers to describe the denominators and numerators of this sequence.\n\nThese approximations can be derived from the continued fraction expansion of formula_9:\nTruncating this expansion to any number of terms produces one of the Pell-number-based approximations in this sequence; for instance,\n\nAs Knuth (1994) describes, the fact that Pell numbers approximate allows them to be used for accurate rational approximations to a regular octagon with vertex coordinates and . All vertices are equally distant from the origin, and form nearly uniform angles around the origin. Alternatively, the points formula_12, formula_13, and formula_14 form approximate octagons in which the vertices are nearly equally distant from the origin and form uniform angles.\n\nA Pell prime is a Pell number that is prime. The first few Pell primes are\nThe indices of these primes within the sequence of all Pell numbers are\nThese indices are all themselves prime. As with the Fibonacci numbers, a Pell number \"P\" can only be prime if \"n\" itself is prime, because if \"d\" is a divisor of \"n\" then \"P\" is a divisor of \"P\".\n\nThe only Pell numbers that are squares, cubes, or any higher power of an integer are 0, 1, and 169 = 13.\n\nHowever, despite having so few squares or other powers, Pell numbers have a close connection to square triangular numbers. Specifically, these numbers arise from the following identity of Pell numbers:\nThe left side of this identity describes a square number, while the right side describes a triangular number, so the result is a square triangular number.\n\nSantana and Diaz-Barrero (2006) proved another identity relating Pell numbers to squares and showing that the sum of the Pell numbers up to \"P\" is always a square:\nFor instance, the sum of the Pell numbers up to \"P\", , is the square of . The numbers forming the square roots of these sums,\nare known as the Newman–Shanks–Williams (NSW) numbers.\n\nIf a right triangle has integer side lengths \"a\", \"b\", \"c\" (necessarily satisfying the Pythagorean theorem ), then (\"a\",\"b\",\"c\") is known as a Pythagorean triple. As Martin (1875) describes, the Pell numbers can be used to form Pythagorean triples in which \"a\" and \"b\" are one unit apart, corresponding to right triangles that are nearly isosceles. Each such triple has the form\nThe sequence of Pythagorean triples formed in this way is\n\nThe companion Pell numbers or Pell–Lucas numbers are defined by the recurrence relation\n\nIn words: the first two numbers in the sequence are both 2, and each successive number is formed by adding twice the previous Pell–Lucas number to the Pell–Lucas number before that, or equivalently, by adding the next Pell number to the previous Pell number: thus, 82 is the companion to 29, and The first few terms of the sequence are : 2, 2, 6, 14, 34, 82, 198, 478,…\n\nLike the relationship between Fibonacci numbers and Lucas numbers,\nfor all natural numbers \"n\".\n\nThe companion Pell numbers can be expressed by the closed form formula\n\nThese numbers are all even; each such number is twice the numerator in one of the rational approximations to formula_9 discussed above.\n\nLike the Lucas sequence, if a Pell–Lucas number \"Q\" is prime, it is necessary that n be either prime or a power of 2. The Pell–Lucas primes are\n\nFor these \"n\" are \n\nThe following table gives the first few powers of the silver ratio \"δ\" = \"δ\" = 1 +  and its conjugate = 1 − . \n\nThe coefficients are the half-companion Pell numbers \"H\" and the Pell numbers \"P\" which are the (non-negative) solutions to .\nA square triangular number is a number\nwhich is both the \"t\"th triangular number and the \"s\"th square number. A \"near-isosceles Pythagorean triple\" is an integer solution to where .\n\nThe next table shows that splitting the odd number \"H\" into nearly equal halves gives a square triangular number when \"n\" is even and a near isosceles Pythagorean triple when n is odd. All solutions arise in this manner.\n\nThe half-companion Pell numbers \"H\" and the Pell numbers \"P\" can be derived in a number of easily equivalent ways.\n\nFrom this it follows that there are \"closed forms\":\n\nand\n\nSo\n\nThe difference between \"H\" and \"P\" is\nwhich goes rapidly to zero. So \nis extremely close to 2\"H\".\n\nFrom this last observation it follows that the integer ratios \"\" rapidly approach ; and and rapidly approach 1 + .\n\nSince is irrational, we cannot have \"\" = , i.e.,\nThe best we can achieve is either\n\nThe (non-negative) solutions to are exactly the pairs with \"n\" even, and the solutions to are exactly the pairs with \"n\" odd. To see this, note first that\n\nso that these differences, starting with , are alternately \n1 and −1. Then note that every positive solution comes in this way from a solution with smaller integers since \nThe smaller solution also has positive integers, with the one exception: which comes from \"H\" = 1 and \"P\" = 0.\n\nThe required equation\nis equivalent to:formula_38\nwhich becomes with the substitutions \"H\" = 2\"t\" + 1 and \"P\" = 2\"s\". Hence the \"n\"th solution is\n\nObserve that \"t\" and \"t\" + 1 are relatively prime, so that  = \"s\" happens exactly when they are adjacent integers, one a square \"H\" and the other twice a square 2\"P\". Since we know all solutions of that equation, we also have\n\nand formula_41\n\nThis alternate expression is seen in the next table.\n\nThe equality occurs exactly when which becomes with the substitutions and . Hence the \"n\"th solution is and .\n\nThe table above shows that, in one order or the other, \"a\" and are and while .\n\n\n"}
{"id": "39388567", "url": "https://en.wikipedia.org/wiki?curid=39388567", "title": "Petkovšek's algorithm", "text": "Petkovšek's algorithm\n\nPetkovšek's algorithm (also Hyper) is a computer algebra algorithm that computes a basis of hypergeometric terms solution of its input linear recurrence equation with polynomial coefficients. Equivalently, it computes a first order right factor of linear difference operators with polynomial coefficients. This algorithm was developed by Marko Petkovšek in his PhD-thesis 1992. The algorithm is implemented in all the major computer algebra systems.\n\nLet formula_1 be a field of characteristic zero. A nonzero sequence formula_2 is called hypergeometric if the ratio of two consecutive terms is rational, i.e. formula_3. The Petkovšek algorithm uses as key concept that this rational function has a specific representation, namely the \"Gosper-Petkovšek normal form\". Let formula_4 be a nonzero rational function. Then there exist monic polynomials formula_5 and formula_6 such that\n\nformula_7\n\nand\n\n\nThis representation of formula_12 is called Gosper-Petkovšek normal form. These polynomials can be computed explicitly. This construction of the representation is an essential part of Gosper's algorithm. Petkovšek added the conditions 2. and 3. of this representation which makes this normal form unique.\n\nUsing the Gosper-Petkovšek representation one can transform the original recurrence equation into a recurrence equation for a polynomial sequence formula_13. The other polynomials formula_14 can be taken as the monic factors of the first coefficient polynomial formula_15 resp. the last coefficient polynomial shifted formula_16. Then formula_17 has to fulfill a certain algebraic equation. Taking all the possible finitely many triples formula_18 and computing the corresponding polynomial solution of the transformed recurrence equation formula_13 gives a hypergeometric solution if one exists.\n\nIn the following pseudocode the degree of a polynomial formula_20 is denoted by formula_21 and the coefficient of formula_22 is denoted by formula_23.\n\nIf one does not end if a solution is found it is possible to combine all hypergeometric solutions to get a general hypergeometric solution of the recurrence equation, i.e. a generating set for the kernel of the recurrence equation in the linear span of hypergeometric sequences.\n\nPetkovšek also showed how the inhomogeneous problem can be solved. He considered the case where the right-hand side of the recurrence equation is a sum of hypergeometric sequences. After grouping together certain hypergeometric sequences of the right-hand side, for each of those groups a certain recurrence equation is solved for a rational solution. These rational solutions can be combined to get a particular solution of the inhomogeneous equation. Together with the general solution of the homogeneous problem this gives the general solution of the inhomogeneous problem.\n\nThe number of signed permutation matrices of size formula_41 can be described by the sequence formula_2 which is determined by the recurrence equationformula_43over formula_44. Taking formula_45 as monic divisors of formula_46 respectively, one gets formula_47. For formula_48 the corresponding recurrence equation which is solved in Petkovšek's algorithm isformula_49This recurrence equation has the polynomial solution formula_50 for an arbitrary formula_51. Hence formula_52 and formula_53 is a hypergeometric solution. In fact it is (up to a constant) the only hypergeometric solution and describes the number of signed permutation matrices.\n\nGiven the sum\n\ncoming from Apéry's proof of the irrationality of formula_54, Zeilberger's algorithm computes the linear recurrence\n\nGiven this recurrence, the algorithm does not return any hypergeometric solution, which proves that formula_58 does not simplify to a hypergeometric term.\n"}
{"id": "338129", "url": "https://en.wikipedia.org/wiki?curid=338129", "title": "Plateau's problem", "text": "Plateau's problem\n\nIn mathematics, Plateau's problem is to show the existence of a minimal surface with a given boundary, a problem raised by Joseph-Louis Lagrange in 1760. However, it is named after Joseph Plateau who experimented with soap films. The problem is considered part of the calculus of variations. The existence and regularity problems are part of geometric measure theory.\n\nVarious specialized forms of the problem were solved, but it was only in 1930 that general solutions were found in the context of mappings (immersions) independently by Jesse Douglas and Tibor Radó. Their methods were quite different; Radó's work built on the previous work of René Garnier and held only for rectifiable simple closed curves, whereas Douglas used completely new ideas with his result holding for an arbitrary simple closed curve. Both relied on setting up minimization problems; Douglas minimized the now-named Douglas integral while Radó minimized the \"energy\". Douglas went on to be awarded the Fields Medal in 1936 for his efforts.\n\nThe extension of the problem to higher dimensions (that is, for \"k\"-dimensional surfaces in \"n\"-dimensional space) turns out to be much more difficult to study. Moreover, while the solutions to the original problem are always regular, it turns out that the solutions to the extended problem may have singularities if \"k\" ≤ \"n\" − 2. In the hypersurface case where \"k\" = \"n\" − 1, singularities occur only for \"n\" ≥ 8.\n\nTo solve the extended problem in special cases, the theory of perimeters (De Giorgi) for codimension 1 and the theory of rectifiable currents (Federer and Fleming) for higher codimension have been developed. Multidimensional Plateau problem in the class of spectral surfaces (parametrized by the spectra of the manifolds with a fixed boundary) was solved in 1969 by Anatoly Fomenko.\n\nPhysical soap films are more accurately modeled by the (M,0,delta)-minimal sets of Frederick Almgren, but the lack of a compactness theorem makes it difficult to prove the existence of an area minimizer. In this context, a persistent open question has been the existence of a least-area soap film. Ernst Robert Reifenberg solved such a \"universal Plateau's problem\" for boundaries which are homeomorphic to single embedded spheres. In his book Almgren claimed to use varifolds to solve the problem for more than one sphere, as well as more general boundaries, but Allard's compactness theorem for integral varifolds produces a minimal surface, not necessarily an area minimizer. In 2015 Jenny Harrison and Harrison Pugh used cohomology to define spanning sets and Hausdorff measure weighted by a bounded Lipschitz function to define area, and solved the problem in this setting. Their paper is currently under review.\n\n\n"}
{"id": "3442905", "url": "https://en.wikipedia.org/wiki?curid=3442905", "title": "Polygraphic substitution", "text": "Polygraphic substitution\n\nA polygraphic substitution is a cipher in which a uniform substitution is performed on blocks of letters. When the length of the block is specifically known, more precise terms are used: for instance, a cipher in which pairs of letters are substituted is bigraphic.\n\nAs a concept, polygraphic substitution contrasts with monoalphabetic (or simple) substitutions in which individual letters are uniformly substituted, or polyalphabetic substitutions in which individual letters are substituted in different ways depending on their position in the text. In theory, there is some overlap in these definitions; one could conceivably consider a Vigenère cipher with an eight-letter key to be an octographic substitution. In practice, this is not a useful observation since it is far more fruitful to consider it to be a polyalphabetic substitution cipher.\n\nIn 1563, Giambattista della Porta devised the first bigraphic substitution. However, it was nothing more than a matrix of symbols. In practice, it would have been all but impossible to memorize, and carrying around the table would lead to risks of falling into enemy hands.\n\nIn 1854, Charles Wheatstone came up with the Playfair cipher, a keyword-based system that could be performed on paper in the field. This was followed up over the next fifty years with the closely related four-square and two-square ciphers, which are slightly more cumbersome but offer slightly better security.\n\nIn 1929, Lester S. Hill developed the Hill cipher, which uses matrix algebra to encrypt blocks of any desired length. However, encryption is very difficult to perform by hand for any sufficiently large block size, although it has been implemented by machine or computer. This is therefore on the frontier between classical and modern cryptography.\n\nPolygraphic systems do provide a significant improvement in security over monoalphabetic substitutions. Given an individual letter 'E' in a message, it could be encrypted using any of 52 instructions depending on its location and neighbors, which can be used to great advantage to mask the frequency of individual letters. However, the security boost is limited; while it generally requires a larger sample of text to crack, it can still be done by hand.\n\nOne can identify a polygraphically-encrypted text by performing a frequency chart of polygrams and not merely of individual letters. These can be compared to the frequency of plaintext English. The distribution of digrams is even more stark than individual letters. For example, the six most common letters in English (23%) represent approximately half of English plaintext, but it takes only the most frequent 8% of the 676 digrams to achieve the same potency. In addition, even in a plaintext many thousands of characters long, one would expect that nearly half of the digrams would not occur, or only barely. In addition, looking over the text one would expect to see a fairly regular scattering of repeated text in multiples of the block length and relatively few that are not multiples. \n\nCracking a code identified as polygraphic is similar to cracking a general monoalphabetic substitution except with a larger 'alphabet'. One identifies the most frequent polygrams, experiments with replacing them with common plaintext polygrams, and attempts to build up common words, phrases, and finally meaning. Naturally, if the investigation led the cryptanalyst to suspect that a code was of a specific type, like a Playfair or order-2 Hill cipher, then they could use a more specific attack.\n\n"}
{"id": "1509837", "url": "https://en.wikipedia.org/wiki?curid=1509837", "title": "Reduced form", "text": "Reduced form\n\nIn statistics, and particularly in econometrics, the reduced form of a system of equations is the result of solving the system for the endogenous variables. This gives the latter as functions of the exogenous variables, if any. In econometrics, the equations of a structural form model are estimated in their theoretically given form, while an alternative approach to estimation is to first solve the theoretical equations for the endogenous variables to obtain reduced form equations, and then to estimate the reduced form equations.\n\nLet \"Y\" be the vector of the variables to be explained (endogeneous variables) by a statistical model and \"X\" be the vector of explanatory (exogeneous) variables. In addition let formula_1 be a vector of error terms. Then the general expression of a structural form is formula_2, where \"f\" is a function, possibly from vectors to vectors in the case of a multiple-equation model. The reduced form of this model is given by formula_3, with \"g\" a function.\n\nExogenous variables are variables which are not determined by the system. If we assume that demand is influenced not only by price, but also by an exogenous variable, \"Z\", we can consider the structural supply and demand model\n\nwhere the terms formula_6 are random errors (deviations of the quantities supplied and demanded from those implied by the rest of each equation). By solving for the unknowns (endogenous variables) \"P\" and \"Q\", this structural model can be rewritten in the reduced form:\n\nwhere the parameters formula_9 depend on the parameters formula_10 of the structural model, and where the reduced form errors formula_11 each depend on the structural parameters and on both structural errors. Note that both endogenous variables depend on the exogenous variable \"Z\".\n\nIf the reduced form model is estimated using empirical data, obtaining estimated values for the coefficients formula_12 some of the structural parameters can be recovered: By combining the two reduced form equations to eliminate \"Z\", the structural coefficients of the supply side model (formula_13 and formula_14) can be derived:\n\nNote however, that this still does not allow us to identify the structural parameters of the demand equation. For that, we would need an exogenous variable which is included in the supply equation of the structural model, but not in the demand equation.\n\nLet \"y\" be a column vector of \"M\" endogenous variables. In the case above with \"Q\" and \"P\", we had \"M\" = 2. Let \"z\" be a column vector of \"K\" exogenous variables; in the case above \"z\" consisted only of \"Z\". The structural linear model is\n\nwhere formula_18 is a vector of structural shocks, and \"A\" and \"B\" are matrices; \"A\" is a square \"M\"  × \"M\" matrix, while \"B\" is \"M\" × \"K\". The reduced form of the system is:\n\nwith vector formula_20 of reduced form errors that each depends on all structural errors, where the matrix \"A\" must be nonsingular for the reduced form to exist and be unique. Again, each endogenous variable depends on potentially each exogenous variable.\n\nWithout restrictions on the \"A\" and \"B\", the coefficients of \"A\" and \"B\" cannot be identified from data on \"y\" and \"z\": each row of the structural model is just a linear relation between \"y\" and \"z\" with unknown coefficients. (This is again the parameter identification problem.) The \"M\" reduced form equations (the rows of the matrix equation \"y\" = Π \"z\" above) can be identified from the data because each of them contains only one endogenous variable.\n\n\n"}
{"id": "15501375", "url": "https://en.wikipedia.org/wiki?curid=15501375", "title": "Representative function", "text": "Representative function\n\nIn mathematics, in particular in the field of the representation theory of groups, a representative function is a function \"f\" on a compact topological group \"G\" obtained by composing a representation of \"G\" on a vector space \"V\" with a linear map from the endomorphisms of \"V\" into \"V\" 's underlying field. Representative functions arise naturally from finite-dimensional representations of \"G\" as the matrix-entry functions of the corresponding matrix representations.\n\nIt follows from the Peter–Weyl theorem that the representative functions on \"G\" are dense in the Hilbert space of square-integrable functions on \"G\".\n\n"}
{"id": "567743", "url": "https://en.wikipedia.org/wiki?curid=567743", "title": "Riemann–Liouville integral", "text": "Riemann–Liouville integral\n\nIn mathematics, the Riemann–Liouville integral associates with a real function \"ƒ\" : R → R another function \"I\"\"ƒ\" of the same kind for each value of the parameter α > 0. The integral is a manner of generalization of the repeated antiderivative of \"ƒ\" in the sense that for positive integer values of α, \"I\"\"ƒ\" is an iterated antiderivative of \"ƒ\" of order α. The Riemann–Liouville integral is named for Bernhard Riemann and Joseph Liouville, the latter of whom was the first to consider the possibility of fractional calculus in 1832. The operator agrees with the Euler transform, after Leonhard Euler, when applied to analytic functions. It was generalized to arbitrary dimensions by Marcel Riesz, who introduced the Riesz potential.\n\nThe Riemann–Liouville integral is defined by\nwhere Γ is the Gamma function and \"a\" is an arbitrary but fixed base point. The integral is well-defined provided \"ƒ\" is a locally integrable function, and α is a complex number in the half-plane re(α) > 0. The dependence on the base-point \"a\" is often suppressed, and represents a freedom in constant of integration. Clearly \"I\"ƒ is an antiderivative of ƒ (of first order), and for positive integer values of α, \"I\"ƒ is an antiderivative of order α by Cauchy formula for repeated integration. Another notation, which emphasizes the basepoint, is\n\nThis also makes sense if \"a\" = −∞, with suitable restrictions on \"ƒ\".\n\nThe fundamental relations hold\nthe latter of which is a semigroup property. These properties make possible not only the definition of fractional integration, but also of fractional differentiation, by taking enough derivatives of \"I\"\"ƒ\".\n\nFix a bounded interval (\"a\",\"b\"). The operator \"I\" associates to each integrable function \"ƒ\" on (\"a\",\"b\") the function \"I\"\"ƒ\" on (\"a\",\"b\") which is also integrable by Fubini's theorem. Thus \"I\" defines a linear operator on L(\"a\",\"b\"):\nFubini's theorem also shows that this operator is continuous with respect to the Banach space structure on L, and that the following inequality holds:\nHere formula_6 denotes the norm on L(\"a\",\"b\").\n\nMore generally, by Hölder's inequality, it follows that if \"ƒ\" ∈ L(\"a\",\"b\"), then \"I\"\"ƒ\" ∈ L(\"a\",\"b\") as well, and the analogous inequality holds\n\nwhere formula_8 is the L norm on the interval (\"a\",\"b\"). Thus \"I\" defines a bounded linear operator from \"L\"(\"a\",\"b\") to itself. Furthermore, \"I\"\"ƒ\" tends to \"ƒ\" in the \"L\" sense as α → 0 along the real axis. That is\nfor all \"p\" ≥ 1. Moreover, by estimating the maximal function of \"I\", one can show that the limit \"I\"\"ƒ\" → \"ƒ\" holds pointwise almost everywhere.\n\nThe operator \"I\" is well-defined on the set of locally integrable function on the whole real line R. It defines a bounded transformation on any of the Banach spaces of functions of exponential type \"X\" = \"L\"(\"e\"d\"t\"), consisting of locally integrable functions for which the norm\nis finite. For ƒ in \"X\", the Laplace transform of \"Iƒ\" takes the particularly simple form\nfor re(\"s\") > σ. Here \"F\"(\"s\") denotes the Laplace transform of \"ƒ\", and this property expresses that \"I\" is a Fourier multiplier.\n\nOne can define fractional-order derivatives of \"ƒ\" as well by\nwhere formula_13 denotes the ceiling function. One also obtains a differintegral interpolating between differentiation and integration by defining\n\nAn alternative fractional derivative was introduced by Caputo in 1967, and produces a derivative that has different properties: it produces zero from constant functions and, more importantly, the initial value terms of the Laplace Transform are expressed by means of the values of that function and of its derivative of integer order rather than the derivatives of fractional order as in the Riemann–Liouville derivative. The Caputo fractional derivative with base point formula_15, is then:\n\nAnother representation is:\n\n\n"}
{"id": "2738036", "url": "https://en.wikipedia.org/wiki?curid=2738036", "title": "Serpentine curve", "text": "Serpentine curve\n\nA serpentine curve is a curve whose equation is of the form\n\nEquivalently, it has a parametric representation \n\nor functional representation\n\nSerpentine curves were studied by L'Hôpital and Huygens, and named and classified by Newton.\n\n"}
{"id": "41372", "url": "https://en.wikipedia.org/wiki?curid=41372", "title": "Sprague–Grundy theorem", "text": "Sprague–Grundy theorem\n\nIn combinatorial game theory, the Sprague–Grundy theorem states that every impartial game under the normal play convention is equivalent to a nimber. The Grundy value or nim-value of an impartial game is then defined as the unique nimber that the game is equivalent to. In the case of a game whose positions (or summands of positions) are indexed by the natural numbers (for example the possible heap sizes in nim-like games), the sequence of nimbers for successive heap sizes is called the nim-sequence of the game.\n\nThe theorem and its proof encapsulate the main results of a theory discovered independently by R. P. Sprague (1935) and P. M. Grundy (1939).\n\nFor the purposes of the Sprague–Grundy theorem, a game is a two-player sequential game of perfect information satisfying the \"ending condition\" (all games come to an end: there are no infinite lines of play) and the \"normal play condition\" (a player who cannot move loses).\n\nAt any given point in the game, a player's position is the set of moves he or she is allowed to make. As an example, we can define the \"zero game\" to be the two-player game where neither player has any legal moves. Referring to the two players as formula_1 (for Alice) and formula_2 (for Bob), we would denote their positions as formula_3, since the set of moves each player can make is empty.\n\nAn impartial game is one in which at any given point in the game, each player is allowed exactly the same set of moves. Normal-play nim is an example of an impartial game. In nim, there are one or more heaps of objects, and two players (we'll call them Alice and Bob), take turns choosing a heap and removing 1 or more objects from it. The winner is the player who removes the final object from the final heap. The game is impartial because for any given configuration of pile sizes, the moves Alice can make on her turn are exactly the same moves Bob would be allowed to make if it were his turn. In contrast, a game such as checkers is not impartial because, supposing Alice were playing red and Bob were playing black, for any given arrangement of pieces on the board, if it were Alice's turn, she would only be allowed to move the red pieces, and if it were Bob's turn, he would only be allowed to move the black pieces.\n\nNote that any configuration of an impartial game can therefore be written as a single position, because the moves will be the same no matter whose turn it is. For example, the position of the \"zero game\" can simply be written formula_4, because if it's Alice's turn, she has no moves to make, and if it's Bob's turn, he has no moves to make either.\n\nFurthermore, note that we can associate a move with the position it leaves the next player in. \nDoing so lets us define positions recursively. For example, consider the following game of Nim played by Alice and Bob.\n\nformula_20.\n\nThe special names formula_6, formula_9, and formula_17 referenced in our example game are called nimbers. In general, the nimber formula_24 corresponds to the position in a game of nim where there are exactly formula_25 objects in exactly one heap. \nFormally, nimbers are defined inductively as follows: formula_6 is formula_4, formula_28, formula_29 and for all formula_30, formula_31.\n\nWhile the word \"nim\"ber comes from the game \"nim\", nimbers can be used to describe the positions of any finite, impartial game, and in fact, the Sprague–Grundy theorem states that every instance of a finite, impartial game can be associated with a \"single\" nimber.\n\nTwo games can be combined by adding their positions together.\nFor example, consider another game of nim with heaps formula_32, formula_33, and formula_34.\n\nWe can combine it with our first example to get a combined game with six heaps: formula_1, formula_2, formula_37, formula_32, formula_33, and formula_34:\n\nTo differentiate between the two games, for the first example game, we'll label its starting position formula_41, and color it blue:\n\nformula_42\n\nFor the second example game, we'll label the starting position formula_43 and color it red:\n\nformula_44.\n\nTo compute the starting position of the combined game, remember that a player can either make a move in the first game, leaving the second game untouched, or make a move in the second game, leaving the first game untouched. So the combined game's starting position is:\n\nformula_45\n\nThe explicit formula for adding positions is: formula_46, which means that addition is both commutative and associative.\n\nPositions in impartial games fall into two outcome classes: either the next player (the one whose turn it is) wins (an formula_47- position), or the previous player wins (a formula_48- position). So, for example, formula_6 is a formula_50-position, while formula_9 is an formula_52-position.\n\nTwo positions formula_53 and formula_54 are equivalent if, no matter what position formula_55 is added to them, they are always in the same outcome class. \nFormally,\nformula_56 if and only if formula_57, formula_58 is in the same outcome class as formula_59.\n\nTo use our running examples, notice that in both the first and second games above, we can show that on every turn, Alice has a move that forces Bob into a formula_50-position. Thus, both formula_41 and formula_43 are formula_52-positions. (Notice that in the combined game, \"Bob\" is the player with the formula_52-positions. In fact, formula_65 is a formula_50-position, which as we will see in Lemma 2, means formula_67.)\n\nAs an intermediate step to proving the main theorem, we show that for every position formula_53 and every formula_50-position formula_1, the equivalence formula_71 holds. By the above definition of equivalence, this amounts to showing that formula_72 and formula_73 share an outcome class for all formula_55.\n\nSuppose that formula_72 is a formula_50-position. Then the previous player has a winning strategy for formula_73: respond to moves in formula_1 according to their winning strategy for formula_1 (which exists by virtue of formula_1 being a formula_50-position), and respond to moves in formula_72 according to their winning strategy for formula_72 (which exists for analogous reason). So formula_73 must also be a formula_50-position.\n\nOn the other hand, if formula_72 is an N-position, then the next player has a winning strategy: choose a formula_50-position from among the formula_72 options, putting their opponent in the case above. Thus, in this case, formula_73 must be a formula_52-position, just like formula_72.\n\nAs these are the only two cases, the lemma holds.\n\nAs a further step, we show that formula_92 if and only if formula_93 is a formula_50-position.\n\nIn the forward direction, suppose that formula_92. Applying the definition of equivalence with formula_96, we find that formula_97 (which is equal to formula_93 by commutativity of addition) is in the same outcome class as formula_99. But formula_99 must be a formula_50-position: for every move made in one copy of formula_53, the previous player can respond with the same move in the other copy, and so always make the last move.\n\nIn the reverse direction, since formula_103 is a formula_50-position by hypothesis, it follows from the first lemma, formula_105, that formula_106. Similarly, since formula_107 is also a formula_50-position, it follows from the first lemma in the form formula_109 that formula_110. By associativity and commutativity, the right-hand sides of these results are equal. Furthermore, formula_111 is an equivalence relation because equality is an equivalence relation on outcome classes. Via the transitivity of formula_111, we can conclude that formula_92.\n\nWe prove that all positions are equivalent to a nimber by structural induction. The more specific result, that the given game's initial position must be equivalent to a nimber, shows that the game is itself equivalent to a nimber.\n\nConsider a position formula_114. By the induction hypothesis, all of the options are equivalent to nimbers, say formula_115. So let formula_116. We will show that formula_117, where formula_118 is the mex (minimum exclusion) of the numbers formula_119, that is, the smallest non-negative integer not equal to some formula_120.\n\nThe first thing we need to note is that formula_121, by way of the second lemma. If formula_122 is zero, the claim is trivially true. Otherwise, consider formula_93. If the next player makes a move to formula_124 in formula_53, then the previous player can move to formula_126 in formula_54, and conversely if the next player makes a move in formula_54. After this, the position is a P-position by the lemma's forward implication. Therefore, formula_93 is a P-position, and, citing the lemma's reverse implication, formula_121.\n\nNow let us show that formula_131 is a P-position, which, using the second lemma once again, means that formula_132. We do so by giving an explicit strategy for the previous player.\n\nSuppose that formula_54 and formula_134 are empty. Then formula_131 is the null set, clearly a P-position.\n\nOr consider the case that the next player moves in the component formula_134 to the option formula_137 where formula_138. Because formula_118 was the \"minimum\" excluded number, the previous player can move in formula_54 to formula_137. And, as shown before, any position plus itself is a P-position.\n\nFinally, suppose instead that the next player moves in the component formula_54 to the option formula_126. If formula_144 then the previous player moves in formula_134 to formula_126; otherwise, if formula_147, the previous player moves in formula_126 to formula_134; in either case the result is a position plus itself. (It is not possible that formula_150 because formula_118 was defined to be different from all the formula_120.)\n\nIn summary, we have formula_92 and formula_132. By transitivity, we conclude that formula_117, as desired.\n\nIf formula_53 is a position of an impartial game, the unique integer formula_118 such that formula_117 is called its Grundy value, or Grundy number, and the function which assigns this value to each such position is called the Sprague–Grundy function. R.L.Sprague and P.M.Grundy independently gave an explicit definition of this function, not based on any concept of equivalence to nim positions, and showed that it had the following properties:\nIt follows straightforwardly from these results that if a position formula_53 has a Grundy value of formula_118, then formula_58 has the same Grundy value as formula_166, and therefore belongs to the same outcome class, for any position formula_55. Thus, although Sprague and Grundy never explicitly stated the theorem described in this article, it follows directly from their results and is credited to them.\nThese results have subsequently been developed into the field of combinatorial game theory, notably by Richard Guy, Elwyn Berlekamp, John Horton Conway and others, where they are now encapsulated in the Sprague–Grundy theorem and its proof in the form described here. The field is presented in the books \"Winning Ways for your Mathematical Plays\" and \"On Numbers and Games\".\n\n\n"}
{"id": "249617", "url": "https://en.wikipedia.org/wiki?curid=249617", "title": "System of equations", "text": "System of equations\n\nIn mathematics, a set of simultaneous equations, also known as a system of equations or an equation system, is a finite set of equations for which common solutions are sought. An equation system is usually classified in the same manner as single equations, namely as a:\n\n"}
{"id": "52435", "url": "https://en.wikipedia.org/wiki?curid=52435", "title": "Tait's conjecture", "text": "Tait's conjecture\n\nIn mathematics, Tait's conjecture states that \"Every 3-connected planar cubic graph has a Hamiltonian cycle (along the edges) through all its vertices\". It was proposed by and disproved by , who constructed a counterexample with 25 faces, 69 edges and 46 vertices. Several smaller counterexamples, with 21 faces, 57 edges and 38 vertices, were later proved minimal by .\nThe condition that the graph be 3-regular is necessary due to polyhedra such as the rhombic dodecahedron, which forms a bipartite graph with six degree-four vertices on one side and eight degree-three vertices on the other side; because any Hamiltonian cycle would have to alternate between the two sides of the bipartition, but they have unequal numbers of vertices, the rhombic dodecahedron is not Hamiltonian.\n\nThe conjecture was significant, because if true, it would have implied the four color theorem: as Tait described, the four-color problem is equivalent to the problem of finding 3-edge-colorings of bridgeless cubic planar graphs. In a Hamiltonian cubic planar graph, such an edge coloring is easy to find: use two colors alternately on the cycle, and a third color for all remaining edges. Alternatively, a 4-coloring of the faces of a Hamiltonian cubic planar graph may be constructed directly, using two colors for the faces inside the cycle and two more colors for the faces outside.\n\nThe key to this counter-example is what is now known as Tutte's fragment, shown on the right.\n\nIf this fragment is part of a larger graph, then any Hamiltonian cycle through the graph must go in or out of the top vertex (and either one of the lower ones). It cannot go in one lower vertex and out the other.\n\nThe fragment can then be used to construct the non-Hamiltonian Tutte graph, by putting\ntogether three such fragments as shown on the picture. The \"compulsory\" edges of the fragments, that must be part of any Hamiltonian path through the fragment, are connected at the central vertex; because any cycle can use only two of these three edges, there can be no Hamiltonian cycle.\n\nThe resulting Tutte graph is 3-connected and planar, so by Steinitz' theorem it is the graph of a polyhedron. In total it has 25 faces, 69 edges and 46 vertices.\nIt can be realized geometrically from a tetrahedron (the faces of which correspond to the four large faces in the drawing, three of which are between pairs of fragments and the fourth of which forms the exterior) by multiply truncating three of its vertices.\n\nAs show, there are exactly six 38-vertex non-Hamiltonian polyhedra that have nontrivial three-edge cuts. They are formed by replacing two of the vertices of a pentagonal prism by the same fragment used in Tutte's example.\n\n\n\nPartly based on sci.math posting by Bill Taylor, used by permission.\"\n"}
{"id": "30767413", "url": "https://en.wikipedia.org/wiki?curid=30767413", "title": "Transdichotomous model", "text": "Transdichotomous model\n\nIn computational complexity theory, and more specifically in the analysis of algorithms with integer data, the transdichotomous model is a variation of the random access machine in which the machine word size is assumed to match the problem size. The model was proposed by Michael Fredman and Dan Willard, who chose its name \"because the dichotomy between the machine model and the problem size is crossed in a reasonable manner.\"\n\nIn a problem such as integer sorting in which there are integers to be sorted, the transdichotomous model assumes that each integer may be stored in a single word of computer memory, that operations on single words take constant time per operation, and that the number of bits that can be stored in a single word is at least . The goal of complexity analysis in this model is to find time bounds that depend only on and not on the actual size of the input values or the machine words. In modeling integer computation, it is necessary to assume that machine words are limited in size, because models with unlimited precision are unreasonably powerful (able to solve PSPACE-complete problems in polynomial time). The transdichotomous model makes a minimal assumption of this type: that there is some limit, and that the limit is large enough to allow random access indexing into the input data.\n\nAs well as its application to integer sorting, the transdichotomous model has also been applied to the design of priority queues and to problems in computational geometry and graph algorithms.\n\n"}
{"id": "273642", "url": "https://en.wikipedia.org/wiki?curid=273642", "title": "Ultraproduct", "text": "Ultraproduct\n\nThe ultraproduct is a mathematical construction that appears mainly in abstract algebra and in model theory, a branch of mathematical logic. An ultraproduct is a quotient of the direct product of a family of structures. All factors need to have the same signature. The ultrapower is the special case of this construction in which all factors are equal.\n\nFor example, ultrapowers can be used to construct new fields from given ones. The hyperreal numbers, an ultrapower of the real numbers, are a special case of this.\n\nSome striking applications of ultraproducts include very elegant proofs of the compactness theorem and the completeness theorem, Keisler's ultrapower theorem, which gives an algebraic characterization of the semantic notion of elementary equivalence, and the Robinson-Zakon presentation of the use of superstructures and their monomorphisms to construct nonstandard models of analysis, leading to the growth of the area of non-standard analysis, which was pioneered (as an application of the compactness theorem) by Abraham Robinson.\n\nThe general method for getting ultraproducts uses an index set \"I\", a structure \"M\" for each element \"i\" of \"I\" (all of the same signature), and an ultrafilter \"U\" on \"I\". The usual choice is for \"I\" to be infinite and \"U\" to contain all cofinite subsets of \"I\"; otherwise, the ultrafilter is principal, and the ultraproduct is isomorphic to one of the factors.\n\nAlgebraic operations on the Cartesian product\n\nare defined in the usual way (for example, for a binary function +, (\"a\" + \"b\") = \"a\" + \"b\" ), and an equivalence relation is defined by \"a\" ~ \"b\" if\n\nand the ultraproduct is the quotient set with respect to ~. The ultraproduct is therefore sometimes denoted by\n\nOne may define a finitely additive measure \"m\" on the index set \"I\" by saying \"m\"(\"A\") = 1 if \"A\" ∈ \"U\" and = 0 otherwise. Then two members of the Cartesian product are equivalent precisely if they are equal almost everywhere on the index set. The ultraproduct is the set of equivalence classes thus generated.\n\nOther relations can be extended the same way:\n\nwhere [\"a\"] denotes the equivalence class of \"a\" with respect to ~.\n\nIn particular, if every \"M\" is an ordered field, then so is the ultraproduct.\n\nAn ultrapower is an ultraproduct for which all the factors \"M\" are equal:\n\nMore generally, the construction above can be carried out whenever \"U\" is a filter on \"I\"; the resulting model formula_6 is then called a reduced product.\n\nThe hyperreal numbers are the ultraproduct of one copy of the real numbers for every natural number, with regard to an ultrafilter over the natural numbers containing all cofinite sets. Their order is the extension of the order of the real numbers. For example, the sequence \"ω\" given by \"ω\" = \"i\" defines an equivalence class representing a hyperreal number that is greater than any real number.\n\nAnalogously, one can define nonstandard integers, nonstandard complex numbers, etc., by taking the ultraproduct of copies of the corresponding structures.\n\nAs an example of the carrying over of relations into the ultraproduct, consider the sequence \"ψ\" defined by \"ψ\" = 2\"i\". Because \"ψ\" > \"ω\" = \"i\" for all \"i\", it follows that the equivalence class of \"ψ\" = 2\"i\" is greater than the equivalence class of \"ω\" = \"i\", so that it can be interpreted as an infinite number which is greater than the one originally constructed. However, let \"χ\" = \"i\" for \"i\" not equal to 7, but \"χ\" = 8. The set of indices on which \"ω\" and \"χ\" agree is a member of any ultrafilter (because \"ω\" and \"χ\" agree almost everywhere), so \"ω\" and \"χ\" belong to the same equivalence class.\n\nIn the theory of large cardinals, a standard construction is to take the ultraproduct of the whole set-theoretic universe with respect to some carefully chosen ultrafilter \"U\". Properties of this ultrafilter \"U\" have a strong influence on (higher order) properties of the ultraproduct; for example, if \"U\" is σ-complete, then the ultraproduct will again be well-founded. (See measurable cardinal for the prototypical example.)\n\nŁoś's theorem, also called \"the fundamental theorem of ultraproducts\", is due to Jerzy Łoś (the surname is pronounced , approximately \"wash\"). It states that any first-order formula is true in the ultraproduct if and only if the set of indices \"i\" such that the formula is true in \"M\" is a member of \"U\". More precisely:\n\nLet σ be a signature, formula_7 an ultrafilter over a set formula_8, and for each formula_9 let formula_10 be a σ-structure. Let formula_11 be the ultraproduct of the formula_10 with respect to formula_13, that is, formula_14 Then, for each formula_15, where formula_16, and for every σ-formula formula_17,\n\nThe theorem is proved by induction on the complexity of the formula formula_17. The fact that formula_13 is an ultrafilter (and not just a filter) is used in the negation clause, and the axiom of choice is needed at the existential quantifier step. As an application, one obtains the transfer theorem for hyperreal fields.\n\nLet \"R\" be a unary relation in the structure \"M\", and form the ultrapower of \"M\". Then the set formula_21 has an analog \"S\" in the ultrapower, and first-order formulas involving S are also valid for \"S\". For example, let \"M\" be the reals, and let \"Rx\" hold if \"x\" is a rational number. Then in \"M\" we can say that for any pair of rationals \"x\" and \"y\", there exists another number \"z\" such that \"z\" is not rational, and \"x\" < \"z\" < \"y\". Since this can be translated into a first-order logical formula in the relevant formal language, Łoś's theorem implies that \"S\" has the same property. That is, we can define a notion of the hyperrational numbers, which are a subset of the hyperreals, and they have the same first-order properties as the rationals.\n\nConsider, however, the Archimedean property of the reals, which states that there is no real number \"x\" such that \"x\" > 1, \"x\" > 1 + 1, \"x\" > 1 + 1 + 1, ... for every inequality in the infinite list. Łoś's theorem does not apply to the Archimedean property, because the Archimedean property cannot be stated in first-order logic. In fact, the Archimedean property is false for the hyperreals, as shown by the construction of the hyperreal number \"ω\" above.\n\nIn model theory and set theory, an ultralimit or limiting ultrapower is a direct limit of a sequence of ultrapowers.\n\nBeginning with a structure, \"A\", and an ultrafilter, \"D\", form an ultrapower, \"A\". Then repeat the process to form \"A\", and so forth. For each \"n\" there is a canonical diagonal embedding formula_22. At limit stages, such as \"A\", form the direct limit of earlier stages. One may continue into the transfinite.\n\n"}
{"id": "52296091", "url": "https://en.wikipedia.org/wiki?curid=52296091", "title": "Vladimir Burkov", "text": "Vladimir Burkov\n\nVladimir Nikolaevich Burkov is a Russian control theorist and the author of more than four hundred publications on control problems, game theory, and combinatorial optimization. Laureate of State Prize of USSR, of Prize of Cabinet Council of USSR, he is a Honoured Scholar of the Russian Federation. Vladimir Burkov is a vice-president of Russian Project Management Association (SOVNET) (the Russian branch of International Project Management Association, IPMA), Member of Russian Academy of Natural Sciences. A professor at Moscow Institute of Physics and Technology and Head of Laboratory at V.A. Trapeznikov Institute of Control Sciences of RAS, in the end of 1960s he pioneered the theory of active systems (which was a Soviet version of the theory of mechanism design).\n\nVladimir Burkov was born on November 17, 1939, in the city of Vologda. In 1963 he graduated from the Moscow Institute of Physics and Technology (MIPT) and was employed by Institute of Automation and Remote Control (since 1970s it is known as ICS RAS, V.A. Trapeznikov Institute of Control Sciences of RAS), where he earned his Candidate of Sciences degree in 1966, and became Doctor of Sciences in 1975. In 1981 he earned professorship at the Chair of Control Sciences at MIPT and since 1974 he works as a Head of Laboratory 57 (Laboratory of active systems) at ICS RAS.\n\nMarried to Elena Burkova, the couple has a daughter Irina, who also earned the doctoral degree for her contributions to control theory.\n\nEarly academic interests of Vladimir Burkov were connected with applied problems of combinatorial optimization; in 1960s he contributed to the boom of project scheduling and network planning, proposed novel models of resource allocation in organizations and in technical systems, solved several extremal graph problems. In particular, Vladimir Burkov proposed a lower-bound estimate of the project makespan in resource-constrained project scheduling problem re-invented in 1998 by A. Mingozzi et al. Two books by Vladimir Burkov, \"Network models and control problems\" and \"Applied problems of graph theory\" put forward the problems being intensively studied until now.\n\nSince late 1960s interests of Vladimir Burkov shift to the studies of specific nature of human being as a controlled object (an agent). In 1969 he pursued an idea of the \"fairplay principle\" (in Russian: принцип открытого управления): plans assigned to selfish agents by the optimal control mechanism must be coordinated with agents' goal functions. Under such an incentive-compatible mechanism truthtelling is benefitiary for agents. The notion of incentive compatibility was independently proposed by Leonid Hurwitz, and later was extended and elaborated by Allan Gibbard, Roger Myerson, and many other researchers. They pioneered the revelation principle, which opened a new era in the studies of economic institutions (mechanism design and contract theory); it was mentioned as the main achievement in 2007s Nobel Memorial Prize in Economic Sciences won by L. Hurwitz, E. Maskin, and R. Myerson.\n\nThe fairplay principle became the foundation of the newly introduced \"theory of active systems\" (a version of mechanism design originated from USSR), which systematically studied control mechanisms in man-machine systems. In 1970s the seminal books and articles determined the directions of theory development for many decades to come (some books of early 2010s are ).\n\nIn 1973 V. Burkov headed the newly created division in the Institute of Automation and Remote Control called \"the Sector of Business Games\"; in 1974 it was re-organized into the Laboratory 57 \"Theory and methods of business games\" later renamed to \"Laboratory of Active Systems\". As of the end of 2016 its headcount is 28 employees including 15 Doctors of Sciences and 5 Candidates of Sciences. During the decades V. Burkov superwised dozens of thesis works.\n\nProfessor Dmitry Novikov, corresponding member of Russian Academy of Sciences (since 2008), was elected a director of ICS RAS on October 17, 2016.\n"}
