{"id": "31174000", "url": "https://en.wikipedia.org/wiki?curid=31174000", "title": "Arnold L. Rosenberg", "text": "Arnold L. Rosenberg\n\nArnold Leonard Rosenberg (born February 11, 1941) is an American computer scientist. He is a distinguished university professor emeritus at the University of Massachusetts Amherst, and despite his retirement from UMass he continues to hold research positions at Northeastern University and Colorado State University.\n\nRosenberg is known, among other contributions, for formulating the Aanderaa–Karp–Rosenberg conjecture stating that many nontrivial properties in graph theory cannot be answered without testing for the presence or absence of every possible edge in a given graph.\n\nRosenberg did both his undergraduate and graduate studies at Harvard University, receiving a bachelor's degree in 1962 and a Ph.D. in 1966 under the supervision of Patrick C. Fischer.\nPrior to joining the UMass faculty, Rosenberg worked at the Thomas J. Watson Research Center from 1965 until 1981, and was a faculty member at Duke University from 1981 until 1985. He was elected a fellow of the Association for Computing Machinery in 1996 for his work on \"graph-theoretic models of computation, emphasizing theoretical studies of parallel algorithms and architectures, VLSI design and layout, and data structures\". In 1997, he was elected as a fellow of the IEEE \"for fundamental contributions to theoretical aspects of computer science and engineering\".\n\n"}
{"id": "21523", "url": "https://en.wikipedia.org/wiki?curid=21523", "title": "Artificial neural network", "text": "Artificial neural network\n\nArtificial neural networks (ANN) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains. The neural network itself is not an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the results to identify cats in other images. They do this without any prior knowledge about cats, for example, that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the learning material that they process.\n\nAn ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it.\n\nIn common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called 'edges'. Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.\n\nThe original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\nWarren McCulloch and Walter Pitts (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two approaches. One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.\n\nIn the late 1940s, D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines. Farley and Clark (1954) first used computational machines, then called \"calculators\", to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956). Rosenblatt (1958) created the perceptron, an algorithm for pattern recognition. With mathematical notation, Rosenblatt described circuitry not in the basic perceptron, such as the exclusive-or circuit that could not be processed by neural networks at the time. In 1959, a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells. The first functional networks with many layers were published by Ivakhnenko and Lapa in 1965, becoming the Group Method of Data Handling.\n\nNeural network research stagnated after machine learning research by Minsky and Papert (1969), who discovered two key issues with the computational machines that processed neural networks. The first was that basic perceptrons were incapable of processing the exclusive-or circuit. The second was that computers didn't have enough processing power to effectively handle the work required by large neural networks. Neural network research slowed until computers achieved far greater processing power. Much of artificial intelligence had focused on high-level (symbolic) models that are processed by using algorithms, characterized for example by expert systems with knowledge embodied in \"if-then\" rules, until in the late 1980s research expanded to low-level (sub-symbolic) machine learning, characterized by knowledge embodied in the parameters of a cognitive model.\n\nA key trigger for renewed interest in neural networks and learning was Werbos's (1975) backpropagation algorithm that effectively solved the exclusive-or problem by making the training of multi-layer networks feasible and efficient. Backpropagation distributed the error term back up through the layers, by modifying the weights at each node.\n\nIn the mid-1980s, parallel distributed processing became popular under the name connectionism. Rumelhart and McClelland (1986) described the use of connectionism to simulate neural processes.\n\nSupport vector machines and other, much simpler methods such as linear classifiers gradually overtook neural networks in machine learning popularity. However, using neural networks transformed some domains, such as the prediction of protein structures.\n\nIn 1992, max-pooling was introduced to help with least shift invariance and tolerance to deformation to aid in 3D object recognition.\nIn 2010, Backpropagation training through max-pooling was accelerated by GPUs and shown to perform better than other pooling variants.\n\nThe vanishing gradient problem affects many-layered feedforward networks that used backpropagation and also recurrent neural networks (RNNs). As errors propagate from layer to layer, they shrink exponentially with the number of layers, impeding the tuning of neuron weights that is based on those errors, particularly affecting deep networks.\n\nTo overcome this problem, Schmidhuber adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning and fine-tuned by backpropagation. Behnke (2003) relied only on the sign of the gradient (Rprop) on problems such as image reconstruction and face localization.\n\nHinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. Once sufficiently many layers have been learned, the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an \"ancestral pass\") from the top level feature activations. In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.\n\nEarlier challenges in training deep neural networks were successfully addressed with methods such as unsupervised pre-training, while available computing power increased through the use of GPUs and distributed computing. Neural networks were deployed on a large scale, particularly in image and visual recognition problems. This became known as \"deep learning\".\n\nComputational devices were created in CMOS, for both biophysical simulation and neuromorphic computing. Nanodevices for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally analog rather than digital (even though the first implementations may use digital devices). Ciresan and colleagues (2010) in Schmidhuber's group showed that despite the vanishing gradient problem, GPUs makes back-propagation feasible for many-layered feedforward neural networks.\n\nBetween 2009 and 2012, recurrent neural networks and deep feedforward neural networks developed in Schmidhuber's research group won eight international competitions in pattern recognition and machine learning. For example, the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three languages to be learned.\n\nCiresan and colleagues won pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge and others. Their neural networks were the first pattern recognizers to achieve human-competitive or even superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem.\n\nResearchers demonstrated (2010) that deep neural networks interfaced to a hidden Markov model with context-dependent states that define the neural network output layer can drastically reduce errors in large-vocabulary speech recognition tasks such as voice search.\n\nGPU-based implementations of this approach won many pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, the ISBI 2012 Segmentation of neuronal structures in EM stacks challenge, the ImageNet Competition and others.\n\nDeep, highly nonlinear neural architectures similar to the neocognitron and the \"standard architecture of vision\", inspired by simple and complex cells, were pre-trained by unsupervised methods by Hinton. A team from his lab won a 2012 contest sponsored by Merck to design software to help find molecules that might identify new drugs.\n\n, the state of the art in deep learning feedforward networks alternated between convolutional layers and max-pooling layers, topped by several fully or sparsely connected layers followed by a final classification layer. Learning is usually done without unsupervised pre-training. In the convolutional layer, there are filters that are convolved with the input. Each filter is equivalent to a weights vector that has to be trained. \n\nSuch supervised deep learning methods were the first to achieve human-competitive performance on certain tasks.\n\nArtificial neural networks were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes, only when invariance extended beyond shift, to all ANN-learned concepts, such as location, type (object class label), scale, lighting and others. This was realized in Developmental Networks (DNs) whose embodiments are Where-What Networks, WWN-1 (2008) through WWN-7 (2013).\n\nAn \"artificial neural network\" is a network of simple elements called \"artificial neurons\", which receive input, change their internal state (\"activation\") according to that input, and produce output depending on the input and activation.\n\nAn artificial neuron mimics the working of a biophysical neuron with inputs and outputs, but is not a biological neuron model.\n\nThe \"network\" forms by connecting the output of certain neurons to the input of other neurons forming a directed, weighted graph. The weights as well as the functions that compute the activation can be modified by a process called \"learning\" which is governed by a \"learning rule\".\n\nA neuron with label formula_1 receiving an input formula_2 from predecessor neurons consists of the following components:\n\nOften the output function is simply the Identity function.\n\nAn \"input neuron\" has no predecessor but serves as input interface for the whole network. Similarly an \"output neuron\" has no successor and thus serves as output interface of the whole network.\n\nThe \"network\" consists of connections, each connection transferring the output of a neuron formula_13 to the input of a neuron formula_1. In this sense formula_13 is the predecessor of formula_1 and formula_1 is the successor of formula_13. Each connection is assigned a weight formula_19. Sometimes a bias term added to total weighted sum of inputs to serve as threshold to shift the activation function.\n\nThe \"propagation function\" computes the \"input\" formula_2 to the neuron formula_1 from the outputs formula_22 of predecessor neurons and typically has the form\nWhen a bias value added with the function, the above form changes to following \n\nThe \"learning rule\" is a rule or an algorithm which modifies the parameters of the neural network, in order for a given input to the network to produce a favored output. This \"learning\" process typically amounts to modifying the weights and thresholds of the variables within the network.\n\nNeural network models can be viewed as simple mathematical models defining a function formula_26 or a distribution over formula_27 or both formula_27 and formula_29. Sometimes models are intimately associated with a particular learning rule. A common use of the phrase \"ANN model\" is really the definition of a \"class\" of such functions (where members of the class are obtained by varying parameters, connection weights, or specifics of the architecture such as the number of neurons or their connectivity).\n\nMathematically, a neuron's network function formula_30 is defined as a composition of other functions formula_31, that can further be decomposed into other functions. This can be conveniently represented as a network structure, with arrows depicting the dependencies between functions. A widely used type of composition is the \"nonlinear weighted sum\", where formula_32, where formula_33 (commonly referred to as the activation function) is some predefined function, such as the hyperbolic tangent or sigmoid function or softmax function or rectifier function. The important characteristic of the activation function is that it provides a smooth transition as input values change, i.e. a small change in input produces a small change in output. The following refers to a collection of functions formula_34 as a vector formula_35.\n\nThis figure depicts such a decomposition of formula_36, with dependencies between variables indicated by arrows. These can be interpreted in two ways.\n\nThe first view is the functional view: the input formula_37 is transformed into a 3-dimensional vector formula_38, which is then transformed into a 2-dimensional vector formula_39, which is finally transformed into formula_36. This view is most commonly encountered in the context of optimization.\n\nThe second view is the probabilistic view: the random variable formula_41 depends upon the random variable formula_42, which depends upon formula_43, which depends upon the random variable formula_27. This view is most commonly encountered in the context of graphical models.\n\nThe two views are largely equivalent. In either case, for this particular architecture, the components of individual layers are independent of each other (e.g., the components of formula_39 are independent of each other given their input formula_38). This naturally enables a degree of parallelism in the implementation.\n\nNetworks such as the previous one are commonly called feedforward, because their graph is a directed acyclic graph. Networks with cycles are commonly called recurrent. Such networks are commonly depicted in the manner shown at the top of the figure, where formula_36 is shown as being dependent upon itself. However, an implied temporal dependence is not shown.\n\nThe possibility of learning has attracted the most interest in neural networks. Given a specific \"task\" to solve, and a class of functions formula_48, learning means using a set of observations to find formula_49 which solves the task in some optimal sense.\n\nThis entails defining a cost function formula_50 such that, for the optimal solution formula_51, formula_52 formula_53 i.e., no solution has a cost less than the cost of the optimal solution (see mathematical optimization).\n\nThe cost function formula_54 is an important concept in learning, as it is a measure of how far away a particular solution is from an optimal solution to the problem to be solved. Learning algorithms search through the solution space to find a function that has the smallest possible cost.\n\nFor applications where the solution is data dependent, the cost must necessarily be a function of the observations, otherwise the model would not relate to the data. It is frequently defined as a statistic to which only approximations can be made. As a simple example, consider the problem of finding the model formula_36, which minimizes formula_56, for data pairs formula_57 drawn from some distribution formula_58. In practical situations we would only have formula_59 samples from formula_58 and thus, for the above example, we would only minimize formula_61. Thus, the cost is minimized over a sample of the data rather than the entire distribution.\n\nWhen formula_62 some form of online machine learning must be used, where the cost is reduced as each new example is seen. While online machine learning is often used when formula_58 is fixed, it is most useful in the case where the distribution changes slowly over time. In neural network methods, some form of online machine learning is frequently used for finite datasets.\n\nWhile it is possible to define an ad hoc cost function, frequently a particular cost (function) is used, either because it has desirable properties (such as convexity) or because it arises naturally from a particular formulation of the problem (e.g., in a probabilistic formulation the posterior probability of the model can be used as an inverse cost). Ultimately, the cost function depends on the task.\n\nA DNN can be discriminatively trained with the standard backpropagation algorithm. Backpropagation is a method to calculate the gradient of the loss function (produces the cost associated with a given state) with respect to the weights in an ANN.\n\nThe basics of continuous backpropagation were derived in the context of control theory by Kelley in 1960 and by Bryson in 1961, using principles of dynamic programming. In 1962, Dreyfus published a simpler derivation based only on the chain rule. Bryson and Ho described it as a multi-stage dynamic system optimization method in 1969. In 1970, Linnainmaa finally published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions. This corresponds to the modern version of backpropagation which is efficient even when the networks are sparse. In 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. In 1974, Werbos mentioned the possibility of applying this principle to Artificial neural networks, and in 1982, he applied Linnainmaa's AD method to neural networks in the way that is widely used today. In 1986, Rumelhart, Hinton and Williams noted that this method can generate useful internal representations of incoming data in hidden layers of neural networks. In 1993, Wan was the first to win an international pattern recognition contest through backpropagation.\n\nThe weight updates of backpropagation can be done via stochastic gradient descent using the following equation:\nwhere, formula_65 is the learning rate, formula_66 is the cost (loss) function and formula_67 a stochastic term. The choice of the cost function depends on factors such as the learning type (supervised, unsupervised, reinforcement, etc.) and the activation function. For example, when performing supervised learning on a multiclass classification problem, common choices for the activation function and cost function are the softmax function and cross entropy function, respectively. The softmax function is defined as formula_68 where formula_69 represents the class probability (output of the unit formula_70) and formula_71 and formula_72 represent the total input to units formula_70 and formula_74 of the same level respectively. Cross entropy is defined as formula_75 where formula_76 represents the target probability for output unit formula_70 and formula_69 is the probability output for formula_70 after applying the activation function.\n\nThese can be used to output object bounding boxes in the form of a binary mask. They are also used for multi-scale regression to increase localization precision. DNN-based regression can learn features that capture geometric information in addition to serving as a good classifier. They remove the requirement to explicitly model parts and their relations. This helps to broaden the variety of objects that can be learned. The model consists of multiple layers, each of which has a rectified linear unit as its activation function for non-linear transformation. Some layers are convolutional, while others are fully connected. Every convolutional layer has an additional max pooling. The network is trained to minimize \"L\" error for predicting the mask ranging over the entire training set containing bounding boxes represented as masks.\n\nAlternatives to backpropagation include Extreme Learning Machines, \"No-prop\" networks, training without backtracking, \"weightless\" networks, and non-connectionist neural networks.\n\nThe three major learning paradigms each correspond to a particular learning task. These are supervised learning, unsupervised learning and reinforcement learning.\n\nSupervised learning uses a set of example pairs formula_80 and the aim is to find a function formula_81 in the allowed class of functions that matches the examples. In other words, we wish to infer the mapping implied by the data; the cost function is related to the mismatch between our mapping and the data and it implicitly contains prior knowledge about the problem domain.\n\nA commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output, formula_82, and the target value formula_83 over all the example pairs. Minimizing this cost using gradient descent for the class of neural networks called multilayer perceptrons (MLP), produces the backpropagation algorithm for training neural networks.\n\nTasks that fall within the paradigm of supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). The supervised learning paradigm is also applicable to sequential data (e.g., for hand writing, speech and gesture recognition). This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.\n\nIn unsupervised learning, some data formula_37 is given and the cost function to be minimized, that can be any function of the data formula_37 and the network's output, formula_36.\n\nThe cost function is dependent on the task (the model domain) and any \"a priori\" assumptions (the implicit properties of the model, its parameters and the observed variables).\n\nAs a trivial example, consider the model formula_87 where formula_88 is a constant and the cost formula_89. Minimizing this cost produces a value of formula_88 that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between formula_37 and formula_30, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples those quantities would be maximized rather than minimized).\n\nTasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.\n\nIn reinforcement learning, data formula_37 are usually not given, but generated by an agent's interactions with the environment. At each point in time formula_94, the agent performs an action formula_95 and the environment generates an observation formula_96 and an instantaneous cost formula_97, according to some (usually unknown) dynamics. The aim is to discover a policy for selecting actions that minimizes some measure of a long-term cost, e.g., the expected cumulative cost. The environment's dynamics and the long-term cost for each policy are usually unknown, but can be estimated.\n\nMore formally the environment is modeled as a Markov decision process (MDP) with states formula_98 and actions formula_99 with the following probability distributions: the instantaneous cost distribution formula_100, the observation distribution formula_101 and the transition formula_102, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two then define a Markov chain (MC). The aim is to discover the policy (i.e., the MC) that minimizes the cost.\n\nArtificial neural networks are frequently used in reinforcement learning as part of the overall algorithm. Dynamic programming was coupled with Artificial neural networks (giving neurodynamic programming) by Bertsekas and Tsitsiklis and applied to multi-dimensional nonlinear problems such as those involved in vehicle routing, natural resources management or medicine because of the ability of Artificial neural networks to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of the original control problems.\n\nTasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.\n\nTraining a neural network model essentially means selecting one model from the set of allowed models (or, in a Bayesian framework, determining a distribution over the set of allowed models) that minimizes the cost. Numerous algorithms are available for training neural network models; most of them can be viewed as a straightforward application of optimization theory and statistical estimation.\n\nMost employ some form of gradient descent, using backpropagation to compute the actual gradients. This is done by simply taking the derivative of the cost function with respect to the network parameters and then changing those parameters in a gradient-related direction. Backpropagation training algorithms fall into three categories:\n\n\nEvolutionary methods, gene expression programming, simulated annealing, expectation-maximization, non-parametric methods and particle swarm optimization are other methods for training neural networks.\n\nThis is a learning method specially designed for cerebellar model articulation controller (CMAC) neural networks. In 2004, a recursive least squares algorithm was introduced to train CMAC neural network online. This algorithm can converge in one step and update all weights in one step with any new input data. Initially, this algorithm had computational complexity of \"O\"(\"N\"). Based on QR decomposition, this recursive learning algorithm was simplified to be \"O\"(\"N\").\n\nThe optimization algorithm repeats a two phase cycle, propagation and weight update. When an input vector is presented to the network, it is propagated forward through the network, layer by layer, until it reaches the output layer. The output of the network is then compared to the desired output, using a loss function. The resulting error value is calculated for each of the neurons in the output layer. The error values are then propagated from the output back through the network, until each neuron has an associated error value that reflects its contribution to the original output.\n\nBackpropagation uses these error values to calculate the gradient of the loss function. In the second phase, this gradient is fed to the optimization method, which in turn uses it to update the weights, in an attempt to minimize the loss function.\n\nLet formula_103 be a neural network with formula_104 connections, formula_105 inputs, and formula_106 outputs.\n\nBelow, formula_107 will denote vectors in formula_108, formula_109 vectors in formula_110, and formula_111 vectors in formula_112. \nThese are called \"inputs\", \"outputs\" and \"weights\" respectively.\n\nThe neural network corresponds to a function formula_113 which, given a weight formula_114, maps an input formula_115 to an output formula_116.\n\nThe optimization takes as input a sequence of \"training examples\" formula_117 and produces a sequence of weights formula_118 starting from some initial weight formula_119, usually chosen at random.\n\nThese weights are computed in turn: first compute formula_120 using only formula_121 for formula_122. The output of the algorithm is then formula_123, giving us a new function formula_124. The computation is the same in each step, hence only the case formula_125 is described.\n\nCalculating formula_126 from formula_127 is done by considering a variable weight formula_114 and applying gradient descent to the function formula_129 to find a local minimum, \nstarting at formula_130.\n\nThis makes formula_126 the minimizing weight found by gradient descent.\n\nTo implement the algorithm above, explicit formulas are required for the gradient of the function formula_132 where the function is formula_133.\n\nThe learning algorithm can be divided into two phases: propagation and weight update.\n\nEach propagation involves the following steps:\n\n\nFor each weight, the following steps must be followed:\n\nThis ratio (percentage) influences the speed and quality of learning; it is called the \"learning rate\". The greater the ratio, the faster the neuron trains, but the lower the ratio, the more accurate the training is. The sign of the gradient of a weight indicates whether the error varies directly with, or inversely to, the weight. Therefore, the weight must be updated in the opposite direction, \"descending\" the gradient.\n\nLearning is repeated (on new batches) until the network performs adequately.\n\nThe following is pseudocode for a stochastic gradient descent algorithm for training a three-layer network (only one hidden layer):\n\nThe lines labeled \"backward pass\" can be implemented using the backpropagation algorithm, which calculates the gradient of the error of the network regarding the network's modifiable weights.\n\nThe choice of learning rate formula_134 is important, since a high value can cause too strong a change, causing the minimum to be missed, while a too low learning rate slows the training unnecessarily.\n\nOptimizations such as Quickprop are primarily aimed at speeding up error minimization; other improvements mainly try to increase reliability.\n\nIn order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements of this algorithm use an adaptive learning rate.\n\nBy using a variable inertia term \"(Momentum)\" formula_135 the gradient and the last change can be weighted such that the weight adjustment additionally depends on the previous change. If the \"Momentum\" formula_135 is equal to 0, the change depends solely on the gradient, while a value of 1 will only depend on the last change.\n\nSimilar to a ball rolling down a mountain, whose current speed is determined not only by the current slope of the mountain but also by its own inertia, inertia can be added:formula_137where:\n\nInertia makes the current weight change formula_153 depend both on the current gradient of the error function (slope of the mountain, 1st summand), as well as on the weight change from the previous point in time (inertia, 2nd summand).\n\nWith inertia, the problems of getting stuck (in steep ravines and flat plateaus) are avoided. Since, for example, the gradient of the error function becomes very small in flat plateaus, a plateau would immediately lead to a \"deceleration\" of the gradient descent. This deceleration is delayed by the addition of the inertia term so that a flat plateau can be escaped more quickly. \n\nTwo modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces \"noise\" into the gradient descent process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the average error of the batch. A common compromise choice is to use \"mini-batches\", meaning small batches and with samples in each batch selected stochastically from the entire data set.\n\nThe Group Method of Data Handling (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov-Gabor polynomials that permit additions and multiplications. It used a deep feedforward multilayer perceptron with eight layers. It is a supervised learning network that grows layer by layer, where each layer is trained by regression analysis. Useless items are detected using a validation set, and pruned through regularization. The size and depth of the resulting network depends on the task.\n\nA convolutional neural network (CNN) is a class of deep, feed-forward networks, composed of one or more convolutional layers with fully connected layers (matching those in typical Artificial neural networks) on top. It uses tied weights and pooling layers. In particular, max-pooling is often structured via Fukushima's convolutional architecture. This architecture allows CNNs to take advantage of the 2D structure of input data.\n\nCNNs are suitable for processing visual and other two-dimensional data. They have shown superior results in both image and speech applications. They can be trained with standard backpropagation. CNNs are easier to train than other regular, deep, feed-forward neural networks and have many fewer parameters to estimate. Examples of applications in computer vision include DeepDream and robot navigation.\n\nA recent development has been that of Capsule Neural Network (CapsNet), the idea behind which is to add structures called capsules to a CNN and to reuse output from several of those capsules to form more stable (with respect to various perturbations) representations for higher order capsules.\n\nLong short-term memory (LSTM) networks are RNNs that avoid the vanishing gradient problem. LSTM is normally augmented by recurrent gates called forget gates. LSTM networks prevent backpropagated errors from vanishing or exploding. Instead errors can flow backwards through unlimited numbers of virtual layers in space-unfolded LSTM. That is, LSTM can learn \"very deep learning\" tasks that require memories of events that happened thousands or even millions of discrete time steps ago. Problem-specific LSTM-like topologies can be evolved. LSTM can handle long delays and signals that have a mix of low and high frequency components.\n\nStacks of LSTM RNNs trained by Connectionist Temporal Classification (CTC) can find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.\n\nIn 2003, LSTM started to become competitive with traditional speech recognizers. In 2007, the combination with CTC achieved first good results on speech data. In 2009, a CTC-trained LSTM was the first RNN to win pattern recognition contests, when it won several competitions in connected handwriting recognition. In 2014, Baidu used CTC-trained RNNs to break the Switchboard Hub5'00 speech recognition benchmark, without traditional speech processing methods. LSTM also improved large-vocabulary speech recognition, text-to-speech synthesis, for Google Android, and photo-real talking heads. In 2015, Google's speech recognition experienced a 49% improvement through CTC-trained LSTM.\n\nLSTM became popular in Natural Language Processing. Unlike previous models based on HMMs and similar concepts, LSTM can learn to recognise context-sensitive languages. LSTM improved machine translation, language modeling and multilingual language processing. LSTM combined with CNNs improved automatic image captioning.\n\nDeep Reservoir Computing and Deep Echo State Networks (deepESNs) provide a framework for efficiently trained models for hierarchical processing of temporal data, while enabling the investigation of the inherent role of RNN layered composition.\n\nA deep belief network (DBN) is a probabilistic, generative model made up of multiple layers of hidden units. It can be considered a composition of simple learning modules that make up each layer.\n\nA DBN can be used to generatively pre-train a DNN by using the learned DBN weights as the initial DNN weights. Backpropagation or other discriminative algorithms can then tune these weights. This is particularly helpful when training data are limited, because poorly initialized weights can significantly hinder model performance. These pre-trained weights are in a region of the weight space that is closer to the optimal weights than were they randomly chosen. This allows for both improved modeling and faster convergence of the fine-tuning phase.\n\nLarge memory storage and retrieval neural networks (LAMSTAR) are fast deep learning neural networks of many layers that can use many filters simultaneously. These filters may be nonlinear, stochastic, logic, non-stationary, or even non-analytical. They are biologically motivated and learn continuously.\n\nA LAMSTAR neural network may serve as a dynamic neural network in spatial or time domains or both. Its speed is provided by Hebbian link-weights that integrate the various and usually different filters (preprocessing functions) into its many layers and to dynamically rank the significance of the various layers and functions relative to a given learning task. This grossly imitates biological learning which integrates various preprocessors (cochlea, retina, \"etc.\") and cortexes (auditory, visual, \"etc.\") and their various regions. Its deep learning capability is further enhanced by using inhibition, correlation and its ability to cope with incomplete data, or \"lost\" neurons or layers even amidst a task. It is fully transparent due to its link weights. The link-weights allow dynamic determination of innovation and redundancy, and facilitate the ranking of layers, of filters or of individual neurons relative to a task.\n\nLAMSTAR has been applied to many domains, including medical and financial predictions, adaptive filtering of noisy speech in unknown noise, still-image recognition, video image recognition, software security and adaptive control of non-linear systems. LAMSTAR had a much faster learning speed and somewhat lower error rate than a CNN based on ReLU-function filters and max pooling, in 20 comparative studies.\n\nThese applications demonstrate delving into aspects of the data that are hidden from shallow learning networks and the human senses, such as in the cases of predicting onset of sleep apnea events, of an electrocardiogram of a fetus as recorded from skin-surface electrodes placed on the mother's abdomen early in pregnancy, of financial prediction or in blind filtering of noisy speech.\n\nLAMSTAR was proposed in 1996 () and was further developed Graupe and Kordylewski from 1997–2002. A modified version, known as LAMSTAR 2, was developed by Schneider and Graupe in 2008.\n\nThe auto encoder idea is motivated by the concept of a \"good\" representation. For example, for a classifier, a good representation can be defined as one that yields a better-performing classifier.\n\nAn \"encoder\" is a deterministic mapping formula_154 that transforms an input vector x into hidden representation y, where formula_155, formula_156 is the weight matrix and b is an offset vector (bias). A \"decoder\" maps back the hidden representation y to the reconstructed input z via formula_157. The whole process of auto encoding is to compare this reconstructed input to the original and try to minimize the error to make the reconstructed value as close as possible to the original.\n\nIn \"stacked denoising auto encoders\", the partially corrupted output is cleaned (de-noised). This idea was introduced in 2010 by Vincent et al. with a specific approach to \"good\" representation, a \"good representation\" is one that can be obtained robustly from a corrupted input and that will be useful for recovering the corresponding clean input\".\" Implicit in this definition are the following ideas:\nThe algorithm starts by a stochastic mapping of formula_158 to formula_159 through formula_160, this is the corrupting step. Then the corrupted input formula_159 passes through a basic auto-encoder process and is mapped to a hidden representation formula_162. From this hidden representation, we can reconstruct formula_163. In the last stage, a minimization algorithm runs in order to have z as close as possible to uncorrupted input formula_158. The reconstruction error formula_165 might be either the cross-entropy loss with an affine-sigmoid decoder, or the squared error loss with an affine decoder.\n\nIn order to make a deep architecture, auto encoders stack. Once the encoding function formula_154 of the first denoising auto encoder is learned and used to uncorrupt the input (corrupted input), the second level can be trained.\n\nOnce the stacked auto encoder is trained, its output can be used as the input to a supervised learning algorithm such as support vector machine classifier or a multi-class logistic regression.\n\nA deep stacking network (DSN) (deep convex network) is based on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Dong. It formulates the learning as a convex optimization problem with a closed-form solution, emphasizing the mechanism's similarity to stacked generalization. Each DSN block is a simple module that is easy to train by itself in a supervised fashion without backpropagation for the entire blocks.\n\nEach block consists of a simplified multi-layer perceptron (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units, and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T, and the input data vectors x form the columns of matrix X. The matrix of hidden units is formula_167. Modules are trained in order, so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class \"y\", and its estimate is concatenated with original input X to form the expanded input for the next block. Thus, the input to the first block contains the original data only, while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem:\nwhich has a closed-form solution.\n\nUnlike other deep architectures, such as DBNs, the goal is not to discover the transformed feature representation. The structure of the hierarchy of this kind of architecture makes parallel learning straightforward, as a batch-mode optimization problem. In purely discriminative tasks, DSNs perform better than conventional DBNs.\n\nThis architecture is a DSN extension. It offers two important improvements: it uses higher-order information from covariance statistics, and it transforms the non-convex problem of a lower-layer to a convex sub-problem of an upper-layer. TDSNs use covariance statistics in a bilinear mapping from each of two distinct sets of hidden units in the same layer to predictions, via a third-order tensor.\n\nWhile parallelization and scalability are not considered seriously in conventional , all learning for s and s is done in batch mode, to allow parallelization. Parallelization allows scaling the design to larger (deeper) architectures and data sets.\n\nThe basic architecture is suitable for diverse tasks such as classification and regression.\n\nThe need for deep learning with real-valued inputs, as in Gaussian restricted Boltzmann machines, led to the \"spike-and-slab\" RBM (\"ss\"RBM), which models continuous-valued inputs with strictly binary latent variables. Similar to basic RBMs and its variants, a spike-and-slab RBM is a bipartite graph, while like GRBMs, the visible units (input) are real-valued. The difference is in the hidden layer, where each hidden unit has a binary spike variable and a real-valued slab variable. A spike is a discrete probability mass at zero, while a slab is a density over continuous domain; their mixture forms a prior.\n\nAn extension of ssRBM called µ-ssRBM provides extra modeling capacity using additional terms in the energy function. One of these terms enables the model to form a conditional distribution of the spike variables by marginalizing out the slab variables given an observation.\n\nCompound hierarchical-deep models compose deep networks with non-parametric Bayesian models. Features can be learned using deep architectures such as DBNs, DBMs, deep auto encoders, convolutional variants, ssRBMs, deep coding networks, DBNs with sparse feature learning, RNNs, conditional DBNs, de-noising auto encoders. This provides a better representation, allowing faster learning and more accurate classification with high-dimensional data. However, these architectures are poor at learning novel classes with few examples, because all network units are involved in representing the input (a ) and must be adjusted together (high degree of freedom). Limiting the degree of freedom reduces the number of parameters to learn, facilitating learning of new classes from few examples. \"Hierarchical Bayesian (HB)\" models allow learning from few examples, for example for computer vision, statistics and cognitive science.\n\nCompound HD architectures aim to integrate characteristics of both HB and deep networks. The compound HDP-DBM architecture is a \"hierarchical Dirichlet process (HDP)\" as a hierarchical model, incorporated with DBM architecture. It is a full generative model, generalized from abstract concepts flowing through the layers of the model, which is able to synthesize new examples in novel classes that look \"reasonably\" natural. All the levels are learned jointly by maximizing a joint log-probability score.\n\nIn a DBM with three hidden layers, the probability of a visible input is:\nwhere formula_170 is the set of hidden units, and formula_171 are the model parameters, representing visible-hidden and hidden-hidden symmetric interaction terms.\n\nA learned DBM model is an undirected model that defines the joint distribution formula_172. One way to express what has been learned is the conditional model formula_173 and a prior term formula_174.\n\nHere formula_173 represents a conditional DBM model, which can be viewed as a two-layer DBM but with bias terms given by the states of formula_176:\n\nA deep predictive coding network (DPCN) is a predictive coding scheme that uses top-down information to empirically adjust the priors needed for a bottom-up inference procedure by means of a deep, locally connected, generative model. This works by extracting sparse features from time-varying observations using a linear dynamical model. Then, a pooling strategy is used to learn invariant feature representations. These units compose to form a deep architecture and are trained by greedy layer-wise unsupervised learning. The layers constitute a kind of Markov chain such that the states at any layer depend only on the preceding and succeeding layers.\n\nDPCNs predict the representation of the layer, by using a top-down approach using the information in upper layer and temporal dependencies from previous states.\n\nDPCNs can be extended to form a convolutional network.\n\nIntegrating external memory with Artificial neural networks dates to early research in distributed representations and Kohonen's self-organizing maps. For example, in sparse distributed memory or hierarchical temporal memory, the patterns encoded by neural networks are used as addresses for content-addressable memory, with \"neurons\" essentially serving as address encoders and decoders. However, the early controllers of such memories were not differentiable.\n\nApart from long short-term memory (LSTM), other approaches also added differentiable memory to recurrent functions. For example:\n\nNeural Turing machines couple LSTM networks to external memory resources, with which they can interact by attentional processes. The combined system is analogous to a Turing machine but is differentiable end-to-end, allowing it to be efficiently trained by gradient descent. Preliminary results demonstrate that neural Turing machines can infer simple algorithms such as copying, sorting and associative recall from input and output examples.\n\nDifferentiable neural computers (DNC) are an NTM extension. They out-performed Neural turing machines, long short-term memory systems and memory networks on sequence-processing tasks.\n\nApproaches that represent previous experiences directly and use a similar experience to form a local model are often called nearest neighbour or k-nearest neighbors methods. Deep learning is useful in semantic hashing where a deep graphical model the word-count vectors obtained from a large set of documents. Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by accessing all the addresses that differ by only a few bits from the address of the query document. Unlike sparse distributed memory that operates on 1000-bit addresses, semantic hashing works on 32 or 64-bit addresses found in a conventional computer architecture.\n\nMemory networks are another extension to neural networks incorporating long-term memory. The long-term memory can be read and written to, with the goal of using it for prediction. These models have been applied in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base and the output is a textual response. A team of electrical and computer engineers from UCLA Samueli School of Engineering has created a physical artificial neural network. That can analyze large volumes of data and identify objects at the actual speed of light.\n\nDeep neural networks can be potentially improved by deepening and parameter reduction, while maintaining trainability. While training extremely deep (e.g., 1 million layers) neural networks might not be practical, CPU-like architectures such as pointer networks and neural random-access machines overcome this limitation by using external random-access memory and other components that typically belong to a computer architecture such as registers, ALU and pointers. Such systems operate on probability distribution vectors stored in memory cells and registers. Thus, the model is fully differentiable and trains end-to-end. The key characteristic of these models is that their depth, the size of their short-term memory, and the number of parameters can be altered independently – unlike models like LSTM, whose number of parameters grows quadratically with memory size.\n\nEncoder–decoder frameworks are based on neural networks that map highly structured input to highly structured output. The approach arose in the context of machine translation, where the input and output are written sentences in two natural languages. In that work, an LSTM RNN or CNN was used as an encoder to summarize a source sentence, and the summary was decoded using a conditional RNN language model to produce the translation. These systems share building blocks: gated RNNs and CNNs and trained attention mechanisms.\n\nMultilayer kernel machines (MKM) are a way of learning highly nonlinear functions by iterative application of weakly nonlinear kernels. They use the kernel principal component analysis (KPCA), as a method for the unsupervised greedy layer-wise pre-training step of deep learning.\n\nLayer formula_178 learns the representation of the previous layer formula_179, extracting the formula_180 principal component (PC) of the projection layer formula_179 output in the feature domain induced by the kernel. For the sake of dimensionality reduction of the updated representation in each layer, a supervised strategy selects the best informative features among features extracted by KPCA. The process is:\nSome drawbacks accompany the KPCA method as the building cells of an MKM.\n\nA more straightforward way to use kernel machines for deep learning was developed for spoken language understanding. The main idea is to use a kernel machine to approximate a shallow neural net with an infinite number of hidden units, then use stacking to splice the output of the kernel machine and the raw input in building the next, higher level of the kernel machine. The number of levels in the deep convex network is a hyper-parameter of the overall system, to be determined by cross validation.\n\nNeural architecture search (NAS) uses machine learning to automate the design of Artificial neural networks. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset and use the results as feedback to teach the NAS network.\n\nUsing Artificial neural networks requires an understanding of their characteristics.\nANN capabilities fall within the following broad categories:\n\n\nBecause of their ability to reproduce and model nonlinear processes, Artificial neural networks have found many applications in a wide range of disciplines.\n\nApplication areas include system identification and control (vehicle control, trajectory prediction, process control, natural resource management), quantum chemistry, game-playing and decision making (backgammon, chess, poker), pattern recognition (radar systems, face identification, signal classification, object recognition and more), sequence recognition (gesture, speech, handwritten and printed text recognition), medical diagnosis, finance (e.g. automated trading systems), data mining, visualization, machine translation, social network filtering and e-mail spam filtering.\n\nArtificial neural networks have been used to diagnose cancers, including lung cancer, prostate cancer, colorectal cancer and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information. Artificial neural networks have also found use accelerating reliability analysis of infrastructures subject to natural disasters and predicting foundation settlements . Other applications include building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. Furthermore, artificial neural networks have found widespread use in the field of big data.\n\nMany types of models are used, defined at different levels of abstraction and modeling different aspects of neural systems. They range from models of the short-term behavior of individual neurons, models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level.\n\nThe multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.\n\nA specific recurrent architecture with rational valued weights (as opposed to full precision real number-valued weights) has the full power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.\n\nModels' \"capacity\" property roughly corresponds to their ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.\n\nModels may not consistently converge on a single solution, firstly because many local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical. However, for CMAC neural network, a recursive least squares algorithm was introduced to train it, and this algorithm can be guaranteed to converge in one step.\n\nApplications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the capacity of the network significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and optimally select hyperparameters to minimize the generalization error. The second is to use some form of \"regularization\". This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.\nSupervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of the output of the network, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.\n\nBy assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based neural network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is very useful in classification as it gives a certainty measure on classifications.\n\nThe softmax activation function is:\n\n<section end=\"theory\" />\n\nA common criticism of neural networks, particularly in robotics, is that they require too much training for real-world operation. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example and by grouping examples in so-called mini-batches. Improving the training efficiency and convergence capability has always been an ongoing research area for neural network. For example, by introducing a recursive least squares algorithm for CMAC neural network, the training process only takes one step to converge.\n\nNo neural network has solved computationally difficult problems such as the n-Queens problem, the travelling salesman problem, or the problem of factoring large integers.\n\nA fundamental objection is that they do not reflect how real neurons function. Back propagation is a critical part of most artificial neural networks, although no such mechanism exists in biological neural networks. How information is coded by real neurons is not known. Sensor neurons fire action potentials more frequently with sensor activation and muscle cells pull more strongly when their associated motor neurons receive action potentials more frequently. Other than the case of relaying information from a sensor neuron to a motor neuron, almost nothing of the principles of how information is handled by biological neural networks is known. This is a subject of active research in Neural coding.\n\nThe motivation behind Artificial neural networks is not necessarily to strictly replicate neural function, but to use biological neural networks as an inspiration. A central claim of artificial neural networks is therefore that it embodies some new and powerful general principle for processing information. Unfortunately, these general principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. Alexander Dewdney commented that, as a result, artificial neural networks have a \"something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything\".\n\nBiological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.\n\nLarge and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may compel a neural network designer to fill many millions of database rows for its connections which can consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons which must often be matched with enormous CPU processing power and time.\n\nSchmidhuber notes that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.\n\nNeuromorphic engineering addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.\n\nArguments against Dewdney's position are that neural networks have been successfully used to solve many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.\n\nTechnology writer Roger Bridgman commented:\n\nNeural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".\nIn spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.\nAlthough it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs non-local learning and shallow vs deep architecture.\n\nAdvocates of hybrid models (combining neural networks and symbolic approaches), claim that such a mixture can better capture the mechanisms of the human mind.\n\nArtificial neural networks have many variations. The simplest, static types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to change during the learning process. The latter are much more complicated, but can shorten learning periods and produce better results. Some types allow/require learning to be \"supervised\" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.\n\n"}
{"id": "31890842", "url": "https://en.wikipedia.org/wiki?curid=31890842", "title": "Bost–Connes system", "text": "Bost–Connes system\n\nIn mathematics, a Bost–Connes system is a quantum statistical dynamical system related to an algebraic number field, whose partition function is related to the Dedekind zeta function of the number field. introduced Bost–Connes systems by constructing one for the rational numbers. extended the construction to imaginary quadratic fields.\n\nSuch systems have been studied for their connection with Hilbert's Twelfth Problem. In the case of a Bost–Connes system over Q, the absolute Galois group acts on the ground states of the system.\n\n"}
{"id": "149217", "url": "https://en.wikipedia.org/wiki?curid=149217", "title": "Bézout's theorem", "text": "Bézout's theorem\n\nBézout's theorem is a statement in algebraic geometry concerning the number of common points, or intersection points, of two plane algebraic curves which do not share a common component (that is, which do not have infinitely many common points). The theorem states that the number of common points of two such curves is at most equal to the product of their degrees, and equality holds if one counts points at infinity and points with complex coordinates (or more generally, coordinates from the algebraic closure of the ground field), and if each point is counted with its intersection multiplicity. It is named after Étienne Bézout.\n\nBézout's theorem refers also to the generalization to higher dimensions: Let there be \"n\" homogeneous polynomials in variables, of degrees formula_1, that define \"n\" hypersurfaces in the projective space of dimension \"n\". If the number of intersection points of the hypersurfaces is finite over an algebraic closure of the ground field, then this number is formula_2 if the points are counted with their multiplicity.\nAs in the case of two variables, in the case of affine hypersurfaces, and when not counting multiplicities nor non-real points, this theorem provides only an upper bound of the number of points, which is often reached. This is often referred to as Bézout's bound.\n\nBézout's theorem is fundamental in computer algebra and effective algebraic geometry, by showing that most problems have a computational complexity that is at least exponential in the number of variables. It follows that in these areas, the best complexity that may be hoped for will occur in algorithms that have a complexity which is polynomial in Bézout's bound.\n\nSuppose that \"X\" and \"Y\" are two plane projective curves defined over a field \"F\" that do not have a common component (this condition means that \"X\" and \"Y\" are defined by polynomials, whose polynomial greatest common divisor is a constant; in particular, it holds for a pair of \"generic\" curves). Then the total number of intersection points of \"X\" and \"Y\" with coordinates in an algebraically closed field \"E\" which contains \"F\", counted with their multiplicities, is equal to the product of the degrees of \"X\" and \"Y\".\n\nThe generalization in higher dimension may be stated as:\n\nLet \"n\" projective hypersurfaces be given in a projective space of dimension \"n\" over an algebraic closed field, which are defined by \"n\" homogeneous polynomials in \"n\" + 1 variables, of degrees formula_3 Then either the number of intersection points is infinite, or the number of intersection points, counted with multiplicity, is equal to the product formula_4 If the hypersurfaces are irreducible and in relative general position, then there are formula_5 intersection points, all with multiplicity 1.\n\nThere are various proofs of this theorem. In particular, it may be deduced by applying iteratively the following generalization: if \"V\" is a projective algebraic set of dimension formula_6 and degree formula_7, and \"H\" is a hypersurface (defined by a polynomial) of degree formula_8, that does not contain any irreducible component of \"V\", then the intersection of \"V\" and \"H\" has dimension formula_9 and degree formula_10 For a (sketched) proof using the Hilbert series see Hilbert series and Hilbert polynomial#Degree of a projective variety and Bézout's theorem.\n\nBézout's theorem has been further generalized as the so-called multi-homogeneous Bézout theorem.\n\nBezout's theorem was essentially stated by Isaac Newton in his proof of lemma 28 of volume 1 of his \"Principia\" in 1687, where he claims that two curves have a number of intersection points given by the product of their degrees. The theorem was later published in 1779 in Étienne Bézout's \"Théorie générale des équations algébriques\". Bézout, who did not have at his disposal modern algebraic notation for equations in several variables, gave a proof based on manipulations with cumbersome algebraic expressions. From the modern point of view, Bézout's treatment was rather heuristic, since he did not formulate the precise conditions for the theorem to hold. This led to a sentiment, expressed by certain authors, that his proof was neither correct nor the first proof to be given.\n\nThe most delicate part of Bézout's theorem and its generalization to the case of \"k\" algebraic hypersurfaces in \"k\"-dimensional projective space is the procedure of assigning the proper intersection multiplicities. If \"P\" is a common point of two plane algebraic curves \"X\" and \"Y\" that is a non-singular point of both of them and, moreover, the tangent lines to \"X\" and \"Y\" at \"P\" are distinct then the intersection multiplicity is one. This corresponds to the case of \"transversal intersection\". If the curves \"X\" and \"Y\" have a common tangent at \"P\" then the multiplicity is at least two. See intersection number for the definition in general.\n\n\nWrite the equations for \"X\" and \"Y\" in homogeneous coordinates as\nwhere \"a\" and \"b\" are homogeneous polynomials of degree \"i\" in \"x\" and \"y\". The points of intersection of \"X\" and \"Y\" correspond to the solutions of the system of equations. Form the Sylvester matrix; in the case \"m\"=4, \"n\"=3 this is\n\nThe determinant |\"S\"| of \"S\", which is also called the resultant of the two polynomials, is 0 exactly when the two equations have a common solution in \"z\". The terms of |\"S\"|, for example (a)(b), all have degree \"mn\", so |\"S\"| is a homogeneous polynomial of degree \"mn\" in \"x\" and \"y\" (recall that \"a\" and \"b\" are themselves polynomials). By the fundamental theorem of algebra, this can be factored into \"mn\" linear factors so there are \"mn\" solutions to the system of equations. The linear factors correspond to the lines that join the origin to the points of intersection of the curves.\n\n\n\n"}
{"id": "316648", "url": "https://en.wikipedia.org/wiki?curid=316648", "title": "Circular error probable", "text": "Circular error probable\n\nIn the military science of ballistics, circular error probable (CEP) (also circular error probability or circle of equal probability ) is a measure of a weapon system's precision. It is defined as the radius of a circle, centered on the mean, whose boundary is expected to include the landing points of 50% of the rounds; said otherwise, it is the median error radius. That is, if a given bomb design has a CEP of , when 100 are targeted at the same point, 50 will fall within a 100 m circle around their average impact point. (The distance between the target point and the average impact point is referred to as bias.)\n\nThere are associated concepts, such as the DRMS (distance root mean square), which is the square root of the average squared distance error, and R95, which is the radius of the circle where 95% of the values would fall in.\n\nThe concept of CEP also plays a role when measuring the accuracy of a position obtained by a navigation system, such as GPS or older systems such as LORAN and Loran-C.\n\nThe original concept of CEP was based on a circular bivariate normal distribution (CBN) with CEP as a parameter of the CBN just as μ and σ are parameters of the normal distribution. Munitions with this distribution behavior tend to cluster around the mean impact point, with most reasonably close, progressively fewer and fewer further away, and very few at long distance. That is, if CEP is \"n\" metres, 50% of rounds land within \"n\" metres of the mean impact, 43.7% between \"n\" and \"2n\", and 6.1% between \"2n\" and \"3n\" metres, and the proportion of rounds that land farther than three times the CEP from the mean is only 0.2%.\n\nCEP is not a good measure of accuracy when this distribution behavior is not met. Precision-guided munitions generally have more \"close misses\" and so are not normally distributed. Munitions may also have larger standard deviation of range errors than the standard deviation of azimuth (deflection) errors, resulting in an elliptical confidence region. Munition samples may not be exactly on target, that is, the mean vector will not be (0,0). This is referred to as bias.\n\nTo incorporate accuracy into the CEP concept in these conditions, CEP can be defined as the square root of the mean square error (MSE). The MSE will be the sum of the variance of the range error plus the variance of the azimuth error plus the covariance of the range error with the azimuth error plus the square of the bias. Thus the MSE results from pooling all these sources of error, geometrically corresponding to radius of a circle within which 50% of rounds will land.\n\nSeveral methods have been introduced to estimate CEP from shot data. Included in these methods are the plug-in approach of Blischke and Halpin (1966), the Bayesian approach of Spall and Maryak (1992), and the maximum likelihood approach of Winkler and Bickert (2012). The Spall and Maryak approach applies when the shot data represent a mixture of different projectile characteristics (e.g., shots from multiple munitions types or from multiple locations directed at one target).\n\nWhile 50% is a very common definition for CEP, the circle dimension can be defined for percentages. Percentiles can be determined by recognizing that the horizontal position error is defined by a 2D vector which components are two uncorrelated orthogonal Gaussian random variables (one for each axis) each having a standard deviation formula_1. The distance error is the magnitude of that vector; it is a property of 2D Gaussian vectors that the magnitude follows the Rayleigh distribution, with a standard deviation formula_2, which by definition is the DRMS (distance root mean square) value. In turn, the properties of the Rayleigh distribution are, that its percentile at level formula_3 is given by the following formula:\n\nformula_4 \n\nor, expressed in terms of the DRMS:\n\nformula_5 \n\nThe relation between formula_6 and formula_7 are given by the following table, where the formula_7 values for DRMS and 2DRMS are specific to the Rayleigh distribution and are found numerically, while the CEP and R95 values are definitions: \n\nWe can then derive a conversion table to convert values expressed for one percentile level, to another. Said conversion table, giving the coefficients formula_9 to convert formula_10 into formula_11, is given by:\n\nExample: a GPS receiver having a 1.25 m DRMS error, will have a 1.25formula_121.73 = 2.16 m R95 radius.\n\nWarning: often, sensor datasheets or other publications state \"RMS\" values which in general, \"but not always\", stand for \"DRMS\" values. Also, be wary of habits coming from properties of a 1D normal distribution, such as the 68-95-99.7 rule, in essence trying to say that \"R95 = 2DRMS\". As shown above, these properties simply \"do not\" translate to the distance errors. Finally, mind that these values are obtained for a theoretical distribution; while generally being true for real data, these may be affected by other effects, which the model does not represent.\n\nThe term is used in the movie \"Clear and Present Danger\" when the ground team reports \"Circular error probability Zero. Impact with high order detonation. Have a nice day.\" Here CEP is meant to convey that the bomb landed exactly on target.\n\n\n"}
{"id": "54829672", "url": "https://en.wikipedia.org/wiki?curid=54829672", "title": "Cubical complex", "text": "Cubical complex\n\nIn mathematics, a cubical complex or cubical set is a set composed of points, line segments, squares, cubes, and their \"n\"-dimensional counterparts. They are used analogously to simplicial complexes and CW complexes in the computation of the homology of topological spaces.\n\nAn elementary interval is a subset formula_1 of the form\n\nfor some formula_3. An elementary cube formula_4 is the finite product of elementary intervals, i.e.\n\nwhere formula_6 are elementary intervals. Equivalently, an elementary cube is any translate of a unit cube formula_7 embedded in Euclidean space formula_8 (for some formula_9 with formula_10). A set formula_11 is a cubical complex (or cubical set) if it can be written as a union of elementary cubes (or possibly, is homeomorphic to such a set).\n\nElementary intervals of length 0 (containing a single point) are called degenerate, while those of length 1 are nondegenerate. The dimension of a cube is the number of nondegenerate intervals in formula_4, denoted formula_13. The dimension of a cubical complex formula_14 is the largest dimension of any cube in formula_14.\n\nIf formula_4 and formula_17 are elementary cubes and formula_18, then formula_4 is a face of formula_17. If formula_4 is a face of formula_17 and formula_23, then formula_4 is a proper face of formula_17. If formula_4 is a face of formula_17 and formula_28, then formula_4 is a primary face of formula_17.\n\nMain article: Cubical homology\n\nIn algebraic topology, cubical complexes are often useful for concrete calculations. In particular, there is a definition of homology for cubical complexes that coincides with the singular homology, but is computable. \n\n"}
{"id": "18614570", "url": "https://en.wikipedia.org/wiki?curid=18614570", "title": "Data binning", "text": "Data binning\n\nData binning (also called Discrete binning or bucketing) is a data pre-processing technique used to reduce the effects of minor observation errors. The original data values which fall in a given small interval, a bin, are replaced by a value representative of that interval, often the central value. It is a form of quantization.\n\nStatistical data binning is a way to group a number of more or less continuous values into a smaller number of \"bins\". For example, if you have data about a group of people, you might want to arrange their ages into a smaller number of age intervals (for example, grouping every five years together). It can also be used in multivariate statistics, binning in several dimensions at once.\n\nIn the context of image processing, binning is the procedure of combining a cluster of pixels into a single pixel. As such, in 2x2 binning, an array of 4 pixels becomes a single larger pixel, reducing the overall number of pixels.\n\nThis aggregation, although associated with loss of information, reduces the amount of data to be processed, facilitating the analysis. For instance, binning the data may also reduce the impact of read noise on the processed image (at the cost of a lower resolution).\n\nHistograms are an example of data binning used in order to observe underlying distributions. They typically occur in one-dimensional space and in equal intervals for ease of visualization.\n\nData binning may be used when small instrumental shifts in the spectral dimension from mass spectrometry (MS) or nuclear magnetic resonance (NMR) experiments will be falsely interpreted as representing different components, when a collection of data profiles is subjected to pattern recognition analysis. A straightforward way to cope with this problem is by using binning techniques in which the spectrum is reduced in resolution to a sufficient degree to ensure that a given peak remains in its bin despite small spectral shifts between analyses. For example, in NMR the chemical shift axis may be discretized and coarsely binned, and in MS the spectral accuracies may be rounded to integer atomic mass unit values.\n\nAlso, several digital camera systems incorporate an automatic pixel binning function to improve image contrast.\n\n"}
{"id": "31243775", "url": "https://en.wikipedia.org/wiki?curid=31243775", "title": "Double limit theorem", "text": "Double limit theorem\n\nIn hyperbolic geometry, Thurston's double limit theorem gives condition for a sequence of quasi-Fuchsian groups to have a convergent subsequence. It was introduced in and is a major step in Thurston's proof of the hyperbolization theorem for the case of manifolds that fiber over the circle.\n\nBy Bers's theorem, quasi-Fuchsian groups (of some fixed genus) are parameterized by points in \"T\"×\"T\", where \"T\" is Teichmüller space of the same genus. Suppose that there is a sequence of quasi-Fuchsian groups corresponding to points (\"g\", \"h\") in \"T\"×\"T\". Also suppose that the sequences \"g\", \"h\" converge to points μ,μ′ in the Thurston boundary of Teichmüller space of projective measured laminations. If the points μ,μ′ have the property that any nonzero measured lamination has positive intersection number with at least one of them, then the sequence of quasi-Fuchsian groups has a subsequence that converges algebraically.\n\n"}
{"id": "38090349", "url": "https://en.wikipedia.org/wiki?curid=38090349", "title": "EdgeRank", "text": "EdgeRank\n\nEdgeRank is the name commonly given to the algorithm that Facebook uses to determine what articles should be displayed in a user's News Feed. As of 2011, Facebook has stopped using the EdgeRank system and uses a machine learning algorithm that, as of 2013, takes more than 100,000 factors into account.\n\nEdgeRank was developed and implemented by Serkan Piantino.\n\nIn 2010, a simplified version of the EdgeRank algorithm was presented as:\n\nwhere:\n\n\nSome of the methods that Facebook uses to adjust the parameters are proprietary and not available to the public.\n\nEdgeRank and its successors have a broad impact on what users actually see out of what they ostensibly follow: for instance, the selection can produce a filter bubble (if users are exposed to updates which confirm their opinions etc.) or alter people's mood (if users are shown a disproportionate amount of positive or negative updates).\n\nAs a result, for Facebook pages, the typical engagement rate is less than 1 % (or less than 0.1 % for the bigger ones) and organic reach 10 % or less for most non-profits.\n\nAs a consequence, for pages it may be nearly impossible to reach any significant audience without paying to promote their content.\n\n\n"}
{"id": "7931806", "url": "https://en.wikipedia.org/wiki?curid=7931806", "title": "Essential matrix", "text": "Essential matrix\n\nIn computer vision, the essential matrix is a formula_1 matrix, formula_2, with some additional properties described below, which relates corresponding points in stereo images assuming that the cameras satisfy the pinhole camera model.\n\nMore specifically, if formula_3 and formula_4 are homogeneous \"normalized\" image coordinates in image 1 and 2, respectively, then\n\nif formula_3 and formula_4 correspond to the same 3D point in the scene.\n\nThe above relation which defines the essential matrix was published in 1981 by H. Christopher Longuet-Higgins, introducing the concept to the computer vision community. Richard Hartley and Andrew Zisserman's book reports that an analogous matrix appeared in photogrammetry long before that. Longuet-Higgins' paper includes an algorithm for estimating formula_2 from a set of corresponding normalized image coordinates as well as an algorithm for determining the relative position and orientation of the two cameras given that formula_2 is known. Finally, it shows how the 3D coordinates of the image points can be determined with the aid of the essential matrix.\n\nThe essential matrix can be seen as a precursor to the fundamental matrix. Both matrices can be used for establishing constraints between matching image points, but the essential matrix can only be used in relation to calibrated cameras since the inner camera parameters must be known in order to achieve the normalization. If, however, the cameras are calibrated the essential matrix can be useful for determining both the relative position and orientation between the cameras and the 3D position of corresponding image points.\n\nThis derivation follows the paper by Longuet-Higgins.\n\nTwo normalized cameras project the 3D world onto their respective image planes. Let the 3D coordinates of a point P be formula_10 and formula_11 relative to each camera's coordinate system. Since the cameras are normalized, the corresponding image coordinates are\n\nA homogeneous representation of the two image coordinates is then given by\n\nwhich also can be written more compactly as\n\nwhere formula_18 and formula_4 are homogeneous representations of the 2D image coordinates and formula_20 and formula_21 are proper 3D coordinates but in two different coordinate systems.\n\nAnother consequence of the normalized cameras is that their respective coordinate systems are related by means of a translation and rotation. This implies that the two sets of 3D coordinates are related as\n\nwhere formula_23 is a formula_1 rotation matrix and formula_25 is a 3-dimensional translation vector.\n\nThe essential matrix is then defined as:\n\nwhere formula_26 is the matrix representation of the cross product with formula_25.\n\nTo see that this definition of the essential matrix describes a constraint on corresponding image coordinates multiply formula_2 from left and right with the 3D coordinates of point P in the two different coordinate systems:\n\n\nFinally, it can be assumed that both formula_37 and formula_38 are > 0, otherwise they are not visible in both cameras. This gives\n\nwhich is the constraint that the essential matrix defines between corresponding image points.\n\nNot every arbitrary formula_1 matrix can be an essential matrix for some stereo cameras. To see this notice that it is defined as the matrix product of one rotation matrix and one skew-symmetric matrix, both formula_1. The skew-symmetric matrix must have two singular values which are equal and another which is zero. The multiplication of the rotation matrix does not change the singular values which means that also the essential matrix has two singular values which are equal and one which is zero. The properties described here are sometimes referred to as \"internal constraints\" of the essential matrix.\n\nIf the essential matrix formula_2 is multiplied by a non-zero scalar, the result is again an essential matrix which defines exactly the same constraint as formula_2 does. This means that formula_2 can be seen as an element of a projective space, that is, two such matrices are considered equivalent if one is a non-zero scalar multiplication of the other. This is a relevant position, for example, if formula_2 is estimated from image data. However, it is also possible to take the position that formula_2 is defined as\n\nand then formula_2 has a well-defined \"scaling\". It depends on the application which position is the more relevant.\n\nThe constraints can also be expressed as\nand \nHere the last equation is matrix constraint, which can be seen as 9 constraints, one for each matrix element. \nThese constraints are often used for determining the essential matrix from five corresponding point pairs.\n\nThe essential matrix has five or six degrees of freedom, depending on whether or not it is seen as a projective element. The rotation matrix formula_23 and the translation vector formula_25 have three degrees of freedom each, in total six. If the essential matrix is considered as a projective element, however, one degree of freedom related to scalar multiplication must be subtracted leaving five degrees of freedom in total.\n\nGiven a set of corresponding image points it is possible to estimate an essential matrix which satisfies the defining epipolar constraint for all the points in the set. However, if the image points are subject to noise, which is the common case in any practical situation, it is not possible to find an essential matrix which satisfies all constraints exactly.\n\nDepending on how the error related to each constraint is measured, it is possible to determine or estimate an essential matrix which optimally satisfies the constraints for a given set of corresponding image points. The most straightforward approach is to set up a total least squares problem, commonly known as the eight-point algorithm.\n\nGiven that the essential matrix has been determined for a stereo camera pair, for example, using the estimation method above this information can be used for determining also the rotation and translation (up to a scaling) between the two camera's coordinate systems. In these derivations formula_2 is seen as a projective element rather than having a well-determined scaling.\n\nThe following method for determining formula_23 and formula_25 is based on performing a SVD of formula_2, see Hartley & Zisserman's book. It is also possible to determine formula_23 and formula_25 without an SVD, for example, following Longuet-Higgins' paper.\n\nAn SVD of formula_2 gives\n\nwhere formula_61 and formula_62 are orthogonal formula_1 matrices and formula_64 is a formula_1 diagonal matrix with\n\nThe diagonal entries of formula_64 are the singular values of formula_2 which, according to the internal constraints of the essential matrix, must consist of two identical and one zero value. Define\n\nand make the following ansatz\n\nSince formula_64 may not completely fulfill the constraints when dealing with real world data (f.e. camera images), the alternative\n\nmay help.\n\nFirst, these expressions for formula_23 and formula_26 do satisfy the defining equation for the essential matrix\n\nSecond, it must be shown that this formula_26 is a matrix representation of the cross product for some formula_25. Since\n\nit is the case that formula_82 is skew-symmetric, i.e., formula_83. This is also the case for our formula_26, since\n\nAccording to the general properties of the matrix representation of the cross product it then follows that formula_26 must be the cross product operator of exactly one vector formula_25.\n\nThird, it must also need to be shown that the above expression for formula_23 is a rotation matrix. It is the product of three matrices which all are orthogonal which means that formula_89, too, is orthogonal or formula_90. To be a proper rotation matrix it must also satisfy formula_91. Since, in this case, formula_2 is seen as a projective element this can be accomplished by reversing the sign of formula_2 if necessary.\n\nSo far one possible solution for formula_23 and formula_25 has been established given formula_2. It is, however, not the only possible solution and it may not even be a valid solution from a practical point of view. To begin with, since the scaling of formula_2 is undefined, the scaling of formula_25 is also undefined. It must lie in the null space of formula_2 since\n\nFor the subsequent analysis of the solutions, however, the exact scaling of formula_25 is not so important as its \"sign\", i.e., in which direction it points. Let formula_102 be normalized vector in the null space of formula_2. It is then the case that both formula_102 and formula_105 are valid translation vectors relative formula_2. It is also possible to change formula_107 into formula_108 in the derivations of formula_23 and formula_25 above. For the translation vector this only causes a change of sign, which has already been described as a possibility. For the rotation, on the other hand, this will produce a different transformation, at least in the general case.\n\nTo summarize, given formula_2 there are two opposite directions which are possible for formula_25 and two different rotations which are compatible with this essential matrix. In total this gives four classes of solutions for the rotation and translation between the two camera coordinate systems. On top of that, there is also an unknown scaling formula_113 for the chosen translation direction.\n\nIt turns out, however, that only one of the four classes of solutions can be realized in practice. Given a pair of corresponding image coordinates, three of the solutions will always produce a 3D point which lies \"behind\" at least one of the two cameras and therefore cannot be seen. Only one of the four classes will consistently produce 3D points which are in front of both cameras. This must then be the correct solution. Still, however, it has an undetermined positive scaling related to the translation component.\n\nIt should be noted that the above determination of formula_23 and formula_25 assumes that formula_2 satisfy the internal constraints of the essential matrix. If this is not the case which, for example, typically is the case if formula_2 has been estimated from real (and noisy) image data, it has to be assumed that it approximately satisfy the internal constraints. The vector formula_102 is then chosen as right singular vector of formula_2 corresponding to the smallest singular value.\n\nThe problem to be solved there is how to compute formula_120 given corresponding normalized image coordinates formula_121 and formula_122. If the essential matrix is known and the corresponding rotation and translation transformations have been determined, this algorithm (described in Longuet-Higgins' paper) provides a solution.\n\nLet formula_123 denote row \"k\" of the rotation matrix formula_23:\n\nCombining the above relations between 3D coordinates in the two coordinate systems and the mapping between 3D and 2D points described earlier gives\n\nor\n\nOnce formula_37 is determined, the other two coordinates can be computed as\n\nThe above derivation is not unique. It is also possible to start with an expression for formula_130 and derive an expression for formula_37 according to\n\nIn the ideal case, when the camera maps the 3D points according to a perfect pinhole camera and the resulting 2D points can be detected without any noise, the two expressions for formula_37 are equal. In practice, however, they are not and it may be advantageous to combine the two estimates of formula_37, for example, in terms of some sort of average.\n\nThere are also other types of extensions of the above computations which are possible. They started with an expression of the primed image coordinates and derived 3D coordinates in the unprimed system. It is also possible to start with unprimed image coordinates and obtain primed 3D coordinates, which finally can be transformed into unprimed 3D coordinates. Again, in the ideal case the result should be equal to the above expressions, but in practice they may deviate.\n\nA final remark relates to the fact that if the essential matrix is determined from corresponding image coordinate, which often is the case when 3D points are determined in this way, the translation vector formula_25 is known only up to an unknown positive scaling. As a consequence, the reconstructed 3D points, too, are undetermined with respect to a positive scaling.\n\n\n\n"}
{"id": "2897680", "url": "https://en.wikipedia.org/wiki?curid=2897680", "title": "Event calculus", "text": "Event calculus\n\nThe event calculus is a logical language for representing and reasoning about events and their effects first presented by Robert Kowalski and Marek Sergot in 1986.\nIt was extended by Murray Shanahan and Rob Miller in the 1990s. Similar to other languages for reasoning about change, the event calculus represents the effects of actions on fluents. However, events can also be external to the system. In the event calculus, one can specify the value of fluents at some given time points, the events that take place at given time points, and their effects.\n\nIn the event calculus, fluents are reified. This means that they are not formalized by means of predicates but by means of functions. A separate predicate is used to tell which fluents hold at a given time point. For example, formula_1 means that the box is on the table at time ; in this formula, is a predicate while is a function.\n\nEvents are also represented as terms. The effects of events are given using the predicates and . In particular, formula_2 means that,\nif the event represented by the term is executed at time ,\nthen the fluent will be true after .\nThe predicate has a similar meaning, with the only difference \nbeing that will be false and not true after .\n\nLike other languages for representing actions, the event calculus formalizes the correct evolution of the fluent via formulae telling the value of each fluent after an arbitrary action has been performed. The event calculus solves the frame problem in a way that is similar to the successor state axioms of the situation calculus: a fluent is true at time if and only if it has been made true in the past and has not been made false in the meantime.\n\nThis formula means that the fluent represented by the term is true at time if:\n\n\nA similar formula is used to formalize the opposite case in which a fluent is false at a given time. Other formulae are also needed for correctly formalizing fluents before they have been effects of an event. These formulae are similar to the above, but formula_8 is replaced by formula_9.\n\nThe predicate, stating that a fluent has been made false during an interval, can be axiomatized, or simply taken as a shorthand, as follows:\n\nThe axioms above relate the value of the predicates , and , but do not specify which fluents are known to be true and which events actually make fluents true or false. This is done by using a set of domain-dependent axioms. The known values of fluents are stated as simple literals formula_11. The effects of events are stated by formulae relating the effects of events with their preconditions. For example, if the event makes the fluent true, but only if is currently true, the corresponding formula in the event calculus is:\n\nThe right-hand expression of this equivalence is composed of a disjunction: for each event and fluent that can be made true by the event, there is a disjunct saying that is actually that event, that is actually that fluent, and that the precondition of the event is met.\n\nThe formula above specifies the truth value of formula_2 for every possible event and fluent. As a result, all effects of all events have to be combined in a single formulae. This is a problem, because the addition of a new event requires modifying an existing formula rather than adding new ones. This problem can be solved by the application of circumscription to a set of formulae each specifying one effect of one event:\n\nThese formulae are simpler than the formula above, because each effect of each event can be specified separately. The single formula telling which events and fluents make formula_2 true has been replaced by a set of smaller formulae, each one telling the effect of an event on a fluent.\nHowever, these formulae are not equivalent to the formula above. Indeed, they only specify sufficient conditions for formula_2 to be true, which should be completed by the fact that is false in all other cases. This fact can be formalized by simply circumscribing the predicate in the formula above. It is important to note that this circumscription is done only on the formulae specifying and not on the domain-independent axioms. The predicate can be specified in the same way is.\n\nA similar approach can be taken for the predicate. The evaluation of this predicate can be enforced by formulae specifying not only when it is true and when it is false:\n\nCircumscription can simplify this specification, as only necessary conditions can be specified:\n\nCircumscribing the predicate , this predicate will be false at all points in which it is not explicitly specified to be true. This circumscription has to be done separately from the circumscription of the other formulae. In other words, if is the set of formulae of the kind formula_22, is the set of formulae formula_23, and are the domain independent axioms, the correct formulation of the domain is:\n\nThe event calculus was originally formulated as a set of Horn clauses augmented with negation as failure and could be run as a Prolog program. \nIn fact, circumscription is one of the several semantics that can be given to negation as failure, and is closely related to the completion semantics (in which \"if\" is interpreted as \"if and only if\" — see logic programming).\n\nThe original event calculus paper of Kowalski and Sergot focused on applications to database updates and narratives. Extensions of the event \ncalculus can also formalize non-deterministic actions, concurrent actions, actions with delayed effects, gradual changes, actions with duration, continuous change, and non-inertial fluents.\n\nKave Eshghi showed how the event calculus can be used for planning, using abduction to generate hypothetical events in abductive logic programming. Van Lambalgen and Hamm showed how the event calculus can also be used to give an algorithmic semantics to tense and aspect in natural language using constraint logic programming.\n\nIn addition to Prolog and its variants, several other tools for reasoning using the event calculus are also available:\n\n\n"}
{"id": "43092207", "url": "https://en.wikipedia.org/wiki?curid=43092207", "title": "Formal ball", "text": "Formal ball\n\nIn topology, a formal ball is an extension of the notion of ball to allow unbounded and negative radius. The concept of formal ball was introduced by Weihrauch and Schreiber in 1981 and the negative radius case (the generalized formal ball) by Tsuiki and Hattori in 2008.\n\nSpecifically, if formula_1 is a metric space and formula_2 the nonnegative real numbers, then an element of formula_3 is a formal ball. Elements of formula_4 are known as generalized formal balls.\n\nFormal balls possess a partial order formula_5 defined by formula_6 if formula_7, identical to that defined by set inclusion.\n\nGeneralized formal balls are interesting because this partial order works just as well for formula_8 as for formula_9, even though a generalized formal ball with negative radius does not correspond to a subset of formula_10.\n\nFormal balls possess the Lawson topology and the Martin topology.\n\n\n\n"}
{"id": "101891", "url": "https://en.wikipedia.org/wiki?curid=101891", "title": "Gian-Carlo Rota", "text": "Gian-Carlo Rota\n\nGian-Carlo Rota (April 27, 1932 – April 18, 1999) was an Italian-born American mathematician and philosopher.\n\nRota was born in Vigevano, Italy. His father, Giovanni, a prominent antifascist, was the brother of the mathematician Rosetta, who was the wife of the writer Ennio Flaiano. Gian-Carlo's family left Italy when he was 13 years old, initially going to Switzerland.\n\nRota attended the Colegio Americano de Quito in Ecuador, and earned degrees at Princeton University and Yale University.\n\nMuch of Rota's career was spent as a professor at the Massachusetts Institute of Technology (MIT), where he was and remains the only person ever to be appointed Professor of Applied Mathematics and Philosophy. Rota was also the Norbert Wiener Professor of Applied Mathematics.\n\nIn addition to his professorships at MIT, Rota held four honorary degrees, from the University of Strasbourg, France (1984); the University of L'Aquila, Italy (1990); the University of Bologna, Italy (1996); and Brooklyn Polytechnic University (1997).\nBeginning in 1966 he was a consultant at Los Alamos National Laboratory, frequently visiting to lecture, discuss, and collaborate, notably with his friend Stanislaw Ulam. He was also a consultant for the Rand Corporation (1966–71) and for the Brookhaven National Laboratory (1969–1973). Rota was elected to the National Academy of Sciences in 1982, was vice president of the American Mathematical Society (AMS) from 1995–97, and was a member of numerous other mathematical and philosophical organizations.\n\nHe taught a difficult but very popular course in probability. He also taught Applications of Calculus, differential equations, and Combinatorial Theory. His philosophy course in phenomenology was offered on Friday nights to keep the enrollment manageable. Among his many eccentricities, he would not teach without a can of Coca-Cola, and handed out prizes ranging from Hershey bars to pocket knives to students who asked questions in class or did well on tests.\n\nRota began his career as a functional analyst, but switched to become a distinguished combinatorialist. His series of ten papers on the \"Foundations of Combinatorics\" in the 1960s is credited with making it a respectable branch of modern mathematics. He said that the one combinatorial idea he would like to be remembered for is the correspondence between combinatorial problems and problems of the location of the zeroes of polynomials. He worked on the theory of incidence algebras (which generalize the 19th-century theory of Möbius inversion) and popularized their study among combinatorialists, set the umbral calculus on a rigorous foundation, unified the theory of Sheffer sequences and polynomial sequences of binomial type, and worked on fundamental problems in probability theory. His philosophical work was largely in the phenomenology of Edmund Husserl.\n\nRota died of atherosclerotic cardiac disease, apparently in his sleep at his home in Cambridge, Massachusetts. He died just a few days short of his 67th birthday. His death was discovered after he failed to arrive in Philadelphia for lectures he had planned to present beginning on Monday 19 April 1999.\n\nA reading room in MIT's Department of Mathematics is dedicated to Rota.\n\n\n"}
{"id": "485168", "url": "https://en.wikipedia.org/wiki?curid=485168", "title": "Hairy ball theorem", "text": "Hairy ball theorem\n\nThe hairy ball theorem of algebraic topology (sometimes called the hedgehog theorem in Europe) states that there is no nonvanishing continuous tangent vector field on even-dimensional \"n\"-spheres. For the ordinary sphere, or 2‑sphere, if \"f\" is a continuous function that assigns a vector in R to every point \"p\" on a sphere such that \"f\"(\"p\") is always tangent to the sphere at \"p\", then there is at least one \"p\" such that \"f\"(\"p\") = 0. In other words, whenever one attempts to comb a hairy ball flat, there will always be at least one tuft of hair at one point on the ball. The theorem was first stated by Henri Poincaré in the late 19th century.\n\nThis is famously stated as \"you can't comb a hairy ball flat without creating a cowlick\", \"you can't comb the hair on a coconut\", or sometimes \"every cow must have at least one cowlick.\" It can also be written as, \"Every smooth vector field on a sphere has a singular point.\" It was first proven in 1912 by Brouwer.\n\nFrom a more advanced point of view: every zero of a vector field has a (non-zero) \"index\", and it can be shown that the sum of all of the indices at all of the zeros must be two. (This is because the Euler characteristic of the 2-sphere is two.) Therefore, there must be at least one zero. This is a consequence of the Poincaré–Hopf theorem. In the case of the torus, the Euler characteristic is 0; and it \"is\" possible to \"comb a hairy doughnut flat\". In this regard, it follows that for any compact regular 2-dimensional manifold with non-zero Euler characteristic, any continuous tangent vector field has at least one zero.\n\nA curious meteorological application of this theorem involves considering the wind as a vector defined at every point continuously over the surface of a planet with an atmosphere. As an idealisation, take wind to be a two-dimensional vector: suppose that relative to the planetary diameter of the Earth, its vertical (i.e., non-tangential) motion is negligible.\n\nOne scenario, in which there is absolutely no wind (air movement), corresponds to a field of zero-vectors. This scenario is uninteresting from the point of view of this theorem, and physically unrealistic (there will always be wind). In the case where there is at least some wind, the Hairy Ball Theorem dictates that at all times there must be at least one point on a planet with no wind at all and therefore a tuft. This corresponds to the above statement that there will always be \"p\" such that \"f\"(\"p\") = 0.\n\nIn a physical sense, this zero-wind point will be the center of a cyclone or anticyclone. (Like the swirled hairs on the tennis ball, the wind will spiral around this zero-wind point - under our assumptions it cannot flow into or out of the point.) In brief, then, the theorem dictates that, given at least some wind on Earth, there must at all times be a cyclone or anticyclone somewhere.\n\nNote that the center with zero wind can be arbitrarily large or small. Mathematical consistency dictates the wind forms a cyclonic wind pattern for at least one point on the planet, but this does not require the cyclone be a violent storm.\n\nThis is not strictly true as the air above the earth has multiple layers, but for each layer there must be a point with zero horizontal windspeed.\n\nA common problem in computer graphics is to generate a non-zero vector in R that is orthogonal to a given non-zero one. There is no single \"continuous\" function that can do this for all non-zero vector inputs. This is a corollary of the hairy ball theorem. To see this, consider the given vector as the radius of a sphere and note that finding a non-zero vector orthogonal to the given one is equivalent to finding a non-zero vector that is tangent to the surface of that sphere where it touches the radius. However, the hairy ball theorem says there exists no \"continuous\" function that can do this for every point on the sphere (i.e. every given vector).\n\nThere is a closely related argument from algebraic topology, using the Lefschetz fixed-point theorem. Since the Betti numbers of a 2-sphere are 1, 0, 1, 0, 0, ... the \"Lefschetz number\" (total trace on homology) of the identity mapping is 2. By integrating a vector field we get (at least a small part of) a one-parameter group of diffeomorphisms on the sphere; and all of the mappings in it are homotopic to the identity. Therefore, they all have Lefschetz number 2, also. Hence they have fixed points (since the Lefschetz number is nonzero). Some more work would be needed to show that this implies there must actually be a zero of the vector field. It does suggest the correct statement of the more general Poincaré-Hopf index theorem.\n\nA consequence of the hairy ball theorem is that any continuous function that maps an even-dimensional sphere into itself has either a fixed point or a point that maps onto its own antipodal point. This can be seen by transforming the function into a tangential vector field as follows.\n\nLet \"s\" be the function mapping the sphere to itself, and let \"v\" be the tangential vector function to be constructed. For each point \"p\", construct the stereographic projection of \"s\"(\"p\") with \"p\" as the point of tangency. Then \"v\"(\"p\") is the displacement vector of this projected point relative to \"p\". According to the hairy ball theorem, there is a \"p\" such that \"v\"(\"p\") = 0, so that \"s\"(\"p\") = \"p\".\n\nThis argument breaks down only if there exists a point \"p\" for which \"s\"(\"p\") is the antipodal point of \"p\", since such a point is the only one that cannot be stereographically projected onto the tangent plane of \"p\".\n\nThe connection with the Euler characteristic χ suggests the correct generalisation: the 2\"n\"-sphere has no non-vanishing vector field for . The difference between even and odd dimensions is that, because the only nonzero Betti numbers of the \"m\"-sphere are b and b, their alternating sum χ is 2 for \"m\" even, and 0 for \"m\" odd.\n\n"}
{"id": "2336108", "url": "https://en.wikipedia.org/wiki?curid=2336108", "title": "Hilbert's twenty-third problem", "text": "Hilbert's twenty-third problem\n\nHilbert's twenty-third problem is the last of Hilbert problems set out in a celebrated list compiled in 1900 by David Hilbert. In contrast with Hilbert's other 22 problems, his 23rd is not so much a specific \"problem\" as an encouragement towards further development of the calculus of variations. His statement of the problem is a summary of the state-of-the-art (in 1900) of the theory of calculus of variations, with some introductory comments decrying the lack of work that had been done of the theory in the mid to late 19th century.\n\nThe problem statement begins with the following paragraph:\n\nSo far, I have generally mentioned problems as definite and special as possible... Nevertheless, I should like to close with a general problem, namely with the indication of a branch of mathematics repeatedly mentioned in this lecture-which, in spite of the considerable advancement lately given it by Weierstrass, does not receive the general appreciation which, in my opinion, it is due-I mean the calculus of variations.\n\nCalculus of variations is a field of mathematical analysis that deals with maximizing or minimizing functionals, which are mappings from a set of functions to the real numbers. Functionals are often expressed as definite integrals involving functions and their derivatives. The interest is in \"extremal\" functions that make the functional attain a maximum or minimum value – or \"stationary\" functions – those where the rate of change of the functional is zero.\n\nFollowing the problem statement, David Hilbert, Emmy Noether, Leonida Tonelli, Henri Lebesgue and Jacques Hadamard among others made significant contributions to the calculus of variations. Marston Morse applied calculus of variations in what is now called Morse theory. Lev Pontryagin, Ralph Rockafellar and F. H. Clarke developed new mathematical tools for the calculus of variations in optimal control theory. The dynamic programming of Richard Bellman is an alternative to the calculus of variations.\n"}
{"id": "10962250", "url": "https://en.wikipedia.org/wiki?curid=10962250", "title": "Howard Jerome Keisler", "text": "Howard Jerome Keisler\n\nHoward Jerome Keisler (born 3 December 1936) is an American mathematician, currently professor emeritus at University of Wisconsin–Madison. His research has included model theory and non-standard analysis.\n\nHis Ph.D. advisor was Alfred Tarski at Berkeley; his dissertation is \"Ultraproducts and Elementary Classes\" (1961).\n\nFollowing Abraham Robinson's work resolving what had long been thought to be inherent logical contradictions in the literal interpretation of Leibniz's notation that Leibniz himself had proposed, that is, interpreting \"dx\" as literally representing an infinitesimally small quantity, Keisler published \"\", a first-year calculus textbook conceptually centered on the use of infinitesimals, rather than the epsilon, delta approach, for developing the calculus.\n\nHe is also known for extending the Henkin construction (of Leon Henkin) to what are now called Henkin–Keisler models.\n\nHe held the named chair of Vilas Professor of Mathematics at Wisconsin.\n\nAmong Keisler's graduate students, several have made notable mathematical contributions, including Frederick Rowbottom who discovered Rowbottom cardinals. Several others have gone on to careers in computer science research and product development, including: Michael Benedikt, a professor of computer science at the University of Oxford, Kevin J. Compton, a professor of computer science at the University of Michigan, Curtis Tuckey, a developer of software-based collaboration environments; Joseph Sgro, a neurologist and developer of vision processor hardware and software, and Edward L. Wimmers, a database researcher at IBM Almaden Research Center.\n\nIn 2012 he became a fellow of the American Mathematical Society.\n\nHis son Jeffrey Keisler is a Fulbright Distinguished Chair.\n\n\n\n"}
{"id": "3125808", "url": "https://en.wikipedia.org/wiki?curid=3125808", "title": "Instantaneous phase", "text": "Instantaneous phase\n\nInstantaneous phase and instantaneous frequency are important concepts in signal processing that occur in the context of the representation and analysis of time-varying functions. The instantaneous phase (or \"local phase\" or simply \"phase\") of a \"complex-valued\" function \"s\"(\"t\"), is the real-valued function:\nwhere arg is the complex argument function.\n\nAnd for a \"real-valued\" function \"s\"(\"t\"), it is determined from the function's analytic representation, \"s\"(\"t\"):\n\nWhen \"φ\"(\"t\") is constrained to its principal value, either the interval (-π, π] or [0, 2π), it is called \"wrapped phase\". Otherwise it is called \"unwrapped phase\", which is a continuous function of argument \"t\", assuming \"s\"(\"t\") is a continuous function of \"t\". Unless otherwise indicated, the continuous form should be inferred.\n\nwhere \"ω\" > 0.\nIn this simple sinusoidal example, the constant \"θ\" is also commonly referred to as \"phase\" or \"phase offset\". \"φ\"(\"t\") is a function of time; \"θ\" is not. In the next example, we also see that the phase offset of a real-valued sinusoid is ambiguous unless a reference (sin or cos) is specified. \"φ\"(\"t\") is unambiguously defined.\n\nwhere \"ω\" > 0.\nIn both examples the local maxima of \"s\"(\"t\") correspond to \"φ\"(\"t\") = 2π\"N\" for integer values of \"N\". This has applications in the field of computer vision.\n\nInstantaneous angular frequency is defined as:\nand instantaneous (ordinary) frequency is defined as:\nwhere \"φ\"(\"t\") must be the \"unwrapped\" instantaneous phase angle. If \"φ\"(\"t\") is wrapped, discontinuities in \"φ\"(\"t\") will result in dirac delta impulses in \"f\"(\"t\").\n\nThe inverse operation, which always unwraps phase, is:\n\nThis instantaneous frequency, \"ω\"(t), can be derived directly from the real and imaginary parts of \"s\"(\"t\"), instead of the complex arg without concern of phase unwrapping.\n\n2\"m\"π and \"m\"π are the integer multiples of π necessary to add to unwrap the phase. At values of time, \"t\", where there is no change to integer \"m\", the derivative of \"φ\"(\"t\") is\n\nThis representation is similar to the wrapped phase representation in that it does not distinguish between multiples of 2π in the phase, but similar to the unwrapped phase representation since it is continuous. A vector-average phase can be obtained as the arg of the sum of the complex numbers without concern about wrap-around.\n\n\n"}
{"id": "11518586", "url": "https://en.wikipedia.org/wiki?curid=11518586", "title": "Intelligence Advanced Research Projects Activity", "text": "Intelligence Advanced Research Projects Activity\n\nThe Intelligence Advanced Research Projects Activity (IARPA) is an organization within the Office of the Director of National Intelligence responsible for leading research to overcome difficult challenges relevant to the United States Intelligence Community. IARPA characterizes its mission as follows: \"To envision and lead high-risk, high-payoff research that delivers innovative technology for future overwhelming intelligence advantage.\"\n\nIARPA funds academic and industry research across a broad range of technical areas, including mathematics, computer science, physics, chemistry, biology, neuroscience, linguistics, political science, and cognitive psychology. Most IARPA research is unclassified and openly published. IARPA transfers successful research results and technologies to other government agencies. Notable IARPA investments include quantum computing, superconducting computing, machine learning, and forecasting tournaments.\n\nIARPA characterizes its mission as follows:\n\nTo envision and lead high-risk, high-payoff research that delivers innovative technology for future overwhelming intelligence advantage.\n\nIn 1958, the first Advanced Research Projects Agency, or ARPA, was created in response to an unanticipated surprise—the Soviet Union's successful launch of Sputnik on October 4, 1957. The ARPA model was designed to anticipate and pre-empt technological surprise. As then-Secretary of Defense Neil McElroy said, \"I want an agency that makes sure no important thing remains undone because it doesn’t fit somebody's mission.\" The ARPA model has been characterized by ambitious technical goals, competitively awarded research led by term-limited staff, and independent testing and evaluation.\n\nAuthorized by the ODNI in 2006, IARPA was modeled after DARPA but focused on national intelligence needs, rather than military needs. The agency was a consolidation of the National Security Agency's Disruptive Technology Office, the National Geospatial-Intelligence Agency's National Technology Alliance, and the Central Intelligence Agency's Intelligence Technology Innovation Center. IARPA operations began on October 1, 2007 with Lisa Porter as founding Director. Its headquarters, a new building in M Square, the University of Maryland's research park in Riverdale Park, Maryland, was dedicated in April 2009.\n\nIARPA's quantum computing research was named \"Science\" magazine's Breakthrough of the Year in 2010. In 2015, IARPA was named to lead foundational research and development in the National Strategic Computing Initiative. IARPA is also a part of other White House science and technology efforts, including the U.S. BRAIN Initiative, and the Nanotechnology-Inspired Grand Challenge for Future Computing. In 2013, \"New York Times\" op-ed columnist David Brooks called IARPA \"one of the government's most creative agencies.\"\n\nIARPA invests in multi-year research programs, in which academic and industry teams compete to solve a well-defined set of technical problems, regularly scored on a shared set of metrics and milestones. Each program is led by an IARPA Program Manager (PM) who is a term-limited Government employee. IARPA programs are meant to enable researchers to pursue ideas that are potentially disruptive to the status quo.\n\nMost IARPA research is unclassified and openly published. Current director Jason Matheny has stated the agency's goals of openness and external engagement to draw in expertise from academia and industry, or even individuals who \"might be working in their basement on some data-science project and might have an idea for how to solve an important problem\". IARPA transfers successful research results and technologies to other government agencies.\n\nIARPA is known for its programs to fund research into anticipatory intelligence, using data science to make predictions about future events ranging from political elections to disease outbreaks to cyberattacks, some of which focus on open-source intelligence. IARPA has pursued these objectives not only through traditional funding programs but also through tournaments and prizes. c is an example of one such program. Other projects involve analysis of images or video that lacks metadata by directly analyzing the media's content itself. Examples given by IARPA include determining the location of an image by analyzing features such as placement of trees or a mountain skyline, or determining whether a video is of a baseball game or a traffic jam. Another program focuses on developing speech recognition tools that can transcribe arbitrary languages.\n\nIARPA is also involved in high-performance computing and alternative computing methods. In 2015, IARPA was named as one of two foundational research and development agencies in the National Strategic Computing Initiative, with the specific charge of \"future computing paradigms offering an alternative to standard semiconductor computing technologies\". One such approach is cryogenic superconducting computing, which seeks to use superconductors such as niobium rather than semiconductors to reduce the energy consumption of future exascale supercomputers.\n\nSeveral programs at IARPA focus on quantum computing and neuroscience. IARPA is a major funder of quantum computing research due to its applications in quantum cryptography. As of 2009, IARPA was said to provide a large portion of quantum computing funding resources in the United States. Quantum computing research funded by IARPA was named Science Magazine's Breakthrough of the Year in 2010, and physicist David Wineland was a winner of the 2012 Nobel Prize in Physics for quantum computing research funded by IARPA. IARPA is also involved in neuromorphic computation efforts as part of the U.S. BRAIN Initiative and the National Nanotechnology Initiative's Grand Challenge for Future Computing. IARPA's MICrONS project seeks to reverse engineer one cubic millimeter of brain tissue and use insights from its study to improve machine learning and artificial intelligence.\n\nBelow are some of the past and current research programs of IARPA.\n\n\n\n\n\n\n"}
{"id": "27295601", "url": "https://en.wikipedia.org/wiki?curid=27295601", "title": "Isolation lemma", "text": "Isolation lemma\n\nIn theoretical computer science, the term isolation lemma (or isolating lemma) refers to randomized algorithms that reduce the number of solutions to a problem to one, should a solution exist.\nThis is achieved by constructing random constraints such that, with non-negligible probability, exactly one solution satisfies these additional constraints if the solution space is not empty.\nIsolation lemmas have important applications in computer science, such as the Valiant–Vazirani theorem and Toda's theorem in computational complexity theory.\n\nThe first isolation lemma was introduced by , albeit not under that name.\nTheir isolation lemma chooses a random number of random hyperplanes, and has the property that, with non-negligible probability, the intersection of any fixed non-empty solution space with the chosen hyperplanes contains exactly one element. This suffices to show the Valiant–Vazirani theorem:\nthere exists a randomized polynomial-time reduction from the satisfiability problem for Boolean formulas to the problem of detecting whether a Boolean formula has a unique solution.\nHere every coordinate of the solution space gets assigned a random weight in a certain range of integers, and the property is that, with non-negligible probability, there is exactly one element in the solution space that has minimum weight. This can be used to obtain a randomized parallel algorithm for the maximum matching problem.\n\nStronger isolation lemmas have been introduced in the literature to fit different needs in various settings.\nFor example, the isolation lemma of has similar guarantees as that of Mulmuley et al., but it uses fewer random bits.\nIn the context of the exponential time hypothesis, prove an isolation lemma for k-CNF formulas.\nNoam Ta-Shma gives an isolation lemma with slightly stronger parameters, and gives non-trivial results even when the size of the weight domain is smaller than the number of variables.\n\nIt is remarkable that the lemma assumes nothing about the nature of the family formula_3: for instance formula_3 may include \"all\" formula_15 nonempty subsets. Since the weight of each set in formula_3 is between formula_17 and formula_18 on average there will be formula_19 sets of each possible weight.\nStill, with high probability, there is a unique set that has minimum weight.\n\n\n"}
{"id": "7095290", "url": "https://en.wikipedia.org/wiki?curid=7095290", "title": "John Howard Redfield", "text": "John Howard Redfield\n\nJohn Howard Redfield (June 8, 1879 – April 17, 1944) was an American mathematician, best known for discovery of what is now called Pólya enumeration theorem (PET) in 1927, ten years ahead of similar but independent discovery made by George Pólya. Redfield was a great-grandson of William Charles Redfield, one of the founders and the first president of AAAS.\n\nRedfield's ability is evident in letters exchanged among Redfield, Percy MacMahon, and Sir Thomas Muir, following the publication of Redfield's paper [1] in 1927. Apparently Redfield sent a copy of his paper to MacMahon. In reply (letter of November 19, 1927), MacMahon expresses the view that Redfield has made a valuable contribution to the subject and goes on to mention a conjecture which he himself made in his recently delivered Rouse-Ball memorial lecture. He also says that it is probable that Redfield's work would lead to a proof of it. Such was the case: in a draft reply dated December 26, 1927, Redfield writes: \nMacMahon, who had failed to prove it himself and then put the matter before men at both Cambridge and Oxford \"without effect\", delightedly wrote to Redfield (letter of January 9, 1928): \nMacMahon urged Redfield to publish his new results and also informed Muir about them. In a letter to Redfield dated December 31, 1931, Muir also encourages him to publish his verification \"without waiting for MacMahon's executors\" and suggests the \"Journal of the London Mathematical Society\" as an appropriate medium. As far as is known, Redfield did not follow up this suggestion, but the proof of MacMahon's conjecture was included in an unpublished manuscript which appears to be a sequel to the paper [3].\n\nA letter from Professor Cletus Oakley to Frank Harary, dated December 19, 1963, reads in part:\n\nRedfield's brother, Alfred, a marine biologist-oceanographer and former Associate Director of the Woods Hole Oceanographic Institution, wrote (letter to E. Keith Lloyd, September 8, 1976):\n\n"}
{"id": "14258729", "url": "https://en.wikipedia.org/wiki?curid=14258729", "title": "Kleene–Rosser paradox", "text": "Kleene–Rosser paradox\n\nIn mathematics, the Kleene–Rosser paradox is a paradox that shows that certain systems of formal logic are inconsistent, in particular the version of Curry's combinatory logic introduced in 1930, and Church's original lambda calculus, introduced in 1932–1933, both originally intended as systems of formal logic. The paradox was exhibited by Stephen Kleene and J. B. Rosser in 1935.\n\nKleene and Rosser were able to show that both systems are able to characterize and enumerate their provably total, definable number-theoretic functions, which enabled them to construct a term that essentially replicates the Richard paradox in formal language.\n\nCurry later managed to identify the crucial ingredients of the calculi that allowed the construction of this paradox, and used this to construct a much simpler paradox, now known as Curry's paradox.\n\n\n"}
{"id": "734644", "url": "https://en.wikipedia.org/wiki?curid=734644", "title": "List of axioms", "text": "List of axioms\n\nThis is a list of axioms as that term is understood in mathematics, by Wikipedia page. In epistemology, the word \"axiom\" is understood differently; see axiom and self-evidence. Individual axioms are almost always part of a larger axiomatic system.\n\n\"Together with the axiom of choice (see below), these are the\" de facto \"standard axioms for contemporary mathematics or set theory. They can be easily adapted to analogous theories, such as mereology.\"\n\n\nSee also Zermelo set theory.\n\n\"With the Zermelo–Fraenkel axioms above, this makes up the system ZFC in which most mathematics is potentially formalisable.\"\n\n\n\n\n\n\n\n"}
{"id": "5971841", "url": "https://en.wikipedia.org/wiki?curid=5971841", "title": "List of mathematicians (X)", "text": "List of mathematicians (X)\n\n"}
{"id": "39728316", "url": "https://en.wikipedia.org/wiki?curid=39728316", "title": "List of polygons", "text": "List of polygons\n\nIn geometry, a polygon is traditionally a plane figure that is bounded by a finite chain of straight line segments closing in a loop to form a closed chain. These segments are called its \"edges\" or \"sides\", and the points where two edges meet are the polygon's \"vertices\" (singular: vertex) or \"corners\".\n\nThe word polygon comes from Late Latin \"polygōnum\" (a noun), from Greek πολύγωνον (\"polygōnon/polugōnon\"), noun use of neuter of πολύγωνος (\"polygōnos/polugōnos\", the masculine adjective), meaning \"many-angled\". Individual polygons are named (and sometimes classified) according to the number of sides, combining a Greek-derived numerical prefix with the suffix \"-gon\", e.g. \"pentagon\", \"dodecagon\". The triangle, quadrilateral and nonagon are exceptions, although the regular forms \"trigon\", \"tetragon\", and \"enneagon\" are sometimes encountered as well.\n\nPolygons are primarily named by prefixes from Greek numbers.\n\nTo construct the name of a polygon with more than 20 and fewer than 100 edges, combine the prefixes as follows. The \"kai\" connector is not included by some authors.\nExtending the system up to 999 is expressed with these prefixes.\n\n"}
{"id": "43530822", "url": "https://en.wikipedia.org/wiki?curid=43530822", "title": "List of shapes with known packing constant", "text": "List of shapes with known packing constant\n\nThe packing constant of a geometric body is the largest average density achieved by packing arrangements of congruent copies of the body. For most bodies the value of the packing constant is unknown. The following is a list of bodies in Euclidean spaces whose packing constant is known. Fejes Tóth proved that in the plane, a point symmetric body has a packing constant that is equal to its translative packing constant and its lattice packing constant. Therefore, any such body for which the lattice packing constant was previously known, such as any ellipse, consequently has a known packing constant. In addition to these bodies, the packing constants of hyperspheres in 8 and 24 dimensions are almost exactly known.\n"}
{"id": "29819533", "url": "https://en.wikipedia.org/wiki?curid=29819533", "title": "Mamikon Mnatsakanian", "text": "Mamikon Mnatsakanian\n\nMamikon A. Mnatsakanian () is Project Associate at Project Mathematics! at the California Institute of Technology. \n\nHe received a Ph.D. in physics in 1969 from Yerevan State University, where he became professor of astrophysics. As an undergraduate he specialized in the development of geometric methods for solving calculus problems by a visual approach that makes no use of formulas, which he later developed into his system of visual calculus.\n\nIn 2010 he was nominated by Caltech for the Ambartsumians International Prize, awarded annually by the President of Armenia, for his contributions in the field of theoretical astrophysics.\n\nIn 1959 he discovered a new proof of the Pythagorean theorem.\n\n\n"}
{"id": "46797936", "url": "https://en.wikipedia.org/wiki?curid=46797936", "title": "Mapping spectrum", "text": "Mapping spectrum\n\nIn algebraic topology, the mapping spectrum formula_1 of spectra \"X\", \"Y\" is characterized by\n"}
{"id": "26015807", "url": "https://en.wikipedia.org/wiki?curid=26015807", "title": "Max-dominated strategy", "text": "Max-dominated strategy\n\nIn game theory a max-dominated strategy is a strategy which is not a best response to any strategy profile of the other players. This is an extension to the notion of strictly dominated strategies, which are max-dominated as well.\n\nA strategy formula_1 of player formula_2 is \"max-dominated\" if for every strategy profile of the other players\nformula_3 there is a strategy formula_4 such that formula_5. This definition means that formula_6 is not a best response to any strategy profile formula_7, since for every such strategy profile there is another strategy formula_8 which gives higher utility than formula_6 for player formula_2.\n\nIt is easy to see that if a strategy formula_1 is strictly dominated by strategy formula_12 then it is also max-dominated, since for every strategy profile of the other players formula_3 we will pick formula_8 to be the strategy for which formula_5.\n\nIt is also notable that even if formula_6 is strictly dominated by a mixed strategy it is also max-dominated.\n\nA strategy formula_1 of player formula_2 is weakly max-dominated if for every strategy profile of the other players formula_3 there is a strategy formula_4 such that formula_21. This definition means that formula_6 is either not a best response or not the only best response to any strategy profile formula_7, since for every such strategy profile there is another strategy formula_8 which gives at least the same utility as formula_6 for player formula_2.\n\nIt is easy to see that if a strategy formula_1 is weakly dominated by strategy formula_12 then it is also weakly max-dominated, since for every strategy profile of the other players formula_3 we will pick formula_8 to be the strategy for which formula_31.\n\nIt is also notable that even if formula_6 is weakly dominated by a mixed strategy it is also weakly max-dominated.\n\nA game formula_33 is said to be max-solvable if by iterated elimination of max-dominated strategies only one strategy profile is left at the end.\n\nMore formally we say that formula_33 is max-solvable if there exists a sequence of games formula_35 such that:\n\nObviously every max-solvable game has a unique pure Nash equilibrium which is the strategy profile left in formula_39.\n\nAs in the previous part one can define respectively the notion of weakly max-solvable games, which are games for which a game with a single strategy profile can be reached by eliminating weakly max-dominated strategies. The main difference would be that weakly max-dominated games may have more than one pure Nash equilibrium, and that the order of elimination might result in different Nash equilibria.\n\nThe prisoner's dilemma is an example of a max-solvable game (as it is also dominance solvable). The strategy cooperate is max-dominated by the strategy defect for both players, since playing defect always gives the player a higher utility, no matter what the other player plays. To see this note that if the row player plays cooperate then the column player would prefer playing defect and go free than playing cooperate and serving one year in jail. If the row player plays defect then the column player would prefer playing defect and serve three years in jail rather than playing cooperate and serving five years in jail.\n\nIn any max-solvable game, best-reply dynamics ultimately leads to the unique pure Nash equilibrium of the game. In order to see this, all we need to do is notice that if formula_41 is an elimination sequence of the game (meaning that first formula_42 is eliminated from the strategy space of some player since it is max-dominated, then formula_43 is eliminated, and so on), then in the best-response dynamics formula_42 will be never played by its player after one iteration of best responses, formula_43 will never be played by its player after two iterations of best responses and so on. The reason for this is that formula_42 is not a best response to any strategy profile of the other players formula_7 so after one iteration of best responses its player must have chosen a different strategy. Since we understand that we will never return to formula_42 in any iteration of the best responses, we can treat the game after one iteration of best responses as if formula_42 has been eliminated from the game, and complete the proof by induction.\n\nIt may come by surprise then that weakly max-solvable games do not necessarily converge to a pure Nash equilibrium when using the best-reply dynamics, as can be seen in the game on the right. If the game starts of the bottom left cell of the matrix, then the following best replay dynamics is possible: the row player moves one row up to the center row, the column player moves to the right column, the row player moves back to the bottom row, the column player moves back to the left column and so on. This obviously never converges to the unique pure Nash equilibrium of the game (which is the upper left cell in the payoff matrix).\n\nDominance (game theory)\n\n"}
{"id": "3015758", "url": "https://en.wikipedia.org/wiki?curid=3015758", "title": "Maximum entropy thermodynamics", "text": "Maximum entropy thermodynamics\n\nIn physics, maximum entropy thermodynamics (colloquially, \"MaxEnt\" thermodynamics) views equilibrium thermodynamics and statistical mechanics as inference processes. More specifically, MaxEnt applies inference techniques rooted in Shannon information theory, Bayesian probability, and the principle of maximum entropy. These techniques are relevant to any situation requiring prediction from incomplete or insufficient data (e.g., image reconstruction, signal processing, spectral analysis, and inverse problems). MaxEnt thermodynamics began with two papers by Edwin T. Jaynes published in the 1957 \"Physical Review\".\n\nCentral to the MaxEnt thesis is the principle of maximum entropy. It demands as given some partly specified model and some specified data related to the model. It selects a preferred probability distribution to represent the model. The given data state \"testable information\" about the probability distribution, for example particular expectation values, but are not in themselves sufficient to uniquely determine it. The principle states that one should prefer the distribution which maximizes the Shannon information entropy.\n\nThis is known as the Gibbs algorithm, having been introduced by J. Willard Gibbs in 1878, to set up statistical ensembles to predict the properties of thermodynamic systems at equilibrium. It is the cornerstone of the statistical mechanical analysis of the thermodynamic properties of equilibrium systems (see partition function).\n\nA direct connection is thus made between the equilibrium thermodynamic entropy \"S\", a state function of pressure, volume, temperature, etc., and the information entropy for the predicted distribution with maximum uncertainty conditioned only on the expectation values of those variables:\n\n\"k\", Boltzmann's constant, has no fundamental physical significance here, but is necessary to retain consistency with the previous historical definition of entropy by Clausius (1865) (see Boltzmann's constant).\n\nHowever, the MaxEnt school argue that the MaxEnt approach is a general technique of statistical inference, with applications far beyond this. It can therefore also be used to predict a distribution for \"trajectories\" Γ \"over a period of time\" by maximising:\n\nThis \"information entropy\" does \"not\" necessarily have a simple correspondence with thermodynamic entropy. But it can be used to predict features of nonequilibrium thermodynamic systems as they evolve over time.\n\nFor non-equilibrium scenarios, in an approximation that assumes local thermodynamic equilibrium, with the maximum entropy approach, the Onsager reciprocal relations and the Green-Kubo relations fall out directly. The approach also creates a theoretical framework for the study of some very special cases of far-from-equilibrium scenarios, making the derivation of the entropy production fluctuation theorem straightforward. For non-equilibrium processes, as is so for macroscopic descriptions, a general definition of entropy for microscopic statistical mechanical accounts is also lacking.\n\n\"Technical note\": For the reasons discussed in the article differential entropy, the simple definition of Shannon entropy ceases to be directly applicable for random variables with continuous probability distribution functions. Instead the appropriate quantity to maximise is the \"relative information entropy,\"\n\n\"H\" is the negative of the Kullback–Leibler divergence, or discrimination information, of \"m\"(\"x\") from \"p\"(\"x\"), where \"m\"(\"x\") is a prior invariant measure for the variable(s). The relative entropy \"H\" is always less than zero, and can be thought of as (the negative of) the number of bits of uncertainty lost by fixing on \"p\"(\"x\") rather than \"m\"(\"x\"). Unlike the Shannon entropy, the relative entropy \"H\" has the advantage of remaining finite and well-defined for continuous \"x\", and invariant under 1-to-1 coordinate transformations. The two expressions coincide for discrete probability distributions, if one can make the assumption that \"m\"(\"x\") is uniform - i.e. the principle of equal a-priori probability, which underlies statistical thermodynamics.\n\nAdherents to the MaxEnt viewpoint take a clear position on some of the conceptual/philosophical questions in thermodynamics. This position is sketched below.\n\nJaynes (1985, 2003, \"et passim\") discussed the concept of probability. According to the MaxEnt viewpoint, the probabilities in statistical mechanics are determined jointly by two factors: by respectively specified particular models for the underlying state space (e.g. Liouvillian phase space); and by respectively specified particular partial descriptions of the system (the macroscopic description of the system used to constrain the MaxEnt probability assignment). The probabilities are objective in the sense that, given these inputs, a uniquely defined probability distribution will result, the same for every rational investigator, independent of the subjectivity or arbitrary opinion of particular persons. The probabilities are epistemic in the sense that they are defined in terms of specified data and derived from those data by definite and objective rules of inference, the same for every rational investigator. Here the word epistemic, which refers to objective and impersonal scientific knowledge, the same for every rational investigator, is used in the sense that contrasts it with opiniative, which refers to the subjective or arbitrary beliefs of particular persons; this contrast was used by Plato and Aristotle, and stands reliable today.\n\nJaynes also used the word 'subjective' in this context because others have used it in this context. He accepted that in a sense, a state of knowledge has a subjective aspect, simply because it refers to thought, which is a mental process. But he emphasized that the principle of maximum entropy refers only to thought which is rational and objective, independent of the personality of the thinker. In general, from a philosophical viewpoint, the words 'subjective' and 'objective' are not contradictory; often an entity has both subjective and objective aspects. Jaynes explicitly rejected the criticism of some writers that, just because one can say that thought has a subjective aspect, thought is automatically non-objective. He explicitly rejected subjectivity as a basis for scientific reasoning, the epistemology of science; he required that scientific reasoning have a fully and strictly objective basis. Nevertheless, critics continue to attack Jaynes, alleging that his ideas are \"subjective\". One writer even goes so far as to label Jaynes' approach as \"ultrasubjectivist\", and to mention \"the panic that the term subjectivism created amongst physicists\".\n\nThe probabilities represent both the degree of knowledge and lack of information in the data and the model used in the analyst's macroscopic description of the system, and also what those data say about the nature of the underlying reality.\n\nThe fitness of the probabilities depends on whether the constraints of the specified macroscopic model are a sufficiently accurate and/or complete description of the system to capture all of the experimentally reproducible behaviour. This cannot be guaranteed, \"a priori\". For this reason MaxEnt proponents also call the method predictive statistical mechanics. The predictions can fail. But if they do, this is informative, because it signals the presence of new constraints needed to capture reproducible behaviour in the system, which had not been taken into account.\n\nThe thermodynamic entropy (at equilibrium) is a function of the state variables of the model description. It is therefore as \"real\" as the other variables in the model description. If the model constraints in the probability assignment are a \"good\" description, containing all the information needed to predict reproducible experimental results, then that includes all of the results one could predict using the formulae involving entropy from classical thermodynamics. To that extent, the MaxEnt \"S\" is as \"real\" as the entropy in classical thermodynamics.\n\nOf course, in reality there is only one real state of the system. The entropy is not a direct function of that state. It is a function of the real state only through the (subjectively chosen) macroscopic model description.\n\nThe Gibbsian ensemble idealises the notion of repeating an experiment again and again on \"different\" systems, not again and again on the \"same\" system. So long-term time averages and the ergodic hypothesis, despite the intense interest in them in the first part of the twentieth century, strictly speaking are not relevant to the probability assignment for the state one might find the system in.\n\nHowever, this changes if there is additional knowledge that the system is being prepared in a particular way some time before the measurement. One must then consider whether this gives further information which is still relevant at the time of measurement. The question of how 'rapidly mixing' different properties of the system are then becomes very much of interest. Information about some degrees of freedom of the combined system may become unusable very quickly; information about other properties of the system may go on being relevant for a considerable time.\n\nIf nothing else, the medium and long-run time correlation properties of the system are interesting subjects for experimentation in themselves. Failure to accurately predict them is a good indicator that relevant macroscopically determinable physics may be missing from the model.\n\nAccording to Liouville's theorem for Hamiltonian dynamics, the hyper-volume of a cloud of points in phase space remains constant as the system evolves. Therefore, the information entropy must also remain constant, if we condition on the original information, and then follow each of those microstates forward in time:\n\nHowever, as time evolves, that initial information we had becomes less directly accessible. Instead of being easily summarisable in the macroscopic description of the system, it increasingly relates to very subtle correlations between the positions and momenta of individual molecules. (Compare to Boltzmann's H-theorem.) Equivalently, it means that the probability distribution for the whole system, in 6N-dimensional phase space, becomes increasingly irregular, spreading out into long thin fingers rather than the initial tightly defined volume of possibilities.\n\nClassical thermodynamics is built on the assumption that entropy is a state function of the macroscopic variables—i.e., that none of the history of the system matters, so that it can all be ignored.\n\nThe extended, wispy, evolved probability distribution, which still has the initial Shannon entropy \"S\", should reproduce the expectation values of the observed macroscopic variables at time \"t\". However it will no longer necessarily be a maximum entropy distribution for that new macroscopic description. On the other hand, the new thermodynamic entropy \"S\" assuredly \"will\" measure the maximum entropy distribution, by construction. Therefore, we expect:\n\nAt an abstract level, this result implies that some of the information we originally had about the system has become \"no longer useful\" at a macroscopic level. At the level of the 6\"N\"-dimensional probability distribution, this result represents coarse graining—i.e., information loss by smoothing out very fine-scale detail.\n\nSome caveats should be considered with the above.\n\n1. Like all statistical mechanical results according to the MaxEnt school, this increase in thermodynamic entropy is only a \"prediction\". It assumes in particular that the initial macroscopic description contains all of the information relevant to predicting the later macroscopic state. This may not be the case, for example if the initial description fails to reflect some aspect of the preparation of the system which later becomes relevant. In that case the \"failure\" of a MaxEnt prediction tells us that there is something more which is relevant that we may have overlooked in the physics of the system.\n\nIt is also sometimes suggested that quantum measurement, especially in the decoherence interpretation, may give an apparently unexpected reduction in entropy per this argument, as it appears to involve macroscopic information becoming available which was previously inaccessible. (However, the entropy accounting of quantum measurement is tricky, because to get full decoherence one may be assuming an infinite environment, with an infinite entropy).\n\n2. The argument so far has glossed over the question of \"fluctuations\". It has also implicitly assumed that the uncertainty predicted at time \"t\" for the variables at time \"t\" will be much smaller than the measurement error. But if the measurements do meaningfully update our knowledge of the system, our uncertainty as to its state is reduced, giving a new \"S\" which is \"less\" than \"S\". (Note that if we allow ourselves the abilities of Laplace's demon, the consequences of this new information can also be mapped backwards, so our uncertainty about the dynamical state at time \"t\" is now \"also\" reduced from \"S\" to \"S\" ).\n\nWe know that \"S > S\"; but we can now no longer be certain that it is greater than \"S = S\". This then leaves open the possibility for fluctuations in \"S\". The thermodynamic entropy may go \"down\" as well as up. A more sophisticated analysis is given by the entropy Fluctuation Theorem, which can be established as a consequence of the time-dependent MaxEnt picture.\n\n3. As just indicated, the MaxEnt inference runs equally well in reverse. So given a particular final state, we can ask, what can we \"retrodict\" to improve our knowledge about earlier states? However the Second Law argument above also runs in reverse: given macroscopic information at time \"t\", we should expect it too to become less useful. The two procedures are time-symmetric. But now the information will become less and less useful at earlier and earlier times. (Compare with Loschmidt's paradox.) The MaxEnt inference would predict that the most probable origin of a currently low-entropy state would be as a spontaneous fluctuation from an earlier high entropy state. But this conflicts with what we know to have happened, namely that entropy has been increasing steadily, even back in the past.\n\nThe MaxEnt proponents' response to this would be that such a systematic failing in the prediction of a MaxEnt inference is a \"good\" thing. It means that there is thus clear evidence that some important physical information has been missed in the specification the problem. If it is correct that the dynamics \"are\" time-symmetric, it appears that we need to put in by hand a prior probability that initial configurations with a low thermodynamic entropy are more likely than initial configurations with a high thermodynamic entropy. This cannot be explained by the immediate dynamics. Quite possibly, it arises as a reflection of the evident time-asymmetric evolution of the universe on a cosmological scale (see arrow of time).\n\nThe Maximum Entropy thermodynamics has some important opposition, in part because of the relative paucity of published results from the MaxEnt school, especially with regard to new testable predictions far-from-equilibrium.\n\nThe theory has also been criticized in the grounds of internal consistency. For instance, Radu Balescu provides a strong criticism of the MaxEnt School and of Jaynes' work. Balescu states that Jaynes' and coworkers theory is based on a non-transitive evolution law that produces ambiguous results. Although some difficulties of the theory can be cured, the theory \"lacks a solid foundation\" and \"has not led to any new concrete result\".\n\nThough the maximum entropy approach is based directly on informational entropy, it is applicable to physics only when there is a clear physical definition of entropy. There is no clear unique general physical definition of entropy for non-equilibrium systems, which are general physical systems considered during a process rather than thermodynamic systems in their own internal states of thermodynamic equilibrium. It follows that the maximum entropy approach will not be applicable to non-equilibrium systems until there is found a clear physical definition of entropy. This problem is related to the fact that heat may be transferred from a hotter to a colder physical system even when local thermodynamic equilibrium does not hold so that neither system has a well defined temperature. Classical entropy is defined for a system in its own internal state of thermodynamic equilibrium, which is defined by state variables, with no non-zero fluxes, so that flux variables do not appear as state variables. But for a strongly non-equilibrium system, during a process, the state variables must include non-zero flux variables. Classical physical definitions of entropy do not cover this case, especially when the fluxes are large enough to destroy local thermodynamic equilibrium. In other words, for entropy for non-equilibrium systems in general, the definition will need at least to involve specification of the process including non-zero fluxes, beyond the classical static thermodynamic state variables. The 'entropy' that is maximized needs to be defined suitably for the problem at hand. If an inappropriate 'entropy' is maximized, a wrong result is likely. In principle, maximum entropy thermodynamics does not refer narrowly and only to classical thermodynamic entropy. It is about informational entropy applied to physics, explicitly depending on the data used to formulate the problem at hand. According to Attard, for physical problems analyzed by strongly non-equilibrium thermodynamics, several physically distinct kinds of entropy need to be considered, including what he calls second entropy. Attard writes: \"Maximizing the second entropy over the microstates in the given initial macrostate gives the most likely target macrostate.\". The physically defined second entropy can also be considered from an informational viewpoint.\n\n\n\n"}
{"id": "17271582", "url": "https://en.wikipedia.org/wiki?curid=17271582", "title": "Meta-IV (specification language)", "text": "Meta-IV (specification language)\n\nThe Meta-IV (pronounced like \"metaphor\") was an early version of the specification language of the Vienna Development Method formal method for the development of computer-based systems.\n\nOne of the first occurrences of Meta-IV in print appears to be\n\"Programming in the Meta-language: A Tutorial\".\nDines Bjørner used it in the very beginning of his tutorial as a footnote\n\nThis paper provides an informal introduction to the \"art\" of abstractly specifying software architectures using the \"VDM\" meta-language. A formal treatment of the semantics, as well as a BNF-like concrete syntax, of a large subset of the meta-language is given in [Jones 78a] following this paper.\nThe spirit of the Meta-IV specification language is well captured by the following passage\n\nWe stress here... that the meta-language is to be used, not for solving algorithmic problems (on a computer), but for specifying, in an implementation-independent way, the architecture (or models) of software. Instead of using informal English mixed with technical jargon, we offer you a very-high-level 'programming' language. We do not offer an interpreter or compiler for this meta-language. And we have absolutely no intention of ever wasting our time trying to mechanize this meta-language. We wish, as we have done in the past, and as we intend to continue doing in the future, to further develop the notation and to express notions in ways for which no mechanical interpreter system can ever be provided.\n\nVDM is a Method. The Meta-IV was the Specification language that accompanied the method, and the VDM-SL is the current standardized form of that language.\n\nSince the VDM-SL has become standardized, then one may use Meta-IV to denote the three specific Schools of\nthe VDM which existed (and to some extent still do) from the 1970s onwards:\n\nA brief account of these different Schools is given in the text \"Mathematical Approaches to Software Quality\".\n\nA comprehensive VDM Bibliography is also available.\n\nfounded by Dines Bjørner\nTo mention:\n\nfounded by Cliff Jones (computer scientist)\nTo mention:\n\nfounded by Mícheál Mac an Airchinnigh\nTo mention:\n\nThe first appearance of the name \"Irish School of the VDM\" occurs in a PhD Thesis:\nMac an Airchinnigh, Mícheál. Conceptual Models and Computing. Ph.D. Thesis. University of Dublin, Trinity College, Dublin, 1990, p. 41:\n\nThere is essential universal agreement on what constitutes the VDM. However, there are basically two major Schools of the VDM largely\ndistinguished by notational differences employed in the specification language \"Meta-IV\" — the Danish School and the English School.\"\n\nand further down on the same page\n\nThere is also the Polish School, which finds expression through the MetaSoft project (Blikle 1987, 1988, 1990). I will frequently need to distinguish between the style of notation and method that I use from those of the other Schools of the VDM. I \"presume\" to use the phrase 'the Irish School of the VDM' to draw that distinction.\nThe Thesis is available online.\n\nOther substantial works related to the School are also online.\n\nThe three Schools were brought under a common organizational structure called VDM Europe which held it first international conference in Brussels, Belgium, March 23–26, 1987. At the time funding was provided under the Esprit Programme of the European Union.\nMeetings were mostly held in the EU Commission buildings in Brussels, Belgium.\n\nVDM Europe eventually was dissolved in favor of Formal Methods Europe, founded in 1992. Minutes of the first meeting of FME are available online.\n\nList of the VDM and FME conferences (http://www.informatik.uni-trier.de/~ley/db/conf/fm/)\n\n\n"}
{"id": "25547443", "url": "https://en.wikipedia.org/wiki?curid=25547443", "title": "Nash blowing-up", "text": "Nash blowing-up\n\nIn algebraic geometry, a Nash blowing-up is a process in which, roughly speaking, each singular point is replaced by all the limiting positions of the tangent spaces at the non-singular points. Strictly speaking, if \"X\" is an algebraic variety of pure codimension \"r\" embedded in a smooth variety of dimension \"n\", formula_1 denotes the set of its singular points and formula_2 it is possible to define a map formula_3, where formula_4 is the Grassmannian of \"r\"-planes in \"n\"-space, by formula_5, where formula_6 is the tangent space of \"X\" at \"a\". Now, the closure of the image of this map together with the projection to \"X\" is called the Nash blowing-up of \"X\".\n\nAlthough (to emphasize its geometric interpretation) an embedding was used to define the Nash embedding it is possible to prove that it doesn't depend on it. \n\n\n\n"}
{"id": "3999801", "url": "https://en.wikipedia.org/wiki?curid=3999801", "title": "Partition regularity", "text": "Partition regularity\n\nIn combinatorics, a branch of mathematics, partition regularity is one notion of largeness for a collection of sets.\n\nGiven a set formula_1, a collection of subsets formula_2 is called \"partition regular\" if every set \"A\" in the collection has the property that, no matter how \"A\" is partitioned into finitely many subsets, at least one of the subsets will also belong to the collection. That is,\nfor any formula_3, and any finite partition formula_4, there exists an \"i\" ≤ \"n\", such that formula_5 belongs to formula_6. Ramsey theory is sometimes characterized as the study of which collections formula_6 are partition regular.\n\n\n\n"}
{"id": "22994", "url": "https://en.wikipedia.org/wiki?curid=22994", "title": "Paul Cohen", "text": "Paul Cohen\n\nPaul Joseph Cohen (April 2, 1934 – March 23, 2007) was an American mathematician. He is best known for his proofs that the continuum hypothesis and the axiom of choice are independent from Zermelo–Fraenkel set theory, for which he was awarded a Fields Medal.\n\nCohen was born in Long Branch, New Jersey, into a Jewish family that had immigrated to the United States from what is now Poland; he grew up in Brooklyn. He graduated in 1950, at age 16, from Stuyvesant High School in New York City.\n\nCohen next studied at the Brooklyn College from 1950 to 1953, but he left without earning his bachelor's degree when he learned that he could start his graduate studies at the University of Chicago with just two years of college. At Chicago, Cohen completed his master's degree in mathematics in 1954 and his Doctor of Philosophy degree in 1958, under supervision of the Professor of Mathematics, Antoni Zygmund. The title of his doctoral thesis was \"Topics in the Theory of Uniqueness of Trigonometrical Series\".\n\nOn June 2, 1995 Cohen received an honorary doctorate from the Faculty of Science and Technology at Uppsala University, Sweden \n\nCohen is noted for developing a mathematical technique called forcing, which he used to prove that neither the continuum hypothesis (CH) nor the axiom of choice can be proved from the standard Zermelo–Fraenkel axioms (ZF) of set theory. In conjunction with the earlier work of Gödel, this showed that both of these statements are logically independent of the ZF axioms: these statements can be neither proved nor disproved from these axioms. In this sense, the continuum hypothesis is undecidable, and it is the most widely known example of a natural statement that is independent from the standard ZF axioms of set theory.\n\nFor his result on the continuum hypothesis, Cohen won the Fields Medal in mathematics in 1966, and also the National Medal of Science in 1967. The Fields Medal that Cohen won continues to be the only Fields Medal to be awarded for a work in mathematical logic, as of 2018.\n\nApart from his work in set theory, Cohen also made many valuable contributions to analysis. He was awarded the Bôcher Memorial Prize in mathematical analysis in 1964 for his paper \"On a conjecture by Littlewood and idempotent measures\", and lends his name to the Cohen–Hewitt factorization theorem.\n\nCohen was a full professor of mathematics at Stanford University, where he supervised Peter Sarnak's graduate research, among those of other students. Cohen was an Invited Speaker at the ICM in 1962 in Stockholm and in 1966 in Moscow.\n\nAngus MacIntyre of the Queen Mary University of London stated about Cohen: \"He was dauntingly clever, and one would have had to be naive or exceptionally altruistic to put one's 'hardest problem' to the Paul I knew in the '60s.\" He went on to compare Cohen to Kurt Gödel, saying: \"Nothing more dramatic than their work has happened in the history of the subject.\" Gödel himself wrote a letter to Cohen in 1963, a draft of which stated, \"Let me repeat that it is really a delight to read your proof of the ind[ependence] of the cont[inuum] hyp[othesis]. I think that in all essential respects you have given the best possible proof & this does not happen frequently. Reading your proof had a similarly pleasant effect on me as seeing a really good play.\"\n\nWhile studying the continuum hypothesis, Cohen is quoted as saying in 1985 that he had \"had the feeling that people thought the problem was hopeless, since there was no new way of constructing models of set theory. Indeed, they thought you had to be slightly crazy even to think about the problem.\"\n\n\"A point of view which the author [Cohen] feels may eventually come to be accepted is that CH is obviously false. The main reason one accepts the axiom of infinity is probably that we feel it absurd to think that the process of adding only one set at a time can exhaust the entire universe. Similarly with the higher axioms of infinity. Now formula_1 is the cardinality of the set of countable ordinals, and this is merely a special and the simplest way of generating a higher cardinal. The set formula_2 [the continuum] is, in contrast, generated by a totally new and more powerful principle, namely the power set axiom. It is unreasonable to expect that any description of a larger cardinal which attempts to build up that cardinal from ideas deriving from the replacement axiom can ever reach formula_2.\n\nThus formula_2 is greater than formula_5, where formula_6, etc. This point of view regards formula_2 as an incredibly rich set given to us by one bold new axiom, which can never be approached by any piecemeal process of construction. Perhaps later generations will see the problem more clearly and express themselves more eloquently.\"\n\nAn \"enduring and powerful product\" of Cohen's work on the continuum hypothesis, and one that has been used by \"countless mathematicians\" is known as \"forcing\", and it is used to construct mathematical models to test a given hypothesis for truth or falsehood.\n\nShortly before his death, Cohen gave a lecture describing his solution to the problem of the continuum hypothesis at the Gödel centennial conference, in Vienna in 2006. A video of this lecture is now available online.\n\n\n\n\n"}
{"id": "56059449", "url": "https://en.wikipedia.org/wiki?curid=56059449", "title": "Polynomial mapping", "text": "Polynomial mapping\n\nIn algebra, a polynomial mapping formula_1 between vector spaces over an infinite field \"k\" is a polynomial in linear functionals with coefficients in \"W\"; i.e., it can be written as\nwhere formula_3 are linear functionals. For example, if formula_4, then it can also be expressed as formula_5 where formula_6 are (scalar-valued) polynomial functions on \"V\".\n\nWhen \"V\", \"W\" are finite-dimensional vector spaces and are viewed as algebraic varieties, then a polynomial mapping is precisely a morphism of algebraic varieties.\n\nOne fundamental outstanding question regarding polynomial mapping is the Jacobian conjecture, which concerns the sufficiency of a polynomial mapping to be invertible.\n\n\n"}
{"id": "25728240", "url": "https://en.wikipedia.org/wiki?curid=25728240", "title": "Polytope families", "text": "Polytope families\n"}
{"id": "25051489", "url": "https://en.wikipedia.org/wiki?curid=25051489", "title": "Pseudoisotopy theorem", "text": "Pseudoisotopy theorem\n\nIn mathematics, the pseudoisotopy theorem is a theorem of Jean Cerf's which refers to the connectivity of a group of diffeomorphisms of a manifold.\n\nGiven a differentiable manifold \"M\" (with or without boundary), a pseudo-isotopy diffeomorphism of \"M\" is a diffeomorphism of \"M\" × [0, 1] which restricts to the identity on formula_1.\n\nGiven formula_2 a pseudo-isotopy diffeomorphism, its restriction to formula_3 is a diffeomorphism formula_4 of \"M\". We say \"g\" is \"pseudo-isotopic to the identity\". One should think of a pseudo-isotopy as something that is almost an isotopy—the obstruction to \"ƒ\" being an isotopy of \"g\" to the identity is whether or not \"ƒ\" preserves the level-sets formula_5 for formula_6.\n\nCerf's theorem states that, provided \"M\" is simply-connected and dim(\"M\") ≥ 5, the group of pseudo-isotopy diffeomorphisms of \"M\" is connected. Equivalently, a diffeomorphism of \"M\" is isotopic to the identity if and only if it is pseudo-isotopic to the identity.\n\nThe starting point of the proof is to think of the height function as a 1-parameter family of smooth functions on \"M\" by considering the function formula_7. One then applies Cerf theory.\n"}
{"id": "2749048", "url": "https://en.wikipedia.org/wiki?curid=2749048", "title": "Quasi-invariant measure", "text": "Quasi-invariant measure\n\nIn mathematics, a quasi-invariant measure \"μ\" with respect to a transformation \"T\", from a measure space \"X\" to itself, is a measure which, roughly speaking, is multiplied by a numerical function of \"T\". An important class of examples occurs when \"X\" is a smooth manifold \"M\", \"T\" is a diffeomorphism of \"M\", and \"μ\" is any measure that locally is a measure with base the Lebesgue measure on Euclidean space. Then the effect of \"T\" on μ is locally expressible as multiplication by the Jacobian determinant of the derivative (pushforward) of \"T\".\n\nTo express this idea more formally in measure theory terms, the idea is that the Radon–Nikodym derivative of the transformed measure μ′ with respect to \"μ\" should exist everywhere; or that the two measures should be equivalent (i.e. mutually absolutely continuous):\n\nThat means, in other words, that \"T\" preserves the concept of a set of measure zero. Considering the whole equivalence class of measures \"ν\", equivalent to \"μ\", it is also the same to say that \"T\" preserves the class as a whole, mapping any such measure to another such. Therefore, the concept of quasi-invariant measure is the same as \"invariant measure class\".\n\nIn general, the 'freedom' of moving within a measure class by multiplication gives rise to cocycles, when transformations are composed.\n\nAs an example, Gaussian measure on Euclidean space R is not invariant under translation (like Lebesgue measure is), but is quasi-invariant under all translations.\n\nIt can be shown that if \"E\" is a separable Banach space and \"μ\" is a locally finite Borel measure on \"E\" that is quasi-invariant under all translations by elements of \"E\", then either dim(\"E\") < +∞ or \"μ\" is the trivial measure \"μ\" ≡ 0.\n\n"}
{"id": "291453", "url": "https://en.wikipedia.org/wiki?curid=291453", "title": "Renormalization", "text": "Renormalization\n\nRenormalization is a collection of techniques in quantum field theory, the statistical mechanics of fields, and the theory of self-similar geometric structures, that are used to treat infinities arising in calculated quantities by altering values of quantities to compensate for effects of their self-interactions. However, even if it were the case that no infinities arise in loop diagrams in quantum field theory, it can be shown that renormalization of mass and fields appearing in the original Lagrangian is necessary.\n\nFor example, an electron theory may begin by postulating an electron with an initial mass and charge. In quantum field theory a cloud of virtual particles, such as photons, positrons, and others surrounds and interacts with the initial electron. Accounting for the interactions of the surrounding particles (e.g. collisions at different energies) shows that the electron-system behaves as if it had a different mass and charge than initially postulated. Renormalization, in this example, mathematically replaces the initially postulated mass and charge of an electron with the experimentally observed mass and charge.  Mathematics and experiments prove that positrons and more massive particles like protons, exhibit \"precisely the same\" observed charge as the electron - even in the presence of much stronger interactions and more intense clouds of virtual particles. \n\nRenormalization specifies relationships between parameters in the theory when parameters describing large distance scales differ from parameters describing small distance scales. In high-energy particle accelerators like the CERN Large Hadron Collider the concept named pileup occurs when undesirable proton-proton collisions interact with data collection for simultaneous, nearby desirable measurements. Physically, the pileup of contributions from an infinity of scales involved in a problem may then result in further infinities. When describing space-time as a continuum, certain statistical and quantum mechanical constructions are not well-defined. To define them, or make them unambiguous, a continuum limit must carefully remove \"construction scaffolding\" of lattices at various scales. Renormalization procedures are based on the requirement that certain physical quantities (such as the mass and charge of an electron) equal observed (experimental) values. That is, the experimental value of the physical quantity yields practical applications, but due to their empirical nature the observed measurement represents areas of quantum field theory that require deeper derivation from theoretical bases.\n\nRenormalization was first developed in quantum electrodynamics (QED) to make sense of infinite integrals in perturbation theory. Initially viewed as a suspect provisional procedure even by some of its originators, renormalization eventually was embraced as an important and self-consistent actual mechanism of scale physics in several fields of physics and mathematics. \n\nToday, the point of view has shifted: on the basis of the breakthrough renormalization group insights of Nikolay Bogolyubov and Kenneth Wilson, the focus is on variation of physical quantities across contiguous scales, while distant scales are related to each other through \"effective\" descriptions. \"All scales\" are linked in a broadly systematic way, and the actual physics pertinent to each is extracted with the suitable specific computational techniques appropriate for each. Wilson clarified which variables of a system are crucial and which are redundant.\n\nRenormalization is distinct from regularization, another technique to control infinities by assuming the existence of new unknown physics at new scales.\n\nThe problem of infinities first arose in the classical electrodynamics of point particles in the 19th and early 20th century.\n\nThe mass of a charged particle should include the mass-energy in its electrostatic field (electromagnetic mass). Assume that the particle is a charged spherical shell of radius . The mass–energy in the field is\n\nwhich becomes infinite as . This implies that the point particle would have infinite inertia, making it unable to be accelerated. Incidentally, the value of that makes formula_2 equal to the electron mass is called the classical electron radius, which (setting formula_3 and restoring factors of and formula_4) turns out to be\n\nwhere formula_6 is the fine-structure constant, and formula_7 is the Compton wavelength of the electron.\n\nRenormalization: The total effective mass of a spherical charged particle includes the actual bare mass of the spherical shell (in addition to the mass mentioned above associated with its electric field). If the shell's bare mass is allowed to be negative, it might be possible to take a consistent point limit. This was called \"renormalization\", and Lorentz and Abraham attempted to develop a classical theory of the electron this way. This early work was the inspiration for later attempts at regularization and renormalization in quantum field theory.\n\nWhen calculating the electromagnetic interactions of charged particles, it is tempting to ignore the \"back-reaction\" of a particle's own field on itself. (Analogous to the back-EMF of circuit analysis.) But this back-reaction is necessary to explain the friction on charged particles when they emit radiation. If the electron is assumed to be a point, the value of the back-reaction diverges, for the same reason that the mass diverges, because the field is inverse-square.\n\nThe Abraham–Lorentz theory had a noncausal \"pre-acceleration.\" Sometimes an electron would start moving \"before\" the force is applied. This is a sign that the point limit is inconsistent.\n\nThe trouble was worse in classical field theory than in quantum field theory, because in quantum field theory a charged particle experiences Zitterbewegung due to interference with virtual particle-antiparticle pairs, thus effectively smearing out the charge over a region comparable to the Compton wavelength. In quantum electrodynamics at small coupling, the electromagnetic mass only diverges as the logarithm of the radius of the particle.\n\nWhen developing quantum electrodynamics in the 1930s, Max Born, Werner Heisenberg, Pascual Jordan, and Paul Dirac discovered that in perturbative corrections many integrals were divergent (see The problem of infinities).\n\nOne way of describing the perturbation theory corrections' divergences was discovered in 1947–49 by Hans Kramers, Hans Bethe, \nJulian Schwinger, Richard Feynman, and Shin'ichiro Tomonaga, and systematized by Freeman Dyson in 1949. The divergences appear in radiative corrections involving Feynman diagrams with closed \"loops\" of virtual particles in them.\n\nWhile virtual particles obey conservation of energy and momentum, they can have any energy and momentum, even one that is not allowed by the relativistic energy–momentum relation for the observed mass of that particle (that is, formula_8 is not necessarily the squared mass of the particle in that process, e.g. for a photon it could be nonzero). Such a particle is called off-shell. When there is a loop, the momentum of the particles involved in the loop is not uniquely determined by the energies and momenta of incoming and outgoing particles. A variation in the energy of one particle in the loop can be balanced by an equal and opposite change in the energy of another particle in the loop, without affecting the incoming and outgoing particles. Thus many variations are possible. So to find the amplitude for the loop process, one must integrate over \"all\" possible combinations of energy and momentum that could travel around the loop.\n\nThese integrals are often \"divergent\", that is, they give infinite answers. The divergences that are significant are the \"ultraviolet\" (UV) ones. An ultraviolet divergence can be described as one that comes from\n\nSo these divergences are short-distance, short-time phenomena.\n\nShown in the pictures at the right margin, there are exactly three one-loop divergent loop diagrams in quantum electrodynamics:\n\nThe three divergences correspond to the three parameters in the theory under consideration:\n\nThe second class of divergence called an infrared divergence, is due to massless particles, like the photon. Every process involving charged particles emits infinitely many coherent photons of infinite wavelength, and the amplitude for emitting any finite number of photons is zero. For photons, these divergences are well understood. For example, at the 1-loop order, the vertex function has both ultraviolet and \"infrared\" divergences. In contrast to the ultraviolet divergence, the infrared divergence does not require the renormalization of a parameter in the theory involved. The infrared divergence of the vertex diagram is removed by including a diagram similar to the vertex diagram with the following important difference: the photon connecting the two legs of the electron is cut and replaced by two on-shell (i.e. real) photons whose wavelengths tend to infinity; this diagram is equivalent to the bremsstrahlung process. This additional diagram must be included because there is no physical way to distinguish a zero-energy photon flowing through a loop as in the vertex diagram and zero-energy photons emitted through bremsstrahlung. From a mathematical point of view, the IR divergences can be regularized by assuming fractional differentiation w.r.t. a parameter, for example:\n\nis well defined at but is UV divergent; if we take the -th fractional derivative with respect to , we obtain the IR divergence\n\nso we can cure IR divergences by turning them into UV divergences.\n\nThe diagram in Figure 2 shows one of the several one-loop contributions to electron–electron scattering in QED. The electron on the left side of the diagram, represented by the solid line, starts out with 4-momentum and ends up with 4-momentum . It emits a virtual photon carrying to transfer energy and momentum to the other electron. But in this diagram, before that happens, it emits another virtual photon carrying 4-momentum , and it reabsorbs this one after emitting the other virtual photon. Energy and momentum conservation do not determine the 4-momentum uniquely, so all possibilities contribute equally and we must integrate.\n\nThis diagram's amplitude ends up with, among other things, a factor from the loop of\n\nThe various factors in this expression are gamma matrices as in the covariant formulation of the Dirac equation; they have to do with the spin of the electron. The factors of are the electric coupling constant, while the formula_12 provide a heuristic definition of the contour of integration around the poles in the space of momenta. The important part for our purposes is the dependency on of the three big factors in the integrand, which are from the propagators of the two electron lines and the photon line in the loop.\n\nThis has a piece with two powers of on top that dominates at large values of (Pokorski 1987, p. 122):\n\nThis integral is divergent and infinite, unless we cut it off at finite energy and momentum in some way.\n\nSimilar loop divergences occur in other quantum field theories.\n\nThe solution was to realize that the quantities initially appearing in the theory's formulae (such as the formula for the Lagrangian), representing such things as the electron's electric charge and mass, as well as the normalizations of the quantum fields themselves, did \"not\" actually correspond to the physical constants measured in the laboratory. As written, they were \"bare\" quantities that did not take into account the contribution of virtual-particle loop effects to \"the physical constants themselves\". Among other things, these effects would include the quantum counterpart of the electromagnetic back-reaction that so vexed classical theorists of electromagnetism. In general, these effects would be just as divergent as the amplitudes under consideration in the first place; so finite measured quantities would, in general, imply divergent bare quantities.\n\nTo make contact with reality, then, the formulae would have to be rewritten in terms of measurable, \"renormalized\" quantities. The charge of the electron, say, would be defined in terms of a quantity measured at a specific kinematic \"renormalization point\" or \"subtraction point\" (which will generally have a characteristic energy, called the \"renormalization scale\" or simply the energy scale). The parts of the Lagrangian left over, involving the remaining portions of the bare quantities, could then be reinterpreted as counterterms, involved in divergent diagrams exactly \"canceling out\" the troublesome divergences for other diagrams.\n\nFor example, in the Lagrangian of QED\n\nthe fields and coupling constant are really \"bare\" quantities, hence the subscript above. Conventionally the bare quantities are written so that the corresponding Lagrangian terms are multiples of the renormalized ones:\n\nGauge invariance, via a Ward–Takahashi identity, turns out to imply that we can renormalize the two terms of the covariant derivative piece\n\ntogether (Pokorski 1987, p. 115), which is what happened to ; it is the same as .\n\nA term in this Lagrangian, for example, the electron-photon interaction pictured in Figure 1, can then be written\n\nThe physical constant , the electron's charge, can then be defined in terms of some specific experiment: we set the renormalization scale equal to the energy characteristic of this experiment, and the first term gives the interaction we see in the laboratory (up to small, finite corrections from loop diagrams, providing such exotica as the high-order corrections to the magnetic moment). The rest is the counterterm. If the theory is \"renormalizable\" (see below for more on this), as it is in QED, the \"divergent\" parts of loop diagrams can all be decomposed into pieces with three or fewer legs, with an algebraic form that can be canceled out by the second term (or by the similar counterterms that come from and ).\n\nThe diagram with the counterterm's interaction vertex placed as in Figure 3 cancels out the divergence from the loop in Figure 2.\n\nHistorically, the splitting of the \"bare terms\" into the original terms and counterterms came before the renormalization group insight due to Kenneth Wilson. According to such renormalization group insights, detailed in the next section, this splitting is unnatural and actually unphysical, as all scales of the problem enter in continuous systematic ways.\n\nTo minimize the contribution of loop diagrams to a given calculation (and therefore make it easier to extract results), one chooses a renormalization point close to the energies and momenta exchanged in the interaction. However, the renormalization point is not itself a physical quantity: the physical predictions of the theory, calculated to all orders, should in principle be \"independent\" of the choice of renormalization point, as long as it is within the domain of application of the theory. Changes in renormalization scale will simply affect how much of a result comes from Feynman diagrams without loops, and how much comes from the remaining finite parts of loop diagrams. One can exploit this fact to calculate the effective variation of physical constants with changes in scale. This variation is encoded by beta-functions, and the general theory of this kind of scale-dependence is known as the renormalization group.\n\nColloquially, particle physicists often speak of certain physical \"constants\" as varying with the energy of interaction, though in fact, it is the renormalization scale that is the independent quantity. This \"running\" does, however, provide a convenient means of describing changes in the behavior of a field theory under changes in the energies involved in an interaction. For example, since the coupling in quantum chromodynamics becomes small at large energy scales, the theory behaves more like a free theory as the energy exchanged in an interaction becomes large---a phenomenon known as asymptotic freedom. Choosing an increasing energy scale and using the renormalization group makes this clear from simple Feynman diagrams; were this not done, the prediction would be the same, but would arise from complicated high-order cancellations.\n\nFor example,\n\nis ill-defined.\n\nTo eliminate the divergence, simply change lower limit of integral into and :\n\nMaking sure , then \n\nSince the quantity is ill-defined, in order to make this notion of canceling divergences precise, the divergences first have to be tamed mathematically using the theory of limits, in a process known as regularization (Weinberg, 1995).\n\nAn essentially arbitrary modification to the loop integrands, or \"regulator\", can make them drop off faster at high energies and momenta, in such a manner that the integrals converge. A regulator has a characteristic energy scale known as the cutoff; taking this cutoff to infinity (or, equivalently, the corresponding length/time scale to zero) recovers the original integrals.\n\nWith the regulator in place, and a finite value for the cutoff, divergent terms in the integrals then turn into finite but cutoff-dependent terms. After canceling out these terms with the contributions from cutoff-dependent counterterms, the cutoff is taken to infinity and finite physical results recovered. If physics on scales we can measure is independent of what happens at the very shortest distance and time scales, then it should be possible to get cutoff-independent results for calculations.\n\nMany different types of regulator are used in quantum field theory calculations, each with its advantages and disadvantages. One of the most popular in modern use is \"dimensional regularization\", invented by Gerardus 't Hooft and Martinus J. G. Veltman, which tames the integrals by carrying them into a space with a fictitious fractional number of dimensions. Another is \"Pauli–Villars regularization\", which adds fictitious particles to the theory with very large masses, such that loop integrands involving the massive particles cancel out the existing loops at large momenta.\n\nYet another regularization scheme is the \"lattice regularization\", introduced by Kenneth Wilson, which pretends that hyper-cubical lattice constructs our space-time with fixed grid size. This size is a natural cutoff for the maximal momentum that a particle could possess when propagating on the lattice. And after doing a calculation on several lattices with different grid size, the physical result is extrapolated to grid size 0, or our natural universe. This presupposes the existence of a scaling limit.\n\nA rigorous mathematical approach to renormalization theory is the so-called causal perturbation theory, where ultraviolet divergences are avoided from the start in calculations by performing well-defined mathematical operations only within the framework of distribution theory. The disadvantage of the method is the fact that the approach is quite technical and requires a high level of mathematical knowledge.\n\nJulian Schwinger discovered a relationship between zeta function regularization and renormalization, using the asymptotic relation:\n\nas the regulator . Based on this, he considered using the values of to get finite results. Although he reached inconsistent results, an improved formula studied by Hartle, J. Garcia, and based on the works by E. Elizalde includes the technique of the zeta regularization algorithm\n\nwhere the \"B\"'s are the Bernoulli numbers and\n\nSo every can be written as a linear combination of .\n\nOr simply using Abel–Plana formula we have for every divergent integral:\n\nvalid when , Here the zeta function is Hurwitz zeta function and Beta is a positive real number.\n\nThe \"geometric\" analogy is given by, (if we use rectangle method) to evaluate the integral so:\n\nUsing Hurwitz zeta regularization plus the rectangle method with step h (not to be confused with Planck's constant).\n\nThe logarithmic divergent integral has the regularization\n\nsince for the Harmonic series formula_28 in the limit formula_29 we must recover the series formula_30\n\nFor multi-loop integrals that will depend on several variables formula_31 we can make a change of variables to polar coordinates and then replace the integral over the angles formula_32 by a sum so we have only a divergent integral, that will depend on the modulus formula_33 and then we can apply the zeta regularization algorithm, the main idea for multi-loop integrals is to replace the factor formula_34 after a change to hyperspherical coordinates so the UV overlapping divergences are encoded in variable . In order to regularize these integrals one needs a regulator, for the case of multi-loop integrals, these regulator can be taken as\n\nso the multi-loop integral will converge for big enough using the Zeta regularization we can analytic continue the variable to the physical limit where and then regularize any UV integral, by replacing a divergent integral by a linear combination of divergent series, which can be regularized in terms of the negative values of the Riemann zeta function .\n\nThe early formulators of QED and other quantum field theories were, as a rule, dissatisfied with this state of affairs. It seemed illegitimate to do something tantamount to subtracting infinities from infinities to get finite answers.\n\nFreeman Dyson argued that these infinities are of a basic nature and cannot be eliminated by any formal mathematical procedures, such as the renormalization method.\n\nDirac's criticism was the most persistent. As late as 1975, he was saying:\n\nAnother important critic was Feynman. Despite his crucial role in the development of quantum electrodynamics, he wrote the following in 1985:\n\nWhile Dirac's criticism was based on the procedure of renormalization itself, Feynman's criticism was very different. Feynman was concerned that all field theories known in the 1960s had the property that the interactions become infinitely strong at short enough distance scales. This property called a Landau pole, made it plausible that quantum field theories were all inconsistent. In 1974, Gross, Politzer and Wilczek showed that another quantum field theory, quantum chromodynamics, does not have a Landau pole. Feynman, along with most others, accepted that QCD was a fully consistent theory.\n\nThe general unease was almost universal in texts up to the 1970s and 1980s. Beginning in the 1970s, however, inspired by work on the renormalization group and effective field theory, and despite the fact that Dirac and various others—all of whom belonged to the older generation—never withdrew their criticisms, attitudes began to change, especially among younger theorists. Kenneth G. Wilson and others demonstrated that the renormalization group is useful in statistical field theory applied to condensed matter physics, where it provides important insights into the behavior of phase transitions. In condensed matter physics, a \"physical\" short-distance regulator exists: matter ceases to be continuous on the scale of atoms. Short-distance divergences in condensed matter physics do not present a philosophical problem since the field theory is only an effective, smoothed-out representation of the behavior of matter anyway; there are no infinities since the cutoff is always finite, and it makes perfect sense that the bare quantities are cutoff-dependent.\n\nIf QFT holds all the way down past the Planck length (where it might yield to string theory, causal set theory or something different), then there may be no real problem with short-distance divergences in particle physics either; \"all\" field theories could simply be effective field theories. In a sense, this approach echoes the older attitude that the divergences in QFT speak of human ignorance about the workings of nature, but also acknowledges that this ignorance can be quantified and that the resulting effective theories remain useful.\n\nBe that as it may, Salam's remark in 1972 seems still relevant\n\nIn QFT, the value of a physical constant, in general, depends on the scale that one chooses as the renormalization point, and it becomes very interesting to examine the renormalization group running of physical constants under changes in the energy scale. The coupling constants in the Standard Model of particle physics vary in different ways with increasing energy scale: the coupling of quantum chromodynamics and the weak isospin coupling of the electroweak force tend to decrease, and the weak hypercharge coupling of the electroweak force tends to increase. At the colossal energy scale of 10 GeV (far beyond the reach of our current particle accelerators), they all become approximately the same size (Grotz and Klapdor 1990, p. 254), a major motivation for speculations about grand unified theory. Instead of being only a worrisome problem, renormalization has become an important theoretical tool for studying the behavior of field theories in different regimes.\n\nIf a theory featuring renormalization (e.g. QED) can only be sensibly interpreted as an effective field theory, i.e. as an approximation reflecting human ignorance about the workings of nature, then the problem remains of discovering a more accurate theory that does not have these renormalization problems. As Lewis Ryder has put it, \"In the Quantum Theory, these [classical] divergences do not disappear; on the contrary, they appear to get worse. And despite the comparative success of renormalisation theory, the feeling remains that there ought to be a more satisfactory way of doing things.\"\n\nFrom this philosophical reassessment, a new concept follows naturally: the notion of renormalizability. Not all theories lend themselves to renormalization in the manner described above, with a finite supply of counterterms and all quantities becoming cutoff-independent at the end of the calculation. If the Lagrangian contains combinations of field operators of high enough dimension in energy units, the counterterms required to cancel all divergences proliferate to infinite number, and, at first glance, the theory would seem to gain an infinite number of free parameters and therefore lose all predictive power, becoming scientifically worthless. Such theories are called \"nonrenormalizable\".\n\nThe Standard Model of particle physics contains only renormalizable operators, but the interactions of general relativity become nonrenormalizable operators if one attempts to construct a field theory of quantum gravity in the most straightforward manner (treating the metric in the Einstein–Hilbert Lagrangian as a perturbation about the Minkowski metric), suggesting that perturbation theory is useless in application to quantum gravity.\n\nHowever, in an effective field theory, \"renormalizability\" is, strictly speaking, a misnomer. In nonrenormalizable effective field theory, terms in the Lagrangian do multiply to infinity, but have coefficients suppressed by ever-more-extreme inverse powers of the energy cutoff. If the cutoff is a real, physical quantity—that is, if the theory is only an effective description of physics up to some maximum energy or minimum distance scale—then these additional terms could represent real physical interactions. Assuming that the dimensionless constants in the theory do not get too large, one can group calculations by inverse powers of the cutoff, and extract approximate predictions to finite order in the cutoff that still have a finite number of free parameters. It can even be useful to renormalize these \"nonrenormalizable\" interactions.\n\nNonrenormalizable interactions in effective field theories rapidly become weaker as the energy scale becomes much smaller than the cutoff. The classic example is the Fermi theory of the weak nuclear force, a nonrenormalizable effective theory whose cutoff is comparable to the mass of the W particle. This fact may also provide a possible explanation for \"why\" almost all of the particle interactions we see are describable by renormalizable theories. It may be that any others that may exist at the GUT or Planck scale simply become too weak to detect in the realm we can observe, with one exception: gravity, whose exceedingly weak interaction is magnified by the presence of the enormous masses of stars and planets.\n\nIn actual calculations, the counterterms introduced to cancel the divergences in Feynman diagram calculations beyond tree level must be \"fixed\" using a set of \" renormalisation conditions\". The common renormalization schemes in use include:\n\nA deeper understanding of the physical meaning and generalization of the\nrenormalization process, which goes beyond the dilatation group of conventional \"renormalizable\" theories, came from condensed matter physics. Leo P. Kadanoff's paper in 1966 proposed the \"block-spin\" renormalization group. The \"blocking idea\" is a way to define the components of the theory at large distances as aggregates of components at shorter distances.\n\nThis approach covered the conceptual point and was given full computational substance in the extensive important contributions of Kenneth Wilson. The power of Wilson's ideas was demonstrated by a constructive iterative renormalization solution of a long-standing problem, the Kondo problem, in 1974, as well as the preceding seminal developments of his new method in the theory of second-order phase transitions and critical phenomena in 1971. He was awarded the Nobel prize for these decisive contributions in 1982.\n\nIn more technical terms, let us assume that we have a theory described\nby a certain function formula_36 of the state variables\nformula_37 and a certain set of coupling constants\nformula_38. This function may be a partition function,\nan action, a Hamiltonian, etc. It must contain the\nwhole description of the physics of the system.\n\nNow we consider a certain blocking transformation of the state\nvariables formula_39,\nthe number of formula_40 must be lower than the number of\nformula_41. Now let us try to rewrite the formula_36\nfunction \"only\" in terms of the formula_40. If this is achievable by a\ncertain change in the parameters, formula_44, then the theory is said to be\nrenormalizable.\nThe most important\ninformation in the RG flow is its fixed points. The possible\nmacroscopic states of the system, at a large scale, are given by this\nset of fixed points. If these fixed points correspond to free field theory,\nthe theory is said to exhibit quantum triviality. Numerous fixed points appear in the study of \nlattice Higgs theories, but the nature of the quantum field theories associated with these remains \nan open question.\n\n\n\n\n\n"}
{"id": "11532414", "url": "https://en.wikipedia.org/wiki?curid=11532414", "title": "Richard Lipton", "text": "Richard Lipton\n\nRichard Jay Lipton (born September 6, 1946) is an American-British computer scientist who has worked in computer science theory, cryptography, and DNA computing. Lipton is Associate Dean of Research, Professor, and the Frederick G. Storey Chair in Computing in the College of Computing at the Georgia Institute of Technology.\n\nIn 1968, Lipton received his undergraduate degree in mathematics from Case Western Reserve University. In 1973, he received his Ph.D. from Carnegie Mellon University; his dissertation, supervised by David Parnas, is entitled \"On Synchronization Primitive Systems\". After graduating, Lipton taught at Yale 1973–1978, at Berkeley 1978–1980, and then at Princeton 1980–2000. Since 2000, Lipton has been at Georgia Tech. While at Princeton, Lipton worked in the field of DNA computing. Since 1996, Lipton has been the chief consulting scientist at Telcordia.\n\nIn 1980, along with Richard M. Karp, Lipton proved that if SAT can be solved by Boolean circuits with a polynomial number of logic gates, then the polynomial hierarchy collapses to its second level. \n\nShowing that a program P has some property is a simple process if the actions inside the program are uninterruptible. However, when the action is interruptible, Lipton showed that through a type of reduction and analysis, it can be shown that the reduced program has that property if and only if the original program has the property. If the reduction is done by treating interruptible operations as one large uninterruptible action, even with these relaxed conditions properties can be proven for a program P. Thus, correctness proofs of a parallel system can often be greatly simplified.\n\nLipton studied and created database security models on how and when to restrict the queries made by users of a database such that private or secret information will not be leaked. Even when the user is restricted to only read operations on a database, secure information could be at risk. For example, querying a database of campaign donations could allow the user to discover the individual donations to political candidates or organizations. If given access to averages of data and unrestricted query access, a user could exploit the properties of those averages to gain illicit information. These queries are considered to have large \"overlap\" creating the insecurity. By bounding the \"overlap\" and number of queries, a secure database can be achieved.\n\nRichard Lipton with Andrew Tomkins introduced a randomized online interval scheduling algorithm, the 2-size version being strongly competitive, and the \"k\"-size version achieving O(logformula_1), as well as demonstrating a theoretical lower-bound of O(logformula_2). This algorithm uses a private-coin for randomization and a \"virtual\" choice to fool a medium adversary.\n\nBeing presented with an event the user must decide whether or not to include the event in the schedule. The 2-size virtual algorithm is described by how it reacts to 1-interval or \"k\"-intervals being presented by the adversary:\n\nAgain, this 2-size algorithm is shown to be strongly-competitive. The generalized \"k\"-size algorithm which is similar to the 2-size algorithm is then shown to be O(logformula_1)-competitive.\n\nLipton showed that randomized testing can be provably useful, given the problem satisfied certain properties. Proving correctness of a program is one of the most important problems presented in computer science. Typically in randomized testing, in order to attain a 1/1000 chance of an error, 1000 tests must be run. However Lipton shows that if a problem has \"easy\" sub-parts, repeated black-box testing can attain \"c\" error rate, with \"c\" a constant less than 1 and \"r\" being the number of tests. Therefore, the probability of error goes to zero exponentially fast as \"r\" grows.\n\nThis technique is useful to check the correctness of many types of problems. \n\nIn the area of game theory, more specifically on non-cooperative game, Lipton together with E.Markakis and A.Mehta proved the existence of epsilon-equilibrium strategies with support logarithmic in the number of pure strategy. Furthermore, the payoff of such strategies can epsilon-approximate the payoffs of exact Nash equilibrium. The limited size (logarithmic) of support provides a natural quasi-polynomial algorithm of computing an epsilon-equilibrium.\n\nLipton and J.Naughton presented an adaptive random sampling algorithm for database querying which is applicable to any query for which answer to the query can be partitioned into disjoint subsets. Compared with most sampling estimation algorithms that statically determines the number of samples needed, the algorithm they proposed decides the number of samples based on the size of samples and tends to keep the running time constant rather than the number of samples.\n\nDeMillo, Lipton and Perlis criticized the idea of formal verification of programs and argued that\n\nChandra, Furst and Lipton generalized the notion of two-party communication protocols to multi-party communication protocols. They proposed a model in which a collection of processes (formula_5) have access to a set of integers (formula_6, formula_7) except one of them, so that formula_8 is denied access to formula_9. These processes are allowed to communicate in order to arrive at a consensus on a predicate. They studied this model’s communication complexity, defined as the number of bits broadcast among all the processes. As an example, they studied the complexity of a \"k\"-party protocol for Exactly-\"N\" (do all formula_9’s sum up to N?), and obtained a lower bound using the tiling method. They further applied this model to study general branching programs and obtained a time lower bound for constant-space branching programs that compute Exactly-\"N\".\n\nWe have no way to prove that Boolean satisfiability problem (often abbreviated as SAT), which is NP-complete, requires exponential (or at least super-polynomial) time (this is the famous P versus NP problem), or linear (or at least super-logarithmic) space to solve. However, in the context of space–time tradeoff, one can prove that SAT cannot be computed if we apply constraints to both time and space. L. Fortnow, Lipton, D. van Melkebeek, and A. Viglas proved that SAT cannot be computed by a Turing machine that takes at most O(\"n\") steps and at most O(\"n\") cells of its read-write tapes.\n\n\n\n\n"}
{"id": "89374", "url": "https://en.wikipedia.org/wiki?curid=89374", "title": "Sequential logic", "text": "Sequential logic\n\nIn digital circuit theory, sequential logic is a type of logic circuit whose output depends not only on the present value of its input signals but on the sequence of past inputs, the input history as well. This is in contrast to \"combinational logic\", whose output is a function of only the present input. That is, sequential logic has \"state\" (\"memory\") while combinational logic does not.\n\nSequential logic is used to construct finite state machines, a basic building block in all digital circuitry. Virtually all circuits in practical digital devices are a mixture of combinational and sequential logic.\n\nA familiar example of a device with sequential logic is a television set with \"channel up\" and \"channel down\" buttons. Pressing the \"up\" button gives the television an input telling it to switch to the next channel above the one it is currently receiving. If the television is on channel 5, pressing \"up\" switches it to receive channel 6. However, if the television is on channel 8, pressing \"up\" switches it to channel \"9\". In order for the channel selection to operate correctly, the television must be aware of which channel it is currently receiving, which was determined by past channel selections. The television stores the current channel as part of its \"state\". When a \"channel up\" or \"channel down\" input is given to it, the sequential logic of the channel selection circuitry calculates the new channel from the input and the current channel.\n\nDigital sequential logic circuits are divided into synchronous and asynchronous types. In synchronous sequential circuits, the state of the device changes only at discrete times in response to a clock signal. In asynchronous circuits the state of the device can change at any time in response to changing inputs.\n\nNearly all sequential logic today is \"clocked\" or \"synchronous\" logic. In a synchronous circuit, an electronic oscillator called a \"clock\" (or clock generator) generates a sequence of repetitive pulses called the \"clock signal\" which is distributed to all the memory elements in the circuit. The basic memory element in sequential logic is the flip-flop. The output of each flip-flop only changes when triggered by the clock pulse, so changes to the logic signals throughout the circuit all begin at the same time, at regular intervals, synchronized by the clock. \nThe output of all the storage elements (flip-flops) in the circuit at any given time, the binary data they contain, is called the \"state\" of the circuit. The state of a synchronous circuit only changes on clock pulses. At each cycle, the next state is determined by the current state and the value of the input signals when the clock pulse occurs.\n\nThe main advantage of synchronous logic is its simplicity. The logic gates which perform the operations on the data require a finite amount of time to respond to changes to their inputs. This is called \"propagation delay\". The interval between clock pulses must be long enough so that all the logic gates have time to respond to the changes and their outputs \"settle\" to stable logic values, before the next clock pulse occurs. As long as this condition is met (ignoring certain other details) the circuit is guaranteed to be stable and reliable. This determines the maximum operating speed of a synchronous circuit.\n\nSynchronous logic has two main disadvantages:\n\nAsynchronous sequential logic is not synchronized by a clock signal; the outputs of the circuit change directly in response to changes in inputs. The advantage of asynchronous logic is that it can be faster than synchronous logic, because the circuit doesn't have to wait for a clock signal to process inputs. The speed of the device is potentially limited only by the propagation delays of the logic gates used.\n\nHowever, asynchronous logic is more difficult to design and is subject to problems not encountered in synchronous designs. The main problem is that digital memory elements are sensitive to the order that their input signals arrive; if two signals arrive at a flip-flop or latch at almost the same time, which state the circuit goes into can depend on which signal gets to the gate first. Therefore, the circuit can go into the wrong state, depending on small differences in the propagation delays of the logic gates. This is called a race condition. This problem is not as severe in synchronous circuits because the outputs of the memory elements only change at each clock pulse. The interval between clock signals is designed to be long enough to allow the outputs of the memory elements to \"settle\" so they are not changing when the next clock comes. Therefore, the only timing problems are due to \"asynchronous inputs\"; inputs to the circuit from other systems which are not synchronized to the clock signal.\n\nAsynchronous sequential circuits are typically used only in a few critical parts of otherwise synchronous systems where speed is at a premium, such as parts of microprocessors and digital signal processing circuits.\n\nThe design of asynchronous logic uses different mathematical models and techniques from synchronous logic, and is an active area of research.\n\n\n"}
{"id": "3875683", "url": "https://en.wikipedia.org/wiki?curid=3875683", "title": "Simple (abstract algebra)", "text": "Simple (abstract algebra)\n\nIn mathematics, the term simple is used to describe an algebraic structure which in some sense cannot be divided by a smaller structure of the same type. Put another way, an algebraic structure is simple if the kernel of every homomorphism is either the whole structure or a single element. Some examples are:\n\n\nThe general pattern is that the structure admits no non-trivial congruence relations.\n\nThe term is used differently in semigroup theory. A semigroup is said to be \"simple\" if it has no nontrivial\nideals, or equivalently, if Green's relation \"J\" is\nthe universal relation. Not every congruence on a semigroup is associated with an ideal, so a simple semigroup may\nhave nontrivial congruences. A semigroup with no nontrivial congruences is called \"congruence simple\".\n\n"}
{"id": "2446477", "url": "https://en.wikipedia.org/wiki?curid=2446477", "title": "Simplicial manifold", "text": "Simplicial manifold\n\nIn physics, the term simplicial manifold commonly refers to one of several loosely defined objects, commonly appearing in the study of Regge calculus. These objects combine attributes of a simplex with those of a manifold. There is no standard usage of this term in mathematics, and so the concept can refer to a triangulation in topology, or a piecewise linear manifold, or one of several different functors from either the category of sets or the category of simplicial sets to the category of manifolds.\n\nA simplicial manifold is a simplicial complex for which the geometric realization is homeomorphic to a topological manifold. This is essentially the concept of a triangulation in topology. This can mean simply that a neighborhood of each vertex (i.e. the set of simplices that contain that point as a vertex) is homeomorphic to a \"n\"-dimensional ball.\n\nA simplicial manifold is also a simplicial object in the category of manifolds. This is a special case of a simplicial space in which, for each \"n\", the space of \"n\"-simplices is a manifold.\n\nFor example, if \"G\" is a Lie group, then the simplicial nerve of \"G\" has the manifold formula_1 as its space of \"n\"-simplices. More generally, \"G\" can be a Lie groupoid.\n"}
{"id": "142207", "url": "https://en.wikipedia.org/wiki?curid=142207", "title": "Singular value decomposition", "text": "Singular value decomposition\n\nIn linear algebra, the singular-value decomposition (SVD) is a factorization of a real or complex matrix. It is the generalization of the eigendecomposition of a positive semidefinite normal matrix (for example, a symmetric matrix with positive eigenvalues) to any formula_1 matrix via an extension of the polar decomposition. It has many useful applications in signal processing and statistics.\n\nFormally, the singular-value decomposition of an formula_1 real or complex matrix formula_3 is a factorization of the form formula_4, where formula_5 is an formula_6 real or complex unitary matrix, formula_7 is an formula_1 rectangular diagonal matrix with non-negative real numbers on the diagonal, and formula_9 is an formula_10 real or complex unitary matrix. The diagonal entries formula_11 of formula_7 are known as the singular values of formula_3. The columns of formula_5 and the columns of formula_9 are called the left-singular vectors and right-singular vectors of formula_3, respectively.\n\nThe singular-value decomposition can be computed using the following observations:\n\nApplications that employ the SVD include computing the pseudoinverse, least squares fitting of data, multivariable control, matrix approximation, and determining the rank, range and null space of a matrix.\n\nSuppose is a matrix whose entries come from the field , which is either the field of real numbers or the field of complex numbers. Then there exists a factorization, called a 'singular value decomposition' of , of the form\n\nwhere \n\nThe diagonal entries of are known as the singular values of . A common convention is to list the singular values in descending order. In this case, the diagonal matrix, , is uniquely determined by (though not the matrices and if is not square, see below).\n\nIn the special, yet common case when is an real square matrix with positive determinant: , and are real matrices as well. can be regarded as a scaling matrix, and can be viewed as rotation matrices. Thus the expression can be intuitively interpreted as a composition of three geometrical transformations: a rotation or reflection, a scaling, and another rotation or reflection. For instance, the figure explains how a shear matrix can be described as such a sequence.\n\nUsing the polar decomposition theorem, we can also consider as the composition of a stretch (positive definite matrix ) with eigenvalue scale factors along the orthogonal eigenvectors of , followed by a single rotation (unitary matrix ). If the rotation is done first, , then is the same and has the same eigenvalues, but is stretched along different (post-rotated) directions. This shows that the SVD is a generalization of the eigenvalue decomposition of pure stretches in orthogonal directions (symmetric matrix ) to arbitrary matrices () which both stretch and rotate.\n\nAs shown in the figure, the singular values can be interpreted as the semiaxis of an ellipse in 2D. This concept can be generalized to -dimensional Euclidean space, with the singular values of any square matrix being viewed as the semiaxis of an -dimensional ellipsoid. Similarly, the singular values of any matrix can be viewed as the semiaxis of an -dimensional ellipsoid in -dimensional space, for example as an ellipse in a (tilted) 2D plane in a 3D space. See below for further details.\n\nSince and are unitary, the columns of each of them form a set of orthonormal vectors, which can be regarded as basis vectors. The matrix maps the basis vector to the stretched unit vector (see below for further details). By the definition of a unitary matrix, the same is true for their conjugate transposes and , except the geometric interpretation of the singular values as stretches is lost. In short, the columns of , and are orthonormal bases. When the formula_3 is a normal matrix, and reduce to the unitary used to diagonalize formula_3. However, when formula_3 is not normal but still diagonalizable, its eigendecomposition and singular value decomposition are distinct.\n\nBecause and are unitary, we know that the columns of yield an orthonormal basis of and the columns of yield an orthonormal basis of (with respect to the standard scalar products on these spaces).\n\nThe linear transformation\n\nhas a particularly simple description with respect to these orthonormal bases: we have\n\nwhere is the -th diagonal entry of , and for .\n\nThe geometric content of the SVD theorem can thus be summarized as follows: for every linear map one can find orthonormal bases of and such that maps the -th basis vector of to a non-negative multiple of the -th basis vector of , and sends the left-over basis vectors to zero. With respect to these bases, the map is therefore represented by a diagonal matrix with non-negative real diagonal entries.\n\nTo get a more visual flavour of singular values and SVD factorization — at least when working on real vector spaces — consider the sphere of radius one in . The linear map maps this sphere onto an ellipsoid in . Non-zero singular values are simply the lengths of the semi-axes of this ellipsoid. Especially when , and all the singular values are distinct and non-zero, the SVD of the linear map can be easily analysed as a succession of three consecutive moves: consider the ellipsoid and specifically its axes; then consider the directions in sent by onto these axes. These directions happen to be mutually orthogonal. Apply first an isometry sending these directions to the coordinate axes of . On a second move, apply an endomorphism diagonalized along the coordinate axes and stretching or shrinking in each direction, using the semi-axes lengths of as stretching coefficients. The composition then sends the unit-sphere onto an ellipsoid isometric to . To define the third and last move , apply an isometry to this ellipsoid so as to carry it over . As can be easily checked, the composition coincides with .\n\nConsider the matrix\n\nA singular-value decomposition of this matrix is given by \n\nNotice is zero outside of the diagonal (grey italics) and one diagonal element is zero (red bold). Furthermore, because the matrices and are unitary, multiplying by their respective conjugate transposes yields identity matrices, as shown below. In this case, because and are real valued, each is an orthogonal matrix.\n\nThis particular singular-value decomposition is not unique. Choosing formula_27 such that\n\nis also a valid singular-value decomposition.\n\nA non-negative real number is a singular value for if and only if there exist unit-length vectors formula_29 in \"K\" and formula_30 in \"K\" such that\n\nThe vectors formula_29 and formula_30 are called left-singular and right-singular vectors for , respectively.\n\nIn any singular-value decomposition\n\nthe diagonal entries of are equal to the singular values of . The first columns of and are, respectively, left- and right-singular vectors for the corresponding singular values. Consequently, the above theorem implies that:\n\nA singular value for which we can find two left (or right) singular vectors that are linearly independent is called \"degenerate\". If formula_35 and formula_36 are two left-singular vectors which both correspond to the singular value σ, then any normalized linear combination of the two vectors is also a left-singular vector corresponding to the singular value σ. The similar statement is true for right-singular vectors. The number of independent left and right-singular vectors coincides, and these singular vectors appear in the same columns of and corresponding to diagonal elements of all with the same value σ.\n\nAs an exception, the left and right-singular vectors of singular value 0 comprise all unit vectors in the kernel and cokernel, respectively, of , which by the rank–nullity theorem cannot be the same dimension if . Even if all singular values are nonzero, if then the cokernel is nontrivial, in which case is padded with orthogonal vectors from the cokernel. Conversely, if , then is padded by orthogonal vectors from the kernel. However, if the singular value of 0 exists, the extra columns of or already appear as left or right-singular vectors.\n\nNon-degenerate singular values always have unique left- and right-singular vectors, up to multiplication by a unit-phase factor \"e\" (for the real case up to a sign). Consequently, if all singular values of a square matrix are non-degenerate and non-zero, then its singular value decomposition is unique, up to multiplication of a column of by a unit-phase factor and simultaneous multiplication of the corresponding column of by the same unit-phase factor.\nIn general, the SVD is unique up to arbitrary unitary transformations applied uniformly to the column vectors of both and spanning the subspaces of each singular value, and up to arbitrary unitary transformations on vectors of and spanning the kernel and cokernel, respectively, of .\n\nThe singular-value decomposition is very general in the sense that it can be applied to any matrix whereas eigenvalue decomposition can only be applied to diagonalizable matrices. Nevertheless, the two decompositions are related.\n\nGiven an SVD of , as described above, the following two relations hold:\n\nThe right-hand sides of these relations describe the eigenvalue decompositions of the left-hand sides. Consequently:\n\nIn the special case that is a normal matrix, which by definition must be square, the spectral theorem says that it can be unitarily diagonalized using a basis of eigenvectors, so that it can be written for a unitary matrix and a diagonal matrix . When is also positive semi-definite, the decomposition is also a singular-value decomposition. Otherwise, it can be recast as an SVD by moving the phase of each to either its corresponding or . The natural connection of the SVD to non-normal matrices is through the polar decomposition theorem: , where is positive semidefinite and normal, and is unitary.\n\nThus while related, the eigenvalue decomposition and SVD differ except for positive semi-definite normal matrices : the eigenvalue decomposition is where is not necessarily unitary and is not necessarily positive semi-definite, while the SVD is where is diagonal and positive semi-definite, and and are unitary matrices that are not necessarily related except through the matrix . While only non-defective square matrices have an eigenvalue decomposition, any formula_1 matrix has a SVD.\n\nThe singular-value decomposition can be used for computing the pseudoinverse of a matrix. Indeed, the pseudoinverse of the matrix with singular-value decomposition is\n\nwhere is the pseudoinverse of , which is formed by replacing every non-zero diagonal entry by its reciprocal and transposing the resulting matrix. The pseudoinverse is one way to solve linear least squares problems.\n\nA set of homogeneous linear equations can be written as for a matrix and vector . A typical situation is that is known and a non-zero is to be determined which satisfies the equation. Such an belongs to 's null space and is sometimes called a (right) null vector of . The vector can be characterized as a right-singular vector corresponding to a singular value of that is zero. This observation means that if is a square matrix and has no vanishing singular value, the equation has no non-zero as a solution. It also means that if there are several vanishing singular values, any linear combination of the corresponding right-singular vectors is a valid solution. Analogously to the definition of a (right) null vector, a non-zero satisfying , with denoting the conjugate transpose of , is called a left null vector of .\n\nA total least squares problem refers to determining the vector which minimizes the 2-norm of a vector under the constraint . The solution turns out to be the right-singular vector of corresponding to the smallest singular value.\n\nAnother application of the SVD is that it provides an explicit representation of the range and null space of a matrix . The right-singular vectors corresponding to vanishing singular values of span the null space of and the left-singular vectors corresponding to the non-zero singular values of span the range of . E.g., in the above example the null space is spanned by the last two columns of and the range is spanned by the first three columns of .\n\nAs a consequence, the rank of equals the number of non-zero singular values which is the same as the number of non-zero diagonal elements in . In numerical linear algebra the singular values can be used to determine the \"effective rank\" of a matrix, as rounding error may lead to small but non-zero singular values in a rank deficient matrix. Singular values beyond a significant gap are assumed to be numerically equivalent to zero.\n\nSome practical applications need to solve the problem of approximating a matrix with another matrix formula_40, said truncated, which has a specific rank . In the case that the approximation is based on minimizing the Frobenius norm of the difference between and formula_40 under the constraint that formula_42 it turns out that the solution is given by the SVD of , namely\n\nwhere formula_44 is the same matrix as except that it contains only the largest singular values (the other singular values are replaced by zero). This is known as the Eckart–Young theorem, as it was proved by those two authors in 1936 (although it was later found to have been known to earlier authors; see ).\n\nThe SVD can be thought of as decomposing a matrix into a weighted, ordered sum of separable matrices. By separable, we mean that a matrix can be written as an outer product of two vectors , or, in coordinates, formula_45. Specifically, the matrix can be decomposed as:\n\nHere and are the -th columns of the corresponding SVD matrices, are the ordered singular values, and each is separable. The SVD can be used to find the decomposition of an image processing filter into separable horizontal and vertical filters. Note that the number of non-zero is exactly the rank of the matrix.\n\nSeparable models often arise in biological systems, and the SVD factorization is useful to analyze such systems. For example, some visual area V1 simple cells' receptive fields can be well described by a Gabor filter in the space domain multiplied by a modulation function in the time domain. Thus, given a linear filter evaluated through, for example, reverse correlation, one can rearrange the two spatial dimensions into one dimension, thus yielding a two-dimensional filter (space, time) which can be decomposed through SVD. The first column of in the SVD factorization is then a Gabor while the first column of represents the time modulation (or vice versa). One may then define an index of separability,\n\nwhich is the fraction of the power in the matrix M which is accounted for by the first separable matrix in the decomposition.\n\nIt is possible to use the SVD of a square matrix to determine the orthogonal matrix closest to . The closeness of fit is measured by the Frobenius norm of . The solution is the product . This intuitively makes sense because an orthogonal matrix would have the decomposition where is the identity matrix, so that if then the product amounts to replacing the singular values with ones.\n\nA similar problem, with interesting applications in shape analysis, is the orthogonal Procrustes problem, which consists of finding an orthogonal matrix which most closely maps to . Specifically,\n\nwhere formula_49 denotes the Frobenius norm.\n\nThis problem is equivalent to finding the nearest orthogonal matrix to a given matrix .\n\nThe Kabsch algorithm (called Wahba's problem in other fields) uses SVD to compute the optimal rotation (with respect to least-squares minimization) that will align a set of points with a corresponding set of points. It is used, among other applications, to compare the structures of molecules.\n\nThe SVD and pseudoinverse have been successfully applied to signal processing, image processing and big data, e.g., in genomic signal processing.\n\nThe SVD is also applied extensively to the study of linear inverse problems, and is useful in the analysis of regularization methods such as that of Tikhonov. It is widely used in statistics where it is related to principal component analysis and to Correspondence analysis, and in signal processing and pattern recognition. It is also used in output-only modal analysis, where the non-scaled mode shapes can be determined from the singular vectors. Yet another usage is latent semantic indexing in natural language text processing.\n\nThe SVD also plays a crucial role in the field of quantum information, in a form often referred to as the Schmidt decomposition. Through it, states of two quantum systems are naturally decomposed, providing a necessary and sufficient condition for them to be entangled: if the rank of the matrix is larger than one.\n\nOne application of SVD to rather large matrices is in numerical weather prediction, where Lanczos methods are used to estimate the most linearly quickly growing few perturbations to the central numerical weather prediction over a given initial forward time period; i.e., the singular vectors corresponding to the largest singular values of the linearized propagator for the global weather over that time interval. \nThe output singular vectors in this case are entire weather systems. These perturbations are then run through the full nonlinear model to generate an ensemble forecast, giving a handle on some of the uncertainty that should be allowed for around the current central prediction.\n\nSVD has also been applied to reduced order modelling. The aim of reduced order modelling is to reduce the number of degrees of freedom in a complex system which is to be modelled. SVD was coupled with radial basis functions to interpolate solutions to three-dimensional unsteady flow problems.\n\nSingular-value decomposition is used in recommender systems to predict people's item ratings. Distributed algorithms have been developed for the purpose of calculating the SVD on clusters of commodity machines.\n\nAnother code implementation of the Netflix Recommendation Algorithm SVD (the third optimal algorithm in the competition conducted by Netflix to find the best collaborative filtering techniques for predicting user ratings for films based on previous reviews) in platform Apache Spark is available in the following GitHub repository implemented by Alexandros Ioannidis. The original SVD algorithm, which in this case is executed in parallel encourages users of the GroupLens website, by consulting proposals for monitoring new films tailored to the needs of each user.\n\nLow-rank SVD has been applied for hotspot detection from spatiotemporal data with application to disease outbreak detection \n. A combination of SVD and higher-order SVD also has been applied for real time event detection from complex data streams (multivariate data with space and time dimensions) in Disease surveillance.\n\nAn eigenvalue of a matrix is characterized by the algebraic relation . When is Hermitian, a variational characterization is also available. Let be a real symmetric matrix. Define\n\nBy the extreme value theorem, this continuous function attains a maximum at some \"u\" when restricted to the closed unit sphere {||\"x\"|| ≤ 1}. By the Lagrange multipliers theorem, \"u\" necessarily satisfies\n\nwhere the nabla symbol, , is the del operator.\n\nA short calculation shows the above leads to (symmetry of is needed here). Therefore, is the largest eigenvalue of . The same calculation performed on the orthogonal complement of \"u\" gives the next largest eigenvalue and so on. The complex Hermitian case is similar; there \"f\"(\"x\") = \"x* M x\" is a real-valued function of real variables.\n\nSingular values are similar in that they can be described algebraically or from variational principles. Although, unlike the eigenvalue case, Hermiticity, or symmetry, of is no longer required.\n\nThis section gives these two arguments for existence of singular-value decomposition.\n\nLet formula_3 be an complex matrix. Since formula_53 is positive semi-definite and Hermitian, by the spectral theorem, there exists an unitary matrix formula_9 such that\n\nwhere formula_56 is diagonal and positive definite, of dimension formula_57, with formula_58 the number of non-zero eigenvalues of formula_53 (which can be shown to verify formula_60). Note that formula_9 is here by definition a matrix whose formula_62-th column is the formula_62-th eigenvector of formula_53, corresponding to the eigenvalue formula_65. Moreover, the formula_66-th column of formula_9, for formula_68, is an eigenvector of formula_53 with eigenvalue formula_70. This can be expressed by writing formula_9 as formula_72, where the columns of formula_73 and formula_74 therefore contain the eigenvectors of formula_53 corresponding to non-zero and zero eigenvalues, respectively. Using this rewriting of formula_9, the equation becomes:\n\nThis implies that\n\nMoreover, the second equation implies formula_79. Finally, the unitarity of formula_9 translates, in terms of formula_73 and formula_74, into the following conditions:\n\nwhere the subscripts on the identity matrices are used to remark that they are of different dimensions.\n\nLet us now define\n\nThen,\n\nsince formula_86 This can be also seen as immediate consequence of the fact that formula_87. Note how this is equivalent to the observation that, if formula_88 if the set of eigenvectors of formula_53 corresponding to non-vanishing eigenvalues, then formula_90 is a set of orthogonal vectors, and formula_91 a (generally not complete) set of \"orthonormal\" vectors. This matches with the matrix formalism used above denoting with formula_73 the matrix whose columns are formula_88, with formula_74 the matrix whose columns are the eigenvectors of formula_53 which vanishing eigenvalue, and formula_96 the matrix whose columns are the vectors formula_91.\n\nWe see that this is almost the desired result, except that formula_96 and formula_73 are in general not unitary, since they might not be square. However, we do know that the number of rows of formula_96 is no smaller than the number of columns, since the dimensions of formula_56 is no greater than formula_102 and formula_103. Also, since\n\nthe columns in formula_96 are orthonormal and can be extended to an orthonormal basis. This means that we can choose formula_106 such that formula_107 is unitary.\n\nFor we already have to make it unitary. Now, define\n\nwhere extra zero rows are added or removed to make the number of zero rows equal the number of columns of , and hence the overall dimensions of formula_109 equal to formula_110. Then\n\nwhich is the desired result:\n\nNotice the argument could begin with diagonalizing rather than (This shows directly that and have the same non-zero eigenvalues).\n\nThe singular values can also be characterized as the maxima of , considered as a function of and , over particular subspaces. The singular vectors are the values of and where these maxima are attained.\n\nLet denote an matrix with real entries. Let and formula_113\n\nConsider the function restricted to . Since both and are compact sets, their product is also compact. Furthermore, since is continuous, it attains a largest value for at least one pair of vectors and . This largest value is denoted and the corresponding vectors are denoted and . Since is the largest value of it must be non-negative. If it were negative, changing the sign of either or would make it positive and therefore larger.\n\nStatement. are left and right-singular vectors of with corresponding singular value \"σ\".\n\nProof. Similar to the eigenvalues case, by assumption the two vectors satisfy the Lagrange multiplier equation:\n\nAfter some algebra, this becomes\n\nMultiplying the first equation from left by formula_116 and the second equation from left by formula_117 and taking into account gives\n\nPlugging this into the pair of equations above, we have\n\nThis proves the statement.\n\nMore singular vectors and singular values can be found by maximizing over normalized which are orthogonal to and , respectively.\n\nThe passage from real to complex is similar to the eigenvalue case.\n\nThe SVD of a matrix is typically computed by a two-step procedure. In the first step, the matrix is reduced to a bidiagonal matrix. This takes O(\"mn\") floating-point operations (flop), assuming that \"m\" ≥ \"n\". The second step is to compute the SVD of the bidiagonal matrix. This step can only be done with an iterative method (as with eigenvalue algorithms). However, in practice it suffices to compute the SVD up to a certain precision, like the machine epsilon. If this precision is considered constant, then the second step takes O(\"n\") iterations, each costing O(\"n\") flops. Thus, the first step is more expensive, and the overall cost is O(\"mn\") flops .\n\nThe first step can be done using Householder reflections for a cost of 4\"mn\" − 4\"n\"/3 flops, assuming that only the singular values are needed and not the singular vectors. If \"m\" is much larger than \"n\" then it is advantageous to first reduce the matrix \"M\" to a triangular matrix with the QR decomposition and then use Householder reflections to further reduce the matrix to bidiagonal form; the combined cost is 2\"mn\" + 2\"n\" flops .\n\nThe second step can be done by a variant of the QR algorithm for the computation of eigenvalues, which was first described by . The LAPACK subroutine DBDSQR implements this iterative method, with some modifications to cover the case where the singular values are very small . Together with a first step using Householder reflections and, if appropriate, QR decomposition, this forms the DGESVD routine for the computation of the singular-value decomposition.\n\nThe same algorithm is implemented in the GNU Scientific Library (GSL). The GSL also offers an alternative method, which uses a one-sided Jacobi orthogonalization in step 2 . This method computes the SVD of the bidiagonal matrix by solving a sequence of 2 × 2 SVD problems, similar to how the Jacobi eigenvalue algorithm solves a sequence of 2 × 2 eigenvalue methods . Yet another method for step 2 uses the idea of divide-and-conquer eigenvalue algorithms .\n\nThere is an alternative way which is not explicitly using the eigenvalue decomposition. Usually the singular-value problem of a matrix is converted into an equivalent symmetric eigenvalue problem such as , , or \nThe approaches using eigenvalue decompositions are based on QR algorithm which is well-developed to be stable and fast. \nNote that the singular values are real and right- and left- singular vectors are not required to form any similarity transformation. Alternating QR decomposition and LQ decomposition can be claimed to use iteratively to find the real diagonal matrix with Hermitian matrices. QR decomposition gives and LQ decomposition of gives . Thus, at every iteration, we have , update and repeat the orthogonalizations.\nEventually, QR decomposition and LQ decomposition iteratively provide unitary matrices for left- and right- singular matrices, respectively. \nThis approach does not come with any acceleration method such as spectral shifts and deflation as in QR algorithm. It is because the shift method is not easily defined without using similarity transformation. But it is very simple to implement where the speed does not matter. Also it give us a good interpretation that only orthogonal/unitary transformations can obtain SVD as the QR algorithm can calculate the eigenvalue decomposition.\n\nThe singular values of a 2 × 2 matrix can be found analytically. Let the matrix be\nformula_121\n\nwhere formula_122 are complex numbers that parameterize the matrix, is the identity matrix, and formula_11 denote the Pauli matrices. Then its two singular values are given by\n\nIn applications it is quite unusual for the full SVD, including a full unitary decomposition of the null-space of the matrix, to be required. Instead, it is often sufficient (as well as faster, and more economical for storage) to compute a reduced version of the SVD. The following can be distinguished for an \"m\"×\"n\" matrix \"M\" of rank \"r\":\n\nOnly the \"n\" column vectors of \"U\" corresponding to the row vectors of \"V*\" are calculated. The remaining column vectors of \"U\" are not calculated. This is significantly quicker and more economical than the full SVD if \"n\" ≪ \"m\". The matrix \"U\" is thus \"m\"×\"n\", Σ is \"n\"×\"n\" diagonal, and \"V\" is \"n\"×\"n\".\n\nThe first stage in the calculation of a thin SVD will usually be a QR decomposition of \"M\", which can make for a significantly quicker calculation if \"n\" ≪ \"m\".\n\nOnly the \"r\" column vectors of \"U\" and \"r\" row vectors of \"V*\" corresponding to the non-zero singular values Σ are calculated. The remaining vectors of \"U\" and \"V*\" are not calculated. This is quicker and more economical than the thin SVD if \"r\" ≪ \"n\". The matrix \"U\" is thus \"m\"×\"r\", Σ is \"r\"×\"r\" diagonal, and \"V\"* is \"r\"×\"n\".\n\nOnly the \"t\" column vectors of \"U\" and \"t\" row vectors of \"V*\" corresponding to the \"t\" largest singular values Σ are calculated. The rest of the matrix is discarded. This can be much quicker and more economical than the compact SVD if \"t\"≪\"r\". The matrix \"U\" is thus \"m\"×\"t\", Σ is \"t\"×\"t\" diagonal, and \"V\"* is \"t\"×\"n\".\n\nOf course the truncated SVD is no longer an exact decomposition of the original matrix \"M\", but as discussed above, the approximate matrix formula_40 is in a very useful sense the closest approximation to \"M\" that can be achieved by a matrix of rank \"t\".\n\nThe sum of the \"k\" largest singular values of \"M\" is a matrix norm, the Ky Fan \"k\"-norm of \"M\". \n\nThe first of the Ky Fan norms, the Ky Fan 1-norm, is the same as the operator norm of \"M\" as a linear operator with respect to the Euclidean norms of \"K\" and \"K\". In other words, the Ky Fan 1-norm is the operator norm induced by the standard \"ℓ\" Euclidean inner product. For this reason, it is also called the operator 2-norm. One can easily verify the relationship between the Ky Fan 1-norm and singular values. It is true in general, for a bounded operator \"M\" on (possibly infinite-dimensional) Hilbert spaces\n\nBut, in the matrix case, (\"M* M\") is a normal matrix, so ||\"M* M\"|| is the largest eigenvalue of (\"M* M\"), i.e. the largest singular value of \"M\".\n\nThe last of the Ky Fan norms, the sum of all singular values, is the trace norm (also known as the 'nuclear norm'), defined by ||\"M\"|| = Tr[(\"M* M\")] (the eigenvalues of \"M* M\" are the squares of the singular values).\n\nThe singular values are related to another norm on the space of operators. Consider the Hilbert–Schmidt inner product on the matrices, defined by\n\nSo the induced norm is\n\nSince the trace is invariant under unitary equivalence, this shows\n\nwhere are the singular values of . This is called the Frobenius norm, Schatten 2-norm, or Hilbert–Schmidt norm of . Direct calculation shows that the Frobenius norm of coincides with:\n\nIn addition, the Frobenius norm and the trace norm (the nuclear norm) are special cases of the Schatten norm.\n\nformula_134 can be represented using mode-\"k\" multiplication of matrix formula_135 applying formula_136 then formula_137 on the result; that is formula_138.\n\nTwo types of tensor decompositions exist, which generalise the SVD to multi-way arrays. One of them decomposes a tensor into a sum of rank-1 tensors, which is called a tensor rank decomposition. The second type of decomposition computes the orthonormal subspaces associated with the different factors appearing in the tensor product of vector spaces in which the tensor lives. This decomposition is referred to in the literature as the higher-order SVD (HOSVD) or Tucker3/TuckerM. In addition, multilinear principal component analysis in multilinear subspace learning involves the same mathematical operations as Tucker decomposition, being used in a different context of dimensionality reduction.\n\nThe SVD singular values of a matrix \"A\" are unique and are invariant with respect to left and/or right unitary transformations of \"A\". In other words, the singular values of \"UAV\", for unitary \"U\" and \"V\", are equal to the singular values of \"A\". This is an important property for applications in which it is necessary to preserve Euclidean distances, and invariance with respect to rotations.\n\nThe Scale-Invariant SVD, or SI-SVD, is analogous to the conventional SVD except that its uniquely-determined singular values are invariant withrespect to diagonal transformations of \"A\". In other words, the singular values of \"DAE\", for nonsingular diagonal matrices \"D\" and \"E\", are equal to the singular values of \"A\". This is an important property for applications for which invariance to the choice of units on variables (e.g., metric versus imperial units) is needed.\n\nTP model transformation numerically reconstruct the HOSVD of functions. For further details please visit:\n\n\nThe factorization can be extended to a bounded operator \"M\" on a separable Hilbert space \"H\". Namely, for any bounded operator \"M\", there exist a partial isometry \"U\", a unitary \"V\", a measure space (\"X\", \"μ\"), and a non-negative measurable \"f\" such that\n\nwhere formula_140 is the multiplication by \"f\" on \"L\"(\"X\", \"μ\").\n\nThis can be shown by mimicking the linear algebraic argument for the matricial case above. \"VT\" V* is the unique positive square root of \"M*M\", as given by the Borel functional calculus for self adjoint operators. The reason why \"U\" need not be unitary is because, unlike the finite-dimensional case, given an isometry \"U\" with nontrivial kernel, a suitable \"U\" may not be found such that\n\nis a unitary operator.\n\nAs for matrices, the singular-value factorization is equivalent to the polar decomposition for operators: we can simply write\n\nand notice that \"U V*\" is still a partial isometry while \"VT\" \"V\"* is positive.\n\nThe notion of singular values and left/right-singular vectors can be extended to compact operator on Hilbert space as they have a discrete spectrum. If is compact, every non-zero in its spectrum is an eigenvalue. Furthermore, a compact self adjoint operator can be diagonalized by its eigenvectors. If is compact, so is . Applying the diagonalization result, the unitary image of its positive square root has a set of orthonormal eigenvectors corresponding to strictly positive eigenvalues For any ,\n\nwhere the series converges in the norm topology on . Notice how this resembles the expression from the finite-dimensional case. are called the singular values of . (resp. ) can be considered the left-singular (resp. right-singular) vectors of .\n\nCompact operators on a Hilbert space are the closure of finite-rank operators in the uniform operator topology. The above series expression gives an explicit such representation. An immediate consequence of this is:\n\nThe singular-value decomposition was originally developed by differential geometers, who wished to determine whether a real bilinear form could be made equal to another by independent orthogonal transformations of the two spaces it acts on. Eugenio Beltrami and Camille Jordan discovered independently, in 1873 and 1874 respectively, that the singular values of the bilinear forms, represented as a matrix, form a complete set of invariants for bilinear forms under orthogonal substitutions. James Joseph Sylvester also arrived at the singular-value decomposition for real square matrices in 1889, apparently independently of both Beltrami and Jordan. Sylvester called the singular values the \"canonical multipliers\" of the matrix \"A\". The fourth mathematician to discover the singular value decomposition independently is Autonne in 1915, who arrived at it via the polar decomposition. The first proof of the singular value decomposition for rectangular and complex matrices seems to be by Carl Eckart and Gale Young in 1936; they saw it as a generalization of the principal axis transformation for Hermitian matrices.\n\nIn 1907, Erhard Schmidt defined an analog of singular values for integral operators (which are compact, under some weak technical assumptions); it seems he was unaware of the parallel work on singular values of finite matrices. This theory was further developed by Émile Picard in 1910, who is the first to call the numbers formula_144 \"singular values\" (or in French, \"valeurs singulières\").\n\nPractical methods for computing the SVD date back to Kogbetliantz in 1954, 1955 and Hestenes in 1958. resembling closely the Jacobi eigenvalue algorithm, which uses plane rotations or Givens rotations. However, these were replaced by the method of Gene Golub and William Kahan published in 1965, which uses Householder transformations or reflections.\nIn 1970, Golub and Christian Reinsch published a variant of the Golub/Kahan algorithm that is still the one most-used today.\n\n\n"}
{"id": "36088541", "url": "https://en.wikipedia.org/wiki?curid=36088541", "title": "Smooth algebra", "text": "Smooth algebra\n\nIn algebra, a commutative \"k\"-algebra \"A\" is said to be 0-smooth if it satisfies the following lifting property: given a \"k\"-algebra \"C\", an ideal \"N\" of \"C\" whose square is zero and a \"k\"-algebra map formula_1, there exists a \"k\"-algebra map formula_2 such that \"u\" is \"v\" followed by the canonical map. If there exists at most one such lifting \"v\", then \"A\" is said to be 0-unramified (or 0-neat). \"A\" is said to be 0-étale if it is 0-smooth and 0-unramified.\n\nA finitely generated \"k\"-algebra \"A\" is 0-smooth over \"k\" if and only if Spec \"A\" is a smooth scheme over \"k\".\n\nA separable algebraic field extension \"L\" of \"k\" is 0-étale over \"k\". The formal power series ring formula_3 is 0-smooth only when formula_4 and formula_5 (i.e., \"k\" has a finite \"p\"-basis.)\n\nLet \"B\" be an \"A\"-algebra and suppose \"B\" is given the \"I\"-adic topology, \"I\" an ideal of \"B\". We say \"B\" is I\"-smooth over \"A if it satisfies the lifting property: given an \"A\"-algebra \"C\", an ideal \"N\" of \"C\" whose square is zero and an \"A\"-algebra map formula_6 that is continuous when formula_7 is given the discrete topology, there exists an \"A\"-algebra map formula_8 such that \"u\" is \"v\" followed by the canonical map. As before, if there exists at most one such lift \"v\", then \"B\" is said to be I\"-unramified over \"A (or I\"-neat). \"B\" is said to be I\"-étale if it is I\"-smooth and I\"-unramified. If \"I\" is the zero ideal and \"A\" is a field, these notions coincide with 0-smooth etc. as defined above.\n\nA standard example is this: let \"A\" be a ring, formula_9 and formula_10 Then \"B\" is \"I\"-smooth over \"A\".\n\nLet \"A\" be a noetherian local \"k\"-algebra with maximal ideal formula_11. Then \"A\" is formula_11-smooth over \"k\" if and only if formula_13 is a regular ring for any finite extension field formula_14 of \"k\".\n\n\n"}
{"id": "929998", "url": "https://en.wikipedia.org/wiki?curid=929998", "title": "Structural proof theory", "text": "Structural proof theory\n\nIn mathematical logic, structural proof theory is the subdiscipline of proof theory that studies proof calculi that support a notion of analytic proof.\n\nThe notion of analytic proof was introduced into proof theory by Gerhard Gentzen for the sequent calculus; the analytic proofs are those that are cut-free. His natural deduction calculus also supports a notion of analytic proof, as was shown by Dag Prawitz; the definition is slightly more complex—the analytic proofs are the normal forms, which are related to the notion of normal form in term rewriting.\n\nThe term \"structure\" in structural proof theory comes from a technical notion introduced in the sequent calculus: the sequent calculus represents the judgement made at any stage of an inference using special, extra-logical operators called structural operators: in formula_1, the commas to the left of the turnstile are operators normally interpreted as conjunctions, those to the right as disjunctions, whilst the turnstile symbol itself is interpreted as an implication. However, it is important to note that there is a fundamental difference in behaviour between these operators and the logical connectives they are interpreted by in the sequent calculus: the structural operators are used in every rule of the calculus, and are not considered when asking whether the subformula property applies. Furthermore, the logical rules go one way only: logical structure is introduced by logical rules, and cannot be eliminated once created, while structural operators can be introduced and eliminated in the course of a derivation.\n\nThe idea of looking at the syntactic features of sequents as special, non-logical operators is not old, and was forced by innovations in proof theory: when the structural operators are as simple as in Getzen's original sequent calculus there is little need to analyse them, but proof calculi of deep inference such as display logic support structural operators as complex as the logical connectives, and demand sophisticated treatment.\n\nThe hypersequent framework extends the ordinary sequent structure to a multiset of sequents, using an additional structural connective | (called the hypersequent bar) to separate different sequents. It has been used to provide analytic calculi for, e.g., modal, intermediate and substructural logics A hypersequent is a structure\n\nformula_2\n\nwhere each formula_3 is an ordinary sequent, called a component of the hypersequent. As for sequents, hypersequents can be based on sets, multisets, or sequences, and the components can be single-conclusion or multi-conclusion sequents. The formula interpretation of the hypersequents depends on the logic under consideration, but is nearly always some form of disjunction. The most common interpretations are as a simple disjunction\n\nformula_4\n\nfor intermediate logics, or as a disjunction of boxes\n\nformula_5\n\nfor modal logics.\n\nIn line with the disjunctive interpretation of the hypersequent bar, essentially all hypersequent calculi include the external structural rules, in particular the external weakening rule\n\nformula_6\n\nand the external contraction rule\n\nformula_7\n\nThe additional expressivity of the hypersequent framework is provided by rules manipulating the hypersequent structure. An important example is provided by the modalised splitting rule\n\nformula_8\n\nfor modal logic S5, where formula_9 means that every formula in formula_10 is of the form formula_11. \n\nAnother example is given by the communication rule for intermediate logic LC\n\nformula_12\n\nNote that in the communication rule the components are single-conclusion sequents.\n\nThe nested sequent calculus is a formalisation that resembles a 2-sided calculus of structures.\n\n"}
{"id": "7400895", "url": "https://en.wikipedia.org/wiki?curid=7400895", "title": "Syntactic predicate", "text": "Syntactic predicate\n\nA syntactic predicate specifies the syntactic validity of applying a production in a formal grammar and is analogous to a semantic predicate that specifies the semantic validity of applying a production. It is a simple and effective means of dramatically improving the recognition strength of an LL parser by providing arbitrary lookahead. In their original implementation, syntactic predicates had the form “( α )?” and could only appear on the left edge of a production. The required syntactic condition α could be any valid context-free grammar fragment.\n\nMore formally, a syntactic predicate is a form of production intersection, used in parser specifications or in formal grammars. In this sense, the term \"predicate\" has the meaning of a mathematical indicator function. If \"p\" and \"p\" are production rules, the language generated by \"both\" \"p\" \"and\" \"p\" is their set intersection.\n\nAs typically defined or implemented, syntactic predicates implicitly order the productions so that predicated productions specified earlier have higher precedence than predicated productions specified later within the same decision. This conveys an ability to disambiguate ambiguous productions because the programmer can simply specify which production should match.\n\nParsing expression grammars (PEGs), invented by Bryan Ford, extend these simple predicates by allowing \"not predicates\" and permitting a predicate to appear anywhere within a production. Moreover, Ford invented packrat parsing to handle these grammars in linear time by employing memoization, at the cost of heap space.\n\nIt is possible to support linear-time parsing of predicates as general as those allowed by PEGs, but reduce the memory cost associated with memoization by avoiding backtracking where some more efficient implementation of lookahead suffices. This approach is implemented by ANTLR version 3, which uses Deterministic finite automata for lookahead; this may require testing a predicate in order to choose between transitions of the DFA (called \"pred-LL(*)\" parsing).\n\nThe term \"syntactic predicate\" was coined by Parr & Quong and differentiates this form of predicate from semantic predicates (also discussed).\n\nSyntactic predicates have been called \"multi-step matching\", \"parse constraints\", and simply \"predicates\" in various literature. (See References section below.) This article uses the term \"syntactic predicate\" throughout for consistency and to distinguish them from semantic predicates.\n\nBar-Hillel \"et al.\" show that the intersection of two regular languages is also a regular language, which is to say that the regular languages are closed under intersection.\n\nThe intersection of a regular language and a context-free language is also closed, and it has been known at least since Hartmanis that the intersection of two context-free languages is not necessarily a context-free language (and is thus not closed). This can be demonstrated easily using the canonical Type 1 language, formula_1:\n\nGiven the strings ', ', and ', it is clear that the only string that belongs to both L and L (that is, the only one that produces a non-empty intersection) is '.\n\nIn most formalisms that use syntactic predicates, the syntax of the predicate is noncommutative, which is to say that the operation of predication is ordered. For instance, using the above example, consider the following pseudo-grammar, where \"X ::= Y PRED Z\" is understood to mean: \"\"Y\" produces \"X\" if and only if \"Y\" also satisfies predicate \"Z\"\":\n\nGiven the string ', in the case where \"Y\" must be satisfied \"first\" (and assuming a greedy implementation), S will generate \"aX\" and \"X\" in turn will generate ', thereby generating '. In the case where \"Z\" must be satisfied first, ANBN will fail to generate ', and thus \"\" is not generated by the grammar. Moreover, if either \"Y\" or \"Z\" (or both) specify any action to be taken upon reduction (as would be the case in many parsers), the order that these productions match determines the order in which those side-effects occur. Formalisms that vary over time (such as adaptive grammars) may rely on these side effects.\n\n\nParr & Quong give this example of a syntactic predicate:\n\nwhich is intended to satisfy the following informally stated constraints of C++:\n\n\nIn the first production of rule stat, the syntactic predicate (declaration)? indicates\nthat declaration is the syntactic context that must be present for the rest of that production to succeed. We can interpret the use of (declaration)? as \"I am not sure if\ndeclaration will match; let me try it out and, if it does not match, I shall try the next\nalternative.\" Thus, when encountering a valid declaration, the rule declaration will be\nrecognized twice—once as syntactic predicate and once during the actual parse to execute semantic actions.\n\nOf note in the above example is the fact that any code triggered by the acceptance of the \"declaration\" production will only occur if the predicate is satisfied.\n\nThe language formula_5 can be represented in various grammars and formalisms as follows:\n\n\n\nUsing a \"bound\" predicate:\n\nUsing two \"free\" predicates:\n\n\n(Note: the following example actually generates formula_6, but is included here because it is the example given by the inventor of conjunctive grammars.):\n\n\nAlthough by no means an exhaustive list, the following parsers and grammar formalisms employ syntactic predicates:\n\n\n"}
{"id": "2515349", "url": "https://en.wikipedia.org/wiki?curid=2515349", "title": "Table of Clebsch–Gordan coefficients", "text": "Table of Clebsch–Gordan coefficients\n\nThis is a table of Clebsch–Gordan coefficients used for adding angular momentum values in quantum mechanics. The overall sign of the coefficients for each set of constant formula_1, formula_2, formula_3 is arbitrary to some degree and has been fixed according to the Condon-Shortley and Wigner sign convention as discussed by Baird and Biedenharn. Tables with the same sign convention may be found in the Particle Data Group's \"Review of Particle Properties\" and in online tables.\n\nThe Clebsch–Gordan coefficients are the solutions to\n\nExplicitly:\n\nThe summation is extended over all integer for which the argument of every factorial is nonnegative.\n\nFor brevity, solutions with and are omitted. They may be calculated using the simple relations\n\nand\n\nThe Clebsch-Gordan coefficients for \"j\" values less than or equal to 5/2 are given by:\n\nWhen , the Clebsch–Gordan coefficients are given by formula_8 .\n\nAlgorithms to produce Clebsch–Gordan coefficients for higher values of formula_1 and formula_2, or for the su(N) algebra instead of su(2), are known.\nA web interface for tabulating SU(N) Clebsch-Gordan coefficients is readily available.\n\n"}
{"id": "31248", "url": "https://en.wikipedia.org/wiki?curid=31248", "title": "Travelling salesman problem", "text": "Travelling salesman problem\n\nThe travelling salesman problem (TSP) asks the following question: \"Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city and returns to the origin city?\" It is an NP-hard problem in combinatorial optimization, important in operations research and theoretical computer science.\n\nThe travelling purchaser problem and the vehicle routing problem are both generalizations of TSP.\n\nIn the theory of computational complexity, the decision version of the TSP (where, given a length \"L\", the task is to decide whether the graph has any tour shorter than \"L\") belongs to the class of NP-complete problems. Thus, it is possible that the worst-case running time for any algorithm for the TSP increases superpolynomially (but no more than exponentially) with the number of cities.\n\nThe problem was first formulated in 1930 and is one of the most intensively studied problems in optimization. It is used as a benchmark for many optimization methods. Even though the problem is computationally difficult, a large number of heuristics and exact algorithms are known, so that some instances with tens of thousands of cities can be solved completely and even problems with millions of cities can be approximated within a small fraction of 1%.\n\nThe TSP has several applications even in its purest formulation, such as planning, logistics, and the manufacture of microchips. Slightly modified, it appears as a sub-problem in many areas, such as DNA sequencing. In these applications, the concept \"city\" represents, for example, customers, soldering points, or DNA fragments, and the concept \"distance\" represents travelling times or cost, or a similarity measure between DNA fragments. The TSP also appears in astronomy, as astronomers observing many sources will want to minimize the time spent moving the telescope between the sources. In many applications, additional constraints such as limited resources or time windows may be imposed.\n\nThe origins of the travelling salesman problem are unclear. A handbook for travelling salesmen from 1832 mentions the problem and includes example tours through Germany and Switzerland, but contains no mathematical treatment.\n\nThe travelling salesman problem was mathematically formulated in the 1800s by the Irish mathematician W.R. Hamilton and by the British mathematician Thomas Kirkman. Hamilton’s Icosian Game was a recreational puzzle based on finding a Hamiltonian cycle. The general form of the TSP appears to have been first studied by mathematicians during the 1930s in Vienna and at Harvard, notably by Karl Menger, who defines the problem, considers the obvious brute-force algorithm, and observes the non-optimality of the nearest neighbour heuristic:\nIt was first considered mathematically in the 1930s by Merrill Flood who was looking to solve a school bus routing problem.\nHassler Whitney at Princeton University introduced the name \"travelling salesman problem\" soon after.\n\nIn the 1950s and 1960s, the problem became increasingly popular in scientific circles in Europe and the USA after the RAND Corporation in Santa Monica offered prizes for steps in solving the problem. Notable contributions were made by George Dantzig, Delbert Ray Fulkerson and Selmer M. Johnson from the RAND Corporation, who expressed the problem as an integer linear program and developed the cutting plane method for its solution. They wrote what is considered the seminal paper on the subject in which with these new methods they solved an instance with 49 cities to optimality by constructing a tour and proving that no other tour could be shorter. Dantzig, Fulkerson and Johnson, however, speculated that given a near optimal solution we may be able to find optimality or prove optimality by adding a small amount of extra inequalities (cuts). They used this idea to solve their initial 49 city problem using a string model. They found they only needed 26 cuts to come to a solution for their 49 city problem. While this paper did not give an algorithmic approach to TSP problems, the ideas that lay within it were indispensable to later creating exact solution methods for the TSP, though it would take 15 years to find an algorithmic approach in creating these cuts. As well as cutting plane methods, Dantzig, Fulkerson and Johnson used branch and bound algorithms perhaps for the first time.\n\nIn the following decades, the problem was studied by many researchers from mathematics, computer science, chemistry, physics, and other sciences. In the 1960s however a new approach was created, that instead of seeking optimal solutions, one would produce a solution whose length is provably bounded by a multiple of the optimal length, and in doing so create lower bounds for the problem; these may then be used with branch and bound approaches. One method of doing this was to create a minimum spanning tree of the graph and then double all its edges, which produces the bound that the length of an optimal tour is at most twice the weight of a minimum spanning tree.\n\nChristofides made a big advance in this approach of giving an approach for which we know the worst-case scenario. Christofides algorithm given in 1976, at worst is 1.5 times longer than the optimal solution. As the algorithm was so simple and quick, many hoped it would give way to a near optimal solution method. However, until 2011 when it was beaten by less than a billionth of a percent, this remained the method with the best worst-case scenario.\n\nRichard M. Karp showed in 1972 that the Hamiltonian cycle problem was NP-complete, which implies the NP-hardness of TSP. This supplied a mathematical explanation for the apparent computational difficulty of finding optimal tours.\n\nGreat progress was made in the late 1970s and 1980, when Grötschel, Padberg, Rinaldi and others managed to exactly solve instances with up to 2392 cities, using cutting planes and branch-and-bound.\n\nIn the 1990s, Applegate, Bixby, Chvátal, and Cook developed the program \"Concorde\" that has been used in many recent record solutions. Gerhard Reinelt published the TSPLIB in 1991, a collection of benchmark instances of varying difficulty, which has been used by many research groups for comparing results. In 2006, Cook and others computed an optimal tour through an 85,900-city instance given by a microchip layout problem, currently the largest solved TSPLIB instance. For many other instances with millions of cities, solutions can be found that are guaranteed to be within 2-3% of an optimal tour.\n\nTSP can be modelled as an undirected weighted graph, such that cities are the graph's vertices, paths are the graph's edges, and a path's distance is the edge's weight. It is a minimization problem starting and finishing at a specified vertex after having visited each other vertex exactly once. Often, the model is a complete graph (\"i.e.\" each pair of vertices is connected by an edge). If no path exists between two cities, adding an arbitrarily long edge will complete the graph without affecting the optimal tour.\n\nIn the \"symmetric TSP\", the distance between two cities is the same in each opposite direction, forming an undirected graph. This symmetry halves the number of possible solutions. In the \"asymmetric TSP\", paths may not exist in both directions or the distances might be different, forming a directed graph. Traffic collisions, one-way streets, and airfares for cities with different departure and arrival fees are examples of how this symmetry could break down.\n\n\nTSP can be formulated as an integer linear program. Label the cities with the numbers 1, …, \"n\" and define:\n\nFor \"i\" = 1, …, \"n\", let formula_2 be a dummy variable, and finally take formula_3 to be the distance from city \"i\" to city \"j\". Then TSP can be written as the following integer linear programming problem:\n\nThe first set of equalities requires that each city be arrived at from exactly one other city, and the second set of equalities requires that from each city there is a departure to exactly one other city. The last constraints enforce that there is only a single tour covering all cities, and not two or more disjointed tours that only collectively cover all cities. To prove this, it is shown below (1) that every feasible solution contains only one closed sequence of cities, and (2) that for every single tour covering all cities, there are values for the dummy variables formula_2 that satisfy the constraints.\n\nTo prove that every feasible solution contains only one closed sequence of cities, it suffices to show that every subtour in a feasible solution passes through city 1 (noting that the equalities ensure there can only be one such tour). For if we sum all the inequalities corresponding to formula_6 for any subtour of \"k\" steps not passing through city 1, we obtain:\n\nwhich is a contradiction.\n\nIt now must be shown that for every single tour covering all cities, there are values for the dummy variables formula_2 that satisfy the constraints.\n\nWithout loss of generality, define the tour as originating (and ending) at city 1. Choose formula_9 if city \"i\" is visited in step \"t\" (\"i\", \"t\" = 1, 2, ..., n). Then\n\nsince formula_2 can be no greater than \"n\" and formula_12 can be no less than 1; hence the constraints are satisfied whenever formula_13 For formula_6, we have:\n\nsatisfying the constraint.\n\nThe traditional lines of attack for the NP-hard problems are the following:\n\nThe most direct solution would be to try all permutations (ordered combinations) and see which one is cheapest (using brute force search). The running time for this approach lies within a polynomial factor of formula_16, the factorial of the number of cities, so this solution becomes impractical even for only 20 cities.\n\nOne of the earliest applications of dynamic programming is the Held–Karp algorithm that solves the problem in time formula_17. This bound has also been reached by Exclusion-Inclusion in an attempt preceding the dynamic programming approach.\nImproving these time bounds seems to be difficult. For example, it has not been determined whether an exact algorithm for TSP that runs in time formula_18 exists.\n\nOther approaches include:\n\n\nAn exact solution for 15,112 German towns from TSPLIB was found in 2001 using the cutting-plane method proposed by George Dantzig, Ray Fulkerson, and Selmer M. Johnson in 1954, based on linear programming. The computations were performed on a network of 110 processors located at Rice University and Princeton University (see the Princeton external link). The total computation time was equivalent to 22.6 years on a single 500 MHz Alpha processor. In May 2004, the travelling salesman problem of visiting all 24,978 towns in Sweden was solved: a tour of length approximately 72,500 kilometres was found and it was proven that no shorter tour exists. In March 2005, the travelling salesman problem of visiting all 33,810 points in a circuit board was solved using \"Concorde TSP Solver\": a tour of length 66,048,945 units was found and it was proven that no shorter tour exists. The computation took approximately 15.7 CPU-years (Cook et al. 2006). In April 2006 an instance with 85,900 points was solved using \"Concorde TSP Solver\", taking over 136 CPU-years, see .\n\nVarious heuristics and approximation algorithms, which quickly yield good solutions have been devised. Modern methods can find solutions for extremely large problems (millions of cities) within a reasonable time which are with a high probability just 2–3% away from the optimal solution.\n\nSeveral categories of heuristics are recognized.\n\nThe nearest neighbour (NN) algorithm (a greedy algorithm) lets the salesman choose the nearest unvisited city as his next move. This algorithm quickly yields an effectively short route. For N cities randomly distributed on a plane, the algorithm on average yields a path 25% longer than the shortest possible path. However, there exist many specially arranged city distributions which make the NN algorithm give the worst route. This is true for both asymmetric and symmetric TSPs. Rosenkrantz et al. showed that the NN algorithm has the approximation factor formula_19 for instances satisfying the triangle inequality. A variation of NN algorithm, called Nearest Fragment (NF) operator, which connects a group (fragment) of nearest unvisited cities, can find shorter route with successive iterations. The NF operator can also be applied on an initial solution obtained by NN algorithm for further improvement in an elitist model, where only better solutions are accepted.\n\nThe bitonic tour of a set of points is the minimum-perimeter monotone polygon that has the points as its vertices; it can be computed efficiently by dynamic programming.\n\nAnother constructive heuristic, Match Twice and Stitch (MTS), performs two sequential matchings, where the second matching is executed after deleting all the edges of the first matching, to yield a set of cycles. The cycles are then stitched to produce the final tour.\n\nThe Christofides algorithm follows a similar outline but combines the minimum spanning tree with a solution of another problem, minimum-weight perfect matching. This gives a TSP tour which is at most 1.5 times the optimal. The Christofides algorithm was one of the first approximation algorithms, and was in part responsible for drawing attention to approximation algorithms as a practical approach to intractable problems. As a matter of fact, the term \"algorithm\" was not commonly extended to approximation algorithms until later; the Christofides algorithm was initially referred to as the Christofides heuristic.\n\nThis algorithm looks at things differently by using a result from graph theory which helps improve on the LB of the TSP which originated from doubling the cost of the minimum spanning tree. Given an Eulerian graph we can find an Eulerian tour in time. So if we had an Eulerian graph with cities from a TSP as vertices then we can easily see that we could use such a method for finding an Eulerian tour to find a TSP solution. By triangular inequality we know that the TSP tour can be no longer than the Eulerian tour and as such we have a LB for the TSP. Such a method is described below.\n\n\nTo improve our lower bound, we therefore need a better way of creating an Eulerian graph. But by triangular inequality, the best Eulerian graph must have the same cost as the best travelling salesman tour, hence finding optimal Eulerian graphs is at least as hard as TSP. One way of doing this that has been proposed is by the concept of minimum weight matching for the creation of which there exist algorithms of formula_20.\nTo make a graph into an Eulerian graph, one starts with the minimum spanning tree. Then all the vertices of odd order must be made even. So a matching for the odd degree vertices must be added which increases the order of every odd degree vertex by one. This leaves us with a graph where every vertex is of even order which is thus Eulerian. Now we can adapt the above method to give Christofides' algorithm,\n\n\nThe pairwise exchange or \"2-opt\" technique involves iteratively removing two edges and replacing these with two different edges that reconnect the fragments created by edge removal into a new and shorter tour. Similarly, the 3-opt technique removes 3 edges and reconnects them to form a shorter tour. These are special cases of the \"k\"-opt method. Note that the label \"Lin–Kernighan\" is an often heard misnomer for 2-opt. Lin–Kernighan is actually the more general k-opt method.\n\nFor Euclidean instances, 2-opt heuristics give on average solutions that are about 5% better than Christofides' algorithm. If we start with an initial solution made with a greedy algorithm, the average number of moves greatly decreases again and is . For random starts however, the average number of moves is . However whilst in order this is a small increase in size, the initial number of moves for small problems is 10 times as big for a random start compared to one made from a greedy heuristic. This is because such 2-opt heuristics exploit `bad' parts of a solution such as crossings. These types of heuristics are often used within Vehicle routing problem heuristics to reoptimize route solutions.\n\nTake a given tour and delete \"k\" mutually disjoint edges. Reassemble the remaining fragments into a tour, leaving no disjoint subtours (that is, don't connect a fragment's endpoints together). This in effect simplifies the TSP under consideration into a much simpler problem. Each fragment endpoint can be connected to other possibilities: of 2\"k\" total fragment endpoints available, the two endpoints of the fragment under consideration are disallowed. Such a constrained 2\"k\"-city TSP can then be solved with brute force methods to find the least-cost recombination of the original fragments. The \"k\"-opt technique is a special case of the \"V\"-opt or variable-opt technique. The most popular of the \"k\"-opt methods are 3-opt, and these were introduced by Shen Lin of Bell Labs in 1965. There is a special case of 3-opt where the edges are not disjoint (two of the edges are adjacent to one another). In practice, it is often possible to achieve substantial improvement over 2-opt without the combinatorial cost of the general 3-opt by restricting the 3-changes to this special subset where two of the removed edges are adjacent. This so-called two-and-a-half-opt typically falls roughly midway between 2-opt and 3-opt, both in terms of the quality of tours achieved and the time required to achieve those tours.\n\nThe variable-opt method is related to, and a generalization of the \"k\"-opt method. Whereas the \"k\"-opt methods remove a fixed number (\"k\") of edges from the original tour, the variable-opt methods do not fix the size of the edge set to remove. Instead they grow the set as the search process continues. The best known method in this family is the Lin–Kernighan method (mentioned above as a misnomer for 2-opt). Shen Lin and Brian Kernighan first published their method in 1972, and it was the most reliable heuristic for solving travelling salesman problems for nearly two decades. More advanced variable-opt methods were developed at Bell Labs in the late 1980s by David Johnson and his research team. These methods (sometimes called Lin–Kernighan–Johnson) build on the Lin–Kernighan method, adding ideas from tabu search and evolutionary computing. The basic Lin–Kernighan technique gives results that are guaranteed to be at least 3-opt. The Lin–Kernighan–Johnson methods compute a Lin–Kernighan tour, and then perturb the tour by what has been described as a mutation that removes at least four edges and reconnecting the tour in a different way, then \"V\"-opting the new tour. The mutation is often enough to move the tour from the local minimum identified by Lin–Kernighan. \"V\"-opt methods are widely considered the most powerful heuristics for the problem, and are able to address special cases, such as the Hamilton Cycle Problem and other non-metric TSPs that other heuristics fail on. For many years Lin–Kernighan–Johnson had identified optimal solutions for all TSPs where an optimal solution was known and had identified the best known solutions for all other TSPs on which the method had been tried.\n\nOptimized Markov chain algorithms which use local searching heuristic sub-algorithms can find a route extremely close to the optimal route for 700 to 800 cities.\n\nTSP is a touchstone for many general heuristics devised for combinatorial optimization such as genetic algorithms, simulated annealing, tabu search, ant colony optimization, river formation dynamics (see swarm intelligence) and the cross entropy method.\n\nArtificial intelligence researcher Marco Dorigo described in 1993 a method of heuristically generating \"good solutions\" to the TSP using a simulation of an ant colony called \"ACS\" (\"ant colony system\"). It models behaviour observed in real ants to find short paths between food sources and their nest, an emergent behaviour resulting from each ant's preference to follow trail pheromones deposited by other ants.\n\nACS sends out a large number of virtual ant agents to explore many possible routes on the map. Each ant probabilistically chooses the next city to visit based on a heuristic combining the distance to the city and the amount of virtual pheromone deposited on the edge to the city. The ants explore, depositing pheromone on each edge that they cross, until they have all completed a tour. At this point the ant which completed the shortest tour deposits virtual pheromone along its complete tour route (\"global trail updating\"). The amount of pheromone deposited is inversely proportional to the tour length: the shorter the tour, the more it deposits.\n\nIn the \"metric TSP\", also known as \"delta-TSP\" or Δ-TSP, the intercity distances satisfy the triangle inequality.\n\nA very natural restriction of the TSP is to require that the distances between cities form a metric to satisfy the triangle inequality; that is the direct connection from \"A\" to \"B\" is never farther than the route via intermediate \"C\":\n\nThe edge spans then build a metric on the set of vertices. When the cities are viewed as points in the plane, many natural distance functions are metrics, and so many natural instances of TSP satisfy this constraint.\n\nThe following are some examples of metric TSPs for various metrics.\n\nThe last two metrics appear, for example, in routing a machine that drills a given set of holes in a printed circuit board. The Manhattan metric corresponds to a machine that adjusts first one co-ordinate, and then the other, so the time to move to a new point is the sum of both movements. The maximum metric corresponds to a machine that adjusts both co-ordinates simultaneously, so the time to move to a new point is the slower of the two movements.\n\nIn its definition, the TSP does not allow cities to be visited twice, but many applications do not need this constraint. In such cases, a symmetric, non-metric instance can be reduced to a metric one. This replaces the original graph with a complete graph in which the inter-city distance formula_22 is replaced by the shortest path between \"A\" and \"B\" in the original graph.\n\nWhen the input numbers can be arbitrary real numbers, Euclidean TSP is a particular case of metric TSP, since distances in a plane obey the triangle inequality. When the input numbers must be integers, comparing lengths of tours involves comparing sums of square-roots.\n\nLike the general TSP, Euclidean TSP is NP-hard in either case. With rational coordinates and discretized metric (distances rounded up to an integer), the problem is NP-complete. With rational coordinates and the actual Euclidean metric, Euclidean TSP is known to be in the Counting Hierarchy, a subclass of PSPACE. With arbitrary real coordinates, Euclidean TSP cannot be in such classes, since there are uncountably many possible inputs. However, Euclidean TSP is probably the easiest version for approximation. For example, the minimum spanning tree of the graph associated with an instance of the Euclidean TSP is a Euclidean minimum spanning tree, and so can be computed in expected O (\"n\" log \"n\") time for \"n\" points (considerably less than the number of edges). This enables the simple 2-approximation algorithm for TSP with triangle inequality above to operate more quickly.\n\nIn general, for any \"c\" > 0, where \"d\" is the number of dimensions in the Euclidean space, there is a polynomial-time algorithm that finds a tour of length at most (1 + 1/\"c\") times the optimal for geometric instances of TSP in\n\ntime; this is called a polynomial-time approximation scheme (PTAS). Sanjeev Arora and Joseph S. B. Mitchell were awarded the Gödel Prize in 2010 for their concurrent discovery of a PTAS for the Euclidean TSP.\n\nIn practice, simpler heuristics with weaker guarantees continue to be used.\n\nIn most cases, the distance between two nodes in the TSP network is the same in both directions. The case where the distance from \"A\" to \"B\" is not equal to the distance from \"B\" to \"A\" is called asymmetric TSP. A practical application of an asymmetric TSP is route optimization using street-level routing (which is made asymmetric by one-way streets, slip-roads, motorways, etc.).\n\nSolving an asymmetric TSP graph can be somewhat complex. The following is a 3×3 matrix containing all possible path weights between the nodes \"A\", \"B\" and \"C\". One option is to turn an asymmetric matrix of size \"N\" into a symmetric matrix of size 2\"N\".\n\nTo double the size, each of the nodes in the graph is duplicated, creating a second \"ghost node\", linked to the original node with a \"ghost\" edge of very low (possibly negative) weight, here denoted −\"w\". (Alternatively, the ghost edges have weight 0, and weight w is added to all other edges.) The original 3×3 matrix shown above is visible in the bottom left and the transpose of the original in the top-right. Both copies of the matrix have had their diagonals replaced by the low-cost hop paths, represented by −\"w\". In the new graph, no edge directly links original nodes and no edge directly links ghost nodes.\n\nThe weight −\"w\" of the \"ghost\" edges linking the ghost nodes to the corresponding original nodes must be low enough to ensure that all ghost edges must belong to any optimal symmetric TSP solution on the new graph (w=0 is not always low enough). As a consequence, in the optimal symmetric tour, each original node appears next to its ghost node (e.g. a possible path is formula_24) and by mergeing the original and ghost nodes again we get an (optimal) solution of the original asymmetric problem (in our example, formula_25).\n\nThere is an analogous problem in geometric measure theory which asks the following: under what conditions may a subset \"E\" of Euclidean space be contained in a rectifiable curve (that is, when is there a curve with finite length that visits every point in \"E\")? This problem is known as the analyst's travelling salesman problem\n\nSuppose formula_26 are formula_27 independent random variables with uniform distribution in the square formula_28, and let formula_29 be the shortest path length (i.e. TSP solution) for this set of points, according to the usual Euclidean distance. It is known that, almost surely,\n\nwhere formula_31 is a positive constant that is not known explicitly. Since formula_32 (see below), it follows from bounded convergence theorem that formula_33, hence lower and upper bounds on formula_31 follow from bounds on formula_35.\n\nThe almost sure limit formula_36 as formula_37 may not exist \nif the independent locations formula_26 are replaced with observations from a stationary ergodic process with uniform marginals.\n\n\n\n\n\n\nwhere 0.522 comes from the points near square boundary which have fewer neighbours,\nand Christine L. Valenzuela and Antonia J. Jones obtained the following other numerical lower bound:\n\nThe problem has been shown to be NP-hard (more precisely, it is complete for the complexity class FP; see function problem), and the decision problem version (\"given the costs and a number \"x\", decide whether there is a round-trip route cheaper than \"x\"\") is NP-complete. The bottleneck traveling salesman problem is also NP-hard. The problem remains NP-hard even for the case when the cities are in the plane with Euclidean distances, as well as in a number of other restrictive cases. Removing the condition of visiting each city \"only once\" does not remove the NP-hardness, since it is easily seen that in the planar case there is an optimal tour that visits each city only once (otherwise, by the triangle inequality, a shortcut that skips a repeated visit would not increase the tour length).\n\nIn the general case, finding a shortest travelling salesman tour is NPO-complete. If the distance measure is a metric (and thus symmetric), the problem becomes APX-complete and Christofides’s algorithm approximates it within 1.5.\nThe best known inapproximability bound is 123/122 .\n\nIf the distances are restricted to 1 and 2 (but still are a metric) the approximation ratio becomes 8/7. In the asymmetric case with triangle inequality, only logarithmic performance guarantees are known, the best current algorithm achieves performance ratio 0.814 log(\"n\"); it is an open question if a constant factor approximation exists.\nThe best known inapproximability bound is 75/74 .\n\nThe corresponding maximization problem of finding the \"longest\" travelling salesman tour is approximable within 63/38. If the distance function is symmetric, the longest tour can be approximated within 4/3 by a deterministic algorithm and within formula_62 by a randomized algorithm.\n\nThe TSP, in particular the Euclidean variant of the problem, has attracted the attention of researchers in cognitive psychology. It has been observed that humans are able to produce near-optimal solutions quickly, in a close-to-linear fashion, with performance that ranges from 1% less efficient for graphs with 10-20 nodes, and 11% more efficient for graphs with 120 nodes. The apparent ease with which humans accurately generate near-optimal solutions to the problem has led researchers to hypothesize that humans use one or more heuristics, with the two most popular theories arguably being the convex-hull hypothesis and the crossing-avoidance heuristic. However, additional evidence suggests that human performance is quite varied, and individual differences as well as graph geometry appear to impact performance in the task. Nevertheless, results suggest that computer performance on the TSP may be improved by understanding and emulating the methods used by humans for these problems, and have also led to new insights into the mechanisms of human thought. The first issue of the \"Journal of Problem Solving\" was devoted to the topic of human performance on TSP, and a 2011 review listed dozens of papers on the subject.\n\nWhen presented with a spatial configuration of food sources, the amoeboid Physarum polycephalum adapts its morphology to create an efficient path between the food sources which can also be viewed as an approximate solution to TSP. It's considered to present interesting possibilities and it has been studied in the area of natural computing.\n\nFor benchmarking of TSP algorithms, TSPLIB is a library of sample instances of the TSP and related problems is maintained, see the TSPLIB external reference. Many of them are lists of actual cities and layouts of actual printed circuits.\n\n\n\n\n"}
{"id": "18796818", "url": "https://en.wikipedia.org/wiki?curid=18796818", "title": "Unitary method", "text": "Unitary method\n\nThe unitary method is an algebraic technique for solving a problem by first finding the value of a single unit, i.e., 1, (by dividing) and then finding the necessary value by multiplying the single unit value. In essence, the unitary method is used to find the value of a unit from the value of a multiple, and hence the value of a multiple. With the unitary method, it is not always necessary to find the value of single unit; let us study it with the help of examples below.\n\nFor example, to solve the problem: \"A man walks 7 miles in 2 hours. How far does he walk in 7 hours?\", one would first calculate how far the man walks in 1 hour. One can safely assume that he would walk half the distance in half the time. Therefore, dividing by 2, the man walks 3.5 miles in 1 hour. Multiplying by 7 for 7 hours, the man walks 7x3.5=24.5 miles, or let us consider the distance travelled by the man be X, then divide it given distance that is 7 (x/7). It is equal to the time taken to travel X distance that is 7 hours divided by the time taken to travel 7 miles, that is 2 hours (7/2), therefore x/7=7/2, hence X=24.5 miles.\n\nThe same method can be applied to the problem: \"A man walks at 4 miles per hour. How long would it take him to cover 5 miles?\". Dividing by 4 shows that the man covers 1 mile in a quarter (0.25) of an hour. Multiplying by 5 shows that the man therefore takes 1 hour and a quarter (1.25 hours) to cover 5 miles. Similarly, by the second method, we can find the value of time taken to cover 5 miles.\nThe 1st method is more preferable and easier.\n\n"}
{"id": "49690041", "url": "https://en.wikipedia.org/wiki?curid=49690041", "title": "Wild problem", "text": "Wild problem\n\nA mathematical problem is wild if it contains the problem of classifying pairs of square matrices up to simultaneous similarity. Examples of wild problems are classifying indecomposable representations of any quiver that is neither a Dynkin quiver (i.e. the underlying undirected graph of the quiver is a (finite) Dynkin diagram) nor a Euclidean quiver (i.e. the underlying undirected graph of the quiver is an Affine Dynkin diagram).\n\nNecessary and sufficient conditions have been proposed to check the simultaneously block triangularization and diagonalization of a finite set of matrices under the assumption that each matrix is over the field of the complex numbers.\n"}
