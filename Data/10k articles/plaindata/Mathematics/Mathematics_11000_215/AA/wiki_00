{"id": "410565", "url": "https://en.wikipedia.org/wiki?curid=410565", "title": "119 (number)", "text": "119 (number)\n\n119 (one hundred [and] nineteen) is the natural number following 118 and preceding 120.\n\n\n\n\n"}
{"id": "1045573", "url": "https://en.wikipedia.org/wiki?curid=1045573", "title": "133 (number)", "text": "133 (number)\n\n133 (one hundred [and] thirty-three) is the natural number following 132 and preceding 134.\n\n133 is an \"n\" whose divisors (excluding \"n\" itself) added up divide φ(\"n\"). It is an octagonal number and a Harshad number. It is also a happy number.\n\n133 is a repdigit in base 11 (111) and base 18 (77), whilst in base 20 it is a cyclic number formed from the reciprocal of the number three.\n\n133 is a semiprime: a product of two prime numbers, namely 7 and 19. Since those prime factors are Gaussian primes, this means that 133 is a Blum integer.\n\n\n\n133 is also:\n\n"}
{"id": "35475314", "url": "https://en.wikipedia.org/wiki?curid=35475314", "title": "A-1 (code)", "text": "A-1 (code)\n\nA-1 was the designation for a code used by the United States Navy during World War I that replaced the Secret Code of 1887, SIGCODE and another system designed for radio communication. The cryptographic system was developed by Lt. W.W. Smith in the Office of Naval Operations by randomly associating key words with 5 letter patterns. \n"}
{"id": "23240037", "url": "https://en.wikipedia.org/wiki?curid=23240037", "title": "Antithetic variates", "text": "Antithetic variates\n\nIn statistics, the antithetic variates method is a variance reduction technique used in Monte Carlo methods. Considering that the error reduction in the simulated signal (using Monte Carlo methods) has a square root convergence, a very large number of sample paths is required to obtain an accurate result. The antithetic variates method reduces the variance of the simulation results.\n\nThe antithetic variates technique consists, for every sample path obtained, in taking its antithetic path — that is given a path formula_1 to also take formula_2. The advantage of this technique is twofold: it reduces the number of normal samples to be taken to generate \"N\" paths, and it reduces the variance of the sample paths, improving the accuracy.\n\nSuppose that we would like to estimate \n\nFor that we have generated two samples\n\nAn unbiased estimate of formula_5 is given by\n\nAnd \n\nso variance is reduced if formula_8 is negative.\n\nIf the law of the variable \"X\" follows a uniform distribution along [0, 1], the first sample will be formula_9, where, for any given \"i\", formula_10 is obtained from \"U\"(0, 1). The second sample is built from formula_11, where, for any given \"i\": formula_12. If the set formula_13 is uniform along [0, 1], so are formula_14. Furthermore, covariance is negative, allowing for initial variance reduction.\n\nWe would like to estimate\n\nThe exact result is formula_16. This integral can be seen as the expected value of formula_17, where\n\nand \"U\" follows a uniform distribution [0, 1].\n\nThe following table compares the classical Monte Carlo estimate (sample size: 2\"n\", where \"n\" = 1500) to the antithetic variates estimate (sample size: \"n\", completed with the transformed sample 1 − \"u\"):\n\nThe use of the antithetic variates method to estimate the result shows an important variance reduction.\n"}
{"id": "8403499", "url": "https://en.wikipedia.org/wiki?curid=8403499", "title": "Aperture card", "text": "Aperture card\n\nAn aperture card is a type of punched card with a cut-out window into which a chip of microfilm is mounted. Such a card is used for archiving or for making multiple inexpensive copies of a document for ease of distribution. The card is typically punched with machine-readable metadata associated with the microfilm image, and printed across the top of the card for visual identification; it may also be punched by hand in the form of an edge-notched card. The microfilm chip is most commonly 35mm in height, and contains an optically reduced image, usually of some type of reference document, such as an engineering drawing, that is the focus of the archiving process. Machinery exists to automatically store, retrieve, sort, duplicate, create, and digitize cards with a high level of automation.\n\nAperture cards have several advantages and disadvantages when compared to digital systems. While many aperture cards still play an important role in archiving, their role is gradually being replaced by digital systems.\n\nAperture cards are used for engineering drawings from all engineering disciplines. The U.S. Department of Defense once made extensive use of aperture cards, and some are still in use, but most data is now digital.\n\nInformation about the drawing, for example the drawing number, could be both punched and printed on the remainder of the card. With the proper machinery, this allows for automated handling. In the absence of such machinery, the cards can still be read by a human with a lens and a light source.\n\nAperture cards have, for archival purposes, some advantages over digital systems. They have a 500-year lifetime, they are human readable, and there is no expense or risk in converting from one digital format to the next when computer systems become obsolete.\n\nMost of the disadvantages are related to the well established differences in analog and digital technology. In particular, searching for given strings within content is considerably slower. Handling physical cards requires proprietary machinery and processing optical film takes significant time.\n\nThe very nature of microfilm cameras and the high contrast properties of microfilm stock itself also impose limits on the amount of detail that can be resolved particularly at the higher reduction ratios (36x or greater) needed to film larger drawings. Faded drawings or those of low or uneven contrast do not reproduce well and significant detail or annotations may be lost.\n\nIn common with other forms of microfilm mis-filing cards after use, particularly in large archives, results in the card being for all intents and purposes lost forever unless it's later found by accident.\n\nAperture cards created from 35mm roll film mounted on to blank cards have to be treated with great care. Bending the card can cause the film to detach and excessive pressure to a stack of cards can cause the mounting glue to ooze creating clumps of cards which will feed through duplicators and other machinery either poorly or not at all. Feeding a de-laminated card through machinery not only risks destroying the image it also risks jamming or damaging the machinery.\n\nA set of cards could be rapidly sorted by drawing number or other punched data using a card sorter. Machines are now available that scan aperture cards and produce a digital version. Aperture card plotters are machines that use a laser to create the image on the film.\n\nAperture cards can be converted to digital documents using scanning equipment and software. Scanning can allow for significant image cleanup and enhancement. Often, the digital image produced is better than the visual quality available prescan.\n\n"}
{"id": "57921107", "url": "https://en.wikipedia.org/wiki?curid=57921107", "title": "Baruch Solomon Löwenstein", "text": "Baruch Solomon Löwenstein\n\nBaruch Solomon Löwenstein (born in mid-nineteenth century, Włodarka, Russia) was a Jewish mathematician. He wrote \"Bikkure ha-Limmudiyyot\", explanations of mathematical passages in the works of Abraham ibn Ezra, Moses Maimonides, and Joseph Delmedigo. He also annotated and published in 1863 a second edition of \"Shebile di-Reḳia\", by Elias ben Ḥayyim Kohen Höchheimer, on the rules of the calendar, with the elements of geometry, trigonometry, and astronomy.\n\n"}
{"id": "2260933", "url": "https://en.wikipedia.org/wiki?curid=2260933", "title": "Bijective numeration", "text": "Bijective numeration\n\nBijective numeration is any numeral system in which every non-negative integer can be represented in exactly one way using a finite string of digits. The name derives from this bijection (one-to-one correspondence) between the set of non-negative integers and the set of finite strings using a finite set of symbols (the \"digits\").\n\nMost ordinary numeral systems, such as the common decimal system, are not bijective because more than one string of digits can represent the same positive integer. In particular, adding leading zeroes does not change the value represented, so \"1\", \"01\" and \"001\" all represent the number one. Even though only the first is usual, the fact that the others are possible means that decimal is not bijective. However, unary, with only one digit, \"is\" bijective.\n\nA bijective base-\"k\" numeration is a bijective positional notation. It uses a string of digits from the set {1, 2, ..., \"k\"} (where \"k\" ≥ 1) to encode each positive integer; a digit's position in the string defines its value as a multiple of a power of \"k\". calls this notation \"k\"-adic, but it should not be confused with the \"p\"-adic numbers: bijective numerals are a system for representing ordinary integers by finite strings of nonzero digits, whereas the \"p\"-adic numbers are a system of mathematical values that contain the integers as a subset and may need infinite sequences of digits in any numerical representation.\n\nThe base-\"k\" bijective numeration system uses the digit-set {1, 2, ..., \"k\"} (\"k\" ≥ 1) to uniquely represent every non-negative integer, as follows:\n\n\n\nIn contrast, standard positional notation can be defined with a similar recursive algorithm where \n\nFor a given base \"k\" ≥ 1,\n\n\nThe bijective base-10 system is a base ten positional numeral system that does not use a digit to represent zero. It instead has a digit to represent ten, such as \"A\".\n\nAs with conventional decimal, each digit position represents a power of ten, so for example 123 is \"one hundred, plus two tens, plus three units.\" All positive integers that are represented solely with non-zero digits in conventional decimal (such as 123) have the same representation in decimal without a zero. Those that use a zero must be rewritten, so for example 10 becomes A, conventional 20 becomes 1A, conventional 100 becomes 9A, conventional 101 becomes A1, conventional 302 becomes 2A2, conventional 1000 becomes 99A, conventional 1110 becomes AAA, conventional 2010 becomes 19AA, and so on.\n\nAddition and multiplication in decimal without a zero are essentially the same as with conventional decimal, except that carries occur when a position exceeds ten, rather than when it exceeds nine. So to calculate 643 + 759, there are twelve units (write 2 at the right and carry 1 to the tens), ten tens (write A with no need to carry to the hundreds), thirteen hundreds (write 3 and carry 1 to the thousands), and one thousand (write 1), to give the result 13A2 rather than the conventional 1402.\n\nIn the bijective base-26 system one may use the Latin alphabet letters \"A\" to \"Z\" to represent the 26 digit values one to twenty-six. (A=1, B=2, C=3, ..., Z=26)\n\nWith this choice of notation, the number sequence (starting from 1) begins A, B, C, ..., X, Y, Z, AA, AB, AC, ..., AX, AY, AZ, BA, BB, BC, ...\n\nEach digit position represents a power of twenty-six, so for example, the numeral ABC represents the value = 731 in base 10.\n\nMany spreadsheets including Microsoft Excel use this system to assign labels to the columns of a spreadsheet, starting A, B, C, ..., Z, AA, AB, ..., AZ, BA, ..., ZZ, AAA, etc. For instance, in Excel 2013, there can be up to 16384 columns, labeled from A to XFD. A variant of this system is used to name variable stars. It can be applied to any problem where a systematic naming using letters is desired, while using the shortest possible strings.\n\nThe fact that every non-negative integer has a unique representation in bijective base-\"k\" (\"k\" ≥ 1) is a \"folk theorem\" that has been rediscovered many times. Early instances are for the case \"k\" = 10, and and for all \"k\" ≥ 1. Smullyan uses this system to provide a Gödel numbering of the strings of symbols in a logical system; Böhm uses these representations to perform computations in the programming language P′′. mentions the special case of \"k\" = 10, and discusses the cases \"k\" ≥ 2. appears to be another rediscovery, and hypothesizes that if ancient numeration systems used bijective base-\"k\", they might not be recognized as such in archaeological documents, due to general unfamiliarity with this system.\n\n"}
{"id": "39082282", "url": "https://en.wikipedia.org/wiki?curid=39082282", "title": "Bony–Brezis theorem", "text": "Bony–Brezis theorem\n\nIn mathematics, the Bony–Brezis theorem, due to the French mathematicians Jean-Michel Bony and Haïm Brezis, gives necessary and sufficient conditions for a closed subset of a manifold to be invariant under the flow defined by a vector field, namely at each point of the closed set the vector field must have non-positive inner product with any exterior normal vector to the set. A vector is an \"exterior normal\" at a point of the closed set if there is a real-valued continuously differentiable function maximized locally at the point with that vector as its derivative at the point. If the closed subset is a smooth submanifold with boundary, the condition states that the vector field should not point outside the subset at boundary points. The generalization to non-smooth subsets is important in the theory of partial differential equations.\n\nThe theorem had in fact been previously discovered by Mitio Nagumo in 1942 and is also known as the Nagumo theorem.\n\nLet \"F\" be closed subset of a C manifold \"M\" and let \"X\" be a vector field on \"M\" which is Lipschitz continuous. The following conditions are equivalent:\n\n\nFollowing , to prove that the first condition implies the second, let \"c\"(\"t\") be an integral curve with\n\"c\"(0) = \"x\" in \"F\" and \"dc/dt\"= \"X\"(\"c\"). Let \"g\" have a local maximum on \"F\" at \"x\". Then \"g\"(\"c\"(\"t\")) ≤ \"g\" (\"c\"(0)) for \"t\" small and positive. Differentiating, this implies that \"g\" '(\"x\")⋅\"X\"(\"x\") ≤ 0.\n\nTo prove the reverse implication, since the result is local. it enough to check it in R. In that case \"X\" locally satisfies a Lipschitz condition\n\nIf \"F\" is closed, the distance function \"D\"(\"x\") = \"d\"(\"x\",\"F\") has the following differentiability property:\n\nwhere the minimum is taken over the closest points \"z\" to \"x\" in \"F\".\n\nThe differentiability property implies that\n\nminimized over closest points \"z\" to \"c\"(\"t\"). For any such \"z\"\n\nSince −|\"y\" − \"c\"(\"t\")| has a local maximum on \"F\" at \"y\" = \"z\", \"c\"(\"t\") − \"z\" is an exterior normal vector at \"z\". So the first term on the right hand side is non-negative. The Lipschitz condition for \"X\" implies the second term is bounded above by 2\"C\"⋅\"D\"(\"c\"(\"t\")). Thus the derivative from the right of\n\nis non-positive, so it is a non-increasing function of \"t\". Thus if \"c\"(0) lies in \"F\", \"D\"(\"c\"(0))=0 and hence \"D\"(\"c\"(\"t\")) = 0 for \"t\" > 0, i.e. \"c\"(\"t\") lies in \"F\" for \"t\" > 0.\n\n"}
{"id": "27309180", "url": "https://en.wikipedia.org/wiki?curid=27309180", "title": "CompCert", "text": "CompCert\n\nCompCert is a formally verified optimizing compiler for a large subset of the C99 programming language which currently targets PowerPC, ARM, RISC-V, x86 and x86-64 architectures. This project, led by Xavier Leroy, started officially in 2005, funded by the French institutes ANR and INRIA. The compiler is specified, programmed and proved in Coq. It aims to be used for programming embedded systems requiring reliability. The performance of its generated code is often close to that of GCC (version 3) at optimization level -O1, and always better than that of GCC without optimizations.\n\nSince 2015 AbsInt offers commercial licenses, provides support and maintenance, and contributes to the advancement of the tool.\n\n"}
{"id": "3834818", "url": "https://en.wikipedia.org/wiki?curid=3834818", "title": "Comparability", "text": "Comparability\n\nIn mathematics, any two elements \"x\" and \"y\" of a set \"P\" that is partially ordered by a binary relation ≤ are comparable when either \"x\" ≤ \"y\" or \"y\" ≤ \"x\". If it is not the case that \"x\" and \"y\" are comparable, then they are called incomparable.\n\nA totally ordered set is exactly a partially ordered set in which every pair of elements is comparable.\n\nIt follows immediately from the definitions of \"comparability\" and \"incomparability\" that both relations are symmetric, that is \"x\" is comparable to \"y\" if and only if \"y\" is comparable to \"x\", and likewise for incomparability.\n\nComparability is denoted by the symbol formula_1, and incomparability by the symbol formula_2.\nThus, for any pair of elements \"x\" and \"y\" of a partially ordered set, exactly one of\nis true.\n\nThe comparability graph of a partially ordered set \"P\" has as vertices the elements of \"P\" and has as edges precisely those pairs {\"x\", \"y\"} of elements for which formula_5 .\n\nWhen classifying mathematical objects (e.g., topological spaces), two \"criteria\" are said to be comparable when the objects that obey one criterion constitute a subset of the objects that obey the other, which is to say when they are comparable under the partial order ⊂. For example, the T and T criteria are comparable, while the T and sobriety criteria are not.\n\n"}
{"id": "18867510", "url": "https://en.wikipedia.org/wiki?curid=18867510", "title": "Conditional mutual information", "text": "Conditional mutual information\n\nIn probability theory, particularly information theory, the conditional mutual information is, in its most basic form, the expected value of the mutual information of two random variables given the value of a third.\n\nFor discrete random variables formula_1 formula_2 and formula_3 we define\nwhere the marginal, joint, and/or conditional probability mass functions are denoted by formula_5 with the appropriate subscript. This can be simplified as\nAlternatively, we may write in terms of joint and conditional entropies as\nThis can be rewritten to show its relationship to mutual information\nusually rearranged as the chain rule for mutual information\nAnother equivalent form of the above is\n\nLike mutual information, conditional mutual information can be expressed as a Kullback–Leibler divergence:\n\nOr as an expected value of simpler Kullback–Leibler divergences:\n\nA more general definition of conditional mutual information, applicable to random variables with continuous or other arbitrary distributions, will depend on the concept of regular conditional probability. (See also.)\n\nLet formula_14 be a probability space, and let the random variables \"X\", \"Y\", and \"Z\" each be defined as a Borel-measurable function from formula_15 to some state space endowed with a topological structure.\n\nConsider the Borel measure (on the σ-algebra generated by the open sets) in the state space of each random variable defined by assigning each Borel set the formula_16-measure of its preimage in formula_17. This is called the pushforward measure formula_18 The support of a random variable is defined to be the topological support of this measure, i.e. formula_19\n\nNow we can formally define the conditional probability measure given the value of one (or, via the product topology, more) of the random variables. Let formula_20 be a measurable subset of formula_21 (i.e. formula_22) and let formula_23 Then, using the disintegration theorem:\nwhere the limit is taken over the open neighborhoods formula_25 of formula_26, as they are allowed to become arbitrarily smaller with respect to set inclusion.\n\nFinally we can define the conditional mutual information via Lebesgue integration:\nwhere the integrand is the logarithm of a Radon–Nikodym derivative involving some of the conditional probability measures we have just defined.\n\nIn an expression such as formula_28 formula_29 formula_30 and formula_31 need not necessarily be restricted to representing individual random variables, but could also represent the joint distribution of any collection of random variables defined on the same probability space. As is common in probability theory, we may use the comma to denote such a joint distribution, e.g. formula_32 Hence the use of the semicolon (or occasionally a colon or even a wedge formula_33) to separate the principal arguments of the mutual information symbol. (No such distinction is necessary in the symbol for joint entropy, since the joint entropy of any number of random variables is the same as the entropy of their joint distribution.)\n\nIt is always true that\nfor discrete, jointly distributed random variables \"X\", \"Y\", \"Z\". This result has been used as a basic building block for proving other inequalities in information theory, in particular, those known as Shannon-type inequalities.\n\nConditioning on a third random variable may either increase or decrease the mutual information: that is, the difference formula_35, called the interaction information, may be positive, negative, or zero. This is the case even when random variables are pairwise independent. Such is the case when: formula_36in which case \"X,Y,Z\" are pairwise independent and in particular formula_37, but formula_38\n\nThe conditional mutual information can be used to inductively define a multivariate mutual information in a set- or measure-theoretic sense in the context of information diagrams. In this sense we define the multivariate mutual information as follows:\nwhere\nThis definition is identical to that of interaction information except for a change in sign in the case of an odd number of random variables. A complication is that this multivariate mutual information (as well as the interaction information) can be positive, negative, or zero, which makes this quantity difficult to interpret intuitively. In fact, for \"n\" random variables, there are formula_41 degrees of freedom for how they might be correlated in an information-theoretic sense, corresponding to each non-empty subset of these variables. These degrees of freedom are bounded by various Shannon- and non-Shannon-type inequalities in information theory.\n"}
{"id": "2056790", "url": "https://en.wikipedia.org/wiki?curid=2056790", "title": "Craig interpolation", "text": "Craig interpolation\n\nIn mathematical logic, Craig's interpolation theorem is a result about the relationship between different logical theories. Roughly stated, the theorem says that if a formula φ implies a formula ψ, and the two have at least one atomic variable symbol in common, then there is a third formula ρ, called an interpolant, such that every nonlogical symbol in ρ occurs both in φ and ψ, φ implies ρ, and ρ implies ψ. The theorem was first proved for first-order logic by William Craig in 1957. Variants of the theorem hold for other logics, such as propositional logic. A stronger form of Craig's theorem for first-order logic was proved by Roger Lyndon in 1959; the overall result is sometimes called the Craig–Lyndon theorem.\n\nIn propositional logic, let\n\nThen φ tautologically implies ψ. This can be verified by writing φ in conjunctive normal form:\nThus, if φ holds, then (P ∨ ~R) holds. In turn, (P ∨ ~R) tautologically implies ψ. Because the two propositional variables occurring in (P ∨ ~R) occur in both φ and ψ, this means that (P ∨ ~R) is an interpolant for the implication φ → ψ.\n\nSuppose that \"S\" and \"T\" are two first-order theories. As notation, let \"S\" ∪ \"T\" denote the smallest theory including both \"S\" and \"T\"; the signature of \"S\" ∪ \"T\" is the smallest one containing the signatures of \"S\" and \"T\". Also let \"S\" ∩ \"T\" be the intersection of the languages of the two theories; the signature of \"S\" ∩ \"T\" is the intersection of the signatures of the two languages.\n\nLyndon's theorem says that if \"S\" ∪ \"T\" is unsatisfiable, then there is an interpolating sentence ρ in the language of \"S\" ∩ \"T\" that is true in all models of \"S\" and false in all models of \"T\". Moreover, ρ has the stronger property that every relation symbol that has a positive occurrence in ρ has a positive occurrence in some formula of \"S\" and a negative occurrence in some formula of \"T\", and every relation symbol with a negative occurrence in ρ has a negative occurrence in some formula of \"S\" and a positive occurrence in some formula of \"T\".\n\nWe present here a constructive proof of the Craig interpolation theorem for propositional logic. Formally, the theorem states:\n\n\"If ⊨φ → ψ then there is a ρ (the \"interpolant\") such that ⊨φ → ρ and ⊨ρ → ψ, where atoms(ρ) ⊆ atoms(φ) ∩ atoms(ψ). Here atoms(φ) is the set of propositional variables occurring in φ, and ⊨ is the semantic entailment relation for propositional logic.\"\n\nProof.\nAssume ⊨φ → ψ. The proof proceeds by induction on the number of propositional variables occurring in φ that do not occur in ψ, denoted |\"atoms\"(φ) − \"atoms\"(ψ)|.\n\nBase case |\"atoms\"(φ) − \"atoms\"(ψ)| = 0: In this case, φ is suitable. This is because since |\"atoms\"(φ) − \"atoms\"(ψ)| = 0, we know that \"atoms\"(φ) ⊆ \"atoms\"(φ) ∩ \"atoms\"(ψ). Moreover we have that ⊨φ → φ and ⊨φ → ψ. This suffices to show that φ is a suitable interpolant in this case.\n\nLet’s assume for the inductive step that the result has been shown for all χ where |\"atoms\"(χ) − \"atoms\"(ψ)| = n. Now assume that |\"atoms\"(φ) − \"atoms\"(ψ)| = n+1. Pick a \"p\" ∈ \"atoms\"(φ) but \"p\" ∉ \"atoms\"(ψ). Now define:\n\nφ' := φ[⊤/\"p\"] ∨ φ[⊥/\"p\"]\n\nHere φ[⊤/\"p\"] is the same as φ with every occurrence of \"p\" replaced by ⊤ and φ[⊥/\"p\"] similarly replaces \"p\" with ⊥. We may observe three things from this definition:\n\nFrom , and the inductive step we have that there is an interpolant ρ such that:\n\nBut from and we know that\n\nHence, ρ is a suitable interpolant for φ and ψ.\n\nQED\n\nSince the above proof is constructive, one may extract an algorithm for computing interpolants. Using this algorithm, if \"n\" = |\"atoms\"(φ') − \"atoms\"(ψ)|, then the interpolant ρ has \"O\"(\"EXP\"(\"n\")) more logical connectives than φ (see Big O Notation for details regarding this assertion). Similar constructive proofs may be provided for the basic modal logic K, intuitionistic logic and μ-calculus, with similar complexity measures.\n\nCraig interpolation can be proved by other methods as well. However, these proofs are generally non-constructive:\n\nCraig interpolation has many applications, among them consistency proofs, model checking, proofs in modular specifications, modular ontologies.\n\n"}
{"id": "3852459", "url": "https://en.wikipedia.org/wiki?curid=3852459", "title": "Cryptovirology", "text": "Cryptovirology\n\nCryptovirology is a field that studies how to use cryptography to design powerful malicious software. The field was born with the observation that public-key cryptography can be used to break the symmetry between what an antivirus analyst sees regarding malware and what the attacker sees. The antivirus analyst sees a public key contained in the malware whereas the attacker sees the public key contained in the malware as well as the corresponding private key (outside the malware) since the attacker created the key pair for the attack. The public key allows the malware to perform trapdoor one-way operations on the victim's computer that only the attacker can undo.\n\nThe first cryptovirology attack, invented by Adam L. Young and Moti Yung, is called \"cryptoviral extortion\" and it was presented at the 1996 IEEE Security & Privacy conference. In this attack a cryptovirus, cryptoworm, or cryptotrojan contains the public key of the attacker and hybrid encrypts the victim's files. The malware prompts the user to send the asymmetric ciphertext to the attacker who will decipher it and return the symmetric decryption key it contains for a fee. The victim needs the symmetric key to decrypt the encrypted files if there is no way to recover the original files (e.g., from backups). The 1996 IEEE paper predicted that cryptoviral extortion attackers would one day demand e-money, long before bitcoin even existed. Many years later the media relabeled cryptoviral extortion as ransomware. In 2016 cryptovirology attacks on healthcare providers reached epidemic levels prompting the U.S. Department of Health and Human Services to issue a Fact Sheet on Ransomware and\nHIPAA.\nThe fact sheet states that when electronic protected health information is encrypted by ransomware a breach has occurred and the attack therefore constitutes a \"disclosure\" that is not permitted under HIPAA, the rationale being that an adversary has taken control of the information. Sensitive data might never leave the victim organization, but the break-in may have allowed data to be sent out undetected. California enacted a law that defines the introduction of ransomware into a computer system with the intent of extortion as being against the law.\n\nThe field also encompasses covert malware attacks in which the attacker \"securely\" steals private information such as symmetric keys, private keys, PRNG state, and the victim's data. Examples of such covert attacks are asymmetric backdoors. An asymmetric backdoor is a backdoor (e.g., in a cryptosystem) that can be used only by the attacker, even after it is found. This contrasts with the traditional backdoor that is symmetric, i.e., anyone that finds it can use it. Kleptography, a subfield of cryptovirology, is the study of asymmetric back doors in key generation algorithms, digital signature algorithms, key exchanges, pseudorandom number generators, encryption algorithms, and other cryptographic algorithms. The NIST Dual EC DRBG random bit generator has an asymmetric backdoor in it. The EC-DRBG algorithm utilizes the discrete-log kleptogram from Kleptography which by definition makes the EC-DRBG a cryptotrojan. Like ransomware, the EC-DRBG cryptotrojan contains and uses the attacker's public key to attack the host system. The cryptographer Ari Juels indicated that\nNSA effectively orchestrated a kleptographic attack on users of the Dual EC DRBG pseudorandom number generation algorithm and that, although security professionals and developers have been testing and implementing kleptographic attacks since 1996, \"you would be hard-pressed to find one in actual use until now\". Due to public outcry of this cryptovirology attack, NIST rescinded the EC-DRBG algorithm from the NIST SP 800-90 standard.\n\nCovert information leakage attacks carried out by cryptoviruses, cryptotrojans, and cryptoworms that, by definition, contain and use the public key of the attacker is a major theme in cryptovirology. In \"deniable password snatching\", a cryptovirus installs a cryptotrojan that asymmetrically encrypts host data and covertly broadcasts it. This makes it available to everyone, noticeable by no one (except the attacker), and only decipherable by the attacker. An attacker caught installing the cryptotrojan claims to be a virus victim. An attacker observed receiving the covert asymmetric broadcast is one of thousands if not millions of receivers and exhibits no identifying information whatsoever. The cryptovirology attack achieves \"end-to-end deniability\". It is a covert asymmetric broadcast of the victim's data. Cryptovirology also encompasses the use of private information retrieval to allow cryptoviruses to search for and steal host data without revealing the data searched for even when the cryptotrojan is under constant surveillance. By definition such a cryptovirus carries within its own coding sequence the query of the attacker and the necessary PIR logic to apply the query to host systems.\n\nThere has been a long-standing misconception that cryptovirology is mostly about extortion attacks (overt attacks). In fact, the vast majority of cryptovirology attacks are covert in nature. This misconception began to fade in 2013 after whistleblowing revealed that the Dual EC DRBG is a cryptovirology attack that covertly leaks the internal state of the pseudorandom number generator\n\nCryptovirology was born in academia. \nIt is an investigation into how modern cryptographic paradigms and tools can be used to strengthen, improve, and develop new malicious software (malware) attacks.\nCryptovirology extends beyond finding protocol failures and design vulnerabilities. It is a forward-engineering discipline that can be used for attacking rather than defending.\n\nA \"questionable encryption scheme\", is an attack tool in cryptovirology. Informally speaking, a questionable encryption scheme is a public key cryptosystem (3-tuple of algorithms) with two supplementary algorithms, forming a 5-tuple of algorithms. It includes a deliberately bogus yet carefully designed key pair generation algorithm that produces a \"fake\" public key. The corresponding private key (witness of non-encryption) cannot be used to decipher data \"encrypted\" using the fake public key. By supplying the key pair to an efficient verification predicate (the 5th algorithm in the 5-tuple) it is proven whether the public key is real or fake. When the public key is fake, it follows that no one can decipher data \"enciphered\" using the fake public key. A questionable encryption scheme has the property that real public keys are computationally indistinguishable from fake public keys when the private key is not available. The private key forms a poly-sized witness of decipherability or indecipherability, whichever may be the case.\n\nAn application of a questionable encryption scheme is a trojan that gathers plaintext from the host, \"encrypts\" it using the trojan's own public key (which may be real or fake), and then exfiltrates the resulting \"ciphertext\". In this attack it is thoroughly intractable to prove that data theft has occurred. This holds even when all core dumps of the trojan and all the information that it broadcasts is entered into evidence. An analyst that jumps to the conclusion that the trojan \"encrypts\" data risks being proven wrong by the malware author (e.g., anonymously).\n\nWhen the public key is fake, the attacker gets no plaintext from the trojan. So what's the use? A spoofing attack is possible in which some trojans are released that use real public keys and steal data and some trojans are released that use fake public keys and do not steal data. Many months after the trojans are discovered and analyzed, the attacker anonymously posts the witnesses of non-encryption for the fake public keys. This proves that those trojans never in fact exfiltrated data. This casts doubt on the true nature of future strains of malware that contain such \"public keys\", since the keys could be real or fake. This attack implies a fundamental limitation on proving data theft.\n\nThere are many other attacks in the field of cryptovirology that are not mentioned here.\n\nWhile viruses in the wild have used cryptography in the past, the only purpose of such usage of cryptography was to avoid detection by antivirus software. For example, the tremor virus used polymorphism as a defensive technique in an attempt to avoid detection by anti-virus software. Though cryptography does assist in such cases to enhance the longevity of a virus, the capabilities of cryptography are not used in the payload. The One-half virus was amongst the first viruses known to have encrypted affected files. However, the One_half virus was not ransomware, that is it did not demand any ransom for decrypting the files that it has encrypted. It also did not use public key cryptography.\nAn example of a virus that informs the owner of the infected machine to pay a ransom is the virus nicknamed Tro_Ransom.A. This virus asks the owner of the infected machine to send $10.99 to a given account through Western Union.<br>\nVirus.Win32.Gpcode.ag is a classic cryptovirus. This virus partially uses a version of 660-bit RSA and encrypts files with many different extensions. It instructs the owner of the machine to email a given mail ID if the owner desires the decryptor. If contacted by email, the user will be asked to pay a certain amount as ransom in return for the decryptor.\n\nTo successfully write a cryptovirus, a thorough knowledge of the various cryptographic primitives such as random number generators, proper recommended cipher text chaining modes etc. are necessary. Wrong choices can lead to poor cryptographic strength. So, usage of preexisting routines would be ideal. Microsoft's Cryptographic API (CAPI), is a possible tool for the same. It has been demonstrated that using just 8 different calls to this API, a cryptovirus can satisfy all its encryption needs.\n\nApart from cryptoviral extortion, there are other potential uses of cryptoviruses. They are used in deniable password snatching, used with cryptocounters,\nused with private information retrieval and used in secure communication between different instances of a distributed cryptovirus.\n\n"}
{"id": "4854281", "url": "https://en.wikipedia.org/wiki?curid=4854281", "title": "Dannie Heineman Prize for Mathematical Physics", "text": "Dannie Heineman Prize for Mathematical Physics\n\nDannie Heineman Prize for Mathematical Physics is an award given each year since 1959 jointly by the American Physical Society and American Institute of Physics. It is established by the Heineman Foundation in honour of Dannie Heineman. As of 2010, the prize consists of US$ 10,000 and a certificate citing the contributions made by the recipient plus travel expenses to attend the meeting at which the prize is bestowed.\n\nSource: American Physical Society\n\n\n"}
{"id": "6988866", "url": "https://en.wikipedia.org/wiki?curid=6988866", "title": "Directed percolation", "text": "Directed percolation\n\nIn statistical physics, directed percolation (DP) refers to a class of models that mimic filtering of fluids through porous materials along a given direction. Varying the microscopic connectivity of the pores, these models display a phase transition from a macroscopically permeable (percolating) to an impermeable (non-percolating) state. Directed percolation is also used as a simple model for epidemic spreading with a transition between survival and extinction of the disease depending on the infection rate.\n\nMore generally, the term directed percolation stands for a universality class of continuous phase transitions which are characterized by the same type of collective behavior on large scales. Directed percolation is probably the simplest universality class of transitions out of thermal equilibrium.\n\nOne of the simplest realizations of DP is bond directed percolation. This model is a directed variant of ordinary (isotropic) percolation and can be introduced as follows. The figure shows a tilted square lattice with bonds connecting neighboring sites. The bonds are permeable (open) with probability formula_1 and impermeable (closed) otherwise. The sites and bonds may be interpreted as holes and randomly distributed channels of a porous medium.\n\nThe difference between ordinary and directed percolation is illustrated to the right. In isotropic percolation a spreading agent (e.g. water) introduced at a particular site percolates along open bonds, generating a cluster of wet sites. Contrarily, in directed percolation the spreading agent can pass open bonds only along a preferred direction in space, as indicated by the arrow. The resulting red cluster is directed in space.\n\nInterpreting the preferred direction as a temporal degree of freedom, directed percolation can be regarded as a stochastic process that evolves in time. In the case of bond DP the time parameter formula_2 is discrete and all sites are updated in parallel. Activating a certain site (called initial seed) at time formula_3 the resulting cluster can be constructed row by row. The corresponding number of active sites formula_4 varies as time evolves.\n\nThe DP universality class is characterized by a certain set of critical exponents. These exponents depend on the spatial dimension formula_5. Above the so-called upper critical dimension formula_6 they are given by their mean-field values while in \nformula_7 dimensions they have been estimated numerically. Current estimates are summarized in the following table:\n\nIn two dimensions, the percolation of water through a thin tissue (such as toilet paper) has the same mathematical underpinnings as the flow of electricity through two-dimensional random networks of resistors. In chemistry, chromatography can be understood with similar models.\n\nThe propagation of a tear or rip in a sheet of paper, in a sheet of metal, or even the formation of a crack in ceramic bears broad mathematical resemblance to the flow of electricity through a random network of electrical fuses. Above a certain critical point, the electrical flow will cause a fuse to pop, possibly leading to a cascade of failures, resembling the propagation of a crack or tear. The study of percolation helps indicate how the flow of electricity will redistribute itself in the fuse network, thus modeling which fuses are most likely to pop next, and how fast they will pop, and what direction the crack may curve in.\n\nExamples can be found not only in physical phenomena, but also in biology, neuroscience, ecology (e.g. evolution), and economics (e.g. diffusion of innovation).\n\nPercolation can be considered to be a branch of the study of dynamical systems or statistical mechanics. In particular, percolation networks exhibit a phase change around a critical threshold.\n\nIn spite of vast success in the theoretical and numerical studies of DP, obtaining convincing experimental evidence has proved challenging. However, in 2007, critical behavior of DP was finally found in the electrohydrodynamic convection of liquid crystal, where a complete set of static and dynamic critical exponents and universal scaling functions of DP were measured in the transition to spatiotemporal intermittency between two turbulent states.\n\n\n\n"}
{"id": "164572", "url": "https://en.wikipedia.org/wiki?curid=164572", "title": "Dissipation", "text": "Dissipation\n\nDissipation is the result of an irreversible process that takes place in homogeneous thermodynamic systems. A dissipative process is a process in which energy (internal, bulk flow kinetic, or system potential) is transformed from some initial form to some final form; the capacity of the final form to do mechanical work is less than that of the initial form. For example, heat transfer is dissipative because it is a transfer of internal energy from a hotter body to a colder one. Following the second law of thermodynamics, the entropy varies with temperature (reduces the capacity of the combination of the two bodies to do mechanical work), but never decreases in an isolated system.\n\nThese processes produce entropy (see entropy production) at a certain rate. The entropy production rate times ambient temperature gives the dissipated power. Important examples of irreversible processes are: heat flow through a thermal resistance, fluid flow through a flow resistance, diffusion (mixing), chemical reactions, and electrical current flow through an electrical resistance (Joule heating).\n\nThermodynamic dissipative processes are essentially irreversible. They produce entropy at a finite rate. In a process in which the temperature is locally continuously defined, the local density of rate of entropy production times local temperature gives the local density of dissipated power.[Definition needed!]\n\nA particular occasion of occurrence of a dissipative process cannot be described by a single individual Hamiltonian formalism. A dissipative process requires a collection of admissible individual Hamiltonian descriptions, exactly which one describes the actual particular occurrence of the process of interest being unknown. This includes friction, and all similar forces that result in decoherency of energy—that is, conversion of coherent or directed energy flow into an indirected or more isotropic distribution of energy.\n\n\"The conversion of mechanical energy into heat is called energy dissipation.\" – \"François Roddier\" The term is also applied to the loss of energy due to generation of unwanted heat in electric and electronic circuits.\n\nIn computational physics, numerical dissipation (also known as \"numerical diffusion\") refers to certain side-effects that may occur as a result of a numerical solution to a differential equation. When the pure advection equation, which is free of dissipation, is solved by a numerical approximation method, the energy of the initial wave may be reduced in a way analogous to a diffusional process. Such a method is said to contain 'dissipation'. In some cases, \"artificial dissipation\" is intentionally added to improve the numerical stability characteristics of the solution.\n\nA formal, mathematical definition of dissipation, as commonly used in the mathematical study of measure-preserving dynamical systems, is given in the article \"wandering set\".\n\nDissipation is the process of converting mechanical energy of downward-flowing water into thermal and acoustical energy. Various devices are designed in stream beds to reduce the kinetic energy of flowing waters to reduce their erosive potential on banks and river bottoms. Very often, these devices look like small waterfalls or cascades, where water flows vertically or over riprap to lose some of its kinetic energy.\n\nImportant examples of irreversible processes are: \n\nWaves or oscillations, lose energy over time, typically from friction or turbulence. In many cases, the \"lost\" energy raises the temperature of the system. For example, a wave that loses amplitude is said to dissipate. The precise nature of the effects depends on the nature of the wave: an atmospheric wave, for instance, may dissipate close to the surface due to friction with the land mass, and at higher levels due to radiative cooling.\n\nThe concept of dissipation was introduced in the field of thermodynamics by William Thomson (Lord Kelvin) in 1852. Lord Kelvin deduced that a subset of the above-mentioned irreversible dissapative processes will occur unless a process is governed by a \"perfect thermodynamic engine\". The processes that Lord Kelvin identified were friction, diffusion, conduction of heat and the absorption of light.\n\n"}
{"id": "24497698", "url": "https://en.wikipedia.org/wiki?curid=24497698", "title": "Elementary sentence", "text": "Elementary sentence\n\nIn mathematical logic, an elementary sentence is one that is stated using only finitary first-order logic, without reference to set theory or using any axioms which have consistency strength equal to set theory.\n\nSaying that a sentence is elementary is a weaker condition than saying it is algebraic.\n\n\n"}
{"id": "57356385", "url": "https://en.wikipedia.org/wiki?curid=57356385", "title": "Enriqueta González Baz", "text": "Enriqueta González Baz\n\nEnriqueta González Baz y de la Vega (September 22, 1915 – December 22, 2002) was a Mexican mathematician, a founder of the Mexican Mathematical Society, and the first woman to earn a degree in mathematics in Mexico.\n\nAfter finishing secondary school, her father sent her two a two-year program in domestic studies, where one of the teachers, Elena Picazo de Murray, recognized her talent. She studied in a night school at the San Ildefonso College, earned a teaching credential at the Escuela Nacional de Maestros, and completed a bachelor's degree in mathematical and physical sciences at the Escuela Nacional Preparatoria of the National Autonomous University of Mexico. She wrote a thesis on special functions, and did additional postgraduate study at Bryn Mawr College.\n\nShe became a mathematics professor in the Escuela Nacional Preparatoria, as well as holding several other associated positions. Among her mathematical works, she translated Solomon Lefschetz's 1930 textbook \"Topology\".\n"}
{"id": "2619911", "url": "https://en.wikipedia.org/wiki?curid=2619911", "title": "Ernst Witt", "text": "Ernst Witt\n\nErnst Witt (26 June 1911 – 3 July 1991) was a German mathematician, one of the leading algebraists of his time.\n\nWitt was born on the island of Alsen, then a part of the German Empire. Shortly after his birth, his parents moved the family to China, and he did not return to Europe until he was nine.\n\nAfter his schooling, Witt went to the University of Freiburg and the University of Göttingen. He joined the NSDAP and was an active party member. Witt completed his Ph.D. at the University of Göttingen in 1934 with Emmy Noether and later became a lecturer. He was then a member of a team led by Helmut Hasse. During World War II he joined a group of five mathematicians, recruited by Wilhelm Fenner, and which included Georg Aumann, Alexander Aigner, Oswald Teichmüller, Johann Friedrich Schultze and their leader professor Wolfgang Franz, to form the backbone of the new mathematical research department in the late 1930s, which would eventually be called: Section IVc of Cipher Department of the High Command of the Wehrmacht (abbr. OKW/Chi).\n\nFrom 1937 until 1979, he taught at the University of Hamburg. He died in Hamburg in 1991, shortly after his 80th birthday.\n\nWitt's work has been highly influential. His invention of the Witt vectors clarifies and generalizes the structure of the p-adic numbers. It has become fundamental to p-adic Hodge theory.\n\nWitt was the founder of the theory of quadratic forms over an arbitrary field. He proved several of the key results, including the Witt cancellation theorem. He defined the Witt ring of all quadratic forms over a field, now a central object in the theory.\n\nThe Poincaré–Birkhoff–Witt theorem is basic to the study of Lie algebras. In algebraic geometry, the Hasse–Witt matrix of an algebraic curve over a finite field determines the cyclic étale coverings of degree \"p\" of a curve in characteristic \"p\".\n\nIn the 1970s, Witt claimed that in 1940 he had discovered what would eventually be named the \"Leech lattice\" many years before John Leech discovered it in 1965, but Witt did not publish his discovery and the details of exactly what he did are unclear; see his collected works .\n\n\n\n"}
{"id": "3195722", "url": "https://en.wikipedia.org/wiki?curid=3195722", "title": "Fatou–Lebesgue theorem", "text": "Fatou–Lebesgue theorem\n\nIn mathematics, the Fatou–Lebesgue theorem establishes a chain of inequalities relating the integrals (in the sense of Lebesgue) of the limit inferior and the limit superior of a sequence of functions to the limit inferior and the limit superior of integrals of these functions. The theorem is named after Pierre Fatou and Henri Léon Lebesgue.\n\nIf the sequence of functions converges pointwise, the inequalities turn into equalities and the theorem reduces to Lebesgue's dominated convergence theorem.\n\nLet \"f\", \"f\", ... denote a sequence of real-valued measurable functions defined on a measure space (\"S\",\"Σ\",\"μ\"). If there exists a Lebesgue-integrable function \"g\" on \"S\" which dominates the sequence in absolute value, meaning that |\"f\"| ≤ \"g\" for all natural numbers \"n\", then all \"f\" as well as the limit inferior and the limit superior of the \"f\" are integrable and\nHere the limit inferior and the limit superior of the \"f\" are taken pointwise. The integral of the absolute value of these limiting functions is bounded above by the integral of \"g\".\n\nSince the middle inequality (for sequences of real numbers) is always true, the directions of the other inequalities are easy to remember.\n\nAll \"f\" as well as the limit inferior and the limit superior of the \"f\" are measurable and dominated in absolute value by \"g\", hence integrable.\n\nThe first inequality follows by applying Fatou's lemma to the non-negative functions \"f\" + \"g\" and using the linearity of the Lebesgue integral. The last inequality is the reverse Fatou lemma.\n\nSince \"g\" also dominates the limit superior of the |\"f\"|,\n\nby the monotonicity of the Lebesgue integral. The same estimates hold for the limit superior of the \"f\".\n\n"}
{"id": "10983", "url": "https://en.wikipedia.org/wiki?curid=10983", "title": "First-order logic", "text": "First-order logic\n\nFirst-order logic—also known as first-order predicate calculus and predicate logic—is a collection of formal systems used in mathematics, philosophy, linguistics, and computer science. First-order logic uses quantified variables over non-logical objects and allows the use of sentences that contain variables, so that rather than propositions such as \"Socrates is a man\" one can have expressions in the form \"there exists X such that X is Socrates and X is a man\" and \"there exists\" is a quantifier while \"X\" is a variable. This distinguishes it from propositional logic, which does not use quantifiers or relations.\n\nA theory about a topic is usually a first-order logic together with a specified domain of discourse over which the quantified variables range, finitely many functions from that domain to itself, finitely many predicates defined on that domain, and a set of axioms believed to hold for those things. Sometimes \"theory\" is understood in a more formal sense, which is just a set of sentences in first-order logic.\n\nThe adjective \"first-order\" distinguishes first-order logic from higher-order logic in which there are predicates having predicates or functions as arguments, or in which one or both of predicate quantifiers or function quantifiers are permitted. In first-order theories, predicates are often associated with sets. In interpreted higher-order theories, predicates may be interpreted as sets of sets.\n\nThere are many deductive systems for first-order logic which are both sound (all provable statements are true in all models) and complete (all statements which are true in all models are provable). Although the logical consequence relation is only semidecidable, much progress has been made in automated theorem proving in first-order logic. First-order logic also satisfies several metalogical theorems that make it amenable to analysis in proof theory, such as the Löwenheim–Skolem theorem and the compactness theorem.\n\nFirst-order logic is the standard for the formalization of mathematics into axioms and is studied in the foundations of mathematics.\nPeano arithmetic and Zermelo–Fraenkel set theory are axiomatizations of number theory and set theory, respectively, into first-order logic.\nNo first-order theory, however, has the strength to uniquely describe a structure with an infinite domain, such as the natural numbers or the real line. Axiom systems that do fully describe these two structures (that is, categorical axiom systems) can be obtained in stronger logics such as second-order logic.\n\nThe foundations of first-order logic were developed independently by Gottlob Frege and Charles Sanders Peirce. For a history of first-order logic and how it came to dominate formal logic, see José Ferreirós (2001).\n\nWhile propositional logic deals with simple declarative propositions, first-order logic additionally covers predicates and quantification.\n\nA predicate takes an entity or entities in the domain of discourse as input while outputs are either True or False. Consider the two sentences \"Socrates is a philosopher\" and \"Plato is a philosopher\". In propositional logic, these sentences are viewed as being unrelated and might be denoted, for example, by variables such as \"p\" and \"q\". The predicate \"is a philosopher\" occurs in both sentences, which have a common structure of \"\"a\" is a philosopher\". The variable \"a\" is instantiated as \"Socrates\" in the first sentence and is instantiated as \"Plato\" in the second sentence. While first-order logic allows for the use of predicates, such as \"is a philosopher\" in this example, propositional logic does not.\n\nRelationships between predicates can be stated using logical connectives. Consider, for example, the first-order formula \"if \"a\" is a philosopher, then \"a\" is a scholar\". This formula is a conditional statement with \"\"a\" is a philosopher\" as its hypothesis and \"\"a\" is a scholar\" as its conclusion. The truth of this formula depends on which object is denoted by \"a\", and on the interpretations of the predicates \"is a philosopher\" and \"is a scholar\".\n\nQuantifiers can be applied to variables in a formula. The variable \"a\" in the previous formula can be universally quantified, for instance, with the first-order sentence \"For every \"a\", if \"a\" is a philosopher, then \"a\" is a scholar\". The universal quantifier \"for every\" in this sentence expresses the idea that the claim \"if \"a\" is a philosopher, then \"a\" is a scholar\" holds for \"all\" choices of \"a\".\n\nThe \"negation\" of the sentence \"For every \"a\", if \"a\" is a philosopher, then \"a\" is a scholar\" is logically equivalent to the sentence \"There exists \"a\" such that \"a\" is a philosopher and \"a\" is not a scholar\". The existential quantifier \"there exists\" expresses the idea that the claim \"\"a\" is a philosopher and \"a\" is not a scholar\" holds for \"some\" choice of \"a\".\n\nThe predicates \"is a philosopher\" and \"is a scholar\" each take a single variable. In general, predicates can take several variables. In the first-order sentence \"Socrates is the teacher of Plato\", the predicate \"is the teacher of\" takes two variables.\n\nAn interpretation (or model) of a first-order formula specifies what each predicate means and the entities that can instantiate the variables. These entities form the domain of discourse or universe, which is usually required to be a nonempty set. For example, in an interpretation with the domain of discourse consisting of all human beings and the predicate \"is a philosopher\" understood as \"was the author of the \"Republic\"\", the sentence \"There exists \"a\" such that \"a\" is a philosopher\" is seen as being true, as witnessed by Plato.\n\nThere are two key parts of first-order logic. The syntax determines which collections of symbols are legal expressions in first-order logic, while the semantics determine the meanings behind these expressions.\n\nUnlike natural languages, such as English, the language of first-order logic is completely formal, so that it can be mechanically determined whether a given expression is legal. There are two key types of legal expressions: terms, which intuitively represent objects, and formulas, which intuitively express predicates that can be true or false. The terms and formulas of first-order logic are strings of symbols, where all the symbols together form the alphabet of the language. As with all formal languages, the nature of the symbols themselves is outside the scope of formal logic; they are often regarded simply as letters and punctuation symbols.\n\nIt is common to divide the symbols of the alphabet into logical symbols, which always have the same meaning, and non-logical symbols, whose meaning varies by interpretation. For example, the logical symbol formula_1 always represents \"and\"; it is never interpreted as \"or\". On the other hand, a non-logical predicate symbol such as Phil(\"x\") could be interpreted to mean \"\"x\" is a philosopher\", \"\"x\" is a man named Philip\", or any other unary predicate, depending on the interpretation at hand.\n\nThere are several logical symbols in the alphabet, which vary by author but usually include:\n\nIt should be noted that not all of these symbols are required – only one of the quantifiers, negation and conjunction, variables, brackets and equality suffice. There are numerous minor variations that may define additional logical symbols:\n\nThe non-logical symbols represent predicates (relations), functions and constants on the domain of discourse. It used to be standard practice to use a fixed, infinite set of non-logical symbols for all purposes. A more recent practice is to use different non-logical symbols according to the application one has in mind. Therefore, it has become necessary to name the set of all non-logical symbols used in a particular application. This choice is made via a signature.\n\nThe traditional approach is to have only one, infinite, set of non-logical symbols (one signature) for all applications. Consequently, under the traditional approach there is only one language of first-order logic. This approach is still common, especially in philosophically oriented books.\n\nIn contemporary mathematical logic, the signature varies by application. Typical signatures in mathematics are {1, ×} or just {×} for groups, or {0, 1, +, ×, <} for ordered fields. There are no restrictions on the number of non-logical symbols. The signature can be empty, finite, or infinite, even uncountable. Uncountable signatures occur for example in modern proofs of the Löwenheim–Skolem theorem.\n\nIn this approach, every non-logical symbol is of one of the following types.\n\nThe traditional approach can be recovered in the modern approach by simply specifying the \"custom\" signature to consist of the traditional sequences of non-logical symbols.\n\nThe formation rules define the terms and formulas of first order logic. When terms and formulas are represented as strings of symbols, these rules can be used to write a formal grammar for terms and formulas. These rules are generally context-free (each production has a single symbol on the left side), except that the set of symbols may be allowed to be infinite and there may be many start symbols, for example the variables in the case of terms.\n\nThe set of terms is inductively defined by the following rules:\nOnly expressions which can be obtained by finitely many applications of rules 1 and 2 are terms. For example, no expression involving a predicate symbol is a term.\n\nThe set of formulas (also called well-formed formulas or WFFs) is inductively defined by the following rules:\nOnly expressions which can be obtained by finitely many applications of rules 1–5 are formulas. The formulas obtained from the first two rules are said to be atomic formulas.\n\nFor example,\nis a formula, if \"f\" is a unary function symbol, \"P\" a unary predicate symbol, and Q a ternary predicate symbol. On the other hand, formula_10 is not a formula, although it is a string of symbols from the alphabet.\n\nThe role of the parentheses in the definition is to ensure that any formula can only be obtained in one way by following the inductive definition (in other words, there is a unique parse tree for each formula). This property is known as unique readability of formulas. There are many conventions for where parentheses are used in formulas. For example, some authors use colons or full stops instead of parentheses, or change the places in which parentheses are inserted. Each author's particular definition must be accompanied by a proof of unique readability.\n\nThis definition of a formula does not support defining an if-then-else function ite(c, a, b), where \"c\" is a condition expressed as a formula, that would return \"a\" if c is true, and \"b\" if it is false. This is because both predicates and functions can only accept terms as parameters, but the first parameter is a formula. Some languages built on first-order logic, such as SMT-LIB 2.0, add this.\n\nFor convenience, conventions have been developed about the precedence of the logical operators, to avoid the need to write parentheses in some cases. These rules are similar to the order of operations in arithmetic. A common convention is:\nMoreover, extra punctuation not required by the definition may be inserted to make formulas easier to read. Thus the formula\nmight be written as\n\nIn some fields, it is common to use infix notation for binary relations and functions, instead of the prefix notation defined above. For example, in arithmetic, one typically writes \"2 + 2 = 4\" instead of \"=(+(2,2),4)\". It is common to regard formulas in infix notation as abbreviations for the corresponding formulas in prefix notation, cf. also term structure vs. representation.\n\nThe definitions above use infix notation for binary connectives such as formula_14. A less common convention is Polish notation, in which one writes formula_3, formula_19, and so on in front of their arguments rather than between them. This convention allows all punctuation symbols to be discarded. Polish notation is compact and elegant, but rarely used in practice because it is hard for humans to read it. In Polish notation, the formula\nbecomes \n\nIn a formula, a variable may occur free or bound (or both). Intuitively, a variable occurrence is free in a formula if it is not quantified: in formula_21, the sole occurrence of variable \"x\" is free while that of \"y\" is bound. The free and bound variable occurrences in a formula are defined inductively as follows.\n\nFor example, in formula_27\"x\" formula_27\"y\" (\"P\"(\"x\")formula_3 \"Q\"(\"x\",\"f\"(\"x\"),\"z\")), \"x\" and \"y\" occur only bound, \"z\" occurs only free, and \"w\" is neither because it does not occur in the formula.\n\nFree and bound variables of a formula need not be disjoint sets: in the formula formula_34, the first occurrence of \"x\", as argument of \"P\", is free while the second one, as argument of \"Q\", is bound.\n\nA formula in first-order logic with no free variable occurrences is called a first-order sentence. These are the formulas that will have well-defined truth values under an interpretation. For example, whether a formula such as Phil(\"x\") is true must depend on what \"x\" represents. But the sentence formula_35 will be either true or false in a given interpretation.\n\nIn mathematics the language of ordered abelian groups has one constant symbol 0, one unary function symbol −, one binary function symbol +, and one binary relation symbol ≤. Then:\n\nThe axioms for ordered abelian groups can be expressed as a set of sentences in the language. For example, the axiom stating that the group is commutative is usually written formula_38\n\nAn interpretation of a first-order language assigns a denotation to all non-logical symbols in that language. It also determines a domain of discourse that specifies the range of the quantifiers. The result is that each term is assigned an object that it represents, and each sentence is assigned a truth value. In this way, an interpretation provides semantic meaning to the terms and formulas of the language. The study of the interpretations of formal languages is called formal semantics. What follows is a description of the standard or Tarskian semantics for first-order logic. (It is also possible to define game semantics for first-order logic, but aside from requiring the axiom of choice, game semantics agree with Tarskian semantics for first-order logic, so game semantics will not be elaborated herein.)\n\nThe domain of discourse \"D\" is a nonempty set of \"objects\" of some kind. Intuitively, a first-order formula is a statement about these objects; for example, formula_39 states the existence of an object \"x\" such that the predicate \"P\" is true where referred to it. The domain of discourse is the set of considered objects. For example, one can take formula_40 to be the set of integer numbers.\n\nThe interpretation of a function symbol is a function. For example, if the domain of discourse consists of integers, a function symbol \"f\" of arity 2 can be interpreted as the function that gives the sum of its arguments. In other words, the symbol \"f\" is associated with the function \"I(f)\" which, in this interpretation, is addition.\n\nThe interpretation of a constant symbol is a function from the one-element set \"D\" to \"D\", which can be simply identified with an object in \"D\". For example, an interpretation may assign the value formula_41 to the constant symbol formula_42.\n\nThe interpretation of an \"n\"-ary predicate symbol is a set of \"n\"-tuples of elements of the domain of discourse. This means that, given an interpretation, a predicate symbol, and \"n\" elements of the domain of discourse, one can tell whether the predicate is true of those elements according to the given interpretation. For example, an interpretation \"I(P)\" of a binary predicate symbol \"P\" may be the set of pairs of integers such that the first one is less than the second. According to this interpretation, the predicate \"P\" would be true if its first argument is less than the second.\n\nThe most common way of specifying an interpretation (especially in mathematics) is to specify a structure (also called a model; see below). The structure consists of a nonempty set \"D\" that forms the domain of discourse and an interpretation \"I\" of the non-logical terms of the signature. This interpretation is itself a function:\n\nA formula evaluates to true or false given an interpretation, and a variable assignment μ that associates an element of the domain of discourse with each variable. The reason that a variable assignment is required is to give meanings to formulas with free variables, such as formula_48. The truth value of this formula changes depending on whether \"x\" and \"y\" denote the same individual.\n\nFirst, the variable assignment μ can be extended to all terms of the language, with the result that each term maps to a single element of the domain of discourse. The following rules are used to make this assignment:\n\nNext, each formula is assigned a truth value. The inductive definition used to make this assignment is called the T-schema.\n\nIf a formula does not contain free variables, and so is a sentence, then the initial variable assignment does not affect its truth value. In other words, a sentence is true according to \"M\" and formula_66 if and only if it is true according to \"M\" and every other variable assignment formula_67.\n\nThere is a second common approach to defining truth values that does not rely on variable assignment functions. Instead, given an interpretation \"M\", one first adds to the signature a collection of constant symbols, one for each element of the domain of discourse in \"M\"; say that for each \"d\" in the domain the constant symbol \"c\" is fixed. The interpretation is extended so that each new constant symbol is assigned to its corresponding element of the domain. One now defines truth for quantified formulas syntactically, as follows:\nThis alternate approach gives exactly the same truth values to all sentences as the approach via variable assignments.\n\nIf a sentence φ evaluates to True under a given interpretation \"M\", one says that \"M\" satisfies φ; this is denoted formula_83. A sentence is satisfiable if there is some interpretation under which it is true.\n\nSatisfiability of formulas with free variables is more complicated, because an interpretation on its own does not determine the truth value of such a formula. The most common convention is that a formula with free variables is said to be satisfied by an interpretation if the formula remains true regardless which individuals from the domain of discourse are assigned to its free variables. This has the same effect as saying that a formula is satisfied if and only if its universal closure is satisfied.\n\nA formula is logically valid (or simply valid) if it is true in every interpretation. These formulas play a role similar to tautologies in propositional logic.\n\nA formula φ is a logical consequence of a formula ψ if every interpretation that makes ψ true also makes φ true. In this case one says that φ is logically implied by ψ.\n\nAn alternate approach to the semantics of first-order logic proceeds via abstract algebra. This approach generalizes the Lindenbaum–Tarski algebras of propositional logic. There are three ways of eliminating quantified variables from first-order logic that do not involve replacing quantifiers with other variable binding term operators:\nThese algebras are all lattices that properly extend the two-element Boolean algebra.\n\nTarski and Givant (1987) showed that the fragment of first-order logic that has no atomic sentence lying in the scope of more than three quantifiers has the same expressive power as relation algebra. This fragment is of great interest because it suffices for Peano arithmetic and most axiomatic set theory, including the canonical ZFC. They also prove that first-order logic with a primitive ordered pair is equivalent to a relation algebra with two ordered pair projection functions.\n\nA first-order theory of a particular signature is a set of axioms, which are sentences consisting of symbols from that signature. The set of axioms is often finite or recursively enumerable, in which case the theory is called effective. Some authors require theories to also include all logical consequences of the axioms. The axioms are considered to hold within the theory and from them other sentences that hold within the theory can be derived.\n\nA first-order structure that satisfies all sentences in a given theory is said to be a model of the theory. An elementary class is the set of all structures satisfying a particular theory. These classes are a main subject of study in model theory.\n\nMany theories have an intended interpretation, a certain model that is kept in mind when studying the theory. For example, the intended interpretation of Peano arithmetic consists of the usual natural numbers with their usual operations. However, the Löwenheim–Skolem theorem shows that most first-order theories will also have other, nonstandard models.\n\nA theory is consistent if it is not possible to prove a contradiction from the axioms of the theory. A theory is complete if, for every formula in its signature, either that formula or its negation is a logical consequence of the axioms of the theory. Gödel's incompleteness theorem shows that effective first-order theories that include a sufficient portion of the theory of the natural numbers can never be both consistent and complete.\n\nFor more information on this subject see List of first-order theories and Theory (mathematical logic)\n\nThe definition above requires that the domain of discourse of any interpretation must be a nonempty set. There are settings, such as inclusive logic, where empty domains are permitted. Moreover, if a class of algebraic structures includes an empty structure (for example, there is an empty poset), that class can only be an elementary class in first-order logic if empty domains are permitted or the empty structure is removed from the class.\n\nThere are several difficulties with empty domains, however:\nThus, when the empty domain is permitted, it must often be treated as a special case. Most authors, however, simply exclude the empty domain by definition.\n\nA deductive system is used to demonstrate, on a purely syntactic basis, that one formula is a logical consequence of another formula. There are many such systems for first-order logic, including Hilbert-style deductive systems, natural deduction, the sequent calculus, the tableaux method, and resolution. These share the common property that a deduction is a finite syntactic object; the format of this object, and the way it is constructed, vary widely. These finite deductions themselves are often called derivations in proof theory. They are also often called proofs, but are completely formalized unlike natural-language mathematical proofs.\n\nA deductive system is sound if any formula that can be derived in the system is logically valid. Conversely, a deductive system is complete if every logically valid formula is derivable. All of the systems discussed in this article are both sound and complete. They also share the property that it is possible to effectively verify that a purportedly valid deduction is actually a deduction; such deduction systems are called effective.\n\nA key property of deductive systems is that they are purely syntactic, so that derivations can be verified without considering any interpretation. Thus a sound argument is correct in every possible interpretation of the language, regardless whether that interpretation is about mathematics, economics, or some other area.\n\nIn general, logical consequence in first-order logic is only semidecidable: if a sentence A logically implies a sentence B then this can be discovered (for example, by searching for a proof until one is found, using some effective, sound, complete proof system). However, if A does not logically imply B, this does not mean that A logically implies the negation of B. There is no effective procedure that, given formulas A and B, always correctly decides whether A logically implies B.\n\nA rule of inference states that, given a particular formula (or set of formulas) with a certain property as a hypothesis, another specific formula (or set of formulas) can be derived as a conclusion. The rule is sound (or truth-preserving) if it preserves validity in the sense that whenever any interpretation satisfies the hypothesis, that interpretation also satisfies the conclusion.\n\nFor example, one common rule of inference is the rule of substitution. If \"t\" is a term and φ is a formula possibly containing the variable \"x\", then φ[\"t\"/\"x\"] is the result of replacing all free instances of \"x\" by \"t\" in φ. The substitution rule states that for any φ and any term \"t\", one can conclude φ[\"t\"/\"x\"] from φ provided that no free variable of \"t\" becomes bound during the substitution process. (If some free variable of \"t\" becomes bound, then to substitute \"t\" for \"x\" it is first necessary to change the bound variables of φ to differ from the free variables of \"t\".)\n\nTo see why the restriction on bound variables is necessary, consider the logically valid formula φ given by formula_87, in the signature of (0,1,+,×,=) of arithmetic. If \"t\" is the term \"x + 1\", the formula φ[\"t\"/\"y\"] is formula_88, which will be false in many interpretations. The problem is that the free variable \"x\" of \"t\" became bound during the substitution. The intended replacement can be obtained by renaming the bound variable \"x\" of φ to something else, say \"z\", so that the formula after substitution is formula_89, which is again logically valid.\n\nThe substitution rule demonstrates several common aspects of rules of inference. It is entirely syntactical; one can tell whether it was correctly applied without appeal to any interpretation. It has (syntactically defined) limitations on when it can be applied, which must be respected to preserve the correctness of derivations. Moreover, as is often the case, these limitations are necessary because of interactions between free and bound variables that occur during syntactic manipulations of the formulas involved in the inference rule.\n\nA deduction in a Hilbert-style deductive system is a list of formulas, each of which is a logical axiom, a hypothesis that has been assumed for the derivation at hand, or follows from previous formulas via a rule of inference. The logical axioms consist of several axiom schemas of logically valid formulas; these encompass a significant amount of propositional logic. The rules of inference enable the manipulation of quantifiers. Typical Hilbert-style systems have a small number of rules of inference, along with several infinite schemas of logical axioms. It is common to have only modus ponens and universal generalization as rules of inference.\n\nNatural deduction systems resemble Hilbert-style systems in that a deduction is a finite list of formulas. However, natural deduction systems have no logical axioms; they compensate by adding additional rules of inference that can be used to manipulate the logical connectives in formulas in the proof.\n\nThe sequent calculus was developed to study the properties of natural deduction systems. Instead of working with one formula at a time, it uses sequents, which are expressions of the form\nwhere A, ..., A, B, ..., B are formulas and the turnstile symbol formula_91 is used as punctuation to separate the two halves. Intuitively, a sequent expresses the idea that formula_92 implies formula_93.\n\nUnlike the methods just described, the derivations in the tableaux method are not lists of formulas. Instead, a derivation is a tree of formulas. To show that a formula A is provable, the tableaux method attempts to demonstrate that the negation of A is unsatisfiable. The tree of the derivation has formula_94 at its root; the tree branches in a way that reflects the structure of the formula. For example, to show that formula_95 is unsatisfiable requires showing that C and D are each unsatisfiable; this corresponds to a branching point in the tree with parent formula_95 and children C and D.\n\nThe resolution rule is a single rule of inference that, together with unification, is sound and complete for first-order logic. As with the tableaux method, a formula is proved by showing that the negation of the formula is unsatisfiable. Resolution is commonly used in automated theorem proving.\n\nThe resolution method works only with formulas that are disjunctions of atomic formulas; arbitrary formulas must first be converted to this form through Skolemization. The resolution rule states that from the hypotheses formula_97 and formula_98, the conclusion formula_99 can be obtained.\n\nMany identities can be proved, which establish equivalences between particular formulas. These identities allow for rearranging formulas by moving quantifiers across other connectives, and are useful for putting formulas in prenex normal form. Some provable identities include:\n\nThere are several different conventions for using equality (or identity) in first-order logic. The most common convention, known as first-order logic with equality, includes the equality symbol as a primitive logical symbol which is always interpreted as the real equality relation between members of the domain of discourse, such that the \"two\" given members are the same member. This approach also adds certain axioms about equality to the deductive system employed. These equality axioms are:\n\nThese are axiom schemas, each of which specifies an infinite set of axioms. The third schema is known as Leibniz's law, \"the principle of substitutivity\", \"the indiscernibility of identicals\", or \"the replacement property\". The second schema, involving the function symbol \"f\", is (equivalent to) a special case of the third schema, using the formula\n\nMany other properties of equality are consequences of the axioms above, for example:\n\nAn alternate approach considers the equality relation to be a non-logical symbol. This convention is known as first-order logic without equality. If an equality relation is included in the signature, the axioms of equality must now be added to the theories under consideration, if desired, instead of being considered rules of logic. The main difference between this method and first-order logic with equality is that an interpretation may now interpret two distinct individuals as \"equal\" (although, by Leibniz's law, these will satisfy exactly the same formulas under any interpretation). That is, the equality relation may now be interpreted by an arbitrary equivalence relation on the domain of discourse that is congruent with respect to the functions and relations of the interpretation.\n\nWhen this second convention is followed, the term normal model is used to refer to an interpretation where no distinct individuals \"a\" and \"b\" satisfy \"a\" = \"b\". In first-order logic with equality, only normal models are considered, and so there is no term for a model other than a normal model. When first-order logic without equality is studied, it is necessary to amend the statements of results such as the Löwenheim–Skolem theorem so that only normal models are considered.\n\nFirst-order logic without equality is often employed in the context of second-order arithmetic and other higher-order theories of arithmetic, where the equality relation between sets of natural numbers is usually omitted.\n\nIf a theory has a binary formula \"A\"(\"x\",\"y\") which satisfies reflexivity and Leibniz's law, the theory is said to have equality, or to be a theory with equality. The theory may not have all instances of the above schemas as axioms, but rather as derivable theorems. For example, in theories with no function symbols and a finite number of relations, it is possible to define equality in terms of the relations, by defining the two terms \"s\" and \"t\" to be equal if any relation is unchanged by changing \"s\" to \"t\" in any argument.\n\nSome theories allow other \"ad hoc\" definitions of equality:\n\nOne motivation for the use of first-order logic, rather than higher-order logic, is that first-order logic has many metalogical properties that stronger logics do not have. These results concern general properties of first-order logic itself, rather than properties of individual theories. They provide fundamental tools for the construction of models of first-order theories.\n\nGödel's completeness theorem, proved by Kurt Gödel in 1929, establishes that there are sound, complete, effective deductive systems for first-order logic, and thus the first-order logical consequence relation is captured by finite provability. Naively, the statement that a formula φ logically implies a formula ψ depends on every model of φ; these models will in general be of arbitrarily large cardinality, and so logical consequence cannot be effectively verified by checking every model. However, it is possible to enumerate all finite derivations and search for a derivation of ψ from φ. If ψ is logically implied by φ, such a derivation will eventually be found. Thus first-order logical consequence is semidecidable: it is possible to make an effective enumeration of all pairs of sentences (φ,ψ) such that ψ is a logical consequence of φ.\n\nUnlike propositional logic, first-order logic is undecidable (although semidecidable), provided that the language has at least one predicate of arity at least 2 (other than equality). This means that there is no decision procedure that determines whether arbitrary formulas are logically valid. This result was established independently by Alonzo Church and Alan Turing in 1936 and 1937, respectively, giving a negative answer to the Entscheidungsproblem posed by David Hilbert in 1928. Their proofs demonstrate a connection between the unsolvability of the decision problem for first-order logic and the unsolvability of the halting problem.\n\nThere are systems weaker than full first-order logic for which the logical consequence relation is decidable. These include propositional logic and monadic predicate logic, which is first-order logic restricted to unary predicate symbols and no function symbols. Other logics with no function symbols which are decidable are the guarded fragment of first-order logic, as well as two-variable logic. The Bernays–Schönfinkel class of first-order formulas is also decidable. Decidable subsets of first-order logic are also studied in the framework of description logics.\n\nThe Löwenheim–Skolem theorem shows that if a first-order theory of cardinality λ has an infinite model, then it has models of every infinite cardinality greater than or equal to λ. One of the earliest results in model theory, it implies that it is not possible to characterize countability or uncountability in a first-order language. That is, there is no first-order formula φ(\"x\") such that an arbitrary structure M satisfies φ if and only if the domain of discourse of M is countable (or, in the second case, uncountable).\n\nThe Löwenheim–Skolem theorem implies that infinite structures cannot be categorically axiomatized in first-order logic. For example, there is no first-order theory whose only model is the real line: any first-order theory with an infinite model also has a model of cardinality larger than the continuum. Since the real line is infinite, any theory satisfied by the real line is also satisfied by some nonstandard models. When the Löwenheim–Skolem theorem is applied to first-order set theories, the nonintuitive consequences are known as Skolem's paradox.\n\nThe compactness theorem states that a set of first-order sentences has a model if and only if every finite subset of it has a model. This implies that if a formula is a logical consequence of an infinite set of first-order axioms, then it is a logical consequence of some finite number of those axioms. This theorem was proved first by Kurt Gödel as a consequence of the completeness theorem, but many additional proofs have been obtained over time. It is a central tool in model theory, providing a fundamental method for constructing models.\n\nThe compactness theorem has a limiting effect on which collections of first-order structures are elementary classes. For example, the compactness theorem implies that any theory that has arbitrarily large finite models has an infinite model. Thus the class of all finite graphs is not an elementary class (the same holds for many other algebraic structures).\n\nThere are also more subtle limitations of first-order logic that are implied by the compactness theorem. For example, in computer science, many situations can be modeled as a directed graph of states (nodes) and connections (directed edges). Validating such a system may require showing that no \"bad\" state can be reached from any \"good\" state. Thus one seeks to determine if the good and bad states are in different connected components of the graph. However, the compactness theorem can be used to show that connected graphs are not an elementary class in first-order logic, and there is no formula φ(\"x\",\"y\") of first-order logic, in the logic of graphs, that expresses the idea that there is a path from \"x\" to \"y\". Connectedness can be expressed in second-order logic, however, but not with only existential set quantifiers, as formula_125 also enjoys compactness.\n\nPer Lindström showed that the metalogical properties just discussed actually characterize first-order logic in the sense that no stronger logic can also have those properties (Ebbinghaus and Flum 1994, Chapter XIII). Lindström defined a class of abstract logical systems, and a rigorous definition of the relative strength of a member of this class. He established two theorems for systems of this type:\n\nAlthough first-order logic is sufficient for formalizing much of mathematics, and is commonly used in computer science and other fields, it has certain limitations. These include limitations on its expressiveness and limitations of the fragments of natural languages that it can describe.\n\nFor instance, first-order logic is undecidable, meaning a sound, complete and terminating decision algorithm for provability is impossible. This has led to the study of interesting decidable fragments such as C, first-order logic with two variables and the counting quantifiers formula_126 and formula_127 (these quantifiers are, respectively, \"there exists at least \"n\"\" and \"there exists at most \"n\"\").\n\nThe Löwenheim–Skolem theorem shows that if a first-order theory has any infinite model, then it has infinite models of every cardinality. In particular, no first-order theory with an infinite model can be categorical. Thus there is no first-order theory whose only model has the set of natural numbers as its domain, or whose only model has the set of real numbers as its domain. Many extensions of first-order logic, including infinitary logics and higher-order logics, are more expressive in the sense that they do permit categorical axiomatizations of the natural numbers or real numbers. This expressiveness comes at a metalogical cost, however: by Lindström's theorem, the compactness theorem and the downward Löwenheim–Skolem theorem cannot hold in any logic stronger than first-order.\n\nFirst-order logic is able to formalize many simple quantifier constructions in natural language, such as \"every person who lives in Perth lives in Australia\". But there are many more complicated features of natural language that cannot be expressed in (single-sorted) first-order logic. \"Any logical system which is appropriate as an instrument for the analysis of natural language needs a much richer structure than first-order predicate logic\".\n\nThere are many variations of first-order logic. Some of these are inessential in the sense that they merely change notation without affecting the semantics. Others change the expressive power more significantly, by extending the semantics through additional quantifiers or other new logical symbols. For example, infinitary logics permit formulas of infinite size, and modal logics add symbols for possibility and necessity.\n\nFirst-order logic can be studied in languages with fewer logical symbols than were described above.\n\nRestrictions such as these are useful as a technique to reduce the number of inference rules or axiom schemas in deductive systems, which leads to shorter proofs of metalogical results. The cost of the restrictions is that it becomes more difficult to express natural-language statements in the formal system at hand, because the logical connectives used in the natural language statements must be replaced by their (longer) definitions in terms of the restricted collection of logical connectives. Similarly, derivations in the limited systems may be longer than derivations in systems that include additional connectives. There is thus a trade-off between the ease of working within the formal system and the ease of proving results about the formal system.\n\nIt is also possible to restrict the arities of function symbols and predicate symbols, in sufficiently expressive theories. One can in principle dispense entirely with functions of arity greater than 2 and predicates of arity greater than 1 in theories that include a pairing function. This is a function of arity 2 that takes pairs of elements of the domain and returns an ordered pair containing them. It is also sufficient to have two predicate symbols of arity 2 that define projection functions from an ordered pair to its components. In either case it is necessary that the natural axioms for a pairing function and its projections are satisfied.\n\nOrdinary first-order interpretations have a single domain of discourse over which all quantifiers range. Many-sorted first-order logic allows variables to have different sorts, which have different domains. This is also called typed first-order logic, and the sorts called types (as in data type), but it is not the same as first-order type theory. Many-sorted first-order logic is often used in the study of second-order arithmetic.\n\nWhen there are only finitely many sorts in a theory, many-sorted first-order logic can be reduced to single-sorted first-order logic. \nOne introduces into the single-sorted theory a unary predicate symbol for each sort in the many-sorted theory, and adds an axiom saying that these unary predicates partition the domain of discourse. For example, if there are two sorts, one adds predicate symbols formula_154 and formula_155 and the axiom\nThen the elements satisfying formula_157 are thought of as elements of the first sort, and elements satisfying formula_158 as elements of the second sort. One can quantify over each sort by using the corresponding predicate symbol to limit the range of quantification. For example, to say there is an element of the first sort satisfying formula φ(\"x\"), one writes\n\nAdditional quantifiers can be added to first-order logic.\n\nInfinitary logic allows infinitely long sentences. For example, one may allow a conjunction or disjunction of infinitely many formulas, or quantification over infinitely many variables. Infinitely long sentences arise in areas of mathematics including topology and model theory.\n\nInfinitary logic generalizes first-order logic to allow formulas of infinite length. The most common way in which formulas can become infinite is through infinite conjunctions and disjunctions. However, it is also possible to admit generalized signatures in which function and relation symbols are allowed to have infinite arities, or in which quantifiers can bind infinitely many variables. Because an infinite formula cannot be represented by a finite string, it is necessary to choose some other representation of formulas; the usual representation in this context is a tree. Thus formulas are, essentially, identified with their parse trees, rather than with the strings being parsed.\n\nThe most commonly studied infinitary logics are denoted \"L\", where α and β are each either cardinal numbers or the symbol ∞. In this notation, ordinary first-order logic is \"L\".\nIn the logic \"L\", arbitrary conjunctions or disjunctions are allowed when building formulas, and there is an unlimited supply of variables. More generally, the logic that permits conjunctions or disjunctions with less than κ constituents is known as \"L\". For example, \"L\" permits countable conjunctions and disjunctions.\n\nThe set of free variables in a formula of \"L\" can have any cardinality strictly less than κ, yet only finitely many of them can be in the scope of any quantifier when a formula appears as a subformula of another. In other infinitary logics, a subformula may be in the scope of infinitely many quantifiers. For example, in \"L\", a single universal or existential quantifier may bind arbitrarily many variables simultaneously. Similarly, the logic \"L\" permits simultaneous quantification over fewer than λ variables, as well as conjunctions and disjunctions of size less than κ.\n\n\nFixpoint logic extends first-order logic by adding the closure under the least fixed points of positive operators.\n\nThe characteristic feature of first-order logic is that individuals can be quantified, but not predicates. Thus\nis a legal first-order formula, but\nis not, in most formalizations of first-order logic. Second-order logic extends first-order logic by adding the latter type of quantification. Other higher-order logics allow quantification over even higher types than second-order logic permits. These higher types include relations between relations, functions from relations to relations between relations, and other higher-type objects. Thus the \"first\" in first-order logic describes the type of objects that can be quantified.\n\nUnlike first-order logic, for which only one semantics is studied, there are several possible semantics for second-order logic. The most commonly employed semantics for second-order and higher-order logic is known as full semantics. The combination of additional quantifiers and the full semantics for these quantifiers makes higher-order logic stronger than first-order logic. In particular, the (semantic) logical consequence relation for second-order and higher-order logic is not semidecidable; there is no effective deduction system for second-order logic that is sound and complete under full semantics.\n\nSecond-order logic with full semantics is more expressive than first-order logic. For example, it is possible to create axiom systems in second-order logic that uniquely characterize the natural numbers and the real line. The cost of this expressiveness is that second-order and higher-order logics have fewer attractive metalogical properties than first-order logic. For example, the Löwenheim–Skolem theorem and compactness theorem of first-order logic become false when generalized to higher-order logics with full semantics.\n\nAutomated theorem proving refers to the development of computer programs that search and find derivations (formal proofs) of mathematical theorems. Finding derivations is a difficult task because the search space can be very large; an exhaustive search of every possible derivation is theoretically possible but computationally infeasible for many systems of interest in mathematics. Thus complicated heuristic functions are developed to attempt to find a derivation in less time than a blind search.\n\nThe related area of automated proof verification uses computer programs to check that human-created proofs are correct. Unlike complicated automated theorem provers, verification systems may be small enough that their correctness can be checked both by hand and through automated software verification. This validation of the proof verifier is needed to give confidence that any derivation labeled as \"correct\" is actually correct.\n\nSome proof verifiers, such as Metamath, insist on having a complete derivation as input. Others, such as Mizar and Isabelle, take a well-formatted proof sketch (which may still be very long and detailed) and fill in the missing pieces by doing simple proof searches or applying known decision procedures: the resulting derivation is then verified by a small, core \"kernel\". Many such systems are primarily intended for interactive use by human mathematicians: these are known as proof assistants. They may also use formal logics that are stronger than first-order logic, such as type theory. Because a full derivation of any nontrivial result in a first-order deductive system will be extremely long for a human to write, results are often formalized as a series of lemmas, for which derivations can be constructed separately.\n\nAutomated theorem provers are also used to implement formal verification in computer science. In this setting, theorem provers are used to verify the correctness of programs and of hardware such as processors with respect to a formal specification. Because such analysis is time-consuming and thus expensive, it is usually reserved for projects in which a malfunction would have grave human or financial consequences.\n\n\n"}
{"id": "47275899", "url": "https://en.wikipedia.org/wiki?curid=47275899", "title": "Francisco Santos Leal", "text": "Francisco Santos Leal\n\nFrancisco (Paco) Santos Leal (born May 28, 1968) is a Spanish mathematician at the University of Cantabria, known for finding a counterexample to the Hirsch conjecture in polyhedral combinatorics. In 2015 he won the Fulkerson Prize for this research.\n\nSantos was born in Valladolid, Spain. He earned a licenciate in mathematics from the University of Cantabria in 1991, and a master's degree in pure mathematics from Joseph Fourier University in Grenoble, France in the same year. He returned to Cantabria for his doctorate, which he finished in 1995, with a thesis on the combinatorial geometry of algebraic curves and Delaunay triangulations supervised by Tomás Recio. He also has a second licenciate, in physics, from Cantabria in 1996. After postdoctoral studies at the University of Oxford he returned to Cantabria as a faculty member in 1997, and was promoted to full professor in 2008. From 2009 to 2013 he has been vice-dean of the Faculty of Sciences at Cantabria.\n\nAs well as being honored by the Fulkerson Prize in 2015 for a counter-example of the Hirsch conjecture, he was a semiplenary speaker at the 2006 International Congress of Mathematicians.\n\nSantos is an Editor-in-Chief of the Electronic Journal of Combinatorics.\n\n"}
{"id": "7207519", "url": "https://en.wikipedia.org/wiki?curid=7207519", "title": "Functional square root", "text": "Functional square root\n\nIn mathematics, a functional square root (sometimes called a half iterate) is a square root of a function with respect to the operation of function composition. In other words, a functional square root of a function is a function satisfying for all .\n\nNotations expressing that is a functional square root of are and .\n\n\nA systematic procedure to produce \"arbitrary\" functional -roots (including, beyond , continuous, negative, and infinitesimal ) relies on the solutions of Schröder's equation.\n\n\n"}
{"id": "29638170", "url": "https://en.wikipedia.org/wiki?curid=29638170", "title": "Fundamental theorem of software engineering", "text": "Fundamental theorem of software engineering\n\nThe fundamental theorem of software engineering (FTSE) is a term originated by Andrew Koenig to describe a remark by Butler Lampson attributed to the late David J. Wheeler:\nThe theorem does not describe an actual theorem that can be proven; rather, it is a general principle for managing complexity through abstraction.\n\nThe theorem is often expanded by the humorous clause \"…except for the problem of too many levels of indirection,\" referring to the fact that too many abstractions may create intrinsic complexity issues of their own. For example, the use of protocol layering in computer networks, which today is ubiquitous, has been criticized in ways that are typical of more general disadvantages of abstraction. Here, the adding of extra levels of indirection may cause higher layers to duplicate the functionality of lower layers, leading to inefficiency, and functionality at one layer may need data present only at another layer, which fundamentally violates the goal of separation into different layers.\n\n"}
{"id": "361184", "url": "https://en.wikipedia.org/wiki?curid=361184", "title": "Generalized hypergeometric function", "text": "Generalized hypergeometric function\n\nIn mathematics, a generalized hypergeometric series is a power series in which the ratio of successive coefficients indexed by \"n\" is a rational function of \"n\". The series, if convergent, defines a generalized hypergeometric function, which may then be defined over a wider domain of the argument by analytic continuation. The generalized hypergeometric series is sometimes just called the hypergeometric series, though this term also sometimes just refers to the Gaussian hypergeometric series. Generalized hypergeometric functions include the (Gaussian) hypergeometric function and the confluent hypergeometric function as special cases, which in turn have many particular special functions as special cases, such as elementary functions, Bessel functions, and the classical orthogonal polynomials.\n\nA hypergeometric series is formally defined as a power series \nin which the ratio of successive coefficients is a rational function of \"n\". That is,\nwhere \"A\"(\"n\") and \"B\"(\"n\") are polynomials in \"n\".\n\nFor example, in the case of the series for the exponential function,\nwe have:\nSo this satisfies the definition with and .\n\nIt is customary to factor out the leading term, so β is assumed to be 1. The polynomials can be factored into linear factors of the form (\"a\" + \"n\") and (\"b\" + \"n\") respectively, where the \"a\" and \"b\" are complex numbers.\n\nFor historical reasons, it is assumed that (1 + \"n\") is a factor of \"B\". If this is not already the case then both \"A\" and \"B\" can be multiplied by this factor; the factor cancels so the terms are unchanged and there is no loss of generality.\n\nThe ratio between consecutive coefficients now has the form \nwhere \"c\" and \"d\" are the leading coefficients of \"A\" and \"B\". The series then has the form\nor, by scaling z by the appropriate factor and rearranging,\n\nThis has the form of an exponential generating function. This series is usually denoted by\n\nor\n\nUsing the rising factorial or Pochhammer symbol\n\nthis can be written\n\nWhen all the terms of the series are defined and it has a non-zero radius of convergence, then the series defines an analytic function. Such a function, and its analytic continuations, is called the hypergeometric function.\n\nThe case when the radius of convergence is 0 yields many interesting series in mathematics, for example the incomplete gamma function has the asymptotic expansion\nwhich could be written \"z\"\"e\" \"F\"(1−\"a\",1;;−\"z\"). However, the use of the term \"hypergeometric series\" is usually restricted to the case where the series defines an actual analytic function.\n\nThe ordinary hypergeometric series should not be confused with the basic hypergeometric series, which, despite its name, is a rather more complicated and recondite series. The \"basic\" series is the q-analog of the ordinary hypergeometric series. There are several such generalizations of the ordinary hypergeometric series, including the ones coming from zonal spherical functions on Riemannian symmetric spaces.\n\nThe series without the factor of \"n\"! in the denominator (summed over all integers \"n\", including negative) is called the bilateral hypergeometric series.\n\nThere are certain values of the \"a\" and \"b\" for which the numerator or the denominator of the coefficients is 0.\n\nExcluding these cases, the ratio test can be applied to determine the radius of convergence.\n\nThe question of convergence for \"p\"=\"q\"+1 when \"z\" is on the unit circle is more difficult. It can be shown that the series converges absolutely at \"z\" = 1 if \nFurther, if \"p\"=\"q\"+1, formula_14 and \"z\" is real, then the following convergence result holds :\n\nIt is immediate from the definition that the order of the parameters \"a\", or the order of the parameters \"b\" can be changed without changing the value of the function. Also, if any of the parameters \"a\" is equal to any of the parameters \"b\", then the matching parameters can be \"cancelled out\", with certain exceptions when the parameters are non-positive integers. For example,\n\nThe following basic identity is very useful as it relates the higher-order hypergeometric functions in terms of integrals over the lower order ones\n\nThe generalized hypergeometric function satisfies\n\nCombining these gives a differential equation satisfied by \"w\" = \"F\":\n\nTake the following operator:\nFrom the differentiation formulas given above, the linear space spanned by \ncontains each of \nSince the space has dimension 2, any three of these \"p\"+\"q\"+2 functions are linearly dependent. These dependencies can be written out to generate a large number of identities involving formula_26.\n\nFor example, in the simplest non-trivial case,\nSo \n\nThis, and other important examples,\n\ncan be used to generate continued fraction expressions known as Gauss's continued fraction.\n\nSimilarly, by applying the differentiation formulas twice, there are formula_37 such functions contained in \nwhich has dimension three so any four are linearly dependent. This generates more identities and the process can be continued. The identities thus generated can be combined with each other to produce new ones in a different way.\n\nA function obtained by adding ±1 to exactly one of the parameters \"a\", \"b\" in \nis called contiguous to \nUsing the technique outlined above, an identity relating formula_41 and its two contiguous functions can be given, six identities relating formula_42 and any two of its four contiguous functions, and fifteen identities relating formula_43 and any two of its six contiguous functions have been found. (The first one was derived in the previous paragraph. The last fifteen were given by Gauss in his 1812 paper.)\n\nA number of other hypergeometric function identities were discovered in the nineteenth and twentieth centuries. A 20th century contribution to the methodology of proving these identities is the Egorychev method.\n\nSaalschütz's theorem is\nFor extension of this theorem, see a research paper by Rakha & Rathie.\n\nDixon's identity, first proved by , gives the sum of a well-poised \"F\" at 1:\nFor generalization of Dixon's identity, see a paper by Lavoie, et al.\n\nDougall's formula gives the sum of a terminating \nwell-poised series: \n\nprovided that \"m\" is a non-negative integer (so that the series terminates) and \nMany of the other formulas for special values of hypergeometric functions can be derived from this as special or limiting cases.\n\nIdentity 1.\nwhere \n\nIdentity 2.\nwhich links Bessel functions to \"F\"; this reduces to Kummer's second formula for \"b\" = 2\"a\":\n\nIdentity 3.\n\nIdentity 4.\nwhich is a finite sum if \"b-d\" is a non-negative integer.\n\nKummer's relation is\n\nClausen's formula\nwas used by de Branges to prove the Bieberbach conjecture.\n\nMany of the special functions in mathematics are special cases of the confluent hypergeometric function or the hypergeometric function; see the corresponding articles for examples.\n\nAs noted earlier, formula_55. The differential equation for this function is formula_56, which has solutions formula_57 where \"k\" is a constant.\n\nAn important case is: \nThe differential equation for this function is \nor \nwhich has solutions \nwhere \"k\" is a constant.\n\nThe functions of the form formula_41 are called confluent hypergeometric limit functions and are closely related to Bessel functions. The relationship is: \nThe differential equation for this function is \nor \nWhen \"a\" is not a positive integer, the substitution \ngives a linearly independent solution \nso the general solution is \nwhere \"k\", \"l\" are constants. (If \"a\" is a positive integer, the independent solution is given by the appropriate Bessel function of the second kind.)\n\nThe functions of the form formula_42 are called confluent hypergeometric functions of the first kind, also written formula_73. The incomplete gamma function formula_74 is a special case.\n\nThe differential equation for this function is\n\nor\n\nWhen \"b\" is not a positive integer, the substitution\n\ngives a linearly independent solution\n\nso the general solution is\n\nwhere \"k\", \"l\" are constants.\n\nWhen a is a non-positive integer, −\"n\", formula_80 is a polynomial. Up to constant factors, these are the Laguerre polynomials. This implies Hermite polynomials can be expressed in terms of \"F\" as well.\n\nThis occurs in connection with the exponential integral function Ei(\"z\").\n\nHistorically, the most important are the functions of the form formula_43. These are sometimes called Gauss's hypergeometric functions, classical standard hypergeometric or often simply hypergeometric functions. The term Generalized hypergeometric function is used for the functions \"F\" if there is risk of confusion. This function was first studied in detail by Carl Friedrich Gauss, who explored the conditions for its convergence.\n\nThe differential equation for this function is\n\nor\n\nIt is known as the hypergeometric differential equation. When \"c\" is not a positive integer, the substitution\n\ngives a linearly independent solution\n\nso the general solution for |\"z\"| < 1 is\n\nwhere \"k\", \"l\" are constants. Different solutions can be derived for other values of \"z\". In fact there are 24 solutions, known as the Kummer solutions, derivable using various identities, valid in different regions of the complex plane.\n\nWhen \"a\" is a non-positive integer, −\"n\",\n\nis a polynomial. Up to constant factors and scaling, these are the Jacobi polynomials. Several other classes of orthogonal polynomials, up to constant factors, are special cases of Jacobi polynomials, so these can be expressed using \"F\" as well. This includes Legendre polynomials and Chebyshev polynomials.\n\nA wide range of integrals of elementary functions can be expressed using the hypergeometric function, e.g.:\n\nThis occurs in connection with Mott polynomials.\n\nThis occurs in the theory of Bessel functions. It provides a way to compute Bessel functions of large arguments.\n\nThe generalized hypergeometric function is linked to the Meijer G-function and the MacRobert E-function. Hypergeometric series were generalised to several variables, for example by Paul Emile Appell and Joseph Kampé de Fériet; but a comparable general theory took long to emerge. Many identities were found, some quite remarkable. A generalization, the q-series analogues, called the basic hypergeometric series, were given by Eduard Heine in the late nineteenth century. Here, the ratios considered of successive terms, instead of a rational function of \"n\", are a rational function of \"q\". Another generalization, the elliptic hypergeometric series, are those series where the ratio of terms is an elliptic function (a doubly periodic meromorphic function) of \"n\".\n\nDuring the twentieth century this was a fruitful area of combinatorial mathematics, with numerous connections to other fields. There are a number of new definitions of general hypergeometric functions, by Aomoto, Israel Gelfand and others; and applications for example to the combinatorics of arranging a number of hyperplanes in complex \"N\"-space (see arrangement of hyperplanes).\n\nSpecial hypergeometric functions occur as zonal spherical functions on Riemannian symmetric spaces and semi-simple Lie groups. Their importance and role can be understood through the following example: the hypergeometric series \"F\" has the Legendre polynomials as a special case, and when considered in the form of spherical harmonics, these polynomials reflect, in a certain sense, the symmetry properties of the two-sphere or, equivalently, the rotations given by the Lie group SO(3). In tensor product decompositions of concrete representations of this group Clebsch–Gordan coefficients are met, which can be written as \"F\" hypergeometric series.\n\nBilateral hypergeometric series are a generalization of hypergeometric functions where one sums over all integers, not just the positive ones.\n\nFox–Wright functions are a generalization of generalized hypergeometric functions where the Pochhammer symbols in the series expression are generalised to gamma functions of linear expressions in the index \"n\".\n\n\n\n"}
{"id": "38594995", "url": "https://en.wikipedia.org/wiki?curid=38594995", "title": "Georges Gonthier", "text": "Georges Gonthier\n\nGeorges Gonthier is one of the leading practitioners in formal mathematics. He led the formalization of the four color theorem and Feit–Thompson proof of the odd-order theorem. (Both were written using the proof assistant Coq.)\n\n\n"}
{"id": "1613052", "url": "https://en.wikipedia.org/wiki?curid=1613052", "title": "Hackenbush", "text": "Hackenbush\n\nHackenbush is a two-player game invented by mathematician John Horton Conway. It may be played on any configuration of colored line segments connected to one another by their endpoints and to a \"ground\" line.\n\nThe game starts with the players drawing a \"ground\" line (conventionally, but not necessarily, a horizontal line at the bottom of the paper or other playing area) and several line segments such that each line segment is connected to the ground, either directly at an endpoint, or indirectly, via a chain of other segments connected by endpoints. Any number of segments may meet at a point and thus there may be multiple paths to ground.\n\nOn his turn, a player \"cuts\" (erases) any line segment of his choice. Every line segment no longer connected to the ground by any path \"falls\" (i.e., gets erased). According to the normal play convention of combinatorial game theory, the first player who is unable to move loses.\n\nHackenbush boards can consist of finitely many (in the case of a \"finite board\") or infinitely many (in the case of an \"infinite board\") line segments. The existence of an infinite number of line segments does not violate the game theory assumption that the game can be finished in a finite amount of time, provided that there are only finitely many line segments directly \"touching\" the ground. On an infinite board, based on the layout of the board the game can continue on forever, assuming there are infinitely many points touching the ground.\n\nIn the original folklore version of Hackenbush, any player is allowed to cut any edge: as this is an impartial game it is comparatively straightforward to give a complete analysis using the Sprague–Grundy theorem. Thus the versions of Hackenbush of interest in combinatorial game theory are more complex partisan games, meaning that the options (moves) available to one player would not necessarily be the ones available to the other player if it were his turn to move given the same position. This is achieved in one of two ways:\n\nBlue-Red Hackenbush is merely a special case of Blue-Red-Green Hackenbush, but it is worth noting separately, as its analysis is often much simpler. This is because Blue-Red Hackenbush is a so-called \"cold game\", which means, essentially, that it can never be an advantage to have the first move.\n\nHackenbush has often been used as an example game for demonstrating the definitions and concepts in combinatorial game theory, beginning with its use in the books \"On Numbers and Games\" and \"Winning Ways for your Mathematical Plays\" by some of the founders of the field. In particular Blue-Red Hackenbush can be used to construct surreal numbers such as nimbers: finite Blue-Red Hackenbush boards can construct dyadic rational numbers, while the values of infinite Blue-Red Hackenbush boards account for real numbers, ordinals, and many more general values that are neither. Blue-Red-Green Hackenbush allows for the construction of additional games whose values are not real numbers, such as star and all other nimbers.\n\nFurther analysis of the game can be made using graph theory by considering the board as a collection of vertices and edges and examining the paths to each vertex that lies on the ground (which should be considered as a distinguished vertex — it does no harm to identify all the ground points together — rather than as a line on the graph).\n\nIn the impartial version of Hackenbush (the one without player specified colors), it can be thought of using nim heaps by breaking the game up into several cases: vertical, convergent, and divergent. Played exclusively with vertical stacks of line segments, also referred to as bamboo stalks, the game directly becomes Nim and can be directly analyzed as such. Divergent segments, or trees, add an additional wrinkle to the game and require use of the colon principle stating that when branches come together at a vertex, one may replace the branches by a non-branching stalk of length equal to their nim sum. This principle changes the representation of the game to the more basic version of the bamboo stalks. The last possible set of graphs that can be made are convergent ones, also known as arbitrarily rooted graphs. By using the fusion principle, we can state that all vertices on any cycle may be fused together without changing the value of the graph. Therefore, any convergent graph can also be interpreted as a simple bamboo stalk graph. By combining all three types of graphs we can add complexity to the game, without ever changing the nim sum of the game, thereby allowing the game to take the strategies of Nim.\n\nThe Colon Principle states that when branches come together at a vertex, one may replace the branches by a non-branching stalk of length equal to their nim sum. Consider a fixed but arbitrary graph, \"G\", and select an arbitrary vertex, \"x\", in \"G\". Let \"H\" and \"H\" be arbitrary trees (or graphs) that have the same Sprague-Grundy value. Consider the two graphs \"G\" = \"G\" : \"H\" and \"G\" = \"G\" : \"H\", where \"G\" : \"H\" represents the graph constructed by attaching the tree \"H\" to the vertex \"x\" of the graph \"G\". The colon principle states that the two graphs \"G1\" and \"G2\" have the same Sprague-Grundy value. Consider the sum of the two games as in figure 5.4. The claim that \"G\" and \"G\" have the same Sprague-Grundy value is equivalent to the claim that the sum of the two games has Sprague-Grundy value 0. In other words, we are to show that the sum \"G\" + \"G\" is a P-position. A player is guaranteed to win if they are the second player to move in \"G\" + \"G\". If the first player moves by chopping one of the edges in \"G\" in one of the games, then the second player chops the same edge in \"G\" in the other game. (Such a pair of moves may delete \"H\" and \"H\" from the games, but otherwise \"H\" and \"H\" are not disturbed.) If the first player moves by chopping an edge in \"H\" or \"H\", then the Sprague-Grundy values of \"H\" and \"H\" are no longer equal, so that there exists a move in \"H\" or \"H\" that keeps the Sprague-Grundy values the same. In this way you will always have a reply to every move he may make. This means you will make the last move and so win.\n\n\n"}
{"id": "24229305", "url": "https://en.wikipedia.org/wiki?curid=24229305", "title": "Half-transitive graph", "text": "Half-transitive graph\n\nIn the mathematical field of graph theory, a half-transitive graph is a graph that is both vertex-transitive and edge-transitive, but not symmetric. In other words, a graph is half-transitive if its automorphism group acts transitively upon both its vertices and its edges, but not on ordered pairs of linked vertices.\n\nEvery connected symmetric graph must be vertex-transitive and edge-transitive, and the converse is true for graphs of odd degree, so that half-transitive graphs of odd degree do not exist. However, there do exist half-transitive graphs of even degree. The smallest half-transitive graph is the Holt graph, with degree 4 and 27 vertices.\n"}
{"id": "14043155", "url": "https://en.wikipedia.org/wiki?curid=14043155", "title": "James Inman", "text": "James Inman\n\nJames Inman (1776-1859), an English mathematician and astronomer, was professor of mathematics at the Royal Naval College, Portsmouth, and author of \"Inman's Nautical Tables\".\n\nInman was born at Tod Hole in Garsdale, the younger son of Richard Inman and Jane Hutchinson. He was educated at Sedbergh Grammar School and St John's College, Cambridge, graduating as first Smith's prizeman and Senior Wrangler for 1800. Among his close college friends was Henry Martyn.\n\nAfter graduating with first class honours in 1800, Inman intended to undertake missionary work in the Middle East, in Syria, but due to a declaration of war could travel no further than Malta, where he continued to study Arabic.\n\nReturning to England, the Board of Longitude appointed him as replacement astronomer (the original astronomer, suffering from severe seasickness, was discharged en route to Australia) on the expedition of under Matthew Flinders charting Australian waters in 1803-1804. Arriving at Sydney too late to join in Flinders' circumnavigation of Australia, he assisted in concluding the expedition. At this time he became a firm friend of Flinders' nephew, John Franklin, then midshipman. He also befriended the \"Investigator's\" artist, William Westall, for whom he later wrote letters of introduction. While on board the East Indiaman \"Warley\" for his return to Britain, he participated in the Battle of Pulo Auro. Here he temporarily commanded a party of lascar pikemen.\n\nHe was ordained into the Anglican ministry in 1805 when he gained his MA. Three years later he received an appointment as Professor of Nautical Mathematics at the Royal Naval College. In 1821 he published \"Navigation and Nautical Astronomy for Seamen\"; these nautical mathematical tables, known as Inman's Nautical Tables, remained in use for many years. In the third edition (1835) he introduced a new table of haversines (the term was his coinage) to simplify the calculation of distances between two points on the surface of the earth using spherical trigonometry. (For details of the calculation, see Haversine formula.)\n\nAt his suggestion, in 1810 the Admiralty established a School of Naval Architecture; the Admiralty also appointed Inman its first Principal. In 1812 he conducted experiments with Flinders which led to the invention of the Flinders Bar, used for marine compass correction. At the same time as teaching in the school and publishing mathematical texts for the use of his pupils, he translated a French text on the architecture of shipbuilding, and continued his own studies, gaining his doctorate in Divinity in 1820. In recognition of his work in nautical astronomy he was elected a Fellow of the Royal Astronomical Society.\n\nHe also directed the design and construction of no less than ten British warships, of which he was proud to state that none ever had the slightest mishap due to an error of design or form.\n\nHe retired in 1839, but continued living in Portsmouth until his death twenty years later, on 7 February 1859, aged 83.\n\nHis wife Mary, daughter of Richard Williams, vicar of All Saints' Church, Oakham, Rutland, was a direct descendant of Hannah Ayscough, the mother of Sir Isaac Newton. James and Mary Inman had seven children. Their eldest son was James Williams Inman (1809-1895), Cambridge BA 1833, MA 1836, headmaster of The King's School, Grantham. Their youngest son Henry Inman (1816-1895), was founder and first commander of the South Australia Police.\n\nIn Franklin's first North American expedition he named Inman Harbour 'after my friend the Professor at the Royal Naval College'. During Franklin's second arctic voyage in 1826 his surveyor named the Inman River, northwest of Coppermine River, Canada, after Inman. In December 1829 a headland of perpendicular cliffs at Tierra del Fuego was named Cape Inman, 'in compliment to the Professor', during the voyages of HMS Adventure and HMS Beagle.\n\n\n\n"}
{"id": "16848", "url": "https://en.wikipedia.org/wiki?curid=16848", "title": "Kurtosis", "text": "Kurtosis\n\nIn probability theory and statistics, kurtosis (from , \"kyrtos\" or \"kurtos\", meaning \"curved, arching\") is a measure of the \"tailedness\" of the probability distribution of a real-valued random variable. In a similar way to the concept of skewness, kurtosis is a descriptor of the shape of a probability distribution and, just as for skewness, there are different ways of quantifying it for a theoretical distribution and corresponding ways of estimating it from a sample from a population. Depending on the particular measure of kurtosis that is used, there are various interpretations of kurtosis, and of how particular measures should be interpreted.\n\nThe standard measure of kurtosis, originating with Karl Pearson, is based on a scaled version of the fourth moment of the data or population. This number is related to the tails of the distribution, not its peak; hence, the sometimes-seen characterization as \"peakedness\" is mistaken. For this measure, higher kurtosis is the result of infrequent extreme deviations (or outliers), as opposed to frequent modestly sized deviations.\n\nThe kurtosis of any univariate normal distribution is 3. It is common to compare the kurtosis of a distribution to this value. Distributions with kurtosis less than 3 are said to be \"platykurtic\", although this does not imply the distribution is \"flat-topped\" as sometimes reported. Rather, it means the distribution produces fewer and less extreme outliers than does the normal distribution. An example of a platykurtic distribution is the uniform distribution, which does not produce outliers. Distributions with kurtosis greater than 3 are said to be \"leptokurtic\". An example of a leptokurtic distribution is the Laplace distribution, which has tails that asymptotically approach zero more slowly than a Gaussian, and therefore produces more outliers than the normal distribution. It is also common practice to use an adjusted version of Pearson's kurtosis, the excess kurtosis, which is the kurtosis minus 3, to provide the comparison to the normal distribution. Some authors use \"kurtosis\" by itself to refer to the excess kurtosis. For the reason of clarity and generality, however, this article follows the non-excess convention and explicitly indicates where excess kurtosis is meant.\n\nAlternative measures of kurtosis are: the L-kurtosis, which is a scaled version of the fourth L-moment; measures based on four population or sample quantiles. These are analogous to the alternative measures of skewness that are not based on ordinary moments.\n\nThe kurtosis is the fourth standardized moment, defined as\nwhere \"μ\" is the fourth central moment and σ is the standard deviation.\nSeveral letters are used in the literature to denote the kurtosis. A very common choice is \"κ\", which is fine as long as it is clear that it does not refer to a cumulant. Other choices include \"γ\", to be similar to the notation for skewness, although sometimes this is instead reserved for the excess kurtosis.\n\nThe kurtosis is bounded below by the squared skewness plus 1:\n\nwhere \"μ\" is the third central moment.\nThe lower bound is realized by the Bernoulli distribution.\nThere is no upper limit to the excess kurtosis of a general probability distribution, and it may be infinite.\n\nA reason why some authors favor the excess kurtosis is that cumulants are extensive. Formulas related to the extensive property are more naturally expressed in terms of the excess kurtosis. For example, let \"X\", ..., \"X\" be independent random variables for which the fourth moment exists, and let \"Y\" be the random variable defined by the sum of the \"X\". The excess kurtosis of \"Y\" is\n\nwhere formula_4 is the standard deviation of formula_5. In particular if all of the \"X\" have the same variance, then this simplifies to\n\nThe reason not to subtract off 3 is that the bare fourth moment better generalizes to multivariate distributions, especially when independence is not assumed. The cokurtosis between pairs of variables is an order four tensor. For a bivariate normal distribution, the cokurtosis tensor has off-diagonal terms that are neither 0 nor 3 in general, so attempting to \"correct\" for an excess becomes confusing. It is true, however, that the joint cumulants of degree greater than two for any multivariate normal distribution are zero.\n\nFor two random variables, \"X\" and \"Y\", not necessarily independent, the kurtosis of the sum, \"X\" + \"Y\", is\nNote that the binomial coefficients appear in the above equation.\n\nThe exact interpretation of the Pearson measure of kurtosis (or excess kurtosis) used to be disputed, but is now settled. As Westfall (2014) notes, \"...its only unambiguous interpretation is in terms of tail extremity; i.e., either existing outliers (for the sample kurtosis) or propensity to produce outliers (for the kurtosis of a probability distribution).\" The logic is simple: Kurtosis is the average (or expected value) of the standardized data raised to the fourth power. Any standardized values that are less than 1 (i.e., data within one standard deviation of the mean, where the \"peak\" would be), contribute virtually nothing to kurtosis, since raising a number that is less than 1 to the fourth power makes it closer to zero. The only data values (observed or observable) that contribute to kurtosis in any meaningful way are those outside the region of the peak; i.e., the outliers. Therefore, kurtosis measures outliers only; it measures nothing about the \"peak.\"\n\nMany incorrect interpretations of kurtosis that involve notions of peakedness have been given. One is that kurtosis measures both the \"peakedness\" of the distribution and the heaviness of its tail. Various other incorrect interpretations have been suggested, such as \"lack of shoulders\" (where the \"shoulder\" is defined vaguely as the area between the peak and the tail, or more specifically as the area about one standard deviation from the mean) or \"bimodality\". Balanda and MacGillivray assert that the standard definition of kurtosis \"is a poor measure of the kurtosis, peakedness, or tail weight of a distribution\" and instead propose to \"define kurtosis vaguely as the location- and scale-free movement of probability mass from the shoulders of a distribution into its center and tails\".\n\nIn 1986 Moors gave an interpretation of kurtosis. Let\nwhere \"X\" is a random variable, \"μ\" is the mean and \"σ\" is the standard deviation. Then\nwhere \"κ\" is the kurtosis, \"var\"() is the variance and \"E\"() is the expectation operator. The kurtosis can now be seen to be a measure of the dispersion of \"Z\" around its expectation. Alternatively it can be seen to be a measure of the dispersion of \"Z\" around +1 and −1. \"κ\" attains its minimal value in a symmetric two-point distribution. In terms of the original variable \"X\", the kurtosis is a measure of the dispersion of \"X\" around the two values \"μ\" ± \"σ\".\n\nHigh values of \"κ\" arise in two circumstances:\n\nThe \"excess kurtosis\" is defined as kurtosis minus 3. There are 3 distinct regimes as described below.\n\nDistributions with zero excess kurtosis are called mesokurtic, or mesokurtotic. The most prominent example of a mesokurtic distribution is the normal distribution family, regardless of the values of its parameters. A few other well-known distributions can be mesokurtic, depending on parameter values: for example, the binomial distribution is mesokurtic for formula_10.\n\nA distribution with positive excess kurtosis is called leptokurtic, or leptokurtotic. \"Lepto-\" means \"slender\". In terms of shape, a leptokurtic distribution has \"fatter tails\". Examples of leptokurtic distributions include the Student's t-distribution, Rayleigh distribution, Laplace distribution, exponential distribution, Poisson distribution and the logistic distribution. Such distributions are sometimes termed \"super-Gaussian\".\n\nA distribution with negative excess kurtosis is called platykurtic, or platykurtotic. \"Platy-\" means \"broad\". In terms of shape, a platykurtic distribution has \"thinner tails\". Examples of platykurtic distributions include the continuous and discrete uniform distributions, and the raised cosine distribution. The most platykurtic distribution of all is the Bernoulli distribution with \"p\" = 1/2 (for example the number of times one obtains \"heads\" when flipping a coin once, a coin toss), for which the excess kurtosis is −2. Such distributions are sometimes termed \"sub-Gaussian\".\n\nThe effects of kurtosis are illustrated using a parametric family of distributions whose kurtosis can be adjusted while their lower-order moments and cumulants remain constant. Consider the Pearson type VII family, which is a special case of the Pearson type IV family restricted to symmetric densities. The probability density function is given by\n\nwhere \"a\" is a scale parameter and \"m\" is a shape parameter.\n\nAll densities in this family are symmetric. The \"k\"th moment exists provided \"m\" > (\"k\" + 1)/2. For the kurtosis to exist, we require \"m\" > 5/2. Then the mean and skewness exist and are both identically zero. Setting \"a\" = 2\"m\" − 3 makes the variance equal to unity. Then the only free parameter is \"m\", which controls the fourth moment (and cumulant) and hence the kurtosis. One can reparameterize with formula_12, where formula_13 is the excess kurtosis as defined above. This yields a one-parameter leptokurtic family with zero mean, unit variance, zero skewness, and arbitrary non-negative excess kurtosis. The reparameterized density is\n\nIn the limit as formula_15 one obtains the density\n\nwhich is shown as the red curve in the images on the right.\n\nIn the other direction as formula_17 one obtains the standard normal density as the limiting distribution, shown as the black curve.\n\nIn the images on the right, the blue curve represents the density formula_18 with excess kurtosis of 2. The top image shows that leptokurtic densities in this family have a higher peak than the mesokurtic normal density, although this conclusion is only valid for this select family of distributions. The comparatively fatter tails of the leptokurtic densities are illustrated in the second image, which plots the natural logarithm of the Pearson type VII densities: the black curve is the logarithm of the standard normal density, which is a parabola. One can see that the normal density allocates little probability mass to the regions far from the mean (\"has thin tails\"), compared with the blue curve of the leptokurtic Pearson type VII density with excess kurtosis of 2. Between the blue curve and the black are other Pearson type VII densities with \"γ\" = 1, 1/2, 1/4, 1/8, and 1/16. The red curve again shows the upper limit of the Pearson type VII family, with formula_19 (which, strictly speaking, means that the fourth moment does not exist). The red curve decreases the slowest as one moves outward from the origin (\"has fat tails\").\n\nSeveral well-known, unimodal and symmetric distributions from different parametric families are compared here. Each has a mean and skewness of zero. The parameters have been chosen to result in a variance equal to 1 in each case. The images on the right show curves for the following seven densities, on a linear scale and logarithmic scale:\n\n\nNote that in these cases the platykurtic densities have bounded support, whereas the densities with positive or zero excess kurtosis are supported on the whole real line.\n\nThere exist platykurtic densities with infinite support,\n\nand there exist leptokurtic densities with finite support.\n\nFor a sample of \"n\" values the sample excess kurtosis is\n\nwhere \"m\" is the fourth sample moment about the mean, \"m\" is the second sample moment about the mean (that is, the sample variance), \"x\" is the \"i\" value, and formula_21 is the sample mean.\n\nThis formula has the simpler representation,\n\nwhere the formula_23 values are the standardized data values using the standard deviation defined using \"n\" rather than \"n\" − 1 in the denominator.\n\nFor example, suppose the data values are 0, 3, 4, 1, 2, 3, 0, 2, 1, 3, 2, 0, 2, 2, 3, 2, 5, 2, 3, 999.\n\nThen the formula_23 values are −0.239, −0.225, −0.221, −0.234, −0.230, −0.225, −0.239, −0.230, −0.234, −0.225, −0.230, −0.239, −0.230, −0.230, −0.225, −0.230, −0.216, −0.230, −0.225, 4.359\n\nand the formula_25 values are 0.003, 0.003, 0.002, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.002, 0.003, 0.003, 360.976.\n\nThe average of these values is 18.05 and the excess kurtosis is thus 18.05 − 3 = 15.05. This example makes it clear that data near the \"middle\" or \"peak\" of the distribution do not contribute to the kurtosis statistic, hence kurtosis does not measure \"peakedness.\" It is simply a measure of the outlier, 999 in this example.\n\nThe variance of the sample kurtosis of a sample of size \"n\" from the normal distribution is\n\nStated differently, under the assumption that the underlying random variable formula_27 is normally distributed, it can be shown that formula_28.\n\nAn upper bound for the sample kurtosis of \"n\" (\"n\" > 2) real numbers is\n\nGiven a sub-set of samples from a population, the sample excess kurtosis above is a biased estimator of the population excess kurtosis. An alternative estimator of the population excess kurtosis is defined as follows:\n\nwhere \"k\" is the unique symmetric unbiased estimator of the fourth cumulant, \"k\" is the unbiased estimate of the second cumulant (identical to the unbiased estimate of the sample variance), \"m\" is the fourth sample moment about the mean, \"m\" is the second sample moment about the mean, \"x\" is the \"i\" value, and formula_31 is the sample mean. Unfortunately, formula_32 is itself generally biased. For the normal distribution it is unbiased.\n\nThe sample kurtosis is a useful measure of whether there is a problem with outliers in a data set. Larger kurtosis indicates a more serious outlier problem, and may lead the researcher to choose alternative statistical methods.\n\nD'Agostino's K-squared test is a goodness-of-fit normality test based on a combination of the sample skewness and sample kurtosis, as is the Jarque–Bera test for normality.\n\nFor non-normal samples, the variance of the sample variance depends on the kurtosis; for details, please see variance.\n\nPearson's definition of kurtosis is used as an indicator of intermittency in turbulence.\n\nApplying band-pass filters to digital images, kurtosis values tend to be uniform, independent of the range of the filter. This behavior, termed \"kurtosis convergence\", can be used to detect image splicing in forensic analysis.\n\nA different measure of \"kurtosis\" is provided by using L-moments instead of the ordinary moments.\n\n\n\n"}
{"id": "5971799", "url": "https://en.wikipedia.org/wiki?curid=5971799", "title": "List of mathematicians (C)", "text": "List of mathematicians (C)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1052096", "url": "https://en.wikipedia.org/wiki?curid=1052096", "title": "Logarithmic form", "text": "Logarithmic form\n\nIn contexts including complex manifolds and algebraic geometry, a logarithmic differential form is a meromorphic differential form with poles of a certain kind. The concept was introduced by Deligne.\n\nLet \"X\" be a complex manifold, \"D\" ⊂ \"X\" a divisor, and ω a holomorphic \"p\"-form on \"X\"−\"D\". If ω and \"d\"ω have a pole of order at most one along \"D\", then ω is said to have a logarithmic pole along \"D\". ω is also known as a logarithmic \"p\"-form. The logarithmic \"p\"-forms make up a subsheaf of the meromorphic \"p\"-forms on \"X\" with a pole along \"D\", denoted\n\nIn the theory of Riemann surfaces, one encounters logarithmic one-forms which have the local expression\n\nfor some meromorphic function (resp. rational function) formula_3, where \"g\" is holomorphic and non-vanishing at 0, and \"m\" is the order of \"f\" at \"0\". That is, for some open covering, there are local representations of this differential form as a logarithmic derivative (modified slightly with the exterior derivative \"d\" in place of the usual differential operator \"d/dz\"). Observe that ω has only simple poles with integer residues. On higher-dimensional complex manifolds, the Poincaré residue is used to describe the distinctive behavior of logarithmic forms along poles.\n\nBy definition of formula_4 and the fact that exterior differentiation \"d\" satisfies \"d\" = 0, one has\nThis implies that there is a complex of sheaves formula_6, known as the \"holomorphic log complex\" corresponding to the divisor \"D\". This is a subcomplex of formula_7, where formula_8 is the inclusion and formula_9 is the complex of sheaves of holomorphic forms on \"X\"−\"D\".\n\nOf special interest is the case where \"D\" has simple normal crossings. Then if formula_10 are the smooth, irreducible components of \"D\", one has formula_11 with the formula_12 meeting transversely. Locally \"D\" is the union of hyperplanes, with local defining equations of the form formula_13 in some holomorphic coordinates. One can show that the stalk of formula_14 at \"p\" satisfies\nand that\nSome authors, e.g., use the term \"log complex\" to refer to the holomorphic log complex corresponding to a divisor with normal crossings.\n\nConsider a once-punctured elliptic curve, given as the locus \"D\" of complex points (\"x\",\"y\") satisfying formula_17 where formula_18 and formula_19 is a complex number. Then \"D\" is a smooth irreducible hypersurface in C and, in particular, a divisor with simple normal crossings. There is a meromorphic two-form on C \n\nwhich has a simple pole along \"D\". The Poincaré residue of ω along \"D\" is given by the holomorphic one-form\n\nVital to the residue theory of logarithmic forms is the Gysin sequence, which is in some sense a generalization of the Residue Theorem for compact Riemann surfaces. This can be used to show, for example, that formula_22 extends to a holomorphic one-form on the projective closure of \"D\" in P, a smooth elliptic curve.\n\nThe holomorphic log complex can be brought to bear on the Hodge theory of complex algebraic varieties. Let \"X\" be a complex algebraic manifold and formula_23 a good compactification. This means that \"Y\" is a compact algebraic manifold and \"D\" = \"Y\"−\"X\" is a divisor on \"Y\" with simple normal crossings. The natural inclusion of complexes of sheaves\nturns out to be a quasi-isomorphism. Thus\nwhere formula_26 denotes hypercohomology of a complex of abelian sheaves. There is a decreasing filtration formula_27 given by\nwhich, along with the trivial increasing filtration formula_29 on logarithmic \"p\"-forms, produces filtrations on cohomology\nOne shows that formula_32 can actually be defined over Q. Then the filtrations formula_33 on cohomology give rise to a mixed Hodge structure on formula_34.\n\nClassically, for example in elliptic function theory, the logarithmic differential forms were recognised as complementary to the differentials of the first kind. They were sometimes called \"differentials of the second kind\" (and, with an unfortunate inconsistency, also sometimes \"of the third kind\"). The classical theory has now been subsumed as an aspect of Hodge theory. For a Riemann surface \"S\", for example, the differentials of the first kind account for the term \"H\" in \"H\"(\"S\"), when by the Dolbeault isomorphism it is interpreted as the sheaf cohomology group \"H\"(\"S\",Ω); this is tautologous considering their definition. The \"H\" direct summand in \"H\"(\"S\"), as well as being interpreted as \"H\"(\"S\",O) where O is the sheaf of holomorphic functions on \"S\", can be identified more concretely with a vector space of logarithmic differentials.\n\nIn algebraic geometry, the sheaf of logarithmic differential \"p\"-forms formula_4 on a smooth projective variety \"X\" along a smooth divisor formula_36 is defined and fits into the exact sequence of locally free sheaves:\n\nwhere formula_38 are the inclusions of irreducible divisors (and the pushforwards along them are extension by zero), and β is called the residue map when \"p\" is 1.\n\nFor example, if \"x\" is a closed point on formula_39 and not on formula_40, then\nform a basis of formula_42 at \"x\", where formula_43 are local coordinates around \"x\" such that formula_44 are local parameters for formula_39.\n\n\n"}
{"id": "1336457", "url": "https://en.wikipedia.org/wiki?curid=1336457", "title": "Loop algebra", "text": "Loop algebra\n\nIn mathematics, loop algebras are certain types of Lie algebras, of particular interest in theoretical physics.\n\nIf is a Lie algebra, the tensor product of with , the algebra of (complex) smooth functions over the circle manifold ,\n\nis an infinite-dimensional Lie algebra with the Lie bracket given by\n\nHere and are elements of and and are elements of .\n\nThis isn't precisely what would correspond to the direct product of infinitely many copies of , one for each point in , because of the smoothness restriction. Instead, it can be thought of in terms of smooth map from to ; a smooth parametrized loop in , in other words. This is why it is called the loop algebra.\n\nSimilarly, a set of all smooth maps from to a Lie group forms an infinite-dimensional Lie group (Lie group in the sense we can define functional derivatives over it) called the loop group. The Lie algebra of a loop group is the corresponding loop algebra.\n\nWe can take the Fourier transform on this loop algebra by defining\n\nas\n\nwhere\n\nis a coordinatization of .\n\nIf is a semisimple Lie algebra, then a nontrivial central extension of its loop algebra gives rise to an affine Lie algebra.\n"}
{"id": "173416", "url": "https://en.wikipedia.org/wiki?curid=173416", "title": "Mathematical physics", "text": "Mathematical physics\n\nMathematical physics refers to the development of mathematical methods for application to problems in physics. The \"Journal of Mathematical Physics\" defines the field as \"the application of mathematics to problems in physics and the development of mathematical methods suitable for such applications and for the formulation of physical theories\". It is a branch of applied mathematics, but deals with physical problems.\n\nThere are several distinct branches of mathematical physics, and these roughly correspond to particular historical periods.\n\nThe rigorous, abstract and advanced reformulation of Newtonian mechanics adopting the Lagrangian mechanics and the Hamiltonian mechanics even in the presence of constraints. Both formulations are embodied in analytical mechanics. It leads, for instance, to discover the deep interplay of the notion of symmetry and that of conserved quantities during the dynamical evolution, stated within the most elementary formulation of Noether's theorem. These approaches and ideas can be, and in fact have been, extended to other areas of physics as statistical mechanics, continuum mechanics, classical field theory and quantum field theory. Moreover, they have provided several examples and basic ideas in differential geometry (e.g. the theory of vector bundles and several notions in symplectic geometry).\n\nThe theory of partial differential equations (and the related areas of variational calculus, Fourier analysis, potential theory, and vector analysis) are perhaps most closely associated with mathematical physics. These were developed intensively from the second half of the 18th century (by, for example, D'Alembert, Euler, and Lagrange) until the 1930s. Physical applications of these developments include hydrodynamics, celestial mechanics, continuum mechanics, elasticity theory, acoustics, thermodynamics, electricity, magnetism, and aerodynamics.\n\nThe theory of atomic spectra (and, later, quantum mechanics) developed almost concurrently with the mathematical fields of linear algebra, the spectral theory of operators, operator algebras and more broadly, functional analysis. Nonrelativistic quantum mechanics includes Schrödinger operators, and it has connections to atomic and molecular physics. Quantum information theory is another subspecialty.\n\nThe special and general theories of relativity require a rather different type of mathematics. This was group theory, which played an important role in both quantum field theory and differential geometry. This was, however, gradually supplemented by topology and functional analysis in the mathematical description of cosmological as well as quantum field theory phenomena. In this area both homological algebra and category theory are important nowadays.\n\nStatistical mechanics forms a separate field, which includes the theory of phase transitions. It relies upon the Hamiltonian mechanics (or its quantum version) and it is closely related with the more mathematical ergodic theory and some parts of probability theory. There are increasing interactions between combinatorics and physics, in particular statistical physics.\n\nThe usage of the term \"mathematical physics\" is sometimes idiosyncratic. Certain parts of mathematics that initially arose from the development of physics are not, in fact, considered parts of mathematical physics, while other closely related fields are. For example, ordinary differential equations and symplectic geometry are generally viewed as purely mathematical disciplines, whereas dynamical systems and Hamiltonian mechanics belong to mathematical physics. John Herapath used the term for the title of his 1847 text on \"mathematical principles of natural philosophy\"; the scope at that time being \n\"the causes of heat, gaseous elasticity, gravitation, and other great phenomena of nature\".\n\nThe term \"mathematical physics\" is sometimes used to denote research aimed at studying and solving problems inspired by physics or thought experiments within a mathematically rigorous framework. In this sense, mathematical physics covers a very broad academic realm distinguished only by the blending of pure mathematics and physics. Although related to theoretical physics, mathematical physics in this sense emphasizes the mathematical rigour of the same type as found in mathematics.\n\nOn the other hand, theoretical physics emphasizes the links to observations and experimental physics, which often requires theoretical physicists (and mathematical physicists in the more general sense) to use heuristic, intuitive, and approximate arguments. Such arguments are not considered rigorous by mathematicians, but that is changing over time.\n\nSuch mathematical physicists primarily expand and elucidate physical theories. Because of the required level of mathematical rigour, these researchers often deal with questions that theoretical physicists have considered to be already solved. However, they can sometimes show (but neither commonly nor easily) that the previous solution was incomplete, incorrect, or simply too naïve. Issues about attempts to infer the second law of thermodynamics from statistical mechanics are examples. Other examples concern the subtleties involved with synchronisation procedures in special and general relativity (Sagnac effect and Einstein synchronisation)\n\nThe effort to put physical theories on a mathematically rigorous footing has inspired many mathematical developments. For example, the development of quantum mechanics and some aspects of functional analysis parallel each other in many ways. The mathematical study of quantum mechanics, quantum field theory, and quantum statistical mechanics has motivated results in operator algebras. The attempt to construct a rigorous quantum field theory has also brought about progress in fields such as representation theory. Use of geometry and topology plays an important role in string theory.\n\nThe roots of mathematical physics can be traced back to the likes of Archimedes in Greece, Ptolemy in Egypt, Alhazen in Iraq, and Al-Biruni in Persia.\n\nIn the first decade of the 16th century, amateur astronomer Nicolaus Copernicus proposed heliocentrism, and published a treatise on it in 1543. He retained the Ptolemaic idea of epicycles, and merely sought to simplify astronomy by constructing simpler sets of epicyclic orbits. Epicycles consist of circles upon circles. According to Aristotelian physics, the circle was the perfect form of motion, and was the intrinsic motion of Aristotle's fifth element—the quintessence or universal essence known in Greek as \"aether\" for the English \"pure air\"—that was the pure substance beyond the sublunary sphere, and thus was celestial entities' pure composition. The German Johannes Kepler [1571–1630], Tycho Brahe's assistant, modified Copernican orbits to \"ellipses\", formalized in the equations of Kepler's laws of planetary motion.\n\nAn enthusiastic atomist, Galileo Galilei in his 1623 book \"The Assayer\" asserted that the \"book of nature\" is written in mathematics. His 1632 book, about his telescopic observations, supported heliocentrism. Having introduced experimentation, Galileo then refuted geocentric cosmology by refuting Aristotelian physics itself. Galilei's 1638 book \"Discourse on Two New Sciences\" established the law of equal free fall as well as the principles of inertial motion, founding the central concepts of what would become today's classical mechanics. By the Galilean law of inertia as well as the principle of Galilean invariance, also called Galilean relativity, for any object experiencing inertia, there is empirical justification for knowing only that it is at \"relative\" rest or \"relative\" motion—rest or motion with respect to another object.\n\nRené Descartes adopted Galilean principles and developed a complete system of heliocentric cosmology, anchored on the principle of vortex motion, Cartesian physics, whose widespread acceptance brought the demise of Aristotelian physics. Descartes sought to formalize mathematical reasoning in science, and developed Cartesian coordinates for geometrically plotting locations in 3D space and marking their progressions along the flow of time.\n\nChristiaan Huygens was the first to use mathematical formulas to describe the laws of physics, and for that reason Huygens is regarded as the first theoretical physicist and the founder of mathematical physics.\n\nIsaac Newton (1642–1727) developed new mathematics, including calculus and several numerical methods such as Newton's method to solve problems in physics. Newton's theory of motion, published in 1687, modeled three Galilean laws of motion along with Newton's law of universal gravitation on a framework of absolute space—hypothesized by Newton as a physically real entity of Euclidean geometric structure extending infinitely in all directions—while presuming absolute time, supposedly justifying knowledge of absolute motion, the object's motion with respect to absolute space. The principle of Galilean invariance/relativity was merely implicit in Newton's theory of motion. Having ostensibly reduced the Keplerian celestial laws of motion as well as Galilean terrestrial laws of motion to a unifying force, Newton achieved great mathematical rigor, but with theoretical laxity.\n\nIn the 18th century, the Swiss Daniel Bernoulli (1700–1782) made contributions to fluid dynamics, and vibrating strings. The Swiss Leonhard Euler (1707–1783) did special work in variational calculus, dynamics, fluid dynamics, and other areas. Also notable was the Italian-born Frenchman, Joseph-Louis Lagrange (1736–1813) for work in analytical mechanics: he formulated Lagrangian mechanics) and variational methods. A major contribution to the formulation of Analytical Dynamics called Hamiltonian dynamics was also made by the Irish physicist, astronomer and mathematician, William Rowan Hamilton (1805-1865). Hamiltonian dynamics had played an important role in the formulation of modern theories in physics, including field theory and quantum mechanics. The French mathematical physicist Joseph Fourier (1768 – 1830) introduced the notion of Fourier series to solve the heat equation, giving rise to a new approach to solving partial differential equations by means of integral transforms.\n\nInto the early 19th century, the French Pierre-Simon Laplace (1749–1827) made paramount contributions to mathematical astronomy, potential theory, and probability theory. Siméon Denis Poisson (1781–1840) worked in analytical mechanics and potential theory. In Germany, Carl Friedrich Gauss (1777–1855) made key contributions to the theoretical foundations of electricity, magnetism, mechanics, and fluid dynamics. In England, George Green (1793-1841) published \"An Essay on the Application of Mathematical Analysis to the Theories of Electricity and Magnetism\" in 1828, which in addition to its significant contributions to mathematics made early progress towards laying down the mathematical foundations of electricity and magnetism.\n\nA couple of decades ahead of Newton's publication of a particle theory of light, the Dutch Christiaan Huygens (1629–1695) developed the wave theory of light, published in 1690. By 1804, Thomas Young's double-slit experiment revealed an interference pattern, as though light were a wave, and thus Huygens's wave theory of light, as well as Huygens's inference that light waves were vibrations of the luminiferous aether, was accepted. Jean-Augustin Fresnel modeled hypothetical behavior of the aether. Michael Faraday introduced the theoretical concept of a field—not action at a distance. Mid-19th century, the Scottish James Clerk Maxwell (1831–1879) reduced electricity and magnetism to Maxwell's electromagnetic field theory, whittled down by others to the four Maxwell's equations. Initially, optics was found consequent of Maxwell's field. Later, radiation and then today's known electromagnetic spectrum were found also consequent of this electromagnetic field.\n\nThe English physicist Lord Rayleigh [1842–1919] worked on sound. The Irishmen William Rowan Hamilton (1805–1865), George Gabriel Stokes (1819–1903) and Lord Kelvin (1824–1907) produced several major works: Stokes was a leader in optics and fluid dynamics; Kelvin made substantial discoveries in thermodynamics; Hamilton did notable work on analytical mechanics, discovering a new and powerful approach nowadays known as Hamiltonian mechanics. Very relevant contributions to this approach are due to his German colleague Carl Gustav Jacobi (1804–1851) in particular referring to canonical transformations. The German Hermann von Helmholtz (1821–1894) made substantial contributions in the fields of electromagnetism, waves, fluids, and sound. In the United States, the pioneering work of Josiah Willard Gibbs (1839–1903) became the basis for statistical mechanics. Fundamental theoretical results in this area were achieved by the German Ludwig Boltzmann (1844-1906). Together, these individuals laid the foundations of electromagnetic theory, fluid dynamics, and statistical mechanics.\n\nBy the 1880s, there was a prominent paradox that an observer within Maxwell's electromagnetic field measured it at approximately constant speed, regardless of the observer's speed relative to other objects within the electromagnetic field. Thus, although the observer's speed was continually lost relative to the electromagnetic field, it was preserved relative to other objects \"in\" the electromagnetic field. And yet no violation of Galilean invariance within physical interactions among objects was detected. As Maxwell's electromagnetic field was modeled as oscillations of the aether, physicists inferred that motion within the aether resulted in aether drift, shifting the electromagnetic field, explaining the observer's missing speed relative to it. The Galilean transformation had been the mathematical process used to translate the positions in one reference frame to predictions of positions in another reference frame, all plotted on Cartesian coordinates, but this process was replaced by Lorentz transformation, modeled by the Dutch Hendrik Lorentz [1853–1928].\n\nIn 1887, experimentalists Michelson and Morley failed to detect aether drift, however. It was hypothesized that motion \"into\" the aether prompted aether's shortening, too, as modeled in the Lorentz contraction. It was hypothesized that the aether thus kept Maxwell's electromagnetic field aligned with the principle of Galilean invariance across all inertial frames of reference, while Newton's theory of motion was spared.\n\nIn the 19th century, Gauss's contributions to non-Euclidean geometry, or geometry on curved surfaces, laid the groundwork for the subsequent development of Riemannian geometry by Bernhard Riemann (1826–1866). Austrian theoretical physicist and philosopher Ernst Mach criticized Newton's postulated absolute space. Mathematician Jules-Henri Poincaré (1854–1912) questioned even absolute time. In 1905, Pierre Duhem published a devastating criticism of the foundation of Newton's theory of motion. Also in 1905, Albert Einstein (1879–1955) published his special theory of relativity, newly explaining both the electromagnetic field's invariance and Galilean invariance by discarding all hypotheses concerning aether, including the existence of aether itself. Refuting the framework of Newton's theory—absolute space and absolute time—special relativity refers to \"relative space\" and \"relative time\", whereby \"length\" contracts and \"time\" dilates along the travel pathway of an object.\n\nIn 1908, Einstein's former professor Hermann Minkowski modeled 3D space together with the 1D axis of time by treating the temporal axis like a fourth spatial dimension—altogether 4D spacetime—and declared the imminent demise of the separation of space and time. Einstein initially called this \"superfluous learnedness\", but later used Minkowski spacetime with great elegance in his general theory of relativity, extending invariance to all reference frames—whether perceived as inertial or as accelerated—and credited this to Minkowski, by then deceased. General relativity replaces Cartesian coordinates with Gaussian coordinates, and replaces Newton's claimed empty yet Euclidean space traversed instantly by Newton's vector of hypothetical gravitational force—an instant action at a distance—with a gravitational \"field\". The gravitational field is Minkowski spacetime itself, the 4D topology of Einstein aether modeled on a Lorentzian manifold that \"curves\" geometrically, according to the Riemann curvature tensor, in the vicinity of either mass or energy. (Under special relativity—a special case of general relativity—even massless energy exerts gravitational effect by its mass equivalence locally \"curving\" the geometry of the four, unified dimensions of space and time.)\n\nAnother revolutionary development of the 20th century was quantum theory, which emerged from the seminal contributions of Max Planck (1856–1947) (on black body radiation) and Einstein's work on the photoelectric effect. This was, at first, followed by a heuristic framework devised by Arnold Sommerfeld (1868–1951) and Niels Bohr (1885–1962), but this was soon replaced by the quantum mechanics developed by Max Born (1882–1970), Werner Heisenberg (1901–1976), Paul Dirac (1902–1984), Erwin Schrödinger (1887–1961), Satyendra Nath Bose (1894–1974), and Wolfgang Pauli (1900–1958). This revolutionary theoretical framework is based on a probabilistic interpretation of states, and evolution and measurements in terms of self-adjoint operators on an infinite dimensional vector space. That is called Hilbert space, introduced in its elementary form by David Hilbert (1862–1943) and Frigyes Riesz (1880-1956), and rigorously defined within the axiomatic modern version by John von Neumann in his celebrated book \"Mathematical Foundations of Quantum Mechanics\", where he built up a relevant part of modern functional analysis on Hilbert spaces, the spectral theory in particular. Paul Dirac used algebraic constructions to produce a relativistic model for the electron, predicting its magnetic moment and the existence of its antiparticle, the positron.\n\nProminent contributors to the 20th century's mathematical physics (although the list contains some typically theoretical, not mathematical, physicists and leaves many contributors out; please also note that since the page can be edited by anyone, sometimes less deserved mentions can pop up in the list) include, ordered by birth date, William Thomson (Lord Kelvin) [1824–1907], Oliver Heaviside [1850–1925], Jules Henri Poincaré [1854–1912] , David Hilbert [1862–1943], Arnold Sommerfeld [1868–1951], Constantin Carathéodory [1873–1950], Albert Einstein [1879–1955], Max Born [1882–1970], George David Birkhoff [1884-1944], Hermann Weyl [1885–1955], Norbert Wiener [1894–1964], Wolfgang Pauli [1900–1958], Paul Dirac [1902–1984], Eugene Wigner [1902–1995], Andrey Kolmogorov [1903-1987], Lars Onsager [1903-1976], John von Neumann [1903–1957], Sin-Itiro Tomonaga [1906–1979], Hideki Yukawa [1907–1981], Nikolay Nikolayevich Bogolyubov [1909–1992], Subrahmanyan Chandrasekhar [1910-1995], Mark Kac [1914–1984], Julian Schwinger [1918–1994], Richard Phillips Feynman [1918–1988], Irving Ezra Segal [1918–1998], Arthur Strong Wightman [1922–2013], Chen-Ning Yang [1922– ], Rudolf Haag [1922–2016], Freeman Dyson [1923– ], Martin Gutzwiller [1925–2014], Abdus Salam [1926–1996], Jürgen Moser [1928–1999], Michael Francis Atiyah [1929– ], Joel Louis Lebowitz [1930– ], Roger Penrose [1931– ], Elliott Hershel Lieb [1932– ], Sheldon Lee Glashow [1932– ], Steven Weinberg [1933– ], Ludvig D. Faddeev [1934–2017], David Ruelle [1935– ], Yakov Grigorevich Sinai [1935– ], Vladimir Igorevich Arnold [1937–2010], Arthur Jaffe [1937– ], Roman Wladimir Jackiw [1939– ], Leonard Susskind [1940– ], Rodney James Baxter [1940– ], Michael Victor Berry [1941- ], Giovanni Gallavotti [1941- ], Stephen William Hawking [1942–2018], Jerrold Eldon Marsden [1942–2010], Alexander Markovich Polyakov [1945– ], Gerardus 't Hooft [1946– ], John L. Cardy [1947– ], Giorgio Parisi [1948– ], Edward Witten [1951– ], Herbert Spohn [1951?– ], Ashoke Sen [1956-] and Juan Martín Maldacena [1968– ].\n\n\n\n\n\n"}
{"id": "402929", "url": "https://en.wikipedia.org/wiki?curid=402929", "title": "Metalanguage", "text": "Metalanguage\n\nBroadly, any metalanguage is language or symbols used when language itself is being discussed or examined. In logic and linguistics, a metalanguage is a language used to make statements about statements in another language (the object language). Expressions in a metalanguage are often distinguished from those in an object language by the use of italics, quotation marks, or writing on a separate line. The structure of sentences and phrases in a metalanguage can be described by a metasyntax.\n\nThere are a variety of recognized metalanguages, including \"embedded\", \"ordered\", and \"nested\" (or, \"hierarchical\").\n\nAn embedded metalanguage is a language formally, naturally and firmly fixed in an object language. This idea is found in Douglas Hofstadter's book, \"Gödel, Escher, Bach\", in a discussion of the relationship between formal languages and number theory: \"... it is in the nature of any formalization of number theory that its metalanguage is embedded within it\". It occurs in natural, or informal, languages, as well—such as in English, where words such as \"noun,\" \"verb,\" or even \"word\" describe features and concepts pertaining to the English language itself.\n\nAn ordered metalanguage is analogous to ordered logic. An example of an ordered metalanguage is the construction of one metalanguage to discuss an object language, followed by the creation of another metalanguage to discuss the first, etc.\n\nA nested (or, \"hierarchical\") metalanguage is similar to an ordered metalanguage in that each level represents a greater degree of abstraction. However, a nested metalanguage differs from an ordered one in that each level includes the one below. The paradigmatic example of a nested metalanguage comes from the Linnean taxonomic system in biology. Each level in the system incorporates the one below it. The language used to discuss genus is also used to discuss species; the one used to discuss orders is also used to discuss genera, etc., up to kingdoms.\n\nNatural language combines nested and ordered metalanguages. In a natural language there is an infinite regress of metalanguages, each with more specialized vocabulary and simpler syntax. Designating the language now as L, the grammar of the language is a discourse in the metalanguage L, which is a sublanguage nested within L. The grammar of L, which has the form of a factual description, is a discourse in the metametalanguage L, which is also a sublanguage of L. The grammar of L, which has the form of a theory describing the syntactic structure of such factual descriptions, is stated in the metametametalanguage L, which likewise is a sublanguage of L. The grammar of L has the form of a metatheory describing the syntactic structure of theories stated in L. L and succeeding metalanguages have the same grammar as L, differing only in reference. Since all of these metalanguages are sublanguages of L, L is a nested metalanguage, but L and sequel are ordered metalanguages. Since all these metalanguages are sublanguages of L they are all embedded languages with respect to the language as a whole.\n\nMetalanguages of formal systems all resolve ultimately to natural language, the 'common parlance' in which mathematicians and logicians converse to define their terms and operations and 'read out' their formulae.\n\nThere are several entities commonly expressed in a metalanguage. In logic usually the object language that the metalanguage is discussing is a formal language, and very often the metalanguage as well.\n\nA deductive system (or, \"deductive apparatus\" of a formal system) consists of the axioms (or axiom schemata) and rules of inference that can be used to derive the theorems of the system.\n\nA metavariable (or \"metalinguistic\" or \"metasyntactic\" variable) is a symbol or set of symbols in a metalanguage which stands for a symbol or set of symbols in some object language. For instance, in the sentence:\n\nThe symbols \"A\" and \"B\" are not symbols of the object language formula_1, they are metavariables in the metalanguage (in this case, English) that is discussing the object language formula_1.\n\nA \"metatheory\" is a theory whose subject matter is some other theory (a theory about a theory). Statements made in the metatheory about the theory are called metatheorems. A metatheorem is a true statement about a formal system expressed in a metalanguage. Unlike theorems proved within a given formal system, a metatheorem is proved within a metatheory, and may reference concepts that are present in the metatheory but not the object theory.\n\nAn interpretation is an assignment of meanings to the symbols and words of a language.\n\nMichael J. Reddy (1979) argues that much of the language we use to talk about language is conceptualized and structured by what he refers to as the conduit metaphor. This paradigm operates through two distinct, related frameworks.\n\nThe \"major framework\" views language as a sealed pipeline between people:\n1. Language transfers people's thoughts and feelings (mental content) to others\n2. Speakers and writers insert their mental content into words\n3. Words are containers\n4. Listeners and writers extract mental content from words\n\nThe \"minor framework\" views language as an open pipe spilling mental content into the void:\n1. Speakers and writers eject mental content into an external space\n2. Mental content is reified (viewed as concrete) in this space\n3. Listeners and writers extract mental content from this space\n\nComputers follow programs, sets of instructions in a formal language. The development of a programming language involves the use of a metalanguage. The act of working with metalanguages in programming is known as \"metaprogramming\". Backus–Naur form, developed in the 1960s by John Backus and Peter Naur, is one of the earliest metalanguages used in computing. Examples of modern-day programming languages which commonly find use in metaprogramming include ML, Lisp, m4, and Yacc.\n\n\n"}
{"id": "1596063", "url": "https://en.wikipedia.org/wiki?curid=1596063", "title": "Mollifier", "text": "Mollifier\n\nIn mathematics, mollifiers (also known as \"approximations to the identity\") are smooth functions with special properties, used for example in distribution theory to create sequences of smooth functions approximating nonsmooth (generalized) functions, via convolution. Intuitively, given a function which is rather irregular, by convolving it with a mollifier the function gets \"mollified\", that is, its sharp features are smoothed, while still remaining close to the original nonsmooth (generalized) function. They are also known as Friedrichs mollifiers after Kurt Otto Friedrichs, who introduced them.\n\nMollifiers were introduced by Kurt Otto Friedrichs in his paper , which is considered a watershed in the modern theory of partial differential equations. The name of this mathematical object had a curious genesis, and Peter Lax tells the whole story in his commentary on that paper published in Friedrichs' \"Selecta\". According to him, at that time, the mathematician Donald Alexander Flanders was a colleague of Friedrichs: since he liked to consult colleagues about English usage, he asked Flanders an advice on how to name the smoothing operator he was using. Flanders was a puritan, nicknamed by his friends Moll after Moll Flanders in recognition of his moral qualities: he suggested to call the new mathematical concept a \"mollifier\" as a pun incorporating both Flanders' nickname and the verb '', meaning 'to smooth over' in a figurative sense.\n\nPreviously, Sergei Sobolev used mollifiers in his epoch making 1938 paper, which contains the proof of the Sobolev embedding theorem: Friedrichs himself acknowledged Sobolev's work on mollifiers stating that:-\"These mollifiers were introduced by Sobolev and the author...\"\".\n\nIt must be pointed out that the term \"mollifier\" has undergone linguistic drift since the time of these foundational works: Friedrichs defined as \"mollifier\" the integral operator whose kernel is one of the functions nowadays called mollifiers. However, since the properties of a linear integral operator are completely determined by its kernel, the name mollfier was inherited by the kernel itself as a result of common usage.\n\n If \"formula_1\" is a smooth function on ℝ\"\", \"n\" ≥ 1, satisfying the following three requirements\n\nwhere formula_4 is the Dirac delta function and the limit must be understood in the space of Schwartz distributions, then \"formula_1\" is a mollifier. The function \"formula_1\" could also satisfy further conditions: for example, if it satisfies\n\nNote 1. When the theory of distributions was still not widely known nor used, property above was formulated by saying that the convolution of the function \"formula_14\" with a given function belonging to a proper Hilbert or Banach space converges as \"ε\" → 0 to that function: this is exactly what Friedrichs did. This also clarifies why mollifiers are related to approximate identities.\n\nNote 2. As briefly pointed out in the \"Historical notes\" section of this entry, originally, the term \"mollifier\" identified the following convolution operator:\n\nwhere formula_16 and \"formula_1\" is a smooth function satisfying the first three conditions stated above and one or more supplementary conditions as positivity and symmetry.\n\nConsider the function \"formula_1\"formula_8 of a variable in ℝ\"\" defined by\n\nformula_20\n\nwhere the numerical constant formula_21 ensures normalization. It is easily seen that this function is infinitely differentiable, non analytic with vanishing derivative for . \"formula_1\" can be therefore used as mollifier as described above: it is also easy to see that \"formula_1\"formula_8 defines a \"positive and symmetric mollifier\".\nAll properties of a mollifier are related to its behaviour under the operation of convolution: we list the following ones, whose proofs can be found in every text on distribution theory.\n\nFor any distribution formula_25, the following family of convolutions indexed by the real number formula_26\n\nwhere formula_28 denotes convolution, is a family of smooth functions.\n\nFor any distribution formula_25, the following family of convolutions indexed by the real number formula_26 converges to formula_25\n\nFor any distribution formula_25,\n\nwhere formula_35 indicates the support in the sense of distributions, and formula_36 indicates their Minkowski addition.\n\nThe basic applications of mollifiers is to prove properties valid for smooth functions also in nonsmooth situations:\n\nIn some theories of generalized functions, mollifiers are used to define the multiplication of distributions: precisely, given two distributions formula_37 and formula_25, the limit of the product of a smooth function and a distribution\n\ndefines (if it exists) their product in various theories of generalized functions.\n\nVery informally, mollifiers are used to prove the identity of two different kind of extension of differential operators: the strong extension and the weak extension. The paper illustrates this concept quite well: however the high number of technical details needed to show what this really means prevent them from being formally detailed in this short description.\n\nBy convolution of the characteristic function of the unit ball formula_40 with the smooth function \"formula_41\" (defined as in with formula_42), one obtains the function\n\nwhich is a smooth function equal to formula_44 on formula_45, with support contained in formula_46. This can be seen easily by observing that if formula_47 ≤ formula_48 and formula_49 ≤ formula_48 then formula_51 ≤ formula_44. Hence for formula_47 ≤ formula_48,\nIt is easy to see how this construction can be generalized to obtain a smooth function identical to one on a neighbourhood of a given compact set, and equal to zero in every point whose distance from this set is greater than a given formula_26. Such a function is called a (smooth) cutoff function: those functions are used to eliminate singularities of a given (generalized) function by multiplication. They leave unchanged the value of the (generalized) function they multiply only on a given set, thus modifying its support: also cutoff functions are the basic parts of smooth partitions of unity.\n\n\n"}
{"id": "25304470", "url": "https://en.wikipedia.org/wiki?curid=25304470", "title": "Morris Birkbeck Pell", "text": "Morris Birkbeck Pell\n\nMorris Birkbeck Pell (31 March 1827, Albion, Illinois, USA – 7 May 1879, Glebe, New South Wales, Australia) was an American-Australian mathematician, professor, lawyer and actuary. He became the inaugural Professor of Mathematics and Natural Philosophy at the University of Sydney in 1852, and continued in the role until ill health enforced his retirement in 1877. He was for many years a member of the University Senate, and councillor and secretary of the Royal Society of New South Wales.\n\nPell's mother Eliza (1797-1880) was a daughter of Morris Birkbeck (1764-1825), the English agricultural innovator, social reformer and antislavery campaigner. In 1817-18 Birkbeck, with George Flower, had founded a utopian colony, the English Settlement, in the Illinois Territory of the United States, and Birkbeck laid out the new town there of Albion, Illinois. A widower since 1804, Birkbeck had brought his seven children with him to America, and it was there that his daughter Eliza met and married Gilbert Titus Pell (1796-1860), who came from a prominent family of New York politicians. Gilbert Pell was descended from Sir John Pell (1643-1702), Lord of Pelham Manor, New York -- who was the son of English mathematician Dr. John Pell (1611-1685), and nephew and heir of early American pioneer and settler Thomas Pell. Gilbert Pell served as a representative in the Illinois legislature, and in the 1850s was appointed United States envoy to Mexico.\n\nMorris Pell was born of this union in the new settlement of Albion in 1827, their third child and only son. In 1835 the family separated and Mrs Pell took her children first to Poughkeepsie, New York, then to Plymouth, England, in 1841, where Morris attended the New Grammar School. In 1849 he graduated as Senior Wrangler in mathematics at Cambridge University—a position once regarded as \"the greatest intellectual achievement attainable in Britain.\"\n\nIn 1852, aged 24, Pell was chosen from twenty-six candidates to become the first Professor of Mathematics and Natural Philosophy at the newly opened University of Sydney, in the British colony of New South Wales, Australia. With his new wife Jane Juliana (née Rusden), his mother and two sisters he sailed from England to Australia on the \"Asiatic\" and became one of the University's three foundation professors. Professor Pell gave the first lecture in Mathematics on 13 October 1852, two days after the University's inauguration, to all 24 students of the University. One of them, William Windeyer, later to become Chancellor of the University, wrote in his diary: \"Went to a lecture at 10 with Mr Pell, who amused as well as instructed, I think I shall like him ...\".\n\nIn 1854, in evidence to a New South Wales Legislative Council select committee on education, Pell advocated the opening of a secular grammar school. In 1859 he testified to the New South Wales Legislative Assembly select committees on the Sydney Grammar School and the University of Sydney, regarding the composition of the University Senate, the adverse effect of clergy on enrolments, the value of liberal studies in the education of businessmen and squatters, and the beneficial effect of the university on secondary education. His evidence resulted in ex-officio membership of the University Senate for professors. He was a member of the Senate from 1861 to 1877 and after resignation was re-elected to the senate in 1878 by members of convocation.\n\nPell was a member of the Australian Philosophical Society from 1856 and served on its council in 1858. Subsequently, Queen Victoria granted Royal Assent to the Society and it was renamed the Royal Society of New South Wales. Pell was a member and its secretary from 1867, and a member of its council from 1869.\n\nFor many years almost crippled by an injury to his spine, Pell resigned in mid-1877 as professor of mathematics at Sydney University, on a pension of £412 10s. \nOn 7 May 1879, aged 52, he died of \"progressive paralysis\" (see Motor neuron disease) and was buried in the Balmain Cemetery in Sydney. He was survived by his estranged wife Julia (née Rusden), five sons and three daughters.\n\n"}
{"id": "24062542", "url": "https://en.wikipedia.org/wiki?curid=24062542", "title": "Noncommutative harmonic analysis", "text": "Noncommutative harmonic analysis\n\nIn mathematics, noncommutative harmonic analysis is the field in which results from Fourier analysis are extended to topological groups that are not commutative. Since locally compact abelian groups have a well-understood theory, Pontryagin duality, which includes the basic structures of Fourier series and Fourier transforms, the major business of non-commutative harmonic analysis is usually taken to be the extension of the theory to all groups \"G\" that are locally compact. The case of compact groups is understood, qualitatively and after the Peter–Weyl theorem from the 1920s, as being generally analogous to that of finite groups and their character theory.\n\nThe main task is therefore the case of \"G\" that is locally compact, not compact and not commutative. The interesting examples include many Lie groups, and also algebraic groups over p-adic fields. These examples are of interest and frequently applied in mathematical physics, and contemporary number theory, particularly automorphic representations.\n\nWhat to expect is known as the result of basic work of John von Neumann. He showed that if the von Neumann group algebra of \"G\" is of type I, then \"L\"(\"G\") as a unitary representation of \"G\" is a direct integral of irreducible representations. It is parametrized therefore by the unitary dual, the set of isomorphism classes of such representations, which is given the hull-kernel topology. The analogue of the Plancherel theorem is abstractly given by identifying a measure on the unitary dual, the Plancherel measure, with respect to which the direct integral is taken. (For Pontryagin duality the Plancherel measure is some Haar measure on the dual group to \"G\", the only issue therefore being its normalization.) For general locally compact groups, or even countable discrete groups, the von Neumann group algebra need not be of type I and the regular representation of \"G\" cannot be written in terms of irreducible representations, even though it is unitary and completely reducible. An example where this happens is the infinite symmetric group, where the von Neumann group algebra is the hyperfinite type II factor. The further theory divides up the Plancherel measure into a discrete and a continuous part. For semisimple groups, and classes of solvable Lie groups, a very detailed theory is available.\n\n\n"}
{"id": "30962657", "url": "https://en.wikipedia.org/wiki?curid=30962657", "title": "Periodic table of shapes", "text": "Periodic table of shapes\n\nThe periodic table of mathematical shapes is popular name given to a project to classify Fano varieties. The project was thought up by Professor Alessio Corti, from the Department of Mathematics at Imperial College London. It aims to categorise all three-, four- and five-dimensional shapes into a single table, analogous to the periodic table of chemical elements. It is meant to hold the equations that describe each shape and, through this, mathematicians and other scientists expect to develop a better understanding of the shapes’ geometric properties and relations. \n\nIt is estimated that 500 million shapes can be defined algebraically in four dimensions, and a few thousand more in the fifth. The project has already won the Philip Leverhulme Prize—worth £70,000—from the Leverhulme Trust.\n\n"}
{"id": "4363670", "url": "https://en.wikipedia.org/wiki?curid=4363670", "title": "Relation algebra", "text": "Relation algebra\n\nIn mathematics and abstract algebra, a relation algebra is a residuated Boolean algebra expanded with an involution called converse, a unary operation. The motivating example of a relation algebra is the algebra 2 of all binary relations on a set \"X\", that is, subsets of the cartesian square \"X\", with \"R\"•\"S\" interpreted as the usual composition of binary relations \"R\" and \"S\", and with the converse of \"R\" as the converse relation.\n\nRelation algebra emerged in the 19th-century work of Augustus De Morgan and Charles Peirce, which culminated in the algebraic logic of Ernst Schröder. The equational form of relation algebra treated here was developed by Alfred Tarski and his students, starting in the 1940s. Tarski and Givant (1987) applied relation algebra to a variable-free treatment of axiomatic set theory, with the implication that mathematics founded on set theory could itself be conducted without variables.\n\nA relation algebra (\"L\", ∧, ∨, , 0, 1, •, I, ˘) is an algebraic structure equipped with the Boolean operations of conjunction \"x\"∧\"y\", disjunction \"x\"∨\"y\", and negation \"x\", the Boolean constants 0 and 1, the relational operations of composition \"x\"•\"y\" and converse \"x\"˘, and the relational constant I, such that these operations and constants satisfy certain equations constituting an axiomatization of a calculus of relations. Roughly, a relation algebra is to a system of binary relations on a set containing the empty (0), complete (1), and identity (I) relations and closed under these five operations as a group is to a system of permutations of a set containing the identity permutation and closed under composition and inverse. However, the first order theory of relation algebras is not complete for such systems of binary relations.\n\nFollowing Jónsson and Tsinakis (1993) it is convenient to define additional operations \"x\"◁\"y\" = \"x\"•\"y\"˘, and, dually, \"x\"▷\"y\" = \"x\"˘•\"y\" . Jónsson and Tsinakis showed that I◁\"x\" = \"x\"▷I, and that both were equal to \"x\"˘. Hence a relation algebra can equally well be defined as an algebraic structure (\"L\", ∧, ∨, , 0, 1, •, I, ◁, ▷). The advantage of this signature over the usual one is that a relation algebra can then be defined in full simply as a residuated Boolean algebra for which I◁\"x\" is an involution, that is, I◁(I◁\"x\") = \"x\" . The latter condition can be thought of as the relational counterpart of the equation 1/(1/\"x\") = \"x\" for ordinary arithmetic reciprocal, and some authors use reciprocal as a synonym for converse.\n\nSince residuated Boolean algebras are axiomatized with finitely many identities, so are relation algebras. Hence the latter form a variety, the variety RA of relation algebras. Expanding the above definition as equations yields the following finite axiomatization.\n\nThe axioms B1-B10 below are adapted from Givant (2006: 283), and were first set out by Tarski in 1948.\n\n\"L\" is a Boolean algebra under binary disjunction, ∨, and unary complementation ():\nThis axiomatization of Boolean algebra is due to Huntington (1933). Note that the meet of the implied Boolean algebra is \"not\" the • operator (even though it distributes over formula_1 like a meet does), nor is the 1 of the Boolean algebra the I constant.\n\n\"L\" is a monoid under binary composition (•) and nullary identity I:\n\nUnary converse ()˘ is an involution with respect to composition:\n\nAxiom B6 defines conversion as an involution, whereas B7 expresses the antidistributive property of conversion relative to composition.\n\nConverse and composition distribute over disjunction:\n\nB10 is Tarski's equational form of the fact, discovered by Augustus De Morgan, that \"A\"•\"B\" ≤ \"C\" \"A\"˘•\"C\" ≤ \"B\" \"C\"•\"B\"˘ ≤ \"A\".\n\nThese axioms are ZFC theorems; for the purely Boolean B1-B3, this fact is trivial. After each of the following axioms is shown the number of the corresponding theorem in Chapter 3 of Suppes (1960), an exposition of ZFC: B4 27, B5 45, B6 14, B7 26, B8 16, B9 23.\n\nThe following table shows how many of the usual properties of binary relations can be expressed as succinct RA equalities or inequalities. Below, an inequality of the form \"A\"≤\"B\" is shorthand for the Boolean equation \"A\"∨\"B\" = \"B\".\n\nThe most complete set of results of this nature is Chapter C of Carnap (1958), where the notation is rather distant from that of this entry. Chapter 3.2 of Suppes (1960) contains fewer results, presented as ZFC theorems and using a notation that more resembles that of this entry. Neither Carnap nor Suppes formulated their results using the RA of this entry, or in an equational manner.\nThe metamathematics of RA are discussed at length in Tarski and Givant (1987), and more briefly in Givant (2006).\n\nRA consists entirely of equations manipulated using nothing more than uniform replacement and the substitution of equals for equals. Both rules are wholly familiar from school mathematics and from abstract algebra generally. Hence RA proofs are carried out in a manner familiar to all mathematicians, unlike the case in mathematical logic generally.\n\nRA can express any (and up to logical equivalence, exactly the) first-order logic (FOL) formulas containing no more than three variables. (A given variable can be quantified multiple times and hence quantifiers can be nested arbitrarily deeply by \"reusing\" variables.) Surprisingly, this fragment of FOL suffices to express Peano arithmetic and almost all axiomatic set theories ever proposed. Hence RA is, in effect, a way of algebraizing nearly all mathematics, while dispensing with FOL and its connectives, quantifiers, turnstiles, and modus ponens. Because RA can express Peano arithmetic and set theory, Gödel's incompleteness theorems apply to it; RA is incomplete, incompletable, and undecidable. (N.B. The Boolean algebra fragment of RA is complete and decidable.)\n\nThe representable relation algebras, forming the class RRA, are those relation algebras isomorphic to some relation algebra consisting of binary relations on some set, and closed under the intended interpretation of the RA operations. It is easily shown, e.g. using the method of pseudoelementary classes, that RRA is a quasivariety, that is, axiomatizable by a universal Horn theory. In 1950, Roger Lyndon proved the existence of equations holding in RRA that did not hold in RA. Hence the variety generated by RRA is a proper subvariety of the variety RA. In 1955, Alfred Tarski showed that RRA is itself a variety. In 1964, Donald Monk showed that RRA has no finite axiomatization, unlike RA, which is finitely axiomatized by definition.\n\nAn RA is a Q-relation algebra (QRA) if, in addition to B1-B10, there exist some \"A\" and \"B\" such that (Tarski and Givant 1987: §8.4):\n\nEssentially these axioms imply that the universe has a (non-surjective) pairing relation whose projections are \"A\" and \"B\". It is a theorem that every QRA is a RRA (Proof by Maddux, see Tarski & Givant 1987: 8.4(iii) ).\n\nEvery QRA is representable (Tarski and Givant 1987). That not every relation algebra is representable is a fundamental way RA differs from QRA and Boolean algebras, which, by Stone's representation theorem for Boolean algebras, are always representable as sets of subsets of some set, closed under union, intersection, and complement.\n\n1. Any Boolean algebra can be turned into a RA by interpreting conjunction as composition (the monoid multiplication •), i.e. \"x\"•\"y\" is defined as \"x\"∧\"y\". This interpretation requires that converse interpret identity (\"ў\" = \"y\"), and that both residuals \"y\"\\\"x\" and \"x\"/\"y\" interpret the conditional \"y\"→\"x\" (i.e., ¬\"y\"∨\"x\").\n\n2. The motivating example of a relation algebra depends on the definition of a binary relation \"R\" on a set \"X\" as any subset \"R\" ⊆ \"X\"², where \"X\"² is the Cartesian square of \"X\". The power set 2 consisting of all binary relations on \"X\" is a Boolean algebra. While 2 can be made a relation algebra by taking \"R\"•\"S\" = \"R\"∧\"S\", as per example (1) above, the standard interpretation of • is instead \"x\"(\"R\"•\"S\")\"z\" = ∃\"y\":\"xRy.ySz\". That is, the ordered pair (\"x\",\"z\") belongs to the relation \"R\"•\"S\" just when there exists \"y\" ∈ \"X\" such that (\"x\",\"y\") ∈ \"R\" and (\"y\",\"z\") ∈ \"S\". This interpretation uniquely determines \"R\"\\\"S\" as consisting of all pairs (\"y\",\"z\") such that for all \"x\" ∈ \"X\", if \"xRy\" then \"xSz\". Dually, \"S\"/\"R\" consists of all pairs (\"x\",\"y\") such that for all \"z\" ∈ \"X\", if \"yRz\" then \"xSz\". The translation \"ў\" = ¬(y\\¬I) then establishes the converse \"R\"˘ of \"R\" as consisting of all pairs (\"y\",\"x\") such that (\"x\",\"y\") ∈ \"R\".\n\n3. An important generalization of the previous example is the power set 2 where \"E\" ⊆ \"X\"² is any equivalence relation on the set \"X\". This is a generalization because \"X\"² is itself an equivalence relation, namely the complete relation consisting of all pairs. While 2 is not a subalgebra of 2 when \"E\" ≠ \"X\"² (since in that case it does not contain the relation \"X\"², the top element 1 being \"E\" instead of \"X\"²), it is nevertheless turned into a relation algebra using the same definitions of the operations. Its importance resides in the definition of a \"representable relation algebra\" as any relation algebra isomorphic to a subalgebra of the relation algebra 2 for some equivalence relation \"E\" on some set. The previous section says more about the relevant metamathematics.\n\n4. Let formula_2 be group. Then the power set formula_3 is a relation algebra with the obvious boolean algebra operations, composition given by the product of group subsets, the converse by the inverse subset (formula_4), and the identity by the singleton subset formula_5. There is a relation algebra homomorphism embedding formula_3 in formula_7 which sends each subset formula_8 to the relation\nformula_9. The image of this homomorphism is the set of all right-invariant relations on formula_2. \n\n5. If group sum or product interprets composition, group inverse interprets converse, group identity interprets I, and if \"R\" is a one-to-one correspondence, so that \"R\"˘•\"R\" = \"R•R\"˘ = I, then \"L\" is a group as well as a monoid. B4-B7 become well-known theorems of group theory, so that RA becomes a proper extension of group theory as well as of Boolean algebra.\n\nDe Morgan founded RA in 1860, but C. S. Peirce took it much further and became fascinated with its philosophical power. The work of DeMorgan and Peirce came to be known mainly in the extended and definitive form Ernst Schröder gave it in Vol. 3 of his \"Vorlesungen\" (1890–1905). \"Principia Mathematica\" drew strongly on Schröder's RA, but acknowledged him only as the inventor of the notation. In 1912, Alwin Korselt proved that a particular formula in which the quantifiers were nested four deep had no RA equivalent. This fact led to a loss of interest in RA until Tarski (1941) began writing about it. His students have continued to develop RA down to the present day. Tarski returned to RA in the 1970s with the help of Steven Givant; this collaboration resulted in the monograph by Tarski and Givant (1987), the definitive reference for this subject. For more on the history of RA, see Maddux (1991, 2006).\n\n\n\n\n\n"}
{"id": "53871768", "url": "https://en.wikipedia.org/wiki?curid=53871768", "title": "Robert J. Berman", "text": "Robert J. Berman\n\nRobert J. Berman is a Swedish mathematical scientist currently at Chalmers University and was awarded the Göran Gustafsson Prize in 2017.\n"}
{"id": "2699476", "url": "https://en.wikipedia.org/wiki?curid=2699476", "title": "Shortest common supersequence problem", "text": "Shortest common supersequence problem\n\nIn computer science, the shortest common supersequence of two sequences X and Y is the shortest sequence which has X and Y as subsequences. This is a problem closely related to the longest common subsequence problem. Given two sequences X = < x...,x > and Y = < y...,y >, a sequence U = < u...,u > is a common supersequence of X and Y if items can be removed from U to produce X or Y.\n\nA shortest common supersequence (SCS) is a common supersequence of minimal length. In the shortest common supersequence problem, the two sequences X and Y are given and the task is to find a shortest possible common supersequence of these sequences. In general, an SCS is not unique.\n\nFor two input sequences, an SCS can be formed from a longest common subsequence (LCS) easily. For example, if Xformula_1 and Yformula_2, the lcs is Zformula_3. By inserting the non-lcs symbols while preserving the symbol order, we get the SCS: Uformula_4.\n\nIt is quite clear that formula_5 for two input sequences. However, for three or more input sequences this does not hold. Note also, that the lcs and the SCS problems are not dual problems.\n\nThe closely related problem of finding a string which is a superstring of a finite set of strings = { ..., } is NP-Complete. Also, good (constant factor) approximations have been found for the average case but not for the worst case. However, it can be formulated as an instance of weighted set cover in such a way that the weight of the optimal solution to the set cover instance is less than twice the length of the shortest superstring . One can then use the O(log())-approximation for weighted set-cover to obtain an O(log())-approximation for the shortest superstring (note that this is \"not\" a constant factor approximation).\n\nFor any string in this alphabet, define () to be the set of all strings which are substrings of . The instance of set cover is formulated as follows: \n\nThe instance can then be solved using an algorithm for weighted set cover, and the algorithm can output an arbitrary concatenation of the strings for which the weighted set cover algorithm outputs ().\n\nConsider the set = { abc, cde, fab }, which becomes the universe of the weighted set cover instance. In this case, = { abcde, fabc }. Then the set of subsets of the universe is\nwhich have costs 3, 3, 3, 5, and 4, respectively.\n\n\n"}
{"id": "31866540", "url": "https://en.wikipedia.org/wiki?curid=31866540", "title": "Sones GraphDB", "text": "Sones GraphDB\n\nSones GraphDB was a graph database developed by the German company sones GmbH, available from 2010 to 2012. Its last version was released in May 2011. sones GmbH, which was based in Erfurt and Leipzig, was declared bankrupt on January 1, 2012.\n\nGraphDB was unique in that its design based on weighted graphs. The open source edition was released in July 2010. The commercially available enterprise version offered a wider variety of functions.\n\nGraphDB was developed in the programming language C# and ran on Microsoft's .NET Framework and on the open source reimplementation Mono.\n\nGraphDB was available as software as a service (SaaS) on the Microsoft cloud Azure Services Platform. GraphDB was also a component of an open source solution stack.\n\nIn 2014 the trademark \"GraphDB\" was acquired by Ontotext. OWLIM, Ontotext's graph database and RDF triplestore, was renamed GraphDB.\n\nGraphDB had index-free adjacency, which meant that it not necessary to manage a global index for relationships between nodes/entities. The linked objects contained direct reference to their adjacent neighboring nodes.\n\nThe sones graph database was able to store and retrieve unstructured properties in any node of the graph. The idea was also to transfer unstructured data to structured data and vice versa.\n\nStructured data could be dynamically extended with high performance in nodes and edges during runtime. Additional properties could easily be entered or deleted from vertex types in a short amount of time.\n\nGraphDB used its own query language, GraphQL, which was similar to SQL. It could be dynamically extended during runtime using plugins such as functions or aggregates.\n\nGraphDB used an object-oriented concept, which enabled better integration into object-oriented programming languages.\n\nIn addition to providing a number of interfaces (e.g., Java, C#, WebShell, WebDAV) the sones graph database also offers a REST API. This enables simpler interaction with state-of-the-art web technologies. A REST-query is all that is needed to execute CRUD operations directly on the database.\n\nThe Traverser API makes it possible to analyze local data. Based on a number of nodes (local), neighboring nodes can be searched recursively (breadth-depth first).\n\nGraphDB has a modular structure consisting of 4 application layers. The storage engines act as the interface to different storage media. The GraphFS serializes and deserializes database objects (nodes and edges) and operates the available storage engines. The actual graph-oriented database logic as well as all functionalities specific to the database are implemented in the GraphDB. The GraphDS provides the interface for using the database. The interfaces between the application layers are generic, which makes it possible to update components separately.\n\n\n"}
{"id": "20933302", "url": "https://en.wikipedia.org/wiki?curid=20933302", "title": "Star coloring", "text": "Star coloring\n\nIn graph-theoretic mathematics, a star coloring of a graph \"G\" is a (proper) vertex coloring in which every path on four vertices uses at least three distinct colors. Equivalently, in a star coloring, the induced subgraphs formed by the vertices of any two colors has connected components that are star graphs. Star coloring has been introduced by .\nThe star chromatic number formula_1 of \"G\" is the least number of colors needed to star color \"G\".\n\nOne generalization of star coloring is the closely related concept of acyclic coloring, where it is required that every cycle uses at least three colors, so the two-color induced subgraphs are forests. If we denote the acyclic chromatic number of a graph \"G\" by formula_2, we have that formula_3, and in fact every star coloring of \"G\" is an acyclic coloring.\n\nThe star chromatic number has been proved to be bounded on every proper minor closed class by . This results was further generalized by to all low-tree-depth colorings (standard coloring and star coloring being low-tree-depth colorings with respective parameter 1 and 2).\n\nIt was demonstrated by that it is NP-complete to determine whether formula_4, even when \"G\" is a graph that is both planar and bipartite.\n\n\n"}
{"id": "1151323", "url": "https://en.wikipedia.org/wiki?curid=1151323", "title": "Thue equation", "text": "Thue equation\n\nIn mathematics, a Thue equation is a Diophantine equation of the form\n\nwhere \"ƒ\" is an irreducible bivariate form of degree at least 3 over the rational numbers, and \"r\" is a nonzero rational number. It is named after Axel Thue who in 1909 proved a theorem, now called Thue's theorem, that a Thue equation has finitely many solutions in integers \"x\" and \"y\".\n\nThe Thue equation is solvable effectively: there is an explicit bound on the solutions \"x\", \"y\" of the form formula_1 where constants \"C\" and \"C\" depend only on the form \"ƒ\". A stronger result holds, that if \"K\" is the field generated by the roots of \"ƒ\" then the equation has only finitely many solutions with \"x\" and \"y\" integers of \"K\" and again these may be effectively determined.\n\nSolving a Thue equation can be described as an algorithm ready for implementation in software. In particular, it is implemented in the following computer algebra systems:\n"}
{"id": "12261835", "url": "https://en.wikipedia.org/wiki?curid=12261835", "title": "Variety (cybernetics)", "text": "Variety (cybernetics)\n\nIn cybernetics, the term variety denotes the total number of distinct states of a system.\n\nThe term Variety was introduced by W. Ross Ashby to denote the count of the total number of states of a system. The condition for dynamic stability under perturbation (or input) was described by his Law of Requisite Variety. Ashby says:\n\nThus, if the order of occurrence is ignored, the collectionc, b, c, a, c, c, a, b, c, b, b, a\nwhich contains twelve elements, contains only three \"distinct\" elements- a, b, c. Such a set will be said to have a variety of three elements.\nHe adds \nThe observer and his powers of discrimination may have to be specified if the variety is to be well defined.\nVariety can be stated as an integer, as above, or as the logarithm to the base 2 of the number \"i.e.\" in bits.\n\nIf a system is to be stable, the number of states of its control mechanism must be greater than or equal to the number of states in the system being controlled. Ashby states the Law as \"variety can destroy variety\". He sees this as aiding the study of problems in biology and a \"wealth of possible applications\" . He sees his approach as introductory to Shannon Information Theory (1948) which deals with the case of \"incessant fluctuations\" or noise. The Requisite Variety condition can be seen as a simple statement of a necessary dynamic equilibrium condition in information theory terms \"cf.\" Newton's third law, Le Chatelier's principle.\n\nLater, in 1970, Conant working with Ashby produced the good regulator theorem which required autonomous systems to acquire an internal model of their environment to persist and achieve stability (e.g. Nyquist stability criterion) or dynamic equilibrium.\n\nStafford Beer defines variety as \"the total number of \"possible\" states of a system, or of an element of a system\", \"cf.\" Ludwig Boltzmann's Wahrscheinlichkeit. Beer restates the Law of Requisite Variety as \"Variety absorbs variety\". Stated more simply the logarithmic measure of variety represents the minimum number of choices (by binary chop) needed to resolve uncertainty. Beer used this to allocate the management resources necessary to maintain process viability.\n\nVariety is one of nine requisites that are required by an ethical regulator.\n\nIn general a description of the required inputs and outputs is established then encoded with the minimum variety necessary. The mapping of input bits to output bits can then produce an estimate of the minimum hardware or software components necessary to produce the desired control behaviour; for example, in a piece of computer software or computer hardware.\n\nThe cybernetician Frank George discussed the variety of teams competing in games like football or rugby to produce goals or tries. A winning chess player might be said to have more variety than his losing opponent. Here a simple ordering is implied. The attenuation and amplification of variety were major themes in Stafford Beer's work in management (the profession of control, as he called it). The number of staff needed to answer telephones, control crowds or tend to patients are clear examples.\n\nThe application of natural and analogue signals to variety analysis require an estimate of Ashby's \"powers of discrimination\" (see above quote). Given the butterfly effect of dynamical systems care must be taken before quantitative measures can be produced. Small quantities, which might be overlooked, can have big effects. In his \"Designing Freedom\" Stafford Beer discusses the patient in a hospital with a temperature denoting fever. Action must be taken immediately to isolate the patient. Here no amount of variety recording the \"patients' average temperature\" would detect this small signal which might have a big effect. Monitoring is required on individuals thus amplifying variety (see \"Algedonic alerts\" in the viable system model or VSM). Beer's work in management cybernetics and VSM is largely based on variety engineering.\n\nFurther applications involving Ashby's view of state counting include the analysis of digital bandwidth requirements, redundancy and software bloat, the bit representation of data types and indexes, analogue to digital conversion, the bounds on finite state machines and data compression. See also, e.g., Excited state, State (computer science), State pattern, State (controls) and Cellular automaton. Requisite Variety can be seen in Chaitin's Algorithmic information theory where a longer, higher variety program or finite state machine produces incompressible output with more variety or information content.\n\nRecently James Lovelock suggested burning and burying carbonized agricultural waste to sequester carbon. A variety calculation requires estimates of global annual agricultural waste production, burial and pyrolysis efficiency to estimate the mass of carbon thus sequestered from the atmosphere.\n\n\n\n"}
{"id": "52549926", "url": "https://en.wikipedia.org/wiki?curid=52549926", "title": "Wilfried Imrich", "text": "Wilfried Imrich\n\nWilfried Imrich (born May 25, 1941) is an Austrian mathematician working mainly in graph theory. He is known for his work on graph products, and authored the books \"Product Graphs: Structure and Recognition\" (Wiley, 2000, with Sandi Klavžar), \"Topics in graph theory: Graphs and their Cartesian Products\" (AK Peters, 2008, with Klavžar and Douglas F. Rall), and \"Handbook of Product Graphs\" (2nd ed., CRC, 2011, with Klavžar and Richard Hammack).\n\nImrich earned his doctorate from the University of Vienna in 1965, under the joint supervision of Nikolaus Hofreiter and Edmund Hlawka.\nHe has worked as a researcher for IBM in Vienna, as an assistant professor at the Technical University of Vienna and the University at Albany, SUNY, as a postdoctoral researcher at Lomonosov University, and, since 1973, as a full professor at the University of Leoben in Austria. He retired in 2009, becoming a professor emeritus at Leoben. He is on board of advisors of the journal \"Ars Mathematica Contemporanea\". Since 2012 he has been a member of the Academia Europaea.\n\n"}
{"id": "49295094", "url": "https://en.wikipedia.org/wiki?curid=49295094", "title": "Zvezdelina Stankova", "text": "Zvezdelina Stankova\n\nZvezdelina Entcheva Stankova (; born 15 September 1969) is a professor of mathematics at Mills College and a teaching professor at the University of California, Berkeley, the founder of the Berkeley Math Circle, and an expert in the combinatorial enumeration of permutations with forbidden patterns<ref>\n"}
