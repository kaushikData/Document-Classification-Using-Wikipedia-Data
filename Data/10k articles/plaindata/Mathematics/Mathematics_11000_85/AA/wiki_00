{"id": "1983545", "url": "https://en.wikipedia.org/wiki?curid=1983545", "title": "148 (number)", "text": "148 (number)\n\n148 (one hundred [and] forty-eight) is the natural number following 147 and before 149.\n\n\n\n\n\n\n\n\n"}
{"id": "20786325", "url": "https://en.wikipedia.org/wiki?curid=20786325", "title": "1991 Bangladesh census", "text": "1991 Bangladesh census\n\nIn 1991, the Bangladesh Bureau of Statistics, conducted a national census in Bangladesh. They recorded data from all of the districts and upazilas and main cities in Bangladesh including statistical data on population size, households, sex and age distribution, marital status, economically active population, literacy and educational attainment, religion, number of children etc.\n\n\n"}
{"id": "312881", "url": "https://en.wikipedia.org/wiki?curid=312881", "title": "Action (physics)", "text": "Action (physics)\n\nIn physics, action is an attribute of the dynamics of a physical system from which the equations of motion of the system can be derived. It is a mathematical functional which takes the trajectory, also called \"path\" or \"history\", of the system as its argument and has a real number as its result. Generally, the action takes different values for different paths. Action has the dimensions of [energy]⋅[time] or [momentum]⋅[length], and its SI unit is joule-second.\n\nHamilton's principle states that the differential equations of motion for \"any\" physical system can be re-formulated as an equivalent integral equation. Thus, there are two distinct approaches for formulating dynamical models.\n\nIt applies not only to the classical mechanics of a single particle, but also to classical fields such as the electromagnetic and gravitational fields. Hamilton's principle has also been extended to quantum mechanics and quantum field theory—in particular the path integral formulation of quantum mechanics makes use of the concept—where a physical system randomly follows one of the possible paths, with the phase of the probability amplitude for each path being determined by the action for the path.\n\nEmpirical laws are frequently expressed as differential equations, which describe how physical quantities such as position and momentum change continuously with time, space or a generalization thereof. Given the initial and boundary conditions for the situation, the \"solution\" to these empirical equations is one or more functions that describe the behavior of the system and are called \"equations of motion\".\n\n\"Action\" is a part of an alternative approach to finding such equations of motion. Classical mechanics postulates that the path actually followed by a physical system is that for which the \"action is minimized\", or more generally, is stationary. In other words, the action satisfies a variational principle: the principle of stationary action (see also below). The action is defined by an integral, and the classical equations of motion of a system can be derived by minimizing the value of that integral.\n\nThis simple principle provides deep insights into physics, and is an important concept in modern theoretical physics.\n\n\"Action\" was defined in several now obsolete ways during the development of the concept.\n\nExpressed in mathematical language, using the calculus of variations, the evolution of a physical system (i.e., how the system actually progresses from one state to another) corresponds to a stationary point (usually, a minimum) of the action.\n\nSeveral different definitions of \"the action\" are in common use in physics. The action is usually an integral over time. However, when the action pertains to fields, it may be integrated over spatial variables as well. In some cases, the action is integrated along the path followed by the physical system.\n\nThe action is typically represented as an integral over time, taken along the path of the system between the initial time and the final time of the development of the system:\nwhere the integrand \"L\" is called the Lagrangian. For the action integral to be well-defined, the trajectory has to be bounded in time and space.\n\nAction has the dimensions of [energy]⋅[time], and its SI unit is joule-second, which is identical to the unit of angular momentum.\n\nIn classical physics, the term \"action\" has a number of meanings.\n\nMost commonly, the term is used for a functional formula_2 which takes a function of time and (for fields) space as input and returns a scalar. In classical mechanics, the input function is the evolution q(\"t\") of the system between two times \"t\" and \"t\", where q represents the generalized coordinates. The action formula_3 is defined as the integral of the Lagrangian \"L\" for an input evolution between the two times:\n\nwhere the endpoints of the evolution are fixed and defined as formula_5 and formula_6. According to Hamilton's principle, the true evolution q(\"t\") is an evolution for which the action formula_3 is stationary (a minimum, maximum, or a saddle point). This principle results in the equations of motion in Lagrangian mechanics.\n\nUsually denoted as formula_8, this is also a functional. Here the input function is the \"path\" followed by the physical system without regard to its parameterization by time. For example, the path of a planetary orbit is an ellipse, and the path of a particle in a uniform gravitational field is a parabola; in both cases, the path does not depend on how fast the particle traverses the path. The abbreviated action formula_8 is defined as the integral of the generalized momenta along a path in the generalized coordinates:\n\nAccording to Maupertuis' principle, the true path is a path for which the abbreviated action formula_8 is stationary.\n\nHamilton's principal function is defined by the Hamilton–Jacobi equations (HJE), another alternative formulation of classical mechanics. This function \"S\" is related to the functional formula_2 by fixing the initial time \"t\" and the initial endpoint q and allowing the upper limits \"t\" and the second endpoint q to vary; these variables are the arguments of the function \"S\". In other words, the action function \"S\" is the indefinite integral of the Lagrangian with respect to time.\n\nWhen the total energy \"E\" is conserved, the Hamilton–Jacobi equation can be solved with the additive separation of variables:\n\nwhere the time-independent function \"W\"(\"q\", \"q\", … \"q\") is called \"Hamilton's characteristic function\". The physical significance of this function is understood by taking its total time derivative\n\nThis can be integrated to give\n\nwhich is just the abbreviated action.\n\nThe Hamilton–Jacobi equations are often solved by additive separability; in some cases, the individual terms of the solution, e.g., \"S\"(\"q\"), are also called an \"action\".\n\nThis is a single variable \"J\" in the action-angle coordinates, defined by integrating a single generalized momentum around a closed path in phase space, corresponding to rotating or oscillating motion:\n\nThe variable \"J\" is called the \"action\" of the generalized coordinate \"q\"; the corresponding canonical variable conjugate to \"J\" is its \"angle\" \"w\", for reasons described more fully under action-angle coordinates. The integration is only over a single variable \"q\" and, therefore, unlike the integrated dot product in the abbreviated action integral above. The \"J\" variable equals the change in \"S\"(\"q\") as \"q\" is varied around the closed path. For several physical systems of interest, J is either a constant or varies very slowly; hence, the variable \"J\" is often used in perturbation calculations and in determining adiabatic invariants.\n\nSee tautological one-form.\n\nAs noted above, the requirement that the action integral be stationary under small perturbations of the evolution is equivalent to a set of differential equations (called the Euler–Lagrange equations) that may be determined using the calculus of variations. We illustrate this derivation here using only one coordinate, \"x\"; the extension to multiple coordinates is straightforward.\n\nAdopting Hamilton's principle, we assume that the Lagrangian \"L\" (the integrand of the action integral) depends only on the coordinate \"x\"(\"t\") and its time derivative \"dx\"(\"t\")/\"dt\", and may also depend explicitly on time. In that case, the action integral can be written as\n\nwhere the initial and final times (\"t\" and \"t\") and the final and initial positions are specified in advance as formula_18 and formula_19. Let \"x\"(\"t\") represent the true evolution that we seek, and let formula_20 be a slightly perturbed version of it, albeit with the same endpoints, formula_21 and formula_22. The difference between these two evolutions, which we will call formula_23, is infinitesimally small at all times:\n\nAt the endpoints, the difference vanishes, i.e., formula_25.\n\nExpanded to first order, the difference between the actions integrals for the two evolutions is\n\nIntegration by parts of the last term, together with the boundary conditions formula_25, yields the equation\n\nThe requirement that formula_2 be stationary implies that the first-order change must be zero for \"any\" possible perturbation ε(\"t\") about the true evolution:\n\nThis can be true only if\n\nThe Euler–Lagrange equation is obeyed provided the functional derivative of the action integral is identically zero:\n\nThe quantity formula_31 is called the \"conjugate momentum\" for the coordinate \"x\". An important consequence of the Euler–Lagrange equations is that if \"L\" does not explicitly contain coordinate \"x\", i.e.\n\nIn such cases, the coordinate \"x\" is called a \"cyclic\" coordinate, and its conjugate momentum is conserved.\n\nSimple examples help to appreciate the use of the action principle via the Euler–Lagrangian equations. A free particle (mass \"m\" and velocity \"v\") in Euclidean space moves in a straight line. Using the Euler–Lagrange equations, this can be shown in polar coordinates as follows. In the absence of a potential, the Lagrangian is simply equal to the kinetic energy\nin orthonormal (\"x\", \"y\") coordinates, where the dot represents differentiation with respect to the curve parameter (usually the time, \"t\").\nIn polar coordinates (\"r\", φ) the kinetic energy and hence the Lagrangian becomes\n\nThe radial \"r\" and angular φ components of the Euler–Lagrangian equations become respectively\n\nThe solution of these two equations is given by\n\nfor a set of constants \"a\", \"b\", \"c\", \"d\" determined by initial conditions.\nThus, indeed, \"the solution is a straight line\" given in polar coordinates.\n\nThe action principle can be extended to obtain the equations of motion for fields, such as the electromagnetic field or gravitational field.\n\nThe Einstein equation utilizes the \"Einstein–Hilbert action\" as constrained by a variational principle.\n\nThe trajectory (path in spacetime) of a body in a gravitational field can be found using the action principle. For a free falling body, this trajectory is a geodesic.\n\nImplications of symmetries in a physical situation can be found with the action principle, together with the Euler–Lagrange equations, which are derived from the action principle. An example is Noether's theorem, which states that to every continuous symmetry in a physical situation there corresponds a conservation law (and conversely). This deep connection requires that the action principle be assumed.\n\nIn quantum mechanics, the system does not follow a single path whose action is stationary, but the behavior of the system depends on all permitted paths and the value of their action. The action corresponding to the various paths is used to calculate the path integral, that gives the probability amplitudes of the various outcomes.\n\nAlthough equivalent in classical mechanics with Newton's laws, the action principle is better suited for generalizations and plays an important role in modern physics. Indeed, this principle is one of the great generalizations in physical science. It is best understood within quantum mechanics, particularly in Richard Feynman's path integral formulation, where it arises out of destructive interference of quantum amplitudes.\n\nMaxwell's equations can also be derived as conditions of stationary action.\n\nWhen relativistic effects are significant, the action of a point particle of mass \"m\" travelling a world line \"C\" parametrized by the proper time formula_38 is\n\nIf instead, the particle is parametrized by the coordinate time \"t\" of the particle and the coordinate time ranges from \"t\" to \"t\", then the action becomes\n\nwhere the Lagrangian is\n\nThe action principle can be generalized still further. For example, the action need not be an integral, because nonlocal actions are possible. The configuration space need not even be a functional space, given certain features such as noncommutative geometry. However, a physical basis for these mathematical extensions remains to be established experimentally.\n\nFor an annotated bibliography, see Edwin F. Taylor who lists, among other things, the following books\n\n"}
{"id": "49481662", "url": "https://en.wikipedia.org/wiki?curid=49481662", "title": "Andreas Brandstädt", "text": "Andreas Brandstädt\n\nAndreas Brandstädt (born 17 January 1949 in Arnstadt, East Germany) is a German mathematician and computer scientist.\n\nHe graduated from the Friedrich Schiller University of Jena, Germany, with a Ph.D. (Dr. rer. nat.) in stochastics in 1976 and a habilitation (Dr. sc. nat.) in complexity theory in 1983.\nSince 1974 he worked there in the group of his academic teacher Gerd Wechsung.\n\nFrom 1991 to 1994, he was the professor for Computer Science in the Department of Mathematics,\nand from 1994 to 2014 he was the professor for Theoretical Computer Science at the University of Rostock, Germany.\n\nHe was a visiting professor at the universities of Metz, Amiens, and Clermont-Ferrand (France) and at the University of Primorska in Koper\n(Slovenia). He was Invited Speaker at various conferences in Argentina, Austria, Belarus, Brasil, Canada, China, France,\nGreece, India, Israel, Norway, Poland, Slovenia, and Switzerland.\n\nBrandstädt is an active researcher in graph algorithms, discrete mathematics, combinatorial optimization, and graph theory. A frequently used tool in his papers is tree structure of graphs and hypergraphs such as for hypertrees, strongly chordal graphs and chordal graphs.\n\nHe frequently took part in program committees such as Workshop on Graph-Theoretic Concepts in Computer Science (and three times was a co-organizer of this\nconference) and is member of the Editorial Board of Discrete Applied Mathematics.\n\n\n"}
{"id": "2019", "url": "https://en.wikipedia.org/wiki?curid=2019", "title": "André Weil", "text": "André Weil\n\nAndré Weil (; ; 6 May 1906 – 6 August 1998) was an influential French mathematician of the 20th century, known for his foundational work in number theory and algebraic geometry. He was a founding member and the \"de facto\" early leader of the mathematical Bourbaki group. The philosopher Simone Weil was his sister.\n\nAndré Weil was born in Paris to agnostic Alsatian Jewish parents who fled the annexation of Alsace-Lorraine by the German Empire after the Franco-Prussian War in 1870–71. The famous philosopher Simone Weil was Weil's only sibling. He studied in Paris, Rome and Göttingen and received his doctorate in 1928. While in Germany, Weil befriended Carl Ludwig Siegel. Starting in 1930, he spent two academic years at Aligarh Muslim University. Aside from mathematics, Weil held lifelong interests in classical Greek and Latin literature, in Hinduism and Sanskrit literature: he taught himself Sanskrit in 1920. After teaching for one year in Aix-Marseille University, he taught for six years in Strasbourg. He married Éveline in 1937.\n\nWeil was in Finland when World War II broke out; he had been traveling in Scandinavia since April 1939. His wife Éveline returned to France without him. Weil was mistakenly arrested in Finland at the outbreak of the Winter War on suspicion of spying; however, accounts of his life having been in danger were shown to be exaggerated. Weil returned to France via Sweden and the United Kingdom, and was detained at Le Havre in January 1940. He was charged with failure to report for duty, and was imprisoned in Le Havre and then Rouen. It was in the military prison in Bonne-Nouvelle, a district of Rouen, from February to May, that Weil completed the work that made his reputation. He was tried on 3 May 1940. Sentenced to five years, he requested to be attached to a military unit instead, and was given the chance to join a regiment in Cherbourg. After the fall of France, he met up with his family in Marseille, where he arrived by sea. He then went to Clermont-Ferrand, where he managed to join his wife Éveline, who had been living in German-occupied France.\n\nIn January 1941, Weil and his family sailed from Marseille to New York. He spent the remainder of the war in the United States, where he was supported by the Rockefeller Foundation and the Guggenheim Foundation. For two years, he taught undergraduate mathematics at Lehigh University, where he was unappreciated, overworked and poorly paid, although he didn't have to worry about being drafted, unlike his American students. But, he hated Lehigh very much for their heavy teaching workload and he swore that he would never talk about \"Lehigh\" any more. He quit the job at Lehigh, and then he moved to Brazil and taught at the Universidade de São Paulo from 1945 to 1947, where he worked with Oscar Zariski. He then returned to the United States and taught at the University of Chicago from 1947 to 1958, before moving to the Institute for Advanced Study, where he would spend the remainder of his career. He was a Plenary Speaker at the ICM in 1950 in Cambridge, Massachusetts, in 1954 in Amsterdam, and in 1978 in Helsinki. In 1979, Weil shared the second Wolf Prize in Mathematics with Jean Leray.\n\nWeil made substantial contributions in a number of areas, the most important being his discovery of profound connections between algebraic geometry and number theory. This began in his doctoral work leading to the Mordell–Weil theorem (1928, and shortly applied in Siegel's theorem on integral points). Mordell's theorem had an \"ad hoc\" proof; Weil began the separation of the infinite descent argument into two types of structural approach, by means of height functions for sizing rational points, and by means of Galois cohomology, which would not be categorized as such for another two decades. Both aspects of Weil's work have steadily developed into substantial theories.\n\nAmong his major accomplishments were the 1940s proof of the Riemann hypothesis for zeta-functions of curves over finite fields, and his subsequent laying of proper foundations for algebraic geometry to support that result (from 1942 to 1946, most intensively). The so-called Weil conjectures were hugely influential from around 1950; these statements were later proved by Bernard Dwork, Alexander Grothendieck, Michael Artin, and finally by Pierre Deligne, who completed the most difficult step in 1973.\n\nWeil introduced the adele ring in the late 1930s, following Claude Chevalley's lead with the ideles, and gave a proof of the Riemann–Roch theorem with them (a version appeared in his \"Basic Number Theory\" in 1967). His 'matrix divisor' (vector bundle \"avant la lettre\") Riemann–Roch theorem from 1938 was a very early anticipation of later ideas such as moduli spaces of bundles. The Weil conjecture on Tamagawa numbers proved resistant for many years. Eventually the adelic approach became basic in automorphic representation theory. He picked up another credited \"Weil conjecture\", around 1967, which later under pressure from Serge Lang (resp. of Serre) became known as the Taniyama–Shimura conjecture (resp. Taniyama–Weil conjecture) based on a roughly formulated question of Taniyama at the 1955 Nikkō conference. His attitude towards conjectures was that one should not dignify a guess as a conjecture lightly, and in the Taniyama case, the evidence was only there after extensive computational work carried out from the late 1960s.\n\nOther significant results were on Pontryagin duality and differential geometry. He introduced the concept of a uniform space in general topology, as a by-product of his collaboration with Nicolas Bourbaki (of which he was a Founding Father). His work on sheaf theory hardly appears in his published papers, but correspondence with Henri Cartan in the late 1940s, and reprinted in his collected papers, proved most influential. He also created the ∅.\n\nHe discovered that the so-called Weil representation, previously introduced in quantum mechanics by Irving Segal and Shale, gave a contemporary framework for understanding the classical theory of quadratic forms. This was also a beginning of a substantial development by others, connecting representation theory and theta functions.\n\nHe also wrote several books on the history of Number Theory. Weil was elected Foreign Member of the Royal Society (ForMemRS) in 1966.\n\nWeil's ideas made an important contribution to the writings and seminars of Bourbaki, before and after World War II.\n\nHe says on page 114 of his autobiography that he was responsible for the null set symbol (Ø) and that it came from the Norwegian alphabet, which he alone among the Bourbaki group was familiar with.\n\nIndian (Hindu) thought had great influence on Weil. He was an agnostic, and he respected religions.\n\nMathematical works:\n\nCollected papers:\n\nAutobiography:\n\nMemoir by his daughter:\n\n\n"}
{"id": "21738491", "url": "https://en.wikipedia.org/wiki?curid=21738491", "title": "Artemas Martin", "text": "Artemas Martin\n\nArtemas Martin (August 3, 1835 – November 7, 1918) was a self-educated American mathematician.\n\nMartin was born on August 3, 1835 in Steuben County, New York,grew up in Venango County, Pennsylvania, and spent most of his life in Erie County, Pennsylvania. He was home-schooled until the age of 14, when he began studying mathematics at the local school, later moving to the Franklin Select School a few miles away and then to the Franklin Academy, finishing his formal education at age approximately 20. He worked as a farmer, oil driller, and schoolteacher. In 1881, he declined an invitation to become a professor of mathematics at the Normal School in Missouri. In 1885, he became the librarian for the Survey Office of the United States Coast Guard, and in 1898 he became a computer in the Division of Tides. He died on November 7, 1918.\n\nMartin was a prolific contributor of problems and solutions to mathematical puzzle columns in popular magazines beginning at the age of 18 in the \"Pittsburgh Almanac\" and the \"Philadelphia Saturday Evening Post\". From 1870 to 1875, he was editor of the \"Stairway Department\" of \"Clark's School Visitor\", one of the magazines to which he had previously contributed. From 1875 to 1876 Martin moved to the \"Normal Monthly\", where he published 16 articles on diophantine analysis. He subsequently became editor of the \"Mathematical Visitor\" in 1877 and of the \"Mathematical Magazine\" in 1882. In 1893 in Chicago, his paper \"On fifth-power numbers whose sum is a fifth power\" was read (but not by him) at the International Mathematical Congress held in connection with the World's Columbian Exposition. He was an Invited Speaker of the ICM in 1912 in Cambridge UK.\n\nMartin maintained an extensive mathematical library, now in the collections of American University.\n\nIn 1877 Martin was given an honorary M.A. from Yale University. In 1882 he was awarded another honorary degree, a Ph.D. from Rutgers University, and his third honorary degree, an LL.D., was given to him in 1885 by Hillsdale College. He was elected to the London Mathematical Society in 1878, the Société Mathématique de France in 1884, the Edinburgh Mathematical Society in 1885, the Philosophical Society of Washington in 1886, the American Association for the Advancement of Science in 1890, and the New York Mathematical Society in 1891. He was also a member of the American Mathematical Society, the Circolo Matematico di Palermo, the Mathematical Association of England, and the Deutsche Mathematiker-Vereinigung.\n"}
{"id": "1729755", "url": "https://en.wikipedia.org/wiki?curid=1729755", "title": "Bi-directional delay line", "text": "Bi-directional delay line\n\nIn mathematics, a bi-directional delay line is a numerical analysis technique used in computer simulation for solving ordinary differential equations by converting them to hyperbolic equations. In this way an explicit solution scheme is obtained with highly robust numerical properties. It was introduced by Auslander in 1968.\n\nIt originates from simulation of hydraulic pipelines where wave propagation was studied. It was then found that it could be used as an efficient numerical technique for numerically insulating different parts of a simulation model in each times step. It is used in the HOPSAN simulation package (Krus \"et al.\" 1990).\n\nIt is also known as the \"Transmission Line Modelling\" (TLM) from an independent development by Johns and O'Brian 1980. This is also extended to partial differential equations.\n\n"}
{"id": "16697636", "url": "https://en.wikipedia.org/wiki?curid=16697636", "title": "Binomial differential equation", "text": "Binomial differential equation\n\nThe binomial differential equation is the ordinary differential equation\n\nLet formula_4 be a polynomial in two variables of order formula_5; where formula_5 is a positive integer. The binomial differential equation becomes\nformula_7 using the substitution formula_8, we get that formula_9, therefore\nformula_10 or we can write formula_11, which is a separable ordinary differential equation, hence \n\nformula_12\n\nSpecial cases:\n\n- If formula_13, we have the differential equation formula_14 and the solution is formula_15, where formula_16 is a constant. \n- If formula_17, i.e., formula_2 divides formula_5 so that there is a positive integer formula_20 such that formula_21, then the solution has the form formula_22. From the tables book of Gradshteyn and Ryzhik we found that \n\n<math>"}
{"id": "12942017", "url": "https://en.wikipedia.org/wiki?curid=12942017", "title": "CAT(k) group", "text": "CAT(k) group\n\nIn mathematics, a CAT(\"k\") group is a group that acts discretely, cocompactly and isometrically on a CAT(\"k\") space.\n"}
{"id": "932711", "url": "https://en.wikipedia.org/wiki?curid=932711", "title": "Carmichael's theorem", "text": "Carmichael's theorem\n\nIn number theory, Carmichael's theorem, named after the American mathematician R.D. Carmichael,\nstates that, for any nondegenerate Lucas sequence of the first kind \"U\"(\"P\",\"Q\") with relatively prime parameters \"P, Q\" and positive discriminant, an element \"U\" with \"n\" ≠ 1, 2, 6 has at least one prime divisor that does not divide any earlier one except the 12th Fibonacci number F(12)=\"U\"(1, -1)=144 and its equivalent \"U\"(-1, -1)=-144.\n\nIn particular, for \"n\" greater than 12, the \"n\"th Fibonacci number F(\"n\") has at least one prime divisor that does not divide any earlier Fibonacci number.\n\nCarmichael (1913, Theorem 21) proved this theorem. Recently, Yabuta (2001) gave a simple proof.\n\nGiven two coprime integers \"P\" and \"Q\", such that formula_1 and , let be the Lucas sequence of the first kind defined by\n\nThen, for \"n\" ≠ 1, 2, 6, \"U\"(\"P\",\"Q\") has at least one prime divisor that does not divide any \"U\"(\"P\",\"Q\") with \"m\" < \"n\", except \"U\"(1, -1)=F(12)=144, \"U\"(-1, -1)=-F(12)=-144.\nSuch a prime \"p\" is called a \"characteristic factor\" or a \"primitive prime divisor\" of \"U\"(\"P\",\"Q\").\nIndeed, Carmichael showed a slightly stronger theorem: For \"n\" ≠ 1, 2, 6, \"U\"(\"P\",\"Q\") has at least one primitive prime divisor not dividing \"D\" except \"U\"(1, -2)=\"U\"(-1, -2)=3, \"U\"(1, -1)=\"U\"(-1, -1)=F(5)=5, \"U\"(1, -1)=F(12)=144, \"U\"(-1, -1)=-F(12)=-144.\n\nNote that \"D\" should be > 0, thus the cases \"U\"(1, 2), \"U\"(1, 2) and \"U\"(1, 2), etc. are not included, since in this case \"D\" = −7 < 0.\n\nThe only exceptions in Fibonacci case for \"n\" up to 12 are:\n\nThe smallest primitive prime divisor of F(\"n\") are\nCarmichael's theorem says that every Fibonacci number, apart from the exceptions listed above, has at least one primitive prime divisor.\n\nIf \"n\" > 1, then the \"n\"th Pell number has at least one prime divisor that does not divide any earlier Pell number. The smallest primitive prime divisor of \"n\"th Pell number are\n\n"}
{"id": "22870422", "url": "https://en.wikipedia.org/wiki?curid=22870422", "title": "Charles Hermite", "text": "Charles Hermite\n\nProf Charles Hermite () FRS FRSE MIAS (24 December 1822 – 14 January 1901) was a French mathematician who did research concerning number theory, quadratic forms, invariant theory, orthogonal polynomials, elliptic functions, and algebra.\n\nHermite polynomials, Hermite interpolation, Hermite normal form, Hermitian operators, and cubic Hermite splines are named in his honor. One of his students was Henri Poincaré.\n\nHe was the first to prove that \"e\", the base of natural logarithms, is a transcendental number. His methods were used later by Ferdinand von Lindemann to prove that π is transcendental.\n\nIn a letter to Thomas Joannes Stieltjes during 1893, Hermite remarked: \"I turn with terror and horror from this lamentable scourge of continuous functions with no derivatives.\"\n\nA crater near the north pole of the moon has been named in his honor.\n\nHermite was born in Dieuze, The Moselle on 24 December 1822, with a deformity in his right foot which would affect his gait for the rest of his life. He was the sixth of seven children of Ferdinand Hermite, and his wife Madeleine Lallemand. His father worked in his mother's family's drapery business, and also pursued a career as an artist. The drapery business relocated to Nancy during 1828 and so did the family.\nHe studied at the Collège de Nancy and then, in Paris, at the Collège Henri IV and at the Lycée Louis-le-Grand.\n\nAs a boy he read some of the writings of Joseph Louis Lagrange on the solution of numerical equations, and of Carl Friedrich Gauss on the theory of numbers. \n\nHermite wanted to study at the École Polytechnique and during 1841 he took a year preparing for the examinations and was tutored by Eugène Charles Catalan. During 1842 he was admitted to the school. However, after one year Hermite was refused the right to continue his studies because of his disability (École Polytechnique is to this day a military academy). He had to struggle to regain his admission which he won but with strict conditions imposed. Hermite found this unacceptable and decided to quit the École Polytechnique without graduating.\n\nDuring 1842, his first original contribution to mathematics, in which he gave a simple proof of the proposition of Niels Abel concerning the impossibility of obtaining an algebraic solution for the equation of the fifth degree, was published in the \"Nouvelles Annales de Mathématiques\".\n\nA correspondence with Carl Jacobi, begun during 1843 and continued the next year, resulted in the insertion, in the complete edition of Jacobi's works, of two articles by Hermite, one concerning the extension to Abelian functions of one of the theorems of Abel on elliptic functions, and the other concerning the transformation of elliptic functions.\n\nAfter spending five years working privately towards his degree, in which he befriended eminent mathematicians Joseph Bertrand, Carl Gustav Jacob Jacobi, and Joseph Liouville, he took and passed the examinations for the baccalauréat, which he was awarded during 1847. He married Joseph Bertrand's sister, Louise Bertrand, during 1848.\n\nDuring 1848, Hermite returned to the École Polytechnique as \"répétiteur and examinateur d'admission\". During 1856 he contracted smallpox. Through the influence of Augustin-Louis Cauchy and of a nun who nursed him, he resumed the practice of his Catholic faith. On 14 July, of that year, he was elected for the vacancy created by the death of Jacques Binet in the Académie des Sciences. During 1869, he succeeded Jean-Marie Duhamel as professor of mathematics, both at the École Polytechnique, where he remained until 1876, and in the Faculty of Sciences of Paris, which was a post he occupied until his death. From 1862 to 1873 he was lecturer at the École Normale Supérieure. Upon his seventieth birthday, on the occasion of his jubilee which was celebrated at the Sorbonne under the auspices of an international committee, he was promoted grand officer of the Légion d'honneur. He died in Paris, 14 January 1901, aged 78.\n\nAn inspiring teacher, Hermite strove to cultivate admiration for simple beauty and discourage rigorous minutiae. His correspondence with Thomas Stieltjes testifies to the great aid he gave those beginning scientific life. His published courses of lectures have exercised a great influence. His important original contributions to pure mathematics, published in the major mathematical journals of the world, dealt chiefly with Abelian and elliptic functions and the theory of numbers. During 1858 he solved the equation of the fifth degree by elliptic functions; and during 1873 he proved \"e\", the base of the natural system of logarithms, to be transcendental. This last was used by Ferdinand von Lindemann to prove during 1882 the same for π.\n\nThe following is a list of his works.:\n\n\n\n\"This article incorporates text from the public-domain Catholic Encyclopedia of 1913.\"\n"}
{"id": "54782330", "url": "https://en.wikipedia.org/wiki?curid=54782330", "title": "Continuous-variable quantum information", "text": "Continuous-variable quantum information\n\nContinuous-variable quantum information is the area of quantum information science that makes use of physical observables, like the strength of an electromagnetic field, whose numerical values belong to continuous intervals. One primary application is quantum computing. In a sense, continuous-variable quantum computation is \"analog\", while quantum computation using qubits is \"digital.\" In more technical terms, the former makes use of Hilbert spaces that are infinite-dimensional, while the Hilbert spaces for systems comprising collections of qubits are finite-dimensional. One motivation for studying continuous-variable quantum computation is to understand what resources are necessary to make quantum computers more powerful than classical ones.\n\nOne approach to implementing continuous-variable quantum information protocols in the laboratory is through the techniques of quantum optics. By modeling each mode of the electromagnetic field as a quantum harmonic oscillator with its associated creation and annihilation operators, one defines a canonically conjugate pair of variables for each mode, the so-called \"quadratures\", which play the role of position and momentum observables. These observables establish a phase space on which Wigner quasiprobability distributions can be defined. Quantum measurements on such a system can be performed using homodyne and heterodyne detectors.\n\nQuantum teleportation of continuous-variable quantum information was achieved by optical methods in 1998. (\"Science\" deemed this experiment one of the \"top 10\" advances of the year.) In 2013, quantum-optics techniques were used to create a \"cluster state\", a type of preparation essential to one-way (measurement-based) quantum computation, involving over 10,000 entangled temporal modes, available two at a time. In another implementation, 60 modes were simultaneously entangled in the frequency domain, in the optical frequency comb of an optical parametric oscillator.\n\nAnother proposal is to modify the ion-trap quantum computer: instead of storing a single qubit in the internal energy levels of an ion, one could in principle use the position and momentum of the ion as continuous quantum variables.\n\nContinuous-variable quantum systems can be used for quantum cryptography, and in particular, quantum key distribution. Quantum computing is another potential application, and a variety of approaches have been considered. The first method, proposed by Seth Lloyd and Samuel L. Braunstein in 1999, was in the tradition of the circuit model: quantum logic gates are created by Hamiltonians that, in this case, are quadratic functions of the harmonic-oscillator quadratures. Later, measurement-based quantum computation was adapted to the setting of infinite-dimensional Hilbert spaces. Yet a third model of continuous-variable quantum computation encodes finite-dimensional systems (collections of qubits) into infinite-dimensional ones. This model is due to Daniel Gottesman, Alexei Kitaev and John Preskill.\n\nIn all approaches to quantum computing, it is important to know whether a task under consideration can be carried out efficiently by a classical computer. An algorithm might be described in the language of quantum mechanics, but upon closer analysis, revealed to be implementable using only classical resources. Such an algorithm would not be taking full advantage of the extra possibilities made available by quantum physics. In the theory of quantum computation using finite-dimensional Hilbert spaces, the Gottesman–Knill theorem demonstrates that there exists a set of quantum processes that can be emulated efficiently on a classical computer. Generalizing this theorem to the continuous-variable case, it can be shown that, likewise, a class of continuous-variable quantum computations can be simulated using only classical analog computations. This class includes, in fact, some computational tasks that use quantum entanglement. When the Wigner quasiprobability representations of all the quantities—states, time evolutions \"and\" measurements—involved in a computation are nonnegative, then they can be interpreted as ordinary probability distributions, indicating that the computation can be modeled as an essentially classical one. This type of construction can be thought of as a continuum generalization of the Spekkens Toy Model.\n\nOccasionally, and somewhat confusingly, the term \"continuous quantum computation\" is used to refer to a different area of quantum computing: the study of how to use quantum systems having \"finite\"-dimensional Hilbert spaces to calculate or approximate the answers to mathematical questions involving continuous functions. A major motivation for investigating the quantum computation of continuous functions is that many scientific problems have mathematical formulations in terms of continuous quantities. A second motivation is to explore and understand the ways in which quantum computers can be more capable or powerful than classical ones. The computational complexity of a problem can be quantified in terms of the minimal computational resources necessary to solve it. In quantum computing, resources include the number of qubits available to a computer and the number of queries that can be made to that computer. The classical complexity of many continuous problems is known. Therefore, when the quantum complexity of these problems is obtained, the question as to whether quantum computers are more powerful than classical can be answered. Furthermore, the degree of the improvement can be quantified. In contrast, the complexity of discrete problems is typically unknown. For example, the classical complexity of integer factorization is unknown.\n\nOne example of a scientific problem that is naturally expressed in continuous terms is path integration. The general technique of path integration has numerous applications including quantum mechanics, quantum chemistry, statistical mechanics, and computational finance. Because randomness is present throughout quantum theory, one typically requires that a quantum computational procedure yield the correct answer, not with certainty, but with high probability. For example, one might aim for a procedure that computes the correct answer with probability at least 3/4. One also specifies a degree of uncertainty, typically by setting the maximum acceptable error. Thus, the goal of a quantum computation could be to compute the numerical result of a path-integration problem to within an error of at most ε with probability 3/4 or more. In this context, it is known that quantum algorithms can outperform their classical counterparts, and the computational complexity of path integration, as measured by the number of times one would expect to have to query a quantum computer to get a good answer, grows as the inverse of ε.\n\nOther continuous problems for which quantum algorithms have been studied include finding matrix eigenvalues, phase estimation, the Sturm–Liouville eigenvalue problem, solving differential equations with the Feynman–Kac formula, initial value problems, function approximation and high-dimensional integration.\n"}
{"id": "6556", "url": "https://en.wikipedia.org/wiki?curid=6556", "title": "Coprime integers", "text": "Coprime integers\n\nIn number theory, two integers and are said to be relatively prime, mutually prime, or coprime (also written co-prime) if the only positive integer (factor) that divides both of them is 1. Consequently, any prime number that divides one does not divide the other. This is equivalent to their greatest common divisor (gcd) being 1. \n\nThe numerator and denominator of a reduced fraction are coprime. As specific examples, 14 and 15 are coprime, being commonly divisible only by 1, while 14 and 21 are not coprime, because they are both divisible by 7. \n\nStandard notations for relatively prime integers and are: and . Graham, Knuth and Patashnik have proposed that the notation formula_1 be used to indicate that and are relatively prime and that the term \"prime\" be used instead of coprime (as in is \"prime\" to ).\n\nA fast way to determine whether two numbers are coprime is given by the Euclidean algorithm.\n\nThe number of integers coprime to a positive integer , between 1 and , is given by Euler's totient function (or Euler's phi function) .\n\nA set of integers can also be called coprime if its elements share no common positive factor except 1. A stronger condition on a set of integers is pairwise coprime, which means that and are coprime for every pair of different integers in the set. The set } is coprime, but it is not pairwise coprime since 2 and 4 are not relatively prime.\n\nThe numbers 1 and −1 are the only integers coprime to every integer, and they are the only integers that are coprime with 0.\n\nA number of conditions are equivalent to and being coprime:\n\nAs a consequence of the third point, if \"a\" and \"b\" are coprime and \"br\" ≡ \"bs\" (mod \"a\"), then \"r\" ≡ \"s\" (mod \"a\"). That is, we may \"divide by \"b\"\" when working modulo \"a\". Furthermore, if \"b\" and \"b\" are both coprime with \"a\", then so is their product \"b\"\"b\" (i.e., modulo \"a\" it is a product of invertible elements, and therefore invertible); this also follows from the first point by Euclid's lemma, which states that if a prime number \"p\" divides a product \"bc\", then \"p\" divides at least one of the factors \"b\", \"c\".\n\nAs a consequence of the first point, if \"a\" and \"b\" are coprime, then so are any powers \"a\" and \"b\".\n\nIf \"a\" and \"b\" are coprime and \"a\" divides the product \"bc\", then \"a\" divides \"c\". This can be viewed as a generalization of Euclid's lemma.\n\nThe two integers \"a\" and \"b\" are coprime if and only if the point with coordinates (\"a\", \"b\") in a Cartesian coordinate system is \"visible\" from the origin (0,0), in the sense that there is no point with integer coordinates on the line segment between the origin and (\"a\", \"b\"). (See figure 1.)\n\nIn a sense that can be made precise, the probability that two randomly chosen integers are coprime is 6/π (see pi), which is about 61%. See below.\n\nTwo natural numbers \"a\" and \"b\" are coprime if and only if the numbers 2 − 1 and 2 − 1 are coprime. As a generalization of this, following easily from the Euclidean algorithm in base \"n\" > 1:\n\nA set of integers \"S\" = {\"a\", \"a\", ... \"a\"} can also be called \"coprime\" or \"setwise coprime\" if the greatest common divisor of all the elements of the set is 1. For example, the integers 6, 10, 15 are coprime because 1 is the only positive integer that divides all of them.\n\nIf every pair in a set of integers is coprime, then the set is said to be \"pairwise coprime\" (or \"pairwise relatively prime\", \"mutually coprime\" or \"mutually relatively prime\"). Pairwise coprimality is a stronger condition than setwise coprimality; every pairwise coprime finite set is also setwise coprime, but the reverse is not true. For example, the integers 4, 5, 6 are (setwise) coprime (because the only positive integer dividing \"all\" of them is 1), but they are not \"pairwise\" coprime (because gcd(4, 6) = 2).\n\nThe concept of pairwise coprimality is important as a hypothesis in many results in number theory, such as the Chinese remainder theorem.\n\nIt is possible for an infinite set of integers to be pairwise coprime. Notable examples include the set of all prime numbers, the set of elements in Sylvester's sequence, and the set of all Fermat numbers.\n\nTwo ideals \"A\" and \"B\" in the commutative ring \"R\" are called coprime (or comaximal) if \"A\" + \"B\" = \"R\". This generalizes Bézout's identity: with this definition, two principal ideals (\"a\") and (\"b\") in the ring of integers Z are coprime if and only if \"a\" and \"b\" are coprime. If the ideals \"A\" and \"B\" of \"R\" are coprime, then \"AB\" = \"A\"∩\"B\"; furthermore, if \"C\" is a third ideal such that \"A\" contains \"BC\", then \"A\" contains \"C\". The Chinese remainder theorem can be generalized to any commutative ring, using coprime ideals.\n\nGiven two randomly chosen integers \"a\" and \"b\", it is reasonable to ask how likely it is that \"a\" and \"b\" are coprime. In this determination, it is convenient to use the characterization that \"a\" and \"b\" are coprime if and only if no prime number divides both of them (see Fundamental theorem of arithmetic).\n\nInformally, the probability that any number is divisible by a prime (or in fact any integer) formula_3 is formula_4; for example, every 7th integer is divisible by 7. Hence the probability that two numbers are both divisible by \"p\" is formula_5, and the probability that at least one of them is not is formula_6. Any finite collection of divisibility events associated to distinct primes is mutually independent. For example, in the case of two events, a number is divisible by primes \"p\" and \"q\" if and only if it is divisible by \"pq\"; the latter event has probability 1/\"pq\". If one makes the heuristic assumption that such reasoning can be extended to infinitely many divisibility events, one is led to guess that the probability that two numbers are coprime is given by a product over all primes,\n\nHere \"ζ\" refers to the Riemann zeta function, the identity relating the product over primes to \"ζ\"(2) is an example of an Euler product, and the evaluation of \"ζ\"(2) as \"π\"/6 is the Basel problem, solved by Leonhard Euler in 1735.\n\nThere is no way to choose a positive integer at random so that each positive integer occurs with equal probability, but statements about \"randomly chosen integers\" such as the ones above can be formalized by using the notion of \"natural density\". For each positive integer \"N\", let \"P\" be the probability that two randomly chosen numbers in formula_8 are coprime. Although \"P\" will never equal formula_9 exactly, with work one can show that in the limit as formula_10, the probability formula_11 approaches formula_9.\n\nMore generally, the probability of \"k\" randomly chosen integers being coprime is formula_13.\n\nAll pairs of positive coprime numbers formula_14 (with formula_15) can be arranged in two disjoint complete ternary trees, one tree starting from formula_16 (for even-odd and odd-even pairs), and the other tree starting from formula_17 (for odd-odd pairs). The children of each vertex formula_18 are generated as follows:\n\nThis scheme is exhaustive and non-redundant with no invalid members.\n\n\n"}
{"id": "28357910", "url": "https://en.wikipedia.org/wiki?curid=28357910", "title": "Dynamic fluid film equations", "text": "Dynamic fluid film equations\n\nFluid films, such as soap films, are commonly encountered in everyday experience. A soap film can be formed by dipping a closed contour wire into a soapy solution as in the figure on the right. Alternatively, a catenoid can be formed by dipping two rings in the soapy solution and subsequently separating them while maintaining the coaxial configuration.\n\nStationary fluid films form surfaces of minimal surface area, leading to the Plateau problem.\n\nOn the other hand, fluid films display rich dynamic properties. They can undergo enormous deformations away from the equilibrium configuration. Furthermore, they display several orders of magnitude variations in thickness from nanometers to millimeters. Thus, a fluid film can simultaneously display nanoscale and macroscale phenomena.\n\nIn the study of the dynamics of free fluid films, such as soap films, it is common to model the film as two dimensional manifolds. Then the variable thickness of the film is captured by the two dimensional density formula_1.\n\nThe dynamics of fluid films can be described by the following system of exact nonlinear Hamiltonian equations which, in that respect, are a complete analogue of Euler's inviscid equations of fluid dynamics. In fact, these equations reduce to Euler's dynamic equations for flows in stationary Euclidean spaces.\n\nThe foregoing relies on the formalism of tensors, including the summation convention and the raising and lowering of tensor indices.\n\nConsider a thin fluid film formula_2 that spans a stationary closed contour boundary. Let formula_3 be the normal component of the velocity field and formula_4 be the contravariant components of the tangential velocity projection. Let formula_5 be the covariant surface derivative, formula_6 be the covariant curvature tensor, formula_7 be the mixed curvature tensor and formula_8 be its trace, that is mean curvature. Furthermore, let the internal energy density per unit mass function be formula_9 so that the total potential energy formula_10 is given by\n\nThis choice of formula_9 :\n\nwhere formula_14 is the surface energy density results in Laplace's classical model for surface tension:\n\nwhere \"A\" is the total area of the soap film.\n\nThe governing system reads\n\nwhere the formula_17-derivative is the central operator, originally\ndue to Jacques Hadamard, in \"The Calculus of Moving Surfaces\". Note that, in compressible models, the combination formula_18 is commonly identified with pressure formula_19. The governing system above was originally formulated in reference 1.\n\nFor the Laplace choice of surface tension formula_20 the system becomes: \n\nNote that on flat (formula_22) stationary (formula_23) manifolds, the\nsystem becomes\n\nwhich is precisely classical Euler's equations of fluid dynamics.\n\nIf one disregards the tangential components of the velocity field, as frequently done in the study of thin fluid film, one arrives at the following simplified system with only two unknowns: the two dimensional density formula_25 and the normal velocity formula_3:\n\n1. Exact nonlinear equations for fluid films and proper adaptations of conservation theorems from classical hydrodynamics P. Grinfeld, J. Geom. Sym. Phys. 16, 2009\n"}
{"id": "5676427", "url": "https://en.wikipedia.org/wiki?curid=5676427", "title": "Estimation lemma", "text": "Estimation lemma\n\nIn mathematics the estimation lemma, also known as the inequality, gives an upper bound for a contour integral. If is a complex-valued, continuous function on the contour and if its absolute value is bounded by a constant for all on , then\n\nwhere is the arc length of . In particular, we may take the maximum\n\nas upper bound. Intuitively, the lemma is very simple to understand. If a contour is thought of as many smaller contour segments connected together, then there will be a maximum for each segment. Out of all the maximum s for the segments, there will be an overall largest one. Hence, if the overall largest is summed over the entire path then the integral of over the path must be less than or equal to it.\n\nFormally, the inequality can be shown to hold using the definition of contour integral, the absolute value inequality for integrals and the formula for the length of a curve as follows:\n\nThe estimation lemma is most commonly used as part of the methods of contour integration with the intent to show that the integral over part of a contour goes to zero as goes to infinity. An example of such a case is shown below.\n\nProblem.\nFind an upper bound for\n\nwhere is the upper half-circle with radius traversed once in the counterclockwise direction.\n\nSolution.\nFirst observe that the length of the path of integration is half the circumference of a circle with radius , hence\nNext we seek an upper bound for the integrand when . By the triangle inequality we see that\ntherefore\nbecause on . Hence\nTherefore, we apply the estimation lemma with . The resulting bound is\n\n\n"}
{"id": "2427912", "url": "https://en.wikipedia.org/wiki?curid=2427912", "title": "False nearest neighbor algorithm", "text": "False nearest neighbor algorithm\n\nThe false nearest neighbor algorithm is an algorithm for estimating the embedding dimension. The concept was proposed by Kennel et al. The main idea is to examine how the number of neighbors of a point along a signal trajectory change with increasing embedding dimension. In too low an embedding dimension, many of the neighbors will be false, but in an appropriate embedding dimension or higher, the neighbors are real. With increasing dimension, the false neighbors will no longer be neighbors. Therefore, by examining how the number of neighbors change as a function of dimension, an appropriate embedding can be determined.\n\n\n"}
{"id": "10138003", "url": "https://en.wikipedia.org/wiki?curid=10138003", "title": "Fourier integral operator", "text": "Fourier integral operator\n\nIn mathematical analysis, Fourier integral operators have become an important tool in the theory of partial differential equations. The class of Fourier integral operators contains differential operators as well as classical integral operators as special cases.\n\nA Fourier integral operator formula_1 is given by:\n\nwhere formula_3 denotes the Fourier transform of formula_4, formula_5 is a standard symbol which is compactly supported in formula_6 and formula_7 is real valued and homogeneous of degree formula_8 in formula_9. It is also necessary to require that formula_10 on the support of \"a.\" Under these conditions, if \"a\" is of order zero, it is possible to show that formula_1 defines a bounded operator from formula_12 to formula_12.\n\nOne motivation for the study of Fourier integral operators is the solution operator for the initial value problem for the wave operator. Indeed, consider the following problem:\n\nand\n\nThe solution to this problem is given by\n\nformula_16\n\nThese need to be interpreted as oscillatory integrals since they do not in general converge. This formally looks like a sum of two Fourier integral operators, however the coefficients in each of the integrals are not smooth at the origin, and so not standard symbols. If we cut out this singularity with a cutoff function, then the so obtained operators still provide solutions to the initial value problem modulo smooth functions. Thus, if we are only interested in the propagation of singularities of the initial data, it is sufficient to consider such operators. In fact, if we allow the sound speed c in the wave equation to vary with position we can still find a Fourier integral operator that provides a solution modulo smooth functions, and Fourier integral operators thus provide a useful tool for studying the propagation of singularities of solutions to variable speed wave equations, and more generally for other hyperbolic equations.\n\n\n"}
{"id": "25130943", "url": "https://en.wikipedia.org/wiki?curid=25130943", "title": "Free independence", "text": "Free independence\n\nIn the mathematical theory of free probability, the notion of free independence was introduced by Dan Voiculescu. The definition of free independence is parallel to the classical definition of independence, except that the role of Cartesian products of measure spaces (corresponding to tensor products of their function algebras) is played by the notion of a free product of (non-commutative) probability spaces.\n\nIn the context of Voiculescu's free probability theory, many classical-probability theorems or phenomena have free probability analogs: the same theorem or phenomenon holds (perhaps with slight modifications) if the classical notion of independence is replaced by free independence. Examples of this include: the free central limit theorem; notions of free convolution; existence of free stochastic calculus and so on.\n\nLet formula_1 be a non-commutative probability space, i.e. a unital algebra formula_2 over formula_3 equipped with a unital linear functional formula_4. As an example, one could take, for a probability measure formula_5,\n\nAnother example may be formula_7, the algebra of formula_8 matrices with the functional given by the normalized trace formula_9. Even more generally, formula_2 could be a von Neumann algebra and formula_11 a state on formula_2. A final example is the group algebra formula_13 of a (discrete) group formula_14 with the functional formula_11 given by the group trace formula_16.\n\nLet formula_17 be a family of unital subalgebras of formula_2.\n\nDefinition. The family formula_17 is called \"freely independent\" if \nformula_20\nwhenever formula_21, formula_22 and formula_23.\n\nIf formula_24, formula_25 is a family of elements of formula_2 (these can be thought of as random variables in formula_2), they are called\n\n\"freely independent\" if the algebras formula_28 generated by formula_29 and formula_30 are freely independent.\n\n\n"}
{"id": "23997144", "url": "https://en.wikipedia.org/wiki?curid=23997144", "title": "Freedman's paradox", "text": "Freedman's paradox\n\nIn statistical analysis, Freedman's paradox, named after David Freedman, is a problem in model selection whereby predictor variables with no relationship to the dependent variable can pass tests of significance – both individually via a t-test, and jointly via an F-test for the significance of the regression. Freedman demonstrated (through simulation and asymptotic calculation) that this is a common occurrence when the number of variables is similar to the number of data points.\n\nSpecifically, if the dependent variable and \"k\" regressors are independent normal variables, and there are \"n\" observations, then as \"k\" and \"n\" jointly go to infinity in the ratio \"k\"/\"n\"=\"ρ\", (1) the \"R\" goes to \"ρ\", (2) the F-statistic for the overall regression goes to 1.0, and (3) the number of spuriously significant regressors goes to \"αk\" where α is the chosen critical probability (probability of Type I error for a regressor). This third result is intuitive because it says that the number of Type I errors equals the probability of a Type I error on an individual parameter times the number of parameters for which significance is tested.\n\nMore recently, new information-theoretic estimators have been developed in an attempt to reduce this problem, in addition to the accompanying issue of model selection bias, whereby estimators of predictor variables that have a weak relationship with the response variable are biased.\n"}
{"id": "54348508", "url": "https://en.wikipedia.org/wiki?curid=54348508", "title": "Gene Grabeel", "text": "Gene Grabeel\n\nGene Grabeel (June 5, 1920 – January 30, 2015) was an American mathematician and cryptanalyst who founded the Venona project.\n\nGrabeel graduated from Mars Hill College and Farmville State Teachers College and initially worked as a high school home economics teacher.\n\nIn his book \"Code Warriors: NSA's Codebreakers and the Secret Intelligence War Against the Soviet Union\", Stephen Budiansky describes how she came into the opportunity to work as a U.S. government cryptanalyst:\nIn 1936, Grabeel began her 36-year career with the Signal Intelligence Service.\nOn February 1, 1943, she founded the Venona project, a counterintelligence program aimed at decrypting Soviet communications.\n\nShe and others spent months sifting through stored and incoming Soviet telegrams.\nGrabeel was born in Rose Hill, Virginia on June 5, 1920.\nShe attended Blackstone Baptist Church.\nShe was a member of the Daughters of the American Revolution and of the 17th Century Colonial Dames.\n\nAfter the 1995 declassification of the Venona project, Grabeel was recognized by the Central Intelligence Agency as an \"American Hero\".\nGrabeel passed away at age 94 on January 30, 2015 in Blackstone, Virginia.\n"}
{"id": "44750391", "url": "https://en.wikipedia.org/wiki?curid=44750391", "title": "Griffiths group", "text": "Griffiths group\n\nIn mathematics, more specifically in algebraic geometry, the Griffiths group of a projective complex manifold \"X\" measures the difference between homological equivalence and algebraic equivalence, which are two important equivalence relations of algebraic cycles.\n\nMore precisely, it is defined as\n\nwhere formula_2 denotes the group of algebraic cycles of some fixed codimension \"k\" and the subscripts indicate the groups that are homologically trivial, respectively algebraically equivalent to zero.\n\nThis group was introduced by Phillip Griffiths who showed that for a general quintic in formula_3 (projective 4-space), the group formula_4 is not a torsion group.\n"}
{"id": "49342572", "url": "https://en.wikipedia.org/wiki?curid=49342572", "title": "Group actions in computational anatomy", "text": "Group actions in computational anatomy\n\nGroup actions are central to Riemannian geometry and defining orbits (control theory). \nThe orbits of computational anatomy consist of anatomical shapes and medical images; the anatomical shapes are submanifolds of differential geometry consisting of points, curves, surfaces and subvolumes.\nThis generalized the ideas of the more familiar orbits of linear algebra which are linear vector spaces. Medical images are scalar and tensor images from medical imaging. The group actions are used to define models of human shape which accommodate variation. These orbits are deformable templates as originally formulated more abstractly in pattern theory.\n\nThe central model of human anatomy in computational anatomy is a Groups and group action, a classic formulation from differential geometry. The orbit is called the space of shapes and forms. The space of shapes are denoted formula_1, with the group formula_2 with law of composition formula_3; the action of the group on shapes is denoted formula_4, where the action of the group formula_5 is defined to satisfy \n\nThe orbit formula_7 of the template becomes the space of all shapes, formula_8.\n\nThe central group in CA defined on volumes in formula_9 are the diffeomorphism group formula_10 which are mappings with 3-components formula_11, law of composition of functions formula_12, with inverse formula_13.\n\nFor sub-manifolds formula_14, parametrized by a chart or immersion formula_15, the diffeomorphic action the flow of the position\n\nMost popular are scalar images, formula_17, with action on the right via the inverse.\n\nMany different imaging modalities are being used with various actions. For images such that formula_19 is a three-dimensional vector then\n\nCao et al.\nexamined actions for mapping MRI images measured via diffusion tensor imaging and represented via there principle eigenvector. \nFor tensor fields a positively oriented orthonormal basis \nformula_22\nof formula_23, termed frames, vector cross product denoted formula_24 then\nThe Fr\\'enet frame of three orthonormal vectors, formula_26 deforms as a tangent, formula_27 deforms like\na normal to the plane generated by formula_28, and formula_27. H is uniquely constrained by the\nbasis being positive and orthonormal.\n\nFor formula_30 non-negative symmetric matrices, an action would become formula_31.\n\nFor mapping MRI DTI images (tensors), then eigenvalues are preserved with the diffeomorphism rotating eigenvectors and preserves the eigenvalues. \nGiven eigenelements\nformula_32, then the action becomes\n\nOrientation distribution function (ODF) characterizes the angular profile of the diffusion probability density function of water molecules and can be reconstructed from High Angular Resolution Diffusion Imaging (HARDI). The ODF is a probability density function defined on a unit sphere, formula_35. In the field of information geometry, the space of ODF forms a Riemannian manifold with the Fisher-Rao metric. For the purpose of LDDMM ODF mapping, the square-root representation is chosen because it is one of the most efficient representations found to date as the various Riemannian operations, such as geodesics, exponential maps, and logarithm maps, are available in closed form. In the following, denote square-root ODF (formula_36) as formula_37, where formula_37 is non-negative to ensure uniqueness and formula_39.\n\nDenote diffeomorphic transformation as formula_40. Group action of diffeomorphism on formula_37, formula_42, needs to guarantee the non-negativity and formula_43. Based on the derivation in, this group action is defined as\n\nwhere formula_45 is the Jacobian of formula_46.\n"}
{"id": "316081", "url": "https://en.wikipedia.org/wiki?curid=316081", "title": "Icosian calculus", "text": "Icosian calculus\n\nThe icosian calculus is a non-commutative algebraic structure discovered by the Irish mathematician William Rowan Hamilton in 1856.\nIn modern terms, he gave a group presentation of the icosahedral rotation group by generators and relations.\n\nHamilton’s discovery derived from his attempts to find an algebra of \"triplets\" or 3-tuples that he believed would reflect the three Cartesian axes. The symbols of the icosian calculus can be equated to moves between vertices on a dodecahedron. Hamilton’s work in this area resulted indirectly in the terms Hamiltonian circuit and Hamiltonian path in graph theory. He also invented the icosian game as a means of illustrating and popularising his discovery.\n\nThe algebra is based on three symbols that are each roots of unity, in that repeated application of any of them yields the value 1 after a particular number of steps. They are:\n\nHamilton also gives one other relation between the symbols:\n\nThe operation is associative but not commutative. They generate a group of order 60, isomorphic to the group of rotations of a regular icosahedron or dodecahedron, and therefore to the alternating group of degree five.\n\nAlthough the algebra exists as a purely abstract construction, it can be most easily visualised in terms of operations on the edges and vertices of a dodecahedron. Hamilton himself used a flattened dodecahedron as the basis for his instructional game.\n\nImagine an insect crawling along a particular edge of Hamilton's labelled dodecahedron in a certain direction, say from formula_3 to formula_4. We can represent this directed edge by formula_5.\n\n\nThe icosian calculus is one of the earliest examples of many mathematical ideas, including:\n"}
{"id": "19347060", "url": "https://en.wikipedia.org/wiki?curid=19347060", "title": "John W. Dawson Jr.", "text": "John W. Dawson Jr.\n\nJohn W. Dawson Jr. (born February 4, 1944) is Professor of Mathematics, Emeritus at Pennsylvania State University at York. Born in Wichita, Kansas, he attended M.I.T. as a National Merit Scholar before earning a doctorate in mathematical logic from the University of Michigan in 1972. An internationally recognized authority on the life and work of Kurt Gödel, Professor Dawson is the author of numerous articles on axiomatic set theory and the history of modern logic. During the years 1982 to 1984, he catalogued Gödel's papers at the Institute for Advanced Study in Princeton, and afterward he served as a co-editor of Gödel's Collected Works. He recently retired as co-Editor-in-Chief of the journal \"History and Philosophy of Logic\".\n\n\n\n"}
{"id": "10349343", "url": "https://en.wikipedia.org/wiki?curid=10349343", "title": "Kantorovich theorem", "text": "Kantorovich theorem\n\nThe Kantorovich theorem (Newton-Kantorovich theorem) is a mathematical statement on the convergence of Newton's method. It was first stated by Leonid Kantorovich in 1940.\n\nNewton's method constructs a sequence of points that under certain conditions will converge to a solution formula_1 of an equation formula_2 or a vector solution of a system of equation formula_3. The Kantorovich theorem gives conditions on the initial point of this sequence. If those conditions are satisfied then a solution exists close to the initial point and the sequence converges to that point.\n\nLet formula_4 be an open subset and formula_5 a differentiable function with a Jacobian formula_6 that is locally Lipschitz continuous (for instance if formula_7 is twice differentiable). That is, it is assumed that for any open subset formula_8 there exists a constant formula_9 such that for any formula_10\n\nholds. The norm on the left is some operator norm that is compatible with the vector norm on the right. This inequality can be rewritten to only use the vector norm. Then for any vector formula_12 the inequality\n\nmust hold.\n\nNow choose any initial point formula_14. Assume that formula_15 is invertible and construct the Newton step formula_16\n\nThe next assumption is that not only the next point formula_17 but the entire ball formula_18 is contained inside the set formula_19. Let formula_20 be the Lipschitz constant for the Jacobian over this ball.\n\nAs a last preparation, construct recursively, as long as it is possible, the sequences formula_21, formula_22, formula_23 according to\n\nNow if formula_25 then \n\nA statement that is more precise but slightly more difficult to prove uses the roots formula_31 of the quadratic polynomial\nand their ratio \nThen\n\n\n"}
{"id": "34757441", "url": "https://en.wikipedia.org/wiki?curid=34757441", "title": "L-stability", "text": "L-stability\n\nL-stability is a special case of A-stability, a property of Runge–Kutta methods for solving ordinary differential equations.\nA method is L-stable if it is A-stable and formula_1 as formula_2, where formula_3 is the stability function of the method (the stability function of a Runge–Kutta method is a rational function and thus the limit as formula_4 is the same as the limit as formula_5). L-stable methods are in general very good at integrating stiff equations.\n"}
{"id": "49418115", "url": "https://en.wikipedia.org/wiki?curid=49418115", "title": "Large deformation diffeomorphic metric mapping", "text": "Large deformation diffeomorphic metric mapping\n\nLarge deformation diffeomorphic metric mapping (LDDMM) is a specific suite of algorithms used for diffeomorphic mapping and manipulating dense imagery based on diffeomorphic metric mapping within the academic discipline of Computational anatomy, to be distinguished from its precursor based on diffeomorphic mapping. The distinction between the two is that diffeomorphic metric maps satisfy the property that the length associated to their flow away from the identity induces a metric on the group of diffeomorphisms, which in turn induces a metric on the orbit of Shapes and Forms within the field of Computational Anatomy. The study of shapes and forms with the metric of diffeomorphic metric mapping is called .\n\nA diffeomorphic mapping system is a system designed to map, manipulate, and transfer information which is stored in many types of spatially distributed medical imagery.\n\nDiffeomorphic mapping is the underlying technology for mapping and analyzing information measured in human anatomical coordinate systems which have been measured via Medical imaging. Diffeomorphic mapping is a broad term that actually refers to a number of different algorithms, processes, and methods. It is attached to many operations and has many applications for analysis and visualization. Diffeomorphic mapping can be used to relate various sources of information which are indexed as a function of spatial position as the key index variable. Diffeomorphisms are by their Latin root structure preserving transformations, which are in turn differentiable and therefore smooth, allowing for the calculation of metric based quantities such as arc length and surface areas. Spatial location and extents in human anatomical coordinate systems can be recorded via a variety of Medical imaging modalities, generally termed multi-modal medical imagery, providing either scalar and or vector quantities at each spatial location. Examples are scalar T1 or T2 Magnetic resonance imagery, or as 3x3 diffusion tensor matrices Diffusion MRI and Diffusion-weighted imaging, to scalar densities associated to Computed Tomography (CT), or functional imagery such as temporal data of functional magnetic resonance imaging and scalar densities such as Positron emission tomography (PET).\n\nComputational anatomy is a subdiscipline within the broader field of Neuroinformatics within Bioinformatics and Medical imaging. The first algorithm for dense image mapping via diffeomorphic metric mapping was Beg's LDDMM for volumes and Joshi's landmark matching for point sets with correspondence, with LDDMM algorithms now available for computing diffeomorphic metric maps between non-corresponding landmarks and landmark matching intrinsic to spherical manifolds, curves, currents and surfaces, tensors, varifolds, and time-series. The term LDDMM was first established as part of the National Institutes of Health supported Biomedical Informatics Research Network.\n\nIn a more general sense, diffeomorphic mapping is any solution that registers or builds correspondences between dense coordinate systems in Medical imaging by ensuring the solutions are diffeomorphic. There are now many codes organized around diffeomorphic registration including ANTS, DARTEL, DEMONS, StationaryLDDMM, FastLDDMM, as examples of actively used computational codes for constructing correspondences between coordinate systems based on dense images.\n\nThe distinction between diffeomorphic metric mapping forming the basis for LDDMM and the earliest methods of diffeomorphic mapping is the introduction of a Hamilton principle of least-action in which large deformations are selected of shortest length corresponding to geodesic flows. This important distinction arises from the original formulation of the Riemannian metric corresponding to the right-invariance. The lengths of these geodesics give the metric in the metric space structure of human anatomy. Non-geodeisc formulations of diffeomorphic mapping in general does not correspond to any metric formulation.\n\nDiffeomorphic mapping 3-dimensional information across coordinate systems is central to high-resolution Medical imaging and the area of Neuroinformatics within the newly emerging field of bioinformatics. Diffeomorphic mapping 3-dimensional coordinate systems as measured via high resolution dense imagery has a long history in 3-D beginning with Computed Axial Tomography (CAT scanning) in the early 80's by the University of Pennsylvania group led by Ruzena Bajcsy, and subsequently the Ulf Grenander school at Brown University with the HAND experiments. In the 90's there were several solutions for image registration which were associated to linearizations of small deformation and non-linear elasticity.\n\nThe central focus of the sub-field of Computational anatomy (CA) within medical imaging is mapping information across anatomical coordinate systems at the 1 millimeter morphome scale. In CA mapping of dense information measured within Magnetic resonance image (MRI) based coordinate systems such as in the brain has been solved via inexact matching of 3D MR images one onto the other. The earliest introduction of the use of diffeomorphic mapping via large deformation flows of diffeomorphisms for transformation of coordinate systems in image analysis and medical imaging was by Christensen, Rabbitt and Miller and Trouve. The introduction of flows, which are akin to the equations of motion used in fluid dynamics, exploit the notion that dense coordinates in image analysis follow the Lagrangian and Eulerian equations of motion. This model becomes more appropriate for cross-sectional studies in which brains and or hearts are not necessarily deformations of one to the other. Methods based on linear or non-linear elasticity energetics which grows with distance from the identity mapping of the template, is not appropriate for cross-sectional study. Rather, in models based on Lagrangian and Eulerian flows of diffeomorphisms, the constraint is associated to topological properties, such as open sets being preserved, coordinates not crossing implying uniqueness and existence of the inverse mapping, and connected sets remaining connected. The use of diffeomorphic methods grew quickly to dominate the field of mapping methods post Christensen's original paper, with fast and symmetric methods becoming available.\n\nSuch methods are powerful in that they introduce notions of regularity of the solutions so that they can be differentiated and local inverses can be calculated. The disadvantages of these methods is that there was no associated global least-action property which could score the flows of minimum energy. This contrasts the geodesic motions which are central to the study of Rigid body kinematics and the many problems solved in Physics via Hamilton's principle of least action. In 1998, Dupuis, Grenander and Miller established the conditions for guaranteeing the existence of solutions for dense image matching in the space of flows of diffeomorphisms. These conditions require an action penalizing kinetic energy measured via the Sobolev norm on spatial derivatives of the flow of vector fields.\n\nThe Large Deformation Diffeomorphic Metric Mapping (LDDMM) code that Faisal Beg derived and implemented for his PhD at Johns Hopkins University developed the earliest algorithmic code which solved for flows with fixed points satisfying the necessary conditions for the dense image matching problem subject to least-action. Computational anatomy now has many existing codes organized around diffeomorphic registration including ANTS, DARTEL, DEMONS, LDDMM, StationaryLDDMM as examples of actively used computational codes for constructing correspondences between coordinate systems based on dense images.\n\nThese large deformation methods have been extended to landmarks without registration via measure matching, curves, surfaces, dense vector and tensor imagery, and varifolds removing orientation.\n\nDeformable shape in Computational Anatomy (CA)is studied via the use of diffeomorphic mapping for establishing correspondences between anatomical coordinates in Medical Imaging. In this setting, three dimensional medical images are modelled as a random deformation of some exemplar, termed the template formula_1, with the set of observed images element in the random orbit model of CA for images formula_2. The template is mapped onto the target by defining a variational problem in which the template is transformed via the diffeomorphism used as a change of coordinate to minimize a squared-error matching condition between the transformed template and the target.\n\nThe diffeomorphisms are generated via smooth flows formula_3 , with formula_4, satisfying the Lagrangian and Eulerian specification of the flow field associated to the ordinary differential equation,\nwith \nformula_6 the Eulerian vector fields determining the flow.\nThe vector fields are guaranteed to be 1-time continuously differentiable formula_7 by modelling them to be in a \nsmooth Hilbert space formula_8 supporting 1-continuous derivative. The inverse formula_9 is defined by the Eulerian vector-field with flow given by\n\nTo ensure smooth flows of diffeomorphisms with inverse, the vector fields with components in formula_10 must be at least 1-time continuously differentiable in space which are modelled as elements of the Hilbert space formula_11 using the Sobolev embedding theorems so that each element formula_12 has 3-times square-integrable weak-derivatives. Thus formula_11 embeds smoothly in 1-time continuously differentiable functions. The diffeomorphism group are flows with vector fields absolutely integrable in Sobolev norm\n\nIn CA the space of vector fields formula_11 are modelled as a reproducing Kernel Hilbert space (RKHS) defined by a 1-1, differential operatorformula_15 determining the norm formula_16 \nwhere the integral is calculated by integration by parts when formula_17 is a generalized function in the dual space formula_18. \nThe differential operator is selected so that the Green's kernel, the inverse of the operator, is continuously differentiable in each variable implying that the vector fields support 1-continuous derivative; see for the necessary conditions on the norm for existence of solutions.\n\nThe original large deformation diffeomorphic metric mapping (LDDMM) algorithms of Beg, Miller, Trouve, Younes was derived taking variations with respect to the vector field parameterization of the group, since formula_19 are in a vector spaces. Beg solved the dense image matching minimizing the action integral of kinetic energy of diffeomorphic flow while\nminimizing endpoint matching term according to\nUpdate until convergence, formula_20 each iteration, with formula_21:\nThis implies that the fixed point at formula_22 satisfies \nwhich in turn implies it satisfies the Conservation equation given by the according to \n\nThe landmark matching problem has a pointwise correspondence defining the endpoint condition with geodesics given by the following minimum:\n\nJoshi originally defined the registered landmark matching probleme. Update until convergence, formula_20 each iteration, with formula_21:\nThis implies that the fixed point satisfy \nwith\n\nThe Calculus of variations was used in Beg to derive the iterative algorithm as a solution which when it converges satisfies the necessary maximizer conditions given by the necessary conditions for a first order variation requiring the variation of the endpoint with respect to a first order variation of the vector field. The directional derivative calculates the Gâteaux derivative as calculated in Beg's original paper and.\n\n</math>\n\nThe LDDMM variational problem is defined as\n\nBeg solved the early LDDMM algorithms by solving the variational matching taking variations with respect to the vector fields. Another solution by Vialard, reparameterizes the optimization problem in terms of the state formula_31, for image formula_32, with the dynamics equation controlling the state by the control given in terms of the advection equation according to formula_33. The endpoint matching term\nformula_34 gives the variational problem:\n"}
{"id": "39490837", "url": "https://en.wikipedia.org/wiki?curid=39490837", "title": "List of things named after André Weil", "text": "List of things named after André Weil\n\nThese are things named after André Weil (1906 – 1998), a French mathematician.\n"}
{"id": "9636455", "url": "https://en.wikipedia.org/wiki?curid=9636455", "title": "Logarithmic number system", "text": "Logarithmic number system\n\nA logarithmic number system (LNS) is an arithmetic system used for representing real numbers in computer and digital hardware, especially for digital signal processing.\n\nIn an LNS, a number, formula_1, is represented by the logarithm, formula_2, of its absolute value as follows:\n\nwhere formula_4 is a bit denoting the sign of formula_1 (formula_6 if formula_7 and formula_8 if formula_9).\n\nThe number formula_2 is represented by a binary word which usually is in the two's complement format. An LNS can be considered as a floating-point number with the significand being always equal to 1 and a non-integer exponent. This formulation simplifies the operations of multiplication, division, powers and roots, since they are reduced down to addition, subtraction, multiplication and division, respectively.\n\nOn the other hand, the operations of addition and subtraction are more complicated and they are calculated by the formula:\n\nwhere the \"sum\" function is defined by formula_13, and the \"difference\" function by formula_14. These functions formula_15 and formula_16 are also known as Gaussian logarithms.\n\nThe simplification of multiplication, division, roots, and powers is counterbalanced by the cost of evaluating these functions for addition and subtraction. This added cost of evaluation may not be critical when using an LNS primarily for increasing the precision of floating-point math operations.\n\nLogarithmic number systems have been independently invented and published at least three times as an alternative to fixed-point and floating-point number systems.\n\nNicholas Kingsbury and Peter Rayner introduced \"logarithmic arithmetic\" for digital signal processing (DSP) in 1971.\n\nA similar LNS named \"signed logarithmic number system\" (SLNS) was described in 1975 by Earl Swartzlander and Aristides Alexopoulos; rather than use two's complement notation for the logarithms, they offset them (scale the numbers being represented) to avoid negative logs.\n\nSamuel Lee and Albert Edgar described a similar system, which they called the \"Focus\" number system, in 1977.\n\nThe mathematical foundations for addition and subtraction in an LNS trace back to Zecchini Leonelli and Carl Friedrich Gauss in the early 1800s.\n\nA LNS has been used in the Gravity Pipe (GRAPE-5) special-purpose supercomputer that won the Gordon Bell Prize in 1999.\n\nA substantial effort to explore the applicability of LNSs as a viable alternative to floating point for general-purpose processing of single-precision real numbers is described in the context of the \"European Logarithmic Microprocessor\" (ELM). A fabricated prototype of the processor, which has a 32-bit cotransformation-based LNS arithmetic logic unit (ALU), demonstrated LNSs as a \"more accurate alternative to floating-point\", with improved speed. Further improvement of the LNS design based on the ELM architecture has shown its capability to offer significantly higher speed and accuracy than floating-point as well.\n\nLNSs are sometimes used in FPGA-based applications where most arithmetic operations are multiplication or division.\n\n\n\n"}
{"id": "17841907", "url": "https://en.wikipedia.org/wiki?curid=17841907", "title": "Malthusian equilibrium", "text": "Malthusian equilibrium\n\nA population is in Malthusian equilibrium when all of its production is used only for subsistence. Malthusian equilibrium is a locally stable and a dynamic equilibrium.\n\n"}
{"id": "34639092", "url": "https://en.wikipedia.org/wiki?curid=34639092", "title": "Maplet", "text": "Maplet\n\nA maplet or maplet arrow (symbol: ↦, commonly pronounced \"maps to\") is a symbol consisting of a vertical line with a rightward-facing arrow. It is used in mathematics and in computer science to denote functions (the expression x ↦ y is also called a maplet). One example of use of the maplet is in Z notation, a formal specification language used in software development.\n\nIn the Unicode character set, the maplet is at the point U+21A6.\n"}
{"id": "7622915", "url": "https://en.wikipedia.org/wiki?curid=7622915", "title": "Parry–Daniels map", "text": "Parry–Daniels map\n\nIn mathematics, the Parry–Daniels map is a function studied in the context of dynamical systems. Typical questions concern the existence of an invariant or ergodic measure for the map.\n\nIt is named after the English mathematician Bill Parry and the British statistician Henry Daniels, who independently studied the map in papers published in 1962.\n\nGiven an integer \"n\" ≥ 1, let Σ denote the \"n\"-dimensional simplex in R given by\n\nLet \"π\" be a permutation such that\n\nThen the Parry–Daniels map\n\nis defined by\n"}
{"id": "46265020", "url": "https://en.wikipedia.org/wiki?curid=46265020", "title": "Pepper (cryptography)", "text": "Pepper (cryptography)\n\nIn cryptography, a pepper is a secret added to an input such as a password prior to being hashed with a cryptographic hash function. As of 2017, NIST recommends using a secret input when storing memorized secrets such as passwords.\n\nA pepper performs a comparable role to a salt, but while a salt is not secret (merely unique) and can be stored alongside the hashed output, a pepper is secret and must not be stored with the output. The hash and salt are usually stored in a database, but a pepper must be stored separately (e.g. in a configuration file) to prevent it from being obtained by the attacker in case of a database breach. Where the salt only has to be long enough to be unique, a pepper has to be secure to remain secret (at least 112 bits is recommended by NIST), otherwise an attacker only needs one known entry to crack the pepper. Finally, the pepper must be generated anew for every application it is deployed in, otherwise a breach of one application would result in lowered security of another application.\n\nA pepper adds security to a database of salts and hashes because unless the attacker is able to obtain the pepper, they cannot crack a single hash, no matter how weak the original password. One downside of hashing passwords instead of encrypting passwords, is that an attacker can brute force the hashes and recover weak passwords. The encryption equivalent of a pepper is the encryption key. By including pepper in the hash, one can have the advantages of both methods: uncrackable passwords so long as the pepper remains unknown to the attacker, and even in the case the pepper is breached, an attacker still has to crack the hashes. For comparison, when encrypting passwords, anyone with knowledge of the encryption key (including system administrators) can instantly decrypt all passwords; hence, it is always recommended to hash passwords instead of encrypting them, even when not using a pepper.\n\nHere is an incomplete example of using a constant pepper when storing passwords. This first table has two username and password combinations.\n\nThe password is not stored, and the 8-byte (64-bit) pepper 44534C70C6883DE2 is stored in a secure location separate from the hashed values.\n\nIn contrast to a salt, a pepper does not on its own protect against identifying users who have the same password, but it does protect against both dictionary attacks and brute-force attacks so long as the attacker does not have the pepper value.\n\n"}
{"id": "285109", "url": "https://en.wikipedia.org/wiki?curid=285109", "title": "Predicate (mathematical logic)", "text": "Predicate (mathematical logic)\n\nIn mathematical logic, a predicate is commonly understood to be a Boolean-valued function \"P\": \"X\"→ {true, false}, called the predicate on \"X\". However, predicates have many different uses and interpretations in mathematics and logic, and their precise definition, meaning and use will vary from theory to theory. So, for example, when a theory defines the concept of a relation, then a predicate is simply the characteristic function (otherwise known as the indicator function) of a relation. However, not all theories have relations, or are founded on set theory, and so one must be careful with the proper definition and semantic interpretation of a predicate.\n\nInformally, a predicate is a statement that may be true or false depending on the values of its variables. It can be thought of as an operator or function that returns a value that is either true or false. For example, predicates are sometimes used to indicate set membership: when talking about sets, it is sometimes inconvenient or impossible to describe a set by listing all of its elements. Thus, a predicate \"P(x)\" will be true or false, depending on whether \"x\" belongs to a set.\n\nPredicates are also commonly used to talk about the properties of objects, by defining the set of all objects that have some property in common. So, for example, when \"P\" is a predicate on \"X\", one might sometimes say \"P\" is a property of \"X\". Similarly, the notation \"P\"(\"x\") is used to denote a sentence or statement \"P\" concerning the variable object x. The set defined by \"P\"(\"x\") is written as {\"x\" | \"P\"(\"x\")}, and is the set of objects for which \"P\" is true.\n\nFor instance, {\"x\" | \"x\" is a natural number less than 4} is the set {1,2,3}.\n\nIf \"t\" is an element of the set {\"x\" | \"P\"(\"x\")}, then the statement \"P\"(\"t\") is \"true\".\n\nHere, \"P\"(\"x\") is referred to as the \"predicate\", and \"x\" the \"placeholder\" of the \"proposition\". Sometimes, \"P\"(\"x\") is also called a (template in the role of) propositional function, as each choice of the placeholder \"x\" produces a proposition.\n\nA simple form of predicate is a Boolean expression, in which case the inputs to the expression are themselves Boolean values, combined using Boolean operations. Similarly, a Boolean expression with inputs predicates is itself a more complex predicate.\n\nThe precise semantic interpretation of an atomic formula and an atomic sentence will vary from theory to theory.\n\n\n\n"}
{"id": "1258607", "url": "https://en.wikipedia.org/wiki?curid=1258607", "title": "Proof assistant", "text": "Proof assistant\n\nIn computer science and mathematical logic, a proof assistant or interactive theorem prover is a software tool to assist with the development of formal proofs by human-machine collaboration. This involves some sort of interactive proof editor, or other interface, with which a human can guide the search for proofs, the details of which are stored in, and some steps provided by, a computer.\n\n\nA popular front-end for proof assistants is the Emacs-based Proof General, developed at the University of Edinburgh.\nCoq includes CoqIDE, which is based on OCaml/Gtk. Isabelle includes Isabelle/jEdit, which is based on jEdit and the Isabelle/Scala infrastructure for document-oriented proof processing.\n\n\n\n\n"}
{"id": "1176581", "url": "https://en.wikipedia.org/wiki?curid=1176581", "title": "RL (complexity)", "text": "RL (complexity)\n\nRandomized Logarithmic-space (RL), sometimes called RLP (Randomized Logarithmic-space Polynomial-time), is the complexity class of computational complexity theory problems solvable in logarithmic space and polynomial time with probabilistic Turing machines with one-sided error. It is named in analogy with RP, which is similar but has no logarithmic space restriction.\n\nThe probabilistic Turing machines in the definition of RL never accept incorrectly but are allowed to reject incorrectly less than 1/3 of the time; this is called \"one-sided error\". The constant 1/3 is arbitrary; any \"x\" with 0 < \"x\" < 1 would suffice. This error can be made 2 times smaller for any polynomial \"p\"(\"x\") without using more than polynomial time or logarithmic space by running the algorithm repeatedly.\n\nSometimes the name RL is reserved for the class of problems solvable by logarithmic-space probabilistic machines in \"unbounded\" time. However, this class can be shown to be equal to NL using a probabilistic counter, and so is usually referred to as NL instead; this also shows that RL is contained in NL. RL is contained in BPL, which is similar but allows two-sided error (incorrect accepts). RL contains L, the problems solvable by deterministic Turing machines in log space, since its definition is just more general.\n\nNoam Nisan showed in 1992 the weak derandomization result that RL is contained in SC, the class of problems solvable in polynomial time and polylogarithmic space on a deterministic Turing machine; in other words, given \"polylogarithmic\" space, a deterministic machine can simulate \"logarithmic\" space probabilistic algorithms.\n\nIt is believed that RL is equal to L, that is, that polynomial-time logspace computation can be completely derandomized; major evidence for this was presented by Reingold et al. in 2005. A proof of this is the holy grail of the efforts in the field of unconditional derandomization of complexity classes. A major step forward was Omer Reingold's proof that SL is equal to L.\n"}
{"id": "22948920", "url": "https://en.wikipedia.org/wiki?curid=22948920", "title": "Robert Longhurst", "text": "Robert Longhurst\n\nRobert Longhurst is an American sculptor who was born in Schenectady, New York in 1949. At an early age he was fascinated by his father’s small figurative woodcarvings.\n\nLonghurst received a Bachelor of Architecture from Kent State University in 1975. He began his artistic career in 1976 in Cincinnati, Ohio. His first commissioned works were three figurative sculptures in black walnut for Cincinnati businessman Joe David who owned Midwest Woodworking company. In 1978 Longhurst completed a life size figure in pine of an Adirondack hermit, Noah John Rondeau for the Adirondack Museum in Blue Mountain Lake, New York.\n\nAlthough Longhurst's career began with figurative works, it soon evolved into non-representational abstraction in exotic woods, marble and granite that draws on his background in Architecture. Many of his pieces are defined as being at the intersection of where the fields of art and math overlap, and they have been discussed by Mathematicians such as Nathaniel Friedman, Reuben Hersh, and Ivars Peterson. Some of his sculptures portray minimal surfaces, which were named after German geometer Alfred Enneper. Nathaniel Friedman writes, \"The surfaces [of Longhurst's sculptures] generally have appealing sections with negative curvature (saddle surfaces). This is a natural intuitive result of Longhurst's feeling for satisfying shape rather than a mathematically deduced result.\"\n\nLonghurst participated in the International Snow Sculpture Championships in Breckenridge, Colorado, in 2000 and 2001, by joining a snow sculpting team from Minnesota. Two of his sculptures representing minimal surfaces were enlarged and carved from blocks of snow measuring 12' high x 10' wide x 10' deep. In the 2000 championships, the team received second place, Artists' Choice Award, and People's Choice Award.\n\nRobert Longhurst lives in Chestertown, New York.\n\n\n"}
{"id": "41177893", "url": "https://en.wikipedia.org/wiki?curid=41177893", "title": "Roy Adler", "text": "Roy Adler\n\nRoy Lee Adler (February 22, 1931 – July 26, 2016) was an American mathematician.\n\nAdler earned his Ph.D. in 1961 from Yale University under the supervision of Shizuo Kakutani (\"On some algebraic aspects of measure preserving transformations\"). He then worked as a mathematician for IBM at the Thomas J. Watson Research Center.\n\nAdler studies dynamical systems, ergodic theory, symbolic and topological dynamics and coding theory. The road coloring problem that was solved by Avraham Trakhtman in 2007 came from him, along with L. W. Goodwyn and Benjamin Weiss.\n\nHe was a fellow of the American Mathematical Society.\n\n"}
{"id": "14312774", "url": "https://en.wikipedia.org/wiki?curid=14312774", "title": "Stretching field", "text": "Stretching field\n\nIn applied mathematics, stretching fields provide the local deformation of an infinitesimal circular fluid element over a finite time interval ∆\"t\". The logarithm of the stretching (after first dividing by ∆\"t\") gives the finite-time Lyapunov exponent λ for separation of nearby fluid elements at each point in a flow. For periodic two-dimensional flows, stretching fields have been shown to be closely related to the mixing of a passive scalar concentration field. Until recently, however, the extension of these ideas to systems that are non-periodic or weakly turbulent has been possible only in numerical simulations.\n"}
{"id": "546039", "url": "https://en.wikipedia.org/wiki?curid=546039", "title": "Syntax (logic)", "text": "Syntax (logic)\n\nIn logic, syntax is anything having to do with formal languages or formal systems without regard to any interpretation or meaning given to them. Syntax is concerned with the rules used for constructing, or transforming the symbols and words of a language, as contrasted with the semantics of a language which is concerned with its meaning.\n\nThe symbols, formulas, systems, theorems, proofs, and interpretations expressed in formal languages are syntactic entities whose properties may be studied without regard to any meaning they may be given, and, in fact, need not be given any.\n\nSyntax is usually associated with the rules (or grammar) governing the composition of texts in a formal language that constitute the well-formed formulas of a formal system.\n\nIn computer science, the term \"syntax\" refers to the rules governing the composition of well-formed expressions in a programming language. As in mathematical logic, it is independent of semantics and interpretation.\n\nA symbol is an idea, abstraction or concept, tokens of which may be marks or a configuration of marks which form a particular pattern. Symbols of a formal language need not be symbols of anything. For instance there are logical constants which do not refer to any idea, but rather serve as a form of punctuation in the language (e.g. parentheses). A symbol or string of symbols may comprise a well-formed formula if the formulation is consistent with the formation rules of the language. Symbols of a formal language must be capable of being specified without any reference to any interpretation of them.\n\nA \"formal language\" is a syntactic entity which consists of a set of finite strings of symbols which are its words (usually called its well-formed formulas). Which strings of symbols are words is determined by the creator of the language, usually by specifying a set of formation rules. Such a language can be defined without reference to any meanings of any of its expressions; it can exist before any interpretation is assigned to it – that is, before it has any meaning.\n\n\"Formation rules\" are a precise description of which strings of symbols are the well-formed formulas of a formal language. It is synonymous with the set of strings over the alphabet of the formal language which constitute well formed formulas. However, it does not describe their semantics (i.e. what they mean).\n\nA proposition is a sentence expressing something true or false. A proposition is identified ontologically as an idea, concept or abstraction whose token instances are patterns of symbols, marks, sounds, or strings of words. Propositions are considered to be syntactic entities and also truthbearers.\n\nA formal theory is a set of sentences in a formal language.\n\nA \"formal system\" (also called a \"logical calculus\", or a \"logical system\") consists of a formal language together with a deductive apparatus (also called a \"deductive system\"). The deductive apparatus may consist of a set of transformation rules (also called \"inference rules\") or a set of axioms, or have both. A formal system is used to derive one expression from one or more other expressions. Formal systems, like other syntactic entities may be defined without any interpretation given to it (as being, for instance, a system of arithmetic).\n\nA formula A is a syntactic consequence within some formal system formula_1 of a set Г of formulas if there is a derivation in formal system formula_1 of A from the set Г.\n\nSyntactic consequence does not depend on any interpretation of the formal system.\n\nA formal system formula_4 is \"syntactically complete\" (also \"deductively complete\", \"maximally complete\", \"negation complete\" or simply \"complete\") iff for each formula A of the language of the system either A or ¬A is a theorem of formula_4. In another sense, a formal system is syntactically complete iff no unprovable axiom can be added to it as an axiom without introducing an inconsistency. Truth-functional propositional logic and first-order predicate logic are semantically complete, but not syntactically complete (for example the propositional logic statement consisting of a single variable \"a\" is not a theorem, and neither is its negation, but these are not tautologies). Gödel's incompleteness theorem shows that no recursive system that is sufficiently powerful, such as the Peano axioms, can be both consistent and complete.\n\nAn \"interpretation\" of a formal system is the assignment of meanings to the symbols, and truth values to the sentences of a formal system. The study of interpretations is called formal semantics. \"Giving an interpretation\" is synonymous with \"constructing a model\". An interpretation is expressed in a metalanguage, which may itself be a formal language, and as such itself is a syntactic entity.\n\n"}
{"id": "21340776", "url": "https://en.wikipedia.org/wiki?curid=21340776", "title": "The College Mathematics Journal", "text": "The College Mathematics Journal\n\nThe College Mathematics Journal is an expository magazine aimed at teachers of college mathematics, particular those teaching the first two years. It is published by Taylor & Francis on behalf of the Mathematical Association of America and is a continuation of \"Two-Year College Mathematics Journal\". It covers all aspects of mathematics. It publishes articles intended to enhance undergraduate instruction and classroom learning, including expository articles, short notes, problems, and \"mathematical ephemera\" such as fallacious proofs, quotations, cartoons, poetry, and humor.\nPaid circulation in 2008 was 9,000 and total circulation was 9,500.\n\nThe MAA gives the George Pólya Awards annually \"for articles of expository excellence\" published in the \"College Mathematics Journal\".\n\n"}
{"id": "276174", "url": "https://en.wikipedia.org/wiki?curid=276174", "title": "Time-scale calculus", "text": "Time-scale calculus\n\nIn mathematics, time-scale calculus is a unification of the theory of difference equations with that of differential equations, unifying integral and differential calculus with the calculus of finite differences, offering a formalism for studying hybrid discrete–continuous dynamical systems. It has applications in any field that requires simultaneous modelling of discrete and continuous data. It gives a new definition of a derivative such that if one differentiates a function which acts on the real numbers then the definition is equivalent to standard differentiation, but if one uses a function acting on the integers then it is equivalent to the forward difference operator.\n\nTime-scale calculus was introduced in 1988 by the German mathematician Stefan Hilger. However, similar ideas have been used before and go back at least to the introduction of the Riemann–Stieltjes integral which unifies sums and integrals.\n\nMany results concerning differential equations carry over quite easily to corresponding results for difference equations, while other results seem to be completely different from their continuous counterparts. The study of dynamic equations on time scales reveals such discrepancies, and helps avoid proving results twice — once for differential equations and once again for difference equations. The general idea is to prove a result for a dynamic equation where the domain of the unknown function is a so-called time scale (also known as a time-set), which may be an arbitrary closed subset of the reals. In this way, results apply not only to the set of real numbers or set of integers but to more general time scales such as a Cantor set.\n\nThe three most popular examples of calculus on time scales are differential calculus, difference calculus, and quantum calculus. Dynamic equations on a time scale have a potential for applications, such as in population dynamics. For example, they can model insect populations that evolve continuously while in season, die out in winter while their eggs are incubating or dormant, and then hatch in a new season, giving rise to a non–overlapping population.\n\nA time scale (or measure chain) is a closed subset of the real line formula_1. The common notation for a general time scale is formula_2.\n\nThe two most commonly encountered examples of time scales are the real numbers formula_1 and the discrete time scale formula_4.\n\nA single point in a time scale is defined as:\n\nThe \"forward jump\" and \"backward jump\" operators represent the closest point in the time scale on the right and left of a given point formula_6, respectively. Formally:\n\nThe \"graininess\" formula_9 is the distance from a point to the closest point on the right and is given by:\n\nFor a right-dense formula_6, formula_12 and formula_13.\nFor a left-dense formula_6, formula_15\n\nFor any formula_16, formula_6 is:\n\nAs illustrated by the figure at right:\n\nContinuity of a time scale is redefined as equivalent to density. A time scale is said to be \"right-continuous at point formula_6\" if it is right dense at point formula_6. Similarly, a time scale is said to be \"left-continuous at point formula_6\" if it is left dense at point formula_6.\n\nTake a function:\n\n(where ℝ could be any Banach space, but is set to the real line for simplicity).\n\nDefinition: The \"delta derivative\" (also Hilger derivative) formula_31 exists if and only if:\n\nFor every formula_32 there exists a neighborhood formula_33 of formula_6 such that:\nfor all formula_36 in formula_33.\n\nTake formula_38 Then formula_39, formula_40, formula_41; is the derivative used in standard calculus. If formula_42 (the integers), formula_43, formula_44, formula_45 is the forward difference operator used in difference equations.\n\nThe \"delta integral\" is defined as the antiderivative with respect to the delta derivative. If formula_46 has a continuous derivative formula_47 one sets\n\nA Laplace transform can be defined for functions on time scales, which uses the same table of transforms for any arbitrary time scale. This transform can be used to solve dynamic equations on time scales. If the time scale is the non-negative integers then the transform is equal to a modified Z-transform:\n\nformula_49\n\nPartial differential equations and partial difference equations are unified as partial dynamic equations on time scales.\n\nMultiple integration on time scales is treated in Bohner (2005).\n\nStochastic differential equations and stochastic difference equations can be generalized to stochastic dynamic equations on time scales.\n\nAssociated with every time scale is a natural measure defined via\n\nwhere formula_51 denotes Lebesgue measure and formula_52 is the backward shift operator defined on formula_1. The delta integral\nturns out to be the usual Lebesgue–Stieltjes integral with respect to this measure\n\nand the delta derivative turns out to be the Radon–Nikodym derivative with respect to this measure\n\nThe Dirac delta and Kronecker delta are unified on time scales as the \"Hilger delta\":\n\nIntegral equations and summation equations are unified as integral equations on time scales.\n\nFractional calculus on time scales is treated in Bastos, Mozyrska, and Torres.\n\n\n\n"}
{"id": "22537688", "url": "https://en.wikipedia.org/wiki?curid=22537688", "title": "Tiny and miny", "text": "Tiny and miny\n\nIn mathematics, tiny and miny are operators that yield infinitesimal values when applied to numbers in combinatorial game theory. Given a positive number G, tiny G (denoted by ⧾ in many texts) is equal to {0||0|-G} for any game G, whereas miny G (analogously denoted ⧿) is tiny G's negative, or {G|0||0}.\n\nTiny and miny aren’t just abstract mathematical operators on combinatorial games: tiny and miny games do occur \"naturally\" in such games as toppling dominoes. Specifically, tiny \"n\", where \"n\" is a natural number, can be generated by placing two black dominoes outside \"n\" + 2 white dominoes.\n\nTiny games and up have certain curious relational characteristics. Specifically, though ⧾ is infinitesimal with respect to ↑ for all positive values of \"x\", ⧾⧾⧾ is equal to up. Expansion of ⧾⧾⧾ into its canonical form yields {0||||||0|||||0||0|-G|||0||||0}. While the expression appears daunting, some careful and persistent expansion of the game tree of ⧾⧾⧾ + ↓ will show that it is a second player win, and that, consequently, ⧾⧾⧾ = ↑. Similarly curious, mathematician John Horton Conway noted, calling it \"amusing,\" that \"↑ is the unique solution of ⧾ = G.\" Conway's assertion is also easily verifiable with canonical forms and game trees.\n\n"}
{"id": "15547697", "url": "https://en.wikipedia.org/wiki?curid=15547697", "title": "Two-dimensional graph", "text": "Two-dimensional graph\n\nA two-dimensional graph is a set of points in two-dimensional space. If the points are real and if Cartesian coordinates are used, each axis depicts the potential values of a particular real variable. Often the variable on the horizontal axis is called \"x\" and the one on the vertical axis is called \"y\", in which case the horizontal and vertical axes are sometimes called the \"x\" axis and \"y\" axis respectively. With real variables on the axes, each point in the graph depicts the values of two real variables.\n\nAlternatively, each point in a graph may depict the value of a single complex variable. This two-dimensional graph is called Argand diagram. In the Argand diagram, the horizontal axis is called the real axis and depicts the potential values of the real part of the complex number, while the vertical axis is called the imaginary axis and depicts the potential values of the imaginary part of the complex number.\n\n[[Image:cubicpoly.svg|right|thumb|400 px| Graph of the function formula_1. In other words, it is not a function. Nevertheless, the set of all points given by the relation is still a two-dimensional graph, as in the accompanying graph of the circle formula_2\n\nAn [[image (mathematics)|image]] of a [[plane curve]] is also a two-dimensional graph, although only some two-dimensional graphs are an image of any plane curve.\n\n[[File:Supply-and-demand.svg|thumb|left|240px|The price P of a product is determined by a balance between production at each price (supply S) and the desires of those with [[purchasing power]] at each price (demand D). The diagram shows a positive shift in demand from D to D, resulting in an increase in price (P) and quantity sold (Q) of the product.]]\n\nIn some contexts it is useful to graph two or more functions together in the same diagram. An example is the [[supply and demand]] graph commonly used in [[Economic graph|economics]], shown here.\n\n[[File:Area.svg|thumb|250px|Geometric shapes in 2 dimensions]]\n\nTwo-dimensional [[geometric shape]]s are sets of points bounded by [[line segment]]s or [[curve]]s, so a shape can also be constructed by graphs of several equations of its boundary. [[Polygon]]s are the shapes that are only bounded by line segments. These can be visualized by using two-dimensional graphs. Graphs of two polygons, a [[parallelogram]] and a [[right triangle]], are shown here along with the graph of a circle.\n\n\n[[Category:Charts]]\n[[Category:Functions and mappings]]"}
{"id": "51038090", "url": "https://en.wikipedia.org/wiki?curid=51038090", "title": "Vanja Dukic", "text": "Vanja Dukic\n\nVanja Dukic is an expert in computational statistics and mathematical epidemiology who works as a professor of applied mathematics at the University of Colorado Boulder. Her research includes work on using internet search engine access patterns to track diseases, and on the effects of climate change on the spread of diseases.\n\nDukic earned a bachelor's degree in finance and actuarial mathematics from Bryant University in 1995.\nShe completed her doctorate at Brown University in 2001, under the joint supervision of biostatisticians Constantine Gatsonis and Joseph Hogan. She worked as a faculty member in the biostatistics program of the Department of Public Health Sciences at the University of Chicago from 2001 to 2010, before moving to Colorado.\n\nIn 2015 she was elected as a Fellow of the American Statistical Association \"for important contributions to Bayesian modeling of complex processes and analysis of Big Data, substantive and collaborative research in infectious diseases and climate change, and service to the profession, including excellence in editorial work.\"\n\n"}
{"id": "30178320", "url": "https://en.wikipedia.org/wiki?curid=30178320", "title": "Vertex of a representation", "text": "Vertex of a representation\n\nIn mathematical finite group theory, the vertex of a representation of a finite group is a subgroup associated to it, that has a special representation called a source. Vertices and sources were introduced by \n"}
{"id": "26898094", "url": "https://en.wikipedia.org/wiki?curid=26898094", "title": "Von Neumann–Morgenstern utility theorem", "text": "Von Neumann–Morgenstern utility theorem\n\nIn decision theory, the von Neumann-Morgenstern utility theorem shows that, under certain axioms of rational behavior, a decision-maker faced with risky (probabilistic) outcomes of different choices will behave as if he or she is maximizing the expected value of some function defined over the potential outcomes at some specified point in the future. This function is known as the von Neumann-Morgenstern utility function. The theorem is the basis for expected utility theory.\n\nIn 1947, John von Neumann and Oskar Morgenstern proved that any individual whose preferences satisfied four axioms has a utility function; such an individual's preferences can be represented on an interval scale and the individual will always prefer actions that maximize expected utility. That is, they proved that an agent is (VNM-)rational \"if and only if\" there exists a real-valued function \"u\" defined by possible outcomes such that every preference of the agent is characterized by maximizing the expected value of \"u\", which can then be defined as the agent's \"VNM-utility\" (it is unique up to adding a constant and multiplying by a positive scalar). No claim is made that the agent has a \"conscious desire\" to maximize \"u\", only that \"u\" exists.\n\nAny individual whose preferences violate von Neumann and Morgenstern's axioms would agree to a Dutch book, which is a set of bets that necessarily leads to a loss. Therefore, it is arguable that any individual who violates the axioms is irrational. The expected utility hypothesis is that rationality can be modeled as maximizing an expected value, which given the theorem, can be summarized as \"rationality is VNM-rationality\".\n\nVNM-utility is a \"decision utility\" in that it is used to describe \"decision preferences\". It is related but not equivalent to so-called \"E-utilities\" (experience utilities), notions of utility intended to measure happiness such as that of Bentham's Greatest Happiness Principle.\n\nIn the theorem, an individual agent is faced with options called \"lotteries\". Given some mutually exclusive outcomes, a lottery is a scenario where each outcome will happen with a given probability, all probabilities summing to one. For example, for two outcomes \"A\" and \"B\",\n\ndenotes a scenario where \"P\"(\"A\") = 25% is the probability of \"A\" occurring and \"P\"(\"B\") = 75% (and exactly one of them will occur). More generally, for a lottery with many possible outcomes \"A\", we write\n\nwith the sum of the formula_3s equalling 1.\n\nThe outcomes in a lottery can themselves be lotteries between other outcomes, and the expanded expression is considered an equivalent lottery: 0.5(0.5\"A\" + 0.5\"B\") + 0.5\"C\" = 0.25\"A\" + 0.25\"B\" + 0.50\"C\".\n\nIf lottery \"M\" is preferred over lottery \"L\", we write formula_4, or equivalently, formula_5. If the agent is indifferent between \"L\" and \"M\", we write the \"indifference relation\" formula_6 If \"M\" is either preferred over or viewed with indifference relative to \"L\", we write formula_7\n\nThe four axioms of VNM-rationality are then \"completeness\", \"transitivity\", \"continuity\", and \"independence\".\n\nCompleteness assumes that an individual has well defined preferences:\n\n(either \"M\" is preferred, \"L\" is preferred, or the individual is indifferent).\n\nTransitivity assumes that preferences are consistent across any three options:\n\nContinuity assumes that there is a \"tipping point\" between being \"better than\" and \"worse than\" a given middle option:\n\nwhere the notation on the left side refers to a situation in which \"L\" is received with probability \"p\" and \"N\" is received with probability (1–\"p\").\n\nInstead of continuity, an alternative axiom can be assumed that does not involve a precise equality, called the Archimedean property. It says that any separation in preference can be maintained under a sufficiently small deviation in probabilities:\n\nOnly one of (3) and (3′) need be assumed, and the other will be implied by the theorem.\n\nIndependence of irrelevant alternatives assumes that a preference holds independently of the possibility of another outcome:\n\nThe independence axiom implies the axiom on reduction of compound lotteries:\n\nTo see how Axiom 4 implies Axiom 4', set formula_29 in the expression in Axiom 4, and expand.\n\nFor any VNM-rational agent (i.e. satisfying axioms 1–4), there exists a function \"u\" which assigns to each outcome \"A\" a real number \"u(A)\" such that for any two lotteries,\n\nwhere \"E(u(L))\", or more briefly \"Eu\"(\"L\") is given by\n\nAs such, \"u\" can be uniquely determined (up to adding a constant and multiplying by a positive scalar) by preferences between \"simple lotteries\", meaning those of the form \"pA\" + (1 − \"p\")\"B\" having only two outcomes. Conversely, any agent acting to maximize the expectation of a function \"u\" will obey axioms 1–4. Such a function is called the agent's von Neumann–Morgenstern (VNM) utility.\n\nThe proof is constructive: it shows how the desired function formula_32 can be built. Here we outline the construction process for the case in which the number of sure outcomes is finite.\n\nSuppose there are \"n\" sure outcomes, formula_33. Note that every sure outcome can be seen as a lottery: it is a degenerate lottery in which the outcome is selected with probability 1. Hence, by the Completeness axiom, it is possible to order the outcomes from worst to best:\n\nWe assume that at least one of the inequalities is strict (otherwise the utility function is trivial—a constant). So formula_35. We use these two extreme outcomes—the worst and the best—as the scaling unit of our utility function, and define:\n\nFor every probability formula_38, define a lottery that selects the best outcome with probability formula_39 and the worst outcome otherwise:\nNote that formula_41 and formula_42.\n\nBy the Continuity axiom, for every sure outcome formula_43, there is a probability formula_44 such that:\n\nand\n\nFor every formula_47, the utility function for outcome formula_43 is defined as\n\nso the utility of every lottery formula_50 is the expectation of \"u\":\n\nTo see why this utility function make sense, consider a lottery formula_52, which selects outcome formula_43 with probability formula_3. But, by our assumption, the decision maker is indifferent between the sure outcome formula_43 and the lottery formula_56. So, by the Reduction axiom, he is indifferent between the lottery formula_57 and the following lottery:\nThe lottery formula_61 is, in effect, a lottery in which the best outcome is won with probability formula_62, and the worst outcome otherwise.\n\nHence, if formula_63, a rational decision maker would prefer the lottery formula_57 over the lottery formula_65, because it gives him a larger chance to win the best outcome.\n\nHence:\n\nVon Neumann and Morgenstern anticipated surprise at the strength of their conclusion. But according to them, the reason their utility function works is that it is constructed precisely to fill the role of something whose expectation is maximized:\n\"Many economists will feel that we are assuming far too much ... Have we not shown too much? ... As far as we can see, our postulates [are] plausible ... We have practically defined numerical utility as being that thing for which the calculus of mathematical expectations is legitimate.\" – \"VNM 1953, § 3.1.1 p.16 and § 3.7.1 p. 28\"\nThus, the content of the theorem is that the construction of \"u\" is possible, and they claim little about its nature.\n\nIt is often the case that a person, faced with real-world gambles with money, does not act to maximize the expected value of their \"dollar assets.\" For example, a person who only possesses $1000 in savings may be reluctant to risk it all for a 20% chance odds to win $10,000, even though\n\nHowever, \"if\" the person is VNM-rational, such facts are automatically accounted for in their utility function \"u\". In this example, we could conclude that\n\nwhere the dollar amounts here really represent \"outcomes\" (cf. \"value\"), the three possible situations the individual could face. In particular, \"u\" can exhibit properties like \"u\"($1)+\"u\"($1) ≠ \"u\"($2) without contradicting VNM-rationality at all. This leads to a quantitative theory of monetary risk aversion.\n\nIn 1738, Daniel Bernoulli published a treatise in which he posits that rational behavior can be described as maximizing the expectation of a function \"u\", which in particular need not be monetary-valued, thus accounting for risk aversion. This is the \"expected utility hypothesis\". As stated, the hypothesis may appear to be a bold claim. The aim of the \"expected utility theorem\" is to provide \"modest conditions\" (i.e. axioms) describing when the expected utility hypothesis holds, which can be evaluated directly and intuitively:\n\"The axioms should not be too numerous, their system is to be as simple and transparent as possible, and each axiom should have an immediate intuitive meaning by which its appropriateness may be judged directly. In a situation like ours this last requirement is particularly vital, in spite of its vagueness: we want to make an intuitive concept amenable to mathematical treatment and to see as clearly as\npossible what hypotheses this requires.\" – \"VNM 1953 § 3.5.2, p. 25\"\nAs such, claims that the expected utility hypothesis does not characterize rationality must reject one of the VNM axioms. A variety of generalized expected utility theories have arisen, most of which drop or relax the independence axiom.\n\nBecause the theorem assumes nothing about the nature of the possible outcomes of the gambles, they could be morally significant events, for instance involving the life, death, sickness, or health of others. A von Neumann–Morgenstern rational agent is capable of acting with great concern for such events, sacrificing much personal wealth or well-being, and all of these actions will factor into the construction/definition of the agent's VNM-utility function. In other words, both what is naturally perceived as \"personal gain\", and what is naturally perceived as \"altruism\", are implicitly balanced in the VNM-utility function of a VNM-rational individual. Therefore, the full range of agent-focussed to agent-neutral behaviors are .\n\nIf the utility of formula_70 is formula_71, a von Neumann–Morgenstern rational agent must be indifferent between formula_72 and formula_73. An agent-focused von Neumann–Morgenstern rational agent therefore cannot favor more equal, or \"fair\", distributions of utility between its own possible future selves.\n\nSome utilitarian moral theories are concerned with quantities called the \"total utility\" and \"average utility\" of collectives, and characterize morality in terms of favoring the utility or happiness of others with disregard for one's own. These notions can be related to, but are distinct from, VNM-utility:\n\nThe term \"E-utility\" for \"experience utility\" has been coined to refer to the types of \"hedonistic\" utility like that of Bentham's greatest happiness principle. Since morality affects decisions, a VNM-rational agent's morals will affect the definition of its own utility function (see above). Thus, the morality of a VNM-rational agent can be characterized by \"correlation\" of the agent's VNM-utility with the VNM-utility, E-utility, or \"happiness\" of others, among other means, but not by \"disregard\" for the agent's own VNM-utility, a contradiction in terms.\n\nSince if \"L\" and \"M\" are lotteries, then \"pL\" + (1 − \"p\")\"M\" is simply \"expanded out\" and considered a lottery itself, the VNM formalism ignores what may be experienced as \"nested gambling\". This is related to the Ellsberg problem where people choose to avoid the perception of \"risks about risks\". Von Neumann and Morgenstern recognized this limitation:\n\n\"...concepts like a \"specific utility of gambling\" cannot be formulated free of contradiction on this level. This may seem to be a paradoxical assertion. But anybody who has seriously tried to axiomatize that elusive concept, will probably concur with it.\" – \"VNM 1953 § 3.7.1, p. 28\".\n\nSince for any two VNM-agents \"X\" and \"Y\", their VNM-utility functions \"u\" and \"u\" are only determined up to additive constants and multiplicative positive scalars, the theorem does not provide any canonical way to compare the two. Hence expressions like \"u\"(\"L\") + \"u\"(\"L\") and \"u\"(\"L\") − \"u\"(\"L\") are not canonically defined, nor are comparisons like \"u\"(\"L\") < \"u\"(\"L\") canonically true or false. In particular, the aforementioned \"total VNM-utility\" and \"average VNM-utility\" of a population are not canonically meaningful without normalization assumptions.\n\nThe expected utility hypothesis, as applied to economics, has limited predictive accuracy, simply because in practice, humans do not always behave VNM-rationally. This is manifested in several experimental outcomes such as the Allais paradox.\nThis can be interpreted as evidence that\n\n"}
{"id": "26901526", "url": "https://en.wikipedia.org/wiki?curid=26901526", "title": "Welch bounds", "text": "Welch bounds\n\nIn mathematics, Welch bounds are a family of inequalities pertinent to the problem of evenly spreading a set of unit vectors in a vector space. The bounds are important tools in the design and analysis of certain methods in telecommunication engineering, particularly in coding theory. The bounds were originally published in a 1974 paper by L. R. Welch.\n\nIf formula_1 are unit vectors in formula_2, define formula_3, where formula_4 is the usual inner product on formula_2. Then the following inequalities hold for formula_6:\n\nIf formula_8, then the vectors formula_9 can form an orthonormal set in formula_2. In this case, formula_11 and the bounds are vacuous. Consequently, interpretation of the bounds is only meaningful if formula_12. This will be assumed throughout the remainder of this article.\n\nThe \"first Welch bound,\" corresponding to formula_13, is by far the most commonly used in applications. Its proof proceeds in two steps, each of which depends on a more basic mathematical inequality. The first step invokes the Cauchy–Schwarz inequality and begins by considering the formula_14 Gram matrix formula_15 of the vectors formula_9; i.e.,\n\nThe trace of formula_15 is equal to the sum of its eigenvalues. Because the rank of formula_15 is at most formula_20, and it is a positive semidefinite matrix, formula_15 has at most formula_20 positive eigenvalues with its remaining eigenvalues all equal to zero. Writing the non-zero eigenvalues of formula_15 as formula_24 with formula_25 and applying the Cauchy-Schwarz inequality to the inner product of an formula_26-vector of ones with a vector whose components are these eigenvalues yields\n\nThe square of the Frobenius norm (Hilbert–Schmidt norm) of formula_15 satisfies\n\nTaking this together with the preceding inequality gives\n\nBecause each formula_31 has unit length, the elements on the main diagonal of formula_15 are ones, and hence its trace is formula_33. So,\n\nor\n\nThe second part of the proof uses an inequality encompassing the simple observation that the average of a set of non-negative numbers can be no greater than the largest number in the set. In mathematical notation, if formula_36 for formula_37, then\n\nThe previous expression has formula_39 non-negative terms in the sum,the largest of which is formula_40. So,\n\nor \n\nwhich is precisely the inequality given by Welch in the case that formula_13.\n\nIn certain telecommunications applications, it is desirable to construct sets of vectors that meet the Welch bounds with equality. Several techniques have been introduced to obtain so-called Welch Bound Equality (WBE) sets of vectors for the \"k\" = 1 bound.\n\nThe proof given above shows that two separate mathematical inequalities are incorporated into the Welch bound when formula_13. The Cauchy–Schwarz inequality is met with equality when the two vectors involved are collinear. In the way it is used in the above proof, this occurs when all the non-zero eigenvalues of the Gram matrix formula_15 are equal, which happens precisely when the vectors formula_1 constitute a tight frame for formula_2.\n\nThe other inequality in the proof is satisfied with equality if and only if formula_48 is the same for every choice of formula_49. In this case, the vectors are equiangular. So this Welch bound is met with equality if and only if the set of vectors formula_9 is an equiangular tight frame in formula_2.\n\n"}
{"id": "36717755", "url": "https://en.wikipedia.org/wiki?curid=36717755", "title": "Émile Cotton", "text": "Émile Cotton\n\nÉmile Clément Cotton (5 February 1872 – 14 March 1950) was a professor of mathematics at the University of Grenoble. His PhD thesis studied differential geometry in three dimensions, with the introduction of the Cotton tensor. He held the professorship from 1904 until his 1942 retirement. He was the brother of Aimé Cotton.\n"}
