{"id": "6317392", "url": "https://en.wikipedia.org/wiki?curid=6317392", "title": "231 (number)", "text": "231 (number)\n\n231 (two hundred [and] thirty-one) is the natural number following 230 and preceding 232.\nTwo hundred [and] thirty-one 231 = 3·7·11, sphenic number, triangular number, hexagonal number, octahedral number, centered octahedral number, the number of integer partitions of 16, Mertens function returns 0, and is the number of cubic inches in a U.S. liquid gallon.\n\n231 is:\n"}
{"id": "34278093", "url": "https://en.wikipedia.org/wiki?curid=34278093", "title": "Anatolii Goldberg", "text": "Anatolii Goldberg\n\nAnatolii Asirovich Goldberg (, , , April 2, 1930 in Kyiv – October 11, 2008 in Netanya) was a Soviet and Israeli mathematician working in complex analysis. His main area of research was the theory of entire and meromorphic functions.\n\nGoldberg received his PhD in 1955 from Lvov University under the direction\nof Lev Volkovyski. He worked as a docent in Uzhgorod University (1955–1963), then in Lvov University (1963–1997), where he became a full professor in 1965, and in Bar Ilan University (1997–2008). Goldberg, jointly with I.V. Ostrovskii and B.Ya. Levin, was awarded the State Prize of Ukraine\nin 1992.\n\nAmong his main achievements are:\n\nHe authored a book and over 150 research papers.\n\nSeveral things are named after him: Goldberg's examples,\nGoldberg's constants, and Goldberg's conjecture.\n\n\n"}
{"id": "2476311", "url": "https://en.wikipedia.org/wiki?curid=2476311", "title": "Arboricity", "text": "Arboricity\n\nThe arboricity of an undirected graph is the minimum number of forests into which its edges can be partitioned. Equivalently it is the minimum number of spanning forests needed to cover all the edges of the graph.\n\nThe figure shows the complete bipartite graph \"K\", with the colors indicating a partition of its edges into three forests. \"K\" cannot be partitioned into fewer forests, because any forest on its eight vertices has at most seven edges, while the overall graph has sixteen edges, more than double the number of edges in a single forest. Therefore, the arboricity of \"K\" is three.\n\nThe arboricity of a graph is a measure of how dense the graph is: graphs with many edges have high arboricity, and graphs with high arboricity must have a dense subgraph.\n\nIn more detail, as any n-vertex forest has at most n-1 edges, the arboricity of a graph with n vertices and m edges is at least formula_1. Additionally, the subgraphs of any graph cannot have arboricity larger than the graph itself, or equivalently the arboricity of a graph must be at least the maximum arboricity of any of its subgraphs. Nash-Williams proved that these two facts can be combined to characterize arboricity: if we let n and m denote the number of vertices and edges, respectively, of any subgraph S of the given graph, then the arboricity of the graph equals formula_2\n\nAny planar graph with formula_3 vertices has at most formula_4 edges, from which it follows by Nash-Williams' formula that planar graphs have arboricity at most three. Schnyder used a special decomposition of a planar graph into three forests called a Schnyder wood to find a straight-line embedding of any planar graph into a grid of small area.\n\nThe arboricity of a graph can be expressed as a special case of a more general matroid partitioning problem, in which one wishes to express a set of elements of a matroid as a union of a small number of independent sets. As a consequence, the arboricity can be calculated by a polynomial-time algorithm .\n\nThe anarboricity of a graph is the maximum number of edge-disjoint nonacyclic subgraphs into which the edges of the graph can be partitioned.\n\nThe star arboricity of a graph is the size of the minimum forest, each tree of which is a star (tree with at most one non-leaf node), into which the edges of the graph can be partitioned. If a tree is not a star itself, its star arboricity is two, as can be seen by partitioning the edges into two subsets at odd and even distances from the tree root respectively. Therefore, the star arboricity of any graph is at least equal to the arboricity, and at most equal to twice the arboricity.\n\nThe linear arboricity of a graph is the minimum number of linear forests (a collection of paths) into which the edges of the graph can be partitioned. The linear arboricity of a graph is closely related to its maximum degree and its slope number.\n\nThe pseudoarboricity of a graph is the minimum number of pseudoforests into which its edges can be partitioned. Equivalently, it is the maximum ratio of edges to vertices in any subgraph of the graph, rounded up to an integer. As with the arboricity, the pseudoarboricity has a matroid structure allowing it to be computed efficiently .\n\nThe thickness of a graph is the minimum number of planar subgraphs into which its edges can be partitioned. As any planar graph has arboricity three, the thickness of any graph is at least equal to a third of the arboricity, and at most equal to the arboricity.\n\nThe degeneracy of a graph is the maximum, over all induced subgraphs of the graph, of the minimum degree of a vertex in the subgraph. The degeneracy of a graph with arboricity formula_5 is at least equal to formula_5, and at most equal to formula_7. The coloring number of a graph, also known as its Szekeres-Wilf number is always equal to its degeneracy plus 1 .\n\nThe strength of a graph is a fractional value whose integer part gives the maximum number of disjoint spanning trees that can be drawn in a graph. It is the packing problem that is dual to the covering problem raised by the arboricity. The two parameters have been studied together by Tutte and Nash-Williams.\n\nThe fractional arboricity is a refinement of the arboricity, as it is defined for a graph formula_8 as formula_9 In other terms, the arboricity of a graph is the ceiling of the fractional arboricity. \n\nThe (a,b)-decomposability generalizes the arboricity. A graph is formula_10-decomposable if its edges can be partitioned into formula_11 sets, each one of them inducing a forest, except one who induces a graph with maximum degree formula_12. A graph with arboricity formula_5 is formula_14-decomposable.\n\nThe tree number is the minimal number of trees covering the edges of a graph.\n\n"}
{"id": "40118816", "url": "https://en.wikipedia.org/wiki?curid=40118816", "title": "BIT Numerical Mathematics", "text": "BIT Numerical Mathematics\n\nBIT Numerical Mathematics is a quarterly peer-reviewed mathematics journal that covers research in numerical analysis. It was established in 1961 by Carl Erik Fröberg and is published by Springer Science+Business Media. The name \"BIT\" is a reverse acronym of \"Tidskrift för Informationsbehandling\" (Swedish: \"Journal of Information Processing\").\n\nPrevious editors-in-chief have been Carl Erik Fröberg (1961-1992), Åke Björck (1993-2002), Axel Ruhe (2003-2015), and Lars Eldén (2016). Since 2017, the editor-in-chief position is shared between Gunilla Kreiss and Lars Eldén.\n\nThe journal is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 1.670.\n"}
{"id": "14158490", "url": "https://en.wikipedia.org/wiki?curid=14158490", "title": "Chaos computing", "text": "Chaos computing\n\nChaos computing is the idea of using chaotic systems for computation. In particular, chaotic systems can be made to produce all types of logic gates and further allow them to be morphed into each other.\n\nChaotic systems generate large numbers of patterns of behavior and are irregular because they switch between these patterns. They exhibit sensitivity to initial conditions which, in practice, means that chaotic systems can switch between patterns extremely fast.\n\nModern digital computers perform computations based upon digital logic operations implemented at the lowest level as logic gates. There are essentially seven basic logic functions implemented as logic gates: AND, OR, NOT, NAND, NOR, XOR and XNOR.\n\nA chaotic morphing logic gate consists of a generic nonlinear circuit that exhibits chaotic dynamics producing various patterns. A control mechanism is used to select patterns that correspond to different logic gates. The sensitivity to initial conditions is used to switch between different patterns extremely fast (well under a computer clock cycle).\n\nAs an example of how chaotic morphing works, consider a generic chaotic system known as the Logistic map. This nonlinear map is very well studied for its chaotic behavior and its functional representation is given by:\n\nIn this case, the value of is chaotic when >~ 3.57... and rapidly switches between different patterns in the value of as one iterates the value of . A simple threshold controller can control or direct the chaotic map or system to produce one of many patterns. The controller basically sets a threshold on the map such that if the iteration (\"chaotic update\") of the map takes on a value of that lies above a given threshold value, *,then the output corresponds to a 1, otherwise it corresponds to a 0. One can then reverse engineer the chaotic map to establish a lookup table of thresholds that robustly produce any of the logic gate operations. Since the system is chaotic, we can then switch between various gates (\"patterns\") exponentially fast.\n\nThe \"ChaoGate\" is an implementation of a chaotic morphing logic gate developed by the inventor of the technology William Ditto, along with Sudeshna Sinha and K. Murali.\n\nA Chaotic computer, made up of a lattice of ChaoGates, has been demonstrated by Chaologix Inc.\n\nRecent research has shown how chaotic computers can be recruited in Fault Tolerant applications, by introduction of dynamic based fault detection methods. Also it has been demonstrated that multidimensional dynamical states available in a single ChaoGate can be exploited to implement parallel chaos computing, and as an example, this parallel architecture can lead to constructing an SR like memory element through one ChaoGate. As another example, it has been proved that any logic function can be constructed directly from just one ChaoGate.\n\n\n\n"}
{"id": "13328208", "url": "https://en.wikipedia.org/wiki?curid=13328208", "title": "Chicago school (mathematical analysis)", "text": "Chicago school (mathematical analysis)\n\nIn mathematics, the Chicago school of mathematical analysis is a school of thought which emphasizes the applications of Fourier analysis to the study of partial differential equations. Mathematician Antoni Zygmund cofounded the school with his doctoral student Alberto Calderón at the University of Chicago in the 1950s.\n\nIn 1986 Zygmund received the National Medal of Science, in part for his \"creation and leadership of the strongest school of analytical research in the contemporary mathematical world.\"\n\n"}
{"id": "1463083", "url": "https://en.wikipedia.org/wiki?curid=1463083", "title": "Convolution (computer science)", "text": "Convolution (computer science)\n\nIn computer science, specifically formal languages, convolution (sometimes referred to as zip) is a function which maps a tuple of sequences into a sequence of tuples.\n\nGiven the three words \"cat\", \"fish\" and \"be\" where |\"cat\"| is 3, |\"fish\"| is 4 and |\"be\"| is 2. Let formula_1 denote the length of the longest word which is \"fish\"; formula_2. The convolution of \"cat\", \"fish\", \"be\" is then 4 tuples of elements:\n\nwhere \"#\" is a symbol not in the original alphabet. In Haskell this truncates to shortest sequence formula_4, where formula_5:\n\nLet Σ be an alphabet, # a symbol not in Σ.\n\nLet \"x\"\"x\"... \"x\", \"y\"\"y\"... \"y\", \"z\"\"z\"... \"z\", ... be \"n\" words (i.e. finite sequences) of elements of Σ. Let formula_1 denote the length of the longest word, i.e. the maximum of |\"x\"|, |\"y\"|, |\"z\"|, ... .\n\nThe convolution of these words is a finite sequence of \"n\"-tuples of elements of (Σ ∪ {#}), i.e. an element of formula_7:\n\nwhere for any index \"i\" > |\"w\"|, the \"w is #.\n\nThe convolution of \"x, y, z, ...\" is denoted conv( \"x, y, z, ...\"), zip( \"x, y, z, ...\") or \"x\" ⋆ \"y\" ⋆ \"z\" ⋆ ...\n\nThe inverse to convolution is sometimes denoted unzip.\n\nA variation of the convolution operation is defined by:\n\nwhere formula_4 is the \"minimum\" length of the input words. It avoids the use of an adjoined element formula_11, but destroys information about elements of the input sequences beyond formula_4.\n\nConvolution functions are often available in programming languages, often referred to as zip. In Lisp-dialects one can simply map the desired function over the desired lists, map is variadic in Lisp so it can take an arbitrary number of lists as argument. An example from Clojure:\n\nIn Common Lisp:\nLanguages such as Python provide a zip() function, older version (Python 2.*) allowed mapping None over lists to get a similar effect. zip() in conjunction with the * operator unzips a list:\nHaskell has a method of convolving sequences but requires a specific function for each arity (zip for two sequences, zip3 for three etc.), similarly the functions unzip and unzip3 are available for unzipping:\nList of languages by support of convolution:\n\n"}
{"id": "25269252", "url": "https://en.wikipedia.org/wiki?curid=25269252", "title": "Coordinate singularity", "text": "Coordinate singularity\n\nA coordinate singularity occurs when an apparent singularity or discontinuity occurs in one coordinate frame, which can be removed by choosing a different frame. \n\nAn example is the apparent (longitudinal) singularity at the 90 degree latitude in spherical coordinates. An object moving due north (for example, along the line 0 degrees longitude) on the surface of a sphere will suddenly experience an instantaneous change in longitude at the pole (i.e., jumping from longitude 0 to longitude 180 degrees). In fact, longitude is not uniquely defined at the poles. This discontinuity, however, is only apparent; it is an artifact of the coordinate system chosen, which is singular at the poles. A different coordinate system would eliminate the apparent discontinuity, e.g. by replacing the latitude/longitude representation with an -vector representation.\nStephen Hawking aptly summed this up, when once asking the question, \"What lies north of the North Pole?\".\n\n"}
{"id": "1471307", "url": "https://en.wikipedia.org/wiki?curid=1471307", "title": "Counting problem (complexity)", "text": "Counting problem (complexity)\n\nIn computational complexity theory and computability theory, a counting problem is a type of computational problem. If \"R\" is a search problem then \n\nis the corresponding counting function and\n\ndenotes the corresponding counting problem. \n\nNote that \"c is a search problem while #\"R\" is a decision problem, however \"c can be C Cook reduced to #\"R\" (for appropriate C) using a binary search (the reason #\"R\" is defined the way it is, rather than being the graph of \"c, is to make this binary search possible).\n\nIf \"NX\" is a complexity class associated with non-deterministic machines then \"#X\" = {\"#R\" | \"R\" ∈ \"NX\"} is the set of counting problems associated with each search problem in \"NX\". In particular, #P is the class of counting problems associated with NP search problems.\nJust as NP has NP-complete problems via many-one reductions, #P has complete problems via parsimonious reductions, problem transformations that preserve the number of solutions.\n\n\n"}
{"id": "18934432", "url": "https://en.wikipedia.org/wiki?curid=18934432", "title": "Cryptography", "text": "Cryptography\n\nCryptography or cryptology (from \"hidden, secret\"; and \"graphein\", \"to write\", or \"-logia\", \"study\", respectively) is the practice and study of techniques for secure communication in the presence of third parties called adversaries. More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages; various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, electrical engineering, communication science, and physics. Applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords, and military communications.\n\nCryptography prior to the modern age was effectively synonymous with \"encryption\", the conversion of information from a readable state to apparent nonsense. The originator of an encrypted message shares the decoding technique only with intended recipients to preclude access from adversaries. The cryptography literature often uses the name Alice (\"A\") for the sender, Bob (\"B\") for the intended recipient, and Eve (\"eavesdropper\") for the adversary. Since the development of rotor cipher machines in World War I and the advent of computers in World War II, the methods used to carry out cryptology have become increasingly complex and its application more widespread.\n\nModern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in practice by any adversary. It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means. These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in integer factorization algorithms, and faster computing technology require these solutions to be continually adapted. There exist information-theoretically secure schemes that cannot be broken even with unlimited computing power—an example is the one-time pad—but these schemes are more difficult to implement than the best theoretically breakable but computationally secure mechanisms.\n\nThe growth of cryptographic technology has raised a number of legal issues in the information age. Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export. In some jurisdictions where the use of cryptography is legal, laws permit investigators to compel the disclosure of encryption keys for documents relevant to an investigation. Cryptography also plays a major role in digital rights management and copyright infringement of digital media.\n\nThe first use of the term \"cryptograph\" (as opposed to \"cryptogram\") dates back to the 19th century—it originated in \"The Gold-Bug\", a novel by Edgar Allan Poe.\n\nUntil modern times, cryptography referred almost exclusively to \"encryption\", which is the process of converting ordinary information (called plaintext) into unintelligible form (called ciphertext). Decryption is the reverse, in other words, moving from the unintelligible ciphertext back to plaintext. A \"cipher\" (or \"cypher\") is a pair of algorithms that create the encryption and the reversing decryption. The detailed operation of a cipher is controlled both by the algorithm and in each instance by a \"key\". The key is a secret (ideally known only to the communicants), usually a short string of characters, which is needed to decrypt the ciphertext. Formally, a \"cryptosystem\" is the ordered list of elements of finite possible plaintexts, finite possible cyphertexts, finite possible keys, and the encryption and decryption algorithms which correspond to each key. Keys are important both formally and in actual practice, as ciphers without variable keys can be trivially broken with only the knowledge of the cipher used and are therefore useless (or even counter-productive) for most purposes.\n\nHistorically, ciphers were often used directly for encryption or decryption without additional procedures such as authentication or integrity checks. There are two kinds of cryptosystems: symmetric and asymmetric. In symmetric systems the same key (the secret key) is used to encrypt and decrypt a message. Data manipulation in symmetric systems is faster than asymmetric systems as they generally use shorter key lengths. Asymmetric systems use a public key to encrypt a message and a private key to decrypt it. Use of asymmetric systems enhances the security of communication. Examples of asymmetric systems include RSA (Rivest-Shamir-Adleman), and ECC (Elliptic Curve Cryptography). Symmetric models include the commonly used AES (Advanced Encryption Standard) which replaced the older DES (Data Encryption Standard).\n\nIn colloquial use, the term \"code\" is often used to mean any method of encryption or concealment of meaning. However, in cryptography, \"code\" has a more specific meaning. It means the replacement of a unit of plaintext (i.e., a meaningful word or phrase) with a code word (for example, \"wallaby\" replaces \"attack at dawn\").\n\nCryptanalysis is the term used for the study of methods for obtaining the meaning of encrypted information without access to the key normally required to do so; i.e., it is the study of how to crack encryption algorithms or their implementations.\n\nSome use the terms \"cryptography\" and \"cryptology\" interchangeably in English, while others (including US military practice generally) use \"cryptography\" to refer specifically to the use and practice of cryptographic techniques and \"cryptology\" to refer to the combined study of cryptography and cryptanalysis. English is more flexible than several other languages in which \"cryptology\" (done by cryptologists) is always used in the second sense above. advises that steganography is sometimes included in cryptology.\n\nThe study of characteristics of languages that have some application in cryptography or cryptology (e.g. frequency data, letter combinations, universal patterns, etc.) is called cryptolinguistics.\n\nBefore the modern era, cryptography focused on message confidentiality (i.e., encryption)—conversion of messages from a comprehensible form into an incomprehensible one and back again at the other end, rendering it unreadable by interceptors or eavesdroppers without secret knowledge (namely the key needed for decryption of that message). Encryption attempted to ensure secrecy in communications, such as those of spies, military leaders, and diplomats. In recent decades, the field has expanded beyond confidentiality concerns to include techniques for message integrity checking, sender/receiver identity authentication, digital signatures, interactive proofs and secure computation, among others.\n\nThe main classical cipher types are transposition ciphers, which rearrange the order of letters in a message (e.g., 'hello world' becomes 'ehlol owrdl' in a trivially simple rearrangement scheme), and substitution ciphers, which systematically replace letters or groups of letters with other letters or groups of letters (e.g., 'fly at once' becomes 'gmz bu podf' by replacing each letter with the one following it in the Latin alphabet). Simple versions of either have never offered much confidentiality from enterprising opponents. An early substitution cipher was the Caesar cipher, in which each letter in the plaintext was replaced by a letter some fixed number of positions further down the alphabet. Suetonius reports that Julius Caesar used it with a shift of three to communicate with his generals. Atbash is an example of an early Hebrew cipher. The earliest known use of cryptography is some carved ciphertext on stone in Egypt (ca 1900 BCE), but this may have been done for the amusement of literate observers rather than as a way of concealing information.\n\nThe Greeks of Classical times are said to have known of ciphers (e.g., the scytale transposition cipher claimed to have been used by the Spartan military). Steganography (i.e., hiding even the existence of a message so as to keep it confidential) was also first developed in ancient times. An early example, from Herodotus, was a message tattooed on a slave's shaved head and concealed under the regrown hair. More modern examples of steganography include the use of invisible ink, microdots, and digital watermarks to conceal information.\n\nIn India, the 2000-year-old Kamasutra of Vātsyāyana speaks of two different kinds of ciphers called Kautiliyam and Mulavediya. In the Kautiliyam, the cipher letter substitutions are based on phonetic relations, such as vowels becoming consonants. In the Mulavediya, the cipher alphabet consists of pairing letters and using the reciprocal ones.\n\nIn Sassanid Persia, there were two secret scripts, according to the Muslim author Ibn al-Nadim: the \"šāh-dabīrīya\" (literally \"King's script\") which was used for official correspondence, and the \"rāz-saharīya\" which was used to communicate secret messages with other countries.\n\nCiphertexts produced by a classical cipher (and some modern ciphers) will reveal statistical information about the plaintext, and that information can often be used to break the cipher. After the discovery of frequency analysis, perhaps by the Arab mathematician and polymath Al-Kindi (also known as \"Alkindus\") in the 9th century, nearly all such ciphers could be broken by an informed attacker. Such classical ciphers still enjoy popularity today, though mostly as puzzles (see cryptogram). Al-Kindi wrote a book on cryptography entitled \"Risalah fi Istikhraj al-Mu'amma\" (\"Manuscript for the Deciphering Cryptographic Messages\"), which described the first known use of frequency analysis cryptanalysis techniques.\n\nLanguage letter frequencies may offer little help for some extended historical encryption techniques such as homophonic cipher that tend to flatten the frequency distribution. For those ciphers, language letter group (or n-gram) frequencies may provide an attack.\n\nEssentially all ciphers remained vulnerable to cryptanalysis using the frequency analysis technique until the development of the polyalphabetic cipher, most clearly by Leon Battista Alberti around the year 1467, though there is some indication that it was already known to Al-Kindi. Alberti's innovation was to use different ciphers (i.e., substitution alphabets) for various parts of a message (perhaps for each successive plaintext letter at the limit). He also invented what was probably the first automatic cipher device, a wheel which implemented a partial realization of his invention. In the Vigenère cipher, a polyalphabetic cipher, encryption uses a \"key word\", which controls letter substitution depending on which letter of the key word is used. In the mid-19th century Charles Babbage showed that the Vigenère cipher was vulnerable to Kasiski examination, but this was first published about ten years later by Friedrich Kasiski.\n\nAlthough frequency analysis can be a powerful and general technique against many ciphers, encryption has still often been effective in practice, as many a would-be cryptanalyst was unaware of the technique. Breaking a message without using frequency analysis essentially required knowledge of the cipher used and perhaps of the key involved, thus making espionage, bribery, burglary, defection, etc., more attractive approaches to the cryptanalytically uninformed. It was finally explicitly recognized in the 19th century that secrecy of a cipher's algorithm is not a sensible nor practical safeguard of message security; in fact, it was further realized that any adequate cryptographic scheme (including ciphers) should remain secure even if the adversary fully understands the cipher algorithm itself. Security of the key used should alone be sufficient for a good cipher to maintain confidentiality under an attack. This fundamental principle was first explicitly stated in 1883 by Auguste Kerckhoffs and is generally called Kerckhoffs's Principle; alternatively and more bluntly, it was restated by Claude Shannon, the inventor of information theory and the fundamentals of theoretical cryptography, as \"Shannon's Maxim\"—'the enemy knows the system'.\n\nDifferent physical devices and aids have been used to assist with ciphers. One of the earliest may have been the scytale of ancient Greece, a rod supposedly used by the Spartans as an aid for a transposition cipher (see image above). In medieval times, other aids were invented such as the cipher grille, which was also used for a kind of steganography. With the invention of polyalphabetic ciphers came more sophisticated aids such as Alberti's own cipher disk, Johannes Trithemius' tabula recta scheme, and Thomas Jefferson's wheel cypher (not publicly known, and reinvented independently by Bazeries around 1900). Many mechanical encryption/decryption devices were invented early in the 20th century, and several patented, among them rotor machines—famously including the Enigma machine used by the German government and military from the late 1920s and during World War II. The ciphers implemented by better quality examples of these machine designs brought about a substantial increase in cryptanalytic difficulty after WWI.\n\nCryptanalysis of the new mechanical devices proved to be both difficult and laborious. In the United Kingdom, cryptanalytic efforts at Bletchley Park during WWII spurred the development of more efficient means for carrying out repetitious tasks. This culminated in the development of the Colossus, the world's first fully electronic, digital, programmable computer, which assisted in the decryption of ciphers generated by the German Army's Lorenz SZ40/42 machine.\n\nJust as the development of digital computers and electronics helped in cryptanalysis, it made possible much more complex ciphers. Furthermore, computers allowed for the encryption of any kind of data representable in any binary format, unlike classical ciphers which only encrypted written language texts; this was new and significant. Computer use has thus supplanted linguistic cryptography, both for cipher design and cryptanalysis. Many computer ciphers can be characterized by their operation on binary bit sequences (sometimes in groups or blocks), unlike classical and mechanical schemes, which generally manipulate traditional characters (i.e., letters and digits) directly. However, computers have also assisted cryptanalysis, which has compensated to some extent for increased cipher complexity. Nonetheless, good modern ciphers have stayed ahead of cryptanalysis; it is typically the case that use of a quality cipher is very efficient (i.e., fast and requiring few resources, such as memory or CPU capability), while breaking it requires an effort many orders of magnitude larger, and vastly larger than that required for any classical cipher, making cryptanalysis so inefficient and impractical as to be effectively impossible.\n\nExtensive open academic research into cryptography is relatively recent; it began only in the mid-1970s. In recent times, IBM personnel designed the algorithm that became the Federal (i.e., US) Data Encryption Standard; Whitfield Diffie and Martin Hellman published their key agreement algorithm; and the RSA algorithm was published in Martin Gardner's \"Scientific American\" column. Following their work in 1976, it became popular to consider cryptography systems based on mathematical problems that are easy to state but have been found difficult to solve. Since then, cryptography has become a widely used tool in communications, computer networks, and computer security generally. Some modern cryptographic techniques can only keep their keys secret if certain mathematical problems are intractable, such as the integer factorization or the discrete logarithm problems, so there are deep connections with abstract mathematics. There are very few cryptosystems that are proven to be unconditionally secure. The one-time pad is one, and was proven to be so by Claude Shannon. There are a few important algorithms that have been proven secure under certain assumptions. For example, the infeasibility of factoring extremely large integers is the basis for believing that RSA is secure, and some other systems, but even so proof of unbreakability is unavailable since the underlying mathematical problem remains open. In practice, these are widely used, and are believed unbreakable in practice by most competent observers. There are systems similar to RSA, such as one by Michael O. Rabin that are provably secure provided factoring \"n = pq\" is impossible; it is quite unusable in practice. The discrete logarithm problem is the basis for believing some other cryptosystems are secure, and again, there are related, less practical systems that are provably secure relative to the solvability or insolvability discrete log problem.\n\nAs well as being aware of cryptographic history, cryptographic algorithm and system designers must also sensibly consider probable future developments while working on their designs. For instance, continuous improvements in computer processing power have increased the scope of brute-force attacks, so when specifying key lengths, the required key lengths are similarly advancing. The potential effects of quantum computing are already being considered by some cryptographic system designers developing post-quantum cryptography; the announced imminence of small implementations of these machines may be making the need for preemptive caution rather more than merely speculative.\n\nEssentially, prior to the early 20th century, cryptography was chiefly concerned with linguistic and lexicographic patterns. Since then the emphasis has shifted, and cryptography now makes extensive use of mathematics, including aspects of information theory, computational complexity, statistics, combinatorics, abstract algebra, number theory, and finite mathematics generally. Cryptography is also a branch of engineering, but an unusual one since it deals with active, intelligent, and malevolent opposition (see cryptographic engineering and security engineering); other kinds of engineering (e.g., civil or chemical engineering) need deal only with neutral natural forces. There is also active research examining the relationship between cryptographic problems and quantum physics (see quantum cryptography and quantum computer).\n\nThe modern field of cryptography can be divided into several areas of study. The chief ones are discussed here; see Topics in Cryptography for more.\n\nSymmetric-key cryptography refers to encryption methods in which both the sender and receiver share the same key (or, less commonly, in which their keys are different, but related in an easily computable way). This was the only kind of encryption publicly known until June 1976.\n\nSymmetric key ciphers are implemented as either block ciphers or stream ciphers. A block cipher enciphers input in blocks of plaintext as opposed to individual characters, the input form used by a stream cipher.\n\nThe Data Encryption Standard (DES) and the Advanced Encryption Standard (AES) are block cipher designs that have been designated cryptography standards by the US government (though DES's designation was finally withdrawn after the AES was adopted). Despite its deprecation as an official standard, DES (especially its still-approved and much more secure triple-DES variant) remains quite popular; it is used across a wide range of applications, from ATM encryption to e-mail privacy and secure remote access. Many other block ciphers have been designed and released, with considerable variation in quality. Many, even some designed by capable practitioners, have been thoroughly broken, such as FEAL.\n\nStream ciphers, in contrast to the 'block' type, create an arbitrarily long stream of key material, which is combined with the plaintext bit-by-bit or character-by-character, somewhat like the one-time pad. In a stream cipher, the output stream is created based on a hidden internal state that changes as the cipher operates. That internal state is initially set up using the secret key material. RC4 is a widely used stream cipher; see . Block ciphers can be used as stream ciphers; see Block cipher modes of operation.\n\nCryptographic hash functions are a third type of cryptographic algorithm. They take a message of any length as input, and output a short, fixed length hash, which can be used in (for example) a digital signature. For good hash functions, an attacker cannot find two messages that produce the same hash. MD4 is a long-used hash function that is now broken; MD5, a strengthened variant of MD4, is also widely used but broken in practice. The US National Security Agency developed the Secure Hash Algorithm series of MD5-like hash functions: SHA-0 was a flawed algorithm that the agency withdrew; SHA-1 is widely deployed and more secure than MD5, but cryptanalysts have identified attacks against it; the SHA-2 family improves on SHA-1, but it isn't yet widely deployed; and the US standards authority thought it \"prudent\" from a security perspective to develop a new standard to \"significantly improve the robustness of NIST's overall hash algorithm toolkit.\" Thus, a hash function design competition was meant to select a new U.S. national standard, to be called SHA-3, by 2012. The competition ended on October 2, 2012 when the NIST announced that Keccak would be the new SHA-3 hash algorithm. Unlike block and stream ciphers that are invertible, cryptographic hash functions produce a hashed output that cannot be used to retrieve the original input data. Cryptographic hash functions are used to verify the authenticity of data retrieved from an untrusted source or to add a layer of security.\n\nMessage authentication codes (MACs) are much like cryptographic hash functions, except that a secret key can be used to authenticate the hash value upon receipt; this additional complication blocks an attack scheme against bare digest algorithms, and so has been thought worth the effort.\n\nSymmetric-key cryptosystems use the same key for encryption and decryption of a message, although a message or group of messages can have a different key than others. A significant disadvantage of symmetric ciphers is the key management necessary to use them securely. Each distinct pair of communicating parties must, ideally, share a different key, and perhaps for each ciphertext exchanged as well. The number of keys required increases as the square of the number of network members, which very quickly requires complex key management schemes to keep them all consistent and secret. The difficulty of securely establishing a secret key between two communicating parties, when a secure channel does not already exist between them, also presents a chicken-and-egg problem which is a considerable practical obstacle for cryptography users in the real world.\nIn a groundbreaking 1976 paper, Whitfield Diffie and Martin Hellman proposed the notion of \"public-key\" (also, more generally, called \"asymmetric key\") cryptography in which two different but mathematically related keys are used—a \"public\" key and a \"private\" key. A public key system is so constructed that calculation of one key (the 'private key') is computationally infeasible from the other (the 'public key'), even though they are necessarily related. Instead, both keys are generated secretly, as an interrelated pair. The historian David Kahn described public-key cryptography as \"the most revolutionary new concept in the field since polyalphabetic substitution emerged in the Renaissance\".\n\nIn public-key cryptosystems, the public key may be freely distributed, while its paired private key must remain secret. In a public-key encryption system, the \"public key\" is used for encryption, while the \"private\" or \"secret key\" is used for decryption. While Diffie and Hellman could not find such a system, they showed that public-key cryptography was indeed possible by presenting the Diffie–Hellman key exchange protocol, a solution that is now widely used in secure communications to allow two parties to secretly agree on a shared encryption key.\n\nDiffie and Hellman's publication sparked widespread academic efforts in finding a practical public-key encryption system. This race was finally won in 1978 by Ronald Rivest, Adi Shamir, and Len Adleman, whose solution has since become known as the RSA algorithm.\n\nThe Diffie–Hellman and RSA algorithms, in addition to being the first publicly known examples of high quality public-key algorithms, have been among the most widely used. Other include the Cramer–Shoup cryptosystem, ElGamal encryption, and various elliptic curve techniques.\n\nTo much surprise, a document published in 1997 by the Government Communications Headquarters (GCHQ), a British intelligence organization, revealed that cryptographers at GCHQ had anticipated several academic developments. Reportedly, around 1970, James H. Ellis had conceived the principles of asymmetric key cryptography. In 1973, Clifford Cocks invented a solution that essentially resembles the RSA algorithm. And in 1974, Malcolm J. Williamson is claimed to have developed the Diffie–Hellman key exchange.\nPublic-key cryptography can also be used for implementing digital signature schemes. A digital signature is reminiscent of an ordinary signature; they both have the characteristic of being easy for a user to produce, but difficult for anyone else to forge. Digital signatures can also be permanently tied to the content of the message being signed; they cannot then be 'moved' from one document to another, for any attempt will be detectable. In digital signature schemes, there are two algorithms: one for \"signing\", in which a secret key is used to process the message (or a hash of the message, or both), and one for \"verification\", in which the matching public key is used with the message to check the validity of the signature. RSA and DSA are two of the most popular digital signature schemes. Digital signatures are central to the operation of public key infrastructures and many network security schemes (e.g., SSL/TLS, many VPNs, etc.).\n\nPublic-key algorithms are most often based on the computational complexity of \"hard\" problems, often from number theory. For example, the hardness of RSA is related to the integer factorization problem, while Diffie–Hellman and DSA are related to the discrete logarithm problem. More recently, elliptic curve cryptography has developed, a system in which security is based on number theoretic problems involving elliptic curves. Because of the difficulty of the underlying problems, most public-key algorithms involve operations such as modular multiplication and exponentiation, which are much more computationally expensive than the techniques used in most block ciphers, especially with typical key sizes. As a result, public-key cryptosystems are commonly hybrid cryptosystems, in which a fast high-quality symmetric-key encryption algorithm is used for the message itself, while the relevant symmetric key is sent with the message, but encrypted using a public-key algorithm. Similarly, hybrid signature schemes are often used, in which a cryptographic hash function is computed, and only the resulting hash is digitally signed.\n\nThe goal of cryptanalysis is to find some weakness or insecurity in a cryptographic scheme, thus permitting its subversion or evasion.\n\nIt is a common misconception that every encryption method can be broken. In connection with his WWII work at Bell Labs, Claude Shannon proved that the one-time pad cipher is unbreakable, provided the key material is truly random, never reused, kept secret from all possible attackers, and of equal or greater length than the message. Most ciphers, apart from the one-time pad, can be broken with enough computational effort by brute force attack, but the amount of effort needed may be exponentially dependent on the key size, as compared to the effort needed to make use of the cipher. In such cases, effective security could be achieved if it is proven that the effort required (i.e., \"work factor\", in Shannon's terms) is beyond the ability of any adversary. This means it must be shown that no efficient method (as opposed to the time-consuming brute force method) can be found to break the cipher. Since no such proof has been found to date, the one-time-pad remains the only theoretically unbreakable cipher.\n\nThere are a wide variety of cryptanalytic attacks, and they can be classified in any of several ways. A common distinction turns on what Eve (an attacker) knows and what capabilities are available. In a ciphertext-only attack, Eve has access only to the ciphertext (good modern cryptosystems are usually effectively immune to ciphertext-only attacks). In a known-plaintext attack, Eve has access to a ciphertext and its corresponding plaintext (or to many such pairs). In a chosen-plaintext attack, Eve may choose a plaintext and learn its corresponding ciphertext (perhaps many times); an example is gardening, used by the British during WWII. In a chosen-ciphertext attack, Eve may be able to \"choose\" ciphertexts and learn their corresponding plaintexts. Finally in a man-in-the-middle attack Eve gets in between Alice (the sender) and Bob (the recipient), accesses and modifies the traffic and then forwards it to the recipient. Also important, often overwhelmingly so, are mistakes (generally in the design or use of one of the protocols involved; see Cryptanalysis of the Enigma for some historical examples of this).\n\nCryptanalysis of symmetric-key ciphers typically involves looking for attacks against the block ciphers or stream ciphers that are more efficient than any attack that could be against a perfect cipher. For example, a simple brute force attack against DES requires one known plaintext and 2 decryptions, trying approximately half of the possible keys, to reach a point at which chances are better than even that the key sought will have been found. But this may not be enough assurance; a linear cryptanalysis attack against DES requires 2 known plaintexts and approximately 2 DES operations. This is a considerable improvement on brute force attacks.\n\nPublic-key algorithms are based on the computational difficulty of various problems. The most famous of these is integer factorization (e.g., the RSA algorithm is based on a problem related to integer factoring), but the discrete logarithm problem is also important. Much public-key cryptanalysis concerns numerical algorithms for solving these computational problems, or some of them, efficiently (i.e., in a practical time). For instance, the best known algorithms for solving the elliptic curve-based version of discrete logarithm are much more time-consuming than the best known algorithms for factoring, at least for problems of more or less equivalent size. Thus, other things being equal, to achieve an equivalent strength of attack resistance, factoring-based encryption techniques must use larger keys than elliptic curve techniques. For this reason, public-key cryptosystems based on elliptic curves have become popular since their invention in the mid-1990s.\n\nWhile pure cryptanalysis uses weaknesses in the algorithms themselves, other attacks on cryptosystems are based on actual use of the algorithms in real devices, and are called \"side-channel attacks\". If a cryptanalyst has access to, for example, the amount of time the device took to encrypt a number of plaintexts or report an error in a password or PIN character, he may be able to use a timing attack to break a cipher that is otherwise resistant to analysis. An attacker might also study the pattern and length of messages to derive valuable information; this is known as traffic analysis and can be quite useful to an alert adversary. Poor administration of a cryptosystem, such as permitting too short keys, will make any system vulnerable, regardless of other virtues. Social engineering and other attacks against the personnel who work with cryptosystems or the messages they handle (e.g., bribery, extortion, blackmail, espionage, torture, ...) may be the most productive attacks of all.\n\nMuch of the theoretical work in cryptography concerns cryptographic \"primitives\"—algorithms with basic cryptographic properties—and their relationship to other cryptographic problems. More complicated cryptographic tools are then built from these basic primitives. These primitives provide fundamental properties, which are used to develop more complex tools called \"cryptosystems\" or \"cryptographic protocols\", which guarantee one or more high-level security properties. Note however, that the distinction between cryptographic \"primitives\" and cryptosystems, is quite arbitrary; for example, the RSA algorithm is sometimes considered a cryptosystem, and sometimes a primitive. Typical examples of cryptographic primitives include pseudorandom functions, one-way functions, etc.\n\nOne or more cryptographic primitives are often used to develop a more complex algorithm, called a cryptographic system, or \"cryptosystem\". Cryptosystems (e.g., El-Gamal encryption) are designed to provide particular functionality (e.g., public key encryption) while guaranteeing certain security properties (e.g., chosen-plaintext attack (CPA) security in the random oracle model). Cryptosystems use the properties of the underlying cryptographic primitives to support the system's security properties. As the distinction between primitives and cryptosystems is somewhat arbitrary, a sophisticated cryptosystem can be derived from a combination of several more primitive cryptosystems. In many cases, the cryptosystem's structure involves back and forth communication among two or more parties in space (e.g., between the sender of a secure message and its receiver) or across time (e.g., cryptographically protected backup data). Such cryptosystems are sometimes called \"cryptographic protocols\".\n\nSome widely known cryptosystems include RSA encryption, Schnorr signature, El-Gamal encryption, PGP, etc. More complex cryptosystems include electronic cash systems, signcryption systems, etc. Some more 'theoretical' cryptosystems include interactive proof systems, (like zero-knowledge proofs), systems for secret sharing, etc.\n\nUntil recently most security properties of most cryptosystems were demonstrated using empirical techniques or using ad hoc reasoning. Recently there has been considerable effort to develop formal techniques for establishing the security of cryptosystems; this has been generally called \"provable security\". The general idea of provable security is to give arguments about the computational difficulty needed to compromise some security aspect of the cryptosystem (i.e., to any adversary).\n\nThe study of how best to implement and integrate cryptography in software applications is itself a distinct field (see Cryptographic engineering and Security engineering).\n\nCryptography has long been of interest to intelligence gathering and law enforcement agencies. Secret communications may be criminal or even treasonous . Because of its facilitation of privacy, and the diminution of privacy attendant on its prohibition, cryptography is also of considerable interest to civil rights supporters. Accordingly, there has been a history of controversial legal issues surrounding cryptography, especially since the advent of inexpensive computers has made widespread access to high quality cryptography possible.\n\nIn some countries, even the domestic use of cryptography is, or has been, restricted. Until 1999, France significantly restricted the use of cryptography domestically, though it has since relaxed many of these rules. In China and Iran, a license is still required to use cryptography. Many countries have tight restrictions on the use of cryptography. Among the more restrictive are laws in Belarus, Kazakhstan, Mongolia, Pakistan, Singapore, Tunisia, and Vietnam.\n\nIn the United States, cryptography is legal for domestic use, but there has been much conflict over legal issues related to cryptography. One particularly important issue has been the export of cryptography and cryptographic software and hardware. Probably because of the importance of cryptanalysis in World War II and an expectation that cryptography would continue to be important for national security, many Western governments have, at some point, strictly regulated export of cryptography. After World War II, it was illegal in the US to sell or distribute encryption technology overseas; in fact, encryption was designated as auxiliary military equipment and put on the United States Munitions List. Until the development of the personal computer, asymmetric key algorithms (i.e., public key techniques), and the Internet, this was not especially problematic. However, as the Internet grew and computers became more widely available, high-quality encryption techniques became well known around the globe.\n\nIn the 1990s, there were several challenges to US export regulation of cryptography. After the source code for Philip Zimmermann's Pretty Good Privacy (PGP) encryption program found its way onto the Internet in June 1991, a complaint by RSA Security (then called RSA Data Security, Inc.) resulted in a lengthy criminal investigation of Zimmermann by the US Customs Service and the FBI, though no charges were ever filed. Daniel J. Bernstein, then a graduate student at UC Berkeley, brought a lawsuit against the US government challenging some aspects of the restrictions based on free speech grounds. The 1995 case Bernstein v. United States ultimately resulted in a 1999 decision that printed source code for cryptographic algorithms and systems was protected as free speech by the United States Constitution.\n\nIn 1996, thirty-nine countries signed the Wassenaar Arrangement, an arms control treaty that deals with the export of arms and \"dual-use\" technologies such as cryptography. The treaty stipulated that the use of cryptography with short key-lengths (56-bit for symmetric encryption, 512-bit for RSA) would no longer be export-controlled. Cryptography exports from the US became less strictly regulated as a consequence of a major relaxation in 2000; there are no longer very many restrictions on key sizes in US-exported mass-market software. Since this relaxation in US export restrictions, and because most personal computers connected to the Internet include US-sourced web browsers such as Firefox or Internet Explorer, almost every Internet user worldwide has potential access to quality cryptography via their browsers (e.g., via Transport Layer Security). The Mozilla Thunderbird and Microsoft Outlook E-mail client programs similarly can transmit and receive emails via TLS, and can send and receive email encrypted with S/MIME. Many Internet users don't realize that their basic application software contains such extensive cryptosystems. These browsers and email programs are so ubiquitous that even governments whose intent is to regulate civilian use of cryptography generally don't find it practical to do much to control distribution or use of cryptography of this quality, so even when such laws are in force, actual enforcement is often effectively impossible.\n\nAnother contentious issue connected to cryptography in the United States is the influence of the National Security Agency on cipher development and policy. The NSA was involved with the design of DES during its development at IBM and its consideration by the National Bureau of Standards as a possible Federal Standard for cryptography. DES was designed to be resistant to differential cryptanalysis, a powerful and general cryptanalytic technique known to the NSA and IBM, that became publicly known only when it was rediscovered in the late 1980s. According to Steven Levy, IBM discovered differential cryptanalysis, but kept the technique secret at the NSA's request. The technique became publicly known only when Biham and Shamir re-discovered and announced it some years later. The entire affair illustrates the difficulty of determining what resources and knowledge an attacker might actually have.\n\nAnother instance of the NSA's involvement was the 1993 Clipper chip affair, an encryption microchip intended to be part of the Capstone cryptography-control initiative. Clipper was widely criticized by cryptographers for two reasons. The cipher algorithm (called Skipjack) was then classified (declassified in 1998, long after the Clipper initiative lapsed). The classified cipher caused concerns that the NSA had deliberately made the cipher weak in order to assist its intelligence efforts. The whole initiative was also criticized based on its violation of Kerckhoffs's Principle, as the scheme included a special escrow key held by the government for use by law enforcement, for example in wiretaps.\n\nCryptography is central to digital rights management (DRM), a group of techniques for technologically controlling use of copyrighted material, being widely implemented and deployed at the behest of some copyright holders. In 1998, U.S. President Bill Clinton signed the Digital Millennium Copyright Act (DMCA), which criminalized all production, dissemination, and use of certain cryptanalytic techniques and technology (now known or later discovered); specifically, those that could be used to circumvent DRM technological schemes. This had a noticeable impact on the cryptography research community since an argument can be made that \"any\" cryptanalytic research violated, or might violate, the DMCA. Similar statutes have since been enacted in several countries and regions, including the implementation in the EU Copyright Directive. Similar restrictions are called for by treaties signed by World Intellectual Property Organization member-states.\n\nThe United States Department of Justice and FBI have not enforced the DMCA as rigorously as had been feared by some, but the law, nonetheless, remains a controversial one. Niels Ferguson, a well-respected cryptography researcher, has publicly stated that he will not release some of his research into an Intel security design for fear of prosecution under the DMCA. Cryptanalyst Bruce Schneier has argued that the DMCA encourages vendor lock-in, while inhibiting actual measures toward cyber-security. Both Alan Cox (longtime Linux kernel developer) and Edward Felten (and some of his students at Princeton) have encountered problems related to the Act. Dmitry Sklyarov was arrested during a visit to the US from Russia, and jailed for five months pending trial for alleged violations of the DMCA arising from work he had done in Russia, where the work was legal. In 2007, the cryptographic keys responsible for Blu-ray and HD DVD content scrambling were discovered and released onto the Internet. In both cases, the MPAA sent out numerous DMCA takedown notices, and there was a massive Internet backlash triggered by the perceived impact of such notices on fair use and free speech.\n\nIn the United Kingdom, the Regulation of Investigatory Powers Act gives UK police the powers to force suspects to decrypt files or hand over passwords that protect encryption keys. Failure to comply is an offense in its own right, punishable on conviction by a two-year jail sentence or up to five years in cases involving national security. Successful prosecutions have occurred under the Act; the first, in 2009, resulted in a term of 13 months' imprisonment. Similar forced disclosure laws in Australia, Finland, France, and India compel individual suspects under investigation to hand over encryption keys or passwords during a criminal investigation.\n\nIn the United States, the federal criminal case of United States v. Fricosu addressed whether a search warrant can compel a person to reveal an encryption passphrase or password. The Electronic Frontier Foundation (EFF) argued that this is a violation of the protection from self-incrimination given by the Fifth Amendment. In 2012, the court ruled that under the All Writs Act, the defendant was required to produce an unencrypted hard drive for the court.\n\nIn many jurisdictions, the legal status of forced disclosure remains unclear.\n\nThe 2016 FBI–Apple encryption dispute concerns the ability of courts in the United States to compel manufacturers' assistance in unlocking cell phones whose contents are cryptographically protected.\n\nAs a potential counter-measure to forced disclosure some cryptographic software supports plausible deniability, where the encrypted data is indistinguishable from unused random data (for example such as that of a drive which has been securely wiped).\n\n\n\n"}
{"id": "6233627", "url": "https://en.wikipedia.org/wiki?curid=6233627", "title": "Double counting (fallacy)", "text": "Double counting (fallacy)\n\nDouble counting is a fallacy in which, when counting events or occurrences in probability or in other areas, a solution counts events two or more times, resulting in an erroneous number of events or occurrences which is higher than the true result. This results in the calculated sum of probabilities for all possible outcomes to be higher than 100%, which is impossible.\n\nFor example, what is the probability of seeing at least one 5 when throwing a pair of dice? An erroneous argument goes as follows: The first die shows a 5 with probability 1/6; the second die shows a 5 with probability 1/6; therefore the probability of seeing a 5 on at least one of the dice is 1/6 + 1/6 = 1/3 = 12/36. However, the correct answer is 11/36, because the erroneous argument has double-counted the event where both dice show 5s.\n\nIn mathematical terms, the previous example calculated the probability of P(A or B) as P(A)+P(B). However, by the inclusion-exclusion principle, P(A or B) = P(A) + P(B) - P(A and B). The principle is used to compensate for double counting by subtracting those objects which were double counted.\n\nAnother example is made in the joke where a man explains to his boss why he has to be an hour late to work every day:\n\nAll of the numbers are correct, but the man is counting them incorrectly. Sleeping, bathing and eating are part of the weekends, holidays and vacation time that are also being included, making these hours double counted.\n\n"}
{"id": "18380182", "url": "https://en.wikipedia.org/wiki?curid=18380182", "title": "Essential range", "text": "Essential range\n\nIn mathematics, particularly measure theory, the essential range of a function is intuitively the 'non-negligible' range of the function: It does not change between two functions that are equal almost everywhere. One way of thinking of the essential range of a function is the set on which the range of the function is most 'concentrated'. The essential range can be defined for measurable real or complex-valued functions on a measure space.\n\nLet \"f\" be a Borel-measurable, complex-valued function defined on a measure space formula_1. Then the essential range of \"f\" is defined to be the set:\n\nIn other words: The essential range of a complex-valued function is the set of all complex numbers \"z\" such that the inverse image of each ε-neighbourhood of \"z\" under \"f\" has positive measure.\n\n\n\n"}
{"id": "10456890", "url": "https://en.wikipedia.org/wiki?curid=10456890", "title": "Euclid–Mullin sequence", "text": "Euclid–Mullin sequence\n\nThe Euclid–Mullin sequence is an infinite sequence of distinct prime numbers, in which each element is the least prime factor of one plus the product of all earlier elements. They are named after the ancient Greek mathematician Euclid, because their definition relies on an idea in Euclid's proof that there are infinitely many primes, and after Albert A. Mullin, who asked about the sequence in 1963.\n\nThe first 51 elements of the sequence are\n\nThese are the only known elements . Finding the next one requires finding the least prime factor of a 335-digit number (which is known to be composite).\n\nThe formula_1th element of the sequence, formula_2, is the least prime factor of\n\nThe first element is therefore the least prime factor of the empty product plus one, which is 2. The element 13 in the sequence is the least prime factor of 2 × 3 × 7 × 43 + 1 = 1806 + 1 = 1807 = 13 × 139.\n\nThe sequence is infinitely long and does not contain repeated elements. This can be proved using the method of Euclid's proof that there are infinitely many primes. That proof is constructive, and the sequence is the result of performing a version of that construction.\n\n asked whether every prime number appears in the Euclid–Mullin sequence and, if not, whether the problem of testing a given prime for membership in the sequence is computable. conjectured, on the basis of heuristic assumptions that the distribution of primes is random, that every prime does appear in the sequence.\nHowever, although similar recursive sequences over other domains do not contain all primes,\nthese problems both remain open for the original Euclid–Mullin sequence. The least prime number not known to be an element of the sequence is 41.\n\nThe positions of the prime numbers from 2 to 97 are:\nwhere ? indicates that the position (or whether it exists at all) is unknown as of 2012.\n\nA related sequence of numbers determined by the largest prime factor of one plus the product of the previous numbers (rather than the smallest prime factor) is also known as the Euclid–Mullin sequence. It grows more quickly, but is not monotonic. The numbers in this sequence are\nNot every prime number appears in this sequence, and the sequence of missing primes,\nhas been proven to be infinite.\n\nIt is also possible to generate modified versions of the Euclid–Mullin sequence by using the same rule of choosing the smallest prime factor at each step, but beginning with a different prime than 2.\n\nAlternatively, taking each number to be one plus the product of the previous numbers (rather than factoring it) gives Sylvester's sequence. The sequence constructed by repeatedly appending all factors of one plus the product of the previous numbers is the same as the sequence of prime factors of Sylvester's sequence. Like the Euclid–Mullin sequence, this is a non-monotonic sequence of primes, but it is known not to include all primes.\n\n"}
{"id": "332256", "url": "https://en.wikipedia.org/wiki?curid=332256", "title": "Gabriel's Horn", "text": "Gabriel's Horn\n\nGabriel's horn (also called Torricelli's trumpet) is a geometric figure which has infinite surface area but finite volume. The name refers to the biblical tradition identifying the Archangel Gabriel as the angel who blows the horn to announce Judgment Day, associating the divine, or infinite, with the finite. The properties of this figure were first studied by Italian physicist and mathematician Evangelista Torricelli in the 17th century.\n\nGabriel's horn is formed by taking the graph of\nwith the domain (thus avoiding the asymptote at ) and rotating it in three dimensions about the -axis. The discovery was made using Cavalieri's principle before the invention of calculus, but today calculus can be used to calculate the volume and surface area of the horn between and , where . Using integration (see Solid of revolution and Surface of revolution for details), it is possible to find the volume and the surface area :\n\nThe surface area formula above gives a lower bound for the area as 2 times the natural logarithm of . There is no upper bound for the natural logarithm of as approaches infinity. That means, in this case, that the horn has an infinite surface area. That is to say;\n\nWhen the properties of Gabriel's horn were discovered, the fact that the rotation of an infinitely large section of the -plane about the -axis generates an object of finite volume was considered paradoxical. While the section lying in the -plane has an infinite area, any other section parallel to it has a finite area. Thus the volume, being calculated from the 'weighted sum' of sections, is finite.\n\nAnother approach is to treat the horn as a stack of disks with diminishing radii. The sum of the radii produces a harmonic series that goes to infinity. However, the correct calculation is the sum of their squares. Every disk has a radius and an area or . The series diverges but converges. In general, for any real , converges.\n\nThe apparent paradox formed part of a dispute over the nature of infinity involving many of the key thinkers of the time including Thomas Hobbes, John Wallis and Galileo Galilei.\n\nThere is a similar phenomenon which applies to lengths and areas in the plane. The area between the curves and from 1 to infinity is finite, but the lengths of the two curves are clearly infinite.\n\nSince the horn has finite volume but infinite surface area, there is an apparent paradox that the horn could be filled with a finite quantity of paint, and yet that paint would not be sufficient to coat its inner surface. The \"paradox\" is resolved by realizing that a finite amount of paint can in fact coat an infinite surface area — it simply needs to get thinner at a fast enough rate. (Much like the series gets smaller fast enough that its sum is finite.) In the case where the horn is filled with paint, this thinning is accomplished by the increasing reduction in diameter of the throat of the horn.\n\nThe converse phenomenon of Gabriel's horn – a surface of revolution that has a \"finite\" surface area but an \"infinite\" volume – cannot occur:\n\nLet be a continuously differentiable function. Write for the solid of revolution of the graph about the -axis. \"If the surface area of is finite, then so is the volume.\"\n\nSince the lateral surface area is finite, the limit superior:\nTherefore, there exists a such that the supremum } is finite. Hence,\nFinally, the volume:\nTherefore: \"if the area is finite, then the volume must also be finite.\"\n\n\n\n"}
{"id": "393115", "url": "https://en.wikipedia.org/wiki?curid=393115", "title": "Generalized Gauss–Bonnet theorem", "text": "Generalized Gauss–Bonnet theorem\n\nIn mathematics, the generalized Gauss–Bonnet theorem (also called Chern–Gauss–Bonnet theorem after Shiing-Shen Chern, Carl Friedrich Gauss and Pierre Ossian Bonnet) presents the Euler characteristic of a closed even-dimensional Riemannian manifold as an integral of a certain polynomial derived from its curvature. It is a direct generalization of the Gauss–Bonnet theorem (named after Carl Friedrich Gauss and Pierre Ossian Bonnet) to higher dimensions.\n\nLet \"M\" be a compact orientable 2\"n\"-dimensional Riemannian manifold without boundary, and let formula_1 be the curvature form of the Levi-Civita connection. This means that formula_1 is an formula_3-valued 2-form on \"M\". So formula_1 can be regarded as a skew-symmetric 2\"n\" × 2\"n\" matrix whose entries are 2-forms, so it is a matrix over the commutative ring formula_5. One may therefore take the Pfaffian formula_6, which turns out to be a 2\"n\"-form.\n\nThe generalized Gauss–Bonnet theorem states that\nwhere formula_8 denotes the Euler characteristic of \"M\".\n\nIn dimension formula_9, for a compact oriented manifold, we get\n\nwhere formula_11 is the full Riemann curvature tensor, formula_12 is the Ricci curvature tensor, and formula_13 is the scalar curvature.\n\nAs with the two-dimensional Gauss–Bonnet theorem, there are generalizations when \"M\" is a manifold with boundary.\n\nThe Gauss–Bonnet theorem can be seen as a special instance in the theory of characteristic classes. The Gauss–Bonnet integrand is the Euler class. Since it is a top-dimensional differential form, it is closed. The naturality of the Euler class means that when you change the Riemannian metric, you stay in the same cohomology class. That means that the integral of the Euler class remains constant as you vary the metric, and so is an invariant of smooth structure.\n\nA far-reaching generalization of the Gauss–Bonnet theorem is the Atiyah–Singer Index Theorem. Let formula_14 be a (weakly) elliptic differential operator between vector bundles. That means that the principal symbol is an isomorphism. (Strong ellipticity would furthermore require the symbol to be positive-definite.) Let formula_15 be the adjoint operator. Then the index is defined as \"dim(ker(D))-dim(ker(D*))\", and by ellipticity is always finite. The Index Theorem states that this \"analytical index\" is constant as you vary the elliptic operator smoothly. It is in fact equal to a \"topological index\", which can be expressed in terms of characteristic classes. The 2-dimensional Gauss–Bonnet theorem arises as the special case where the topological index is defined in terms of Betti numbers and the analytical index is defined in terms of the Gauss–Bonnet integrand.\n\n\n"}
{"id": "16351599", "url": "https://en.wikipedia.org/wiki?curid=16351599", "title": "George Abram Miller", "text": "George Abram Miller\n\nGeorge Abram Miller (31 July 1863 – 10 February 1951) was an early group theorist. Much of his work consisted of classifying groups satisfying some condition, such as having a small number of prime divisors or small order or having a small permutation representation; although such results were considered important by his contemporaries they have mostly been rendered obsolete by modern computer algebra systems. He was president of the Mathematical Association of America 1921–1922 and was an Invited Speaker of the ICM in 1924 in Toronto. Miller's \"Collected Works\" were edited by Henry Roy Brahana and published by University of Illinois Press, the first two volumes appearing in 1935 and 1939. The final three volumes were published in 1946, 1955, and 1959. His doctoral students include H. L. Rietz.\n\n\n"}
{"id": "676328", "url": "https://en.wikipedia.org/wiki?curid=676328", "title": "Graph homomorphism", "text": "Graph homomorphism\n\nIn the mathematical field of graph theory, a graph homomorphism is a mapping between two graphs that respects their structure. More concretely, it is a function between the vertex sets of two graphs that maps adjacent vertices to adjacent vertices.\n\nHomomorphisms generalize various notions of graph colorings and allow the expression of an important class of constraint satisfaction problems, such as certain scheduling or frequency assignment problems.\nThe fact that homomorphisms can be composed leads to rich algebraic structures: a preorder on graphs, a distributive lattice, and a category (one for undirected graphs and one for directed graphs).\nThe computational complexity of finding a homomorphism between given graphs is prohibitive in general, but a lot is known about special cases that are solvable in polynomial time. Boundaries between tractable and intractable cases have been an active area of research.\n\nIn this article, unless stated otherwise, \"graphs\" are finite, undirected graphs with loops allowed, but multiple edges (parallel edges) disallowed.\nA graph homomorphism \"f\"  from a graph \"G\" = (\"V\"(\"G\"), \"E\"(\"G\")) to a graph \"H\" = (\"V\"(\"H\"), \"E\"(\"H\")), written\nis a function from \"V\"(\"G\") to \"V\"(\"H\") that maps endpoints of each edge in \"G\" to endpoints of an edge in \"H\". Formally, {\"u\",\"v\"} ∈ \"E\"(\"G\") implies {\"f\"(\"u\"),\"f\"(\"v\")} ∈ \"E\"(\"H\"), for all pairs of vertices \"u\", \"v\" in \"V\"(\"G\").\nIf there exists any homomorphism from \"G\" to \"H\", then \"G\" is said to be homomorphic to \"H\" or \"H\"-colorable. This is often denoted as just:\n\nThe above definition is extended to directed graphs. Then, for a homomorphism \"f\" : \"G\" → \"H\", (\"f\"(\"u\"),\"f\"(\"v\")) is an arc (directed edge) of \"H\" whenever (\"u\",\"v\") is an arc of \"G\".\n\nThere is an injective homomorphism from \"G\" to \"H\" (i.e., one that never maps distinct vertices to one vertex) if and only if \"G\" is a subgraph of \"H\".\nIf a homomorphism \"f\" : \"G\" → \"H\" is a bijection (a one-to-one correspondence between vertices of \"G\" and \"H\") whose inverse function is also a graph homomorphism, then \"f\" is a graph isomorphism.\n\nCovering maps are a special kind of homomorphisms that mirror the definition and many properties of covering maps in topology.\nThey are defined as surjective homomorphisms (i.e., something maps to each vertex) that are also locally bijective, that is, a bijection on the neighbourhood of each vertex.\nAn example is the bipartite double cover, formed from a graph by splitting each vertex \"v\" into \"v\" and \"v\" and replacing each edge \"u\",\"v\" with edges \"u\",\"v\" and \"v\",\"u\". The function mapping \"v\" and \"v\" in the cover to \"v\" in the original graph is a homomorphism and a covering map.\n\nGraph homeomorphism is a different notion, not related directly to homomorphisms. Roughly speaking, it requires injectivity, but allows mapping edges to paths (not just to edges). Graph minors are a still more relaxed notion.\n\nTwo graphs \"G\" and \"H\" are homomorphically equivalent if\n\"G\" → \"H\" and \"H\" → \"G\".\n\nA retraction is a homomorphism \"r\" from a graph \"G\" to a subgraph \"H\" of \"G\" such that \"r\"(\"v\") = \"v\" for each vertex \"v\" of \"H\".\nIn this case the subgraph \"H\" is called a retract of \"G\".\n\nA core is a graph with no homomorphism to any proper subgraph. Equivalently, a core can be defined as a graph that does not retract to any proper subgraph.\nEvery graph \"G\" is homomorphically equivalent to a unique core (up to isomorphism), called \"the core\" of \"G\". Notably, this is not true in general for infinite graphs.\nHowever, the same definitions apply to directed graphs and a directed graph is also equivalent to a unique core.\nEvery graph and every directed graph contains its core as a retract and as an induced subgraph.\n\nFor example, all complete graphs \"K\" and all odd cycles (cycle graphs of odd length) are cores.\nEvery 3-colorable graph \"G\" that contains a triangle (that is, has the complete graph \"K\" as a subgraph) is homomorphically equivalent to \"K\". This is because, on one hand, a 3-coloring of \"G\" is the same as a homomorphism \"G\" → \"K\", as explained below. On the other hand, every subgraph of \"G\" trivially admits a homomorphism into \"G\", implying \"K\" → \"G\". This also means that \"K\" is the core of any such graph \"G\". Similarly, every bipartite graph that has at least one edge is equivalent to \"K\".\n\nA \"k\"-coloring, for some integer \"k\", is an assignment of one of \"k\" colors to each vertex of a graph \"G\" such that the endpoints of each edge get different colors. The \"k\"-colorings of \"G\" correspond exactly to homomorphisms from \"G\" to the complete graph \"K\". Indeed, the vertices of \"K\" correspond to the \"k\" colors, and two colors are adjacent as vertices of \"K\" if and only if they are different. Hence a function defines a homomorphism to \"K\" if and only if it maps adjacent vertices of \"G\" to different colors (i.e., it is a \"k\"-coloring). In particular, \"G\" is \"k\"-colorable if and only if it is \"K\"-colorable.\n\nIf there are two homomorphisms \"G\" → \"H\" and \"H\" → \"K\", then their composition \"G\" → \"K\" is also a homomorphism. In other words, if a graph \"H\" can be colored with \"k\" colors, and there is a homomorphism from \"G\" to \"H\", then \"G\" can also be \"k\"-colored. Therefore, \"G\" → \"H\" implies χ(\"G\") ≤ χ(\"H\"), where \"χ\" denotes the chromatic number of a graph (the least \"k\" for which it is \"k\"-colorable).\n\nGeneral homomorphisms can also be thought of as a kind of coloring: if the vertices of a fixed graph \"H\" are the available \"colors\" and edges of \"H\" describe which colors are \"compatible\", then an \"H\"-coloring of \"G\" is an assignment of colors to vertices of \"G\" such that adjacent vertices get compatible colors.\nMany notions of graph coloring fit into this pattern and can be expressed as graph homomorphisms into different families of graphs.\nCircular colorings can be defined using homomorphisms into circular complete graphs, refining the usual notion of colorings.\nFractional and \"b\"-fold coloring can be defined using homomorphisms into Kneser graphs.\nT-colorings correspond to homomorphisms into certain infinite graphs.\nAn oriented coloring of a directed graph is a homomorphism into any oriented graph.\nAn L(2,1)-coloring is a homomorphism into the complement of the path graph that is locally injective, meaning it is required to be injective on the neighbourhood of every vertex.\n\nAnother interesting connection concerns orientations of graphs.\nAn orientation of an undirected graph \"G\" is any directed graph obtained by choosing one of the two possible orientations for each edge.\nAn example of an orientation of the complete graph \"K\" is the transitive tournament \"\" with vertices 1,2,…,\"k\" and arcs from \"i\" to \"j\" whenever \"i\" < \"j\".\nA homomorphism between orientations of graphs \"G\" and \"H\" yields a homomorphism between the undirected graphs \"G\" and \"H\", simply by disregarding the orientations.\nOn the other hand, given a homomorphism \"G\" → \"H\" between undirected graphs, any orientation ' of \"H\" can be pulled back to an orientation ' of \"G\" so that ' has a homomorphism to '.\nTherefore, a graph \"G\" is \"k\"-colorable (has a homomorphism to \"K\") if and only if some orientation of \"G\" has a homomorphism to \"\".\n\nA folklore theorem states that for all \"k\", a directed graph \"G\" has a homomorphism to \"\" if and only if it admits no homomorphism from the directed path \"\".\nHere \"\" is the directed graph with vertices 1, 2, …, \"n\" and edges from \"i\" to \"i\" + 1, for \"i\" = 1, 2, …, \"n\" − 1.\nTherefore, a graph is \"k\"-colorable if and only if it has an orientation that admits no homomorphism from \"\".\nThis statement can be strengthened slightly to say that a graph is \"k\"-colorable if and only if some orientation contains no directed path of length \"k\" (no \"\" as a subgraph). \nThis is the Gallai–Hasse–Roy–Vitaver theorem.\n\nSome scheduling problems can be modeled as a question about finding graph homomorphisms. As an example, one might want to assign workshop courses to time slots in a calendar so that two courses attended by the same student are not too close to each other in time. The courses form a graph \"G\", with an edge between any two courses that are attended by some common student. The time slots form a graph \"H\", with an edge between any two slots that are distant enough in time. For instance, if one wants a cyclical, weekly schedule, such that each student gets their workshop courses on non-consecutive days, then \"H\" would be the complement graph of \"C\". A graph homomorphism from \"G\" to \"H\" is then a schedule assigning courses to time slots, as specified. To add a requirement saying that, e.g., no single student has courses on both Friday and Monday, it suffices to remove the corresponding edge from \"H\".\n\nA simple frequency allocation problem can be specified as follows: a number of transmitters in a wireless network must choose a frequency channel on which they will transmit data. To avoid interference, transmitters that are geographically close should use channels with frequencies that are far apart. If this condition is approximated with a single threshold to define 'geographically close' and 'far apart', then a valid channel choice again corresponds to a graph homomorphism. It should go from the graph of transmitters \"G\", with edges between pairs that are geographically close, to the graph of channels \"H\", with edges between channels that are far apart. While this model is rather simplified, it does admit some flexibility: transmitter pairs that are not close but could interfere because of geographical features can added to the edges of \"G\". Those that do not communicate at the same time can be removed from it. Similarly, channel pairs that are far apart but exhibit harmonic interference can be removed from the edge set of \"H\".\n\nIn each case, these simplified models display many of the issues that have to be handled in practice. Constraint satisfaction problems, which generalize graph homomorphism problems, can express various additional types of conditions (such as individual preferences, or bounds on the number of coinciding assignments). This allows the models to be made more realistic and practical.\n\nGraphs and directed graphs can be viewed as a special case of the far more general notion called relational structures (defined as a set with a tuple of relations on it). Directed graphs are structures with a single binary relation (adjacency) on the domain (the vertex set). Under this view, homomorphisms of such structures are exactly graph homomorphisms.\nIn general, the question of finding a homomorphism from one relational structure to another is a constraint satisfaction problem (CSP).\nThe case of graphs gives a concrete first step that helps to understand more complicated CSPs.\nMany algorithmic methods for finding graph homomorphisms, like backtracking, constraint propagation and local search, apply to all CSPs.\n\nFor graphs \"G\" and \"H\", the question of whether \"G\" has a homomorphism to \"H\" corresponds to a CSP instance with only one kind of constraint, as follows. The \"variables\" are the vertices of \"G\" and the \"domain\" for each variable is the vertex set of \"H\". An \"evaluation\" is a function that assigns to each variable an element of the domain, so a function \"f\" from \"V\"(\"G\") to \"V\"(\"H\"). Each edge or arc (\"u\",\"v\") of \"G\" then corresponds to the \"constraint\" ((\"u\",\"v\"), E(\"H\")). This is a constraint expressing that the evaluation should map the arc (\"u\",\"v\") to a pair (\"f\"(\"u\"),\"f\"(\"v\")) that is in the relation \"E\"(\"H\"), that is, to an arc of \"H\". A solution to the CSP is an evaluation that respects all constraints, so it is exactly a homomorphism from \"G\" to \"H\".\n\nCompositions of homomorphisms are homomorphisms. \nIn particular, the relation → on graphs is transitive (and reflexive, trivially), so it is a preorder on graphs.\nLet the equivalence class of a graph \"G\" under homomorphic equivalence be [\"G\"].\nThe equivalence class can also be represented by the unique core in [\"G\"].\nThe relation → is a partial order on those equivalence classes; it defines a poset.\n\nLet \"G\" < \"H\" denote that there is a homomorphism from \"G\" to \"H\", but no homomorphism from \"H\" to \"G\".\nThe relation → is a dense order, meaning that for all (undirected) graphs \"G\", \"H\" such that \"G\" < \"H\", there is a graph \"K\" such that \"G\" < \"K\" < \"H\" (this holds except for the trivial cases \"G\" = \"K\" or \"K\").\nFor example, between any two complete graphs (except \"K\", \"K\") there are infinitely many circular complete graphs, corresponding to rational numbers between natural numbers.\n\nThe poset of equivalence classes of graphs under homomorphisms is a distributive lattice, with the join of [\"G\"] and [\"H\"] defined as (the equivalence class of) the disjoint union [\"G\" ∪ \"H\"], and the meet of [\"G\"] and [\"H\"] defined as the tensor product [\"G\" × \"H\"] (the choice of graphs \"G\" and \"H\" representing the equivalence classes [\"G\"] and [\"H\"] does not matter).\nThe join-irreducible elements of this lattice are exactly connected graphs. This can be shown using the fact that a homomorphism maps a connected graph into one connected component of the target graph.\nThe meet-irreducible elements of this lattice are exactly the multiplicative graphs. These are the graphs \"K\" such that a product \"G\" × \"H\" has a homomorphism to \"K\" only when one of \"G\" or \"H\" also does. Identifying multiplicative graphs lies at the heart of Hedetniemi's conjecture.\n\nGraph homomorphisms also form a category, with graphs as objects and homomorphisms as arrows.\nThe initial object is the empty graph, while the terminal object is the graph with one vertex and one loop at that vertex.\nThe tensor product of graphs is the category-theoretic product and \nthe exponential graph is the exponential object for this category.\nSince these two operations are always defined, the category of graphs is a cartesian closed category.\nFor the same reason, the lattice of equivalence classes of graphs under homomorphisms is in fact a Heyting algebra.\n\nFor directed graphs the same definitions apply. In particular → is a partial order on equivalence classes of directed graphs. It is distinct from the order → on equivalence classes of undirected graphs, but contains it as a suborder. This is because every undirected graph can be thought of as a directed graph where every arc (\"u\",\"v\") appears together with its inverse arc (\"v\",\"u\"), and this does not change the definition of homomorphism. The order → for directed graphs is again a distributive lattice and a Heyting algebra, with join and meet operations defined as before. However, it is not dense. There is also a category with directed graphs as objects and homomorphisms as arrows, which is again a cartesian closed category.\n\nThere are many incomparable graphs with respect to the homomorphism preorder, that is, pairs of graphs such that neither admits a homomorphism into the other.\nOne way to construct them is to consider the odd girth of a graph \"G\", the length of its shortest odd-length cycle.\nThe odd girth is, equivalently, the smallest odd number \"g\" for which there exists a homomorphism from the cycle graph on \"g\" vertices to \"G\". For this reason, if \"G\" → \"H\", then the odd girth of \"G\" is greater than or equal to the odd girth of \"H\".\n\nOn the other hand, if \"G\" → \"H\", then the chromatic number of \"G\" is less than or equal to the chromatic number of \"H\". \nTherefore, if \"G\" has strictly larger odd girth than \"H\" and strictly larger chromatic number than \"H\", then \"G\" and \"H\" are incomparable.\nFor example, the Grötzsch graph is 4-chromatic and triangle-free (it has girth 4 and odd girth 5), so it is incomparable to the triangle graph \"K\".\n\nExamples of graphs with arbitrarily large values of odd girth and chromatic number are Kneser graphs and generalized Mycielskians.\nA sequence of such graphs, with simultaneously increasing values of both parameters, gives infinitely many incomparable graphs (an antichain in the homomorphism preorder).\nOther properties, such as density of the homomorphism preorder, can be proved using such families.\nConstructions of graphs with large values of chromatic number and girth, not just odd girth, are also possible, but more complicated (see Girth and graph coloring).\n\nAmong directed graphs, it is much easier to find incomparable pairs. For example, consider the directed cycle graphs \"\", with vertices 1, 2, …, \"n\" and edges from \"i\" to \"i\" + 1 (for \"i\" = 1, 2, …, \"n\" − 1) and from \"n\" to 1.\nThere is a homomorphism from \"\" to \"\" (\"n\", \"k\" ≥ 3) if and only if \"n\" is a multiple of \"k\". \nIn particular, directed cycle graphs \"\" with \"n\" prime are all incomparable.\n\nIn the graph homomorphism problem, an instance is a pair of graphs (\"G\",\"H\") and a solution is a homomorphism from \"G\" to \"H\". The general decision problem, asking whether there is any solution, is NP-complete. However, limiting allowed instances gives rise to a variety of different problems, some of which are much easier to solve. Methods that apply when restraining the left side \"G\" are very different than for the right side \"H\", but in each case a dichotomy (a sharp boundary between easy and hard cases) is known or conjectured.\n\nThe homomorphism problem with a fixed graph \"H\" on the right side of each instance is also called the \"H\"-coloring problem. When \"H\" is the complete graph \"K\", this is the graph \"k\"-coloring problem, which is solvable in polynomial time for \"k\" = 0, 1, 2, and NP-complete otherwise.\nIn particular, \"K\"-colorability of a graph \"G\" is equivalent to \"G\" being bipartite, which can be tested in linear time.\nMore generally, whenever \"H\" is a bipartite graph, \"H\"-colorability is equivalent to \"K\"-colorability (or \"K\" / \"K\"-colorability when \"H\" is empty/edgeless), hence equally easy to decide.\nPavol Hell and Jaroslav Nešetřil proved that, for undirected graphs, no other case is tractable:\n\nThis is also known as the \"dichotomy theorem\" for (undirected) graph homomorphisms, since it divides \"H\"-coloring problems into NP-complete or P problems, with no intermediate cases.\nFor directed graphs, the situation is more complicated and in fact equivalent to the much more general question of characterizing the complexity of constraint satisfaction problems.\nIt turns out that \"H\"-coloring problems for directed graphs are just as general and as diverse as CSPs with any other kinds of constraints. Formally, a (finite) \"constraint language\" (or \"template\") \"Γ\" is a finite domain and a finite set of relations over this domain. CSP(\"Γ\") is the constraint satisfaction problem where instances are only allowed to use constraints in \"Γ\". \n\nIntuitively, this means that every algorithmic technique or complexity result that applies to \"H\"-coloring problems for directed graphs \"H\" applies just as well to general CSPs. In particular, one can ask whether the Hell–Nešetřil theorem can be extended to directed graphs. By the above theorem, this is equivalent to the Feder–Vardi conjecture on CSP dichotomy, which states that for every constraint language \"Γ\", CSP(\"Γ\") is NP-complete or in P.\n\nThe homomorphism problem with a single fixed graph \"G\" on left side of input instances can be solved by brute-force in time |\"V\"(\"H\")|, so polynomial in the size of the input graph \"H\". In other words, the problem is trivially in P for graphs \"G\" of bounded size. The interesting question is then what other properties of \"G\", beside size, make polynomial algorithms possible.\n\nThe crucial property turns out to be treewidth, a measure of how tree-like the graph is. For a graph \"G\" of treewidth at most \"k\" and a graph \"H\", the homomorphism problem can be solved in time |\"V\"(\"H\")| with a standard dynamic programming approach. In fact, it is enough to assume that the core of \"G\" has treewidth at most \"k\". This holds even if the core is not known.\n\nThe exponent in the |\"V\"(\"H\")|-time algorithm cannot be lowered significantly: no algorithm with running time |\"V\"(\"H\")| exists, assuming the exponential time hypothesis (ETH), even if the inputs are restricted to any class of graphs of unbounded treewidth.\nThe ETH is an unproven assumption similar to P ≠ NP, but stronger.\nUnder the same assumption, there are also essentially no other properties that can be used to get polynomial time algorithms. This is formalized as follows:\n\nOne can ask whether the problem is at least solvable in a time arbitrarily highly dependent on \"G\", but with a fixed polynomial dependency on the size of \"H\".\nThe answer is again positive if we limit \"G\" to a class of graphs with cores of bounded treewidth, and negative for every other class.\nIn the language of parameterized complexity, this formally states that the homomorphism problem in formula_1 parameterized by the size (number of edges) of \"G\" exhibits a dichotomy. It is fixed-parameter tractable if graphs in formula_1 have cores of bounded treewidth, and W[1]-complete otherwise.\n\nThe same statements hold more generally for constraint satisfaction problems (or for relational structures, in other words). The only assumption needed is that constraints can involve only a bounded number of variables (all relations are of some bounded arity, 2 in the case of graphs). The relevant parameter is then the treewidth of the primal constraint graph.\n\n\n\n\n"}
{"id": "795837", "url": "https://en.wikipedia.org/wiki?curid=795837", "title": "Half time (electronics)", "text": "Half time (electronics)\n\nIn signal processing, the half time is the time it takes for the amplitude of a pulse to drop from 100% to 50% of its peak value.\n"}
{"id": "270977", "url": "https://en.wikipedia.org/wiki?curid=270977", "title": "Hermite spline", "text": "Hermite spline\n\nIn the mathematical subfield of numerical analysis, a Hermite spline is a spline curve where each polynomial of the spline is in Hermite form.\n\n"}
{"id": "17183245", "url": "https://en.wikipedia.org/wiki?curid=17183245", "title": "Hermite–Hadamard inequality", "text": "Hermite–Hadamard inequality\n\nIn mathematics, the Hermite–Hadamard inequality, named after Charles Hermite and Jacques Hadamard and sometimes also called Hadamard's inequality, states that if a function ƒ : [\"a\", \"b\"] → R is convex, then the following chain of inequalities hold:\n\nSuppose that −∞ < \"a\" < \"b\" < ∞, and let \"f\":[\"a\", \"b\"] → ℝ be an integrable real function.\nUnder the above conditions the following sequence of functions is called the sequence of iterated integrals of \"f\",where \"a\" ≤ \"s\" ≤ \"b\".:\n\nLet [\"a\", \"b\"] = [0, 1] and \"f\"(\"s\") ≡ 1. Then the sequence of iterated integrals of 1 is defined on [0, 1], and\n\nLet [a,b] = [−1,1] and \"f\"(\"s\") ≡ 1. Then the sequence of iterated integrals of 1 is defined on [−1, 1], and\n\nLet [\"a\", \"b\"] = [0, 1] and \"f\"(\"s\") = \"e\". Then the sequence of iterated integrals of \"f\" is defined on [0, 1], and\n\nSuppose that −∞ < \"a\" < \"b\" < ∞, and let f:[a,b]→R be a convex function, \"a\" < \"x\" < \"b\", \"i\" = 1, ..., \"n\", such that \"x\" ≠ \"x\", if \"i\" ≠ \"j\". Then the following holds:\n\nwhere\n\nIn the concave case ≤ is changed to ≥.\n\nRemark 1. If f is convex in the strict sense then ≤ is changed to < and equality holds iff f is linear function.\n\nRemark 2. The inequality is sharp in the following limit sense: let formula_8 and formula_9\n\n"}
{"id": "404404", "url": "https://en.wikipedia.org/wiki?curid=404404", "title": "I. J. Good", "text": "I. J. Good\n\nIrving John (\"I. J.\"; \"Jack\") Good (9 December 1916 – 5 April 2009)\nwas a British mathematician who worked as a cryptologist at Bletchley Park with Alan Turing. After the Second World War, Good continued to work with Turing on the design of computers and Bayesian statistics at the University of Manchester. Good moved to the United States where he was professor at Virginia Tech.\n\nHe was born Isadore Jacob Gudak to a Polish Jewish family in London. He later anglicised his name to Irving John Good and signed his publications \"I. J. Good.\"\n\nAn originator of the concept now known as \"intelligence explosion,\" Good served as consultant on supercomputers to Stanley Kubrick, director of the 1968 film \"\".\n\nGood was born Isadore Jacob Gudak to Polish Jewish parents in London. His father was a watchmaker, who later managed and owned a successful fashionable jewellery shop, and was also a notable Yiddish writer writing under the pen name of Moshe Oved. Good was educated at the Haberdashers' Aske's Boys' School, at the time in Hampstead in northwest London, where, according to Dan van der Vat, Good effortlessly outpaced the mathematics curriculum.\n\nGood studied mathematics at Jesus College, Cambridge, graduating in 1938 and winning the Smith's Prize in 1940. He did research under G.H. Hardy and Besicovitch before moving to Bletchley Park in 1941 on completing his doctorate.\n\nOn 27 May 1941, having just obtained his doctorate at Cambridge, Good walked into Hut 8, Bletchley's facility for breaking German naval ciphers, for his first shift. This was the day that Britain's Royal Navy destroyed the after it had sunk the Royal Navy's . Bletchley had contributed to \"Bismarck\"s destruction by discovering, through wireless-traffic analysis, that the German flagship was sailing for Brest, France, rather than Wilhelmshaven, from which she had set out.\nHut 8 had not, however, been able to decrypt on a current basis the 22 German Naval Enigma messages that had been sent to \"Bismarck\". The German Navy's Enigma cyphers were considerably more secure than those of the German Army or Air Force, which had been well penetrated by 1940. Naval messages were taking three to seven days to decrypt, which usually made them operationally useless for the British. This was about to change, however, with Good's help.\n\nGood served with Turing for nearly two years.\nSubsequently, he worked with Donald Michie in Max Newman's group on the Fish ciphers, leading to the development of the Colossus computer.\n\nGood was a member of the Bletchley Chess Club which defeated the Oxford University Chess Club 8–4 in a twelve-board team match held on 2 December 1944. Good played fourth board for Bletchley Park, with C.H.O'D. Alexander, Harry Golombek and James Macrae Aitken in the top three spots. He won his game against Sir Robert Robinson.\n\nIn 1947 Newman invited Good to join him and Turing at Manchester University. There for three years Good lectured in mathematics and researched computers, including the Manchester Mark 1.\n\nIn 1948 Good was recruited by the Government Communications Headquarters (GCHQ), successor to Bletchley Park. He remained there until 1959, while also taking up a brief associate professorship at Princeton University and a short consultancy with IBM.\n\nFrom 1959 until he moved to the US in 1967, Good held government-funded positions and from 1964 a senior research fellowship at Trinity College, Oxford, and the Atlas Computer Laboratory, where he continued his interests in computing, statistics and chess. He later left Oxford, declaring it \"a little stiff\".\n\nIn 1967 Good moved to the United States, where he was appointed a research professor of statistics at Virginia Polytechnic Institute and State University. In 1969 he was appointed a University Distinguished Professor at Virginia Tech, and in 1994 Emeritus University Distinguished Professor.\nIn 1973 he was elected as a Fellow of the American Statistical Association.\n\nHe later said about his arrival in Virginia (from Britain) in 1967 to start teaching at VPI, where he taught from 1967 to 1994:\n\nGood's published work ran to over three million words.\nHe was known for his work on Bayesian statistics. He published a number of books on probability theory. In 1958 he published an early version of what later became known as the fast Fourier transform but it did not became widely known. He played chess to county standard and helped popularise Go, an Asian boardgame, through a 1965 article in \"New Scientist\" (he had learned the rules from Alan Turing). In 1965 he originated the concept now known as \"intelligence explosion\" or the \"technological singularity, which anticipates the eventual advent of superhuman intelligence:\n\nGood's authorship of treatises such as \"Speculations Concerning the First Ultraintelligent Machine\" and \"Logic of Man and Machine\" (both 1965) made him the obvious person for Stanley Kubrick to consult when filming \"\" (1968), one of whose principal characters was the paranoid HAL 9000 supercomputer. In 1995 Good was elected a member of the Academy of Motion Picture Arts and Sciences.\n\nAccording to his assistant, Leslie Pendleton, in 1998 Good wrote in an unpublished autobiographical statement that he suspected an ultraintelligent machine would lead to the extinction of man.\n\nThe slender, bushy-moustached Good was blessed with a sense of humour. He published a paper under the names IJ Good and \"K Caj Doog\"—the latter, his own nickname spelled backwards. In a 1988 paper, he introduced its subject by saying, \"Many people have contributed to this topic but I shall mainly review the writings of I. J. Good because I have read them all carefully.\" In Virginia he chose, as his vanity licence plate, \"007IJG,\" in subtle reference to his Second World War intelligence work.\n\nGood never married. After going through ten assistants in his first thirteen years at Virginia, he hired Leslie Pendleton, who proved up to the task of managing his quirks. He wanted to marry her, but she refused. Although there was speculation, they were never more than friends, but she was his assistant, companion, and friend for the rest of his life.\n\nGood died on 5 April 2009 of natural causes in Radford, Virginia, aged 92.\n\n\n\n\n"}
{"id": "39530422", "url": "https://en.wikipedia.org/wiki?curid=39530422", "title": "Icosahedral number", "text": "Icosahedral number\n\nAn icosahedral number is a figurate number that represents an icosahedron. The \"n\"th icosahedral number is given by the formula\n\nThe first such numbers are 1, 12, 48, 124, 255, 456, 742, 1128, 1629, 2260, 3036, 3972, 5083, … .\n"}
{"id": "58462412", "url": "https://en.wikipedia.org/wiki?curid=58462412", "title": "Join-based tree algorithms", "text": "Join-based tree algorithms\n\nIn computer science, join-based tree algorithms are a class of algorithms for self-balancing binary search trees.\nThe algorithmic framework is based on a single operation \"join\". Under this framework, the \"join\" operation captures all balancing criteria of different balancing schemes, and all other functions \"join\" have generic implementation across different balancing schemes. The \"join-based algorithms\" can be applied to at least four balancing schemes: AVL trees, red-black trees, weight-balanced trees and treaps.\n\nThe \"join\"formula_1 operation takes as input two binary balanced trees formula_2 and formula_3 of the same balancing scheme, and a key formula_4, and outputs a new balanced binary tree formula_5 whose in-order traversal is the in-order traversal of formula_2, then formula_4 then the in-order traversal of formula_3. In particular, if the trees are search trees, which means that the in-order of the trees maintain a total ordering on keys, it must satisfy the condition that all keys in formula_2 are smaller than formula_4 and all keys in formula_3 are greater than formula_4.\n\nThe \"join\" operation was first defined by Tarjan on red-black trees, which runs in worst-case logarithmic time. Later Sleator and Tarjan described an \"join\" algorithm for splay trees which runs in amortized logarithmic time. Later Adams extended \"join\" to weight-balanced trees and used it for fast set-set functions including union, intersection and set difference. In 1998, Blelloch and Reid-Miller extended \"join\" on treaps, and proved the bound of the set functions to be formula_13 for two trees of size formula_14 and formula_15, which is optimal in the comparison model. They also brought up parallelism in Adams' algorithm by using a divide-and-conquer scheme. In 2016, Blelloch et al. formally proposed the join-based algorithms, and formalized the \"join\" algorithm for four different balancing schemes: AVL trees, red-black trees, weight-balanced trees and treaps. In the same work they proved that Adams' algorithms on union, intersection and difference are work-optimal on all the four balancing schemes.\n\nThe function \"join\"formula_16 considers rebalancing the tree, and thus depends on the input balancing scheme. If the two trees are balanced, \"join\" simply creates a new node with left subtree , root and right subtree . Suppose that is heavier (this \"heavier\" depends on the balancing scheme) than (the other case is symmetric). \"Join\" follows the right spine of until a node which is balanced with . At this point a new node with left child , root and right child is created to replace c. The new node may invalidate the balancing invariant. This can be fixed with rotations.\n\nThe following is the \"join\" algorithms on different balancing schemes.\n\nThe \"join\" algorithm for AVL trees:\n\nHere formula_17 of a node formula_18 the height of formula_18. expose(v)=(l,k,r) means to extract a tree node formula_18's left child formula_21, the key of the node formula_4, and the right child formula_23. Node(l,k,r) means to create a node of left child formula_21, key formula_4, and right child formula_23.\n\nThe \"join\" algorithm for red-black trees:\n\nHere formula_27 of a node formula_18 means twice the black height of a black node, and the twice the black height of a red node. expose(v)=(l,⟨k,c⟩,r) means to extract a tree node formula_18's left child formula_21, the key of the node formula_4, the color of the node formula_32 and the right child formula_23. Node(l,⟨k,c⟩,r) means to create a node of left child formula_21, key formula_4, color formula_32 and right child formula_23.\n\nThe \"join\" algorithm for weight-balanced trees:\n\nHere balanceformula_38 means two weigthts formula_39 and formula_40 are balanced. expose(v)=(l,k,r) means to extract a tree node formula_18's left child formula_21, the key of the node formula_4 and the right child formula_23. Node(l,k,r) means to create a node of left child formula_21, key formula_4 and right child formula_23.\n\nIn the following, expose(v)=(l,k,r) means to extract a tree node formula_18's left child formula_21, the key of the node formula_4 and the right child formula_23. Node(l,k,r) means to create a node of left child formula_21, key formula_4 and right child formula_23. right(formula_18) and left(formula_18) extracts the right child and the left child of a tree nodeformula_18, respectively. formula_58 extract the key of a node formula_18. \"formula_60\" means that two statements formula_61 and formula_62 can run in parallel.\n\nTo split a tree into two trees, those smaller than key \"x\", and those larger than key \"x\", we first draw a path from the root by inserting \"x\" into the tree. After this insertion, all values less than \"x\" will be found on the left of the path, and all values greater than \"x\" will be found on the right. By applying \"Join\", all the subtrees on the left side are merged bottom-up using keys on the path as intermediate nodes from bottom to top to form the left tree, and the right part is asymmetric. For some applications, \"Split\" also returns a boolean value denoting if \"x\" appears in the tree. The cost of \"Split\" is formula_63, order of the height of the tree. \n\nThe split algorithm is as follows:\n\nThis function is defined similarly as \"join\" but without the middle key. It first splits out the last key formula_4 of the left tree, and then join the rest part of the left tree with the right tree with formula_4.\nThe algorithm is as follows:\n\nThe cost is formula_63 for a tree of size formula_67.\n\nThe insertion and deletion algorithms, when making use of \"join\" can be independent of balancing schemes. For an insertion, the algorithm compares the key to be inserted with the key in the root, inserts it to the left/right subtree if the key is smaller/greater than the key in the root, and joins the two subtrees back with the root. A deletion compares the key to be deleted with the key in the root. If they are equal, return join2 on the two subtrees. Otherwise, delete the key from the corresponding subtree, and join the two subtrees back with the root.\nThe algorithms are as follows:\n\nBoth insertion and deletion requires formula_63 time if formula_69.\n\nSeveral set operations have been defined on weight-balanced trees: union, intersection and set difference. The union of two weight-balanced trees and representing sets and , is a tree that represents . The following recursive function computes this union:\n\nSimilarly, the algorithms of intersection and set-difference are as follows:\n\nThe complexity of each of union, intersection and difference is formula_70 for two weight-balanced trees of sizes formula_14 and formula_15. This complexity is optimal in terms of the number of comparisons. More importantly, since the recursive calls to union, intersection or difference are independent of each other, they can be executed in parallel with a parallel depth formula_73. When formula_74, the join-based implementation applies the same computation as in a single-element insertion or deletion if the root of the larger tree is used to split the smaller tree.\n\nThe algorithm for building a tree can make use of the union algorithm, and use the divide-and-conquer scheme:\n\nThis algorithm costs formula_75 work and has formula_76 depth. A more-efficient algorithm makes use of a parallel sorting algorithm.\n\nThis algorithm costs formula_75 work and has formula_63 depth assuming the sorting algorithm has formula_75 work and formula_63 depth.\n\nThis function selects all entries in a tree satisfying an indicator formula_81, and return a tree containing all selected entries. It recursively filters the two subtrees, and join them with the root if the root satisfies formula_81, otherwise \"join2\" the two subtrees.\n\nThis algorithm costs work formula_83 and depth formula_63 on a tree of size formula_67, assuming formula_81 has constant cost.\n\nThe join-based algorithms are applied to support interface for sets, maps, and augmented maps in libarays such as Hackage, SML/NJ, and PAM.\n\n"}
{"id": "26113571", "url": "https://en.wikipedia.org/wiki?curid=26113571", "title": "Jyotirmimamsa", "text": "Jyotirmimamsa\n\nIn Hindu astronomy, Jyotirmimamsa (analysis of astronomy) is a treatise on the methodology of astronomical studies authored by Nilakantha Somayaji (1444–1544) in around 1504 CE. Nilakantha somayaji was an important astronomer-mathematician of the Kerala school of astronomy and mathematics and was the author of the much celebrated astronomical work titled Tantrasamgraha. This book stresses the necessity and importance of astronomical observations to obtain correct parameters for computations and to develop more and more accurate theories. It even discounts the role of revealed wisdom and divine intuitions in studying astronomical phenomena. Jyotirmimamsa is sometimes cited as proof to establish that modern methodologies of scientific investigations are not unknown to ancient and medieval Indians.\n\nThe nature of the astronomical and mathematical work, the divine intuition, the experimental details of the science, corrections to the planetary parameters, reasons for the corrections for the planetary revolutions, Vedic authority for inference in astronomy, relative accuracy of different systems, and correction through eclipses, true motion, position, etc., of planets are some of the topics discussed in Jyotirmimamsa.\n\nThe following is an outline of the various topics discussed in Jyotirmimamsa.\n\n\n"}
{"id": "2295976", "url": "https://en.wikipedia.org/wiki?curid=2295976", "title": "Karel deLeeuw", "text": "Karel deLeeuw\n\nKarel deLeeuw, or de Leeuw ( – ), was a mathematics professor at Stanford University, specializing in harmonic analysis and functional analysis.\n\nBorn in Chicago, Illinois, he attended the Illinois Institute of Technology and the University of Chicago, earning a B.S. degree in 1950. He stayed at Chicago to earn an M.S. degree in mathematics in 1951, then went to Princeton University, where he obtained a Ph.D. degree in 1954. His thesis, titled \"The relative cohomology structure of formations\", was written under the direction of Emil Artin.\n\nAfter first teaching mathematics at Dartmouth College and the University of Wisconsin–Madison, he joined the Stanford University faculty in 1957, becoming a full professor in 1966. During sabbaticals and leaves he also spent time at the Institute for Advanced Study and at Churchill College, Cambridge (where he was a Fulbright Fellow). He was also a Member-at-Large of the Council of the American Mathematical Society.\n\nDeLeeuw was murdered by Theodore Streleski, a Stanford doctoral student for 19 years, whom he briefly advised. DeLeeuw's widow Sita deLeeuw was critical of media coverage of the crime, saying, \"The media, in their eagerness to give Streleski a forum, become themselves accomplices in the murder—giving Streleski what he wanted in the first place.\"\n\nA memorial lecture series was established in 1978 by the Stanford Department of Mathematics to honor deLeeuw's memory.\n\n"}
{"id": "32103592", "url": "https://en.wikipedia.org/wiki?curid=32103592", "title": "Langlands–Deligne local constant", "text": "Langlands–Deligne local constant\n\nIn mathematics, the Langlands–Deligne local constant (or local Artin root number up to an elementary function of \"s\") is an elementary function associated with a representation of the Weil group of a local field. The functional equation \nof an Artin L-function has an elementary function ε(ρ,\"s\") appearing in it, equal to a constant called the Artin root number times an elementary real function of \"s\", and Langlands discovered that ε(ρ,\"s\") can be written in a canonical way as a product\nof local constants ε(ρ, \"s\", ψ) associated to primes \"v\".\n\nTate proved the existence of the local constants in the case that ρ is 1-dimensional in Tate's thesis.\nThe original proof of the existence of the local constants by used local methods and was rather long and complicated, and never published. later discovered a simpler proof using global methods.\n\nThe local constants ε(ρ, \"s\", ψ) depend on a representation ρ of the Weil group and a choice of character ψ of the additive group of \"E\". They satisfy the following conditions:\n\nBrauer's theorem on induced characters implies that these three properties characterize the local constants.\n\nThere are several different conventions for denoting the local constants.\n\n"}
{"id": "4258134", "url": "https://en.wikipedia.org/wiki?curid=4258134", "title": "Lebesgue's density theorem", "text": "Lebesgue's density theorem\n\nIn mathematics, Lebesgue's density theorem states that for any Lebesgue measurable set formula_1, the \"density\" of \"A\" is 0 or 1 at almost every point in formula_2. Additionally, the \"density\" of \"A\" is 1 at almost every point in \"A\". Intuitively, this means that the \"edge\" of \"A\", the set of points in \"A\" whose \"neighborhood\" is partially in \"A\" and partially outside of \"A\", is negligible.\n\nLet μ be the Lebesgue measure on the Euclidean space R and \"A\" be a Lebesgue measurable subset of R. Define the approximate density of \"A\" in a ε-neighborhood of a point \"x\" in R as\n\nwhere \"B\" denotes the closed ball of radius ε centered at \"x\".\n\nLebesgue's density theorem asserts that for almost every point \"x\" of \"A\" the density\n\nexists and is equal to 1.\n\nIn other words, for every measurable set \"A\", the density of \"A\" is 0 or 1 almost everywhere in R. However, it is a curious fact that if μ(\"A\") > 0 and , then there are always points of R where the density is neither 0 nor 1.\n\nFor example, given a square in the plane, the density at every point inside the square is 1, on the edges is 1/2, and at the corners is 1/4. The set of points in the plane at which the density is neither 0 nor 1 is non-empty (the square boundary), but it is negligible.\n\nThe Lebesgue density theorem is a particular case of the Lebesgue differentiation theorem.\n\nThus, this theorem is also true for every finite Borel measure on R instead of Lebesgue measure, see Discussion.\n\n\n"}
{"id": "43532254", "url": "https://en.wikipedia.org/wiki?curid=43532254", "title": "Leonard Schulman", "text": "Leonard Schulman\n\nLeonard J. Y. Schulman (born September 14, 1963) is Professor of Computer Science in the Computing and Mathematical Sciences Department at the California Institute of Technology. He is known for work on algorithms, information theory, coding theory, and quantum computation.\n\nSchulman is the son of theoretical physicist Lawrence Schulman.\n\nSchulman studied at the Massachusetts Institute of Technology, where he completed a BS degree in Mathematics in 1988 and a PhD degree in Applied Mathematics in 1992.\nHe was a faculty member in the College of Computing at the Georgia Institute of Technology from 1995-2000 before joining the faculty of the California Institute of Technology in 2000. He serves as the director of the Center for Mathematics of Information at Caltech and also participates in the Institute for Quantum Information and Matter.\n\nSchulman's research centers broadly around algorithms and information. He has made notable contributions to varied areas within this space including clustering, derandomization, quantum information theory, and coding theory. One example, which was named a Computing Reviews \"Notable Paper\" in 2012, is his work on quantifying the effectiveness of Lloyd-type methods for the k-means problem.\n\nSchulman received the MIT Bucsela Prize in 1988, an NSF Mathematical Sciences Postdoctoral Fellowship in 1992 and an NSF CAREER award in 1999. His work received the IEEE S.A. Schelkunoff Prize in 2005. He was named the editor-in-chief of the SIAM Journal on Computing in 2013. Schulman was also recognized for the ACM Notable Paper in 2012 and received the UAI Best Paper Award in 2016.\n\n"}
{"id": "2630881", "url": "https://en.wikipedia.org/wiki?curid=2630881", "title": "Lightface analytic game", "text": "Lightface analytic game\n\nIn descriptive set theory, a lightface analytic game is a game whose payoff set \"A\" is a formula_1 subset of Baire space; that is, there is a tree \"T\" on formula_2 which is a computable subset of formula_3, such that \"A\" is the projection of the set of all branches of \"T\".\n\nThe determinacy of all lightface analytic games is equivalent to the existence of 0.\n"}
{"id": "19852914", "url": "https://en.wikipedia.org/wiki?curid=19852914", "title": "Manipulability ellipsoid", "text": "Manipulability ellipsoid\n\nIn robotics, the manipulability ellipsoid is the geometric interpretation of the scaled eigenvectors resulting from the singular value decomposition of the jacobian that describes a robot's motion.\n"}
{"id": "46801272", "url": "https://en.wikipedia.org/wiki?curid=46801272", "title": "Marcos Dajczer", "text": "Marcos Dajczer\n\nMarcos Dajczer (born 19 November 1948, in Buenos Aires) is an Argentine-born Brazilian mathematician whose research concerns geometry and topology.\n\nDajczer obtained his Ph.D. from the Instituto Nacional de Matemática Pura e Aplicada in 1980 under the supervision of Manfredo do Carmo.\n\nIn 2006, he received Brazil's National Order of Scientific Merit honour for his work in mathematics. He was a Guggenheim Fellow in 1985.\n\nDo Carmo–Dajczer theorem is named after his teacher and him.\n\n"}
{"id": "10151043", "url": "https://en.wikipedia.org/wiki?curid=10151043", "title": "Moni Naor", "text": "Moni Naor\n\nMoni Naor () is an Israeli computer scientist, currently a professor at the Weizmann Institute of Science. Naor received his Ph.D. in 1989 at the University of California, Berkeley. His advisor was Manuel Blum.\n\nHe works in various fields of computer science, mainly the foundations of cryptography. He is notable for initiating research on public key systems secure against chosen ciphertext attack and creating non-malleable cryptography, visual cryptography (with Adi Shamir), and suggesting various methods for verifying that users of a computer system are human (leading to the notion of CAPTCHA). His research on Small-bias sample space, give a general framework for combining small k-wise independent spaces with small formula_1-biased spaces to obtain formula_2-almost k-wise independent spaces of small size. In 1994 he was the first, with Amos Fiat, to formally study the problem of practical broadcast encryption. Along with Benny Chor, Amos Fiat, and Benny Pinkas, he made a contribution to the development of Traitor tracing, a copyright infringement detection system which works by tracing the source of leaked files rather than by direct copy protection.\n\n\n"}
{"id": "30865852", "url": "https://en.wikipedia.org/wiki?curid=30865852", "title": "O-minimal theory", "text": "O-minimal theory\n\nIn mathematical logic, and more specifically in model theory, an infinite structure (\"M\",<...) which is totally ordered by < is called an o-minimal structure if and only if every definable subset \"X\" ⊂ \"M\" (with parameters taken from \"M\") is a finite union of intervals and points.\n\nO-minimality can be regarded as a weak form of quantifier elimination. A structure \"M\" is o-minimal if and only if every formula with one free variable and parameters in \"M\" is equivalent to a quantifier-free formula involving only the ordering, also with parameters in \"M\". This is analogous to the minimal structures, which are exactly the analogous property down to equality.\n\nA theory \"T\" is an o-minimal theory if every model of \"T\" is o-minimal. It is known that the complete theory \"T\" of an o-minimal structure is an o-minimal theory. This result is remarkable because, in contrast, the complete theory of a minimal structure need not be a strongly minimal theory, that is, there may be an elementarily equivalent structure which is not minimal.\n\nO-minimal structures can be defined without recourse to model theory. Here we define a structure on a nonempty set \"M\" in a set-theoretic manner, as a sequence \"S\" = (\"S\"), \"n\" = 0,1,2... such that\n\nIf \"M\" has a dense linear order without endpoints on it, say <, then a structure \"S\" on \"M\" is called o-minimal if it satisfies the extra axioms\n\nThe \"o\" stands for \"order\", since any o-minimal structure requires an ordering on the underlying set.\n\nO-minimal structures originated in model theory and so have a simpler — but equivalent — definition using the language of model theory. Specifically if \"L\" is a language including a binary relation <, and (\"M\",<...) is an \"L\"-structure where < is interpreted to satisfy the axioms of a dense linear order, then (\"M\",<...) is called an o-minimal structure if for any definable set \"X\" ⊆ \"M\" there are finitely many open intervals \"I\"..., \"I\" with endpoints in \"M\" ∪ {±∞} and a finite set \"X\" such that\n\nExamples of o-minimal theories are:\n\nIn the case of RCF, the definable sets are the semialgebraic sets. Thus the study of o-minimal structures and theories generalises real algebraic geometry. A major line of current research is based on discovering expansions of the real ordered field that are o-minimal. Despite the generality of application, one can show a great deal about the geometry of set definable in o-minimal structures. There is a cell decomposition theorem, Whitney and Verdier stratification theorems and a good notion of dimension and Euler characteristic.\n\n\n\n"}
{"id": "22451128", "url": "https://en.wikipedia.org/wiki?curid=22451128", "title": "Offered load", "text": "Offered load\n\nIn the mathematical theory of probability, offered load is a concept in queuing theory. The offered load is a measure of traffic in a queue. The offered load is given by Little's law: the arrival rate into the queue (symbolized with λ) multiplied by the mean holding time (symbolized by τ), the average amount of time spent by items in the queue. Offered load is expressed in Erlang units or call-seconds per hour, a dimensionless measure. \n\n"}
{"id": "3934869", "url": "https://en.wikipedia.org/wiki?curid=3934869", "title": "Ordered graph", "text": "Ordered graph\n\nAn ordered graph is a graph with a total order over its nodes.\n\nIn an ordered graph, the parents of a node are the nodes that are adjacent to it and precede it in the ordering. More precisely, formula_1 is a parent of formula_2 in the ordered graph formula_3 if formula_4 and formula_5. The width of a node is the number of its parents, and the width of an ordered graph is the maximal width of its nodes.\n\nThe induced graph of an ordered graph is obtained by adding some edges to an ordering graph, using the method outlined below. The induced width of an ordered graph is the width of its induced graph. \n\nGiven an ordered graph, its induced graph is another ordered graph obtained by joining some pairs of nodes that are both parents of another node. In particular, nodes are considered in turn according to the ordering, from last to first. For each node, if two of its parents are not joined by an edge, that edge is added. In other words, when considering node formula_1, if both formula_2 and formula_8 are parents of it and are not joined by an edge, the edge formula_9 is added to the graph. Since the parents of a node are always connected with each other, the induced graph is always chordal.\n\nAs an example, the induced graph of an ordered graph is calculated. The ordering is represented by the position of its nodes in the figures: a is the last node and d is the first.\n\nNode formula_10 is considered first. Its parents are formula_11 and formula_12, as they are both joined to formula_10 and both precede formula_10 in the ordering. Since they are not joined by an edge, one is added.\n\nNode formula_11 is considered second. While this node only has formula_16 as a parent in the original graph, it also has formula_12 as a parent in the partially built induced graph. Indeed, formula_12 is joined to formula_11 and also precede formula_11 in the ordering. As a result, an edge joining formula_12 and formula_16 is added.\n\nConsidering formula_16 does not produce any change, as this node has no parents.\n\nProcessing nodes in order matters, as the introduced edges may create new parents, which are then relevant to the introduction of new edges. The following example shows that a different ordering produces a different induced graph of the same original graph. The ordering is the same as above but formula_11 and formula_12 are swapped.\n\nAs in the previous case, both formula_11 and formula_12 are parents of formula_10. Therefore, an edge between them is added. According to the new order, the second node that is considered is formula_12. This node has only one parent (formula_11). Therefore, no new edge is added. The third considered node is formula_11. Its only parent is formula_16. Indeed, formula_11 and formula_12 are not joined this time. As a result, no new edge is introduced. Since formula_16 has no parent as well, the final induced graph is the one above. This induced graph differs from the one produced by the previous ordering.\n\n"}
{"id": "472456", "url": "https://en.wikipedia.org/wiki?curid=472456", "title": "Pierre Boutroux", "text": "Pierre Boutroux\n\nPierre Léon Boutroux (; 6 December 1880 – 15 August 1922) was a French mathematician and historian of science. Boutroux is chiefly known for his work in the history and philosophy of mathematics.\n\nHe was born in Paris on 6 December 1880 into a well connected family of the French intelligentsia. His father was the philosopher Émile Boutroux. His mother was Aline Catherine Eugénie Poincaré, sister of the scientist and mathematician Henri Poincaré. A cousin of Aline, Raymond Poincaré was to be President of France.\n\nHe occupied the mathematics chair at Princeton University from 1913 until 1914. He occupied the History of sciences chair from 1920 to 1922.\n\nBoutroux published his major work \"Les principes de l'analyse mathématique\" in two volumes; \"Volume 1\" in 1914 and \"Volume 2\" in 1919. This is a comprehensive view of the whole field of mathematics at the time.\n\nHe was an Invited Speaker of the ICM in 1904 at Heidelberg, in 1908 at Rome, and in 1920 at Strasbourg.\n\nHe died on 15 August 1922, aged 41 years.\n\n\n\n"}
{"id": "17543768", "url": "https://en.wikipedia.org/wiki?curid=17543768", "title": "Pseudo-order", "text": "Pseudo-order\n\nIn constructive mathematics, a pseudo-order is a constructive generalisation of a linear order to the continuous case. The usual trichotomy law does not hold in the constructive continuum because of its indecomposability, so this condition is weakened. \n\nA pseudo-order is a binary relation satisfying the following conditions:\nThis first condition is simply antisymmetry. It follows from the first two conditions that a pseudo-order is transitive. The second condition is often called \"co-transitivity\" or \"comparison\" and is the constructive substitute for trichotomy. In general, given two elements of a pseudo-ordered set, it is not always the case that either one is less than the other or else they are equal, but given any nontrivial interval, any element is either above the lower bound, or below the upper bound.\n\nThe third condition is often taken as the definition of equality. The natural apartness relation on a pseudo-ordered set is given by\nand equality is defined by the negation of apartness.\n\nThe negation of the pseudo-order is a partial order which is close to a total order: if \"x\" ≤ \"y\" is defined as the negation of \"y\" < \"x\", then we have\nUsing classical logic one would then conclude that \"x\" ≤ \"y\" or \"y\" ≤ \"x\", so it would be a total order. However, this inference is not valid in the constructive case.\n\nThe prototypical pseudo-order is that of the real numbers: one real number is less than another if there exists (one can construct) a rational number greater than the former and less than the latter. In other words, \"x\" < \"y\" if there exists a rational number \"z\" such that \"x\" < \"z\" < \"y\".\n\nhttps://books.google.com/books/about/Intuitionism.html?id=4rhLAAAAMAAJ\n"}
{"id": "35922668", "url": "https://en.wikipedia.org/wiki?curid=35922668", "title": "Quillen's lemma", "text": "Quillen's lemma\n\nIn algebra, Quillen's lemma states that an endomorphism of a simple module over the enveloping algebra of a finite-dimensional Lie algebra over a field \"k\" is algebraic over \"k\". In contrast to a version of Schur's lemma due to Dixmier, it does not require \"k\" to be uncountable. Quillen's original short proof uses generic flatness.\n"}
{"id": "32561553", "url": "https://en.wikipedia.org/wiki?curid=32561553", "title": "Ruy de Queiroz", "text": "Ruy de Queiroz\n\nRuy J. Guerra B. de Queiroz (born January 11, 1958 in Recife) is an associate professor at Universidade Federal de Pernambuco and holds significant works in the research fields of Mathematical logic, proof theory, foundations of mathematics and philosophy of mathematics. He is the founder of the Workshop on Logic, Language, Information and Computation (WoLLIC), which has been organised annually since 1994, typically in June or July.\n\nRuy de Queiroz received his B.Eng in Electrical Engineering from Escola Politecnica de Pernambuco in 1980, his M.Sc in Informatics from Universidade Federal de Pernambuco in 1984, and his Ph.D in Computing from the Imperial College, London in 1990, for which he defended the Dissertation \"Proof Theory and Computer Programming. An Essay into the Logical Foundations of Computation\".\n\nIn the late 1980s, Ruy de Queiroz has offered a reformulation of Martin-Löf type theory based on a novel reading of Wittgenstein’s \"meaning-is-use\", where the explanation of the consequences of a given proposition gives the meaning to the logical constant dominating the proposition. This amounts to a non-dialogical interpretation of logical constants via the effect of elimination rules over introduction rules, which finds a parallel in Paul Lorenzen's and Jaakko Hintikka's dialogue/game-semantics. This led to a type theory called \"Meaning as Use Type Theory\". In reference to the use of Wittgenstein's dictum, he has shown that the aspect concerning the explanation of the consequences of a proposition is present since a very early date when in a letter to Bertrand Russell, where Wittgenstein refers to the universal quantifier only having meaning when one sees what follows from it.\n\nSince later in the 1990s, Ruy de Queiroz has been engaged, jointly with Dov Gabbay, in a program of providing a general account of the functional interpretation of classical and non-classical logics via the notion of labeled natural deduction. As a result, novel accounts of the functional interpretation of the existential quantifier, as well as the notion of propositional equality, were put forward, the latter allowing for a recasting of Richard Statman's notion of direct computation, and a novel approach to the dichotomy \"intensional versus extensional\" accounts of propositional equality via the Curry-Howard correspondence.\n\nSince the early 2000s, Ruy de Queiroz has been investigating, jointly with Anjolina de Oliveira, a geometric perspective of natural deduction based on a graph-based account of Kneale's symmetric natural deduction.\n\n\n\nRuy de Queiroz has taught several disciplines related to logic and theoretical computer science, including Set Theory, Recursion Theory (as a follow-up to a course given by Solomon Feferman), Logic for Computer Science, Discrete Mathematics, Theory of Computation, Proof Theory, Model Theory, Foundations of Cryptography. He has had seven Ph.D. students in the fields of Mathematical Logic and Theoretical Computer Science.\n\n\n"}
{"id": "768388", "url": "https://en.wikipedia.org/wiki?curid=768388", "title": "Southeast (disambiguation)", "text": "Southeast (disambiguation)\n\nSoutheast is a compass point.\n\nSoutheast, south-east, south east, southeastern, south-eastern, or south eastern may also refer to:\n\n\n\n\n"}
{"id": "305463", "url": "https://en.wikipedia.org/wiki?curid=305463", "title": "Steinhaus–Moser notation", "text": "Steinhaus–Moser notation\n\nIn mathematics, Steinhaus–Moser notation is a notation for expressing certain extremely large numbers. It is an extension of Steinhaus's polygon notation.\n\netc.: written in an ()-sided polygon is equivalent with \"the number inside nested -sided polygons\". In a series of nested polygons, they are associated inward. The number inside two triangles is equivalent to inside one triangle, which is equivalent to raised to the power of .\n\nSteinhaus defined only the triangle, the square, and the circle , which is equivalent to the pentagon defined above.\n\nSteinhaus defined:\n\nMoser's number is the number represented by \"2 in a megagon\", where a megagon is a polygon with \"mega\" sides, not to be confused with the megagon, with one million sides.\n\nAlternative notations:\n\nA mega, ②, is already a very large number, since ② =\nsquare(square(2)) = square(triangle(triangle(2))) =\nsquare(triangle(2)) = \nsquare(triangle(4)) =\nsquare(4) =\nsquare(256) =\ntriangle(triangle(triangle(...triangle(256)...))) [256 triangles] =\ntriangle(triangle(triangle(...triangle(256)...))) [255 triangles] ~\ntriangle(triangle(triangle(...triangle(3.2 × 10)...))) [254 triangles] =\n\nUsing the other notation:\n\nmega = M(2,1,5) = M(256,256,3)\n\nWith the function formula_7 we have mega = formula_8 where the superscript denotes a functional power, not a numerical power.\n\nWe have (note the convention that powers are evaluated from right to left):\nSimilarly:\netc.\n\nThus:\n\nRounding more crudely (replacing the 257 at the end by 256), we get mega ≈ formula_17, using Knuth's up-arrow notation.\n\nAfter the first few steps the value of formula_18 is each time approximately equal to formula_19. In fact, it is even approximately equal to formula_20 (see also approximate arithmetic for very large numbers). Using base 10 powers we get:\n\nIt has been proven that in Conway chained arrow notation,\n\nand, in Knuth's up-arrow notation,\n\nTherefore, Moser's number, although incomprehensibly large, is vanishingly small compared to Graham's number:\n\n\n"}
{"id": "18033223", "url": "https://en.wikipedia.org/wiki?curid=18033223", "title": "Stencil code", "text": "Stencil code\n\nStencil codes are a class of iterative kernels\nwhich update array elements according to some fixed pattern, called a stencil. \nThey are most commonly found in the codes of computer simulations, e.g. for computational fluid dynamics in the context of scientific and engineering applications. \nOther notable examples include solving partial differential equations, the Jacobi kernel, the Gauss–Seidel method, image processing and cellular automata. \nThe regular structure of the arrays sets stencil codes apart from other modeling methods such as the Finite element method. Most finite difference codes which operate on regular grids can be formulated as stencil codes.\n\nStencil codes perform a sequence of sweeps (called timesteps) through a given array. Generally this is a 2- or 3-dimensional regular grid. The elements of the arrays are often referred to as cells. In each timestep, the stencil code updates all array elements. Using neighboring array elements in a fixed pattern (called the stencil), each cell's new value is computed. In most cases boundary values are left unchanged, but in some cases (e.g. LBM codes) those need to be adjusted during the computation as well. Since the stencil is the same for each element, the pattern of data accesses is repeated.\n\nMore formally, we may define stencil codes as a 5-tuple formula_1 with the following meaning:\n\n\nSince \"I\" is a \"k\"-dimensional integer interval, the array will always have the topology of a finite regular grid. The array is also called simulation space and individual cells are identified by their index formula_8. The stencil is an ordered set of formula_6 relative coordinates. We can now obtain for each cell formula_10 the tuple of its neighbors indices formula_11\n\nTheir states are given by mapping the tuple formula_11 to the corresponding tuple of states formula_14, where formula_15 is defined as follows:\n\nThis is all we need to define the system's state for the following time steps formula_17 with formula_18:\n\nNote that formula_20 is defined on formula_21 and not just on formula_22 since the boundary conditions need to be set, too. Sometimes the elements of formula_11 may be defined by a vector addition modulo the simulation space's dimension to realize toroidal topologies:\n\nThis may be useful for implementing periodic boundary conditions, which simplifies certain physical models.\n\nTo illustrate the formal definition, we'll have a look at how a two dimensional Jacobi iteration can be defined. The update function computes the arithmetic mean of a cell's four neighbors. In this case we set off with an initial solution of 0. The left and right boundary are fixed at 1, while the upper and lower boundaries are set to 0. After a sufficient number of iterations, the system converges against a saddle-shape.\n\nThe shape of the neighborhood used during the updates depends on the application itself. The most common stencils are the 2D or 3D versions of the von Neumann neighborhood and Moore neighborhood. The example above uses a 2D von Neumann stencil while LBM codes generally use its 3D variant. Conway's Game of Life uses the 2D Moore neighborhood. That said, other stencils such as a 25-point stencil for seismic wave propagation can be found, too.\n\nMany simulation codes may be formulated naturally as stencil codes. Since computing time and memory consumption grow linearly with the number of array elements, parallel implementations of stencil codes are of paramount importance to research. \nThis is challenging since the computations are tightly coupled (because of the cell updates depending on neighboring cells) and most stencil codes are memory bound (i.e. the ratio of memory accesses to calculations is high). \nVirtually all current parallel architectures have been explored for executing stencil codes efficiently; \nat the moment GPGPUs have proven to be most efficient.\n\nDue to both the importance of stencil codes to computer simulations and their high computational requirements, there are a number of efforts which aim at creating reusable libraries to support scientists in implementing new stencil codes. The libraries are mostly concerned with the parallelization, but may also tackle other challenges, such as IO, steering and checkpointing. They may be classified by their API.\n\nThis is a traditional design. The library manages a set of \"n\"-dimensional scalar arrays, which the user code may access to perform updates. The library handles the synchronization of the boundaries (dubbed ghost zone or halo). The advantage of this interface is that the user code may loop over the arrays, which makes it easy to integrate legacy codes\n. The disadvantage is that the library can not handle cache blocking (as this has to be done within the loops)\nor wrapping of the code for accelerators (e.g. via CUDA or OpenCL). Notable implementations include Cactus, a physics problem solving environment, and waLBerla.\n\nThese libraries move the interface to updating single simulation cells: only the current cell and its neighbors are exposed to the user code, e.g. via getter/setter methods. The advantage of this approach is that the library can control tightly which cells are updated in which order, which is useful not only to implement cache blocking, \nbut also to run the same code on multi-cores and GPUs. This approach requires the user to recompile the source code together with the library. Otherwise a function call for every cell update would be required, which would seriously impair performance. This is only feasible with techniques such as class templates or metaprogramming, which is also the reason why this design is only found in newer libraries. Examples are Physis and LibGeoDecomp.\n\n\n"}
{"id": "213508", "url": "https://en.wikipedia.org/wiki?curid=213508", "title": "Subtyping", "text": "Subtyping\n\nIn programming language theory, subtyping (also subtype polymorphism or inclusion polymorphism) is a form of type polymorphism in which a subtype is a datatype that is related to another datatype (the supertype) by some notion of substitutability, meaning that program elements, typically subroutines or functions, written to operate on elements of the supertype can also operate on elements of the subtype. If S is a subtype of T, the subtyping relation is often written S <: T, to mean that any term of type S can be \"safely used in a context where\" a term of type T is expected. The precise semantics of subtyping crucially depends on the particulars of what \"safely used in a context where\" means in a given programming language. The type system of a programming language essentially defines its own subtyping relation, which may well be trivial should the language support no (or very little) conversion mechanisms.\n\nDue to the subtyping relation, a term may belong to more than one type. Subtyping is therefore a form of type polymorphism. In object-oriented programming the term 'polymorphism' is commonly used to refer solely to this \"subtype polymorphism\", while the techniques of parametric polymorphism would be considered \"generic programming\".\n\nFunctional programming languages often allow the subtyping of records. Consequently, simply typed lambda calculus extended with record types is perhaps the simplest theoretical setting in which a useful notion of subtyping may be defined and studied . Because the resulting calculus allows terms to have more than one type, it is no longer a \"simple\" type theory. Since functional programming languages, by definition, support function literals, which can also be stored in records, records types with subtyping provide some of the features of object-oriented programming. Typically, functional programming languages also provide some, usually restricted, form of parametric polymorphism. In a theoretical setting, it is desirable to study the interaction of the two features; a common theoretical setting is system F. Various calculi that attempt to capture the theoretical properties of object-oriented programming may be derived from system F.\n\nThe concept of subtyping is related to the linguistic notions of hyponymy and holonymy. It is also related to the concept of bounded quantification in mathematical logic. Subtyping should not be confused with the notion of (class or object) inheritance from object-oriented languages; subtyping is a relation between types (interfaces in object-oriented parlance) whereas inheritance is a relation between implementations stemming from a language feature that allows new objects to be created from existing ones. In a number of object-oriented languages, subtyping is called interface inheritance, with inheritance referred to as \"implementation inheritance\".\n\nThe notion of subtyping in programming languages dates back to the 1960s; it was introduced in Simula derivatives. The first formal treatments of subtyping were given by John C. Reynolds in 1980 who used category theory to formalize implicit conversions, and Luca Cardelli (1985).\n\nThe concept of subtyping has gained visibility (and synonymy with polymorphism in some circles) with the mainstream adoption of object-oriented programming. In this context, the principle of safe substitution is often called the Liskov substitution principle, after Barbara Liskov who popularized it in a keynote address at a conference on object-oriented programming in 1987. Because it must consider mutable objects, the ideal notion of subtyping defined by Liskov and Jeannette Wing, called behavioral subtyping is considerably stronger than what can be implemented in a type checker. (See Function types below for details.)\n\nA simple practical example of subtypes is shown in the diagram, right. The type \"bird\" has three subtypes \"duck\", \"cuckoo\" and \"ostrich\". Conceptually, each of these is a variety of the basic \"bird\" that inherits many \"bird\" characteristics but has some specific differences. The UML notation is used in this diagram, with open-headed arrows showing the direction and type of the relationship between the supertype and its subtypes.\n\nAs a more practical example, a language might allow integer values to be used wherever floating point values are expected (codice_1 <: codice_2), or it might define a generic type <samp>Number</samp> as a common supertype of integers and the reals. In this second case, we only have codice_1 <: codice_4 and codice_2 <: codice_4, but codice_1 and codice_2 are not subtypes of each other.\n\nProgrammers may take advantage of subtyping to write code in a more abstract manner than would be possible without it. Consider the following example:\n\nIf integer and real are both subtypes of codice_4, and an operator of comparison with an arbitrary Number is defined for both types, then values of either type can be passed to this function. However, the very possibility of implementing such an operator highly constrains the Number type (for example, one can't compare an integer with a complex number), and actually only comparing integers with integers and reals with reals makes sense. Rewriting this function so that it would only accept 'x' and 'y' of the same type requires bounded polymorphism.\n\nSubtyping in type theory is characterized by the fact that any expression of type \"A\" may also be given type \"B\" if \"A\"<samp><:</samp>\"B\"; the formal typing rule that codifies this is known as the \"subsumption\" rule.\n\nType theorists make a distinction between nominal subtyping, in which only types declared in a certain way may be subtypes of each other, and structural subtyping, in which the structure of two types determines whether or not one is a subtype of the other. The class-based object-oriented subtyping described above is nominal; a structural subtyping rule for an object-oriented language might say that if objects of type \"A\" can handle all of the messages that objects of type \"B\" can handle (that is, if they define all the same methods), then \"A\" is a subtype of \"B\" regardless of whether either inherits from the other. This so-called \"duck typing\" is common in dynamically typed object-oriented languages. Sound structural subtyping rules for types other than object types are also well known.\n\nImplementations of programming languages with subtyping fall into two general classes: \"inclusive\" implementations, in which the representation of any value of type \"A\" also represents the same value at type \"B\" if \"A\"<samp><:</samp>\"B\", and \"coercive\" implementations, in which a value of type \"A\" can be \"automatically converted\" into one of type \"B\". The subtyping induced by subclassing in an object-oriented language is usually inclusive; subtyping relations that relate integers and floating-point numbers, which are represented differently, are usually coercive.\n\nIn almost all type systems that define a subtyping relation, it is reflexive (meaning \"A\"<samp><:</samp>\"A\" for any type \"A\") and transitive (meaning that if \"A\"<samp><:</samp>\"B\" and \"B\"<samp><:</samp>\"C\" then \"A\"<samp><:</samp>\"C\"). This makes it a preorder on types.\n\nTypes of records give rise to the concepts of \"width\" and \"depth\" subtyping. These express two different ways of obtaining a new type of record that allows the same operations as the original record type.\n\nRecall that a record is a collection of (named) fields. Since a subtype is a type which allows all operations allowed on the original type, a record subtype should support the same operations on the fields as the original type supported.\n\nOne kind of way to achieve such support, called \"width subtyping\", adds more fields to the record. More formally, every (named) field appearing in the width supertype will appear in the width subtype. Thus, any operation feasible on the supertype will be supported by the subtype.\n\nThe second method, called \"depth subtyping\", replaces the various fields with their subtypes. That is, the fields of the subtype are subtypes of the fields of the supertype. Since any operation supported for a field in the supertype is supported for its subtype, any operation feasible on the record supertype is supported by the record subtype. Depth subtyping only makes sense for immutable records: for example, you can assign 1.5 to the 'x' field of a real point (a record with two real fields), but you can't do the same to the 'x' field of an integer point (which, however, is a deep subtype of the real point type) because 1.5 is not an integer (see Variance).\n\nSubtyping of records can be defined in System F, which combines parametric polymorphism with subtyping of record types and is a theoretical basis for many functional programming languages that support both features.\n\nSome systems also support subtyping of labeled disjoint union types (such as algebraic data types). The rule for width subtyping is reversed: every tag appearing in the width subtype must appear in the width supertype.\n\nIf \"T\" → \"T\" is a function type, then a subtype of it is any function type \"S\" → \"S\" with the property that \"T\" <: \"S\" and \"S\" <: \"T\". This can be summarised using the following typing rule: \nformula_1\nThe argument type of \"S\" → \"S\" is said to be contravariant because the subtyping relation is reversed for it, whereas the return type is covariant. Informally, this reversal occurs because the refined type is \"more liberal\" in the types it accepts and \"more conservative\" in the type it returns. This is what exactly works in Scala: a \"n\"-ary function is internally a class that inherits the FunctionN(-A1, -A2, …, -An, +B) trait (which can be seen as a general interface in Java-like languages), where \"A1\", \"A2\", … \"An\" are the parameter types, and \"B\" is its return type; \"-\" before the type means the type is contravariant while \"+\" means covariant.\n\nIn languages that allow side effects, like most object-oriented languages, subtyping is generally not sufficient to guarantee that a function can be safely used in the context of another. Liskov's work in this area focused on behavioral subtyping, which besides the type system safety discussed in this article also requires that subtypes preserve all invariants guaranteed by the supertypes in some contract. This definition of subtyping is generally undecidable, so it cannot be verified by a type checker.\n\nThe subtyping of mutable references is similar to the treatment of function arguments and return values. Write-only references (or \"sinks\") are contravariant, like function arguments; read-only references (or \"sources\") are covariant, like return values. Mutable references which act as both sources and sinks are invariant.\n\nSubtyping and inheritance are independent (orthogonal) relationships. They may coincide, but none is a special case of the other. In other words, between two types \"S\" and \"T\", all combinations of subtyping and inheritance are possible:\nThe first case is illustrated by independent types, such as codice_10 and codice_2.\n\nThe second case can be illustrated by codice_12 and codice_13; in most object oriented programming languages, codice_13 is not derived by inheritance from codice_12, however codice_16. Since an codice_12 value can always be replaced by an codice_13 value, the Liskov substitution principle is satisfied; therefore codice_13 can be considered a subtype of codice_12.\n\nThe third case is a consequence of function subtyping input contravariance. Assume a super class of type \"T\" having a method \"m\" returning an object of the same type (\"i.e.\" the type of \"m\" is \"T → T\", also note that the first argument of \"m\" is this/self) and a derived class type \"S\" from \"T\". By inheritance, the type of \"m\" in \"S\" is \"S → S\". In order for \"S\" to be a subtype of \"T\" the type of \"m\" in \"S\" must be a subtype of the type of \"m\" in \"T\", in other words: \"S → S ≤: T → T\". By bottom-up application of the function subtyping rule, this means: \"S ≤: T\" and \"T ≤: S\", which is only possible if \"S\" and \"T\" are the same. Since inheritance is an irreflexive relation, \"S\" can't be a subtype of \"T\".\n\nSubtyping and inheritance are compatible when all inherited fields and methods of the derived type have types which are subtypes of the corresponding fields and methods from the inherited type .\n\nIn coercive subtyping systems, subtypes are defined by implicit type conversion functions from subtype to supertype. For each subtyping relationship (\"S\" <: \"T\"), a coercion function \"coerce\": \"S\" → \"T\" is provided, and any object \"s\" of type \"S\" is regarded as the object \"coerce\"(\"s\") of type \"T\". A coercion function may be defined by composition: if \"S\" <: \"T\" and \"T\" <: \"U\" then \"s\" may be regarded as an object of type \"u\" under the compound coercion (\"coerce\" ∘ \"coerce\"). The type coercion from a type to itself \"coerce\" is the identity function \"id\"\n\nCoercion functions for records and disjoint union subtypes may be defined componentwise; in the case of width-extended records, type coercion simply discards any components which are not defined in the supertype. The type coercion for function types may be given by \"f\"'(\"s\") = \"coerce\"(\"f\"(\"coerce\"(\"t\"))), reflecting the contravariance of function arguments and covariance of return values.\n\nThe coercion function is uniquely determined given the subtype and supertype. Thus, when multiple subtyping relationships are defined, one must be careful to guarantee that all type coercions are coherent. For instance, if an integer such as 2 : \"int\" can be coerced to a floating point number (say, 2.0 : \"float\"), then it is not admissible to coerce 2.1 : \"float\" to 2 : \"int\", because the compound coercion \"coerce\" given by \"coerce\" ∘ \"coerce\" would then be distinct from the identity coercion \"id\".\n\n\nTextbooks\n\n\nPapers\n\n"}
{"id": "28866227", "url": "https://en.wikipedia.org/wiki?curid=28866227", "title": "Sumner's conjecture", "text": "Sumner's conjecture\n\nDavid Sumner (a graph theorist at the University of South Carolina) conjectured in 1971 that tournaments are universal graphs for polytrees. More precisely, Sumner's conjecture (also called Sumner's universal tournament conjecture) states that every orientation of every formula_1-vertex tree is a subgraph of every formula_2-vertex tournament. The conjecture remains unproven; call it \"one of the most well-known problems on tournaments.\"\n\nLet polytree formula_3 be a star formula_4, in which all edges are oriented outward from the central vertex to the leaves. Then, formula_3 cannot be embedded in the tournament formed from the vertices of a regular formula_6-gon by directing every edge clockwise around the polygon. For, in this tournament, every vertex has indegree and outdegree equal to formula_7, while the central vertex in formula_3 has larger outdegree formula_9. Thus, if true, Sumner's conjecture would give the best possible size of a universal graph for polytrees.\n\nHowever, in every tournament of formula_10 vertices, the average outdegree is formula_11, and the maximum outdegree is an integer greater than or equal to the average. Therefore, there exists a vertex of outdegree formula_12, which can be used as the central vertex for a copy of formula_3.\n\nThe following partial results on the conjecture are known.\n\n conjectured that every orientation of an formula_1-vertex path graph (with formula_37) can be embedded as a subgraph into every formula_1-vertex tournament. After partial results by this was proven by .\n\nHavet and Thomassé in turn conjectured a strengthening of Sumner's conjecture, that every tournament on formula_39 vertices contains as a subgraph every polytree with at most formula_22 leaves.\n\n\n"}
{"id": "408354", "url": "https://en.wikipedia.org/wiki?curid=408354", "title": "Taxicab geometry", "text": "Taxicab geometry\n\nA taxicab geometry is a form of geometry in which the usual distance function or metric of Euclidean geometry is replaced by a new metric in which the distance between two points is the sum of the absolute differences of their Cartesian coordinates. The taxicab metric is also known as rectilinear distance, L\" distance, L\" distance or formula_1 norm (see \"L\" space), snake distance, city block distance, Manhattan distance or Manhattan length, with corresponding variations in the name of the geometry. The latter names allude to the grid layout of most streets on the island of Manhattan, which causes the shortest path a car could take between two intersections in the borough to have length equal to the intersections' distance in taxicab geometry.\n\nThe geometry has been used in regression analysis since the 18th century, and today is often referred to as LASSO. The geometric interpretation dates to non-Euclidean geometry of the 19th century and is due to Hermann Minkowski.\n\nThe taxicab distance, formula_2, between two vectors formula_3 in an \"n\"-dimensional real vector space with fixed Cartesian coordinate system, is the sum of the lengths of the projections of the line segment between the points onto the coordinate axes. More formally,\nwhere formula_5 are vectors\n\nFor example, in the plane, the taxicab distance between formula_7 and formula_8 is formula_9\n\nTaxicab distance depends on the rotation of the coordinate system, but does not depend on its reflection about a coordinate axis or its translation. Taxicab geometry satisfies all of Hilbert's axioms (a formalization of Euclidean geometry) except for the side-angle-side axiom, as two triangles with equally \"long\" two sides and an identical angle between them are typically not congruent unless the mentioned sides happen to be parallel.\n\nA circle is a set of points with a fixed distance, called the \"radius\", from a point called the \"center\". In taxicab geometry, distance is determined by a different metric than in Euclidean geometry, and the shape of circles changes as well. Taxicab circles are squares with sides oriented at a 45° angle to the coordinate axes. The image to the right shows why this is true, by showing in red the set of all points with a fixed distance from a center, shown in blue. As the size of the city blocks diminishes, the points become more numerous and become a rotated square in a continuous taxicab geometry. While each side would have length formula_10 using a Euclidean metric, where \"r\" is the circle's radius, its length in taxicab geometry is 2\"r\". Thus, a circle's circumference is 8\"r\". Thus, the value of a geometric analog to formula_11 is 4 in this geometry. The formula for the unit circle in taxicab geometry is formula_12 in Cartesian coordinates and\n\nin polar coordinates.\n\nA circle of radius 1 (using this distance) is the von Neumann neighborhood of its center.\n\nA circle of radius \"r\" for the Chebyshev distance (L metric) on a plane is also a square with side length 2\"r\" parallel to the coordinate axes, so planar Chebyshev distance can be viewed as equivalent by rotation and scaling to planar taxicab distance. However, this equivalence between L and L metrics does not generalize to higher dimensions.\n\nWhenever each pair in a collection of these circles has a nonempty intersection, there exists an intersection point for the whole collection; therefore, the Manhattan distance forms an injective metric space.\n\nIn chess, the distance between squares on the chessboard for rooks is measured in taxicab distance; kings and queens use Chebyshev distance, and bishops use the taxicab distance (between squares of the same color) on the chessboard rotated 45 degrees, i.e., with its diagonals as coordinate axes. To reach from one square to another, only kings require the number of moves equal to their respective distance; rooks, queens and bishops require one or two moves (on an empty board, and assuming that the move is possible at all in the bishop's case).\n\nIn solving an underdetermined system of linear equations, the regularisation term for the parameter vector is expressed in terms of the formula_14-norm (taxicab geometry) of the vector. This approach appears in the signal recovery framework called compressed sensing.\n\nTaxicab geometry can be used to assess the differences in discrete frequency distributions. For example, in RNA splicing positional distributions of hexamers, which plot the probability of each hexamer appearing at each given nucleotide near a splice site, can be compared with L1-distance. Each position distribution can be represented as a vector where each entry represents the likelihood of the hexamer starting at a certain nucleotide. A large L1-distance between the two vectors indicates a significant difference in the nature of the distributions while a small distance denotes similarly shaped distributions. This is equivalent to measuring the area between the two distribution curves because the area of each segment is the absolute difference between the two curves' likelihoods at that point. When summed together for all segments, it provides the same measure as L1-distance.\n\nThe \"L\" metric was used in regression analysis in 1757 by Roger Joseph Boscovich. The geometric interpretation dates to the late 19th century and the development of non-Euclidean geometries, notably by Hermann Minkowski and his Minkowski inequality, of which this geometry is a special case, particularly used in the geometry of numbers, . The formalization of \"L\" spaces is credited to .\n\n\n\n"}
{"id": "995908", "url": "https://en.wikipedia.org/wiki?curid=995908", "title": "Tomographic reconstruction", "text": "Tomographic reconstruction\n\nTomographic reconstruction is a type of multidimensional inverse problem where the challenge is to yield an estimate of a specific system from a finite number of projections. The mathematical basis for tomographic imaging was laid down by Johann Radon. A notable example of applications is the reconstruction of computed tomography (CT) where cross-sectional images of patients are obtained in non-invasive manner. Recent developments have seen the Radon transform and its inverse used for tasks related to realistic object insertion required for testing and evaluating computed tomography use in airport security.\n\nThis article applies in general to reconstruction methods for all kinds of tomography, but some of the terms and physical descriptions refer directly to the reconstruction of X-ray computed tomography.\n\nThe projection of an object, resulting from the tomographic measurement process at a given angle formula_1, is made up of a set of line integrals (see Fig. 1). A set of many such projections under different angles organized in 2D is called sinogram (see Fig. 3). In X-ray CT, the line integral represents the total attenuation of the beam of x-rays as it travels in a straight line through the object. As mentioned above, the resulting image is a 2D (or 3D) model of the attenuation coefficient. That is, we wish to find the image formula_2. The simplest and easiest way to visualise the method of scanning is the system of parallel projection, as used in the first scanners. For this discussion we consider the data to be collected as a series of parallel rays, at position, across a projection at angle formula_1. This is repeated for various angles. Attenuation occurs exponentially in tissue:\n\nwhere formula_2 is the attenuation coefficient as a function of position. Therefore, generally the total attenuation formula_6 of a ray at position, on the projection at angle formula_1, is given by the line integral:\n\nUsing the coordinate system of Figure 1, the value of formula_9 onto which the point formula_10 will be projected at angle formula_1 is given by:\n\nSo the equation above can be rewritten as\n\nwhere formula_14 represents formula_2. This function is known as the Radon transform (or \"sinogram\") of the 2D object.\n\nThe Fourier Transform of the projection can be written as\n\nformula_16 where formula_17\n\nformula_18 represents a slice of the 2D Fourier transform of formula_14 at angle formula_1. Using the inverse Fourier transform, the inverse Radon transform formula can be easily derived.\n\nformula_21\n\nwhere formula_22 is the derivative of the Hilbert transform of formula_23\n\nIn theory, the inverse Radon transformation would yield the original image. The projection-slice theorem tells us that if we had an infinite number of one-dimensional projections of an object taken at an infinite number of angles, we could perfectly reconstruct the original object, formula_14. However, there will only be a finite number of projections available in practice.\n\nAssuming formula_14 has effective diameter formula_26 and desired resolution is formula_27, rule of thumb number of projections needed for reconstruction is formula_28\n\nPractical reconstruction algorithms have been developed to implement the process of reconstruction of a 3-dimensional object from its projections. These algorithms are designed largely based on the mathematics of the Radon transform, statistical knowledge of the data acquisition process and geometry of the data imaging system.\n\nReconstruction can be made using interpolation. Assume formula_29-projections of formula_14 are generated at equally spaced angles, each sampled at same rate. The Discrete Fourier transform on each projection will yield sampling in the frequency domain. Combining all the frequency-sampled projections would generate a polar raster in the frequency domain. The polar raster will be sparse so interpolation is used to fill the unknown DFT points and reconstruction can be done through inverse Discrete Fourier transform. Reconstruction performance may improve by designing methods to change the sparsity of the polar raster, facilitating the effectiveness of interpolation.\n\nFor instance, a concentric square raster in the frequency domain can be obtained by changing the angle between each projection as follow:\n\nformula_31\n\nwhere formula_32 is highest frequency to be evaluated.\n\nThe concentric square raster improves computational efficiency by allowing all the interpolation positions to be on rectangular DFT lattice. Furthermore, it reduces the interpolation error. Yet, the Fourier-Transform algorithm has a disadvantage of producing inherently noisy output.\n\nIn practice of tomographic image reconstruction, often a stabilized and discretized version of the inverse Radon transform is used, known as the filtered back projection algorithm.\n\nWith a sampled discrete system, the inverse Radon Transform is\n\nformula_33\n\nformula_34\n\nwhere formula_35 is the angular spacing between the projections and formula_36 is radon kernel with frequency response formula_37.\n\nThe name back-projection comes from the fact that 1D projection needs to be filtered by 1D Radon kernel (back-projected) in order to obtain a 2D signal. The filter used does not contain DC gain, thus adding DC bias may be desirable. Reconstruction using back-projection allows better resolution than interpolation method described above. However, it induces greater noise because the filter is prone to amplify high-frequency content.\n\nIterative algorithm is computationally intensive but it allows to include \"a priori\" information about the system formula_14.\n\nLet formula_29 be the number of projections, formula_40 be the distortion operator for formula_41th projection taken at an angle formula_42. formula_43 are set of parameters to optimize the conversion of iterations.\n\nformula_44\n\nformula_45\n\nAn alternative family of recursive tomographic reconstruction algorithms are the Algebraic Reconstruction Techniques and iterative Sparse Asymptotic Minimum Variance.\n\nUse of a noncollimated fan beam is common since a collimated beam of radiation is difficult to obtain. Fan beams will generate series of line integrals, not parallel to each other, as projections. The fan-beam system will require 360 degrees range of angles which impose mechanical constraint, however, it allows faster signal acquisition time which may be advantageous in certain settings such as in the field of medicine. Back projection follows a similar 2 step procedure that yields reconstruction by computing weighted sum back-projections obtained from filtered projections.\n\nFor flexible tomographic reconstruction, open source toolboxes are available, such as TomoPy, ODL or the ASTRA toolbox. TomoPy is an open-source Python toolbox to perform tomographic data processing and image reconstruction tasks at the Advanced Photon Source at Argonne National Laboratory. TomoPy toolbox is specifically designed to be easy to use and deploy at a synchrotron facility beamline. It supports reading many common synchrotron data formats from disk through Scientific Data Exchange, and includes several other processing algorithms commonly used for synchrotron data. TomoPy also includes several reconstruction algorithms, which can be run on multi-core workstations and large-scale computing facilities. The ASTRA Toolbox is a MATLAB toolbox of high-performance GPU primitives for 2D and 3D tomography, from 2009–2014 developed by iMinds-Vision Lab, University of Antwerp and since 2014 jointly developed by iMinds-VisionLab, UAntwerpen and CWI, Amsterdam. The toolbox supports parallel, fan, and cone beam, with highly flexible source/detector positioning. A large number of reconstruction algorithms are available through TomoPy and the ASTRA toolkit, including FBP, Gridrec, ART, SIRT, SART, BART, CGLS, PML, MLEM and OSEM. Recently, the ASTRA toolbox has been integrated in the TomoPy framework. By integrating the ASTRA toolbox in the TomoPy framework, the optimized GPU-based reconstruction methods become easily available for synchrotron beamline users, and users of the ASTRA toolbox can more easily read data and use TomoPy’s other functionality for data filtering and artifact correction.\n\nShown in the gallery is the complete process for a simple object tomography and the following tomographic reconstruction based on ART.\n\n\n"}
{"id": "8421352", "url": "https://en.wikipedia.org/wiki?curid=8421352", "title": "UML-based web engineering", "text": "UML-based web engineering\n\nUWE (UML-based Web Engineering) is a software engineering approach for the development of Web applications. UWE provides a UML profile (UML extension), a metamodel, model-driven development process and tool support (ArgoUWE) for the systematic design of Web applications. UWE follows the separation of concerns building separate models for requirements, content, hypertext, presentation, process, adaptivity and architecture. \n\nThe key aspects that distinguish UWE are reliance on OMG standards and an open source environment. \n\n\n"}
{"id": "1869181", "url": "https://en.wikipedia.org/wiki?curid=1869181", "title": "Unique negative dimension", "text": "Unique negative dimension\n\nUnique negative dimension (UND) is a complexity measure for the model of learning from positive examples.\nThe unique negative dimension of a class formula_1 of concepts is the size of the maximum subclass formula_2 such that for every concept formula_3, we have formula_4 is nonempty.\n\nThis concept was originally proposed by M. Gereb-Graus in \"Complexity of learning from one-side examples\", Technical Report TR-20-89, Harvard University Division of Engineering and Applied Science, 1989.\n\n"}
{"id": "54321373", "url": "https://en.wikipedia.org/wiki?curid=54321373", "title": "Étale spectrum", "text": "Étale spectrum\n\nIn algebraic geometry, the étale spectrum of a commutative ring or an E-ring, denoted by Spec or Spét, is an analog of the prime spectrum Spec of a commutative ring that is obtained by replacing Zariski topology with étale topology. The precise definition depends on one's formalism. But the idea of the definition itself is simple. The usual prime spectrum Spec enjoys the relation: for a scheme (\"S\", \"O\") and a commutative ring \"A\",\nwhere Hom on the left is for morphisms of schemes and Hom on the right ring homomorphisms. This is to say Spec is the right adjoint to the global section functor formula_2. So, roughly, one can (and typically does) simply define the étale spectrum Spét to be the right adjoint to the global section functor on the category of \"spaces\" with étale topology.\n\nOver a field of characteristic zero, K. Behrend constructs the étale spectrum of a graded algebra called a perfect resolving algebra. He then defines a differential graded scheme (a type of a derived scheme) as one that is étale-locally such an étale spectrum.\n\nThe notion makes sense in the usual algebraic geometry but appears more frequently in the context of derived algebraic geometry.\n\n"}
{"id": "56030164", "url": "https://en.wikipedia.org/wiki?curid=56030164", "title": "Χ-bounded", "text": "Χ-bounded\n\nIn graph theory, a formula_1-bounded family formula_2 of graphs is one for which there is some function formula_3 such that, for every integer formula_4 the graphs in formula_2 with no formula_4-vertex clique can be colored with at most formula_7 colors. This concept and its notation were formulated by András Gyárfás. The use of the Greek letter chi in the term formula_1-bounded is based on the fact that the chromatic number of a graph formula_9 is commonly denoted formula_10.\n\nIt is not true that the family of all graphs is formula_1-bounded.\nAs and showed, there exist triangle-free graphs of arbitrarily large chromatic number, so for these graphs it is not possible to define a finite value of formula_12.\nThus, formula_1-boundedness is a nontrivial concept, true for some graph families and false for others.\n\nEvery class of graphs of bounded chromatic number is (trivially) formula_1-bounded, with formula_7 equal to the bound on the chromatic number. This includes, for instance, the planar graphs, the bipartite graphs, and the graphs of bounded degeneracy. Complementarily, the graphs in which the independence number is bounded are also formula_1-bounded, as Ramsey's theorem implies that they have large cliques.\n\nVizing's theorem can be interpreted as stating that the line graphs are formula_1-bounded, with formula_18. The claw-free graphs more generally are also formula_1-bounded with formula_20. This can be seen by using Ramsey's theorem to show that, in these graphs, a vertex with many neighbors must be part of a large clique.\nThis bound is nearly tight in the worst case, but connected claw-free graphs that include three mutually-nonadjacent vertices have even smaller chromatic number, formula_21.\n\nOther formula_1-bounded graph families include:\n\nHowever, although intersection graphs of convex shapes, circle graphs, and outerstring graphs are all special cases of string graphs, the string graphs themselves are not formula_1-bounded.\nThey include as a special case the intersection graphs of line segments, which are also not formula_1-bounded.\n\nAccording to the Gyárfás–Sumner conjecture, for every tree formula_26, the graphs that do not contain formula_26 as an induced subgraph are formula_1-bounded. \nFor instance, this would include the case of claw-free graphs, as a claw is a special kind of tree.\nHowever, the conjecture is known to be true only for certain special trees, including paths and radius-two trees.\n\nAnother unsolved problem on formula_1-bounded was posed by Louis Esperet, who asked whether every hereditary class of graphs that is formula_1-bounded has a function formula_7 that grows at most polynomially as a function of formula_4.\n"}
