{"id": "6317356", "url": "https://en.wikipedia.org/wiki?curid=6317356", "title": "208 (number)", "text": "208 (number)\n\n208 (two hundred [and] eight) is the natural number following 207 and preceding 209. \n\n208 is a practical number,\na tetranacci number, a rhombic matchstick number, a happy number, and a member of Aronson's sequence.\nThere are exactly 208 five-bead necklaces drawn from a set of beads with four colors,\nand 208 generalized weak orders on three labeled points.\n"}
{"id": "57181697", "url": "https://en.wikipedia.org/wiki?curid=57181697", "title": "AMS Distinguished Public Service Award", "text": "AMS Distinguished Public Service Award\n\nThe AMS Distinguished Public Service Award, awarded every 2 years by the American Mathematical Society, recognizes a research mathematician who has made a distinguished contribution to the mathematics profession during the preceding five years. It was first awarded in 1990.\n\nThe recipients of the AMS Distinguished Public Service Award are:\n\n"}
{"id": "16269602", "url": "https://en.wikipedia.org/wiki?curid=16269602", "title": "Abstract additive Schwarz method", "text": "Abstract additive Schwarz method\n\nIn mathematics, the abstract additive Schwarz method, named after Hermann Schwarz, is an abstract version of the additive Schwarz method for boundary value problems on partial differential equations, formulated only in terms of linear algebra without reference to domains, subdomains, etc. Many if not all domain decomposition methods can be cast as abstract additive Schwarz method, which is often the first and most convenient approach to their analysis.\n"}
{"id": "31018340", "url": "https://en.wikipedia.org/wiki?curid=31018340", "title": "Annals of Applied Probability", "text": "Annals of Applied Probability\n\nIts 2009 MCQ was 1.02. Its 2015 impact factor was 1.755, and it was 1.454 in 2014.\n"}
{"id": "37758875", "url": "https://en.wikipedia.org/wiki?curid=37758875", "title": "Antifragile", "text": "Antifragile\n\nAntifragile: Things That Gain From Disorder is a book by Nassim Nicholas Taleb published on November 27, 2012, by Random House in the United States and Penguin in the United Kingdom. This book builds upon ideas from his previous works including \"Fooled by Randomness\" (2001), (2007–2010), and \"The Bed of Procrustes\" (2010–2016) and is the fourth book in the five-volume philosophical treatise on uncertainty titled \"Incerto\". Some of the ideas are expanded in Taleb’s fifth book \"Skin in the Game: Hidden Asymmetries in Daily Life\" (2018).\n\nTaleb introduces the book as follows: \"Some things benefit from shocks; they thrive and grow when exposed to volatility, \"randomness\", disorder, and stressors and love adventure, risk, and uncertainty. Yet, in spite of the ubiquity of the phenomenon, there is no word for the exact opposite of fragile. Let us call it antifragile. Antifragility is beyond resilience or robustness. The resilient resists shocks and stays the same; the antifragile gets better\". \nThe phenomenon is well studied in medicine, where for example Wolff's law describes how bones grow stronger due to external load.\nHormesis is an example of mild antifragility, where the stressor is a poisonous substance and the antifragile becomes better overall from a small dose of the stressor. This is different from robustness or resilience in that the antifragile system improves with, not withstands, stressors, where the stressors are neither too large or small. The larger point, according to Taleb, is that depriving systems of vital stressors is not necessarily a good thing and can be downright harmful.\n\nMore technically, Taleb defines antifragility as a nonlinear response: \"Simply, antifragility is defined as a convex response to a stressor or source of harm (for some range of variation), leading to a positive sensitivity to increase in volatility (or variability, stress, dispersion of outcomes, or uncertainty, what is grouped under the designation \"disorder cluster\"). Likewise fragility is defined as a concave sensitivity to stressors, leading a negative sensitivity to increase in volatility. The relation between fragility, convexity and sensitivity to disorder is mathematical, obtained by theorem, not derived from empirical data mining or some historical narrative. It is a priori\".\n\nAs the book progresses, Taleb covers in great depth the domain of the fragile and the opposing domain of the antifragile showing how fragility can be detected, measured, and transformed. Recurring themes throughout the book include Skin in the Game, Via Negativa, Lindy Effect, Barbell Strategy and the Green Lumber Fallacy.\n\nThe concept of antifragility has been applied in physics, risk analysis, molecular biology, transportation planning, engineering, Aerospace (NASA), megaproject management, and computer science.\n\nIn computer science, there is a structured proposal for an \"Antifragile Software Manifesto\", to react to traditional system designs. The major idea is to develop antifragility by design, building a system which improves from environment's input.\n\nTo have \"skin in the game\" is to have incurred risk by being involved in achieving a goal. Taleb extends the definition to include any risk so that “Every captain goes down with every ship”. This removes the agency problem or in other words “Situation in which the manager of a business is not the true owner, so he follows a strategy that cosmetically seems to be sound, but in a hidden way benefits him and makes him antifragile at the expense (fragility) of the true owners or society. When he is right, he collects large benefits; when he is wrong, others pay the price. Typically this problem leads to fragility, as it is easy to hide risks. It also affects politicians and academics. A major source of fragility.”\n\nTaleb’s next book \"Skin in the Game: Hidden Asymmetries in Daily Life\" furthers the idea, asserting it is necessary for fairness, commercial efficiency, and risk management, as well as being necessary to understand the world.\n\nVia negativa is a type of theological thinking that attempts to describe God by negation or in other words, by what God is not. Taleb expanded this definition to include more generally the focus on what something is not, in action, what to avoid or what not to do. Avoiding the doctor for minor illnesses or removing certain food from one’s diet to improve health are examples.\n\nA technology, or anything nonperishable, increases in life expectancy with every day of its life—unlike perishable items (such as humans, cats, dogs, and tomatoes). So a book that has been a hundred years in print is likely to stay in print another hundred years. The opposite is Neomania, a love of change for its own sake, a form of philistinism that does not comply with the Lindy effect and that understands fragility. Forecasts the future by adding, not subtracting.\n\nIn finance, a Barbell strategy is formed when a Trader invests in Long and Short duration bonds, but does not invest in Intermediate duration bonds. This strategy is useful when interest rates are rising; as the short term maturities are rolled over they receive a higher interest rate, raising the value. Taleb generalizes the phenomenon and applies it to other domains. Essentially it is the transformation of anything from fragile to antifragile.\n\nThe Green Lumber Fallacy refers to a kind of fallacy where one mistakes one important kind of knowledge for another; in other words, \"mistaking the source of important or even necessary knowledge, for another less visible from the outside, less tractable one... how many things we call 'relevant knowledge' aren’t so much so\". Mathematically, it is the use of an incorrect function that, by chance, returns the correct output, such that one conflates \"g\" (x) with \"f\" (x). The root of the fallacy is that although people may be focusing on the right things, due to complexity of the thing, they are not good enough to figure it out intellectually.\n\nThe term \"green lumber\" refers to a story by authors Jim Paul and Brendan Moynihan in their book \"What I Learned Losing A Million Dollars\", where a trader made a fortune trading lumber he thought was literally \"green\" rather than fresh cut. \"This gets at the idea that a supposed understanding of an investment rationale, a narrative or a theoretical model is unhelpful in practical trading.\"\n\nAn early occurrence of this fallacy is found in the ancient story of Thales. Aristotle explains that Thales reserved presses ahead of the olive harvest at a discount only to rent them out at a high price when demand peaked, following his predictions of a particularly good harvest. Aristotle attributes Thales’ success to his ability to correctly forecast the weather. However, it was not his ability to forecast that made Thales successful but that \"Thales put himself in a position to take advantage of his lack of knowledge… that he did not need to understand too much the messages from the stars… that was the very first option on record\".\n\nThe Green Lumber Fallacy only becomes a problem (namely, the Green Lumber Problem) when the perpetuation of the fallacy has a high, and opaque, negative impact. For example:\n\nToward the end of the book Taleb provides examples of the problems of agency and cherry-picking, calling them the Robert Rubin problem, the Joseph Stiglitz problem, and the Alan Blinder problem. In the last chapter (p. 412), for example, Taleb criticizes Alan Blinder, the former vice chairman of the board of governors of the Federal Reserve System for trying to sell him an investment product at Davos in 2008 which would allow an investor to circumvent the regulations limiting deposit insurance and to benefit from coverage for near unlimited amounts. Taleb commented that the scheme \"would allow the super-rich to scam taxpayers by getting free government-sponsored insurance\". He also criticized Blinder for using ex-regulators to game the system which they built in the first place and for voicing his opposition to policies of bank insurance that would hurt his business, i.e., claiming that what is good for his business is \"for the public good\". The event has been discussed in the media, but not denied by Blinder.\n\n\"Antifragile\" was a \"New York Times\" Bestseller and praised by critics in a litany of notable periodicals including the \"Harvard Business Review\", \"Fortune\" magazine, the \"New Statesman\", and \"The Economist\", and Forbes. Although Boyd Tonkin of \"The Independent\" criticized Taleb’s style as \"vulgar, silly, slapdash and infuriating\", of the ideas in the book he remarked \"time and again I returned to two questions about his core ideas: Is he right, and does it matter? My verdict? Yes, and yes.\" Michael Shermer gave the book a generally favorable review but Taleb responded in \"Nature\" magazine that \"Michael Shermer mischaracterizes the concept of ‘antifragility’... The relation of fragility, convexity and sensitivity to disorder is thus mathematical and not derived from empirical data.\"\n\nLess favorable reviews include Michiko Kakutani of \"The New York Times\", who described the book as being \"maddening, bold, repetitious, judgmental, intemperate, erudite, reductive, shrewd, self-indulgent, self-congratulatory, provocative, pompous, penetrating, perspicacious and pretentious.\" Taleb responded in turn by noting one of five errors from her review and questioning \"Is she crazy enough to engage a technical subject without asking for specialist advice, or even engaging in something as basic as Google search?\"\n\nSome of the negative reviews focus on Taleb's style and the overall structure of the book, particularly the difficulty to easily summarize on a cursory review. So although the book has a table of contents, chapter summaries and map, a summary of the book is difficult to discern as the content headers and summaries have no noticeable pattern and many of the titles are abstruse (e.g., Hungry Donkeys) which according to the author is by design intended to handicap book reviewers, forcing them to read the book in its entirety.\n\n"}
{"id": "30577937", "url": "https://en.wikipedia.org/wiki?curid=30577937", "title": "Attractiveness principle", "text": "Attractiveness principle\n\nAttractiveness Principle is one of System Dynamics archetypes. System archetypes describe common patterns of behavior in dynamic complex systems. Attractiveness principle is a variation of Limits to Growth archetype, with restrictions caused by multiple limits. The limiting factors here are each of different character and usually cannot be dealt with the same way and/or (and very likely) they cannot be all addressed.\n\nAttractiveness principle is a concept that incorporates the fact that any product or kind of business cannot ever be \"“all things to all people”\" though companies very often strive to follow this way. One needs to make necessary decisions on the characteristics of the product as it cannot be perfect in all dimensions. If she doesn’t, the product is not going to be successful as of the natural constraints (limited resources) it will have to face – sooner or later. It is a fact of life that (assuming we know the relationships among the system’s elements) we can influence, inhibit or remove some of these limits through making expert changes in the system. The archetype can help us to get the insight into the system behavior so we could identify and decide which limiting factors to inhibit before they inhibit the results we want to achieve. But there will always be some limits we are not able to reduce and simply “we have to learn to live with them” and make compromises between our goals.\n\nKnowledge of the attractiveness principle system archetype is essential in management of various projects and businesses. Managers decide which problem is more attractive in terms of possible future improvement of the company – the origin of the archetype’s name actually came from this point. Manager as a decision-maker needs efficient support in solving such complex problems, and system dynamics can play this role. Its main advantage is the ability to reach higher complexity and to provide simultaneous calculations. These can be used to determine the future possible behavior of the system. If managers are able to recognize the archetype in a problem it often helps them to solve it with less cost and they are also often able to change its structure.\n\nThe term attractiveness principle was first used by inventor of system dynamics Jay W. Forrester. According to Forrester, the only way to control growth is to control attractiveness. Other references on this topic can be found in The Systems Thinker and in The Fifth Discipline Fieldbook in articles and parts by Michael Goodman and Art Kleiner.\n\nThe system is made up of a reinforcing loop and at least two balancing loops. See causal loop diagram and stock and flow diagram for the insight into model fundamentals.\n\nThe Reinforcing Loop (R1 in Figure 1 and 2) represents accelerating growth – a growing action is producing results. This is a positive feedback loop – the more the growing action taken, the higher the results level, and yet the result itself produces even more of growing action.\nBalancing loops (B2 and B3 in Figure 1 and 2) represent the way the system turns back to its original state. Result produced in the reinforcing loop is influenced within the balancing loop. There are (at least) two limits causing the slowing actions in the system and adding to them. Limiting actions start to influence the system at various levels of results, generally. Since that moment slowing actions act in the system simultaneously. Both the slowing actions contribute to the total slowing action. Total slowing action then inhibits the results (this process is delayed in time).\nIf we get back to the reinforcing loop then we can see the inhibited results are reducing growing action which is leading to the reduced results again.\n\nTrade-offs must be calculated to decide which ones of limits to focus on and address first. The one that is more attractive in terms of future benefit to the results should be chosen to be dealt with. It is necessary to compare the future situations after removing each of the slowing actions and their values in terms of reaching the desired result. But not only the one that will have a greater impact should be chosen but a possible synergetic effect when removing interdependent limits should be considered when making a decision.\n\nGraphs in Figure 3 and 4 show the results of simulation in Simgua simulation tool.\n\nYou can also run the Simgua Attractiveness principle model.\n\nThere is a project with a negative impact of a risk. Some people were taken off the team, there are unexpected changes in the project content and the economic circumstances have changed, too. The indicators of its quality, schedule and costs need to be kept up. Management’s task is to allocate the resources as well as possible in terms of the project’s indicators. Due to the fact the resources are limited it is necessary to make tradeoffs among opportunities.\n\nJay W. Forrester studied the attractiveness principle of geographical areas. He states that all the places in the world tend to the equilibrium where they are all equally attractive, no matter the population class. Let attractivity be the overall rating of a city in terms of its desirability for potential inhabitants. If a city has high attractivity people move to this city, which increases the prices of housing which is getting scarce, cause overloading of job opportunities (leading to unemployment), the environmental stress is rising, city getting overcrowded etc. These changes demonstrate the impact of the movement as of an equalizing process which makes the mentioned city less and less attractive – to the (idealized) point when no one wants to move to it anymore.\nWe can illustrate this situation by Forester’s words:\nAs stated by Richard C. Duncan, using Forrester’s Word dynamics model to predict the behavior of Third World countries shows that it is not possible to stop the immigration from these countries to USA as these countries can never reach the USA’s level of geographical attractiveness (and so there always will be a tendency to immigrate).\n\nHere is a list of possible effective strategies to deal with Attractiveness principle in praxis based on.\n\nIt is important to have in mind that dynamic complexity is very often counterintuitive – cause and effect are distant in time and space, but decision-makers rather tend to look for causes “near” their effects. The solution is not to concentrate on the symptoms of the problem, but on its causes.\n\n\n"}
{"id": "17919686", "url": "https://en.wikipedia.org/wiki?curid=17919686", "title": "Bareiss algorithm", "text": "Bareiss algorithm\n\nIn mathematics, the Bareiss algorithm, named after Erwin Bareiss, is an algorithm to calculate the determinant or the echelon form of a matrix with integer entries using only integer arithmetic; any divisions that are performed are guaranteed to be exact (there is no remainder). The method can also be used to compute the determinant of matrices with (approximated) real entries, avoiding the introduction any round-off errors beyond those already present in the input.\n\nDuring the execution of Bareiss algorithm, every integer that is computed is the determinant of a submatrix of the input matrix. This allows, using the Hadamard inequality, to bound the size of these integers. Otherwise, the Bareiss algorithm may be viewed as a variant of Gaussian elimination and needs roughly the same number of arithmetic operations.\n\nIt follows that, for an \"n\" × \"n\" matrix of maximum (absolute) value 2 for each entry, the Bareiss algorithm runs in O(\"n\") elementary operations with an O(\"n\" 2) bound on the absolute value of intermediate values needed. Its computational complexity is thus O(\"n\"\"L\" (log(\"n\") + \"L\")) when using elementary arithmetic or O(\"n\"\"L\" (log(\"n\") + \"L\") log(log(\"n\") + \"L\"))) by using fast multiplication.\n\nThe general Bareiss algorithm is distinct from the Bareiss algorithm for Toeplitz matrices.\n\nIn some Spanish-speaking countries, this algorithm is also known as Bareiss-Montante, because of René Mario Montante Pardo, a professor of the Universidad Autónoma de Nuevo León, Mexico, that popularized the method among his students. \n\n"}
{"id": "45428442", "url": "https://en.wikipedia.org/wiki?curid=45428442", "title": "Bernstein–Kushnirenko theorem", "text": "Bernstein–Kushnirenko theorem\n\nBernstein–Kushnirenko theorem (also known as BKK theorem or Bernstein–Khovanskii–Kushnirenko theorem ), proven by David Bernstein and in 1975, is a theorem in algebra. It states that the number of non-zero complex solutions of a system of Laurent polynomial equations formula_1 is equal to the mixed volume of the Newton polytopes of the polynomials formula_2, assuming that all non-zero coefficients of formula_3 are generic.\nA more precise statement is as follows:\n\nLet formula_4 be a finite subset of formula_5. Consider the subspace formula_6 of the Laurent polynomial algebra formula_7 consisting of Laurent polynomials whose exponents are in formula_4. That is:\n\nwhere formula_10 and for each formula_11 we have used the shorthand notation formula_12 to write the monomial formula_13.\n\nNow take formula_14 finite subsets formula_15 with the corresponding subspaces of Laurent polynomials formula_16.\nConsider a generic system of equations from these subspaces, that is:\n\nwhere each formula_18 is a generic element in the (finite dimensional vector space) formula_19.\n\nThe Bernstein–Kushnirenko theorem states that the number of solutions formula_20 of such a system \nis equal to \n\nwhere formula_22 denotes the Minkowski mixed volume and for each formula_23, formula_24 is the convex hull of the finite set of points formula_25. Clearly formula_24 is a \nconvex lattice polytope. It can be interpreted as the Newton polytope of a generic element of the subspace formula_19.\n\nIn particular, if all the sets formula_25 are the same formula_29, then the number of solutions of a generic system of Laurent polynomials from formula_6 is equal to \n\nwhere formula_32 is the convex hull of formula_4 and vol is the usual formula_14-dimensional Euclidean volume. Note that even though the volume of a lattice polytope is not necessarily an integer, it becomes an integer after multiplying by formula_35.\n\nKushnirenko's name is also spelt Kouchnirenko. David Bernstein is a brother of Joseph Bernstein. Askold Khovanskii has found about 15 different proofs of this theorem.\n"}
{"id": "32768148", "url": "https://en.wikipedia.org/wiki?curid=32768148", "title": "Bessel–Maitland function", "text": "Bessel–Maitland function\n\nIn mathematics, the Bessel–Maitland function, or Wright generalized Bessel function, is a generalization of the Bessel function, introduced by . The word \"Maitland\" in the name of the function seems to be the result of confusing Edward Maitland Wright's middle and last names. It is given by\n"}
{"id": "31837848", "url": "https://en.wikipedia.org/wiki?curid=31837848", "title": "Bit-length", "text": "Bit-length\n\nBit-length is the number of binary digits, called bits, necessary to represent an integer in the binary number system.\n\nAt their most fundamental level, digital computers and telecommunications devices (as opposed to analog devices) can process only data that has been expressed in binary format. The binary format expresses data as an arbitrary length series of values with one of two choices: Yes/No, 1/0, True/False, etc., all of which can be expressed electronically as On/Off. For information technology applications, the amount of information being processed is an important design consideration. The term bit-length is technical shorthand for this measure.\n\nFor example, computer processors are often designed to process data group into words of a given length of bits (8 bit, 16 bit, 32 bit, 64 bit, etc.). The bit-length of each word defines, for one thing, how many memory locations can be independently addressed by the processor. In public-key cryptography, keys are defined by their length expressed in binary digits - their bit length.\n\n"}
{"id": "27566216", "url": "https://en.wikipedia.org/wiki?curid=27566216", "title": "Cave5D", "text": "Cave5D\n\nCave5D is an adaptation of Vis5D to the CAVE for immersive virtual reality. It is released under the GNU GPL.\n\n\n"}
{"id": "28382324", "url": "https://en.wikipedia.org/wiki?curid=28382324", "title": "Dana Angluin", "text": "Dana Angluin\n\nDana Angluin is a professor of computer science at Yale University. She contributed to the foundations of computational learning theory.\n\nAngluin received her B.S. and Ph.D. at University of California, Berkeley. Her thesis, entitled \"An application of the theory of computational complexity to the study of inductive inference\" was one of the first works to apply complexity theory to the field of inductive inference.\n\nAngluin joined the faculty at Yale in 1979.\n\nDana Angluin has authored many papers and been a pioneer in many fields specifically learning regular sets from queries and counterexamples, robot navigation with distance queries, self-stabilizing universal algorithms and query learning of regular tree languages. A lot of Angluin's work involves queries, a field in which she has made many great contributions. Angluin also has worked in the field of robotics as well dealing with navigation with distance queries.\n\nProfessor Angluin is interested in machine learning and computational learning theory. Algorithmic modeling and analysis of learning tasks gives insight into the phenomena of learning, and suggests avenues for the creation of tools to help people learn, and for the design of \"smarter\" software and artificial agents that flexibly adapt their behavior. Professor Angluin's thesis was among the first work to apply computational complexity theory to the field of inductive inference. Her work on learning from positive data reversed a previous dismissal of that topic, and established a flourishing line of research. Her work on learning with queries established the models and the foundational results for learning with membership queries. Recently, her work has focused on the areas of coping with errors in the answers to queries, map-learning by mobile robots, and fundamental questions in modeling the interaction of a teacher and a learner.\n\nProfessor Angluin helped found the Computational Learning Theory (COLT) conference, and has served on program committees for COLT and on the COLT Steering committee. She served as an area editor for Information and Computation from 1989–1992. She organized Yale's Computer Science Department's Perlis Symposium in April 2001: \"From Statistics to Chat: Trends in Machine Learning\". She is a member of the Association for Computing Machinery and the Association for Women in Mathematics.\n\nAngluin has also published works on Ada Lovelace and her involvement with the Analytical Engine. Angluin is highly regarded as one of the best researchers in her field of Computer Science. Angluin continues to make more progress in her chosen field of queries at Yale.\n\nRepresentative Publications:\n\n\n"}
{"id": "249208", "url": "https://en.wikipedia.org/wiki?curid=249208", "title": "Divergence of the sum of the reciprocals of the primes", "text": "Divergence of the sum of the reciprocals of the primes\n\nThe sum of the reciprocals of all prime numbers diverges; that is:\n\nThis was proved by Leonhard Euler in 1737, and strengthens Euclid's 3rd-century-BC result that there are infinitely many prime numbers.\n\nThere are a variety of proofs of Euler's result, including a lower bound for the partial sums stating that\n\nfor all natural numbers . The double natural logarithm (ln ln) indicates that the divergence might be very slow, which is indeed the case. See Meissel–Mertens constant.\n\nFirst, we describe how Euler originally discovered the result. He was considering the harmonic series\n\nHe had already used the following \"product formula\" to show the existence of infinitely many primes.\n\nHere the product is taken over the set of all primes.\n\nSuch infinite products are today called Euler products. The product above is a reflection of the fundamental theorem of arithmetic. Euler noted that if there were only a finite number of primes, then the product on the right would clearly converge, contradicting the divergence of the harmonic series.\n\nEuler considered the above product formula and proceeded to make a sequence of audacious leaps of logic. First, he took the natural logarithm of each side, then he used the Taylor series expansion for as well as the sum of a converging series:\n\nfor a fixed constant . Then he invoked the relation\n\nwhich he explained, for instance in a later 1748 work, by setting in the Taylor series expansion\n\nThis allowed him to conclude that\n\nIt is almost certain that Euler meant that the sum of the reciprocals of the primes less than is asymptotic to as approaches infinity. It turns out this is indeed the case, and a more precise version of this fact was rigorously proved by Franz Mertens in 1874. Thus Euler obtained a correct result by questionable means.\n\nThe following proof by contradiction is due to Paul Erdős.\n\nLet denote the th prime number. Assume that the sum of the reciprocals of the primes converges; i.e.,\n\nThen there exists a smallest positive integer such that\n\nFor a positive integer , let denote the set of those in which are not divisible by any prime greater than (or equivalently all which are a product of powers of primes ). We will now derive an upper and a lower estimate for , the number of elements in . For large , these bounds will turn out to be contradictory.\n\nUpper estimate:\n\nLower estimate:\n\nThis produces a contradiction: when , the estimates (2) and (3) cannot both hold, because .\n\nHere is another proof that actually gives a lower estimate for the partial sums; in particular, it shows that these sums grow at least as fast as . The proof is an adaptation of the product expansion idea of Euler. In the following, a sum or product taken over always represents a sum or product taken over a specified set of primes.\n\nThe proof rests upon the following four inequalities:\n\n\n\n\nCombining all these inequalities, we see that\n\nDividing through by and taking the natural logarithm of both sides gives\n\nas desired. ∎\n\nUsing\n\n(see the Basel problem), the above constant can be improved to ; in fact it turns out that\n\nwhere is the Meissel–Mertens constant (somewhat analogous to the much more famous Euler–Mascheroni constant).\n\nFrom Dusart's inequality, we get\n\nThen\n\nby the integral test for convergence. This shows that the series on the left diverges.\n\nWhile the partial sums of the reciprocals of the primes eventually exceed any integer value, they never equal an integer.\n\nOne proof is by induction: The first partial sum is , which has the form . If the th partial sum (for ) has the form , then the st sum is\n\nas the st prime is odd; since this sum also has an form, this partial sum cannot be an integer (because 2 divides the denominator but not the numerator), and the induction continues.\n\nAnother proof rewrites the expression for the sum of the first reciprocals of primes (or indeed the sum of the reciprocals of \"any\" set of primes) in terms of the least common denominator, which is the product of all these primes. Then each of these primes divides all but one of the numerator terms and hence does not divide the numerator itself; but each prime \"does\" divide the denominator. Thus the expression is irreducible and is non-integer.\n\n\n"}
{"id": "4723370", "url": "https://en.wikipedia.org/wiki?curid=4723370", "title": "Enumerated type", "text": "Enumerated type\n\nIn computer programming, an enumerated type (also called enumeration, enum, or factor in the R programming language, and a categorical variable in statistics) is a data type consisting of a set of named values called \"elements\", \"members\", \"enumeral\", or \"enumerators\" of the type. The enumerator names are usually identifiers that behave as constants in the language. An enumerated type can be seen as a degenerate tagged union of unit type. A variable that has been declared as having an enumerated type can be assigned any of the enumerators as a value. In other words, an enumerated type has values that are different from each other, and that can be compared and assigned, but are not specified by the programmer as having any particular concrete representation in the computer's memory; compilers and interpreters can represent them arbitrarily.\n\nFor example, the four suits in a deck of playing cards may be four enumerators named \"Club\", \"Diamond\", \"Heart\", and \"Spade\", belonging to an enumerated type named \"suit\". If a variable \"V\" is declared having \"suit\" as its data type, one can assign any of those four values to it.\n\nAlthough the enumerators are usually distinct, some languages may allow the same enumerator to be listed twice in the type's declaration. The names of enumerators need not be semantically complete or compatible in any sense. For example, an enumerated type called \"color\" may be defined to consist of the enumerators \"Red\", \"Green\", \"Zebra\", \"Missing\", and \"Bacon\". In some languages, the declaration of an enumerated type also intentionally defines an ordering of its members; in others, the enumerators are unordered; in others still, an implicit ordering arises from the compiler concretely representing enumerators as integers.\n\nSome enumerator types may be built into the language. The Boolean type, for example is often a pre-defined enumeration of the values \"False\" and \"True\". Many languages allow users to define new enumerated types.\n\nValues and variables of an enumerated type are usually implemented as fixed-length bit strings, often in a format and size compatible with some integer type. Some languages, especially system programming languages, allow the user to specify the bit combination to be used for each enumerator. In type theory, enumerated types are often regarded as tagged unions of unit types. Since such types are of the form formula_1, they may also be written as natural numbers.\n\nSome early programming languages did not originally have enumerated types. If a programmer wanted a variable, for example \"myColor\", to have a value of red, the variable red would be declared and assigned some arbitrary value, usually an integer constant. The variable red would then be assigned to \"myColor\". Other techniques assigned arbitrary values to strings containing the names of the enumerators.\n\nThese arbitrary values were sometimes referred to as magic numbers since there often was no explanation as to how the numbers were obtained or whether their actual values were significant. These magic numbers could make the source code harder for others to understand and maintain.\n\nEnumerated types, on the other hand, make the code more self-documenting. Depending on the language, the compiler could automatically assign default values to the enumerators thereby hiding unnecessary detail from the programmer. These values may not even be visible to the programmer (see information hiding). Enumerated types can also prevent a programmer from writing illogical code such as performing mathematical operations on the values of the enumerators. If the value of a variable that was assigned an enumerator were to be printed, some programming languages could also print the name of the enumerator rather than its underlying numerical value. A further advantage is that enumerated types can allow compilers to enforce semantic correctness. For instance:<br>\ncodice_1<br>\ncan be forbidden, whilst <br>\ncodice_2<br>\nis accepted, even if \"TRIANGLE\" and \"RED\" are both internally represented as \"1\".\n\nConceptually, an enumerated type is similar to a list of nominals (numeric codes), since each possible value of the type is assigned a distinctive natural number. A given enumerated type is thus a concrete implementation of this notion. When order is meaningful and/or used for comparison, then an enumerated type becomes an ordinal type.\n\nProgramming languages tend to have their own, oftentimes multiple, programming styles and naming conventions. Enumerations frequently follow either a PascalCase or uppercase convention, while lowercase and others are seen less frequently.\n\nIn Pascal, an enumerated type can be implicitly declared by listing the values in a parenthesised list:\nThe declaration will often appear in a type synonym declaration, such that it can be used for multiple variables:\nThe order in which the enumeration values are given matters. An enumerated type is an ordinal type, and the codice_3 and codice_4 functions will give the prior or next value of the enumeration, and codice_5 can convert enumeration values to their integer representation. Standard Pascal does not offer a conversion from arithmetic types to enumerations, however. Extended Pascal offers this functionality via an extended codice_4 function. Some other Pascal dialects allow it via type-casts. Some modern descendants of Pascal, such as Modula-3, provide a special conversion syntax using a method called codice_7; Modula-3 also treats codice_8 and codice_9 as special pre-defined enumerated types and uses codice_10 and codice_7 for standard ASCII decoding and encoding.\n\nPascal style languages also allow enumeration to be used as array index:\n\nIn Ada, the use of \"=\" was replaced with \"is\" leaving the definition quite similar:\n\nIn addition to codice_12, codice_13, codice_14 and codice_15 Ada also supports simple string conversions via codice_16 and codice_17.\n\nSimilar to C-style languages Ada allows the internal representation of the enumeration to be specified:\n\nUnlike C-style languages Ada also allows the number of bits of the enumeration to be specified:\n\nAdditionally, one can use enumerations as indexes for arrays, like in Pascal, but there are attributes defined for enumerations\nLike Modula-3 Ada treats codice_18 and codice_19 as special pre-defined (in package \"codice_20\") enumerated types. Unlike Modula-3 one can also define own character types:\n\nThe original K&R dialect of the programming language C had no enumerated types. They were added in the ANSI standard for C, which became ANSI C (sometimes termed C89). In C, enumerations are created by explicit definitions (the codice_21 keyword by itself does not cause allocation of storage) which use the codice_21 keyword and are reminiscent of struct and union definitions:\n\nC exposes the integer representation of enumeration values directly to the programmer. Integers and enum values can be mixed freely, and all arithmetic operations on enum values are permitted. It is even possible for an enum variable to hold an integer that does not represent any of the enumeration values. In fact, according to the language definition, the above code will define codice_23, codice_24, codice_25, and codice_26 as constants of type codice_27, which will only be converted (silently) to codice_28 if they are stored in a variable of that type.\n\nC also allows the programmer to choose the values of the enumeration constants explicitly, even without type. For example,\n\ncould be used to define a type that allows mathematical sets of suits to be represented as an codice_28 by bitwise logic operations.\n\nIn C, enumerations assign related names to a set of integer values. In Swift, enumerations are much more flexible and need not provide a value for each case of the enumeration. If a value (termed a \"raw\" value) is provided for each enumeration case, the value can be a string, a character, or a value of any integer or floating-point type.\n\nAlternatively, enumeration cases can specify associated values of any type to be stored along with each different case value, much as unions or variants do in other languages. One can define a common set of related cases as part of one enumeration, each of which has a different set of values of appropriate types associated with it.\n\nIn Swift, enumerations are a first-class type. They adopt many features traditionally supported only by classes, such as computed properties to provide additional information about the enumeration’s current value, and instance methods to provide functionality related to the values the enumeration represents. Enumerations can also define initializers to provide an initial case value and can be extended to expand their functionality beyond their original implementation; and can conform to protocols to provide standard functionality.\n\nUnlike C and Objective-C, Swift enumeration cases are not assigned a default integer value when they are created. In the CardSuit example above, clubs, diamonds, hearts, and spades do not implicitly equal 0, 1, 2 and 3. Instead, the different enumeration cases are fully-fledged values in their own right, with an explicitly-defined type of CardSuit.\n\nMultiple cases can appear on a single line, separated by commas:\nWhen working with enumerations that store integer or string raw values, one doesn’t need to explicitly assign a raw value for each case because Swift will automatically assign the values.\n\nFor instance, when integers are used for raw values, the implicit value for each case is one more than the previous case. If the first case doesn’t have a value set, its value is 0.\n\nThe enumeration below is a refinement of the earlier Planet enumeration, with integer raw values to represent each planet’s order from the sun:\nIn the example above, Planet.mercury has an explicit raw value of 1, Planet.venus has an implicit raw value of 2, and so on.\n\n\"Details are found in Swift documentation online here.\"\n\nDynamically typed languages in the syntactic tradition of C (e.g., Perl or JavaScript) do not, in general, provide enumerations. But in Perl programming the same result can be obtained with the shorthand strings list and hashes (possibly slices):\n\nPerl 6 does provide enumerations. There are multiple ways to declare enumerations in Perl 6, all creating a back-end Map.\n\nEnumerated types in the C# programming language preserve most of the \"small integer\" semantics of C's enums. Some arithmetic operations are not defined for enums, but an enum value can be explicitly converted to an integer and back again, and an enum variable can have values that were not declared by the enum definition. For example, given\n\nthe expressions codice_30 and codice_31 are allowed directly (because it may make sense to step through the sequence of values or ask how many steps there are between two values), but codice_32 is deemed to make less sense and is only allowed if the values are first converted to integers.\n\nC# also provides the C-like feature of being able to define specific integer values for enumerations. By doing this it is possible to perform binary operations on enumerations, thus treating enumeration values as sets of flags. These flags can be tested using binary operations or with the Enum type's builtin 'HasFlag' method.\n\nThe enumeration definition defines names for the selected integer values and is syntactic sugar, as it is possible to assign to an enum variable other integer values that are not in the scope of the enum definition.\n\nC++ has enumeration types that are directly inherited from C's and work mostly like these, except that an enumeration is a real type in C++, giving added compile-time checking. Also (as with structs), the C++ codice_21 keyword is automatically combined with a \"typedef\", so that instead of naming the type codice_34, simply name it codice_35. This can be simulated in C using a typedef: codice_36\n\nC++11 provides a second, type-safe enumeration type that is not implicitly converted to an integer type. It allows io streaming to be defined for that type. Additionally the enumerations do not leak, so they have to be used with Enumeration codice_37. This is specified by the phrase \"enum class\". For example:\n\nThe \"underlying type\" is an implementation-defined integral type that is large enough to hold all enumerated values (it doesn't have to be the smallest possible type!). In C++ you can specify the underlying type directly. That allows \"forward declarations\" of enumerations:\n\nGo uses the codice_38 keyword to create enumerated constants.\n\nThe J2SE version 5.0 of the Java programming language added enumerated types whose declaration syntax is\nsimilar to that of C:\n\nThe Java type system, however, treats enumerations as a type separate from integers, and intermixing of enum and integer values is not allowed. In fact, an enum type in Java is actually a special compiler-generated class rather than an arithmetic type, and enum values behave as global pre-generated instances of that class. Enum types can have instance methods and a constructor (the arguments of which can be specified separately for each enum value). All enum types implicitly extend the abstract class. An enum type cannot be instantiated directly.\n\nInternally, each enum value contains an integer, corresponding to the order in which they are declared in the source code, starting from 0. The programmer cannot set a custom integer for an enum value directly, but one can define overloaded constructors that can then assign arbitrary values to self-defined members of the enum class. Defining getters allows then access to those self-defined members. The internal integer can be obtained from an enum value using the method, and the list of enum values of an enumeration type can be obtained in order using the codice_39 method. It is generally discouraged for programmers to convert enums to integers and vice versa. Enumerated types are codice_40, using the internal integer; as a result, they can be sorted.\n\nThe Java standard library provides utility classes to use with enumerations. The class implements a codice_41 of enum values; it is implemented as a bit array, which makes it very compact and as efficient as explicit bit manipulation, but safer. The class implements a codice_42 of enum values to object. It is implemented as an array, with the integer value of the enum value serving as the index.\n\nA helpful addition to the standard set of datatypes from JavaScript is the 'enum'. Like languages like C#, an enum is a way of giving more friendly names to sets of numeric values.\n\nBy default, enums begin numbering their members starting at 0. This can be changed by manually setting the value of one its members. For example, the prior example can start at 1 instead of 0:\n\nOr, even manually set all the values in the enum:\n\nA handy feature of enums in TypeScript is that you can also go from a numeric value to the name of that value in the enum. For example, if presented with the value 2 but unsure which that mapped to in the enum, one could look up the corresponding name:\n\nAn codice_21 module was added to the Python standard library in version 3.4.\n\nThere is also a functional API for creating enumerations with automatically generated indices (starting with one):\n\nPython enumerations do not enforce semantic correctness (a meaningless comparison to an incompatible enumeration always returns \"False\" rather than raising a \"TypeError\"):\n\nFortran only has enumerated types for interoperability with C; hence, the semantics is similar to C and, as in C, the enum values are just integers and no further type check is done. The C example from above can be written in Fortran as\n\nEnumerated datatypes in Visual Basic (up to version 6) and VBA are automatically assigned the \"codice_44\" datatype and also become a datatype themselves:\n\nExample Code in vb.Net\n\nIn functional programming languages in the ML lineage (e.g., Standard ML (SML), OCaml, and Haskell), an algebraic data type with only nullary constructors can be used to implement an enumerated type. For example (in the syntax of SML signatures):\n\nIn these languages the small-integer representation is completely hidden from the programmer, if indeed such a representation is employed by the implementation. However, Haskell has the codice_45 type class which a type can derive or implement to get a mapping between the type and codice_46.\n\nCommon Lisp uses the member type specifier, e.g.,\n\nthat states that object is of type cardsuit if it is codice_47 to club, diamond, heart or spade. The member type specifier is not valid as a Common Lisp Object System (CLOS) parameter specializer, however. Instead, codice_48, which is the equivalent to codice_49 may be used (that is, only one member of the set may be specified with an eql type specifier, however, it may be used as a CLOS parameter specializer.) In other words, to define methods to cover an enumerated type, a method must be defined for each specific element of that type.\n\nAdditionally,\n\nmay be used to define arbitrary enumerated types at runtime. For instance\n\nwould refer to a type equivalent to the prior definition of cardsuit, as of course would simply have been using\n\nbut may be less confusing with the function codice_50 for stylistic reasons.\n\nSome databases support enumerated types directly. MySQL provides an enumerated type codice_51 with allowable values specified as strings when a table is created. The values are stored as numeric indices with the empty string stored as 0, the first string value stored as 1, the second string value stored as 2, etc. Values can be stored and retrieved as numeric indexes or string values.\n\nXML Schema supports enumerated types through the enumeration facet used for constraining most primitive datatypes such as strings.\n\n\n"}
{"id": "10396781", "url": "https://en.wikipedia.org/wiki?curid=10396781", "title": "Extravagant number", "text": "Extravagant number\n\nAn extravagant number (also known as a \"wasteful\" number) is a natural number that has fewer digits than the number of digits in its prime factorization (including exponents). For example, in base-10 arithmetic 4 = 2², 6 = 2×3, 8 = 2³, and 9 = 3² are extravagant numbers .\n\nExtravagant numbers can be defined in any base. There are infinitely many extravagant numbers, no matter what base is used.\n\n\n"}
{"id": "58507539", "url": "https://en.wikipedia.org/wiki?curid=58507539", "title": "Feldman–Hájek theorem", "text": "Feldman–Hájek theorem\n\nIn probability theory, the Feldman–Hájek theorem or Feldman–Hájek dichotomy is a fundamental result in the theory of Gaussian measures. It states that two Gaussian measures formula_1 and formula_2 on a locally convex space formula_3 are either equivalent measures or else mutually singular: there is no possibility of an intermediate situation in which, for example, formula_1 has a density with respect to formula_2 but not vice versa. In the special case that formula_3 is a Hilbert space, it is possible to give a explicit description of the circumstances under which formula_1 and formula_2 are equivalent: writing formula_9 and formula_10 for the means of formula_1 and formula_2, and formula_13 and formula_14 for their covariance operators, equivalence of formula_1 and formula_2 holds if and only if\n\nA simple consequence of the Feldman–Hájek theorem is that dilating a Gaussian measure on an infinite-dimensional Hilbert space formula_3 (i.e. taking formula_24 for some scale factor formula_25) always yields two mutually singular Gaussian measures, except for the trivial dilation with formula_26, since formula_27 is Hilbert–Schmidt only when formula_26.\n"}
{"id": "57181134", "url": "https://en.wikipedia.org/wiki?curid=57181134", "title": "Ferran Sunyer i Balaguer Prize", "text": "Ferran Sunyer i Balaguer Prize\n\nThe Ferran Sunyer i Balaguer Prize is a prize in mathematics, first awarded in 1993. It honors the memory of Ferran Sunyer i Balaguer (1912–1967), a self-taught Catalan mathematician who, despite a serious physical disability, was very active in research in classical analysis. This award acknowledges an outstanding mathematical monograph of an expository nature, presenting the latest developments in an active area of mathematics research. The annually awarded prize consists of as of 2017. The winning monograph is also published in Birkhauser-Verlag's series \"Progress in Mathematics\". It is awarded by the Ferran Sunyer i Balaguer Foundation.\n\nThe recipients of the Ferran Sunyer i Balaguer Prize are:\n\n\nWebsite\n"}
{"id": "14417429", "url": "https://en.wikipedia.org/wiki?curid=14417429", "title": "Francis Wollaston (philosopher)", "text": "Francis Wollaston (philosopher)\n\nFrancis John Hyde Wollaston FRS (13 April 1762, London – 12 October 1823) was an English natural philosopher and Jacksonian Professor at the University of Cambridge.\n\nFrancis John Hyde Wollaston was the son of Francis Wollaston (1731–1815) and Althea Hyde, and brother to William Hyde Wollaston (1766-1828). He was educated in Scarning, Norfolk and at Charterhouse before entering Sidney Sussex College, Cambridge in 1779. He graduated as senior wrangler in 1783, became a fellow of Trinity Hall in 1785, and was ordained a priest in 1787.\n\nWollaston was elected a Fellow of the Royal Society in 1786. From 1792 to 1813 he was Jacksonian Professor at Cambridge. Resigning his Trinity Hall fellowship to marry Frances Hayles in 1793, he became Rector of South Weald the following year. In 1807 he was elected Master of Sidney Sussex College, but the election was declared invalid on the grounds that he had never been a fellow of Sidney Sussex. On resigning his professorship in 1813, he assumed additional clerical duties: from 1813 to 1823 he was rector of Cold Norton and Archdeacon of Essex.\n\n"}
{"id": "10208241", "url": "https://en.wikipedia.org/wiki?curid=10208241", "title": "Gordon–Luecke theorem", "text": "Gordon–Luecke theorem\n\nIn mathematics, the Gordon–Luecke theorem on knot complements states that if the complements of two tame knots are homeomorphic, then the knots are equivalent. In particular, any homeomorphism between knot complements must take a meridian to a meridian. \n\nThe theorem is usually stated as \"knots are determined by their complements\"; however this is slightly ambiguous as it considers two knots to be equivalent if there is a self-homeomorphism taking one knot to the other. Thus mirror images are neglected. Often two knots are considered equivalent if they are \"isotopic\". The correct version in this case is that if two knots have complements which are orientation-preserving homeomorphic, then they are isotopic. \n\nThese results follow from the following (also called the Gordon–Luecke theorem): no nontrivial Dehn surgery on a nontrivial knot in the 3-sphere can yield the 3-sphere. \n\nThe theorem was proved by Cameron Gordon and John Luecke. Essential ingredients of the proof are their joint work with Marc Culler and Peter Shalen on the cyclic surgery theorem, combinatorial techniques in the style of Litherland, thin position, and Scharlemann cycles.\n\nFor link complements, it is not in fact true that links are determined by their complements. For example, JHC Whitehead proved that there are infinitely many links whose complements are all homeomorphic to the Whitehead link. His construction is to twist along a disc spanning an unknotted component (as is the case for either component of the Whitehead link). Another method is to twist along an annulus spanning two components. Gordon proved that for the class of links where these two constructions are not possible there are finitely many links \"in this class\" with a given complement.\n\n"}
{"id": "55907189", "url": "https://en.wikipedia.org/wiki?curid=55907189", "title": "Hecke algebra acting on modular forms", "text": "Hecke algebra acting on modular forms\n\nIn number theory in mathematics, the Hecke algebra is the algebra generated by Hecke operators. The algebra is commutative.\n\n"}
{"id": "6338664", "url": "https://en.wikipedia.org/wiki?curid=6338664", "title": "Hermes8", "text": "Hermes8\n\nIn cryptography, Hermes8 is the name of a stream cypher algorithm designed by Ulrich Kaiser. It has been submitted to the eSTREAM Project of the eCRYPT network. It has been classified as an 'archive' algorithm and will not be further considered.\n\nIn the paper \"An Analysis of the Hermes8 Stream Ciphers\" the authors claim, 'an attack on the latest version of the cipher (Hermes8F), which requires very few known keystream bytes and recovers the cipher's secret key in less than a second on a normal PC'.\n\n"}
{"id": "11174336", "url": "https://en.wikipedia.org/wiki?curid=11174336", "title": "In-place matrix transposition", "text": "In-place matrix transposition\n\nIn-place matrix transposition, also called in-situ matrix transposition, is the problem of transposing an \"N\"×\"M\" matrix in-place in computer memory, ideally with \"O\"(1) (bounded) additional storage, or at most with additional storage much less than \"NM\". Typically, the matrix is assumed to be stored in row-major order or column-major order (i.e., contiguous rows or columns, respectively, arranged consecutively).\n\nPerforming an in-place transpose (in-situ transpose) is most difficult when \"N\" ≠ \"M\", i.e. for a non-square (rectangular) matrix, where it involves a complicated permutation of the data elements, with many cycles of length greater than 2. In contrast, for a square matrix (\"N\" = \"M\"), all of the cycles are of length 1 or 2, and the transpose can be achieved by a simple loop to swap the upper triangle of the matrix with the lower triangle. Further complications arise if one wishes to maximize memory locality in order to improve cache line utilization or to operate out-of-core (where the matrix does not fit into main memory), since transposes inherently involve non-consecutive memory accesses.\n\nThe problem of non-square in-place transposition has been studied since at least the late 1950s, and several algorithms are known, including several which attempt to optimize locality for cache, out-of-core, or similar memory-related contexts.\n\nOn a computer, one can often avoid explicitly transposing a matrix in memory by simply accessing the same data in a different order. For example, software libraries for linear algebra, such as BLAS, typically provide options to specify that certain matrices are to be interpreted in transposed order to avoid data movement.\n\nHowever, there remain a number of circumstances in which it is necessary or desirable to physically reorder a matrix in memory to its transposed ordering. For example, with a matrix stored in row-major order, the rows of the matrix are contiguous in memory and the columns are discontiguous. If repeated operations need to be performed on the columns, for example in a fast Fourier transform algorithm (e.g. Frigo & Johnson, 2005), transposing the matrix in memory (to make the columns contiguous) may improve performance by increasing memory locality. Since these situations normally coincide with the case of very large matrices (which exceed the cache size), performing the transposition in-place with minimal additional storage becomes desirable.\n\nAlso, as a purely mathematical problem, in-place transposition involves a number of interesting number theory puzzles that have been worked out over the course of several decades.\n\nFor example, consider the 2×4 matrix:\n\nIn row-major format, this would be stored in computer memory as the sequence (11, 12, 13, 14, 21, 22, 23, 24), i.e. the two rows stored consecutively. If we transpose this, we obtain the 4×2 matrix:\n\nwhich is stored in computer memory as the sequence (11, 21, 12, 22, 13, 23, 14, 24).\nIf we number the storage locations 0 to 7, from left to right, then this permutation consists of four cycles:\n\nThat is, the value in position 0 goes to position 0 (a cycle of length 1, no data motion). Next, the value in position 1 (in the original storage: 11, 12, 13, 14, 21, 22, 23, 24) goes to position 2 (in the transposed storage 11, 21, 12, 22, 13, 23, 14, 24), while the value in position 2 (11, 12, 13, 14, 21, 22, 23, 24) goes to position 4 (11, 21, 12, 22, 13, 23, 14, 24), and position 4 (11, 12, 13, 14, 21, 22, 23, 24) goes back to position 1 (11, 21, 12, 22, 13, 23, 14, 24). Similarly for the values in position 7 and positions (3 6 5).\n\nIn the following, we assume that the \"N\"×\"M\" matrix is stored in row-major order with zero-based indices. This means that the (\"n\",\"m\") element, for \"n\" = 0,…,\"N\"−1 and \"m\" = 0,…,\"M\"−1, is stored at an address \"a\" = \"Mn\" + \"m\" (plus some offset in memory, which we ignore). In the transposed \"M\"×\"N\" matrix, the corresponding (\"m\",\"n\") element is stored at the address \"a' \" = \"Nm\" + \"n\", again in row-major order. We define the \"transposition permutation\" to be the function \"a' \" = \"P\"(\"a\") such that:\nThis defines a permutation on the numbers formula_5.\n\nIt turns out that one can define simple formulas for \"P\" and its inverse (Cate & Twigg, 1977). First:\n\nwhere \"mod\" is the modulo operation. \nIf 0 ≤ \"a\" = \"Mn\" + \"m\" < \"MN\" − 1, then \"Na\" mod (\"MN\"−1) = \"MN\" \"n\" + \"Nm\" mod (\"MN\" − 1) = \"n\" + \"Nm\". \nSecond, the inverse permutation is given by:\n\nAs proved by Cate & Twigg (1977), the number of fixed points (cycles of length 1) of the permutation is precisely , where gcd is the greatest common divisor. For example, with \"N\" = \"M\" the number of fixed points is simply \"N\" (the diagonal of the matrix). If and are coprime, on the other hand, the only two fixed points are the upper-left and lower-right corners of the matrix.\n\nThe number of cycles of any length \"k\">1 is given by (Cate & Twigg, 1977):\n\nwhere μ is the Möbius function and the sum is over the divisors \"d\" of \"k\".\n\nFurthermore, the cycle containing \"a\"=1 (i.e. the second element of the first row of the matrix) is always a cycle of maximum length \"L\", and the lengths \"k\" of all other cycles must be divisors of \"L\" (Cate & Twigg, 1977).\n\nFor a given cycle \"C\", every element formula_9 has the same greatest common divisor formula_10. \nLet \"s\" be the smallest element of the cycle, and formula_11. From the definition of the permutation \"P\" above, every other element \"x\" of the cycle is obtained by repeatedly multiplying \"s\" by \"N\" modulo \"MN\"−1, and therefore every other element is divisible by \"d\". But, since \"N\" and are coprime, \"x\" cannot be divisible by any factor of larger than \"d\", and hence formula_10.\nThis theorem is useful in searching for cycles of the permutation, since an efficient search can look only at multiples of divisors of \"MN\"−1 (Brenner, 1973).\n\nLaflin & Brebner (1970) pointed out that the cycles often come in pairs, which is exploited by several algorithms that permute pairs of cycles at a time. In particular, let \"s\" be the smallest element of some cycle \"C\" of length \"k\". It follows that \"MN\"−1−\"s\" is also an element of a cycle of length \"k\" (possibly the same cycle). \nThe length \"k\" of the cycle containing \"s\" is the smallest \"k\" > 0 such that formula_13. Clearly, this is the same as the smallest \"k\">0 such that formula_14, since we are just multiplying both sides by −1, and formula_15. \n\nThe following briefly summarizes the published algorithms to perform in-place matrix transposition. Source code implementing some of these algorithms can be found in the references, below.\n\nFor a square \"N\"×\"N\" matrix \"A\" = \"A\"(\"n\",\"m\"), in-place transposition is easy because all of the cycles have length 1 (the diagonals \"A\") or length 2 (the upper triangle is swapped with the lower triangle). Pseudocode to accomplish this (assuming zero-based array indices) is:\n\nThis type of implementation, while simple, can exhibit poor performance due to poor cache-line utilization, especially when \"N\" is a power of two (due to cache-line conflicts in a CPU cache with limited associativity). The reason for this is that, as \"m\" is incremented in the inner loop, the memory address corresponding to \"A\"(\"n\",\"m\") or \"A\"(\"m\",\"n\") jumps discontiguously by \"N\" in memory (depending on whether the array is in column-major or row-major format, respectively). That is, the algorithm does not exploit locality of reference.\n\nOne solution to improve the cache utilization is to \"block\" the algorithm to operate on several numbers at once, in blocks given by the cache-line size; unfortunately, this means that the algorithm depends on the size of the cache line (it is \"cache-aware\"), and on a modern computer with multiple levels of cache it requires multiple levels of machine-dependent blocking. Instead, it has been suggested (Frigo \"et al.\", 1999) that better performance can be obtained by a recursive algorithm: divide the matrix into four submatrices of roughly equal size, transposing the two submatrices along the diagonal recursively and transposing and swapping the two submatrices above and below the diagonal. (When \"N\" is sufficiently small, the simple algorithm above is used as a base case, as naively recurring all the way down to \"N\"=1 would have excessive function-call overhead.) This is a cache-oblivious algorithm, in the sense that it can exploit the cache line without the cache-line size being an explicit parameter.\n\nFor non-square matrices, the algorithms are more complicated. Many of the algorithms prior to 1980 could be described as \"follow-the-cycles\" algorithms. That is, they loop over the cycles, moving the data from one location to the next in the cycle. In pseudocode form:\n\nThe differences between the algorithms lie mainly in how they locate the cycles, how they find the starting addresses in each cycle, and how they ensure that each cycle is moved exactly once. Typically, as discussed above, the cycles are moved in pairs, since \"s\" and \"MN\"−1−\"s\" are in cycles of the same length (possibly the same cycle). Sometimes, a small scratch array, typically of length \"M\"+\"N\" (e.g. Brenner, 1973; Cate & Twigg, 1977) is used to keep track of a subset of locations in the array that have been visited, to accelerate the algorithm.\n\nIn order to determine whether a given cycle has been moved already, the simplest scheme would be to use \"O\"(\"MN\") auxiliary storage, one bit per element, to indicate whether a given element has been moved. To use only \"O\"(\"M\"+\"N\") or even auxiliary storage, more complicated algorithms are required, and the known algorithms have a worst-case linearithmic computational cost of at best, as first proved by Knuth (Fich \"et al.\", 1995; Gustavson & Swirszcz, 2007).\n\nSuch algorithms are designed to move each data element exactly once. However, they also involve a considerable amount of arithmetic to compute the cycles, and require heavily non-consecutive memory accesses since the adjacent elements of the cycles differ by multiplicative factors of \"N\", as discussed above.\n\nSeveral algorithms have been designed to achieve greater memory locality at the cost of greater data movement, as well as slightly greater storage requirements. That is, they may move each data element more than once, but they involve more consecutive memory access (greater spatial locality), which can improve performance on modern CPUs that rely on caches, as well as on SIMD architectures optimized for processing consecutive data blocks. The oldest context in which the spatial locality of transposition seems to have been studied is for out-of-core operation (by Alltop, 1975), where the matrix is too large to fit into main memory (\"core\").\n\nFor example, if \"d\" = gcd(\"N\",\"M\") is not small, one can perform the transposition using a small amount (\"NM\"/\"d\") of additional storage, with at most three passes over the array (Alltop, 1975; Dow, 1995). Two of the passes involve a sequence of separate, small transpositions (which can be performed efficiently out of place using a small buffer) and one involves an in-place \"d\"×\"d\" square transposition of formula_16 blocks (which is efficient since the blocks being moved are large and consecutive, and the cycles are of length at most 2). This is further simplified if N is a multiple of M (or vice versa), since only one of the two out-of-place passes is required.\n\nAnother algorithm for non-coprime dimensions, involving multiple subsidiary transpositions, was described by Catanzaro et al. (2014). For the case where is small, Dow (1995) describes another algorithm requiring additional storage, involving a square transpose preceded or followed by a small out-of-place transpose. Frigo & Johnson (2005) describe the adaptation of these algorithms to use cache-oblivious techniques for general-purpose CPUs relying on cache lines to exploit spatial locality.\n\nWork on out-of-core matrix transposition, where the matrix does not fit in main memory and must be stored largely on a hard disk, has focused largely on the \"N\" = \"M\" square-matrix case, with some exceptions (e.g. Alltop, 1975). Recent reviews of out-of-core algorithms, especially as applied to parallel computing, can be found in e.g. Suh & Prasanna (2002) and Krishnamoorth et al. (2004).\n\n\n"}
{"id": "7840768", "url": "https://en.wikipedia.org/wiki?curid=7840768", "title": "Injective metric space", "text": "Injective metric space\n\nIn metric geometry, an injective metric space, or equivalently a hyperconvex metric space, is a metric space with certain properties generalizing those of the real line and of L distances in higher-dimensional vector spaces. These properties can be defined in two seemingly different ways: hyperconvexity involves the intersection properties of closed balls in the space, while injectivity involves the isometric embeddings of the space into larger spaces. However it is a theorem of Aronszajn and Panitchpakdi (1956; see e.g. ) that these two different types of definitions are equivalent.\n\nA metric space \"X\" is said to be hyperconvex if it is convex and its closed balls have the binary Helly property. That is,\n\nEquivalently, if a set of points \"p\" and radii \"r > 0\" satisfies \"r\" + \"r\" ≥ \"d\"(\"p\",\"p\") for each \"i\" and \"j\", then there is a point \"q\" of the metric space that is within distance \"r\" of each \"p\".\n\nA retraction of a metric space \"X\" is a function \"ƒ\" mapping \"X\" to a subspace of itself, such that\nA \"retract\" of a space \"X\" is a subspace of \"X\" that is an image of a retraction.\nA metric space  \"X\" is said to be injective if, whenever \"X\" is isometric to a subspace \"Z\" of a space \"Y\", that subspace \"Z\" is a retract of \"Y\".\n\nExamples of hyperconvex metric spaces include\nDue to the equivalence between hyperconvexity and injectivity, these spaces are all also injective.\n\nIn an injective space, the radius of the minimum ball that contains any set \"S\" is equal to half the diameter of \"S\". This follows since the balls of radius half the diameter, centered at the points of \"S\", intersect pairwise and therefore by hyperconvexity have a common intersection; a ball of radius half the diameter centered at a point of this common intersection contains all of \"S\". Thus, injective spaces satisfy a particularly strong form of Jung's theorem.\n\nEvery injective space is a complete space , and every metric map (or, equivalently, nonexpansive mapping, or short map) on a bounded injective space has a fixed point (; ). A metric space is injective if and only if it is an injective object in the category of metric spaces and metric maps. For additional properties of injective spaces see .\n\n"}
{"id": "968734", "url": "https://en.wikipedia.org/wiki?curid=968734", "title": "Integrability conditions for differential systems", "text": "Integrability conditions for differential systems\n\nIn mathematics, certain systems of partial differential equations are usefully formulated, from the point of view of their underlying geometric and algebraic structure, in terms of a system of differential forms. The idea is to take advantage of the way a differential form \"restricts\" to a submanifold, and the fact that this restriction is compatible with the exterior derivative. This is one possible approach to certain over-determined systems, for example, including Lax pairs of integrable systems. A Pfaffian system is specified by 1-forms alone, but the theory includes other types of example of differential system.\n\nGiven a collection of differential 1-forms formula_1 on an formula_2-dimensional manifold formula_3, an integral manifold is a submanifold whose tangent space at every point formula_4 is annihilated by each formula_5.\n\nA maximal integral manifold is a submanifold\n\nsuch that the kernel of the restriction map on forms\n\nis spanned by the formula_5 at every point formula_9 of formula_10. If in addition the formula_5 are linearly independent, then formula_10 is (formula_13)-dimensional. Note that formula_14 need not be an embedded submanifold.\n\nA Pfaffian system is said to be completely integrable if formula_3 admits a foliation by maximal integral manifolds. (Note that the foliation need not be regular; i.e. the leaves of the foliation might not be embedded submanifolds.)\n\nAn integrability condition is a condition on the formula_16 to guarantee that there will be integral submanifolds of sufficiently high dimension.\n\nThe necessary and sufficient conditions for complete integrability of a Pfaffian system are given by the Frobenius theorem. One version states that if the ideal formula_17 algebraically generated by the collection of α inside the ring Ω(\"M\") is differentially closed, in other words\n\nthen the system admits a foliation by maximal integral manifolds. (The converse is obvious from the definitions.)\n\nNot every Pfaffian system is completely integrable in the Frobenius sense. For example, consider the following one-form :\n\nIf \"d\"θ were in the ideal generated by θ we would have, by the skewness of the wedge product\n\nBut a direct calculation gives\n\nwhich is a nonzero multiple of the standard volume form on R. Therefore, there are no two-dimensional leaves, and the system is not completely integrable.\n\nOn the other hand, for the curve defined by\n\nthen θ defined as above is 0, and hence the curve is easily verified to be a solution (i.e. an integral curve) for the above Pfaffian system for any nonzero constant \"c\".\n\nIn Riemannian geometry, we may consider the problem of finding an orthogonal coframe \"θ\", i.e., a collection of 1-forms forming a basis of the cotangent space at every point with formula_23 which are closed (dθ = 0, \"i\" = 1, 2, ..., \"n\"). By the Poincaré lemma, the θ locally will have the form d\"x\" for some functions \"x\" on the manifold, and thus provide an isometry of an open subset of \"M\" with an open subset of R. Such a manifold is called locally flat.\n\nThis problem reduces to a question on the coframe bundle of \"M\". Suppose we had such a closed coframe\n\nIf we had another coframe formula_25, then the two coframes would be related by an orthogonal transformation\n\nIf the connection 1-form is \"ω\", then we have\n\nOn the other hand,\n\nBut formula_29 is the Maurer–Cartan form for the orthogonal group. Therefore, it obeys the structural equation\nformula_30 and this is just the curvature of M: formula_31\nAfter an application of the Frobenius theorem, one concludes that a manifold M is locally flat if and only if its curvature vanishes.\n\nMany generalizations exist to integrability conditions on differential systems which are not necessarily generated by one-forms. The most famous of these are the Cartan–Kähler theorem, which only works for real analytic differential systems, and the Cartan–Kuranishi prolongation theorem. See \"Further reading\" for details.\n\n"}
{"id": "7992036", "url": "https://en.wikipedia.org/wiki?curid=7992036", "title": "Integrodifference equation", "text": "Integrodifference equation\n\nIn mathematics, an integrodifference equation is a recurrence relation on a function space, of the following form:\n\nwhere formula_2 is a sequence in the function space and formula_3 is the domain of those functions. In most applications, for any formula_4, formula_5 is a probability density function on formula_3. Note that in the definition above, formula_7 can be vector valued, in which case each element of formula_8 has a scalar valued integrodifference equation associated with it. Integrodifference equations are widely used in mathematical biology, especially theoretical ecology, to model the dispersal and growth of populations. In this case, formula_9 is the population size or density at location formula_10 at time formula_11, formula_12 describes the local population growth at location formula_10 and formula_14, is the probability of moving from point formula_15 to point formula_10, often referred to as the dispersal kernel. Integrodifference equations are most commonly used to describe univoltine populations, including, but not limited to, many arthropod, and annual plant species. However, multivoltine populations can also be modeled with integrodifference equations, as long as the organism has non-overlapping generations. In this case, formula_11 is not measured in years, but rather the time increment between broods.\n\nIn one spatial dimension, the dispersal kernel often depends only on the distance between the source and the destination, and can be \nwritten as formula_18. In this case, some natural conditions on f and k imply that there is a well-defined\nspreading speed for waves of invasion generated from compact initial conditions. The wave speed is often calculated\nby studying the linearized equation\nwhere formula_20.\nThis can be written as the convolution\nUsing a moment-generating-function transformation\nit has been shown that the critical wave speed\n\nOther types of equations used to model population dynamics through space include reaction-diffusion equations and metapopulation equations. However, diffusion equations do not as easily allow for the inclusion of explicit dispersal patterns and are only biologically accurate for populations with overlapping generations. Metapopulation equations are different from integrodifference equations in the fact that they break the population down into discrete patches rather than a continuous landscape.\n"}
{"id": "2010784", "url": "https://en.wikipedia.org/wiki?curid=2010784", "title": "Ivor Grattan-Guinness", "text": "Ivor Grattan-Guinness\n\nIvor Owen Grattan-Guinness (23 June 1941 – 12 December 2014) was a historian of mathematics and logic.\n\nGrattan-Guinness was born in Bakewell, England; his father was a mathematics teacher and educational administrator. He gained his bachelor degree as a Mathematics Scholar at Wadham College, Oxford, and an MSc (Econ) in Mathematical Logic and the Philosophy of Science at the London School of Economics in 1966. He gained both the doctorate (PhD) in 1969, and higher doctorate (D.Sc.) in 1978, in the History of Science at the University of London. He was Emeritus Professor of the History of Mathematics and Logic at Middlesex University, and a Visiting Research Associate at the London School of Economics.\n\nHe was awarded the Kenneth O. May Medal for services to the History of Mathematics by the International Commission on the History of Mathematics (ICHM) on 31 July 2009, at Budapest, on the occasion of the 23rd International Congress for the History of Science. In 2010, he was elected an Honorary Member of the Bertrand Russell Society.\n\nGrattan-Guinness spent much of his career at Middlesex University. He was a fellow at the Institute for Advanced Study in Princeton, New Jersey, United States, and a member of the International Academy of the History of Science.\n\nFrom 1974 to 1981, Grattan-Guinness was editor of the history of science journal \"Annals of Science\". In 1979 he founded the journal \"History and Philosophy of Logic\", and edited it until 1992. He was an associate editor of \"Historia Mathematica\" for twenty years from its inception in 1974, and again from 1996.\n\nHe also acted as advisory editor to the editions of the writings of C.S. Peirce and Bertrand Russell, and to several other journals and book series. He was a member of the Executive Committee of the International Commission on the History of Mathematics from 1977 to 1993.\n\nGrattan-Guinness gave over 570 invited lectures to organisations and societies, or to conferences and congresses, in over 20 countries around the world. These lectures include tours undertaken in Australia, New Zealand, Italy, South Africa and Portugal.\n\nFrom 1986 to 1988, Grattan-Guinness was the President of the British Society for the History of Mathematics, and for 1992 the Vice-President. In 1991, he was elected an effective member of the Académie Internationale d'Histoire des Sciences. He was the Associate Editor for mathematicians and statisticians for the Oxford Dictionary of National Biography (2004).\n\nGrattan-Guinness took an interest in the phenomenon of coincidence and has written on it for the Society for Psychical Research. He claimed to have a recurrent affinity with one particular number, namely the square of 15 (225), even recounting one occasion when a car was in front of him with the number plate IGG225, i.e. his very initials and that number. He died of heart failure on 12 December 2014, aged 73, survived by his wife Enid Grattan-Guinness.\n\nThe work of Grattan-Guinness touched on all historical periods, but he specialised in the development of the calculus and mathematical analysis, and their applications to mechanics and mathematical physics, and in the rise of set theory and mathematical logic. He was especially interested in characterising how past thinkers, far removed from us in time, view their findings differently from the way we see them now (for example, Euclid). He has emphasised the importance of ignorance as an epistemological notion in this task. He did extensive research with original sources both published and unpublished, thanks to his reading and spoken knowledge of the main European languages.\n\n\n\n\n\n"}
{"id": "14838867", "url": "https://en.wikipedia.org/wiki?curid=14838867", "title": "Journal of Algebra", "text": "Journal of Algebra\n\nJournal of Algebra (ISSN 0021-8693) is an international mathematical research journal in algebra. An imprint of Academic Press, it is published by Elsevier. \"Journal of Algebra\" was founded by Graham Higman, who was its editor from 1964 to 1984. From 1985 until 2000, Walter Feit served as its editor-in-chief.\n\nIn 2004, \"Journal of Algebra\" announced (vol. 276, no. 1 and 2) the creation of a new section on computational algebra, with a separate editorial board. The first issue completely devoted to computational algebra was vol. 292, no. 1 (October 2005).\n\nThe Editor-in-Chief of the \"Journal of Algebra\" is Michel Broué, Université Paris Diderot, and Gerhard Hiß, Rheinisch-Westfälische Technische Hochschule Aachen (RWTH) is Editor of the computational algebra section. \n\n\n"}
{"id": "23067360", "url": "https://en.wikipedia.org/wiki?curid=23067360", "title": "Lexis diagram", "text": "Lexis diagram\n\nIn demography (the branch of statistics that deals with the study of populations) a Lexis diagram (named after economist and social scientist Wilhelm Lexis) is a two dimensional diagram that is used to represent events (such as births or deaths) that occur to individuals belonging to different cohorts. Calendar time is usually represented on the horizontal axis, while age is represented on the vertical axis. In some textbooks the y-axis is plotted backwards, with age 0 at the top of the page and increasing downwards. However, other arrangements of the axes are also seen. As an example the death of an individual in 2009 at age 80 is represented by the point (2009,80); the cohort of all persons born in 1929 is represented by a diagonal line starting at (1929,0) and continuing through (1930,1) and so on.\n\n\n"}
{"id": "17076220", "url": "https://en.wikipedia.org/wiki?curid=17076220", "title": "Line-intercept sampling", "text": "Line-intercept sampling\n\nIn statistics, line-intercept sampling (LIS) is a method of sampling elements in a region whereby an element is sampled if a chosen line segment, called a “transect”, intersects the element. \n\nLine intercept sampling has proven to be a reliable, versatile, and easy to implement method to analyze an area containing various objects of interest. It has recently also been applied to estimating variances during particulate material sampling.\n\n"}
{"id": "601070", "url": "https://en.wikipedia.org/wiki?curid=601070", "title": "List of inequalities", "text": "List of inequalities\n\nThis page lists Wikipedia articles about named mathematical inequalities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3039388", "url": "https://en.wikipedia.org/wiki?curid=3039388", "title": "List of topology topics", "text": "List of topology topics\n\nThis is a list of topology topics, by Wikipedia page. See also:\n\n\n\n\n\n"}
{"id": "9514440", "url": "https://en.wikipedia.org/wiki?curid=9514440", "title": "Math Curse", "text": "Math Curse\n\nMath Curse is a children's picture book written by Jon Scieszka and illustrated by Lane Smith, suitable for ages six through ninety-nine years. Published in 1995 through Viking Press, the book tells the story of a student who is cursed by the way mathematics works in everyday life.\n\nThe nameless student, begins with a seemingly innocent statement by her math teacher- \"you know, almost everything in life can be considered a math problem.\" The next morning, the hero finds herself thinking of the time she needs to get up along the lines of algebra. Next comes the mathematical school of probability, followed by charts and statistics. As the narrator slowly turns into a \"math zombie\", everything in her life is transformed into a problem. A class treat of cupcakes becomes a study in fractions, while a trip to the store turns into a problem of money. Finally, she is left painstakingly calculating how many minutes of \"math madness\" will be in her life now that she is a \"mathematical lunatic.\" Her sister asks her what her problem is, and she responds, \"365 days x 24 hours x 60 minutes.\" Finally, she collapses on her bed, and dreams that she is trapped in a blackboard-room covered in math problems. Armed with only a piece of chalk, she must escape and she manages to do just that by breaking the chalk in half, because \"two halves make a whole.\" She escapes through this \"whole\", and awakens the next morning with the ability to solve any problem. Her curse is broken...until the next day, when her science teacher mentions that in life, everything can be viewed as a science experiment.\n\nThe book is full of actual math problems (and some rather unrelated questions, such as \"What does this inkblot look like?\"). Readers can try to solve the problems and check their answers, which are located on the back cover of the book.\n\nThe book was also adapted for the stage by Heath Corson and Kathleen Collins in 1997. It was first performed at the A Red Orchid Theatre in Chicago, Illinois, in 1997, with subsequent productions at other locations. Its West Coast premiere was in 2003 at the Powerhouse Theatre of Santa Monica, California. Directed by Collins, the cast included Kerry Lacy, Thomas Colby, Will Moran, Andrew David James, and Emily Marver. The play met with warm reviews and succeeded with its audiences as well as local school children.\n\nThe book was critically acclaimed, winning a number of awards and accolades, including Maine's Student Favorite Book Award, the Texas Bluebonnet Award, and New Hampshire's The Great Stone Face Book Award.\n"}
{"id": "13558237", "url": "https://en.wikipedia.org/wiki?curid=13558237", "title": "Mathematics, Form and Function", "text": "Mathematics, Form and Function\n\nMathematics, Form and Function is a survey of the whole of mathematics, including its origins and deep structure, by the American mathematician Saunders Mac Lane.\n\nThroughout his book, and especially in chapter I.11, Mac Lane informally discusses how mathematics is grounded in more ordinary concrete and abstract human activities. The following table is adapted from one given on p. 35 of Mac Lane (1986). The rows are very roughly ordered from most to least fundamental. For a bullet list that can be compared and contrasted with this table, see section 3 of \"Where Mathematics Comes From\".\n\nAlso see the related diagrams appearing on the following pages of Mac Lane (1986): 149, 184, 306, 408, 416, 422-28.\n\nMac Lane (1986) cites a related monograph by Lars Gårding (1977).\n\nMac Lane cofounded category theory with Samuel Eilenberg, which enables a unified treatment of mathematical structures and of the relations among them, at the cost of breaking away from their cognitive grounding. Nevertheless, his views—however informal—are a valuable contribution to the philosophy and anthropology of mathematics. His views anticipate, in some respects, the more detailed account of the cognitive basis of mathematics given by George Lakoff and Rafael E. Núñez in their \"Where Mathematics Comes From\". Lakoff and Núñez argue that mathematics emerges via conceptual metaphors grounded in the human body, its motion through space and time, and in human sense perceptions.\n\n\n"}
{"id": "21368075", "url": "https://en.wikipedia.org/wiki?curid=21368075", "title": "N-group (category theory)", "text": "N-group (category theory)\n\nIn mathematics, an n\"-group, or n\"-dimensional higher group, is a special kind of \"n\"-category that generalises the concept of group to higher-dimensional algebra. Here, \"n\" may be any natural number or infinity. The thesis of Alexander Grothendieck's student Hoàng Xuân Sính was an in-depth study of 2-groups under the monniker 'gr-category'.\n\nThe general definition of \"n\"-group is a matter of ongoing research. However, it is expected that every topological space will have a \"homotopy \"n\"-group\" at every point, which will encapsulate the Postnikov tower of the space up to the homotopy group π, or the entire Postnikov tower for \"n\" = ∞.\n\nThe definition and many properties of 2-groups are already known. A 1-group is simply a group, and the only 0-group is trivial. 2-groups can be described using crossed modules.\n\n"}
{"id": "1064136", "url": "https://en.wikipedia.org/wiki?curid=1064136", "title": "Observational equivalence", "text": "Observational equivalence\n\nObservational equivalence is the property of two or more underlying entities being indistinguishable on the basis of their observable implications. Thus, for example, two scientific theories are observationally equivalent if all of their empirically testable predictions are identical, in which case empirical evidence cannot be used to distinguish which is closer to being correct; indeed, it may be that they are actually two different perspectives on one underlying theory.\n\nIn econometrics, two parameter values (or two \"structures,\" from among a class of statistical models) are considered observationally equivalent if they both result in the same probability distribution of observable data. This term often arises in relation to the identification problem.\n\nIn the formal semantics of programming languages, two terms \"M\" and \"N\" are observationally equivalent if and only if, in all contexts \"C\"[...] where \"C\"[\"M\"] is a valid term, it is the case that \"C\"[\"N\"] is also a valid term with the same value. Thus it is not possible, within the system, to distinguish between the two terms. This definition can be made precise only with respect to a particular calculus, one that comes with its own specific definitions of \"term\", \"context\", and the \"value of a term\".\n\n"}
{"id": "30697444", "url": "https://en.wikipedia.org/wiki?curid=30697444", "title": "Partial cube", "text": "Partial cube\n\nIn graph theory, a partial cube is a graph that is an isometric subgraph of a hypercube. In other words, a partial cube is a subgraph of a hypercube that preserves distances—the distance between any two vertices in the subgraph is the same as the distance between those vertices in the hypercube. Equivalently, a partial cube is a graph whose vertices can be labeled with bit strings of equal length in such a way that the distance between two vertices in the graph is equal to the Hamming distance between their labels. Such a labeling is called a \"Hamming labeling\"; it represents an isometric embedding of the partial cube into a hypercube.\n\n was the first to study isometric embeddings of graphs into hypercubes. The graphs that admit such embeddings were characterized by and , and were later named partial cubes. A separate line of research on the same structures, in the terminology of families of sets rather than of hypercube labelings of graphs, was followed by and , among others.\n\nEvery tree is a partial cube. For, suppose that a tree \"T\" has \"m\" edges, and number these edges (arbitrarily) from 0 to \"m\" − 1. Choose a root vertex \"r\" for the tree, arbitrarily, and label each vertex \"v\" with a string of \"m\" bits that has a 1 in position \"i\" whenever edge \"i\" lies on the path from \"r\" to \"v\" in \"T\". For instance, \"r\" itself will have a label that is all zero bits, its neighbors will have labels with a single 1-bit, etc. Then the Hamming distance between any two labels is the distance between the two vertices in the tree, so this labeling shows that \"T\" is a partial cube.\n\nEvery hypercube graph is itself a partial cube, which can be labeled with all the different bitstrings of length equal to the dimension of the hypercube.\n\nMore complex examples include the following:\n\nMany of the theorems about partial cubes are based directly or indirectly upon a certain binary relation defined on the edges of the graph. This relation, first described by and given an equivalent definition in terms of distances by , is denoted by formula_1. Two edges formula_2 and formula_3 are defined to be in the relation formula_1, written formula_5, if\nformula_6. This relation is reflexive and symmetric, but in general it is not transitive.\n\nWinkler showed that a connected graph is a partial cube if and only if it is bipartite and the relation formula_1 is transitive. In this case, it forms an equivalence relation and each equivalence class separates two connected subgraphs of the graph from each other. A Hamming labeling may be obtained by assigning one bit of each label to each of the equivalence classes of the Djoković–Winkler relation; in one of the two connected subgraphs separated by an equivalence class of edges, all of the vertices have a 0 in that position of their labels, and in the other connected subgraph all of the vertices have a 1 in the same position.\n\nPartial cubes can be recognized, and a Hamming labeling constructed, in formula_8 time, where formula_9 is the number of vertices in the graph. Given a partial cube, it is straightforward to construct the equivalence classes of the Djoković–Winkler relation by doing a breadth first search from each vertex, in total time formula_10; the formula_8-time recognition algorithm speeds this up by using bit-level parallelism to perform multiple breadth first searches in a single pass through the graph, and then applies a separate algorithm to verify that the result of this computation is a valid partial cube labeling.\n\nThe isometric dimension of a partial cube is the minimum dimension of a hypercube onto which it may be isometrically embedded, and is equal to the number of equivalence classes of the Djoković–Winkler relation. For instance, the isometric dimension of an formula_9-vertex tree is its number of edges, formula_13. An embedding of a partial cube onto a hypercube of this dimension is unique, up to symmetries of the hypercube.\n\nEvery hypercube and therefore every partial cube may be embedded isometrically into an integer lattice, and the lattice dimension of the partial cube is the minimum dimension of an integer lattice for which this is possible. The lattice dimension may be significantly smaller than the isometric dimension; for instance, for a tree it is half the number of leaves in the tree (rounded up to the nearest integer). The lattice dimension of any graph, and a lattice embedding of minimum dimension, may be found in polynomial time by an algorithm based on maximum matching in an auxiliary graph.\n\nOther types of dimension of partial cubes have also been defined, based on embeddings into more specialized structures.\n\nIsometric embeddings of graphs into hypercubes have an important application in chemical graph theory. A \"benzenoid graph\" is a graph consisting of all vertices and edges lying on and in the interior of a cycle in a hexagonal lattice. Such graphs are the molecular graphs of the benzenoid hydrocarbons, a large class of organic molecules. Every such graph is a partial cube. A Hamming labeling of such a graph can be used to compute the Wiener index of the corresponding molecule, which can then be used to predict certain of its chemical properties.\n\nA different molecular structure formed from carbon, the diamond cubic, also forms partial cube graphs.\n\n"}
{"id": "45092620", "url": "https://en.wikipedia.org/wiki?curid=45092620", "title": "Paul Kelly (mathematician)", "text": "Paul Kelly (mathematician)\n\nPaul Joseph Kelly (June 26, 1915 – July 15, 1995) was an American mathematician who worked in geometry and graph theory.\n\nKelly was born in Riverside, California. He earned bachelor's and master's degrees from the University of California, Los Angeles before moving to the University of Wisconsin–Madison for doctoral studies; he earned his Ph.D. in 1942 with a dissertation concerning geometric transformations under the supervision of Stanislaw Ulam.\n\nHe spent the rest of the war years serving in the United States Air Force as a First Lieutenant, before returning to academia with a teaching appointment at the University of Southern California in 1946. He moved to the University of California, Santa Barbara in 1949, and was chair there from 1957 to 1962. At UCSB, his students included Brian Alspach (through whom he has nearly 30 academic descendants) and Phyllis Chinn. He retired in 1982.\n\nKelly is known for posing the reconstruction conjecture with his advisor Ulam, which states that every graph is uniquely determined by the ensemble of subgraphs formed by deleting one vertex in each possible way. He also proved a special case of this conjecture, for trees.\n\nHe is the coauthor of three textbooks: \"Projective geometry and projective metrics\" (1953, with Herbert Busemann), \"Geometry and convexity\" (1979, with Max L. Weiss), and \"The non-Euclidean, hyperbolic plane : Its structure and consistency\" (1981, with Gordon Matthews).\n\n"}
{"id": "9927028", "url": "https://en.wikipedia.org/wiki?curid=9927028", "title": "Polynomial greatest common divisor", "text": "Polynomial greatest common divisor\n\nIn algebra, the greatest common divisor (frequently abbreviated as GCD) of two polynomials is a polynomial, of the highest possible degree, that is a factor of both the two original polynomials. This concept is analogous to the greatest common divisor of two integers.\n\nIn the important case of univariate polynomials over a field the polynomial GCD may be computed, like for the integer GCD, by Euclid's algorithm using long division. The polynomial GCD is defined only up to the multiplication by an invertible constant.\n\nThe similarity between the integer GCD and the polynomial GCD allows us to extend to univariate polynomials all the properties that may be deduced from Euclid's algorithm and Euclidean division. Moreover, the polynomial GCD has specific properties that make it a fundamental notion in various areas of algebra. Typically, the roots of the GCD of two polynomials are the common roots of the two polynomials, and this allows us to get information on the roots without computing them. For example, the multiple roots of a polynomial are the roots of the GCD of the polynomial and its derivative, and further GCD computations allow us to compute the square-free factorization of the polynomial, which provides polynomials whose roots are the roots of a given multiplicity.\n\nThe greatest common divisor may be defined and exists, more generally, for multivariate polynomials over a field or the ring of integers, and also over a unique factorization domain. There exist algorithms to compute them as soon as one has a GCD algorithm in the ring of coefficients. These algorithms proceed by a recursion on the number of variables to reduce the problem to a variant of Euclid's algorithm. They are a fundamental tool in computer algebra, because computer algebra systems use them systematically to simplify fractions. Conversely, most of the modern theory of polynomial GCD has been developed to satisfy the need of efficiency of computer algebra systems.\n\nLet \"p\" and \"q\" be polynomials with coefficients in an integral domain \"F\", typically a field or the integers. \nA greatest common divisor of \"p\" and \"q\" is a polynomial \"d\" that divides \"p\" and \"q\" and such that every common divisor of \"p\" and \"q\" also divides \"d\". Every pair of polynomials (not both zero) has a GCD if and only if \"F\" is a unique factorization domain.\n\nIf \"F\" is a field and \"p\" and \"q\" are not both zero, for \"d\" to be a greatest common divisor it is sufficient that it divides both \"p\" and \"q\" and it has the greatest degree among the polynomials having this property. If \"p\" = \"q\" = 0, the GCD is 0. However, some authors consider that it is not defined in this case.\n\nThe greatest common divisor of \"p\" and \"q\" is usually denoted \"gcd(\"p\", \"q\")\".\n\nThe greatest common divisor is not unique: if \"d\" is a GCD of \"p\" and \"q\", then the polynomial \"f\" is another GCD if and only if there is an invertible element \"u\" of \"F\" such that \nand \nIn other words, the GCD is unique up to the multiplication by an invertible constant.\n\nIn the case of the integers, this indetermination has been settled by choosing, as the GCD, the unique one which is positive (there is another one, which is its opposite). With this convention, the GCD of two integers is also the greatest (for the usual ordering) common divisor. However, since there is no natural total order for polynomials over an integral domain, one cannot proceed in the same way here. For univariate polynomials over a field, one can additionally require the GCD to be monic (i.e. it has 1 as coefficient of the highest degree), but in more general cases there is no general convention. Therefore, equalities like \"d\" = gcd(\"p\", \"q\") or gcd(\"p\", \"q\") = gcd(\"r\", \"s\") are usual abuses of notation which should be read \"\"d\" is a GCD of \"p\" and \"q\" and \"p\", \"q\" has the same set of GCD as \"r\", \"s\"\". In particular, gcd(\"p\", \"q\") = 1 means that the invertible constants are the only common divisors, and thus that \"p\" and \"q\" are coprime.\n\n\nThere are several ways to find the greatest common divisor of two polynomials. Two of them are:\n\n\nTo find the GCD of two polynomials using factoring, simply factor the two polynomials completely. Then, take the product of all common factors. At this stage, we do not necessarily have a monic polynomial, so finally multiply this by a constant to make it a monic polynomial. This will be the GCD of the two polynomials as it includes all common divisors and is monic.\n\nExample one: Find the GCD of and .\n\nThus, their GCD is .\n\nFactoring polynomials can be difficult, especially if the polynomials have large degree. The Euclidean algorithm is a method which works for any pair of polynomials. It makes repeated use of Euclidean division. When using this algorithm on two numbers, the size of the numbers decreases at each stage. With polynomials, the degree of the polynomials decreases at each stage. The last nonzero remainder, made monic if necessary, is the GCD of the two polynomials.\n\nMore specifically, assume we wish to find the gcd of two polynomials and , where we can suppose \n\nWe can find two polynomials and which satisfy (see Polynomial long division)\n\n\nThe polynomial is called the quotient and is the remainder. Notice that a polynomial divides and if and only if it divides and . We deduce \nThen set \nRepeat the polynomial long division to get new polynomials and so on. At each stage we have \nso the sequence will eventually reach a point at which \nand we will have found our GCD: \n\nExample: Find the GCD of and .\n\nSince is the last nonzero remainder, the GCD of these polynomials is .\n\nThis method works only if one may test the equality to zero of the elements of the field of the coefficients, so one needs a description of the coefficients as elements of some finitely generated field, for which the generators and relations are known exactly. If the coefficients are floating point numbers, known only approximately, then one uses completely different techniques, usually based on SVD.\n\nThis induces a new difficulty: For all these fields except the finite ones, the coefficients are fractions. If the fractions are not simplified during the computation, the size of the coefficients grows exponentially during the computation, which makes it impossible except for very small degrees. On the other hand, it is highly time consuming to simplify the fractions immediately. Therefore, two different alternative methods have been introduced (see below):\n\nThe case of univariate polynomials over a field is specially important for several reasons. Firstly, it is the most elementary case and therefore appear in most first courses in algebra. Secondly, it is very similar to the case of the integers, and this analogy is the source of the notion of Euclidean domain. A third reason is that the theory and the algorithms for the multivariate case and for coefficients in a unique factorization domain are strongly based on this particular case. Last but not least, polynomial GCD algorithms and derived algorithms allow one to get useful information on the roots of a polynomial, without computing them.\n\nEuclidean division of polynomials, which is used in Euclid's algorithm for computing GCDs, is very similar to Euclidean division of integers. Its existence is based on the following theorem: Given two univariate polynomials \"a\" and \"b\" ≠ 0 defined over a field, there exist two polynomials \"q\" (the \"quotient\") and \"r\" (the \"remainder\") which satisfy\nand\nwhere \"deg(...)\" denotes the degree and the degree of the zero polynomial is defined as being negative. Moreover, \"q\" and \"r\" are uniquely defined by these relations.\n\nThe difference from Euclidean division of the integers is that, for the integers, the degree is replaced by the absolute value, and that to have uniqueness one has to suppose that \"r\" is non-negative. The rings for which such a theorem exists are called Euclidean domains.\n\nLike for the integers, the Euclidean division of the polynomials may be computed by the long division algorithm. This algorithm is usually presented for paper-and-pencil computation, but it works well on computers, when formalized as follows (note that the names of the variables correspond exactly to the regions of the paper sheet in a pencil-and-paper computation of long division). In the following computation \"deg\" stands for the degree of its argument (with the convention ), and \"lc\" stands for the leading coefficient, the coefficient of the highest degree of the variable.\n\nThe proof of the validity of this algorithm relies on the fact that during the whole \"while\" loop, we have and is a non-negative integer that decreases at each iteration. Thus the proof of the validity of this algorithm also proves the validity of Euclidean division.\n\nAs for the integers, Euclidean division allows us to define Euclid's algorithm for computing GCDs.\n\nStarting from two polynomials \"a\" and \"b\", Euclid's algorithm consists of recursively replacing the pair by (where \"\" denotes the remainder of the Euclidean division, computed by the algorithm of the preceding section), until \"b\" = 0. The GCD is the last non zero remainder.\n\nEuclid's algorithm may be formalized in the recursive programming style as:\n\nIn the imperative programming style, the same algorithm becomes, giving a name to each intermediate remainder:\nThe sequence of the degrees of the is strictly decreasing. Thus after, at most, steps, one get a null remainder, say . As and have the same divisors, the set of the common divisors is not changed by Euclid's algorithm and thus all pairs have the same set of common divisors. The common divisors of and are thus the common divisors of and 0. Thus is a GCD of and .\nThis not only proves that Euclid's algorithm computes GCDs, but also proves that GCDs exist.\n\nBézout's identity is a GCD related theorem, initially proved for the integers, which is valid for every principal ideal domain. In the case of the univariate polynomials over a field, it may be stated as follows.\n\n\\text{zout's identity)}</math>\nand\n\nThe interest of this result in the case of the polynomials is that there is an efficient algorithm to compute the polynomials and , This algorithm differs from Euclid's algorithm by a few more computations done at each iteration of the loop. It is therefore called extended GCD algorithm. Another difference with Euclid's algorithm is that it also uses the quotient, denoted \"quo\", of the Euclidean division instead of only the remainder. This algorithm works as follows.\n\nThe proof that the algorithm satisfies its output specification relies on the fact that, for every we have \nthe latter equality implying \nThe assertion on the degrees follows from the fact that, at every iteration, the degrees of and increase at most as the degree of decreases.\n\nAn interesting feature of this algorithm is that, when the coefficients of Bezout's identity are needed, one gets for free the quotient of the input polynomials by their GCD.\n\nAn important application of extended GCD algorithm is that it allows one to compute division in algebraic field extensions.\n\nLet an algebraic extension of a field , generated by an element whose minimal polynomial has degree . The elements of are usually represented by univariate polynomials over of degree less than .\n\nThe addition in is simply the addition of polynomials:\n\nThe multiplication in is the multiplication of polynomials followed by the division by :\n\nThe inverse of a non zero element of is the coefficient in Bézout's identity , which may be computed by extended GCD algorithm. (the GCD is 1 because the minimal polynomial is irreducible). The degrees inequality in the specification of extended GCD algorithm shows that a further division by is not needed to get deg() < deg().\n\nIn the case of univariate polynomials, there is a strong relationship between greatest common divisors and resultants. In fact the resultant of two polynomials \"P\", \"Q\" is a polynomial function of the coefficients of \"P\" and \"Q\" which has the value zero if and only if the GCD of \"P\" and \"Q\" is not constant.\n\nThe subresultants theory is a generalization of this property that allows characterizing generically the GCD of two polynomials, and the resultant is the 0-th subresultant polynomial.\n\nThe \"i\"-th \"subresultant polynomial\" \"S\"(\"P\" ,\"Q\") of two polynomials \"P\" and \"Q\" is a polynomial of degree at most \"i\" whose coefficients are polynomial functions of the coefficients of \"P\" and \"Q\", and the \"i\"-th \"principal subresultant coefficient\" \"s\"(\"P\" ,\"Q\") is the coefficient of degree \"i\" of \"S\"(\"P\", \"Q\"). They have the property that the GCD of \"P\" and \"Q\" has a degree \"d\" if and only if \n\nIn this case, \"S\"(\"P\" ,\"Q\") is a GCD of \"P\" and \"Q\" and\n\nEvery coefficient of the subresultant polynomials is defined as the determinant of a submatrix of the Sylvester matrix of \"P\" and \"Q\". This implies that subresultants \"specialize\" well. More precisely, subresultants are defined for polynomials over any commutative ring \"R\", and have the following property.\n\nLet \"φ\" be a ring homomorphism of \"R\" into another commutative ring \"S\". It extends to another homomorphism, denoted also \"φ\" between the polynomials rings over \"R\" and \"S\". Then, if \"P\" and \"Q\" are univariate polynomials with coefficients in \"R\" such that \nand \nthen the subresultant polynomials and the principal subresultant coefficients of \"φ\"(\"P\") and \"φ\"(\"Q\") are the image by \"φ\" of those of \"P\" and \"Q\".\n\nThe subresultants have two important properties which make them fundamental for the computation on computers of the GCD of two polynomials with integer coefficients.\nFirstly, their definition through determinants allows bounding, through Hadamard inequality, the size of the coefficients of the GCD.\nSecondly, this bound and the property of good specialization allow to compute the GCD of two polynomials with integer coefficients through modular computation and Chinese remainder theorem (see below).\n\nLet \nbe two univariate polynomials with coefficients in a field \"K\". Let us denote by formula_38 the \"K\" vector space of dimension \"i\" the polynomials of degree less than \"i\". For non-negative integer \"i\" such that \"i\" ≤ \"m\" and \"i\" ≤ \"n\", let \nbe the linear map such that\n\nThe resultant of \"P\" and \"Q\" is the determinant of the Sylvester matrix, which is the (square) matrix of formula_41 on the bases of the powers of \"X\". Similarly, the \"i\"-subresultant polynomial is defined in term of determinants of submatrices of the matrix of formula_42\n\nLet us describe these matrices more precisely;\n\nLet \"p\" = 0 for \"i\" < 0 or \"i\" > \"m\", and \"q\" = 0 for \"i\" < 0 or \"i\" > \"n\". The Sylvester matrix is the (\"m\" + \"n\") × (\"m\" + \"n\")-matrix such that the coefficient of the \"i\"-th row and the \"j\"-th column is \"p\" for \"j\" ≤ \"n\" and \"q\" for \"j\" > \"n\":\n\nThe matrix \"T\" of formula_44 is the (\"m\" + \"n\" − \"i\") × (\"m\" + \"n\" − 2\"i\")-submatrix of \"S\" which is obtained by removing the last \"i\" rows of zeros in the submatrix of the columns 1 to \"n\" − \"i\" and \"n\" + 1 to \"m\" + \"n\" − \"i\" of \"S\" (that is removing \"i\" columns in each block and the \"i\" last rows of zeros). The \"principal subresultant coefficient\" \"s\" is the determinant of the \"m\" + \"n\" − 2\"i\" first rows of \"T\".\n\nLet \"V\" be the (\"m\" + \"n\" − 2\"i\") × (\"m\" + \"n\" − \"i\") matrix defined as follows. First we add (\"i\" + 1) columns of zeros to the right of the (\"m\" + \"n\" − 2\"i\" − 1) × (\"m\" + \"n\" − 2\"i\" − 1) identity matrix. Then we border the bottom of the resulting matrix by a row consisting in (\"m\" + \"n\" − \"i\" − 1) zeros followed by \"X\", \"X\", ..., \"X\", 1:\n\nWith this notation, the \"i\"-th \"subresultant polynomial\" is the determinant of the matrix product \"VT\". Its coefficient of degree \"j\" is the determinant of the square submatrix of \"T\" consisting in its \"m\" + \"n\" − 2\"i\" − 1 first rows and the (\"m\" + \"n\" − \"i\" − \"j\")-th row.\n\nIt is not obvious that, as defined, the subresultants have the desired properties. In fact the proof is rather simple if the properties of linear algebra and those of polynomials are put together.\n\nAs defined, the columns of the matrix \"T\" are the vectors of the coefficients of some polynomials belonging to the image of formula_44. The definition of the \"i\"-th subresultant polynomial \"S\" shows that the vector of its coefficients is a linear combination of these column vectors, and thus that \"S\" belongs to the image of formula_42\n\nIf the degree of the GCD is greater than \"i\", then Bézout's identity shows that every non zero polynomial in the image of formula_44 has a degree larger than \"i\". This implies that \"S\"=0.\n\nIf, on the other hand, the degree of the GCD is \"i\", then Bézout's identity again allows proving that the multiples of the GCD that have a degree lower than \"m\" + \"n\" − \"i\" are in the image of formula_44. The vector space of these multiples has the dimension \"m\" + \"n\" − 2\"i\" and has a base of polynomials of pairwise different degrees, not smaller than \"i\". This implies that the submatrix of the \"m\" + \"n\" − 2\"i\" first rows of the column echelon form of \"T\" is the identity matrix and thus that \"s\" is not 0. Thus \"S\" is a polynomial in the image of formula_44, which is a multiple of the GCD and has the same degree. It is thus a greatest common divisor.\n\nMost root-finding algorithms behave badly with polynomials that have multiple roots. It is therefore useful to detect and remove them before calling a root-finding algorithm. A GCD computation allows detection of the existence of multiple roots, because the multiple roots of a polynomial are the roots of the GCD of the polynomial and its derivative.\n\nAfter computing the GCD of the polynomial and its derivative, further GCD computations provide the complete \"square-free factorization\" of the polynomial, which is a factorization \nwhere, for each \"i\", the polynomial \"f\" either is 1 if \"f\" does not have any root of multiplicity \"i\" or is a square-free polynomial (that is a polynomial without multiple root) whose roots are exactly the roots of multiplicity \"i\" of \"f\" (see Yun's algorithm).\n\nThus the square-free factorization reduces root finding of a polynomial with multiple roots to root finding of several square-free polynomials of lower degree. The square-free factorization is also the first step in most polynomial factorization algorithms.\n\nThe \"Sturm sequence\" of a polynomial with real coefficients is the sequence of the remainders provided by a variant of Euclid's algorithm applied to the polynomial and its derivative. For getting the Sturm sequence, one simply replaces the instruction \nof Euclid's algorithm by \n\nLet \"V\"(\"a\") be the number of changes of signs in the sequence, when evaluated at a point \"a\". Sturm's theorem asserts that \"V\"(\"a\") − \"V\"(\"b\") is the number of real roots of the polynomial in the interval [\"a\", \"b\"]. Thus the Sturm sequence allows computing the number of real roots in a given interval. By subdividing the interval until every subinterval contains at most one root, this provides an algorithm that locates the real roots in intervals of arbitrary small length.\n\nIn this section, we consider polynomials over a unique factorization domain \"R\", typically the ring of the integers, and over its field of fractions \"F\", typically the field of the rational numbers, and we denote \"R\"[\"X\"] and \"F\"[\"X\"] the rings of polynomials in a set of variables over these rings.\n\nThe \"content\" of a polynomial \"p\" ∈ \"R\"[\"X\"], denoted \"cont(\"p\")\", is the GCD of its coefficients. A polynomial \"q\" ∈ \"F\"[\"X\"] may be written\n\nwhere \"p\" ∈ \"R\"[\"X\"] and \"c\" ∈ \"R\": it suffices to take for \"c\" a multiple of all denominators of the coefficients of \"q\" (for example their product) and \"p\" = \"cq\". The \"content\" of \"q\" is defined as:\nIn both cases, the content is defined up to the multiplication by a unit of \"R\".\n\nThe \"primitive part\" of a polynomial in \"R\"[\"X\"] or \"F\"[\"X\"] is defined by \n\nIn both cases, it is a polynomial in \"R\"[\"X\"] that is \"primitive\", which means that 1 is a GCD of its coefficients.\n\nThus every polynomial in \"R\"[\"X\"] or \"F\"[\"X\"] may be factorized as \nand this factorization is unique up to the multiplication of the content by a unit of \"R\" and of the primitive part by the inverse of this unit.\n\nGauss's lemma implies that the product of two primitive polynomials is primitive. It follows that \nand\n\nThe relations of the preceding section imply a strong relation between the GCD's in \"R\"[\"X\"] and in \"F\"[\"X\"]. In order to avoid ambiguities, the notation \"gcd\" will be indexed, in the following, by the ring in which the GCD is computed.\n\nIf \"q\" and \"q\" belong to \"F\"[\"X\"], then\n\nIf \"p\" and \"p\" belong to \"R\"[\"X\"], then\nand\n\nThus the computation of polynomial GCD's is essentially the same problem over \"F\"[\"X\"] and over \"R\"[\"X\"].\n\nFor univariate polynomials over the rational numbers one may think that Euclid's algorithm is a convenient method for computing the GCD. However, it involves to simplify a large number of fractions of integers, and the resulting algorithm is not efficient. For this reason, methods have been designed to modify Euclid's algorithm for working only with polynomials over the integers. They consist in replacing Euclidean division, which introduces fractions, by a so-called \"pseudo-division\", and replacing the remainder sequence of Euclid's algorithm by so-called \"pseudo-remainder sequences\" (see below).\n\nIn the previous section we have seen that the GCD of polynomials in \"R\"[\"X\"] may be deduced from GCDs in \"R\" and in \"F\"[\"X\"]. A closer look on the proof shows that this allows us to prove the existence of GCDs in \"R\"[\"X\"], if they exist in \"R\" and in \"F\"[\"X\"]. In particular, if GCDs exist in \"R\", and if \"X\" is reduced to one variable, this proves that GCDs exist in \"R\"[\"X\"] (Euclid's algorithm proves the existence of GCDs in \"F\"[\"X\"]).\n\nA polynomial in \"n\" variables may be considered as a univariate polynomial over the ring of polynomials in (\"n\" − 1) variables. Thus a recursion on the number of variables shows that if GCDs exists and may be computed in \"R\", then they exist and may be computed in every multivariate polynomial ring over \"R\". In particular, if \"R\" is either the ring of the integers or a field, then GCDs exist in \"R\"[\"x\"..., \"x\"], and what precedes provides an algorithm to compute them.\n\nThe proof that a polynomial ring over a unique factorization domain is also a unique factorization domain is similar, but it does not provide an algorithm, because there is no general algorithm to factor univariate polynomials over a field (there are examples of fields for which there does not exist any factorization algorithm for the univariate polynomials).\n\nIn this section, we consider an integral domain \"Z\" (typically the ring Z of the integers) and its field of fractions \"Q\" (typically the field Q of the rational numbers). Given two polynomials \"A\" and \"B\" in the univariate polynomial ring \"Z\"[\"X\"], the Euclidean division (over \"Q\") of \"A\" by \"B\" provides a quotient and a remainder which may not belong to \"Z\"[\"X\"].\n\nFor, if one applies Euclid's algorithm to the following polynomials \nand\nthe successive remainders of Euclid's algorithm are\nOne sees that, despite the small degree and the small size of the coefficients of the input polynomials, one has to manipulate and simplify integer fractions of rather large size.\nThe \"pseudo-division\" has been introduced to allow a variant of Euclid's algorithm for which all remainders belong to \"Z\"[\"X\"].\n\nIf formula_69 and formula_70 and \"a\" ≥ \"b\", the pseudo-remainder of the pseudo-division of \"A\" by \"B\", denoted by prem(\"A\",\"B\") is \nwhere lc(\"B\") is the leading coefficient of \"B\" (the coefficient of \"X\").\n\nThe pseudo-remainder of the pseudo-division of two polynomials in \"Z\"[\"X\"] belongs always to \"Z\"[\"X\"].\n\nA pseudo-remainder sequence is the sequence of the (pseudo) remainders \"r\" obtained by replacing the instruction \nof Euclid's algorithm by \nwhere \"α\" is an element of \"Z\" that divides exactly every coefficient of the numerator. Different choices of \"α\" give different pseudo-remainder sequences, which are described in the next subsections.\n\nAs the common divisors of two polynomials are not changed if the polynomials are multiplied by invertible constants (in \"Q\"), the last non zero term in a pseudo-remainder sequence is a GCD (in \"Q\"[\"X\"]) of the input polynomials. Therefore, pseudo-remainder sequences allows computing GCD's in \"Q\"[\"X\"] without introducing fractions in \"Q\".\n\nThe simplest (to define) remainder sequence consists in taking always \"α\"=1. In practice, it is not interesting, as the size of the coefficients grow exponentially with the degree of the input polynomials. This appears clearly on the example of the preceding section, for which the successive pseudo-remainders are \nThe number of digits of the coefficients of the successive remainders is more than doubled at each iteration of the algorithm. This is a typical behavior of the trivial pseudo-remainder sequences.\n\nThe \"primitive pseudo-remainder sequence\" consists in taking for \"α\" the content of the numerator. Thus all the \"r\" are primitive polynomials.\n\nThe primitive pseudo-remainder sequence is the pseudo-remainder sequence, which generates the smallest coefficients. However it requires to compute a number of GCD's in \"Z\", and therefore is not sufficiently efficient to be used in practice, especially when \"Z\" is itself a polynomial ring.\n\nWith the same input as in the preceding sections, the successive remainders, after division by their content are \nThe small size of the coefficients hides the fact that a number of integers GCD and divisions by the GCD have been computed.\n\nA subresultant sequence can be also computed with pseudo-remainders. The process consists in choosing \"α\" is such a way that every \"r\" is a subresultant polynomial. Surprisingly, the computation of \"α\" is very easy (see below). On the other hand, the proof of correctness of the algorithm is difficult, because it should take into account all the possibilities for the difference of degrees of two consecutive remainders.\n\nThe coefficients in the subresultant sequence are rarely much larger than those of the primitive pseudo-remainder sequence. As GCD computations in \"Z\" are not needed, the subresultant sequence with pseudo-remainders gives the most efficient computation.\n\nWith the same input as in the preceding sections, the successive remainders are\nThe coefficients have a reasonable size. They are obtained without any GCD computation, only exact divisions. This makes this algorithm more efficient than that of primitive pseudo-remainder sequences.\n\nThe algorithm computing the subresultant sequence with pseudo-remainders is given below. In this algorithm, the input is a pair of polynomials in \"Z\"[X]. The are the successive pseudo remainders in \"Z\"[X], the variables \"i\" and are non negative integers, and the Greek letters denote elements in \"Z\". The functions deg() and rem() denote the degree of a polynomial and the remainder of the Euclidean division. In the algorithm, this remainder is always in \"Z\"[X]. Finally the divisions denoted / are always exact and have their result either in \"Z\"[X] or in \"Z\".\n\nend do.\nNote: \"lc\" stands for the leading coefficient, the coefficient of the highest degree of the variable.\n\nThis algorithm computes not only the greatest common divisor (the last non zero ), but also all the subresultant polynomials: The remainder is the -th subresultant polynomial. If , the -th subresultant polynomial is . All the other subresultant polynomials are zero.\n\nOne may use pseudo-remainders for constructing sequences having the same properties as Sturm sequences. This requires to control the signs of the successive pseudo-remainders, in order to have the same signs as in the Sturm sequence. This may be done by defining a modified pseudo-remainder as follows.\n\nIf formula_69 and formula_70 and \"a\" ≥ \"b\", the modified pseudo-remainder prem2(\"A\", \"B\") of the pseudo-division of \"A\" by \"B\" is \nwhere |lc(\"B\")| is the absolute value of the leading coefficient of \"B\" (the coefficient of \"X\").\n\nFor input polynomials with integers coefficients, this allows retrieval of Sturm sequences consisting of polynomials with integer coefficients. The subresultant pseudo-remainder sequence may be modified similarly, in which case the signs of the remainders coincide with those computed over the rationals.\n\nNote that the algorithm for computing the subresultant pseudo-remainder sequence given above will compute wrong subresultant polynomials if one uses formula_89 instead of formula_90.\n\nIf \"f\" and \"g\" are polynomials in \"F\"[\"x\"] for some finitely generated field \"F\", the Euclidean Algorithm is the most natural way to compute their GCD. However, modern computer algebra systems only use it if \"F\" is finite because of a phenomenon called intermediate expression swell. Although degrees keep decreasing during the Euclidean algorithm, if \"F\" is not finite then the bitsize of the polynomials can increase (sometimes dramatically) during the computations because repeated arithmetic operations in \"F\" tends to lead to larger expressions. For example, the addition of two rational numbers whose denominators are bounded by \"b\" leads to a rational number whose denominator is bounded by \"b\", so in the worst case, the bitsize could nearly double with just one operation.\n\nTo expedite the computation, take a ring \"D\" for which \"f\" and \"g\" are in \"D\"[\"x\"], and take an ideal \"I\" such that \"D\"/\"I\" is a finite ring. Then compute the GCD over this finite ring with the Euclidean Algorithm. Using reconstruction techniques (Chinese remainder theorem, rational reconstruction, etc.) one can recover the GCD of \"f\" and \"g\" from its image modulo a number of ideals \"I\". One can prove that this works provided that one discards modular images with non-minimal degree, and avoids ideals \"I\" modulo which a leading coefficient vanishes.\n\nSuppose formula_91, formula_92, formula_93 and formula_94. If we take formula_95 then formula_96 is a finite ring (not a field since formula_97 is not maximal in formula_98). The Euclidean algorithm applied to the images of formula_99 in formula_100 succeeds and returns 1. This implies that the GCD of formula_99 in formula_102 must be 1 as well. Note that this example could easily be handled by any method because the degrees were too small for expression swell to occur, but it illustrates that if two polynomials have GCD 1, then the modular algorithm is likely to terminate after a single ideal formula_97.\n\n\n"}
{"id": "14433598", "url": "https://en.wikipedia.org/wiki?curid=14433598", "title": "Quadratic residue code", "text": "Quadratic residue code\n\nA quadratic residue code is a type of cyclic code.\n\nExamples of quadratic\nresidue codes include the formula_1 Hamming code\nover formula_2, the formula_3 binary Golay code\nover formula_2 and the formula_5 ternary Golay code\nover formula_6.\n\nThere is a quadratic residue code of length formula_7\nover the finite field formula_8 whenever formula_7\nand formula_10 are primes, formula_7 is odd, and \nformula_10 is a quadratic residue modulo formula_7.\nIts generator polynomial as a cyclic code is given by\nwhere formula_15 is the set of quadratic residues of\nformula_7 in the set formula_17 and\nformula_18 is a primitive formula_7th root of\nunity in some finite extension field of formula_8.\nThe condition that formula_10 is a quadratic residue\nof formula_7 ensures that the coefficients of formula_23\nlie in formula_8. The dimension of the code is\nformula_25.\nReplacing formula_26 by another primitive formula_7-th\nroot of unity formula_28 either results in the same code\nor an equivalent code, according to whether or not formula_29\nis a quadratic residue of formula_7.\n\nAn alternative construction avoids roots of unity. Define\nfor a suitable formula_32. When formula_33\nchoose formula_34 to ensure that formula_35.\nIf formula_10 is odd, choose formula_37,\nwhere formula_38 or formula_39 according to whether\nformula_7 is congruent to formula_41 or formula_42\nmodulo formula_43. Then formula_44 also generates\na quadratic residue code; more precisely the ideal of\nformula_45 generated by formula_44\ncorresponds to the quadratic residue code.\n\nThe minimum weight of a quadratic residue code of length formula_7\nis greater than formula_48; this is the square root bound.\n\nAdding an overall parity-check digit to a quadratic residue code\ngives an extended quadratic residue code. When\nformula_49 (mod formula_43) an extended quadratic\nresidue code is self-dual; otherwise it is equivalent but not\nequal to its dual. By the Gleason–Prange theorem (named for Andrew Gleason and Eugene Prange), the automorphism group of an extended quadratic residue\ncode has a subgroup which is isomorphic to\neither formula_51 or formula_52.\n\n"}
{"id": "12518595", "url": "https://en.wikipedia.org/wiki?curid=12518595", "title": "Ray–Dutt twist", "text": "Ray–Dutt twist\n\nThe Ray–Dutt twist is a mechanism proposed for the racemization of octahedral complexes containing three bidentate chelate rings. Such complexes typically adopt an octahedral molecular geometry in their ground states, in which case they possess helical chirality. The pathway entails formation of an intermediate of C point group symmetry. An alternative pathway that also does not break any metal-ligand bonds is called the Bailar twist. Both of these mechanism product complexes wherein the ligating atoms (X in the scheme) are arranged in an approximate trigonal prism.\n\nThis pathway is called the Ray–Dutt twist in honor of Prafulla Chandra Ray and N. K. Dutt, the inorganic chemists who proposed this process.\n\n"}
{"id": "38266327", "url": "https://en.wikipedia.org/wiki?curid=38266327", "title": "Rhombitetrapentagonal tiling", "text": "Rhombitetrapentagonal tiling\n\nIn geometry, the rhombitetrapentagonal tiling is a uniform tiling of the hyperbolic plane. It has Schläfli symbol of t{4,5}.\n\nThe dual is called the \"deltoidal tetrapentagonal tiling\" with face configuration V.4.4.4.5.\n\n\n\n"}
{"id": "14098497", "url": "https://en.wikipedia.org/wiki?curid=14098497", "title": "Rüdiger Gamm", "text": "Rüdiger Gamm\n\nRüdiger Gamm (born July 10, 1971) is a German \"mental calculator\". He attained the ability to mentally evaluate large arithmetic expressions at the age of 21. He can also speak backwards, and calculate calendars. Featured on the Discovery Channel program \"The Real Superhumans\", he was examined by Allan Snyder, an expert on savants, who concluded that Gamm's ability was not a result of savant syndrome but connected to genetics.\n\nIn terms of mental calculations, Rüdiger's most notable talent is the ability to memorize large powers. In the 2008 Mental Calculation World Cup in Leipzig, he recited 81, which took approximately 2 minutes and 30 seconds. In the tournament itself, he performed strongly, finishing in 5th position overall. He also held a seminar in 2012 at the BOLDTalks event at DUCTAC (Dubai).\n\nRüdiger Gamm was born on July 10, 1971, in Welzheim, Germany. Gamm stated that he learnt how to speak backwards before learning how to speak forwards which prompted classmates to tease him or avoid him. Gamm was a self-proclaimed underachiever at school and stated \"I was the worst in my class at maths. I failed my exam six times and hated school a lot. The only thing I was interested in was bodybuilding. I wanted to be like Arnold Schwarzenegger, rather than a mathematician.\" Gamm recalls, shortly after leaving college, listening to the radio and calculating alongside a champion mathematician. After Gamm answered the calculations faster than the champion, he started training his brain into the field of mental math and, a year later, appeared on the German TV game show Wetten, dass..? (in the United States it is known as Wanna Bet? and in the United Kingdom known as You Bet!.) Gamm won the show with the highest score achieved and received a prize of 8,400 Deutsche Marks.\n\n"}
{"id": "45378348", "url": "https://en.wikipedia.org/wiki?curid=45378348", "title": "Set intersection oracle", "text": "Set intersection oracle\n\nA set intersection oracle (SIO) is a data structure which represents a collection of sets and can quickly answer queries about whether the set intersection of two given sets is non-empty.\n\nThe input to the problem is \"n\" finite sets. The sum of the sizes of all sets is \"N\" (which also means that there are at most \"N\" distinct elements). The SIO should quickly answer any query of the form:\n\nWithout any pre-processing, a query can be answered by inserting the elements of \"S\" into a temporary hash table and then checking for each element of \"S\" whether it is in the hash table. The query time is formula_1.\n\nAlternatively, we can pre-process the sets and create an \"n\"-by-\"n\" table where the intersection information is already entered. Then the query time is formula_2, but the memory required is formula_3.\n\nDefine a \"large set\" as a set with at least formula_4 elements. Obviously there are at most formula_4 such sets. Create a table of intersection data between every large set to every other large set. This requires formula_6 memory. Additionally, for each large set, keep a hash table of all its elements. This requires additional formula_7 memory.\n\nGiven two sets, there are three possible cases:\n\nIn general, if we define a \"large set\" as a set with at least formula_11 elements, then the number of large set is at most formula_12 so the memory required is formula_13, and the query time is formula_14.\n\nThe SIO problem can be reduced to the approximate distance oracle (DO) problem, in the following way.\n\nThis graph has the following properties:\n\nSo, with a DO whose approximation factor of less than 2, we can solve the SIO problem.\n\nIt is believed that the SIO problem does not have a non-trivial solution. I.e., it requires formula_15 space to answer queries in time formula_2. If this conjecture is true, this implies that there is no DO with an approximation factor of less than 2 and a constant query time.\n"}
{"id": "920526", "url": "https://en.wikipedia.org/wiki?curid=920526", "title": "Silver ratio", "text": "Silver ratio\n\nIn mathematics, two quantities are in the silver ratio (also silver mean or silver constant) if the ratio of the sum of the smaller and twice the larger of those quantities, to the larger quantity, is the same as the ratio of the larger one to the smaller one (see below). This defines the silver ratio as an irrational mathematical constant, whose value of one plus the square root of 2 is approximately 2.4142135623. Its name is an allusion to the golden ratio; analogously to the way the golden ratio is the limiting ratio of consecutive Fibonacci numbers, the silver ratio is the limiting ratio of consecutive Pell numbers. The silver ratio is denoted by .\n\nMathematicians have studied the silver ratio since the time of the Greeks (although perhaps without giving a special name until recently) because of its connections to the square root of 2, its convergents, square triangular numbers, Pell numbers, octagons and the like.\n\nThe relation described above can be expressed algebraically:\n\nor equivalently,\n\nThe silver ratio can also be defined by the simple continued fraction [2; 2, 2, 2, ...]:\n\nThe convergents of this continued fraction (, , , , , ...) are ratios of consecutive Pell numbers. These fractions provide accurate rational approximations of the silver ratio, analogous to the approximation of the golden ratio by ratios of consecutive Fibonacci numbers.\n\nFor comparison, two quantities \"a\", \"b\" with \"a\" > \"b\" > 0 are said to be in the \"golden ratio\" if,\n\nHowever, they are in the \"silver ratio\" if,\n\nEquivalently,\n\nTherefore,\n\nMultiplying by and rearranging gives\n\nUsing the quadratic formula, two solutions can be obtained. Because is the ratio of positive quantities, it is necessarily positive, so,\n\nThe silver ratio is a Pisot–Vijayaraghavan number (PV number), as its conjugate has absolute value less than 1. In fact it is the second smallest quadratic PV number after the golden ratio. This means the distance from to the nearest integer is . Thus, the sequence of fractional parts of , (taken as elements of the torus) converges. In particular, this sequence is not equidistributed mod 1.\n\nThe lower powers of the silver ratio are\n\nThe powers continue in the pattern\nwhere\nFor example, using this property:\n\nUsing and as initial conditions, a Binet-like formula results from solving the recurrence relation\nwhich becomes\n\nThe silver ratio is intimately connected to trigonometric ratios for .\n\nSo the area of a regular octagon with side length is given by\n\nThe paper sizes under ISO 216 are rectangles in the proportion 1: (approximately 1:1.4142135 decimal), sometimes called \"A4 rectangles\". Removing a largest possible square from a sheet of such paper leaves a rectangle with proportions which is the same as , the silver ratio. Removing a largest square from one of \"these\" sheets leaves one again with aspect ratio 1:. A rectangle whose aspect ratio is the silver ratio is sometimes called a silver rectangle by analogy with golden rectangles. Confusingly, \"silver rectangle\" can also refer to the paper sizes specified by ISO 216.\nRemoving the largest possible square from either kind yields a silver rectangle of the other kind, and then repeating the process once more gives a rectangle of the original shape but smaller by a linear factor of .\n\nHowever, only the 1: rectangles (rectangles with the shape of ISO 216 paper) have the property that by cutting the rectangle in half across its long side produces two smaller rectangles of the same aspect ratio.\n\nThe silver rectangle is connected to the regular octagon. If a regular octagon is partitioned into two isosceles trapezoids and a rectangle, then the rectangle is a silver rectangle with an aspect ratio of 1:, and the 4 sides of the trapezoids are in a ratio of 1:1:1:. If the edge length of a regular octagon is , then the inradius of the octagon (the distance between opposite sides) is , and the area of the octagon is .\n\n\n\n"}
{"id": "16911683", "url": "https://en.wikipedia.org/wiki?curid=16911683", "title": "Superperfect number", "text": "Superperfect number\n\nIn mathematics, a superperfect number is a positive integer \"n\" that satisfies\n\nwhere σ is the divisor summatory function. Superperfect numbers are a generalization of perfect numbers. The term was coined by Suryanarayana (1969).\n\nThe first few superperfect numbers are :\n\nTo illustrate: it can be seen that 16 is a superperfect number as σ(16) = 1 + 2 + 4 + 8 + 16 = 31, and σ(31) = 1 + 31 = 32, thus σ(σ(16)) = 32 = 2 × 16.\n\nIf \"n\" is an \"even\" superperfect number, then \"n\" must be a power of 2, 2, such that 2 − 1 is a Mersenne prime.\n\nIt is not known whether there are any odd superperfect numbers. An odd superperfect number \"n\" would have to be a square number such that either \"n\" or σ(\"n\") is divisible by at least three distinct primes. There are no odd superperfect numbers below 7.\n\nPerfect and superperfect numbers are examples of the wider class of \"m\"-superperfect numbers, which satisfy\n\ncorresponding to \"m\"=1 and 2 respectively. For \"m\" ≥ 3 there are no even \"m\"-superperfect numbers.\n\nThe \"m\"-superperfect numbers are in turn examples of (\"m\",\"k\")-perfect numbers which satisfy\n\nWith this notation, perfect numbers are (1,2)-perfect, multiperfect numbers are (1,\"k\")-perfect, superperfect numbers are (2,2)-perfect and \"m\"-superperfect numbers are (\"m\",2)-perfect. Examples of classes of (\"m\",\"k\")-perfect numbers are:\n\n"}
{"id": "22288224", "url": "https://en.wikipedia.org/wiki?curid=22288224", "title": "Symbolic-numeric computation", "text": "Symbolic-numeric computation\n\nIn mathematics and computer science, symbolic-numeric computation is the use of software that combines symbolic and numeric methods to solve problems.\n\n\nProfessional organizations\n"}
{"id": "21171254", "url": "https://en.wikipedia.org/wiki?curid=21171254", "title": "Type-2 fuzzy sets and systems", "text": "Type-2 fuzzy sets and systems\n\nType-2 fuzzy sets and systems generalize standard Type-1 fuzzy sets and systems so that more uncertainty can be handled. From the very beginning of fuzzy sets, criticism was made about the fact that the membership function of a type-1 fuzzy set has no uncertainty associated with it, something that seems to contradict the word \"fuzzy\", since that word has the connotation of lots of uncertainty. So, what does one do when there is uncertainty about the value of the membership function? The answer to this question was provided in 1975 by the inventor of fuzzy sets, Prof. Lotfi A. Zadeh, when he proposed more sophisticated kinds of fuzzy sets, the first of which he called a type-2 fuzzy set. A type-2 fuzzy set lets us incorporate uncertainty about the membership function into fuzzy set theory, and is a way to address the above criticism of type-1 fuzzy sets head-on. And, if there is no uncertainty, then a type-2 fuzzy set reduces to a type-1 fuzzy set, which is analogous to probability reducing to determinism when unpredictability vanishes.\n\nIn order to symbolically distinguish between a type-1 fuzzy set and a type-2 fuzzy set, a tilde symbol is put over the symbol for the fuzzy set; so, A denotes a type-1 fuzzy set, whereas Ã denotes the comparable type-2 fuzzy set. When the latter is done, the resulting type-2 fuzzy set is called a general type-2 fuzzy set (to distinguish it from the special interval type-2 fuzzy set).\n\nProf. Zadeh didn't stop with type-2 fuzzy sets, because in that 1976 paper he also generalized all of this to type-\"n\" fuzzy sets. The present article focuses only on type-2 fuzzy sets because they are the \"next step\" in the logical progression from type-1 to type-\"n\" fuzzy sets, where \"n\" = 1, 2, … . Although some researchers are beginning to explore higher than type-2 fuzzy sets, as of early 2009, this work is in its infancy.\nThe membership function of a general type-2 fuzzy set, Ã, is three-dimensional (Fig. 1), where the third dimension is the value of the membership function at each point on its two-dimensional domain that is called its footprint of uncertainty (FOU).\n\nFor an interval type-2 fuzzy set that third-dimension value is the same (e.g., 1) everywhere, which means that no new information is contained in the third dimension of an interval type-2 fuzzy set. So, for such a set, the third dimension is ignored, and only the FOU is used to describe it. It is for this reason that an interval type-2 fuzzy set is sometimes called a \"first-order uncertainty\" fuzzy set model, whereas a general type-2 fuzzy set (with its useful third-dimension) is sometimes referred to as a \"second-order uncertainty\" fuzzy set model.\nThe FOU represents the blurring of a type-1 membership function, and is completely described by its two bounding functions (Fig. 2), a lower membership function (LMF) and an upper membership function (UMF), both of which are type-1 fuzzy sets! Consequently, it is possible to use type-1 fuzzy set mathematics to characterize and work with interval type-2 fuzzy sets. This means that engineers and scientists who already know type-1 fuzzy sets will not have to invest a lot of time learning about general type-2 fuzzy set mathematics in order to understand and use interval type-2 fuzzy sets.\n\nWork on type-2 fuzzy sets languished during the 1980s and early-to-mid 1990's, although a small number of articles were published about them. People were still trying to figure out what to do with type-1 fuzzy sets, so even though Zadeh proposed type-2 fuzzy sets in 1976, the time was not right for researchers to drop what they were doing with type-1 fuzzy sets to focus on type-2 fuzzy sets. This changed in the latter part of the 1990s as a result of Prof. Jerry Mendel and his student's works on type-2 fuzzy sets and systems. Since then, more and more researchers around the world are writing articles about type-2 fuzzy sets and systems.\n\nInterval type-2 fuzzy sets have received the most attention because the mathematics that is needed for such sets—primarily Interval arithmetic—is much simpler than the mathematics that is needed for general type-2 fuzzy sets. So, the literature about interval type-2 fuzzy sets is large, whereas the literature about general type-2 fuzzy sets is much smaller. Both kinds of fuzzy sets are being actively researched by an ever-growing number of researchers around the world and have resulted in successful employment in variety of domains such as robot control.\n\nFormilleri for the following have already been worked out for interval type-2 fuzzy sets:\n\n\nType-2 fuzzy sets are finding very wide applicability in rule-based fuzzy logic systems (FLSs) because they let uncertainties be modeled by them whereas such uncertainties cannot be modeled by type-1 fuzzy sets. A block diagram of a type-2 FLS is depicted in Fig. 3. This kind of FLS is used in fuzzy logic control, fuzzy logic signal processing, rule-based classification, etc., and is sometimes referred to as a \"function approximation\" application of fuzzy sets, because the FLS is designed to minimize an error function.\n\nThe following discussions, about the four components in the Fig. 3 rule-based FLS, are given for an interval type-2 FLS, because to-date they are the most popular kind of type-2 FLS; however, most of the discussions are also applicable for a general type-2 FLS.\n\nRules, that are either provided by subject experts or are extracted from numerical data, are expressed as a collection of IF-THEN statements, e.g.,\n\nFuzzy sets are associated with the terms that appear in the antecedents (IF-part) or consequents (THEN-part) of rules, and with the inputs to and the outputs of the FLS. Membership functions are used to describe these fuzzy sets, and in a type-1 FLS they are all type-1 fuzzy sets, whereas in an interval type-2 FLS at least one membership function is an interval type-2 fuzzy set.\n\nAn interval type-2 FLS lets any one or all of the following kinds of uncertainties be quantified:\n\n\nIn Fig. 3, measured (crisp) inputs are first transformed into fuzzy sets in the Fuzzifier block because it is fuzzy sets and not numbers that activate the rules which are described in terms of fuzzy sets and not numbers. Three kinds of fuzzifiers are possible in an interval type-2 FLS. When measurements are:\n\nIn Fig. 3, after measurements are fuzzified, the resulting input fuzzy sets are mapped into fuzzy output sets by the Inference block. This is accomplished by first quantifying each rule using fuzzy set theory, and by then using the mathematics of fuzzy sets to establish the output of each rule, with the help of an inference mechanism. If there are \"M\" rules then the fuzzy input sets to the Inference block will activate only a subset of those rules, where the subset contains at least one rule and usually way fewer than \"M\" rules. Inference is done one rule at a time. So, at the output of the Inference block, there will be one or more \"fired-rule fuzzy output sets\".\n\nIn most engineering applications of a FLS, a number (and not a fuzzy set) is needed as its final output, e.g., the consequent of the rule given above is \"Rotate the valve a bit to the right.\" No automatic valve will know what this means because \"a bit to the right\" is a linguistic expression, and a valve must be turned by numerical values, i.e. by a certain number of degrees. Consequently, the fired-rule output fuzzy sets have to be converted into a number, and this is done in the Fig. 3 Output Processing block.\n\nIn a type-1 FLS, output processing, called Defuzzification, maps a type-1 fuzzy set into a number. There are many ways for doing this, e.g., compute the union of the fired-rule output fuzzy sets (the result is another type-1 fuzzy set) and then compute the center of gravity of the membership function for that set; compute a weighted average of the center of gravities of each of the fired rule consequent membership functions; etc.\n\nThings are somewhat more complicated for an interval type-2 FLS, because to go from an interval type-2 fuzzy set to a number (usually) requires two steps (Fig. 3). The first step, called type-reduction, is where an interval type-2 fuzzy set is reduced to an interval-valued type-1 fuzzy set. There are as many type-reduction methods as there are type-1 defuzzification methods. An algorithm developed by Karnik and Mendel now known as the KM Algorithm is used for type-reduction. Although this algorithm is iterative, it is very fast.\n\nThe second step of Output Processing, which occurs after type-reduction, is still called defuzzification. Because a type-reduced set of an interval type-2 fuzzy set is always a finite interval of numbers, the defuzzified value is just the average of the two end-points of this interval.\n\nIt is clear from Fig. 3 that there can be two outputs to an interval type-2 FLS—crisp numerical values and the type-reduced set. The latter provides a measure of the uncertainties that have flowed through the interval type-2 FLS, due to the (possibly) uncertain input measurements that have activated rules whose antecedents or consequents or both are uncertain. Just as standard deviation is widely used in probability and statistics to provide a measure of unpredictable uncertainty about a mean value, the type-reduced set can provided a measure of uncertainty about the crisp output of an interval type-2 FLS.\n\nAnother application for fuzzy sets has also been inspired by Prof. Zadeh — Computing With Words. Different acronyms have been used for \"computing with words,\" e.g., CW and CWW. According to Zadeh:\nOf course, he did not mean that computers would actually compute using words—single words or phrases—rather than numbers. He meant that computers would be activated by words, which would be converted into a mathematical representation using fuzzy sets and that these fuzzy sets would be mapped by a CWW engine into some other fuzzy set after which the latter would be converted back into a word. A natural question to ask is: Which kind of fuzzy set—type-1 or type-2—should be used as a model for a word? Mendel has argued, on the basis of Karl Popper's concept of Falsificationism, that using a type-1 fuzzy set as a model for a word is scientifically incorrect. An interval type-2 fuzzy set should be used as a (first-order uncertainty) model for a word. Much research is under way about CWW.\n\nType-2 fuzzy sets were applied in image processing, video processing and computer vision, as well as Failure Mode And Effect Analysis.\n\n\n\nThere are two I\"EEE Expert Now\" multi-media modules that can be accessed from the IEEE at: http://www.ieee.org/web/education/Expert_Now_IEEE/Catalog/AI.html\n\nFreeware MATLAB implementations, which cover general and interval type-2 fuzzy sets and systems, as well as type-1 fuzzy systems, are available at: http://sipi.usc.edu/~mendel/software.\nSoftware supporting discrete interval type-2 fuzzy logic systems is available at:\nDIT2FLS Toolbox - http://dit2fls.com/projects/dit2fls-toolbox/\nDIT2FLS Library Package - http://dit2fls.com/projects/dit2fls-library-package/\n\nJava libraries including source code for type-1, interval- and general type-2 fuzzy systems are available at: http://juzzy.wagnerweb.net/.\n\nAn open source Matlab/Simulink Toolbox for Interval Type-2 Fuzzy Logic Systems is available at: http://web.itu.edu.tr/kumbasart/type2fuzzy.htm\n"}
{"id": "3126156", "url": "https://en.wikipedia.org/wiki?curid=3126156", "title": "Whitney's planarity criterion", "text": "Whitney's planarity criterion\n\nIn mathematics, Whitney's planarity criterion is a matroid-theoretic characterization of planar graphs, named after Hassler Whitney. It states that a graph \"G\" is planar if and only if its graphic matroid is also cographic (that is, it is the dual matroid of another graphic matroid).\n\nIn purely graph-theoretic terms, this criterion can be stated as follows: There must be another (dual) graph \"G\"'=(\"V\"',\"E\"') and a bijective correspondence between the edges \"E\"' and the edges \"E\" of the original graph \"G\", such that a subset \"T\" of \"E\" forms a spanning tree of \"G\" if and only if the edges corresponding to the complementary subset \"E\"-\"T\" form a spanning tree of \"G\"'.\n\nAn equivalent form of Whitney's criterion is that a graph \"G\" is planar if and only if it has a dual graph whose graphic matroid is dual to the graphic matroid of \"G\". \nA graph whose graphic matroid is dual to the graphic matroid of \"G\" is known as an algebraic dual of \"G\". This, Whitney's planarity criterion can be expressed succinctly as: a graph is planar if and only if it has an algebraic dual.\n\nIf a graph is embedded into a topological surface such as the plane, in such a way that every face of the embedding is a topological disk, then the dual graph of the embedding is defined as the graph (or in some cases multigraph) \"H\" that has a vertex for every face of the embedding, and an edge for every adjacency between a pair of faces.\nAccording to Whitney's criterion, the following conditions are equivalent:\n\nIt is possible to define dual graphs of graphs embedded on nonplanar surfaces such as the torus, but these duals do not generally have the correspondence between cuts, cycles, and spanning trees required by Whitney's criterion.\n"}
{"id": "20115268", "url": "https://en.wikipedia.org/wiki?curid=20115268", "title": "Wildfire modeling", "text": "Wildfire modeling\n\nIn computational science, wildfire modeling is concerned with numerical simulation of wildland fires in order to understand and predict fire behavior. Wildfire modeling can ultimately aid wildland fire suppression, namely increase safety of firefighters and the public, reduce risk, and minimize damage. Wildfire modeling can also aid in protecting ecosystems, watersheds, and air quality.\n\nWildfire modeling attempts to reproduce fire behavior, such as how quickly the fire spreads, in which direction, how much heat it generates. A key input to behavior modeling is the Fuel Model, or type of fuel, through which the fire is burning. Behavior modeling can also include whether the fire transitions from the surface (a \"surface fire\") to the tree crowns (a \"crown fire\"), as well as extreme fire behavior including rapid rates of spread, fire whirls, and tall well-developed convection columns. Fire modeling also attempts to estimate fire effects, such as the ecological and hydrological effects of the fire, fuel consumption, tree mortality, and amount and rate of smoke produced.\n\nWildland fire behavior is affected by weather, fuel characteristics, and topography.\n\nWeather influences fire through wind and moisture. Wind increases the fire spread in the wind direction, higher temperature makes the fire burn faster, while higher relative humidity, and precipitation (rain or snow) may slow it down or extinguish it altogether. Weather involving fast wind changes can be particularly dangerous, since they can suddenly change the fire direction and behavior. Such weather includes cold fronts, foehn winds, thunderstorm downdrafts, sea and land breeze, and diurnal slope winds.\n\nWildfire fuel includes grass, wood, and anything else that can burn. Small dry twigs burn faster while large logs burn slower; dry fuel ignites more easily and burns faster than wet fuel.\n\nTopography factors that influence wildfires include the orientation toward the sun, which influences the amount of energy received from the sun, and the slope (fire spreads faster uphill). Fire can accelerate in narrow canyons and it can be slowed down or stopped by barriers such as creeks and roads.\n\nThese factors act in combination. Rain or snow increases the fuel moisture, high relative humidity slows the drying of the fuel, while winds can make fuel dry faster. Wind can change the fire-accelerating effect of slopes to effects such as downslope windstorms (called Santa Anas, foehn winds, East winds, depending on the geographic location). Fuel properties may vary with topography as plant density varies with elevation or aspect with respect to the sun.\n\nIt has long been recognized that \"fires create their own weather.\" That is, the heat and moisture created by the fire feed back into the atmosphere, creating intense winds that drive the fire behavior. The heat produced by the wildfire changes the temperature of the atmosphere and creates strong updrafts, which can change the direction of surface winds. The water vapor released by the fire changes the moisture balance of the atmosphere. The water vapor can be carried away, where the latent heat stored in the vapor is released through condensation.\n\nLike all models in computational science, fire models need to strike a balance between fidelity, availability of data, and fast execution. Wildland fire models span a vast range of complexity, from simple cause and effect principles to the most physically complex presenting a difficult supercomputing challenge that cannot hope to be solved faster than real time.\n\nForest-fire models have been developed since 1940 to the present, but a lot of chemical and thermodynamic questions related to fire behaviour are still to be resolved. Scientists and their forest fire models from 1940 till 2003 are listed in article. Models can be divided into three groups: Empirical, Semi-empirical, and Physically based.\n\nConceptual models from experience and intuition from past fires can be used to anticipate the future. Many semi-empirical fire spread equations, as in those published by the USDA Forest Service, Forestry Canada, Nobel, Bary, and Gill, and Cheney, Gould, and Catchpole for Australasian fuel complexes have been developed for quick estimation of fundamental parameters of interest such as fire spread rate, flame length, and fireline intensity of surface fires at a point for specific fuel complexes, assuming a representative point-location wind and terrain slope. Based on the work by Fons's in 1946, and Emmons in 1963, the quasi-steady equilibrium spread rate calculated for a surface fire on flat ground in no-wind conditions was calibrated using data of piles of sticks burned in a flame chamber/wind tunnel to represent other wind and slope conditions for the fuel complexes tested.\n\nTwo-dimensional fire growth models such as FARSITE and Prometheus, the Canadian wildland fire growth model designed to work in Canadian fuel complexes, have been developed that apply such semi-empirical relationships and others regarding ground-to-crown transitions to calculate fire spread and other parameters along the surface. Certain assumptions must be made in models such as FARSITE and Prometheus to shape the fire growth. For example, Prometheus and FARSITE use the Huygens principle of wave propagation. A set of equations that can be used to propagate (shape and direction) a fire front using an elliptical shape was developed by Richards in 1990. Although more sophisticated applications use a three-dimensional numerical weather prediction system to provide inputs such as wind velocity to one of the fire growth models listed above, the input was passive and the feedback of the fire upon the atmospheric wind and humidity are not accounted for.\n\nA simplified physically based two-dimensional fire spread models based upon conservation laws that use radiation as the dominant heat transfer mechanism and convection, which represents the effect of wind and slope, lead to reaction–diffusion systems of partial differential equations.\n\nMore complex physical models join computational fluid dynamics models with a wildland fire component and allow the fire to feed back upon the atmosphere. These models include NCAR's Coupled Atmosphere-Wildland Fire-Environment (CAWFE) model developed in 2005, WRF-Fire at NCAR and University of Colorado Denver which combines the Weather Research and Forecasting Model with a spread model by the level-set method, University of Utah's Coupled Atmosphere-Wildland Fire Large Eddy Simulation developed in 2009, Los Alamos National Laboratory's FIRETEC developed in, the WUI (Wildland Urban Interface) Fire Dynamics Simulator (WFDS) developed in 2007, and, to some degree, the two-dimensional model FIRESTAR. These tools have different emphases and have been applied to better understand the fundamental aspects of fire behavior, such as fuel inhomogeneities on fire behavior, feedbacks between the fire and the atmospheric environment as the basis for the universal fire shape, and are beginning to be applied to wildland urban interface house-to-house fire spread at the community-scale.\n\nThe cost of added physical complexity is a corresponding increase in computational cost, so much so that a full three-dimensional explicit treatment of combustion in wildland fuels by direct numerical simulation (DNS) at scales relevant for atmospheric modeling does not exist, is beyond current supercomputers, and does not currently make sense to do because of the limited skill of weather models at spatial resolution under 1 km. Consequently, even these more complex models parameterize the fire in some way, for example, papers by Clark use equations developed by Rothermel for the USDA forest service to calculate local fire spread rates using fire-modified local winds. And, although FIRETEC and WFDS carry prognostic conservation equations for the reacting fuel and oxygen concentrations, the computational grid cannot be fine enough to resolve the reaction rate-limiting mixing of fuel and oxygen, so approximations must be made concerning the subgrid-scale temperature distribution or the combustion reaction rates themselves. These models also are too small-scale to interact with a weather model, so the fluid motions use a computational fluid dynamics model confined in a box much smaller than the typical wildfire.\n\nAttempts to create the most complete theoretical model were made by Albini F.A. in USA and Grishin A.M. in Russia. Grishin's work is based on the fundamental laws of physics, conservation and theoretical justifications are provided. The simplified two-dimensional model of running crown forest fire was developed in Belarusian State University by Barovik D.V. and Taranchuk V.B..\n\nData assimilation periodically adjusts the model state to incorporate new data using statistical methods. Because fire is highly nonlinear and irreversible, data assimilation for fire models poses special challenges, and standard methods, such as the ensemble Kalman filter (EnKF) do not work well. Statistical variability of corrections and especially large corrections may result in nonphysical states, which tend to be preceded or accompanied by large spatial gradients. In order to ease this problem, the regularized EnKF penalizes large changes of spatial gradients in the Bayesian update in EnKF. The regularization technique has a stabilizing effect on the simulations in the ensemble but it does not improve much the ability of the EnKF to track the data: The posterior ensemble is made out of linear combinations of the prior ensemble, and if a reasonably close location and shape of the fire cannot be found between the linear combinations, the data assimilation is simply out of luck, and the ensemble cannot approach the data. From that point on, the ensemble evolves essentially without regard to the data. This is called filter divergence. So, there is clearly a need to adjust the simulation state by a position change rather than an additive correction only. The \"morphing EnKF\" combines the ideas of data assimilation with image registration and morphing to provide both additive and position correction in a natural manner, and can be used to change a model state reliably in response to data.\n\nThe limitations on fire modeling are not entirely computational. At this level, the models encounter limits in knowledge about the composition of pyrolysis products and reaction pathways, in addition to gaps in basic understanding about some aspects of fire behavior such as fire spread in live fuels and surface-to-crown fire transition.\n\nThus, while more complex models have value in studying fire behavior and testing fire spread in a range of scenarios, from the application point of view, FARSITE and Palm-based applications of BEHAVE have shown great utility as practical in-the-field tools because of their ability to provide estimates of fire behavior in real time. While the coupled fire-atmosphere models have the ability to incorporate the ability of the fire to affect its own local weather, and model many aspects of the explosive, unsteady nature of fires that cannot be incorporated in current tools, it remains a challenge to apply these more complex models in a faster-than-real-time operational environment. Also, although they have reached a certain degree of realism when simulating specific natural fires, they must yet address issues such as identifying what specific, relevant operational information they could provide beyond current tools, how the simulation time could fit the operational time frame for decisions (therefore, the simulation must run substantially faster than real time), what temporal and spatial resolution must be used by the model, and how they estimate the inherent uncertainty in numerical weather prediction in their forecast. These operational constraints must be used to steer model development.\n\n\n"}
