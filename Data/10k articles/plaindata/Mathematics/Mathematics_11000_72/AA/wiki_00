{"id": "22659407", "url": "https://en.wikipedia.org/wiki?curid=22659407", "title": "ATLAS of Finite Groups", "text": "ATLAS of Finite Groups\n\nThe ATLAS of Finite Groups, often simply known as the ATLAS, is a group theory book by John Horton Conway, Robert Turner Curtis, Simon Phillips Norton, Richard Alan Parker and Robert Arnott Wilson (with computational assistance from J. G. Thackray), published in December 1985 by Oxford University Press and reprinted with corrections in 2003 (). It lists basic information about 93 finite simple groups, the information being generally: its order, Schur multiplier, outer automorphism group, various constructions (such as presentations), conjugacy classes of maximal subgroups (with characters group action they define), and, most importantly, character tables (including power maps on the conjugacy classes) of the group itself and bicyclic extensions given by stem extensions and automorphism groups. In certain cases (such as for the Chevalley groups formula_1), the character table is not listed and only basic information is given.\n\nThe ATLAS is a recognizable large format book (sized 420mm by 300mm) with a cherry red cardboard cover and spiral binding. The names of the authors, all six letters long, with initials for the first and second letter, are printed on the cover in the form of an array which evokes the idea of a character table.\n\nThe ATLAS is being continued in the form of an electronic database, the ATLAS of Finite Group Representations.\n"}
{"id": "34905254", "url": "https://en.wikipedia.org/wiki?curid=34905254", "title": "Actuarial credentialing and exams", "text": "Actuarial credentialing and exams\n\nThe actuarial credentialing and exam process usually requires passing a rigorous series of professional examinations, most often taking several years in total, before one can become recognized as a credentialed actuary. In some countries, such as Denmark, most study takes place in a university setting. In others, such as the U.S., most study takes place during employment through a series of examinations. In the UK, and countries based on its process, there is a hybrid university-exam structure.\n\nThe education system in Australia is divided into three components: an exam-based curriculum; a professionalism course; and work experience . The system is governed by the Institute of Actuaries of Australia.\n\nThe exam-based curriculum is in three parts. Part I relies on exemptions from an accredited under-graduate degree from either Monash University, Macquarie University, University of New South Wales, University of Melbourne, Australian National University or Curtin University . The courses cover subjects including finance, financial mathematics, economics, contingencies, demography, models, probability and statistics. Students may also gain exemptions by passing the exams of the Institute of Actuaries in London . Part II is the Actuarial control cycle and is also offered by each of the universities above . Part III consists of four half-year courses of which two are compulsory and the other two allow specialization .\n\nTo become an Associate, one needs to complete Part I and Part II of the accreditation process, perform 3 years of recognized work experience, and complete a professionalism course.\n\nTo become a Fellow, candidates must complete Part I, II, III, and take a professionalism course. Work experience is not required however, as the Institute deems that those who've successfully completed Part III have shown enough level of professionalism.\n\nThe Actuarial Society of Bangladesh is the unique Professional Body of Actuaries in Bangladesh. Actuarial Society of Bangladesh follow the curriculum of The Institute and Faculty of Actuaries, United Kingdom. \nNo University in Bangladesh provide the academic degrees like bachelor's degree, master's degrees etc. in Actuarial Science. University of Dhaka, Rajshahi University, Jahangirnagar University provide a few course of Actuarial Science in Statistics, Applied Statistics department. From 2015 Dhaka university and East west university are providing Master's in Actuarial Science.\n\nThe Canadian Institute of Actuaries (the CIA) recognizes fellows of both the Society of Actuaries and the Casualty Actuary Society, provided that they have specialized study in Canadian actuarial practice. For fellows of the SOA, this is fulfilled by taking the CIA’s Practice Education Course (PEC). For fellows of the Casualty Actuarial Society, this is fulfilled by taking the nation specific Exam 6-Canada, instead of Exam 6-United States . Unlike their American counterparts, the CIA only has one class of actuary: Fellow. Further, the CIA requires three years of actuarial practice within the previous decade, and 18 months of Canadian actuarial practice within the last three years, to become a fellow .\nThe CIA also offers an associate designation.\n\nIn Denmark it normally takes five years of study at the University of Copenhagen to become an actuary with no professional experience requirement. There is a focus on statistics and probability theory, and a requirement for a master's thesis . By Danish law, responsibility for the practise of any life insurance business must be taken by a formally acknowledged and approved actuary. Approval as a formally responsible actuary requires three to five years of professional experience.\n\nCurrent rules for the German Actuarial Society require an actuary to pass more than 13 exams. \n\nIn Greece the only specialized school of actuaries is the Department of Statistics and Actuary-Finance Mathematics of the University of the Aegean, in Samos. The duration of studies is four years, with a practice period included, and the certificate given is a bachelor's degree. The Diploma of Actuary is given by the Actuaries Union of Greece, after successful exams within the Union. Other schools that offer actuary directions can be found throughout the rest departments of Statistics in the various universities of the country, most notably that of the Athens University of Economics and Business (OPA/ASOEE), which is also the top economic university of Greece.\n\nThe Actuarial Society of India (now converted into Institute of Actuaries of India) offers both associate-ship and fellowship classes of membership. However, prospective candidates must be admitted to the society as students before they achieve associate-ship or fellowship. The exam sequence is similar to the British model, with Core and Specialty technical and application exams. The exams are conducted twice a year during the months of May–June and October–November . Starting from January 2012, the institute has started conducting entrance exam. Only those applicants who clear the entrance test can appear for the Core Technical papers.\n\nItalian actuaries also receive their training through university plus a single examination given by the state (\"Esame di Stato\"). The studies usually take a total of five years to complete, three (\"Triennale\") plus two (\"Magistrale\"), because students need to pass at least 30 exams (the exact number depends on the university and curriculum), many with both written and oral components on actuarial and economic topics. After university, to become qualified to sign statements of actuarial opinion, students must pass the \"Esame di Stato\", which is offered twice a year in Rome and Trieste; the \"Esame di Stato\" consists of two written sections, a practical portion, and an oral exam. The association of qualified actuaries is called \"Ordine degli Attuari\" (\"Order of Actuaries\").\n\nUnlike in the United States, in Mexico actuarial training consists of a full four or five-year licenciatura (bachelor) degree course. Only a few universities in the country offer the degree; some of them are the National Autonomous University of Mexico (UNAM), Autonomous University of Yucatán (UADY), Universidad de las Americas Puebla (UDLAP), Universidad Anahuac, Autonomous Technological Institute of Mexico (ITAM), Autonomous University of Guadalajara (UAG), and Autonomous University of Nuevo León (UANL).\n\nIn Norway the education to become an actuary takes five years. The education usually consists of a bachelor's degree (three years) and a master's degree (two years). The bachelor's degree needs to contain a specific amount of courses in mathematics and statistics. The master's degree usually consists of one year of courses and one year writing a master's degree about a topic related to the actuarial profession. The University of Bergen and The University of Oslo offers the education to become an actuary in Norway .\nTo become an international qualified actuary, a person with a Norwegian actuarial education must also take two courses in economics (macroeconomics and accounting) and a course in ethics. The ethics course, which lasts a day, is offered by the Norwegian Society of Actuaries .\n\nIn Portugal the only school that offers a degree in actuarial science is ISEG at the University of Lisbon. It is a two-year master's degree, fully integrated into the Bologna regimen. The programme is, since 2017/18, accredited by the Institute and Faculty of Actuaries in the UK, leading to exemptions based on the student overall performance during the course, or leading to exemptions from individual exams based on the student's performance in certain modules of the Masters.\n\nActuaries in South Africa are served by the Actuarial Society of South Africa (ASSA). Until recently the requirement to qualify as an actuary in South Africa was to pass the exams hosted by the UK bodies. Starting in 2010, a South African actuarial qualification hosted by ASSA has replaced this arrangement (ASSA's website). Key changes include exam syllabuses based on South African specific content. The UK actuarial professional bodies however still supports Actuaries qualification through the UK. Students may receive exemption from part of the examinations for qualification from approved universities. The South Africa qualification does have mutual recognition with many of the international actuarial bodies as well as approval of the syllabus from the International Actuarial Association.\n\nOne may obtain the Chartered Enterprise Risk Actuary (CERA) designation through the ASSA.\n\nActuarial training in Sweden takes place at Stockholm University. The five-year master's program (for those with no previous university-level knowledge in mathematics, or without a bachelor's degree in mathematics) covers the subjects mathematics, mathematical statistics, insurance mathematics, financial mathematics, insurance law and insurance economics. The program operates under the Division of Mathematical Statistics . For those with a bachelor's degree in mathematics statistics or with a master's degree in mathematics, a two years full-time master's degree Aktuarieprogrammet has been created since 2002, at Stockholm University, which has a long history of research on insurance mathematics.\n\nQualification in Turkey consists of a series of exams administered by an exam board made up of representatives of the Actuarial Society of Turkey, the government and universities. The exams are split into 3 levels: first level (essentials of insurance and economy, mathematics, statistics and probability, financial mathematics); second level (accounting and financial reporting, insurance mathematics (life and non-life), risk analysis, actuarial modeling); and third level (investment and risk management, non-life insurance, life insurance, health insurance, pension systems). After completing the first level exams, a candidate becomes an \"actuarial trainee\", after the second level an \"assistant actuary\", and after the third level and 3 years of related work experience the candidate becomes an \"actuary\".\n\nQualification in the United Kingdom and Ireland consists of a combination of exams and courses provided by the professional body, the Institute and Faculty of Actuaries. The exams may only be taken upon having officially joined the body, unlike many other countries where exams may be taken earlier. Most trainee actuaries study while working for an actuarial employer using resources provided by ActEd (The Actuarial Education Company, a subsidiary of BPP Actuarial Education Ltd.), which is contracted to provide actuarial tuition for students on behalf of Institute and Faculty Education Ltd (IFE), a subsidiary of the Institute and Faculty of Actuaries.\n\nHowever, a candidate may offer proof of having previously covered topics (at a high enough standard, usually while at university) to be exempt from taking certain subjects.\n\nThe exams themselves are split into four sections: Core Technical (CT), Core Applications (CA), Specialist Technical (ST), and Specialist Applications (SA). For students who joined the Profession after June 2004, a further requirement that the student carry out a \"Work-based skills\" exercise has been brought into effect. This involves the student submitting a series of essays to the Profession detailing the work that he or she has performed. In addition to exams, essays and courses, it is required that the candidate have at least three years' experience of actuarial work under supervision of a recognized actuary to qualify as a Fellow of the Institute of Actuaries (FIA) or of the Faculty of Actuaries (FFA) .\n\nActuaries can also gain partial credit towards Fellowship of the Institute and Faculty of Actuaries by following an actuarial science degree at an accredited university. At the undergraduate level the only locally accredited programmes are currently at University of Manchester, University College Dublin, Queen's University Belfast, Heriot-Watt University, University of Edinburgh, the London School of Economics, University of Southampton, City University, London, University of Leicester and the University of Kent. Full-time accredited masters programmes are provided only by the University of Kent, Heriot-Watt University, University of Leicester and City University; part-time accredited master's degrees are offered by Imperial College London and the University of Leicester. Actuarial programmes that offer the possibility of exemption from individual professional exams are also available at City University, London, Heriot-Watt University, the London School of Economics, the University of Southampton, Swansea University, the University of Kent and the University of Warwick. In Ireland exemptions are offered by National University of Ireland, Galway, Dublin City University, University College Cork. Some South African universities are also accredited by the Institute and Faculty of Actuaries. These universities include the University of Pretoria, University of Cape Town, Stellenbosch University, University of the Free State and the University of the Witwatersrand. ISEG in Lisbon, Portugal, offers the possibility of exemption from some professional exams of the Institute and Faculty of Actuaries.\n\nNote that the UK Profession is currently introducing the Certified Actuarial Analyst (CAA) qualification to \"provide those working in financial and actuarial roles at a technical level around the world with valuable skills and a well respected qualification\".\n\nIn the U.S., for life, health, and pension actuaries, exams are given by the Society of Actuaries, while for property-casualty actuaries the exams are administered by the Casualty Actuarial Society. The Society of Actuaries’ requirements for Associateship include passing five preliminary examinations, demonstrating educational experience in economics, corporate finance and applied statistics—called validation by educational experience (VEE), completing an eight-module self-learning series, and taking a course on professionalism . For Fellowship, three other modules, three or four exams depending on specialty track, and a special fellowship admission course is added . The Casualty Actuarial Society requires the successful completion of seven examinations, two modules, and economics and corporate finance VEE's for Associateship and three additional exams for Fellowship. In addition to these requirements, casualty actuarial candidates must also complete professionalism education and be recommended for membership by existing members . One may become a Chartered (or Certified) Enterprise Risk Analyst (CERA) through either the SOA or the CAS.\n\nTo sign certain statements of actuarial opinion, however, American actuaries must be members of the American Academy of Actuaries. Academy membership requirements include membership in one of the recognized actuarial societies, at least three years of full-time equivalent experience in responsible actuarial work, and either residency in the United States for at least three years or a non-resident or new resident who meets certain requirements . Continuing education is required after certification for all actuaries who sign statements of actuarial opinion .\n\nIn the pension area, American actuaries must pass three examinations to become an Enrolled Actuary. Some pension-related filings to the Internal Revenue Service and the Pension Benefit Guaranty Corporation require the signature of an Enrolled Actuary. Many Enrolled Actuaries belong to the Conference of Consulting Actuaries or the American Society of Pension Professionals and Actuaries.\n\nIn 2009, the Society of Actuaries began a high-level accreditation system for universities, recognizing the best actuarial schools as Centers of Actuarial Excellence. There are two sets of criteria that must be met: A Criteria and B Criteria. Additionally, a site visit must be performed by a team of CAE committee members who evaluate the University and conduct interviews with students and faculty. The designation is retained for five years and if a criteria is not met, then the University must provide a plan for how they will address the problem within a reasonable time frame.\n\nThere are five preliminary exams. Most of the exams are multiple choice and administered on computers at Prometric testing centers. Candidates are allowed to use a calculator from an approved list. The exams are timed and last between three and four hours. Some tests provide instant feedback as to whether or not a candidate has passed that particular exam (see table below). All test scores (on a 0-10 scale with 6 or higher passing) are posted six to eight weeks after the test. However, due to the way the test is scaled, the scores can range from 0-10, but there are also situations where the highest grade for a test is a 9 even if every single question was answered correctly.\n\nThrough the end of 2013, four of the preliminary exams (all but MLC) were jointly sponsored by CAS and SOA. In late 2012, SOA announced its intention to end joint sponsorship beginning with tests administered in January 2014. CAS has not announced plans to develop alternative forms of the jointly sponsored exams; however, it accepts SOA exams for CAS credit.\n\nSOA administers exam MLC, which covers life contingencies topics. Starting in May 2014, MLC includes both multiple choice and open-response questions. SOA made this change because, in their view, strict multiple-choice questions are not sufficient or adequate to test whether the candidates are familiar and fluent in the material. The test is four-hours long, allows calculators, and is administered via a paper-and-pencil format. Multiple choice questions account for 40% of the exam, and open-response questions account for 60% of the exam. Candidates may freely move between the two sections. The two sections are graded separately. However, since the multiple-choice questions are easier, only candidates who have answered a certain percentage of the multiple-choice questions correctly have their written answers graded.\n\nCAS develops exam S, as a full alternative to SOA's exam MLC. Exam S covers many topics within statistics, survival models, and stochastic processes. Between the beginning of 2014 and the end of 2015, CAS offered two interim exams: exam LC, covering many life contingencies topics, and exam ST, covering statistical and stochastic methods. Those candidates who passed 3L or MLC before 2014 are exempt from taking LC and ST. Additionally, those candidates passing exam LC, exam ST, and the Statistics VEE by August 2016 are exempt from taking exam S.\n\nCandidates for CAS and SOA membership must pass standardized tests in introductory economics and corporate finance. Candidates for SOA membership must pass an additional standardized test in applied statistics. Economics has two components: macroeconomics and microeconomics. Applied statistics has two components: regression and time series. Instead of passing exams, candidates may earn credit by passing an approved college class with a B- or better grade or by completing an approved correspondence class.\n\nTo earn associate membership (ACAS), a candidate must pass the preliminary exams, VEE, two online modules, exam five, and exam six. For an associate to become a fellow (FCAS), exams seven through nine must be passed. The exams are administered on paper-and-pencil. Exam questions generally require an open answer; however, multiple-choice questions are allowed, too. Exam six comes in two versions: one for candidates in the United States and one for candidates in Canada.\n\nThe two modules that must be completed to become an associate are...\n\nThe exam schedule above started in 2011. Candidates who passed exams offered before 2011 are granted credit according to the following schedule. To earn credit for new exam five, a candidate must pass old exam five and old exam six. Candidates who passed \"one\" of old exam five and old exam six must pass a special exam covering the remaining material on new exam five.\n\nAfter passing the preliminary exams, SOA candidates complete the Fundamentals of Actuarial Practice e-learning course and the Associateship Professionalism Course. FAP contains eight learning modules and two assessments. APC is a live, in-person seminar held in different places around the country. This completes the requirements for associate membership (ASA).\n\nAssociates select one of six areas of competence for further training. Each area has three or four exams and three learning modules. Exam one for \"Retirement Benefits\" has alternative requirements specific to Canada and the United States. For all tracks other than \"Corporate Finance and Enterprise Risk Management,\" a candidate may pass the \"Enterprise Risk Management\" exam as a substitute for exam three. After completing the exams and modules, candidates must pass the \"Decision Making and Communication Module\" and the \"Fellowship Admissions Course\" before earning promotion to fellow (FSA).\n\nChartered Enterprise Risk Analyst (CERA) is a global designation awarded by more than ten international actuarial bodies, including the CAS and SOA. Each body designs its own syllabus and requirements to award the designation, subject to approval by the international CERA body.\n\nCAS candidates must complete all of the requirements to become a FCAS except for exam eight. They must also complete exam ST9, Enterprise Risk Management Specialist Technical, administered by the Institute and Faculty of Actuaries (U.K.) and the Enterprise Risk Management and Modeling Seminar for CERA Qualification\n\nSOA candidates must complete all of the preliminary exams except for exam MLC. Candidates must also pass VEE Economics, VEE Corporate Finance, Fundamentals of Actuarial Practice, the Enterprise Risk Management exam, the Enterprise Risk Management module, and the Associateship Professionalism Course.\n\nIn the US the term \"Enrolled Actuary\" is applied to an individual who has taken certain exams sponsored by the Joint Board for the Enrollment of Actuaries relating to pension plans. Enrollment in the Joint Board is a requirement for SOA Retirement Fellows working in the US.\n\nThe American equivalent of the Canadian Institute of Actuaries is the American Academy of Actuaries (AAA).\n\nMany other countries pattern their requirements after the larger societies of the US or UK. In general, the websites of these organizations are often the easiest source for finding out about membership requirements and resources.\n\n\n\n\n\n\n"}
{"id": "35320528", "url": "https://en.wikipedia.org/wiki?curid=35320528", "title": "Annual growth rate", "text": "Annual growth rate\n\nAnnual growth rate (AGR) is the change in the value of a measurement over the period of a year.\n\nAnnual growth rate is a useful tool to identify trends in investments. According to a survey of nearly 200 senior marketing managers conducted by The Marketing Accountability Standards Board, 69% of subjects responded that they consider average annual growth rate to be a useful measurement. The formula used to calculate annual growth rate uses the previous year as a base. Over longer periods of time, compound annual growth rate (CAGR) is generally an acceptable metric for average growth rates.\n\nPerceptions of the success or failure of many enterprises and businesses are based on assessments of their growth. Measurements of year-on-year growth, however, are complicated by two simple factors:\n\n\n\"Percentage growth is the central plank of year-on-year analysis. Dividing the results for the current period by the results for the prior period will yield a comparative figure. Subtracting one from the other will highlight the increase or decrease between periods. When evaluating the comparatives, one might say that results in Year 2 were, for example, 110% of those in Year 1. To convert this figure to a growth rate, one need only subtract 100%. The periods considered are often years, but any time-frame can be chosen.\"\n\nThe first step of this process is to identify the value of the investment at the beginning and end of the year. The next step is to subtract the beginning value from the end value. Dividing the difference by the beginning value, and then multiplying the answer by 100 converts it to a percentage.\n\nWorks cited\n"}
{"id": "294358", "url": "https://en.wikipedia.org/wiki?curid=294358", "title": "Anticommutativity", "text": "Anticommutativity\n\nIn mathematics, anticommutativity is a specific property of some non-commutative operations. In mathematical physics, where symmetry is of central importance, these operations are mostly called antisymmetric operations, and are extended in an associative setting to cover more than two arguments. Swapping the position of two arguments of an antisymmetric operation yields a result, which is the \"inverse\" of the result with unswapped arguments. The notion \"inverse\" refers to a group structure on the operation's codomain, possibly with another operation, such as addition.\n\nA prominent example of an anticommutative operation is the Lie bracket.\n\nAn formula_1-ary operation is antisymmetric if swapping the order of any two arguments negates the result. For example, a binary operation \"∗\" is anti-commutative (with respect to addition) if for all \"x\" and \"y\", \n\nMore formally, a map formula_2 from the set of all \"n\"-tuples of elements in a set \"A\" (where \"n\" is a non-negative integer) to a group formula_3 is anticommutative with respect to the group operation \"+\" if and only if\n\nwhere formula_5 is the result of permuting formula_6 with the permutation formula_7 and formula_8 is the identity map for even permutations formula_9 and maps each element of \"A\" to its inverse for odd permutations formula_9. In an associative setting it is convenient to denote this with a binary operation \"∗\":\n\nThis equality expresses the following concept:\n\nParticularly important is the case . A binary operation formula_13 is anticommutative if and only if \n\nThis means that is the additive inverse of the element in formula_12.\n\nIn the most frequent cases in physics, where formula_16 carries already a field structure, the fact\n\nimplies that applying an anticommutative operation to any collection of operands yields zero, if any two operands are equal (provided the characteristic of the field is not formula_18). That is\n\nIf the group formula_20 is such that\n\ni.e. \"the only element equal to its inverse is the neutral element\", then for all the ordered tuples such that formula_22 for at least two different index formula_23\n\nIn the case \"formula_25\" this means\n\nExamples of anticommutative binary operations include:\n\nSee also: graded-commutative ring\n\n\n"}
{"id": "34529648", "url": "https://en.wikipedia.org/wiki?curid=34529648", "title": "Bloch's principle", "text": "Bloch's principle\n\nBloch's Principle is a philosophical principle in mathematics\nstated by André Bloch.\n\nBloch states the principle in Latin as: \"Nihil est in infinito quod non prius fuerit in finito,\" and explains this as follows: Every proposition in whose statement the actual infinity occurs can be always considered a consequence, almost immediate, of a proposition where it does not occur, a proposition in \"finite terms\".\n\nBloch mainly applied this principle to the theory of functions of a complex variable. Thus, for example, according to this principle, Picard's theorem corresponds to Schottky's theorem, and Valiron's theorem corresponds to Bloch's theorem.\n\nBased on his Principle, Bloch was able to predict or conjecture several\nimportant results such as the Ahlfors's Five Islands theorem,\nCartan's theorem on holomorphic curves omitting hyperplanes, Hayman's result that an exceptional set of radii is unavoidable in Nevanlinna theory.\n\nIn the more recent times several general theorems were proved which can be regarded as rigorous statements in the spirit of the Bloch Principle.\n\nLet formula_1 be a sequence of meromorphic functions in a region \"D\", which is not a normal family.\nThen there exist a sequence of points formula_2 in \"D\" and positive numbers formula_3 with formula_4 such that\n\nwhere \"f\" is a non-constant meromorphic function in the complex plane.\n\nLet \"X\" be a compact complex analytic manifold, such that every holomorphic map from the complex plane\nto \"X\" is constant. Then there exists a metric on \"X\" such that every holomorphic map from the unit disc with the Poincaré metric to \"X\" does not increase distances.\n"}
{"id": "39614877", "url": "https://en.wikipedia.org/wiki?curid=39614877", "title": "Chern Prize (ICCM)", "text": "Chern Prize (ICCM)\n\nThe Chern Prize in Mathematics was established in 2001 in honor of Professor Shiing-Shen Chern. The Chern Prize is presented every three years at the International Congress of Chinese Mathematicians to Chinese mathematicians and those of Chinese descent for \"exceptional contributions to mathematical research or to public service activities in support of mathematics\". Winners are selected by a committee of mathematicians to recognize the achievements of mathematicians of Chinese descent. In 2010, a special commemorative event was held in Beijing in addition to the normal award presentation to celebrate the centennial of Professor Chern's birth.\n\n"}
{"id": "14721784", "url": "https://en.wikipedia.org/wiki?curid=14721784", "title": "Coupon collector's problem", "text": "Coupon collector's problem\n\nIn probability theory, the coupon collector's problem describes the \"collect all coupons and win\" contests. It asks the following question: Each box of a brand of cereals contains a coupon, and there are \"n\" different types of coupons, what is the probability that more than \"t\" boxes need to be bought to collect all \"n\" coupons? An alternative statement is: Given \"n\" coupons, how many coupons do you expect you need to draw with replacement before having drawn each coupon at least once? The mathematical analysis of the problem reveals that the expected number of trials needed grows as formula_1. For example, when \"n\" = 50 it takes about 225 trials on average to collect all 50 coupons.\n\nLet \"T\" be the time to collect all \"n\" coupons, and let \"t\" be the time to collect the \"i\"-th coupon after \"i\" − 1 coupons have been collected. Think of \"T\" and \"t\" as random variables. Observe that the probability of collecting a coupon is \"p\" = (\"n\" − (\"i\" − 1))/\"n\". Therefore, \"t\" has geometric distribution with expectation 1/\"p\". By the linearity of expectations we have:\n\nHere \"H\" is the \"n\"-th harmonic number. Using the asymptotics of the harmonic numbers, we obtain:\n\nwhere formula_4 is the Euler–Mascheroni constant.\n\nNow one can use the Markov inequality to bound the desired probability:\n\nUsing the independence of random variables \"t\", we obtain:\n\nsince formula_7 (see Basel problem).\n\nNow one can use the Chebyshev inequality to bound the desired probability:\n\nA different upper bound can be derived from the following observation. Let formula_9 denote the event that the formula_10-th coupon was not picked in the first formula_11 trials. Then:\n\nThus, for formula_13, we have formula_14.\n\n\n\n\n\n\n\n"}
{"id": "9177825", "url": "https://en.wikipedia.org/wiki?curid=9177825", "title": "Dini's theorem", "text": "Dini's theorem\n\nIn the mathematical field of analysis, Dini's theorem says that if a monotone sequence of continuous functions converges pointwise on a compact space and if the limit function is also continuous, then the convergence is uniform.\nIf \"X\" is a compact topological space, and { \"f\" } is a monotonically increasing sequence (meaning for all \"n\" and \"x\") of continuous real-valued functions on \"X\" which converges pointwise to a continuous function \"f\", then the convergence is uniform. The same conclusion holds if { \"f\" } is monotonically decreasing instead of increasing. The theorem is named after Ulisse Dini.\n\nThis is one of the few situations in mathematics where pointwise convergence implies uniform convergence; the key is the greater control implied by the monotonicity. Note also that the limit function must be continuous, since a uniform limit of continuous functions is necessarily continuous.\nLet ε > 0 be given. For each \"n\", let \"g\" = \"f\" − \"f\", and let \"E\" be the set of those \"x\" ∈ \"X\" such that \"g\"( \"x\" ) < ε. Each \"g\" is continuous, and so each \"E\" is open (because each \"E\" is the preimage of an open set under \"g\", a nonnegative continuous function). Since { \"f\" } is monotonically increasing, { \"g\" } is monotonically decreasing, it follows that the sequence \"E\" is ascending. Since \"f\" converges pointwise to \"f\", it follows that the collection { \"E\" } is an open cover of \"X\". By compactness, there is a finite subcover, and since \"E\" are ascending the largest of these is a cover too. Thus we obtain that there is some positive integer \"N\" such that \"E\" = \"X\". That is, if \"n\" > \"N\" and \"x\" is a point in \"X\", then |\"f\"( \"x\" ) − \"f\"( \"x\" )| < ε, as desired.\n\n\n"}
{"id": "51619662", "url": "https://en.wikipedia.org/wiki?curid=51619662", "title": "Directed information", "text": "Directed information\n\nDirected information, formula_1, is a measure of information theory and it measures the amount of information that flows from the process formula_2 to formula_3, where formula_2 denotes the vector formula_5 and formula_3 denotes formula_7. The term \"directed information\" was coined by James Massey and is defined as\nwhere formula_9 is the conditional mutual information.\n\nNote that if formula_10, directed information becomes mutual information formula_11. Directed information has many applications in problems where causality plays an important role such as capacity of channel with feedback, capacity of discrete memoryless networks with feedback, gambling with causal side information, compression with causal side information,\nand in real-time control communication settings.\n"}
{"id": "8714937", "url": "https://en.wikipedia.org/wiki?curid=8714937", "title": "Discharging method (discrete mathematics)", "text": "Discharging method (discrete mathematics)\n\nThe discharging method is a technique used to prove lemmas in structural graph theory. Discharging is most well known for its central role in the proof of the Four Color Theorem. The discharging method is used to prove that every graph in a certain class contains some subgraph from a specified list. The presence of the desired subgraph is then often used to prove a coloring result.\n\nMost commonly, discharging is applied to planar graphs.\nInitially, a \"charge\" is assigned to each face and each vertex of the graph.\nThe charges are assigned so that they sum to a small positive number. During the \"Discharging Phase\" the charge at each face or vertex may be redistributed to nearby faces and vertices, as required by a set of discharging rules. However, each discharging rule maintains the sum of the charges. The rules are designed so that after the discharging phase each face or vertex with positive charge lies in one of the desired subgraphs. Since the sum of the charges is positive, some face or vertex must have a positive charge. Many discharging arguments use one of a few standard initial charge functions (these are listed below). Successful application of the discharging method requires creative design of discharging rules.\n\nIn 1904, Wernicke introduced the discharging method to prove the following theorem, which was part of an attempt to prove the four color theorem.\n\nTheorem: If a planar graph has minimum degree 5, then it either has an edge\nwith endpoints both of degree 5 or one with endpoints of degrees 5 and 6.\n\nProof:\nWe use formula_1, formula_2, and formula_3 to denote the sets of vertices, faces, and edges, respectively.\nWe call an edge \"light\" if its endpoints are both of degree 5 or are of degrees 5 and 6.\nEmbed the graph in the plane. To prove the theorem, it is sufficient to only consider planar triangulations (for the following reason). We arbitrarily add edges to the graph until it is a triangulation. \nSince the original graph had minimum degree 5, each endpoint of a new edge has degree at least 6. \nSo, none of the new edges are light.\nThus, if the triangulation contains a light edge, then that edge must have been in the original graph.\n\nWe give the charge formula_4 to each vertex formula_5 and the charge formula_6 to each face formula_7, where formula_8 denotes the degree of a vertex and the length of a face. (Since the graph is a triangulation, the charge on each face is 0.) Recall that the sum of all the degrees in the graph is equal to twice the number of edges; similarly, the sum of all the face lengths equals twice the number of edges. Using Euler's Formula, it's easy to see that the sum of all the charges is 12:\n\nformula_9\n\nWe use only a single discharging rule:\n\n\nWe consider which vertices could have positive final charge.\nThe only vertices with positive initial charge are vertices of degree 5. \nEach degree 5 vertex gives a charge of 1/5 to each neighbor. \nSo, each vertex is given a total charge of at most formula_10. \nThe initial charge of each vertex v is formula_4. \nSo, the final charge of each vertex is at most formula_12. Hence, a vertex can only have positive final charge if it has degree at most 7. Now we show that each vertex with positive final charge is adjacent to an endpoint of a light edge.\n\nIf a vertex formula_5 has degree 5 or 6 and has positive final charge, then v received charge from an adjacent degree 5 vertex formula_14, so edge formula_15 is light. If a vertex formula_5 has degree 7 and has positive final charge, then formula_5 received charge from at least 6 adjacent degree 5 vertices. Since the graph is a triangulation, the vertices adjacent to v must form a cycle, and since it has only degree 7, the degree 5 neighbors cannot be all separated by vertices of higher degree; at least two of the degree 5 neighbors of formula_5 must be adjacent to each other on this cycle. This yields the light edge.\n\n"}
{"id": "21029881", "url": "https://en.wikipedia.org/wiki?curid=21029881", "title": "Disorder problem", "text": "Disorder problem\n\nIn the study of stochastic processes in mathematics, a disorder problem or quickest detection problem (formulated by Kolmogorov) is the problem of using ongoing observations of a stochastic process to detect as soon as possible when the probabilistic properties of the process have changed. This is a type of change detection problem.\n\nAn example case is to detect the change in the drift parameter of a Wiener process.\n\n\n"}
{"id": "28052186", "url": "https://en.wikipedia.org/wiki?curid=28052186", "title": "Dwork family", "text": "Dwork family\n\nIn algebraic geometry, a Dwork family is a one-parameter family of hypersurfaces depending on an integer \"n\", studied by Bernard Dwork. Originally considered by Dwork in the context of local zeta-functions, such families have been shown to have relationships with mirror symmetry and extensions of the modularity theorem.\n\nThe Dwork family is given by the equations\n\nfor all formula_2.\n"}
{"id": "10273", "url": "https://en.wikipedia.org/wiki?curid=10273", "title": "Embryo drawing", "text": "Embryo drawing\n\nEmbryo drawing is the illustration of embryos in their developmental sequence. In plants and animals, an embryo develops from a zygote, the single cell that results when an egg and sperm fuse during fertilization. In animals, the zygote divides repeatedly to form a ball of cells, which then forms a set of tissue layers that migrate and fold to form an early embryo. Images of embryos provide a means of comparing embryos of different ages, and species. To this day, embryo drawings are made in undergraduate developmental biology lessons.\n\nComparing different embryonic stages of different animals is a tool that can be used to infer relationships between species, and thus biological evolution. This has been a source of quite some controversy, both now and in the past. Ernst Haeckel pioneered in this field. By comparing different embryonic stages of different vertebrate species, he formulated the recapitulation theory. This theory states that an animal's embryonic development follows exactly the same sequence as the sequence of its evolutionary ancestors. Haeckel's work and the ensuing controversy linked the fields of developmental biology and comparative anatomy into comparative embryology. From a more modern perspective, Haeckel's drawings were the beginnings of the field of evolutionary developmental biology (evo-devo).\n\nThe study of comparative embryology aims to prove or disprove that vertebrate embryos of different classes (e.g. mammals vs. fish) follow a similar developmental path due to their common ancestry. Such developing vertebrates have similar genes, which determine the basic body plan. However, further development allows for the distinguishing of distinct characteristics as adults.\n\nIn current biology, fundamental research in developmental biology and evolutionary developmental biology is no longer driven by morphological comparisons between embryos, but more by molecular biology. This is partly because Haeckel's drawings were very inaccurate.\n\nThe exactness of Ernst Haeckel's drawings of embryos has caused much controversy among Intelligent Design proponents recently and Haeckel's intellectual opponents in the past. Although the early embryos of different species exhibit similarities, Haeckel apparently exaggerated these similarities in support of his Recapitulation theory, sometimes known as the Biogenetic Law or \"Ontogeny recapitulates phylogeny\". Furthermore, Haeckel even proposed theoretical life-forms to accommodate certain stages in embryogenesis. A recent review concluded that the \"biogenetic law is supported by several recent studies - if applied to single characters only\".\n\nCritics in the late 19th and early 20th centuries, Karl von Baer and Wilhelm His, did not believe that living embryos reproduce the evolutionary process and produced embryo drawings of their own which emphasized the differences in early embryological development. Late 20th and early 21st century critic Stephen Jay Gould have objected to the continued use of Haeckel's embryo drawings in textbooks.\n\nOn the other hand, Michael K. Richardson, Professor of Evolutionary Developmental Zoology, Leiden University, while recognizing that some criticisms of the drawings are legitimate (indeed, it was he and his co-workers who began the modern criticisms in 1998), has supported the drawings as teaching aids, and has said that \"on a fundamental level, Haeckel was correct\"\n\nHaeckel's illustrations show vertebrate embryos at different stages of development, which exhibit embryonic resemblance as support for evolution, recapitulation as evidence of the Biogenetic Law, and phenotypic divergence as evidence of von Baer's laws. The series of twenty-four embryos from the early editions of Haeckel's \"Anthropogenie\" remain the most famous. The different species are arranged in columns, and the different stages in rows. Similarities can be seen along the first two rows; the appearance of specialized characters in each species can be seen in the columns and a diagonal interpretation leads one to Haeckel's idea of recapitulation.\n\nHaeckel's embryo drawings are primarily intended to express his theory of embryonic development, the Biogenetic Law, which in turn assumes (but is not crucial to) the evolutionary concept of common descent. His postulation of embryonic development coincides with his understanding of evolution as a developmental process. In and around 1800, embryology fused with comparative anatomy as the primary foundation of morphology. Ernst Haeckel, along with Karl von Baer and Wilhelm His, are primarily influential in forming the preliminary foundations of ‘phylogenetic embryology’ based on principles of evolution. Haeckel's ‘Biogenetic Law’ portrays the parallel relationship between an embryo's development and phylogenetic history. The term, ‘recapitulation,’ has come to embody Haeckel's Biogenetic Law, for embryonic development is a recapitulation of evolution. Haeckel proposes that all classes of vertebrates pass through an evolutionarily conserved “phylotypic” stage of development, a period of reduced phenotypic diversity among higher embryos. Only in later development do particular differences appear. Haeckel portrays a concrete demonstration of his Biogenetic Law through his \"Gastrea\" theory, in which he argues that the early cup-shaped gastrula stage of development is a universal feature of multi-celled animals. An ancestral form existed, known as the gastrea, which was a common ancestor to the corresponding gastrula.\n\nHaeckel argues that certain features in embryonic development are conserved and palingenetic, while others are caenogenetic. Caenogenesis represents “the blurring of ancestral resemblances in development,” which are said to be the result of certain adaptations to embryonic life due to environmental changes. In his drawings, Haeckel cites the notochord, pharyngeal arches and clefts, pronephros and neural tube as palingenetic features. However, the yolk sac, extra-embryonic membranes, egg membranes and endocardial tube are considered caenogenetic features. The addition of terminal adult stages and the telescoping, or driving back, of such stages to descendant's embryonic stages are likewise representative of Haeckelian embryonic development. In addressing his embryo drawings to a general audience, Haeckel does not cite any sources, which gives his opponents the freedom to make assumptions regarding the originality of his work.\n\nHaeckel was not the only one to create a series of drawings representing embryonic development. Karl E. von Baer and Haeckel both struggled to model one of the most complex problems facing embryologists at the time: the arrangement of general and special characters during development in different species of animals. In relation to developmental timing, von Baer's scheme of development differs from Haeckel's scheme. Von Baer's scheme of development need not be tied to developmental stages defined by particular characters, where recapitulation involves heterochrony. Heterochrony represents a gradual alteration in the original phylogenetic sequence due to embryonic adaptation.\nAs well, von Baer early noted that embryos of different species could not be easily distinguished from one another as in adults.\n\nVon Baer's laws governing embryonic development are specific rejections of recapitulation. As a response to Haeckel's theory of recapitulation, von Baer enunciates his most notorious laws of development. Von Baer's laws state that general features of animals appear earlier in the embryo than special features, where less general features stem from the most general, each embryo of a species departs more and more from a predetermined passage through the stages of other animals, and there is never a complete morphological similarity between an embryo and a lower adult. Von Baer's embryo drawings display that individual development proceeds from general features of the developing embryo in early stages through differentiation into special features specific to the species, establishing that linear evolution could not occur. Embryological development, in von Baer's mind, is a process of differentiation, \"a movement from the more homogeneous and universal to the more heterogeneous and individual.\"\n\nVon Baer argues that embryos will resemble each other before attaining characteristics differentiating them as part of a specific family, genus or species, but embryos are not the same as the final forms of lower organisms.\n\nWilhelm His was one of Haeckel's most authoritative and primary opponents advocating physiological embryology. His \"Anatomie menschlicher Embryonen\" (Anatomy of human embryos) employs a series of his most important drawings chronicling developing embryos from the end of the second week through the end of the second month of pregnancy. His, in opposition to Haeckel, seeks to take human embryos out of the hands of Darwinist proponents. In 1878, His begins to engage in serious study of the anatomy of human embryos for his drawings. During the 19th century, embryologists often obtained early human embryos from abortions and miscarriages, postmortems of pregnant women and collections in anatomical museums. In order to construct his series of drawings, His collected specimens which he manipulated into a form that he could operate with.\n\nIn His’ \"Normentafel\", he displays specific individual embryos rather than ideal types. His does not produce norms from aborted specimens, but rather visualizes the embryos in order to make them comparable and specifically subjects his embryo specimens to criticism and comparison with other cases. Ultimately, His’ critical work in embryonic development comes with his production of a series of embryo drawings of increasing length and degree of development. His’ depiction of embryological development strongly differs from Haeckel's depiction, for His argues that the phylogenetic explanation of ontogenetic events is unnecessary. His argues that all ontogenetic events are the “mechanical” result of differential cell growth. His’ embryology is not explained in terms of ancestral history.\n\nThe debate between Haeckel and His ultimately becomes fueled by the description of an embryo that Wilhelm Krause propels directly into the ongoing feud between Haeckel and His. Haeckel speculates that the allantois is formed in a similar way in both humans and other mammals. His, on the other hand, accuses Haeckel of altering and playing with the facts. Although Haeckel is proven right about the allantois, the utilization of Krause's embryo as justification turns out to be problematic, for the embryo is that of a bird rather than a human. The underlying debate between Haeckel and His derives from differing viewpoints regarding the similarity or dissimilarity of vertebrate embryos. In response to Haeckel's evolutionary claim that all vertebrates are essentially identical in the first month of embryonic life as proof of common descent, His responds by insisting that a more skilled observer would recognize even sooner that early embryos can be distinguished. His also counteracts Haeckel's sequence of drawings in the \"Anthropogenie\" with what he refers to as “exact” drawings, highlighting specific differences. Ultimately, His goes so far as to accuse Haeckel of “faking” his embryo illustrations to make the vertebrate embryos appear more similar than in reality. His also accuses Haeckel of creating early human embryos that he conjured in his imagination rather than obtained through empirical observation. His completes his denunciation of Haeckel by pronouncing that Haeckel had “‘relinquished the right to count as an equal in the company of serious researchers.’”\n\nHaeckel encountered numerous oppositions to his artistic depictions of embryonic development during the late nineteenth and early twentieth centuries. Haeckel’s opponents believe that he de-emphasizes the differences between early embryonic stages in order to make the similarities between embryos of different species more pronounced.\n\nThe first suggestion of fakery against Haeckel was made in late 1868 by Ludwig Rutimeyer in the \"Archiv für Anthropogenie\". Rutimeyer was a professor of zoology and comparative anatomy at the University of Basel, who rejected natural selection as simply mechanistic and proposed an anti-materialist view of nature. Rutimeyer claimed that Haeckel “had taken to kinds of liberty with established truth.” Rutimeyer claimed that Haeckel presented the same image three consecutive times as the embryo of the dog, the chicken, and the turtle.\n\nTheodor Bischoff (1807–1882), was a strong opponent of Darwinism. As a pioneer in mammalian embryology, he was one of Haeckel's strongest critics. Although Bischoff's 1840 surveys depict how similar the early embryos of man are to other vertebrates, he later demanded that such hasty generalization was inconsistent with his recent findings regarding the dissimilarity between hamster embryos and those of rabbits and dogs. Nevertheless, Bischoff's main argument was in reference to Haeckel's drawings of human embryos, for Haeckel is later accused of miscopying the dog embryo from him. Throughout Haeckel's time, criticism of his embryo drawings was often due in part to his critics' belief in his representations of embryological development as “crude schemata.”\n\nMichael Richardson and his colleagues in a July 1997 issue of \"Anatomy and Embryology\", demonstrated that Haeckel falsified his drawings in order to exaggerate the similarity of the phylotypic stage.\nIn a March 2000 issue of \"Natural History\", Stephen Jay Gould argued that Haeckel \"exaggerated the similarities by idealizations and omissions.\" As well, Gould argued that Haeckel's drawings are simply inaccurate and falsified. On the other hand, one of those who criticized Haeckel's drawings, Michael Richardson, has argued that \"Haeckel's much-criticized drawings are important as phylogenetic hypotheses, teaching aids, and evidence for evolution\".\nBut even Richardson admitted in \"Science\" Magazine in 1997 that his team's investigation of Haeckel's drawings were showing them to be \"one of the most famous fakes in biology.\"\n\nSome version of Haeckel's drawings can be found in many modern biology textbooks in discussions of the history of embryology, with clarification that these are no longer considered valid .\n\nAlthough Charles Darwin accepted Haeckel's support for natural selection, he was tentative in using Haeckel's ideas in his writings; with regard to embryology, Darwin relied far more on von Baer's work. Haeckel's work was published in 1866 and 1874, years after Darwin's \"The Origin of Species\" (1859).\n\nDespite the numerous oppositions, Haeckel has influenced many disciplines in science in his drive to integrate such disciplines of taxonomy and embryology into the Darwinian framework and to investigate phylogenetic reconstruction through his Biogenetic Law. As well, Haeckel served as a mentor to many important scientists, including Anton Dohrn, Richard and Oscar Hertwig, Wilhelm Roux, and Hans Driesch.\n\nOne of Haeckel's earliest proponents was Carl Gegenbaur at the University of Jena (1865–1873), during which both men were absorbing the impact of Darwin's theory. The two quickly sought to integrate their knowledge into an evolutionary program. In determining the relationships between \"phylogenetic linkages\" and \"evolutionary laws of form,\" both Gegenbaur and Haeckel relied on a method of comparison. As Gegenbaur argued, the task of comparative anatomy lies in explaining the form and organization of the animal body in order to provide evidence for the continuity and evolution of a series of organs in the body. Haeckel then provided a means of pursuing this aim with his biogenetic law, in which he proposed to compare an individual's various stages of development with its ancestral line. Although Haeckel stressed comparative embryology and Gegenbaur promoted the comparison of adult structures, both believed that the two methods could work in conjunction to produce the goal of evolutionary morphology.\n\nThe philologist and anthropologist, Friedrich Müller, used Haeckel's concepts as a source for his ethnological research, involving the systematic comparison of the folklore, beliefs and practices of different societies. Müller's work relies specifically on theoretical assumptions that are very similar to Haeckel's and reflects the German practice to maintain strong connections between empirical research and the philosophical framework of science. Language is particularly important, for it establishes a bridge between natural science and philosophy. For Haeckel, language specifically represented the concept that all phenomena of human development relate to the laws of biology. Although Müller did not specifically have an influence in advocating Haeckel's embryo drawings, both shared a common understanding of development from lower to higher forms, for Müller specifically saw humans as the last link in an endless chain of evolutionary development.\n\nModern acceptance of Haeckel's Biogenetic Law, despite current rejection of Haeckelian views, finds support in the certain degree of parallelism between ontogeny and phylogeny. A. M. Khazen, on the one hand, states that \"ontogeny is obliged to repeat the main stages of phylogeny.\" A. S. Rautian, on the other hand, argues that the reproduction of ancestral patterns of development is a key aspect of certain biological systems. Dr. Rolf Siewing acknowledges the similarity of embryos in different species, along with the laws of von Baer, but does not believe that one should compare embryos with adult stages of development. According to M. S. Fischer, reconsideration of the Biogenetic Law is possible as a result of two fundamental innovations in biology since Haeckel's time: cladistics and developmental genetics.\n\nIn defense of Haeckel's embryo drawings, the principal argument is that of \"schematisation.\" Haeckel's drawings were not intended to be technical and scientific depictions, but rather schematic drawings and reconstructions for a specifically lay audience. Therefore, as R. Gursch argues, Haeckel's embryo drawings should be regarded as \"reconstructions.\" Although his drawings are open to criticism, his drawings should not be considered falsifications of any sort. Although modern defense of Haeckel's embryo drawings still considers the inaccuracy of his drawings, charges of fraud are considered unreasonable. As Erland Nordenskiöld argues, charges of fraud against Haeckel are unnecessary. R. Bender ultimately goes so far as to reject His's claims regarding the fabrication of certain stages of development in Haeckel's drawings, arguing that Haeckel's embryo drawings are faithful representations of real stages of embryonic development in comparison to published embryos.\n\nHaeckel's embryo drawings, as comparative plates, were at first only copied into biology textbooks, rather than texts on the study of embryology. Even though Haeckel's program in comparative embryology virtually collapsed after the First World War, his embryo drawings have often been reproduced and redrawn with increased precision and accuracy in works that have kept the study of comparative embryology alive. Nevertheless, neither His-inspired human embryology nor developmental biology are concerned with the comparison of vertebrate embryos. Although Stephen Jay Gould's 1977 book \"Ontogeny and Phylogeny\" helps to reassess Haeckelian embryology, it does not address the controversy over Haeckel's embryo drawings. Nevertheless, new interest in evolution in and around 1977 inspired developmental biologists to look more closely at Haeckel's illustrations.\n\n\n"}
{"id": "21278889", "url": "https://en.wikipedia.org/wiki?curid=21278889", "title": "End extension", "text": "End extension\n\nIn model theory and set theory, which are disciplines within mathematics, a model formula_1 of some axiom system of set theory formula_2 in the language of set theory is an end extension of formula_3, in symbols formula_4, if \n\nThe following is an equivalent definition of end extension: formula_5 is a substructure of formula_6, and formula_14 for all formula_8.\n\nFor example, formula_16 is an end extension of formula_17 if formula_18 and formula_19 are transitive sets, and formula_20.\n"}
{"id": "50167582", "url": "https://en.wikipedia.org/wiki?curid=50167582", "title": "Equation xʸ=yˣ", "text": "Equation xʸ=yˣ\n\nIn general, exponentiation fails to be commutative. However, the equation formula_1 holds in special cases, such as formula_2\n\nThe equation formula_1 is mentioned in a letter of Bernoulli to Goldbach (29 June 1728). The letter contains a statement that when formula_4 the only solutions in natural numbers are formula_5 and formula_6 although there are infinitely many solutions in rational numbers.\nThe reply by Goldbach (31 January 1729) contains general solution of the equation obtained by substituting formula_7 A similar solution was found by Euler.\n\nJ. van Hengel pointed out that if formula_8 are positive integers with formula_9 then formula_10 therefore it is enough to consider possibilities formula_11 and formula_12 in order to find solutions in natural numbers.\n\nThe problem was discussed in a number of publications. In 1960, the equation was among the questions on the William Lowell Putnam Competition which prompted Alvin Hausner to extend results to algebraic number fields.\n\nAn infinite set of trivial solutions in positive real numbers is given by formula_13\n\nNontrivial solutions can be found by assuming formula_14 and letting formula_15\nThen\nRaising both sides to the power formula_17 and dividing by formula_18\nThen nontrivial solutions in positive real numbers are expressed as\n\nSetting formula_22 or formula_23 generates the nontrivial solution in positive integers, formula_24\n\nOther pairs consisting of algebraic numbers exist, such as formula_25 and formula_26, as well as formula_27 and formula_28.\n\nThe trivial and non-trivial solutions intersect when formula_29. The equations above cannot be evaluated directly, but we can take the limit as formula_30. This is most conveniently done by substituting formula_31 and letting formula_32, so\nThus, the line formula_34 and the curve for formula_35 intersect at .\n\nThe equation formula_36 produces a graph where the line and curve intersect at formula_37. The curve also terminates at (0,1) and (1,0), instead of continuing on for infinity.\n\nThe equation formula_38 produces a graph where the curve and line intersect at (1,1). The curve (which is actually the positive section of \"y\"=1/\"x\") becomes asymptotic to 0, as opposed to 1.\n\n"}
{"id": "44308703", "url": "https://en.wikipedia.org/wiki?curid=44308703", "title": "Flajolet–Martin algorithm", "text": "Flajolet–Martin algorithm\n\nThe Flajolet–Martin algorithm is an algorithm for approximating the number of distinct elements in a stream with a single pass and space-consumption logarithmic in the maximal number of possible distinct elements in the stream (the count-distinct problem). The algorithm was introduced by Philippe Flajolet and G. Nigel Martin in their 1984 article \"Probabilistic Counting Algorithms for Data Base Applications\". Later it has been refined in \"LogLog counting of large cardinalities\" by Marianne Durand and Philippe Flajolet, and \"HyperLogLog: The analysis of a near-optimal cardinality estimation algorithm\" by Philippe Flajolet et al.\n\nIn their 2010 article \"An optimal algorithm for the distinct elements problem\", Daniel M. Kane, Jelani Nelson and David P. Woodruff give an improved algorithm, which uses nearly optimal space and has optimal \"O\"(1) update and reporting times.\n\nAssume that we are given a hash function formula_1 that maps input formula_2 to integers in the range formula_3, and where the outputs are sufficiently uniformly distributed. Note that the set of integers from 0 to formula_4 corresponds to the set of binary strings of length formula_5. For any non-negative integer formula_6, define formula_7 to be the formula_8-th bit in the binary representation of formula_6, such that:\n\nWe then define a function formula_11 that outputs the position of the least-significant set bit in the binary representation of formula_6:\n\nwhere formula_14. Note that with the above definition we are using 0-indexing for the positions. For example, formula_15, since the least significant bit is a 1 (0th position), and formula_16, since the least significant bit is at the 3rd position. At this point, note that under the assumption that the output of our hash function is uniformly distributed, then the probability of observing a hash output ending with formula_17 (a one, followed by formula_8 zeroes) is formula_19, since this corresponds to flipping formula_8 heads and then a tail with a fair coin.\n\nNow the Flajolet–Martin algorithm for estimating the cardinality of a multiset formula_21 is as follows:\n\nThe idea is that if formula_33 is the number of distinct elements in the multiset formula_21, then formula_35 is accessed approximately formula_36 times, formula_37 is accessed approximately formula_38 times and so on. Consequently, if formula_39, then formula_40 is almost certainly 0, and if formula_41, then formula_40 is almost certainly 1. If formula_43, then formula_40 can be expected to be either 1 or 0.\n\nThe correction factor formula_32 is found by calculations, which can be found in the original article.\n\nA problem with the Flajolet–Martin algorithm in the above form is that the results vary significantly. A common solution has been to run the algorithm multiple times with formula_8 different hash functions and combine the results from the different runs. One idea is to take the mean of the formula_8 results together from each hash function, obtaining a single estimate of the cardinality. The problem with this is that averaging is very susceptible to outliers (which are likely here). A different idea is to use the median, which is less prone to be influences by outliers. The problem with this is that the results can only take form formula_31, where formula_27 is integer. A common solution is to combine both the mean and the median: Create formula_50 hash functions and split them into formula_8 distinct groups (each of size formula_52). Within each group use the median for aggregating together the formula_52 results, and finally take the mean of the formula_8 group estimates as the final estimate.\n\nThe 2007 HyperLogLog algorithm splits the multiset into subsets and estimates their cardinalities, then it uses the harmonic mean to combine them into an estimate for the original cardinality.\n\n"}
{"id": "40081829", "url": "https://en.wikipedia.org/wiki?curid=40081829", "title": "Forensic statistics", "text": "Forensic statistics\n\nForensic statistics is the application of probability models and statistical techniques to scientific evidence, such as DNA evidence, and the law. In contrast to \"everyday\" statistics, to not engender bias or unduly draw conclusions, forensic statisticians report likelihoods as likelihood ratios (LR). This ratio of probabilities is then used by juries or judges to draw inferences or conclusions and decide legal matters. Jurors and judges rely on the strength of a DNA match, given by statistics, to make conclusions and determine guilt or innocence in legal matters.\n\nIn forensic science, the DNA evidence received for DNA profiling often contains a mixture of more than one person’s DNA. DNA profiles are generated using a set procedure, however, the interpretation of a DNA profile becomes more complicated when the sample contains a mixture of DNA. Regardless of the number of contributors to the forensic sample, statistics and probabilities must be used to provide weight to the evidence and to describe what the results of the DNA evidence mean. In a single-source DNA profile, the statistic used is termed a random match probability (RMP). RMPs can also be used in certain situations to describe the results of the interpretation of a DNA mixture. Other statistical tools to describe DNA mixture profiles include likelihood ratios (LR) and combined probability of inclusion (CPI), also known as random man not excluded (RMNE).\n\nComputer programs have been implemented with forensic DNA statistics for assessing the biological relationships between two or more people. Forensic science uses several approaches for DNA statistics with computer programs such as; match probability, exclusion probability, likelihood ratios, Bayesian approaches, and paternity and kinship testing.\n\nAlthough the precise origin of this term remains unclear, it is apparent that the term was used in the 1980s and 1990s. Among the first forensic statistics conferences were two held in 1991 and 1993.\n\nRandom match probabilities (RMP) are used to estimate and express the rarity of a DNA profile. RMP can be defined as the probability that someone else in the population, chosen at random, would have the same genotype as the genotype of the contributor of the forensic evidence. RMP is calculated using the genotype frequencies at all the loci, or how common or rare the alleles of a genotype are. The genotype frequencies are multiplied across all loci, using the product rule, to calculate the RMP. This statistic gives weight to the evidence either for or against a particular suspect being a contributor to the DNA mixture sample.\n\nRMP can only be used as a statistic to describe the DNA profile if it is from a single source or if the analyst is able to differentiate between the peaks on the electropherogram from the major and minor contributors of a mixture. Since the interpretation of DNA mixtures with more than two contributors is very difficult for analysts to do without computer software, RMP becomes difficult to calculate with a mixture of more than two people. If the major and minor contributor peaks can not be differentiated, there are other statistical methods that may be used. \n\nIf the DNA mixture contains a ratio of 4:1 of major to minor contributors, a modified random match probability (mRMP) may be able to be used as a statistical tool. For calculation of mRMP, the analyst must first deduce a major and minor contributor and their genotypes based on the peak heights given in the electropherogram. Computer software is often used in labs conducting DNA analysis in order to more accurately calculate the mRMP, since calculations for each of the most probable genotypes at each locus become tedious and inefficient for the analyst to do by hand.\n\nSometimes it can be very difficult to determine the number of contributors in a DNA mixture. If the peaks are easily distinguished and the number of contributors is able to be determined, a likelihood ratio (LR) is used. LRs consider probabilities of events happening and rely on alternative pairs of hypotheses against which the evidence is assessed. These alternative pairs of hypotheses in forensic cases are the prosecutor’s hypothesis and the defense hypothesis. In forensic biology cases, the hypotheses often state that the DNA came from a particular person or the DNA came from an unknown person. For example, the prosecution may hypothesize the DNA sample contains DNA from the victim and the suspect, while the defense may hypothesize that the sample contains DNA from the victim and an unknown person. The probabilities of the hypotheses are expressed as a ratio, with the prosecutor’s hypothesis being in the numerator. The ratio then expresses the likelihood of both of the events in relation to each other. For the hypotheses where the mixture contains the suspect, the probability is 1, because one can distinguish the peaks and easily tell if the suspect can be excluded as a contributor at each locus based on his/her genotype. The probability of 1 assumes the suspect can not be excluded as a contributor. To determine the probabilities of the unknowns, all genotype possibilities must be determined for that locus. \n\nOnce the calculation of the likelihood ratio is made, the number calculated is turned into a statement to provide meaning to the statistic. For the previous example, if the LR calculated is x, then the LR means that the probability of the evidence is x times more likely if the sample contains the victim and the suspect than if it contains the victim and an unknown person. Likelihood ratio can also be defined as 1/RMP. \n\nCombined probability of inclusion (CPI) is a common statistic used when the analyst can not differentiate between the peaks from a major and minor contributor to a sample and the number of contributors can not be determined. CPI is also commonly known as random man not excluded (RMNE). This statistical calculation is done by adding all the frequencies of observed alleles and then squaring the value, which yields the value for probability of inclusion (PI). These values are then multiplied across all loci, resulting in the value for CPI. The value is squared so that all the possible combinations of genotypes are included in the calculation. \n\nOnce the calculation is done, a statement is made about the meaning of this calculation and what it means. For example, if the CPI calculated is 0.5, this means that the probability of someone chosen at random in the population not being excluded as a contributor to the DNA mixture is 0.5. \n\nCPI relates to the evidence (the DNA mixture) and it is not dependent on the profile of any suspect. Therefore, CPI is a statistical tool that can be used to provide weight or strength to evidence when no other information about the crime is known. This is advantageous in situations where the genotypes in the DNA mixture can not be distinguished from one another. However, this statistic is not very discriminating and is not as powerful of a tool as likelihood ratios and random match probabilities can be when some information about the DNA mixture, such as the number of contributors or the genotypes of each contributor, can be distinguished. Another limitation to CPI is that it is not usable as a tool for the interpretation of a DNA mixture.\n\nBlood stains are an important part of forensic statistics, as the analysis of blood drop collisions may help to picture the event that had previously gone on. Commonly blood stains are an elliptical shape, because of this blood stains are usually easy to determine the blood droplets angle through the formula “\"α = arcsin d/a\"”. In this formula 'a' and 'd' are simply estimations of the axis of the ellipse. From these calculations, a visualization of the event causing the stains is able to be drawn, and alongside further information such as the velocity of the entity that caused such stains.\"\n\n"}
{"id": "692369", "url": "https://en.wikipedia.org/wiki?curid=692369", "title": "Fractional coloring", "text": "Fractional coloring\n\nFractional coloring is a topic in a young branch of graph theory known as fractional graph theory. It is a generalization of ordinary graph coloring. In a traditional graph coloring, each vertex in a graph is assigned some color, and adjacent vertices — those connected by edges — must be assigned different colors. In a fractional coloring however, a \"set\" of colors is assigned to each vertex of a graph. The requirement about adjacent vertices still holds, so if two vertices are joined by an edge, they must have no colors in common.\n\nFractional graph coloring can be viewed as the linear programming relaxation of traditional graph coloring. Indeed, fractional coloring problems are much more amenable to a linear programming approach than traditional coloring problems.\n\nA \"b\"-fold coloring of a graph \"G\" is an assignment of sets of size \"b\" to vertices of a graph such that adjacent vertices receive disjoint sets.\nAn \"a\":\"b\"-coloring is a \"b\"-fold coloring out of \"a\" available colors.\nEquivalently, it can be defined as a homomorphism to the Kneser graph .\nThe \"b\"-fold chromatic number χ(\"G\") is the least \"a\" such that an \"a\":\"b\"-coloring exists.\n\nThe fractional chromatic number χ(\"G\") is defined to be\n\nNote that the limit exists because χ(\"G\") is \"subadditive\", meaning χ(\"G\") ≤ χ(\"G\") + χ(\"G\").\n\nThe fractional chromatic number can equivalently be defined in probabilistic terms. χ(\"G\") is the smallest \"k\" for which there exists a probability distribution over the independent sets of \"G\" such that for each vertex \"v\", given an independent set \"S\" drawn from the distribution, \n\nSome properties of χ(\"G\"):\n\nand\nFurthermore, the fractional chromatic number approximates the chromatic number within a logarithmic factor, in fact:\n\nHere n(\"G\") is the order of \"G\", α(\"G\") is the independence number, ω(\"G\") is the clique number, and χ(\"G\") is the chromatic number.\nKneser graphs give examples where χ(\"G\")/χ(\"G\") is arbitrarily large, since χ()=\"m\"-2\"n\"+2, while χ()=\"m\"/\"n\".\n\nThe fractional chromatic number χ(\"G\") of a graph \"G\" can be obtained as a solution to a linear program. Let formula_6(\"G\") be the set of all independent sets of \"G\", and let formula_6(\"G\",\"x\") be the set of all those independent sets which include vertex \"x\". For each independent set \"I\", define a nonnegative real variable \"x\". Then χ(\"G\") is the minimum value of\n\nThe dual of this linear program computes the \"fractional clique number\", a relaxation to the rationals of the integer concept of clique number. That is, a weighting of the vertices of \"G\" such that the total weight assigned to any independent set is at most \"1\". The strong duality theorem of linear programming guarantees that the optimal solutions to both linear programs have the same value. Note however that each linear program may have size that is exponential in the number of vertices of \"G\", and that computing the fractional chromatic number of a graph is NP-hard. This stands in contrast to the problem of fractionally coloring the edges of a graph, which can be solved in polynomial time. This is a straightforward consequence of Edmonds' matching polytope theorem.\n\nApplications of fractional graph coloring include \"activity scheduling\". In this case, the graph \"G\" is a \"conflict graph\": an edge in \"G\" between the nodes \"u\" and \"v\" denotes that \"u\" and \"v\" cannot be active simultaneously. Put otherwise, the set of nodes that are active simultaneously must be an independent set in graph \"G\".\n\nAn optimal fractional graph coloring in \"G\" then provides a shortest possible schedule, such that each node is active for (at least) 1 time unit in total, and at any point in time the set of active nodes is an independent set. If we have a solution \"x\" to the above linear program, we simply traverse all independent sets \"I\" in an arbitrary order. For each \"I\", we let the nodes in \"I\" be active for formula_11 time units; meanwhile, each node not in \"I\" is inactive.\n\nIn more concrete terms, each node of \"G\" might represent a \"radio transmission\" in a wireless communication network; the edges of \"G\" represent \"interference\" between radio transmissions. Each radio transmission needs to be active for 1 time unit in total; an optimal fractional graph coloring provides a minimum-length schedule (or, equivalently, a maximum-bandwidth schedule) that is conflict-free.\n\nIf one further required that each node must be active \"continuously\" for 1 time unit (without switching it off and on every now and then), then traditional graph vertex coloring would provide an optimal schedule: first the nodes of color 1 are active for 1 time unit, then the nodes of color 2 are active for 1 time unit, and so on. Again, at any point in time, the set of active nodes is an independent set.\n\nIn general, fractional graph coloring provides a shorter schedule than non-fractional graph coloring; there is an integrality gap. It may be possible to find a shorter schedule, at the cost of switching devices (such as radio transmitters) on and off more than once.\n\n"}
{"id": "648062", "url": "https://en.wikipedia.org/wiki?curid=648062", "title": "G2 manifold", "text": "G2 manifold\n\nIn differential geometry, a \"G\" manifold is a seven-dimensional Riemannian manifold with holonomy group contained in \"G\". The group formula_1 is one of the five exceptional simple Lie groups. It can be described as the automorphism group of the octonions, or equivalently, as a proper subgroup of special orthogonal group SO(7) that preserves a spinor in the eight-dimensional spinor representation or lastly as the subgroup of the general linear group GL(7) which preserves the non-degenerate 3-form formula_2, the associative form. The Hodge dual, formula_3 is then a parallel 4-form, the coassociative form. These forms are calibrations in the sense of Reese Harvey and H. Blaine Lawson, and thus define special classes of 3- and 4-dimensional submanifolds.\n\nIf \"M\" is a formula_1-manifold, then \"M\" is:\n\nThe fact that formula_1 might possibly be the holonomy group of certain Riemannian 7-manifolds was first suggested by the 1955 classification theorem of Marcel Berger, and this remained consistent with the simplified proof later given by Jim Simons in 1962. Although not a single example of such a manifold had yet been discovered, Edmond Bonan then made an interesting contribution by showing that, \nif such a manifold did in fact exist, it would carry both a parallel 3-form and a parallel 4-form, and that it would necessarily be Ricci-flat.\nThe first local examples of 7-manifolds with holonomy formula_1 were finally constructed around 1984 by\nRobert Bryant, and his full proof of their existence appeared in the Annals in 1987 \nNext, complete (but still noncompact) 7-manifolds with holonomy formula_1 were constructed by Bryant and Simon Salamon in 1989. The first compact 7-manifolds with holonomy formula_1 were constructed by Dominic Joyce in 1994, and compact formula_1 manifolds are sometimes known as \"Joyce manifolds\", especially in the physics literature. In 2013, it was shown by M. Firat Arikan, Hyunjoo Cho, and Sema Salur that any manifold with a spin structure, and, hence, a formula_1-structure, admits a compatible almost contact metric structure, and an explicit compatible almost contact structure was constructed for manifolds with formula_1-structure. In the same paper, it was shown that certain classes of formula_1-manifolds admit a contact structure.\n\nThese manifolds are important in string theory. They break the original supersymmetry to 1/8 of the original amount. For example, M-theory compactified on a formula_1 manifold leads to a realistic four-dimensional (11-7=4) theory with N=1 supersymmetry. The resulting low energy effective supergravity contains a single supergravity supermultiplet, a number of chiral supermultiplets equal to the third Betti number of the formula_1 manifold and a number of U(1) vector supermultiplets equal to the second Betti number.\n\n\n"}
{"id": "23835696", "url": "https://en.wikipedia.org/wiki?curid=23835696", "title": "Gordan's lemma", "text": "Gordan's lemma\n\nIn convex geometry, Gordan's lemma states that the semigroup of integral points in the dual cone of a rational convex polyhedral cone is finitely generated. In algebraic geometry, the prime spectrum of the semigroup algebra of such a semigroup is, by definition, an affine toric variety; thus, the lemma says an affine toric variety is indeed an algebraic variety. The lemma is named after the German mathematician Paul Gordan (1837–1912).\n\nThere are topological and algebraic proofs.\n\nLet formula_1 be the cone as given in the lemma. Let formula_2 be the integral vectors so that formula_3 Then the formula_4's generate the dual cone formula_5; indeed, writing \"C\" for the cone generated by formula_4's, we have: formula_7, which must be the equality. Now, if \"x\" is in the semigroup\n\nthen it can be written as\n\nwhere formula_10 are nonnegative integers and formula_11. But since \"x\" and the first sum on the right-hand side are integral, the second sum is also integral and thus there can only be finitely many possibilities for the second sum (the topological reason). Hence, formula_12 is finitely generated.\n\nThe proof is based on a fact that a semigroup \"S\" is finitely generated if and only if its semigroup algebra formula_13 is finitely generated algebra over formula_14. To prove Gordan's lemma, by induction (cf. the proof above), it is enough to prove the statement: for any unital subsemigroup \"S\" of formula_15,\n\nPut formula_17, which has a basis formula_18. It has formula_19-grading given by\nBy assumption, \"A\" is finitely generated and thus is Noetherian. It follows from the algebraic lemma below that formula_21 is a finitely generated algebra over formula_22. Now, the semigroup formula_23 is the image of \"S\" under a linear projection, thus finitely generated and so formula_24 is finitely generated. Hence, formula_25 is finitely generated then.\n\nLemma: Let \"A\" be a formula_19-graded ring. If \"A\" is a Noetherian ring, then formula_27 is a finitely generated formula_22-algebra.\n\nProof: Let \"I\" be the ideal of \"A\" generated by all homogeneous elements of \"A\" of positive degree. Since \"A\" is Noetherian, \"I\" is actually generated by finitely many formula_29, homogeneous of positive degree. If \"f\" is homogeneous of positive degree, then we can write formula_30 with formula_31 homogeneous. If \"f\" has sufficieny large degree, then each formula_31 has degree positive and strictly less than that of \"f\". Also, each degree piece formula_33 is a finitely generated formula_22-module. (Proof: Let formula_35 be an increasing chain of finitely generated submodules of formula_33 with union formula_33. Then the chain of the ideals formula_38 stabilizes in finite steps; so does the chain formula_39) Thus, by induction on degree, we see formula_40 is a finitely generated formula_22-algebra.\n\n\n"}
{"id": "33598275", "url": "https://en.wikipedia.org/wiki?curid=33598275", "title": "Graham Brightwell", "text": "Graham Brightwell\n\nGraham Brightwell is a British mathematician working in the field of discrete mathematics.\n\nHe was a research student at the University of Cambridge and obtained his PhD in 1988 writing on \"Linear Extensions of Partially Ordered Sets\" under the supervision of Béla Bollobás.\n\nHe has published nearly 100 papers in pure mathematics, including over a dozen with Béla Bollobás. His research interests include random combinatorial structures; partially ordered sets; algorithms; random graphs; discrete mathematics and graph theory.\n\nProfessor Brightwell started playing Othello in 1985, after finding himself sharing an apartment with Imre Leader. He has finished 3 times as runner-up in the World Othello Championship and is a 5-time British Champion, and has served as chairman of the British Othello Federation and as editor of the British Othello Newsletter.\n\nHe is currently a Professor at the London School of Economics.\n"}
{"id": "9025098", "url": "https://en.wikipedia.org/wiki?curid=9025098", "title": "Hermite normal form", "text": "Hermite normal form\n\nIn linear algebra, the Hermite normal form is an analogue of reduced echelon form for matrices over the integers Z. Just as reduced echelon form can be used to solve problems about the solution to the linear system Ax=b where x is in R, the Hermite normal form can solve problems about the solution to the linear system Ax=b where this time x is restricted to have integer coordinates only. Other applications of the Hermite normal form include integer programming, cryptography, and abstract algebra.\n\nVarious authors may prefer to talk about Hermite normal form in either row-style or column-style. They are essentially the same up to transposition.\n\nAn m by n matrix A with integer entries has a (row) Hermite normal form H if there is a square unimodular matrix U where H=UA and H has the following restrictions:\nThe third condition is not standard among authors, for example some sources force non-pivots to be nonpositive or place no sign restriction on them. However, these definitions are equivalent by using a different unimodular matrix U. A unimodular matrix is a square invertible integer matrix whose determinant is 1 or -1.\n\nA m by n matrix A with integer entries has a (column) Hermite normal form H if there is a square unimodular matrix U where H=AU and H has the following restrictions:\nNote that the row-style definition has a unimodular matrix U multiplying A on the left (meaning U is acting on the rows of A), while the column-style definition has the unimodular matrix action on the columns of A. The two definitions of Hermite normal forms are simply transposes of each other.\n\nEvery m by n matrix A with integer entries has a unique m by n matrix H, such that H=UA for some square unimodular matrix U.\n\nIn the examples below, H is the Hermite normal form of the matrix A, and U is a unimodular matrix such that UA=H.\n\nformula_2\n\nformula_3\n\nIf \"A\" has only one row then either \"H = A\" or \"H = -A\", depending on whether the single row of \"A\" has a positive or negative leading coefficient.\n\nThere are many algorithms for computing the Hermite normal form dating back to 1851. It was not until 1979 when an algorithm for computing the Hermite normal form that ran in strongly polynomial time was first developed; that is, the number of steps to compute the Hermite normal form is bounded above by a polynomial in the dimensions of the input matrix, and the space used by the algorithm (intermediate numbers) is bounded by a polynomial in the binary encoding size of the numbers in the input matrix. One class of algorithms is based on Gaussian elimination in that special elementary matrices are repeatedly used. The LLL algorithm can also be used to efficiently compute the Hermite normal form.\n\nA typical lattice in R has the form formula_4 where the a are in R. If the \"columns\" of a matrix A are the a, the lattice can be associated with the columns of a matrix, and A is said to be a basis of L. Because the Hermite normal form is unique, it can be used to answer many questions about two lattice descriptions. For what follows, formula_5 denotes the lattice generated by the columns of A. Because the basis is in the columns of the matrix A, the column-style Hermite normal form must be used. Given two bases for a lattice, A and A', the equivalence problem is to decide if formula_6 This can be done by checking if the column-style Hermite normal form of A and A' are the same up to the addition of zero columns. This strategy is also useful for deciding if a lattice is a subset (formula_7 if and only if formula_8), deciding if a vector v is in a lattice (formula_9 if and only if formula_10), and for other calculations.\n\nThe linear system Ax=b has an integer solution x if and only if the system Hy=b has an integer solution y where Uy=x and H is the column-style Hermite normal form of H. Checking that Hy=b has an integer solution is easier than Ax=b because the matrix H is triangular.\n\nMany mathematical software packages can compute the Hermite normal form:\n\n"}
{"id": "20885039", "url": "https://en.wikipedia.org/wiki?curid=20885039", "title": "History of combinatorics", "text": "History of combinatorics\n\nThe mathematical field of combinatorics was studied to varying degrees in numerous ancient societies. Its study in Europe dates to the work of Leonardo Fibonacci in the 13th century AD, which introduced Arabian and Indian ideas to the continent. It has continued to be studied in the modern era.\n\nThe earliest recorded use of combinatorial techniques comes from problem 79 of the Rhind papyrus, which dates to the 16th century BCE. The problem concerns a certain geometric series, and has similarities to Fibonacci's problem of counting the number of compositions of 1s and 2s that sum to a given total.\n\nIn Greece, Plutarch wrote that Xenocrates of Chalcedon (396–314 BC) discovered the number of different syllables possible in the Greek language. This would have been the first attempt on record to solve a difficult problem in permutations and combinations. The claim, however, is implausible: this is one of the few mentions of combinatorics in Greece, and the number they found, 1.002 × 10, seems too round to be more than a guess.\n\nThe Bhagavati Sutra had the first mention of a combinatorics problem; the problem asked how many possible combinations of tastes were possible from selecting tastes in ones, twos, threes, etc. from a selection of six different tastes (sweet, pungent, astringent, sour, salt, and bitter). The Bhagavati is also the first text to mention the choose function. In the second century BC, Pingala included an enumeration problem in the Chanda Sutra (also Chandahsutra) which asked how many ways a six-syllable meter could be made from short and long notes. Pingala found the number of meters that had formula_1 long notes and formula_2 short notes; this is equivalent to finding the binomial coefficients.\n\nThe ideas of the Bhagavati were generalized by the Indian mathematician Mahavira in 850 AD, and Pingala's work on prosody was expanded by Bhāskara II and Hemacandra in 1100 AD. Bhaskara was the first known person to find the generalised choice function, although Brahmagupta may have known earlier. Hemacandra asked how many meters existed of a certain length if a long note was considered to be twice as long as a short note, which is equivalent to finding the Fibonacci numbers. \nThe ancient Chinese book of divination I Ching describes a hexagram as a permutation with repetitions of six lines where each line can be one of two states: solid or dashed. In describing hexagrams in this fashion they determine that there are formula_3 possible hexagrams. A Chinese monk also may have counted the number of configurations to a game similar to Go around 700 AD. Although China had relatively few advancements in enumerative combinatorics, around 100 AD they solved the Lo Shu Square which is the combinatorial design problem of the normal magic square of order three. Magic squares remained an interest of China, and they began to generalize their original formula_4 square between 900 and 1300 AD. China corresponded with the Middle East about this problem in the 13th century. The Middle East also learned about binomial coefficients from Indian work and found the connection to polynomial expansion. The work of Hindus influenced Arabs as seen in the work of al-Khalil ibn Ahmad who considered the possible arrangements of letters to form syllables. His calculations show an understanding of permutations and combinations. In a passage from the work of Arab mathematician Umar al-Khayyami that dates to around 1100, it is corroborated that the Hindus had knowledge of binomial coefficients, but also that their methods reached the middle east.\n\nIn Greece, Plutarch wrote that Xenocrates discovered the number of different syllables possible in the Greek language. While unlikely, this is one of the few mentions of Combinatorics in Greece. The number they found, 1.002 × 10, also seems too round to be more than a guess.\n\nAbū Bakr ibn Muḥammad ibn al Ḥusayn Al-Karaji (c.953-1029) wrote on the binomial theorem and Pascal's triangle. In a now lost work known only from subsequent quotation by al-Samaw'al, Al-Karaji introduced the idea of argument by mathematical induction.\n\nThe philosopher and astronomer Rabbi Abraham ibn Ezra (c. 1140) counted the permutations with repetitions in vocalization of Divine Name. He also established the symmetry of binomial coefficients, while a closed formula was obtained later by the talmudist and mathematician Levi ben Gerson (better known as Gersonides), in 1321.\nThe arithmetical triangle— a graphical diagram showing relationships among the binomial coefficients— was presented by mathematicians in treatises dating as far back as the 10th century, and would eventually become known as Pascal's triangle. Later, in Medieval England, campanology provided examples of what is now known as Hamiltonian cycles in certain Cayley graphs on permutations.\n\nCombinatorics came to Europe in the 13th century through mathematicians Leonardo Fibonacci and Jordanus de Nemore. Fibonacci's Liber Abaci introduced many of the Arabian and Indian ideas to Europe, including that of the Fibonacci numbers. Jordanus was the first person to arrange the binomial coefficients in a triangle, as he did in proposition 70 of \"De Arithmetica\". This was also done in the Middle East in 1265, and China around 1300. Today, this triangle is known as Pascal's triangle.\n\nPascal's contribution to the triangle that bears his name comes from his work on formal proofs about it, and the connections he made between Pascal's triangle and probability. From a letter Leibniz sent to Daniel Bernoulli we learn that Leibniz was formally studying the mathematical theory of partitions in the 17th century, although no formal work was published. Together with Leibniz, Pascal published De Arte Combinatoria in 1666 which was reprinted later. Pascal and Leibniz are considered the founders of modern combinatorics.\n\nBoth Pascal and Leibniz understood that the binomial expansion was equivalent to the choice function. The notion that algebra and combinatorics corresponded was expanded by De Moivre, who found the expansion of a multinomial. De Moivre also found the formula for derangements using the principle of principle of inclusion-exclusion, a method different from Nikolaus Bernoulli, who had found it previously. De Moivre also managed to approximate the binomial coefficients and factorial, and found a closed form for the Fibonacci numbers by inventing generating functions.\n\nIn the 18th century, Euler worked on problems of combinatorics, and several problems of probability which are linked to combinatorics. Problems Euler worked on include the Knights tour, Graeco-Latin square, Eulerian numbers, and others. To solve the Seven Bridges of Königsberg problem he invented graph theory, which also led to the formation of topology. Finally, he broke ground with partitions by the use of generating functions.\n\nIn the 19th century, the subject of partially ordered sets and lattice theory originated in the work of Dedekind, Peirce, and Schröder. However, it was Garrett Birkhoff's seminal work in his book \"Lattice Theory\" published in 1967, and the work of John von Neumann that truly established the subjects. In the 1930s, Hall (1936) and Weisner (1935) independently stated the general Möbius inversion formula. In 1964, Gian-Carlo Rota's \"On the Foundations of Combinatorial Theory I. Theory of Miibius Functions \" introduced poset and lattice theory as theories in Combinatorics. Richard P. Stanley has had a big impact in contemporary combinatorics for his work in matroid theory, for introducing Zeta polynomials, for explicitly defining Eulerian posets, developing the theory of binomial posets along with Rota and Peter Doubilet, and more.\n\n"}
{"id": "1678626", "url": "https://en.wikipedia.org/wiki?curid=1678626", "title": "Hyperfunction", "text": "Hyperfunction\n\nIn mathematics, hyperfunctions are generalizations of functions, as a 'jump' from one holomorphic function to another at a boundary, and can be thought of informally as distributions of infinite order. Hyperfunctions were introduced by Mikio Sato in 1958 in Japanese, (1959, 1960 in English), building upon earlier work by Laurent Schwartz, Grothendieck and others.\n\nA hyperfunction on the real line can be conceived of as the 'difference' between one holomorphic function defined on the upper half-plane and another on the lower half-plane. That is, a hyperfunction is specified by a pair (\"f\", \"g\"), where \"f\" is a holomorphic function on the upper half-plane and \"g\" is a holomorphic function on the lower half-plane.\n\nInformally, the hyperfunction is what the difference formula_1 would be at the real line itself. This difference is not affected by adding the same holomorphic function to both \"f\" and \"g\", so if h is a holomorphic function on the whole complex plane, the hyperfunctions (\"f\", \"g\") and (\"f\" + \"h\", \"g\" + \"h\") are defined to be equivalent.\n\nThe motivation can be concretely implemented using ideas from sheaf cohomology. Let formula_2 be the sheaf of holomorphic functions on formula_3 Define the hyperfunctions on the real line as the first local cohomology group:\n\nConcretely, let formula_5 and formula_6 be the upper half-plane and lower half-plane respectively. Then formula_7 so\n\nSince the zeroth cohomology group of any sheaf is simply the global sections of that sheaf, we see that a hyperfunction is a pair of holomorphic functions one each on the upper and lower complex halfplane modulo entire holomorphic functions.\n\nMore generally one can define formula_9 for any open set formula_10 as the quotient formula_11 where formula_12 is any open set with formula_13. One can show that this definition does not depend on the choice of formula_14 giving another reason to think of hyperfunctions as \"boundary values\" of holomorphic functions.\n\n\n\n\n\nLet formula_10 be any open subset.\n\n\n\n\n\n\n\n"}
{"id": "1592773", "url": "https://en.wikipedia.org/wiki?curid=1592773", "title": "John Jebb (reformer)", "text": "John Jebb (reformer)\n\nJohn Jebb (1736–1786) was an English divine, medical doctor, and religious and political reformer.\n\nHe was the son of John Jebb, Dean of Cashel, a member of the Irish branch of a distinguished family which came originally from Mansfield in Nottinghamshire: among his Irish cousins was John Jebb, Bishop of Limerick. He was educated at Peterhouse, Cambridge, where he was elected fellow in 1761, having previously been Second Wrangler at Cambridge in 1757. He was a man of independent judgement, and he and his wife Ann warmly supported the movement of 1771 for abolishing university and clerical subscription to the Thirty-nine Articles. In his lectures on the Greek Testament he is said to have expressed Socinian views. In 1775 he resigned his Suffolk church livings, and two years afterwards graduated M.D. at St Andrews. He practised medicine in London and was elected a fellow of the Royal Society in 1779. He and Ann continued to be involved in political reform.\n\nLike Edmund Law and Francis Blackburne, he was an advocate of soul sleep.\n\n\n"}
{"id": "7006101", "url": "https://en.wikipedia.org/wiki?curid=7006101", "title": "Leftover hash lemma", "text": "Leftover hash lemma\n\nThe leftover hash lemma is a lemma in cryptography first stated by Russell Impagliazzo, Leonid Levin, and Michael Luby.\n\nImagine that you have a secret key that has uniform random bits, and you would like to use this secret key to encrypt a message. Unfortunately, you were a bit careless with the key, and know that an adversary was able to learn about bits of that key, but you do not know which. Can you still use your key, or do you have to throw it away and choose a new key? The leftover hash lemma tells us that we can produce a key of about bits, over which the adversary has almost no knowledge. Since the adversary knows all but bits, this is almost optimal.\n\nMore precisely, the leftover hash lemma tells us that we can extract a length asymptotic to formula_1 (the min-entropy of ) bits from a random variable that are almost uniformly distributed. In other words, an adversary who has some partial knowledge about , will have almost no knowledge about the extracted value. That is why this is also called privacy amplification (see privacy amplification section in the article Quantum key distribution).\n\nRandomness extractors achieve the same result, but use (normally) less randomness.\n\nLet be a random variable over formula_2 and let formula_3. Let formula_4 be a 2-universal hash function. If \nthen for uniform over formula_6 and independent of , we have: \n\nwhere is uniform over formula_8 and independent of .\n\nformula_9 is the min-entropy of , which measures the amount of randomness has. The min-entropy is always less than or equal to the Shannon entropy. Note that formula_10 is the probability of correctly guessing . (The best guess is to guess the most probable value.) Therefore, the min-entropy measures how difficult it is to guess .\n\nformula_11 is a statistical distance between and .\n\n\n"}
{"id": "39482883", "url": "https://en.wikipedia.org/wiki?curid=39482883", "title": "List of things named after W. V. D. Hodge", "text": "List of things named after W. V. D. Hodge\n\nThese are things named after W. V. D. Hodge, a Scottish mathematician.\n\n"}
{"id": "24075754", "url": "https://en.wikipedia.org/wiki?curid=24075754", "title": "Martin Demaine", "text": "Martin Demaine\n\nMartin L. (Marty) Demaine (born 1942) is an artist and mathematician, the Angelika and Barton Weller artist in residence at the Massachusetts Institute of Technology.\nDemaine attended Medford High School in Medford, Massachusetts. After studying glassblowing in England, he began his artistic career by blowing art glass in New Brunswick in the early 1970s. The Demaine Studio, located in Miramichi Bay and later at Opus Village in Mactaquac, was the first one-man glass studio in Canada, part of the international studio glass movement. Demaine's pieces from this period are represented in the permanent collections of half a dozen major museums including the Canadian Museum of Civilization and the National Gallery of Canada. Since joining MIT, Demaine has begun blowing glass again, as an instructor at the MIT Glass Lab; his newer work features innovative glassblowing techniques intended as a puzzle to his fellow glassblowers.\n\nMartin Demaine is the father of MIT Computer Science professor and MacArthur Fellow Erik Demaine; in 1987 (when Erik was six) they together founded the Erik and Dad Puzzle Company which distributed puzzles throughout Canada. Erik was home-schooled by Martin, and although Martin never received any higher degree than his high school diploma, his home-schooling caused Erik to be awarded a B.S. at age 14 and a Ph.D. and MIT professorship at age 20, making him the youngest professor ever hired by MIT.\nThe two Demaines continue to work closely together and have many joint works of both mathematics and art, including three pieces of mathematical origami in the permanent collection of the Museum of Modern Art, New York; their joint mathematical works focus primarily on the mathematics of folding and unfolding objects out of flat materials such as paper and on the computational complexity of games and puzzles. Martin and Erik are also featured in the movie \"Between the Folds\", a documentary on modern origami.\n\nDemaine is a citizen of both Canada and the United States.\n\n"}
{"id": "41309901", "url": "https://en.wikipedia.org/wiki?curid=41309901", "title": "Multidimensional transform", "text": "Multidimensional transform\n\nIn mathematical analysis and applications, multidimensional transforms are used to analyze the frequency content of signals in a domain of two or more dimensions.\n\nOne of the more popular multidimensional transforms is the Fourier transform, which converts a signal from a time/space domain representation to a frequency domain representation. The discrete-domain multidimensional Fourier transform (FT) can be computed as follows:\n\nwhere \"F\" stands for the multidimensional Fourier transform, \"m\" stands for multidimensional dimension. Define \"f\" as a multidimensional discrete-domain signal. The inverse multidimensional Fourier transform is given by\n\nThe multidimensional Fourier transform for continuous-domain signals is defined as follows:\n\nSimilar properties of the 1-D FT transform apply, but instead of the input parameter being just a single entry, it's a Multi-dimensional (MD) array or vector. Hence, it's x(n1,…,nM) instead of x(n).\n\nif formula_4 , and formula_5 then,\n\nif formula_7, then\nformula_8\n\nif formula_9, then\n\nif formula_4, and formula_12\n\nthen,\n\nor,\n\nIf formula_9, then\n\nIf formula_9, then\n\nIf formula_19, then\n\nIf formula_9, then\n\nif formula_23 , and formula_24 then,\n\nformula_25\n\nif formula_26, then\n\nformula_27\n\nA special case of the Parseval's theorem is when the two multi-dimensional signals are the same. In this case, the theorem portrays the energy conservation of the signal and the term in the summation or integral is the energy-density of the signal.\n\nOne property is the separability property. A signal or system is said to be separable if it can be expressed as a product of 1-D functions with different independent variables. This phenomenon allows computing the FT transform as a product of 1-D FTs instead of multi-dimensional FT.\n\nif formula_7, formula_29,\nformula_30 ... formula_31, and if \nformula_32, then\n\nformula_33, so\n\nformula_34\n\nA fast Fourier transform (FFT) is an algorithm to compute the discrete Fourier transform (DFT) and its inverse. An FFT computes the DFT and produces exactly the same result as evaluating the DFT definition directly; the only difference is that an FFT is much faster. (In the presence of round-off error, many FFT algorithms are also much more accurate than evaluating the DFT definition directly).There are many different FFT algorithms involving a wide range of mathematics, from simple complex-number arithmetic to group theory and number theory. See more in FFT.\n\nThe multidimensional discrete Fourier transform (DFT) is a sampled version of the discrete-domain FT by evaluating it at sample frequencies that are uniformly spaced. The DFT is given by:\n\nfor , .\n\nThe inverse multidimensional DFT equation is\n\nfor .\n\nThe discrete cosine transform (DCT) is used in a wide range of applications such as data compression, feature extraction, Image reconstruction, multi-frame detection and so on. The multidimensional DCT is given by:\n\nfor , \"i\" = 1, 2, ..., \"r\".\n\nThe multidimensional Laplace transform is useful for the solution of boundary value problems. Boundary value problems in two or more variables characterized by partial differential equations can be solved by a direct use of the Laplace transform. The Laplace transform for an M-dimensional case is defined as\n\nformula_38\n\nwhere F stands for the s-domain representation of the signal f(t).\n\nA special case (along 2 dimensions) of the multi-dimensional Laplace transform of function f(x,y) is defined as\n\nformula_39\n\nformula_40 is called the image of formula_41 and formula_41 is known as the original of formula_40. This special case can be used to solve the Telegrapher's equations.\n\nThe multidimensional Z transform is used to map the discrete time domain multidimensional signal to the Z domain. This can be used to check the stability of filters. The equation of the multidimensional Z transform is given by\n\nformula_44\n\nwhere F stands for the z-domain representation of the signal f(n).\n\nA special case of the multidimensional Z transform is the 2D Z transform which is given as\n\nformula_45\n\nThe Fourier transform is a special case of the Z transform evaluated along the unit circle (in 1D) and unit bi-circle (in 2D). i.e. at\n\nformula_46 where z and w are vectors.\n\nPoints (\"z\",\"z\") for which formula_47 formula_48 are located in the ROC.\n\nAn example:\n\nIf a sequence has a support as shown in Figure 1.1a, then its ROC is shown in Figure 1.1b. This follows that |\"F\"(\"z\",\"z\")| < ∞ .\n\nformula_49 lies in the ROC, then all pointsformula_50that satisfy |z1|≥|z01| and |z2|≥|z02 lie in the ROC.\n\nTherefore, for figure 1.1a and 1.1b, the ROC would be\n\nwhere \"L\" is the slope.\n\nThe 2D Z-transform, similar to the Z-transform, is used in multidimensional signal processing to relate a two-dimensional discrete-time signal to the complex frequency domain in which the 2D surface in 4D space that the Fourier transform lies on is known as the unit surface or unit bicircle.\n\nThe DCT and DFT are often used in signal processing and image processing, and they are also used to efficiently solve partial differential equations by spectral methods. The DFT can also be used to perform other operations such as convolutions or multiplying large integers. The DFT and DCT have seen wide usage across a large number of fields, we only sketch a few examples below.\n\nThe DCT is used in JPEG image compression, MJPEG, MPEG, DV, Daala, and Theora video compression. There, the two-dimensional DCT-II of \"N\"x\"N\" blocks are computed and the results are quantized and entropy coded. In this case, \"N\" is typically 8 and the DCT-II formula is applied to each row and column of the block. The result is an 8x8 transform coefficient array in which the: (0,0) element (top-left) is the DC (zero-frequency) component and entries with increasing vertical and horizontal index values represent higher vertical and horizontal spatial frequencies, as shown in the picture on the right.\n\nIn image processing, one can also analyze and describe unconventional cryptographic methods based on 2D DCTs, for inserting non-visible binary watermarks into the 2D image plane, and According to different orientations, the 2-D directional DCT-DWT hybrid transform can be applied in denoising ultrasound images. 3-D DCT can also be used to transform video data or 3-D image data in watermark embedding schemes in transform domain.\n\nWhen the DFT is used for spectral analysis, the {\"x\"} sequence usually represents a finite set of uniformly spaced time-samples of some signal \"x\"(\"t\") where \"t\" represents time. The conversion from continuous time to samples (discrete-time) changes the underlying Fourier transform of \"x\"(\"t\") into a discrete-time Fourier transform (DTFT), which generally entails a type of distortion called aliasing. Choice of an appropriate sample-rate (see \"Nyquist rate\") is the key to minimizing that distortion. Similarly, the conversion from a very long (or infinite) sequence to a manageable size entails a type of distortion called \"leakage\", which is manifested as a loss of detail (aka resolution) in the DTFT. Choice of an appropriate sub-sequence length is the primary key to minimizing that effect. When the available data (and time to process it) is more than the amount needed to attain the desired frequency resolution, a standard technique is to perform multiple DFTs, for example to create a spectrogram. If the desired result is a power spectrum and noise or randomness is present in the data, averaging the magnitude components of the multiple DFTs is a useful procedure to reduce the variance of the spectrum (also called a periodogram in this context); two examples of such techniques are the Welch method and the Bartlett method; the general subject of estimating the power spectrum of a noisy signal is called spectral estimation.\n\nA final source of distortion (or perhaps \"illusion\") is the DFT itself, because it is just a discrete sampling of the DTFT, which is a function of a continuous frequency domain. That can be mitigated by increasing the resolution of the DFT. That procedure is illustrated at Sampling the DTFT.\n\nDiscrete Fourier transforms are often used to solve partial differential equations, where again the DFT is used as an approximation for the Fourier series (which is recovered in the limit of infinite \"N\"). The advantage of this approach is that it expands the signal in complex exponentials \"e\", which are eigenfunctions of differentiation: \"d\"/\"dx\" \"e\" = \"in\" \"e\". Thus, in the Fourier representation, differentiation is simple—we just multiply by \"i n\". (Note, however, that the choice of \"n\" is not unique due to aliasing; for the method to be convergent, a choice similar to that in the trigonometric interpolation section above should be used.) A linear differential equation with constant coefficients is transformed into an easily solvable algebraic equation. One then uses the inverse DFT to transform the result back into the ordinary spatial representation. Such an approach is called a spectral method.\n\nDCTs are also widely employed in solving partial differential equations by spectral methods, where the different variants of the DCT correspond to slightly different even/odd boundary conditions at the two ends of the array.\n\nLaplace transforms are used to solve partial differential equations. The general theory for obtaining solutions in this technique is developed by theorems on Laplace transform in n dimensions.\n\nThe multidimensional Z transform can also be used to solve partial differential equations.\n\nOne very important factor is that we must apply a non-destructive method to obtain those rare valuables information (from the HVS viewing point, is focused in whole colorimetric and spatial information) about works of art and zero-damage on them.\nWe can understand the arts by looking at a color change or by measuring the surface uniformity change. Since the whole image will be very huge, so we use a double raised cosine window to truncate the image:\n\nwhere \"N\" is the image dimension and \"x\", \"y\" are the coordinates from the center of image spans from 0 to \"N\"/2.\nThe author wanted to compute an equal value for spatial frequency such as:\n\nwhere \"FFT\" denotes the fast Fourier transform, and \"f\" is the spatial frequency spans from 0 to . The proposed FFT-based imaging approach is diagnostic technology to ensure a long life and stable to culture arts. This is a simple, cheap which can be used in \nmuseums without affecting their daily use. But this method doesn’t allow a quantitative measure of the corrosion rate.\n\nThe inverse multidimensional Laplace transform can be applied to simulate nonlinear circuits. This is done so by formulating a circuit as a state-space and expanding the Inverse Laplace Transform based on Laguerre function expansion.\n\nThe Lagurre method can be used to simulate a weakly nonlinear circuit and the Laguerre method can invert a multidimensional Laplace transform efficiently with a high accuracy.\n\nIt is observed that a high accuracy and significant speedup can be achieved for simulating large nonlinear circuits using multidimensional Laplace transforms.\n\n"}
{"id": "15947157", "url": "https://en.wikipedia.org/wiki?curid=15947157", "title": "Non-smooth mechanics", "text": "Non-smooth mechanics\n\nNon-smooth mechanics is a modeling approach in mechanics which does not require the time evolutions of the positions and of the velocities to be smooth functions anymore. Due to possible impacts, the velocities of the mechanical system are even allowed to undergo jumps at certain time instants in order to fulfill the kinematical restrictions. Consider for example a rigid model of a ball which falls on the ground. Just before the impact between ball and ground, the ball has non-vanishing pre-impact velocity. At the impact time instant, the velocity must jump to a post-impact velocity which is at least zero, or else penetration would occur. Non-smooth mechanical models are often used in contact dynamics.\n\n\n"}
{"id": "663680", "url": "https://en.wikipedia.org/wiki?curid=663680", "title": "Northeast (disambiguation)", "text": "Northeast (disambiguation)\n\nNortheast is a compass point.\n\nNortheast, north-east, north east, northeastern or north-eastern or north eastern may also refer to:\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3100586", "url": "https://en.wikipedia.org/wiki?curid=3100586", "title": "Ore's theorem", "text": "Ore's theorem\n\nOre's theorem is a result in graph theory proved in 1960 by Norwegian mathematician Øystein Ore. It gives a sufficient condition for a graph to be Hamiltonian, essentially stating that a graph with sufficiently many edges must contain a Hamilton cycle. Specifically, the theorem considers the sum of the degrees of pairs of non-adjacent vertices: if every such pair has a sum that at least equals the total number of vertices in the graph, then the graph is Hamiltonian.\n\nLet be a (finite and simple) graph with vertices. We denote by the degree of a vertex in , i.e. the number of incident edges in to . Then, Ore's theorem states that if\n\nthen is Hamiltonian.\n\nIt is equivalent to show that every non-Hamiltonian graph does not obey condition (*). Accordingly, let be a graph on vertices that is not Hamiltonian, and let be formed from by adding edges one at a time that do not create a Hamiltonian cycle, until no more edges can be added. Let and be any two non-adjacent vertices in . Then adding edge to would create at least one new Hamiltonian cycle, and the edges other than in such a cycle must form a Hamiltonian path in with and . For each index in the range , consider the two possible edges in from to and from to . At most one of these two edges can be present in , for otherwise the cycle would be a Hamiltonian cycle. Thus, the total number of edges incident to either or is at most equal to the number of choices of , which is . Therefore, does not obey property (*), which requires that this total number of edges () be greater than or equal to . Since the vertex degrees in are at most equal to the degrees in , it follows that also does not obey property (*).\n\n describes the following simple algorithm for constructing a Hamiltonian cycle in a graph meeting Ore's condition.\n\n\nEach step increases the number of consecutive pairs in the cycle that are adjacent in the graph, by one or two pairs (depending on whether \"v\" and \"v\" are already adjacent), so the outer loop can only happen at most \"n\" times before the algorithm terminates, where \"n\" is the number of vertices in the given graph. By an argument similar to the one in the proof of the theorem, the desired index \"j\" must exist, or else the nonadjacent vertices \"v\" and \"v\" would have too small a total degree. Finding \"i\" and \"j\", and reversing part of the cycle, can all be accomplished in time O(\"n\"). Therefore, the total time for the algorithm is O(\"n\"), matching the number of edges in the input graph.\n\nOre's theorem is a generalization of Dirac's theorem that, when each vertex has degree at least , the graph is Hamiltonian. For, if a graph meets Dirac's condition, then clearly each pair of vertices has degrees adding to at least .\n\nIn turn Ore's theorem is generalized by the Bondy–Chvátal theorem. One may define a closure operation on a graph in which, whenever two nonadjacent vertices have degrees adding to at least , one adds an edge connecting them; if a graph meets the conditions of Ore's theorem, its closure is a complete graph. The Bondy–Chvátal theorem states that a graph is Hamiltonian if and only if its closure is Hamiltonian; since the complete graph is Hamiltonian, Ore's theorem is an immediate consequence.\n\nOre's theorem may also be strengthened to give a stronger conclusion than Hamiltonicity as a consequence of the degree condition in the theorem. Specifically, every graph satisfying the conditions of Ore's theorem is either a regular complete bipartite graph or is pancyclic .\n\n"}
{"id": "499019", "url": "https://en.wikipedia.org/wiki?curid=499019", "title": "Organon", "text": "Organon\n\nThe Organon (Greek: Ὄργανον, meaning \"instrument, tool, organ\") is the standard collection of Aristotle's six works on logic. The name \"Organon\" was given by Aristotle's followers, the Peripatetics. They are as follows:\n\nThe order of the works is not chronological (which is now hard to determine) but was deliberately chosen by Theophrastus to constitute a well-structured system. Indeed, parts of them seem to be a scheme of a lecture on logic. The arrangement of the works was made by Andronicus of Rhodes around 40 BC.\n\nAristotle's \"Metaphysics\" has some points of overlap with the works making up the \"Organon\" but is not traditionally considered part of it; additionally there are works on logic attributed, with varying degrees of plausibility, to Aristotle that were not known to the Peripatetics.\n\nWhereas the modern \"Organon\" comprises only the above six works, the medieval \"Organon\", including the Arabic tradition, appends to the list Aristotle's \"Rhetoric\" and his \"Poetics\".\n\nThe \"Organon\" was used in the school founded by Aristotle at the Lyceum, and some parts of the works seem to be a scheme of a lecture on logic. So much so that after Aristotle's death, his publishers (Andronicus of Rhodes in 50 BC, for example) collected these works.\n\nFollowing the collapse of the Western Roman Empire in the fifth century, much of Aristotle's work was lost in the Latin West. The \"Categories\" and \"On Interpretation\" are the only significant logical works that were available in the early Middle Ages. These had been translated into Latin by Boethius. The other logical works were not available in Western Christendom until translated into Latin in the 12th century. However, the original Greek texts had been preserved in the Greek-speaking lands of the Eastern Roman Empire (aka Byzantium). In the mid-twelfth century, James of Venice translated into Latin the \"Posterior Analytics\" from Greek manuscripts found in Constantinople.\n\nThe books of Aristotle were available in the early Arab Empire, and after 750 AD Muslims had most of them, including the \"Organon\", translated into Arabic, sometimes via earlier Syriac translations. They were studied by Islamic and Jewish scholars, including Rabbi Moses Maimonides (1135–1204) and the Muslim Judge Ibn Rushd, known in the West as Averroes (1126–1198); both were originally from Cordoba, Spain, although the former left Iberia and by 1168 lived in Egypt.\n\nAll the major scholastic philosophers wrote commentaries on the \"Organon\". Aquinas, Ockham and Scotus wrote commentaries on \"On Interpretation\". Ockham and Scotus wrote commentaries on the \"Categories\" and \"Sophistical Refutations\". Grosseteste wrote an influential commentary on the \"Posterior Analytics\".\n\nIn the Enlightenment there was a revival of interest in logic as the basis of rational enquiry, and a number of texts, most successfully the Port-Royal Logic, polished Aristotelian term logic for pedagogy. During this period, while the logic certainly was based on that of Aristotle, Aristotle's writings themselves were less often the basis of study. There was a tendency in this period to regard the logical systems of the day to be complete, which in turn no doubt stifled innovation in this area. However Francis Bacon published his \"Novum Organum\" (\"The New \"Organon\"\") as a scathing attack in 1620. Immanuel Kant thought that there was nothing else to invent after the work of Aristotle, and the famous logic historian Karl von Prantl claimed that any logician who said anything new about logic was \"confused, stupid or perverse.\" These examples illustrate the force of influence which Aristotle's works on logic had. Indeed, he had already become known by the Scholastics (medieval Christian scholars) as \"The Philosopher\", due to the influence he had upon medieval theology and philosophy. His influence continued into the Early Modern period and Organon was the basis of school philosophy even in the beginning of 18th century.\nSince the logical innovations of the 19th century, particularly the formulation of modern predicate logic, Aristotelian logic had for a time fallen out of favor among many analytic philosophers.\n\nHowever the logic historian John Corcoran and others have shown that the works of George Boole and Gottlob Frege—which laid the groundwork for modern mathematical logic—each represent a continuation and extension to Aristote's logic and in no way contradict or displace it. \nBoole fully accepted and endorsed Aristotle’s logic, and Frege included Aristotle's square of opposition at the end of his groundbreaking Begriffsschrift to show the harmony of his theory with the Aristotelian tradition.\n\n\n\n\n"}
{"id": "6473626", "url": "https://en.wikipedia.org/wiki?curid=6473626", "title": "Outline of geometry", "text": "Outline of geometry\n\nGeometry is a branch of mathematics concerned with questions of shape, size, relative position of figures, and the properties of space. Geometry is one of the oldest mathematical sciences.\n\n\n\nHistory of geometry\n\n\n\n\n\n\n\n"}
{"id": "64493", "url": "https://en.wikipedia.org/wiki?curid=64493", "title": "Percentage", "text": "Percentage\n\nIn mathematics, a percentage is a number or ratio expressed as a fraction of 100. It is often denoted using the percent sign, \"%\", or the abbreviations \"pct.\", \"pct\"; sometimes the abbreviation \"pc\" is also used. A percentage is a dimensionless number (pure number).\n\nFor example, 45% (read as \"forty-five percent\") is equal to , 45:100, or 0.45. \nPercentages are often used to express a proportionate part of a total.\n\nIf 50% of the total number of students in the class are male, that means that 50 out of every 100 students are male. If there are 500 students, then 250 of them are male.\n\nAn increase of $0.15 on a price of $2.50 is an increase by a fraction of = 0.06. Expressed as a percentage, this is a 6% increase.\n\nWhile many percentage values are between 0 and 100, there is no mathematical restriction and percentages may take on other values. For example, it is common to refer to 111% or −35%, especially for percent changes and comparisons.\n\nIn Ancient Rome, long before the existence of the decimal system, computations were often made in fractions which were multiples of . For example, Augustus levied a tax of on goods sold at auction known as \"centesima rerum venalium\". Computation with these fractions was equivalent to computing percentages. As denominations of money grew in the Middle Ages, computations with a denominator of 100 became more standard and from the late 15th century to the early 16th century it became common for arithmetic texts to include such computations. Many of these texts applied these methods to profit and loss, interest rates, and the Rule of Three. By the 17th century it was standard to quote interest rates in hundredths.\n\nThe term \"per cent\" is derived from the Latin \"per centum\", meaning \"by the hundred\". \nThe sign for \"per cent\" evolved by gradual contraction of the Italian term \"per cento\", meaning \"for a hundred\". The \"per\" was often abbreviated as \"p.\" and eventually disappeared entirely. The \"cento\" was contracted to two circles separated by a horizontal line, from which the modern \"%\" symbol is derived.\n\nThe percent value is computed by multiplying the numeric value of the ratio by 100. For example, to find 50 apples as a percentage of 1250 apples, first compute the ratio = 0.04, and then multiply by 100 to obtain 4%. The percent value can also be found by multiplying first, so in this example the 50 would be multiplied by 100 to give 5,000, and this result would be divided by 1250 to give 4%.\n\nTo calculate a percentage of a percentage, convert both percentages to fractions of 100, or to decimals, and multiply them. For example, 50% of 40% is:\nIt is not correct to divide by 100 and use the percent sign at the same time. (E.g. , not , which actually is . A term such as would also be incorrect, this would be read as 1 percent even if the intent was to say 100%.)\n\nWhenever we talk about a percentage, it is important to specify what it is relative to, i.e. what is the total that corresponds to 100%. The following problem illustrates this point.\n\nWe are asked to compute the ratio of female computer science majors to all computer science majors. We know that 60% of all students are female, and among these 5% are computer science majors, so we conclude that × = or 3% of all students are female computer science majors. Dividing this by the 10% of all students that are computer science majors, we arrive at the answer: = or 30% of all computer science majors are female.\n\nThis example is closely related to the concept of conditional probability.\n\nDue to inconsistent usage, it is not always clear from the context what a percentage is relative to. When speaking of a \"10% rise\" or a \"10% fall\" in a quantity, the usual interpretation is that this is relative to the \"initial value\" of that quantity. For example, if an item is initially priced at $200 and the price rises 10% (an increase of $20), the new price will be $220. Note that this final price is 110% of the initial price (100% + 10% = 110%).\n\nSome other examples of percent changes:\n\nIn general, a change of percent in a quantity results in a final amount that is 100 +  percent of the original amount (equivalently, 1 + 0.01 times the original amount).\n\nPercent changes applied sequentially \"do not add up\" in the usual way. For example, if the 10% increase in price considered earlier (on the $200 item, raising its price to $220) is followed by a 10% decrease in the price (a decrease of $22), the final price will be $198, \"not\" the original price of $200. The reason for the apparent discrepancy is that the two percent changes (+10% and −10%) are measured relative to \"different\" quantities ($200 and $220, respectively), and thus do not \"cancel out\".\n\nIn general, if an increase of percent is followed by a decrease of percent, and the initial amount was , the final amount is ; thus the net change is an overall decrease by percent \"of\" percent (the square of the original percent change when expressed as a decimal number). Thus, in the above example, after an increase and decrease of , the final amount, $198, was 10% of 10%, or 1%, less than the initial amount of $200. The net change is the same for a decrease of percent followed by an increase of percent; the final amount is .\n\nThis can be expanded for a case where you do not have the same percent change. If the initial percent change is and the second percent change is , and the initial amount was , then the final amount is . To change the above example, after an increase of and decrease of , the final amount, $209, is 4.5% more than the initial amount of $200.\n\nAs shown above, percent changes can be applied in any order and have the same effect.\n\nIn the case of interest rates, a very common but ambiguous way to say that an interest rate rose from 10% per annum to 15% per annum, for example, is to say that the interest rate increased by 5%, which could \"theoretically\" mean that it increased from 10% per annum to 10.05% per annum. It is clearer to say that the interest rate increased by 5 percentage points (pp). The same confusion between the different concepts of percent(age) and percentage points can potentially cause a major misunderstanding when journalists report about election results, for example, expressing both new results and differences with earlier results as percentages. For example, if a party obtains 41% of the vote and this is said to be a 2.5% increase, does that mean the earlier result was 40% (since 41 = ) or 38.5% (since 41 = )?\n\nIn financial markets, it is common to refer to an increase of one percentage point (e.g. from 3% per annum to 4% per annum) as an increase of \"100 basis points\".\n\nIn British English, \"percent\" is usually written as two words (\"per cent\"), although \"percentage\" and \"percentile\" are written as one word. In American English, \"percent\" is the most common variant (but \"per mille\" is written as two words).\n\nIn the early 20th century, there was a dotted abbreviation form \"\"per cent.\", as opposed to \"per cent\". The form \"per cent.\"\" is still in use in the highly formal language found in certain documents like commercial loan agreements (particularly those subject to, or inspired by, common law), as well as in the Hansard transcripts of British Parliamentary proceedings. The term has been attributed to Latin \"per centum\". The concept of considering values as parts of a hundred is originally Greek. The symbol for percent (%) evolved from a symbol abbreviating the Italian \"per cento\". In some other languages, the form \"procent\" or \"prosent\" is used instead. Some languages use both a word derived from \"percent\" and an expression in that language meaning the same thing, e.g. Romanian \"procent\" and \"la sută\" (thus, \"10%\" can be read or sometimes written \"ten for [each] hundred\", similarly with the English \"one out of ten\"). Other abbreviations are rarer, but sometimes seen.\n\nGrammar and style guides often differ as to how percentages are to be written. For instance, it is commonly suggested that the word percent (or per cent) be spelled out in all texts, as in \"1 percent\" and not \"1%\". Other guides prefer the word to be written out in humanistic texts, but the symbol to be used in scientific texts. Most guides agree that they always be written with a numeral, as in \"5 percent\" and not \"five percent\", the only exception being at the beginning of a sentence: \"Ten percent of all writers love style guides.\" Decimals are also to be used instead of fractions, as in \"3.5 percent of the gain\" and not \" percent of the gain\". However the titles of bonds issued by governments and other issuers use the fractional form, e.g. \"% Unsecured Loan Stock 2032 Series 2\". (When interest rates are very low, the number 0 is included if the interest rate is less than 1%, e.g. \"% Treasury Stock\", not \"% Treasury Stock\".) It is also widely accepted to use the percent symbol (%) in tabular and graphic material.\n\nIn line with common English practice, style guides—such as \"The Chicago Manual of Style\"—generally state that the number and percent sign are written without any space in between.\nHowever, the International System of Units and the ISO 31-0 standard require a space.\n\nThe word \"percentage\" is often a misnomer in the context of sports statistics, when the referenced number is expressed as a decimal proportion, not a percentage: \"The Phoenix Suns' Shaquille O'Neal led the NBA with a .609 field goal percentage (FG%) during the 2008–09 season.\" (O'Neal made 60.9% of his shots, not 0.609%.) Likewise, the winning percentage of a team, the fraction of matches that the club has won, is also usually expressed as a decimal proportion; a team that has a .500 winning percentage has won 50% of their matches. The practice is probably related to the similar way that batting averages are quoted.\n\nAs \"percent\" it is used to describe the steepness of the slope of a road or railway, formula for which is 100 ×  which could also be expressed as the tangent of the angle of inclination times 100. This is the ratio of distances a vehicle would advance vertically and horizontally, respectively, when going up- or downhill, expressed in percent.\n\nPercentage is also used to express composition of a mixture by mass percent and mole percent.\n\n\n\n"}
{"id": "15794879", "url": "https://en.wikipedia.org/wiki?curid=15794879", "title": "Quantum digital signature", "text": "Quantum digital signature\n\nA Quantum Digital Signature (QDS) refers to the quantum mechanical equivalent of either a classical digital signature or, more generally, a handwritten signature on a paper document. Like a handwritten signature, a digital signature is used to protect a document, such as a digital contract, against forgery by another party or by one of the participating parties.\n\nAs e-commerce has become more important in society, the need to certify the origin of exchanged information has arisen. Modern digital signatures enhance security based on the difficulty of solving a mathematical problem, such as finding the factors of large numbers (as used in the RSA algorithm). Unfortunately, the task of solving these problems becomes feasible when a quantum computer is available (see Shor's algorithm). To face this new problem, new quantum digital signature schemes are in development to provide protection against tampering, even from parties in possession of quantum computers and using powerful quantum cheating strategies.\n\nThe public-key method of cryptography allows a sender to sign a message (often only the cryptographic hash of the message) with a sign key in such a way that any recipient can, using the corresponding public key, check the authenticity of the message. To allow this, the public key is made broadly available to all potential recipients. To make sure only the legal author of the message can validly sign the message, the public key is created from a random, private sign key, using a one-way function. This is a function that is designed such that computing the result given the input is very easy, but computing the input given the result is very difficult. A classic example is the multiplication of two very large primes: The multiplication is easy, but factoring the product without knowing the primes is normally considered infeasible.\n\nLike classical digital signatures, quantum digital signatures make use of asymmetric keys. Thus, a person who wants to sign a message creates one or more pairs of sign and corresponding public keys. In general we can divide quantum digital signature schemes into two groups:\n\nIn both cases f is a one-way quantum function that has the same properties as a classical one-way function.\nThat is, the result is easy to compute, but, in contrast to the classical scheme, the function is \"impossible\" to invert, even if one uses powerful quantum cheating strategies.\n\nThe most famous scheme for the first method above is provided by Gottesman and Chuang \n\nMost of the requirements for a classical digital signature scheme also apply to the quantum digital signature scheme.\n\nIn detail\n\nA classical one-way function as said above is based on a classical infeasible mathematical task, whereas a quantum one-way function exploits the uncertainty principle which makes it impossible even for a quantum computer to compute the inverse.\nThis is done by providing a quantum output state, with whom one cannot learn enough about the input string to reproduce it.\nIn case of the first group of schemes this is shown by Holevo's theorem, which says, that from a given n-qubit quantum state one cannot extract more than n classical bits of information.\nOne possibility to ensure that the scheme uses less qubits for a bit string of a certain length is by using nearly orthogonal states\nThat gives us the possibility to induce a basis with more than two states.\nSo to describe an information of formula_6 bits, we can use less than n qubits.\nAn example with a 3 qubit basis\nOnly m qubits are needed to describe n classical bits when formula_10 holds.\n\nBecause of Holevo's theorem and the fact, that m can be much smaller than n, we can only get m bits out of the n bits message. More general, if one gets T copies of the public key he can extract at most Tm bits of the private key.\nIf formula_11 is big formula_12 becomes very large, which makes it impossible for a dishonest person to guess the sign key.\n\n\"Note: You cannot distinguish between non-orthogonal states, if you only have a small amount of identical states. That's how the quantum one-way functions works. \"<br>\n\"Nevertheless formula_13 leaks information about the private key, in contrast to the classical public key, which forces one to get nothing or all about the private key. \"\n\nIn the classical case we create a classical public key out of a classical sign key, thus it is easy to provide every potential recipient with a copy of the public key. The public key can be freely distributed.\nThis becomes more difficult in the quantum case, because copying a quantum state is forbidden by the no cloning theorem, as long as the state itself is unknown.\nSo public keys can only be created and distributed by a person who knows the exact quantum state he wants to create, thus who knows the sign key (This can be the sender or in more general a trustful institution).\nNevertheless, in contrast to the classical public key there is an upper bound for the number of public quantum keys T which can be created, without enabling one to guess the sign key and thus endangering the security of the scheme (formula_14 has to be big)\n\nTo make sure that every recipient gets identical results when testing the authenticity of a message, public keys distributed have to be the same.\nThis is straightforward in the classical case, because one can easily compare two classical bit strings and see if those match.\nNevertheless, in the quantum state it is more complicated.\nTo test, if two public quantum states are the same one has to compare the following\nThis is done with the following quantum circuit which uses one Fredkin gate \"F\", one Hadamard gate \"H\" and an ancilla qubit \"a\".\nFirst of all the ancilla qubit is set to a symmetric state formula_16.\n\nRight after the ancilla qubit is used as a control on the targets formula_17 and formula_18 in a Fredkin Gate.\n\nFurthermore, a Hadamard gate is applied on the ancilla qubit and finally the first qubit gets measured.\nIf both states are the same, the result formula_7 is measured.\nIf both states are nearly orthogonal, the result can be either formula_20 or formula_21.\n\nThe calculation of the swap test in more detail:\n\nThe overall state\n\nAfter the Fredkin gate is applied\n\nformula_25\n\nAfter the Hadamard gate is applied on the first qubit\n\nformula_26\n\nAfter sorting for formula_27\n\nformula_28\n\nNow it is easy to see, if the states formula_29 then formula_30, which gives us a 0 whenever it is measured.\n\nLet Person A (Alice) want to send a message to Person B (Bob).\nHash algorithms won't be considered, so Alice has to sign every single bit of her message. Message-Bit b formula_31.\n\nAlice chooses M pairs of private keys formula_32\nThe function which maps formula_3 is known to all parties.\nAlice now computes the corresponding public keys formula_36 and gives all of them to the recipients. She can make as many copies as she needs, but has to take care, not to endanger the security formula_37.\n\n\"Her level of security limits the number of identical public keys she can create\"\n\nIf\n\n\"Remember: In this example Alice picks only one bit b and signs it. She has to do that for every single bit in her message \"\n\nBob now possesses\n\nNow Bob calculates formula_42 for all received private keys (either formula_40).\n\nAfter he has done so he makes use of the swap test to compare the calculated states with the received public keys.\nSince the swap test has some probability to give the wrong answer he has to do it for all the M keys and counts how many incorrect keys he gets r. It is obvious, that M is some kind of a security parameter. It is more unlikely to validate a bit wrong for bigger M.\n\nOne problem which arises especially for small M is, that the number of incorrect keys different recipients measure differ with probability. So to define only one threshold is not enough, because it would cause a message to be validated differently, when the number of incorrect keys r is very close to the defined threshold.\n\nThis can be prevented by defining more than one threshold.\nBecause the number of errors increase proportional with M, the thresholds are defined like\n\n\"If we assume perfect channels without noise, so the bit can't be changed due to the transfer, then the threshold formula_48 can be set to zero, because the swap test passes always, when the compared states are the same\"\n\n"}
{"id": "12106740", "url": "https://en.wikipedia.org/wiki?curid=12106740", "title": "Random geometric graph", "text": "Random geometric graph\n\nIn graph theory, a random geometric graph (RGG) is the mathematically simplest spatial network, namely an undirected graph constructed by randomly placing \"N\" nodes in some metric space (according to a specified probability distribution) and connecting two nodes by a link if and only if their distance is in a given range, e.g. smaller than a certain neighborhood radius, \"r\".\n\nRandom geometric graphs resemble real human social networks in a number of ways. For instance, they spontaneously demonstrate community structure - clusters of nodes with high modularity. Other random graph generation algorithms, such as those generated using the Erdős–Rényi model or Barabási–Albert (BA) model do not create this type of structure. Additionally, random geometric graphs display degree assortativity: \"popular\" nodes (those with many links) are particularly likely to be linked to other popular nodes.\n\nA real-world application of RGGs is the modeling of ad hoc networks.\n\nThe connectivity properties in RGGs have been well studied since their introduction by Edgar Gilbert in 1961 , which stated wireless communication networks as a suggested application; nodes (vertices) represent wireless devices distributed in the plane according to some point process and connect if their euclidean distance is less than some critical transmission range r. \nSince then the connectivity properties of RGGs have been widely studied both on the plane and in other spaces such as hyperbolic space . \n\nGilbert’s initial paper provided a lower bound (by creating an associated branching process) and upper bound (by using results from bond percolation) for the critical transmission range needed for a giant component in the plane. \nPenrose showed for a uniform Poisson point process that the main obstacle to full connectivity is isolated nodes as the number of nodes N →∞; a result which has since been used as an approximation for full connectivity.\nFurthermore, assuming a homogeneous Poisson point process, the probability a node is isolated is a ”local” event and thus the distribution of isolated nodes can be modelled by a limiting Poisson process . \nFor the 1-dimensional case the main obstacle for full connectivity is no loner isolated nodes since the network is likely to split into two clusters of arbitrary sizes. \n\nIn 1988 Waxman generalised the standard RGG by introducing a probabilistic connection function as opposed to the deterministic one suggested by Gilbert. \nThe example introduced by Waxman was a stretched exponential where two nodes i and j connect with probability given by H = βexp(-), where is the euclidean separation and β, r are parameters determined by the system. \nThis type of RGG with probabilistic connection function is often referred to a soft Random Geometric Graph, which now has two sources of randomness; the location of nodes (vertices) and the formation of links(edges). \nThis connection function has been generalised further in the literature H = βexp(-()) which is often used to study wireless networks without interference. \nThe parameter η represents how the signal decays with distance, when η = 2 is free space, η > 2 models a more cluttered environment like a town ( = 6 models cities like New York) whilst η < 2 models highly reflective environments. \nWe notice that for η =1 is the Waxman model, whilst as η→∞ and β = 1 we have the standard RGG. \nIntuitively these type of connection functions model how the probability of a link being made decays with distance. \n\nIn the high density limit for a network with exponential connection function the number of isolated nodes is Poisson distributed, and the resulting network contains a unique giant component and isolated nodes only . Therefore by ensuring there are no isolated nodes, in the dense regime, the network is a.a.s fully connected; similar to the results shown in for the disk model. \nOften the properties of these networks such as betweenness centrality and connectivity are studied in the limit as the density → ∞ which often means border effects become negligible.\nHowever, in real life where networks are finite, although can still be extremely dense, border effects will impact on full connectivity; in fact showed that for full connectivity, with an exponential connection function, is greatly impacted by boundary effects as nodes near the corner/face of a domain are less likely to connect compared with those in the bulk. As a result full connectivity can be expressed as a sum of the contributions from the bulk and the geometries boundaries.\nA more general analysis of the connection functions in wireless networks has shown that the probability of full connectivity can be well approximated expressed by a few moments of the\nconnection function and the regions geometry .\n\nThe simplest choice for the node distribution is to sprinkle them uniformly and independently in the embedding space.\n\n"}
{"id": "1076541", "url": "https://en.wikipedia.org/wiki?curid=1076541", "title": "Roger Schank", "text": "Roger Schank\n\nRoger Carl Schank (born 1946) is an American artificial intelligence theorist, cognitive psychologist, learning scientist, educational reformer, and entrepreneur.\n\nBeginning in the late 1960s, he pioneered conceptual dependency theory (within the context of natural language understanding) and case-based reasoning, both of which challenged cognitivist views of memory and reasoning.\n\nIn 1989, Schank was granted $30 million in a 10-year commitment to his research and development by Andersen Consulting, through which he founded the Institute for the Learning Sciences (ILS) at Northwestern University in Chicago.\n\nFor his undergraduate degree, Schank studied mathematics at Carnegie Mellon University in Pittsburgh PA, and later was awarded a PhD in linguistics at the University of Texas in Austin and went on to work in faculty positions at Stanford University and then at Yale University. In 1974, he became professor of computer science and psychology at Yale University. In 1981, Schank became Chairman of Computer Science at Yale and director of the Yale Artificial Intelligence Project.\n\nIn 1989, Schank was granted $30 million in a 10-year commitment to his research and development by Andersen Consulting, allowing him to leave Yale and set up the Institute for the Learning Sciences (ILS) at Northwestern University in Chicago, bringing along 25 of his Yale colleagues. ILS attracted other corporate sponsors such as IBM and Ameritech, as well as government sponsors such as the U.S. Army, EPA and the National Guard, leading to a focus on the development of educational software, especially in employee training. ILS was later absorbed by the School of Education as a separate department.\n\nWhen Carnegie Mellon University's Silicon Valley campus was established in 2002, Schank came to serve as Chief Educational Officer at the institution.\n\nWhile at Yale in 1979, Schank was among the first to \"capitalize on the expected boom\" in AI when he founded Cognitive Systems, a company that went public in 1986. Schank resigned as chairman and chief executive in 1988 for personal reasons, but stayed as a board member and advisor.\n\nIn 1994, Schank founded Cognitive Arts Corporation (originally named Learning Sciences Corporation) to market the software developed at ILS, and led the company until it was sold in 2003.\n\nFrom 2005 to 2007, Schank was the chief learning officer of Trump University.\n\nIn 2001 he founded Socratic Arts, a company that sells e-learning software to both businesses and schools.\n\nIn 2008, Schank built a story-centered curriculum (SCC) at the Business Engineering School of La Salle International Graduate School of Ramon Llull University, Barcelona to teach MBA students to launch their own businesses or to go to work.\n\nIn 2012, Schank founded XTOL (Experiential Training Online) which \"designs learn-by-doing experiential short courses for use by universities, corporations and professional organizations, as well as Master's programs in partnership with degree-granting universities around the world.\"\n\nSchank believes that the educational system is fundamentally broken and that software will need to replace conventional teaching methods. To serve this purpose, he founded Engines for Education in 2001, a not-for-profit organization which designs and implements curricula for primary and secondary schools and hosts the Virtual International Science and Technology Academy (VISTA).\n\nSchank was a leading pioneer of artificial intelligence and cognitive psychology in the 1970s and 1980s. His innovations in these fields were conceptual dependency theory and case-based reasoning, both of which challenged cognitivist views of memory and reasoning.\n\nIn 1969 Schank introduced the conceptual dependency theory for natural language understanding. This model, partly based on the work of Sydney Lamb, was extensively used by Schank's students at Yale University, such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.\n\nCase-based reasoning (CBR) is based on Schank's model of dynamic memory and was the basis for the earliest CBR systems: Janet Kolodner's CYRUS and Michael Lebowitz's IPP.\n\nOther schools of CBR and closely allied fields emerged in the 1980s, investigating such topics as CBR in legal reasoning, memory-based reasoning (a way of reasoning from examples on massively parallel machines), and combinations of CBR with other reasoning methods. In the 1990s, interest in CBR grew, as evidenced by the establishment of an International Conference on Case-Based Reasoning in 1995, as well as European, German, British, Italian, and other CBR workshops.\n\nCBR technology has produced a number of successful deployed systems, the earliest being Lockheed's CLAVIER, a system for laying out composite parts to be baked in an industrial convection oven. CBR has been used extensively in help desk applications such as the Compaq SMART system and has found a major application area in the health sciences.\n\n\n\n"}
{"id": "4234672", "url": "https://en.wikipedia.org/wiki?curid=4234672", "title": "Roland Fraïssé", "text": "Roland Fraïssé\n\nRoland Fraïssé (; 12 March 1920 – 30 March 2008) was a French mathematical logician.\n\nFraïssé received his doctoral degree from the University of Paris in 1953. In his thesis, Fraïssé used the back-and-forth method to determine whether two model-theoretic structures were elementarily equivalent. This method of determining elementary equivalence was later formulated as the Ehrenfeucht–Fraïssé game. Fraïssé worked primarily in relation theory. Another of his important works was the Fraïssé construction of a Fraïssé limit of finite structures. He also introduced the notion of compensor in the theory of posets.\n\nMost of his career was spent as Professor at the University of Provence in Marseille, France.\n\n"}
{"id": "3196691", "url": "https://en.wikipedia.org/wiki?curid=3196691", "title": "School Mathematics Project", "text": "School Mathematics Project\n\nThe School Mathematics Project is a developer of mathematics textbooks for secondary schools, formerly based in Southampton in the UK. \n\nNow generally known as SMP, it began as a research project inspired by a 1961 conference chaired by Bryan Thwaites at the University of Southampton, which itself was precipitated by calls to reform mathematics teaching in the wake of the Sputnik launch by the Soviet Union, the same circumstances which prompted the wider New Math movement. It maintained close ties with the former Collaborative Group for Research in Mathematics Education at the university.\n\nInstead of dwelling on 'traditional' areas such as arithmetic and geometry, SMP dwelt on subjects such as set theory, graph theory and logic, non-cartesian co-ordinate systems, matrix mathematics, affine transforms, vectors and non-decimal number systems.\n\nThe SMP is now a registered charity, and continues to publish textbooks in association with Cambridge University Press for GCSE and both AQA and Edexcel A-level exams. It also published an educational comic called \"Mathematical Mike and his Dog Dingle.\" \n\nThe computer paper tape motif on early educational material reads \"THE SCHOOL MATHEMATICS PROJECT DIRECTED BY BRYAN THWAITES\".\n"}
{"id": "37895661", "url": "https://en.wikipedia.org/wiki?curid=37895661", "title": "SequenceL", "text": "SequenceL\n\nSequenceL is a general purpose functional programming language and auto-parallelizing (Parallel computing) compiler and tool set, whose primary design objectives are performance on multi-core processor hardware, ease of programming, platform portability/optimization, and code clarity and readability. Its main advantage is that it can be used to write straightforward code that automatically takes full advantage of all the processing power available, without programmers needing to be concerned with identifying parallelisms, specifying vectorization, avoiding race conditions, and other challenges of manual directive-based programming approaches such as OpenMP.\n\nPrograms written in SequenceL can be compiled to multithreaded code that runs in parallel, with no explicit indications from a programmer of how or what to parallelize. , versions of the SequenceL compiler generate parallel code in C++ and OpenCL, which allows it to work with most popular programming languages, including C, C++, C#, Fortran, Java, and Python. A platform-specific runtime manages the threads safely, automatically providing parallel performance according to the number of cores available, currently supporting x86, OpenPOWER/POWER8, and ARM platforms.\n\nSequenceL was initially developed over a 20-year period starting in 1989, mostly at Texas Tech University. Primary funding was from NASA, which originally wanted to develop a specification language which was \"self-verifying\"; that is, once written, the requirements could be \"executed\", and the results verified against the desired outcome.\n\nThe principal researcher on the project was initially Dr. Daniel Cooke, who was soon joined by Dr. Nelson Rushton (another Texas Tech professor) and later Dr. Brad Nemanich (then a PhD student under Cooke). The goal of creating a language that was simple enough to be readable, but unambiguous enough to be executable, drove the inventors to settle on a functional, declarative language approach, where a programmer describes desired results, rather than the means to achieve them. The language is then free to solve the problem in the most efficient manner that it can find.\n\nAs the language evolved, the researchers developed new computational approaches, including \"consume-simplify-produce\" (CSP). In 1998, research began to apply SequenceL to parallel computing. This culminated in 2004 when it took its more complete form with the addition of the \"normalize-transpose\" (NT) semantic, which coincided with the major vendors of central processing units (CPUs) making a major shift to multi-core processors rather than continuing to increase clock speeds. NT is the semantic work-horse, being used to simplify and decompose structures, based on a dataflow-like execution strategy similar to GAMMA and NESL. The NT semantic achieves a goal similar to that of the Lämmel and Peyton-Jones’ boilerplate elimination. \nAll other features of the language are definable from these two laws - including recursion, subscripting structures, function references, and evaluation of function bodies.\n\nThough it was not the original intent, these new approaches allowed the language to parallelize a large fraction of the operations it performed, transparently to the programmer. In 2006, a prototype auto-parallelizing compiler was developed at Texas Tech University. In 2009, Texas Tech licensed the intellectual property to Texas Multicore Technologies (TMT), for follow-on commercial development. In January 2017 TMT released v3, which includes a free Community Edition for download in addition to the commercial Professional Edition.\n\nSequenceL is designed to be as simple as possible to learn and use, focusing on algorithmic code where it adds value, e.g., the inventors chose not to reinvent I/O since C handled that well. As a result, the full language reference for SequenceL is only 40 pages, with copious examples, and its formal grammar has around 15 production rules.\n\nSequenceL is strictly evaluated (like Lisp), statically typed with type inference (like Haskell), and uses a combination of infix and prefix operators that resemble standard, informal mathematical notation (like C, Pascal, Python, etc.). It is a purely declarative language, meaning that a programmer defines functions, in the mathematical sense, without giving instructions for their implementation. For example, the mathematical definition of matrix multiplication is as follows:\n\nThe SequenceL definition mirrors that definition more or less exactly:\n\nThe subscripts following each parameter \"A\" and \"B\" on the left hand side of the definition indicate that \"A\" and \"B\" are depth-2 structures (i.e., lists of lists of scalars), which are here thought of as matrices. From this formal definition, SequenceL infers the dimensions of the defined product from the formula for its (\"i\", \"j\")'th entry (as the set of pairs (\"i\", \"j\") for which the right hand side is defined) and computes each entry by the same formula as in the informal definition above. Notice there are no explicit instructions for iteration in this definition, or for the order in which operations are to be carried out. Because of this, the SequenceL compiler can perform operations in any order (including parallel order) which satisfies the defining equation. In this example, computation of coordinates in the product will be parallelized in a way that, for large matrices, scales linearly with the number of processors.\n\nAs noted above, SequenceL has no built-in constructs for input/output (I/O) since it was designed to work in an additive manner with other programming languages. The decision to compile to multithreaded C++ and support the 20+ Simplified Wrapper and Interface Generator (SWIG) languages (C, C++, C#, Java, Python, etc.) means it easily fits into extant design flows, training, and tools. It can be used to enhance extant applications, create multicore libraries, and even create standalone applications by linking the resulting code with other code which performs I/O tasks. SequenceL functions can also be queried from an interpreter with given inputs, like Python and other interpreted languages.\n\nThe main non-scalar construct of SequenceL is the sequence, which is essentially a list. Sequences may be nested to any level. To avoid the routine use of recursion common in many purely functional languages, SequenceL uses a technique termed \"normalize–transpose\" (NT), in which scalar operations are automatically distributed over elements of a sequence. For example, in SequenceL we have\nThis results not from overloading the '+' operator, but from the effect of NT that extends to all operations, both built-in and user-defined.\nAs another example, if f() is a 3-argument function whose arguments are scalars, then for any appropriate x and z we will have\nThe NT construct can be used for multiple arguments at once, as in, for example\nIt also works when the expected argument is a non-scalar of any type T, and the actual argument is a list of objects of type T (or, in greater generality, any data structure whose coordinates are of type T). For example, if A is a matrix and X is a list of matrices [X, ..., X], and given the above definition of matrix multiply, in SequenceL we would have\n\nAs a rule, NTs eliminate the need for iteration, recursion, or high level functional operators to \nThis tends to account for most uses of iteration and recursion.\n\nA good example that demonstrates the above concepts would be in finding prime numbers. A prime number is defined as\n\nSo a positive integer \"z\" is prime if no numbers from 2 through \"z\"-1, inclusive, divide evenly. SequenceL allows this problem to be programmed by literally transcribing the above definition into the language.\n\nIn SequenceL, a sequence of the numbers from 2 through \"z\"-1, inclusive, is just (2...(\"z\"-1)), so a program to find all of the primes between 100 and 200 can be written:\n\nWhich, in English just says,\n\nIf that condition isn’t met, the function returns nothing. As a result, running this program yields\n\nThe string \"between 100 and 200\" doesn’t appear in the program. Rather, a programmer will typically pass that part in as the argument. Since the program expects a scalar as an argument, passing it a sequence of numbers instead will cause SequenceL to perform the operation on each member of the sequence automatically. Since the function returns empty for failing values, the result will be the input sequence, but filtered to return only those numbers that satisfy the criteria for primes:\n\nIn addition to solving this problem with a very short and readable program, SequenceL’s evaluation of the nested sequences would all be performed in parallel.\n\nThe following software components are available and supported by TMT for use in writing SequenceL code. All components are available on x86 platforms running Windows, macOS, and most varieties of Linux (including CentOS, RedHat, OpenSUSE, and Ubuntu), and on ARM and IBM POWER platforms running most varieties of Linux.\n\nA command-line interpreter allows writing code directly into a command shell, or loading code from prewritten text files. This code can be executed, and the results evaluated, to assist in checking code correctness, or finding a quick answer. It is also available via the popular Eclipse integrated development environment (IDE). Code executed in the interpreter does not run in parallel; it executes in one thread.\n\nA command-line compiler reads SequenceL code and generates highly parallelized, vectorized, C++, and optionally OpenCL, which must be linked with the SequenceL runtime library to execute.\n\nThe runtime environment is a pre-compiled set of libraries which works with the compiled parallelized C++ code to execute optimally on the target platform. It builds on Intel Threaded Building Blocks (TBB) and handles things such as cache optimization, memory management, work queues-stealing, and performance monitoring.\n\nAn Eclipse integrated development environment plug-in provides standard editing abilities (function rollup, chromacoding, etc.), and a SequenceL debugging environment. This plug-in runs against the SequenceL Interpreter, so cannot be used to debug the multithreaded code; however, by providing automatic parallelization, debugging of parallel SequenceL code is really verifying correctness of sequential SequenceL code. That is, if it runs correctly sequentially, it should run correctly in parallel – so debugging in the interpreter is sufficient.\n\nVarious math and other standard function libraries are included as SequenceL source code to streamline the programming process and serve as best practice examples. These may be imported, in much the same way that C or C++ libraries are #included.\n\n\n"}
{"id": "46248976", "url": "https://en.wikipedia.org/wiki?curid=46248976", "title": "Sum of two squares theorem", "text": "Sum of two squares theorem\n\nIn number theory, the sum of two squares theorem says when an integer can be written as a sum of two squares, that is, when for some integers , .\n\nThis theorem supplements Fermat's two-square theorem which says when a prime number can be written as a sum of two squares, in that it also covers the case for composite numbers.\n\nThe prime decomposition of the number 2450 is given by 2450 = 257. Of the primes occurring in this decomposition, 2, 5, and 7, only 7 is congruent to 3 modulo 4. Its exponent in the decomposition, 2, is even. Therefore, the theorem states, it is expressible as the sum of two squares. Indeed, .\n\nThe prime decomposition of the number 3430 is 257. This time, the exponent of 7 in the decomposition is 3, an odd number. So 3430 cannot be written as the sum of two squares.\n\n"}
{"id": "54960916", "url": "https://en.wikipedia.org/wiki?curid=54960916", "title": "Tardos function", "text": "Tardos function\n\nIn graph theory and circuit complexity, the Tardos function is a graph invariant introduced by Éva Tardos in 1988 that has the following properties:\n\nTo define her function, Tardos uses a polynomial-time approximation scheme for the Lovász number, based on the ellipsoid method and provided by . Approximating the Lovász number of the complement and then rounding the approximation to an integer would not necessarily produce a monotone function, however. To make the result monotone, Tardos approximates the Lovász number of the complement to within an additive error of formula_1, adds formula_2 to the approximation, and then rounds the result to the nearest integer. Here formula_3 denotes the number of edges in the given graph, and formula_4 denotes the number of vertices.\n\nTardos used her function to prove an exponential separation between the capabilities of monotone Boolean logic circuits and arbitrary circuits.\nA result of Alexander Razborov, previously used to show that the clique number required exponentially large monotone circuits, also shows that the Tardos function requires exponentially large monotone circuits despite being computable by a non-monotone circuit of polynomial size.\nLater, the same function was used to provide a counterexample to a purported proof of P ≠ NP by Norbert Blum.\n"}
{"id": "261272", "url": "https://en.wikipedia.org/wiki?curid=261272", "title": "The Method of Mechanical Theorems", "text": "The Method of Mechanical Theorems\n\nThe Method of Mechanical Theorems (), also referred to as The Method, is considered one of the major surviving works of the ancient Greek polymath Archimedes. \"The Method\" takes the form of a letter from Archimedes to Eratosthenes, the chief librarian at the Library of Alexandria, and contains the first attested explicit use of \"indivisibles\" (sometimes referred to as infinitesimals). The work was originally thought to be lost, but in 1906 was rediscovered in the celebrated Archimedes Palimpsest. The palimpsest includes Archimedes' account of the \"mechanical method\", so-called because it relies on the law of the lever, which was first demonstrated by Archimedes, and of the center of mass (or centroid), which he had found for many special shapes.\n\nArchimedes did not admit the method of indivisibles as part of rigorous mathematics, and therefore did not publish his method in the formal treatises that contain the results. In these treatises, he proves the same theorems by exhaustion, finding rigorous upper and lower bounds which both converge to the answer required. Nevertheless, the mechanical method was what he used to discover the relations for which he later gave rigorous proofs.\n\nTo explain Archimedes' method today, it is convenient to make use of a little bit of Cartesian geometry, although this of course was unavailable at the time. His idea is to use the law of the lever to determine the areas of figures from the known center of mass of other figures. The simplest example in modern language is the area of the parabola. Archimedes uses a more elegant method, but in Cartesian language, his method is calculating the integral\n\nwhich can easily be checked nowadays using elementary integral calculus.\n\nThe idea is to mechanically balance the parabola (the curved region being integrated above) with a certain triangle that is made of the same material. The parabola is the region in the \"x\"-\"y\" plane between the \"x\"-axis and \"y\" = \"x\" as \"x\" varies from 0 to 1. The triangle is the region in the \"x\"-\"y\" plane between the \"x\"-axis and the line \"y\" = \"x\", also as \"x\" varies from 0 to 1.\n\nSlice the parabola and triangle into vertical slices, one for each value of \"x\". Imagine that the \"x\"-axis is a lever, with a fulcrum at \"x\" = 0. The law of the lever states that two objects on opposite sides of the fulcrum will balance if each has the same torque, where an object's torque equals its weight times its distance to the fulcrum. For each value of \"x\", the slice of the triangle at position x has a mass equal to its height \"x\", and is at a distance \"x\" from the fulcrum; so it would balance the corresponding slice of the parabola, of height \"x\", if the latter were moved to \"x\" = −1, at a distance of 1 on the other side of the fulcrum.\n\nSince each pair of slices balances, moving the whole parabola to \"x\" = −1 would balance the whole triangle. This means that if the original uncut parabola is hung by a hook from the point \"x\" = −1 (so that the whole mass of the parabola is attached to that point), it will balance the triangle sitting between \"x\" = 0 and \"x\" = 1.\n\nThe center of mass of a triangle can be easily found by the following method, also due to Archimedes. If a median line is drawn from any one of the vertices of a triangle to the opposite edge \"E\", the triangle will balance on the median, considered as a fulcrum. The reason is that if the triangle is divided into infinitesimal line segments parallel to \"E\", each segment has equal length on opposite sides of the median, so balance follows by symmetry. This argument can be easily made rigorous by exhaustion by using little rectangles instead of infinitesimal lines, and this is what Archimedes does in On the Equilibrium of Planes.\n\nSo the center of mass of a triangle must be at the intersection point of the medians. For the triangle in question, one median is the line \"y\" = \"x\"/2, while a second median is the line \"y\" = 1 − \"x\". Solving these equations, we see that the intersection of these two medians is above the point \"x\" = 2/3, so that the total effect of the triangle on the lever is as if the total mass of the triangle were pushing down on (or hanging from) this point. The total torque exerted by the triangle is its area, 1/2, times the distance 2/3 of its center of mass from the fulcrum at \"x\" = 0. This torque of 1/3 balances the parabola, which is at a distance -1 from the fulcrum. Hence, the area of the parabola must be 1/3 to give it the opposite torque.\n\nThis type of method can be used to find the area of an arbitrary section of a parabola, and similar arguments can be used to find the integral of any power of \"x\", although higher powers become complicated without algebra. Archimedes only went as far as the integral of \"x\", which he used to find the center of mass of a hemisphere, and in other work, the center of mass of a parabola.\n\nConsider the parabola in the figure to the right. Pick two points on the parabola and call them \"A\" and \"B\".\n\nSuppose the line segment \"AC\" is parallel to the axis of symmetry of the parabola. Further suppose that the line segment \"BC\" lies on a line that is tangent to the parabola at \"B\".\nThe first proposition states:\n\nLet \"D\" be the midpoint of \"AC\". Construct a line segment \"JB\" through \"D\", where the distance from \"J\" to \"D\" is equal to the distance from \"B\" to \"D\". We will think of the segment \"JB\" as a \"lever\" with \"D\" as its fulcrum. As Archimedes had previously shown, the center of mass of the triangle is at the point \"I\" on the \"lever\" where \"DI\" :\"DB\" = 1:3. Therefore, it suffices to show that if the whole weight of the interior of the triangle rests at \"I\", and the whole weight of the section of the parabola at \"J\", the lever is in equilibrium.\n\nConsider an infinitely small cross-section of the triangle given by the segment \"HE\", where the point \"H\" lies on \"BC\", the point \"E\" lies on \"AB\", and \"HE\" is parallel to the axis of symmetry of the parabola. Call the intersection of \"HE\" and the parabola \"F\" and the intersection of \"HE\" and the lever \"G\". If the whole weight of the triangle rests at \"I\", it exerts the same torque on the lever \"JB\" as it does on \"HE\". Thus, we wish to show that if the weight of the cross-section \"HE\" rests at \"G\" and the weight of the cross-section \"EF\" of the section of the parabola rests at \"J\", then the lever is in equilibrium. In other words, it suffices to show that \"EF\" :\"GD\" = \"EH\" :\"JD\". But that is a routine consequence of the equation of the parabola. Q.E.D.\n\nAgain, to illuminate the mechanical method, it is convenient to use a little bit of coordinate geometry. If a sphere of radius 1 is placed with its center at \"x\" = 1, the vertical cross sectional radius formula_2 at any \"x\" between 0 and 2 is given by the following formula:\n\nThe mass of this cross section, for purposes of balancing on a lever, is proportional to the area:\n\nArchimedes then considered rotating the triangular region between \"y\" = 0 and \"y\" = \"x\" and \"x\" = \"2\" on the \"x\"-\"y\" plane around the \"x\"-axis, to form a cone. The cross section of this cone is a circle of radius formula_5\n\nand the area of this cross section is\n\nSo if slices of the cone and the sphere \"both\" are to be weighed together, the combined cross-sectional area is:\n\nIf the two slices are placed together at distance 1 from the fulcrum, their total weight would be exactly balanced by a circle of area formula_9 at a distance \"x\" from the fulcrum on the other side. This means that the cone and the sphere together, if all their material were moved to \"x = 1\", would balance a cylinder of base radius 1 and length 2 on the other side.\n\nAs \"x\" ranges from 0 to 2, the cylinder will have a center of gravity a distance 1 from the fulcrum, so all the weight of the cylinder can be considered to be at position 1. The condition of balance ensures that the volume of the cone plus the volume of the sphere is equal to the volume of the cylinder.\n\nThe volume of the cylinder is the cross section area, formula_9 times the height, which is 2, or formula_11. Archimedes could also find the volume of the cone using the mechanical method, since, in modern terms, the integral involved is exactly the same as the one for area of the parabola. The volume of the cone is 1/3 its base area times the height. The base of the cone is a circle of radius 2, with area formula_11, while the height is 2, so the area is formula_13. Subtracting the volume of the cone from the volume of the cylinder gives the volume of the sphere:\n\nThe dependence of the volume of the sphere on the radius is obvious from scaling, although that also was not trivial to make rigorous back then. The method then gives the familiar formula for the volume of a sphere. By scaling the dimensions linearly Archimedes easily extended the volume result to spheroids.\n\nArchimedes argument is nearly identical to the argument above, but his cylinder had a bigger radius, so that the cone and the cylinder hung at a greater distance from the fulcrum. He considered this argument to be his greatest achievement, requesting that the accompanying figure of the balanced sphere, cone, and cylinder be engraved upon his tombstone.\n\nTo find the surface area of the sphere, Archimedes argued that just as the area of the circle could be thought of as infinitely many infinitesimal right triangles going around the circumference (see Measurement of the Circle), the volume of the sphere could be thought of as divided into many cones with height equal to the radius and base on the surface. The cones all have the same height, so their volume is 1/3 the base area times the height.\n\nArchimedes states that the total volume of the sphere is equal to the volume of a cone whose base has the same surface area as the sphere and whose height is the radius. There are no details given for the argument, but the obvious reason is that the cone can be divided into infinitesimal cones by splitting the base area up, and the each cone makes a contribution according to its base area, just the same as in the sphere.\n\nLet the surface of the sphere be \"S\". The volume of the cone with base area \"S\" and height \"r\" is formula_15, which must equal the volume of the sphere: formula_16. Therefore, the surface area of the sphere must be formula_17, or \"four times its largest circle\". Archimedes proves this rigorously in On the Sphere and Cylinder.\n\nOne of the remarkable things about the \"Method\" is that Archimedes finds two shapes defined by sections of cylinders, whose volume does not involve \"π\", despite the shapes having curvilinear boundaries. This is a central point of the investigation—certain curvilinear shapes could be rectified by ruler and compass, so that there are nontrivial rational relations between the volumes defined by the intersections of geometrical solids.\n\nArchimedes emphasizes this in the beginning of the treatise, and invites the reader to try to reproduce the results by some other method. Unlike the other examples, the volume of these shapes is not rigorously computed in any of his other works. From fragments in the palimpsest, it appears that Archimedes did inscribe and circumscribe shapes to prove rigorous bounds for the volume, although the details have not been preserved.\n\nThe two shapes he considers are the intersection of two cylinders at right angles, which is the region of (\"x\", \"y\", \"z\") obeying:\n\nand the circular prism, which is the region obeying:\n\nBoth problems have a slicing which produces an easy integral for the mechanical method. For the circular prism, cut up the \"x\"-axis into slices. The region in the \"y\"-\"z\" plane at any x is the interior of a right triangle of side length formula_20 whose area is formula_21, so that the total volume is:\n\nwhich can be easily rectified using the mechanical method. Adding to each triangular section a section of a triangular pyramid with area formula_23 balances a prism whose cross section is constant.\n\nFor the intersection of two cylinders, the slicing is lost in the manuscript, but it can be reconstructed in an obvious way in parallel to the rest of the document: if the x-z plane is the slice direction, the equations for the cylinder give that formula_24 while formula_25, which defines a region which is a square in the \"x\"-\"z\" plane of side length formula_26, so that the total volume is:\n\nAnd this is the same integral as for the previous example.\n\nA series of propositions of geometry are proved in the palimpsest by similar arguments. One theorem is that the location of a center of mass of a hemisphere is located 5/8 of the way from the pole to the center of the sphere. This problem is notable, because it is evaluating a cubic integral.\n\n\n"}
{"id": "1076615", "url": "https://en.wikipedia.org/wiki?curid=1076615", "title": "The Road to Reality", "text": "The Road to Reality\n\nThe Road to Reality: A Complete Guide to the Laws of the Universe is a book on modern physics by the British mathematical physicist Roger Penrose, published in 2004. It covers the basics of the Standard Model of particle physics, discussing general relativity and quantum mechanics, and discusses the possible unification of these two theories.\n\nThe book discusses the physical world. Many fields that 19th century scientists believed were separate, such as electricity and magnetism, are aspects of more fundamental properties. Some texts, both popular and university level, introduce these topics as separate concepts, and then reveal their combination much later. \"The Road to Reality\" reverses this process, first expounding the underlying mathematics of space–time, then showing how electromagnetism and other phenomena fall out fully formed.\n\nThe book is just over 1100 pages, of which the first 383 are dedicated to mathematics—Penrose's goal is to acquaint inquisitive readers with the mathematical tools needed to understand the remainder of the book in depth. Physics enters the discussion on page 383 with the topic of space–time. From there it moves on to fields in spacetime, deriving the classical electrical and magnetic forces from first principles; that is, if one lives in spacetime of a particular sort, these fields develop naturally as a consequence. Energy and conservation laws appear in the discussion of Lagrangians and Hamiltonians, before moving on to a full discussion of quantum physics, particle theory and quantum field theory. A discussion of the measurement problem in quantum mechanics is given a full chapter; superstrings are given a chapter near the end of the book, as are loop gravity and twistor theory. The book ends with an exploration of other theories and possible ways forward.\n\nThe final chapters reflect Penrose's personal perspective, which differs in some respects from what he regards as the current fashion among theoretical physicists. He is skeptical about string theory, to which he prefers loop quantum gravity. He is optimistic about his own approach, twistor theory. He also holds some controversial views about the role of consciousness in physics, as laid out in his earlier books (see \"Shadows of the Mind\").\n\n\n"}
{"id": "2431128", "url": "https://en.wikipedia.org/wiki?curid=2431128", "title": "Theorem on friends and strangers", "text": "Theorem on friends and strangers\n\nThe theorem on friends and strangers is a mathematical theorem in an area of mathematics called Ramsey theory.\n\nSuppose a party has six people. Consider any two of them. They might be meeting for the first time—in which case we will call them mutual strangers; or they might have met before—in which case we will call them mutual acquaintances. The theorem says:\n\nA proof of the theorem requires nothing but a three-step logic. It is convenient to phrase the problem in graph-theoretic language. \n\nSuppose a graph has 6 vertices and every pair of (distinct) vertices is joined by an edge. Such a graph is called a complete graph (because there cannot be any more edges). A complete graph on formula_1 vertices is denoted by the symbol formula_2.\n\nNow take a formula_3. It has 15 edges in all. Let the 6 vertices stand for the 6 people in our party. Let the edges be coloured red or blue depending on whether the two people represented by the vertices connected by the edge are mutual strangers or mutual acquaintances, respectively. The theorem now asserts:\n\nChoose any one vertex; call it \"P\". There are five edges leaving \"P\". They are each coloured red or blue. The pigeonhole principle says that at least three of them must be of the same colour; for if there are less than three of one colour, say red, then there are at least three that are blue.\n\nLet \"A\", \"B\", \"C\" be the other ends of these three edges, all of the same colour, say blue. If any one of \"AB\", \"BC\", \"CA\" is blue, then that edge together with the two edges from P to the edge's endpoints forms a blue triangle. If none of \"AB\", \"BC\", \"CA\" is blue, then all three edges are red and we have a red triangle, namely, \"ABC\".\n\nThe utter simplicity of this argument, which so powerfully produces a very interesting conclusion, is what makes the theorem appealing. In 1930, in a paper entitled 'On a Problem in Formal Logic,' Frank P. Ramsey proved a very general theorem (now known as Ramsey's theorem) of which this theorem is a simple case. This theorem of Ramsey forms the foundation of the area known as Ramsey theory in combinatorics.\n\nThe conclusion to the theorem does not hold if we replace the party of six people by a party of less than six. To show this, we give a coloring of \"K\" with red and blue that does not contain a triangle with all edges the same color. We draw \"K\" as a pentagon surrounding a star (a pentagram). We color the edges of the pentagon red and the edges of the star blue.\nThus, 6 is the smallest number for which we can claim the conclusion of the theorem. In Ramsey theory, we write this fact as:\n\n\n"}
{"id": "416598", "url": "https://en.wikipedia.org/wiki?curid=416598", "title": "Uncomfortable science", "text": "Uncomfortable science\n\nUncomfortable science, as identified by statistician John Tukey, comprises situations in which there is a need to draw an inference from a limited sample of data, where further samples influenced by the same cause system will not be available. More specifically, it involves the analysis of a finite natural phenomenon for which it is difficult to overcome the problem of using a common sample of data for both exploratory data analysis and confirmatory data analysis. This leads to the danger of systematic bias through testing hypotheses suggested by the data.\n\nA typical example is Bode's law, which provides a simple numerical rule for the distances of the planets in the solar system from the Sun. Once the rule has been derived, through the trial and error matching of various rules with the observed data (exploratory data analysis), there are not enough planets remaining for a rigorous and independent test of the hypothesis (confirmatory data analysis). We have exhausted the natural phenomena. The agreement between data and the numerical rule should be no surprise, as we have deliberately chosen the rule to match the data. If we are concerned about what Bode's law tells us about the cause system of planetary distribution then we demand confirmation that will not be available until better information about other planetary systems becomes available.\n\n"}
{"id": "1112432", "url": "https://en.wikipedia.org/wiki?curid=1112432", "title": "Unordered pair", "text": "Unordered pair\n\nIn mathematics, an unordered pair or pair set is a set of the form {\"a\", \"b\"}, i.e. a set having two elements \"a\" and \"b\" with no particular relation between them. In contrast, an ordered pair (\"a\", \"b\") has \"a\" as its first element and \"b\" as its second element. \n\nWhile the two elements of an ordered pair (\"a\", \"b\") need not be distinct, modern authors only call {\"a\", \"b\"} an unordered pair if \"a\" ≠ \"b\".\nBut for a few authors a singleton is also considered an unordered pair, although today, most would say that {a,a} is a multiset. It is typical to use the term unordered pair even in the situation where the elements a and b could be equal, as long as this equality has not yet been established.\n\nA set with precisely two elements is also called a 2-set or (rarely) a binary set.\n\nAn unordered pair is a finite set; its cardinality (number of elements) is 2 or (if the two elements are not distinct) 1.\n\nIn axiomatic set theory, the existence of unordered pairs is required by an axiom, the axiom of pairing.\n\nMore generally, an unordered n-tuple is a set of the form {\"a\", \"a\"... \"a\"}.\n"}
{"id": "48545958", "url": "https://en.wikipedia.org/wiki?curid=48545958", "title": "Waldegrave problem", "text": "Waldegrave problem\n\nIn probability and game theory, the Waldegrave problem refers to a problem first described in the second edition of Montmort`s \"Essay d'analyse sur les jeux de hazard\". This problem is remarkable in that it is the first appearance to a mixed strategy solution in game theory. Montmort originally called Waldegrave’s Problem the \"Problème de la Poulle\" or the Problem of the Pool. He provides a minimax mixed strategy solution to a two-person version of the card game le Her. It was Isaac Todhunter who called it Waldegrave’s Problem. The general description of the problem is as follows: Suppose there are n+1 players with each player putting one unit into the pot or pool. The first two players play each other and the winner plays the third player. The loser of each game puts one unit into the pot. Play continues in like fashion through all the players until one of the players has beaten all the others in succession. The original problem, stated in a letter dated 10 April, 1711, from Montmort to Nicholas Bernoulli is for n = 2 and is attributed to \"M. de Waldegrave\". The problem, according to Montmort, is to find the expectation of each player and the probability that the pool will be won within a specified number of games.\n"}
{"id": "420373", "url": "https://en.wikipedia.org/wiki?curid=420373", "title": "Π-calculus", "text": "Π-calculus\n\nIn theoretical computer science, the -calculus (or pi-calculus) is a process calculus. The -calculus allows channel names to be communicated along the channels themselves, and in this way it is able to describe concurrent computations whose network configuration may change during the computation.\n\nThe -calculus is simple, it has very few terms and so is a very small language , yet is very expressive. Functional programs can be encoded into the -calculus, and the encoding emphasises the dialogue nature of computation, drawing connections with game semantics. Extensions of the -calculus, such as the spi calculus and applied , have been successful in reasoning about cryptographic protocols. Beside the original use in describing concurrent systems, the -calculus has also been used to reason about business processes and molecular biology.\n\nThe -calculus belongs to the family of process calculi, mathematical formalisms for describing and analyzing properties of concurrent computation. In fact, the -calculus, like the λ-calculus, is so minimal that it does not contain primitives such as numbers, booleans, data structures, variables, functions, or even the usual control flow statements (such as codice_1, codice_2).\n\nCentral to the -calculus is the notion of \"name\". The simplicity of the calculus lies in the dual role that names play as \"communication channels\" and \"variables\".\n\nThe process constructs available in the calculus are the following (a precise definition is given in the following section):\n\n\nAlthough the minimalism of the -calculus prevents us from writing programs in the normal sense, it is easy to extend the calculus. In particular, it is easy to define both control structures such as recursion, loops and sequential composition and datatypes such as first-order functions, truth values, lists and integers. Moreover, extensions of the have been proposed which take into account distribution or public-key cryptography. The \"applied \" due to Abadi and Fournet put these various extensions on a formal footing by extending the with arbitrary datatypes.\n\nBelow is a tiny example of a process which consists of three parallel components. The channel name is only known by the first two components.\n\nThe first two components are able to communicate on the channel , and the name formula_7 becomes bound to formula_14. The next step in the process is therefore\n\nNote that the remaining formula_7 is not affected because it is defined in an inner scope.\nThe second and third parallel components can now communicate on the channel name formula_14, and the name formula_18 becomes bound to . The next step in the process is now\n\nNote that since the local name has been output, the scope of is extended to cover the third component as well. Finally, the channel can be used for sending the name . After that all concurrently executing processes have stopped\n\nLet Χ be a set of objects called \"names\". The abstract syntax for the -calculus is built from the following BNF grammar (where \"x\" and \"y\" are any names from Χ):\n\nIn the concrete syntax below, the prefixes bind more tightly than the parallel composition (|), and parentheses are used to disambiguate.\n\nNames are bound by the restriction and input prefix constructs. Formally, the set of free names of a process in –calculus are defined inductively by the table below. The set of bound names of a process are defined as the names of a process that are not in the set of free names.\n\nCentral to both the reduction semantics and the labelled transition semantics is the notion of structural congruence. Two processes are structurally congruent, if they are identical up to structure. In particular, parallel composition is commutative and associative.\n\nMore precisely, structural congruence is defined as the least equivalence relation preserved by the process constructs and satisfying:\n\n\"Alpha-conversion\":\n\n\"Axioms for parallel composition\":\n\n\"Axioms for restriction\":\n\n\"Axiom for replication\":\n\n\"Axiom relating restriction and parallel\":\n\nThis last axiom is known as the \"scope extension\" axiom. This axiom is central, since it describes how a bound name may be extruded by an output action, causing the scope of to be extended. In cases where is a free name of formula_3, alpha-conversion may be used to allow extension to proceed.\n\nWe write formula_35 if formula_2 can perform a computation step, following which it is now formula_37.\nThis \"reduction relation\" formula_38 is defined as the least relation closed under a set of reduction rules.\n\nThe main reduction rule which captures the ability of processes to communicate through channels is the following:\n\nThere are three additional rules:\n\nThe latter rule states that processes that are structurally congruent have the same reductions.\n\nConsider again the process\n\nApplying the definition of the reduction semantics, we get the reduction\n\nNote how, applying the reduction substitution axiom, free occurrences of formula_7 are now labeled as formula_14.\n\nNext, we get the reduction\n\nNote that since the local name has been output, the scope of is extended to cover the third component as well. This was captured using the scope extension axiom.\n\nNext, using the reduction substitution axiom, we get\n\nFinally, using the axioms for parallel composition and restriction, we get\n\nAlternatively, one may give the pi-calculus a labelled transition semantics (as has been done with the Calculus of Communicating Systems). Transitions in this semantics are of the form:\n\nThis notation signifies that formula_2 after the action formula_62 becomes formula_37. formula_62 can be an \"input action\" formula_65, an \"output action\" \"formula_66\", or a tau-action corresponding to an internal communication.\n\nA standard result about the labelled semantics is that it agrees with the reduction semantics in the sense that \nformula_35 if and only if \nformula_68 for some action τ.\n\nThe syntax given above is a minimal one. However, the syntax may be modified in various ways.\n\nA \"nondeterministic choice operator\" formula_69 can be added to the syntax.\n\nA test for \"name equality\" formula_70 can be added to the syntax. This \"match operator\" can proceed as formula_2 if and only if and formula_7 are the same name.\nSimilarly, one may add a \"mismatch operator\" for name inequality. Practical programs which can pass names (URLs or pointers) often use such functionality: for directly modelling such functionality inside the calculus, this and related extensions are often useful.\n\nThe \"asynchronous -calculus\"\nallows only outputs with no suffix, i.e. output atoms of the form formula_73, yielding a smaller calculus. However, any process in the original calculus can be represented by the smaller asynchronous -calculus using an extra channel to simulate explicit acknowledgement from the receiving process. Since a continuation-free output can model a message-in-transit, this fragment shows that the original -calculus, which is intuitively based on synchronous communication, has an expressive asynchronous communication model inside its syntax. However, the nondeterministic choice operator defined above cannot be expressed in this way, as an unguarded choice would be converted into a guarded one; this fact has been used to demonstrate that the asynchronous calculus is strictly less expressive than the synchronous one (with the choice operator).\n\nThe \"polyadic -calculus\" allows communicating more than one name in a single action: formula_74 \"(polyadic output)\" and formula_75 \"(polyadic input)\". This polyadic extension, which is useful especially when studying types for name passing processes, can be encoded in the monadic calculus by passing the name of a private channel through which the multiple arguments are then passed in sequence. The encoding is defined recursively by the clauses\n\nformula_76 is encoded as formula_77\n\nformula_78 is encoded as formula_79\n\nAll other process constructs are left unchanged by the encoding.\n\nIn the above, formula_80 denotes the encoding of all prefixes in the continuation formula_2 in the same way.\n\nThe full power of replication formula_82 is not needed. Often, one only considers \"replicated input\" formula_83, whose structural congruence axiom is formula_84.\n\nReplicated input process such as formula_85 can be understood as servers, waiting on channel\nthe process formula_86, where a is the name passed by the client to the\nserver, during the latter's invocation.\n\nA \"higher order -calculus\" can be defined where not only names but processes are sent through channels.\nThe key reduction rule for the higher order case is\n\nformula_87\n\nHere, formula_88 denotes a \"process variable\" which can be instantiated by a process term. Sangiorgi\nestablished that the ability to pass processes does not\nincrease the expressivity of the -calculus: passing a process \"P\" can be\nsimulated by just passing a name that points to \"P\" instead.\n\nThe -calculus is a universal model of computation. This was first observed by Milner in his paper \"Functions as Processes\", in which he presents two encodings of the lambda-calculus in the -calculus. One encoding simulates the eager (call-by-value) evaluation strategy, the other encoding simulates the normal-order (call-by-name) strategy. In both of these, the crucial insight is the modeling of environment bindings – for instance, \" is bound to term formula_89\" – as replicating agents that respond to requests for their bindings by sending back a connection to the term formula_89.\n\nThe features of the -calculus that make these encodings possible are name-passing and replication (or, equivalently, recursively defined agents). In the absence of replication/recursion, the -calculus ceases to be Turing-powerful. This can be seen by the fact that bisimulation equivalence becomes decidable for the recursion-free calculus and even for the finite-control -calculus where the number of parallel components in any process is bounded by a constant.\n\nAs for process calculi, the -calculus allows for a definition of bisimulation equivalence. In the -calculus, the definition of bisimulation equivalence (also known as bisimilarity) may be based on either the reduction semantics or on the labelled transition semantics.\n\nThere are (at least) three different ways of defining \"labelled bisimulation equivalence\" in the -calculus: Early, late and open bisimilarity. This stems from the fact that the -calculus is a value-passing process calculus.\n\nIn the remainder of this section, we let formula_91 and formula_92 denote processes and formula_93 denote binary relations over processes.\n\nEarly and late bisimilarity were both formulated by Milner, Parrow and Walker in their original paper on the -calculus.\n\nA binary relation formula_93 over processes is an \"early bisimulation\" if for every pair of processes formula_95,\n\nProcesses formula_91 and formula_92 are said to be early bisimilar, written formula_110 if the pair formula_111 for some early bisimulation formula_93.\n\nIn late bisimilarity, the transition match must be independent of the name being transmitted.\nA binary relation formula_93 over processes is a \"late bisimulation\" if for every pair of processes formula_95,\nProcesses formula_91 and formula_92 are said to be late bisimilar, written formula_128 if the pair formula_111 for some late bisimulation formula_93.\n\nBoth formula_131 and formula_132 suffer from the problem that they are not \"congruence relations\" in the sense that they are not preserved by all process constructs. More precisely, there exist processes formula_91 and formula_92 such that formula_110 but formula_136. One may remedy this problem by considering the maximal congruence relations included in formula_131 and formula_132, known as \"early congruence\" and \"late congruence\", respectively.\n\nFortunately, a third definition is possible, which avoids this problem, namely that of \"open bisimilarity\", due to Sangiorgi.\n\nA binary relation formula_93 over processes is an \"open bisimulation\" if for every pair of elements formula_95 and for every name substitution formula_141 and every action formula_62, whenever formula_143 then there exists some formula_98 such that formula_145 and formula_105.\n\nProcesses formula_91 and formula_92 are said to be open bisimilar, written formula_149 if the pair formula_111 for some open bisimulation formula_93.\n\nEarly, late and open bisimilarity are distinct. The containments are proper, so formula_152.\n\nIn certain subcalculi such as the asynchronous pi-calculus, late, early and open bisimilarity are known to coincide. However, in this setting a more appropriate notion is that of \"asynchronous bisimilarity\".\n\nThe reader should note that, in the literature, the term \"open bisimulation\" usually refers to a more sophisticated notion, where processes and relations are indexed by distinction relations; details are in Sangiorgi's paper cited above.\n\nAlternatively, one may define bisimulation equivalence directly from the reduction semantics. We write formula_153 if process formula_91 immediately allows an input or an output on name formula_155.\n\nA binary relation formula_93 over processes is a \"barbed bisimulation\" if it is a symmetric relation which satisfies that for every pair of elements formula_95 we have that\n\nand\n\nsuch that formula_105.\n\nWe say that formula_91 and formula_92 are \"barbed bisimilar\" if there exists a barbed bisimulation formula_93 where formula_111.\n\nDefining a context as a term with a hole [] we say that two processes P and Q are \"barbed congruent\", written formula_168, if for every context formula_169 we have that formula_170 and formula_171 are barbed bisimilar. It turns out that barbed congruence coincides with the congruence induced by early bisimilarity.\n\nThe -calculus has been used to describe many different kinds of concurrent systems. In fact, some of the most recent applications lie outside the realm of traditional computer science.\n\nIn 1997, Martin Abadi and Andrew Gordon proposed an extension of the -calculus, the Spi-calculus, as a formal notation for describing and reasoning about cryptographic protocols. The spi-calculus extends the -calculus with primitives for encryption and decryption. In 2001, Martin Abadi and Cedric Fournet generalised the handling of cryptographic protocols to produce the applied calculus. There is now a large body of work devoted to variants of the applied calculus, including a number of experimental verification tools. One example is the tool ProVerif due to Bruno Blanchet, based on a translation of the applied -calculus into Blanchet's logic programming framework. Another example is Cryptyc , due to Andrew Gordon and Alan Jeffrey, which uses Woo and Lam's method of correspondence assertions as the basis for type systems that can check for authentication properties of cryptographic protocols.\n\nAround 2002, Howard Smith and Peter Fingar became interested that -calculus would become a description tool for modelling business processes. By July 2006, there is discussion in the community about how useful this would be. Most recently, the -calculus has formed the theoretical basis of Business Process Modeling Language (BPML), and of Microsoft's XLANG.\n\nThe -calculus has also attracted interest in molecular biology. In 1999, Aviv Regev and Ehud Shapiro showed that one can describe a cellular signaling pathway (the so-called RTK/MAPK cascade) and in particular the molecular \"lego\" which implements these tasks of communication in an extension of the -calculus. Following this seminal paper, other authors described the whole metabolic network of a minimal cell.\n\nThe -calculus was originally developed by Robin Milner, Joachim Parrow and David Walker in 1992, based on ideas by Uffe Engberg and Mogens Nielsen. It can be seen as a continuation of Milner's work on the process calculus CCS (Calculus of Communicating Systems). In his Turing lecture, Milner describes the development of the -calculus as an attempt to capture the uniformity of values and processes in actors.\n\nThe following programming languages are implementations either of the -calculus or of its variants:\n\n\n\n"}
