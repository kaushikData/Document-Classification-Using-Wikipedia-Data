{"id": "1107021", "url": "https://en.wikipedia.org/wiki?curid=1107021", "title": "Accumulation function", "text": "Accumulation function\n\nThe accumulation function \"a\"(\"t\") is a function defined in terms of time \"t\" expressing the ratio of the value at time \"t\" (future value) and the initial investment (present value). It is used in interest theory. \n\nThus \"a\"(0)=1 and the value at time \"t\" is given by:\n\nwhere the initial investment is formula_2\n\nFor various interest-accumulation protocols, the accumulation function is as follows (with \"i\" denoting the interest rate and \"d\" denoting the discount rate):\n\nIn the case of a positive rate of return, as in the case of interest, the accumulation function is an increasing function.\n\nThe logarithmic or continuously compounded return, sometimes called force of interest, is a function of time defined as follows: \n\nwhich is the rate of change with time of the natural logarithm of the accumulation function.\n\nConversely:\n\nreducing to\n\nfor constant formula_10.\n\nThe effective annual percentage rate at any time is:\n\n"}
{"id": "37490344", "url": "https://en.wikipedia.org/wiki?curid=37490344", "title": "Alexandrov theorem", "text": "Alexandrov theorem\n\nIn mathematical analysis, the Alexandrov theorem, named after Aleksandr Danilovich Aleksandrov, states that if is an open subset of and    is a convex function, then has a second derivative almost everywhere.\n\nIn this context, having a second derivative at a point means having a second-order Taylor expansion at that point with a local error smaller than any quadratic.\n\nThe result is closely related to Rademacher's theorem.\n\n"}
{"id": "34505733", "url": "https://en.wikipedia.org/wiki?curid=34505733", "title": "András P Huhn", "text": "András P Huhn\n\nAndrás P Huhn (Szeged, 26 January 1947 – Szeged, 6 June 1985) was a Hungarian mathematician. Huhn's theorem on the representation of distributive semilattices is named after him.\n\n"}
{"id": "30639232", "url": "https://en.wikipedia.org/wiki?curid=30639232", "title": "Archive for Mathematical Logic", "text": "Archive for Mathematical Logic\n"}
{"id": "31605745", "url": "https://en.wikipedia.org/wiki?curid=31605745", "title": "Betweenness centrality", "text": "Betweenness centrality\n\nIn graph theory, betweenness centrality is a measure of centrality in a graph based on shortest paths. For every pair of vertices in a connected graph, there exists at least one shortest path between the vertices such that either the number of edges that the path passes through (for unweighted graphs) or the sum of the weights of the edges (for weighted graphs) is minimized. The betweenness centrality for each vertex is the number of these shortest paths that pass through the vertex.\n\nBetweenness centrality finds wide application in network theory: it represents the degree of which nodes stand between each other. For example, in a telecommunications network, a node with higher betweenness centrality would have more control over the network, because more information will pass through that node. Betweenness centrality was devised as a general measure of centrality: it applies to a wide range of problems in network theory, including problems related to social networks, biology, transport and scientific cooperation.\n\nAlthough earlier authors have intuitively described centrality as based on betweenness, gave the first formal definition of betweenness centrality.\n\nThe betweenness centrality of a node formula_1 is given by the expression:\n\nwhere formula_3 is the total number of shortest paths from node formula_4 to node formula_5 and formula_6 is the number of those paths that pass through formula_1.\n\nNote that the betweenness centrality of a node scales with the number of pairs of nodes as implied by the summation indices. Therefore, the calculation may be rescaled by dividing through by the number of pairs of nodes not including formula_1, so that formula_9. The division is done by formula_10 for directed graphs and formula_11 for undirected graphs, where formula_12 is the number of nodes in the giant component. Note that this scales for the highest possible value, where one node is crossed by every single shortest path. This is often not the case, and a normalization can be performed without a loss of precision\nwhich results in:\nNote that this will always be a scaling from a smaller range into a larger range, so no precision is lost.\n\nIn a weighted network the links connecting the nodes are no longer treated as binary interactions, but are weighted in proportion to their capacity, influence, frequency, etc., which adds another dimension of heterogeneity within the network beyond the topological effects. A node's strength in a weighted network is given by the sum of the weights of its adjacent edges.\n\nWith formula_17 and formula_18 being adjacency and weight matrices between nodes formula_19 and formula_20, respectively.\nAnalogous to the power law distribution of degree found in scale free networks, the strength of a given node follows a power law distribution as well.\n\nA study of the average value formula_22 of the strength for vertices with betweenness formula_23 shows that the functional behavior can be approximated by a scaling form \n\nPercolation Centrality is a version of weighted betweenness centrality, but it considers the 'state' of the source and target nodes of each shortest path in calculating this weight. Percolation of a ‘contagion’ occurs in complex networks in a number of scenarios. For example, viral or bacterial infection can spread over social networks of people, known as contact networks. The spread of disease can also be considered at a higher level of abstraction, by contemplating a network of towns or population centres, connected by road, rail or air links. Computer viruses can spread over computer networks. Rumours or news about business offers and deals can also spread via social networks of people. In all of these scenarios, a ‘contagion’ spreads over the links of a complex network, altering the ‘states’ of the nodes as it spreads, either recoverably or otherwise. For example, in an epidemiological scenario, individuals go from ‘susceptible’ to ‘infected’ state as the infection spreads. The states the individual nodes can take in the above examples could be binary (such as received/not received a piece of news), discrete (susceptible/infected/recovered), or even continuous (such as the proportion of infected people in a town), as the contagion spreads. The common feature in all these scenarios is that the spread of contagion results in the change of node states in networks. Percolation centrality (PC) was proposed with this in mind, which specifically measures the importance of nodes in terms of aiding the percolation through the network. This measure was proposed by Piraveenan et al.\n\nThe Percolation Centrality is defined for a given node, at a given time, as the proportion of ‘percolated paths’ that go through that node. A ‘percolated path’ is a shortest path between a pair of nodes, where the source node is percolated (e.g., infected). The target node can be percolated or non-percolated, or in a partially percolated state.\n"}
{"id": "11167471", "url": "https://en.wikipedia.org/wiki?curid=11167471", "title": "Boxicity", "text": "Boxicity\n\nIn graph theory, boxicity is a graph invariant, introduced by Fred S. Roberts in 1969.\n\nThe boxicity of a graph is the minimum dimension in which a given graph can be represented as an intersection graph of axis-parallel boxes. That is, there must exist a one-to-one correspondence between the vertices of the graph and a set of boxes, such that two boxes intersect if and only if there is an edge connecting the corresponding vertices.\n\nThe figure shows a graph with six vertices, and a representation of this graph as an intersection graph of rectangles (two-dimensional boxes). This graph cannot be represented as an intersection graph of boxes in any lower dimension, so its boxicity is two.\n\nA graph has boxicity at most one if and only if it is an interval graph; the boxicity of an arbitrary graph \"G\" is the minimum number of interval graphs on the same set of vertices such that the intersection of the edges sets of the interval graphs is \"G\". Every outerplanar graph has boxicity at most two, and every planar graph has boxicity at most three.\n\nIf a bipartite graph has boxicity two, it can be represented as an intersection graph of axis-parallel line segments in the plane.\n\nMany graph problems can be solved or approximated more efficiently for graphs with bounded boxicity than they can for other graphs; for instance, the maximum clique problem can be solved in polynomial time for graphs with bounded boxicity. For some other graph problems, an efficient solution or approximation can be found if a low-dimensional box representation is known. However, finding such a representation may be difficult:\nit is NP-complete to test whether the boxicity of a given graph is at most some given value \"K\", even for \"K\" = 2.\n\nDespite being hard for its natural parameter, boxicity is fixed-parameter tractable when parameterized by the vertex cover number of the input graph.\n\nLouis Esperet proved the following bound of the boxicity of \"G\" graph with \"m\" edges, which is asymptotically optimal:\nformula_1.\n\n\nLouis Esperet proved the following connection between the Colin de Verdière invariant and boxicity of the same graph:\nformula_2, and conjectured that the boxicity of \"G\" is at most the Colin de Verdière invariant of \"G\".\n\n"}
{"id": "3828249", "url": "https://en.wikipedia.org/wiki?curid=3828249", "title": "Calculator (macOS)", "text": "Calculator (macOS)\n\nCalculator is a basic calculator application made by Apple Inc. and bundled with macOS. It has three modes: basic, scientific, and programmer. Basic includes a number pad, buttons for adding, subtracting, multiplying, and dividing, as well as memory keys. Scientific mode supports exponents and trigonometric functions, and programmer mode gives the user access to more options related to computer programming.\n\nThe Calculator program has a long associated history with the beginning of the Macintosh platform, where a simple four-function calculator program was a standard desk accessory from the earliest system versions. Though no higher math capability was included, third-party developers provided upgrades, and Apple released the Graphing Calculator application with the first PowerPC release (7.1.2) of the Mac OS, and it was a standard component through Mac OS 9. Apple currently ships a different application called Grapher.\n\nCalculator has Reverse Polish notation support, and can also speak the buttons pressed and result returned.\n\nThe calculator also includes some basic conversion functions to convert between units in the following categories:\n\n\nCurrency exchange rates may be updated from the Internet.\n\nThe Calculator appeared first as a desk accessory in first version of Macintosh System for the 1984 Macintosh 128k. Its original incarnation was developed by Chris Espinosa and its appearance was designed, in part, by Steve Jobs when Espinosa, flustered by Jobs's dissatisfaction with all of his prototype designs, conceived an application called \"The Steve Jobs Roll Your Own Calculator Construction Set\" that allowed Jobs to tailor the look of the calculator to his liking. Its design was maintained with the same basic math operations until the final release of classic Mac OS in 2002.\n\nA Dashboard Calculator widget is included in all versions of macOS from Mac OS X Tiger onwards. It only has the basic mode of its desktop counterpart. Since the release of OS X Yosemite, there is also a simple calculator widget available in the notifications area.\n\nSince the release of Mac OS X Leopard, simple arithmetic functions can be calculated from Spotlight feature. They include the standard addition, subtraction, division, multiplication, exponentiation and the use of the percent sign to denote percentage.\n"}
{"id": "42245935", "url": "https://en.wikipedia.org/wiki?curid=42245935", "title": "Chromatic homotopy theory", "text": "Chromatic homotopy theory\n\nIn mathematics, chromatic homotopy theory is a subfield of stable homotopy theory that studies complex-oriented cohomology theories from the \"chromatic\" point of view, which is based on Quillen's work relating cohomology theories to formal groups. In this picture, theories are classified in terms of their \"chromatic levels\"; i.e., the heights of the formal groups that define the theories via the Landweber exact functor theorem. Typical theories it studies include: complex K-theory, elliptic cohomology, Morava K-theory and tmf.\n\n\n"}
{"id": "1828681", "url": "https://en.wikipedia.org/wiki?curid=1828681", "title": "Colatitude", "text": "Colatitude\n\nIn a spherical coordinate system, a colatitude is the complementary angle of a given latitude, i.e. the difference between 90° and the latitude. Southern latitudes are given a negative value and are thus denoted with a minus sign.\n\nThe colatitude corresponds to the conventional polar angle in spherical coordinates, as opposed to the latitude as used in cartography.\n\nLatitude and colatitude sum up to 90°.\n\nThe colatitude is most \nuseful in astronomy because it refers to the zenith distance of the celestial poles. For example, at latitude 42°N, Polaris (approximately on the North celestial pole) has an altitude of 42°, so the distance from the zenith (overhead point) to Polaris is .\n\nAdding the declination of a star to the observer's colatitude gives the maximum latitude of that star (its angle from the horizon at culmination or upper transit). For example, if Alpha Centauri is seen with a latitude of 72° north (108° south) and its declination is known (60°S), then it can be determined that the observer's colatitude is (i.e. their latitude is ).\n\nStars whose declinations exceed the observer's colatitude are called circumpolar because they will never set as seen from that latitude. If an object's declination is further south on the celestial sphere than the value of the colatitude, then it will never be seen from that location. For example, Alpha Centauri will always be visible at night from Perth, Western Australia because the colatitude is , and 60° is greater than 58°; on the other hand, the star will never rise in Juneau because its declination of −60° is less than −32° (the negation of Juneau's colatitude). Additionally, colatitude is used as part of the Schwarzschild metric in general relativity.\n"}
{"id": "3280193", "url": "https://en.wikipedia.org/wiki?curid=3280193", "title": "Composant", "text": "Composant\n\nIn point-set topology, the composant of a point \"p\" in a continuum \"A\" is the union of all proper subcontinua of \"A\" that contain \"p\". If a continuum is indecomposable, then its composants are pairwise disjoint. The composants of a continuum are dense in that continuum.\n\n"}
{"id": "7522685", "url": "https://en.wikipedia.org/wiki?curid=7522685", "title": "Continuous group action", "text": "Continuous group action\n\nIn topology, a continuous group action on a topological space \"X\" is a group action of a topological group \"G\" that is continuous: i.e.,\nis a continuous map. Together with the group action, \"X\" is called a \"G\"-space.\n\nIf formula_2 is a continuous group homomorphism of topological groups and if \"X\" is a \"G\"-space, then \"H\" can act on \"X\" \"by restriction\": formula_3, making \"X\" a \"H\"-space. Often \"f\" is either an inclusion or a quotient map. In particular, any topological space may be thought of as a \"G\"-space via formula_4 (and \"G\" would act trivially.)\n\nTwo basic operations are that of taking the space of points fixed by a subgroup \"H\" and that of forming a quotient by \"H\". We write formula_5 for the set of all \"x\" in \"X\" such that formula_6. For example, if we write formula_7 for the set of continuous maps from a \"G\"-space \"X\" to another \"G\"-space \"Y\", then, with the action formula_8,\nformula_9 consists of \"f\" such that formula_10; i.e., \"f\" is an equivariant map. We write formula_11. Note, for example, for a \"G\"-space \"X\" and a closed subgroup \"H\", formula_12.\n\n\n"}
{"id": "31735504", "url": "https://en.wikipedia.org/wiki?curid=31735504", "title": "Dissociation number", "text": "Dissociation number\n\nIn the mathematical discipline of graph theory, a subset of vertices in a graph \"G\" \nis called dissociation if it induces a subgraph with maximum degree 1. The number of vertices in a maximum cardinality dissociation set in \"G\" is called the dissociation number of \"G\", denoted by diss(\"G\"). The problem of computing diss(\"G\") (dissociation number problem) was firstly studied by Yannakakis. The problem is NP-hard even in the class of bipartite and planar graphs.\n\n"}
{"id": "22211457", "url": "https://en.wikipedia.org/wiki?curid=22211457", "title": "Erdős–Pósa theorem", "text": "Erdős–Pósa theorem\n\nIn the mathematical discipline of graph theory, the Erdős–Pósa theorem, named after Paul Erdős and Lajos Pósa, states that there is a function such that for each positive integer , every graph either contains vertex-disjoint circuits or it has a feedback vertex set of vertices that intersects every circuit. Furthermore, in the sense of Big O notation. Because of this theorem, circuits are said to \"have the Erdős–Pósa property\".\n\nThe theorem claims that for any finite number there is an appropriate (least) value , with the property that in every graph with no vertex-disjoint circuits all circuits can be covered by vertices. This generalized an unpublished result of Béla Bollobás, which states that . obtained the bounds for the general case. The result suggests that although there are infinitely many different graphs with no disjoint circuits, they split into finitely many simply describable classes. For the case , gave a complete characterization. proved and .\n\nA family of graphs or hypergraphs is defined to have the Erdős–Pósa property if there exists a function such that for every (hyper-)graph and every integer one of the following is true:\n\n\nThe definition is often phrased as follows. If one denotes by the maximum number of vertex disjoint subgraphs of isomorphic to a graph in and by the minimum number of vertices whose deletion from leaves a graph without a subgraph isomorphic to a graph in , then , for some function not depending on .\n\n"}
{"id": "9331", "url": "https://en.wikipedia.org/wiki?curid=9331", "title": "Euclid", "text": "Euclid\n\nEuclid (; – \"Eukleídēs\", ; fl. 300 BC), sometimes given the name Euclid of Alexandria to distinguish him from Euclides of Megara, was a Greek mathematician, often referred to as the \"founder of geometry\" or the \"father of geometry\". He was active in Alexandria during the reign of Ptolemy I (323–283 BC). His \"Elements\" is one of the most influential works in the history of mathematics, serving as the main textbook for teaching mathematics (especially geometry) from the time of its publication until the late 19th or early 20th century. In the \"Elements\", Euclid deduced the theorems of what is now called Euclidean geometry from a small set of axioms. Euclid also wrote works on perspective, conic sections, spherical geometry, number theory, and rigor.\n\nEuclid is the anglicized version of the Greek name Εὐκλείδης, which means \"renowned, glorious\".\n\nVery few original references to Euclid survive, so little is known about his life. He was likely born c. 325 BC, although the place and circumstances of both his birth and death are unknown and may only be estimated roughly relative to other people mentioned with him. He is rarely mentioned by name by other Greek mathematicians from Archimedes (c. 287 BC – c. 212 BC) onward, and is usually referred to as \"ὁ στοιχειώτης\" (\"the author of Elements\"). The few historical references to Euclid were written centuries after he lived, namely by Proclus c. 450 AD.\n\nA detailed biography of Euclid is given by Arabian authors, mentioning, for example, a birth town of Tyre. This biography is generally believed to be fictitious. If he came from Alexandria, he would have known the Serapeum of Alexandria, and the Library of Alexandria, and may have worked there during his time. Euclid's arrival in Alexandria came about ten years after its founding by Alexander the Great, which means he arrived c. 322 BC.\n\nProclus introduces Euclid only briefly in his \"Commentary on the Elements\". According to Proclus, Euclid supposedly belonged to Plato's \"persuasion\" and brought together the \"Elements\", drawing on prior work of Eudoxus of Cnidus and of several pupils of Plato (particularly Theaetetus and Philip of Opus.) Proclus believes that Euclid is not much younger than these, and that he must have lived during the time of Ptolemy I (c. 367 BC – 282 BC) because he was mentioned by Archimedes. Although the apparent citation of Euclid by Archimedes has been judged to be an interpolation by later editors of his works, it is still believed that Euclid wrote his works before Archimedes wrote his. Proclus later retells a story that, when Ptolemy I asked if there was a shorter path to learning geometry than Euclid's \"Elements\", \"Euclid replied there is no royal road to geometry.\" This anecdote is questionable since it is similar to a story told about Menaechmus and Alexander the Great.\nEuclid died c. 270 BC, presumably in Alexandria. In the only other key reference to Euclid, Pappus of Alexandria (c. 320 AD) briefly mentioned that Apollonius \"spent a very long time with the pupils of Euclid at Alexandria, and it was thus that he acquired such a scientific habit of thought\" c. 247–222 BC.\n\nBecause the lack of biographical information is unusual for the period (extensive biographies being available for most significant Greek mathematicians several centuries before and after Euclid), some researchers have proposed that Euclid was not a historical personage, and that his works were written by a team of mathematicians who took the name Euclid from Euclid of Megara (à la Bourbaki). However, this hypothesis is not well accepted by scholars and there is little evidence in its favor.\n\nAlthough many of the results in \"Elements\" originated with earlier mathematicians, one of Euclid's accomplishments was to present them in a single, logically coherent framework, making it easy to use and easy to reference, including a system of rigorous mathematical proofs that remains the basis of mathematics 23 centuries later.\n\nThere is no mention of Euclid in the earliest remaining copies of the \"Elements\", and most of the copies say they are \"from the edition of Theon\" or the \"lectures of Theon\", while the text considered to be primary, held by the Vatican, mentions no author. The only reference that historians rely on of Euclid having written the \"Elements\" was from Proclus, who briefly in his \"Commentary on the Elements\" ascribes Euclid as its author.\n\nAlthough best known for its geometric results, the \"Elements\" also includes number theory. It considers the connection between perfect numbers and Mersenne primes (known as the Euclid–Euler theorem), the infinitude of prime numbers, Euclid's lemma on factorization (which leads to the fundamental theorem of arithmetic on uniqueness of prime factorizations), and the Euclidean algorithm for finding the greatest common divisor of two numbers.\n\nThe geometrical system described in the \"Elements\" was long known simply as \"geometry\", and was considered to be the only geometry possible. Today, however, that system is often referred to as \"Euclidean geometry\" to distinguish it from other so-called \"non-Euclidean geometries\" that mathematicians discovered in the 19th century.\n\nThe Papyrus Oxyrhynchus 29 (P. Oxy. 29) is a fragment of the second book of the \"Elements\" of Euclid, unearthed by Grenfell and Hunt 1897 in Oxyrhynchus. More recent scholarship suggests a date of 75–125 AD.\n\nThe classic translation of T. L. Heath, reads:\n\nIn addition to the \"Elements\", at least five works of Euclid have survived to the present day. They follow the same logical structure as \"Elements\", with definitions and proved propositions.\n\nOther works are credibly attributed to Euclid, but have been lost.\n\nThe European Space Agency's (ESA) Euclid spacecraft was named in his honor.\n\n\n\n"}
{"id": "24925594", "url": "https://en.wikipedia.org/wiki?curid=24925594", "title": "Fermi–Pustyl'nikov model", "text": "Fermi–Pustyl'nikov model\n\nThe Fermi–Pustyl'nikov model, named after Enrico Fermi and Lev Pustyl'nikov, is a model of the Fermi acceleration mechanism.\n\nA point mass falls with a constant acceleration vertically on the infinitely heavy horizontal wall, which moves vertically in accordance with analytic periodic law in time. The point interacts with the wall by the law of elastic collision. For this model it was proved that under some general conditions the velocity and energy of the point at the moments of collisions with the wall tend to infinity for an open set of initial data having the infinite Lebesgue measure. This model was introduced in 1968 in, and studied in, by L. D. Pustyl'nikov in connection with justification of the Fermi acceleration mechanism.\n\n(See also and references therein).\n"}
{"id": "51159653", "url": "https://en.wikipedia.org/wiki?curid=51159653", "title": "Fibonacci word fractal", "text": "Fibonacci word fractal\n\nThe Fibonacci word fractal is a fractal curve defined on the plane from the Fibonacci word.\n\nThis curve is built iteratively by applying, to the Fibonacci word 0100101001001...etc., the Odd–Even Drawing rule:\n\nFor each digit at position \"k\" :\nTo a Fibonacci word of length formula_1 (the \"n\" Fibonacci number) is associated a curve formula_2 made of formula_1 segments. The curve displays three different aspects whether \"n\" is in the form 3\"k\", 3\"k\" + 1, or 3\"k\" + 2.\n\nFile:Fibonacci fractal self-similarities.png|Self-similarities at different scales.\nFile:FWF Dimensions.png|Dimensions.\nFile:Fibonacci fractal F21 & F20.png|Construction by juxtaposition (1)\nFile:Fibonacci Fractal F22 & F21.png|Construction by juxtaposition (2)\nFile:Fibonacci word fractalX.jpg|\nFile:FWF alternative construction.png|Construction by iterated suppression of square patterns.\nFile:FWF octogons.png|Construction by iterated octagons.\nFile:Fibonacci word gaskett.png|Construction by iterated collection of 8 square patterns around each square pattern.\nFile:Fibo 60deg F18.png|With a 60° angle.\nFile:Inverted Fibonacci fractal.png|Inversion of \"0\" and \"1\".\nFile:Fibonacci word fractal variants.png|Variants generated from the dense Fibonacci word. \nFile:Fibonacci word fractal compact variant.jpg|The \"compact variant\"\nFile:Fibonacci word fractal svastika variant.jpg|The \"svastika variant\"\nFile:Fibonacci word fractal diagonal variant.jpg|The \"diagonal variant\"\nFile:FWF PI8.png|The \"pi/8 variant\"\nFile:FWF Samuel Monnier.jpg|Artist creation (Samuel Monnier).\n</gallery>\n\nThe juxtaposition of four formula_23 curves allows the construction of a closed curve enclosing a surface whose area is not null. This curve is called a \"Fibonacci Tile\".\nThe Fibonacci snowflake is a Fibonacci tile defined by:\nwith formula_28 and formula_29, formula_30\"turn left\" et formula_31\"turn right\", and formula_32,\n\nSeveral remarkable properties ::\n\n\n"}
{"id": "23784721", "url": "https://en.wikipedia.org/wiki?curid=23784721", "title": "Frucht graph", "text": "Frucht graph\n\nIn the mathematical field of graph theory, the Frucht graph is a 3-regular graph with 12 vertices, 18 edges, and no nontrivial symmetries. It was first described by Robert Frucht in 1939.\n\nThe Frucht graph is a pancyclic Halin graph with chromatic number 3, chromatic index 3, radius 3, and diameter 4. As with every Halin graph, the Frucht graph is polyhedral (planar and 3-vertex-connected) and Hamiltonian, with girth 3. Its independence number is 5.\n\nThe Frucht graph can be constructed from the LCF notation: [−5,−2,−4,2,5,−2,2,5,−2,−5,4,2].\n\nThe Frucht graph is one of the two smallest cubic graphs possessing only a single graph automorphism, the identity (that is, every vertex can be distinguished topologically from every other vertex). Such graphs are called asymmetric (or identity) graphs. Frucht's theorem states that any group can be realized as the group of symmetries of a graph, and a strengthening of this theorem also due to Frucht states that any group can be realized as the symmetries of a 3-regular graph; the Frucht graph provides an example of this realization for the trivial group.\n\nThe characteristic polynomial of the Frucht graph is formula_1.\n"}
{"id": "23499524", "url": "https://en.wikipedia.org/wiki?curid=23499524", "title": "Hansen's problem", "text": "Hansen's problem\n\nHansen's problem is a problem in planar surveying, named after the astronomer Peter Andreas Hansen (1795–1874), who worked on the geodetic survey of Denmark. There are two known points \"A\" and \"B\", and two unknown points \"P\" and \"P\". From \"P\" and \"P\" an observer measures the angles made by the lines of sight to each of the other three points. The problem is to find the positions of \"P\" and \"P\". See figure; the angles measured are (\"α\", \"β\", \"α\", \"β\").\n\nSince it involves observations of angles made at unknown points, the problem is an example of resection (as opposed to intersection).\n\nDefine the following angles: \n\"γ\" = \"P\"\"AP\", \"δ\" = \"P\"\"BP\", \"φ\" = \"P\"\"AB\", \"ψ\" = \"P\"\"BA\".\nAs a first step we will solve for \"φ\" and \"ψ\".\nThe sum of these two unknown angles is equal to the sum of \"β\" and \"β\", yielding the equation\n\nA second equation can be found more laboriously, as follows. The law of sines yields\n\nCombining these, we get\n\nEntirely analogous reasoning on the other side yields\n\nSetting these two equal gives\n\nUsing a known trigonometric identity this ratio of sines can be expressed as the tangent of an angle difference:\n\nThis is the second equation we need. Once we solve the two equations for the two unknowns formula_8 and formula_9, we can use either of the two expressions above for formula_10 to find \"P\"\"P\" since \"AB\" is known. We can then find all the other segments using the law of sines.\n\nWe are given four angles (\"α\", \"β\", \"α\", \"β\") and the distance \"AB\". The calculation proceeds as follows:\n\n\n"}
{"id": "178048", "url": "https://en.wikipedia.org/wiki?curid=178048", "title": "Hari Seldon", "text": "Hari Seldon\n\nHari Seldon is a fictional character in Isaac Asimov's \"Foundation\" series. In his capacity as mathematics professor at Streeling University on the planet Trantor, Seldon develops psychohistory, an algorithmic science that allows him to predict the future in probabilistic terms. On the basis of his psychohistory he is able to predict the eventual fall of the Galactic Empire and to develop a means to shorten the millennia of chaos to follow. The significance of his discoveries lies behind his nickname \"Raven\" Seldon.\n\nIn the first five books of the \"Foundation\" series, Hari Seldon made only one in-the-flesh appearance, in the first part of the first book (\"Foundation\"), although he did appear other times in pre-recorded messages to reveal a Seldon Crisis. After writing five books in chronological order, Asimov went back with two books to better describe the initial process. The two prequels—\"Prelude to Foundation\" and \"Forward the Foundation\"—describe his life in considerable detail. He is also the central character of the Second Foundation Trilogy written after Asimov's death (\"Foundation's Fear\" by Gregory Benford, \"Foundation and Chaos\" by Greg Bear, and \"Foundation's Triumph\" by David Brin), which are set after Asimov's two prequels.\n\nGalactic Empire First Minister and psychohistorian Hari Seldon was born in the 10th month of the 11,988th year of the Galactic Era (GE) (-79 Foundation Era (FE)) and died 12,069 GE (1 FE).\n\nHe was born on the planet Helicon in the Arcturus sector where his father worked as a tobacco grower in a hydroponics plant.\n\nHe shows incredible mathematical abilities at a very early age. He also learns martial arts on Helicon that later help him on Trantor, the principal art being Heliconian Twisting (a form seemingly equal parts Jiu Jitsu, Krav Maga, and Submission Wrestling). Helicon is said to be \"less notable for its mathematics, and more for its martial arts\" (\"Prelude to Foundation\"). Seldon is awarded a Ph.D. in mathematics for his work on turbulence at the University of Helicon. There he becomes an assistant professor specializing in the mathematical analysis of social structures. \n\nSeldon is the subject of a biography by Gaal Dornick. Seldon is Emperor Cleon I's second and last First Minister, the first being Eto Demerzel/R. Daneel Olivaw. He is deposed as First Minister after Cleon I's assassination.\n\nUsing psychohistory, Seldon mathematically determines what he calls \"The Seldon Plan\"—a plan to determine the right time and place to set up a new society, one that would replace the collapsing Galactic Empire by sheer force of social pressure, but over only a thousand-year time span, rather than the ten-to-thirty-thousand-year time span that would normally have been required, and thus reduce the human suffering from living in a time of barbarism. The Foundation is placed on Terminus, a resource-poor planet entirely populated by scientists and their families. The planet—or so Seldon claimed—was originally occupied to create the \"Encyclopedia Galactica,\" a vast compilation of the knowledge of a dying galactic empire. In reality, Terminus had a much larger role in his Plan, which larger role he had to conceal from its inhabitants at first.\n\nSeldon visits Trantor to attend the Decennial Mathematics Convention. He presents a paper which indicates that one could theoretically predict the Galactic Empire's future. He is able to show that Galactic society can be represented in a simulation simpler than itself (in a finite number of iterations before the onset of chaotic noise smears discerning sets of events). He does so using a technique invented that past century. At first, Seldon has no idea how this could be done in practice, and he is fairly confident that no one could actually fulfill the possibility. Shortly after his presentation, he becomes a lightning rod for political forces who want to use psychohistory for their own purposes. The rest of the novel tells of his flight, which lasts for approximately a year and which takes him through the complex and variegated world of Trantor. During his flight to escape the various political factions, he discovers how psychohistory can be made a practical science. It is in this novel that he meets his future wife Dors Venabili, future adopted son Raych Seldon, and future partner Yugo Amaryl.\n\nThis novel is actually told as a sequence of short stories, just as was the case with the original trilogy. They take place at intervals a decade or more apart, and tell the story of Hari's life, starting about ten years after \"Prelude\" and ending with his death. The stories contrast his increasingly successful professional life with his increasingly unsuccessful personal life.\n\nSeldon becomes involved in politics when Eto Demerzel becomes a target for a smear campaign conducted by Laskin Joranum. He eventually takes Demerzel's place as First Minister, despite his reluctance to divide his attention between government and the development of Psychohistory. His career comes to an end when Cleon I is assassinated by his gardener (a random event Seldon could not have predicted) and the seizure of power by a military junta. Seldon eventually causes the fall of the junta by dropping subtle false hints about what Psychohistory foresees, leading to the Junta making unpopular decisions. However, an agent of the Junta inside Seldon's team, having deduced that Dors is a robot, builds a device that ultimately kills her, leaving Seldon heartbroken. Years later, Seldon discovers that his granddaughter Wanda has telepathic abilities and begins searching for others like her but fails. Raych eventually decides to move his family to the planet Santanni, but Wanda chooses to remain with her elderly grandfather. However, just after they arrive a rebellion breaks out on the planet and Raych is killed in the fighting. His wife and child are lost when their ship disappears. Seldon eventually finds Stettin Palver, another telepath who becomes Wanda's husband and the pair are eventually instrumental in creating the Second Foundation.\n\nSeldon mentions two indigenous species of Helicon: the lamec and the greti. The first is a hardworking animal, while the latter is dangerous as indicated by the native Helicon saying \"If you ride a greti, you find you can't get off; for then it will eat you.\" The saying is similar to the age-old Chinese proverb \"He who rides the tiger finds it difficult to dismount\" (騎虎難下), and the words \"lamec\" and \"greti\" are anagrams of \"camel\" and \"tiger\", respectively.\n\nIn his old age, he gains the nickname \"Raven\" for his dire predictions of the future.\n\nAlthough Hari Seldon was a fictional character, he has had an effect on contemporary thinkers. Historian Ian Morris has discussed the applicability and inspiration of Hari Seldon to statistics and prediction. Hari Seldon's name is cited in an article in \"The Economist\" discussing the use of statistics in epidemiology, the process through which societies change collective political thinking, and \"a general computer model of society.\" There is speculation in \"Forbes\" that Seldon's psychohistory is being manifested in today's emergence of Big Data. In fact, the fictional character of Seldon has even been labeled as a \"paradigmatic figure\" in Big Data research. Seldon is quite often named in research as a metaphorical literary reference point.\n\nPeople that give credit to Hari Seldon for career choices they made include economist and \"New York Times\" columnist Paul Krugman and Newt Gingrich.\n"}
{"id": "596556", "url": "https://en.wikipedia.org/wiki?curid=596556", "title": "Hartley transform", "text": "Hartley transform\n\nIn mathematics, the Hartley transform (HT) is an integral transform closely related to the Fourier transform (FT), but which transforms real-valued functions to real-valued functions. It was proposed as an alternative to the Fourier transform by Ralph V. L. Hartley in 1942, and is one of many known Fourier-related transforms. Compared to the Fourier transform, the Hartley transform has the advantages of transforming real functions to real functions (as opposed to requiring complex numbers) and of being its own inverse.\n\nThe discrete version of the transform, the discrete Hartley transform (DHT), was introduced by Ronald N. Bracewell in 1983.\n\nThe two-dimensional Hartley transform can be computed by an analog optical process similar to an optical Fourier transform (OFT), with the proposed advantage that only its amplitude and sign need to be determined rather than its complex phase. However, optical Hartley transforms do not seem to have seen widespread use.\n\nThe Hartley transform of a function \"f\"(\"t\") is defined by:\n\nwhere formula_2 can in applications be an angular frequency and\n\nis the cosine-and-sine or \"Hartley\" kernel. In engineering terms, this transform takes a signal (function) from the time-domain to the Hartley spectral domain (frequency domain).\n\nThe Hartley transform has the convenient property of being its own inverse (an involution):\n\nThe above is in accord with Hartley's original definition, but (as with the Fourier transform) various minor details are matters of convention and can be changed without altering the essential properties:\n\nThis transform differs from the classic Fourier transform \nformula_11 in the choice of the kernel. In the Fourier transform, we have the exponential kernel:\nformula_12\nwhere \"i\" is the imaginary unit.\n\nThe two transforms are closely related, however, and the Fourier transform (assuming it uses the same formula_13 normalization convention) can be computed from the Hartley transform via:\n\nThat is, the real and imaginary parts of the Fourier transform are simply given by the even and odd parts of the Hartley transform, respectively.\n\nConversely, for real-valued functions \"f\"(\"t\"), the Hartley transform is given from the Fourier transform's real and imaginary parts:\n\nwhere formula_16 and formula_17 denote the real and imaginary parts of the complex Fourier transform.\n\nThe Hartley transform is a real linear operator, and is symmetric (and Hermitian). From the symmetric and self-inverse properties, it follows that the transform is a unitary operator (indeed, orthogonal).\n\nThere is also an analogue of the convolution theorem for the Hartley transform. If two functions formula_18 and formula_19 have Hartley transforms formula_20 and formula_21, respectively, then their convolution formula_22 has the Hartley transform:\n\nSimilar to the Fourier transform, the Hartley transform of an even/odd function is even/odd, respectively.\n\nThe properties of the \"Hartley kernel\", for which Hartley introduced the name \"cas\" function (from \"cosine and sine\") in 1942, follow directly from trigonometry, and its definition as a phase-shifted trigonometric function formula_24. For example, it has an angle-addition identity of:\n\nAdditionally:\n\nand its derivative is given by:\n\n\n\n"}
{"id": "17919602", "url": "https://en.wikipedia.org/wiki?curid=17919602", "title": "Hierarchical and recursive queries in SQL", "text": "Hierarchical and recursive queries in SQL\n\nA hierarchical query is a type of SQL query that handles hierarchical model data. They are special cases of more general recursive fixpoint queries, which compute transitive closures.\n\nIn standard hierarchical queries are implemented by way of recursive \"common table expressions\" (CTEs). Unlike Oracle's earlier connect-by clause, recursive CTEs were designed with fixpoint semantics from the beginning. Recursive CTEs from the standard were relatively close to the existing implementation in IBM DB2 version 2. Recursive CTEs are also supported by Microsoft SQL Server (since SQL Server 2008 R2), Firebird 2.1, PostgreSQL 8.4+, SQLite 3.8.3+, IBM Informix version 11.50+, CUBRID and MySQL 8.0.1+. Tableau and TIBCO Spotfire do not support CTEs, while Oracle 11g Release 2's implementation lacks fixpoint semantics.\n\nWithout common table expressions or connected-by clauses it is possible to achieve hierarchical queries with user-defined recursive functions.\n\nA common table expression, or CTE, (in SQL) is a temporary named result set, derived from a simple query and defined within the execution scope of a codice_1, codice_2, codice_3, or codice_4 statement.\n\nCTEs can be thought of as alternatives to derived tables (subquery), views, and inline user-defined functions.\n\nCommon table expressions are supported by Teradata, DB2, Firebird, Microsoft SQL Server, Oracle (with recursion since 11g release 2), PostgreSQL (since 8.4), MariaDB (since 10.2), MySQL (since 8.0), SQLite (since 3.8.3), HyperSQL and H2 (experimental). Oracle calls CTEs \"subquery factoring\".\n\nThe syntax for a recursive CTE is as follows:\n\nwhere codice_5‘s syntax is:\n\nRecursive CTEs (or \"recursive subquery factoring\" in Oracle jargon) can be used to traverse relations (as graphs or trees) although the syntax is much more involved because there are no automatic pseudo-columns created (like codice_6 above); if these are desired, they have to be created in the code. See MSDN documentation or IBM documentation for tutorial examples.\n\nThe codice_7 keyword is not usually needed after WITH in systems other than PostgreSQL.\n\nIn SQL:1999 a recursive (CTE) query may appear anywhere a query is allowed. It's possible, for example, to name the result using codice_8 [codice_7] codice_10. Using a CTE inside an codice_11, one can populate a table with data generated from a recursive query; random data generation is possible using this technique without using any procedural statements.\n\nSome Databases, like PostgreSQL, support a shorter CREATE RECURSIVE VIEW format which is internally translated into WITH RECURSIVE coding.\n\nAn example of a recursive query computing the factorial of numbers from 0 to 9 is the following:\n\nAn alternative syntax is the non-standard codice_12 construct; it was introduced by Oracle in the 1980s. Prior to Oracle 10g, the construct was only useful for traversing acyclic graphs because it returned an error on detecting any cycles; in version 10g Oracle introduced the NOCYCLE feature (and keyword), making the traversal work in the presence of cycles as well.\n\ncodice_12 is supported by EnterpriseDB, Oracle database, CUBRID, IBM Informix and DB2 although only if it is enabled as a compatibility mode. The syntax is as follows:\n\n\nThe output from the above query would look like:\n\n\nThe following example returns the last name of each employee in department 10, each manager above that employee in the hierarchy, the number of levels between manager and employee, and the path between the two:\n\n\n\nAcademic textbooks. Note that these cover only the SQL:1999 standard (and Datalog), but not the Oracle extension.\n\n"}
{"id": "2928775", "url": "https://en.wikipedia.org/wiki?curid=2928775", "title": "Hofstadter's butterfly", "text": "Hofstadter's butterfly\n\nIn physics, Hofstadter's butterfly is a mathematical object describing the theorised behaviour of electrons in a magnetic field, discovered in 1976 by Douglas Hofstadter. It takes its name from its visual resemblance to a butterfly. It is a fractal structure and as such it shows self-similarity, meaning that small fragments of the structure contain a (distorted) copy of the entire structure. It is one of the rare non-random fractal structures discovered in physics, along with KAM theorem.\nA central concern in Hofstadter's paper is how the two distinct behaviours predicted for 'rational' and 'irrational' magnetic fields must be unified in the continuous physical world.\n\nTo test whether Hofstadter's butterfly describes real electron behaviour requires accurate measurements. Such were not possible when he wrote his paper. However, more recent experimental research has confirmed the characteristic butterfly shape.\n\nHofstadter described the structure in 1976 in an article on the energy levels of Bloch electrons in magnetic fields. It gives a graphical representation of the spectrum of the almost Mathieu operator for formula_1 at different frequencies. The intricate mathematical structure of this spectrum was discovered by Soviet physicist Mark Ya. Azbel' in 1964. However, Azbel' did not plot the structure as a geometrical object.\n\nWritten while Hofstadter was at the University of Oregon, his paper was influential in directing further research. Hofstadter predicted on theoretical grounds that the allowed energy level values of an electron in a two-dimensional square lattice, as a function of a magnetic field applied to the system, formed what is now known as a fractal set. That is, the distribution of energy levels for small scale changes in the applied magnetic field recursively repeat patterns seen in the large-scale structure. \"Gplot\", as Hofstadter called the figure, was described as a recursive structure in his 1976 article in \"Physical Review B\", written before Benoit Mandelbrot's newly coined word \"fractal\" was introduced in an English text. Hofstadter also discusses the figure in his 1979 book \"Gödel, Escher, Bach\". The structure became generally known as \"Hofstadter's butterfly\".\n\nIn 1997 the Hofstadter butterfly was reproduced in experiments with microwave guide equipped by an array of scatterers. Similarity of the mathematical description of the microwave guide with scatterers and of Bloch's waves in magnetic field allowed to reproduce the Hofstadter butterfly for periodic sequences of the scatterers.\n\nIn 2013, three separate groups of researchers independently reported evidence of the Hofstadter butterfly spectrum in graphene devices fabricated on hexagonal boron nitride substrates. In this instance the butterfly spectrum results from interplay between the applied magnetic field, and the large scale moiré pattern that develops when the graphene lattice is oriented with near zero-angle mismatch to the boron nitride.\n\nIn September 2017, John Martinis group in Google, in collaboration with Angelakis group in CQT Singapore, published results from a simulation of 2D electrons in a magnetic field using interacting photons in 9 superconducting qubits. The simulation recovered Hofstadter's butterfly, as expected.\n"}
{"id": "318742", "url": "https://en.wikipedia.org/wiki?curid=318742", "title": "Hyperbolic space", "text": "Hyperbolic space\n\nIn mathematics, hyperbolic space is a homogeneous space that has a constant negative curvature, where in this case the curvature is the sectional curvature.\nIt is hyperbolic geometry in more than 2 dimensions, and is distinguished from Euclidean spaces with zero curvature that define the Euclidean geometry, and elliptic geometry that have a constant positive curvature.\n\nWhen embedded to a Euclidean space (of a higher dimension), every point of a hyperbolic space is a saddle point. Another distinctive property is the amount of space covered by the \"n\"-ball in hyperbolic \"n\"-space: it increases exponentially with respect to the radius of the ball for large radii, rather than polynomially.\n\nHyperbolic \"n\"-space, denoted H, is the maximally symmetric, simply connected, \"n\"-dimensional Riemannian manifold with a constant negative sectional curvature. \nHyperbolic space is a space exhibiting hyperbolic geometry. \nIt is the negative-curvature analogue of the \"n\"-sphere. Although hyperbolic space H is diffeomorphic to R, its negative-curvature metric gives it very different geometric properties.\n\nHyperbolic 2-space, H, is also called the hyperbolic plane.\n\nHyperbolic space, developed independently by Nikolai Lobachevsky and János Bolyai, is a geometrical space analogous to Euclidean space, but such that Euclid's parallel postulate is no longer assumed to hold. Instead, the parallel postulate is replaced by the following alternative (in two dimensions):\nIt is then a theorem that there are infinitely many such lines through \"P\". This axiom still does not uniquely characterize the hyperbolic plane up to isometry; there is an extra constant, the curvature , which must be specified. However, it does uniquely characterize it up to homothety, meaning up to bijections which only change the notion of distance by an overall constant. By choosing an appropriate length scale, one can thus assume, without loss of generality, that .\n\nModels of hyperbolic spaces that can be embedded in a flat (e.g. Euclidean) spaces may be constructed. In particular, the existence of model spaces implies that the parallel postulate is logically independent of the other axioms of Euclidean geometry.\n\nThere are several important models of hyperbolic space: the Klein model, the hyperboloid model, the Poincaré ball model and the Poincaré half space model. These all model the same geometry in the sense that any two of them can be related by a transformation that preserves all the geometrical properties of the space, including isometry (though not with respect to the metric of a Euclidean embedding).\n\nThe hyperboloid model realizes hyperbolic space as a hyperboloid in R = {(\"x\"...,\"x\")|\"x\"∈R, \"i\"=0,1...,\"n\"}. The hyperboloid is the locus H of points whose coordinates satisfy\nIn this model a \"line\" (or geodesic) is the curve formed by the intersection of H with a plane through the origin in R.\n\nThe hyperboloid model is closely related to the geometry of Minkowski space. The quadratic form\nwhich defines the hyperboloid, polarizes to give the bilinear form\nThe space R, equipped with the bilinear form \"B\", is an (\"n\"+1)-dimensional Minkowski space R.\n\nOne can associate a \"distance\" on the hyperboloid model by defining the distance between two points \"x\" and \"y\" on H to be\nThis function satisfies the axioms of a metric space. It is preserved by the action of the Lorentz group on R. Hence the Lorentz group acts as a transformation group preserving isometry on H.\n\nAn alternative model of hyperbolic geometry is on a certain domain in projective space. The Minkowski quadratic form \"Q\" defines a subset given as the locus of points for which in the homogeneous coordinates \"x\". The domain \"U\" is the Klein model of hyperbolic space.\n\nThe lines of this model are the open line segments of the ambient projective space which lie in \"U\". The distance between two points \"x\" and \"y\" in \"U\" is defined by\nThis is well-defined on projective space, since the ratio under the inverse hyperbolic cosine is homogeneous of degree 0.\n\nThis model is related to the hyperboloid model as follows. Each point corresponds to a line \"L\" through the origin in R, by the definition of projective space. This line intersects the hyperboloid H in a unique point. Conversely, through any point on H, there passes a unique line through the origin (which is a point in the projective space). This correspondence defines a bijection between \"U\" and H. It is an isometry, since evaluating along reproduces the definition of the distance given for the hyperboloid model.\n\nA closely related pair of models of hyperbolic geometry are the Poincaré ball and Poincaré half-space models.\n\nThe ball model comes from a stereographic projection of the hyperboloid in R onto the hyperplane {\"x\" = 0}. In detail, let \"S\" be the point in R with coordinates (−1,0,0...,0): the \"South pole\" for the stereographic projection. For each point \"P\" on the hyperboloid H, let \"P\" be the unique point of intersection of the line \"SP\" with the plane {\"x\" = 0}.\n\nThis establishes a bijective mapping of H into the unit ball\nin the plane {\"x\" = 0}.\n\nThe geodesics in this model are semicircles that are perpendicular to the boundary sphere of \"B\". Isometries of the ball are generated by spherical inversion in hyperspheres perpendicular to the boundary.\n\nThe half-space model results from applying inversion in a circle with centre a boundary point of the Poincaré ball model \"B\" above and a radius of twice the radius.\n\nThis sends circles to circles and lines, and is moreover a conformal transformation. Consequently, the geodesics of the half-space model are lines and circles perpendicular to the boundary hyperplane.\n\nEvery complete, connected, simply connected manifold of constant negative curvature −1 is isometric to the real hyperbolic space H. As a result, the universal cover of any closed manifold \"M\" of constant negative curvature −1, which is to say, a hyperbolic manifold, is H. Thus, every such \"M\" can be written as H/Γ where Γ is a torsion-free discrete group of isometries on H. That is, Γ is a lattice in SO(\"n\",1).\n\nTwo-dimensional hyperbolic surfaces can also be understood according to the language of Riemann surfaces. According to the uniformization theorem, every Riemann surface is either elliptic, parabolic or hyperbolic. Most hyperbolic surfaces have a non-trivial fundamental group π=Γ; the groups that arise this way are known as Fuchsian groups. The quotient space H²/Γ of the upper half-plane modulo the fundamental group is known as the Fuchsian model of the hyperbolic surface. The Poincaré half plane is also hyperbolic, but is simply connected and noncompact. It is the universal cover of the other hyperbolic surfaces.\n\nThe analogous construction for three-dimensional hyperbolic surfaces is the Kleinian model.\n\n\n"}
{"id": "393736", "url": "https://en.wikipedia.org/wiki?curid=393736", "title": "Inductive reasoning", "text": "Inductive reasoning\n\nInductive reasoning is a method of reasoning in which the premises are viewed as supplying some evidence for the truth of the conclusion (in contrast to \"deductive\" reasoning and \"abductive\" reasoning). While the conclusion of a deductive argument is certain, the truth of the conclusion of an inductive argument may be \"probable\", based upon the evidence given.\n\nMany dictionaries define inductive reasoning as the derivation of general principles from specific observations, though some sources find this usage \"outdated\".\n\nUnlike deductive arguments, inductive reasoning allows for the possibility that the conclusion is false, even if all of the premises are true. Instead of being valid or invalid, inductive arguments are either \"strong\" or \"weak\", which describes how \"probable\" it is that the conclusion is true. Another crucial difference is that deductive certainty is impossible in non-axiomatic systems, such as reality, leaving inductive reasoning as the primary route to (probabilistic) knowledge of such systems.\n\nGiven that \"if \"A\" is true then that would cause \"B\", \"C\", and \"D\" to be true\", an example of deduction would be \"\"A\" is true therefore we can deduce that \"B\", \"C\", and \"D\" are true\". An example of induction would be \"\"B\", \"C\", and \"D\" are observed to be true therefore \"A\" might be true\". \"A\" is a reasonable explanation for \"B\", \"C\", and \"D\" being true.\n\nFor example:\nNote however that this is not necessarily the case. Other events with the potential to affect global climate also coincide with the extinction of the non-avian dinosaurs. For example, the release of volcanic gases (particularly sulfur dioxide) during the formation of the Deccan Traps in India.\n\nA classical example of an \"incorrect\" inductive argument was presented by John Vickers:\n\nThe correct conclusion would be, \"We expect that all swans are white\".\n\nThe definition of \"inductive\" reasoning described in this article excludes mathematical induction, which is a form of \"deductive\" reasoning that is used to strictly prove properties of recursively defined sets. The deductive nature of mathematical induction is based on the non-finite number of cases involved when using mathematical induction, in contrast with the finite number of cases involved in an enumerative induction procedure with a finite number of cases like proof by exhaustion. Both mathematical induction and proof by exhaustion are examples of complete induction. Complete induction is a type of masked deductive reasoning.\n\nAn argument is deductive when the conclusion is necessary given the premises. That is, the conclusion cannot be false if the premises are true.\n\nIf a deductive conclusion follows duly from its premises then it is valid; otherwise it is invalid (that an argument is invalid is not to say it is false. It may have a true conclusion, just not on account of the premises). An examination of the above examples will show that the relationship between premises and conclusion is such that the truth of the conclusion is already implicit in the premises. Bachelors are unmarried because we \"say\" they are; we have defined them so. Socrates is mortal because we have included him in a set of beings that are mortal.\n\nFor inductive reasoning the premises or prior data provide support for the conclusion, but they do not guarantee it. The result is a conclusion having, it is often said, a “degree of certainty.” The phrase is not optimal since certainty is absolute and does not come in degrees; what is really meant is degrees approaching certainty. Succinctly put: deduction is about certainty/necessity; induction is about \"probability\". This is the best way to understand and remember the difference between inductive vs. deductive reasoning. Any single assertion will answer to one of these two criteria. (There is also modal logic, which deals with the distinction between the necessary and the \"possible\" in a way not concerned with probabilities among things deemed possible.)\n\nThe philosophical definition of inductive reasoning is more nuanced than simple progression from particular/individual instances to broader generalizations. Rather, the premises of an inductive logical argument indicate some degree of support (inductive probability) for the conclusion but do not entail it; that is, they suggest truth but do not ensure it. In this manner, there is the possibility of moving from general statements to individual instances (for example, statistical syllogisms, discussed below).\n\nFor a move from particular to universal, Aristotle in the 300s BCE used the Greek word \"epagogé\", which Cicero translated into the Latin word \"inductio\". In the 300s CE, Sextus Empiricus maintained that all knowledge derives from sensory experience—concluded in his \"Outlines of Pyrrhonism\" that acceptance of universal statements as true cannot be justified by induction.\n\nIn 1620, early modern philosopher Francis Bacon repudiated mere experience and enumerative induction, and sought to couple those with neutral and minute and many varied observations before to uncover the natural world's structure and causal relations beyond the present scope of experience via his method of inductivism, which nonetheless required enumerative induction as a component.\n\nThe supposedly radical empiricist David Hume's 1740 stance found enumerative induction to have no rational, let alone logical, basis but to be a custom of the mind and an everyday requirement to live, although observations could be coupled with the principle uniformity of nature—another logically invalid conclusion, thus the problem of induction—to seemingly justify enumerative induction and reason toward unobservables, including causality counterfactually, simply that modifying such an aspect prevents or produces such outcome.\n\nAwakened from \"dogmatic slumber\" by a German translation of Hume's work, Kant sought to explain the possibility of metaphysics. In 1781, Kant's \"Critique of Pure Reason\" introduced the distinction \"rationalism\", a path toward knowledge distinct from \"empiricism\". Kant sorted statements into two types. The analytic are true by virtue of their terms' arrangement and meanings—thus are tautologies, merely logical truths, true by necessity—whereas the synthetic arrange meanings to refer to states of facts, contingencies. Finding it impossible to know objects as they truly are in themselves, however, Kant found the philosopher's task not peering behind the veil of appearance to view the \"noumena\", but simply handling \"phenomena\".\n\nReasoning that the mind must contain its own categories organizing sense data, making experience of \"space\" and \"time\" possible, Kant concluded uniformity of nature \"a priori\". A class of synthetic statements was not contingent but true by necessity, then, the synthetic \"a priori\". Kant thus saved both metaphysics and Newton's law of universal gravitation, but incidentally discarded scientific realism and developed transcendental idealism. Kant's transcendental idealism prompted the trend German idealism. G F W Hegel's absolute idealism flourished across continental Europe and fueled nationalism.\n\nDeveloped by Saint-Simon, and promulgated in the 1830s by his former student Comte was positivism, the first late modern philosophy of science. In the French Revolution's aftermath, fearing society's ruin again, Comte opposed metaphysics. Human knowledge had evolved from religion to metaphysics to science, said Comte, which had flowed from mathematics to astronomy to physics to chemistry to biology to sociology—in that order—describing increasingly intricate domains, all of society's knowledge having become scientific, as questions of theology and of metaphysics were unanswerable. Comte found enumerative induction reliable by its grounding on experience available, and asserted science's use as improving human society, not metaphysical truth.\n\nAccording to Comte, scientific method frames predictions, confirms them, and states laws—positive statements—irrefutable by theology or by metaphysics. Regarding experience to justify enumerative induction by having shown uniformity of nature, Mill welcomed Comte's positivism, but thought laws susceptible to recall or revision, and withheld from Comte's Religion of Humanity. Comte was confident to lay laws as irrefutable foundation of other knowledge, and the churches, honoring eminent scientists, sought to focus public mindset on \"altruism\"—a term Comte coined—to apply science for humankind's social welfare via Comte's spearheaded science, sociology.\n\nDuring the 1830s and 1840s, while Comte and Mill were the leading philosophers of science, William Whewell found enumerative induction not nearly so simple, but, amid the dominance of inductivism, described \"superinduction\". Whewell proposed recognition of \"the peculiar import of the term \"Induction\"\", as \"there is some Conception \"superinduced\" upon the facts\", that is, \"the Invention of a new Conception in every inductive inference\". Rarely spotted by Whewell's predecessors, such mental inventions rapidly evade notice. Whewell explained,\n\nThese \"superinduced\" explanations may well be flawed, but their accuracy is suggested when they exhibit what Whewell termed \"consilience\"—that is, simultaneously predicting the inductive generalizations in multiple areas—a feat that, according to Whewell, can establish their truth. Perhaps to accommodate prevailing view of science as inductivist method, Whewell devoted several chapters to \"methods of induction\" and sometimes said \"logic of induction\"—and yet stressed it lacks rules and cannot be trained.\n\nOriginator of pragmatism, C S Peirce who, as did Gottlob Frege independently, in the 1870s performed vast investigations that clarified the basis of deductive inference as mathematical proof, recognized induction but continuously insisted on a third type of inference that Peirce variously termed \"abduction\" or \"retroduction\" or \"hypothesis\" or \"presumption\". Later philosophers gave Peirce's abduction, \"etc\", the synonym \"inference to the best explanation\" (IBE).\n\nHaving highlighted Hume's problem of induction, John Maynard Keynes posed \"logical probability\" as its answer—but then figured not quite. Bertrand Russell found Keynes's \"Treatise on Probability\" the best examination of induction, and if read with Jean Nicod's \"Le Probleme logique de l'induction\" as well as R B Braithwaite's review of it in the October 1925 issue of \"Mind\", to provide \"most of what is known about induction\", although the \"subject is technical and difficult, involving a good deal of mathematics\". Two decades later, Russell proposed enumerative induction as an \"independent logical principle\". Russell found,\n\nIn a 1965 paper, Gilbert Harman explained that enumerative induction is not an autonomous phenomenon, but is simply a masked consequence of inference to the best explanation (IBE). IBE is otherwise synonym to C S Peirce's abduction. Many philosophers of science espousing scientific realism have maintained that IBE is the way that scientists develop approximately true scientific theories about nature.\n\nInductive reasoning has been criticized by thinkers as far back as Sextus Empiricus. The classic philosophical treatment of the problem of induction was given by the Scottish philosopher David Hume.\n\nAlthough the use of inductive reasoning demonstrates considerable success, its application has been questionable. Recognizing this, Hume highlighted the fact that our mind draws uncertain conclusions from relatively limited experiences. In deduction, the truth value of the conclusion is based on the truth of the premise. In induction, however, the dependence on the premise is always uncertain. As an example, let's assume \"all ravens are black.\" The fact that there are numerous black ravens supports the assumption. However, the assumption becomes inconsistent with the fact that there are white ravens. Therefore, the general rule of \"all ravens are black\" is inconsistent with the existence of the white raven. Hume further argued that it is impossible to justify inductive reasoning: specifically, that it cannot be justified deductively, so our only option is to justify it inductively. Since this is circular he concluded that our use of induction is unjustifiable with the help of Hume's Fork.\n\nHowever, Hume then stated that even if induction were proved unreliable, we would still have to rely on it. So instead of a position of severe skepticism, Hume advocated a practical skepticism based on common sense, where the inevitability of induction is accepted. Bertrand Russell illustrated his skepticism in a story about a turkey, fed every morning without fail, who following the laws of induction concludes this will continue, but then his throat is cut on Thanksgiving Day.\n\nKarl Popper. had declared in 1963, \"Induction, \"i.e.\" inference based on many observations, is a myth. It is neither a psychological fact, nor a fact of ordinary life, nor one of scientific procedure\". Popper's 1972 book \"Objective Knowledge\"—whose first chapter is devoted to the problem of induction—opens, \"I think I have solved a major philosophical problem: the problem of induction\". Within Popper's schema, enumerative induction is \"a kind of optical illusion\" cast by the steps of conjecture and refutation during the \"problem shift\". An imaginative leap, the \"tentative solution\" is improvised, lacking inductive rules to guide it. The resulting, unrestricted generalization is deductive, an entailed consequence of all, included explanatory considerations. Controversy continued, however, with Popper's putative solution not generally accepted.\n\nBy now, inductive inference has been shown to exist, but is found rarely, as in programs of machine learning in Artificial Intelligence (AI). Popper's stance on induction is strictly falsified—enumerative induction exists—but is overwhelmingly absent from science. Although much talked of nowadays by philosophers, abduction or IBE lacks rules of inference and the discussants provide nothing resembling such, as the process proceeds by humans' imaginations and perhaps creativity.\n\nInductive reasoning is also known as hypothesis construction because any conclusions made are based on current knowledge and predictions. As with deductive arguments, biases can distort the proper application of inductive argument, thereby preventing the reasoner from forming the most logical conclusion based on the clues. Examples of these biases include the availability heuristic, confirmation bias, and the predictable-world bias\n\nThe availability heuristic causes the reasoner to depend primarily upon information that is readily available to them. People have a tendency to rely on information that is easily accessible in the world around them. For example, in surveys, when people are asked to estimate the percentage of people who died from various causes, most respondents would choose the causes that have been most prevalent in the media such as terrorism, and murders, and airplane accidents rather than causes such as disease and traffic accidents, which have been technically \"less accessible\" to the individual since they are not emphasized as heavily in the world around them.\n\nThe confirmation bias is based on the natural tendency to confirm rather than to deny a current hypothesis. Research has demonstrated that people are inclined to seek solutions to problems that are more consistent with known hypotheses rather than attempt to refute those hypotheses. Often, in experiments, subjects will ask questions that seek answers that fit established hypotheses, thus confirming these hypotheses. For example, if it is hypothesized that Sally is a sociable individual, subjects will naturally seek to confirm the premise by asking questions that would produce answers confirming that Sally is in fact a sociable individual.\n\nThe predictable-world bias revolves around the inclination to perceive order where it has not been proved to exist, either at all or at a particular level of abstraction. Gambling, for example, is one of the most popular examples of predictable-world bias. Gamblers often begin to think that they see simple and obvious patterns in the outcomes and, therefore, believe that they are able to predict outcomes based upon what they have witnessed. In reality, however, the outcomes of these games are difficult to predict and highly complex in nature. However, in general, people tend to seek some type of simplistic order to explain or justify their beliefs and experiences, and it is often difficult for them to realise that their perceptions of order may be entirely different from the truth.\n\nThe following are types of inductive argument. Notice that while similar, each has a different form.\n\nIn contrast to the binary valid/invalid for deductive arguments, inductive arguments are rated in terms of strong or weak along a continuum. An inductive argument is strong in proportion to the probability that its conclusion is correct. We may call an inductive argument plausible, probable, reasonable, justified or strong, but never certain or necessary. Logic affords no bridge from the probable to the certain.\n\nThe futility of attaining certainty through some critical mass of probability can be illustrated with a coin-toss exercise. Suppose someone shows me a coin and says the coin is either a fair one or two-headed. He flips it ten times, and ten times it comes up heads. At this point there is strong reason to believe it is two-headed. After all, the chance of ten heads in a row is .000976 – less than one in one thousand. Then, after 100 flips, still every toss has come up heads. Now there is “virtual” certainty that the coin is two-headed. Still, one can neither logically or empirically rule out that the next toss will produce tails. No matter how many times in a row it comes up heads this remains the case. If one programed a machine to flip a coin over and over continuously, at some point the result would be a string of 100 heads. In the fullness of time all combinations will appear.\n\nAs for the slim prospect of getting ten out of ten heads from a fair coin - the outcome that made the coin appear biased - many may be surprised to learn that the chance of any combination of heads or tails is equally unlikely (e.g. H-H-T-T-H-T-H-H-H-T) – and yet it occurs in \"every\" trial of ten tosses. That means \"all\" results for ten tosses have the same probability as getting ten out of ten heads, which is .000976. If one records the heads-tails series, for whatever result, that exact series had a chance of .000976.\n\nThe conclusion for a valid deductive argument is already contained in the premises since because its truth is strictly a matter of logical relations. It cannot say more than its premises. Inductive premises, on the other hand, draw their substance from fact and evidence, and the conclusion accordingly makes a factual claim or prediction. Its reliability varies proportionally with the evidence. Induction wants to reveal something \"new\" about the world. One could say that induction wants to say \"more\" than is contained in the premises.\n\nTo better see the difference between inductive and deductive arguments, consider that it would not make sense to say, \"All rectangles so far examined have four right angles, so the next one I see will have four right angles.\" This would treat logical relations as something factual and discoverable, and thus variable and uncertain. Likewise, speaking deductively we may permissibly say. \"All unicorns can fly; I have a unicorn named Charlie; Charlie can fly.\" This deductive argument is valid because the logical relations hold; we are not interested in their factual soundness. A faulty inductive argument might take the form, \"All Swans so far observed were white, therefore it is settled that all swans white.\" This argument is a case of induction posing as deduction, and fails for the reasons discussed above. \n\nInductive reasoning is inherently uncertain. It only deals in degrees to which, given the premises, the conclusion is \"credible\" according to some theory of evidence. Examples include a many-valued logic, Dempster–Shafer theory, or probability theory with rules for inference such as Bayes' rule. Unlike deductive reasoning, it does not rely on universals holding over a closed domain of discourse to draw conclusions, so it can be applicable even in cases of epistemic uncertainty (technical issues with this may arise however; for example, the second axiom of probability is a closed-world assumption).\n\nAn example of an inductive argument:\n\nThis argument could have been made every time a new biological life form was found, and would have been correct every time; however, it is still possible that in the future a biological life form not requiring liquid water could be discovered.\n\nAs a result, the argument may be stated less formally as:\n\nA generalization (more accurately, an \"inductive generalization\") proceeds from a premise about a sample to a conclusion about the population.\n\nThere are 20 balls—either black or white—in an urn. To estimate their respective numbers, you draw a sample of four balls and find that three are black and one is white. A good inductive generalization would be that there are 15 black and five white balls in the urn.\n\nHow much the premises support the conclusion depends upon (a) the number in the sample group, (b) the number in the population, and (c) the degree to which the sample represents the population (which may be achieved by taking a random sample). The hasty generalization and the biased sample are generalization fallacies.\n\nThis is a \"Statistical\" , aka \"Sample Projection.\"\nThe measure is highly reliable within a well-defined margin of error provided the sample is large and random. It is readily quantifiable. Compare the preceding argument with the following. “Six of the ten people in my book club are Libertarians. About 60% of people are Libertarians.” The argument is weak because the sample is non-random and the sample size is very small\n\nThis is \"Inductive generalization\". This inference is less reliable that the Statistical Generalization, first, because the sample events are non-random, and because it is not reducible to mathematical expression. Statistically speaking, there is simply no way to know, measure and calculate as to the circumstances affecting performance that will obtain in the future. On a philosophical level, the argument relies on the presupposition that the operation of future events will mirror the past. In other words, it takes for granted a uniformity of nature, an unproven principle that cannot be derived from the empirical data itself. Arguments that tacitly presuppose this uniformity are sometimes called \"Humean\" after the philosopher who was first to subject them to philosophical scrutiny. \n\nA statistical syllogism proceeds from a generalization to a conclusion about an individual.\n\nThis is a \"Statistical Syllogism\". Even though one cannot be sure Bob will attend university we can be fully assured of the exact probability for this outcome (given no further information). Arguably the argument is too strong and might be accused of “cheating.” After all, the probability is given in the premise. Typically, Inductive reasoning seeks to \"formulate\" a probability. Two dicto simpliciter fallacies can occur in statistical syllogisms: \"accident\" and \"converse accident\".\n\nSimple induction proceeds from a premise about a sample group to a conclusion about another individual.\n\nThis is a combination of a generalization and a statistical syllogism, where the conclusion of the generalization is also the first premise of the statistical syllogism.\n\nThe basic form of inductive inference, simply \"induction\", reasons from particular instances to all instances, and is thus an unrestricted generalization. If one observes 100 swans, and all 100 were white, one might infer a universal categorical proposition of the form \"All swans are white\". As this reasoning form's premises, even if true, do not entail the conclusion's truth, this is a form of inductive inference. The conclusion might be true, and might be thought probably true, yet it can be false. Questions regarding the justification and form of enumerative inductions have been central in philosophy of science, as enumerative induction has a pivotal role in the traditional model of the scientific method.\n\nThis is \"Enumerative Induction\", aka \"Simple Induction\" or \"Simple Predictive Induction\". It is a subcategory of Inductive Generalization. In everyday practice this is perhaps the most common form of induction. For the preceding argument, the conclusion is tempting but makes a prediction well in excess of the evidence. First, it assumes that life forms observed until now can tell us how future cases will be – an appeal to uniformity. Second, the concluding \"All\" is a very bold assertion. A single contrary instance foils the argument. And last, to quantify the level of probability in any mathematical form is problematic. By what standard do we measure our earthly sample of known life against all (possible) life? For suppose we do discover some new organism - let’s say some microorganism floating in the Mesosphere, or better yet, on some asteroid - and it is cellular. Doesn't the addition of this corroborating evidence oblige us to raise our probability assessment for the subject proposition? It is generally deemed reasonable to answer this question, yes; and for a good many this “yes” is not only reasonable but incontrovertible. Very well; so then just \"how much\" should this new data change our probability assessment. Here, consensus melts away; and in its place arises a question about whether we can talk of probability coherently at all without numerical quantification. \n\nThis is Enumerative Induction in its \"weak form\". It truncates “all” to a mere single instance, and by making a far weaker claim considerably strengthens the probability of its conclusion. Otherwise, it has the same shortcomings as the strong form: its sample population is non-random, and quantification methods are elusive.\n\nThe process of analogical inference involves noting the shared properties of two or more things, and from this basis inferring that they also share some further property:\n\nAnalogical reasoning is very frequent in common sense, science, philosophy and the humanities, but sometimes it is accepted only as an auxiliary method. A refined approach is case-based reasoning.\n\nThis is \"analogical induction\", according to which things alike in certain ways are more prone to be alike in other ways. This form of induction was explored in detail my philosopher John Stewart Mill in his System of Logic, wherein he states,\n\nAnalogical induction is a subcategory of inductive generalization because it assumes a pre-established uniformity governing events. Analogical induction requires an auxiliary examination of the \"relevancy\" of the characteristics cited as common to the pair. In the preceding example, if I add the premise that both stones were mentioned in the records of early Spanish explorers, this common attribute is extraneous to the stones and does not contribute to their probable affinity. \n\nA pitfall of analogy is that features can be cherry-picked: While objects may show striking similarities, two things juxtaposed may respectively possess other characteristics not identified in the analogy that are characteristics sharply \"dis\"similar. Thus, analogy can mislead if not all relevant comparisons are made.\n\nA causal inference draws a conclusion about a causal connection based on the conditions of the occurrence of an effect. Premises about the correlation of two things can indicate a causal relationship between them, but additional factors must be confirmed to establish the exact form of the causal relationship.\n\nA prediction draws a conclusion about a future individual from a past sample.\n\nAs a logic of induction rather than a theory of belief, Bayesian inference does not determine which beliefs are \"a priori\" rational, but rather determines how we should rationally change the beliefs we have when presented with evidence. We begin by committing to a prior probability for a hypothesis based on logic or previous experience, and when faced with evidence, we adjust the strength of our belief in that hypothesis in a precise manner using Bayesian logic.\n\nAround 1960, Ray Solomonoff founded the theory of universal inductive inference, the theory of prediction based on observations; for example, predicting the next symbol based upon a given series of symbols. This is a formal inductive framework that combines algorithmic information theory with the Bayesian framework. Universal inductive inference is based on solid philosophical foundations, and can be considered as a mathematically formalized Occam's razor. Fundamental ingredients of the theory are the concepts of algorithmic probability and Kolmogorov complexity.\n\n\n"}
{"id": "291912", "url": "https://en.wikipedia.org/wiki?curid=291912", "title": "Introduction to gauge theory", "text": "Introduction to gauge theory\n\nA gauge theory is a type of theory in physics. The word gauge means a measurement, a thickness, an in-between distance, (as in railroad tracks) or a resulting number of units per certain parameter (a number of loops in an inch of fabric or a number of lead balls in a pound of ammunition). Modern theories describe physical forces in terms of fields, e.g., the electromagnetic field, the gravitational field, and fields that describe forces between the elementary particles. A general feature of these field theories is that the fundamental fields cannot be directly measured; however, some associated quantities can be measured, such as charges, energies, and velocities. For example, say you cannot measure the diameter of a lead ball, but you can determine how many lead balls, which are equal in every way, are required to make a pound. Using the number of balls, the elemental mass of lead, and the formula for calculating the volume of a sphere from its diameter, one could indirectly determine the diameter of a single lead ball. In field theories, different configurations of the unobservable fields can result in identical observable quantities. A transformation from one such field configuration to another is called a gauge transformation; the lack of change in the measurable quantities, despite the field being transformed, is a property called gauge invariance. For example, if you could measure the color of lead balls and discover that when you change the color, you still fit the same number of balls in a pound, the property of \"color\" would show gauge invariance. Since any kind of invariance under a field transformation is considered a symmetry, gauge invariance is sometimes called gauge symmetry. Generally, any theory that has the property of gauge invariance is considered a gauge theory.\n\nFor example, in electromagnetism the electric and magnetic fields, E and B are observable, while the potentials \"V\" (\"voltage\") and A (the vector potential) are not. Under a gauge transformation in which a constant is added to \"V\", no observable change occurs in E or B.\n\nWith the advent of quantum mechanics in the 1920s, and with successive advances in quantum field theory, the importance of gauge transformations has steadily grown. Gauge theories constrain the laws of physics, because all the changes induced by a gauge transformation have to cancel each other out when written in terms of observable quantities. Over the course of the 20th century, physicists gradually realized that all forces (fundamental interactions) arise from the constraints imposed by \"local\" gauge symmetries, in which case the transformations vary from point to point in space and time. Perturbative quantum field theory (usually employed for scattering theory) describes forces in terms of force-mediating particles called gauge bosons. The nature of these particles is determined by the nature of the gauge transformations. The culmination of these efforts is the Standard Model, a quantum field theory that accurately predicts all of the fundamental interactions except gravity.\n\nThe earliest field theory having a gauge symmetry was Maxwell's formulation, in 1864–65, of electrodynamics (\"A Dynamical Theory of the Electromagnetic Field\"). The importance of this symmetry remained unnoticed in the earliest formulations. Similarly unnoticed, Hilbert had derived Einstein's equations of general relativity by postulating a symmetry under any change of coordinates. Later Hermann Weyl, inspired by success in Einstein's general relativity, conjectured (incorrectly, as it turned out) in year 1919 that invariance under the change of scale or \"gauge\" (a term inspired by the various track gauges of railroads) might also be a local symmetry of electromagnetism. Although Weyl's choice of the gauge was incorrect, the name \"gauge\" stuck to the approach. After the development of quantum mechanics, Weyl, Fock and London modified their gauge choice by replacing the scale factor with a change of wave phase, and applying it successfully to electromagnetism. Gauge symmetry was generalized mathematically in 1954 by Chen Ning Yang and Robert Mills in an attempt to describe the strong nuclear forces. This idea, dubbed Yang–Mills theory, later found application in the quantum field theory of the weak force, and its unification with electromagnetism in the electroweak theory.\n\nThe importance of gauge theories for physics stems from their tremendous success in providing a unified framework to describe the quantum-mechanical behavior of electromagnetism, the weak force and the strong force. This gauge theory, known as the Standard Model, accurately describes experimental predictions regarding three of the four fundamental forces of nature.\n\nHistorically, the first example of gauge symmetry to be discovered was classical electromagnetism. A static electric field can be described in terms of an electric potential (voltage) that is defined at every point in space, and in practical work it is conventional to take the Earth as a physical reference that defines the zero level of the potential, or ground. But only \"differences\" in potential are physically measurable, which is the reason that a voltmeter must have two probes, and can only report the voltage difference between them. Thus one could choose to define all voltage differences relative to some other standard, rather than the Earth, resulting in the addition of a constant offset. If the potential formula_1 is a solution to Maxwell's equations then, after this gauge transformation, the new potential formula_2 is also a solution to Maxwell's equations and no experiment can distinguish between these two solutions. In other words, the laws of physics governing electricity and magnetism (that is, Maxwell equations) are invariant under gauge transformation. Maxwell's equations have a gauge symmetry.\n\nGeneralizing from static electricity to electromagnetism, we have a second potential, the magnetic vector potential A, which can also undergo gauge transformations. These transformations may be local. That is, rather than adding a constant onto \"V\", one can add a function that takes on different values at different points in space and time. If A is also changed in certain corresponding ways, then the same E and B fields result. The detailed mathematical relationship between the fields E and B and the potentials \"V\" and A is given in the article Gauge fixing, along with the precise statement of the nature of the gauge transformation. The relevant point here is that the fields remain the same under the gauge transformation, and therefore Maxwell's equations are still satisfied.\n\nGauge symmetry is closely related to charge conservation. Suppose that there existed some process by which one could briefly violate conservation of charge by creating a charge \"q\" at a certain point in space, 1, moving it to some other point 2, and then destroying it. We might imagine that this process was consistent with conservation of energy. We could posit a rule stating that creating the charge required an input of energy \"E\"=\"qV\" and destroying it released \"E\"=\"qV\", which would seem natural since \"qV\" measures the extra energy stored in the electric field because of the existence of a charge at a certain point. Outside of the interval during which the particle exists, conservation of energy would be satisfied, because the net energy released by creation and destruction of the particle, \"qV\"-\"qV\", would be equal to the work done in moving the particle from 1 to 2, \"qV\"-\"qV\". But although this scenario salvages conservation of energy, it violates gauge symmetry. Gauge symmetry requires that the laws of physics be invariant under the transformation formula_2, which implies that no experiment should be able to measure the absolute potential, without reference to some external standard such as an electrical ground. But the proposed rules \"E\"=\"qV\" and \"E\"=\"qV\" for the energies of creation and destruction \"would\" allow an experimenter to determine the absolute potential, simply by comparing the energy input required to create the charge \"q\" at a particular point in space in the case where the potential is formula_1 and formula_5 respectively. The conclusion is that if gauge symmetry holds, and energy is conserved, then charge must be conserved.\n\nAs discussed above, the gauge transformations for classical (i.e., non-quantum mechanical) general relativity are arbitrary coordinate transformations. Technically, the transformations must be invertible, and both the transformation and its inverse must be smooth, in the sense of being differentiable an arbitrary number of times.\n\nSome global symmetries under changes of coordinate predate both general relativity and the concept of a gauge. For example, Galileo and Newton introduced the notion of translation invariance, an advancement from the Aristotelian concept that different places in space, such as the earth versus the heavens, obeyed different physical rules.\n\nSuppose, for example, that one observer examines the properties of a hydrogen atom on Earth, the other—on the Moon (or any other place in the universe), the observer will find that their hydrogen atoms exhibit completely identical properties. Again, if one observer had examined a hydrogen atom today and the other—100 years ago (or any other time in the past or in the future), the two experiments would again produce completely identical results. The invariance of the properties of a hydrogen atom with respect to the time and place where these properties were investigated is called translation invariance.\n\nRecalling our two observers from different ages: the time in their experiments is shifted by 100 years. If the time when the older observer did the experiment was \"t\", the time of the modern experiment is \"t\"+100 years. Both observers discover the same laws of physics. Because light from hydrogen atoms in distant galaxies may reach the earth after having traveled across space for billions of years, in effect one can do such observations covering periods of time almost all the way back to the Big Bang, and they show that the laws of physics have always been the same.\n\nIn other words, if in the theory we change the time \"t\" to \"t\"+100 years (or indeed any other time shift) the theoretical predictions do not change.\n\nIn Einstein's general relativity, coordinates like \"x\", \"y\", \"z\", and \"t\" are not only \"relative\" in the global sense of translations like formula_6, rotations, etc., but become completely arbitrary, so that, for example, one can define an entirely new time-like coordinate according to some arbitrary rule such as formula_7, where formula_8 has units of time, and yet Einstein's equations will have the same form.\n\nInvariance of the form of an equation under an arbitrary coordinate transformation is customarily referred to as general covariance, and equations with this property are referred to as written in the covariant form. General covariance is a special case of gauge invariance.\n\nMaxwell's equations can also be expressed in a generally covariant form, which is as invariant under general coordinate transformation as Einstein's field equation.\n\nUntil the advent of quantum mechanics, the only well known example of gauge symmetry was in electromagnetism, and the general significance of the concept was not fully understood. For example, it was not clear whether it was the fields E and B or the potentials V and A that were the fundamental quantities; if the former, then the gauge transformations could be considered as nothing more than a mathematical trick.\n\nIn quantum mechanics, a particle such as an electron is also described as a wave. For example, if the double-slit experiment is performed with electrons, then a wave-like interference pattern is observed. The electron has the highest probability of being detected at locations where the parts of the wave passing through the two slits are in phase with one another, resulting in constructive interference. The frequency of the electron \"wave\" is related to the kinetic energy of an individual electron \"particle\" via the quantum-mechanical relation \"E\" = \"hf\". If there are no electric or magnetic fields present in this experiment, then the electron's energy is constant, and, for example, there will be a high probability of detecting the electron along the central axis of the experiment, where by symmetry the two parts of the wave are in phase.\n\nBut now suppose that the electrons in the experiment are subject to electric or magnetic fields. For example, if an electric field was imposed on one side of the axis but not on the other, the results of the experiment would be affected. The part of the electron wave passing through that side oscillates at a different rate, since its energy has had −\"eV\" added to it, where −\"e\" is the charge of the electron and \"V\" the electrical potential. The results of the experiment will be different, because phase relationships between the two parts of the electron wave have changed, and therefore the locations of constructive and destructive interference will be shifted to one side or the other. It is the electric potential that occurs here, not the electric field, and this is a manifestation of the fact that it is the potentials and not the fields that are of fundamental significance in quantum mechanics.\n\nIt is even possible to have cases in which an experiment's results differ when the potentials are changed, even if no charged particle is ever exposed to a different field. One such example is the Aharonov–Bohm effect, shown in the figure. In this example, turning on the solenoid only causes a magnetic field B to exist within the solenoid. But the solenoid has been positioned so that the electron cannot possibly pass through its interior. If one believed that the fields were the fundamental quantities, then one would expect that the results of the experiment would be unchanged. In reality, the results are different, because turning on the solenoid changed the vector potential A in the region that the electrons do pass through. Now that it has been established that it is the potentials V and A that are fundamental, and not the fields E and B, we can see that the gauge transformations, which change V and A, have real physical significance, rather than being merely mathematical artifacts.\n\nNote that in these experiments, the only quantity that affects the result is the \"difference\" in phase between the two parts of the electron wave. Suppose we imagine the two parts of the electron wave as tiny clocks, each with a single hand that sweeps around in a circle, keeping track of its own phase. Although this cartoon ignores some technical details, it retains the physical phenomena that are important here. If both clocks are sped up by the same amount, the phase relationship between them is unchanged, and the results of experiments are the same. Not only that, but it is not even necessary to change the speed of each clock by a \"fixed\" amount. We could change the angle of the hand on each clock by a \"varying\" amount θ, where θ could depend on both the position in space and on time. This would have no effect on the result of the experiment, since the final observation of the location of the electron occurs at a single place and time, so that the phase shift in each electron's \"clock\" would be the same, and the two effects would cancel out. This is another example of a gauge transformation: it is local, and it does not change the results of experiments.\n\nIn summary, gauge symmetry attains its full importance in the context of quantum mechanics. In the application of quantum mechanics to electromagnetism, i.e., quantum electrodynamics, gauge symmetry applies to both electromagnetic waves and electron waves. These two gauge symmetries are in fact intimately related. If a gauge transformation θ is applied to the electron waves, for example, then one must also apply a corresponding transformation to the potentials that describe the electromagnetic waves. Gauge symmetry is required in order to make quantum electrodynamics a renormalizable theory, i.e., one in which the calculated predictions of all physically measurable quantities are finite.\n\nThe description of the electrons in the subsection above as little clocks is in effect a statement of the mathematical rules according to which the phases of electrons are to be added and subtracted: they are to be treated as ordinary numbers, except that in the case where the result of the calculation falls outside the range of 0≤θ<360°, we force it to \"wrap around\" into the allowed range, which covers a circle. Another way of putting this is that a phase angle of, say, 5° is considered to be completely equivalent to an angle of 365°. Experiments have verified this testable statement about the interference patterns formed by electron waves. Except for the \"wrap-around\" property, the algebraic properties of this mathematical structure are exactly the same as those of the ordinary real numbers.\n\nIn mathematical terminology, electron phases form an Abelian group under addition, called the circle group or \"U\"(1). \"Abelian\" means that addition commutes, so that θ + φ = φ + θ. Group means that addition associates and has an identity element, namely \"0\". Also, for every phase there exists an inverse such that the sum of a phase and its inverse is 0. Other examples of abelian groups are the integers under addition, 0, and negation, and the nonzero fractions under product, 1, and reciprocal.\nAs a way of visualizing the choice of a gauge, consider whether it is possible to tell if a cylinder has been twisted. If the cylinder has no bumps, marks, or scratches on it, we cannot tell. We could, however, draw an arbitrary curve along the cylinder, defined by some function θ(\"x\"), where \"x\" measures distance along the axis of the cylinder. Once this arbitrary choice (the choice of gauge) has been made, it becomes possible to detect it if someone later twists the cylinder.\n\nIn 1954, Chen Ning Yang and Robert Mills proposed to generalize these ideas to noncommutative groups. A noncommutative gauge group can describe a field that, unlike the electromagnetic field, interacts with itself. For example, general relativity states that gravitational fields have energy, and special relativity concludes that energy is equivalent to mass. Hence a gravitational field induces a further gravitational field. The nuclear forces also have this self-interacting property.\n\nSurprisingly, gauge symmetry can give a deeper explanation for the existence of interactions, such as the electric and nuclear interactions. This arises from a type of gauge symmetry relating to the fact that all particles of a given type are experimentally indistinguishable from one another. Imagine that Alice and Betty are identical twins, labeled at birth by bracelets reading A and B. Because the girls are identical, nobody would be able to tell if they had been switched at birth; the labels A and B are arbitrary, and can be interchanged. Such a permanent interchanging of their identities is like a global gauge symmetry. There is also a corresponding local gauge symmetry, which describes the fact that from one moment to the next, Alice and Betty could swap roles while nobody was looking, and nobody would be able to tell. If we observe that Mom's favorite vase is broken, we can only infer that the blame belongs to one twin or the other, but we cannot tell whether the blame is 100% Alice's and 0% Betty's, or vice versa. If Alice and Betty are in fact quantum-mechanical particles rather than people, then they also have wave properties, including the property of superposition, which allows waves to be added, subtracted, and mixed arbitrarily. It follows that we are not even restricted to complete swaps of identity. For example, if we observe that a certain amount of energy exists in a certain location in space, there is no experiment that can tell us whether that energy is 100% A's and 0% B's, 0% A's and 100% B's, or 20% A's and 80% B's, or some other mixture. The fact that the symmetry is local means that we cannot even count on these proportions to remain fixed as the particles propagate through space. The details of how this is represented mathematically depend on technical issues relating to the spins of the particles, but for our present purposes we consider a spinless particle, for which it turns out that the mixing can be specified by some arbitrary choice of gauge θ(\"x\"), where an angle θ = 0° represents 100% A and 0% B, θ = 90° means 0% A and 100% B, and intermediate angles represent mixtures.\n\nAccording to the principles of quantum mechanics, particles do not actually have trajectories through space. Motion can only be described in terms of waves, and the momentum \"p\" of an individual particle is related to its wavelength λ by \"p\" = \"h\"/\"λ\". In terms of empirical measurements, the wavelength can only be determined by observing a change in the wave between one point in space and another nearby point (mathematically, by differentiation). A wave with a shorter wavelength oscillates more rapidly, and therefore changes more rapidly between nearby points. Now suppose that we arbitrarily fix a gauge at one point in space, by saying that the energy at that location is 20% A's and 80% B's. We then measure the two waves at some other, nearby point, in order to determine their wavelengths. But there are two entirely different reasons that the waves could have changed. They could have changed because they were oscillating with a certain wavelength, or they could have changed because the gauge function changed from a 20-80 mixture to, say, 21-79. If we ignore the second possibility, the resulting theory doesn't work; strange discrepancies in momentum will show up, violating the principle of conservation of momentum. Something in the theory must be changed.\n\nAgain there are technical issues relating to spin, but in several important cases, including electrically charged particles and particles interacting via nuclear forces, the solution to the problem is to impute physical reality to the gauge function θ(\"x\"). We say that if the function θ oscillates, it represents a new type of quantum-mechanical wave, and this new wave has its own momentum \"p\" = \"h\"/\"λ\", which turns out to patch up the discrepancies that otherwise would have broken conservation of momentum. In the context of electromagnetism, the particles A and B would be charged particles such as electrons, and the quantum mechanical wave represented by θ would be the electromagnetic field. (Here we ignore the technical issues raised by the fact that electrons actually have spin 1/2, not spin zero. This oversimplification is the reason that the gauge field θ comes out to be a scalar, whereas the electromagnetic field is actually represented by a vector consisting of \"V\" and A.) The result is that we have an explanation for the presence of electromagnetic interactions: if we try to construct a gauge-symmetric theory of identical, non-interacting particles, the result is not self-consistent, and can only be repaired by adding electric and magnetic fields that cause the particles to interact.\n\nAlthough the function θ(\"x\") describes a wave, the laws of quantum mechanics require that it also have particle properties. In the case of electromagnetism, the particle corresponding to electromagnetic waves is the photon. In general, such particles are called gauge bosons, where the term \"boson\" refers to a particle with integer spin. In the simplest versions of the theory, gauge bosons are massless, but it is also possible to construct versions in which they have mass, as is the case for the gauge bosons that transmit the nuclear decay forces.\n\nThese books are intended for general readers and employ the barest minimum of mathematics.\n"}
{"id": "44496861", "url": "https://en.wikipedia.org/wiki?curid=44496861", "title": "James' space", "text": "James' space\n\nIn the area of mathematics known as functional analysis, James' space is an important example in the theory of Banach spaces and commonly serves as useful counterexample to general statements concerning the structure of general Banach spaces. The space was first introduced in 1950 in a short paper by Robert C. James.\n\nJames' space serves as an example of a space that is isometrically isomorphic to its double dual, while not being reflexive. Furthermore, James' space has a basis, while having no unconditional basis.\n\nLet formula_1 denote the family of all finite increasing sequences of integers of odd length. For any sequence of real numbers formula_2 and formula_3 we define the quantity\n\nJames' space, denoted by J, is defined to be all elements \"x\" from \"c\" satisfying \nformula_5, endowed with the norm formula_6.\n\n\n"}
{"id": "35228294", "url": "https://en.wikipedia.org/wiki?curid=35228294", "title": "Joseph Ehrenfried Hofmann", "text": "Joseph Ehrenfried Hofmann\n\nJoseph Ehrenfried Hofmann (* 7 March 1900 in Munich, † 7 May 1973 in Günzburg ) was a German historian of mathematics, known for his research on Gottfried Wilhelm Leibniz.\n\nAfter graduating from high school in 1919 at the Wilhelm Gymnasium in Munich, Hofmann studied in Munich (Ph.D., 1927, Walther von Dyck and George Faber ) and was briefly an assistant in Munich and Darmstadt, before he went into the teaching profession (in Gunzburg, Nördlingen). As a student he was drawn to the history of mathematics, after Faber him for publishing the works of Euler heranzog. A big influence was Henry Wieleitner, with whom he published several works on the history of calculus. As a school teacher, he continued his historical studies. In 1939 he habilitated in the history of mathematics at the University of Berlin. 1940 to 1945 he was entrusted with the Leibniz edition of the Berlin Academy of Sciences. 1947 until his retirement in 1963 he was again high school teacher in Gunzburg. He also had (in part-time) professor of the History of Mathematics at the Albert-Ludwigs-University Freiburg, the Humboldt University of Berlin, the Eberhard Karls University of Tuebingen (honorary professorship in 1950) and the Technical University of Karlsruhe . He organized regular symposia on the history of mathematics at the Mathematical Research Institute in Oberwolfach, where he worked right after the war.\nHofmann was considered an expert in the development of calculus by Leibniz, whose time in Paris he studied carefully and so significantly to the Enlightenment (or reassurance) of long in the history of mathematics echoing priority dispute between Leibniz and Isaac Newton contributed to the invention of calculus. He was co-editor of the works of Leibniz, and others of Nicholas of Cusa, Johann Bernoulli (and other reprint editions of her work in publishing and Georg Olms, Hildesheim, such as mathematics history of Abraham Gotthelf Kästner ). He also worked on number theory by Leonhard Euler and Pierre de Fermat . He discovered including some new works of Fermat (published 1943).\nHe died after he drove off a car on the morning walk.\n\nwith Oskar Becker : History of Mathematics, Bonn, Atheneum Publishing, 1951 (derived from Hofmann Part 2 and 3)\n\n"}
{"id": "10620457", "url": "https://en.wikipedia.org/wiki?curid=10620457", "title": "Lambek–Moser theorem", "text": "Lambek–Moser theorem\n\nIn combinatorial number theory, the Lambek–Moser theorem is a generalization of Beatty's theorem that defines a partition of the positive integers into two subsets from any monotonic integer-valued function. Conversely, any partition of the positive integers into two subsets may be defined from a monotonic function in this way.\n\nThe theorem was discovered by Leo Moser and Joachim Lambek. provides a visual proof of the result.\n\nThe theorem applies to any non-decreasing and unbounded function that maps positive integers to non-negative integers. From any such function , define to be the integer-valued function that is as close as possible to the inverse function of , in the sense that, for all ,\nIt follows from this definition that .\nFurther, let\n\nThen the result states that and are strictly increasing and that the ranges of and form a partition of the positive integers.\n\nLet ; then formula_1.\nThus and formula_2\nFor the values of are the pronic numbers\nwhile the values of are\nThese two sequences are complementary: each positive integer belongs to exactly one of them. The Lambek–Moser theorem states that this phenomenon is not specific to the pronic numbers, but rather it arises for any choice of with the appropriate properties.\n\nBeatty's theorem, defining a partition of the integers from rounding their multiples by an irrational number , can be seen as an instance of the Lambek–Moser theorem. In Beatty's theorem, formula_3 and formula_4 where formula_5. The condition that (and therefore ) be greater than one implies that these two functions are non-decreasing; the derived functions are formula_6 and formula_7 The sequences of values of and forming the derived partition are known as Beatty sequences.\n\nThe Lambek–Moser theorem is universal, in the sense that it can explain any partition of the integers into two infinite parts. If and are any two infinite subsets forming a partition of the integers, one may construct a pair of functions and from which this partition may be derived using the Lambek–Moser theorem: define and .\n\nFor instance, consider the partition of integers into even and odd numbers: let be the even numbers and be the odd numbers.\nThen , so and similarly . These two functions and form an inverse pair, and the partition generated via the Lambek–Moser theorem from this pair is just the partition of the positive integers into even and odd numbers.\n\nLambek and Moser discuss formulas involving the prime-counting function for the functions and arising in this way from the partition of the positive integers into prime numbers and composite numbers.\n\n\n"}
{"id": "43805888", "url": "https://en.wikipedia.org/wiki?curid=43805888", "title": "Langley’s Adventitious Angles", "text": "Langley’s Adventitious Angles\n\nLangley’s Adventitious Angles is a mathematical problem posed by Edward Mann Langley in \"The Mathematical Gazette\" in 1922.\n\nIn its original form the problem was as follows: formula_1 is an isosceles triangle.\n\nA solution was developed by James Mercer in 1923. This solution involves drawing one additional line, and then making repeated use of the fact that the internal angles of a triangle add up to 180° to prove that several triangles drawn within the large triangle are all isosceles.\n\nMany other solutions are possible. Cut the Knot list twelve different solutions and several alternative problems with the same 80-80-20 triangle but different internal angles.\n\nA quadrilateral such as BCEF in which the angles formed by the diagonals with the sides are all rational (when measured in degrees) is called an adventitious quadrangle. Numerous constructions for other adventitious quadrangles beyond the one appearing in Langley's puzzle are known. They form several infinite families and an additional set of sporadic examples.\n\nClassifying the adventitious quadrangles (which need not be convex) turns out to be equivalent to classifying all triple intersections of diagonals in regular polygons. This was solved by Gerrit Bol in 1936 (Beantwoording van prijsvraag # 17, Nieuw-Archief voor Wiskunde 18, pages 14-66). He in fact classified (though with a few errors) all multiple intersections of diagonals in regular polygons. His results (all done by hand!) were confirmed with computer, and the errors corrected, by Bjorn Poonen and Michael Rubinstein in 1998. The article contains a history of the problem and a picture featuring the regular 30-gon and its diagonals. \n\nIn 2015, an anonymous Japanese woman using the pen name \"aerile re\" published the first known method (the method of 3 circumcenters) for using elementary geometry to find a special class of adventitious quadrangles. This work solves the first of the three unsolved problems listed by Rigby in his 1978 paper.\n\n"}
{"id": "4095925", "url": "https://en.wikipedia.org/wiki?curid=4095925", "title": "Mass-to-light ratio", "text": "Mass-to-light ratio\n\nIn astrophysics and physical cosmology the mass to light ratio, normally designated with the Greek upsilon symbol formula_1, is the quotient between the total mass of a spatial volume (typically on the scales of a galaxy or a cluster) and its luminosity. These ratios are often reported using the value calculated for the Sun as a baseline ratio which is a constant formula_2 = 5133 kg/W equal to a solar mass () divided by a solar luminosity (), (/). The mass to light ratios of galaxies and clusters are all much greater than formula_2 due in part to the fact that most of the matter in these objects does not reside within stars and observations suggest that a large fraction is present in the form of dark matter.\n\nLuminosities are obtained from photometric observations, correcting the observed brightness of the object for the distance dimming and extinction effects. In general, unless a complete spectrum of the radiation emitted by the object is obtained, a model must be extrapolated through either power law or blackbody fits. The luminosity thus obtained is known as the bolometric luminosity.\n\nMasses are often calculated from the dynamics of the virialized system or from gravitational lensing. Typical mass to light ratios for galaxies range from 2 to 10 formula_2 while on the largest scales, the mass to light ratio of the observable universe is approximately 100 formula_2, in concordance with the current best fit cosmological model.\n"}
{"id": "58472531", "url": "https://en.wikipedia.org/wiki?curid=58472531", "title": "Maximum-entropy random graph model", "text": "Maximum-entropy random graph model\n\nMaximum-entropy random graph models are random graph models used to study complex networks subject to the principle of maximum entropy under a set of structural constraints\n, which may be global, distributional, or local.\n\nAny random graph model (at a fixed set of parameter values) results in a probability distribution on graphs, and those that are maximum entropy within the considered class of distributions have the special property of being maximally unbiased null models for network inference (e.g. biological network inference). Each model defines a family of probability distributions on the set of graphs of size formula_1 (for each formula_2 for some finite formula_3), parameterized by a collection of constraints on formula_4 observables formula_5 defined for each graph formula_6 (such as fixed expected average degree, degree distribution of a particular form, or specific degree sequence), enforced in the graph distribution alongside entropy maximization by the method of Lagrange multipliers. Note that in this context \"maximum entropy\" refers not to the entropy of a single graph, but rather the entropy of the whole probabilistic ensemble of random graphs.\n\nSeveral commonly studied random network models are in fact maximum entropy, for example the ER graphs formula_7 and formula_8 (which each have one global constraint on the number of edges), as well as the configuration model (CM). and soft configuration model (SCM) (which each have formula_1 local constraints, one for each nodewise degree-value). In the two pairs of models mentioned above, an important distinction is in whether the constraint is sharp (i.e. satisfied by every element of the set of size-formula_1 graphs with nonzero probability in the ensemble), or soft (i.e. satisfied on average across the whole ensemble). The former (sharp) case corresponds to a microcanonical ensemble, the condition of maximum entropy yielding all graphs formula_6 satisfying formula_12 as equiprobable; the latter (soft) case is canonical, producing an exponential random graph model (ERGM).\n\nSuppose we are building a random graph model consisting of a probability distribution formula_13 on the set formula_14 of simple graphs with formula_1 vertices. The Gibbs entropy formula_16 of this ensemble will be given by\n\nWe would like the ensemble-averaged values formula_18 of observables formula_5 (such as average degree, average clustering, or average shortest path length) to be tunable, so we impose formula_4 \"soft\" constraints on the graph distribution:\n\nwhere formula_22 label the constraints. Application of the method of Lagrange multipliers to determine the distribution formula_13 that maximizes formula_16 while satisfying formula_25, and the normalization condition formula_26 results in the following:\n\nwhere formula_28 is a normalizing constant (the partition function) and formula_29 are parameters (Lagrange multipliers) coupled to the correspondingly indexed graph observables, which may be tuned to yield graph samples with desired values of those properties, on average; the result is an exponential family and canonical ensemble; specificially yielding an ERGM.\n\nIn the canonical framework above, constraints were imposed on ensemble-averaged quantities formula_31. Although these properties will on average take on values specifiable by appropriate setting of formula_29, each specific instance formula_6 may have formula_34, which may be undesirable. Instead, we may impose a much stricter condition: every graph with nonzero probability must satisfy formula_35 exactly. Under these \"sharp\" constraints, the maximum-entropy distribution is determined. We exemplify this with the Erdős–Rényi model formula_7.\n\nThe sharp constraint in formula_7 is that of a fixed number of edges formula_38, that is formula_39, for all graphs formula_6 drawn from the ensemble (instantiated with a probability denoted formula_41). This restricts the sample space from formula_14 (all graphs on formula_1 vertices) to the subset formula_44. This is in direct analogy to the microcanonical ensemble in classical statistical mechanics, wherein the system is restricted to a thin manifold in the phase space of all states of a particular energy value. \n\nUpon restricting our sample space to formula_45, we have no external constraints (besides normalization) to satisfy, and thus we'll select formula_41 to maximize formula_16 without making use of Lagrange multipliers. It is well-known that the entropy-maximizing distribution in the absence of external constraints is the uniform distribution over the sample space (see maximum entropy probability distribution), from which we obtain:\n\nwhere the last expression in terms of binomial coefficients is the number of ways to place formula_38 edges among formula_50 possible edges, and thus is the cardinality of formula_45.\n\nA variety of maximum-entropy ensembles have been studied on generalizations of simple graphs. These include, for example, ensembles of simplicial complexes \n, and weighted random graphs with a given expected degree sequence \n\n"}
{"id": "8569325", "url": "https://en.wikipedia.org/wiki?curid=8569325", "title": "Mishnat ha-Middot", "text": "Mishnat ha-Middot\n\nThe Mishnat ha-Middot (; \"treatise of measures\") is considered the earliest known Hebrew treatise on geometry. The treatise was discovered in the Munich Library by Moritz Steinschneider, who dated it between 800 and 1200 C.E. Hermann Schapira argued the treatise dates from an earlier period and Solomon Gandz conjectured Rabbi Nehemiah (c. 150 C.E.) to be the author. The content resembles both the work of Hero of Alexandria (c. 100 C.E.) and that of al-Khwārizmī (c. 800 C.E.) and the proponents of the earlier dating therefore see it linking Greek and Islamic mathematics.\n\nThe \"Mishnat ha-Middot\" argues against the common belief that the Bible defines the geometric ratio π (pi) as being exactly equal to 3 and defines it as 3 1/7 instead. \n\n\n"}
{"id": "9612488", "url": "https://en.wikipedia.org/wiki?curid=9612488", "title": "Misiurewicz point", "text": "Misiurewicz point\n\nIn mathematics, a Misiurewicz point is a parameter in the Mandelbrot set (the parameter space of quadratic polynomials) for which the critical point is strictly preperiodic (i.e., it becomes periodic after finitely many iterations but is not periodic itself). By analogy, the term \"Misiurewicz point\" is also used for parameters in a Multibrot set where the unique critical point is strictly preperiodic. (This term makes less sense for maps in greater generality that have more than one (free) critical point because some critical points might be periodic and others not.)\n\nA parameter formula_1 is a Misiurewicz point formula_2 if it satisfies the equations\n\nand \n\nso :\n\nwhere :\n\nMisiurewicz points are named after the Polish-American mathematician Michał Misiurewicz.\n\nNote that the term \"Misiurewicz point\" is used ambiguously: Misiurewicz originally investigated maps in which all critical points were non-recurrent (that is, there is a neighborhood of every critical point that is not visited by the orbit of this critical point), and this meaning is firmly established in the context of dynamics of iterated interval maps. The case that for a quadratic polynomial the unique critical point is strictly preperiodic is only a very special case; in this restricted sense (as described above) this term is used in complex dynamics; a more appropriate term would be Misiurewicz–Thurston points (after William Thurston, who investigated postcritically finite rational maps).\n\n\nA complex quadratic polynomial has only one critical point. By a suitable conjugation any quadratic polynomial can be transformed into a map of the form formula_13 which has a single critical point at formula_14. The Misiurewicz points of this family of maps are roots of the equations\n\n(subject to the condition that the critical point is not periodic), \nwhere :\n\nFor example, the Misiurewicz points with \"k\"=2 and \"n\"=1, denoted by \"M\", are roots of\n\nThe root \"c\"=0 is not a Misiurewicz point because the critical point is a fixed point when \"c\"=0, and so is periodic rather than pre-periodic. This leaves a single Misiurewicz point \"M\" at \"c\" = −2.\n\nMisiurewicz points belong to the boundary of the Mandelbrot set. Misiurewicz points are dense in the boundary of the Mandelbrot set.\n\nIf formula_1 is a Misiurewicz point, then the associated filled Julia set is equal to the Julia set, and means the filled Julia set has no interior.\n\nIf formula_1 is a Misiurewicz point, then in the corresponding Julia set all periodic cycles are repelling (in particular the cycle that the critical orbit falls onto).\n\nThe Mandelbrot set and Julia set formula_24 are locally asymptotically self-similar around Misiurewicz points.\n\nMisiurewicz points can be classified according to number of external rays that land on them :, points where branches meet\n\nAccording to the Branch Theorem of the Mandelbrot set, all branch points of the Mandelbrot set are Misiurewicz points (plus, in a combinatorial sense, hyperbolic components represented by their centers).\n\nMany (actually, most) Misiurewicz parameters in the Mandelbrot set look like `centers of spirals'. The explanation for this is the following: at a Misiurewicz parameter, the critical value jumps onto a repelling periodic cycle after finitely many iterations; at each point on the cycle, the Julia set is asymptotically self-similar by a complex multiplication by the derivative of this cycle. If the derivative is non-real, then this implies that the Julia set, near the periodic cycle, has a spiral structure. A similar spiral structure thus occurs in the Julia set near the critical value and, by Tan Lei's aforementioned theorem, also in the Mandelbrot set near any Misiurewicz parameter for which the repelling orbit has non-real multiplier. Depending on the value of the multiplier, the spiral shape can seem more or less pronounced. The number of the arms at the spiral equals the number of branches at the Misiurewicz parameter, and this equals the number of branches at the critical value in the Julia set. (Even the `principal Misiurewicz point in the 1/3-limb', at the end of the parameter rays at angles 9/56, 11/56, and 15/56, turns out to be asymptotically a spiral, with infinitely many turns, even though this is hard to see without maginification.)\n\nExternal arguments of Misiurewicz points, measured in turns are :\n\nwhere: a and b are positive integers and b is odd, subscript number shows base of numeral system.\n\nPoint formula_25 :\nPoint formula_27 \n\nNotice that it is z-plane (dynamical plane) not c-plane (parameter plane) and point formula_29 is not the same point as formula_30.\n\nPoint formula_31 is landing point of only one external ray ( parameter ray) of angle 1/2 .\n\nPoint formula_32 is near a Misiurewicz point formula_33. It is \n\nPoint formula_39 is near a Misiurewicz point formula_40, \n\nPoint formula_45 is a principal Misiurewicz point of the 1/3 limb. It has 3 external rays 9/56, 11/56 and 15/56.\n\n\n"}
{"id": "23598664", "url": "https://en.wikipedia.org/wiki?curid=23598664", "title": "Nate Ackerman", "text": "Nate Ackerman\n\nNate Ackerman (born March 4, 1978 as Nathanael Leedom Ackerman) is a British-American mathematician and wrestler. He is the son of Peter Ackerman and Joanne Leedom-Ackerman.\n\nAckerman competed in the 2004 Summer Olympic Games as part of the Great Britain National Team. He also competed in the 1999, 2001, 2002, 2003, 2005 and 2011 World Championships. Ackerman's best international finish was 10th at the 2002 Commonwealth Games.\n\nAckerman was born in New York City, New York, United States to Joanne Leedom-Ackerman and Peter Ackerman. He was educated at the American School in London and then Harvard University, where he graduated in June 2000. He received his Ph.D. in mathematics in 2006 from Massachusetts Institute of Technology. He is currently a lecturer in mathematics at Harvard University.\n"}
{"id": "26259580", "url": "https://en.wikipedia.org/wiki?curid=26259580", "title": "Orbit trap", "text": "Orbit trap\n\nIn mathematics, an orbit trap is a method of colouring fractal images based upon how close an iterative function, used to create the fractal, approaches a geometric shape, called a \"trap\". Typical traps are points, lines, circles, flower shapes and even raster images. Orbit traps are typically used to colour two dimensional fractals representing the complex plane.\n\nA point based orbit trap colours a point based upon how close a function's orbit comes to a single point, typically the origin.\n\nA line based orbit trap colours a point based upon how close a function's orbit comes to one or more lines, typically vertical or horizontal (x=a or y=a lines). Pickover stalks are an example of a line based orbit trap which use two lines.\n\nOrbit traps are typically used with the class of two-dimensional fractals based on an iterative function. A program that creates such a fractal colours each pixel, which represent discrete points in the complex plane, based upon the behaviour of those points when they pass through a function a set number of times.\n\nThe best known example of this kind of fractal is the Mandelbrot set, which is based upon the function \"z\" = \"z\" + \"c\". The most common way of colouring Mandelbrot images is by taking the number of iterations required to reach a certain bailout value and then assigning that value a colour. This is called the escape time algorithm.\n\nA program that colours the Mandelbrot set using a point-based orbit trap will assign each pixel with a “distance” variable, that will typically be very high when first assigned:\n\nAs the program passes the complex value through the iterative function it will check the distance between each point in the orbit and the trap point. The value of the distance variable will be the shortest distance found during the iteration:\n\n"}
{"id": "1459902", "url": "https://en.wikipedia.org/wiki?curid=1459902", "title": "Otto Blumenthal", "text": "Otto Blumenthal\n\nLudwig Otto Blumenthal (20 July 1876 – 12 November 1944) was a German mathematician and professor at RWTH Aachen University.\n\nHe was born in Frankfurt, Hesse-Nassau. A student of David Hilbert, Blumenthal was an editor of \"Mathematische Annalen\". When the Civil Service Act of 1933 became law in 1933, after Hitler became Chancellor, Blumenthal was dismissed from his position at RWTH Aachen University.\n\nBlumenthal, who was of Jewish background, emigrated from the Nazis to the Netherlands, lived in Utrecht and was deported via Westerbork to the concentration camp, Theresienstadt in Bohemia (now Czech Republic), where he died.\n\nIn 1913, Blumenthal made a fundamental, though often overlooked, contribution to aerodynamics by building on Joukowsky's work to extract the complex transformation that carries the latter's name, making it an example of Stigler's Law.\n\n"}
{"id": "2995958", "url": "https://en.wikipedia.org/wiki?curid=2995958", "title": "Potential isomorphism", "text": "Potential isomorphism\n\nIn mathematical logic and in particular in model theory, a potential isomorphism is a collection of finite partial isomorphisms between two models which satisfies certain closure conditions. Existence of a partial isomorphism entails elementary equivalence, however the converse is not generally true, but it holds for ω-saturated models.\n\nA potential isomorphism between two models \"M\" and \"N\" is a non-empty collection \"F\" of finite partial isomorphisms between \"M\" and \"N\" which satisfy the following two properties:\n\n\nA notion of Ehrenfeucht-Fraïssé game is an exact characterisation of elementary equivalence and potential isomorphism can be seen as an approximation of it. Another notion that is similar to potential isomorphism is that of local isomorphism.\n\n"}
{"id": "458709", "url": "https://en.wikipedia.org/wiki?curid=458709", "title": "Predual", "text": "Predual\n\nIn mathematics, the predual of an object \"D\" is an object \"P\" whose dual space is \"D\".\n\nFor example, the predual of the space of bounded operators is the space of trace class operators. The predual of the space of differential forms is the space of chainlets. A predual is not always guaranteed to be unique or exist, however.\n"}
{"id": "1812809", "url": "https://en.wikipedia.org/wiki?curid=1812809", "title": "Pseudorandom generator", "text": "Pseudorandom generator\n\nIn theoretical computer science and cryptography, a pseudorandom generator (PRG) for a class of statistical tests is a deterministic procedure that maps a random seed to a longer pseudorandom string such that no statistical test in the class can distinguish between the output of the generator and the uniform distribution. The random seed is typically a short binary string drawn from the uniform distribution.\n\nMany different classes of statistical tests have been considered in the literature, among them the class of all Boolean circuits of a given size.\nIt is not known whether good pseudorandom generators for this class exist, but it is known that their existence is in a certain sense equivalent to (unproven) circuit lower bounds in computational complexity theory.\nHence the construction of pseudorandom generators for the class of Boolean circuits of a given size rests on currently unproven hardness assumptions.\n\nLet formula_1 be a class of functions.\nThese functions are the \"statistical tests\" that the pseudorandom generator will try to fool, and they are usually algorithms.\nSometimes the statistical tests are also called \"adversaries\" or \"distinguishers\".\n\nA function formula_2 with formula_3 is a \"pseudorandom generator\" against formula_4 with \"bias\" formula_5 if, for every formula_6 in formula_4, the statistical distance between the distributions formula_8 and formula_9 is at most formula_5, where formula_11 is the uniform distribution on formula_12.\n\nThe quantity formula_13 is called the \"seed length\" and the quantity formula_14 is called the \"stretch\" of the pseudorandom generator.\n\nA pseudorandom generator against a family of adversaries formula_15 with bias formula_16 is a family of pseudorandom generators formula_17, where formula_18 is a pseudorandom generator against formula_19 with bias formula_16 and seed length formula_21.\n\nIn most applications, the family formula_4 represents some model of computation or some set of algorithms, and one is interested in designing a pseudorandom generator with small seed length and bias, and such that the output of the generator can be computed by the same sort of algorithm.\n\nIn cryptography, the class formula_4 usually consists of all circuits of size polynomial in the input and with a single bit output, and one is interested in designing pseudorandom generators that are computable by a polynomial-time algorithm and whose bias is negligible in the circuit size.\nThese pseudorandom generators are sometimes called cryptographically secure pseudorandom generators (CSPRGs).\n\nIt is not known if cryptographically secure pseudorandom generators exist.\nProving that they exist is difficult since their existence implies P ≠ NP, which is widely believed but a famously open problem.\nThe existence of cryptographically secure pseudorandom generators is widely believed as well and they are necessary for many applications in cryptography.\n\nThe pseudorandom generator theorem shows that cryptographically secure pseudorandom generators exist if and only if one-way functions exist.\n\nPseudorandom generators have numerous applications in cryptography. For instance, pseudorandom generators provide an efficient analog of one-time pads. It is well known that in order to encrypt a message \"m\" in a way that the cipher text provides no information on the plaintext, the key \"k\" used must be random over strings of length |m|. Perfectly secure encryption is very costly in terms of key length. Key length can be significantly reduced using a pseudorandom generator if perfect security is replaced by semantic security. Common constructions of stream ciphers are based on pseudorandom generators.\n\nPseudorandom generators may also be used to construct symmetric key cryptosystems, where a large number of messages can be safely encrypted under the same key. Such a construction can be based on a pseudorandom function family, which generalizes the notion of a pseudorandom generator.\n\nIn the 1980s, simulations in physics began to use pseudorandom generators to produce sequences with billions of elements, and by the late 1980s, evidence had developed that a few common generators gave incorrect results in such cases as phase transition properties of the 3D Ising model and shapes of diffusion-limited aggregates. Then in the 1990s, various idealizations of physics simulations—based on random walks, correlation functions, localization of eigenstates, etc., were used as tests of pseudorandom generators.\n\nNIST announced SP800-22 Randomness tests to test whether a pseudorandom generator produces high quality random bits. Yongge Wang showed that NIST testing is not enough to detect weak pseudorandom generators and developed statistical distance based testing technique LILtest.\n\nA main application of pseudorandom generators lies in the derandomization of computation that relies on randomness, without corrupting the result of the computation.\nPhysical computers are deterministic machines, and obtaining true randomness can be a challenge.\nPseudorandom generators can be used to efficiently simulate randomized algorithms with using little or no randomness.\nIn such applications, the class formula_4 describes the randomized algorithm or class of randomized algorithms that one wants to simulate, and the goal is to design an \"efficiently computable\" pseudorandom generator against formula_4 whose seed length is as short as possible.\nIf a full derandomization is desired, a completely deterministic simulation proceeds by replacing the random input to the randomized algorithm with the pseudorandom string produced by the pseudorandom generator.\nThe simulation does this for all possible seeds and averages the output of the various runs of the randomized algorithm in a suitable way.\n\nA fundamental question in computational complexity theory is whether all polynomial time randomized algorithms for decision problems can be deterministically simulated in polynomial time. The existence of such a simulation would imply that BPP = P. To perform such a simulation, it is sufficient to construct pseudorandom generators against the family F of all circuits of size \"s\"(\"n\") whose inputs have length \"n\" and output a single bit, where \"s\"(\"n\") is an arbitrary polynomial, the seed length of the pseudorandom generator is O(log \"n\") and its bias is ⅓.\n\nIn 1991, Noam Nisan and Avi Wigderson provided a candidate pseudorandom generator with these properties. In 1997 Russell Impagliazzo and Avi Wigderson proved that the construction of Nisan and Wigderson is a pseudorandom generator assuming that there exists a decision problem that can be computed in time 2 on inputs of length \"n\" but requires circuits of size 2.\n\nWhile unproven assumption about circuit complexity are needed to prove that the Nisan–Wigderson generator works for time-bounded machines, it is natural to restrict the class of statistical tests further such that we need not rely on such unproven assumptions.\nOne class for which this has been done is the class of machines whose work space is bounded by formula_26.\nUsing a repeated squaring trick known as Savitch's theorem, it is easy to show that every probabilistic log-space computation can be simulated in space formula_27.\nNoam Nisan (1992) showed that this derandomization can actually be achieved with a pseudorandom generator of seed length formula_27 that fools all formula_26-space machines.\nNisan's generator has been used by Saks and Zhou (1999) to show that probabilistic log-space computation can be simulated deterministically in space formula_30.\nThis result is still the best known derandomization result for general log-space machines in 2012.\n\nWhen the statistical tests consist of all multivariate linear functions over some finite field formula_31, one speaks of epsilon-biased generators.\nThe construction of achieves a seed length of formula_32, which is optimal up to constant factors.\nPseudorandom generators for linear functions often serve as a building block for more complicated pseudorandom generators.\n\n proves that taking the sum of formula_33 small-bias generators fools polynomials of degree formula_33.\nThe seed length is formula_35.\n\nConstant depth circuits that produce a single output bit.\n\nThe pseudorandom generators used in cryptography and universal algorithmic derandomization have not been proven to exist, although their existence is widely believed. Proofs for their existence would imply proofs of lower bounds on the circuit complexity of certain explicit functions. Such circuit lower bounds cannot be proved in the framework of natural proofs assuming the existence of stronger variants of cryptographic pseudorandom generators.\n\n"}
{"id": "25525656", "url": "https://en.wikipedia.org/wiki?curid=25525656", "title": "Quantum KZ equations", "text": "Quantum KZ equations\n\nIn mathematical physics, the quantum KZ equations or quantum Knizhnik–Zamolodchikov equations or qKZ equations are the analogue for quantum affine algebras of the Knizhnik–Zamolodchikov equations for affine Kac–Moody algebras. They are a consistent system of difference equations satisfied by the \"N\"-point functions, the vacuum expectations of products of primary fields. In the limit as the deformation parameter \"q\" approaches 1, the \"N\"-point functions of the quantum affine algebra tend to those of the affine Kac–Moody algebra and the difference equations become partial differential equations. The quantum KZ equations have been used to study exactly solved models in quantum statistical mechanics.\n\n\n"}
{"id": "42185330", "url": "https://en.wikipedia.org/wiki?curid=42185330", "title": "Ramanujan–Sato series", "text": "Ramanujan–Sato series\n\nIn mathematics, a Ramanujan–Sato series generalizes Ramanujan’s pi formulas such as,\n\nto the form\n\nby using other well-defined sequences of integers formula_3 obeying a certain recurrence relation, sequences which may be expressed in terms of binomial coefficients formula_4, and formula_5 employing modular forms of higher levels.\n\nRamanujan made the enigmatic remark that there were \"corresponding theories\", but it was only recently that H.H. Chan and S. Cooper found a general approach that used the underlying modular congruence subgroup formula_6, while G. Almkvist has experimentally found numerous other examples also with a general method using differential operators.\n\nLevels 1–4A were given by Ramanujan (1917), level 5 by H.H. Chan and S. Cooper (2012), 6A by Chan, Tanigawa, Yang, and Zudilin, 6B by Sato (2002), 6C by H. Chan, S. Chan, and Z. Liu (2004), 6D by H. Chan and H. Verrill (2009), level 7 by S. Cooper (2012), part of level 8 by Almkvist and Guillera (2012), part of level 10 by Y. Yang, and the rest by H.H. Chan and S.Cooper.\n\nThe notation \"j\"(\"τ\") is derived from Zagier and \"T\" refers to the relevant McKay–Thompson series.\n\nExamples for levels 1–4 were given by Ramanujan in his 1917 paper. Given formula_7 as in the rest of this article. Let,\n\nwith the j-function \"j\"(\"τ\"), Eisenstein series \"E\", and Dedekind eta function \"η\"(\"τ\"). The first expansion is the McKay–Thompson series of class 1A () with a(0) = 744. Note that, as first noticed by J. McKay, the coefficient of the linear term of \"j\"(\"τ\") almost equals formula_9, which is the degree of the smallest nontrivial irreducible representation of the Monster group. Similar phenomena will be observed in the other levels. Define\n\nThen the two modular functions and sequences are related by\n\nif the series converges and the sign chosen appropriately, though squaring both sides easily removes the ambiguity. Analogous relationships exist for the higher levels.\n\nExamples:\n\nand formula_15 is a fundamental unit. The first belongs to a family of formulas which were rigorously proven by the Chudnovsky brothers in 1989 \nand later used to calculate 10 trillion digits of π in 2011. The second formula, and the ones for higher levels, was established by H.H. Chan and S. Cooper in 2012.\n\nUsing Zagier’s notation for the modular function of level 2,\n\nNote that the coefficient of the linear term of \"j\"(\"τ\") is one more than formula_17 which is the smallest degree > 1 of the irreducible representations of the Baby Monster group. Define,\n\nThen,\n\nif the series converges and the sign chosen appropriately.\n\nExamples:\n\nThe first formula, found by Ramanujan and mentioned at the start of the article, belongs to a family proven by D. Bailey and the Borwein brothers in a 1989 paper.\n\nDefine,\n\nwhere formula_24 is the smallest degree > 1 of the irreducible representations of the Fischer group \"Fi\" and,\n\nExamples:\n\nDefine,\n\nwhere the first is the 24th power of the Weber modular function formula_30. And,\n\nExamples:\n\nDefine,\n\nand,\n\nwhere the first is the product of the central binomial coefficients and the Apéry numbers ()\n\nExamples:\n\nIn 2002, Sato established the first results for level > 4. It involved Apéry numbers which were first used to establish the irrationality of formula_40. First, define,\n\nJ. Conway and S. Norton showed there are linear relations between the McKay–Thompson series \"T\", one of which was,\n\nor using the above eta quotients \"j\",\n\nFor the modular function j\", one can associate it with \"three\" different sequences. (A similar situation happens for the level 10 function j\".) Let,\n\nwhich, respectively, is the product of the central binomial coefficients formula_51 with the Franel numbers, , and (-1)^k . Note that the second sequence, \"α\"(\"k\") is the number of 2n-step polygons on a cubic lattice. Their complements,\n\nThere are also associated sequences, namely the Apéry numbers,\n\nthe Domb numbers (unsigned) or the number of 2\"n\"-step polygons on a diamond lattice,\n\nand the Almkvist-Zudilin numbers,\n\nThe modular functions can be related as,\n\nif the series converges and the sign chosen appropriately. It can also be observed that,\n\nwhich implies,\n\nand similarly using α and α'.\n\nOne can use a value for \"j\" in three ways. For example, starting with,\n\nand noting that formula_62 then,\n\nas well as,\n\nthough the formulas using the complements apparently do not yet have a rigorous proof. For the other modular functions,\n\nDefine\n\nand,\n\nExample:\n\nNo pi formula has yet been found using j.\n\nDefine,\n\nThe expansion of the first is the McKay–Thompson series of class 4B (and is a square root of another function) while the second, if unsigned, is that of class 8A given by the third. Let,\n\nwhere the first is the product of the central binomial coefficient and a sequence related to an arithmetic-geometric mean (),\n\nExamples:\n\nthough no pi formula is yet known using \"j\"(\"τ\").\n\nDefine,\n\nThe expansion of the first is the McKay–Thompson series of class 3C (and related to the cube root of the j-function), while the second is that of class 9A. Let,\n\nwhere the first is the product of the central binomial coefficients and (though with different signs).\n\nExamples:\n\nDefine,\n\nJust like the level 6, there are also linear relations between these,\n\nor using the above eta quotients \"j\",\n\nLet,\n\ntheir complements,\n\nand,\n\nthough closed-forms are not yet known for the last three sequences.\n\nThe modular functions can be related as,\n\nif the series converges. In fact, it can also be observed that,\n\nSince the exponent has a fractional part, the sign of the square root must be chosen appropriately though it is less an issue when \"j\" is positive.\n\nJust like level 6, the level 10 function \"j\" can be used in three ways. Starting with,\n\nand noting that formula_101 then,\n\nas well as,\n\nthough the ones using the complements do not yet have a rigorous proof. A conjectured formula using one of the last three sequences is,\n\nwhich implies there might be examples for all sequences of level 10.\n\nDefine the McKay–Thompson series of class 11A,\n\nwhere,\n\nand,\n\nNo closed-form in terms of binomial coefficients is yet known for the sequence but it obeys the recurrence relation,\n\nwith initial conditions \"s\"(0) = 1, \"s\"(1) = 4.\n\nExample:\n\nAs pointed out by Cooper, there are analogous sequences for certain higher levels.\n\nR. Steiner found examples using Catalan numbers formula_110, \n\nand for this a modular form with a second periodic for k exists: formula_112. \nOther similar series are \n\nwith the last (comments in ) found by using a linear combination of higher parts of Wallis-Lambert series for 4/Pi and Euler series for the circumference of an ellipse. \n\nUsing the definition of Catalan numbers with the Gamma-function the first and last for example give the identities\n\nThe last is also equivalent to,\n\nand is related to the fact that,\n\nwhich is a consequence of Stirling's approximation.\n\n\n"}
{"id": "3011773", "url": "https://en.wikipedia.org/wiki?curid=3011773", "title": "Riemann–Roch theorem for smooth manifolds", "text": "Riemann–Roch theorem for smooth manifolds\n\nIn mathematics, a Riemann–Roch theorem for smooth manifolds is a version of results such as the Hirzebruch–Riemann–Roch theorem or Grothendieck–Riemann–Roch theorem (GRR) without a hypothesis making the smooth manifolds involved carry a complex structure. Results of this kind were obtained by Michael Atiyah and Friedrich Hirzebruch in 1959, reducing the requirements to something like a spin structure.\n\nLet \"X\" and \"Y\" be oriented smooth closed manifolds,\nand \"f\": \"X\" → \"Y\" a continuous map.\nLet \"v\"=\"f\"(\"TY\") − \"TX\" in the K-group \nK(X).\nIf dim(X) ≡ dim(Y) mod 2, then\nwhere ch is the Chern character, d(v) an element of \nthe integral cohomology group \"H\"(\"Y\", \"Z\") satisfying\n\"d\"(\"v\") ≡ \"f\" \"w\"(T\"Y\")-\"w\"(T\"X\") mod 2,\nf the Gysin homomorphism for K-theory,\nand f the Gysin homomorphism for cohomology\nThis theorem was first proven by Atiyah and Hirzebruch.\n\nThe theorem is proven by considering several special cases. \nIf \"Y\" is the Thom space of a vector bundle \"V\" over \"X\",\nthen the Gysin maps are just the Thom isomorphism.\nThen, using the splitting principle, it suffices to check the theorem via explicit computation for line\nbundles.\n\nIf \"f\": \"X\" → \"Y\" is an embedding, then the \nThom space of the normal bundle of \"X\" in \"Y\" can be viewed as a tubular neighborhood of \"X\"\nin \"Y\", and excision gives a map\nand\nThe Gysin map for K-theory/cohomology is defined to be the composition of the Thom isomorphism with these maps.\nSince the theorem holds for the map from \"X\" to the Thom space of \"N\",\nand since the Chern character commutes with \"u\" and \"v\", the theorem is also true for embeddings.\n\"f\": \"X\" → \"Y\".\n\nFinally, we can factor a general map \"f\": \"X\" → \"Y\"\ninto an embedding \nand the projection\nThe theorem is true for the embedding.\nThe Gysin map for the projection is the Bott-periodicity isomorphism, which commutes with the Chern character,\nso the theorem holds in this general case also.\n\nAtiyah and Hirzebruch then specialised and refined in the case \"X\" = a point, where the condition becomes the existence of a spin structure on \"Y\". Corollaries are on Pontryagin classes and the J-homomorphism.\n"}
{"id": "28162944", "url": "https://en.wikipedia.org/wiki?curid=28162944", "title": "Robert Fountain (mental calculator)", "text": "Robert Fountain (mental calculator)\n\nRobert Fountain is a British mental calculator. He won the first Mental Calculation World Cup in 2004 and the second Mental Calculation World Cup in 2006. In 1999 he was recognised by the Mind Sports Organisation as the first Grandmaster of Mental Calculation. Fountain was inspired to take up mental calculation as a hobby at the age of 11 after seeing Wim Klein performing on television. He is co-author of \"The Mental Calculator's Handbook\".\n\n"}
{"id": "27135564", "url": "https://en.wikipedia.org/wiki?curid=27135564", "title": "Sign sequence", "text": "Sign sequence\n\nIn mathematics, a sign sequence, or ±1–sequence or bipolar sequence, is a sequence of numbers, each of which is either 1 or −1. One example is the sequence (1, −1, 1, −1 ...).\n\nSuch sequences are commonly studied in discrepancy theory.\n\nAround 1932, mathematician Paul Erdős conjectured that for any infinite ±1-sequence formula_1 and any integer \"C\", there exist integers \"k\" and \"d\" such that\n\nThe Erdős Discrepancy Problem asks for a proof or disproof of this conjecture.\n\nIn February 2014, Alexei Lisitsa and Boris Konev of the University of Liverpool showed that every sequence of 1161 or more elements satisfies the conjecture in the special case \"C\" = 2, which proves the conjecture for \"C\" ≤ 2. This was the best such bound available at the time. Their proof relied on a SAT-solver computer algorithm whose output takes up 13 gigabytes of data, more than the entire text of Wikipedia at that time, so it cannot be independently verified by human mathematicians without further use of a computer.\n\nIn September 2015, Terence Tao announced a proof of the conjecture, building on work done in 2010 during Polymath5 (a form of crowdsourcing applied to mathematics) and a suggestion made by German mathematician Uwe Stroinski on Tao's blog. His proof was published in 2016, as the first paper in the new journal \"Discrete Analysis\".\n\nA Barker code is a sequence of \"N\" values of +1 and −1,\nsuch that\nfor all formula_5.\n\nBarker codes of lengths 11 and 13 are used in direct-sequence spread spectrum and pulse compression radar systems because of their low autocorrelation properties.\n\n\n"}
{"id": "21620407", "url": "https://en.wikipedia.org/wiki?curid=21620407", "title": "Supersolvable arrangement", "text": "Supersolvable arrangement\n\nIn mathematics, a supersolvable arrangement is a hyperplane arrangement which has a maximal flag with only modular elements.\nA complex hyperplane arrangement is supersolvable if and only if its complement is fiber-type. \n\nExamples include arrangements associated with Coxeter groups of type A and B.\n\nIt is known that all Orlik–Solomon algebras of supersolvable arrangements are Koszul algebras; whether the converse is true is an open problem.\n"}
{"id": "57047243", "url": "https://en.wikipedia.org/wiki?curid=57047243", "title": "Symmetric power", "text": "Symmetric power\n\nIn mathematics, the \"n\"-th symmetric power of an object \"X\" is the quotient of the \"n\"-fold product formula_1 by the permutation action of the symmetric group formula_2.\n\nMore precisely, the notion exists at least in the following three areas:\n"}
{"id": "22019166", "url": "https://en.wikipedia.org/wiki?curid=22019166", "title": "Toshikazu Sunada", "text": "Toshikazu Sunada\n\nSunada's work covers complex analytic geometry, spectral geometry, dynamical systems, probability, graph theory, and discrete geometric analysis. Among his numerous contributions, the most famous one is a general construction of isospectral manifolds (1985), which is based on his geometric model of number theory, and is considered to be a breakthrough in the problem proposed by Mark Kac in \"Can one hear the shape of a drum?\" (see Hearing the shape of a drum). Sunada's idea was taken up by C. Gordon, D. Webb, and S. Wolpert when they constructed a counterexample for Kac's problem. For this work, Sunada was awarded the Iyanaga Prize of the Mathematical Society of Japan (MSJ) in 1987. He was also awarded Publication Prize of MSJ in 2013, the Hiroshi Fujiwara Prize for Mathematical Sciences in 2017, and the Prize for Science and Technology (the Commendation for Science and Technology by the Minister of Education, Culture, Sports, Science and Technology) in 2018.\n\nIn a joint work with Atsushi Katsuda, Sunada also established a geometric analogue of Dirichlet's theorem on arithmetic progressions in the context of dynamical systems (1988). One can see, in this work as well as the one above, how the concepts and ideas in totally different fields (geometry, dynamical systems, and number theory) are put together to formulate problems and to produce new results.\n\nHis study of discrete geometric analysis includes a graph-theoretic interpretation of Ihara zeta functions, a discrete analogue of periodic magnetic Schrödinger operators as well as the large time asymptotic behaviors of random walk on crystal lattices. The study of random walk led him to the discovery of a \"mathematical twin\" of the diamond crystal out of an infinite universe of hypothetical crystals (2005). He named it the K crystal due to its mathematical relevance (see the linked article). What was noticed by him is that the K crystal has the \"strong isotropy property\", meaning that for any two vertices \"x\" and \"y\" of the crystal net, and for any ordering of the edges adjacent to \"x\" and any ordering of the edges adjacent to \"y\", there is a net-preserving congruence taking \"x\" to \"y\" and each \"x\"-edge to the similarly ordered \"y\"-edge. This property is shared only by the diamond crystal \n(the strong isotropy should not be confused with the edge-transitivity or the notion of symmetric graph; for instance, the primitive cubic lattice is a symmetric graph, but not strongly isotropic). The K crystal and the diamond crystal as networks in space are examples of “standard realizations”, the notion introduced by Sunada and M. Kotani as a graph-theoretic version of Albanese maps (Abel-Jacobi maps) in algebraic geometry.\n\nFor his work, see also Isospectral, Reinhardt domain, Ihara zeta function, Ramanujan graph, quantum ergodicity, quantum walk.\n\n"}
{"id": "927644", "url": "https://en.wikipedia.org/wiki?curid=927644", "title": "Underwriting", "text": "Underwriting\n\nUnderwriting services are provided by some large specialist financial institutions, such as banks, insurance or investment houses, whereby they guarantee payment in case of damage or financial loss and accept the financial risk for liability arising from such guarantee. An underwriting arrangement may be created in a number of situations including insurance, issue of securities in a public offering, and in bank lending, among others.\n\nThe name derives from the Lloyd's of London insurance market. Financial bankers, who would accept some of the risk on a given venture (historically a sea voyage with associated risks of shipwreck) in exchange for a premium, would literally write their names under the risk information that was written on a Lloyd's slip created for this purpose.\n\nSecurities underwriting is the process by which investment banks raise investment capital from investors on behalf of corporations and governments that are issuing securities (both equity and debt capital). The services of an underwriter are typically used during a public offering in a primary market.\n\nThis is a way of distributing a newly issued security, such as stocks or bonds, to investors. A syndicate of banks (the lead managers) underwrites the transaction, which means they have taken on the risk of distributing the securities. Should they not be able to find enough investors, they will have to hold some securities themselves. Underwriters make their income from the price difference (the \"underwriting spread\") between the price they pay the issuer and what they collect from investors or from broker-dealers who buy portions of the offering.\n\nOnce the underwriting agreement is struck, the underwriter bears the risk of being unable to sell the underlying securities, and the cost of holding them on its books until such time in the future that they may be favorably sold.\n\nIf the instrument is desirable, the underwriter and the securities issuer may choose to enter into an exclusivity agreement. In exchange for a higher price paid upfront to the issuer, or other favorable terms, the issuer may agree to make the underwriter the exclusive agent for the initial sale of the securities instrument. That is, even though third-party buyers might approach the issuer directly to buy, the issuer agrees to sell exclusively through the underwriter.\n\nIn summary, the securities issuer gets cash up front, access to the contacts and sales channels of the underwriter, and is insulated from the market risk of being unable to sell the securities at a good price. The underwriter gets a profit from the markup, plus possibly an exclusive sales agreement.\n\nAlso if the securities are priced significantly below market price (as is often the custom), the underwriter also curries favor with powerful end customers by granting them an immediate profit (see flipping), perhaps in a quid pro quo. This practice, which is typically justified as the reward for the underwriter for taking on the market risk, is occasionally criticized as unethical, such as the allegations that Frank Quattrone acted improperly in doling out hot IPO stock during the dot com bubble.\n\nIn banking, underwriting is the detailed credit analysis preceding the granting of a loan, based on credit information furnished by the borrower; such underwriting falls into several areas:\n\n\nUnderwriting can also refer to the purchase of corporate bonds, commercial paper, government securities, municipal general-obligation bonds by a commercial bank or dealer bank for its own account or for resale to investors. Bank underwriting of corporate securities is carried out through separate holding-company affiliates, called securities affiliates or Section 20 affiliates.\n\nInsurance underwriters evaluate the risk and exposures of potential clients. They decide how much coverage the client should receive, how much they should pay for it, or whether even to accept the risk and insure them. Underwriting involves measuring risk exposure and determining the premium that needs to be charged to insure that risk. The function of the underwriter is to protect the company's book of business from risks that they feel will make a loss and issue insurance policies at a premium that is commensurate with the exposure presented by a risk.\n\nEach insurance company has its own set of underwriting guidelines to help the underwriter determine whether or not the company should accept the risk. The information used to evaluate the risk of an applicant for insurance will depend on the type of coverage involved. For example, in underwriting automobile coverage, an individual's driving record is critical. However, the type of automobile is actually far more critical. As part of the underwriting process for life or health insurance, medical underwriting may be used to examine the applicant's health status (other factors may be considered as well, such as age & occupation). The factors that insurers use to classify risks are generally objective, clearly related to the likely cost of providing coverage, practical to administer, consistent with applicable law, and designed to protect the long-term viability of the insurance program.\n\nThe underwriters may decline the risk or may provide a quotation in which the premiums have been loaded (including the amount needed to generate a profit, in addition to covering expenses) or in which various exclusions have been stipulated, which restrict the circumstances under which a claim would be paid. Depending on the type of insurance product (line of business), insurance companies use automated underwriting systems to encode these rules, and reduce the amount of manual work in processing quotations and policy issuance. This is especially the case for certain simpler life or personal lines (auto, homeowners) insurance. Some insurance companies, however, rely on agents to underwrite for them. This arrangement allows an insurer to operate in a market closer to its clients without having to establish a physical presence.\n\nTwo major categories of exclusion in insurance underwriting are moral hazard and correlated losses. With a moral hazard, the consequences of the customer's actions are insured, making the customer more likely to take costly actions. For example, bedbugs are typically excluded from homeowners' insurance to avoid paying for the consequence of recklessly bringing in a used mattress. Insured events are generally those outside the control of the customer, for example (typical in life insurance) death by automobile accident, contrasted with death by suicide. Correlated losses are those that can affect a large number of customers at the same time, thus potentially bankrupting the insurance company. This is why typical homeowner's policies cover damage from fire or falling trees (usually affecting an individual house), but not floods or earthquakes (which affect many houses at the same time).\n\nIn evaluation of a real estate loan, in addition to assessing the borrower, the property itself is scrutinized. Underwriters use the debt service coverage ratio to figure out whether the property is capable of redeeming its own value.\n\nForensic underwriting is the \"after-the-fact\" process used by lenders to determine what went wrong with a mortgage. Forensic underwriting is a borrower's ability to work out a modification scenario with their current lien holder, not to qualify them for a new loan or a refinance. This is typically done by an underwriter staffed with a team of people who are experienced in every aspect of the real estate field.\n\nUnderwriting may also refer to financial sponsorship of a venture, and is also used as a term within public broadcasting (both public television and radio) to describe funding given by a company or organization for the operations of the service, in exchange for a mention of their product or service within the station's programming.\n\nUnderwriting activity in the mergers and acquisitions, equity issuance, debt issuance, syndicated loans and U.S. municipal bond markets is reported in the Thomson Financial league tables.\n\n\n"}
{"id": "48307461", "url": "https://en.wikipedia.org/wiki?curid=48307461", "title": "Weak measurement", "text": "Weak measurement\n\nIn quantum mechanics (and computation & information), weak measurements are a type of quantum measurement that results in an observer obtaining very little information about the system on average, but also disturbs the state very little. From Busch's theorem the system is necessarily disturbed by the measurement. In the literature weak measurements are also known as unsharp, fuzzy, dull, noisy, approximate, and gentle measurements. Additionally weak measurements are often confused with the distinct but related concept of the weak value.\n\nWeak measurements were first thought about in the context of weak continuous measurements of quantum systems (i.e. quantum filtering and quantum trajectories). The physics of continuous quantum measurements is as follows. Consider using an ancilla, e.g. a field or a current, to probe a quantum system. The interaction between the system and the probe correlates the two systems. Typically the interaction only weakly correlates the system and ancilla. (Specifically, the interaction unitary need only to be expanded to first or second order in perturbation theory.) By measuring the ancilla and then using quantum measurement theory, the state of the system conditioned on the results of the measurement can be determined. In order to obtain a strong measurement, many ancilla must be coupled and then measured. In the limit where there is a continuum of ancilla the measurement process becomes continuous in time. This process was described first by:\nMensky; Belavkin; Barchielli, Lanz, Prosperi; Barchielli; Caves; Caves and Milburn. Later on Howard Carmichael and Howard M. Wiseman also made important contributions to the field.\n\nIt should be noted that the notion of a weak measurement is often misattributed to Aharonov, Albert and Vaidman. In their article they consider an example of a weak measurement (and perhaps coin the phrase \"weak measurement\") and use it to motivate their definition of a weak value, which they defined there for the first time.\n\nThere is no universally accepted definition of a weak measurement. One approach is to declare a weak measurement to be a generalized measurement where some or all of the Kraus operators are close to the identity. The approach taken below is to interact two systems weakly and then measure one of them. After detailing this approach we will illustrate it with examples.\n\nConsider a system that starts in the quantum state formula_1 and an ancilla that starts in the state formula_2, the combined initial state is formula_3. These two systems interact via the Hamiltonian formula_4, which generates the time evolutions formula_5 (in units where formula_6), where formula_7 is the \"interaction strength\", which has units of inverse time. Assume a fixed interaction time formula_8 and that formula_9 is small, such that formula_10. A series expansion of formula_11 in formula_12 gives\n\nBecause it was only necessary to expand the unitary to a low order in perturbation theory, we call this is a weak interaction. Further, the fact that the unitary is predominately the identity operator, as formula_12 and formula_15 are small, implies that the state after the interaction is not radically different from the initial state. The combined state of the system after interaction is\n\nNow we perform a measurement on the ancilla to find out about the system, this is known as an ancilla-coupled measurement. We will consider measurements in a basis formula_17 (on the ancilla system) such that formula_18. The measurements action on both systems is described by the action of the projectors formula_19 on the joint state formula_20. From quantum measurement theory we know the conditional state after the measurement is\n\nwhere formula_22 is a normalization factor for the wavefunction. Notice the ancilla system state records the outcome of the measurement. The object formula_23 is an operator on the system Hilbert space and is called a Kraus operator.\n\nWith respect to the Kraus operators the post-measurement state of the combined system is\n\nThe objects formula_25 are elements of what is called a POVM and must obey formula_26 so that the corresponding probabilities sum to unity: formula_27. As the ancilla system is no longer correlated with the primary system, it is simply recording the outcome of the measurement, we can trace over it. Doing so gives the conditional state of the primary system alone:\n\nwhich we still label by the outcome of the measurement formula_29. Indeed, these considerations allow one to derive a quantum trajectory.\n\nWe will use the canonical example of Gaussian Kraus operators given by Barchielli, Lanz, Prosperi; and Caves and Milburn. Take formula_30, where the position and momentum on both systems have the usual Canonical commutation relation formula_31. Take the initial wavefunction of the ancilla to have a Gaussian distribution\n\nThe position wavefunction of the ancilla is\n\nThe Kraus operators are (compared to the discussion above, we set formula_34)\n\nwhile the corresponding POVM elements are\n\nwhich obey formula_37. An alternative representation is often seen in the literature. Using the spectral representation of the position operator formula_38, we can write\n\nNotice that formula_40. That is, in a particular limit these operators limit to a strong measurement of position; for other values of formula_41 we refer to the measurement as finite-strength; and as formula_42, we say the measurement is weak.\n\nAs stated above, Busch's theorem prevents a free lunch: there can be no information gain without disturbance. However, the tradeoff between information gain and disturbance has been characterized by many authors, including Fuchs and Peres; Fuchs; Fuchs and Jacobs; and Banaszek.\n\nRecently the information-gain–disturbance tradeoff relation has been examined in the context of what is called the \"gentle-measurement lemma\".\n\nSince the early days it has been clear that the primary use of weak measurement would be for feedback control or adaptive measurements of quantum systems. Indeed, this motivated much of Belavkin's work, and an explicit example was given by Caves and Milburn. An early application of an adaptive weak measurements was that of Dolinar's receiver, which has been realized experimentally Another interesting application of weak measurements is to use weak measurements followed by a unitary to synthesize other generalized measurements. Wiseman and Milburn's book is a good reference for many of the modern developments.\n\n"}
{"id": "57935108", "url": "https://en.wikipedia.org/wiki?curid=57935108", "title": "William J. Firey", "text": "William J. Firey\n\nWilliam James Firey (1923–2004) was an American mathematician, specializing in the geometry of convex bodies.\n\nBorn in Montana, Firey moved with his family to Seattle when he was 6 years old. During WW II, he served in the U.S. Army as a medical technician in Europe. He married in 1946. During the first years of their marriage, the couple worked for the United States Forest Service during summers in fire look-out stations in the Washington Cascades.\n\nFirey received in 1948 his bachelor's degree from the University of Washington, in 1949 his master's degree from the University of Toronto, and in 1954 his Ph.D. from Stanford University. He was a faculty member at Washington State University for 8 years and then became a professor at Oregon State University, where he retired as professor emeritus in 1988. He was a visiting professor at several universities and made several trips to the Mathematical Research Institute of Oberwolfach.\n\nIn 1974 Firey was an Invited Speaker at the International Congress of Mathematicians in Vancouver.\n\nUpon his death he was survived by his widow and his daughter and predeceased by his son.\n\n"}
