{"id": "4973089", "url": "https://en.wikipedia.org/wiki?curid=4973089", "title": "AP Statistics", "text": "AP Statistics\n\nAdvanced Placement Statistics (AP Statistics, AP Stat or AP Stats) is a college-level high school statistics course offered in the United States through the College Board's Advanced Placement program. This course is equivalent to a one semester, non-calculus-based introductory college statistics course and is normally offered to juniors and seniors in high school.\n\nOne of the College Board's more recent additions, the AP Statistics exam was first administered in May 1996 to supplement the AP program's math offerings, which had previously consisted of only AP Calculus AB and BC. In the United States, enrollment in AP Statistics classes has increased at a higher rate than in any other AP class.\n\nStudents may receive college credit or upper-level college course placement upon the successful completion of a three-hour exam ordinarily administered in May. The exam consists of a multiple choice section and a free response section that are both 90 minutes long. Each section is weighted equally in determining the students' composite scores.\n\nThe Advanced Placement program has offered students the opportunity to pursue college-level courses while in high school. Along with the Educational Testing Service, the College Board administered the first AP Statistics exam in May 1997. The course was first taught to students in the 1996-1997 academic year. Prior to that, the only mathematics courses offered in the AP program included AP Calculus AB and BC. Students who didn't have a strong background in college-level math, however, found the AP Calculus program inaccessible and sometimes declined to take a math course in their senior year. Since the number of students required to take statistics in college is almost as large as the number of students required to take calculus, the College Board decided to add an introductory statistics course to the AP program. Since the prerequisites for such a program doesn't require mathematical concepts beyond those typically taught in a second-year algebra course, the AP program's math offerings became accessible to a much wider audience of high school students. The AP Statistics program addressed a practical need as well, since the number of students enrolling in majors that use statistics has grown. A total of 7,667 students took the exam during the first administration, which is the highest number of students to take an AP exam in its first year. Since then, the number of students taking the exam rapidly grew to 98,033 in 2007, making it one of the 10 largest AP exams.\n\nIf the course is provided by their school, students normally take AP Statistics in their junior or senior year and may decide to take it concurrently with a pre-calculus course. This offering is intended to imitate a one-semester, non-calculus based college statistics course, but high schools can decide to offer the course over one semester, two trimesters, or a full academic year.\n\nThe six-member AP Statistics Test Development Committee is responsible for developing the curriculum. Appointed by the College Board, the committee consists of three college statistics teachers and three high school statistics teachers who are typically asked to serve for terms of three years.\n\nEmphasis is placed not on actual arithmetic computation, but rather on conceptual understanding and interpretation. The course curriculum is organized around four basic themes; the first involves exploring data and covers 20–30% of the exam. Students are expected to use graphical and numerical techniques to analyze distributions of data, including univariate, bivariate, and categorical data. The second theme involves planning and conducting a study and covers 10–15% of the exam. Students must be aware of the various methods of data collection through sampling or experimentation and the sorts of conclusions that can be drawn from the results. The third theme involves probability and its role in anticipating patterns in distributions of data. This theme covers 20–30% of the exam. The fourth theme, which covers 30–40% of the exam, involves statistical inference using point estimation, confidence intervals, and significance tests.\n\nAlong with the course curriculum, the exam is developed by the AP Statistics Test Development Committee as well. With the help of other college professors, the committee creates a large pool of possible questions that is pre-tested with college students taking statistics courses. The test is then refined to an appropriate level of difficulty and clarity. Afterwards, the Educational Testing Service is responsible for printing and administering the exam.\n\nThe exam is offered every year in May. Students are not expected to memorize any formulas; rather, a list of common statistical formulas related to descriptive statistics, probability, and inferential statistics is provided. Moreover, tables for the normal, Student's t and chi-squared distributions are given as well. Students are also expected to use graphing calculators with statistical capabilities. The exam is three hours long with ninety minutes allotted to complete each of its two sections: multiple choice and free-response. The multiple choice portion of the exam consists of forty questions with five possible answers each. The free response section contains six open-ended questions that are often long and divided into multiple parts. The first five of these questions may require twelve minutes each to answer and normally relate to one topic or category. The sixth question consists of a broad-ranging investigative task and may require approximately twenty-five minutes to answer.\n\nThe multiple choice section is scored immediately after the exam by computer. One point is awarded for each correct answer, no points are credited or deducted for unanswered questions, and points are no longer deducted for having an incorrect answer.\n\nStudents' answers to the free-response section are reviewed in early June by readers that include high school and college statistics teachers gathered in a designated location. The readers use a pre-made rubric to assess the answers and normally grade only one question in a given exam. Each question is graded on a scale from 0 to 4, with a 4 representing the most complete response. Communication and clarity in the answers receive a lot of emphasis in the grading.\n\nBoth sections are weighted equally when the composite score is calculated. The composite score is reported on a scale from 1 to 5, with a score of 5 being the highest possible.\n\n\n\n\n\n"}
{"id": "655", "url": "https://en.wikipedia.org/wiki?curid=655", "title": "Abacus", "text": "Abacus\n\nThe abacus (\"plural\" abaci or abacuses), also called a counting frame, is a calculating tool that was in use in Europe, China and Russia, centuries before the adoption of the written Hindu–Arabic numeral system. The exact origin of the abacus is still unknown. Today, abacuses are often constructed as a bamboo frame with beads sliding on wires, but originally they were beans or stones moved in grooves in sand or on tablets of wood, stone, or metal.\n\nAbacuses come in different designs. Some designs, like the bead frame consisting of beads divided into tens, are used mainly to teach arithmetic, although they remain popular in the post-Soviet states as a tool. Other designs, such as the Japanese soroban, have been used for practical calculations even involving several digits. For any particular abacus design, there usually are numerous different methods to perform a certain type of calculation, which may include basic operations like addition and multiplication, or even more complex ones, such as calculating square roots. Some of these methods may work with non-natural numbers (numbers such as and ).\n\nAlthough today many use calculators and computers instead of abacuses to calculate, abacuses still remain in common use in some countries. Merchants, traders and clerks in some parts of Eastern Europe, Russia, China and Africa use abacuses, and they are still used to teach arithmetic to children. Some people who are unable to use a calculator because of visual impairment may use an abacus.\n\nThe use of the word \"abacus\" dates before 1387 AD, when a Middle English work borrowed the word from Latin to describe a sandboard abacus. The Latin word came from Greek ἄβαξ \"abax\" which means something without base, and improperly, any piece of rectangular board or plank. \nAlternatively, without reference to ancient texts on etymology, it has been suggested that it means \"a square tablet strewn with dust\", or \"drawing-board covered with dust (for the use of mathematics)\" (the exact shape of the Latin perhaps reflects the genitive form of the Greek word, ἄβακoς \"abakos\"). Whereas the table strewn with dust definition is popular, there are those that do not place credence in this at all and in fact state that it is not proven. Greek ἄβαξ itself is probably a borrowing of a Northwest Semitic, perhaps Phoenician, word akin to Hebrew \"ʾābāq\" (אבק), \"dust\" (or in post-Biblical sense meaning \"sand used as a writing surface\").\n\nThe preferred plural of \"abacus\" is a subject of disagreement, with both \"abacuses\" and \"abaci\" (hard \"c\") in use. The user of an abacus is called an \"abacist\".\n\nThe period 2700–2300 BC saw the first appearance of the Sumerian abacus, a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system.\n\nSome scholars point to a character from the Babylonian cuneiform which may have been derived from a representation of the abacus. It is the belief of Old Babylonian scholars such as Carruccio that Old Babylonians \"may have used the abacus for the operations of addition and subtraction; however, this primitive device proved difficult to use for more complex calculations\".\n\nThe use of the abacus in Ancient Egypt is mentioned by the Greek historian Herodotus, who writes that the Egyptians manipulated the pebbles from right to left, opposite in direction to the Greek left-to-right method. Archaeologists have found ancient disks of various sizes that are thought to have been used as counters. However, wall depictions of this instrument have not been discovered.\n\nDuring the Achaemenid Empire, around 600 BC the Persians first began to use the abacus. Under the Parthian, Sassanian and Iranian empires, scholars concentrated on exchanging knowledge and inventions with the countries around them – India, China, and the Roman Empire, when it is thought to have been exported to other countries.\n\nThe earliest archaeological evidence for the use of the Greek abacus dates to the 5th century BC. Also Demosthenes (384 BC–322 BC) talked of the need to use pebbles for calculations too difficult for your head. A play by Alexis from the 4th century BC mentions an abacus and pebbles for accounting, and both Diogenes and Polybius mention men that sometimes stood for more and sometimes for less, like the pebbles on an abacus. The Greek abacus was a table of wood or marble, pre-set with small counters in wood or metal for mathematical calculations. This Greek abacus saw use in Achaemenid Persia, the Etruscan civilization, Ancient Rome and, until the French Revolution, the Western Christian world.\n\nA tablet found on the Greek island Salamis in 1846 AD (the Salamis Tablet), dates back to 300 BC, making it the oldest counting board discovered so far. It is a slab of white marble long, wide, and thick, on which are 5 groups of markings. In the center of the tablet is a set of 5 parallel lines equally divided by a vertical line, capped with a semicircle at the intersection of the bottom-most horizontal line and the single vertical line. Below these lines is a wide space with a horizontal crack dividing it. Below this crack is another group of eleven parallel lines, again divided into two sections by a line perpendicular to them, but with the semicircle at the top of the intersection; the third, sixth and ninth of these lines are marked with a cross where they intersect with the vertical line. Also from this time frame the \"Darius Vase\" was unearthed in 1851. It was covered with pictures including a \"treasurer\" holding a wax tablet in one hand while manipulating counters on a table with the other.\n\nThe earliest known written documentation of the Chinese abacus dates to the 2nd century BC.\n\nThe Chinese abacus, known as the \"suanpan\" (算盤, lit. \"calculating tray\"), is typically tall and comes in various widths depending on the operator. It usually has more than seven rods. There are two beads on each rod in the upper deck and five beads each in the bottom. The beads are usually rounded and made of a hardwood. The beads are counted by moving them up or down towards the beam; beads moved toward the beam are counted, while those moved away from it are not. The \"suanpan\" can be reset to the starting position instantly by a quick movement along the horizontal axis to spin all the beads away from the horizontal beam at the center.\n\nThe prototype of the Chinese abacus is the appeared during the Han Dynasty, and the beads are oval. In the Song Dynasty or before used the 4:1 type or four beads abacus similar to the modern abacus or commony known as Japanese style abacus, \"you can make a number by hand,\" and \"beads are counted\", which can be expressed as a decimal number. Therefore, the abacus is designed as a four-bead abacus.\n\nIn the early Ming Dynasty, the abacus began to appear in the form of 1:5 abacus. The upper deck had one bead and the bottom had five beads. \"you can make a number by hand,\" and \"the number of beads will be counted\". Binary or any of the following numbers, so the abacus is designed as a five-bead abacus.\n\nIn the late Ming Dynasty, the abacus styles that appeared in the form of 2:5. The upper deck had two beads, and the bottom had five beads. \"You can make a number by hand,\" and \"Beads are counted.\" It can be expressed in hexadecimal or any of the following numbers, and because the calculation method at that time is a Chinese catty equal to sixteen tael（一斤十六兩）which means hexadecimal, the abacus is designed as a two-five bead.\n\n\"Suanpan\" can be used for functions other than counting. Unlike the simple counting board used in elementary schools, very efficient suanpan techniques have been developed to do multiplication, division, addition, subtraction, square root and cube root operations at high speed. There are currently schools teaching students how to use it.\n\nIn the long scroll \"Along the River During the Qingming Festival\" painted by Zhang Zeduan during the Song dynasty (960–1297), a \"suanpan\" is clearly visible beside an account book and doctor's prescriptions on the counter of an apothecary's (Feibao).\n\nThe similarity of the Roman abacus to the Chinese one suggests that one could have inspired the other, as there is some evidence of a trade relationship between the Roman Empire and China. However, no direct connection can be demonstrated, and the similarity of the abacuses may be coincidental, both ultimately arising from counting with five fingers per hand. Where the Roman model (like most modern Korean and Japanese) has 4 plus 1 bead per decimal place, the standard \"suanpan\" has 5 plus 2. (Incidentally, this allows use with a hexadecimal numeral system, which was used for traditional Chinese measures of weight.) Instead of running on wires as in the Chinese, Korean, and Japanese models, the beads of Roman model run in grooves, presumably making arithmetic calculations much slower.\n\nAnother possible source of the \"suanpan\" is Chinese counting rods, which operated with a decimal system but lacked the concept of zero as a place holder. The zero was probably introduced to the Chinese in the Tang dynasty (618–907) when travel in the Indian Ocean and the Middle East would have provided direct contact with India, allowing them to acquire the concept of zero and the decimal point from Indian merchants and mathematicians.\n\nThe normal method of calculation in ancient Rome, as in Greece, was by moving counters on a smooth table. Originally pebbles (\"calculi\") were used. Later, and in medieval Europe, jetons were manufactured. Marked lines indicated units, fives, tens etc. as in the Roman numeral system. This system of 'counter casting' continued into the late Roman empire and in medieval Europe, and persisted in limited use into the nineteenth century. Due to Pope Sylvester II's reintroduction of the abacus with modifications, it became widely used in Europe once again during the 11th century This abacus used beads on wires, unlike the traditional Roman counting boards, which meant the abacus could be used much faster.\n\nWriting in the 1st century BC, Horace refers to the wax abacus, a board covered with a thin layer of black wax on which columns and figures were inscribed using a stylus.\n\nOne example of archaeological evidence of the Roman abacus, shown here in reconstruction, dates to the 1st century AD. It has eight long grooves containing up to five beads in each and eight shorter grooves having either one or no beads in each. The groove marked I indicates units, X tens, and so on up to millions. The beads in the shorter grooves denote fives –five units, five tens etc., essentially in a bi-quinary coded decimal system, related to the Roman numerals. The short grooves on the right may have been used for marking Roman \"ounces\" (i.e. fractions).\n\nThe decimal number system invented in India replaced the abacus in Western Europe.\n\nThe \"Abhidharmakośabhāṣya\" of Vasubandhu (316-396), a Sanskrit work on Buddhist philosophy, says that the second-century CE philosopher Vasumitra said that \"placing a wick (Sanskrit \"vartikā\") on the number one (\"ekāṅka\") means it is a one, while placing the wick on the number hundred means it is called a hundred, and on the number one thousand means it is a thousand\". It is unclear exactly what this arrangement may have been. Around the 5th century, Indian clerks were already finding new ways of recording the contents of the Abacus. Hindu texts used the term \"śūnya\" (zero) to indicate the empty column on the abacus.\n\nIn Japanese, the abacus is called \"soroban\" (, lit. \"Counting tray\"), imported from China in the 14th century. It was probably in use by the working class a century or more before the ruling class started, as the class structure did not allow for devices used by the lower class to be adopted or used by the ruling class. The 1/4 abacus, which is suited to decimal calculation popular appeared circa 1930, and became widespread as the Japanese abandoned hexadecimal weight calculation which was still common in China.\n\nToday's Japanese abacus is a 1:4 type, four-bead abacus was introduced from China in the Muromachi era. It adopts the form of the upper deck one bead and the bottom four beads. The top bead on the upper deck was equal to five and the bottom one is equal to one like the Chinese or Korean abacus , and the decimal number can be expressed, so the abacus is designed as one four abacus. The beads are always in the shape of a diamond. The quotient division is generally used instead of the division method; at the same time, in order to make the multiplication and division digits consistently use the division multiplication. Later, Japan had a 3:5 abacus called天三算盤, which is now the Ize Rongji collection of Shansi Village in Yamagata City. There were also had 2:5 beads abacus.\nWith the four-bead abacus spread, it is also common to use Japanese abacus around the world. There are also improved Japanese abacus in various places. One of the Japanese-made abacus made in China is an aluminum frame plastic bead abacus. The file is next to the four beads, and the \"clearing\" button, press the clearing button, immediately put the upper bead to the upper position, the lower bead is dialed to the lower position, immediately clearing, easy to use.\n\nThe abacus is still manufactured in Japan today even with the proliferation, practicality, and affordability of pocket electronic calculators. The use of the soroban is still taught in Japanese primary schools as part of mathematics, primarily as an aid to faster mental calculation. Using visual imagery of a soroban, one can arrive at the answer in the same time as, or even faster than, is possible with a physical instrument.\n\nThe Chinese abacus migrated from China to Korea around 1400 AD. Koreans call it \"jupan\" (주판), \"supan\" (수판) or \"jusan\" (주산).\nThe four beads abacus( 1:4 ) was introduced to Korea Goryeo Dynaty from the China during Song Dynasty, later the five beads abacus (5:1) abacus was introduced to Korean from China during the Ming Dynasty.\n\nSome sources mention the use of an abacus called a \"nepohualtzintzin\" in ancient Aztec culture. This Mesoamerican abacus used a 5-digit base-20 system.\nThe word Nepōhualtzintzin comes from Nahuatl and it is formed by the roots; \"Ne\" – personal -; \"pōhual\" or \"pōhualli\" – the account -; and \"tzintzin\" – small similar elements. Its complete meaning was taken as: counting with small similar elements by somebody. Its use was taught in the Calmecac to the \"temalpouhqueh\" , who were students dedicated to take the accounts of skies, from childhood.\n\nThe Nepōhualtzintzin was divided in two main parts separated by a bar or intermediate cord. In the left part there were four beads, which in the first row have unitary values (1, 2, 3, and 4), and in the right side there are three beads with values of 5, 10, and 15 respectively. In order to know the value of the respective beads of the upper rows, it is enough to multiply by 20 (by each row), the value of the corresponding account in the first row.\n\nAltogether, there were 13 rows with 7 beads in each one, which made up 91 beads in each Nepōhualtzintzin. This was a basic number to understand, 7 times 13, a close relation conceived between natural phenomena, the underworld and the cycles of the heavens. One Nepōhualtzintzin (91) represented the number of days that a season of the year lasts, two Nepōhualtzitzin (182) is the number of days of the corn's cycle, from its sowing to its harvest, three Nepōhualtzintzin (273) is the number of days of a baby's gestation, and four Nepōhualtzintzin (364) completed a cycle and approximate a year (1 days short). When translated into modern computer arithmetic, the Nepōhualtzintzin amounted to the rank from 10 to the 18 in floating point, which calculated stellar as well as infinitesimal amounts with absolute precision, meant that no round off was allowed.\n\nThe rediscovery of the Nepōhualtzintzin was due to the Mexican engineer David Esparza Hidalgo, who in his wanderings throughout Mexico found diverse engravings and paintings of this instrument and reconstructed several of them made in gold, jade, encrustations of shell, etc. There have also been found very old Nepōhualtzintzin attributed to the Olmec culture, and even some bracelets of Mayan origin, as well as a diversity of forms and materials in other cultures.\n\nGeorge I. Sanchez, \"Arithmetic in Maya\", Austin-Texas, 1961 found another base 5, base 4 abacus in the Yucatán Peninsula that also computed calendar data. This was a finger abacus, on one hand 0, 1, 2, 3, and 4 were used; and on the other hand 0, 1, 2 and 3 were used. Note the use of zero at the beginning and end of the two cycles. Sanchez worked with Sylvanus Morley, a noted Mayanist.\n\nThe quipu of the Incas was a system of colored knotted cords used to record numerical data, like advanced tally sticks – but not used to perform calculations. Calculations were carried out using a yupana (Quechua for \"counting tool\"; see figure) which was still in use after the conquest of Peru. The working principle of a yupana is unknown, but in 2001 an explanation of the mathematical basis of these instruments was proposed by Italian mathematician Nicolino De Pasquale. By comparing the form of several yupanas, researchers found that calculations were based using the Fibonacci sequence 1, 1, 2, 3, 5 and powers of 10, 20 and 40 as place values for the different fields in the instrument. Using the Fibonacci sequence would keep the number of grains within any one field at a minimum.\n\nThe Russian abacus, the \"schoty\" (счёты), usually has a single slanted deck, with ten beads on each wire (except one wire, usually positioned near the user, with four beads for quarter-ruble fractions). Older models have another 4-bead wire for quarter-kopeks, which were minted until 1916. The Russian abacus is often used vertically, with wires from left to right in the manner of a book. The wires are usually bowed to bulge upward in the center, to keep the beads pinned to either of the two sides. It is cleared when all the beads are moved to the right. During manipulation, beads are moved to the left. For easy viewing, the middle 2 beads on each wire (the 5th and 6th bead) usually are of a different color from the other eight beads. Likewise, the left bead of the thousands wire (and the million wire, if present) may have a different color.\n\nAs a simple, cheap and reliable device, the Russian abacus was in use in all shops and markets throughout the former Soviet Union, and the usage of it was taught in most schools until the 1990s. Even the 1874 invention of mechanical calculator, Odhner arithmometer, had not replaced them in Russia and likewise the mass production of Felix arithmometers since 1924 did not significantly reduce their use in the Soviet Union. The Russian abacus began to lose popularity only after the mass production of microcalculators had started in the Soviet Union in 1974. Today it is regarded as an archaism and replaced by the handheld calculator.\n\nThe Russian abacus was brought to France around 1820 by the mathematician Jean-Victor Poncelet, who served in Napoleon's army and had been a prisoner of war in Russia. The abacus had fallen out of use in western Europe in the 16th century with the rise of decimal notation and algorismic methods. To Poncelet's French contemporaries, it was something new. Poncelet used it, not for any applied purpose, but as a teaching and demonstration aid. The Turks and the Armenian people also used abacuses similar to the Russian schoty. It was named a \"coulba\" by the Turks and a \"choreb\" by the Armenians.\n\nAround the world, abacuses have been used in pre-schools and elementary schools as an aid in teaching the numeral system and arithmetic.\n\nIn Western countries, a bead frame similar to the Russian abacus but with straight wires and a vertical frame has been common (see image). It is still often seen as a plastic or wooden toy.\n\nThe wire frame may be used either with positional notation like other abacuses (thus the 10-wire version may represent numbers up to 9,999,999,999), or each bead may represent one unit (so that e.g. 74 can be represented by shifting all beads on 7 wires and 4 beads on the 8th wire, so numbers up to 100 may be represented). In the bead frame shown, the gap between the 5th and 6th wire, corresponding to the color change between the 5th and the 6th bead on each wire, suggests the latter use.\n\nThe red-and-white abacus is used in contemporary primary schools for a wide range of number-related lessons. The twenty bead version, referred to by its Dutch name \"rekenrek\" (\"calculating frame\"), is often used, sometimes on a string of beads, sometimes on a rigid framework.\n\nBy learning how to calculate with abacus, one can improve his mental calculation which becomes faster and more accurate in doing large number calculations. Abacus‐based mental calculation (AMC) was derived from the abacus which means doing calculation, including addition, subtraction, multiplication, and division, in mind with an imaged abacus. It is a high-level cognitive skill that run through calculations with an effective algorithm. People doing long-term AMC training shows higher numerical memory capacity and has more effectively connected neural pathways. They are able to retrieve memory to deal with complex processes to calculate. The processing of AMC involves both the visuospatial and visuomotor processing which generate the visual abacus and perform the movement of the imagery bead. Since the only thing needed to be remembered is the finial position of beads, it takes less memory and less computation time.\n\nAn adapted abacus, invented by Tim Cranmer, called a Cranmer abacus is still commonly used by individuals who are blind. A piece of soft fabric or rubber is placed behind the beads so that they do not move inadvertently. This keeps the beads in place while the users feel or manipulate them. They use an abacus to perform the mathematical functions multiplication, division, addition, subtraction, square root and cube root.\n\nAlthough blind students have benefited from talking calculators, the abacus is still very often taught to these students in early grades, both in public schools and state schools for the blind. The abacus teaches mathematical skills that can never be replaced with talking calculators and is an important learning tool for blind students. Blind students also complete mathematical assignments using a braille-writer and Nemeth code (a type of braille code for mathematics) but large multiplication and long division problems can be long and difficult. The abacus gives blind and visually impaired students a tool to compute mathematical problems that equals the speed and mathematical knowledge required by their sighted peers using pencil and paper. Many blind people find this number machine a very useful tool throughout life.\n\nThe binary abacus is used to explain how computers manipulate numbers. The abacus shows how numbers, letters, and signs can be stored in a binary system on a computer, or via ASCII. The device consists of a series of beads on parallel wires arranged in three separate rows. The beads represent a switch on the computer in either an \"on\" or \"off\" position.\n\n\n\n\n"}
{"id": "61532", "url": "https://en.wikipedia.org/wiki?curid=61532", "title": "Absolute convergence", "text": "Absolute convergence\n\nIn mathematics, an infinite series of numbers is said to converge absolutely (or to be absolutely convergent) if the sum of the absolute values of the summands is finite. More precisely, a real or complex series formula_1 is said to converge absolutely if formula_2 for some real number formula_3. Similarly, an improper integral of a function, formula_4, is said to converge absolutely if the integral of the absolute value of the integrand is finite—that is, if formula_5\n\nAbsolute convergence is important for the study of infinite series because its definition is strong enough to have properties of finite sums that not all convergent series possess, yet is broad enough to occur commonly. (A convergent series that is not absolutely convergent is called conditionally convergent.) Absolutely convergent series behave \"nicely\". For instance, rearrangements do not change the value of the sum. This is not true for conditionally convergent series: The alternating harmonic series formula_6 converges to formula_7, while its rearrangement formula_8 (in which the repeating pattern of signs is two positive terms followed by one negative term) converges to formula_9.\n\nOne may study the convergence of series formula_10 whose terms \"a\" are elements of an arbitrary abelian topological group. The notion of absolute convergence requires more structure, namely a norm, which is a real-valued function formula_11 on abelian group \"G\" (written additively, with identity element 0) such that:\n\nIn this case, the function formula_17 induces on \"G\" the structure of a metric space (a type of topology). We can therefore consider \"G\"-valued series and define such a series to be absolutely convergent if formula_18\n\nIn particular, these statements apply using the norm |\"x\"| (absolute value) in the space of real numbers or complex numbers.\n\nIf \"G\" is complete with respect to the metric \"d\", then every absolutely convergent series is convergent. The proof is the same as for complex-valued series: use the completeness to derive the Cauchy criterion for convergence—a series is convergent if and only if its tails can be made arbitrarily small in norm—and apply the triangle inequality.\n\nIn particular, for series with values in any Banach space, absolute convergence implies convergence. The converse is also true: if absolute convergence implies convergence in a normed space, then the space is a Banach space.\n\nIf a series is convergent but not absolutely convergent, it is called conditionally convergent. An example of a conditionally convergent series is the alternating harmonic series. Many standard tests for divergence and convergence, most notably including the ratio test and the root test, demonstrate absolute convergence. This is because a power series is absolutely convergent on the interior of its disk of convergence.\n\nSuppose that formula_19 is convergent. Then equivalently, formula_20 is convergent, which implies that formula_21 and formula_22 converge by termwise comparison of non-negative terms. It suffices to show that the convergence of these series implies the convergence of formula_23 and formula_24, for then, the convergence of formula_25 would follow, by the definition of the convergence of complex-valued series. \n\nThe preceding discussion shows that we need only prove that convergence of formula_26 implies the convergence of formula_27.\n\nLet formula_26 be convergent. Since formula_29, we have\nSince formula_31 is convergent, formula_32 is a bounded monotonic sequence of partial sums, and formula_33 must also converge. Noting that formula_34 is the difference of convergent series, we conclude that it too is a convergent series, as desired.\n\nBy applying the Cauchy criterion for the convergence of a complex series, we can also prove this fact as a simple implication of the triangle inequality. By the Cauchy criterion, formula_35 converges if and only if for any formula_36, there exists formula_37 such that formula_38 for any formula_39. But the triangle inequality implies that formula_40, so that formula_41 for any formula_39, which is exactly the Cauchy criterion for formula_43.\n\nThe above result can be easily generalized to every Banach space . Let be an absolutely convergent series in \"X\". As formula_44 is a Cauchy sequence of real numbers, for any and large enough natural numbers it holds:\n\nBy the triangle inequality for the norm , one immediately gets:\nwhich means that formula_47 is a Cauchy sequence in \"X\", hence the series is convergent in \"X\".\n\nIn the general context of a \"G\"-valued series, a distinction is made between absolute and unconditional convergence, and the assertion that a real or complex series which is not absolutely convergent is necessarily conditionally convergent (meaning not unconditionally convergent) is then a theorem, not a definition. This is discussed in more detail below.\n\nGiven a series formula_48 with values in a normed abelian group \"G\" and a permutation σ of the natural numbers, one builds a new series formula_49, said to be a rearrangement of the original series. A series is said to be unconditionally convergent if all rearrangements of the series are convergent to the same value.\n\nWhen \"G\" is complete, absolute convergence implies unconditional convergence:\n\nThe issue of the converse is interesting. For real series it follows from the Riemann rearrangement theorem that unconditional convergence implies absolute convergence. Since a series with values in a finite-dimensional normed space is absolutely convergent if each of its one-dimensional projections is absolutely convergent, it follows that absolute and unconditional convergence coincide for R-valued series.\n\nBut there are unconditionally and non-absolutely convergent series with values in Banach space ℓ, for example:\n\nwhere formula_53 is an orthonormal basis. A theorem of A. Dvoretzky and C. A. Rogers asserts that every infinite-dimensional Banach space admits an unconditionally convergent series that is not absolutely convergent.\n\nFor any ε > 0, we can choose some formula_54, such that:\n\nLet\n\nFinally for any integer formula_57 let\n\nThen\n\nThis shows that\n\nthat is:\n\nQ.E.D.\n\nThe Cauchy product of two series converges to the product of the sums if at least one of the series converges absolutely. That is, suppose that\n\nThe Cauchy product is defined as the sum of terms \"c\" where:\n\nThen, if \"either\" the \"a\" or \"b\" sum converges absolutely, then\n\nThe integral formula_66 of a real or complex-valued function is said to converge absolutely if formula_67 One also says that formula_68 is absolutely integrable. The issue of absolute integrability is intricate and depends on whether the Riemann, Lebesgue, or Kurzweil-Henstock (gauge) integral is considered; for the Riemann integral, it also depends on whether we only consider integrability in its proper sense (formula_68 and formula_70 both bounded), or permit the more general case of improper integrals. \n\nAs a standard property of the Riemann integral, when formula_71 is a bounded interval, every continuous function is bounded and (Riemann) integrable, and since formula_68 continuous implies formula_73 continuous, every continuous function is absolutely integrable. In fact, since formula_74 is Riemann integrable on formula_75 if formula_68 is (properly) integrable and formula_77 is continuous, it follows that formula_78 is properly Riemann integrable if formula_68 is. However, this implication does not hold in the case of improper integrals. For instance, the function formula_80 is improperly Riemann integrable on its unbounded domain, but it is not absolutely integrable: formula_81, but formula_82. Indeed, more generally, given any series formula_83 one can consider the associated step function formula_84 defined by formula_85. Then formula_86 converges absolutely, converges conditionally or diverges according to the corresponding behavior of formula_87\n\nThe situation is different for the Lebesgue integral, which does not handle bounded and unbounded domains of integration separately (\"see below\"). The fact that the integral of formula_73 is unbounded in the examples above implies that formula_68 is also not integrable in the Lebesgue sense. In fact, in the Lebesgue theory of integration, given that formula_68 is measurable, formula_91 is (Lebesgue) integrable if and only if formula_73 is (Lebesgue) integrable. However, the hypothesis that formula_68 is measurable is crucial; it is not generally true that absolutely integrable functions on formula_75 are integrable (simply because they may fail to be measurable): let formula_95 be a nonmeasurable subset and consider formula_96 where formula_97 is the characteristic function of formula_98. Then formula_68 is not Lebesgue measurable and thus not integrable, but formula_100 is a constant function and clearly integrable. \n\nOn the other hand, a function formula_68 may be Kurzweil-Henstock integrable (or \"gauge integrable\") while formula_73 is not. This includes the case of improperly Riemann integrable functions.\n\nIn a general sense, on any measure space formula_70, the Lebesgue integral of a real-valued function is defined in terms of its positive and negative parts, so the facts:\n\n\nare essentially built into the definition of the Lebesgue integral. In particular, applying the theory to the counting measure on a set \"S\", one recovers the notion of unordered summation of series developed by Moore–Smith using (what are now called) nets. When \"S\" = N is the set of natural numbers, Lebesgue integrability, unordered summability and absolute convergence all coincide.\n\nFinally, all of the above holds for integrals with values in a Banach space. The definition of a Banach-valued Riemann integral is an evident modification of the usual one. For the Lebesgue integral one needs to circumvent the decomposition into positive and negative parts with Daniell's more functional analytic approach, obtaining the Bochner integral.\n\n\n"}
{"id": "23804321", "url": "https://en.wikipedia.org/wiki?curid=23804321", "title": "Alberto Bressan", "text": "Alberto Bressan\n\nAlberto Bressan (born 15 June 1956) is an Italian mathematician at Penn State University. His primary field of research is mathematical analysis including hyperbolic systems of conservation laws, impulsive control of Lagrangian systems, and non-cooperative differential games.\nHe obtained his PhD in mathematics from University of Colorado under Jerrold Bebernes in 1982. Bressan received a full professorship at the SISSA in Trieste, Italy in 1991. In 2003, he moved to Penn State University to assume a full professorship there — a position he still holds. He won the Bôcher Memorial Prize in 2008 and the Analysis of Partial Differential Equations Prize of the SIAM in 2007 for his work in PDEs. He was appointed to the Eberly Family Chair in Mathematics at Penn State in August 2008.\n\nIn addition to the above, his honors include the \"A. Feltrinelli prize for Mathematics, Mechanics and Applications\" of the Accademia Nazionale dei Lincei in Rome. In 2012 he became a fellow of the American Mathematical Society.\n\nBressan has contributed several important results in the theory of hyperbolic conservation laws and was invited to give a plenary talk at the International Congress of Mathematicians at Beijing in August 2002.\n\nHis early work was on certain mathematical problems from the theory of combustion. His research to date includes a number of key results in such diverse areas as: hyperbolic systems of conservation laws and nonlinear wave equations, impulsive control of Lagrangian system, systems of Hamilton–Jacobi equations (related to non-cooperative differential games), Nash equilibrium solutions in feedback form for infinite-horizon, discounted differential games, dynamic blocking problems (mathematical models of wild fire confinement), and optimization problems for elliptic equations.\n\n"}
{"id": "39975515", "url": "https://en.wikipedia.org/wiki?curid=39975515", "title": "Angelic non-determinism", "text": "Angelic non-determinism\n\nA term coined by C.A.R Hoare, which describes the execution of a non-deterministic program where all choices that are made favor termination.\n"}
{"id": "57499027", "url": "https://en.wikipedia.org/wiki?curid=57499027", "title": "Bfloat16 floating-point format", "text": "Bfloat16 floating-point format\n\nThe bfloat16 floating-point format is a computer number format occupying 16 bits in computer memory; it represents a wide dynamic range of numeric values by using a floating radix point. This format is a truncated (16-bit) version of the 32-bit IEEE 754 single-precision floating-point format (binary32) with the intent of accelerating machine learning and near-sensor computing. It preserves the approximate dynamic range of 32-bit floating-point numbers by retaining 8 exponent bits, but supports only a 8-bit precision rather than the 24-bit significand of the binary32 format. More so than single-precision 32-bit floating-point numbers, bfloat16 numbers are unsuitable for integer calculations, but this is not their intended use.\n\nThe bfloat16 format is utilized in upcoming Intel AI processors, such as Nervana NNP-L1000, Xeon processors, and Intel FPGAs, Google Cloud TPUs, and TensorFlow.\n\nbfloat16 has the following format:\n\nThe bfloat16 format, being a truncated IEEE 754 single-precision 32-bit float, allows for fast conversion to and from an IEEE 754 single-precision 32-bit float, and preserves the exponent bits while reducing the significand. Preserving the exponent bits maintains the 32-bit float's range of ~1e-38 to ~3e38.\n\nThe bits are laid out as follows:\n\nContrast to a IEEE 754 single-precision 32-bit float:\n\nThe bfloat16 binary floating-point exponent is encoded using an offset-binary representation, with the zero offset being 127; also known as exponent bias in the IEEE 754 standard.\n\n\nThus, in order to get the true exponent as defined by the offset-binary representation, the offset of 127 has to be subtracted from the value of the exponent field.\n\nThe minimum and maximum values of the exponent field (00 and FF) are interpreted specially, like in the IEEE 754 standard formats.\n\nThe minimum positive normal value is 2 ≈ 1.18 × 10 and the minimum positive (subnormal) value is 2 = 2 ≈ 9.2 × 10.\n\nJust as in IEEE 754, positive and negative infinity are represented with their corresponding sign bits, all 8 exponent bits set (FF) and all significand bits zero. Explicitly,\nval s_exponent_signcnd\n+inf = 0_11111111_0000000\n-inf = 1_11111111_0000000\nJust as in IEEE 754, NaN values are represented with either sign bit, all 8 exponent bits set (FF) and not all significand bits zero. Explicitly,\nval s_exponent_signcnd\n+NaN = 0_11111111_klmnopq\n-NaN = 1_11111111_klmonpq\nwhere at least one of \"k, l, m, n, o, p,\" or \"q\" is 1. As with IEEE 754, NaN values can be quiet or signaling, although there are no known uses of signaling bfloat16 NaNs as of September 2018.\n\nThese examples are given in bit \"representation\", in hexadecimal and binary, of the floating-point value. This includes the sign, (biased) exponent, and significand.\n\nThe maximum positive finite value of a normal bfloat16 number is 3.38953139 × 10, slightly below (2 − 1) × 2 × 2 = 3.402823466 × 10, the max finite positive value representable in single precision.\n\n 0000 = 0 00000000 0000000 = 0\n\n 4049 = 0 10000000 1001001 = 3.1415927410 ≈ π ( pi )\n\n ffc0 0001 = x 11111111 1000001 => qNaN (on x86 and ARM processors)\n\n"}
{"id": "11956019", "url": "https://en.wikipedia.org/wiki?curid=11956019", "title": "Blaschke selection theorem", "text": "Blaschke selection theorem\n\nThe Blaschke selection theorem is a result in topology and convex geometry about sequences of convex sets. Specifically, given a sequence formula_1 of convex sets contained in a bounded set, the theorem guarantees the existence of a subsequence formula_2 and a convex set formula_3 such that formula_4 converges to formula_3 in the Hausdorff metric. The theorem is named for Wilhelm Blaschke.\n\n\nAs an example of its use, the isoperimetric problem can be shown to have a solution. That is, there exists a curve of fixed length that encloses the maximum area possible. Other problems likewise can be shown to have a solution:\n\n"}
{"id": "58628757", "url": "https://en.wikipedia.org/wiki?curid=58628757", "title": "Blockbusting (game)", "text": "Blockbusting (game)\n\nBlockbusting is a solved combinatorial game introduced in 1987 by Elwyn Berlekamp illustrating a generalisation of overheating.\n\nThe analysis of Blockbusting may be used as the basis of a strategy for the combinatorial game of Domineering.\n\nBlockbusting is a partisan game for two players known as Red and Blue (or Right and Left) played on an formula_1 strip of squares called \"parcels\".\nEach player, in turn, claims and colors one previously unclaimed parcel until all parcels have been claimed.\nAt the end, Left's score is the number of pairs of neighboring parcels both of which he has claimed.\nLeft therefore tries to maximize that number while Right tries to minimize it.\nAdjacent Right-Right pairs do not affect the score.\n\nAlthough the purpose of the game is to further the study of combinatorial game theory,\nBerlekamp provides an interpretation alluding to the practice of blockbusting by real estate agents:\nthe players may be seen as rival agents buying up all the parcels on a street,\nwhere Left is a segregationist trying to place his clients as neighbors of one another\nwhile Right is an integrationist trying to break them up.\n\nThe operation of overheating introduced to analyze Blockbusting was later adapted by Berlekamp and David Wolfe\nto warming to analyze the end-game of Go.\n"}
{"id": "1773377", "url": "https://en.wikipedia.org/wiki?curid=1773377", "title": "Cache-oblivious algorithm", "text": "Cache-oblivious algorithm\n\nIn computing, a cache-oblivious algorithm (or cache-transcendent algorithm) is an algorithm designed to take advantage of a CPU cache without having the size of the cache (or the length of the cache lines, etc.) as an explicit parameter. An optimal cache-oblivious algorithm is a cache-oblivious algorithm that uses the cache optimally (in an asymptotic sense, ignoring constant factors). Thus, a cache-oblivious algorithm is designed to perform well, without modification, on multiple machines with different cache sizes, or for a memory hierarchy with different levels of cache having different sizes. Cache-oblivious algorithms are contrasted with explicit \"blocking,\" as in loop nest optimization, which explicitly breaks a problem into blocks that are optimally sized for a given cache.\n\nOptimal cache-oblivious algorithms are known for the Cooley–Tukey FFT algorithm, matrix multiplication, sorting, matrix transposition, and several other problems. Because these algorithms are only optimal in an asymptotic sense (ignoring constant factors), further machine-specific tuning may be required to obtain nearly optimal performance in an absolute sense. The goal of cache-oblivious algorithms is to reduce the amount of such tuning that is required.\n\nTypically, a cache-oblivious algorithm works by a recursive divide and conquer algorithm, where the problem is divided into smaller and smaller subproblems. Eventually, one reaches a subproblem size that fits into cache, regardless of the cache size. For example, an optimal cache-oblivious matrix multiplication is obtained by recursively dividing each matrix into four sub-matrices to be multiplied, multiplying the submatrices in a depth-first fashion. In tuning for a specific machine, one may use a hybrid algorithm which uses blocking tuned for the specific cache sizes at the bottom level, but otherwise uses the cache-oblivious algorithm.\n\nThe idea (and name) for cache-oblivious algorithms was conceived by Charles E. Leiserson as early as 1996 and first published by Harald Prokop in his master's thesis at the Massachusetts Institute of Technology in 1999. There were many predecessors, typically analyzing specific problems; these are discussed in detail in Frigo et al. 1999. Early examples cited include Singleton 1969 for a recursive Fast Fourier Transform, similar ideas in Aggarwal et al. 1987, Frigo 1996 for matrix multiplication and LU decomposition, and Todd Veldhuizen 1996 for matrix algorithms in the Blitz++ library.\n\nCache-oblivious algorithms are typically analyzed using an idealized model of the cache, sometimes called the cache-oblivious model. This model is much easier to analyze than a real cache's characteristics (which have complicated associativity, replacement policies, etc.), but in many cases is provably within a constant factor of a more realistic cache's performance. It is different than the external memory model because cache-oblivious algorithms do not know the block size or the cache size.\n\nIn particular, the cache-oblivious model is an abstract machine (i.e. a theoretical model of computation). It is similar to the RAM machine model which replaces the Turing machine's infinite tape with an infinite array. Each location within the array can be accessed in formula_1 time, similar to the Random access memory on a real computer. Unlike the RAM machine model, it also introduces a cache: a second level of storage between the RAM and the CPU. The other differences between the two models are listed below. In the cache-oblivious model:\n\n\nTo measure the complexity of an algorithm that executes within the cache-oblivious model, we measure the number of cache misses that the algorithm experiences. Because the model captures the fact that accessing elements in the cache is much faster than accessing things in main memory, the running time of the algorithm is defined only by the number of memory transfers between the cache and main memory. This is similar to the external memory model, which all of the features above, but cache-oblivious algorithms are independent of cache parameters (formula_2 and formula_7). The benefit of such an algorithm is that what is efficient on a cache-oblivious machine is likely to be efficient across many real machines without fine tuning for particular real machine parameters. For many problems, an optimal cache-oblivious algorithm will also be optimal for a machine with more than two memory hierarchy levels.\n\nFor example, it is possible to design a variant of unrolled linked lists which is cache-oblivious and allows list traversal of formula_12 elements in formula_13 time, where formula_2 is the cache size in elements. For a fixed formula_2, this is formula_16 time. However, the advantage of the algorithm is that it can scale to take advantage of larger cache line sizes (larger values of formula_2).\n\nThe simplest cache-oblivious algorithm presented in Frigo et al. is an out-of-place matrix transpose operation (in-place algorithms have also been devised for transposition, but are much more complicated for non-square matrices). Given \"m\"×\"n\" array A and \"n\"×\"m\" array B, we would like to store the transpose of formula_18 in formula_2. The naive solution traverses one array in row-major order and another in column-major. The result is that when the matrices are large, we get a cache miss on every step of the column-wise traversal. The total number of cache misses is formula_20.\nThe cache-oblivious algorithm has optimal work complexity formula_21 and optimal cache complexity formula_22. The basic idea is to reduce the transpose of two large matrices into the transpose of small (sub)matrices. We do this by dividing the matrices in half along their larger dimension until we just have to perform the transpose of a matrix that will fit into the cache. Because the cache size is not known to the algorithm, the matrices will continue to be divided recursively even after this point, but these further subdivisions will be in cache. Once the dimensions formula_23 and formula_24 are small enough so an \"input\" array of size formula_25 and an output array of size formula_26 fit into the cache, both row-major and column-major traversals result in formula_21 work and formula_28 cache misses. By using this divide and conquer approach we can achieve the same level of complexity for the overall matrix.\n\nMost cache-oblivious algorithms rely on a divide-and-conquer approach. They reduce the problem, so that it eventually fits in cache no matter how small the cache is, and end the recursion at some small size determined by the function-call overhead and similar cache-unrelated optimizations, and then use some cache-efficient access pattern to merge the results of these small, solved problems.\n\nLike external sorting in the external memory model, cache-oblivious sorting is possible in two variants: funnelsort, which resembles mergesort, and cache-oblivious distribution sort, which resembles quicksort. Like their external memory counterparts, both achieve a running time of formula_29, which matches a lower bound and is thus asymptotically optimal.\n\n"}
{"id": "43934478", "url": "https://en.wikipedia.org/wiki?curid=43934478", "title": "Cartan pair", "text": "Cartan pair\n\nIn the mathematical fields of Lie theory and algebraic topology, the notion of Cartan pair is a technical condition on the relationship between a reductive Lie algebra formula_1 and a subalgebra formula_2 reductive in formula_1. \n\nA reductive pair formula_4 is said to be Cartan if the relative Lie algebra cohomology \nis isomorphic to the tensor product of the characteristic subalgebra \nand an exterior subalgebra formula_7 of formula_8, where\n\nOn the level of Lie groups, if \"G\" is a compact, connected Lie group and \"K\" a closed connected subgroup, there are natural fiber bundles\nwhere \nformula_17\nis the homotopy quotient, here homotopy equivalent to the regular quotient, and\nThen the characteristic algebra is the image of formula_19, the transgression formula_20 from the primitive subspace \"P\" of formula_21 is that arising from the edge maps in the Serre spectral sequence of the universal bundle formula_22, and the subspace formula_9 of formula_24 is the kernel of formula_25.\n\n"}
{"id": "45325150", "url": "https://en.wikipedia.org/wiki?curid=45325150", "title": "Congruence-permutable algebra", "text": "Congruence-permutable algebra\n\nIn universal algebra, two congruences formula_1 are said to be permutable if formula_2. A congruence-permutable algebra is one in which all congruences are permutable. A variety of algebras is congruence-permutable if every algebra in the variety is congruence-permutable.\n\nA Mal'cev term is a term \"M\" for which\n\nA congruence-permutable variety is one whose signature contains a Mal'cev term.\n\nMost classical varieties in abstract algebra, such as groups, rings, and Lie algebras, are congruence-permutable. Any variety that contains a group operation is congruence-permutable, and the Mal'cev term is formula_4.\n"}
{"id": "1977696", "url": "https://en.wikipedia.org/wiki?curid=1977696", "title": "Conic constant", "text": "Conic constant\n\nIn geometry, the conic constant (or Schwarzschild constant, after Karl Schwarzschild) is a quantity describing conic sections, and is represented by the letter \"K\". For negative \"K\" it is given by\n\nwhere \"e\" is the eccentricity of the conic section.\n\nThe equation for a conic section with apex at the origin and tangent to the y axis is\n\nwhere \"K\" is the conic constant and \"R\" is the radius of curvature at \"x\" = 0.\n\nThis formulation is used in geometric optics to specify oblate elliptical (\"K\" > 0), spherical (\"K\" = 0), prolate elliptical (0 > \"K\" > −1), parabolic (\"K\" = −1), and hyperbolic (\"K\" < −1) lens and mirror surfaces. When the paraxial approximation is valid, the optical surface can be treated as a spherical surface with the same radius.\n\nSome non-optical design references use the letter \"p\" as the conic constant. In these cases, \"p\" = \"K\" + 1.\n"}
{"id": "39395641", "url": "https://en.wikipedia.org/wiki?curid=39395641", "title": "Connection (affine bundle)", "text": "Connection (affine bundle)\n\nLet be an affine bundle modelled over a vector bundle . A connection on is called the affine connection if it as a section of the jet bundle of is an affine bundle morphism over . In particular, this is the case of an affine connection on the tangent bundle of a smooth manifold .\n\nWith respect to affine bundle coordinates on , an affine connection on is given by the tangent-valued connection form\n"}
{"id": "26270834", "url": "https://en.wikipedia.org/wiki?curid=26270834", "title": "Consensus dynamics", "text": "Consensus dynamics\n\nConsensus dynamics or agreement dynamics is an area of research lying at the intersection of systems theory and graph theory. A major topic of investigation is the agreement or consensus problem in multi-agent systems that concerns processes by which a collection of interacting agents achieve a common goal. Networks of agents that exchange information to reach consensus include: physiological systems, gene networks, large-scale energy systems and fleets of vehicles on land, in the air or in space. The agreement protocol or consensus protocol is an unforced dynamical system that is governed by the interconnection topology and the initial condition for each agent. Other problems are the rendezvous problem, synchronization, flocking, formation control. One solution paradigm is distributed constraint reasoning.\n\n\n"}
{"id": "18062300", "url": "https://en.wikipedia.org/wiki?curid=18062300", "title": "Cyclic (mathematics)", "text": "Cyclic (mathematics)\n\nThere are many terms in mathematics that begin with cyclic:\n\n"}
{"id": "82341", "url": "https://en.wikipedia.org/wiki?curid=82341", "title": "Factorization", "text": "Factorization\n\nIn mathematics, factorization (also factorisation in some forms of British English) or factoring consists of writing a number or another mathematical object as a product of several \"factors\", usually smaller or simpler objects of the same kind. For example, is a factorization of the integer , and is a factorization of the polynomial .\n\nFactorization is not usually considered meaningful within number systems possessing division, such as the real or complex numbers, since any formula_1 can be trivially written as formula_2 whenever formula_3 is not zero. However, a meaningful factorization for a rational number or a rational function can be obtained by writing it in lowest terms and separately factoring its numerator and denominator. \n\nFactorization was first considered by ancient Greek mathematicians in the case of integers. They proved the fundamental theorem of arithmetic, which asserts that every positive integer may be factored into a product of prime numbers, which cannot be further factored into integers greater than 1. Moreover, this factorization is unique up to the order of the factors. Although integer factorization is a sort of inverse to multiplication, it is much more difficult algorithmically, a fact which is exploited in the RSA cryptosystem to implement public-key cryptography.\n\nPolynomial factorization has also been studied for centuries. In elementary algebra, factoring a polynomial reduces the problem of finding its roots to finding the roots of the factors. Polynomials with coefficients in the integers or in a field possess the unique factorization property, a version of the fundamental theorem of arithmetic with prime numbers replaced by irreducible polynomials. In particular, a univariate polynomial with complex coefficients admits a unique (up to ordering) factorization into linear polynomials: this is a version of the fundamental theorem of algebra. In this case, the factorization can be done with root-finding algorithms. The case of polynomials with integer coefficients is fundamental for computer algebra. There are efficient computer algorithms for computing (complete) factorizations within the ring of polynomials with rational number coefficients (see factorization of polynomials).\n\nA commutative ring possessing the unique factorization property is called a unique factorization domain. There are number systems, such as certain rings of algebraic integers, which are not unique factorization domains. However, rings of algebraic integers satisfy the weaker property of Dedekind domains: ideals factor uniquely into prime ideals.\n\n\"Factorization\" may also refer to more general decompositions of a mathematical object into the product of smaller or simpler objects. For example, every function may be factored into the composition of a surjective function with an injective function. Matrices possess many kinds of matrix factorizations. For example, every matrix has a unique LUP factorization as a product of a lower triangular matrix with all diagonal entries equal to one, an upper triangular matrix , and a permutation matrix ; this is a matrix formulation of Gaussian elimination.\n\nBy the fundamental theorem of arithmetic, every integer greater than 1 has a unique (up to the order of the factors) factorization into prime numbers, which are those integers which cannot be further factorized into the product of integers greater than one.\n\nFor computing the factorization of an integer , one needs an algorithm for finding a divisor of or deciding that is prime. When such a divisor is found, the repeated application of this algorithm to the factors and gives eventually the complete factorization of .\n\nFor finding a divisor of , if any, it suffices to test all values of such that and . In fact, if is a divisor of such that , then is a divisor of such that .\n\nIf one tests the values of in increasing order, the first divisor that is found is necessarily a prime number, and the \"cofactor\" cannot have any divisor smaller than . For getting the complete factorization, it suffices thus to continue the algorithm by searching a divisor of that is not smaller than and not greater than .\n\nThere is no need to test all values of for applying the method. In principle, it suffices to test only prime divisors. This needs to have a table of prime numbers that may be generated for example with the sieve of Eratosthenes. As the method of factorization does essentially the same work as the sieve of Eratosthenes, it is generally more efficient to test for a divisor only those numbers for which it is not immediately clear whether they are prime or not. Typically, one may proceed by testing 2, 3, 5, and the numbers > 5, whose last digit is 1, 3, 7, 9 and the sum of digits is not a multiple of 3.\n\nThis method works well for factoring small integers, but is inefficient for larger integers. For example, Pierre de Fermat was unable to discover that the 6th Fermat number \nis not a prime number. In fact, applying the above method would require more than 10,000 divisions, for a number that has 10 decimal digits.\n\nThere are more efficient factoring algorithms. However they remain relatively inefficient, as, with the present state of the art, one cannot factorize, even with the more powerful computers, a number of 500 decimal digits that is the product of two randomly chosen prime numbers. This insures the security of the RSA cryptosystem, which is widely used for secure internet communication.\n\nFor factoring into primes:\n\nManipulating expressions is the basis of algebra. Factorization is one of the most important methods for expression manipulation for several reasons. If one can put an equation in a factored form , then the solving problem splits into two independent (and generally easier) problems and . When an expression can be factored, the factors are often much simpler, and may, therefore, offer some insight on the problem. For example,\nhaving 16 multiplications, 4 subtractions and 3 additions, may be factored into the much simpler expression \nwith only two multiplications and three subtractions. Moreover, the factored form gives immediately the roots \"x = a,b,c\" of the polynomial in represented by these expressions.\n\nOn the other hand, factorization is not always possible, and when it is possible, the factors are not always simpler. For example, formula_7 can be factored into two irreducible factors formula_8 and formula_9.\n\nVarious methods have been developed for finding factorizations; some are described below.\n\nSolving algebraic equations may be viewed as a problem of factorization. In fact, the fundamental theorem of algebra can be stated as follows. Every polynomial in of degree with complex coefficients may be factorized into linear factors formula_10 for , where the s are the roots of the polynomial. Even though the structure of the factorization is known in these cases, the\ns generally cannot be computed in terms of radicals (\"n\" roots), by the Abel–Ruffini theorem. In most cases, the best that can be done is computing approximate values of the roots with a root-finding algorithm.\n\nThe systematic use of algebraic manipulations for simplifying expressions (more specifically equations)) may be dated to 9th century, with al-Khwarizmi's book \"The Compendious Book on Calculation by Completion and Balancing\", which is titled with two such types of manipulation. However, even for solving quadratic equations, factoring method was not used before Harriot’s work published in 1631, ten years after his death.\n\nIn his book \"Artis Analyticae Praxis ad Aequationes Algebraicas Resolvendas\", Harriot drew, in a first section, tables for addition, subtraction, multiplication and division of monomials, binomials, and trinomials. Then, in a second section, he set up the equation , and showed that this matches the form of multiplication he had previously provided, giving the factorization .\n\nThe methods that are described below apply to any expression that is a sum, or may be transformed into a sum. Therefore, they are most often applied to polynomials, even if they may applied also when the terms of the sum are not monomials, that is product of variables and constants\n\nIt may occur that all terms of a sum are products and that some factors are common to all terms. In this case, the distributive law allows factoring out this common factor. If there are several such common factors, it is worth to divide out the greatest such common factor. Also, if there are integer coefficients, one may factor out the greatest common divisor of these coefficients.\n\nFor example,\nsince 2 is the greatest common divisor of 6, 8, and 10, and formula_12 divides all terms.\n\nGrouping terms may allow using other methods for getting a factorization.\n\nFor example, to factor \none may remark that the first two terms have a common factor , and the last two terms have the common factor . Thus\nThen a simple inspection shows the common factor , leading to the factorization \n\nIn general, this works for sums of 4 terms that have been obtained as the product of two binomials. Although not frequently, this may work also for more complicated examples.\n\nSometimes, some term grouping lets appear a part of a Recognizable pattern. It is then useful to add terms for completing the pattern, and subtract them for not changing the value of the expression. \n\nA typical use of this is the completing the square method for getting quadratic formula. \n\nAnother example is the factorization of formula_16 If one introduces the imaginary square root of –1, commonly denoted , then one has a difference of squares\nHowever, one may also want a factorization with real number coefficients. By adding and subtracting formula_18 and grouping three terms together, one may recognize the square of a binomial:\nSubtracting and adding formula_20 also yields the factorization \nThese factorizations work not only over the complex numbers, but also over any field, where either 1, 2 or –2 is a square. In a finite field, the product of two non-squares is a square; this implies that the polynomial formula_22 which is irreducible over the integers, is reducible modulo every prime number. For example\n\nMany identities provide an equality between a sum and a product. Above methods may be used for letting appear in an expression the sum side of some identity, which may therefore be replaced by a product. \n\nBelow are identities whose left-hand-side are commonly used as patterns (this means that the variables and that appear in these identities may represent any subexpression of the expression that has to be factorized.\n\n\n\n\n\nThe th roots of unity are the complex numbers that are a zero of a function root of the polynomial formula_46 They are thus the numbers \nfor formula_48\n\nIt follows that for any two expressions and , one has:\n\nIf and are real expressions, and one want real factors, one has to replace every pair of complex conjugate factors by its product. As the complex conjugate of formula_52 is formula_53 and \none has the following real factorizations (one pass to one to the other by changing into or , and applying usual trigonometric formulas)\n\nThe cosines that appear in these factorizations are algebraic numbers, and may be expressed in terms of radicals (this is possible, because their Galois group is cyclic), however these radical expressions are too complicate for being used, except for low values of . For example\n\nOften one want a factorization with rational coefficients. Such a factorization involves cyclotomic polynomials. For expressing rational factorizations of sums and differences or powers, we need a notation for the homogenization of a polynomial: if formula_60 its \"homogenization\" is the bivariate polynomial formula_61 Then, one has\nwhere the products are taken over all divisors of , or all divisors of that do not divide , and formula_64 is the th cyclotomic polynomial.\n\nFor example, \nsince the divisors of 6 are 1, 2, 3, 6, and the divisors of 12 that do not divide 6 are 4 and 12.\n\nFor polynomials, factorization is strongly related with the problem of solving algebraic equations. An algebraic equation has the form\nwhere \nwhere is a polynomial in , such that formula_69\nA solution of this equation (also called root of the polynomial) is a value of such that\n\nIf \nis a factorization of as a product of two polynomials, then the roots of are the union of the roots of and the roots of . Thus solving is reduced to the simpler problems of solving and .\n\nConversely, the factor theorem asserts that, if is a root of , then may be factored as\nwhere is the quotient of Euclidean division of by .\n\nIf the coefficients of are real or complex numbers, the fundamental theorem of algebra asserts that has a real or complex root. Using the factor theorem recursively, it results that\nwhere formula_74 are the real or complex roots of , with some of them possibly repeated. This complete factorization is unique up to the order of the factors.\n\nIf the coefficients of are real, one want generally a factorization where factors have real coefficients. In this case, the factors of the complete factorization may have some factors that have the degree two. This factorization may easily be deduced form the above complete factorization. In fact, if is a non-real root of , then its complex conjugate is also a root of . So, the product \nis a factor of that has real coefficients. This grouping of non-real factors may be continued until getting eventually a factorization with real factors that are polynomials of degrees one or two.\n\nFor computing these real or complex factorizations, one has to know the roots of the polynomial. In general, they may not be computed exactly, and only approximative values of the roots may be obtained. See Root-finding algorithm for a summary of the numerous efficient algorithms that have been designed for this purpose.\n\nMost algebraic equations that are encountered in practice have integer or rational coefficients, and one may want a factorization with factors of the same kind. The fundamental theorem of arithmetic may be generalized to this case. That is, polynomials with integer or rational coefficients have the unique factorization property. More precisely, every polynomial with rational coefficients may be factorized in a product\nwhere is a rational number and formula_77 are non-constant polynomials with integer coefficients that are irreducible and primitive; this means that none formula_78 may be written as the product two polynomials (with integer coefficients) that are neither 1 nor –1 (integers are considered as polynomials of degree zero). Moreover, this factorization is unique up to the order of the factors and the multiplication by –1 of an even number of factors.\n\nThere are efficient algorithms for computing this factorization, which are implemented in most computer algebra systems. See Factorization of polynomials. Unfortunately, for a paper-and-pencil computation, these algorithms are too complicate for being usable. Beside general heuristics that are described above, only a few methods are available in this case, which generally work only for polynomials of low degree, with few nonzero coefficients. The main such methods are described in next subsections.\n\nEvery polynomial with rational coefficients, may be factorized, in a unique way, as the product of a rational number and a polynomial with integer coefficients, which is primitive (that is, the greatest common divisor of the coefficients is 1), and has a positive leading coefficient (coefficient of the term of the highest degree). For example:\n\nIn this factorization, the rational number is called the content, and the primitive polynomial is the primitive part. The computation of this factorization may be done as follows: firstly, reduce all coefficients to a common denominator, for getting the quotient by an integer of a polynomial with integer coefficients. Then one divides out the greater common divisor of the coefficients of this polynomial for getting the primitive part, the content being formula_81 Finally, if needed, one changes the signs of and all coefficients of the primitive part.\n\nThis factorization may produce a result that is larger than the original polynomial (typically when there are many coprime denominators), but, even when this is the case, the primitive part is generally easier to manipulate for further factorization.\n\nThe factor theorem states that, if is a root of a polynomial\n(that is ), then there is a factorization \nwhere \nwith formula_85 and\nfor .\n\nThis may be useful when, either by inspection, or by using some external information, one knows a root of the polynomial. For computing , instead of using the above formula, one may also use polynomial long division or synthetic division.\n\nFor example, for the polynomial formula_87 one may easily see that the sum of its coefficients is 1. Thus is a root. As , and formula_88 one has \n\nSearching rational roots of a polynomial makes sense only for polynomials with rational coefficients. Primitive part-content factorization (see above) reduces the problem of searching rational roots to the case of polynomials with integer coefficients such that the greatest common divisor of the coefficients is one, \n\nIf formula_90 is a rational root of such a polynomial \nthe factor theorem shows that one has a factorization\nwhere both factors have integer coefficients (the fact that has integer coefficients results from the above formula for the quotient of by formula_93).\n\nComparing the coefficients of degree and the constant coefficients in the above equality shows that, if formula_90 is a rational root in reduced form, then is a divisor of formula_95 and is a divisor of formula_96 Therefore there is a finite number of possibilities for and , which can be systematically examined.\n\nFor example, if the polynomial \nhas a rational root formula_98 then must divides 6, that is formula_99 and must divides 2, that is formula_100 Moreover, if , all terms of the polynomial are negative, and, therefore, a root cannot be negative. That is, one must have \nA direct computation shows that formula_102 is a root, and that there is no other rational root. Applying the factor theorem leads finally to the factorization\nformula_103\n\nFor quadratic polynomials, the above method may be adapted, leading to the so called \"ac method\" of factorization. \n\nLet consider the quadratic polynomial \nwith integer coefficients. If it has a rational root, its denominator must divides evenly. So, it may be written as a possibly reducible fraction formula_105 By Vieta's formulas, the other root is\nwith formula_107\nThus the second root is also rational, and the second Vieta's formula gives\nthat is \nChecking all pairs of integers whose product is gives the rational roots, if any.\n\nFor example, let consider the quadratic polynomial\nInspection of the factors of leads to , giving the two roots \n\nand the factorization \n\nAny univariate quadratic polynomial formula_113 can be factored using the quadratic formula:\nwhere formula_115 and formula_116 are the two roots of the polynomial.\n\nIf are all real, the factors are real if and only if the discriminant formula_117 is non-negative. Otherwise, the quadratic polynomial cannot be factorized into non-constant real factors.\n\nThe quadratic formula is valid when the coefficients belong to any field of characteristic different from two, and, in particular, for coefficients in a finite field with an odd number of elements.\n\nThere are also formulas for roots of cubic and quartic polynomials, which are, in general, too complicated for practical use. The Abel–Ruffini theorem shows that there are no general root formulas in terms of radicals for polynomials of degree five or higher.\n\nIt may occur that one knows some relationship between the roots of a polynomial and its coefficients. Using this knowledge may help factoring the polynomial and finding its roots. Galois theory is based on a systematic study of the relations between roots and coefficients, that include Vieta's formulas. \n\nHere, we consider the simpler case where two roots formula_118\nand formula_119 of a polynomial formula_120 satisfy the relation\nwhere is a polynomial. \n\nThis implies that formula_118 is a common root of formula_123 and formula_124 Its is therefore a root of the greatest common divisor of these two polynomials. It follows that this greatest common divisor is a non constant factor of formula_124 Euclidean algorithm for polynomials allows computing this greatest common factor.\n\nFor example, if one know or guess that:\nformula_126 \nhas two roots that sum to zero, one may apply Euclidean algorithm to formula_120 and formula_128 The first division step consists in adding formula_120 to formula_130 giving the remainder of \nThen, dividing formula_120 by formula_133 gives zero as a new remainder, and as a quotient, leading to the complete factorization\n\nThe integers and the polynomials over a field share the property of unique factorization, that is, every nonzero element may be factored into a product of an invertible element (a unit, ±1 in the case of integers) and a product of irreducible elements (prime numbers, in the case of integers), and this factorization is unique up to rearranging the factors and shifting units among the factors. Integral domains which share this property are called unique factorization domains (UFD).\n\nGreatest common divisors exist in UFDs, and conversely, every integral domain in which greatest common divisors exist is an UFD. Every principal ideal domain is an UFD.\n\nA Euclidean domain is a integral domain on which is defined a Euclidean division similar to that of integers. Every Euclidean domain is a principal ideal domain, and thus a UFD. \n\nIn a Euclidean domain, Euclidean division allows defining a Euclidean algorithm for computing greatest common divisors. However this does not imply the existence of a factorization algorithm. There is an explicit example of a field such that there cannot exist any factorization algorithm in the Euclidean domain of the univariate polynomials over .\n\nIn algebraic number theory, the study of Diophantine equations led mathematicians, during 19th century, to introduce generalizations of the integers called algebraic integers. The first ring of algebraic integers that have been considered were Gaussian integers and Eisenstein integers, which share with usual integers the property of being principal ideal domains, and have thus the unique factorization property. \n\nUnfortunately, it soon appeared that most rings of algebraic integers are not principal and do not have unique factorization. The simplest example is formula_135 in which\nand all these factors are irreducible.\n\nThis lack of unique factorization is a major difficulty for solving Diophantine equations. For example, many wrong proofs of Fermat's Last Theorem (probably including Fermat's \"truly marvelous proof of this, which this margin is too narrow to contain\") were based on the implicit supposition of unique factorization.\n\nThis difficulty was resolved by Dedekind, who proved that the rings of algebraic integers have unique factorization of ideals: in these rings, every ideal is a product of prime ideals, and this factorization is unique up the order of the factors. The integral domains that have this unique factorization property are now called Dedekind domains. They have many nice properties that make them fundamental in algebraic number theory.\n\nMatrix rings are non-commutative and have no unique factorization: there are, in general, many ways of writing a matrix as a product of matrices. Thus, the factorization problem consists of finding factors of specified types. For example the LU decomposition gives a matrix as the product of a lower triangular matrix by an upper triangular matrix. As this is not always possible, one generally considers the \"LUP decomposition\" having a permutation matrix as its third factor.\n\nSee Matrix decomposition for the most common types of matrix factorizations.\n\nA logical matrix represents a binary relation, and matrix multiplication corresponds to composition of relations. Decomposition of a relation through factorization serves to profile the nature of the relation, such as a difunctional relation.\n\n\n\n"}
{"id": "21682838", "url": "https://en.wikipedia.org/wiki?curid=21682838", "title": "Forbidden subgraph problem", "text": "Forbidden subgraph problem\n\nIn extremal graph theory, the forbidden subgraph problem is the following problem: given a graph \"G\", find the maximal number of edges in an \"n\"-vertex graph which does not have a subgraph isomorphic to \"G\". In this context, \"G\" is called a forbidden subgraph.\n\nIt is also called the Turán-type problem and the corresponding number is called the Turán number for graph \"G\". It is called so in memory of Pál Turán, who determined this number for all \"n\" and all complete graphs formula_1.\n\nAn equivalent problem is how many edges in an \"n\"-vertex graph guarantee that it has a subgraph isomorphic to \"G\"?\n\nThe problem may be generalized for a set of forbidden subgraphs \"S\": find the maximal number of edges in an \"n\"-vertex graph which does not have a subgraph isomorphic to any graph form \"S\".\n\n"}
{"id": "21317755", "url": "https://en.wikipedia.org/wiki?curid=21317755", "title": "Foundations of geometry", "text": "Foundations of geometry\n\nFoundations of geometry is the study of geometries as axiomatic systems. There are several sets of axioms which give rise to Euclidean geometry or to non-Euclidean geometries. These are fundamental to the study and of historical importance, but there are a great many modern geometries that are not Euclidean which can be studied from this viewpoint. The term axiomatic geometry can be applied to any geometry that is developed from an axiom system, but is often used to mean Euclidean geometry studied from this point of view. The completeness and independence of general axiomatic systems are important mathematical considerations, but there are also issues to do with the teaching of geometry which come into play.\n\nBased on ancient Greek methods, an \"axiomatic system\" is a formal description of a way to establish the \"mathematical truth\" that flows from a fixed set of assumptions. Although applicable to any area of mathematics, geometry is the branch of elementary mathematics in which this method has most extensively been successfully applied.\n\nThere are several components of an axiomatic system.\n\nAn \"interpretation\" of an axiomatic system is some particular way of giving concrete meaning to the primitives of that system. If this association of meanings makes the axioms of the system true statements, then the interpretation is called a model of the system. In a model, all the theorems of the system are automatically true statements.\n\nIn discussing axiomatic systems several properties are often focused on:\n\n\"Euclidean geometry\" is a mathematical system attributed to the Alexandrian Greek mathematician Euclid, which he described (although non-rigorously by modern standards) in his textbook on geometry: the \"Elements\". Euclid's method consists in assuming a small set of intuitively appealing axioms, and deducing many other propositions (theorems) from these. Although many of Euclid's results had been stated by earlier mathematicians, Euclid was the first to show how these propositions could fit into a comprehensive deductive and logical system. The \"Elements\" begins with plane geometry, still taught in secondary school as the first axiomatic system and the first examples of formal proof. It goes on to the solid geometry of three dimensions. Much of the \"Elements\" states results of what are now called algebra and number theory, explained in geometrical language.\n\nFor over two thousand years, the adjective \"Euclidean\" was unnecessary because no other sort of geometry had been conceived. Euclid's axioms seemed so intuitively obvious (with the possible exception of the parallel postulate) that any theorem proved from them was deemed true in an absolute, often metaphysical, sense. Today, however, many other geometries which are not Euclidean are known, the first ones having been discovered in the early 19th century.\n\nEuclid's \"Elements\" is a mathematical and geometric treatise consisting of 13 books written by the ancient Greek mathematician Euclid in Alexandria c. 300 BC. It is a collection of definitions, postulates (axioms), propositions (theorems and constructions), and mathematical proofs of the propositions. The thirteen books cover Euclidean geometry and the ancient Greek version of elementary number theory. With the exception of Autolycus' \"On the Moving Sphere\", the \"Elements\" is one of the oldest extant Greek mathematical treatises, and it is the oldest extant axiomatic deductive treatment of mathematics. It has proven instrumental in the development of logic and modern science.\n\nEuclid's \"Elements\" has been referred to as the most successful and influential textbook ever written. Being first set in type in Venice in 1482, it is one of the very earliest mathematical works to be printed after the invention of the printing press and was estimated by Carl Benjamin Boyer to be second only to the Bible in the number of editions published, with the number reaching well over one thousand. For centuries, when the quadrivium was included in the curriculum of all university students, knowledge of at least part of Euclid's \"Elements\" was required of all students. Not until the 20th century, by which time its content was universally taught through other school textbooks, did it cease to be considered something all educated people had read.\n\nThe \"Elements\" are mainly a systematization of earlier knowledge of geometry. It is assumed that its superiority over earlier treatments was recognized, with the consequence that there was little interest in preserving the earlier ones, and they are now nearly all lost.\n\nBooks I–IV and VI discuss plane geometry. Many results about plane figures are proved, e.g., \"If a triangle has two equal angles, then the sides subtended by the angles are equal.\" The Pythagorean theorem is proved.\n\nBooks V and VII–X deal with number theory, with numbers treated geometrically via their representation as line segments with various lengths. Notions such as prime numbers and rational and irrational numbers are introduced. The infinitude of prime numbers is proved.\n\nBooks XI–XIII concern solid geometry. A typical result is the 1:3 ratio between the volume of a cone and a cylinder with the same height and base.\n\nNear the beginning of the first book of the \"Elements\", Euclid gives five postulates (axioms) for plane geometry, stated in terms of constructions (as translated by Thomas Heath):\n\n\"Let the following be postulated\":\n\nAlthough Euclid's statement of the postulates only explicitly asserts the existence of the constructions, they are also assumed to produce unique objects.\n\nThe success of the \"Elements\" is due primarily to its logical presentation of most of the mathematical knowledge available to Euclid. Much of the material is not original to him, although many of the proofs are supposedly his. Euclid's systematic development of his subject, from a small set of axioms to deep results, and the consistency of his approach throughout the \"Elements\", encouraged its use as a textbook for about 2,000 years. The \"Elements\" still influences modern geometry books. Further, its logical axiomatic approach and rigorous proofs remain the cornerstone of mathematics.\n\nThe standards of mathematical rigor have changed since Euclid wrote the \"Elements\". Modern attitudes towards, and viewpoints of, an axiomatic system can make it appear that Euclid was in some way \"sloppy\" or \"careless\" in his approach to the subject, but this is an ahistorical illusion. It is only after the foundations were being carefully examined in response to the introduction of non-Euclidean geometry that what we now consider \"flaws\" began to emerge. Mathematician and historian W. W. Rouse Ball put these criticisms in perspective, remarking that \"the fact that for two thousand years [the \"Elements\"] was the usual text-book on the subject raises a strong presumption that it is not unsuitable for that purpose.\"\n\nSome of the main issues with Euclid's presentation are:\n\nEuclid's list of axioms in the \"Elements\" was not exhaustive, but represented the principles that seemed the most important. His proofs often invoke axiomatic notions which were not originally presented in his list of axioms. He does not go astray and prove erroneous things because of this since he is actually making use of implicit assumptions whose validity appears to be justified by the diagrams which accompany his proofs. Later mathematicians have incorporated Euclid's implicit axiomatic assumptions in the list of formal axioms, thereby greatly extending that list.\n\nFor example, in the first construction of Book 1, Euclid used a premise that was neither postulated nor proved: that two circles with centers at the distance of their radius will intersect in two points. Later, in the fourth construction, he used superposition (moving the triangles on top of each other) to prove that if two sides and their angles are equal then they are congruent; during these considerations he uses some properties of superposition, but these properties are not described explicitly in the treatise. If superposition is to be considered a valid method of geometric proof, all of geometry would be full of such proofs. For example, propositions I.1 – I.3 can be proved trivially by using superposition.\n\nTo address these issues in Euclid's work, later authors have either attempted to \"fill in the holes\" in Euclid's presentation–the most notable of these attempts is due to D. Hilbert–or to organize the axiom system around different concepts, as G.D. Birkhoff has done.\n\nThe German mathematician Moritz Pasch (1843–1930) was the first to accomplish the task of putting Euclidean geometry on a firm axiomatic footing. In his book, \"Vorlesungen über neuere Geometrie\" published in 1882, Pasch laid the foundations of the modern axiomatic method. He originated the concept of primitive notion (which he called \"Kernbegriffe\") and together with the axioms (\"Kernsätzen\") he constructs a formal system which is free from any intuitive influences. According to Pasch, the only place where intuition should play a role is in deciding what the primitive notions and axioms should be. Thus, for Pasch, \"point\" is a primitive notion but \"line\" (straight line) is not, since we have good intuition about points but no one has ever seen or had experience with an infinite line. The primitive notion that Pasch uses in its place is \"line segment\".\n\nPasch observed that the ordering of points on a line (or equivalently containment properties of line segments) is not properly resolved by Euclid's axioms; thus, Pasch's theorem, stating that if two line segment containment relations hold then a third one also holds, cannot be proven from Euclid's axioms. The related Pasch's axiom concerns the intersection properties of lines and triangles.\n\nPasch's work on the foundations set the standard for rigor, not only in geometry but also in the wider context of mathematics. His breakthrough ideas are now so commonplace that it is difficult to remember that they had a single originator. Pasch's work directly influenced many other mathematicians, in particular D. Hilbert and the Italian mathematician Giuseppe Peano (1858–1932). Peano's work, largely a translation of Pasch's treatise into the notation of symbolic logic (which Peano invented), uses the primitive notions of \"point\" and \"betweeness\". Peano breaks the empirical tie in the choice of primitive notions and axioms that Pasch required. For Peano, the entire system is purely formal, divorced from any empirical input.\n\nThe Italian mathematician Mario Pieri (1860–1913) took a different approach and considered a system in which there were only two primitive notions, that of \"point\" and of \"motion\". Pasch had used four primitives and Peano had reduced this to three, but both of these approaches relied on some concept of betweeness which Pieri replaced by his formulation of motion. In 1905 Pieri gave the first axiomatic treatment of complex projective geometry which did not start by building real projective geometry.\n\nPieri was a member of a group of Italian geometers and logicians that Peano had gathered around himself in Turin. This group of assistants, junior colleagues and others were dedicated to carrying out Peano's logico–geometrical program of putting the foundations of geometry on firm axiomatic footing based on Peano's logical symbolism. Besides Pieri, Burali-Forti, Padoa and Fano were in this group. In 1900 there were two international conferences held back-to-back in Paris, the International Congress of Philosophy and the Second International Congress of Mathematicians. This group of Italian mathematicians was very much in evidence at these congresses, pushing their axiomatic agenda. Padoa gave a well regarded talk and Peano, in the question period after David Hilbert's famous address on unsolved problems, remarked that his colleagues had already solved Hilbert's second problem.\n\nAt the University of Göttingen, during the 1898–1899 winter term, the eminent German mathematician David Hilbert (1862–1943) presented a course of lectures on the foundations of geometry. At the request of Felix Klein, Professor Hilbert was asked to write up the lecture notes for this course in time for the summer 1899 dedication ceremony of a monument to C.F. Gauss and Wilhelm Weber to be held at the university. The rearranged lectures were published in June 1899 under the title \"Grundlagen der Geometrie\" (Foundations of Geometry). The influence of the book was immediate. According to :\n\nBy developing a postulate set for Euclidean geometry that does not depart too greatly in spirit from Euclid's own, and by employing a minimum of symbolism, Hilbert succeeded in convincing mathematicians to a far greater extent than had Pasch and Peano, of the purely hypothetico-deductive nature of geometry. But the influence of Hilbert's work went far beyond this, for, backed by the author's great mathematical authority, it firmly implanted the postulational method, not only in the field of geometry, but also in essentially every other branch of mathematics. The stimulus to the development of the foundations of mathematics provided by Hilbert's little book is difficult to overestimate. Lacking the strange symbolism of the works of Pasch and Peano, Hilbert's work can be read, in great part, by any intelligent student of high school geometry.\n\nIt is difficult to specify the axioms used by Hilbert without referring to the publication history of the \"Grundlagen\" since Hilbert changed and modified them several times. The original monograph was quickly followed by a French translation, in which Hilbert added V.2, the Completeness Axiom. An English translation, authorized by Hilbert, was made by E.J. Townsend and copyrighted in 1902. This translation incorporated the changes made in the French translation and so is considered to be a translation of the 2nd edition. Hilbert continued to make changes in the text and several editions appeared in German. The 7th edition was the last to appear in Hilbert's lifetime. New editions followed the 7th, but the main text was essentially not revised. The modifications in these editions occur in the appendices and in supplements. The changes in the text were large when compared to the original and a new English translation was commissioned by Open Court Publishers, who had published the Townsend translation. So, the 2nd English Edition was translated by Leo Unger from the 10th German edition in 1971. This translation incorporates several revisions and enlargements of the later German editions by Paul Bernays. The differences between the two English translations are due not only to Hilbert, but also to differing choices made by the two translators. What follows will be based on the Unger translation.\n\nHilbert's axiom system is constructed with six primitive notions: \"point\", \"line\", \"plane\", \"betweenness\", \"lies on (containment)\", and \"congruence\".\n\nAll points, lines, and planes in the following axioms are distinct unless otherwise stated.\n\n\n\n\n\n\nWhen the monograph of 1899 was translated into French, Hilbert added:\n\nThis axiom is not needed for the development of Euclidean geometry, but is needed to establish a bijection between the real numbers and the points on a line. This was an essential ingredient in Hilbert's proof of the consistency of his axiom system.\n\nBy the 7th edition of the \"Grundlagen\", this axiom had been replaced by the axiom of line completeness given above and the old axiom V.2 became Theorem 32.\n\nAlso to be found in the 1899 monograph (and appearing in the Townsend translation) is:\n\nHowever, E.H. Moore and R.L. Moore independently proved that this axiom is redundant, and the former published this result in an article appearing in the \"Transactions of the American Mathematical Society\" in 1902. Hilbert moved the axiom to Theorem 5 and renumbered the axioms accordingly (old axiom II-5 (Pasch's axiom) now became II-4).\n\nWhile not as dramatic as these changes, most of the remaining axioms were also modified in form and/or function over the course of the first seven editions.\n\nGoing beyond the establishment of a satisfactory set of axioms, Hilbert also proved the consistency of his system relative to the theory of real numbers by constructing a model of his axiom system from the real numbers. He proved the independence of some of his axioms by constructing models of geometries which satisfy all except the one axiom under consideration. Thus, there are examples of geometries satisfying all except the Archimedean axiom V.1 (non-Archimedean geometries), all except the parallel axiom IV.1 (non-Euclidean geometries) and so on. Using the same technique he also showed how some important theorems depended on certain axioms and were independent of others. Some of his models were very complex and other mathematicians tried to simplify them. For instance, Hilbert's model for showing the independence of Desargues theorem from certain axioms ultimately led Ray Moulton to discover the non-Desarguesian Moulton plane. These investigations by Hilbert virtually inaugurated the modern study of abstract geometry in the twentieth century.\n\nIn 1932, G. D. Birkhoff created a set of four postulates of Euclidean geometry sometimes referred to as \"Birkhoff's axioms\". These postulates are all based on basic geometry that can be experimentally verified with a scale and protractor. In a radical departure from the synthetic approach of Hilbert, Birkhoff was the first to build the foundations of geometry on the real number system. It is this powerful assumption that permits the small number of axioms in this system.\n\nBirkhoff uses four undefined terms: \"point\", \"line\", \"distance\" and \"angle\". His postulates are:\nPostulate I: Postulate of Line Measure. \nThe points \"A\", \"B\", ... of any line can be put into 1:1 correspondence with the real numbers \"x\" so that |\"x\" −\"x\"| = d(\"A, B\") for all points \"A\" and \"B\".\n\nPostulate II: Point-Line Postulate. \nThere is one and only one straight line, \"ℓ\", that contains any two given distinct points \"P\" and \"Q\".\n\nPostulate III: Postulate of Angle Measure. \nThe rays {\"ℓ, m, n\", ...} through any point \"O\" can be put into 1:1 correspondence with the real numbers \"a\" (mod 2\"π\") so that if \"A\" and \"B\" are points (not equal to \"O\") of \"ℓ\" and \"m\", respectively, the difference \"a\" − \"a\" (mod 2π) of the numbers associated with the lines \"ℓ\" and \"m\" is formula_1\"AOB\". Furthermore, if the point \"B\" on \"m\" varies continuously in a line \"r\" not containing the vertex \"O\", the number \"a\" varies continuously also.\n\nPostulate IV: Postulate of Similarity. \nIf in two triangles \"ABC\" and \"A'B'C' \" and for some constant \"k\" > 0, \"d\"(\"A', B' \") = \"kd\"(\"A, B\"), \"d\"(\"A', C' \") = \"kd\"(\"A, C\") and formula_1\"B'A'C' \" = ±formula_1\"BAC\", then \"d\"(\"B', C' \") = \"kd\"(\"B, C\"), formula_1 \"C'B'A' \" = ±formula_1\"CBA\", and formula_1\"A'C'B' \" = ±formula_1\"ACB\".\n\nWhether or not it is wise to teach Euclidean geometry from an axiomatic viewpoint at the high school level has been a matter of debate. There have been many attempts to do so and not all of them have been successful. In 1904, George Bruce Halsted published a high school geometry text based on Hilbert's axiom set. Logical criticisms of this text led to a highly revised second edition. In reaction to the launching of the Russian satellite Sputnik there was a call to revise the school mathematics curriculum. From this effort there arose the New Math program of the 1960s. With this as a background, many individuals and groups set about to provide textual material for geometry classes based on an axiomatic approach.\n\nSaunders Mac Lane (1909–2005), a mathematician, wrote a paper in 1959 in which he proposed a set of axioms for Euclidean geometry in the spirit of Birkhoff's treatment using a distance function to associate real numbers with line segments. This was not the first attempt to base a school level treatment on Birkhoff's system, in fact, Birkhoff and Ralph Beatley had written a high school text in 1940 which developed Euclidean geometry from five axioms and the ability to measure line segments and angles. However, in order to gear the treatment to a high school audience, some mathematical and logical arguments were either ignored or slurred over.\n\nIn Mac Lane's system there are four primitive notions (undefined terms): \"point\", \"distance\", \"line\" and \"angle measure\". There are also 14 axioms, four giving the properties of the distance function, four describing properties of lines, four discussing angles (which are directed angles in this treatment), a similarity axiom (essentially the same as Birkhoff's) and a continuity axiom which can be used to derive the Crossbar theorem and its converse. The increased number of axioms has the pedagogical advantage of making early proofs in the development easier to follow and the use of a familiar metric permits a rapid advancement through basic material so that the more \"interesting\" aspects of the subject can be gotten to sooner.\n\nIn the 1960s a new set of axioms for Euclidean geometry, suitable for high school geometry courses, was introduced by the School Mathematics Study Group (SMSG), as a part of the New math curricula. This set of axioms follows the Birkhoff model of using the real numbers to gain quick entry into the geometric fundamentals. However, whereas Birkhoff tried to minimize the number of axioms used, and most authors were concerned with the independence of the axioms in their treatments, the SMSG axiom list was intentionally made large and redundant for pedagogical reasons. The SMSG only produced a mimeographed text using these axioms, but Edwin E. Moise, a member of the SMSG, wrote a high school text based on this system, and a college level text, , with some of the redundancy removed and modifications made to the axioms for a more sophisticated audience.\n\nThere are eight undefined terms: \"point\", \"line\", \"plane\", \"lie on\", \"distance\", \"angle measure\", \"area\" and \"volume\". The 22 axioms of this system are given individual names for ease of reference. Amongst these are to be found: the Ruler Postulate, the Ruler Placement Postulate, the Plane Separation Postulate, the Angle Addition Postulate, the Side angle side (SAS) Postulate, the Parallel Postulate (in Playfair's form), and Cavalieri's principle.\n\nAlthough much of the New math curriculum has been drastically modified or abandoned, the geometry portion has remained relatively stable. Modern high school textbooks use axiom systems that are very similar to those of the SMSG. For example, the texts produced by the University of Chicago School Mathematics Project (UCSMP) use a system which, besides some updating of language, differs mainly from the SMSG system in that it includes some transformation concepts under its \"Reflection Postulate\".\n\nThere are only three undefined terms: \"point\", \"line\" and \"plane\". There are eight \"postulates\", but most of these have several parts (which are generally called \"assumptions\" in this system). Counting these parts, there are 32 axioms in this system. Amongst the postulates can be found the point-line-plane postulate, the Triangle inequality postulate, postulates for distance, angle measurement, corresponding angles, area and volume, and the Reflection postulate. The reflection postulate is used as a replacement for the SAS postulate of SMSG system.\n\nOswald Veblen (1880 – 1960) provided a new axiom system in 1904 when he replaced the concept of \"betweeness\", as used by Hilbert and Pasch, with a new primitive, \"order\". This permitted several primitive terms used by Hilbert to become defined entities, reducing the number of primitive notions to two, \"point\" and \"order\".\n\nMany other axiomatic systems for Euclidean geometry have been proposed over the years. A comparison of many of these can be found in a 1927 monograph by Henry George Forder. Forder also gives, by combining axioms from different systems, his own treatment based on the two primitive notions of \"point\" and \"order\". He also provides a more abstract treatment of one of Pieri's systems (from 1909) based on the primitives \"point\" and \"congruence\".\n\nStarting with Peano, there has been a parallel thread of interest amongst logicians concerning the axiomatic foundations of Euclidean geometry. This can be seen, in part, in the notation used to describe the axioms. Pieri claimed that even though he wrote in the traditional language of geometry, he was always thinking in terms of the logical notation introduced by Peano, and used that formalism to see how to prove things. A typical example of this type of notation can be found in the work of E. V. Huntington (1874 – 1952) who, in 1913, produced an axiomatic treatment of three-dimensional Euclidean geometry based upon the primitive notions of \"sphere\" and \"inclusion\" (one sphere lying within another). Beyond notation there is also interest in the logical structure of the theory of geometry. Alfred Tarski proved that a portion of geometry, which he called \"elementary\" geometry, is a first order logical theory (see Tarski's axioms).\n\nModern text treatments of the axiomatic foundations of Euclidean geometry follow the pattern of H.G. Forder and Gilbert de B. Robinson who mix and match axioms from different systems to produce different emphasizes. is a modern example of this approach.\n\nIn view of the role which mathematics plays in science and implications of scientific knowledge for all of our beliefs, revolutionary changes in man's understanding of the nature of mathematics could not but mean revolutionary changes in his understanding of science, doctrines of philosophy, religious and ethical beliefs, and, in fact, all intellectual disciplines.\nIn the first half of the nineteenth century a revolution took place in the field of geometry that was as scientifically important as the Copernican revolution in astronomy and as philosophically profound as the Darwinian theory of evolution in its impact on the way we think. This was the consequence of the discovery of non-Euclidean geometry. For over two thousand years, starting in the time of Euclid, the postulates which grounded geometry were considered self-evident truths about physical space. Geometers thought that they were deducing other, more obscure truths from them, without the possibility of error. This view became untenable with the development of hyperbolic geometry. There were now two incompatible systems of geometry (and more came later) that were self-consistent and compatible with the observable physical world. \"From this point on, the whole discussion of the relation between geometry and physical space was carried on in quite different terms.\"\n\nTo obtain a non-Euclidean geometry, the parallel postulate (or its equivalent) \"must\" be replaced by its negation. Negating the Playfair's axiom form, since it is a compound statement (... there exists one and only one ...), can be done in two ways. Either there will exist more than one line through the point parallel to the given line or there will exist no lines through the point parallel to the given line. In the first case, replacing the parallel postulate (or its equivalent) with the statement \"In a plane, given a point P and a line \"ℓ\" not passing through P, there exist two lines through P which do not meet \"ℓ\"\" and keeping all the other axioms, yields hyperbolic geometry. The second case is not dealt with as easily. Simply replacing the parallel postulate with the statement, \"In a plane, given a point P and a line \"ℓ\" not passing through P, all the lines through P meet \"ℓ\"\", does not give a consistent set of axioms. This follows since parallel lines exist in absolute geometry, but this statement would say that there are no parallel lines. This problem was known (in a different guise) to Khayyam, Saccheri and Lambert and was the basis for their rejecting what was known as the \"obtuse angle case\". In order to obtain a consistent set of axioms which includes this axiom about having no parallel lines, some of the other axioms must be tweaked. The adjustments to be made depend upon the axiom system being used. Amongst others these tweaks will have the effect of modifying Euclid's second postulate from the statement that line segments can be extended indefinitely to the statement that lines are unbounded. Riemann's elliptic geometry emerges as the most natural geometry satisfying this axiom.\n\nIt was Gauss who coined the term \"non-Euclidean geometry\". He was referring to his own, unpublished work, which today we call \"hyperbolic geometry\". Several authors still consider \"non-Euclidean geometry\" and \"hyperbolic geometry\" to be synonyms. In 1871, Felix Klein, by adapting a metric discussed by Arthur Cayley in 1852, was able to bring metric properties into a projective setting and was thus able to unify the treatments of hyperbolic, euclidean and elliptic geometry under the umbrella of projective geometry. Klein is responsible for the terms \"hyperbolic\" and \"elliptic\" (in his system he called Euclidean geometry \"parabolic\", a term which has not survived the test of time and is used today only in a few disciplines.) His influence has led to the common usage of the term \"non-Euclidean geometry\" to mean either \"hyperbolic\" or \"elliptic\" geometry.\n\nThere are some mathematicians who would extend the list of geometries that should be called \"non-Euclidean\" in various ways. In other disciplines, most notably mathematical physics, where Klein's influence was not as strong, the term \"non-Euclidean\" is often taken to mean \"not\" Euclidean.\n\nFor two thousand years, many attempts were made to prove the parallel postulate using Euclid's first four postulates. A possible reason that such a proof was so highly sought after was that, unlike the first four postulates, the parallel postulate isn't self-evident. If the order the postulates were listed in the Elements is significant, it indicates that Euclid included this postulate only when he realised he could not prove it or proceed without it. Many attempts were made to prove the fifth postulate from the other four, many of them being accepted as proofs for long periods of time until the mistake was found. Invariably the mistake was assuming some 'obvious' property which turned out to be equivalent to the fifth postulate. Eventually it was realized that this postulate may not be provable from the other four. According to this opinion about the parallel postulate (Postulate 5) does appear in print:\n\nApparently the first to do so was G. S. Klügel (1739–1812), a doctoral student at the University of Gottingen, with the support of his teacher A. G. Kästner, in the former's 1763 dissertation \"Conatuum praecipuorum theoriam parallelarum demonstrandi recensio\" (Review of the Most Celebrated Attempts at Demonstrating the Theory of Parallels). In this work Klügel examined 28 attempts to prove Postulate 5 (including Saccheri's), found them all deficient, and offered the opinion that Postulate 5 is unprovable and is supported solely by the judgment of our senses.\nThe beginning of the 19th century would finally witness decisive steps in the creation of non-Euclidean geometry. Circa 1813, Carl Friedrich Gauss and independently around 1818, the German professor of law Ferdinand Karl Schweikart had the germinal ideas of non-Euclidean geometry worked out, but neither published any results. Then, around 1830, the Hungarian mathematician János Bolyai and the Russian mathematician Nikolai Ivanovich Lobachevsky separately published treatises on what we today call hyperbolic geometry. Consequently, hyperbolic geometry has been called Bolyai-Lobachevskian geometry, as both mathematicians, independent of each other, are the basic authors of non-Euclidean geometry. Gauss mentioned to Bolyai's father, when shown the younger Bolyai's work, that he had developed such a geometry several years before, though he did not publish. While Lobachevsky created a non-Euclidean geometry by negating the parallel postulate, Bolyai worked out a geometry where both the Euclidean and the hyperbolic geometry are possible depending on a parameter \"k\". Bolyai ends his work by mentioning that it is not possible to decide through mathematical reasoning alone if the geometry of the physical universe is Euclidean or non-Euclidean; this is a task for the physical sciences. \nThe independence of the parallel postulate from Euclid's other axioms was finally demonstrated by Eugenio Beltrami in 1868.\n\nThe various attempted proofs of the parallel postulate produced a long list of theorems that are equivalent to the parallel postulate. Equivalence here means that in the presence of the other axioms of the geometry each of these theorems can be assumed to be true and the parallel postulate can be proved from this altered set of axioms. This is not the same as logical equivalence. In different sets of axioms for Euclidean geometry, any of these can replace the Euclidean parallel postulate. The following partial list indicates some of these theorems that are of historical interest.\n\nAbsolute geometry is a geometry based on an axiom system consisting of all the axioms giving Euclidean geometry except for the parallel postulate or any of its alternatives. The term was introduced by János Bolyai in 1832. It is sometimes referred to as neutral geometry, as it is neutral with respect to the parallel postulate.\n\nIn Euclid's \"Elements\", the first 28 propositions and Proposition I.31 avoid using the parallel postulate, and therefore are valid theorems in absolute geometry. Proposition I.31 proves the existence of parallel lines (by construction). Also, the Saccheri–Legendre theorem, which states that the sum of the angles in a triangle is at most 180°, can be proved.\n\nThe theorems of absolute geometry hold in hyperbolic geometry as well as in Euclidean geometry.\n\nAbsolute geometry is inconsistent with elliptic geometry: in elliptic geometry there are no parallel lines at all, but in absolute geometry parallel lines do exist. Also, in elliptic geometry, the sum of the angles in any triangle is greater than 180°.\n\nLogically, the axioms do not form a complete theory since one can add extra independent axioms without making the axiom system inconsistent. One can extend absolute geometry by adding different axioms about parallelism and get incompatible but consistent axiom systems, giving rise to Euclidean or hyperbolic geometry. Thus every theorem of absolute geometry is a theorem of hyperbolic geometry and Euclidean geometry. However the converse is not true. Also, absolute geometry is \"not\" a categorical theory, since it has models that are not isomorphic.\n\nIn the axiomatic approach to hyperbolic geometry (also referred to as Lobachevskian geometry or Bolyai–Lobachevskian geometry), one additional axiom is added to the axioms giving absolute geometry. The new axiom is \"Lobachevsky's parallel postulate\" (also known as the \"characteristic postulate of hyperbolic geometry\"):\nWith this addition, the axiom system is now complete.\n\nAlthough the new axiom asserts only the existence of two lines, it is readily established that there are an infinite number of lines through the given point which do not meet the given line. Given this plenitude, one must be careful with terminology in this setting, as the term \"parallel line\" no longer has the unique meaning that it has in Euclidean geometry. Specifically, let \"P\" be a point not on a given line formula_8. Let \"PA\" be the perpendicular drawn from \"P\" to formula_8 (meeting at point \"A\"). The lines through \"P\" fall into two classes, those that meet formula_8 and those that don't. The characteristic postulate of hyperbolic geometry says that there are at least two lines of the latter type. Of the lines which don't meet formula_8, there will be (on each side of \"PA\") a line making the smallest angle with \"PA\". Sometimes these lines are referred to as the \"first\" lines through \"P\" which don't meet formula_8 and are variously called \"limiting, asymptotic\" or \"parallel\" lines (when this last term is used, these are the only parallel lines). All other lines through \"P\" which do not meet formula_8 are called \"non-intersecting\" or \"ultraparallel\" lines.\n\nSince hyperbolic geometry and Euclidean geometry are both built on the axioms of absolute geometry, they share many properties and propositions. However, the consequences of replacing the parallel postulate of Euclidean geometry with the characteristic postulate of hyperbolic geometry can be dramatic. To mention a few of these:\n\nAdvocates of the position that Euclidean geometry is the one and only \"true\" geometry received a setback when, in a memoir published in 1868, \"Fundamental theory of spaces of constant curvature\", Eugenio Beltrami gave an abstract proof of equiconsistency of hyperbolic and Euclidean geometry for any dimension. He accomplished this by introducing several models of non-Euclidean geometry that are now known as the Beltrami–Klein model, the Poincaré disk model, and the Poincaré half-plane model, together with transformations that relate them. For the half-plane model, Beltrami cited a note by Liouville in the treatise of Monge on differential geometry. Beltrami also showed that \"n\"-dimensional Euclidean geometry is realized on a horosphere of the (\"n\" + 1)-dimensional hyperbolic space, so the logical relation between consistency of the Euclidean and the non-Euclidean geometries is symmetric.\n\nAnother way to modify the Euclidean parallel postulate is to assume that there are no parallel lines in a plane. Unlike the situation with hyperbolic geometry, where we just add one new axiom, we can not obtain a consistent system by adding this statement as a new axiom to the axioms of absolute geometry. This follows since parallel lines provably exist in absolute geometry. Other axioms must be changed.\n\nStarting with Hilbert's axioms the necessary changes involve removing Hilbert's four axioms of order and replacing them with these seven axioms of separation concerned with a new undefined relation.\n\nThere is an undefined (primitive) relation between four points, \"A\", \"B\", \"C\" and \"D\" denoted by (\"A\",\"C\"|\"B\",\"D\") and read as \"\"A\" and \"C\" separate \"B\" and \"D\"\", satisfying these axioms:\n\nSince the Hilbert notion of \"betweeness\" has been removed, terms which were defined using that concept need to be redefined. Thus, a line segment \"AB\" defined as the points \"A\" and \"B\" and all the points \"between\" \"A\" and \"B\" in absolute geometry, needs to be reformulated. A line segment in this new geometry is determined by three collinear points \"A\", \"B\" and \"C\" and consists of those three points and all the points not separated from \"B\" by \"A\" and \"C\". There are further consequences. Since two points do not determine a line segment uniquely, three noncollinear points do not determine a unique triangle, and the definition of triangle has to be reformulated.\n\nOnce these notions have been redefined, the other axioms of absolute geometry (incidence, congruence and continuity) all make sense and are left alone. Together with the new axiom on the nonexistence of parallel lines we have a consistent system of axioms giving a new geometry. The geometry that results is called (plane) \"Elliptic geometry\".\n\nEven though elliptic geometry is not an extension of absolute geometry (as Euclidean and hyperbolic geometry are), there is a certain \"symmetry\" in the propositions of the three geometries that reflects a deeper connection which was observed by Felix Klein. Some of the propositions which exhibit this property are:\n\n\nOther results, such as the exterior angle theorem, clearly emphasize the difference between elliptic and the geometries that are extensions of absolute geometry.\n\nAbsolute geometry is an extension of ordered geometry, and thus, all theorems in ordered geometry hold in absolute geometry. The converse is not true. Absolute geometry assumes the first four of Euclid's Axioms (or their equivalents), to be contrasted with affine geometry, which does not assume Euclid's third and fourth axioms. Ordered geometry is a common foundation of both absolute and affine geometry.\n\n\n\n"}
{"id": "31844219", "url": "https://en.wikipedia.org/wiki?curid=31844219", "title": "Greg Hjorth", "text": "Greg Hjorth\n\nGreg Hjorth (14 June 1963 – 13 January 2011) was an Australian Professor of Mathematics, chess International Master (1984) and joint (with Ian Rogers) Commonwealth Champion in 1983. He worked in the field of mathematical logic.\n\nHjorth earned his Ph.D. in 1993, under the direction of W. Hugh Woodin, with a dissertation entitled \"On the influence of second uniform indiscernible\". He held faculty positions at the University of California, Los Angeles and the University of Melbourne. Among his most important contributions to set theory was the so-called theory of \"turbulence\", used in the theory of Borel equivalence relations.\n\nHjorth won the Doeberl Cup in Canberra in 1982, 1985 and 1987 and played for Australia in the Chess Olympiads of 1982, 1984 and 1986.\n\nHis best single performance was at Brighton (BCF Championship) 1984, where he scored four of seven possible points (57%) against 2551-rated opposition, for a performance rating of 2570.\n\nHjorth died of a heart attack in Melbourne, on 13 January 2011.\n\n\n"}
{"id": "537663", "url": "https://en.wikipedia.org/wiki?curid=537663", "title": "Human body weight", "text": "Human body weight\n\nHuman body weight refers to a person's mass or weight. Body weight is measured in kilograms, a measure of mass, throughout the world, although in some countries such as the United States it is measured in pounds, or as in the United Kingdom, stones and pounds. Most hospitals, even in the United States, now use kilograms for calculations, but use kilograms and pounds together for other purposes.\n\nStrictly speaking, body weight is the measurement of weight without items located on the person. Practically though, body weight may be measured with clothes on, but without shoes or heavy accessories such as mobile phones and wallets and using manual or digital weighing scales. Excess or reduced body weight is regarded as an indicator of determining a person's health, with body volume measurement providing an extra dimension by calculating the distribution of body weight.\n\nThere are a number of methods to estimate weight in children for circumstances (such as emergencies) when actual weight cannot be measured. Most involve a parent or health care provider guessing the child's weight through weight-estimation formulas. These formulas base their findings on the child's age and tape-based systems of weight estimation. Of the many formulas that have been used for estimating body weight, some include the APLS formula, the Leffler formula, and Theron formula. There are also several types of tape-based systems for estimating children's weight, with the most well-known being the Broselow tape. The Broselow tape is based on length with weight read from the appropriate color area. Newer systems, such as the PAWPER tape, make use of a simple two-step process to estimate weight: the length-based weight estimation is modified according to the child's body habitus to increase the accuracy of the final weight prediction.\n\nThe Leffler formula is used for children 0–10 years of age. In those less than a year old it is\n\nformula_1\n\nand for those 1–10 years old it is\n\nformula_2\n\nwhere \"m\" is the number of kilograms the child weighs and \"a\" and \"a\" respectively are the number of months or years old the child is.\n\nThe Theron formula is\n\nformula_3\n\nwhere \"m\" and \"a are as above.\n\nBody weight varies throughout the day, as the amount of water in the body is not constant. It changes frequently due to activities such as drinking, urinating, or exercise. Professional sports participants may deliberately dehydrate themselves to enter a lower weight class, a practice known as weight cutting.\n\nIdeal body weight (IBW) was initially introduced by Devine in 1974 to allow estimation of drug clearances in obese patients; researchers have since shown that the metabolism of certain drugs relates more to IBW than total body weight. The term was based on the use of insurance data that demonstrated the relative mortality for males and females according to different height–weight combinations.\n\nThe most common estimation of IBW is by the Devine formula; other models exist and have been noted to give similar results. Other methods used in estimating the ideal body weight are body mass index and the Hamwi method. The IBW is not the perfect fat measurement as it does not show the fat or muscle percentage in one's body. For example, athletes' results show that they are overweight when they are actually very fit and healthy. Machines like the dual-energy X-ray absorptiometry (DXA) can accurately measure the percentage and weight of (fat, muscle, bone) in a body.\n\nThe Devine formula for calculating ideal body weight in adults is as follows:\n\n\nThe Hamwi method is used to calculate the ideal body weight of the general adult:\n\n\nParticipants in sports such as boxing, mixed martial arts, wrestling, rowing, judo, Olympic weightlifting, and powerlifting are classified according to their body weight, measured in units of mass such as pounds or kilograms. See, e.g., wrestling weight classes, boxing weight classes, judo at the 2004 Summer Olympics, boxing at the 2004 Summer Olympics.\n\nIdeal body weight, specifically the Devine formula, is used clinically for multiple reasons, most commonly in estimating renal function in drug dosing, and predicting pharmacokinetics in morbidly obese patients.\n\nResearchers at the London School of Hygiene & Tropical Medicine published a study of average weights of adult humans in the journal BMC Public Health and at the United Nations conference Rio+20.\n\n"}
{"id": "4649761", "url": "https://en.wikipedia.org/wiki?curid=4649761", "title": "Iterated monodromy group", "text": "Iterated monodromy group\n\nIn geometric group theory and dynamical systems the iterated monodromy group of a covering map is a group describing the monodromy action of the fundamental group on all iterations of the covering. A single covering map between spaces is therefore used to create a tower of coverings, by placing the covering over itself repeatedly. In terms of the Galois theory of covering spaces, this construction on spaces is expected to correspond to a construction on groups. The iterated monodromy group provides this construction, and it is applied to encode the combinatorics and symbolic dynamics of the covering, and provide examples of self-similar groups.\n\nThe iterated monodromy group of \"f\" is the following quotient group:\n\nwhere :\n\n\nThe iterated monodromy group acts by automorphism on the rooted tree of preimages\nwhere a vertex formula_10 is connected by an edge with formula_11.\n\nLet :\n\nIf formula_12 is finite (or has a finite set of accumulation points), then the iterated monodromy group of \"f\" is the iterated monodromy group of the covering formula_14, where formula_15 is the Riemann sphere.\n\nIterated monodromy groups of rational functions usually have exotic properties from the point of view of classical group theory. Most of them are infinitely presented, many have intermediate growth.\n\nThe Basilica group is the iterated monodromy group of the polynomial formula_16\n\n\n\n"}
{"id": "49411394", "url": "https://en.wikipedia.org/wiki?curid=49411394", "title": "Johannes Mollerup", "text": "Johannes Mollerup\n\nJohannes Mollerup (born 3 December 1872 in Nyborg; died 27 June 1937) was a Danish mathematician.'\n\nMollerup studied at the University of Copenhagen, and received his doctorate in 1903.\n\nTogether with Harald Bohr, he developed the Bohr–Mollerup theorem which provides an easy characterization of the gamma function.\n"}
{"id": "182890", "url": "https://en.wikipedia.org/wiki?curid=182890", "title": "Kronecker delta", "text": "Kronecker delta\n\nIn mathematics, the Kronecker delta (named after Leopold Kronecker) is a function of two variables, usually just non-negative integers. The function is 1 if the variables are equal, and 0 otherwise:\nwhere the Kronecker delta is a piecewise function of variables and . For example, , whereas .\n\nThe Kronecker delta appears naturally in many areas of mathematics, physics and engineering, as a means of compactly expressing its definition above.\n\nIn linear algebra, the identity matrix has entries equal to the Kronecker delta:\nwhere and take the values , and the inner product of vectors can be written as \n\nThe restriction to positive integers is common, but there is no reason it cannot have negative integers as well as positive, or any discrete rational numbers. If and above take rational values, then for example\nThis latter case is for convenience.\n\nThe following equations are satisfied: \nTherefore, the matrix can be considered as an identity matrix.\n\nAnother useful representation is the following form:\nThis can be derived using the formula for the finite geometric series.\n\nUsing the Iverson bracket:\n\nOften, a single-argument notation is used, which is equivalent to setting :\n\nIn linear algebra, it can be thought of as a tensor, and is written . Sometimes the Kronecker delta is called the substitution tensor.\n\nSimilarly, in digital signal processing, the same concept is represented as a sequence or discrete function on (the integers):\n\nThe function is referred to as an \"impulse\", or \"unit impulse\". When it is the input to a discrete-time signal processing element, the output is called the impulse response of the element.\n\nThe Kronecker delta has the so-called \"sifting\" property that for :\nand if the integers are viewed as a measure space, endowed with the counting measure, then this property coincides with the defining property of the Dirac delta function\nand in fact Dirac's delta was named after the Kronecker delta because of this analogous property. In signal processing it is usually the context (discrete or continuous time) that distinguishes the Kronecker and Dirac \"functions\". And by convention, generally indicates continuous time (Dirac), whereas arguments like , , , , , and are usually reserved for discrete time (Kronecker). Another common practice is to represent discrete sequences with square brackets; thus: . It is important to note that the Kronecker delta is not the result of directly sampling the Dirac delta function.\n\nThe Kronecker delta forms the multiplicative identity element of an incidence algebra.\n\nIn probability theory and statistics, the Kronecker delta and Dirac delta function can both be used to represent a discrete distribution. If the support of a distribution consists of points , with corresponding probabilities , then the probability mass function of the distribution over can be written, using the Kronecker delta, as\n\nEquivalently, the probability density function of the distribution can be written using the Dirac delta function as\n\nUnder certain conditions, the Kronecker delta can arise from sampling a Dirac delta function. For example, if a Dirac delta impulse occurs exactly at a sampling point and is ideally lowpass-filtered (with cutoff at the critical frequency) per the Nyquist–Shannon sampling theorem, the resulting discrete-time signal will be a Kronecker delta function.\n\nIf it is considered as a type tensor, the Kronecker tensor can be written\n\nThis tensor represents:\n\nThe or multi-index Kronecker delta of order is a type tensor that is a completely antisymmetric in its upper indices, and also in its lower indices.\n\nTwo definitions that differ by a factor of are in use. Below, the version is presented has nonzero components scaled to be . The second version has nonzero components that are , with consequent changes scaling factors in formulae, such as the scaling factors of in \"§ Properties of generalized Kronecker delta\" below disappearing.\n\nIn terms of the indices:\n\nLet be the symmetric group of degree , then:\n\nUsing anti-symmetrization:\n\nIn terms of a determinant:\n\nUsing the Laplace expansion (Laplace's formula) of determinant, it may be defined recursively:\nwhere the caron, , indicates an index that is omitted from the sequence.\n\nWhen (the dimension of the vector space), in terms of the Levi-Civita symbol:\n\nThe generalized Kronecker delta may be used for anti-symmetrization:\n\nFrom the above equations and the properties of anti-symmetric tensors, we can derive the properties of the generalized Kronecker delta:\nwhich are the generalized version of formulae written in \"\". The last formula is equivalent to the Cauchy–Binet formula.\n\nReducing the order via summation of the indices may be expressed by the identity\n\nUsing both the summation rule for the case and the relation with the Levi-Civita symbol,\nthe summation rule of the Levi-Civita symbol is derived:\n\nFor any integer , using a standard residue calculation we can write an integral representation for the Kronecker delta as the integral below, where the contour of the integral goes counterclockwise around zero. This representation is also equivalent to a definite integral by a rotation in the complex plane.\n\nThe Kronecker comb function with period is defined (using DSP notation) as:\n\nwhere and are integers. The Kronecker comb thus consists of an infinite series of unit impulses units apart, and includes the unit impulse at zero. It may be considered to be the discrete analog of the Dirac comb.\n\nThe Kronecker delta is also called degree of mapping of one surface into another. Suppose a mapping takes place from surface to that are boundaries of regions, and which is simply connected with one-to-one correspondence. In this framework, if and are parameters for , and to are each oriented by the outer normal :\n\nwhile the normal has the direction of\n\nLet , , be defined and smooth in a domain containing , and let these equations define the mapping of onto . Then the degree of mapping is times the solid angle of the image of with respect to the interior point of , . If is the origin of the region, , then the degree, is given by the integral:\n\n"}
{"id": "15659323", "url": "https://en.wikipedia.org/wiki?curid=15659323", "title": "Laplace operators in differential geometry", "text": "Laplace operators in differential geometry\n\nIn differential geometry there are a number of second-order, linear, elliptic differential operators bearing the name Laplacian. This article provides an overview of some of them.\n\nThe connection Laplacian, also known as the rough Laplacian, is a differential operator acting on the various tensor bundles of a manifold, defined in terms of a Riemannian- or pseudo-Riemannian metric. When applied to functions (i.e. tensors of rank 0), the connection\nLaplacian is often called the Laplace–Beltrami operator. It is defined as the trace of the second covariant derivative:\n\nwhere \"T\" is any tensor, formula_2 is the Levi-Civita connection associated to the metric, and the trace is taken with respect to\nthe metric. Recall that the second covariant derivative of \"T\" is defined as\n\nNote that with this definition, the connection Laplacian has negative spectrum. On functions, it agrees with\nthe operator given as the divergence of the gradient.\n\nIf the connection of interest is the Levi-Civita connection one can find a convenient formula for the Laplacian of a scalar function in terms of partial derivatives with respect to a coordinate system:\n\nwhere formula_5 is a scalar function, formula_6 is absolute value of the determinant of the metric (absolute value is necessary in the pseudo-Riemannian case, e.g. in General Relativity) and formula_7 denotes the inverse of the metric tensor.\n\nThe Hodge Laplacian, also known as the Laplace–de Rham operator, is a differential operator acting on differential forms. (Abstractly,\nit is a second order operator on each exterior power of the cotangent bundle.) This operator is defined on any manifold equipped with\na Riemannian- or pseudo-Riemannian metric.\n\nwhere d is the exterior derivative or differential and δ is the codifferential. The Hodge Laplacian on a compact manifold has nonnegative spectrum.\n\nThe connection Laplacian may also be taken to act on differential forms by restricting it to act on skew-symmetric tensors. The connection Laplacian differs from the Hodge Laplacian by means of a Weitzenböck identity.\n\nThe Bochner Laplacian is defined differently from the connection Laplacian, but the two will turn out to differ only by a sign, whenever the former is defined. Let \"M\" be a compact, oriented manifold equipped with a metric. Let \"E\" be a vector bundle over \"M\" equipped with a fiber metric and a compatible connection, formula_2. This connection gives rise to a differential operator\nwhere formula_11 denotes smooth sections of \"E\", and \"T\"M is the cotangent bundle of \"M\". It is possible to take the formula_12-adjoint of formula_2, giving a differential operator\nThe Bochner Laplacian is given by\nwhich is a second order operator acting on sections of the vector bundle \"E\". Note that the connection Laplacian and Bochner Laplacian differ only by a sign:\n\nThe Lichnerowicz Laplacian is defined on symmetric tensors by taking formula_17 to be the symmetrized covariant derivative. The Lichnerowicz Laplacian is then defined by formula_18, where formula_19 is the formal adjoint. The Lichnerowicz Laplacian differs from the usual tensor Laplacian by a Weitzenbock formula involving the Riemann curvature tensor, and has natural applications in the study of Ricci flow and the prescribed Ricci curvature problem.\n\nOn a Riemannian manifold, one can define the conformal Laplacian as an operator on smooth functions; it differs from the Laplace–Beltrami operator by a term involving the scalar curvature of the underlying metric. In dimension \"n\" ≥ 3, the conformal Laplacian, denoted \"L\", acts on a smooth function \"u\" by\n\nwhere Δ is the Laplace-Beltrami operator (of negative spectrum), and \"R\" is the scalar curvature. This operator often makes an appearance when studying how the scalar curvature behaves under a conformal change of a Riemannian metric. If \"n\" ≥ 3 and \"g\" is a metric and \"u\" is a smooth, positive function, then the conformal metric\n\nhas scalar curvature given by\n\n"}
{"id": "675231", "url": "https://en.wikipedia.org/wiki?curid=675231", "title": "Line graph", "text": "Line graph\n\nIn the mathematical discipline of graph theory, the line graph of an undirected graph \"G\" is another graph \"L\"(\"G\") that represents the adjacencies between edges of \"G\". The name line graph comes from a paper by although both and used the construction before this. Other terms used for the line graph include the covering graph, the derivative, the edge-to-vertex dual, the conjugate, the representative graph, and the ϑ-obrazom, as well as the edge graph, the interchange graph, the adjoint graph, and the derived graph.\n\nVarious extensions of the concept of a line graph have been studied, including line graphs of line graphs, line graphs of multigraphs, line graphs of hypergraphs, and line graphs of weighted graphs.\n\nGiven a graph \"G\", its line graph \"L\"(\"G\") is a graph such that\nThat is, it is the intersection graph of the edges of \"G\", representing each edge by the set of its two endpoints.\n\nThe following figures show a graph (left, with blue vertices) and its line graph (right, with green vertices). Each vertex of the line graph is shown labeled with the pair of endpoints of the corresponding edge in the original graph. For instance, the green vertex on the right labeled 1,3 corresponds to the edge on the left between the blue vertices 1 and 3. Green vertex 1,3 is adjacent to three other green vertices: 1,4 and 1,2 (corresponding to edges sharing the endpoint 1 in the blue graph) and 4,3 (corresponding to an edge sharing the endpoint 3 in the blue graph).\n\nProperties of a graph \"G\" that depend only on adjacency between edges may be translated into equivalent properties in \"L\"(\"G\") that depend on adjacency between vertices. For instance, a matching in \"G\" is a set of edges no two of which are adjacent, and corresponds to a set of vertices in \"L\"(\"G\") no two of which are adjacent, that is, an independent set.\n\nThus,\n\nIf the line graphs of two connected graphs are isomorphic, then the underlying graphs are isomorphic, except in the case of the triangle graph \"K\" and the claw \"K\", which have isomorphic line graphs but are not themselves isomorphic.\n\nAs well as \"K\" and \"K\", there are some other exceptional small graphs with the property that their line graph has a higher degree of symmetry than the graph itself. For instance, the diamond graph \"K\" (two triangles sharing an edge) has four graph automorphisms but its line graph \"K\" has eight. In the illustration of the diamond graph shown, rotating the graph by 90 degrees is not a symmetry of the graph, but is a symmetry of its line graph. However, all such exceptional cases have at most four vertices. A strengthened version of the Whitney isomorphism theorem states that, for connected graphs with more than four vertices, there is a one-to-one correspondence between isomorphisms of the graphs and isomorphisms of their line graphs.\n\nAnalogues of the Whitney isomorphism theorem have been proven for the line graphs of multigraphs, but are more complicated in this case.\n\nThe line graph of the complete graph \"K\" is also known as the triangular graph, the Johnson graph \"J\"(\"n\",2), or the complement of the Kneser graph \"KG\". Triangular graphs are characterized by their spectra, except for \"n\" = 8. They may also be characterized (again with the exception of \"K\") as the strongly regular graphs with parameters srg(\"n\"(\"n\" − 1)/2, 2(\"n\" − 2), \"n\" − 2, 4). The three strongly regular graphs with the same parameters and spectrum as \"L\"(\"K\") are the Chang graphs, which may be obtained by graph switching from \"L\"(\"K\").\n\nThe line graph of a bipartite graph is perfect (see Kőnig's theorem), but need not be bipartite as the example of the claw graph shows. The line graphs of bipartite graphs form one of the key building blocks of perfect graphs, used in the proof of the strong perfect graph theorem. A special case of these graphs are the rook's graphs, line graphs of complete bipartite graphs. Like the line graphs of complete graphs, they can be characterized with one exception by their numbers of vertices, numbers of edges, and number of shared neighbors for adjacent and non-adjacent points. The one exceptional case is \"L\"(\"K\"), which shares its parameters with the Shrikhande graph. When both sides of the bipartition have the same number of vertices, these graphs are again strongly regular.\n\nMore generally, a graph \"G\" is said to be a line perfect graph if \"L\"(\"G\") is a perfect graph. The line perfect graphs are exactly the graphs that do not contain a simple cycle of odd length greater than three. Equivalently, a graph is line perfect if and only if each of its biconnected components is either bipartite or of the form \"K\" (the tetrahedron) or \"K\" (a book of one or more triangles all sharing a common edge). Every line perfect graph is itself perfect.\n\nAll line graphs are claw-free graphs, graphs without an induced subgraph in the form of a three-leaf tree. As with claw-free graphs more generally, every connected line graph \"L\"(\"G\") with an even number of edges has a perfect matching; equivalently, this means that if the underlying graph \"G\" has an even number of edges, its edges can be partitioned into two-edge paths.\n\nThe line graphs of trees are exactly the claw-free block graphs. These graphs have been used to solve a problem in extremal graph theory, of constructing a graph with a given number of edges and vertices whose largest tree induced as a subgraph is as small as possible.\n\nAll eigenvalues of the adjacency matrix formula_1 of a line graph are at least −2. The reason for this is that formula_1 can be written as formula_3, where formula_4 is the signless incidence matrix of the pre-line graph and formula_5 is the identity. In particular, formula_6 is the Gramian matrix of a system of vectors: all graphs with this property have been called generalized line graphs.\n\nFor an arbitrary graph \"G\", and an arbitrary vertex \"v\" in \"G\", the set of edges incident to \"v\" corresponds to a clique in the line graph \"L\"(\"G\"). The cliques formed in this way partition the edges of \"L\"(\"G\"). Each vertex of \"L\"(\"G\") belongs to exactly two of them (the two cliques corresponding to the two endpoints of the corresponding edge in \"G\").\n\nThe existence of such a partition into cliques can be used to characterize the line graphs:\nA graph \"L\" is the line graph of some other graph or multigraph if and only if it is possible to find a collection of cliques in \"L\" (allowing some of the cliques to be single vertices) that partition the edges of \"L\", such that each vertex of \"L\" belongs to exactly two of the cliques. It is the line graph of a graph (rather than a multigraph) if this set of cliques satisfies the additional condition that no two vertices of \"L\" are both in the same two cliques. Given such a family of cliques, the underlying graph \"G\" for which \"L\" is the line graph can be recovered by making one vertex in \"G\" for each clique, and an edge in \"G\" for each vertex in \"L\" with its endpoints being the two cliques containing the vertex in \"L\". By the strong version of Whitney's isomorphism theorem, if the underlying graph \"G\" has more than four vertices, there can be only one partition of this type.\n\nFor example, this characterization can be used to show that the following graph is not a line graph:\nIn this example, the edges going upward, to the left, and to the right from the central degree-four vertex do not have any cliques in common. Therefore, any partition of the graph's edges into cliques would have to have at least one clique for each of these three edges, and these three cliques would all intersect in that central vertex, violating the requirement that each vertex appear in exactly two cliques. Thus, the graph shown is not a line graph.\n\nAnother characterization of line graphs was proven in (and reported earlier without proof by ). He showed that there are nine minimal graphs that are not line graphs, such that any graph that is not a line graph has one of these nine graphs as an induced subgraph. That is, a graph is a line graph if and only if no subset of its vertices induces one of these nine graphs. In the example above, the four topmost vertices induce a claw (that is, a complete bipartite graph \"K\"), shown on the top left of the illustration of forbidden subgraphs. Therefore, by Beineke's characterization, this example cannot be a line graph. For graphs with minimum degree at least 5, only the six subgraphs in the left and right columns of the figure are needed in the characterization.\n\n and described linear time algorithms for recognizing line graphs and reconstructing their original graphs. generalized these methods to directed graphs. described an efficient data structure for maintaining a dynamic graph, subject to vertex insertions and deletions, and maintaining a representation of the input as a line graph (when it exists) in time proportional to the number of changed edges at each step.\n\nThe algorithms of and are based on characterizations of line graphs involving odd triangles (triangles in the line graph with the property that there exists another vertex adjacent to an odd number of triangle vertices). However, the algorithm of uses only Whitney's isomorphism theorem. It is complicated by the need to recognize deletions that cause the remaining graph to become a line graph, but when specialized to the static recognition problem only insertions need to be performed, and the algorithm performs the following steps:\nEach step either takes constant time, or involves finding a vertex cover of constant size within a graph \"S\" whose size is proportional to the number of neighbors of \"v\". Thus, the total time for the whole algorithm is proportional to the sum of the numbers of neighbors of all vertices, which (by the handshaking lemma) is proportional to the number of input edges.\n\n consider the sequence of graphs\nThey show that, when \"G\" is a finite connected graph, only four behaviors are possible for this sequence:\nIf \"G\" is not connected, this classification applies separately to each component of \"G\".\n\nFor connected graphs that are not paths, all sufficiently high numbers of iteration of the line graph operation produce graphs that are Hamiltonian.\n\nWhen a planar graph \"G\" has maximum vertex degree three, its line graph is planar, and every planar embedding of \"G\" can be extended to an embedding of \"L\"(\"G\"). However, there exist planar graphs with higher degree whose line graphs are nonplanar. These include, for example, the 5-star \"K\", the gem graph formed by adding two non-crossing diagonals within a regular pentagon, and all convex polyhedra with a vertex of degree four or more.\n\nAn alternative construction, the medial graph, coincides with the line graph for planar graphs with maximum degree three, but is always planar. It has the same vertices as the line graph, but potentially fewer edges: two vertices of the medial graph are adjacent if and only if the corresponding two edges are consecutive on some face of the planar embedding. The medial graph of the dual graph of a plane graph is the same as the medial graph of the original plane graph.\n\nFor regular polyhedra or simple polyhedra, the medial graph operation can be represented geometrically by the operation of cutting off each vertex of the polyhedron by a plane through the midpoints of all its incident edges. This operation is known variously as the second truncation, degenerate truncation, or rectification.\n\nThe total graph \"T\"(\"G\") of a graph \"G\" has as its vertices the elements (vertices or edges) of \"G\", and has an edge between two elements whenever they are either incident or adjacent. The total graph may also be obtained by subdividing each edge of \"G\" and then taking the square of the subdivided graph.\n\nThe concept of the line graph of \"G\" may naturally be extended to the case where \"G\" is a multigraph. In this case, the characterizations of these graphs can be simplified: the characterization in terms of clique partitions no longer needs to prevent two vertices from belonging to the same to cliques, and the characterization by forbidden graphs has seven forbidden graphs instead of nine.\n\nHowever, for multigraphs, there are larger numbers of pairs of non-isomorphic graphs that have the same line graphs. For instance a complete bipartite graph \"K\" has the same line graph as the dipole graph and Shannon multigraph with the same number of edges. Nevertheless, analogues to Whitney's isomorphism theorem can still be derived in this case.\n\nIt is also possible to generalize line graphs to directed graphs. If \"G\" is a directed graph, its directed line graph or line digraph has one vertex for each edge of \"G\". Two vertices representing directed edges from \"u\" to \"v\" and from \"w\" to \"x\" in \"G\" are connected by an edge from \"uv\" to \"wx\" in the line digraph when \"v\" = \"w\". That is, each edge in the line digraph of \"G\" represents a length-two directed path in \"G\". The de Bruijn graphs may be formed by repeating this process of forming directed line graphs, starting from a complete directed graph.\n\nIn a line graph \"L\"(\"G\"), each vertex of degree \"k\" in the original graph \"G\" creates \"k(k-1)/2\" edges in the line graph. For many types of analysis this means high-degree nodes in \"G\" are over-represented in the line graph \"L\"(\"G\"). For instance, consider a random walk on the vertices of the original graph \"G\". This will pass along some edge \"e\" with some frequency \"f\". On the other hand, this edge \"e\" is mapped to a unique vertex, say \"v\", in the line graph \"L\"(\"G\"). If we now perform the same type of random walk on the vertices of the line graph, the frequency with which \"v\" is visited can be completely different from \"f\". If our edge \"e\" in \"G\" was connected to nodes of degree \"O(k)\", it will be traversed \"O(k)\" more frequently in the line graph \"L\"(\"G\"). Put another way, the Whitney graph isomorphism theorem guarantees that the line graph almost always encodes the topology of the original graph \"G\" faithfully but it does not guarantee that dynamics on these two graphs have a simple relationship. One solution is to construct a weighted line graph, that is, a line graph with weighted edges. There are several natural ways to do this. For instance if edges \"d\" and \"e\" in the graph \"G\" are incident at a vertex \"v\" with degree \"k\", then in the line graph \"L\"(\"G\") the edge connecting the two vertices \"d\" and \"e\" can be given weight \"1/(k-1)\". In this way every edge in \"G\" (provided neither end is connected to a vertex of degree '1') will have strength \"2\" in the line graph \"L\"(\"G\") corresponding to the two ends that the edge has in \"G\". It is straightforward to extend this definition of a weighted line graph to cases where the original graph \"G\" was directed or even weighted. The principle in all cases is to ensure the line graph \"L\"(\"G\") reflects the dynamics as well as the topology of the original graph \"G\".\n\nThe edges of a hypergraph may form an arbitrary family of sets, so the line graph of a hypergraph is the same as the intersection graph of the sets from the family.\n\n"}
{"id": "24095830", "url": "https://en.wikipedia.org/wiki?curid=24095830", "title": "List of important publications in theoretical computer science", "text": "List of important publications in theoretical computer science\n\nThis is a list of important publications in theoretical computer science, organized by field.\n\nSome reasons why a particular publication might be regarded as important:\n\nThe review of this early text by Carl Smith of Purdue University (in the \"Society for Industrial and Applied Mathematics Reviews\"), reports that this a text with an \"appropriate blend of intuition and rigor… in the exposition of proofs\" that presents \"the fundamental results of classical recursion theory [RT]... in a style... accessible to undergraduates with minimal mathematical background\". While he states that it \"would make an excellent introductory text for an introductory course in [RT] for mathematics students\", he suggests that an \"instructor must be prepared to substantially augment the material… \" when it used with computer science students (given a dearth of material on RT applications to this area).\n\n\nDescription: The paper presented the tree automaton, an extension of the automata. The tree automaton had numerous applications to proofs of correctness of programs.\n\n\nDescription: Mathematical treatment of automata, proof of core properties, and definition of non-deterministic finite automaton.\n\n\nDescription: A popular textbook.\n\nDescription: This article introduced what is now known as the Chomsky hierarchy, a containment hierarchy of classes of formal grammars that generate formal languages.\n\n\nDescription: This article set the limits of computer science. It defined the Turing Machine, a model for all computations.\nOn the other hand, it proved the undecidability of the halting problem and Entscheidungsproblem and by doing so found the limits of possible computation.\n\nThe first textbook on the theory of recursive functions. The book went through many editions and earned Péter the Kossuth Prize from the Hungarian government. Reviews by Raphael M. Robinson and Stephen Kleene praised the book for providing an effective elementary introduction for students.\nDescription: this paper introduced finite automata, regular expressions, and regular languages, and established their connection.\n\nBesides the estimable press bringing these recent texts forward, they are very positively reviewed in \"ACM's SIGACT News\" by Daniel Apon of the University of Arkansas, who identifies them as \"textbooks for a course in complexity theory, aimed at early graduate… or... advanced undergraduate students… [with] numerous, unique strengths and very few weaknesses,\" and states that both are:\nThe reviewer notes that there is \"a definite attempt in [Arora and Barak] to include very up-to-date material, while Goldreich focuses more on developing a contextual and historical foundation for each concept presented,\" and that he \"applaud[s] all… authors for their outstanding contributions.\"\n\nDescription: The Blum axioms.\n\nDescription: This paper showed that PH is contained in IP.\n\nDescription: This paper introduced the concept of NP-Completeness and proved that Boolean satisfiability problem (SAT) is NP-Complete. Note that similar ideas were developed independently slightly later by Leonid Levin at \"Levin, Universal Search Problems. Problemy Peredachi Informatsii 9(3):265-266, 1973\".\n\nDescription: The main importance of this book is due to its extensive list of more than 300 NP-Complete problems. This list became a common reference and definition. Though the book was published only few years after the concept was defined such an extensive list was found.\n\nDescription: This technical report was the first publication talking about what later was renamed computational complexity\n\n\nDescription: Constructed the \"Klee–Minty cube\" in dimension \"D\", whose 2 corners are each visited by Dantzig's simplex algorithm for linear optimization.\n\nDescription: This paper showed that the existence of one way functions leads to computational randomness.\n\nDescription: IP is a complexity class whose characterization (based on interactive proof systems) is quite different from the usual time/space bounded computational classes. In this paper, Shamir extended the technique of the previous paper by Lund, et al., to show that PSPACE is contained in IP, and hence IP = PSPACE, so that each problem in one complexity class is solvable in the other.\n\n\nDescription: This paper showed that 21 different problems are NP-Complete and showed the importance of the concept.\n\nDescription: This paper introduced the concept of zero knowledge.\n\n\nDescription: Gödel discusses the idea of efficient universal theorem prover.\n\nDescription: This paper gave computational complexity its name and seed.\n\nDescription: There is a polynomial time algorithm to find a maximum matching in a graph that is not bipartite and another step toward the idea of computational complexity. For more information see .\n\nDescription: This paper creates a theoretical framework for trapdoor functions and described some of their applications, like in cryptography. Note that the concept of trapdoor functions was brought at \"New directions in cryptography\" six years earlier (See section V \"Problem Interrelationships and Trap Doors.\").\n\n\nDescription: An introduction to computational complexity theory, the book explains its author's characterization of P-SPACE and other results.\n\nDescription: These three papers established the surprising fact that certain problems in NP remain hard even when only an approximative solution is required. See PCP theorem.\n\nDescription: First definition of the complexity class P. One of the founding papers of complexity theory.\n\nDescription: The DPLL algorithm. The basic algorithm for SAT and other NP-Complete problems.\n\nDescription: First description of resolution and unification used in automated theorem proving; used in Prolog and logic programming.\n\nDescription: The use of an algorithm for minimum spanning tree as an approximation algorithm for the NP-Complete travelling salesman problem. Approximation algorithms became a common method for coping with NP-Complete problems.\n\n\nDescription: For long, there was no provably polynomial time algorithm for the linear programming problem. Khachiyan was the first to provide an algorithm that was polynomial (and not just was fast enough most of the time as previous algorithms). Later, Narendra Karmarkar presented a faster algorithm at: Narendra Karmarkar, \"A new polynomial time algorithm for linear programming\", Combinatorica, vol 4, no. 4, p. 373–395, 1984.\n\nDescription: The paper presented the Miller-Rabin primality test and outlined the program of randomized algorithms.\n\nDescription: This article described simulated annealing which is now a very common heuristic for NP-Complete problems.\n\n\nDescription: This monograph has three popular algorithms books and a number of fascicles. The algorithms are written in both English and MIX assembly language (or MMIX assembly language in more recent fascicles). This makes algorithms both understandable and precise. However, the use of a low-level programming language frustrates some programmers more familiar with modern structured programming languages.\n\n\nDescription: An early, influential book on algorithms and data structures, with implementations in Pascal.\n\n\nDescription: One of the standard texts on algorithms for the period of approximately 1975–1985.\n\nDescription: Explains the \"Why\"s of algorithms and data-structures. Explains the \"Creative Process\", the \"Line of Reasoning\", the \"Design Factors\" behind innovative solutions.\n\n\nDescription: A very popular text on algorithms in the late 1980s. It was more accessible and readable (but more elementary) than Aho, Hopcroft, and Ullman. There are more recent editions.\n\n\nDescription: This textbook has become so popular that it is almost the de facto standard for teaching basic algorithms. The 1st edition (with first three authors) was published in 1990, the 2nd edition in 2001, and the 3rd in 2009.\n\n\nDescription: Proposed a computational and combinatorial approach to probability.\n\n\nDescription: This was the beginning of algorithmic information theory and Kolmogorov complexity. Note that though Kolmogorov complexity is named after Andrey Kolmogorov, he said that the seeds of that idea are due to Ray Solomonoff. Andrey Kolmogorov contributed a lot to this area but in later articles.\n\nDescription: An introduction to algorithmic information theory by one of the important people in the area.\n\nDescription: This paper created the field of information theory.\n\nDescription: In this paper, Hamming introduced the idea of error-correcting code. He created the Hamming code and the Hamming distance and developed methods for code optimality proofs.\n\nDescription: The Huffman coding.\n\nDescription: The LZ77 compression algorithm.\n\nDescription: A popular introduction to information theory.\n\nDescription: Robert Floyd's landmark paper Assigning Meanings to Programs introduces the method of inductive assertions and describes how a program annotated with first-order assertions may be shown to satisfy a pre- and post-condition specification - the paper also introduces the concepts of loop invariant and verification condition.\n\nDescription: Tony Hoare's paper An Axiomatic Basis for Computer Programming describes a set of inference (i.e. formal proof) rules for fragments of an Algol-like programming language described in terms of (what are now called) Hoare-triples.\n\nDescription: Edsger Dijkstra's paper Guarded Commands, Nondeterminacy and Formal Derivation of Programs (expanded by his 1976 postgraduate-level textbook A Discipline of Programming) proposes that, instead of formally verifying a program after it has been written (i.e. post facto), programs and their formal proofs should be developed hand-in-hand (using predicate transformers to progressively refine weakest pre-conditions), a method known as program (or formal) refinement (or derivation), or sometimes \"correctness-by-construction\".\n\n\nDescription: The paper that introduced invariance proofs of concurrent programs.\n\n\nDescription: In this paper, along with the same authors paper \"Verifying Properties of Parallel Programs: An Axiomatic Approach. Commun. ACM 19(5): 279-285 (1976)\", the axiomatic approach to parallel programs verification was presented.\n\n\nDescription: Edsger Dijkstra's classic postgraduate-level textbook A Discipline of Programming extends his earlier paper Guarded Commands, Nondeterminacy and Formal Derivation of Programs and firmly establishes the principle of formally deriving programs (and their proofs) from their specification.\n\n\nDescription: Joe Stoy's Denotational Semantics is the first (postgraduate level) book-length exposition of the mathematical (or functional) approach to the formal semantics of programming languages (in contrast to the operational and algebraic approaches).\n\n \nDescription: The use of temporal logic was suggested as a method for formal verification.\n\n\nDescription: Model checking was introduced as a procedure to check correctness of concurrent programs.\n\n\nDescription: Tony Hoare's (original) communicating sequential processes (CSP) paper introduces the idea of concurrent processes (i.e. programs) that do not share variables but instead cooperate solely by exchanging synchronous messages.\n\n\nDescription: Robin Milner's A Calculus of Communicating Systems (CCS) paper describes a process algebra permitting systems of concurrent processes to be reasoned about formally, something which has not been possible for earlier models of concurrency (semaphores, critical sections, original CSP).\n\n\nDescription: Cliff Jones' textbook Software Development: A Rigorous Approach is the first full-length exposition of the Vienna Development Method (VDM), which had evolved (principally) at IBM's Vienna research lab over the previous decade and which combines the idea of program refinement as per Dijkstra with that of data refinement (or reification) whereby algebraically-defined abstract data types are formally transformed into progressively more \"concrete\" representations.\n\n\nDescription: David Gries' textbook The Science of Programming describes Dijkstra's weakest precondition method of formal program derivation, except in a very much more accessible manner than Dijkstra's earlier \"A Discipline of Programming\".\n\nIt shows how to construct programs that work correctly (without bugs, other than from typing errors). It does this by showing how to use precondition and postcondition predicate expressions and program proving techniques to guide the way programs are created.\n\nThe examples in the book are all small-scale, and clearly academic (as opposed to real-world). They emphasize basic algorithms, such as sorting and merging, and string manipulation. Subroutines (functions) are included, but object-oriented and functional programming environments are not addressed.\n\n\nDescription: Tony Hoare's Communicating Sequential Processes (CSP) textbook (currently the third most cited computer science reference of all time) presents an updated CSP model in which cooperating processes do not even have program variables and which, like CCS, permits systems of processes to be reasoned about formally.\n\nDescription: Girard's linear logic was a breakthrough in designing typing systems for sequential and concurrent computation, especially for resource conscious typing systems.\n\n\nDescription: This paper introduces the Pi-Calculus, a generalisation of CCS which allows process mobility. The calculus is extremely simple and has become the dominant paradigm in the theoretical study of programming languages, typing systems and program logics.\n\nDescription: Mike Spivey's classic textbook The Z Notation: A Reference Manual summarises the formal specification language Z notation which, although originated by Jean-Raymond Abrial, had evolved (principally) at Oxford University over the previous decade.\n\n\nDescription: Robin Milner's textbook Communication and Concurrency is a more accessible, although still technically advanced, exposition of his earlier CCS work.\n\n\nDescription: the up-to-date version of Predicative programming. The basis for C.A.R. Hoare's UTP. The simplest and most comprehensive formal methods.\n"}
{"id": "21362023", "url": "https://en.wikipedia.org/wiki?curid=21362023", "title": "List of numerical libraries", "text": "List of numerical libraries\n\nThis is a list of numerical libraries, which are libraries used in software development for performing numerical calculations. It is not a complete listing but is instead a list of numerical libraries with articles on Wikipedia, with few exceptions.\n\nThe choice of a typical library depends on a diverse range of requirements such as: desired features (for e.g.: large dimensional linear algebra, parallel computation, partial differential equations), commercial/opensource nature, readability of API, portability or platform/compiler dependence (for e.g.: Linux, Windows, Visual C++, GCC), performance in speed, ease-of-use, continued support from developers, standard compliance, specialized optimization in code for specific application scenarios or even the size of the code-base to be installed.\n\nAs we find comprehensive surveys rarely available, there is almost always (at least initially) a difficult choice among a number of possible libraries.\nOften it tends to be at the discretion of the user based on his own taste and comforts, only due to the lack of proper information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1124696", "url": "https://en.wikipedia.org/wiki?curid=1124696", "title": "Long hundred", "text": "Long hundred\n\nThe long hundred, great hundred, or twelfty is the \"hundred\" of six score (120) used in Germanic languages prior to the 15th century. The number was simply described as hundred and translated into Latin in Germanic-speaking countries as (Roman numeral c.), but the qualifier \"long\" is now added because present English uses the word \"hundred\" exclusively to refer to the number of five score (100) instead.\n\nThe long hundred was 120 but the long thousand was reckoned decimally as 10 long hundreds (1200).\n\nThe word is cognate with \"hunderd\" in Old Frisian, \"hundrað\" in Old Norse, and \"hundert\" in Old German.\n\nThe existence of a non-decimal base in the earliest traces of the Germanic languages is attested by the presence of glosses such as \"tentywise\" or \"ten-count\" denoting that certain numbers are to be understood as decimal. Such glosses would not be expected where decimal counting was usual. In the Gothic Bible, some marginalia glosses a five hundred (\"fimf hundram\") in the text as being understood \"taihuntewjam\" (\"tentywise\"). Similar words are known in most other Germanic languages. Old Norse clearly used such a system, with its words for \"one hundred and eighty\" meaning 200 and \"two hundred\" meaning 240. Its use in medieval England and Scotland is documented by Stevenson and Goodare, although Goodare notes that it was sometimes avoided by using numbers such as \"seven score\". The Assize of Weights and Measures, one of England's statutes of uncertain date from , shows both the short and long hundred in competing use: the hundred of kippers is formed by six score fish and the hundred of hemp canvas and linen cloth is formed by six score ells but the hundred of pounds to be used in measuring bulk goods is five times twenty and the hundred of fresh herring is five score fish. Within the original Latin text, the numeral c. is used for a value of 120: \n\nThe reckoning by long hundreds waned as Arabic numerals spread throughout Europe during and after the 14th century.\n\n\n"}
{"id": "677078", "url": "https://en.wikipedia.org/wiki?curid=677078", "title": "Meander (mathematics)", "text": "Meander (mathematics)\n\nIn mathematics, a meander or closed meander is a self-avoiding closed curve which intersects a line a number of times. Intuitively, a meander can be viewed as a road crossing a river through a number of bridges.\n\nGiven a fixed oriented line \"L\" in the Euclidean plane R, a meander of order \"n\" is a non-self-intersecting closed curve in R which transversally intersects the line at 2\"n\" points for some positive integer \"n\". The line and curve together form a meandric system. Two meanders are said to be equivalent if there is a homeomorphism of the whole plane that takes \"L\" to itself and takes one meander to the other.\n\nThe meander of order 1 intersects the line twice:\n\nThe meanders of order 2 intersect the line four times.\n\nThe number of distinct meanders of order \"n\" is the meandric number \"M\". The first fifteen meandric numbers are given below .\n\nA meandric permutation of order \"n\" is defined on the set {1, 2, ..., 2\"n\"} and is determined by a meandric system in the following way:\n\nIn the diagram on the right, the order 4 meandric permutation is given by (1 8 5 4 3 6 7 2). This is a permutation written in cyclic notation and not to be confused with one-line notation.\n\nIf π is a meandric permutation, then π consists of two cycles, one containing of all the even symbols and the other all the odd symbols. Permutations with this property are called \"alternate permutations\", since the symbols in the original permutation alternate between odd and even integers. However, not all alternate permutations are meandric because it may not be possible to draw them without introducing a self-intersection in the curve. For example, the order 3 alternate permutation, (1 4 3 6 5 2), is not meandric.\n\nGiven a fixed oriented line \"L\" in the Euclidean plane R, an open meander of order \"n\" is a non-self-intersecting oriented curve in R which transversally intersects the line at \"n\" points for some positive integer \"n\". Two open meanders are said to be equivalent if they are homeomorphic in the plane.\n\nThe open meander of order 1 intersects the line once:\n\nThe open meander of order 2 intersects the line twice:\n\nThe number of distinct open meanders of order \"n\" is the open meandric number \"m\". The first fifteen open meandric numbers are given below .\n\nGiven a fixed oriented ray \"R\" in the Euclidean plane R, a semi-meander of order \"n\" is a non-self-intersecting closed curve in R which transversally intersects the ray at \"n\" points for some positive integer \"n\". Two semi-meanders are said to be equivalent if they are homeomorphic in the plane.\n\nThe semi-meander of order 1 intersects the ray once:\n\nThe semi-meander of order 2 intersects the ray twice:\n\nThe number of distinct semi-meanders of order \"n\" is the semi-meandric number \"M\" (usually denoted with an overline instead of an underline). The first fifteen semi-meandric numbers are given below .\n\nThere is an injective function from meandric to open meandric numbers:\n\nEach meandric number can be bounded by semi-meandric numbers:\n\nFor \"n\" > 1, meandric numbers are even:\n\n"}
{"id": "3531649", "url": "https://en.wikipedia.org/wiki?curid=3531649", "title": "Metaman", "text": "Metaman\n\nMetaman: The Merging of Humans and Machines into a Global Superorganism () is a 1993 book by author Gregory Stock. The title refers to a superorganism comprising humanity and its technology. \n\nWhile many people have had ideas about a global brain, they have tended to suppose that this can be improved or altered by humans according to their will. Metaman can be seen as a development that directs humanity's will to its own ends, whether it likes it or not, through the operation of market forces. While it is difficult to think of making a life-form based on metals that can mine its own 'food', it is possible to imagine a superorganism that incorporates humans as its \"cells\" and entices them to sustain it (communalness), just as our cells interwork to sustain us.\n\n"}
{"id": "22885030", "url": "https://en.wikipedia.org/wiki?curid=22885030", "title": "Modal fictionalism", "text": "Modal fictionalism\n\nModal fictionalism is a term used in philosophy, and more specifically in the metaphysics of modality, to describe the position that holds that modality can be analysed in terms of a fiction about possible worlds. The theory comes in two versions: Strong and Timid. Both positions were first exposed by Gideon Rosen starting from 1990. \n\nAccording to strong fictionalism about possible worlds (another name for strong modal fictionalism), the following bi-conditionals are necessary and specify the truth-conditions for certain cases of modal claims:\n\nRecent supporters of this view added further specifications of these bi-conditionals to counter certain objections. In the case of claims of possibility, the revised bi-conditional is thus spelled out: (1.1) it is possible that P iff At this universe, presently, the translation of P into the language of a fiction F holds according to F.\n\nAccording to a timid version of fictionalism about possible worlds, our possible worlds can be properly understood as involving reference to a fiction, but the aforementioned bi-conditionals should not be taken as an analysis of certain cases of modality. \n\n\nThis objection can be spelled out in at least two ways: artificiality as contingency or artificiality as lack of accessibility.\n\n\n\n"}
{"id": "1946702", "url": "https://en.wikipedia.org/wiki?curid=1946702", "title": "Moyal product", "text": "Moyal product\n\nIn mathematics, the Moyal product (after José Enrique Moyal; also called the star product or Weyl–Groenewold product, after Hermann Weyl and Hilbrand J. Groenewold) is perhaps the best-known example of a phase-space star product. It is an associative, non-commutative product, ★, on the functions on ℝ, equipped with its Poisson bracket (with a generalization to symplectic manifolds, described below). It is a special case of the ★-product of the \"algebra of symbols\" of a universal enveloping algebra.\n\nThe Moyal product is named after José Enrique Moyal, but is also sometimes called the Weyl–Groenewold product as it was introduced by H. J. Groenewold in his 1946 doctoral dissertation, in a trenchant appreciation of the Weyl correspondence. Moyal actually appears not to know about the product in his celebrated article and was crucially lacking it in his legendary correspondence with Dirac, as illustrated in his biography. The popular naming after Moyal appears to have emerged only in the 1970s, in homage to his flat phase-space quantization picture.\n\nThe product for smooth functions \"f\" and \"g\" on ℝ takes the form\n\nwhere each \"C\" is a certain bidifferential operator of order \"n\" characterized by the following properties (see below for an explicit formula):\n\n\nNote that, if one wishes to take functions valued in the real numbers, then an alternative version eliminates the formula_6 in condition 2 and eliminates condition 4.\n\nIf one restricts to polynomial functions, the above algebra is isomorphic to the Weyl algebra \"A\", and the two offer alternative realizations of the Weyl map of the space of polynomials in variables (or the symmetric algebra of a vector space of dimension 2\"n\").\n\nTo provide an explicit formula, consider a constant Poisson bivector Π on ℝ:\nwhere Π is a complex number for each \"i\", \"j\".\n\nThe star product of two functions formula_8 and formula_9 can then be defined as\n\nwhere ħ is the reduced Planck constant, treated as a formal parameter here. This is a special case of what is known as the Berezin formula on the algebra of symbols and can be given a closed form (which follows from the Baker–Campbell–Hausdorff formula). The closed form can be obtained by using the exponential:\nwhere formula_12 is the multiplication map, formula_13, and the exponential is treated as a power series:\n\nThat is, the formula for formula_15 is\n\nAs indicated, often one eliminates all occurrences of formula_6 above, and the formulas then restrict naturally to real numbers.\n\nNote that if the functions \"f\" and \"g\" are polynomials, the above infinite sums become finite (reducing to the ordinary Weyl-algebra case).\n\nThe relationship of the Moyal product to the generalized ★-product used in the definition of the \"algebra of symbols\" of a universal enveloping algebra follows from the fact that the Weyl algebra is the universal enveloping algebra of the Heisenberg algebra (modulo that the center equals the unit).\n\nOn any symplectic manifold, one can, at least locally, choose coordinates so as to make the symplectic structure \"constant\", by Darboux's theorem; and, using the associated Poisson bivector, one may consider the above formula. For it to work globally,\nas a function on the whole manifold (and not just a local formula), one must equip the symplectic manifold with a torsion-free symplectic connection. This makes it a Fedosov manifold.\n\nMore general results for \"arbitrary Poisson manifolds\" (where the Darboux theorem does not apply) are given by the Kontsevich quantization formula.\n\nA simple explicit example of the construction and utility of the -product (for the simplest case of a two-dimensional euclidean phase space) is given in the article on the Wigner–Weyl transform: two Gaussians compose with this -product according to a hyperbolic tangent law:\n\n\"Every correspondence prescription\" between phase space and Hilbert space, however, induces \"its own\" proper -product.\n\nSimilar results are seen in the Segal–Bargmann space and in the theta representation of the Heisenberg group, where the creation and annhilation operators formula_19 and formula_20 are understood to act on the complex plane (respectively, the upper half-plane for the Heisenberg group), so that the position and momenta operators are given by formula_21 and formula_22. This situation is clearly different from the case where the positions are taken to be real-valued, but does offer insights into the overall algebraic structure of the Heisenberg algebra and its envelope, the Weyl algebra.\n"}
{"id": "18021657", "url": "https://en.wikipedia.org/wiki?curid=18021657", "title": "Noether's second theorem", "text": "Noether's second theorem\n\nIn mathematics and theoretical physics, Noether's second theorem relates symmetries of an action functional with a system of differential equations. The action \"S\" of a physical system is an integral of a so-called Lagrangian function \"L\", from which the system's behavior can be determined by the principle of least action. \n\nSpecifically, the theorem says that if the action has an infinite-dimensional Lie algebra of infinitesimal symmetries parameterized linearly by \"k\" arbitrary functions and their derivatives up to order \"m\", then the functional derivatives of \"L\" satisfy a system of \"k\" differential equations.\n\nNoether's second theorem is sometimes used in gauge theory. Gauge theories are the basic elements of all modern field theories of physics, such as the prevailing Standard Model.\n\n\n\n"}
{"id": "26548937", "url": "https://en.wikipedia.org/wiki?curid=26548937", "title": "Nonlinear complementarity problem", "text": "Nonlinear complementarity problem\n\nIn applied mathematics, a nonlinear complementarity problem (NCP) with respect to a mapping \"ƒ\" : R → R, denoted by NCP\"ƒ\", is to find a vector \"x\" ∈ R such that\n\nwhere \"ƒ\"(\"x\") is a smooth mapping.\n\n"}
{"id": "9421082", "url": "https://en.wikipedia.org/wiki?curid=9421082", "title": "Order-5 square tiling", "text": "Order-5 square tiling\n\nIn geometry, the order-5 square tiling is a regular tiling of the hyperbolic plane. It has Schläfli symbol of {4,5}.\n\nThis tiling is topologically related as a part of sequence of regular polyhedra and tilings with vertex figure (4).\n\nThis hyperbolic tiling is related to a semiregular infinite skew polyhedron with the same vertex figure in Euclidean 3-space.\n\n\n\n"}
{"id": "25065", "url": "https://en.wikipedia.org/wiki?curid=25065", "title": "Parameter", "text": "Parameter\n\nA parameter (from the Ancient Greek παρά, \"para\": \"beside\", \"subsidiary\"; and μέτρον, \"metron\": \"measure\"), generally, is any characteristic that can help in defining or classifying a particular system (meaning an event, project, object, situation, etc.). That is, a parameter is an element of a system that is useful, or critical, when identifying the system, or when evaluating its performance, status, condition, etc.\n\n\"Parameter\" has more specific meanings within various disciplines, including mathematics, computing and computer programming, engineering, statistics, logic and linguistics. Within and across these fields, careful distinction must be maintained of the different usages of the term \"parameter\" and of other terms often associated with it, such as argument, property, axiom, variable, function, attribute, etc.\n\nMathematical functions have one or more arguments that are designated in the definition by variables. A function definition can also contain parameters, but unlike variables, parameters are not listed among the arguments that the function takes. When parameters are present, the definition actually defines a whole family of functions, one for every valid set of values of the parameters. For instance, one could define a general quadratic function by declaring\n\nHere, the variable \"x\" designates the function's argument, but \"a\", \"b\", and \"c\" are parameters that determine which particular quadratic function is being considered. A parameter could be incorporated into the function name to indicate its dependence on the parameter. For instance, one may define the base-\"b\" logarithm by the formula\nwhere \"b\" is a parameter that indicates which logarithmic function is being used. It is not an argument of the function, and will, for instance, be a constant when considering the derivative formula_3.\n\nIn some informal situations it is a matter of convention (or historical accident) whether some or all of the symbols in a function definition are called parameters. However, changing the status of symbols between parameter and variable changes the function as a mathematical object. For instance, the notation for the falling factorial power\ndefines a polynomial function of \"n\" (when \"k\" is considered a parameter), but is not a polynomial function of \"k\" (when \"n\" is considered a parameter). Indeed, in the latter case, it is only defined for non-negative integer arguments. More formal presentations of such situations typically start out with a function of several variables (including all those that might sometimes be called \"parameters\") such as\nas the most fundamental object being considered, then defining functions with fewer variables from the main one by means of currying.\n\nSometimes it is useful to consider all functions with certain parameters as \"parametric family\", i.e. as an indexed family of functions. Examples from probability theory are given further below.\n\n\nW.M. Woods ... a mathematician ... writes ... \"... a variable is one of the many things a \"parameter\" is not.\" ... The dependent variable, the speed of the car, depends on the independent variable, the position of the gas pedal.\n\n[Kilpatrick quoting Woods] \"Now ... the engineers ... change the lever arms of the linkage ... the speed of the car ... will still depend on the pedal position ...\" but in a ... different manner\". You have changed a parameter\"\n\n\nIn the context of a mathematical model, such as a probability distribution, the distinction between variables and parameters was described by Bard as follows:\n\nIn analytic geometry, curves are often given as the image of some function. The argument of the function is invariably called \"the parameter\". A circle of radius 1 centered at the origin can be specified in more than one form:\nHence these equations, which might be called functions elsewhere are in analytic geometry characterized as parametric equations and the independent variables are considered as parameters.\n\nIn mathematical analysis, integrals dependent on a parameter are often considered. These are of the form\nIn this formula, \"t\" is the argument of the function \"F\", and on the right-hand side the \"parameter\" on which the integral depends. When evaluating the integral, \"t\" is held constant, and so it is considered to be a parameter. If we are interested in the value of \"F\" for different values of \"t\", we then consider \"t\" to be a variable. The quantity \"x\" is a \"dummy variable\" or \"variable of integration\" (confusingly, also sometimes called a \"parameter of integration\").\n\nIn statistics and econometrics, the probability framework above still holds, but attention shifts to estimating the parameters of a distribution based on observed data, or testing hypotheses about them. In frequentist estimation parameters are considered \"fixed but unknown\", whereas in Bayesian estimation they are treated as random variables, and their uncertainty is described as a distribution.\n\nIn estimation theory of statistics, \"statistic\" or estimator refers to samples, whereas \"parameter\" or estimand refers to populations, where the samples are taken from. A statistic is a numerical characteristic of a sample that can be used as an estimate of the corresponding parameter, the numerical characteristic of the population from which the sample was drawn.\n\nFor example, the sample mean (estimator), denoted formula_9, can be used as an estimate of the \"mean\" parameter (estimand), denoted \"μ\", of the population from which the sample was drawn. Similarly, the sample variance (estimator), denoted \"S\", can be used to estimate the \"variance\" parameter (estimand), denoted \"σ\", of the population from which the sample was drawn. (Note that the sample standard deviation (\"S\") is not an unbiased estimate of the population standard deviation (\"σ\"): see Unbiased estimation of standard deviation.)\n\nIt is possible to make statistical inferences without assuming a particular parametric family of probability distributions. In that case, one speaks of \"non-parametric statistics\" as opposed to the parametric statistics just described. For example, a test based on Spearman's rank correlation coefficient would be called non-parametric since the statistic is computed from the rank-order of the data disregarding their actual values (and thus regardless of the distribution they were sampled from), whereas those based on the Pearson product-moment correlation coefficient are parametric tests since it is computed directly from the data values and thus estimates the parameter known as the population correlation.\n\nIn probability theory, one may describe the distribution of a random variable as belonging to a \"family\" of probability distributions, distinguished from each other by the values of a finite number of \"parameters\". For example, one talks about \"a Poisson distribution with mean value λ\". The function defining the distribution (the probability mass function) is:\nThis example nicely illustrates the distinction between constants, parameters, and variables. \"e\" is Euler's number, a fundamental mathematical constant. The parameter λ is the mean number of observations of some phenomenon in question, a property characteristic of the system. \"k\" is a variable, in this case the number of occurrences of the phenomenon actually observed from a particular sample. If we want to know the probability of observing \"k\" occurrences, we plug it into the function to get formula_11. Without altering the system, we can take multiple samples, which will have a range of values of \"k\", but the system is always characterized by the same λ.\n\nFor instance, suppose we have a radioactive sample that emits, on average, five particles every ten minutes. We take measurements of how many particles the sample emits over ten-minute periods. The measurements exhibit different values of \"k\", and if the sample behaves according to Poisson statistics, then each value of \"k\" will come up in a proportion given by the probability mass function above. From measurement to measurement, however, λ remains constant at 5. If we do not alter the system, then the parameter λ is unchanged from measurement to measurement; if, on the other hand, we modulate the system by replacing the sample with a more radioactive one, then the parameter λ would increase.\n\nAnother common distribution is the normal distribution, which has as parameters the mean μ and the variance σ².\n\nIn these above examples, the distributions of the random variables are completely specified by the type of distribution, i.e. Poisson or normal, and the parameter values, i.e. mean and variance. In such a case, we have a parameterized distribution.\n\nIt is possible to use the sequence of moments (mean, mean square, ...) or cumulants (mean, variance, ...) as parameters for a probability distribution: see Statistical parameter.\n\nIn computing, a parameter is defined as \"a reference or value that is passed to a function, procedure, subroutine, command, or program\". For example, the name of a file, (a parameter), is passed to a computer program, which then performs a specific function; that is, a program may be passed the name of a file on which it will perform the specific function.\n\nIn computer programming, two notions of parameter are commonly used, and are referred to as parameters and arguments—or more formally as a formal parameter and an actual parameter.\n\nFor example, in the definition of a function such as\n\"x\" is the \"formal parameter\" (the \"parameter\") of the defined function.\n\nWhen the function is evaluated for a given value, as in\n3 is the \"actual parameter\" (the \"argument\") for evaluation by the defined function; it is a given value (actual value) that is substituted for the \"formal parameter\" of the defined function. (In casual usage the terms \"parameter\" and \"argument\" might inadvertently be interchanged, and thereby used incorrectly.)\n\nThese concepts are discussed in a more precise way in functional programming and its foundational disciplines, lambda calculus and combinatory logic. Terminology varies between languages; some computer languages such as C define parameter and argument as given here, while Eiffel uses an alternative convention.\n\nIn engineering (especially involving data acquisition) the term \"parameter\" sometimes loosely refers to an individual measured item. This usage isn't consistent, as sometimes the term \"channel\" refers to an individual measured item, with \"parameter\" referring to the setup information about that channel.\n\n\"Speaking generally, properties are those physical quantities which directly describe the physical attributes of the system; parameters are those combinations of the properties which suffice to determine the response of the system. Properties can have all sorts of dimensions, depending upon the system being considered; parameters are dimensionless, or have the dimension of time or its reciprocal.\"\n\nThe term can also be used in engineering contexts, however, as it is typically used in the physical sciences.\n\nIn environmental science and particularly in chemistry and microbiology, a parameter is used to describe a discrete chemical or microbiological entity that can be assigned a value: commonly a concentration, but may also be a logical entity (present or absent), a statistical result such as a 95%ile value or in some cases a subjective value.\n\nWithin linguistics, the word \"parameter\" is almost exclusively used to denote a binary switch in a Universal Grammar within a Principles and Parameters framework.\n\nIn logic, the parameters passed to (or operated on by) an \"open predicate\" are called \"parameters\" by some authors (e.g., Prawitz, \"Natural Deduction\"; Paulson, \"Designing a theorem prover\"). Parameters locally defined within the predicate are called \"variables\". This extra distinction pays off when defining substitution (without this distinction special provision must be made to avoid variable capture). Others (maybe most) just call parameters passed to (or operated on by) an open predicate \"variables\", and when defining substitution have to distinguish between \"free variables\" and \"bound variables\".\n\nIn music theory, a parameter denotes an element which may be manipulated (composed), separately from the other elements. The term is used particularly for pitch, loudness, duration, and timbre, though theorists or composers have sometimes considered other musical aspects as parameters. The term is particularly used in serial music, where each parameter may follow some specified series. Paul Lansky and George Perle criticized the extension of the word \"parameter\" to this sense, since it is not closely related to its mathematical sense, but it remains common. The term is also common in music production, as the functions of audio processing units (such as the attack, release, ratio, threshold, and other variables on a compressor) are defined by parameters specific to the type of unit (compressor, equalizer, delay, etc.).\n\n"}
{"id": "48208082", "url": "https://en.wikipedia.org/wiki?curid=48208082", "title": "Philippa Gardner", "text": "Philippa Gardner\n\nPhilippa Anne Gardner (born 29 July 1965) is a British computer scientist and academic. She has been Professor of Theoretical Computer Science at Imperial College London since 2009. She was Director of the Research Institute in Automated Program Analysis and Verification between 2013 and 2016.\n\nGardner was born on 29 July 1965 in Exeter, Devon, England. In 1988, she got her M.Sc. degree in logic and computation from Bristol University, supervised by John Shepherdson. Her doctoral studies were supervised by Gordon Plotkin at the University of Edinburgh; she was awarded her Doctor of Philosophy (PhD) degree in 1992. Her doctoral thesis was titled \"Representing Logics in Type Theory\".\n\nAfter being awarded an EPSRC Advanced Fellowship at Cambridge University with Robin Milner, Gardner held a BP Research Fellowship with The Royal Society of Edinburgh between 1994-1996. She took a lectureship with Imperial College London in 2001. She was appointed Professor of Theoretical Computer Science in 2009.\n\nHer current research looks at program verification. Gardner's role with the Research Institute in Automated Program Analysis and Verification is funded by GCHQ and the Engineering and Physical Sciences Research Council (EPSRC).\n\nGardner was on the Newton International Fellowships Committee: Physical Sciences, for The Royal Society, from 2010 to 2012.\n\nGardner was awarded the President & Rector's Award for Excellence in Teaching at Imperial College London in 2013.\n"}
{"id": "2739252", "url": "https://en.wikipedia.org/wiki?curid=2739252", "title": "Primitive permutation group", "text": "Primitive permutation group\n\nIn mathematics, a permutation group \"G\" acting on a non-empty set \"X\" is called primitive if \"G\" acts transitively on \"X\" and \"G\" preserves no nontrivial partition of \"X\", where nontrivial partition means a partition that isn't a partition into singleton sets or partition into one set \"X\". Otherwise, if \"G\" is transitive and \"G\" does preserve a nontrivial partition, \"G\" is called imprimitive. \n\nWhile primitive permutation groups are transitive by definition, not all transitive permutation groups are primitive. The requirement that a primitive group be transitive is necessary only when \"X\" is a 2-element set and the action is trivial; otherwise, the condition that \"G\" preserves no nontrivial partition implies that \"G\" is transitive. This is because for non-transitive actions either the orbits of \"G\" form a nontrivial partition preserved by \"G\", or the group action is trivial, in which case any nontrivial partition of \"X\" (which exists for |\"X\"|≥\"3\") is preserved by \"G\".\n\nThis terminology was introduced by Évariste Galois in his last letter, in which he used the French term \"équation primitive\" for an equation whose Galois group is primitive.\n\nIn the same letter he stated also the following theorem.\n\nIf \"G\" is a primitive solvable group acting on a finite set \"X\", then the order of \"X\" is a power of a prime number \"p\", \"X\" may be identified with an affine space over the finite field with \"p\" elements and \"G\" acts on \"X\" as a subgroup of the affine group.\n\nAn imprimitive permutation group is an example of an induced representation; examples include coset representations \"G\"/\"H\" in cases where \"H\" is not a maximal subgroup. When \"H\" is maximal, the coset representation is primitive. \n\nIf the set \"X\" is finite, its cardinality is called the \"degree\" of \"G\". \nThe numbers of primitive groups of small degree were stated by Robert Carmichael in 1937:\n\nNote the large number of primitive groups of degree 16. As Carmichael notes, all of these groups, except for the symmetric and alternating group, are subgroups of the affine group on the 4-dimensional space over the 2-element finite field.\n\nBoth formula_1 and the group generated by formula_5 are primitive.\nThe group generated by formula_9 is not primitive, since the partition formula_10 where formula_11 and formula_12 is preserved under formula_9, i.e. formula_14 and formula_15.\n\n\n"}
{"id": "42676762", "url": "https://en.wikipedia.org/wiki?curid=42676762", "title": "Quantum algorithm for linear systems of equations", "text": "Quantum algorithm for linear systems of equations\n\nThe quantum algorithm for linear systems of equations, designed by Aram Harrow, Avinatan Hassidim, and Seth Lloyd, is a quantum algorithm formulated in 2009 for solving linear systems. The algorithm estimates the result of a scalar measurement on the solution vector to a given linear system of equations.\n\nThe algorithm is one of the main fundamental algorithms expected to provide a speedup over their classical counterparts, along with Shor's factoring algorithm, Grover's search algorithm and quantum simulation. Provided the linear system is a sparse and has a low condition number formula_1, and that the user is interested in the result of a scalar measurement on the solution vector, instead of the values of the solution vector itself, then the algorithm has a runtime of formula_2, where formula_3 is the number of variables in the linear system. This offers an exponential speedup over the fastest classical algorithm, which runs in formula_4 (or formula_5 for positive semidefinite matrices).\n\nAn implementation of the quantum algorithm for linear systems of equations was first demonstrated in 2013 by Cai et al., Barz et al. and Pan et al. in parallel. The demonstrations consisted of simple linear equations on specially designed quantum devices. The first demonstration of a general-purpose version of the algorithm appeared in 2018 in the work of Zhao et al.\n\nDue to the prevalence of linear systems in virtually all areas of science and engineering, the quantum algorithm for linear systems of equations has the potential for widespread applicability.\n\nThe problem we are trying to solve is: given a formula_6 Hermitian matrix formula_7 and a unit vector formula_8, find the solution vector formula_9 satisfying formula_10. This algorithm assumes that the user is not interested in the values of formula_9 itself, but rather the result of applying some operator formula_12 onto x, formula_13.\n\nFirst, the algorithm represents the vector formula_8 as a quantum state of the form:\n\nNext, Hamiltonian simulation techniques are used to apply the unitary operator formula_16 to formula_17 for a superposition of different times formula_18. The ability to decompose formula_17 into the eigenbasis of formula_7 and to find the corresponding eigenvalues formula_21 is facilitated by the use of quantum phase estimation.\n\nThe state of the system after this decomposition is approximately:\n\nwhere formula_23 is the eigenvector basis of formula_7, and formula_25.\n\nWe would then like to perform the linear map taking formula_26 to formula_27, where formula_28 is a normalizing constant. The linear mapping operation is not unitary and thus will require a number of repetitions as it has some probability of failing. After it succeeds, we uncompute the formula_26 register and are left with a state proportional to:\n\nWhere formula_31 is a quantum-mechanical representation of the desired solution vector \"x\". To read out all components of \"x\" would require the procedure be repeated at least \"N\" times. However, it is often the case that one is not interested in formula_32 itself, but rather some expectation value of a linear operator \"M\" acting on \"x\". By mapping \"M\" to a quantum-mechanical operator and performing the quantum measurement corresponding to \"M\", we obtain an estimate of the expectation value formula_13. This allows for a wide variety of features of the vector \"x\" to be extracted including normalization, weights in different parts of the state space, and moments without actually computing all the values of the solution vector \"x\".\n\nFirstly, the algorithm requires that the matrix formula_7 be Hermitian so that it can be converted into a unitary operator. In the case where formula_7 is not Hermitian, define \n\nAs formula_28 is Hermitian, the algorithm can now be used to solve formula_38 to obtain formula_39.\n\nSecondly, The algorithm requires an efficient procedure to prepare formula_17, the quantum representation of b. It is assumed that there exists some linear operator formula_41 that can take some arbitrary quantum state formula_42 to formula_17 efficiently or that this algorithm is a subroutine in a larger algorithm and is given formula_17 as input. Any error in the preparation of state formula_17 is ignored.\n\nFinally, the algorithm assumes that the state formula_46 can be prepared efficiently. Where\n\nfor some large formula_48. The coefficients of formula_46 are chosen to minimize a certain quadratic loss function which induces error in the formula_50 subroutine described below.\n\nHamiltonian simulation is used to transform the Hermitian matrix formula_7 into a unitary operator, which can then be applied at will. This is possible if \"A\" is \"s\"-sparse and efficiently row computable, meaning it has at most \"s\" nonzero entries per row and given a row index these entries can be computed in time O(\"s\"). Under these assumptions, quantum Hamiltonian simulation allows formula_16 to be simulated in time formula_53.\n\nThe key subroutine to the algorithm, denoted formula_50, is defined as follows and incorporates a phase estimation subroutine:\n\n1. Prepare formula_56 on register \"C\"\n\n2. Apply the conditional Hamiltonian evolution (sum)\n\n3. Apply the Fourier transform to the register \"C\". Denote the resulting basis states with formula_57 for \"k\" = 0, ..., \"T\" − 1. Define formula_58.\n\n4. Adjoin a three-dimensional register \"S\" in the state\n\n5. Reverse steps 1–3, uncomputing any garbage produced along the way.\n\nThe phase estimation procedure in steps 1-3 allows for the estimation of eigenvalues of \"A\" up to error formula_60.\n\nThe ancilla register in step 4 is necessary to construct a final state with inverted eigenvalues corresponding to the diagonalized inverse of \"A\". In this register, the functions \"f\", \"g\", are called filter functions. The states 'nothing', 'well' and 'ill' are used to instruct the loop body on how to proceed; 'nothing' indicates that the desired matrix inversion has not yet taken place, 'well' indicates that the inversion has taken place and the loop should halt, and 'ill' indicates that part of formula_17 is in the ill-conditioned subspace of \"A\" and the algorithm will not be able to produce the desired inversion. Producing a state proportional to the inverse of \"A\" requires 'well' to be measured, after which the overall state of the system collapses to the desired state by the extended Born rule.\n\nThe body of the algorithm follows the amplitude amplification procedure: starting with formula_62, the following operation is repeatedly applied:\n\nwhere\n\nand\n\nAfter each repetition, formula_66 is measured and will produce a value of 'nothing', 'well', or 'ill' as described above. This loop is repeated until formula_66 is measured, which occurs with a probability formula_68. Rather than repeating formula_69 times to minimize error, amplitude amplification is used to achieve the same error resilience using only formula_70 repetitions.\n\nAfter successfully measuring 'well' on formula_66 the system will be in a state proportional to:\n\nFinally, we perform the quantum-mechanical operator corresponding to M and obtain an estimate of the value of formula_13.\n\nThe best classical algorithm which produces the actual solution vector formula_9 is Gaussian elimination, which runs in formula_75 time.\n\nIf \"A\" is \"s\"-sparse and positive semi-definite, then the Conjugate Gradient method can be used to find the solution vector formula_9 can be found in formula_77 time by minimizing the quadratic function formula_78.\n\nWhen only a summary statistic of the solution vector formula_9 is needed, as is the case for the quantum algorithm for linear systems of equations, a classical computer can find an estimate of formula_80 in formula_5.\n\nThe quantum algorithm for solving linear systems of equations originally proposed by Harrow et al. was shown to be formula_82. The runtime of this algorithm was subsequently improved to formula_83 by Andris Ambainis. Since the HHL algorithm maintains its logarithmic scaling only for sparse or low rank matrices, Wossnig et al. extended the HHL algorithm based on a quantum singular value estimation technique and provide a linear system algorithm for dense matrices which runs in formula_84 time compared to the formula_85 of the standard HHL algorithm.\n\nAn important factor in the performance of the matrix inversion algorithm is the condition number of formula_7 formula_1, which represents the ratio of formula_7's largest and smallest eigenvalues. As the condition number increases, the ease with which the solution vector can be found using gradient descent methods such as the conjugate gradient method decreases, as formula_7 becomes closer to a matrix which cannot be inverted and the solution vector becomes less stable. This algorithm assumes that all elements of the matrix formula_7 lie between formula_91 and 1, in which case the claimed run-time proportional to formula_92 will be achieved. Therefore, the speedup over classical algorithms is increased further when formula_1 is a formula_94.\n\nIf the run-time of the algorithm were made poly-logarithmic in formula_1 then problems solvable on \"n\" qubits could be solved in poly(\"n\") time, causing the complexity class BQP to be equal to PSPACE.\n\nPerforming the Hamiltonian simulation, which is the dominant source of error, is done by simulating formula_16. Assuming that formula_7 is s-sparse, this can be done with an error bounded by a constant formula_98, which will translate to the additive error achieved in the output state formula_31.\n\nThe phase estimation step errs by formula_100 in estimating formula_101, which translates into a relative error of formula_102 in formula_103. If formula_104, taking formula_105 induces a final error of formula_98. This requires that the overall run-time efficiency be increased proportional to formula_107 to minimize error.\n\nWhile there does not yet exist a quantum computer that can truly offer a speedup over a classical computer, implementation of a \"proof of concept\" remains an important milestone in the development of a new quantum algorithm. Demonstrating the quantum algorithm for linear systems of equations remained a challenge for years after its proposal until 2013 when it was demonstrated by Cai et al., Barz et al. and Pan et al. in parallel.\n\nPublished in Physical Review Letters 110, 230501 (2013), Cai et al. reported an experimental demonstration of the simplest meaningful instance of this algorithm, that is, solving 2*2 linear equations for various input vectors. The quantum circuit is optimized and compiled into a linear optical network with four photonic quantum bits (qubits) and four controlled logic gates, which is used to coherently implement every subroutine for this algorithm. For various input vectors, the quantum computer gives solutions for the linear equations with reasonably high precision, ranging from fidelities of 0.825 to 0.993.\n\nOn February 5, 2013, Barz et al. demonstrated the quantum algorithm for linear systems of equations on a photonic quantum computing architecture. This implementation used two consecutive entangling gates on the same pair of polarization-encoded qubits. Two separately controlled NOT gates were realized where the successful operation of the first was heralded by a measurement of two ancillary photons. Barz et al. found that the fidelity in the obtained output state ranged from 64.7% to 98.1% due to the influence of higher-order emissions from spontaneous parametric down-conversion.\n\nOn February 8, 2013 Pan et al. reported a proof-of-concept experimental demonstration of the quantum algorithm using a 4-qubit nuclear magnetic resonance quantum information processor. The implementation was tested using simple linear systems of only 2 variables. Across three experiments they obtain the solution vector with over 96% fidelity.\n\nQuantum computers are devices that harness quantum mechanics to perform computations in ways that classical computers cannot. For certain problems, quantum algorithms supply exponential speedups over their classical counterparts, the most famous example being Shor's factoring algorithm. Few such exponential speedups are known, and those that are (such as the use of quantum computers to simulate other quantum systems) have so far found limited use outside the domain of quantum mechanics. This algorithm provides an exponentially faster method of estimating features of the solution of a set of linear equations, which is a problem ubiquitous in science and engineering, both on its own and as a subroutine in more complex problems.\n\nClader et al. provided an preconditioned version of the linear systems algorithm that provided two advances. First, they demonstrated how a preconditioner could be included within the quantum algorithm. This expands the class of problems that can achieve the promised exponential speedup, since the scaling of HHL and the best classical algorithms are both polynomial in the condition number. The second advance was the demonstration of how to use HHL to solve for the radar cross-section of a complex shape. This was one of the first end to end examples of how to use HHL to solve a concrete problem exponentially faster than the best known classical algorithm. \n\nDominic Berry proposed a new algorithm for solving linear time dependent differential equations as an extension of the quantum algorithm for solving linear systems of equations. Berry provides an efficient algorithm for solving the full-time evolution under sparse linear differential equations on a quantum computer.\n\nWiebe et al. provide a new quantum algorithm to determine the quality of a least-squares fit in which a continuous function is used to approximate a set of discrete points by extending the quantum algorithm for linear systems of equations. As the amount of discrete points increases, the time required to produce a least-squares fit using even a quantum computer running a quantum state tomography algorithm becomes very large. Wiebe et al. find that in many cases, their algorithm can efficiently find a concise approximation of the data points, eliminating the need for the higher-complexity tomography algorithm.\n\nMachine learning is the study of systems that can identify trends in data. Tasks in machine learning frequently involve manipulating and classifying a large volume of data in high-dimensional vector spaces. The runtime of classical machine learning algorithms is limited by a polynomial dependence on both the volume of data and the dimensions of the space. Quantum computers are capable of manipulating high-dimensional vectors using tensor product spaces are thus the perfect platform for machine learning algorithms.\n\nThe quantum algorithm for linear systems of equations has been applied to a support vector machine, which is an optimized linear or non-linear binary classifier. A support vector machine can be used for supervised machine learning, in which training set of already classified data is available, or unsupervised machine learning, in which all data given to the system is unclassified. Rebentrost et al. show that a quantum support vector machine can be used for big data classification and achieve an exponential speedup over classical computers.\n\nOn June 2018, Zhao et al. developed an algorithm for performing Bayesian training of deep neural networks in quantum computers with an exponential speedup over classical training due to the use of the quantum algorithm for linear systems of equations, providing also the first general-purpose implementation of the algorithm to be run in cloud-based quantum computers.\n"}
{"id": "4303385", "url": "https://en.wikipedia.org/wiki?curid=4303385", "title": "Reduction strategy (code optimization)", "text": "Reduction strategy (code optimization)\n\nIn code optimization during the translation of computer programs into an executable form, and in mathematical reduction generally, a reduction strategy for a term rewriting system determines which reducible subterms (or reducible expressions, redexes) should be reduced (\"contracted\") within a term; it may be the case that a term may contain multiple redexes which are disjoint from one another and that choosing to contract one redex before another may have no influence on the resulting reduced form of the term, or that the redexes in a term do overlap and that choosing to contract one of the overlapping redexes over the other may result in a different reduced form of the term. It is the choice of which redex at each step in the reduction to contract that determines the strategy chosen. This can be seen as a practical application of the theoretical notion of reduction strategy in lambda calculus.\n\n"}
{"id": "39539859", "url": "https://en.wikipedia.org/wiki?curid=39539859", "title": "Richard Arratia", "text": "Richard Arratia\n\nRichard Alejandro Arratia is a mathematician noted for his work in combinatorics and probability theory.\n\nArratia developed the ideas of interlace polynomials with Béla Bollobás and Gregory Sorkin, found an equivalent formulation of the Stanley–Wilf conjecture as the convergence of a limit, and was the first to investigate the lengths of superpatterns of permutations.\n\nHe has also written highly cited papers on the Chen–Stein method on distances between probability distributions,\n\nHe is a coauthor of the book \"Logarithmic Combinatorial Structures: A Probabilistic Approach\".\n\nArratia earned his Ph.D. in 1979 from the University of Wisconsin–Madison under the supervision of David Griffeath. He is currently a professor of mathematics at the University of Southern California.\n\n"}
{"id": "37920220", "url": "https://en.wikipedia.org/wiki?curid=37920220", "title": "Stochastic Petri net", "text": "Stochastic Petri net\n\nStochastic Petri nets are a form of Petri net where the transitions fire after a probabilistic delay determined by a random variable. \n\nA \"stochastic Petri net\" is a five-tuple \"SPN\" = (\"P\", \"T\", \"F\", \"M\", \"Λ\") where:\n\nThe reachability graph of stochastic Petri nets can be mapped directly to a Markov process. It satisfies the Markov property, since its states depend only on the current marking. \nEach state in the reachability graph is mapped to a state in the Markov process, and the firing of a transition with firing rate λ corresponds to a Markov state transition with probability λ.\n\n\n"}
{"id": "841685", "url": "https://en.wikipedia.org/wiki?curid=841685", "title": "Tractrix", "text": "Tractrix\n\nA tractrix (from the Latin verb \"trahere\" \"pull, drag\"; plural: tractrices) is the curve along which an object moves, under the influence of friction, when pulled on a horizontal plane by a line segment attached to a tractor (pulling) point that moves at a right angle to the initial line between the object and the puller at an infinitesimal speed. It is therefore a curve of pursuit. It was first introduced by Claude Perrault in 1670, and later studied by Isaac Newton (1676) and Christiaan Huygens (1692).\n\nSuppose the object is placed at (or in the example shown at right), and the puller in the origin, so is the length of the pulling thread (4 in the example at right). Then the puller starts to move along the axis in the positive direction. At every moment, the thread will be tangent to the curve described by the object, so that it becomes completely determined by the movement of the puller. Mathematically, the movement will be described then by the differential equation\nwith the initial condition whose solution is\n\nThe first term of this solution can also be written \nwhere is the inverse hyperbolic secant function.\n\nThe negative branch denotes the case where the puller moves in the negative direction from the origin. Both branches belong to the tractrix, meeting at the cusp point .\n\nThe essential property of the tractrix is constancy of the distance between a point on the curve and the intersection of the tangent line at with the asymptote of the curve.\nThe tractrix might be regarded in a multitude of ways:\n\nThe function admits a horizontal asymptote. The curve is symmetrical with respect to the -axis. The curvature radius is .\n\nA great implication that the tractrix had was the study of the revolution surface of it around its asymptote: the pseudosphere. Studied by Eugenio Beltrami in 1868, as a surface of constant negative Gaussian curvature, the pseudosphere is a local model of non-Euclidean geometry. The idea was carried further by Kasner and Newman in their book \"Mathematics and the Imagination\", where they show a toy train dragging a pocket watch to generate the tractrix.\n\n\nIn 1927, P. G. A. H. Voigt patented a horn loudspeaker design based on the assumption that a wave front traveling through the horn is spherical of a constant radius. The idea is to minimize distortion caused by internal reflection of sound within the horn. The resulting shape is the surface of revolution of a tractrix.An important application is in the forming technology for sheet metal. In particular a tractrix profile is used for the corner of the die on which the sheet metal is bent during deep drawing.\n\nA toothed belt-pulley design provides improved efficiency for mechanical power transmission using a tractix catenary shape for its teeth. This shape minimizes the friction of the belt teeth engaging the pulley, because the moving teeth engage and disengage with minimal sliding contact. Original timing belt designs used simpler trapezoidal or circular tooth shapes, which cause significant sliding and friction.\n\nA history of all these machines can be seen in an article by H. J. M. Bos\n\n\n\n"}
{"id": "2462396", "url": "https://en.wikipedia.org/wiki?curid=2462396", "title": "Transitive set", "text": "Transitive set\n\nIn set theory, a set \"A\" is called transitive if either of the following equivalent conditions hold:\nSimilarly, a class \"M\" is transitive if every element of \"M\" is a subset of \"M\".\n\nUsing the definition of ordinal numbers suggested by John von Neumann, ordinal numbers are defined as hereditarily transitive sets: an ordinal number is a transitive set whose members are also transitive (and thus ordinals). The class of all ordinals is a transitive class.\n\nAny of the stages \"V\" and \"L\" leading to the construction of the von Neumann universe \"V\" and Gödel's constructible universe \"L\" are transitive sets. The universes \"L\" and \"V\" themselves are transitive classes.\n\nThis is a complete list of all finite transitive sets with up to 20 brackets:\n\nA set \"X\" is transitive if and only if formula_48, where formula_49 is the union of all elements of \"X\" that are sets, formula_50. If \"X\" is transitive, then formula_49 is transitive. If \"X\" and \"Y\" are transitive, then \"X\"∪\"Y\"∪{\"X\",\"Y\"} is transitive. In general, if \"X\" is a class all of whose elements are transitive sets, then formula_52 is transitive.\n\nA set \"X\" which does not contain urelements is transitive if and only if it is a subset of its own power set, formula_53 The power set of a transitive set without urelements is transitive.\n\nThe transitive closure of a set \"X\" is the smallest (with respect to inclusion) transitive set which contains \"X\". Suppose one is given a set \"X\", then the transitive closure of \"X\" is\n\nProof. Denote formula_55 and formula_56. Then we claim that the set \n\nis transitive, and whenever formula_58 is a transitive set containing formula_59 then formula_60.\n\nAssume formula_61. Then formula_62 for some formula_63 and so formula_64. Since formula_65, formula_66. Thus formula_67 is transitive.\n\nNow let formula_58 be as above. We prove by induction that formula_69 thus proving that formula_60. Since formula_71 we have a base. Now assume formula_72. Then formula_73. But formula_58 is transitive so formula_75 whence formula_76. Done.\n\nNote that this is the set of all of the objects related to \"X\" by the transitive closure of the membership relation.\n\nTransitive classes are often used for construction of interpretations of set theory in itself, usually called inner models. The reason is that properties defined by bounded formulas are absolute for transitive classes.\n\nA transitive set (or class) that is a model of a formal system of set theory is called a transitive model of the system (provided that the element relation of the model is the restriction of the true element relation to the universe of the model). Transitivity is an important factor in determining the absoluteness of formulas.\n\nIn the superstructure approach to non-standard analysis, the non-standard universes satisfy strong transitivity.\n\n\n\n"}
{"id": "51352780", "url": "https://en.wikipedia.org/wiki?curid=51352780", "title": "Universal vertex", "text": "Universal vertex\n\nIn graph theory, a universal vertex is a vertex of an undirected graph that is adjacent to all other vertices of the graph. It may also be called a dominating vertex, as it forms a one-element dominating set in the graph.\n\nA graph that contains a universal vertex may be called a cone. In this context, the universal vertex may also be called the apex of the cone. However, this terminology conflicts with the terminology of apex graphs, in which an apex is a vertex whose removal leaves a planar subgraph.\n\nThe stars are exactly the trees that have a universal vertex, and may be constructed by adding a universal vertex to an independent set. The wheel graphs, similarly, may be formed by adding a universal vertex to a cycle graph. In geometry, the three-dimensional pyramids have wheel graphs as their skeletons, and more generally the graph of any higher-dimensional pyramid has a universal vertex as the apex of the pyramid.\n\nThe trivially perfect graphs (the comparability graphs of order-theoretic trees) always contain a universal vertex, the root of the tree, and more strongly they may be characterized as the graphs in which every connected induced subgraph contains a universal vertex.\nThe connected threshold graphs form a subclass of the trivially perfect graphs, so they also contain a universal vertex; they may be defined as the graphs that can be formed by repeated addition of either a universal vertex or an isolated vertex (one with no incident edges).\n\nEvery graph with a universal vertex is a dismantlable graph, and almost all dismantlable graphs have a universal vertex.\n\nIn a graph with vertices, a universal vertex is a vertex whose degree is exactly . Therefore, like the split graphs, graphs with a universal vertex can be recognized purely by their degree sequences, without looking at the structure of the graph.\n"}
{"id": "753225", "url": "https://en.wikipedia.org/wiki?curid=753225", "title": "Uses of trigonometry", "text": "Uses of trigonometry\n\nAmongst the lay public of non-mathematicians and non-scientists, trigonometry is known chiefly for its application to measurement problems, yet is also often used in ways that are far more subtle, such as its place in the theory of music; still other uses are more technical, such as in number theory. The mathematical topics of Fourier series and Fourier transforms rely heavily on knowledge of trigonometric functions and find application in a number of areas, including statistics.\n\nIn Chapter XI of The Age of Reason, the American revolutionary and Enlightenment thinker Thomas Paine wrote: \n\nFor the 25 years preceding the invention of the logarithm in 1614, prosthaphaeresis was the only known generally applicable way of approximating products quickly. It used the identities for the trigonometric functions of sums and differences of angles in terms of the products of trigonometric functions of those angles.\n\nScientific fields that make use of trigonometry include:\n\nThat these fields involve trigonometry does not mean knowledge of trigonometry is needed in order to learn anything about them. It \"does\" mean that \"some\" things in these fields cannot be understood without trigonometry. For example, a professor of music may perhaps know nothing of mathematics, but would probably know that Pythagoras was the earliest known contributor to the mathematical theory of music.\n\nIn \"some\" of the fields of endeavor listed above it is easy to imagine how trigonometry could be used. For example, in navigation and land surveying, the occasions for the use of trigonometry are in at least some cases simple enough that they can be described in a beginning trigonometry textbook. In the case of music theory, the application of trigonometry is related to work begun by Pythagoras, who observed that the sounds made by plucking two strings of different lengths are consonant if both lengths are small integer multiples of a common length. The resemblance between the shape of a vibrating string and the graph of the sine function is no mere coincidence. In oceanography, the resemblance between the shapes of some waves and the graph of the sine function is also not coincidental. In some other fields, among them climatology, biology, and economics, there are seasonal periodicities. The study of these often involves the periodic nature of the sine and cosine function.\n\nMany fields make use of trigonometry in more advanced ways than can be discussed in a single article. Often those involve what are called Fourier series, after the 18th- and 19th-century French mathematician and physicist Joseph Fourier. Fourier series have a surprisingly diverse array of applications in many scientific fields, in particular in all of the phenomena involving seasonal periodicities mentioned above, and in wave motion, and hence in the study of radiation, of acoustics, of seismology, of modulation of radio waves in electronics, and of electric power engineering.\n\nA Fourier series is a sum of this form:\n\nwhere each of the squares (formula_2) is a different number, and one is adding infinitely many terms. Fourier used these for studying heat flow and diffusion (diffusion is the process whereby, when you drop a sugar cube into a gallon of water, the sugar gradually spreads through the water, or a pollutant spreads through the air, or any dissolved substance spreads through any fluid).\n\nFourier series are also applicable to subjects whose connection with wave motion is far from obvious. One ubiquitous example is digital compression whereby images, audio and video data are compressed into a much smaller size which makes their transmission feasible over telephone, internet and broadcast networks. Another example, mentioned above, is diffusion. Among others are: the geometry of numbers, isoperimetric problems, recurrence of random walks, quadratic reciprocity, the central limit theorem, Heisenberg's inequality.\n\nA more abstract concept than Fourier series is the idea of Fourier transform. Fourier transforms involve integrals rather than sums, and are used in a similarly diverse array of scientific fields. Many natural laws are expressed by relating \"rates of change\" of quantities to the quantities themselves. For example: The rate of change of population is sometimes jointly proportional to (1) the present population and (2) the amount by which the present population falls short of the carrying capacity. This kind of relationship is called a differential equation. If, given this information, one tries to express population as a function of time, one is trying to \"solve\" the differential equation. Fourier transforms may be used to convert some differential equations to algebraic equations for which methods of solving them are known. Fourier transforms have many uses. In almost any scientific context in which the words spectrum, harmonic, or resonance are encountered, Fourier transforms or Fourier series are nearby.\n\nIntelligence quotients are sometimes held to be distributed according to the bell-shaped curve. About 40% of the area under the curve is in the interval from 100 to 120; correspondingly, about 40% of the population scores between 100 and 120 on IQ tests. Nearly 9% of the area under the curve is in the interval from 120 to 140; correspondingly, about 9% of the population scores between 120 and 140 on IQ tests, etc. Similarly many other things are distributed according to the \"bell-shaped curve\", including measurement errors in many physical measurements. Why the ubiquity of the \"bell-shaped curve\"? There is a theoretical reason for this, and it involves Fourier transforms and hence trigonometric functions. That is one of a variety of applications of Fourier transforms to statistics.\n\nTrigonometric functions are also applied when statisticians study seasonal periodicities, which are often represented by Fourier series.\n\nThere is a hint of a connection between trigonometry and number theory. Loosely speaking, one could say that number theory deals with qualitative properties rather than quantitative properties of numbers.\n\nDiscard the ones that are not in lowest terms; keep only those that are in lowest terms:\n\nThen bring in trigonometry:\n\nThe value of the sum is −1, because 42 has an \"odd\" number of prime factors and none of them is repeated: 42 = 2 × 3 × 7. (If there had been an \"even\" number of non-repeated factors then the sum would have been 1; if there had been any repeated prime factors (e.g., 60 = 2 × 2 × 3 × 5) then the sum would have been 0; the sum is the Möbius function evaluated at 42.) This hints at the possibility of applying Fourier analysis to number theory.\n\nVarious types of equations can be solved using trigonometry.\n\nFor example, a linear difference equation or linear differential equation with constant coefficients has solutions expressed in terms of the eigenvalues of its characteristic equation; if some of the eigenvalues are complex, the complex terms can be replaced by trigonometric functions of real terms, showing that the dynamic variable exhibits oscillations.\n\nSimilarly, cubic equations with three real solutions have an algebraic solution that is unhelpful in that it contains cube roots of complex numbers; again an alternative solution exists in terms of trigonometric functions of real terms.\n"}
{"id": "17624207", "url": "https://en.wikipedia.org/wiki?curid=17624207", "title": "Worker's compensation (Germany)", "text": "Worker's compensation (Germany)\n\nWorker's compensation in Germany is a national, compulsory program that insures workers for injuries or illness incurred through their employment, or the commute to or from their employment. Wage earners, apprentices, family helpers and students including children in kindergarten are covered by this program. Almost all self-employed persons can voluntarily become insured. The German worker's compensation laws were the first of their kind.\n\nThe Sickness Bill became law in 1883 and the Accident Bill in 1884. Otto Von Bismarck, Chancellor of the German Empire, introduced the programs to assist workers in the event of accidental injury, illness or old age. This initial system was financed by workers and employers.\nThe Sickness Insurance law paid indemnity for up to 13 weeks. The first 4 weeks were at 50% of prior wages, from the fifth week on the benefit was 66.7% of previous earnings. Workers who were completely disabled received benefits at 67% after the 13 week, financed entirely by employers. If the disabled person required constant care, then up to 100% of previous wages were awarded.\n\nThe German compensation system was used as a model for many other nation's worker's compensation programs.\n\nToday, in Germany, every worker is a member of a related Workers Compensation Institute (\"Berufsgenossenschaft\") and almost all self-employed persons can voluntarily become insured members of an institute as well. The institutes have an approximately 90% return-to-work rate, using vocational retraining and upgraded vocational qualifications as key strategies.\n\nAll accidents in the workplace - or in the commute to and from it - are covered. 67 diseases are considered occupational diseases and are also covered by the program.\n\nThe workers' compensation program is funded by employers (except for the government's coverage\nfor students and children and a government subsidy to the Agricultural Accident Fund). The average employer contribution (1996) was 1.42% of payroll.\n\nAn injured worker has a right to appeal to the committee of their Institute. The next level of appeal after this committee is to a \"Sozialgericht\" court. The institute must cover the costs of the appeal process, so there are no costs to the worker.\n\nDisability benefits are paid as a weekly \"wage loss” compensation. Workers unable to perform their current job due to injury or illness receive periodic payments of 80% of their prior gross earnings until returning to work (up to a maximum total payment). If rehabilitation is prognosticated to be impossible the worker receives the benefit for 78 weeks.\n\nWages are paid for six weeks by the employer before the employee goes onto short-term disability benefits.\n\nWorkers who have a loss of earning capacity due to work injury or occupational disease of 20% or more receive a pension equal to 66.7% of their previous year's earnings, up to the specified maximum. This is paid until the age of 65, unless he or she begins receiving old-age pension earlier than age 65.\n\nMedical care benefits are comprehensive, with the total cost of physical rehabilitation and appliances being covered. Institutes provide all medical care benefits. The Institutes control the choice of doctor and hospital.\n"}
