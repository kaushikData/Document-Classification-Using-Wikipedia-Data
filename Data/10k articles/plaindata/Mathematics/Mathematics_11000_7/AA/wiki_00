{"id": "40414931", "url": "https://en.wikipedia.org/wiki?curid=40414931", "title": "3D mirror symmetry", "text": "3D mirror symmetry\n\nIn theoretical physics, 3D mirror symmetry is a version of mirror symmetry in 3-dimensional gauge theories with N=4 supersymmetry, or 8 supercharges. It was first proposed by Kenneth Intriligator and Nathan Seiberg in their 1996 paper, Mirror symmetry in three-dimensional gauge theories, as a relation between pairs of 3-dimensional gauge theories, such that the Coulomb branch of the moduli space of one is the Higgs branch of the moduli space of the other. It was demonstrated using D-brane cartoons by Amihay Hanany and Edward Witten 4 months later, where they found that it is a consequence of S-duality in type IIB string theory.\n\nFour months later 3D mirror symmetry was extended to N=2 gauge theories resulting from supersymmetry breaking in N=4 theories. Here it was given a physical interpretation in terms of vortices. In 3-dimensional gauge theories, vortices are particles. BPS vortices, which are those vortices that preserve some supersymmetry, have masses which are given by the FI term of the gauge theory. In particular, on the Higgs branch, where the squarks are massless and condense yielding nontrivial vacuum expectation values (VEVs), the vortices are massive. On the other hand, Intriligator and Seiberg interpret the Coulomb branch of the gauge theory, where the scalar in the vector multiplet has a VEV, as being the regime where massless vortices condense. Thus the duality between the Coulomb branch in one theory and the Higgs branch in the dual theory is the duality between squarks and vortices.\n\nIn this theory, the instantons are Hooft–Polyakov magnetic monopoles whose actions are proportional to the VEV of the scalar in the vector multiplet. In this case, instanton calculations again reproduce the nonperturbative super potential. In particular, in the N=4 case with SU(2) gauge symmetry, the metric on the moduli space was found by Nathan Seiberg and Edward Witten using holomorphy and supersymmetric nonrenormalization theorems several days before Intriligator and Seiberg's 3-dimensional mirror symmetry paper appeared. Their results were reproduced using standard instanton techniques.\n"}
{"id": "3670685", "url": "https://en.wikipedia.org/wiki?curid=3670685", "title": "Ageometresia", "text": "Ageometresia\n\nAgeometresia or ageometria is a word describing a defect in a work of geometry.\n\nAn early usage of the word was in writings of François Viète on Copernicus. As another instance, Johannes Kepler, having no direct and geometrical method of finding certain matters in his elliptical theory, namely how to calculate the true anomaly from the mean anomaly, has been charged by others with ageometresia.\n\nAlthough Viète wrote in Latin, the word \"ageometresia\" is Greek, and the same Greek word has also subsequently been used by writers in English. As well as its usage to indicate faults in the works of professional mathematicians, \"ageometria\" has also been used to describe a form of dyscalculia, a disability that prevents students from understanding geometry.\n"}
{"id": "736", "url": "https://en.wikipedia.org/wiki?curid=736", "title": "Albert Einstein", "text": "Albert Einstein\n\nAlbert Einstein (; ; 14 March 1879 – 18 April 1955) was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics). His work is also known for its influence on the philosophy of science. He is best known to the general public for his mass–energy equivalence formula , which has been dubbed \"the world's most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory.\n\nNear the beginning of his career, Einstein thought that Newtonian mechanics was no longer enough to reconcile the laws of classical mechanics with the laws of the electromagnetic field. This led him to develop his special theory of relativity during his time at the Swiss Patent Office in Bern (1902–1909), Switzerland. However, he realized that the principle of relativity could also be extended to gravitational fields, and he published a paper on general relativity in 1916 with his theory of gravitation. He continued to deal with problems of statistical mechanics and quantum theory, which led to his explanations of particle theory and the motion of molecules. He also investigated the thermal properties of light which laid the foundation of the photon theory of light. In 1917, he applied the general theory of relativity to model the structure of the universe.\n\nExcept for one year in Prague, Einstein lived in Switzerland between 1895 and 1914, during which time he renounced his German citizenship in 1896, then received his academic diploma from the Swiss federal polytechnic school (later the Eidgenössische Technische Hochschule, ETH) in Zürich in 1900. After being stateless for more than five years, he acquired Swiss citizenship in 1901, which he kept for the rest of his life. In 1905, he was awarded a PhD by the University of Zurich. The same year, he published four groundbreaking papers during his renowned \"annus mirabilis\" (miracle year) which brought him to the notice of the academic world at the age of 26. Einstein taught theoretical physics at Zurich between 1912 and 1914 before he left for Berlin, where he was elected to the Prussian Academy of Sciences.\n\nIn 1933, while Einstein was visiting the United States, Adolf Hitler came to power. Because of his Jewish background, Einstein did not return to Germany. He settled in the United States and became an American citizen in 1940. On the eve of World War II, he endorsed a letter to President Franklin D. Roosevelt alerting him to the potential development of \"extremely powerful bombs of a new type\" and recommending that the US begin similar research. This eventually led to the Manhattan Project. Einstein supported the Allies, but he generally denounced the idea of using nuclear fission as a weapon. He signed the Russell–Einstein Manifesto with British philosopher Bertrand Russell, which highlighted the danger of nuclear weapons. He was affiliated with the Institute for Advanced Study in Princeton, New Jersey, until his death in 1955.\n\nEinstein published more than 300 scientific papers and more than 150 non-scientific works. His intellectual achievements and originality have made the word \"Einstein\" synonymous with \"genius\". Eugene Wigner wrote of Einstein in comparison to his contemporaries that \"Einstein's understanding was deeper even than Jancsi von Neumann's. His mind was both more penetrating and more original than von Neumann's. And that is a very remarkable statement.\"\n\nAlbert Einstein was born in Ulm, in the Kingdom of Württemberg in the German Empire, on 14 March 1879. His parents were Hermann Einstein, a salesman and engineer, and Pauline Koch. In 1880, the family moved to Munich, where Einstein's father and his uncle Jakob founded \"Elektrotechnische Fabrik J. Einstein & Cie\", a company that manufactured electrical equipment based on direct current.\n\nThe Einsteins were non-observant Ashkenazi Jews, and Albert attended a Catholic elementary school in Munich, from the age of 5, for three years. At the age of 8, he was transferred to the Luitpold Gymnasium (now known as the Albert Einstein Gymnasium), where he received advanced primary and secondary school education until he left the German Empire seven years later.\n\nIn 1894, Hermann and Jakob's company lost a bid to supply the city of Munich with electrical lighting because they lacked the capital to convert their equipment from the direct current (DC) standard to the more efficient alternating current (AC) standard. The loss forced the sale of the Munich factory. In search of business, the Einstein family moved to Italy, first to Milan and a few months later to Pavia. When the family moved to Pavia, Einstein, then 15, stayed in Munich to finish his studies at the Luitpold Gymnasium. His father intended for him to pursue electrical engineering, but Einstein clashed with authorities and resented the school's regimen and teaching method. He later wrote that the spirit of learning and creative thought was lost in strict rote learning. At the end of December 1894, he travelled to Italy to join his family in Pavia, convincing the school to let him go by using a doctor's note. During his time in Italy he wrote a short essay with the title \"On the Investigation of the State of the Ether in a Magnetic Field\".\n\nEinstein always excelled at math and physics from a young age, reaching a mathematical level years ahead of his peers. The twelve year old Einstein taught himself algebra and Euclidean geometry over a single summer. Einstein also independently discovered his own original proof of the Pythagorean theorem at age 12. A family tutor Max Talmud says that after he had given the 12 year old Einstein a geometry textbook, after a short time \"[Einstein] had worked through the whole book. He thereupon devoted himself to higher mathematics... Soon the flight of his mathematical genius was so high I could not follow.\" His passion for geometry and algebra led the twelve year old to become convinced that nature could be understood as a \"mathematical structure\". Einstein started teaching himself calculus at 12, and as a 14 year old he says he had \"mastered integral and differential calculus\".\n\nAt age 13, Einstein was introduced to Kant's \"Critique of Pure Reason\", and Kant became his favorite philosopher, his tutor stating: \"At the time he was still a child, only thirteen years old, yet Kant's works, incomprehensible to ordinary mortals, seemed to be clear to him.\"\n\nIn 1895, at the age of 16, Einstein took the entrance examinations for the Swiss Federal Polytechnic in Zürich (later the Eidgenössische Technische Hochschule, ETH). He failed to reach the required standard in the general part of the examination, but obtained exceptional grades in physics and mathematics. On the advice of the principal of the Polytechnic, he attended the Argovian cantonal school (gymnasium) in Aarau, Switzerland, in 1895 and 1896 to complete his secondary schooling. While lodging with the family of professor Jost Winteler, he fell in love with Winteler's daughter, Marie. Albert's sister Maja later married Winteler's son Paul. In January 1896, with his father's approval, Einstein renounced his citizenship in the German Kingdom of Württemberg to avoid military service. In September 1896, he passed the Swiss Matura with mostly good grades, including a top grade of 6 in physics and mathematical subjects, on a scale of 1–6. At 17, he enrolled in the four-year mathematics and physics teaching diploma program at the Zürich Polytechnic. Marie Winteler, who was a year older, moved to Olsberg, Switzerland, for a teaching post.\n\nEinstein's future wife, a 20-year old Serbian woman Mileva Marić, also enrolled at the Polytechnic that year. She was the only woman among the six students in the mathematics and physics section of the teaching diploma course. Over the next few years, Einstein and Marić's friendship developed into romance, and they read books together on extra-curricular physics in which Einstein was taking an increasing interest. In 1900, Einstein passed the exams in Maths and Physics and was awarded the Federal Polytechnic teaching diploma. There have been claims that Marić collaborated with Einstein on his 1905 papers, known as the \"Annus Mirabilis\" papers, but historians of physics who have studied the issue find no evidence that she made any substantive contributions.\n\nAn early correspondence between Einstein and Marić was discovered and published in 1987 which revealed that the couple had a daughter named \"Lieserl\", born in early 1902 in Novi Sad where Marić was staying with her parents. Marić returned to Switzerland without the child, whose real name and fate are unknown. The contents of Einstein's letter in September 1903 suggest that the girl was either given up for adoption or died of scarlet fever in infancy.\nEinstein and Marić married in January 1903. In May 1904, their son Hans Albert Einstein was born in Bern, Switzerland. Their son Eduard was born in Zürich in July 1910. The couple moved to Berlin in April 1914, but Marić returned to Zürich with their sons after learning that Einstein's chief romantic attraction was his first and second cousin Elsa. They divorced on 14 February 1919, having lived apart for five years. Eduard had a breakdown at about age 20 and was diagnosed with schizophrenia. His mother cared for him and he was also committed to asylums for several periods, finally being committed permanently after her death.\n\nIn letters revealed in 2015, Einstein wrote to his early love Marie Winteler about his marriage and his strong feelings for her. He wrote in 1910, while his wife was pregnant with their second child: \"I think of you in heartfelt love every spare minute and am so unhappy as only a man can be\". He spoke about a \"misguided love\" and a \"missed life\" regarding his love for Marie.\n\nEinstein married Elsa Löwenthal in 1919, after having a relationship with her since 1912. She was a first cousin maternally and a second cousin paternally. They emigrated to the United States in 1933. Elsa was diagnosed with heart and kidney problems in 1935 and died in December 1936.\n\nAmong Einstein's well-known friends were Michele Besso, Paul Ehrenfest, Marcel Grossmann, János Plesch, Daniel Q. Posin, Maurice Solovine, and Stephen Wise.\n\nAfter graduating in 1900, Einstein spent almost two frustrating years searching for a teaching post. He acquired Swiss citizenship in February 1901, but for medical reasons was not conscripted. With the help of Marcel Grossmann's father, he secured a job in Bern at the Federal Office for Intellectual Property, the patent office, as an assistant examiner – level III.\n\nEinstein evaluated patent applications for a variety of devices including a gravel sorter and an electromechanical typewriter. In 1903, his position at the Swiss Patent Office became permanent, although he was passed over for promotion until he \"fully mastered machine technology\".\n\nMuch of his work at the patent office related to questions about transmission of electric signals and electrical–mechanical synchronization of time, two technical problems that show up conspicuously in the thought experiments that eventually led Einstein to his radical conclusions about the nature of light and the fundamental connection between space and time.\n\nWith a few friends he had met in Bern, Einstein started a small discussion group in 1902, self-mockingly named \"The Olympia Academy\", which met regularly to discuss science and philosophy. Their readings included the works of Henri Poincaré, Ernst Mach, and David Hume, which influenced his scientific and philosophical outlook.\n\nIn 1900, Einstein's paper \"Folgerungen aus den Capillaritätserscheinungen\" (\"Conclusions from the Capillarity Phenomena\") was published in the journal \"Annalen der Physik\". On 30 April 1905, Einstein completed his thesis, with Alfred Kleiner, Professor of Experimental Physics, serving as \"pro-forma\" advisor. As a result, Einstein was awarded a PhD by the University of Zürich, with his dissertation \"A New Determination of Molecular Dimensions\".\n\nIn that same year, which has been called Einstein's \"annus mirabilis\" (miracle year), he published four groundbreaking papers, on the photoelectric effect, Brownian motion, special relativity, and the equivalence of mass and energy, which were to bring him to the notice of the academic world, at the age of 26.\n\nBy 1908, he was recognized as a leading scientist and was appointed lecturer at the University of Bern. The following year, after giving a lecture on electrodynamics and the relativity principle at the University of Zürich, Alfred Kleiner recommended him to the faculty for a newly created professorship in theoretical physics. Einstein was appointed associate professor in 1909.\n\nEinstein became a full professor at the German Charles-Ferdinand University in Prague in April 1911, accepting Austrian citizenship in the Austro-Hungarian Empire to do so. During his Prague stay, he wrote 11 scientific works, five of them on radiation mathematics and on the quantum theory of solids. In July 1912, he returned to his alma mater in Zürich. From 1912 until 1914, he was professor of theoretical physics at the ETH Zurich, where he taught analytical mechanics and thermodynamics. He also studied continuum mechanics, the molecular theory of heat, and the problem of gravitation, on which he worked with mathematician and friend Marcel Grossmann.\n\nOn 3 July 1913, he was voted for membership in the Prussian Academy of Sciences in Berlin. Max Planck and Walther Nernst visited him the next week in Zurich to persuade him to join the academy, additionally offering him the post of director at the Kaiser Wilhelm Institute for Physics, which was soon to be established. (Membership in the academy included paid salary and professorship without teaching duties at the Humboldt University of Berlin.) He was officially elected to the academy on 24 July, and he accepted to move to the German Empire the next year. His decision to move to Berlin was also influenced by the prospect of living near his cousin Elsa, with whom he had developed a romantic affair. He joined the academy and thus the Berlin University on 1 April 1914. As World War I broke out that year, the plan for Kaiser Wilhelm Institute for Physics was aborted. The institute was established on 1 October 1917, with Einstein as its director. In 1916, Einstein was elected president of the German Physical Society (1916–1918).\n\nBased on calculations Einstein made in 1911, about his new theory of general relativity, light from another star should be bent by the Sun's gravity. In 1919, that prediction was confirmed by Sir Arthur Eddington during the solar eclipse of 29 May 1919. Those observations were published in the international media, making Einstein world-famous. On 7 November 1919, the leading British newspaper \"The Times\" printed a banner headline that read: \"Revolution in Science – New Theory of the Universe – Newtonian Ideas Overthrown\".\n\nIn 1920, he became a Foreign Member of the Royal Netherlands Academy of Arts and Sciences. In 1922, he was awarded the 1921 Nobel Prize in Physics \"for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect\". While the general theory of relativity was still considered somewhat controversial, the citation also does not treat the cited work as an \"explanation\" but merely as a \"discovery of the law\", as the idea of photons was considered outlandish and did not receive universal acceptance until the 1924 derivation of the Planck spectrum by S. N. Bose. Einstein was elected a Foreign Member of the Royal Society (ForMemRS) in 1921. He also received the Copley Medal from the Royal Society in 1925.\n\nEinstein visited New York City for the first time on 2 April 1921, where he received an official welcome by Mayor John Francis Hylan, followed by three weeks of lectures and receptions. He went on to deliver several lectures at Columbia University and Princeton University, and in Washington he accompanied representatives of the National Academy of Science on a visit to the White House. On his return to Europe he was the guest of the British statesman and philosopher Viscount Haldane in London, where he met several renowned scientific, intellectual and political figures, and delivered a lecture at King's College London. \n\nHe also published an essay, \"My First Impression of the U.S.A.,\" in July 1921, in which he tried briefly to describe some characteristics of Americans, much as had Alexis de Tocqueville, who published his own impressions in \"Democracy in America\" (1835). For some of his observations, Einstein was clearly surprised: \"What strikes a visitor is the joyous, positive attitude to life ... The American is friendly, self-confident, optimistic, and without envy.\"\n\nIn 1922, his travels took him to Asia and later to Palestine, as part of a six-month excursion and speaking tour, as he visited Singapore, Ceylon and Japan, where he gave a series of lectures to thousands of Japanese. After his first public lecture, he met the emperor and empress at the Imperial Palace, where thousands came to watch. In a letter to his sons, he described his impression of the Japanese as being modest, intelligent, considerate, and having a true feel for art. In his own travel diaries from his 1922-23 visit to Asia, he expresses some views on the Chinese, Japanese and Indian people, which have been described as xenophobic and racist judgments when they were rediscovered in 2018.\n\nBecause of Einstein's travels to the Far East, he was unable to personally accept the Nobel Prize for Physics at the Stockholm award ceremony in December 1922. In his place, the banquet speech was held by a German diplomat, who praised Einstein not only as a scientist but also as an international peacemaker and activist.\n\nOn his return voyage, he visited Palestine for 12 days in what would become his only visit to that region. He was greeted as if he were a head of state, rather than a physicist, which included a cannon salute upon arriving at the home of the British high commissioner, Sir Herbert Samuel. During one reception, the building was stormed by people who wanted to see and hear him. In Einstein's talk to the audience, he expressed happiness that the Jewish people were beginning to be recognized as a force in the world.\n\nEinstein visited Spain for two weeks in 1923, where he briefly met Santiago Ramón y Cajal and also received a diploma from King Alfonso XIII naming him a member of the Spanish Academy of Sciences.\n\nFrom 1922 to 1932, Einstein was a member of the International Committee on Intellectual Cooperation of the League of Nations in Geneva (with a few months of interruption in 1923–1924), a body created to promote international exchange between scientists, researchers, teachers, artists and intellectuals. Originally slated to serve as the Swiss delegate, Secretary-General Eric Drummond was persuaded by Catholic activists Oskar Halecki and Giuseppe Motta to instead have him become the German delegate, thus allowing Gonzague de Reynold to take the Swiss spot, from which he promoted traditionalist Catholic values. Einstein’s former physics professor Hendrik Lorentz and the French chemist Marie Curie were also members of the committee.\n\nIn December 1930, Einstein visited America for the second time, originally intended as a two-month working visit as a research fellow at the California Institute of Technology. After the national attention he received during his first trip to the US, he and his arrangers aimed to protect his privacy. Although swamped with telegrams and invitations to receive awards or speak publicly, he declined them all.\n\nAfter arriving in New York City, Einstein was taken to various places and events, including Chinatown, a lunch with the editors of \"The New York Times\", and a performance of \"Carmen\" at the Metropolitan Opera, where he was cheered by the audience on his arrival. During the days following, he was given the keys to the city by Mayor Jimmy Walker and met the president of Columbia University, who described Einstein as \"the ruling monarch of the mind\". Harry Emerson Fosdick, pastor at New York's Riverside Church, gave Einstein a tour of the church and showed him a full-size statue that the church made of Einstein, standing at the entrance. Also during his stay in New York, he joined a crowd of 15,000 people at Madison Square Garden during a Hanukkah celebration.\nEinstein next traveled to California, where he met Caltech president and Nobel laureate, Robert A. Millikan. His friendship with Millikan was \"awkward\", as Millikan \"had a penchant for patriotic militarism,\" where Einstein was a pronounced pacifist. During an address to Caltech's students, Einstein noted that science was often inclined to do more harm than good.\n\nThis aversion to war also led Einstein to befriend author Upton Sinclair and film star Charlie Chaplin, both noted for their pacifism. Carl Laemmle, head of Universal Studios, gave Einstein a tour of his studio and introduced him to Chaplin. They had an instant rapport, with Chaplin inviting Einstein and his wife, Elsa, to his home for dinner. Chaplin said Einstein's outward persona, calm and gentle, seemed to conceal a \"highly emotional temperament,\" from which came his \"extraordinary intellectual energy\".\n\nChaplin's film, \"City Lights\", was to premiere a few days later in Hollywood, and Chaplin invited Einstein and Elsa to join him as his special guests. Walter Isaacson, Einstein's biographer, described this as \"one of the most memorable scenes in the new era of celebrity\". Chaplin visited Einstein at his home on a later trip to Berlin, and recalled his \"modest little flat\" and the piano at which he had begun writing his theory. Chaplin speculated that it was \"possibly used as kindling wood by the Nazis.\"\n\nIn February 1933 while on a visit to the United States, Einstein knew he could not return to Germany with the rise to power of the Nazis under Germany's new chancellor, Adolf Hitler.\n\nWhile at American universities in early 1933, he undertook his third two-month visiting professorship at the California Institute of Technology in Pasadena. He and his wife Elsa returned to Belgium by ship in March, and during the trip they learned that their cottage was raided by the Nazis and his personal sailboat confiscated. Upon landing in Antwerp on 28 March, he immediately went to the German consulate and surrendered his passport, formally renouncing his German citizenship. The Nazis later sold his boat and converted his cottage into a Hitler Youth camp.\n\nIn April 1933, Einstein discovered that the new German government had passed laws barring Jews from holding any official positions, including teaching at universities. Historian Gerald Holton describes how, with \"virtually no audible protest being raised by their colleagues\", thousands of Jewish scientists were suddenly forced to give up their university positions and their names were removed from the rolls of institutions where they were employed.\n\nA month later, Einstein's works were among those targeted by the German Student Union in the Nazi book burnings, with Nazi propaganda minister Joseph Goebbels proclaiming, \"Jewish intellectualism is dead.\" One German magazine included him in a list of enemies of the German regime with the phrase, \"not yet hanged\", offering a $5,000 bounty on his head. In a subsequent letter to physicist and friend Max Born, who had already emigrated from Germany to England, Einstein wrote, \"... I must confess that the degree of their brutality and cowardice came as something of a surprise.\" After moving to the US, he described the book burnings as a \"spontaneous emotional outburst\" by those who \"shun popular enlightenment,\" and \"more than anything else in the world, fear the influence of men of intellectual independence.\"\n\nEinstein was now without a permanent home, unsure where he would live and work, and equally worried about the fate of countless other scientists still in Germany. He rented a house in De Haan, Belgium, where he lived for a few months. In late July 1933, he went to England for about six weeks at the personal invitation of British naval officer Commander Oliver Locker-Lampson, who had become friends with Einstein in the preceding years. To protect Einstein, Locker-Lampson had two assistants watch over him at his secluded cottage outside London, with a photo of them carrying shotguns and guarding Einstein, published in the \"Daily Herald\" on 24 July 1933.\n\nLocker-Lampson took Einstein to meet Winston Churchill at his home, and later, Austen Chamberlain and former Prime Minister Lloyd George. Einstein asked them to help bring Jewish scientists out of Germany. British historian Martin Gilbert notes that Churchill responded immediately, and sent his friend, physicist Frederick Lindemann, to Germany to seek out Jewish scientists and place them in British universities. Churchill later observed that as a result of Germany having driven the Jews out, they had lowered their \"technical standards\" and put the Allies' technology ahead of theirs.\n\nEinstein later contacted leaders of other nations, including Turkey's Prime Minister, İsmet İnönü, to whom he wrote in September 1933 requesting placement of unemployed German-Jewish scientists. As a result of Einstein's letter, Jewish invitees to Turkey eventually totaled over \"1,000 saved individuals\".\n\nLocker-Lampson also submitted a bill to parliament to extend British citizenship to Einstein, during which period Einstein made a number of public appearances describing the crisis brewing in Europe. In one of his speeches he denounced Germany's treatment of Jews, while at the same time he introduced a bill promoting Jewish citizenship in Palestine, as they were being denied citizenship elsewhere. In his speech he described Einstein as a \"citizen of the world\" who should be offered a temporary shelter in the UK. Both bills failed, however, and Einstein then accepted an earlier offer from the Institute for Advanced Study, in Princeton, New Jersey, US, to become a resident scholar.\n\nIn October 1933 Einstein returned to the US and took up a position at the Institute for Advanced Study, noted for having become a refuge for scientists fleeing Nazi Germany. At the time, most American universities, including Harvard, Princeton and Yale, had minimal or no Jewish faculty or students, as a result of their Jewish quotas, which lasted until the late 1940s.\n\nEinstein was still undecided on his future. He had offers from several European universities, including Christ Church, Oxford where he stayed for three short periods between May 1931 and June 1933 and was offered a 5-year studentship, but in 1935 he arrived at the decision to remain permanently in the United States and apply for citizenship.\n\nEinstein's affiliation with the Institute for Advanced Study would last until his death in 1955. He was one of the four first selected (two of the others being John von Neumann and Kurt Gödel) at the new Institute, where he soon developed a close friendship with Gödel. The two would take long walks together discussing their work. Bruria Kaufman, his assistant, later became a physicist. During this period, Einstein tried to develop a unified field theory and to refute the accepted interpretation of quantum physics, both unsuccessfully.\n\nIn 1939, a group of Hungarian scientists that included émigré physicist Leó Szilárd attempted to alert Washington to ongoing Nazi atomic bomb research. The group's warnings were discounted. Einstein and Szilárd, along with other refugees such as Edward Teller and Eugene Wigner, \"regarded it as their responsibility to alert Americans to the possibility that German scientists might win the race to build an atomic bomb, and to warn that Hitler would be more than willing to resort to such a weapon.\" To make certain the US was aware of the danger, in July 1939, a few months before the beginning of World War II in Europe, Szilárd and Wigner visited Einstein to explain the possibility of atomic bombs, which Einstein, a pacifist, said he had never considered. He was asked to lend his support by writing a letter, with Szilárd, to President Roosevelt, recommending the US pay attention and engage in its own nuclear weapons research.\n\nThe letter is believed to be \"arguably the key stimulus for the U.S. adoption of serious investigations into nuclear weapons on the eve of the U.S. entry into World War II\". In addition to the letter, Einstein used his connections with the Belgian Royal Family and the Belgian queen mother to get access with a personal envoy to the White House's Oval Office. Some say that as a result of Einstein's letter and his meetings with Roosevelt, the US entered the \"race\" to develop the bomb, drawing on its \"immense material, financial, and scientific resources\" to initiate the Manhattan Project.\n\nFor Einstein, \"war was a disease ... [and] he called for resistance to war.\" By signing the letter to Roosevelt, some argue he went against his pacifist principles. In 1954, a year before his death, Einstein said to his old friend, Linus Pauling, \"I made one great mistake in my life—when I signed the letter to President Roosevelt recommending that atom bombs be made; but there was some justification—the danger that the Germans would make them ...\"\n\nEinstein became an American citizen in 1940. Not long after settling into his career at the Institute for Advanced Study (in Princeton, New Jersey), he expressed his appreciation of the meritocracy in American culture when compared to Europe. He recognized the \"right of individuals to say and think what they pleased\", without social barriers, and as a result, individuals were encouraged, he said, to be more creative, a trait he valued from his own early education.\n\nEinstein joined the National Association for the Advancement of Colored People (NAACP) in Princeton, where he campaigned for the civil rights of African Americans. He considered racism America's \"worst disease,\" seeing it as \"handed down from one generation to the next\". As part of his involvement, he corresponded with civil rights activist W. E. B. Du Bois and was prepared to testify on his behalf during his trial in 1951. When Einstein offered to be a character witness for Du Bois, the judge decided to drop the case.\n\nIn 1946 Einstein visited Lincoln University in Pennsylvania, a historically black college, where he was awarded an honorary degree. (Lincoln was the first university in the United States to grant college degrees to African Americans; alumni include Langston Hughes and Thurgood Marshall.) Einstein gave a speech about racism in America, adding, \"I do not intend to be quiet about it.\" A resident of Princeton recalls that Einstein had once paid the college tuition for a black student.\n\nEinstein was a figurehead leader in helping establish the Hebrew University of Jerusalem, which opened in 1925, and was among its first Board of Governors. Earlier, in 1921, he was asked by the biochemist and president of the World Zionist Organization, Chaim Weizmann, to help raise funds for the planned university. He also submitted various suggestions as to its initial programs.\n\nAmong those, he advised first creating an Institute of Agriculture in order to settle the undeveloped land. That should be followed, he suggested, by a Chemical Institute and an Institute of Microbiology, to fight the various ongoing epidemics such as malaria, which he called an \"evil\" that was undermining a third of the country's development. Establishing an Oriental Studies Institute, to include language courses given in both Hebrew and Arabic, for scientific exploration of the country and its historical monuments, was also important.\n\nChaim Weizmann later became Israel's first president. Upon his death while in office in November 1952 and at the urging of Ezriel Carlebach, Prime Minister David Ben-Gurion offered Einstein the position of President of Israel, a mostly ceremonial post. The offer was presented by Israel's ambassador in Washington, Abba Eban, who explained that the offer \"embodies the deepest respect which the Jewish people can repose in any of its sons\". Einstein declined, and wrote in his response that he was \"deeply moved\", and \"at once saddened and ashamed\" that he could not accept it.\n\nEinstein developed an appreciation for music at an early age, and later wrote: \"If I were not a physicist, I would probably be a musician. I often think in music. I live my daydreams in music. I see my life in terms of music... I get most joy in life out of music.\"\n\nHis mother played the piano reasonably well and wanted her son to learn the violin, not only to instill in him a love of music but also to help him assimilate into German culture. According to conductor Leon Botstein, Einstein began playing when he was 5, although he did not enjoy it at that age.\n\nWhen he turned 13, he discovered the violin sonatas of Mozart, whereupon \"Einstein fell in love\" with Mozart's music and studied music more willingly. He taught himself to play without \"ever practicing systematically\", he said, deciding that \"love is a better teacher than a sense of duty.\" At age 17, he was heard by a school examiner in Aarau as he played Beethoven's violin sonatas, the examiner stating afterward that his playing was \"remarkable and revealing of 'great insight'.\" What struck the examiner, writes Botstein, was that Einstein \"displayed a deep love of the music, a quality that was and remains in short supply. Music possessed an unusual meaning for this student.\"\n\nMusic took on a pivotal and permanent role in Einstein's life from that period on. Although the idea of becoming a professional musician himself was not on his mind at any time, among those with whom Einstein played chamber music were a few professionals, and he performed for private audiences and friends. Chamber music had also become a regular part of his social life while living in Bern, Zürich, and Berlin, where he played with Max Planck and his son, among others. He is sometimes erroneously credited as the editor of the 1937 edition of the Köchel catalogue of Mozart's work; that edition was prepared by Alfred Einstein, who may have been a distant relation.\n\nIn 1931, while engaged in research at the California Institute of Technology, he visited the Zoellner family conservatory in Los Angeles, where he played some of Beethoven and Mozart's works with members of the Zoellner Quartet. Near the end of his life, when the young Juilliard Quartet visited him in Princeton, he played his violin with them, and the quartet was \"impressed by Einstein's level of coordination and intonation\".\n\nEinstein's political view was in favor of socialism and critical of capitalism, which he detailed in his essays such as \"Why Socialism?\". Einstein offered and was called on to give judgments and opinions on matters often unrelated to theoretical physics or mathematics. He strongly advocated the idea of a democratic global government that would check the power of nation-states in the framework of a world federation. The FBI created a secret dossier on Einstein in 1932, and by the time of his death his FBI file was 1,427 pages long.\n\nEinstein was deeply impressed by Mahatma Gandhi. He exchanged written letters with Gandhi, and called him \"a role model for the generations to come\" in a letter writing about him.\n\nEinstein spoke of his spiritual outlook in a wide array of original writings and interviews. Einstein stated that he had sympathy for the impersonal pantheistic God of Baruch Spinoza's philosophy. He did not believe in a personal God who concerns himself with fates and actions of human beings, a view which he described as naïve. He clarified, however, that \"I am not an atheist\", preferring to call himself an agnostic, or a \"deeply religious nonbeliever.\" When asked if he believed in an afterlife, Einstein replied, \"No. And one life is enough for me.\"\n\nEinstein was primarily affiliated with non-religious humanist and Ethical Culture groups in both the UK and US. He served on the advisory board of the First Humanist Society of New York, and was an honorary associate of the Rationalist Association, which publishes \"New Humanist\" in Britain. For the seventy-fifth anniversary of the New York Society for Ethical Culture, he stated that the idea of Ethical Culture embodied his personal conception of what is most valuable and enduring in religious idealism. He observed, \"Without 'ethical culture' there is no salvation for humanity.\"\n\nOn 17 April 1955, Einstein experienced internal bleeding caused by the rupture of an abdominal aortic aneurysm, which had previously been reinforced surgically by Rudolph Nissen in 1948. He took the draft of a speech he was preparing for a television appearance commemorating the State of Israel's seventh anniversary with him to the hospital, but he did not live long enough to complete it.\n\nEinstein refused surgery, saying, \"I want to go when I want. It is tasteless to prolong life artificially. I have done my share; it is time to go. I will do it elegantly.\" He died in Princeton Hospital early the next morning at the age of 76, having continued to work until near the end.\n\nDuring the autopsy, the pathologist of Princeton Hospital, Thomas Stoltz Harvey, removed Einstein's brain for preservation without the permission of his family, in the hope that the neuroscience of the future would be able to discover what made Einstein so intelligent. Einstein's remains were cremated and his ashes were scattered at an undisclosed location.\n\nIn a memorial lecture delivered on 13 December 1965, at UNESCO headquarters, nuclear physicist J. Robert Oppenheimer summarized his impression of Einstein as a person: \"He was almost wholly without sophistication and wholly without worldliness ... There was always with him a wonderful purity at once childlike and profoundly stubborn.\"\n\nThroughout his life, Einstein published hundreds of books and articles. He published more than 300 scientific papers and 150 non-scientific ones. On 5 December 2014, universities and archives announced the release of Einstein's papers, comprising more than 30,000 unique documents. Einstein's intellectual achievements and originality have made the word \"Einstein\" synonymous with \"genius.\" In addition to the work he did by himself he also collaborated with other scientists on additional projects including the Bose–Einstein statistics, the Einstein refrigerator and others.\n\nThe \"Annus Mirabilis\" papers are four articles pertaining to the photoelectric effect (which gave rise to quantum theory), Brownian motion, the special theory of relativity, and E = mc that Einstein published in the \"Annalen der Physik\" scientific journal in 1905. These four works contributed substantially to the foundation of modern physics and changed views on space, time, and matter. The four papers are:\n\nEinstein's first paper submitted in 1900 to \"Annalen der Physik\" was on capillary attraction. It was published in 1901 with the title \"Folgerungen aus den Capillaritätserscheinungen\", which translates as \"Conclusions from the capillarity phenomena\". Two papers he published in 1902–1903 (thermodynamics) attempted to interpret atomic phenomena from a statistical point of view. These papers were the foundation for the 1905 paper on Brownian motion, which showed that Brownian movement can be construed as firm evidence that molecules exist. His research in 1903 and 1904 was mainly concerned with the effect of finite atomic size on diffusion phenomena.\n\nEinstein returned to the problem of thermodynamic fluctuations, giving a treatment of the density variations in a fluid at its critical point. Ordinarily the density fluctuations are controlled by the second derivative of the free energy with respect to the density. At the critical point, this derivative is zero, leading to large fluctuations. The effect of density fluctuations is that light of all wavelengths is scattered, making the fluid look milky white. Einstein relates this to Rayleigh scattering, which is what happens when the fluctuation size is much smaller than the wavelength, and which explains why the sky is blue. Einstein quantitatively derived critical opalescence from a treatment of density fluctuations, and demonstrated how both the effect and Rayleigh scattering originate from the atomistic constitution of matter.\n\nEinstein's \"Zur Elektrodynamik bewegter Körper\" (\"On the Electrodynamics of Moving Bodies\") was received on 30 June 1905 and published 26 September of that same year. It reconciled conflicts between Maxwell's equations (the laws of electricity and magnetism) and the laws of Newtonian mechanics by introducing changes to the laws of mechanics. Observationally, the effects of these changes are most apparent at high speeds (where objects are moving at speeds close to the speed of light). The theory developed in this paper later became known as Einstein's special theory of relativity.\n\nThis paper predicted that, when measured in the frame of a relatively moving observer, a clock carried by a moving body would appear to slow down, and the body itself would contract in its direction of motion. This paper also argued that the idea of a luminiferous aether—one of the leading theoretical entities in physics at the time—was superfluous.\n\nIn his paper on mass–energy equivalence, Einstein produced \"E\" = \"mc\" as a consequence of his special relativity equations. Einstein's 1905 work on relativity remained controversial for many years, but was accepted by leading physicists, starting with Max Planck.\n\nEinstein originally framed special relativity in terms of kinematics (the study of moving bodies). In 1908, Hermann Minkowski reinterpreted special relativity in geometric terms as a theory of spacetime. Einstein adopted Minkowski's formalism in his 1915 general theory of relativity.\n\nGeneral relativity (GR) is a theory of gravitation that was developed by Einstein between 1907 and 1915. According to general relativity, the observed gravitational attraction between masses results from the warping of space and time by those masses. General relativity has developed into an essential tool in modern astrophysics. It provides the foundation for the current understanding of black holes, regions of space where gravitational attraction is so strong that not even light can escape.\n\nAs Einstein later said, the reason for the development of general relativity was that the preference of inertial motions within special relativity was unsatisfactory, while a theory which from the outset prefers no state of motion (even accelerated ones) should appear more satisfactory. Consequently, in 1907 he published an article on acceleration under special relativity. In that article titled \"On the Relativity Principle and the Conclusions Drawn from It\", he argued that free fall is really inertial motion, and that for a free-falling observer the rules of special relativity must apply. This argument is called the equivalence principle. In the same article, Einstein also predicted the phenomena of gravitational time dilation, gravitational red shift and deflection of light.\n\nIn 1911, Einstein published another article \"On the Influence of Gravitation on the Propagation of Light\" expanding on the 1907 article, in which he estimated the amount of deflection of light by massive bodies. Thus, the theoretical prediction of general relativity could for the first time be tested experimentally.\n\nIn 1916, Einstein predicted gravitational waves, ripples in the curvature of spacetime which propagate as waves, traveling outward from the source, transporting energy as gravitational radiation. The existence of gravitational waves is possible under general relativity due to its Lorentz invariance which brings the concept of a finite speed of propagation of the physical interactions of gravity with it. By contrast, gravitational waves cannot exist in the Newtonian theory of gravitation, which postulates that the physical interactions of gravity propagate at infinite speed.\n\nThe first, indirect, detection of gravitational waves came in the 1970s through observation of a pair of closely orbiting neutron stars, PSR B1913+16. The explanation of the decay in their orbital period was that they were emitting gravitational waves. Einstein's prediction was confirmed on 11 February 2016, when researchers at LIGO published the first observation of gravitational waves, detected on Earth on 14 September 2015, exactly one hundred years after the prediction.\n\nWhile developing general relativity, Einstein became confused about the gauge invariance in the theory. He formulated an argument that led him to conclude that a general relativistic field theory is impossible. He gave up looking for fully generally covariant tensor equations, and searched for equations that would be invariant under general linear transformations only.\n\nIn June 1913, the Entwurf (\"draft\") theory was the result of these investigations. As its name suggests, it was a sketch of a theory, less elegant and more difficult than general relativity, with the equations of motion supplemented by additional gauge fixing conditions. After more than two years of intensive work, Einstein realized that the hole argument was mistaken and abandoned the theory in November 1915.\n\nIn 1917, Einstein applied the general theory of relativity to the structure of the universe as a whole. He discovered that the general field equations predicted a universe that was dynamic, either contracting or expanding. As observational evidence for a dynamic universe was not known at the time, Einstein introduced a new term, the cosmological constant, to the field equations, in order to allow the theory to predict a static universe. The modified field equations predicted a static universe of closed curvature, in accordance with Einstein's understanding of Mach's principle in these years. This model became known as the Einstein World or Einstein's static universe.\n\nFollowing the discovery of the recession of the nebulae by Edwin Hubble in 1929, Einstein abandoned his static model of the universe, and proposed two dynamic models of the cosmos, The Friedmann-Einstein universe of 1931 and the Einstein–de Sitter universe of 1932. In each of these models, Einstein discarded the cosmological constant, claiming that it was \"in any case theoretically unsatisfactory\".\n\nIn many Einstein biographies, it is claimed that Einstein referred to the cosmological constant in later years as his \"biggest blunder\". The astrophysicist Mario Livio has recently cast doubt on this claim, suggesting that it may be exaggerated.\n\nIn late 2013, a team led by the Irish physicist Cormac O'Raifeartaigh discovered evidence that, shortly after learning of Hubble's observations of the recession of the nebulae, Einstein considered a steady-state model of the universe. In a hitherto overlooked manuscript, apparently written in early 1931, Einstein explored a model of the expanding universe in which the density of matter remains constant due to a continuous creation of matter, a process he associated with the cosmological constant. As he stated in the paper, \"In what follows, I would like to draw attention to a solution to equation (1) that can account for Hubbel's [\"sic\"] facts, and in which the density is constant over time\" ... \"If one considers a physically bounded volume, particles of matter will be continually leaving it. For the density to remain constant, new particles of matter must be continually formed in the volume from space.\"\n\nIt thus appears that Einstein considered a steady-state model of the expanding universe many years before Hoyle, Bondi and Gold. However, Einstein's steady-state model contained a fundamental flaw and he quickly abandoned the idea.\n\nGeneral relativity includes a dynamical spacetime, so it is difficult to see how to identify the conserved energy and momentum. Noether's theorem allows these quantities to be determined from a Lagrangian with translation invariance, but general covariance makes translation invariance into something of a gauge symmetry. The energy and momentum derived within general relativity by Noether's prescriptions do not make a real tensor for this reason.\n\nEinstein argued that this is true for fundamental reasons, because the gravitational field could be made to vanish by a choice of coordinates. He maintained that the non-covariant energy momentum pseudotensor was in fact the best description of the energy momentum distribution in a gravitational field. This approach has been echoed by Lev Landau and Evgeny Lifshitz, and others, and has become standard.\n\nThe use of non-covariant objects like pseudotensors was heavily criticized in 1917 by Erwin Schrödinger and others.\n\nIn 1935, Einstein collaborated with Nathan Rosen to produce a model of a wormhole, often called Einstein–Rosen bridges. His motivation was to model elementary particles with charge as a solution of gravitational field equations, in line with the program outlined in the paper \"Do Gravitational Fields play an Important Role in the Constitution of the Elementary Particles?\". These solutions cut and pasted Schwarzschild black holes to make a bridge between two patches.\n\nIf one end of a wormhole was positively charged, the other end would be negatively charged. These properties led Einstein to believe that pairs of particles and antiparticles could be described in this way.\n\nIn order to incorporate spinning point particles into general relativity, the affine connection needed to be generalized to include an antisymmetric part, called the torsion. This modification was made by Einstein and Cartan in the 1920s.\n\nThe theory of general relativity has a fundamental law—the Einstein equations which describe how space curves, the geodesic equation which describes how particles move may be derived from the Einstein equations.\n\nSince the equations of general relativity are non-linear, a lump of energy made out of pure gravitational fields, like a black hole, would move on a trajectory which is determined by the Einstein equations themselves, not by a new law. So Einstein proposed that the path of a singular solution, like a black hole, would be determined to be a geodesic from general relativity itself.\n\nThis was established by Einstein, Infeld, and Hoffmann for pointlike objects without angular momentum, and by Roy Kerr for spinning objects.\n\nIn a 1905 paper, Einstein postulated that light itself consists of localized particles (\"quanta\"). Einstein's light quanta were nearly universally rejected by all physicists, including Max Planck and Niels Bohr. This idea only became universally accepted in 1919, with Robert Millikan's detailed experiments on the photoelectric effect, and with the measurement of Compton scattering.\n\nEinstein concluded that each wave of frequency \"f\" is associated with a collection of photons with energy \"hf\" each, where \"h\" is Planck's constant. He does not say much more, because he is not sure how the particles are related to the wave. But he does suggest that this idea would explain certain experimental results, notably the photoelectric effect.\n\nIn 1907, Einstein proposed a model of matter where each atom in a lattice structure is an independent harmonic oscillator. In the Einstein model, each atom oscillates independently—a series of equally spaced quantized states for each oscillator. Einstein was aware that getting the frequency of the actual oscillations would be difficult, but he nevertheless proposed this theory because it was a particularly clear demonstration that quantum mechanics could solve the specific heat problem in classical mechanics. Peter Debye refined this model.\n\nThroughout the 1910s, quantum mechanics expanded in scope to cover many different systems. After Ernest Rutherford discovered the nucleus and proposed that electrons orbit like planets, Niels Bohr was able to show that the same quantum mechanical postulates introduced by Planck and developed by Einstein would explain the discrete motion of electrons in atoms, and the periodic table of the elements.\n\nEinstein contributed to these developments by linking them with the 1898 arguments Wilhelm Wien had made. Wien had shown that the hypothesis of adiabatic invariance of a thermal equilibrium state allows all the blackbody curves at different temperature to be derived from one another by a simple shifting process. Einstein noted in 1911 that the same adiabatic principle shows that the quantity which is quantized in any mechanical motion must be an adiabatic invariant. Arnold Sommerfeld identified this adiabatic invariant as the action variable of classical mechanics.\n\nIn 1924, Einstein received a description of a statistical model from Indian physicist Satyendra Nath Bose, based on a counting method that assumed that light could be understood as a gas of indistinguishable particles. Einstein noted that Bose's statistics applied to some atoms as well as to the proposed light particles, and submitted his translation of Bose's paper to the \"Zeitschrift für Physik\". Einstein also published his own articles describing the model and its implications, among them the Bose–Einstein condensate phenomenon that some particulates should appear at very low temperatures. It was not until 1995 that the first such condensate was produced experimentally by Eric Allin Cornell and Carl Wieman using ultra-cooling equipment built at the NIST–JILA laboratory at the University of Colorado at Boulder. Bose–Einstein statistics are now used to describe the behaviors of any assembly of bosons. Einstein's sketches for this project may be seen in the Einstein Archive in the library of the Leiden University.\n\nAlthough the patent office promoted Einstein to Technical Examiner Second Class in 1906, he had not given up on academia. In 1908, he became a \"Privatdozent\" at the University of Bern. In \"Über die Entwicklung unserer Anschauungen über das Wesen und die Konstitution der Strahlung\" (\"\"), on the quantization of light, and in an earlier 1909 paper, Einstein showed that Max Planck's energy quanta must have well-defined momenta and act in some respects as independent, point-like particles. This paper introduced the \"photon\" concept (although the name \"photon\" was introduced later by Gilbert N. Lewis in 1926) and inspired the notion of wave–particle duality in quantum mechanics. Einstein saw this wave–particle duality in radiation as concrete evidence for his conviction that physics needed a new, unified foundation.\n\nIn a series of works completed from 1911 to 1913, Planck reformulated his 1900 quantum theory and introduced the idea of zero-point energy in his \"second quantum theory\". Soon, this idea attracted the attention of Einstein and his assistant Otto Stern. Assuming the energy of rotating diatomic molecules contains zero-point energy, they then compared the theoretical specific heat of hydrogen gas with the experimental data. The numbers matched nicely. However, after publishing the findings, they promptly withdrew their support, because they no longer had confidence in the correctness of the idea of zero-point energy.\n\nIn 1917, at the height of his work on relativity, Einstein published an article in \"Physikalische Zeitschrift\" that proposed the possibility of stimulated emission, the physical process that makes possible the maser and the laser.\nThis article showed that the statistics of absorption and emission of light would only be consistent with Planck's distribution law if the emission of light into a mode with n photons would be enhanced statistically compared to the emission of light into an empty mode. This paper was enormously influential in the later development of quantum mechanics, because it was the first paper to show that the statistics of atomic transitions had simple laws.\n\nEinstein discovered Louis de Broglie's work and supported his ideas, which were received skeptically at first. In another major paper from this era, Einstein gave a wave equation for de Broglie waves, which Einstein suggested was the Hamilton–Jacobi equation of mechanics. This paper would inspire Schrödinger's work of 1926.\n\nEinstein was displeased with modern quantum mechanics as it had evolved after 1925. Contrary to popular belief, his doubts were not due to a conviction that God \"is not playing at dice.\" Indeed, it was Einstein himself, in his 1917 paper that proposed the possibility of stimulated emission, who first proposed the fundamental role of chance in explaining quantum processes. Rather, he objected to what quantum mechanics implies about the nature of reality. Einstein believed that a physical reality exists independent of our ability to observe it. In contrast, Bohr and his followers maintained that all we can know are the results of measurements and observations, and that it makes no sense to speculate about an ultimate reality that exists beyond our perceptions.\n\n The Bohr–Einstein debates were a series of public disputes about quantum mechanics between Einstein and Niels Bohr, who were two of its founders. Their debates are remembered because of their importance to the philosophy of science. Their debates would influence later interpretations of quantum mechanics.\n\nIn 1935, Einstein returned quantum mechanics, in particular to the question of its completeness, in the \"EPR paper\". In a thought experiment, he considered two particles which had interacted such that their properties were strongly correlated. No matter how far the two particles were separated, a precise position measurement on one particle would result in equally precise knowledge of the position of the other particle; likewise a precise momentum measurement of one particle would result in equally precise knowledge of the momentum of the other particle, without needing to disturb the other particle in any way.\n\nGiven Einstein's concept of local realism, there were two possibilities: (1) either the other particle had these properties already determined, or (2) the process of measuring the first particle instantaneously affected the reality of the position and momentum of the second particle. Einstein rejected this second possibility (popularly called \"spooky action at a distance\").\n\nEinstein's belief in local realism led him to assert that, while the correctness of quantum mechanics was not in question, it must be incomplete. But as a physical principle, local realism was shown to be incorrect when the Aspect experiment of 1982 confirmed Bell's theorem, which J. S. Bell had delineated in 1964. The results of these and subsequent experiments demonstrate that quantum physics cannot be represented by any version of the picture of physics in which \"particles are regarded as unconnected independent classical-like entities, each one being unable to communicate with the other after they have separated.\"\n\nAlthough Einstein was wrong about local realism, his clear prediction of the unusual properties of its opposite, \"entangled quantum states\", has resulted in the EPR paper becoming among the top ten papers published in Physical Review. It is considered a centerpiece of the development of quantum information theory.\n\nFollowing his research on general relativity, Einstein entered into a series of attempts to generalize his geometric theory of gravitation to include electromagnetism as another aspect of a single entity. In 1950, he described his \"unified field theory\" in a \"Scientific American\" article titled \"On the Generalized Theory of Gravitation\". Although he continued to be lauded for his work, Einstein became increasingly isolated in his research, and his efforts were ultimately unsuccessful.\nIn his pursuit of a unification of the fundamental forces, Einstein ignored some mainstream developments in physics, most notably the strong and weak nuclear forces, which were not well understood until many years after his death. Mainstream physics, in turn, largely ignored Einstein's approaches to unification. Einstein's dream of unifying other laws of physics with gravity motivates modern quests for a theory of everything and in particular string theory, where geometrical fields emerge in a unified quantum-mechanical setting.\n\nEinstein conducted other investigations that were unsuccessful and abandoned. These pertain to force, superconductivity, and other research.\n\nIn addition to longtime collaborators Leopold Infeld, Nathan Rosen, Peter Bergmann and others, Einstein also had some one-shot collaborations with various scientists.\n\nEinstein and De Haas demonstrated that magnetization is due to the motion of electrons, nowadays known to be the spin. In order to show this, they reversed the magnetization in an iron bar suspended on a torsion pendulum. They confirmed that this leads the bar to rotate, because the electron's angular momentum changes as the magnetization changes. This experiment needed to be sensitive, because the angular momentum associated with electrons is small, but it definitively established that electron motion of some kind is responsible for magnetization.\n\nEinstein suggested to Erwin Schrödinger that he might be able to reproduce the statistics of a Bose–Einstein gas by considering a box. Then to each possible quantum motion of a particle in a box associate an independent harmonic oscillator. Quantizing these oscillators, each level will have an integer occupation number, which will be the number of particles in it.\n\nThis formulation is a form of second quantization, but it predates modern quantum mechanics. Erwin Schrödinger applied this to derive the thermodynamic properties of a semiclassical ideal gas. Schrödinger urged Einstein to add his name as co-author, although Einstein declined the invitation.\n\nIn 1926, Einstein and his former student Leó Szilárd co-invented (and in 1930, patented) the Einstein refrigerator. This absorption refrigerator was then revolutionary for having no moving parts and using only heat as an input. On 11 November 1930, was awarded to Einstein and Leó Szilárd for the refrigerator. Their invention was not immediately put into commercial production, and the most promising of their patents were acquired by the Swedish company Electrolux.\n\nWhile traveling, Einstein wrote daily to his wife Elsa and adopted stepdaughters Margot and Ilse. The letters were included in the papers bequeathed to The Hebrew University. Margot Einstein permitted the personal letters to be made available to the public, but requested that it not be done until twenty years after her death (she died in 1986). Einstein had expressed his interest in the plumbing profession and was made an honorary member of the Plumbers and Steamfitters Union. Barbara Wolff, of The Hebrew University's Albert Einstein Archives, told the BBC that there are about 3,500 pages of private correspondence written between 1912 and 1955.\n\nCorbis, successor to The Roger Richman Agency, licenses the use of his name and associated imagery, as agent for the university.\n\nIn the period before World War II, \"The New Yorker\" published a vignette in their \"The Talk of the Town\" feature saying that Einstein was so well known in America that he would be stopped on the street by people wanting him to explain \"that theory\". He finally figured out a way to handle the incessant inquiries. He told his inquirers \"Pardon me, sorry! Always I am mistaken for Professor Einstein.\"\n\nEinstein has been the subject of or inspiration for many novels, films, plays, and works of music. He is a favorite model for depictions of mad scientists and absent-minded professors; his expressive face and distinctive hairstyle have been widely copied and exaggerated. \"Time\" magazine's Frederic Golden wrote that Einstein was \"a cartoonist's dream come true\".\n\nMany popular quotations are often misattributed to him.\n\nEinstein received numerous awards and honors and in 1922 he was awarded the 1921 Nobel Prize in Physics \"for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect\". None of the nominations in 1921 met the criteria set by Alfred Nobel, so the 1921 prize was carried forward and awarded to Einstein in 1922.\n\n\n"}
{"id": "614988", "url": "https://en.wikipedia.org/wiki?curid=614988", "title": "Alfréd Rényi", "text": "Alfréd Rényi\n\nAlfréd Rényi (20 March 1921 – 1 February 1970) was a Hungarian mathematician who made contributions in combinatorics, graph theory, number theory but mostly in probability theory.\n\nRényi was born in Budapest to Artur Rényi and Barbara Alexander; his father was a mechanical engineer while his mother was the daughter of a philosopher and literary critic, Bernhard Alexander; his uncle was Franz Alexander a Hungarian-American psychoanalyst and physician.\nHe was prevented from enrolling in university in 1939 due to the anti-Jewish laws then in force, but enrolled at the University of Budapest in 1940 and finished his studies in 1944. At this point he was drafted to forced labour service, escaped, and completed his Ph.D. in 1947 at the University of Szeged, under the advisement of Frigyes Riesz. He married Katalin Schulhof (who used Kató Rényi as her married name), herself a mathematician, in 1946; their daughter Zsuzsanna was born in 1948. After a brief assistant professorship at Budapest, he was appointed Professor Extraordinary at the University of Debrecen in 1949. In 1950, he founded the Mathematics Research Institute of the Hungarian Academy of Sciences, now bearing his name, and directed it until his early death. He also headed the Department of Probability and Mathematical Statistics of the Eötvös Loránd University, from 1952. He was elected a corresponding member (1949), full member (1956) of the Hungarian Academy of Sciences.\n\nRényi proved, using the large sieve, that there is a number formula_1 such that every even number is the sum of a prime number and a number that can be written as the product of at most formula_1 primes. Chen's theorem, a strengthening of this result, shows that the theorem is true for \"K\" = 2, for all sufficiently large even numbers. The case \"K\" = 1 is the still-unproven Goldbach conjecture.\n\nIn information theory, he introduced the spectrum of Rényi entropies of order α, giving an important generalisation of the Shannon entropy and the Kullback–Leibler divergence. The Rényi entropies give a spectrum of useful diversity indices, and lead to a spectrum of fractal dimensions. The Rényi–Ulam game is a guessing game where some of the answers may be wrong.\n\nIn probability theory, he is also known for his parking constants, which characterize the solution to the following problem: given a street of given length and cars of constant length parking on a random free position on the street, what is the density of cars when there are no more free positions? The solution to that problem is approximately equal to 74.75979% .\n\nHe wrote 32 joint papers with Paul Erdős, the most well-known of which are his papers introducing the Erdős–Rényi model of random graphs.\n\nRényi, who was addicted to coffee, is the source of the quote: \"A mathematician is a device for turning coffee into theorems\", which is generally ascribed to Erdős. It has been suggested that this sentence was originally formulated in German, where it can be interpreted as a wordplay on the double meaning of the word \"Satz\" (theorem or coffee residue), but it is more likely that the original formulation was in Hungarian.\n\nHe is also famous for having said, \"If I feel unhappy, I do mathematics to become happy. If I am happy, I do mathematics to keep happy.\"\n\nThe Alfréd Rényi Prize, awarded by the Hungarian Academy of Science, was established in his honor.\n\n\n"}
{"id": "33187484", "url": "https://en.wikipedia.org/wiki?curid=33187484", "title": "Baker's technique", "text": "Baker's technique\n\nIn theoretical computer science, Baker's technique is a method for designing polynomial-time approximation schemes (PTASs) for problems on planar graphs. It is named after Brenda Baker, who announced it in a 1983 conference and published it in the \"Journal of the ACM\" in 1994.\n\nThe idea for Baker's technique is to break the graph into layers, such that the problem can be solved optimally on each layer, then combine the solutions from each layer in a reasonable way that will result in a feasible solution. This technique has given PTASs for the following problems: subgraph isomorphism, maximum independent set, minimum vertex cover, minimum dominating set, minimum edge dominating set, maximum triangle matching, and many others.\n\nThe bidimensionality theory of Erik Demaine, Fedor Fomin, Hajiaghayi, and Dimitrios Thilikos and its offshoot \"simplifying decompositions\" (,) generalizes and greatly expands the applicability of Baker's technique\nfor a vast set of problems on planar graphs and more generally graphs excluding a fixed minor, such as bounded genus graphs, as well as to other classes of graphs not closed under taking minors such as the 1-planar graphs.\n\nThe example that we will use to demonstrate Baker's technique is the maximum weight independent set problem.\n\n INDEPENDENT-SET(formula_1,formula_2,formula_3)\n\nNotice that the above algorithm is feasible because each formula_21 is the union of disjoint independent sets.\n\nDynamic programming is used when we compute the maximum-weight independent set for each formula_16. This dynamic program works because each formula_16 is a formula_24-outerplanar graph. Many NP-complete problems can be solved with dynamic programming on formula_24-outerplanar graphs.\n\n"}
{"id": "38731424", "url": "https://en.wikipedia.org/wiki?curid=38731424", "title": "Cantic order-4 hexagonal tiling", "text": "Cantic order-4 hexagonal tiling\n\nIn geometry, the tritetratrigonal tiling or cantic order-4 hexagonal tiling is a uniform tiling of the hyperbolic plane. It has Schläfli symbol of t{(4,4,3)} or h{6,4}.\n\n\n\n"}
{"id": "24104095", "url": "https://en.wikipedia.org/wiki?curid=24104095", "title": "Cartesian product", "text": "Cartesian product\n\nIn set theory (and, usually, in other parts of mathematics), a Cartesian product is a mathematical operation that returns a set (or product set or simply product) from multiple sets. That is, for sets \"A\" and \"B\", the Cartesian product is the set of all ordered pairs where and . Products can be specified using set-builder notation, e.g.\n\nA table can be created by taking the Cartesian product of a set of rows and a set of columns. If the Cartesian product is taken, the cells of the table contain ordered pairs of the form .\n\nMore generally, a Cartesian product of \"n\" sets, also known as an \"n\"-fold Cartesian product, can be represented by an array of \"n\" dimensions, where each element is an \"n\"-tuple. An ordered pair is a 2-tuple or couple.\n\nThe Cartesian product is named after René Descartes, whose formulation of analytic geometry gave rise to the concept, which is further generalized in terms of direct product.\n\nAn illustrative example is the standard 52-card deck. The standard playing card ranks {A, K, Q, J, 10, 9, 8, 7, 6, 5, 4, 3, 2} form a 13-element set. The card suits form a four-element set. The Cartesian product of these sets returns a 52-element set consisting of 52 ordered pairs, which correspond to all 52 possible playing cards. \n\nBoth sets are distinct, even disjoint.\n\nThe main historical example is the Cartesian plane in analytic geometry. In order to represent geometrical shapes in a numerical way and extract numerical information from shapes' numerical representations, René Descartes assigned to each point in the plane a pair of real numbers, called its coordinates. Usually, such a pair's first and second components are called its \"x\" and \"y\" coordinates, respectively (see picture). The set of all such pairs (i.e. the Cartesian product with ℝ denoting the real numbers) is thus assigned to the set of all points in the plane.\n\nA formal definition of the Cartesian product from set-theoretical principles follows from a definition of ordered pair. The most common definition of ordered pairs, the Kuratowski definition, is formula_2. Under this definition, formula_3 is an element of formula_4, and formula_5 is a subset of that set, where formula_6 represents the power set operator. Therefore, the existence of the Cartesian product of any two sets in ZFC follows from the axioms of pairing, union, power set, and specification. Since functions are usually defined as a special case of relations, and relations are usually defined as subsets of the Cartesian product, the definition of the two-set Cartesian product is necessarily prior to most other definitions.\n\nLet \"A\", \"B\", \"C\", and \"D\" be sets.\n\nThe Cartesian product is not commutative,\nbecause the ordered pairs are reversed unless at least one of the following conditions is satisfied:\n\nFor example:\n\nStrictly speaking, the Cartesian product is not associative (unless one of the involved sets is empty).\nIf for example \"A\" = {1}, then (\"A\" × \"A\") × \"A\" = { ((1,1),1) } ≠ { (1,(1,1)) } = \"A\" × (\"A\" × \"A\").\n\nThe Cartesian product behaves nicely with respect to intersections (see leftmost picture).\n\nIn most cases the above statement is not true if we replace intersection with union (see middle picture).\n\nIn fact, we have that:\n\nFor the set difference we also have the following identity:\n\nHere are some rules demonstrating distributivity with other operators (see rightmost picture):\nwhere formula_17 denotes the absolute complement of \"A\".\n\nOther properties related with subsets are:\n\nThe cardinality of a set is the number of elements of the set. For example, defining two sets: } and Both set \"A\" and set \"B\" consist of two elements each. Their Cartesian product, written as , results in a new set which has the following elements:\n\nEach element of \"A\" is paired with each element of \"B\". Each pair makes up one element of the output set.\nThe number of values in each element of the resulting set is equal to the number of sets whose cartesian product is being taken; 2 in this case.\nThe cardinality of the output set is equal to the product of the cardinalities of all the input sets. That is,\nIn this case, |\"A\" × \"B\"| = 4\n\nSimilarly\nand so on.\n\nThe set is infinite if either \"A\" or \"B\" is infinite and the other set is not the empty set.\n\nThe Cartesian product can be generalized to the \"n\"-ary Cartesian product over \"n\" sets \"X\", ..., \"X\" as the set\n\nof \"n\"-tuples. If tuples are defined as nested ordered pairs, it can be identified with . If a tuple is defined as a function on that takes its value at \"i\" to be the \"i\"th element of the tuple, then the Cartesian product \"X\"×...×\"X\" is the set of functions\n\nThe Cartesian square of a set \"X\" is the Cartesian product .\nAn example is the 2-dimensional plane where R is the set of real numbers: R is the set of all points where \"x\" and \"y\" are real numbers (see the Cartesian coordinate system).\n\nThe \"n\"-ary Cartesian power of a set \"X\" can be defined as\n\nAn example of this is , with R again the set of real numbers, and more generally R.\n\nThe \"n\"-ary cartesian power of a set \"X\" is isomorphic to the space of functions from an \"n\"-element set to \"X\". As a special case, the 0-ary cartesian power of \"X\" may be taken to be a singleton set, corresponding to the empty function with codomain \"X\".\n\nIt is possible to define the Cartesian product of an arbitrary (possibly infinite) indexed family of sets. If \"I\" is any index set, and formula_23 is a family of sets indexed by \"I\", then the Cartesian product of the sets in \"X\" is defined to be \n\nthat is, the set of all functions defined on the index set such that the value of the function at a particular index \"i\" is an element of \"X\". Even if each of the \"X\" is nonempty, the Cartesian product may be empty if the axiom of choice (which is equivalent to the statement that every such product is nonempty) is not assumed.\n\nFor each \"j\" in \"I\", the function \ndefined by formula_26 is called the \"j\"th projection map.\n\nCartesian power is a Cartesian product where all the factors \"X\" are the same set \"X\". In this case, \nis the set of all functions from \"I\" to \"X\", and is frequently denoted \"X\". This case is important in the study of cardinal exponentiation. An important special case is when the index set is formula_28, the natural numbers: this Cartesian product is the set of all infinite sequences with the \"i\"th term in its corresponding set \"X\". For example, each element of \ncan be visualized as a vector with countably infinite real number components. This set is frequently denoted formula_30, or formula_31.\n\nIf several sets are being multiplied together, e.g. \"X\", \"X\", \"X\", …, then some authors choose to abbreviate the Cartesian product as simply ×\"X\".\n\nIf \"f\" is a function from \"A\" to \"B\" and \"g\" is a function from \"X\" to \"Y\", their Cartesian product is a function from to with\n\nThis can be extended to tuples and infinite collections of functions.\nThis is different from the standard cartesian product of functions considered as sets.\n\nLet formula_33 be a set and formula_34. Then the \"cylinder\" of formula_35 with respect to formula_33 is the Cartesian product formula_37 of formula_35 and formula_33. \n\nNormally, formula_33 is considered to be the universe of the context and is left away. For example, if formula_35 is a subset of the natural numbers formula_28, then the cylinder of formula_35 is formula_44.\n\nAlthough the Cartesian product is traditionally applied to sets, category theory provides a more general interpretation of the product of mathematical structures. This is distinct from, although related to, the notion of a Cartesian square in category theory, which is a generalization of the fiber product.\n\nExponentiation is the right adjoint of the Cartesian product; thus any category with a Cartesian product (and a final object) is a Cartesian closed category.\n\nIn graph theory the Cartesian product of two graphs \"G\" and \"H\" is the graph denoted by whose vertex set is the (ordinary) Cartesian product and such that two vertices (\"u\",\"v\") and (\"u\"′,\"v\"′) are adjacent in if and only if and \"v\" is adjacent with \"v\"′ in \"H\", \"or\" and \"u\" is adjacent with \"u\"′ in \"G\". The Cartesian product of graphs is not a product in the sense of category theory. Instead, the categorical product is known as the tensor product of graphs.\n\n\n"}
{"id": "2719109", "url": "https://en.wikipedia.org/wiki?curid=2719109", "title": "Channel use", "text": "Channel use\n\nChannel use is a quantity used in signal processing or telecommunication related to symbol rate and channel capacity. Capacity is measured in bits per input symbol into the channel (bits per channel use). If a symbol enters the channel every \"T\" seconds (for every symbol period a symbol is transmitted) the channel capacity in bits per second is \"C/T\". The phrase \"1 bit per channel use\" denotes the transmission of 1 symbol (of duration \"T\") containing 1 data bit.\n\n"}
{"id": "8971904", "url": "https://en.wikipedia.org/wiki?curid=8971904", "title": "Coercive function", "text": "Coercive function\n\nIn mathematics, a coercive function is a function that \"grows rapidly\" at the extremes of the space on which it is defined. Depending on the context\ndifferent exact definitions of this idea are in use.\n\nA vector field \"f\" : R → R is called coercive if\n\nwhere \"formula_2\" denotes the usual dot product and formula_3 denotes the usual Euclidean norm of the vector \"x\".\n\nA coercive vector field is in particular norm-coercive since\nformula_4 for\nformula_5, by\nCauchy Schwarz inequality.\nHowever a norm-coercive mapping\n\"f\" : R → R\nis not necessarily a coercive vector field. For instance\nthe rotation\n\"f\" : R → R, \"f(x) = (-x, x)\"\nby 90° is a norm-coercive mapping which fails to be a coercive vector field since\nformula_6 for every formula_7.\n\nA self-adjoint operator formula_8 where formula_9 is a real Hilbert space, is called coercive if there exists a constant formula_10 such that\n\nfor all formula_12 in formula_13\n\nA bilinear form formula_14 is called coercive if there exists a constant formula_10 such that\n\nfor all formula_12 in formula_13\n\nIt follows from the Riesz representation theorem that any symmetric (defined as:formula_19 for all formula_20 in formula_9), continuous (formula_22 for all formula_20 in formula_9 and some constant formula_25) and coercive bilinear form formula_26 has the representation\n\nfor some self-adjoint operator formula_8 which then turns out to be a coercive operator. Also, given a coercive self-adjoint operator formula_29 the bilinear form formula_26 defined as above is coercive.\n\nIf formula_31 is a coercive operator then it is a coercive mapping (in the sense of coercivity of a vector field, where one has to replace the dot product with the more general inner product). Indeed, formula_32 for big formula_3 (if formula_3 is bounded, then it readily follows); then replacing formula_12 by formula_36 we get that formula_37 is a coercive operator.\nOne can also show that the converse holds true if formula_37 is self-adjoint. The definitions of coercivity for vector fields, operators, and bilinear forms are closely related and compatible.\n\nA mapping\nformula_39 between two normed vectorspaces\nformula_40 and formula_41\nis called norm-coercive iff\n\nMore generally, a function formula_39 between two topological spaces formula_44 and formula_45 is called coercive if for every compact subset formula_46 of formula_45 there exists a compact subset formula_48 of formula_44 such that\n\nThe composition of a bijective proper map followed by a coercive map is coercive.\n\nAn (extended valued) function\nformula_51\nis called coercive iff\nA realvalued coercive function formula_53\nis in particular norm-coercive. However a norm-coercive function\nformula_53 is not necessarily coercive.\nFor instance the identity function on formula_55 is norm-coercive\nbut not coercive.\n\nSee also: radially unbounded functions\n\n"}
{"id": "46362605", "url": "https://en.wikipedia.org/wiki?curid=46362605", "title": "Color appearance model", "text": "Color appearance model\n\nA color appearance model (abbreviated CAM) is a mathematical model that seeks to describe the perceptual aspects of human color vision, i.e. viewing conditions under which the appearance of a color does not tally with the corresponding physical measurement of the stimulus source. (In contrast, a color model defines a coordinate space to describe colors, such as the RGB and CMYK color models.)\n\nColor originates in the mind of the observer; “objectively”, there is only the spectral power distribution of the light that meets the eye. In this sense, \"any\" color perception is subjective. However, successful attempts have been made to map the spectral power distribution of light to human sensory response in a quantifiable way. In 1931, using psychophysical measurements, the International Commission on Illumination (CIE) created the XYZ color space which successfully models human color vision on this basic sensory level.\n\nHowever, the XYZ color model presupposes specific viewing conditions (such as the retinal locus of stimulation, the luminance level of the light that meets the eye, the background behind the observed object, and the luminance level of the surrounding light). Only if all these conditions stay constant will two identical stimuli with thereby identical XYZ tristimulus values create an identical color \"appearance\" for a human observer. If some conditions change in one case, two identical stimuli with thereby identical XYZ tristimulus values will create \"different\" color \"appearances\" (and vice versa: two different stimuli with thereby different XYZ tristimulus values might create an \"identical\" color \"appearance\").\n\nTherefore, if viewing conditions vary, the XYZ color model is not sufficient, and a color appearance model is required to model human color perception.\n\nThe basic challenge for any color appearance model is that human color perception does not work in terms of XYZ tristimulus values, but in terms of appearance parameters (hue, lightness, brightness, chroma, colorfulness and saturation). So any color appearance model needs to provide transformations (which factor in viewing conditions) from the XYZ tristimulus values to these appearance parameters (at least hue, lightness and chroma).\n\nThis section describes some of the color appearance phenomena that color appearance models try to deal with.\n\nChromatic adaptation describes the ability of human color perception to abstract from the white point (or color temperature) of the illuminating light source when observing a reflective object. For the human eye, a piece of white paper looks white no matter whether the illumination is blueish or yellowish. This is the most basic and most important of all color appearance phenomena, and therefore a chromatic adaptation transform (CAT) that tries to emulate this behavior is a central component of any color appearance model.\n\nThis allows for an easy distinction between simple tristimulus-based color models and color appearance models. A simple tristimulus-based color model ignores the white point of the illuminant when it describes the surface color of an illuminated object; if the white point of the illuminant changes, so does the color of the surface as reported by the simple tristimulus-based color model. In contrast, a color appearance model takes the white point of the illuminant into account (which is why a color appearance model requires this value for its calculations); if the white point of the illuminant changes, the color of the surface as reported by the color appearance model remains the same.\n\nChromatic adaptation is a prime example for the case that two different stimuli with thereby different XYZ tristimulus values create an \"identical\" color \"appearance\". If the color temperature of the illuminating light source changes, so do the spectral power distribution and thereby the XYZ tristimulus values of the light reflected from the white paper; the color \"appearance\", however, stays the same (white).\n\nSeveral effects change the perception of hue by a human observer:\n\n\nSeveral effects change the perception of contrast by a human observer:\n\n\nThere is an effect which changes the perception of colorfulness by a human observer:\n\n\nThere is an effect which changes the perception of brightness by a human observer:\n\n\nSpatial phenomena only affect colors at a specific location of an image, because the human brain interprets this location in a specific contextual way (e.g. as a shadow instead of gray color). These phenomena are also known as optical illusions. Because of their contextuality, they are especially hard to model; color appearance models that try to do this are referred to as image color appearance models (iCAM).\n\nSince the color appearance parameters and color appearance phenomena are numerous and the task is complex, there is no single color appearance model that is universally applied; instead, various models are used.\n\nThis section lists some of the color appearance models in use. The chromatic adaptation transforms for some of these models are listed in LMS color space.\n\nIn 1976, the CIE set out to replace the many existing, incompatible color difference models by a new, universal model for color difference. They tried to achieve this goal by creating a \"perceptually uniform\" color space, i.e. a color space where identical spatial distance between two colors equals identical amount of perceived color difference. Though they succeeded only partially, they thereby created the CIELAB (“L*a*b*”) color space which had all the necessary features to become the first color appearance model. While CIELAB is a very rudimentary color appearance model, it is one of the most widely used because it has become one of the building blocks of color management with ICC profiles. Therefore, it is basically omnipresent in digital imaging.\n\nOne of the limitations of CIELAB is that it does not offer a full-fledged chromatic adaptation in that it performs the von Kries transform method directly in the XYZ color space (often referred to as “wrong von Kries transform”), instead of changing into the LMS color space first for more precise results. ICC profiles circumvent this shortcoming by using the Bradford transformation matrix to the LMS color space (which had first appeared in the LLAB color appearance model) in conjunction with CIELAB.\n\nThe Nayatani et al. color appearance model focuses on illumination engineering and the color rendering properties of light sources.\n\nThe Hunt color appearance model focuses on color image reproduction (its creator worked in the Kodak Research Laboratories). Development already started in the 1980s and by 1995 the model had become very complex (including features no other color appearance model offers, such as incorporating rod cell responses) and allowed to predict a wide range of visual phenomena. It had a very significant impact on CIECAM02, but because of its complexity the Hunt model itself is difficult to use.\n\nRLAB tries to improve upon the significant limitations of CIELAB with a focus on image reproduction. It performs well for this task and is simple to use, but not comprehensive enough for other applications.\n\nLLAB is similar to RLAB, also tries to stay simple, but additionally tries to be more comprehensive than RLAB. In the end, it traded some simplicity for comprehensiveness, but was still not fully comprehensive. Since CIECAM97s was published soon thereafter, LLAB never gained widespread usage.\n\nAfter starting the evolution of color appearance models with CIELAB, in 1997, the CIE wanted to follow up itself with a comprehensive color appearance model. The result was CIECAM97s, which was comprehensive, but also complex and partly difficult to use. It gained widespread acceptance as a standard color appearance model until CIECAM02 was published.\n\nEbner and Fairchild addressed the issue of non-constant lines of hue in their color space dubbed \"IPT\". The IPT color space converts D65-adapted XYZ data (XD65, YD65, ZD65) to long-medium-short cone response data (LMS) using an adapted form of the Hunt-Pointer-Estevez matrix (MHPE(D65)).\n\nThe IPT color appearance model excels at providing a formulation for hue where a constant hue value equals a constant perceived hue independent of the values of lightness and chroma (which is the general ideal for any color appearance model, but hard to achieve). It is therefore well-suited for gamut mapping implementations.\n\nITU-R BT.2100 includes a color space called \"ICtCp\", which improves the original IPT by exploring higher dynamic\nrange and larger colour gamuts.\n\nAfter the success of CIECAM97s, the CIE developed CIECAM02 as its successor and published it in 2002. It performs better and is simpler at the same time. Apart from the rudimentary CIELAB model, CIECAM02 comes closest to an internationally agreed upon “standard” for a (comprehensive) color appearance model.\n\niCAM06 is an image color appearance model. As such, it does not treat each pixel of an image independently, but in the context of the complete image. This allows it to incorporate spatial color appearance parameters like contrast, which makes it well-suited for HDR images. It is also a first step to deal with spatial appearance phenomena.\n"}
{"id": "5491838", "url": "https://en.wikipedia.org/wiki?curid=5491838", "title": "Computational visualistics", "text": "Computational visualistics\n\nThe term Computational visualistics is used for addressing the whole range of investigating pictures scientifically \"in\" the computer.\n\nImages take a rather prominent place in contemporary life in the western societies. Together with language, they have been connected to human culture from the very beginning. For about one century – after several millennia of written word's dominance – their part is increasing again remarkably. Steps toward a general science of images, which we may call 'general visualistics' in analogy to general linguistics, have only been taken recently. So far, a unique scientific basis for circumscribing and describing the heterogeneous phenomenon \"image\" in an interpersonally verifiable manner hasstill been missing while distinct aspects falling in the domain of visualistics have predominantly been dealt with in several other disciplines, among them in particular philosophy, psychology, and art history. Last (though not least), important contributions to certain aspects of a new science of images have come from computer science.\n\nIn computer science, too, considering pictures evolved originally along several more or less independent questions, which lead to proper sub-disciplines: computer graphics is certainly the most \"visible\" among them. Only just recently, the effort has been increased to finally form a unique and partially autonomous branch of computer science dedicated to images in general. In analogy to computational linguistics, the artificial expression \"computational visualistics\" is used for addressing the whole range of investigating scientifically pictures \"in\" the computer.\n\nFor a science of images within computer science, the abstract data type »image« (or perhaps several such types) stands in the center of interest together with the potential implementations (cf. Schirra 2005). There \nare three main groups of algorithms for that data type to be considered in computational visualistics:\n\nIn the field called image processing, the focus of attention is formed by the operations that take (at least) one picture (and potentially several secondary parameters that are not images) and relate it to another picture. With these operations, we can define algorithms for improving the quality of images (e.g., contrast reinforcement), and procedures for extracting certain parts of an image (e.g., edge finding) or for stamping out pictorial patterns following a particular Gestalt criterion (e.g., blue screen technique). Compression algorithms for the efficient storing or transmitting of pictorial data also belong into this field.\n\nTwo disciplines share the operations transforming images into non-pictorial data items. The field of pattern recognition is actually not restricted to pictures. But it has performed important precursory work for computational visualistics since the early 1950s in those areas that essentially classify information in given images: the identification of simple geometric Gestalts (e.g., \"circular region\"), the classification of letters (recognition of handwriting), the \"seeing\" of spatial objects in the images or even the association of stylistic attributes of the representation. That is, the images are to be associated with instances of a non-pictorial data type forming a description of some of their aspects. The neighboring field of computer vision is the part of AI (artificial intelligence) in which computer scientists try to teach – loosely speaking – computers the ability of visual perception. Therefore, a problem rather belongs to computer vision to the degree to which its goal is \"semantic\", i.e., the result approximates the human seeing of objects in a picture.\n\nThe investigation of possibilities gained by the operations that result in instances of the data type »image« but take as a starting point instances of non-pictorial data types is performed in particular in computer graphics and information visualization. The former deals with images in the closer sense, i.e., those pictures showing spatial configurations of objects (in the colloquial meaning of 'object') in a more or less naturalistic representation like, e.g., in virtual architecture. The starting point of the picture-generating algorithms in computer graphics is usually a data type that allows us to describe the geometry in three dimensions and the lighting of the scene to be depicted together with the important optical properties of the surfaces considered. Scientists in information visualization are interested in presenting pictorially any other data type, in particular those that consist of non-visual components in a \"space\" of states: in order to do so, a convention of visual presentation has firstly to be determined – e.g., a code of colors or certain icons. The well-known fractal images (e.g., of the Mandelbrot set) form a borderline case of information visualization since an abstract mathematical property has been visualized.\n\nThe subject of computational visualistics was introduced at the University of Magdeburg, Germany, in the fall of 1996. It was initiated by Thomas Strothotte, Prof. for computer graphics in Magdeburg and largely supported by Jörg Schirra together with a whole team of interdisciplinary researchers from the social and technical sciences as well as from medicine.\nThis five-year diploma programme has computer science courses as its core: students learn about digital methods and electronic tools for solving picture-related problems. The technological areas of endeavour are complemented by courses on pictures in the humanities. In\naddition to learning about the traditional (i.e. not computerized) contexts of using pictures, students intensively practice their communicative skills. As the third component of the program, an application subject such as biology and medicine gives students an early opportunity to apply their knowledge in that they learn the skills needed for co-operating with clients and experts in other fields where digital image data are essential, e.g. microscopy and radiologic image data in biology and medicine. Bachelor and Master's programmes have been introduced in 2006. \n\nThe expression 'computational visualistics' is also used for a similar degree programme of the University at Koblenz-Landau.\n\n\n"}
{"id": "15718248", "url": "https://en.wikipedia.org/wiki?curid=15718248", "title": "Conference on Implementation and Application of Automata", "text": "Conference on Implementation and Application of Automata\n\nCIAA, the International Conference on Implementation and Application of Automata is an annual academic conference in the field of computer science. \nIts purpose is to bring together members of the academic, research, and industrial community who have an interest in the theory, implementation, and application of automata and related structures. There, the conference concerns research on all aspects of implementation and application of automata and related structures, including theoretical aspects. In 2000, the conference grew out of the Workshop on Implementation of Automata (WIA).\n\nLike most theoretical computer science conferences its contributions are strongly peer-reviewed; the articles appear in proceedings published in Springer Lecture Notes in Computer Science. Extended versions of selected papers of each year's conference alternatingly appear in the journals Theoretical Computer Science and International Journal of Foundations of Computer Science. Every year a best paper award is presented.\n\nSince the focus of the conference is on applied theory, \ncontributions usually come from a widespread range of application domains. \nTypical topics of the conference include, among others, the following, \nas they relate to automata:\n\n\nThe CIAA conference series was founded by Sheng Yu and Derick Wood.\nSince 2013, the Steering committee is chaired by Kai Salomaa.\n\n\n"}
{"id": "1793003", "url": "https://en.wikipedia.org/wiki?curid=1793003", "title": "Copula (probability theory)", "text": "Copula (probability theory)\n\nIn probability theory and statistics, a copula is a multivariate probability distribution for which the marginal probability distribution of each variable is uniform. Copulas are used to describe the dependence between random variables. Their name comes from the Latin for \"link\" or \"tie\", similar but unrelated to grammatical copulas in linguistics. Copulas have been used widely in quantitative finance to model and minimize tail risk and portfolio optimization applications.\n\nSklar's theorem states that any multivariate joint distribution can be written in terms of univariate marginal-distribution functions and a copula which describes the dependence structure between the variables.\n\nCopulas are popular in high-dimensional statistical applications as they allow one to easily model and estimate the distribution of random vectors by estimating marginals and copulae separately. There are many parametric copula families available, which usually have parameters that control the strength of dependence. Some popular parametric copula models are outlined below.\n\nTwo-dimensional copulas are known in some other areas of mathematics under the name \"permutons\" and \"doubly-stochastic measures\".\n\nConsider a random vector formula_1. Suppose its marginals are continuous, i.e. the marginal CDFs formula_2 are continuous functions. By applying the probability integral transform to each component, the random vector\nhas uniformly distributed marginals.\n\nThe copula of formula_1 is defined as the joint cumulative distribution function of formula_5:\n\nThe copula \"C\" contains all information on the dependence structure between the components of formula_1 whereas the marginal cumulative distribution functions formula_8 contain all information on the marginal distributions.\n\nThe importance of the above is that the reverse of these steps can be used to generate pseudo-random samples from general classes of multivariate probability distributions. That is, given a procedure to generate a sample formula_5 from the copula distribution, the required sample can be constructed as\nThe inverses formula_11 are unproblematic as the formula_8 were assumed to be continuous. The above formula for the copula function can be rewritten to correspond to this as:\n\nIn probabilistic terms, formula_14 is a \"d\"-dimensional copula if \"C\" is a joint cumulative distribution function of a \"d\"-dimensional random vector on the unit cube formula_15 with uniform marginals.\n\nIn analytic terms, formula_14 is a \"d\"-dimensional copula if\n\nFor instance, in the bivariate case, formula_22 is a bivariate copula if formula_23, formula_24 and formula_25 for all formula_26 and formula_27.\n\nSklar's theorem, named after Abe Sklar, provides the theoretical foundation for the application of copulas.\nSklar's theorem states that every multivariate cumulative distribution function\nof a random vector formula_1 can be expressed in terms of its marginals formula_30 and\na copula formula_31. Indeed:\n\nIn case that the multivariate distribution has a density formula_33, and this is available, it holds further that\nwhere formula_35 is the density of the copula.\n\nThe theorem also states that, given formula_36, the copula is unique on formula_37, which is the cartesian product of the ranges of the marginal cdf's. This implies that the copula is unique if the marginals formula_8 are continuous.\n\nThe converse is also true: given a copula formula_39 and margins formula_40 then formula_41 defines a \"d\"-dimensional cumulative distribution function.\n\nThe Fréchet–Hoeffding Theorem (after Maurice René Fréchet and Wassily Hoeffding) states that for any Copula formula_14 and any formula_43 the following bounds hold:\nThe function W is called lower Fréchet–Hoeffding bound and is defined as\nThe function M is called upper Fréchet–Hoeffding bound and is defined as\n\nThe upper bound is sharp: \"M\" is always a copula, it corresponds to comonotone random variables.\n\nThe lower bound is point-wise sharp, in the sense that for fixed u, there is a copula formula_47 such that formula_48. However, \"W\" is a copula only in two dimensions, in which case it corresponds to countermonotonic random variables.\n\nIn two dimensions, i.e. the bivariate case, the Fréchet–Hoeffding Theorem states\n\nSeveral families of copulas have been described.\n\nThe Gaussian copula is a distribution over the unit cube formula_15. It is constructed from a multivariate normal distribution over formula_51 by using the probability integral transform.\n\nFor a given correlation matrix formula_52, the Gaussian copula with parameter matrix formula_53 can be written as\nwhere formula_55 is the inverse cumulative distribution function of a standard normal and formula_56 is the joint cumulative distribution function of a multivariate normal distribution with mean vector zero and covariance matrix equal to the correlation matrix formula_53. While there is no simple analytical formula for the copula function, formula_58, it can be upper or lower bounded, and approximated using numerical integration. The density can be written as\nwhere formula_60 is the identity matrix.\n\nArchimedean copulas are an associative class of copulas. Most common Archimedean copulas admit an explicit formula, something not possible for instance for the Gaussian copula.\nIn practice, Archimedean copulas are popular because they allow modeling dependence in arbitrarily high dimensions with only one parameter, governing the strength of dependence.\n\nA copula C is called Archimedean if it admits the representation\n\nwhere formula_62 is a continuous, strictly decreasing and convex function such that formula_63. formula_64 is a parameter within some parameter space formula_65. formula_66 is the so-called generator function and formula_67 is its pseudo-inverse defined by\n\nMoreover, the above formula for C yields a copula for formula_69 if and only if formula_69 is d-monotone on formula_71.\nThat is, if it is formula_72 times differentiable and the derivatives satisfy\n\nfor all formula_74 and formula_75 and formula_76 is nonincreasing and convex.\n\nThe following tables highlight the most prominent bivariate Archimedean copulas, with their corresponding generator. Note that not all of them are completely monotone, i.e. \"d\"-monotone for all formula_77 or \"d\"-monotone for certain formula_78 only.\n\nIn statistical applications, many problems can be formulated in the following way. One is interested in the expectation of a response function formula_79 applied to some random vector formula_80. If we denote the cdf of this random vector with formula_36, the quantity of interest can thus be written as\n\nIf formula_36 is given by a copula model, i.e.,\n\nthis expectation can be rewritten as\n\nIn case the copula C is absolutely continuous, i.e. C has a density c, this equation can be written as\n\nand if each marginal distribution has the density formula_87 it holds further that\n\nIf copula and margins are known (or if they have been estimated), this expectation can be approximated through the following Monte Carlo algorithm:\n\nWhen studying multivariate data, one might want to investigate the underlying copula. Suppose we have observations\nfrom a random vector formula_1 with continuous margins. The corresponding \"true\" copula observations would be\nHowever, the marginal distribution functions formula_8 are usually not known. Therefore, one can construct pseudo copula observations by using the empirical distribution functions\ninstead. Then, the pseudo copula observations are defined as\nThe corresponding empirical copula is then defined as\nThe components of the pseudo copula samples can also be written as formula_101, where formula_102 is the rank of the observation formula_103:\nTherefore, the empirical copula can be seen as the empirical distribution of the rank transformed data.\n\nIn quantitative finance copulas are applied to risk management, to portfolio management and optimization, and to derivatives pricing.\n\nFor the former, copulas are used to perform stress-tests and robustness checks that are especially important during \"downside/crisis/panic regimes\" where extreme downside events may occur (e.g., the global financial crisis of 2007–2008). The formula was also adapted for financial markets and was used to estimate the probability distribution of losses on pools of loans or bonds.\n\nDuring a downside regime, a large number of investors who have held positions in riskier assets such as equities or real estate may seek refuge in 'safer' investments such as cash or bonds. This is also known as a flight-to-quality effect and investors tend to exit their positions in riskier assets in large numbers in a short period of time. As a result, during downside regimes, correlations across equities are greater on the downside as opposed to the upside and this may have disastrous effects on the economy. For example, anecdotally, we often read financial news headlines reporting the loss of hundreds of millions of dollars on the stock exchange in a single day; however, we rarely read reports of positive stock market gains of the same magnitude and in the same short time frame.\n\nCopulas aid in analyzing the effects of downside regimes by allowing the modelling of the marginals and dependence structure of a multivariate probability model separately. For example, consider the stock exchange as a market consisting of a large number of traders each operating with his/her own strategies to maximize profits. The individualistic behaviour of each trader can be described by modelling the marginals. However, as all traders operate on the same exchange, each trader's actions have an interaction effect with other traders'. This interaction effect can be described by modelling the dependence structure. Therefore, copulas allow us to analyse the interaction effects which are of particular interest during downside regimes as investors tend to herd their trading behaviour and decisions. (See also agent-based computational economics, where price is treated as an emergent phenomenon, resulting from the interaction of the various market participants, or agents.)\n\nThe users of the formula have been criticized for creating \"evaluation cultures\" that continued to use simple copulæ despite the simple versions being acknowledged as inadequate for that purpose. Thus, previously, scalable copula models for large dimensions only allowed the modelling of elliptical dependence structures (i.e., Gaussian and Student-t copulas) that do not allow for correlation asymmetries where correlations differ on the upside or downside regimes. However, the recent development of vine copulas (also known as pair copulas) enables the flexible modelling of the dependence structure for portfolios of large dimensions.\nThe Clayton canonical vine copula allows for the occurrence of extreme downside events and has been successfully applied in portfolio optimization and risk management applications. The model is able to reduce the effects of extreme downside correlations and produces improved statistical and economic performance compared to scalable elliptical dependence copulas such as the Gaussian and Student-t copula.\n\nOther models developed for risk management applications are panic copulas that are glued with market estimates of the marginal distributions to analyze the effects of panic regimes on the portfolio profit and loss distribution. Panic copulas are created by Monte Carlo simulation, mixed with a re-weighting of the probability of each scenario.\n\nAs regards derivatives pricing, dependence modelling with copula functions is widely used in applications of financial risk assessment and actuarial analysis – for example in the pricing of collateralized debt obligations (CDOs). Some believe the methodology of applying the Gaussian copula to credit derivatives to be one of the reasons behind the global financial crisis of 2008–2009; see .\n\nDespite this perception, there are documented attempts within the financial industry, occurring before the crisis, to address the limitations of the Gaussian copula and of copula functions more generally, specifically the lack of dependence dynamics. The Gaussian copula is lacking as it only allows for an elliptical dependence structure, as dependence is only modeled using the variance-covariance matrix. This methodology is limited such that it does not allow for dependence to evolve as the financial markets exhibit asymmetric dependence, whereby correlations across assets significantly increase during downturns compared to upturns. Therefore, modeling approaches using the Gaussian copula exhibit a poor representation of extreme events. There have been attempts to propose models rectifying some of the copula limitations.\n\nAdditional to CDOs, Copulas have been applied to other asset classes as a flexible tool in analyzing multi-asset derivative products. The first such application outside credit was to use a copula to construct an basket implied volatility surface, taking into account the volatility smile of basket components. Copulas have since gained popularity in pricing and risk management of options on multi-assets in the presence of a volatility smile, in equity-, foreign exchange- and fixed income derivatives.\n\nRecently, copula functions have been successfully applied to the database formulation for the reliability analysis of highway bridges, and to various multivariate simulation studies in civil, mechanical and offshore engineering. Researchers are also trying these functions in field of transportation to understand interaction of individual driver behavior components which in totality shapes up the nature of an entire traffic flow.\n\nCopulas are being used for reliability analysis of complex systems of machine components with competing failure modes.\nCopulas are being used for warranty data analysis in which the tail dependence is analysed \n\nCopulas are used in modelling turbulent partially premixed combustion, which is common in practical combustors.\n\nCopula functions have been successfully applied to the analysis of neuronal dependencies\n\nCopulas have been extensively used in climate- and weather-related research.\n\nCopulas have been used to estimate the solar irradiance variability in spatial networks and temporally for single locations.\n\nLarge synthetic traces of vectors and stationary time series can be generated using empirical copula while preserving the entire dependence structure of small datasets. Such empirical traces are useful in various simulation-based performance studies.\n\n\n"}
{"id": "9938459", "url": "https://en.wikipedia.org/wiki?curid=9938459", "title": "Disjoint union of graphs", "text": "Disjoint union of graphs\n\nIn graph theory, a branch of mathematics, the disjoint union of graphs is an operation that combines two or more graphs to form a larger graph.\nIt is analogous to the disjoint union of sets, and is constructed by making the vertex set of the result be the disjoint union of the vertex sets of the given graphs, and by making the edge set of the result be the disjoint union of the edge sets of the given graphs. Any disjoint union of two or more nonempty graphs is necessarily disconnected.\n\nThe disjoint union is also called the graph sum, and may be represented either by a plus sign or a circled plus sign: If formula_1 and formula_2 are two graphs, then formula_3 or formula_4 denotes their disjoint union.\n\nCertain special classes of graphs may be represented using disjoint union operations. In particular:\nMore generally, every graph is the disjoint union of connected graphs, its connected components.\n\nThe cographs are the graphs that can be constructed from single-vertex graphs by a combination of disjoint union and complement operations.\n"}
{"id": "16336160", "url": "https://en.wikipedia.org/wiki?curid=16336160", "title": "Distributed algorithmic mechanism design", "text": "Distributed algorithmic mechanism design\n\nDistributed algorithmic mechanism design (DAMD) is an extension of algorithmic mechanism design.\n\nDAMD differs from Algorithmic mechanism design since the algorithm is computed in a distributed manner rather than by a central authority. This greatly improves computation time since the burden is shared by all agents within a network.\n\nOne major obstacle in DAMD is ensuring that agents reveal the true costs or preferences related to a given scenario. Often these agents would rather lie in order to improve their own utility.\nDAMD is full of new challenges since one can no longer assume an obedient networking and mechanism infrastructure where rational players control the message paths and mechanism computation.\n\nGame theory and distributed computing both deal with a system with many agents, in which the agents may possibly pursue different goals. However they have different focuses. For instance one of the concerns of distributed computing is to prove the correctness of algorithms that tolerate faulty agents and agents performing actions concurrently. On the other hand, in game theory the focus is on devising a strategy which leads us to an equilibrium in the system.\nNash equilibrium is the most commonly-used notion of equilibrium in game theory. However Nash equilibrium does not deal with faulty or unexpected behavior. A protocol that reaches Nash equilibrium is guaranteed to execute correctly in the face of rational agents, with no agent being able to improve its utility by deviating from the protocol.\n\nThere is no trusted center as there is in AMD. Thus, mechanisms must be implemented by the agents themselves. The solution preference assumption requires that each agent prefers any outcome to no outcome at all: thus, agents have no incentive to disagree on an outcome or cause the algorithm to fail. In other words, as Afek et al. said, “agents cannot gain if the algorithm fails”. As a result, though agents have preferences, they have no incentive to fail the algorithm.\n\nA mechanism is considered to be truthful if the agents gain nothing by lying about \ntheir or other agents' values.\nA good example would be a leader election algorithm that selects a computation server within a network. The algorithm specifies that agents should send their total computational power to each other, after which the most powerful agent is chosen as the leader to complete the task. In this algorithm agents may lie about their true computation power because they are potentially in danger of being tasked with CPU-intensive jobs which will reduce their power to complete local jobs. This can be overcome with the help of truthful mechanisms which, without any a priori knowledge of the existing data and inputs of each agent, cause each agent to respond truthfully to requests.\n\nA well-known truthful mechanism in game theory is the Vickrey auction.\n\nLeader election is a fundamental problem in distributed computing and there are numerous protocols to solve this problem. System agents are assumed to be rational, and therefore prefer having a leader to not having one. The agents may also have different preferences regarding who becomes the leader (an agent may prefer that he himself becomes the leader). Standard protocols may choose leaders based on the lowest or highest ID of system agents. However, since agents have an incentive to lie about their ID in order to improve their utility such protocols are rendered useless in the setting of algorithmic mechanism design. \nA protocol for leader election in the presence of rational agents has been introduced by Ittai et al.:\nThis protocol correctly elects a leader while reaching equilibrium and is truthful since no agent can benefit by lying about its input.\n\n\n"}
{"id": "29218464", "url": "https://en.wikipedia.org/wiki?curid=29218464", "title": "Frobenius manifold", "text": "Frobenius manifold\n\nIn the mathematical field of differential geometry, a Frobenius manifold, introduced by Dubrovin, is a flat Riemannian manifold with a certain compatible multiplicative structure on the tangent space. The concept generalizes the notion of Frobenius algebra to tangent bundles.\n\nFrobenius manifolds occur naturally in the subject of symplectic topology, more specifically quantum cohomology. The broadest definition is in the category of Riemannian supermanifolds. We will limit the discussion here to smooth (real) manifolds. A restriction to complex manifolds is also possible.\n\nLet \"M\" be a smooth manifold. An \"affine flat\" structure on \"M\" is a sheaf \"T\" of vector spaces that pointwisely span \"TM\" the tangent bundle and the tangent bracket of pairs of its sections vanishes.\n\nAs a local example consider the coordinate vectorfields over a chart of \"M\". A manifold admits an affine flat structure if one can glue together such vectorfields for a covering family of charts.\n\nLet further be given a Riemannian metric \"g\" on \"M\". It is compatible to the flat structure if \"g\"(\"X\", \"Y\") is locally constant for all flat vector fields \"X\" and \"Y\".\n\nA Riemannian manifold admits a compatible affine flat structure if and only if its curvature tensor vanishes everywhere.\n\nA family of \"commutative products *\" on \"TM\" is equivalent to a section \"A\" of \"S\"(T\"M\") ⊗ \"TM\" via\n\nWe require in addition the property\n\nTherefore, the composition \"g\"∘\"A\" is a symmetric 3-tensor.\n\nThis implies in particular that a linear Frobenius manifold (\"M\", \"g\", *) with constant product is a Frobenius algebra \"M\".\n\nGiven (\"g\", \"T\", \"A\"), a \"local potential Φ\" is a local smooth function such that\n\nfor all flat vector fields \"X\", \"Y\", and \"Z\".\n\nA \"Frobenius manifold\" (\"M\", \"g\", *) is now a flat Riemannian manifold (\"M\", \"g\") with symmetric 3-tensor \"A\" that admits everywhere a local potential and is associative.\n\nThe associativity of the product * is equivalent to the following quadratic PDE in the local potential \"Φ\"\nwhere Einstein's sum convention is implied, Φ denotes the partial derivative of the function Φ by the coordinate vectorfield ∂/∂\"x\" which are all assumed to be flat. \"g\" are the coefficients of the inverse of the metric.\n\nThe equation is therefore called associativity equation or Witten–Dijkgraaf–Verlinde–Verlinde (WDVV) equation.\n\nBeside Frobenius algebras, examples arise from quantum cohomology. Namely, given a semipositive symplectic manifold (\"M\", \"ω\") then there exists an open neighborhood \"U\" of 0 in its even quantum cohomology QH(\"M\", \"ω\") with Novikov ring over C such that the big quantum product * for \"a\" in \"U\" is analytic. Now \"U\" together with the intersection form \"g\" = <·,·> is a (complex) Frobenius manifold.\n\nThe second large class of examples of Frobenius manifolds come from the singularity theory. Namely, the space of miniversal deformations of an isolated singularity has a Frobenius manifold structure. This Frobenius manifold structure also relates to Kyoji Saito's primitive forms.\n\n 2. Yu.I. Manin, S.A. Merkulov: \"Semisimple Frobenius (super)manifolds and quantum cohomology of P\", Topol. Methods in Nonlinear Analysis 9 (1997), pp. 107–161\n"}
{"id": "255245", "url": "https://en.wikipedia.org/wiki?curid=255245", "title": "Fubini's theorem", "text": "Fubini's theorem\n\nIn mathematical analysis Fubini's theorem, introduced by Guido Fubini in 1907, is a result that gives conditions under which it is possible to compute a double integral using iterated integrals. One may switch the order of integration if the double integral yields a finite answer when the integrand is replaced by its absolute value.\nAs a consequence it allows the order of integration to be changed in iterated integrals.\nFubini's theorem implies that the two repeated integrals of a function of two variables are equal if the function is integrable. Tonelli's theorem, introduced by Leonida Tonelli in 1909, is similar but applies to functions that are non-negative rather than integrable.\n\nThe special case of Fubini's theorem for continuous functions on a product of closed bounded subsets of real vector spaces was known to Euler in the 18th century. extended this to bounded measurable functions on a product of intervals. conjectured that the theorem could be extended to functions that were integrable rather than bounded, and this was proved by . gave a variation of Fubini's theorem that applies to non-negative functions rather than integrable functions.\n\nIf \"X\" and \"Y\" are measure spaces with measures, there are several natural ways to define a product measure on their product.\n\nThe product \"X\"×\"Y\" of measure spaces (in the sense of category theory) has as its measurable sets the σ-algebra generated by the products \"A\"×\"B\" of measurable subsets of \"X\" and \"Y\".\n\nA measure μ on \"X\"×\"Y\" is called a product measure if μ(\"A\"×\"B\")=μ(\"A\")μ(\"B\") for measurable subsets \"A⊂X\" and \"B⊂Y\" and measures µ on \"X\" and µ on \"Y\". In general there may be many different product measures on \"X\"×\"Y\". Fubini's theorem and Tonelli's theorem both need technical conditions to avoid this complication; the most common way is to assume all measure spaces are σ-finite, in which case there is a unique product measure on \"X\"×\"Y\". There is always a unique maximal product measure on \"X\"×\"Y\", where the measure of a measurable set is the inf of the measures of sets containing it that are countable unions of products of measurable sets. The maximal product measure can be constructed by applying Carathéodory's extension theorem to the additive function μ such that μ(\"A\"×\"B\")=μ(\"A\")μ(\"B\") on the ring of sets generated by products of measurable sets. (Carathéodory's extension theorem gives a measure on a measure space that in general contains more measurable sets than the measure space \"X\"×\"Y\", so strictly speaking the measure should be restricted to the σ-algebra generated by the products \"A\"×\"B\" of measurable subsets of \"X\" and \"Y\".)\n\nThe product of two complete measure spaces is not usually complete. For example, the product of the Lebesgue measure on the unit interval \"I\" with itself is not the Lebesgue measure on the square \"I\"×\"I\". There is a variation of Fubini's theorem for complete measures, which uses the completion of the product of measures rather than the uncompleted product.\n\nSuppose \"X\" and \"Y\" are σ-finite measure spaces, and suppose that \"X × Y\" is given the product measure (which is unique as \"X\" and \"Y\" are σ-finite). Fubini's theorem states that if \"f(x,y)\" is \"X × Y\" integrable, meaning that it is measurable and\nthen\n\nThe first two integrals are iterated integrals with respect to two measures, respectively, and the third is an integral with respect to the product measure. The partial integrals formula_4 and formula_5 need not be defined everywhere, but this does not matter as the points where they are not defined form a set of measure 0.\n\nIf the above integral of the absolute value is not finite, then the two iterated integrals may have different values. See below for an illustration of this possibility.\n\nThe condition that \"X\" and \"Y\" are σ-finite is usually harmless because in practice almost all measure spaces one wishes to use Fubini's theorem for are σ-finite.\nFubini's theorem has some rather technical extensions to the case when \"X\" and \"Y\" are not assumed to be σ-finite . The main extra complication in this case is that there may be more than one product measure on \"X\"×\"Y\". Fubini's theorem continues to hold for the maximal product measure, but can fail for other product measures. For example, there is a product measure and a non-negative measurable function \"f\" for which the double integral of |\"f\"| is zero but the two iterated integrals have different values; see the section on counterexamples below for an example of this. Tonelli's theorem and the Fubini–Tonelli theorem (stated below) can fail on non σ-finite spaces even for the maximal product measure.\n\nTonelli's theorem (named after Leonida Tonelli) is a successor of Fubini's theorem. The conclusion of Tonelli's theorem is identical to that of Fubini's theorem, but the assumption that formula_6 has a finite integral is replaced by the assumption that formula_7 is non-negative.\n\nTonelli's theorem states that if (\"X\", \"A\", μ) and (\"Y\", \"B\", ν) are σ-finite measure spaces, while \"f\" from \"X×Y\" to [0,∞] is non-negative and measurable, then\n\nA special case of Tonelli's theorem is in the interchange of the summations, as in formula_9, where formula_10 are non-negative for all \"x\" and \"y\". The crux of the theorem is that the interchange of order of summation holds even if the series diverges. In effect, the only way a change in order of summation can change the sum is when there exist some subsequences that diverge to formula_11 and others diverging to formula_12. With all elements non-negative, this does not happen in the stated example.\n\nWithout the condition that the measure spaces are σ-finite it is possible for all three of these integrals to have different values. \nSome authors give generalizations of Tonelli's theorem to some measure spaces that are not σ-finite but these generalizations often add conditions that immediately reduce the problem to the σ-finite case. For example, one could take the σ-algebra on \"A\"×\"B\" to be that generated by the product of subsets of finite measure, rather than that generated by all products of measurable subsets, though this has the undesirable consequence that the projections from the product to its factors \"A\" and \"B\" are not measurable. Another way is to add the condition that the support of \"f\" is contained in a countable union of products of sets of finite measure. gives some rather technical extensions of Tonelli's theorem to some non σ-finite spaces. None of these generalizations have found any significant applications outside abstract measure theory, largely because almost all measure spaces of practical interest are σ-finite.\n\nCombining Fubini's theorem with Tonelli's theorem gives\nthe Fubini–Tonelli theorem (often just called Fubini's theorem), which states that if \"X\" and \"Y\" are σ-finite measure spaces, and if \"f\" is a measurable function, then\nBesides if any one of the three integrals\nis finite\nthen\n\nThe absolute value of \"f\" in the conditions above can be replaced by either the positive or the negative part of \"f\"; these forms include Tonelli's theorem as a special case as the negative part of a non-negative function is zero and so has finite integral. Informally all these conditions say that the double integral of \"f\" is well defined, though possibly infinite.\n\nThe advantage of the Fubini–Tonelli over Fubini's theorem is that the repeated integrals of the absolute value of |\"f\"| may be easier to study than the double integral. As in Fubini's theorem, the single integrals may fail to be defined on a measure 0 set.\n\nThe versions of Fubini's and Tonelli's theorems above do not apply to integration on the product of the real line R with itself with Lebesgue measure. The problem is that Lebesgue measure on R×R is not the product of Lebesgue measure on R with itself, but rather the completion of this: a product of two complete measure spaces \"X\" and \"Y\" is not in general complete. For this reason one sometimes uses versions of Fubini's theorem for complete measures: roughly speaking one just replaces all measures by their completions. The various versions of Fubini's theorem are similar to the versions above, with the following minor differences:\n\nProofs of the Fubini and Tonelli theorems are necessarily somewhat technical, as they have to use a hypothesis related to σ-finiteness. Most proofs involve building up to the full theorems by proving them for increasingly complicated functions as follows.\n\nThe following examples show how Fubini's theorem and Tonelli's theorem can fail if any of their hypotheses are omitted.\n\nSuppose that \"X\" is the unit interval with the Lebesgue measurable sets and Lebesgue measure, and \"Y\" is the unit interval with all subsets measurable and the counting measure, so that \"Y\" is not σ-finite. If \"f\" is the characteristic function of the diagonal of \"X\"×\"Y\", then integrating \"f\" along \"X\" gives the 0 function on \"Y\", but integrating \"f\" along \"Y\" gives the function 1 on \"X\". So the two iterated integrals are different. This shows that Tonelli's theorem can fail for spaces that are not σ-finite no matter what product measure is chosen. The measures are both decomposable, showing that Tonelli's theorem fails for decomposable measures (which are slightly more general than σ-finite measures).\n\nFubini's theorem holds for spaces even if they are not assumed to be σ-finite provided one uses the maximal product measure.\nIn the example above, for the maximal product measure, the diagonal has infinite measure so the double integral of |\"f\"| is infinite, and Fubini's theorem holds vacuously.\nHowever, if we give \"X\"×\"Y\" the product measure such that the measure of a set is the sum of the Lebesgue measures of its horizontal sections, then the double integral of |\"f\"| is zero, but the two iterated integrals still have different values. This gives an example of a product measure where Fubini's theorem fails.\n\nThis gives an example of two different product measures on the same product of two measure spaces. For products of two σ-finite measure spaces, there is only one product measure.\n\nSuppose that \"X\" is the first uncountable ordinal, with the finite measure where the measurable sets are either countable (with measure 0) or the sets of countable complement (with measure 1). The (non-measurable) subset \"E\" of \"X\"×\"X\" given by pairs (\"x\",\"y\") with \"x\"<\"y\" is countable on every horizontal line and has countable complement on every vertical line. If \"f\" is the characteristic function of \"E\" then the two iterated integrals of \"f\" are defined and have different values 1 and 0. The function \"f\" is not measurable. This shows that Tonelli's theorem can fail for non-measurable functions.\n\nA variation of the example above shows that Fubini's theorem can fail for non-measurable functions even if |\"f\"| is integrable and both repeated integrals are well defined: if we take \"f\" to be 1 on \"E\" and –1 on the complement of \"E\", then |\"f\"| is integrable on the product with integral 1, and both repeated integrals are well defined, but have different values 1 and –1.\n\nAssuming the continuum hypothesis, one can identify \"X\" with the unit interval \"I\", so there is a bounded non-negative function on \"I\"×\"I\" whose two iterated integrals (using Lebesgue measure) are both defined but unequal. This example was found by .\nThe stronger versions of Fubini's theorem on a product of two unit intervals with Lebesgue measure, where the function is no longer assumed to be measurable but merely that the two iterated integrals are well defined and exist, are independent of the standard Zermelo–Fraenkel axioms of set theory. The continuum hypothesis and Martin's axiom both imply that there exists a function on the unit square whose iterated integrals are not equal, while showed that it is consistent with ZFC that a strong Fubini-type theorem for [0, 1] does hold, and whenever the two iterated integrals exist they are equal. See List of statements undecidable in ZFC.\n\nFubini's theorem tells us that (for measurable functions on a product of σ-finite measure spaces) if the integral of the absolute value is finite, then the order of integration does not matter; if we integrate first with respect to \"x\" and then with respect to \"y\", we get the same result as if we integrate first with respect to \"y\" and then with respect to \"x\". The assumption that the integral of the absolute value is finite is \"Lebesgue integrability\", and without it the two repeated integrals can have different values.\n\nA simple example to show that the repeated integrals can be different in general is to take the two measure spaces to be the positive integers, and to take the function \"f\"(\"x\",\"y\") to be 1 if \"x\"=\"y\", −1 if \"x\"=\"y\"+1, and 0 otherwise. Then the two repeated integrals have different values 0 and 1.\n\nAnother example is as follows for the function\nThe iterated integrals\n\nand\n\nhave different values. The corresponding double integral\ndoes not converge absolutely (in other words the integral of the absolute value is not finite):\n\n\n\n"}
{"id": "35195665", "url": "https://en.wikipedia.org/wiki?curid=35195665", "title": "Hurwitz class number", "text": "Hurwitz class number\n\nIn mathematics, the Hurwitz class number \"H\"(\"N\"), introduced by Adolf Hurwitz, is a modification of the class number of positive definite binary quadratic forms of discriminant –\"N\", where forms are weighted by 2/\"g\" for \"g\" the order of their automorphism group, and where \"H\"(0) = –1/12.\n"}
{"id": "33931444", "url": "https://en.wikipedia.org/wiki?curid=33931444", "title": "Interdependence theory", "text": "Interdependence theory\n\nInterdependence theory is a social exchange theory that shows how the rewards and costs associated with interpersonal relationships collaborate with peoples' expectations from them. \nThis theory comes from the idea that closeness is the key to all relationships; that people communicate to become closer to one another. The theory states that there are rewards and costs to any relationship and that people try to maximize the rewards while minimizing the costs.\n\nInterdependence theory was first introduced by Harold Kelley and John Thibaut in 1959 in their book, \"The Social Psychology of Groups\". In their second book, \"Interpersonal Relations: A Theory of Interdependence,\" the theory was completely formulized in 1978.\n\nInterdependence theory stipulates that an ideal relationship is characterized with high levels of rewards and low levels of costs. Rewards are \"exchanged resources that are pleasurable and gratifying,\" while costs are \"exchanged resources that result in a loss or punishment.\" There are different types of rewards and costs discussed in this theory. This theory distinguishes between four types of rewards and costs. These types are as follows: emotional, social, instrumental, and opportunity.\n\nEmotional rewards and costs are the positive and negative feelings, respectively, that are experienced in a relationship. These types of rewards and costs are especially pertinent to close relationships.\n\nSocial rewards and costs are those related with a person's social appearance and the ability to interact in social environments. Social rewards deal with the positive aspect of a person's social appearance and the enjoyable social situations in which one must engage. On the other hand, social costs are those that relate to the negative aspect of a person's social appearance and the uninteresting social situations to which a person must attend.\n\nInstrumental rewards and costs deal with activities and/or tasks in a relationship. Instrumental rewards are those that are obtained when a person's partner is proficient in handling tasks, such as getting all the laundry finished. Instrumental costs are just the opposite; they occur when a person's relationship partner causes unnecessary work or the partner impedes the other's progress in a task, such as one person in a relationship not doing any of the housework.\n\nOpportunity rewards and costs are associated with the opportunities that arise in relationships. Opportunity rewards are those gains that a person is able to receive in their relationship, which they would not be able to receive on their own. Opportunity costs occur when a person must give up something that they normally would not for the sake of the relationship.\n\nWith every relationship there is an outcome. These outcomes are determined by comparing the amount of rewards present in a relationship versus the amount of costs present. \"According to interdependence theory, people mentally account for rewards and costs so they can evaluate the outcome of their relationship as either positive or negative.\" The outcome is determined to be positive when the rewards outweigh the costs in a relationship. Conversely, the outcome is negative when the costs outweigh the rewards.\n\nInterdependence theory also takes into account comparison level. This involves the expectation of the kinds of outcomes a person expects to receive in a relationship. These expectations are 'compared' to a person's past relationships and current observations of others' relationships. \"Satisfaction depends on expectation, which is shaped by prior experience, especially gripping events of the recent past.\" A person will have a high comparison level, if all the relationships that they have been exposed to are happy. Therefore, to determine whether or not someone is in a satisfying relationship, it is necessary to consider both the rewards and costs evident in that relationship as well as that person's own comparison level.\n\nComparison level correlated with the rewards and costs of a relationship determine satisfaction and commitment for a particular person in a relationship. However, there are some situations where people may be committed, but not have a satisfying relationship or they may be satisfied in their relationship, but not committed to it. Thus, the quality of alternatives helps people understand the 'alternatives' they have outside of their current relationship. Alternatives can be any other option rather than the one currently held. When people have good alternatives, they tend to be less committed to their relationships. By contrast, when people have poor alternatives, they tend to be highly committed to their relationships.\n\n"}
{"id": "6267642", "url": "https://en.wikipedia.org/wiki?curid=6267642", "title": "Kalman–Yakubovich–Popov lemma", "text": "Kalman–Yakubovich–Popov lemma\n\nThe Kalman–Yakubovich–Popov lemma is a result in system analysis and control theory which states: Given a number formula_1, two n-vectors B, C and an n x n Hurwitz matrix A, if the pair formula_2 is completely controllable, then a symmetric matrix P and a vector Q satisfying\n\nexist if and only if\nMoreover, the set formula_6 is the unobservable subspace for the pair formula_7.\n\nThe lemma can be seen as a generalization of the Lyapunov equation in stability theory. It establishes a relation between a linear matrix inequality involving the state space constructs A, B, C and a condition in the frequency domain.\n\nThe Kalman–Popov–Yakubovich lemma which was first formulated and proved in 1962 by Vladimir Andreevich Yakubovich where it was stated that for the strict frequency inequality. The case of nonstrict frequency inequality was published in 1963 by Rudolf E. Kalman. In that paper the relation to solvability of the Lur’e equations was also established. Both papers considered scalar-input systems. The constraint on the control dimensionality was removed in 1964 by Gantmakher and Yakubovich and independently by Vasile Mihai Popov. Extensive review of the topic can be found in .\n\nGiven formula_8 with formula_9 for all formula_10 and formula_11 controllable, the following are equivalent: \n\nThe corresponding equivalence for strict inequalities holds even if formula_11 is not controllable. \n"}
{"id": "5842384", "url": "https://en.wikipedia.org/wiki?curid=5842384", "title": "Katětov–Tong insertion theorem", "text": "Katětov–Tong insertion theorem\n\nThe Katětov–Tong insertion theorem is a theorem of point-set topology proved independently by Miroslav Katětov and Hing Tong in the 1950s. \n\nThe theorem states the following:\n\nLet formula_1 be a normal topological space and let formula_2 be functions with g upper semicontinuous, h lower semicontinuous and formula_3. There exists a continuous function formula_4 with formula_5\n\nThis theorem has a number of applications and is the first of many classical insertion theorems. In particular it implies the Tietze extension theorem and consequently Urysohn's lemma, and so the conclusion of the theorem is equivalent to normality.\n"}
{"id": "53039", "url": "https://en.wikipedia.org/wiki?curid=53039", "title": "Key (cryptography)", "text": "Key (cryptography)\n\nIn cryptography, a key is a piece of information (a parameter) that determines the functional output of a cryptographic algorithm. For encryption algorithms, a key specifies the transformation of plaintext into ciphertext, and vice versa for decryption algorithms. Keys also specify transformations in other cryptographic algorithms, such as digital signature schemes and message authentication codes.\n\nIn designing security systems, it is wise to assume that the details of the cryptographic algorithm are already available to the attacker. This is known as Kerckhoffs' principle — \"\"only secrecy of the key provides security\", or, reformulated as Shannon's maxim, \"the enemy knows the system\"\". The history of cryptography provides evidence that it can be difficult to keep the details of a widely used algorithm secret (see security through obscurity). A key is often easier to protect (it's typically a small piece of information) than an encryption algorithm, and easier to change if compromised. Thus, the security of an encryption system in most cases relies on some key being kept secret.\n\nTrying to keep keys secret is one of the most difficult problems in practical cryptography; see key management. An attacker who obtains the key (by, for example, theft, extortion, dumpster diving, assault, torture, or social engineering) can recover the original message from the encrypted data, and issue signatures.\n\nKeys are generated to be used with a given \"suite of algorithms\", called a cryptosystem. Encryption algorithms which use the same key for both encryption and decryption are known as symmetric key algorithms. A newer class of \"public key\" cryptographic algorithms was invented in the 1970s. These asymmetric key algorithms use a pair of keys—or \"keypair\"—a public key and a private one. Public keys are used for encryption or signature verification; private ones decrypt and sign. The design is such that finding out the private key is extremely difficult, even if the corresponding public key is known. As that design involves lengthy computations, a keypair is often used to exchange an on-the-fly symmetric key, which will only be used for the current session. RSA and DSA are two popular public-key cryptosystems; DSA keys can only be used for signing and verifying, not for encryption.\n\nPart of the security brought about by cryptography concerns confidence about who signed a given document, or who replies at the other side of a connection. Assuming that keys are not compromised, that question consists of determining the owner of the relevant public key. To be able to tell a key's owner, public keys are often enriched with attributes such as names, addresses, and similar identifiers. The packed collection of a public key and its attributes can be digitally signed by one or more supporters. In the PKI model, the resulting object is called a certificate and is signed by a certificate authority (CA). In the PGP model, it is still called a \"key\", and is signed by various people who personally verified that the attributes match the subject.\n\nIn both PKI and PGP models, compromised keys can be revoked. Revocation has the side effect of disrupting the relationship between a key's attributes and the subject, which may still be valid. In order to have a possibility to recover from such disruption, signers often use different keys for everyday tasks: Signing with an \"intermediate certificate\" (for PKI) or a \"subkey\" (for PGP) facilitates keeping the principal private key in an offline safe.\n\nDeleting a key on purpose to make the data inaccessible is called crypto-shredding.\n\nFor the one-time pad system the key must be at least as long as the message. In encryption systems that use a cipher algorithm, messages can be much longer than the key. The key must, however, be long enough so that an attacker cannot try all possible combinations.\n\nA key length of 80 bits is generally considered the minimum for strong security with symmetric encryption algorithms. 128-bit keys are commonly used and considered very strong. See the key size article for a more complete discussion.\n\nThe keys used in public key cryptography have some mathematical structure. For example, public keys used in the RSA system are the product of two prime numbers. Thus public key systems require longer key lengths than symmetric systems for an equivalent level of security. 3072 bits is the suggested key length for systems based on factoring and integer discrete logarithms which aim to have security equivalent to a 128 bit symmetric cipher. Elliptic curve cryptography may allow smaller-size keys for equivalent security, but these algorithms have only been known for a relatively short time and current estimates of the difficulty of searching for their keys may not survive. As early as 2004, a message encrypted using a 109-bit key elliptic curve algorithm had been broken by brute force. The current rule of thumb is to use an ECC key twice as long as the symmetric key security level desired. Except for the random one-time pad, the security of these systems has not been proven mathematically , so a theoretical breakthrough could make everything one has encrypted an open book (see P versus NP problem). This is another reason to err on the side of choosing longer keys.\n\nTo prevent a key from being guessed, keys need to be generated truly randomly and contain sufficient entropy. The problem of how to safely generate truly random keys is difficult, and has been addressed in many ways by various cryptographic systems. There is a RFC on generating randomness (RFC 4086, \"Randomness Requirements for Security\"). Some operating systems include tools for \"collecting\" entropy from the timing of unpredictable operations such as disk drive head movements. For the production of small amounts of keying material, ordinary dice provide a good source of high quality randomness.\n\nFor most computer security purposes and for most users, \"key\" is not synonymous with \"password\" (or \"passphrase\"), although a password can in fact be used as a key. The primary practical difference between keys and passwords is that the latter are intended to be generated, read, remembered, and reproduced by a human user (though the user may delegate those tasks to password management software). A key, by contrast, is intended for use by the software that is implementing the cryptographic algorithm, and so human readability etc. is not required. In fact, most users will, in most cases, be unaware of even the existence of the keys being used on their behalf by the security components of their everyday software applications.\n\nIf a password \"is\" used as an encryption key, then in a well-designed crypto system it would not be used as such on its own. This is because passwords tend to be human-readable and,hence, may not be particularly strong. To compensate, a good crypto system will use the \"password-acting-as-key\" not to perform the primary encryption task itself, but rather to act as an input to a key derivation function (KDF). That KDF uses the password as a starting point from which it will then generate the actual secure encryption key itself. Various methods such as adding a salt and key stretching may be used in the generation.\n"}
{"id": "398404", "url": "https://en.wikipedia.org/wiki?curid=398404", "title": "Lucas number", "text": "Lucas number\n\nThe Lucas numbers or Lucas series are an integer sequence named after the mathematician François Édouard Anatole Lucas (1842–91), who studied both that sequence and the closely related Fibonacci numbers. Lucas numbers and Fibonacci numbers form complementary instances of Lucas sequences.\n\nThe Lucas sequence has the same recursive relationship as the Fibonacci sequence, where each term is the sum of the two previous terms, but with different starting values. This produces a sequence where the ratios of successive terms approach the golden ratio, and in fact the terms themselves are roundings of integer powers of the golden ratio. The sequence also has a variety of relationships with the Fibonacci numbers, like the fact that adding any two Fibonacci numbers two terms apart in the Fibonacci sequence results in the Lucas number in between.\n\nSimilar to the Fibonacci numbers, each Lucas number is defined to be the sum of its two immediate previous terms, thereby forming a Fibonacci integer sequence. The first two Lucas numbers are \"L\" = 2 and \"L\" = 1 as opposed to the first two Fibonacci numbers \"F\" = 0 and \"F\" = 1. Though closely related in definition, Lucas and Fibonacci numbers exhibit distinct properties.\n\nThe Lucas numbers may thus be defined as follows:\n\nThe sequence of Lucas numbers is:\n\nAll Fibonacci-like integer sequences appear in shifted form as a row of the Wythoff array; the Fibonacci sequence itself is the first row and the Lucas sequence is the second row. Also like all Fibonacci-like integer sequences, the ratio between two consecutive Lucas numbers converges to the golden ratio.\n\nUsing \"L\" = \"L\" − \"L\", one can extend the Lucas numbers to negative integers to obtain a doubly infinite sequence: \nThe formula for terms with negative indices in this sequence is\n\nThe Lucas numbers are related to the Fibonacci numbers by the identities\n\nTheir closed formula is given as:\n\nwhere formula_16 is the golden ratio. Alternatively, as for formula_17 the magnitude of the term formula_18 is less than 1/2, formula_3 is the closest integer to formula_20 or, equivalently, the integer part of formula_21, also written as formula_22.\n\nCombining the above with Binet's formula,\n\na formula for formula_20 is obtained:\n\nIf \"F\" ≥ 5 is a Fibonacci number then no Lucas number is divisible by \"F\".\n\n\"L\" is congruent to 1 mod \"n\" if \"n\" is prime, but some composite values of \"n\" also have this property. These are the Bruckman-Lucas pseudoprimes.\n\nA Lucas prime is a Lucas number that is prime. The first few Lucas primes are\n\nFor these \"n\"s are\n\nIf \"L\" is prime then \"n\" is either 0, prime, or a power of 2. \"L\" is prime for \"m\" = 1, 2, 3, and 4 and no other known values of \"m\".\n\nLet\n\nbe the generating series of the Lucas numbers. By a direct computation,\n\nwhich can be rearranged as\n\nThe partial fraction decomposition is given by\n\nwhere formula_30 is the golden ratio and formula_31 is its conjugate.\n\nIn the same way as Fibonacci polynomials are derived from the Fibonacci numbers, the Lucas polynomials \"L\"(\"x\") are a polynomial sequence derived from the Lucas numbers.\n\n\n"}
{"id": "3461604", "url": "https://en.wikipedia.org/wiki?curid=3461604", "title": "László Babai", "text": "László Babai\n\nLászló \"Laci\" Babai (born July 20, 1950 in Budapest) is a Hungarian professor of computer science and mathematics at the University of Chicago. His research focuses on computational complexity theory, algorithms, combinatorics, and finite groups, with an emphasis on the interactions between these fields.\n\nIn 1968, Babai won a gold medal at the International Mathematical Olympiad. Babai studied mathematics at Eötvös Loránd University from 1968 to 1973, received a Ph.D. from the Hungarian Academy of Sciences in 1975, and received a D.Sc. from the Hungarian Academy of Sciences in 1984. He held a teaching position at Eötvös Loránd University since 1971; in 1987 he took joint positions as a professor in algebra at Eötvös Loránd and in computer science at the University of Chicago. In 1995, he began a joint appointment in the mathematics department at Chicago and gave up his position at Eötvös Loránd.\n\nHe is the author of over 180 academic papers.\nHis notable accomplishments include the introduction of interactive proof systems, the introduction of the term Las Vegas algorithm, and the introduction of group theoretic methods in graph isomorphism testing. In November 2015, he announced a quasipolynomial time algorithm for the graph isomorphism problem.\n\nHe is editor-in-chief of the refereed online journal \"Theory of Computing\". Babai was also involved in the creation of the Budapest Semesters in Mathematics program and first coined the name.\n\nFrom 10 November to 1 December 2015, Babai gave three lectures on «Graph Isomorphism in Quasipolynomial Time» in the «Combinatorics and Theoretical Computer Science» Seminar at the University of Chicago. He outlined a proof to show that the Graph isomorphism problem can be solved in quasi-polynomial time. A video of the first talk was published on 10 December 2015 and a preprint was uploaded to arXiv.org on 11 December 2015 and a note published on 14 January 2017.\n\nIn 1988, Babai won the Hungarian State Prize, in 1990 he was elected as a corresponding member of the Hungarian Academy of Sciences, and in 1994 he became a full member. In 1999 the Budapest University of Technology and Economics awarded him an honorary doctorate.\n\nIn 1993, Babai was awarded the Gödel Prize together with Shafi Goldwasser, Silvio Micali, Shlomo Moran, and Charles Rackoff, for their papers on interactive proof systems.\n\nIn 2015, he was elected a fellow of the American Academy of Arts and Sciences, and won the Knuth Prize.\n\n\n"}
{"id": "34809998", "url": "https://en.wikipedia.org/wiki?curid=34809998", "title": "László Fuchs", "text": "László Fuchs\n\nLászló Fuchs (born June 24, 1924 in Budapest) is a Hungarian-American mathematician, the Evelyn and John G. Phillips Distinguished Professor Emeritus in Mathematics at Tulane University. He is known for his research and textbooks in group theory and abstract algebra.\n\nFuchs was born on June 24, 1924 in Budapest, into an academic family: his father was a linguist and a member of the Hungarian Academy of Sciences. He earned a bachelor's degree in 1946 and a doctorate in 1947 from Eötvös Loránd University. After teaching high school mathematics for two years, and then holding positions at Eötvös Loránd, the Mathematical Research Institute of the Hungarian Academy of Sciences, and the University of Miami, he joined the Tulane faculty in 1968. At Tulane, Fuchs chaired the mathematics department from 1977 to 1979. He retired in 2004.\n\nFuchs has nearly 100 academic descendants, many of them through his student at Eötvös Loránd, George Grätzer.\nHe was treasurer of the János Bolyai Mathematical Society from 1949 until 1963, and secretary-general of the society from 1963 to 1966.\n\n\nFuchs won the Kossuth Prize in 1953. He is a foreign member of the Hungarian Academy of Sciences.\nTwo conferences were dedicated to him on the occasion of his 70th birthday, and another on his 75th.\n\nAt Tulane University, Fuchs held the W. R. Irby Professorship from 1979 to 1992, and the Evelyn and John G. Phillips Distinguished Professorship from then until his retirement.\n\nIn 2012 he became a fellow of the American Mathematical Society.\n"}
{"id": "51910368", "url": "https://en.wikipedia.org/wiki?curid=51910368", "title": "Marion Scheepers", "text": "Marion Scheepers\n\nMarion Scheepers is a mathematician and a teaching faculty of the Department of Mathematics of Boise State University in Boise, Idaho since 1988.\n\nHe was born in December 1957, in Thabazimbi, South Africa. He completed his Ph.D. thesis entitled “The Meager-Nowhere Dense Game” at the University of Kansas under the supervision of Fred Galvin. His research interests cover set theory and its relatives, game theory, cryptology, elementary number theory and algorithmic phenomena in biology.\n\nPresently, Scheepers is studying biological encryption mechanisms in certain single-cell organisms in collaboration with researchers from the University of Witten-Herdecke in Germany, and the Boise State Department of Biological Sciences. For this study, he has received grant funding from the National Science Foundation.\n\nHe is particularly known for his work on selection principles and on infinite topological and set-theoretic games. He introduced themes that are common to many selection principles and is responsible for the Scheepers diagram.\n"}
{"id": "2139849", "url": "https://en.wikipedia.org/wiki?curid=2139849", "title": "Mass ratio", "text": "Mass ratio\n\nIn aerospace engineering, mass ratio is a measure of the efficiency of a rocket. It describes how much more massive the vehicle is with propellant than without; that is, the ratio of the rocket's \"wet mass\" (vehicle plus contents plus propellant) to its \"dry mass\" (vehicle plus contents). A more efficient rocket design requires less propellant to achieve a given goal, and would therefore have a lower mass ratio; however, for any given efficiency a higher mass ratio typically permits the vehicle to achieve higher delta-v.\n\nThe mass ratio is a useful quantity for back-of-the-envelope rocketry calculations: it is an easy number to derive from either formula_1 or from rocket and propellant mass, and therefore serves as a handy bridge between the two. It is also a useful for getting an impression of the size of a rocket: while two rockets with mass fractions of, say, 92% and 95% may appear similar, the corresponding mass ratios of 12.5 and 20 clearly indicate that the latter system requires much more propellant.\n\nTypical multistage rockets have mass ratios in the range from 8 to 20. The Space Shuttle, for example, has a mass ratio around 16.\n\nThe definition arises naturally from Tsiolkovsky's rocket equation:\n\nwhere\n\nThis equation can be rewritten in the following equivalent form:\n\nThe fraction on the left-hand side of this equation is the rocket's mass ratio by definition.\n\nThis equation indicates that a Δv of formula_4 times the exhaust velocity requires a mass ratio of formula_5. For instance, for a vehicle to achieve a formula_6 of 2.5 times its exhaust velocity would require a mass ratio of formula_7 (approximately 12.2). One could say that a \"velocity ratio\" of formula_4 requires a mass ratio of formula_5.\n\nSutton defines the mass ratio inversely as:\n\nIn this case, the values for mass fraction are always less than 1.\n\n"}
{"id": "54492474", "url": "https://en.wikipedia.org/wiki?curid=54492474", "title": "Newton's minimal resistance problem", "text": "Newton's minimal resistance problem\n\nNewton's Minimal Resistance Problem is a problem of finding a solid of revolution which experiences a minimum resistance when it moves through a homogeneous fluid with constant velocity in the direction of the axis of revolution, named after Isaac Newton, who studied the problem in 1685 and published it in 1687 in his Principia Mathematica. This is the first example of a problem solved in what is now called the calculus of variations, appearing a decade before the brachistochrone problem. Newton published the solution in Principia Mathematica without his derivation and David Gregory was the first person who approached Newton and persuaded him to write an analysis for him. Then the derivation was shared with his students and peers by Gregory.\n\nAccording to I Bernard Cohen, in his Guide to Newton’s Principia, \"The key to Newton’s reasoning was found in the 1880s, when the earl of Portsmouth gave his family’s vast collection of Newton’s scientific and mathematical papers to Cambridge University. Among Newton’s manuscripts they found the draft text of a letter, … in which Newton elaborated his mathematical argument. [This]\nwas never fully understood, however, until the publication of the major manuscript documents by D. T. Whiteside [1974], whose analytical and historical commentary has enabled students of Newton not only to follow fully Newton’s path to discovery and proof, but also Newton’s later (1694) recomputation of the surface of least resistance\".\n\nEventhough Newton's model for the fluid was wrong as per our current understanding, the fluid he had considered finds its application in Hypersonic flow theory as a limiting case.\n\nIn Proposition 34 of Book 2 of the Principia, Newton wrote, \"If in a rare medium, consisting of equal particles freely disposed at equal distances from each other, a globe and a cylinder described on equal diameter move with equal velocities in the direction of the axis of the cylinder, the resistance of the globe will be but half as great as that of the cylinder.\"\n\nFollowing this proposition is a scholium containing the famous condition that the curve which, when rotated about its axis, generates the solid that experiences less resistance than any other solid having a fixed length, and width.\n\nIn modern form, Newton's problem is to minimize the following integral:\n\nwhere formula_2 represents the curve which generates a solid when it is rotated about the x-axis and formula_3.\n\nI is the reduction in resistance caused by the particles impinging upon the sloping surface DNG, formed by rotating the curve, instead of perpendicularly upon the horizontal projection of DNG on the rear disc DA from the direction of motion, in Fig. 1. Note that the front of the solid is the disc BG, the triangles GBC and GBR are not part of it, but are used below by Newton to express the minimum condition.\n\nThis integral is related to the total resistance experienced by the body by the following relation: formula_4\n\nThe problem is to find the curve that generates the solid that experiences less resistance than any other solid having a fixed axial length = L, and a fixed width, H.\n\nSince the solid must taper in the direction of motion, H is the radius of the disc forming the rear surface of the curve rotated about the x-axis. The units are chosen so that the constant of proportionality is unity. Also, note that formula_5, and the integral, which is evaluated between x = 0 and x = L is negative. Let y = h when x = L.\n\nWhen the curve is the horizontal line, DK, so the solid is a cylinder, formula_6, the integral is zero and the resistance of the cylinder is: formula_7, which explains the constant term.\n\nThe simplest way to apply the Euler–Lagrange equation to this problem is to rewrite the resistance as:\n\nSubstituting the integrand formula_10 into the Euler-Lagrange equation\n\nAlthough the curves that satisfy the minimum condition cannot be described by a simple function, y = f(x), they may be plotted using p as a parameter, to obtain the corresponding coordinates (x,y) of the curves. The equation of x as a function of p is obtained from the minimum condition (1), and an equivalent of it was first found by Newton.\n\nDifferentiating: formula_16, and integrating\n\nSince formula_19, when formula_20, and formula_21, when formula_22, the constants formula_23 can be determined in terms of H, h and L. Because y from equation (1) can never be zero or negative, the front surface of any solid satisfying the minimum condition must be a disc, GB.\n\nAs this was the first example of this type of problem, Newton had to invent a completely new method of solution. Also, he went much deeper in his analysis of the problem than simply finding the condition (1).\n\nWhile a solid of least resistance must satisfy (1), the converse is not true. Fig. 2 shows the family of curves that satisfy it for different values of formula_15. As formula_25 increases the radius, Bg = h, of the disc at x = L decreases and the curve becomes steeper.\n\nDirectly before the minimum resistance problem, Newton stated that if on any elliptical or oval figure rotated about its axis, p becomes greater than unity, one with less resistance can be found. This is achieved by replacing the part of the solid that has p > 1 with the frustum of a cone whose vertex angle is a right angle, as shown in Fig. 2 for curve formula_26. This has less resistance than formula_27. Newton does not prove this, but adds that it might have applications in shipbuilding. Whiteside supplies a proof and contends that Newton would have used the same reasoning.\n\nIn Fig. 2, since the solid generated from the curve Dng satisfies the minimum condition and has p < 1 at g, it experiences less resistance than that from any other curve with the same end point g. However, for the curve DνΓ, with p > 1 at end point Γ, this is not the case for although the curve satisfies the minimum condition, the resistance experienced by φγ and γΓ together is less than that by φΓ.\n\nNewton concluded that of all solids that satisfy the minimum resistance condition, the one experiencing the least resistance, DNG in Fig. 2, is the one that has p = 1 at G. This is shown schematically in Fig. 3 where the overall resistance of the solid varies against the radius of the front surface disc, the minimum occurring when h = BG, corresponding to p = 1 at G.\n\nIn the Principia, in Fig. 1 the condition for the minimum resistance solid is translated into a geometric form as follows: draw GR parallel to the tangent at N, so that formula_28, and equation (1) becomes: formula_29\n\nAt G, formula_30, formula_31, and formula_32, so formula_33 which appears in the Principia in the form:\n\nAlthough this appears fairly simple, it has several subtleties that have caused much confusion.\n\nIn Fig 4, assume DNSG is the curve that when rotated about AB generates the solid whose resistance is less than any other such solid with the same heights, AD = H, BG = h and length, AB = L.\n\nFig. 5. shows the infinitesimal region of the curve about N and I in more detail. Although NI, Nj and NJ are really curved, they can be approximated by straight lines provided NH is sufficiently small.\n\nLet HM = y, AM = x, NH = u, and HI = w = dx. Let the tangent at each point on the curve, formula_35. The reduction of the resistance of the sloping ring NI compared to the vertical ring NH rotated about AB is formula_36 (2)\n\nLet the minimum resistance solid be replaced by an identical one, except that the arc between points I and K is shifted by a small distance to the right formula_37, or to the left formula_38, as shown in more detail in Fig. 5. In either case, HI becomes formula_39.\n\nThe resistance of the arcs of the curve DN and SG are unchanged. Also, the resistance of the arc IK is not changed by being shifted, since the slope remains the same along its length. The only change to the overall resistance of DNSG is due to the change to the gradient of arcs NI and KS. The 2 displacements have to be equal for the slope of the arc IK to be unaffected, and the new curve to end at G.\n\nThe new resistance due to particles impinging upon NJ or Nj, rather that NI is:\n\nformula_40 + w.(terms in ascending powers of formula_41 starting with the 2nd).\n\nThe result is a change of resistance of: formula_42 + higher order terms, the resistance being reduced if o > 0 (NJ less resisted than NI).\n\nThis is the original 1685 derivation where he obtains the above result using the series expansion in powers of o. In his 1694 revisit he differentiates (2) with respect to w. He sent details of his later approach to David Gregory, and these are included as an appendix in Motte’s translation of the Principia.\n\nSimilarly, the change in resistance due to particles impinging upon SL or Sl rather that SK is: formula_43 + higher order terms.\n\nThe overall change in the resistance of the complete solid, formula_44 + w.(terms in ascending powers of formula_41 starting with the 2nd).\nFig 6 represents the total resistance of DNJLSG, or DNjlSG as a function of o. Since the original curve DNIKSG has the least resistance, any change o of whatever sign, must result in an increase in the resistance. This is only possible if the coefficient of o in the expansion of formula_46 is zero, so:\n\nformula_47 (2)\n\nIf this was not the case, it would be possible to choose a value of o with a sign that produced a curve DNJLSG, or DNjlSG with less resistance than the original curve, contrary to the initial assumption. The approximation of taking straight lines for the finite arcs, NI and KS becomes exact in the limit as HN and OS approach zero. Also, NM and HM can be taken as equal, as can OT and ST.\n\nHowever, N and S on the original curve are arbitrary points, so for any 2 points anywhere on the curve the above equality must apply. This is only possible if in the limit of any infinitesimal arc HI, anywhere on the curve, the expression,\n\nformula_48 is a constant. (3)\n\nThis has to be the case since, if formula_48 was to vary along the curve, it would be possible to find 2 infinitesimal arcs NI and KS such that (2) was false, and the coefficient of o in the expansion of formula_50 would be non-zero. Then a solid with less resistance could be produced by choosing a suitable value of o.\n\nThis is the reason for the constant term in the minimum condition in (3). As noted above, Newton went further, and claimed that the resistance of the solid is less than that of any other with the same length and width, when the slope at G is equal to unity. Therefore, in this case, the constant in (3) is equal to one quarter of the radius of the front disc of the solid, formula_51.\n"}
{"id": "22867876", "url": "https://en.wikipedia.org/wiki?curid=22867876", "title": "Orthogonal collocation", "text": "Orthogonal collocation\n\nOrthogonal collocation is a method for the numerical solution of partial differential equations. It uses collocation at the zeros of some orthogonal polynomials to transform the partial differential equation (PDE) to a set of ordinary differential equations (ODEs). The ODEs can then be solved by any method. It has been shown that it is usually advantageous to choose the collocation points as the zeros of the corresponding Jacobi polynomial (independent of the PDE system).\n"}
{"id": "9807915", "url": "https://en.wikipedia.org/wiki?curid=9807915", "title": "Percy John Daniell", "text": "Percy John Daniell\n\nPercy John Daniell (9 January 1889 – 25 May 1946) was a pure and applied mathematician. In a series of papers published between 1918 and 1928, he developed and expanded a generalized theory of integration and differentiation, which is today known as the Daniell integral. In the setting of integration, he also worked on results that lead to the Daniell-Kolmogorov extension theorem in the theory of stochastic processes, independently of Andrey Kolmogorov. He was an Invited Speaker of the ICM in 1920 at Strasbourg.\n\nDaniell was born in Valparaiso, Chile. His family returned to England in 1895. Daniell attended King Edward's School, Birmingham and proceeded to Trinity College, Cambridge (where he was the last Senior Wrangler in 1909). At this time Daniell was an applied mathematician/theoretical physicist. For a year he lectured at the University of Liverpool and then he was appointed to the new Rice Institute in Houston, Texas. The Rice Institute had him spend a year at the University of Göttingen studying with Max Born and David Hilbert. Daniell was at Rice from 1914 to 1923 when he returned to England to a chair at the University of Sheffield. During World War II Daniell advised the British Ministry of Supply. The strain of work during the war took a heavy toll on his health. He died on 25 May 1946, after having collapsed at his home a few weeks earlier.\n\nAldrich, J. (2007) \"But you have to remember P.J.Daniell of Sheffield\" Electronic Journ@l for History of Probability and Statistics December 2007.\n\n"}
{"id": "31134176", "url": "https://en.wikipedia.org/wiki?curid=31134176", "title": "Phyre", "text": "Phyre\n\nPhyre and Phyre2 (Protein Homology/AnalogY Recognition Engine; pronounced as 'fire') are web-based services for protein structure prediction that are free for non-commercial use. Phyre is among the most popular methods for protein structure prediction having been cited over 1500 times. Like other remote homology recognition techniques (see protein threading), it is able to regularly generate reliable protein models when other widely used methods such as PSI-BLAST cannot. Phyre2 has been designed (funded by the BBSRC) to ensure a user-friendly interface for users inexpert in protein structure prediction methods.\n\nThe Phyre and Phyre2 servers predict the three-dimensional structure of a protein sequence using the principles and techniques of homology modeling.\nBecause the structure of a protein is more conserved in evolution than its amino acid sequence, a protein sequence of interest (the target) can be modeled with reasonable accuracy on a very distantly related sequence of known structure (the template), provided that the relationship between target and template can be discerned through sequence alignment. Currently the most powerful and accurate methods for detecting and aligning remotely related sequences rely on profiles or hidden Markov models (HMMs). These profiles/HMMs capture the mutational propensity of each position in an amino acid sequence based on observed mutations in related sequences and can be thought of as an 'evolutionary fingerprint' of a particular protein.\n\nTypically, the amino acid sequences of a representative set of all known three-dimensional protein structures is compiled, and these sequences are processed by scanning against a large protein sequence database. The result is a database of profiles or HMMs, one for each known 3D structure. A user sequence of interest is similarly processed to form a profile/HMM. This user profile is then scanned against the database of profiles using profile-profile or HMM-HMM alignment techniques. These alignments can also take into account patterns of predicted or known secondary structure elements and can be scored using various statistical models. See protein structure prediction for more information.\nThe first Phyre server was released in June 2005 and uses a profile-profile alignment algorithm based on each protein's position-specific scoring matrix. The Phyre2 server was publicly released February 2011 as a replacement for the original Phyre server and provides extra functionality over Phyre, a more advanced interface, fully updated fold library and uses the HHpred / HHsearch package for homology detection among other improvements.\n\nAfter pasting a protein amino acid sequence into the Phyre or Phyre2 submission form, a user will typically wait between 30 minutes and several hours (depending on factors such as sequence length, number of homologous sequences and frequency and length of insertions and deletions) for a prediction to complete. An email containing summary information and the predicted structure in PDB format are sent to the user together with a link to a web page of results. The Phyre2 results screen is divided into three main sections, described below.\n\nThe user-submitted protein sequence is first scanned against a large sequence database using PSI-BLAST. The profile generated by PSI-BLAST is then processed by the neural network secondary structure prediction program PsiPred and the protein disorder predictor Disopred. The predicted presence of alpha-helices, beta-strands and disordered regions is shown graphically together with a color-coded confidence bar.\n\nMany proteins contain multiple protein domains. Phyre2 provides a table of template matches color-coded by confidence and indicating the region of the user sequence matched. This can aid in the determination of the domain composition of a protein.\n\nThe main results table in Phyre2 provides confidence estimates, images and links to the three-dimensional predicted models and information derived from either Structural Classification of Proteins database (SCOP) or the Protein Data Bank (PDB) depending on the source of the detected template. For each match a link takes the user to a detailed view of the alignment between the user sequence and the sequence of known three-dimensional structure.\n\nThe detailed alignment view permits a user to examine individual aligned residues, matches between predicted and known secondary structure elements and the ability to toggle information regarding patterns of sequence conservation and secondary structure confidence. In addition Jmol is used to permit interactive 3D viewing of the protein model.\n\nPhyre2 uses a fold library that is updated weekly as new structures are solved. It uses a more up-to-date interface and offers additional functionality over the Phyre server \nas described below.\n\nThe batch processing feature permits users to submit more than one sequence to Phyre2 by uploading a file of sequences in FASTA format. By default, users have a limit of 100 sequences in a batch. This limit can be raised by contacting the administrator.\nBatch jobs are processed in the background on free computing power as it becomes available. Thus, batch jobs will often take longer than individually submitted jobs, but this is necessary to allow a fair distribution of computing resources to all Phyre2 users.\n\nOne to one threading allows you to upload both a sequence you wish modelled AND the template on which to model it. Users sometimes have a protein sequence that they wish to model on a specific template of their choice. This may be for example a newly solved structure that is not in the Phyre2 database or because of some additional biological information that indicates the chosen template would produce a more accurate model than the one(s) automatically chosen by Phyre2.\n\nInstead of predicting the 3D structure of a protein sequence, often users have a solved structure and they are interested in determining if there is a related structure in a genome of interest. In Phyre2 an uploaded protein structure can be converted into a hidden Markov model and then scanned against a set of genomes (more than 20 genomes as of March 2011). This functionality is called \"BackPhyre\" to indicate how Phyre2 is being used in reverse.\n\nSometimes Phyre2 can't detect any confident matches to known structures. However, the fold library database increases by about 40-100 new structures each week. So even though there might be no decent templates this week, there may well be in the coming weeks.\nPhyrealarm allows users to submit a protein sequence to be automatically scanned against new entries added to the fold library every week. If a confident hit is detected, the user is automatically notified by email together with the results of the Phyre2 search. Users can also control the level of alignment coverage and confidence in the match required to trigger an email alert.\n\nPhyre2 is coupled to the 3DLigandSite server for protein binding site prediction. 3DLigandSite has been one of the top performing servers for binding site prediction at the Critical Assessment of Techniques for Protein Structure Prediction (CASP) in (CASP8 and CASP9). Confident models produced by Phyre2 (confidence >90%) are automatically submitted to 3DLigandSite.\n\nThe program memsat_svm is used to predict the presence and topology of any transmembrane helices present in the user protein sequence.\n\nPhyre2 permits users to choose 'Intensive' modelling from the main submission screen. This mode:\n\nApplications of Phyre and Phyre2 include protein structure prediction, function prediction, domain prediction, domain boundary prediction, evolutionary classification of proteins, guiding site-directed mutagenesis and solving protein crystal structures by molecular replacement. In the CASP8 blind protein structure prediction experiment, Phyre_de_novo (the predecessor of Phyre2) was ranked 4th out of 71 automatic structure prediction servers. In CASP9, Phyre2 was ranked 5th on all template-based modelling (TBM) targets and 2nd on the more difficult TBM/FM (free modelling) targets out of the 79 participating servers.\n\nPhyre and Phyre2 are the successors to the 3D-PSSM protein structure prediction system which has over 1400 citations to date. 3D-PSSM was designed and developed by Lawrence Kelley and Bob MacCallum in the Biomolecular modelling Lab at the Cancer Research UK. Phyre and Phyre2 were Lawrence Kelley in the Structural bioinformatics group, Imperial College London. Components of the Phyre and Phyre2 systems were developed by Benjamin Jefferys, Alex Herbert, and Riccardo Bennett-Lovsey. Research and development of both servers was supervised by Michael Sternberg.\n"}
{"id": "23404066", "url": "https://en.wikipedia.org/wiki?curid=23404066", "title": "Prime end", "text": "Prime end\n\nIn mathematics, the prime end compactification is a method to compactify a topological disc (i.e. a simply connected open set in the plane) by adding a circle in an appropriate way.\n\nThe concept of prime ends was introduced by Constantin Carathéodory to describe the boundary behavior of conformal maps in the complex plane in geometric terms. The theory has been generalized to more general open sets. The expository paper of provides a good account of this theory with complete proofs: it also introduces a definition which make sense in any open set and dimension. Also gives an accessible introduction to prime ends in the context of complex dynamical systems.\n\nThe set of prime ends of the domain  is the set of equivalence classes of chains of arcs converging to a point on the boundary of .\n\nIn this way, a point in the boundary may correspond to many points in the prime ends of , and conversely, many points in the boundary may correspond to a point in the prime ends of .\n\nCarathéodory's principal theorem on the correspondence between boundaries under conformal mappings can be expressed as follows:\n\nIf maps the unit disk conformally and one-to-one onto the domain , it induces a one-to-one mapping between the points on the unit circle and the prime ends of .\n\n"}
{"id": "25643591", "url": "https://en.wikipedia.org/wiki?curid=25643591", "title": "Quasiperiodicity", "text": "Quasiperiodicity\n\nQuasiperiodicity is the property of a system that displays irregular periodicity. Periodic behavior is defined as recurring at regular intervals, such as \"every 24 hours\". Quasiperiodic behavior is a pattern of recurrence with a component of unpredictability that does not lend itself to precise measurement. It is different from the mathematical concept of an almost periodic function, which has increasing regularity over multiple periods. The mathematical definition of quasiperiodic function is a completely different concept; the two should not be confused.\n\nIn climatology, oscillations that appear to follow a regular pattern but which do not have a fixed period are called \"quasiperiodic\".\n\nWithin a dynamical system such as the ocean-atmosphere oscillations may occur regularly, when they are forced by a regular external forcing: for example, the familiar winter-summer cycle is forced by variations in sunlight from the (very close to perfectly) periodic motion of the earth around the sun. Or, like the recent ice age cycles, they may be less regular but still locked by external forcing. However, when the system contains the potential for an oscillation, but there is no strong external forcing it to be phase-locked to it, the \"period\" is likely to be irregular.\n\nThe canonical example of quasiperiodicity in climatology is El Niño-Southern Oscillation. In the modern era, it has a \"period\" somewhere between four and twelve years and a peak spectral density around five years.\n\n"}
{"id": "596646", "url": "https://en.wikipedia.org/wiki?curid=596646", "title": "Recommender system", "text": "Recommender system\n\nA recommender system or a recommendation system (sometimes replacing \"system\" with a synonym such as platform or engine) is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item.\n\nRecommender systems are utilized in a variety of areas including movies, music, news, books, research articles, search queries, social tags, and products in general. There are also recommender systems for experts, collaborators, jokes, restaurants, garments, financial services, life insurance, romantic partners (online dating), and Twitter pages.\n\nRecommender systems typically produce a list of recommendations in one of two ways – through collaborative filtering or through content-based filtering (also known as the personality-based approach). Collaborative filtering approaches build a model from a user's past behaviour (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that the user may have an interest in. Content-based filtering approaches utilize a series of discrete characteristics of an item in order to recommend additional items with similar properties. These approaches are often combined (see Hybrid Recommender Systems).\n\nThe differences between collaborative and content-based filtering can be demonstrated by comparing two popular music recommender systems – Last.fm and Pandora Radio.\n\nEach type of system has its strengths and weaknesses. In the above example, Last.fm requires a large amount of information about a user to make accurate recommendations. This is an example of the cold start problem, and is common in collaborative filtering systems. Whereas Pandora needs very little information to start, it is far more limited in scope (for example, it can only make recommendations that are similar to the original seed).\n\nRecommender systems are a useful alternative to search algorithms since they help users discover items they might not have found otherwise. Of note, recommender systems are often implemented using search engines indexing non-traditional data.\n\nRecommender systems were first mentioned in a technical report as a \"digital bookshelf\" in 1990 by Jussi Karlgren at Columbia University, and implemented at scale and worked through in technical reports and publications from 1994 onwards by Jussi Karlgren, then at SICS,\n\nand research groups led by Pattie Maes at MIT, Will Hill at Bellcore, and Paul Resnick, also at MIT\n\nwhose work with GroupLens was awarded the 2010 ACM Software Systems Award.\n\nMontaner provided the first overview of recommender systems from an intelligent agent perspective. Adomavicius provided a new, alternate overview of recommender systems. Herlocker provides an additional overview of evaluation techniques for recommender systems, and Beel et al. discussed the problems of offline evaluations. Beel et al. have also provided literature surveys on available research paper recommender systems and existing challenges.\n\nRecommender systems have been the focus of several granted patents.\n\nOne approach to the design of recommender systems that has wide use is collaborative filtering. Collaborative filtering methods are based on collecting and analyzing a large amount of information on users’ behaviors, activities or preferences and predicting what users will like based on their similarity to other users. A key advantage of the collaborative filtering approach is that it does not rely on machine analyzable content and therefore it is capable of accurately recommending complex items such as movies without requiring an \"understanding\" of the item itself. Many algorithms have been used in measuring user similarity or item similarity in recommender systems. For example, the k-nearest neighbor (k-NN) approach and the Pearson Correlation as first implemented by Allen.\n\nCollaborative filtering is based on the assumption that people who agreed in the past will agree in the future, and that they will like similar kinds of items as they liked in the past.\n\nWhen building a model from a user's behavior, a distinction is often made between explicit and implicit forms of data collection.\n\nExamples of explicit data collection include the following:\n\nExamples of implicit data collection include the following:\n\nThe recommender system compares the collected data to similar and dissimilar data collected from others and calculates a list of recommended items for the user. Several commercial and non-commercial examples are listed in the article on collaborative filtering systems.\n\nOne of the most famous examples of collaborative filtering is item-to-item collaborative filtering (people who buy x also buy y), an algorithm popularized by Amazon.com's recommender system. Other examples include:\nCollaborative filtering approaches often suffer from three problems: cold start, scalability, and sparsity.\n\nA particular type of collaborative filtering algorithm uses matrix factorization, a low-rank matrix approximation technique.\n\nCollaborative filtering methods are classified as memory-based and model based collaborative filtering. A well-known example of memory-based approaches is user-based algorithm and that of model-based approaches is Kernel-Mapping Recommender.\n\nAnother common approach when designing recommender systems is content-based filtering. Content-based filtering methods are based on a description of the item and a profile of the user’s preferences.\n\nIn a content-based recommender system, keywords are used to describe the items and a user profile is built to indicate the type of item this user likes. In other words, these algorithms try to recommend items that are similar to those that a user liked in the past (or is examining in the present). In particular, various candidate items are compared with items previously rated by the user and the best-matching items are recommended. This approach has its roots in information retrieval and information filtering research.\n\nTo abstract the features of the items in the system, an item presentation algorithm is applied. A widely used algorithm is the \ntf–idf representation (also called vector space representation).\n\nTo create a user profile, the system mostly focuses on two types of information:\n\n1. A model of the user's preference.\n\n2. A history of the user's interaction with the recommender system.\n\nBasically, these methods use an item profile (i.e., a set of discrete attributes and features) characterizing the item within the system. The system creates a content-based profile of users based on a weighted vector of item features. The weights denote the importance of each feature to the user and can be computed from individually rated content vectors using a variety of techniques. Simple approaches use the average values of the rated item vector while other sophisticated methods use machine learning techniques such as Bayesian Classifiers, cluster analysis, decision trees, and artificial neural networks in order to estimate the probability that the user is going to like the item.\n\nDirect feedback from a user, usually in the form of a like or dislike button, can be used to assign higher or lower weights on the importance of certain attributes (using Rocchio classification or other similar techniques).\n\nA key issue with content-based filtering is whether the system is able to learn user preferences from users' actions regarding one content source and use them across other content types. When the system is limited to recommending content of the same type as the user is already using, the value from the recommendation system is significantly less than when other content types from other services can be recommended. For example, recommending news articles based on browsing of news is useful, but would be much more useful when music, videos, products, discussions etc. from different services can be recommended based on news browsing.\n\nPandora Radio is an example of a content-based recommender system that plays music with similar characteristics to that of a song provided by the user as an initial seed. There are also a large number of content-based recommender systems aimed at providing movie recommendations, a few such examples include Rotten Tomatoes, Internet Movie Database, Jinni, Rovi Corporation, and Jaman. Document related recommender systems aim at providing document recommendations to knowledge workers. Public health professionals have been studying recommender systems to personalize health education and preventative strategies.\n\nRecent research has demonstrated that a hybrid approach, combining collaborative filtering and content-based filtering could be more effective in some cases. Hybrid approaches can be implemented in several ways: by making content-based and collaborative-based predictions separately and then combining them; by adding content-based capabilities to a collaborative-based approach (and vice versa); or by unifying the approaches into one model (see for a complete review of recommender systems). Several studies empirically compare the performance of the hybrid with the pure collaborative and content-based methods and demonstrate that the hybrid methods can provide more accurate\nrecommendations than pure approaches. These methods can also be used to overcome some of the common problems in recommender systems such as cold start and the sparsity problem.\n\nNetflix is a good example of the use of hybrid recommender systems. The website makes recommendations by comparing the watching and searching habits of similar users (i.e., collaborative filtering) as well as by offering movies that share characteristics with films that a user has rated highly (content-based filtering).\n\nA variety of techniques have been proposed as the basis for recommender systems: collaborative, content-based, knowledge-based, and demographic techniques. Each of these techniques has known shortcomings, such as the well known cold-start problem for collaborative and content-based systems (what to do with new users with few ratings) and the knowledge engineering bottleneck in knowledge-based approaches. A hybrid recommender system is one that combines multiple techniques together to achieve some synergy between them.\n\n\nThe term hybrid recommender system is used here to describe any recommender system that combines multiple recommendation techniques together to produce its output. There is no reason why several different techniques of the same type could not be hybridized, for example, two different content-based recommenders could work together, and a number of projects have investigated this type of hybrid: NewsDude,\nwhich uses both naive Bayes and k-nearest neighbors classifiers in its news recommendations is just one example.\n\nSeven hybridization techniques:\n\nTypically, research on recommender systems is concerned about finding the most accurate recommendation algorithms. However, there are a number of factors that are also important.\n\n\n\nOne growing area of research in the area of recommender systems is mobile recommender systems. With the increasing ubiquity of internet-accessing smart phones, it is now possible to offer personalized, context-sensitive recommendations. This is a particularly difficult area of research as mobile data is more complex than data that recommender systems often have to deal with (it is heterogeneous, noisy, requires spatial and temporal auto-correlation, and has validation and generality problems). Additionally, mobile recommender systems suffer from a transplantation problem – recommendations may not apply in all regions (for instance, it would be unwise to recommend a recipe in an area where all of the ingredients may not be available).\n\nOne example of a mobile recommender system is one that offers potentially profitable driving routes for taxi drivers in a city. This system takes input data in the form of GPS traces of the routes that taxi drivers took while working, which include location (latitude and longitude), time stamps, and operational status (with or without passengers). It uses this data to recommend a list of pickup points along a route, with the goal of optimizing occupancy times and profits. This type of system is obviously location-dependent, and since it must operate on a handheld or embedded device, the computation and energy requirements must remain low.\n\nAnother example of mobile recommendation is what (Bouneffouf et al., 2012) developed for professional users. Using GPS traces of the user and his agenda, it suggests suitable information depending on his situation and interests. The system uses machine learning\ntechniques and reasoning processes in order to dynamically adapt the mobile recommender system to the evolution of the user’s interest. The author called his algorithm hybrid-ε-greedy.\n\nMobile recommendation systems have also been successfully built using the \"Web of Data\" as a source for structured information. A good example of such system is SMARTMUSEUM The system uses semantic modelling, information retrieval, and machine learning\ntechniques in order to recommend content matching user interests, even when presented with sparse or minimal user data.\n\nThere are three factors that could affect the mobile recommender systems and the accuracy of prediction results: the context, the recommendation method and privacy.\n\nThe majority of existing approaches to recommender systems focus on recommending the most relevant content to users using contextual information and do not take into account the risk of disturbing the user in specific situation. However, in many applications, such as recommending personalized content, it is also important to consider the risk of upsetting the user so as not to push recommendations in certain circumstances, for instance, during a professional meeting, early morning, or late at night. Therefore, the performance of the recommender system depends in part on the degree to which it has incorporated the risk into the recommendation process.\n\n\"The risk in recommender systems is the possibility to disturb or to upset the user\nwhich leads to a bad answer of the user\".\n\nIn response to these challenges, the authors in \"DRARS, A Dynamic Risk-Aware Recommender System\" have developed a dynamic risk sensitive recommendation system called DRARS (Dynamic Risk-Aware Recommender System), which models the context-aware recommendation as a bandit problem. This system combines a content-based technique and a contextual bandit algorithm. They have shown that DRARS improves the Upper Confidence Bound (UCB) policy, the currently available best algorithm, by calculating the most optimal exploration value to maintain a trade-off between exploration and exploitation based on the risk level of the current user's situation. The authors conducted experiments in an industrial context with real data and real users and have shown that taking into account the risk level of users' situations significantly increased the performance of the recommender systems.\n\nOne of the events that energized research in recommender systems was the Netflix Prize. From 2006 to 2009, Netflix sponsored a competition, offering a grand prize of $1,000,000 to the team that could take an offered dataset of over 100 million movie ratings and return recommendations that were 10% more accurate than those offered by the company's existing recommender system. This competition energized the search for new and more accurate algorithms. On 21 September 2009, the grand prize of US$1,000,000 was given to the BellKor's Pragmatic Chaos team using tiebreaking rules.\n\nThe most accurate algorithm in 2007 used an ensemble method of 107 different algorithmic approaches, blended into a single prediction:\n\nPredictive accuracy is substantially improved when blending multiple predictors. \"Our experience is that most efforts should be concentrated in deriving substantially different approaches, rather than refining a single technique.\" Consequently, our solution is an ensemble of many methods.\n\nMany benefits accrued to the web due to the Netflix project. Some teams have taken their technology and applied it to other markets. Some members from the team that finished second place founded Gravity R&D, a recommendation engine that's active in the RecSys community. 4-Tell, Inc. created a Netflix project–derived solution for ecommerce websites.\n\nA second contest was planned, but was ultimately canceled in response to an ongoing lawsuit and concerns from the Federal Trade Commission.\n\nEvaluation is important in assessing the effectiveness of recommendation algorithms. The commonly used metrics are the mean squared error and root mean squared error, the latter having been used in the Netflix Prize. The information retrieval metrics such as precision and recall or DCG are useful to assess the quality of a recommendation method. Recently, diversity, novelty, and coverage are also considered as important aspects in evaluation. However, many of the classic evaluation measures are highly criticized. Often, results of so-called offline evaluations do not correlate with actually assessed user-satisfaction. The authors conclude \"we would suggest treating results of offline evaluations [i.e. classic performance measures] with skepticism\".\n\nMulti-criteria recommender systems (MCRS) can be defined as recommender systems that incorporate preference information upon multiple criteria. Instead of developing recommendation techniques based on a single criterion values, the overall preference of user u for the item i, these systems try to predict a rating for unexplored items of u by exploiting preference information on multiple criteria that affect this overall preference value. Several researchers approach MCRS as a multi-criteria decision making (MCDM) problem, and apply MCDM methods and techniques to implement MCRS systems. See this chapter for an extended introduction.\n\nIn some cases, users are allowed to leave text review or feedback on the items. These user-generated text are implicit data for the recommender system because they are potentially rich resource of both feature/aspects of the item, and the user’s evaluation/sentiment to the item. Features extracted from the user-generated reviews are improved meta-data of items, because as they also reflect aspects of the item like meta-data, extracted features are widely concerned by the users. Sentiments extracted from the reviews can be seen as user’s rating scores on the corresponding features. Popular approaches of opinion-based recommender system utilize various techniques including text mining, information retrieval and sentiment analysis (see also Multimodal sentiment analysis).\n\nTo measure the effectiveness of recommender systems, and compare different approaches, three types of evaluations are available: user studies, online evaluations (A/B tests), and offline evaluations. User studies are rather small scale. A few dozens or hundreds of users are presented recommendations created by different recommendation approaches, and then the users judge, which recommendations are best. In A/B tests, recommendations are shown to typically thousands of users of a real product, and the recommender system randomly picks at least two different recommendation approaches to generate recommendations. The effectiveness is measured with implicit measures of effectiveness such as conversion rate or click-through rate. Offline evaluations are based on historic data, e.g. a dataset that contains information about how users previously rated movies. The effectiveness of recommendation approaches is then measured based on how well a recommendation approach can predict the users' ratings in the dataset. While a rating is an explicit expression of whether a user liked a movie, such information is not available in all domains. For instance, in the domain of citation recommender systems, users typically do not rate a citation or recommended article. In such cases, offline evaluations may use implicit measures of effectiveness. For instance, it may be assumed that a recommender system is effective that is able to recommend as many articles as possible that are contained in a research article's reference list. However, this kind of offline evaluations is seen critical by many researchers. For instance, it has been shown that results of offline evaluations have low correlation with results from user studies or A/B tests. A dataset popular for offline evaluation has been shown to contain duplicate data and thus to lead to wrong conclusions in the evaluation of algorithms.\n\nIn recent years there was a growing understanding in the community that lots of previous research had little impact on the practical application of recommender systems. Ekstrand, Konstan, et al. criticize that \"it is currently difficult to reproduce and extend recommender systems research results,” and that evaluations are “not handled consistently\". Konstan and Adomavicius conclude that \"the Recommender Systems research community is facing a crisis where a significant number of papers present results that contribute little to collective knowledge […] often because the research lacks the […] evaluation to be properly judged and, hence, to provide meaningful contributions.\". As a consequence, lots of research about recommender systems can be considered as not reproducible. Hence, operators of recommender systems find little guidance in the current research for answering the question, which recommendation approaches to use in a recommender systems. Said & Bellogín conducted a study of recent papers published in the field, as well as benchmarked some of the most popular frameworks for recommendation and found large inconsistencies in results, even when the same algorithms and data sets were used. Some researchers demonstrated that minor variations in the recommendation algorithms or scenarios led to strong changes in the effectiveness of a recommender system. They conclude that seven actions are necessary to improve the current situation: \"(1) survey other research fields and learn from them, (2) find a common understanding of reproducibility, (3) identify and understand the determinants that affect reproducibility, (4) conduct more comprehensive experiments (5) modernize publication practices, (6) foster the development and use of recommendation frameworks, and (7) establish best-practice guidelines for recommender-systems research.\"\n\nWhile reproducibility has not been considered for a long time in the recommender-system community, this aspects is much more considered recently, with several workshops and conferences focusing on reproducibility in recommender system research.\n\nAnticipatory design is the combination of Internet of Things, User experience design and machine learning. Anticipatory design differs from conventional design, in that within anticipatory design the goal is to simplify the process and minimize difficulty by making decisions on behalf of the users. Examples of services that use various levels of anticipatory designs are: Amazon and Netflix production recommendation which recommend products based on previous behavior, the mobile application Peapod which uses a recommendation engine to allow users to fill their shopping basket based on previous orders, and the Nest thermostat which predicts the perfect room temperature based on user input and time of day.\n\n\n\n\n"}
{"id": "3225574", "url": "https://en.wikipedia.org/wiki?curid=3225574", "title": "Reflection principle", "text": "Reflection principle\n\nIn set theory, a branch of mathematics, a reflection principle says that it is possible to find sets that resemble the class of all sets. There are several different forms of the reflection principle depending on exactly what is meant by \"resemble\". Weak forms of the reflection principle are theorems of ZF set theory due to , while stronger forms can be new and very powerful axioms for set theory. \n\nThe name \"reflection principle\" comes from the fact that properties of the universe of all sets are \"reflected\" down to a smaller set.\n\nA naive version of the reflection principle states that \"for any property of the universe of all sets we can find a set with the same property\". This leads to an immediate contradiction: \nthe universe of all sets contains all sets, but there is no set with the property that it contains all sets. To get useful (and non-contradictory) reflection principles we need to be more careful about what we mean by \"property\" and what properties we allow. \n\nTo find non-contradictory reflection principles we might argue informally as follows. Suppose that we have some collection \"A\" of methods for forming sets (for example, taking powersets, subsets, the axiom of replacement, and so on). We can imagine taking all sets obtained by repeatedly applying all these methods, and form these sets into a class \"V\", which can be thought of as a model of some set theory. But now we can introduce the following new principle for forming sets: \"the collection of all sets obtained from some set by repeatedly applying all methods in the collection \"A\" is also a set\". If we allow this new principle for forming sets, we can now continue past \"V\", and consider the class \"W\" of all sets formed using the principles \"A\" and the new principle. In this class \"W\", \"V\" is just a set, closed under\nall the set-forming operations of \"A\". In other words the universe \"W\" contains a \"set\" \"V\" which resembles \"W\" in that it is closed under all the methods \"A\".\n\nWe can use this informal argument in two ways. We can try to formalize it in (say) ZF set theory; by doing this we obtain some theorems of ZF set theory, called reflection theorems. \nAlternatively we can use this argument to motivate introducing new axioms for set theory.\n\nIn trying to formalize the argument for the reflection principle of the previous section in ZF set theory, it turns out to be necessary to add some conditions about the collection of properties \"A\" (for example, \"A\" might be finite). Doing this produces \nseveral closely related \"reflection theorems\" of ZFC all of which state that we can find a set that is almost a model of ZFC. \n\nOne form of the reflection principle in ZFC says that for any finite set of axioms of ZFC we can find a countable transitive model satisfying these axioms. (In particular this proves that, unless inconsistent, ZFC is not finitely axiomatizable because if it were it would prove the existence of a model of itself, and hence prove its own consistency, contradicting Gödel's second incompleteness theorem.) This version of the reflection theorem is closely related to the Löwenheim–Skolem theorem. \n\nAnother version of the reflection principle says that for any finite number of formulas of ZFC we can find a set \"V\" in the cumulative hierarchy such that all the formulas in the set are absolute for \"V\" (which means very roughly that they hold in \"V\" if and only if they hold in the universe of all sets). So this says that the set \"V\" resembles the universe of all sets, at least as far as the given finite number of formulas is concerned. In particular for any formula of ZFC there is a theorem of ZFC that the formula is logically equivalent to a version of it with all quantifiers relativized to \"V\" See .\n\nIf κ is a strong inaccessible, then there is a closed unbounded subset \"C\" of κ, such that for every α∈\"C\", the identity function from V to V is an elementary embedding.\n\nBernays used a reflection principle as an axiom for one version of set theory (not Von Neumann–Bernays–Gödel set theory, which is a weaker theory). His reflection principle stated roughly that if \"A\" is a class with some property, then one can find a transitive set \"u\" such that \"A∩u\" has the same property when considered as a subset of the \"universe\" \"u\". This is quite a powerful axiom and implies the existence of several of the smaller large cardinals, such as inaccessible cardinals. (Roughly speaking, the class of all ordinals in ZFC is an inaccessible cardinal apart from the fact that it is not a set, and the reflection principle can then be used to show that there is a set which has the same property, in other words which is an inaccessible cardinal.) Unfortunately, this cannot be axiomatized directly in ZFC, and a class theory like MK normally has to be used. The consistency of Bernays's reflection principle is implied by the existence of a ω-Erdős cardinal. \n\nThere are many more powerful reflection principles, which are closely related to the various large cardinal axioms. For almost every known large cardinal axiom there is a known reflection principle that implies it, and conversely all but the most powerful known reflection principles are implied by known large cardinal axioms . An example of this is the wholeness axiom, which implies the existence of super-n-huge cardinals for all finite n and its consistency is implied by an I3 rank-into-rank cardinal.\n\n\n"}
{"id": "6779393", "url": "https://en.wikipedia.org/wiki?curid=6779393", "title": "Riesz potential", "text": "Riesz potential\n\nIn mathematics, the Riesz potential is a potential named after its discoverer, the Hungarian mathematician Marcel Riesz. In a sense, the Riesz potential defines an inverse for a power of the Laplace operator on Euclidean space. They generalize to several variables the Riemann–Liouville integrals of one variable.\n\nIf 0 < α < \"n\", then the Riesz potential \"I\"\"f\" of a locally integrable function \"f\" on R is the function defined by\n\nwhere the constant is given by\n\nThis singular integral is well-defined provided \"f\" decays sufficiently rapidly at infinity, specifically if \"f\" ∈ L(R) with 1 ≤ \"p\" < \"n\"/α. In fact, for any 1 ≤ \"p\" (p>1 is classical, due to Sobolev, while for p=1 see ), the rate of decay of \"f\" and that of \"I\"\"f\" are related in the form of an inequality (the Hardy–Littlewood–Sobolev inequality)\nwhere formula_3 is the vector-valued Riesz transform. More generally, the operators \"I\" are well-defined for complex α such that 0 < Re α < \"n\".\n\nThe Riesz potential can be defined more generally in a weak sense as the convolution\n\nwhere \"K\" is the locally integrable function:\nThe Riesz potential can therefore be defined whenever \"f\" is a compactly supported distribution. In this connection, the Riesz potential of a positive Borel measure μ with compact support is chiefly of interest in potential theory because \"I\"μ is then a (continuous) subharmonic function off the support of μ, and is lower semicontinuous on all of R.\n\nConsideration of the Fourier transform reveals that the Riesz potential is a Fourier multiplier.\nIn fact, one has\nand so, by the convolution theorem,\n\nThe Riesz potentials satisfy the following semigroup property on, for instance, rapidly decreasing continuous functions\nprovided\nFurthermore, if 2 < Re α <\"n\", then\nOne also has, for this class of functions,\n\n\n"}
{"id": "38948504", "url": "https://en.wikipedia.org/wiki?curid=38948504", "title": "Riesz–Markov–Kakutani representation theorem", "text": "Riesz–Markov–Kakutani representation theorem\n\nIn mathematics, the Riesz–Markov–Kakutani representation theorem relates linear functionals on spaces of continuous functions on a locally compact space to measures in measure theory. The theorem is named for who introduced it for continuous functions on the unit interval, who extended the result to some non-compact spaces, and who extended the result to compact Hausdorff spaces.\n\nThere are many closely related variations of the theorem, as the linear functionals can be complex, real, or positive, the space they are defined on may be the unit interval or a compact space or a locally compact space, the continuous functions may be vanishing at infinity or have compact support, and the measures can be Baire measures or regular Borel measures or Radon measures or signed measures or complex measures.\n\nThe following theorem represents positive linear functionals on \"C\"(\"X\"), the space of continuous compactly supported complex-valued functions on a locally compact Hausdorff space \"X\". The Borel sets in the following statement refer to the σ-algebra generated by the \"open\" sets.\n\nA non-negative countably additive Borel measure μ on a locally compact Hausdorff space \"X\" is regular if and only if\n\nholds whenever \"E\" is open or when \"E\" is Borel and \"μ\"(\"E\") < ∞ .\n\nTheorem. Let \"X\" be a locally compact Hausdorff space. For any positive linear functional formula_3 on \"C\"(\"X\"), there is a unique regular Borel measure μ on \"X\" such that\nfor all \"f\" in C(\"X\").\n\nOne approach to measure theory is to start with a Radon measure, defined as a positive linear functional on \"C\"(\"X\"). This is the way adopted by Bourbaki; it does of course assume that \"X\" starts life as a topological space, rather than simply as a set. For locally compact spaces an integration theory is then recovered.\n\nWithout the condition of regularity the Borel measure need not be unique. For example, let \"X\" be the set of ordinals at most equal to the first uncountable ordinal Ω, with the topology generated by \"open intervals\". The linear functional taking a continuous function to its value at Ω corresponds to the regular Borel measure with a point mass at Ω. However it also corresponds to the (non-regular) Borel measure that assigns measure 1 to any Borel set formula_5 if there is closed and unbounded set formula_6 with formula_7, and assigns measure 0 to other Borel sets. (In particular the singleton {Ω} gets measure 0, contrary to the point mass measure.)\n\nHistorical remark: In its original form by F. Riesz (1909) the theorem states that every continuous linear functional \"A\"[\"f\"] over the space \"C\"([0, 1]) of continuous functions in the interval [0,1] can be represented in the form\n\nwhere \"α\"(\"x\") is a function of bounded variation on the interval [0, 1], and the integral is a Riemann–Stieltjes integral. Since there is a one-to-one correspondence between Borel regular measures in the interval and functions of bounded variation (that assigns to each function of bounded variation the corresponding Lebesgue–Stieltjes measure, and the integral with respect to the Lebesgue–Stieltjes measure agrees with the Riemann–Stieltjes integral for continuous functions ), the above stated theorem generalizes the original statement of F. Riesz. (See Gray(1984), for a historical discussion).\n\nThe following theorem, also referred to as the \"Riesz–Markov theorem\", gives a concrete realisation of the topological dual space of \"C\"(\"X\"), the set of continuous functions on \"X\" which vanish at infinity. The Borel sets in the statement of the theorem also refers to the σ-algebra generated by the \"open\" sets.\n\nIf μ is a complex-valued countably additive Borel measure, μ is called regular if the non-negative countably additive measure |μ| is regular as defined above.\n\nTheorem. Let \"X\" be a locally compact Hausdorff space. For any continuous linear functional ψ on \"C\"(\"X\"), there is a unique \"regular\" countably additive complex Borel measure μ on \"X\" such that\nfor all \"f\" in \"C\"(\"X\"). The norm of ψ as a linear functional is the total variation of μ, that is\nFinally, ψ is positive if and only if the measure μ is non-negative.\n\nOne can deduce this statement about linear functionals from the statement about positive linear functionals by first showing that a bounded linear functional can be written as a finite linear combination of positive ones.\n\n"}
{"id": "66819", "url": "https://en.wikipedia.org/wiki?curid=66819", "title": "Root mean square", "text": "Root mean square\n\nIn statistics and its applications, the root mean square (abbreviated RMS or rms) is defined as the square root of the mean square (the arithmetic mean of the squares of a set of numbers).\nThe RMS is also known as the quadratic mean and is a particular case of the generalized mean with exponent 2.\nRMS can also be defined for a continuously varying function in terms of an integral of the squares of the instantaneous values during a cycle.\n\nFor alternating electric current, RMS is equal to the value of the direct current that would produce the same average power dissipation in a resistive load.\n\nIn estimation theory, the root mean square error of an estimator is a measure of the imperfection of the fit of the estimator to the data.\n\nThe RMS value of a set of values (or a continuous-time waveform) is the square root of the arithmetic mean of the squares of the values, or the square of the function that defines the continuous waveform. In physics, the RMS current is the \"value of the direct current that dissipates power in a resistor.\"\n\nIn the case of a set of \"n\" values formula_1, the RMS\n\nThe corresponding formula for a continuous function (or waveform) \"f(t)\" defined over the interval formula_3 is\n\nand the RMS for a function over all time is\n\nThe RMS over all time of a periodic function is equal to the RMS of one period of the function. The RMS value of a continuous function or signal can be approximated by taking the RMS of a sequence of equally spaced samples. Additionally, the RMS value of various waveforms can also be determined without calculus, as shown by Cartwright.\n\nIn the case of the RMS statistic of a random process, the expected value is used instead of the mean.\n\nIf the waveform is a pure sine wave, the relationships between amplitudes (peak-to-peak, peak) and RMS are fixed and known, as they are for any continuous periodic wave. However, this is not true for an arbitrary waveform, which may or may not be periodic or continuous. For a zero-mean sine wave, the relationship between RMS and peak-to-peak amplitude is:\n\nFor other waveforms the relationships are not the same as they are for sine waves. For example, for either a triangular or sawtooth wave\n\nWaveforms made by summing known simple waveforms have an RMS that is the root of the sum of squares of the component RMS values, if the component waveforms are orthogonal (that is, if the average of the product of one simple waveform with another is zero for all pairs other than a waveform times itself).\n\nwhere formula_9 refers to the direct current component of the signal and formula_10 is the alternating current component of the signal.\n\nElectrical engineers often need to know the power, \"P\", dissipated by an electrical resistance, \"R\". It is easy to do the calculation when there is a constant current, \"I\", through the resistance. For a load of \"R\" ohms, power is defined simply as:\n\nHowever, if the current is a time-varying function, \"I(t)\", this formula must be extended to reflect the fact that the current (and thus the instantaneous power) is varying over time. If the function is periodic (such as household AC power), it is still meaningful to discuss the \"average\" power dissipated over time, which is calculated by taking the average power dissipation:\nSo, the RMS value, \"I\", of the function \"I(t)\" is the constant current that yields the same power dissipation as the time-averaged power dissipation of the current \"I(t)\".\n\nAverage power can also be found using the same method that in the case of a time-varying voltage, \"V(t)\", with RMS value \"V\",\n\nThis equation can be used for any periodic waveform, such as a sinusoidal or sawtooth waveform, allowing us to calculate the mean power delivered into a specified load.\n\nBy taking the square root of both these equations and multiplying them together, the power is found to be:\n\nBoth derivations depend on voltage and current being proportional (i.e., the load, \"R\", is purely resistive). Reactive loads (i.e., loads capable of not just dissipating energy but also storing it) are discussed under the topic of AC power.\n\nIn the common case of alternating current when \"I(t)\" is a sinusoidal current, as is approximately true for mains power, the RMS value is easy to calculate from the continuous case equation above. If \"I\" is defined to be the peak current, then:\n\nwhere \"t\" is time and \"ω\" is the angular frequency (\"ω\" = 2π/\"T\", where \"T\" is the period of the wave).\n\nSince \"I\" is a positive constant:\n\nUsing a trigonometric identity to eliminate squaring of trig function:\n\nbut since the interval is a whole number of complete cycles (per definition of RMS), the \"sin\" terms will cancel out, leaving:\n\nA similar analysis leads to the analogous equation for sinusoidal voltage:\n\nWhere \"I\" represents the peak current and \"V\" represents the peak voltage.\n\nBecause of their usefulness in carrying out power calculations, listed voltages for power outlets (e.g., 120 V in the USA, or 230 V in Europe) are almost always quoted in RMS values, and not peak values. Peak values can be calculated from RMS values from the above formula, which implies \"V\" = \"V\" × , assuming the source is a pure sine wave. Thus the peak value of the mains voltage in the USA is about 120 × , or about 170 volts. The peak-to-peak voltage, being double this, is about 340 volts. A similar calculation indicates that the peak mains voltage in Europe is about 325 volts, and the peak-to-peak mains voltage, about 650 volts.\n\nRMS quantities such as electric current are usually calculated over one cycle. However, for some purposes the RMS current over a longer period is required when calculating transmission power losses. The same principle applies, and (for example) a current of 10 amps used for 12 hours each day represents an RMS current of 5 amps in the long term.\n\nThe term \"RMS power\" is sometimes erroneously used in the audio industry as a synonym for \"mean power\" or \"average power\" (it is proportional to the square of the RMS voltage or RMS current in a resistive load). For a discussion of audio power measurements and their shortcomings, see Audio power.\n\nIn the physics of gas molecules, the root-mean-square speed is defined as the square root of the average squared-speed. The RMS speed of an ideal gas is calculated using the following equation:\n\nwhere \"R\" represents the ideal gas constant, 8.314 J/(mol·K), \"T\" is the temperature of the gas in kelvins, and \"M\" is the molar mass of the gas in kilograms per mole. The generally accepted terminology for speed as compared to velocity is that the former is the scalar magnitude of the latter. Therefore, although the average speed is between zero and the RMS speed, the average velocity for a stationary gas is zero.\n\nWhen two data sets—one set from theoretical prediction and the other from actual measurement of some physical variable, for instance—are compared, the RMS of the pairwise differences of the two data sets can serve as a measure how far on average the error is from 0. The mean of the pairwise differences does not measure the variability of the difference, and the variability as indicated by the standard deviation is around the mean instead of 0. Therefore, the RMS of the differences is a meaningful measure of the error.\n\nThe RMS can be computed in the frequency domain, using Parseval's theorem. For a sampled signal formula_20, where formula_21 is the sampling period,\n\nwhere formula_23 and \"N\" is number of samples and FFT coefficients.\n\nIn this case, the RMS computed in the time domain is the same as in the frequency domain:\n\nIf formula_25 is the arithmetic mean and formula_26 is the standard deviation of a population or a waveform then:\n\nFrom this it is clear that the RMS value is always greater than or equal to the average, in that the RMS includes the \"error\" / square deviation as well.\n\nPhysical scientists often use the term \"root mean square\" as a synonym for standard deviation when it can be assumed the input signal has zero mean, i.e., referring to the square root of the mean squared deviation of a signal from a given baseline or fit.\nThis is useful for electrical engineers in calculating the \"AC only\" RMS of a signal. Standard deviation being the root mean square of a signal's variation about the mean, rather than about 0, the DC component is removed (i.e. RMS(signal) = Stdev(signal) if the mean signal is 0).\n\n\n"}
{"id": "31080143", "url": "https://en.wikipedia.org/wiki?curid=31080143", "title": "S (set theory)", "text": "S (set theory)\n\nS is an axiomatic set theory set out by George Boolos in his 1989 article, \"Iteration Again\". S, a first-order theory, is two-sorted because its ontology includes “stages” as well as sets. Boolos designed S to embody his understanding of the “iterative conception of set“ and the associated iterative hierarchy. S has the important property that all axioms of Zermelo set theory \"Z\", except the axiom of extensionality and the axiom of choice, are theorems of S or a slight modification thereof.\n\nAny grouping together of mathematical, abstract, or concrete objects, however formed, is a \"collection\", a synonym for what other set theories refer to as a class. The things that make up a collection are called elements or members. A common instance of a collection is the domain of discourse of a first-order theory. \n\nAll sets are collections, but there are collections that are not sets. A synonym for collections that are not sets is proper class. An essential task of axiomatic set theory is to distinguish sets from proper classes, if only because mathematics is grounded in sets, with proper classes relegated to a purely descriptive role.\n\nThe Von Neumann universe implements the “iterative conception of set” by stratifying the universe of sets into a series of \"stages\", with the sets at a given stage being possible members of the sets formed at all higher stages. The notion of stage goes as follows. Each stage is assigned an ordinal number. The lowest stage, stage 0, consists of all entities having no members. We assume that the only entity at stage 0 is the empty set, although this stage would include any urelements we would choose to admit. Stage \"n\", \"n\">0, consists of all possible sets formed from elements to be found in any stage whose number is less than \"n\". Every set formed at stage \"n\" can also be formed at every stage greater than \"n\".\n\nHence the stages form a nested and well-ordered sequence, and would form a hierarchy if set membership were transitive. The iterative conception has gradually become more accepted, despite an imperfect understanding of its historical origins.\n\nThe iterative conception of set steers clear, in a well-motivated way, of the well-known paradoxes of Russell, Burali-Forti, and Cantor. These paradoxes all result from the unrestricted use of the principle of comprehension of naive set theory. Collections such as \"the class of all sets\" or \"the class of all ordinals\" include sets from all stages of the iterative hierarchy. Hence such collections cannot be formed at any given stage, and thus cannot be sets.\n\nThis section follows Boolos (1998: 91). The variables \"x\" and \"y\" range over sets, while \"r\", \"s\", and \"t\" range over stages. There are three primitive two-place predicates:\n\nThe axioms below include a defined two-place set-stage predicate, \"Bxr\", which abbreviates:\n\"Bxr\" is read as “set \"x\" is formed before stage \"r\".”\n\nIdentity, denoted by infix ‘=’, does not play the role in S it plays in other set theories, and Boolos does not make fully explicit whether the background logic includes identity. S has no axiom of extensionality and identity is absent from the other S axioms. Identity does appear in the axiom schema distinguishing S+ from S, and in the derivation in S of the pairing, null set, and infinity axioms of Z.\n\nThe symbolic axioms shown below are from Boolos (1998: 91), and govern how sets and stages behave and interact. The natural language versions of the axioms are intended to aid the intuition.\nThe axioms come in two groups of three. The first group consists of axioms pertaining solely to stages and the stage-stage relation ‘<’.\n\nTra: formula_2\n\nA consequence of \"Net\" is that every stage is earlier than some stage.\n\nInf: formula_3\n\nEvery set is formed at some stage in the hierarchy.\n\nWhen: formula_4\n\nA set is formed at some stage iff its members are formed at earlier stages.\n\nLet \"A\"(\"y\") be a formula of S where \"y\" is free but \"x\" is not. Then the following axiom schema holds:\n\nSpec: formula_5\n\nIf there exists a stage \"r\" such that all sets satisfying \"A\"(\"y\") are formed at a stage earlier than \"r\", then there exists a set \"x\" whose members are just those sets satisfying \"A\"(\"y\"). The role of \"Spec\" in S is analogous to that of the axiom schema of specification of Z.\n\nBoolos’s name for Zermelo set theory minus extensionality was \"Z-\". Boolos derived in S all axioms of \"Z-\" except the axiom of choice. The purpose of this exercise was to show how most of conventional set theory can be derived from the iterative conception of set, assumed embodied in S. Extensionality does not follow from the iterative conception, and so is not a theorem of S. However, S + Extensionality is free of contradiction if S is free of contradiction.\n\nBoolos then altered \"Spec\" to obtain a variant of S he called S+, such that the axiom schema of replacement is derivable in S+ + Extensionality. Hence S+ + Extensionality has the power of ZF. Boolos also argued that the axiom of choice does not follow from the iterative conception, but did not address whether Choice could be added to S in some way. Hence S+ + Extensionality cannot prove those theorems of the conventional set theory ZFC whose proofs require Choice.\n\nInf guarantees the existence of stages ω, and of ω + \"n\" for finite \"n\", but not of stage ω + ω. Nevertheless, S yields enough of Cantor's paradise to ground almost all of contemporary mathematics.\n\nBoolos compares S at some length to a variant of the system of Frege’s \"Grundgesetze\", in which Hume's principle, taken as an axiom, replaces Frege’s Basic Law V, an unrestricted comprehension axiom which made Frege's system inconsistent; see Russell's paradox.\n\n"}
{"id": "262288", "url": "https://en.wikipedia.org/wiki?curid=262288", "title": "Symmetric difference", "text": "Symmetric difference\n\nIn mathematics, the symmetric difference, also known as the disjunctive union, of two sets is the set of elements which are in either of the sets and not in their intersection. The symmetric difference of the sets \"A\" and \"B\" is commonly denoted by\nor\nor\n\nFor example, the symmetric difference of the sets formula_4 and formula_5 is formula_6.\n\nThe power set of any set becomes an abelian group under the operation of symmetric difference, with the empty set as the neutral element of the group and every element in this group being its own inverse. The power set of any set becomes a Boolean ring with symmetric difference as the addition of the ring and intersection as the multiplication of the ring.\n\nThe symmetric difference is equivalent to the union of both relative complements, that is:\n\nThe symmetric difference can also be expressed using the XOR operation ⊕ on the predicates describing the two sets in set-builder notation:\n\nThe same fact can be stated as the indicator function (which we denote here by formula_9) of the symmetric difference being the XOR (or addition mod 2) of the indicator functions of its two arguments: formula_10 or using the Iverson bracket notation formula_11.\n\nThe symmetric difference can also be expressed as the union of the two sets, minus their intersection:\n\nIn particular, formula_13; the equality in this non-strict inclusion occurs if and only if formula_14 and formula_15 are disjoint sets. Furthermore, if we denote formula_16 and formula_17, then formula_18 and formula_19 are always disjoint, so formula_18 and formula_19 partition formula_22. Consequently, assuming intersection and symmetric difference as primitive operations, the union of two sets can be well \"defined\" in terms of symmetric difference by the right-hand side of the equality\n\nThe symmetric difference is commutative and associative (and consequently the leftmost set of parentheses in the previous expression were thus redundant):\n\nThe empty set is neutral, and every set is its own inverse:\n\nTaken together, we see that the power set of any set \"X\" becomes an abelian group if we use the symmetric difference as operation. (More generally, any field of sets forms a group with the symmetric difference as operation.) A group in which every element is its own inverse (or, equivalently, in which every element has order 2) is sometimes called a Boolean group; the symmetric difference provides a prototypical example of such groups. Sometimes the Boolean group is actually defined as the symmetric difference operation on a set. In the case where \"X\" has only two elements, the group thus obtained is the Klein four-group.\n\nEquivalently, a Boolean group is an elementary abelian 2-group. Consequently, the group induced by the symmetric difference is in fact a vector space over the field with 2 elements Z. If \"X\" is finite, then the singletons form a basis of this vector space, and its dimension is therefore equal to the number of elements of \"X\". This construction is used in graph theory, to define the cycle space of a graph.\n\nFrom the property of the inverses in a Boolean group, it follows that the symmetric difference of two repeated symmetric differences is equivalent to the repeated symmetric difference of the join of the two multisets, where for each double set both can be removed. In particular:\n\nThis implies triangle inequality: the symmetric difference of \"A\" and \"C\" is contained in the union of the symmetric difference of \"A\" and \"B\" and that of \"B\" and \"C\". (But note that for the diameter of the symmetric difference the triangle inequality does not hold.)\n\nIntersection distributes over symmetric difference:\nand this shows that the power set of \"X\" becomes a ring with symmetric difference as addition and intersection as multiplication. This is the prototypical example of a Boolean ring.\nFurther properties of the symmetric difference:\n\n\nThe symmetric difference can be defined in any Boolean algebra, by writing\nThis operation has the same properties as the symmetric difference of sets.\n\nThe repeated symmetric difference is in a sense equivalent to an operation on a multiset of sets giving the set of elements which are in an odd number of sets.\n\nAs above, the symmetric difference of a collection of sets contains just elements which are in an odd number of the sets in the collection:\nEvidently, this is well-defined only when each element of the union formula_43 is contributed by a finite number of elements of formula_44.\n\nSuppose formula_45 is a multiset and formula_46. Then there is a formula for formula_47, the number of elements in formula_48, given solely in terms of intersections of elements of formula_44:\n\nAs long as there is a notion of \"how big\" a set is, the symmetric difference between two sets can be considered a measure of how \"far apart\" they are. \nFirst consider a finite set \"S\" and the counting measure on subsets given by their size. Now consider two subsets of \"S\" and set their distance apart as the size of their symmetric difference. This distance is in fact a metric so that the power set on \"S\" is a metric space. If \"S\" has \"n\" elements, then the distance from the empty set to \"S\" is \"n\", and this is the maximum distance for any pair of subsets.\n\nUsing the ideas of measure theory, the separation of measurable sets can be defined to be the measure of their symmetric difference. \nIf μ is a σ-finite measure defined on a σ-algebra Σ, the function\nis a pseudometric on Σ. \"d\" becomes a metric if Σ is considered modulo the equivalence relation \"X\" ~ \"Y\" if and only if formula_52. It is sometimes called Fréchet-Nikodym metric. The resulting metric space is separable if and only if L(μ) is separable.\n\nIf formula_53, we have: formula_54. Indeed,\n\nLet formula_56 be some measure space and let formula_57 and formula_58.\n\nSymmetric difference is measurable: formula_59.\n\nWe write formula_60 iff formula_61. The relation \"formula_62\" is an equivalence relation on the formula_63-measurable sets.\n\nWe write formula_64 iff to each formula_65 there's some formula_66 such that formula_67. The relation \"formula_68\" is a partial order on the family of subsets of formula_63.\n\nWe write formula_70 iff formula_64 and formula_72. The relation \"formula_62\" is an equivalence relationship between the subsets of formula_63.\n\nThe \"symmetric closure\" of formula_75 is the collection of all formula_63-measurable sets that are formula_62 to some formula_65. The symmetric closure of formula_75 contains formula_75. If formula_75 is a sub-formula_82-algebra of formula_63, so is the symmetric closure of formula_75.\n\nformula_60 iff formula_86 formula_87-a.e.\n\nThe Hausdorff distance and the (area of the) symmetric difference are both pseudo-metrics on the set of measurable geometric shapes. However, they behave quite differently. The figure at the right shows two sequences of shapes, \"Red\" and \"Red ∪ Green\". When the Hausdorff distance between them becomes smaller, the area of the symmetric difference between them becomes larger, and vice versa. By continuing these sequences in both directions, it is possible to get two sequences such that the Hausdorff distance between them converges to 0 and the symmetric distance between them diverges, or vice versa.\n\n"}
{"id": "20522132", "url": "https://en.wikipedia.org/wiki?curid=20522132", "title": "Syncategorematic term", "text": "Syncategorematic term\n\nIn scholastic logic, a syncategorematic term (or syncategorema) is a word that cannot serve as the subject or the predicate of a proposition, and thus cannot stand for any of Aristotle's categories, but can be used with other terms to form a proposition. Words such as 'all', 'and', 'if' are examples of such terms.\n\nThe distinction between categorematic and syncategorematic terms was established in ancient Greek grammar. Words that designate self-sufficient entities (i.e., nouns or adjectives) were called categorematic, and those that do not stand by themselves were dubbed syncategorematic, (i.e., prepositions, logical connectives, etc.). Priscian in his \"Institutiones grammaticae\" translates the word as \"consignificantia\". Scholastics retained the difference, which became a dissertable topic after the 13th century revival of logic. William of Sherwood, a representative of terminism, wrote a treatise called \"Syncategoremata\". Later his pupil, Peter of Spain, produced a similar work entitled \"Syncategoreumata\".\n\nIn propositional calculus, a syncategorematic term is a term that has no individual meaning (a term with an individual meaning is called \"categorematic\"). Whether a term is syncategorematic or not is determined by the way it is defined or introduced in the language.\n\nIn the common definition of propositional logic, examples of syncategorematic terms are the logical connectives. Let us take the connective formula_1 for instance, its semantic rule is:\n\nformula_2 iff formula_3\n\nSo its meaning is defined when it occurs in combination with two formulas formula_4 and formula_5. But it has no meaning when taken in isolation, i.e. formula_6 is not defined.\n\nWe could however define the formula_1 in a different manner, e.g., using λ-abstraction: formula_8, which expects a pair of Boolean-valued arguments, i.e., arguments that are either \"TRUE\" or \"FALSE\", defined as formula_9 and formula_10 respectively. This is an expression of type formula_11. Its meaning is thus a binary function from pairs of entities of type truth-value to an entity of type truth-value. Under this definition it would be non-syncategorematic, or categorematic. Note that while this definition would formally define the formula_1 function, it requires the use of formula_13-abstraction, in which case the formula_13 itself is introduced syncategorematically, thus simply moving the issue up another level of abstraction.\n\n\n"}
{"id": "54419", "url": "https://en.wikipedia.org/wiki?curid=54419", "title": "Tangram", "text": "Tangram\n\nThe tangram () is a dissection puzzle consisting of seven flat shapes, called \"tans\", which are put together to form shapes. The objective of the puzzle is to form a specific shape (given only an outline or silhouette) using all seven pieces, which may not overlap. It is reputed to have been invented in China during the Song Dynasty, and then carried over to Europe by trading ships in the early 19th century. It became very popular in Europe for a time then, and then again during World War I. It is one of the most popular dissection puzzles in the world. A Chinese psychologist has termed the tangram \"the earliest psychological test in the world\", albeit one made for entertainment rather than for analysis.\n\nThe origin of the word 'tangram' is unclear. The '-gram' element is apparently from Greek γράμμα ('written character, letter, that which is drawn'). The 'tan-' element is variously conjectured to be Chinese \"t'an\" 'to extend' or Cantonese \"t'ang\" 'Chinese'.\n\nThe tangram had already existed in China for a long time when it was first brought to America by Captain M. Donnaldson, on his ship, \"Trader\", in 1815. When it docked in Canton, the captain was given a pair of author Sang-Hsia-koi's tangram books from 1815. They were then brought with the ship to Philadelphia, where it docked in February 1816. The first tangram book to be published in America was based on the pair brought by Donnaldson.\nThe puzzle was originally popularized by Sam Loyd's \"The Eighth Book Of Tan\", a fictitious history of the tangram, which claimed that the game was invented 4,000 years prior by a god named Tan. The book included 700 shapes, some of which are impossible to solve.\n\nThe puzzle eventually reached England, where it became very fashionable. The craze quickly spread to other European countries. This was mostly due to a pair of British tangram books, \"The Fashionable Chinese Puzzle\", and the accompanying solution book, \"Key\". Soon, tangram sets were being exported in great number from China, made of various materials, from glass, to wood, to tortoise shell.\n\nMany of these unusual and exquisite tangram sets made their way to Denmark. Danish interest in tangrams skyrocketed around 1818, when two books on the puzzle were published, to much enthusiasm. The first of these was \"Mandarinen\" (About the Chinese Game). This was written by a student at Copenhagen University, which was a non-fictional work about the history and popularity of tangrams. The second, \"Det nye chinesiske Gaadespil\" (The new Chinese Puzzle Game), consisted of 339 puzzles copied from \"The Eighth Book of Tan\", as well as one original.\n\nOne contributing factor in the popularity of the game in Europe was that although the Catholic Church forbade many forms of recreation on the sabbath, they made no objection to puzzle games such as the tangram.\n\nTangrams were first introduced to the German public by industrialist Friedrich Adolf Richter around 1891. The sets were made out of stone or false earthenware, and marketed under the name \"The Anchor Puzzle\".\n\nMore internationally, the First World War saw a great resurgence of interest in tangrams, on the homefront and trenches of both sides. During this time, it occasionally went under the name of \"The Sphinx\" an alternative title for the \"Anchor Puzzle\" sets.\n\nA tangram paradox is a dissection fallacy: Two figures composed with the same set of pieces, one of which seems to be a proper subset of the other. One famous paradox is that of the two monks, attributed to Dudeney, which consists of two similar shapes, one with and the other missing a foot. In reality, the area of the foot is compensated for in the second figure by a subtly larger body. Another tangram paradox is proposed by Sam Loyd in \"The 8th Book of Tan\":\n\nThe two monks paradox – two similar shapes but one missing a foot:\nThe Magic Dice Cup tangram paradox – from Sam Loyd's book \"The Eighth Book of Tan\" (1903). Each of these cups was composed using the same seven geometric shapes. But the first cup is whole, and the others contain vacancies of different sizes. (Notice that the one on the left is slightly shorter than the other two. The one in the middle is ever-so-slightly wider than the one on the right, and the one on the left is narrower still.)\nClipped square tangram paradox – from Loyd's book \"The Eighth Book of Tan\" (1903):\n\nOver 6500 different tangram problems have been created from 19th century texts alone, and the current number is ever-growing. Fu Traing Wang and Chuan-Chin Hsiung proved in 1942 that there are only thirteen convex tangram configurations (config segment drawn between any two points on the configuration's edge always pass through the configuration's interior, i.e., configurations with no recesses in the outline).\n\nChoosing a unit of measurement so that the seven pieces can be assembled to form a square of side one unit and having area one square unit, the seven pieces are:\n\n\nOf these seven pieces, the parallelogram is unique in that it has no reflection symmetry but only rotational symmetry, and so its mirror image can be obtained only by flipping it over. Thus, it is the only piece that may need to be flipped when forming certain shapes.\n\n\n\n"}
{"id": "44770306", "url": "https://en.wikipedia.org/wiki?curid=44770306", "title": "Tim Cochran", "text": "Tim Cochran\n\nThomas \"Tim\" Daniel Cochran (April 7, 1955 – December 16, 2014) was a professor of Mathematics at Rice University specializing in topology, especially low-dimensional topology, the theory of knots and links and associated algebra.\n\nTim Cochran was a valedictorian for the Severna Park High School Class of 1973. Later, he was an undergraduate at Massachusetts Institute of Technology, and received his Ph.D. from the University of California, Berkeley in 1982 (\"Embedding 4-manifolds in S\"). He then returned to MIT as a C.L.E. Moore Postdoctoral Instructor from 1982 to 1984. He was an NSF postdoctoral fellow from 1985 to 1987. Following brief appointments at Berkeley and Northwestern University, he started at Rice University as an associate professor in 1990. He became a full professor at Rice University in 1998. He died unexpectedly, aged 59, on December 16, 2014, while on a year-long sabbatical leave supported by a fellowship from the Simons Foundation.\n\nWith his coauthors Kent Orr and Peter Teichner, Cochran defined the solvable filtration of the knot concordance group, whose lower levels encapsulate many classical knot concordance invariants.\n\nCochran was also responsible for naming the slam-dunk move for surgery diagrams in low-dimensional topology.\n\nWhile at Rice, he was named an Outstanding Faculty Associate (1992–93), and received the Faculty Teaching and Mentoring Award from the Rice Graduate Student Association (2014)\n\nHe was named a fellow of the American Mathematical Society in 2014, for \"contributions to low-dimensional topology, specifically knot and link concordance, and for mentoring numerous junior mathematicians\".\n\n\n"}
{"id": "173844", "url": "https://en.wikipedia.org/wiki?curid=173844", "title": "Transpose", "text": "Transpose\n\nIn linear algebra, the transpose of a matrix is an operator which flips a matrix over its diagonal, that is it switches the row and column indices of the matrix by producing another matrix denoted as A (also written A′, A, A or A). It is achieved by any one of the following equivalent actions:\n\nFormally, the \"i\"-th row, \"j\"-th column element of A is the \"j\"-th row, \"i\"-th column element of A:\n\nIf A is an matrix, then A is an matrix.\n\nThe transpose of a matrix was introduced in 1858 by the British mathematician Arthur Cayley.\n\n\n\n\nFor matrices A, B and scalar \"c\" we have the following properties of transpose:\n\nwhich is written as a b in Einstein summation convention.\n\nA square matrix whose transpose is equal to itself is called a symmetric matrix; that is, A is symmetric if\n\nA square matrix whose transpose is equal to its negative is called a skew-symmetric matrix; that is, A is skew-symmetric if\n\nA square complex matrix whose transpose is equal to the matrix with every entry replaced by its complex conjugate (denoted here with an overline) is called a Hermitian matrix (equivalent to the matrix being equal to its conjugate transpose); that is, A is Hermitian if\n\nA square complex matrix whose transpose is equal to the negation of its complex conjugate is called a skew-Hermitian matrix; that is, A is skew-Hermitian if\n\nA square matrix whose transpose is equal to its inverse is called an orthogonal matrix; that is, A is orthogonal if\n\nA square complex matrix whose transpose is equal to its conjugate inverse is called a unitary matrix; that is, A is unitary if\n\nIf A is an \"m\" × \"n\" matrix and A is its transpose, then the result of matrix multiplication with these two matrices gives two square matrices: A A is \"m\" × \"m\" and A A is \"n\" × \"n\". Furthermore, these products are symmetric matrices. Indeed, the matrix product A A has entries that are the inner product of a row of A with a column of A. But the columns of A are the rows of A, so the entry corresponds to the inner product of two rows of A. If \"p\" is the entry of the product, it is obtained from rows i and j in A. The entry \"p\" is also obtained from these rows, thus \"p\" = \"p\", and the product matrix (\"p\") is symmetric. Similarly, the product A A is a symmetric matrix.\n\nA quick proof of the symmetry of A A results from the fact that it is its own transpose:\n\nThe transpose may be defined more generally:\n\nIf is a linear map between right \"R\"-modules \"V\" and \"W\" with respective dual modules \"V\" and \"W\", the \"transpose\" of \"f\" is the linear map\nEquivalently, the transpose \"f\" is defined by the relation\nwhere is the natural pairing of each dual space with its respective vector space. This definition also applies unchanged to left modules and to vector spaces.\n\nThe definition of the transpose may be seen to be independent of any bilinear form on the vector spaces, unlike the adjoint (below).\n\nIf the matrix \"A\" describes a linear map with respect to bases of \"V\" and \"W\", then the matrix \"A\" describes the transpose of that linear map with respect to the dual bases.\n\nEvery linear map to the dual space defines a bilinear form , with the relation . By defining the transpose of this bilinear form as the bilinear form \"B\" defined by the transpose i.e. , we find that . Here, Ψ is the natural homomorphism into the double dual.\n\nIf the vector spaces \"V\" and \"W\" have respectively nondegenerate bilinear forms \"B\" and \"B\", a concept known as the \"adjoint,\" which is closely related to the transpose\",\" may be defined:\n\nIf is a linear map between vector spaces \"V\" and \"W\", we define \"g\" as the \"adjoint\" of \"f\" if satisfies\n\nThese bilinear forms define an isomorphism between \"V\" and \"V\", and between \"W\" and \"W\", resulting in an isomorphism between the transpose and adjoint of \"f\". The matrix of the adjoint of a map is the transposed matrix only if the bases are orthonormal with respect to their bilinear forms. In this context, many authors use the term transpose to refer to the adjoint as defined here.\n\nThe adjoint allows us to consider whether is equal to . In particular, this allows the orthogonal group over a vector space \"V\" with a quadratic form to be defined without reference to matrices (nor the components thereof) as the set of all linear maps for which the adjoint equals the inverse.\n\nOver a complex vector space, one often works with sesquilinear forms (conjugate-linear in one argument) instead of bilinear forms. The Hermitian adjoint of a map between such spaces is defined similarly, and the matrix of the Hermitian adjoint is given by the conjugate transpose matrix if the bases are orthonormal.\n\nOn a computer, one can often avoid explicitly transposing a matrix in memory by simply accessing the same data in a different order. For example, software libraries for linear algebra, such as BLAS, typically provide options to specify that certain matrices are to be interpreted in transposed order to avoid the necessity of data movement.\n\nHowever, there remain a number of circumstances in which it is necessary or desirable to physically reorder a matrix in memory to its transposed ordering. For example, with a matrix stored in row-major order, the rows of the matrix are contiguous in memory and the columns are discontiguous. If repeated operations need to be performed on the columns, for example in a fast Fourier transform algorithm, transposing the matrix in memory (to make the columns contiguous) may improve performance by increasing memory locality.\n\nIdeally, one might hope to transpose a matrix with minimal additional storage. This leads to the problem of transposing an \"n\" × \"m\" matrix in-place, with O(1) additional storage or at most storage much less than \"mn\". For \"n\" ≠ \"m\", this involves a complicated permutation of the data elements that is non-trivial to implement in-place. Therefore, efficient in-place matrix transposition has been the subject of numerous research publications in computer science, starting in the late 1950s, and several algorithms have been developed.\n\n\n\n"}
{"id": "24047271", "url": "https://en.wikipedia.org/wiki?curid=24047271", "title": "Type constructor", "text": "Type constructor\n\nIn the area of mathematical logic and computer science known as type theory, a type constructor is a feature of a typed formal language that builds new types from old ones. Basic types are considered to be built using nullary type constructors. Some type constructors take another type as an argument, e.g., the constructors for product types, function types, power types and list types. New types can be defined by recursively composing type constructors.\n\nFor example, simply typed lambda calculus can be seen as a language with a single type constructor—the function type constructor. Product types can generally be considered \"built-in\" in typed lambda calculi via currying.\n\nAbstractly, a type constructor is an \"n\"-ary type operator taking as argument zero or more types, and returning another type. Making use of currying, \"n\"-ary type operators can be (re)written as a sequence of applications of unary type operators. Therefore, we can view the type operators as a simply typed lambda calculus, which has only one basic type, usually denoted formula_1, and pronounced \"type\", which is the type of all types in the underlying language, which are now called \"proper types\" in order to distinguish them from the types of the type operators in their own calculus, which are called \"kinds\".\n\nInstituting a simply typed lambda calculus over the type operators results in more than just a formalization of type constructors though. Higher-order type operators become possible. Type operators correspond to the 2nd axis in the lambda cube, leading to the simply typed lambda-calculus with type operators, λ; while this is not so well known, combining type operators with polymorphic lambda calculus (system F) yields system F-omega.\n\n\n"}
{"id": "54455302", "url": "https://en.wikipedia.org/wiki?curid=54455302", "title": "V. Frederick Rickey", "text": "V. Frederick Rickey\n\nVincent Frederick Rickey (born 17 December 1941) is an American logician and historian of mathematics.\n\nRickey received his B.S. (1963), M.S. (1966), and Ph.D. (1968) from the University of Notre Dame in South Bend, Indiana. His Ph.D. was entitled \"An Axiomatic Theory of Syntax\". He joined the academic staff of Ohio's Bowling Green State University in 1968, became there a full professor in 1979, and retired there in 1998. He was then a mathematics professor at the United States Military Academy from 1998 until his retirement in 2011. He was a visiting professor at the University of Notre Dame (1971–1972), Indiana University at South Bend (1977–1978), the University of Vermont (1984–1985), and the United States Military Academy (1989–1990). He was a Visiting Mathematician (1994–1995) at the Mathematical Association of America (MAA) headquarters in Washington, D.C. and while on this sabbatical he was involved in the founding of the undergraduate magazine \"Math Horizons\".\n\nHe is an expert on the logical systems of Stanisław Leśniewski and has been a member of the editorial boards of the \"Notre Dame Journal of Formal Logic\" and \"The Philosopher's Index\". Rickey has broad interests in the history of mathematics with a particular interest in the historical development of calculus and the use of this history to motivate and inspire students.\n\nHe is a multiple awardee. He got the first statewide Distinguished Teaching Award from the Ohio section of the MAA, one of the first national MAA awards for Distinguished Teaching, four times the Kappa Mu Epsilon honorary society award for Excellence in Teaching Mathematics (1991, 1988, 1975, 1971), and the Outstanding Civilian Service Medal from the Department of the Army in 1990 for performance while serving as the visiting professor of mathematics at the United States Military Academy.\n\n"}
{"id": "54447016", "url": "https://en.wikipedia.org/wiki?curid=54447016", "title": "Victor J. Katz", "text": "Victor J. Katz\n\nVictor Joseph Katz (born 31 December 1942, Philadelphia) is an American mathematician, historian of mathematics, and teacher known for using the history of mathematics in teaching mathematics.\n\nKatz received in 1963 from Princeton University a bachelor's degree and in 1968 from Brandeis University a Ph.D. in mathematics under Maurice Auslander with thesis \"The Brauer group of a regular local ring\". He became at Federal City College an assistant professor and then in 1973 an associate professor and, after the merger of Federal City College into the University of the District of Columbia in 1977, a full professor there in 1980. He retired there as professor emeritus in 2005.\n\nAs a mathematician Katz specializes in algebra, but he is mainly known for his work on the history of mathematics and its uses in teaching. He wrote a textbook \"History of Mathematics: An Introduction\" (1993), for which he won in 1995 the Watson Davis and Helen Miles Davis Prize. He organized workshops and congresses for the MAA and the National Council of Teachers of Mathematics. The MAA published a collection of teaching materials by Katz as a compact disk with the title \"Historical Modules for the Teaching and Learning of Mathematics\". With Frank Swetz, he was a founding editor of a free online journal on the history of mathematics under the aegis of the MAA; the journal is called \"Convergence: Where Mathematics, History, and Teaching Interact\". In the journal \"Convergence\", Katz and Swetz published a series \"Mathematical Treasures\". For a study of the possibilities for using mathematical history in schools, Katz received a grant from the National Science Foundation.\n\nHe has been married since 1969 to Phyllis Katz (née Friedman), a science educator who developed and directed the U.S. national nonprofit organization \"Hands On Science Outreach, Inc.\" (HOSO). The couple have three children.\n\n\n\n"}
{"id": "245523", "url": "https://en.wikipedia.org/wiki?curid=245523", "title": "Weight function", "text": "Weight function\n\nA weight function is a mathematical device used when performing a sum, integral, or average to give some elements more \"weight\" or influence on the result than other elements in the same set. The result of this application of a weight function is a weighted sum or weighted average. Weight functions occur frequently in statistics and analysis, and are closely related to the concept of a measure. Weight functions can be employed in both discrete and continuous settings. They can be used to construct systems of calculus called \"weighted calculus\" and \"meta-calculus\".\n\nIn the discrete setting, a weight function formula_1 is a positive function defined on a discrete set formula_2, which is typically\nfinite or countable. The weight function formula_3 corresponds to the \"unweighted\" situation in which all elements have equal weight. One can then apply this weight to various concepts.\n\nIf the function formula_4 is a real-valued function, then the \"unweighted sum of formula_5 on formula_2\" is defined as\n\nbut given a \"weight function\" formula_1, the weighted sum or conical combination is defined as\n\nOne common application of weighted sums arises in numerical integration.\n\nIf \"B\" is a finite subset of \"A\", one can replace the unweighted cardinality \"|B|\" of \"B\" by the \"weighted cardinality\" \n\nIf \"A\" is a finite non-empty set, one can replace the unweighted mean or average \n\nby the weighted mean or weighted average \n\nIn this case only the \"relative\" weights are relevant.\n\nWeighted means are commonly used in statistics to compensate for the presence of bias. For a quantity formula_5 measured multiple independent times formula_14 with variance formula_15, the best estimate of the signal is obtained \nby averaging all the measurements with weight formula_16, and\nthe resulting variance is smaller than each of the independent measurements\nformula_17. The maximum likelihood method weights the difference between fit and data using the same weights formula_18.\n\nThe expected value of a random variable is the weighted average of the possible values it might take on, with the weights being the respective probabilities. More generally, the expected value of a function of a random variable is the probability-weighted average of the values the function takes on for each possible value of the random variable.\n\nIn regressions in which the dependent variable is assumed to be affected by both current and lagged (past) values of the independent variable, a distributed lag function is estimated, this function being a weighted average of the current and various lagged independent variable values. Similarly, a moving average model specifies an evolving variable as a weighted average of current and various lagged values of a random variable.\n\nThe terminology \"weight function\" arises from mechanics: if one has a collection of \"formula_19\" objects on a lever, with weights formula_20 (where weight is now interpreted in the physical sense) and locations :formula_21, then the lever will be in balance if the fulcrum of the lever is at the center of mass \n\nwhich is also the weighted average of the positions formula_23.\n\nIn the continuous setting, a weight is a positive measure such as formula_24 on some domain formula_25,which is typically a subset of a Euclidean space formula_26, for instance formula_25 could be an interval formula_28. Here formula_29 is Lebesgue measure and formula_30 is a non-negative measurable function. In this context, the weight function formula_31 is sometimes referred to as a density.\n\nIf formula_32 is a real-valued function, then the \"unweighted\" integral\n\ncan be generalized to the \"weighted integral\" \n\nNote that one may need to require formula_5 to be absolutely integrable with respect to the weight formula_24 in order for this integral to be finite.\n\nIf \"E\" is a subset of formula_25, then the volume vol(\"E\") of \"E\" can be generalized to the \"weighted volume\" \n\nIf formula_25 has finite non-zero weighted volume, then we can replace the unweighted average \n\nby the weighted average \n\nIf formula_42 and formula_43 are two functions, one can generalize the unweighted bilinear form \n\nto a weighted bilinear form \n\nSee the entry on orthogonal polynomials for examples of weighted orthogonal functions.\n\n"}
{"id": "1326206", "url": "https://en.wikipedia.org/wiki?curid=1326206", "title": "Zoe Heriot", "text": "Zoe Heriot\n\nZoe Heriot (sometimes spelled Zoe Herriot) is a fictional character played by Wendy Padbury in the long-running British science fiction television series \"Doctor Who\". A young astrophysicist who lived on a space wheel in the 21st century, she was a companion of the Second Doctor and a regular in the programme from 1968 to 1969.\n\nZoe first appears in the serial \"The Wheel in Space\", where she is the librarian on board Space Station W3, also known as the Wheel. When the Cybermen attack, she aids the Doctor and Jamie in defeating them before stowing away aboard the TARDIS. In David Whitaker's script for \"The Wheel in Space\", Zoe's last name is spelled \"Heriot\", but the double-\"r\" misspelling is also seen in reference works.\n\nZoe's age is not given in the series, but according to initial publicity she was fifteen when she joined the TARDIS crew. She holds a degree in pure mathematics and is a genius, with intelligence scores comparable to the Doctor's. Coupled with her photographic memory and the advanced learning techniques of her era, this makes her somewhat like a human calculator, able to perform complicated mathematics in her head. Part of the reason for her wanting to travel with the Doctor is her chafing at the restrictions and sterile surroundings of her station-bound existence. However, her real-world experience is severely limited, causing her to get herself into trouble frequently.\n\nTogether with the Doctor and Jamie, she meets the Cybermen again when they invade 20th century London, enters the surreal Land of Fiction, fights the Ice Warriors and survives the battlefields of the War Chief's war games. Her journeys with the Doctor come to an end in that serial, when the Time Lords finally catch up with the Doctor. As well as forcing a regeneration on him and exiling him to Earth, the Time Lords return Jamie and Zoe to their own times, wiping their memories of their experiences with the Doctor (save for their first encounters with him) in the process.\n\nZoe's life after her return to her own time is not further explored in the series. In the spin-off short story \"The Tip of the Mind\" by Peter Anghelides, it is revealed that although her intellect allows her to resist the memory blocks by the Time Lords, she is unable to access the memories of her time with the Doctor consciously. This causes her strange dreams, and makes her work suffer. An encounter with the Third Doctor makes the memory blocks permanent, but she ultimately never reaches her full potential.\n\nPadbury has appeared in \"Doctor Who\" audio adventures from Big Finish Productions, first as a character other than Zoe in the full-cast audio \"Davros\", and then as Zoe in \"Fear of the Daleks\", part of the \"Companion Chronicles\" talking book series. The latter story portrays an older Zoe having detailed dreams of her adventures with the Doctor; she suspects that something is blocking her memory, but she does not know what, and is seeing a psychiatric counselor in an effort to understand the \"dreams\". She has returned as Zoe in several more Big Finish plays such as \"Echoes of Grey\" and her second Companion Chronicle, \"Prison in Space\", based on an unmade TV story. \"Echoes of Grey\" and subsequent follow-up stories, \"The Memory Cheats\" and \"The Uncertainty Principle\" depict Zoe being essentially taken prisoner by an unnamed Company whose experiments in creating a new form of life were disrupted by the Doctor, Jamie and Zoe- the TARDIS arriving on Earth a few decades in Zoe's future- in \"Echoes of Grey\", subsequent stories seeing the Company arresting Zoe's older, contemporary self on trumped-up charges to try and provoke Zoe into unlocking her lost memories of her time with the Doctor so that they can harness the secret of time travel. This comes to a head in the audio \"Second Chances\" when Zoe has a 'flashback' of the destruction of a space station that will actually take place a few days in the future from her current point in time, allowing Zoe to interact with her past self as she tries to change her history and prevent the station's destruction. Although this part of her plan fails, the audio ends with Zoe using a computer virus that has mutated to attack physical matter to destroy the Company and escape, although her mental state and memories of her time with the Doctor are still lost.\n\nAn older Zoe is reunited with the Doctor in his sixth incarnation in \"Legend of the Cybermen\", where she returns to the Land of Fiction after she is nearly converted by the Cybermen after leaving the Doctor, her attempt to trap the Cybermen in the Land resulting in the universe being endangered by the risk that the Cybermen will harness the computer that controls the Land and use that to influence the imagination of the entire universe. After Zoe draws the Doctor into the Land, she created a fictional duplicate of Jamie to protect the Doctor until he learns the full situation and helps Zoe drive the Cybermen out, at the cost of Zoe losing her memories again as the Doctor takes her home. Zoe and Jamie also meet the Sixth Doctor in the audio \"Last of the Cybermen\", but in this case the Sixth Doctor has swapped with the Second shortly before the events of \"The War Games\", the Doctor's attempt to change his personal history to let his companions keep their memories failing when history is 'reset' as the Second Doctor returns to his proper place in history..\n\nThe novelisation of the TV serial \"The Mind Robber\" mentions, in passing, that Zoe discussed that adventure \"long afterwards\", suggesting that she eventually recovered some or all of her lost memories.\n\nWendy Padbury returned to \"Doctor Who\" as an illusory image of Zoe in the 20th anniversary episode, \"The Five Doctors\".\n\n\n\n\n\n\n\n\n\n"}
