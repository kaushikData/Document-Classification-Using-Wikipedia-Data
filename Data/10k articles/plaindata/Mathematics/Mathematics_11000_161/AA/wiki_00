{"id": "38249632", "url": "https://en.wikipedia.org/wiki?curid=38249632", "title": "1-planar graph", "text": "1-planar graph\n\nIn topological graph theory, a 1-planar graph is a graph that can be drawn in the Euclidean plane in such a way that each edge has at most one crossing point, where it crosses a single additional edge. If a 1-planar graph, one of the most natural generalizations of planar graphs, is drawn that way, the drawing is called a 1-plane graph or 1-planar embedding of the graph.\n\n1-planar graphs were first studied by , who showed that they can be colored with at most seven colors. Later, the precise number of colors needed to color these graphs, in the worst case, was shown to be six. The example of the complete graph \"K\", which is 1-planar, shows that 1-planar graphs may sometimes require six colors. However, the proof that six colors are always enough is more complicated.\nRingel's motivation was in trying to solve a variation of total coloring for planar graphs, in which one simultaneously colors the vertices and faces of a planar graph in such a way that no two adjacent vertices have the same color, no two adjacent faces have the same color, and no vertex and face that are adjacent to each other have the same color. This can obviously be done using eight colors by applying the four color theorem to the given graph and its dual graph separately, using two disjoint sets of four colors. However, fewer colors may be obtained by forming an auxiliary graph that has a vertex for each vertex or face of the given planar graph, and in which two auxiliary graph vertices are adjacent whenever they correspond to adjacent features of the given planar graph. A vertex coloring of the auxiliary graph corresponds to a vertex-face coloring of the original planar graph. This auxiliary graph is 1-planar, from which it follows that Ringel's vertex-face coloring problem may also be solved with six colors. The graph \"K\" cannot be formed as an auxiliary graph in this way, but nevertheless the vertex-face coloring problem also sometimes requires six colors; for instance, if the planar graph to be colored is a triangular prism, then its eleven vertices and faces require six colors, because no three of them may be given a single color.\n\nEvery 1-planar graph with \"n\" vertices has at most 4\"n\" − 8 edges. More strongly, each 1-planar drawing has at most \"n\" − 2 crossings; removing one edge from each crossing pair of edges leaves a planar graph, which can have at most 3\"n\" − 6 edges, from which the 4\"n\" − 8 bound on the number of edges in the original 1-planar graph immediately follows. However, unlike planar graphs (for which all maximal planar graphs on a given vertex set have the same number of edges as each other), there exist maximal 1-planar graphs (graphs to which no additional edges can be added while preserving 1-planarity) that have significantly fewer than 4\"n\" − 8 edges. The bound of 4\"n\" − 8 on the maximum possible number of edges in a 1-planar graph can be used to show that the complete graph \"K\" on seven vertices is not 1-planar, because this graph has 21 edges and in this case 4\"n\" − 8 = 20 < 21.\n\nA 1-planar graph is said to be an optimal 1-planar graph if it has exactly 4\"n\" − 8 edges, the maximum possible. In a 1-planar embedding of an optimal 1-planar graph, the uncrossed edges necessarily form a quadrangulation (a polyhedral graph in which every face is a quadrilateral). Every quadrangulation gives rise to an optimal 1-planar graph in this way, by adding the two diagonals to each of its quadrilateral faces. It follows that every optimal 1-planar graph is Eulerian (all of its vertices have even degree), that the minimum degree in such a graph is six, and that every optimal 1-planar graph has at least eight vertices of degree exactly six. Additionally, every optimal 1-planar graph is 4-vertex-connected, and every 4-vertex cut in such a graph is a separating cycle in the underlying quadrangulation.\n\nThe graphs that have straight 1-planar drawings (that is, drawings in which each edge is represented by a line segment, and in which each line segment is crossed by at most one other edge) have a slightly tighter bound of 4\"n\" − 9 on the maximum number of edges, achieved by infinitely many graphs.\n\nA complete classification of the 1-planar complete graphs, complete bipartite graphs, and more generally complete multipartite graphs is known. Every complete bipartite graph of the form \"K\" is 1-planar, as is every complete tripartite graph of the form \"K\". Other than these infinite sets of examples, the only complete multipartite 1-planar graphs are \"K\", \"K\", \"K\", \"K\", \"K\", and their subgraphs. The minimal non-1-planar complete multipartite graphs are \"K\", \"K\", \"K\", \"K\", and \"K\".\nFor instance, the complete bipartite graph \"K\" is 1-planar because it is a subgraph of \"K\", but \"K\" is not 1-planar.\n\nIt is NP-complete to test whether a given graph is 1-planar, and it remains NP-complete even for the graphs formed from planar graphs by adding a single edge and for graphs of bounded bandwidth. The problem is fixed-parameter tractable when parameterized by cyclomatic number or by tree-depth, so it may be solved in polynomial time when those parameters are bounded.\n\nIn contrast to Fáry's theorem for planar graphs, not every 1-planar graph may be drawn 1-planarly with straight line segments for its edges. However, testing whether a 1-planar drawing may be straightened in this way can be done in polynomial time. Additionally, every 3-vertex-connected 1-planar graph has a 1-planar drawing in which at most one edge, on the outer face of the drawing, has a bend in it. This drawing can be constructed in linear time from a 1-planar embedding of the graph. The 1-planar graphs have bounded book thickness, but some 1-planar graphs including \"K\" have book thickness at least four.\n\n1-planar graphs have bounded local treewidth, meaning that there is a (linear) function \"f\" such that the 1-planar graphs of diameter \"d\" have treewidth at most \"f\"(\"d\"); the same property holds more generally for the graphs that can be embedded onto a surface of bounded genus with a bounded number of crossings per edge. They also have separators, small sets of vertices the removal of which decomposes the graph into connected components whose size is a constant fraction of the size of the whole graph. Based on these properties, numerous algorithms for planar graphs, such as Baker's technique for designing approximation algorithms, can be extended to 1-planar graphs. For instance, this method leads to a polynomial-time approximation scheme for the maximum independent set of a 1-planar graph.\n\nThe class of graphs analogous to outerplanar graphs for 1-planarity are called the outer-1-planar graphs. These are graphs that can be drawn in a disk, with the vertices on the boundary of the disk, and with at most one crossing per edge. These graphs can always be drawn (in an outer-1-planar way) with straight edges and right angle crossings. By using dynamic programming on the SPQR tree of a given graph, it is possible to test whether it is outer-1-planar in linear time. The triconnected components of the graph (nodes of the SPQR tree) can consist only of cycle graphs, bond graphs, and four-vertex complete graphs, from which it also follows that outer-1-planar graphs are planar and have treewidth at most three.\n\nThe 1-planar graphs include the 4-map graphs, graphs formed from the adjacencies of regions in the plane with at most four regions meeting in any point. Conversely, every optimal 1-planar graph is a 4-map graph. However, 1-planar graphs that are not optimal 1-planar may not be map graphs.\n\n1-planar graphs have been generalized to \"k\"-planar graphs, graphs for which each edge is crossed at most \"k\" times (0-planar graphs are exactly the planar graphs). Ringel defined the local crossing number of \"G\" to be the least non-negative integer \"k\" such that \"G\" has a \"k\"-planar drawing. Because the local crossing number is the maximum degree of the intersection graph of the edges of an optimal drawing, and the thickness (minimum number of planar graphs into which the edges can be partitioned) can be seen as the chromatic number of an intersection graph of an appropriate drawing, it follows from Brooks' theorem that the thickness is at most one plus the local crossing number. The \"k\"-planar graphs with \"n\" vertices have at most \"O\"(\"k\"\"n\") edges, and treewidth \"O\"((\"kn\")). A shallow minor of a \"k\"-planar graph, with depth \"d\", is itself a (2\"d\" + 1)\"k\"-planar graph, so the shallow minors of 1-planar graphs and of \"k\"-planar graphs are also sparse graphs, implying that the 1-planar and \"k\"-planar graphs have bounded expansion.\n\nNonplanar graphs may also be parameterized by their crossing number, the minimum number of pairs of edges that cross in any drawing of the graph. A graph with crossing number \"k\" is necessarily \"k\"-planar, but not necessarily vice versa. For instance, the Heawood graph has crossing number 3, but it is not necessary for its three crossings to all occur on the same edge of the graph, so it is 1-planar, and can in fact be drawn in a way that simultaneously optimizes the total number of crossings and the crossings per edge.\n\nAnother related concept for nonplanar graphs is graph skewness, the minimal number of edges that must be removed to make a graph planar.\n"}
{"id": "57254124", "url": "https://en.wikipedia.org/wiki?curid=57254124", "title": "ADCIRC", "text": "ADCIRC\n\nThe ADCIRC model is a high-performance, cross-platform numerical ocean circulation model popular in simulating storm surge, tides, and coastal circulation problems.\nOriginally developed by Drs. Rick Luettich and Joannes Westerink,\nthe model is developed and maintained by a combination of academic, governmental, and corporate partners, including the University of North Carolina at Chapel Hill, the University of Notre Dame, and the US Army Corps of Engineers.\nThe ADCIRC system includes an independent multi-algorithmic wind forecast model and also has advanced coupling capabilities, allowing it to integrate effects from sediment transport, ice, waves, surface runoff, and baroclinicity.\n\nThe model is free, with source code made available by request via the website, allowing users to run the model on any system with a Fortran compiler. A pre-compiled Windows version of the model can also be purchased alongside the SMS (hydrology software). ADCIRC is coded in Fortran, and can be used with native binary, text, or netCDF file formats. \n\nThe model formulation\nis based on the shallow water equations, solving the continuity equation (represented in the form of the Generalized Wave Continuity Equation)\nand the momentum equations (with advective, Coriolis, eddy viscosity, and surface stress terms included). ADCIRC utilizes the finite element method in either three-dimensional or two-dimensional depth-integrated form on a triangular unstructured grid with Cartesian or spherical coordinates. It can run in either barotropic or baroclinic modes, allowing inclusion of changes in water density and properties such as salinity and temperature. ADCIRC can be run either in serial mode (e.g. on a personal computer) or in parallel on supercomputers via MPI. The model has been optimized to be highly parallelized, in order to facilitate rapid computation of large, complex problems.\n\nADCIRC is able to apply several different bottom friction formulations including Manning's n-based bottom drag due to changes in land coverage (such as forests, cities, and seafloor composition), as well as utilize atmospheric forcing data (wind stress and atmospheric pressure) from several sources, and further reduce the strength of the wind forcing due to surface roughness effects.\nThe model is also able to incorporate effects such as time-varying topography and bathymetry, boundary fluxes from rivers or other sources, tidal potential, and sub-grid scale features like levees. \n\nADCIRC is frequently coupled to a wind wave model such as STWAVE, SWAN, or WAVEWATCH III, especially in storm surge applications where wave radiation stress can have important effects on ocean circulation and vice versa. In these applications, the model is able to take advantage of tight coupling with wave models to increase calculation accuracy.\n\n"}
{"id": "8559560", "url": "https://en.wikipedia.org/wiki?curid=8559560", "title": "Algebraic fraction", "text": "Algebraic fraction\n\nIn algebra, an algebraic fraction is a fraction whose numerator and denominator are algebraic expressions. Two examples of algebraic fractions are formula_1 and formula_2. Algebraic fractions are subject to the same laws as arithmetic fractions.\n\nA rational fraction is an algebraic fraction whose numerator and denominator are both polynomials. Thus formula_1 is a rational fraction, but not formula_4 because the numerator contains a square root function.\n\nIn the algebraic fraction formula_5, the dividend \"a\" is called the \"numerator\" and the divisor \"b\" is called the \"denominator\". The numerator and denominator are called the \"terms\" of the algebraic fraction.\n\nA \"complex fraction\" is a fraction whose numerator or denominator, or both, contains a fraction. A \"simple fraction\" contains no fraction either in its numerator or its denominator. A fraction is in \"lowest terms\" if the only factor common to the numerator and the denominator is 1.\n\nAn expression which is not in fractional form is an \"integral expression\". An integral expression can always be written in fractional form by giving it the denominator 1. A \"mixed expression\" is the algebraic sum of one or more integral expressions and one or more fractional terms.\n\nIf the expressions \"a\" and \"b\" are polynomials, the algebraic fraction is called a \"rational algebraic fraction\" or simply \"rational fraction\". Rational fractions are also known as rational expressions. A rational fraction formula_6 is called \"proper\" if formula_7, and \"improper\" otherwise. For example, the rational fraction formula_8 is proper, and the rational fractions formula_9 and formula_10 are improper. Any improper rational fraction can be expressed as the sum of a polynomial (possibly constant) and a proper rational fraction. In the first example of an improper fraction one has\n\nwhere the second term is a proper rational fraction. The sum of two proper rational fractions is a proper rational fraction as well. The reverse process of expressing a proper rational fraction as the sum of two or more fractions is called resolving it into partial fractions. For example,\n\nHere, the two terms on the right are called partial fractions.\n\nAn \"irrational fraction\" is one that contains the variable under a fractional exponent. An example of an irrational fraction is\nThe process of transforming an irrational fraction to a rational fraction is known as rationalization. Every irrational fraction in which the radicals are monomials may be rationalized by finding the least common multiple of the indices of the roots, and substituting the variable for another variable with the least common multiple as exponent. In the example given, the least common multiple is 6, hence we can substitute formula_14 to obtain\n"}
{"id": "950777", "url": "https://en.wikipedia.org/wiki?curid=950777", "title": "Analytic signal", "text": "Analytic signal\n\nIn mathematics and signal processing, an analytic signal (also known as Gabor analytic signal) is a complex-valued function that has no negative frequency components.  The real and imaginary parts of an analytic signal are real-valued functions related to each other by the Hilbert transform.\n\nThe analytic representation of a real-valued function is an \"analytic signal\", comprising the original function and its Hilbert transform. This representation facilitates many mathematical manipulations. The basic idea is that the negative frequency components of the Fourier transform (or spectrum) of a real-valued function are superfluous, due to the Hermitian symmetry of such a spectrum. These negative frequency components can be discarded with no loss of information, provided one is willing to deal with a complex-valued function instead. That makes certain attributes of the function more accessible and facilitates the derivation of modulation and demodulation techniques, such as single-sideband. As long as the manipulated function has no negative frequency components (that is, it is still \"analytic\"), the conversion from complex back to real is just a matter of discarding the imaginary part. The analytic representation is a generalization of the phasor concept: while the phasor is restricted to time-invariant amplitude, phase, and frequency, the analytic signal allows for time-variable parameters.\n\nAnalytic signals are also defined using the Fourier quadrature transforms (FQTs) which are derived from the Fourier sine and cosine transforms. These analytic signal representations, also known as Fourier-Singh analytic signal (FSAS) representations, are used for time-frequency-energy (TFE) representation and analysis of nonlinear and non-stationary time-series and other data. This kind of TFE representation is obtained using the discrete cosine tranform (DCT) based Fourier decomposition method (FDM). These FQTs and FSAS representations are effective alternatives of the Hilbert transform (1905) and Gabor analytic signal representation (1946), respectively . \n\nIf formula_1 is a \"real-valued\" function with Fourier transform formula_2, then the transform has Hermitian symmetry about the formula_3 axis:\nwhere formula_5 is the complex conjugate of formula_2.\nThe function:\nwhere\n\ncontains only the \"non-negative frequency\" components of formula_2. And the operation is reversible, due to the Hermitian symmetry of formula_2:\nThe analytic signal of formula_1 is the inverse Fourier transform of formula_14:\nwhere\n\nThen:\n\nA corollary of Euler's formula is  formula_24  In general, the analytic representation of a simple sinusoid is obtained by expressing it in terms of complex-exponentials, discarding the negative frequency component, and doubling the positive frequency component. And the analytic representation of a sum of sinusoids is the sum of the analytic representations of the individual sinusoids.\n\nHere we use Euler's formula to identify and discard the negative frequency.\n\nThen:\n\nThis is another example of using the Hilbert transform method to remove negative frequency components. We note that nothing prevents us from computing formula_27 for a complex-valued formula_1. But it might not be a reversible representation, because the original spectrum is not symmetrical in general. So except for this example, the general discussion assumes real-valued formula_1.\n\nThen:\n\nSince formula_34, restoring the negative frequency components is a simple matter of discarding formula_35 which may seem counter-intuitive. We can also note that the complex conjugate formula_36 comprises \"only\" the negative frequency components. And therefore formula_37 restores the suppressed positive frequency components.\n\nAn analytic signal can also be expressed in polar coordinates, in terms of its time-variant magnitude and phase angle:\nwhere:\n\nIn the accompanying diagram, the blue curve depicts formula_1 and the red curve depicts the corresponding formula_42.\n\nThe time derivative of the unwrapped instantaneous phase has units of \"radians/second\", and is called the \"instantaneous angular frequency\":\n\nThe \"instantaneous frequency\" (in hertz) is therefore:\n\nThe instantaneous amplitude, and the instantaneous phase and frequency are in some applications used to measure and detect local features of the signal. Another application of the analytic representation of a signal relates to demodulation of modulated signals. The polar coordinates conveniently separate the effects of amplitude modulation and phase (or frequency) modulation, and effectively demodulates certain kinds of signals.\n\nAnalytic signals are often shifted in frequency (down-converted) toward 0 Hz, possibly creating [non-symmetrical] negative frequency components:\nwhere formula_46 is an arbitrary reference angular frequency.\n\nThis function goes by various names, such as \"complex envelope\" and \"complex baseband\". The complex envelope is not unique; it is determined by the choice of formula_46. This concept is often used when dealing with passband signals. If formula_1 is a modulated signal, formula_46 might be equated to its carrier frequency. \n\nIn other cases, formula_46 is selected to be somewhere in the middle of the desired passband. Then a simple low-pass filter with real coefficients can excise the portion of interest. Another motive is to reduce the highest frequency, which reduces the minimum rate for alias-free sampling. A frequency shift does not undermine the mathematical tractability of the complex signal representation. So in that sense, the down-converted signal is still \"analytic\". However, restoring the real-valued representation is no longer a simple matter of just extracting the real component. Up-conversion may be required, and if the signal has been sampled (discrete-time), interpolation (upsampling) might also be necessary to avoid aliasing.\n\nIf formula_46 is chosen larger than the highest frequency of formula_52 then formula_53 has no positive frequencies. In that case, extracting the real component restores them, but in reverse order; the low-frequency components are now high ones and vice versa. This can be used to demodulate a type of single sideband signal called \"lower sideband\" or \"inverted sideband\".\n\n\nSometimes formula_46 is chosen to minimize\n\nAlternatively, formula_46 can be chosen to minimize the mean square error in linearly approximating the \"unwrapped\" instantaneous phase formula_57:\nor another alternative (for some optimum formula_59):\n\nIn the field of time-frequency signal processing, it was shown that the analytic signal was needed in the definition of the Wigner–Ville distribution so that the method can have the desirable properties needed for practical applications.\n\nSometimes the phrase \"complex envelope\" is given the simpler meaning of the complex amplitude of a (constant-frequency) phasor;\nother times the complex envelope formula_61 as defined above is interpreted as a time-dependent generalization of the complex amplitude. Their relationship is not unlike that in the real-valued case: varying envelope generalizing constant amplitude.\n\nThe concept of analytic signal is well-defined for signals of a single variable which typically is time. For signals of two or more variables, an analytic signal can be defined in different ways, and two approaches are presented below.\n\nA straightforward generalization of the analytic signal can be done for a multi-dimensional signal once it is established what is meant by \"negative frequencies\" for this case. This can be done by introducing a unit vector formula_62 in the Fourier domain and label any frequency vector formula_63 as negative if formula_64. The analytic signal is then produced by removing all negative frequencies and multiply the result by 2, in accordance to the procedure described for the case of one-variable signals. However, there is no particular direction for formula_62 which must be chosen unless there are some additional constraints. Therefore, the choice of formula_62 is ad hoc, or application specific.\n\nThe real and imaginary parts of the analytic signal correspond to the two elements of the vector-valued monogenic signal, as it is defined for one-variable signals. However, the monogenic signal can be extended to arbitrary number of variables in a straightforward manner, producing an -dimensional vector-valued function for the case of \"n\"-variable signals.\n\n\n\n\n"}
{"id": "1525019", "url": "https://en.wikipedia.org/wiki?curid=1525019", "title": "Apostolos Doxiadis", "text": "Apostolos Doxiadis\n\nApostolos K. Doxiadis (; born 1953) is a Greek writer. He is best known for his international bestsellers \"Uncle Petros and Goldbach's Conjecture\" (2000) and \"Logicomix\" (2009).\n\nDoxiadis was born in Australia, where his father, the architect Constantinos Apostolou Doxiadis was working. Soon after his birth, the family returned to Athens, where Doxiadis grew up. Though his earliest interests were in poetry, fiction and the theatre, an intense interest in mathematics led Doxiadis to leave school at age fifteen, to attend Columbia University, in New York, from which he obtained a bachelor's degree in Mathematics in May 1972. He then attended the École Pratique des Hautes Études in Paris from which he got a master's degree, with a thesis on the mathematical modeling of the nervous system. His father’s death and family reasons made him return to Greece in 1975, interrupting his graduate studies. In Greece, although involved for some years with the computer software industry, Doxiadis returned to his childhood and adolescence loves of theatre and the cinema, before becoming a full-time writer.\n\nDoxiadis began to write in Greek. His first published work was \"A Parallel Life\" (\"Βίος Παράλληλος\", 1985), a novella set in the monastic communities of 4th century CE Egypt. His first novel, \"Makavettas\" (\"Μακαβέττας\", 1988), recounted the adventures of a fictional power-hungry colonel at the time of the Greek military junta of 1967–1974. Written in a tongue-in-cheek imitation of Greek folk military memoirs, such as that of Yannis Makriyannis, it follows the plot of Shakespeare’s \"Macbeth\", of which the eponymous hero’s name is a Hellenized form. Doxiadis next novel, \"Uncle Petros and Goldbach’s Conjecture\" (\"Ο Θείος Πέτρος και η Εικασία του Γκόλντμπαχ\", 1992), was the first long work of fiction whose plot takes place in the world of pure mathematics research. The first Greek critics did not find the mathematical themes appealing, and it received mediocre reviews, unlike Doxiadis’s first two works, which were well received. Τhe novella \"The Three Little Men\" (\"Τα Τρία Ανθρωπάκια\", 1998), attempts a modern-day retelling of the tale of a classic fairy-tale.\n\nIn 1998, Doxiadis translated into English, significantly re-working, his third novel, which was published in England in 2000 as \"Uncle Petros and Goldbach's Conjecture\" (UK publisher: Faber and Faber; United States publisher: Bloomsbury USA.) The book became an international bestseller, and has been published to date in more than thirty-five languages. It has received the praise of, among others, Nobel Laureate John Nash, British mathematician Sir Michael Atiyah, critic George Steiner and psychiatrist Oliver Sacks. \"Uncle Petros\" is one of the \"1001 Books You Must Read Before You Die\".\nDoxiadis’ next project, which took over five years to complete, was the graphic novel \"Logicomix\" (2009), a number one bestseller on the New York Times Bestseller List and an international bestseller, already published in over twenty languages. \"Logicomix\" was co-authored with computer scientist Christos Papadimitriou, with art work by Alecos Papadatos (pencils) and Annie Di Donna (color). Renowned comics historian and critic R. C. Harvey, in the \"Comics Journal\", called \"Logicomix\" “a tour-de-force” a “virtuoso performance”, while \"The Sunday Times\"’ Brian Appleyard called it “probably the best and certainly the most extraordinary graphic novel” he has read. \"Logicomix\" is one of Paul Gravett’s \"1001 Comics You Must Read Before you Die.\" \n\nIn the early stage of his career, Doxiadis directed in the professional theatre, in Athens, and worked as translator, translating, among other plays, William Shakespeare’s \"Romeo and Juliet\", \"Hamlet\" and \"Midsummer Night’s Dream\", as well as Eugene O’Neill’s \"Mourning Becomes Electra\".\n\nHe has written two plays for the theatre. The first was a full-length shadow-puppet play \"The Tragical History of Jackson Pollock, Abstract Expressionist\" (1999), in English, of which he also designed and directed the Athens performance. In this play, Doxiadis realized some of his views on “epic theatre”, in other words a theatre based on storytelling. His second play, \"Incompleteness\" (2005), is an imaginary account of the last seventeen days in the life of the great logician Kurt Gödel, which Gödel spent in a Princeton, New Jersey, hospital, refusing to eat out of fear that he was being poisoned. The play was staged in Athens, in 2006, as Dekati Evdomi Nyhta (Seventeenth Night) with the actor Yorgos Kotanidis in the role of Kurt Gödel.\n\nDoxiadis has also written and directed two feature-length films, in Greek, \"Underground Passage\" (\"Υπόγεια Διαδρομή\", 1983) and \"Terirem\" (\"Τεριρέμ\", 1987). The latter won the CICAE (International Confederation of Art Cinemas) prize for Best Film in the 1988 Berlin International Film Festival.\n\nDoxiadis has a lifelong interest in logic, cognitive psychology and rhetoric, as well as the theoretical study of narrative. In 2007, he organized, with mathematician Barry Mazur, a meeting on the theoretical investigation of the relationship of mathematics and narrative, whose proceedings were published as \"Circles Disturbed, The Interplay of Mathematics and Narrative\" (2012). Doxiadis has lectured extensively on his theoretical interests. Doxiadis’ recent work has led him to formulate a theory about the development of deductive proof in Classical Greece, which lays emphasis on influences from pre-existing patterns in narrative and, especially, Archaic Age Poetry.\n\n\"Uncle Petros and Goldbach’s Conjecture\" was the first recipient of the Premio Peano the first international award for books inspired by mathematics and short-listed for the Prix Médicis. \"Logicomix\" has earned numerous awards, among them the Bertrand Russell Society Award, the Royal Booksellers Association Award (the Netherlands), the New Atlantic Booksellers Award (USA), the Prix Tangente (France), the Premio Carlo Boscarato (Italy), the Comicdom Award (Greece). It was chosen as \"Book of the Year\" by \"TIME Magazine\", \"Publishers Weekly\", \"The Washington Post\", \"The Financial Times\", \"The Globe and Mail\", and other publications.\n\n"}
{"id": "12928081", "url": "https://en.wikipedia.org/wiki?curid=12928081", "title": "Applied Probability Trust", "text": "Applied Probability Trust\n\nThe Applied Probability Trust is a UK-based non-profit foundation for study and research in the mathematical sciences, founded in 1964 and based at the University of Sheffield. It publishes two specialist and two general interest journals.\n\n\n"}
{"id": "1711336", "url": "https://en.wikipedia.org/wiki?curid=1711336", "title": "Apéry's theorem", "text": "Apéry's theorem\n\nIn mathematics, Apéry's theorem is a result in number theory that states the Apéry's constant ζ(3) is irrational. That is, the number\ncannot be written as a fraction \"p\"/\"q\" with \"p\" and \"q\" being integers.\n\nThe special values of the Riemann zeta function at even integers \"2n\" (\"n > 0\") can be shown in terms of Bernoulli numbers to be irrational, while it remains open whether the function's values are in general rational or not at the odd integers \"2n + 1\" (\"n\" > 1) (though they are conjectured to be irrational).\n\nEuler proved that if \"n\" is a positive integer then\nfor some rational number \"p\"/\"q\". Specifically, writing the infinite series on the left as ζ(2\"n\") he showed\nwhere the \"B\" are the rational Bernoulli numbers. Once it was proved that π is always irrational this showed that ζ(2\"n\") is irrational for all positive integers \"n\".\n\nNo such representation in terms of π is known for the so-called zeta constants for odd arguments, the values ζ(2\"n\"+1) for positive integers \"n\". It has been conjectured that the ratios of these quantities\nare transcendental for every integer \"n\" ≥ 1.\n\nBecause of this, no proof could be found to show that the zeta constants with odd arguments were irrational, even though they were (and still are) all believed to be transcendental. However, in June 1978, Roger Apéry gave a talk titled \"Sur l'irrationalité de ζ(3).\" During the course of the talk he outlined proofs that ζ(3) and ζ(2) were irrational, the latter using methods simplified from those used to tackle the former rather than relying on the expression in terms of π. Due to the wholly unexpected nature of the result and Apéry's blasé and very sketchy approach to the subject many of the mathematicians in the audience dismissed the proof as flawed. However Henri Cohen, Hendrik Lenstra, and Alfred van der Poorten suspected Apéry was on to something and set out to confirm his proof. Two months later they finished verification of Apéry's proof, and on August 18 Cohen delivered a lecture giving full details of the proof. After the lecture Apéry himself took to the podium to explain the source of some of his ideas.\n\nApéry's original proof was based on the well known irrationality criterion from Peter Gustav Lejeune Dirichlet, which states that a number ξ is irrational if there are infinitely many coprime integers \"p\" and \"q\" such that\nfor some fixed \"c\",δ>0.\n\nThe starting point for Apéry was the series representation of ζ(3) as\nRoughly speaking, Apéry then defined a sequence \"c\" which converges to ζ(3) about as fast as the above series, specifically\nHe then defined two more sequences \"a\" and \"b\" that, roughly, have the quotient \"c\". These sequences were\nand\nThe sequence \"a\"/\"b\" converges to ζ(3) fast enough to apply the criterion, but unfortunately \"a\" is not an integer after \"n\"=2. Nevertheless, Apéry showed that even after multiplying \"a\" and \"b\" by a suitable integer to cure this problem the convergence was still fast enough to guarantee irrationality.\n\nWithin a year of Apéry's result an alternative proof was found by Frits Beukers, who replaced Apéry's series with integrals involving the shifted Legendre polynomials formula_10. Using a representation that would later be generalized to Hadjicostas's formula, Beukers showed that\nfor some integers \"A\" and \"B\" (sequences and ). Using partial integration and the assumption that ζ(3) was rational and equal to \"a\"/\"b\", Beukers eventually derived the inequality\nwhich is a contradiction since the right-most expression tends to zero and so must eventually fall below 1/\"b\".\n\nA more recent proof by Wadim Zudilin is more reminiscent of Apéry's original proof, and also has similarities to a fourth proof by Yuri Nesterenko. These later proofs again derive a contradiction from the assumption that ζ(3) is rational by constructing sequences that tend to zero but are bounded below by some positive constant. They are somewhat less transparent than the earlier proofs, relying as they do on hypergeometric series.\n\nApéry and Beukers could simplify their proofs to work on ζ(2) as well thanks to the series representation\nDue to the success of Apéry's method a search was undertaken for a number ξ with the property that\nIf such a ξ were found then the methods used to prove Apéry's theorem would be expected to work on a proof that ζ(5) is irrational. Unfortunately, extensive computer searching has failed to find such a constant, and in fact it is now known that if ξ exists and if it is an algebraic number of degree at most 25, then the coefficients in its minimal polynomial must be enormous, at least 10, so extending Apéry's proof to work on the higher odd zeta constants doesn't seem likely to work.\n\nDespite this, many mathematicians working in this area expect a breakthrough sometime soon. Indeed, recent work by Wadim Zudilin and Tanguy Rivoal has shown that infinitely many of the numbers ζ(2\"n\"+1) must be irrational, and even that at least one of the numbers ζ(5), ζ(7), ζ(9), and ζ(11) must be irrational. Their work uses linear forms in values of the zeta function and estimates upon them to bound the dimension of a vector space spanned by values of the zeta function at odd integers. Hopes that Zudilin could cut his list further to just one number did not materialise, but work on this problem is still an active area of research. Higher zeta constants have application to physics: they describe correlation functions in quantum spin chains.\n"}
{"id": "15890853", "url": "https://en.wikipedia.org/wiki?curid=15890853", "title": "Bateman transform", "text": "Bateman transform\n\nIn the mathematical study of partial differential equations, the Bateman transform is a method for solving the Laplace equation in four dimensions and wave equation in three by using a line integral of a holomorphic function in three complex variables. It is named after the English mathematician Harry Bateman, who first published the result in .\n\nThe formula asserts that if \"ƒ\" is a holomorphic function of three complex variables, then\n\nis a solution of the Laplace equation, which follows by differentiation under the integral. Furthermore, Bateman asserted that the most general solution of the Laplace equation arises in this way.\n\n"}
{"id": "33447798", "url": "https://en.wikipedia.org/wiki?curid=33447798", "title": "Canberra distance", "text": "Canberra distance\n\nThe Canberra distance is a numerical measure of the distance between pairs of points in a vector space, introduced in 1966\nand refined in 1967 by G. N. Lance and W. T. Williams. It is a weighted version of \"L\"₁ (Manhattan) distance.\nThe Canberra distance has been used as a metric for comparing ranked lists and for intrusion detection in computer security.\n\nThe Canberra distance \"d\" between vectors p and q in an \"n\"-dimensional real vector space is given as follows:\n\nwhere \nare vectors.\n\nThe Canberra metric, Adkins form, divides the distance d by (n-Z) where Z is the number of attributes that are 0 for p and q.\n\n"}
{"id": "1170166", "url": "https://en.wikipedia.org/wiki?curid=1170166", "title": "Chirality (chemistry)", "text": "Chirality (chemistry)\n\nChirality is a geometric property of some molecules and ions. A chiral molecule/ion is non-superposable on its mirror image. The presence of an asymmetric carbon center is one of several structural features that induce chirality in organic and inorganic molecules. The term \"chirality\" is derived from the Ancient Greek word for hand, χεῖρ (\"kheir\").\n\nThe mirror images of a chiral molecule or ion are called enantiomers or optical isomers. Individual enantiomers are often designated as either right-handed or left-handed. Chirality is an essential consideration when discussing the stereochemistry in organic and inorganic chemistry. The concept is of great practical importance because most biomolecules and pharmaceuticals are chiral.\n\nChiral molecules and ions are described by various ways of designating their absolute configuration, which codify either the entity's geometry or its ability to rotate plane-polarized light, a common technique in studying chirality.\n\nChirality is based on molecular symmetry elements. Specifically, a chiral compound can contain no improper axis of rotation (S), which includes planes of symmetry and inversion center. Chiral molecules are always dissymmetric (lacking S) but not always asymmetric (lacking all symmetry elements except the trivial identity). Asymmetric molecules are always chiral.\n\nIn general, chiral molecules have point chirality at a single \"stereogenic\" atom, which has four different substituents. The two enantiomers of such compounds are said to have different absolute configurations at this center. This center is thus stereogenic (i.e., a grouping within a molecular entity that may be considered a focus of stereoisomerism). The stereogenic atom (also known as the \"stereocenter\") is usually carbon, as in many biological molecules. However a stereocenter can coincide with any atom, including metals (as in many chiral coordination compounds), phosphorus, or sulfur. The low barrier of nitrogen inversion make most \"N\"-chiral amines (NRR′R″) impossible to resolve, but \"P\"-chiral phosphines (PRR′R″) as well as S-chiral sulfoxides (OSRR′) are optically stable.\n\nWhile the presence of a stereogenic atom describes the great majority of chiral molecules, many variations and exceptions exist. For instance it is not necessary for the chiral substance to have a stereogenic atom. Examples include 1-bromo-3-chloro-5-fluoroadamantane, methylethylphenyltetrahedrane, certain calixarenes and fullerenes, which have inherent chirality. The C-symmetric species 1,1′-bi-2-naphthol (BINOL), 1,3-dichloroallene have axial chirality. (\"E\")-cyclooctene and many ferrocenes have planar chirality.\n\nWhen the optical rotation for an enantiomer is too low for practical measurement, the species is said to exhibit cryptochirality.\n\nEven isotopic differences must be considered when examining chirality. Illustrative is the derivative of benzyl alcohol PhCHDOH, which is chiral. The \"S\" enantiomer has [\"α\"] = +0.715°.\n\nMany biologically active molecules are chiral, including the naturally occurring amino acids (the building blocks of proteins) and sugars. In biological systems, most of these compounds are of the same chirality: most amino acids are levorotatory () and sugars are dextrorotatory (). Typical naturally occurring proteins are made of -amino acids and are known as \"left-handed proteins\"; the comparably rarer -amino acids produce \"right-handed proteins\".\n\nThe origin of this homochirality in biology is the subject of much debate. Most scientists believe that Earth life's \"choice\" of chirality was purely random, and that if carbon-based life forms exist elsewhere in the universe, their chemistry could theoretically have opposite chirality. However, there is some suggestion that early amino acids could have formed in comet dust. In this case, circularly polarised radiation (which makes up 17% of stellar radiation) could have caused the selective destruction of one chirality of amino acids, leading to a selection bias which ultimately resulted in all life on Earth being homochiral.\n\nEnzymes, which are chiral, often distinguish between the two enantiomers of a chiral substrate. One could imagine an enzyme as having a glove-like cavity that binds a substrate. If this glove is right-handed, then one enantiomer will fit inside and be bound, whereas the other enantiomer will have a poor fit and is unlikely to bind.\n\n-forms of amino acids tend to be tasteless, whereas -forms tend to taste sweet. Spearmint leaves contain the -enantiomer of the chemical carvone or \"R\"-(−)-carvone and caraway seeds contain the -enantiomer or \"S\"-(+)-carvone. These smell different to most people because our olfactory receptors are chiral.\n\nChirality is important in context of ordered phases as well, for example the addition of a small amount of an optically active molecule to a nematic phase (a phase that has long range orientational order of molecules) transforms that phase to a chiral nematic phase (or cholesteric phase). Chirality in context of such phases in polymeric fluids has also been studied in this context.\n\nChirality is a symmetry property, not a characteristic of any part of the periodic table. Thus many inorganic materials, molecules, and ions are chiral. Quartz is an example from the mineral kingdom. Such noncentric materials are of interest for applications in nonlinear optics.\n\nIn the areas of coordination chemistry and organometallic chemistry, chirality is pervasive and of practical importance. A famous example is tris(bipyridine)ruthenium(II) complex in which the three bipyridine ligands adopt a chiral propeller-like arrangement. The two enantiomers of complexes such as [Ru(2,2′-bipyridine)] may be designated as Λ (capital lambda, the Greek version of \"L\") for a left-handed twist of the propeller described by the ligands, and Δ (capital delta, Greek \"D\") for a right-handed twist (pictured).\n\nChiral ligands confer chirality to a metal complex, as illustrated by metal-amino acid complexes. If the metal exhibits catalytic properties, its combination with a chiral ligand is the basis of asymmetric catalysis.\n\nThe term \"optical activity\" is derived from the interaction of chiral materials with polarized light. In a solution, the (−)-form, or levorotatory form, of an optical isomer rotates the plane of a beam of linearly polarized light counterclockwise. The (+)-form, or dextrorotatory form, of an optical isomer does the opposite. The rotation of light is measured using a polarimeter and is expressed as the optical rotation.\n\n\nThe rotation of plane polarized light by chiral substances was first observed by Jean-Baptiste Biot in 1815, and gained considerable importance in the sugar industry, analytical chemistry, and pharmaceuticals. Louis Pasteur deduced in 1848 that this phenomenon has a molecular basis. The term \"chirality\" itself was coined by Lord Kelvin in 1894. Different enantiomers or diastereomers of a compound were formerly called optical isomers due to their different optical properties. At one time, chirality was thought to be associated with organic chemistry, but this misconception was overthrown by the resolution of a purely inorganic compound, hexol, by Alfred Werner.\n\n\n"}
{"id": "12637839", "url": "https://en.wikipedia.org/wiki?curid=12637839", "title": "Chuu-Lian Terng", "text": "Chuu-Lian Terng\n\nChuu-Lian Terng () is a Taiwanese-American mathematician. She received her B.S. from National Taiwan University in 1971 and her Ph.D. from Brandeis University in 1976 under the supervision of Richard Palais, whom she later married. She is currently a professor at University of California at Irvine.\nShe was a professor at Northeastern University for many years. Before joining Northeastern, she spent two years at the University of California, Berkeley and four years at Princeton University. She also spent two years at the Institute for Advanced Study (IAS) in Princeton and two years at the Max-Planck Institute in Bonn, Germany.\n\nIn 1999, her Association for Women in Mathematics press release reads:\n\nHer early research concerned the classification of natural vector bundles and natural differential operators between them. She then became interested in submanifold geometry. Her main contributions are developing a structure theory for isoparametric submanifolds in R and constructing soliton equations from special submanifolds. Recently, Terng and Karen Uhlenbeck (University of Texas at Austin) have developed a general approach to integrable PDEs that explains their hidden symmetries in terms of loop group actions. She is co-author of the book Submanifold Geometry and Critical Point Theory and an editor of the Journal of Differential Geometry survey volume 4 on \"Integrable systems\".\n\nProfessor Terng served as President of the Association for Women in Mathematics (AWM) from 1995 to 1997 and as Member-at-Large of the Council of the American Mathematical Society (AMS) from 1989 to 1992. She is currently on the Advisory Board of the National Center for Theoretical Sciences in Taiwan, the Steering Committee of the IAS/Park City Summer Institute, and the Editorial Board of the Transactions of the AMS.\n\n\n"}
{"id": "58789158", "url": "https://en.wikipedia.org/wiki?curid=58789158", "title": "Clara Latimer Bacon", "text": "Clara Latimer Bacon\n\nDr Clara Latimer Bacon (13 August 1866 – 14 April 1948) was a mathematician and Professor of Mathematics at Goucher College. She was the first woman to be granted a PhD in mathematics from Johns Hopkins University.\n\nBacon was born in Hillsgrove, Illinois, the eldest of four children. Her father was a farmer. \n\nShe graduated from Hedding College in Abingdon, USA, in 1886 with a bachelor's degree. She achieved a second bachelor's degree from Wellesley College in 1890. Bacon studied for her Masters degree at the University of Chicago, completing her thesis in 1903 and graduating in September 1904, after six summers of study. \n\nShe achieved her PhD from Johns Hopkins University in 1911, one of only four women to receive a PhD from the university that year, the first year that women were granted PhDs without special approval from the trustees. At Johns Hopkins, Bacon was a student of the geometer Frank Morley, who was her dissertation adviser. Her thesis was published in American Journal of Mathematics in 1913. Her research in her Masters and PhD theses was on planar geometry.\n\nShe was Emeritus Professor of Mathematics at Goucher College, formerly known as Women's College Baltimore, in Maryland, USA, after working on the faculty from 1897 to 1934. She began teaching there in 1897, at the invitation of Dr John Franklin Goucher, and in 1905 became an associate professor, and in 1914 a full professor.\nBacon was a member of the American Mathematical Society and the Mathematical Association of America. She was president of the Baltimore chapter of the American Association of University Professors and supported the League of Women Voters.\n\nA student hall of residence at Goucher College, Bacon House, is named in her honour.\n\nShe died on 14 April 1948, aged 81.\n"}
{"id": "17404231", "url": "https://en.wikipedia.org/wiki?curid=17404231", "title": "Conjugate Fourier series", "text": "Conjugate Fourier series\n\nIn the mathematical field of Fourier analysis, the conjugate Fourier series arises by realizing the Fourier series formally as the boundary values of the real part of a holomorphic function on the unit disc. The imaginary part of that function then defines the conjugate series. studied the delicate questions of convergence of this series, and its relationship with the Hilbert transform.\n\nIn detail, consider a trigonometric series of the form\n\nin which the coefficients \"a\" and \"b\" are real numbers. This series is the real part of the power series\n\nalong the unit circle with formula_3. The imaginary part of \"F\"(\"z\") is called the conjugate series of \"f\", and is denoted\n\n\n"}
{"id": "30525205", "url": "https://en.wikipedia.org/wiki?curid=30525205", "title": "Constructible topology", "text": "Constructible topology\n\nIn commutative algebra, the constructible topology on the spectrum formula_1 of a commutative ring formula_2 is a topology where each closed set is the image of formula_3 in formula_1 for some algebra \"B\" over \"A\". An important feature of this construction is that the map formula_5 is a closed map with respect to the constructible topology.\n\nWith respect to this topology, formula_1 is a compact, Hausdorff, and totally disconnected topological space. In general the constructible topology is a finer topology than the Zariski topology, but the two topologies will coincide if and only if formula_7 is a von Neumann regular ring, where formula_8 is the nilradical of \"A\".\n\nDespite the terminology being similar, the constructible topology is not the same as the set of all constructible sets.\n\n\n"}
{"id": "25366628", "url": "https://en.wikipedia.org/wiki?curid=25366628", "title": "Cyclograph", "text": "Cyclograph\n\nA cyclograph (also known as an arcograph) is an instrument for drawing arcs of large diameter circles whose centres are inconveniently or inaccessibly located, one version of which was invented by Scottish architect and mathematician Peter Nicholson.\n\nIn his autobiography, published in 1904, polymath Herbert Spencer eloquently describes his own near re-invention of Nicholson's cyclograph while working as a civil engineer for the Birmingham and Gloucester Railway.\n\n"}
{"id": "1519547", "url": "https://en.wikipedia.org/wiki?curid=1519547", "title": "Diocles (mathematician)", "text": "Diocles (mathematician)\n\nDiocles (; c. 240 BC – c. 180 BC) was a Greek mathematician and geometer.\n\nAlthough little is known about the life of Diocles, it is known that he was a contemporary of Apollonius and that he flourished sometime around the end of the 3rd century BC and the beginning of the 2nd century BC. \n\nDiocles is thought to be the first person to prove the focal property of the parabola. His name is associated with the geometric curve called the Cissoid of Diocles, which was used by Diocles to solve the problem of doubling the cube. The curve was alluded to by Proclus in his commentary on Euclid and attributed to Diocles by Geminus as early as the beginning of the 1st century.\n\nFragments of a work by Diocles entitled \"On burning mirrors\" were preserved by Eutocius in his commentary of Archimedes' \"On the Sphere and the Cylinder\". Historically, \"On burning mirrors\" had a large influence on Arabic mathematicians, particularly on al-Haytham, the 11th-century polymath of Cairo whom Europeans knew as \"Alhazen\". The treatise contains sixteen propositions that are proved by conic sections. One of the fragments contains propositions seven and eight, which is a solution to the problem of dividing a sphere by a plane so that the resulting two volumes are in a given ratio. Proposition ten gives a solution to the problem of doubling the cube. This is equivalent to solving a certain cubic equation. Another fragment contains propositions eleven and twelve, which use the cissoid to solve the problem of finding two mean proportionals in between two magnitudes. Since this treatise covers more topics than just burning mirrors, it may be the case that \"On burning mirrors\" is the aggregate of three shorter works by Diocles.\n\n"}
{"id": "3758115", "url": "https://en.wikipedia.org/wiki?curid=3758115", "title": "Edge contraction", "text": "Edge contraction\n\nIn graph theory, an edge contraction is an operation which removes an edge from a graph while simultaneously merging the two vertices that it previously joined. Edge contraction is a fundamental operation in the theory of graph minors. Vertex identification is a less restrictive form of this operation.\n\nThe edge contraction operation occurs relative to a particular edge, \"e\". The edge \"e\" is removed and its two incident vertices, \"u\" and \"v\", are merged into a new vertex \"w\", where the edges incident to \"w\" each correspond to an edge incident to either \"u\" or \"v\". More generally, the operation may be performed on a set of edges by contracting each edge (in any order). \n\nAs defined below, an edge contraction operation may result in a graph with multiple edges even if the original graph was a simple graph. However, some authors disallow the creation of multiple edges, so that edge contractions performed on simple graphs always produce simple graphs.\n\nLet G=(V,E) be a graph (\"or directed graph\") containing an edge \"e\"=(\"u\",\"v\") with \"u\"≠\"v\". Let \"f\" be a function which maps every vertex in V\\{\"u\",\"v\"} to itself, and otherwise, maps it to a new vertex \"w\".\nThe contraction of \"e\" results in a new graph G′=(V′,E′), where V′=(V\\{\"u\",\"v\"})∪{\"w\"}, E′=E\\{\"e\"}, and for every \"x\"∈V, \"x′\"=\"f\"(\"x\")∈V′ is incident to an edge \"e′\"∈E′ if and only if, the corresponding edge, \"e\"∈E is incident to \"x\" in G.\n\nVertex identification (sometimes called vertex contraction) removes the restriction that the \"contraction\" must occur over vertices sharing an incident edge. (Thus, edge contraction is a special case of vertex identification.) The operation may occur on any pair (or subset) of vertices in the graph. Edges between two \"contracting\" vertices are sometimes removed. If v and v' are vertices of distinct components of G, then we can create a new graph G' by identifying v and v' in G as a new vertex v in G'. More generally, given a partition of the vertex set, one can identify vertices in the partition; the resulting graph is known as a quotient graph.\n\nVertex cleaving which is the same as vertex splitting, means one vertex is being split into two, where these two new vertices are adjacent to the vertices that the original vertex was adjacent to. This is the reverse operation of vertex identification.\n\nPath contraction occurs upon the set of edges in a path that \"contract\" to form a single edge between the endpoints of the path. Edges incident to vertices along the path are either eliminated, or arbitrarily (or systematically) connected to one of the endpoints.\n\nGiven two disjoint graphs G and G, where G contains vertices u and v and G contains vertices u and v. Suppose we can obtain the graph G by identifying the vertices u of G and u of G as the vertex u of G and identifying the vertices v of G and v of G as the vertex v of G. In a \"twisting\" G' of G with respect to the vertex set {u, v}, we identify, instead, u with v and v with u.\n\nBoth edge and vertex contraction techniques are valuable in proof by induction on the number of vertices or edges in a graph, where it can be assumed that a property holds for all smaller graphs and this can be used to prove the property for the larger graph.\n\nEdge contraction is used in the recursive formula for the number of spanning trees of an arbitrary connected graph, and in the recurrence formula for the chromatic polynomial of a simple graph.\n\nContractions are also useful in structures where we wish to simplify a graph by identifying vertices that represent essentially equivalent entities. One of the most common examples is the reduction of a general directed graph to an acyclic directed graph by contracting all of the vertices in each strongly connected component. If the relation described by the graph is transitive, no information is lost as long as we label each vertex with the set of labels of the vertices that were contracted to form it.\n\nAnother example is the coalescing performed in global graph coloring register allocation, where vertices are contracted (where it is safe) in order to eliminate move operations between distinct variables.\n\nEdge contraction is used in 3D modelling packages (either manually, or through some feature of the modelling software) to consistently reduce vertex count, aiding in the creation of low-polygon models.\n\n\n"}
{"id": "5811775", "url": "https://en.wikipedia.org/wiki?curid=5811775", "title": "Electrical capacitance tomography", "text": "Electrical capacitance tomography\n\nElectrical capacitance tomography (ECT) is a method for determination of the dielectric permittivity distribution in the interior of an object from external capacitance measurements. It is a close relative of electrical impedance tomography and is proposed as a method for industrial process monitoring, although it has yet to see widespread use. Potential applications include the measurement of flow of fluids in pipes and measurement of the concentration of one fluid in another, or the distribution of a solid in a fluid. \n\nAlthough capacitance sensing methods were in widespread use the idea of using capacitance measurement to form images is attributed to Maurice Beck and co-workers at UMIST in the 1980s.\n\nAlthough usually called tomography, the technique differs from conventional tomographic methods, in which high resolution images are formed of slices of a material. The measurement electrodes, which are metallic plates, must be sufficiently large to give a measureable change in capacitance. This means that very few electrodes are used and eight or twelve electrodes is common. An N-electrode system can only provide N(N−1)/2 independent measurements. This means that the technique is limited to producing very low resolution images of approximate slices. However, ECT is fast, and relatively inexpensive.\n\n"}
{"id": "1114931", "url": "https://en.wikipedia.org/wiki?curid=1114931", "title": "Exotic sphere", "text": "Exotic sphere\n\nIn differential topology, an exotic sphere is a differentiable manifold \"M\" that is homeomorphic but not diffeomorphic to the standard Euclidean n-sphere. That is, \"M\" is a sphere from the point of view of all its topological properties, but carrying a smooth structure that is not the familiar one (hence the name \"exotic\").\n\nThe first exotic spheres were constructed by in dimension formula_1 as formula_2-bundles over formula_3. He showed that there are at least 7 differentiable structures on the 7-sphere. In any dimension showed that the diffeomorphism classes of oriented exotic spheres form the non-trivial elements of an abelian monoid under connected sum, which is a finite abelian group if the dimension is not 4. The classification of exotic spheres by showed that the oriented exotic 7-spheres are the non-trivial elements of a cyclic group of order 28 under the operation of connected sum.\n\nThe unit \"n\"-sphere, formula_4, is the set of all (\"n\"+1)-tuples formula_5 of real numbers, such that the sum formula_6. (formula_7 is a circle; formula_8 is the surface of an ordinary ball of radius one in 3 dimensions.) Topologists consider a space, \"X\", to be an \"n\"-sphere if every point in \"X\" can be assigned to exactly one point in the unit \"n\"-sphere in a continuous way, which means that sufficiently nearby points in \"X\" get assigned to nearby points in \"S\" and vice versa. For example, a point \"x\" on an \"n\"-sphere of radius \"r\" can be matched with a point on the unit \"n\"-sphere by adjusting its distance from the origin by formula_9.\n\nIn differential topology, a more stringent condition is added, that the functions matching points in \"X\" with points in formula_4 should be smooth, that is they should have derivatives of all orders everywhere. To calculate derivatives, one needs to have local coordinate systems defined consistently in \"X\". Mathematicians were surprised in 1956 when Milnor showed that consistent coordinate systems could be set up on the 7-sphere in two different ways that were equivalent in the continuous sense, but not in the differentiable sense. Milnor and others set about trying to discover how many such exotic spheres could exist in each dimension and to understand how they relate to each other. No exotic structures are possible on the 1-, 2-, 3-, 5-, 6- or 12-spheres. Some higher-dimensional spheres have only two possible differentiable structures, others have thousands. Whether exotic 4-spheres exist, and if so how many, is an unsolved problem.\n\nThe monoid of smooth structures on \"n\"-spheres is the collection of oriented smooth \"n\"-manifolds which are homeomorphic to the \"n\"-sphere, taken up to orientation-preserving diffeomorphism. The monoid operation is the connected sum. Provided formula_11, this monoid is a group and is isomorphic to the group formula_12 of \"h\"-cobordism classes of oriented homotopy \"n\"-spheres, which is finite and abelian. In dimension 4 almost nothing is known about the monoid of smooth spheres, beyond the facts that it is finite or countably infinite, and abelian, though it is suspected to be infinite; see the section on Gluck twists. All homotopy \"n\"-spheres are homeomorphic to the \"n\"-sphere by the generalized Poincaré conjecture, proved by Stephen Smale in dimensions bigger than 4, Michael Freedman in dimension 4, and Grigori Perelman in dimension 3. In dimension 3, Edwin E. Moise proved that every topological manifold has an essentially unique smooth structure (see Moise's theorem), so the monoid of smooth structures on the 3-sphere is trivial.\n\nThe group formula_12 has a cyclic subgroup\n\nrepresented by \"n\"-spheres that bound parallelizable manifolds. The structures of formula_14 and the quotient\n\nare described separately in the paper , which was influential in the development of surgery theory. In fact, these calculations can be formulated in a modern language in terms of the surgery exact sequence as indicated here.\n\nThe group formula_14 is a cyclic group, and is trivial or order 2 except in case formula_18, in which case it can be large, with its order related to the Bernoulli numbers. It is trivial if \"n\" is even. If \"n\" is 1 mod 4 it has order 1 or 2; in particular it has order 1 if \"n\" is 1, 5, 13, 29, or 61, and proved that it has order 2 if is not of the form . It follows from the now almost completely resolved Kervaire invariant problem that it has order 2 for all \"n\" bigger than 125; the case is still open.\nThe order of \"bP\" for is\n\nwhere \"B\" is the numerator of , and \"B\" is a Bernoulli number. (The formula in the topological literature differs slightly because topologists use a different convention for naming Bernoulli numbers; this article uses the number theorists' convention.)\n\nThe quotient group Θ/\"bP\" has a description in terms of stable homotopy groups of spheres modulo the image of the J-homomorphism; it is either equal to the quotient or index 2. More precisely there is an injective map\nwhere π is the \"n\"th stable homotopy group of spheres, and \"J\" is the image of the \"J\"-homomorphism. As with \"bP\", the image of \"J\" is a cyclic group, and is trivial or order 2 except in case formula_21 in which case it can be large, with its order related to the Bernoulli numbers. The quotient group formula_22 is the \"hard\" part of the stable homotopy groups of spheres, and accordingly formula_16 is the hard part of the exotic spheres, but almost completely reduces to computing homotopy groups of spheres. The map is either an isomorphism (the image is the whole group), or an injective map with index 2. The latter is the case if and only if there exists an \"n\"-dimensional framed manifold with Kervaire invariant 1, which is known as the Kervaire invariant problem. Thus a factor of 2 in the classification of exotic spheres depends on the Kervaire invariant problem.\n\n, the Kervaire invariant problem is almost completely solved, with only the case remaining open; see that article for details. This is primarily the work of , which proved that such manifolds only existed in dimension , and , which proved that there were no such manifolds for dimension and above. Manifolds with Kervaire invariant 1 have been constructed in dimension 2, 6, 14, 30, and 62, but dimension 126 is open, with no manifold being either constructed or disproven.\n\nThe order of the group Θ is given in this table from (except that the entry for is wrong by a factor of 2 in their paper; see the correction in volume III p. 97 of Milnor's collected works).\n\nNote that for dim \"n = 4k-1\", then Θ are 28 = 2(2-1), 992 = 2(2-1), 16256 = 2(2-1), and 523264 = 2(2-1). Further entries in this table can be computed from the information above together with the table of stable homotopy groups of spheres.\n\nOne of the first examples of an exotic sphere found by was the following: Take two copies of \"B\"×\"S\", each with boundary \"S\"×\"S\", and glue them together by identifying (\"a\",\"b\") in the boundary with (\"a\", \"a\"\"ba\"), (where we identify each \"S\" with the group of unit quaternions). The resulting manifold has a natural smooth structure and is homeomorphic to \"S\", but is not diffeomorphic to \"S\". Milnor showed that it is not the boundary of any smooth 8-manifold with vanishing 4th Betti number, and has no orientation-reversing diffeomorphism to itself; either of these properties implies that it is not a standard 7-sphere. Milnor showed that this manifold has a Morse function with just two critical points, both non-degenerate, which implies that it is topologically a sphere.\n\nAs shown by (see also ) the intersection of the complex manifold of points in C satisfying\nwith a small sphere around the origin for \"k\" = 1, 2, ..., 28 gives all 28 possible smooth structures on the oriented 7-sphere. Similar manifolds are called Brieskorn spheres.\n\nGiven an (orientation-preserving) diffeomorphism , gluing the boundaries of two copies of the standard disk \"D\" together by \"f\" yields a manifold called a \"twisted sphere\" (with \"twist\" \"f\"). It is homotopy equivalent to the standard \"n\"-sphere because the gluing map is homotopic to the identity (being an orientation-preserving diffeomorphism, hence degree 1), but not in general diffeomorphic to the standard sphere. \nSetting formula_25 to be the group of twisted \"n\"-spheres (under connect sum), one obtains the exact sequence\n\nFor , every exotic \"n\"-sphere is diffeomorphic to a twisted sphere, a result proven by Stephen Smale which can be seen as a consequence of the \"h\"-cobordism theorem. (In contrast, in the piecewise linear setting the left-most map is onto via radial extension: every piecewise-linear-twisted sphere is standard.) The group Γ of twisted spheres is always isomorphic to the group Θ. The notations are different because it was not known at first that they were the same for ; for example, the case is equivalent to the Poincaré conjecture.\n\nIn 1970 Jean Cerf proved the pseudoisotopy theorem which implies that formula_27 is the trivial group provided formula_28, so formula_29 provided formula_28.\n\nIf \"M\" is a piecewise linear manifold then the problem of finding the compatible smooth structures on \"M\" depends on knowledge of the groups . More precisely, the obstructions to the existence of any smooth structure lie in the groups for various values of \"k\", while if such a smooth structure exists then all such smooth structures can be classified using the groups .\nIn particular the groups Γ vanish if , so all PL manifolds of dimension at most 7 have a smooth structure, which is essentially unique if the manifold has dimension at most 6.\n\nThe following finite abelian groups are essentially the same:\n\nIn 4 dimensions it is not known whether there are any exotic smooth structures on the 4-sphere. The statement that they do not exist is known as the \"smooth Poincaré conjecture\", and is discussed by who say that it is believed to be false.\n\nSome candidates proposed for exotic 4-spheres are the Cappell–Shaneson spheres () and those derived by Gluck twists . Gluck twist spheres are constructed by cutting out a tubular neighborhood of a 2-sphere \"S\" in \"S\" and gluing it back in using a diffeomorphism of its boundary \"S\"×\"S\". The result is always homeomorphic to \"S\". Many cases over the years were ruled out as possible counterexamples to the smooth 4 dimensional Poincaré conjecture. For example, , , , , , , , .\n\n\n\n\n"}
{"id": "542526", "url": "https://en.wikipedia.org/wiki?curid=542526", "title": "Flat map", "text": "Flat map\n\nThe flat map in differential geometry is a name for the mapping that converts vectors into corresponding 1-forms, given a non-degenerate (0,2)-tensor. \n\n"}
{"id": "21452055", "url": "https://en.wikipedia.org/wiki?curid=21452055", "title": "Forcing function (differential equations)", "text": "Forcing function (differential equations)\n\nIn a system of differential equations used to describe a time-dependent process, a forcing function is a function that appears in the equations and is only a function of time, and not of any of the other variables. In effect, it is a constant for each value of \"t\".\n\nIn the more general case, any nonhomogeneous source function in any variable can be described as a forcing function, and the resulting solution can often be determined using a superposition of linear combinations of the homogeneous solutions and the forcing term.\n"}
{"id": "412108", "url": "https://en.wikipedia.org/wiki?curid=412108", "title": "Hessian matrix", "text": "Hessian matrix\n\nIn mathematics, the Hessian matrix or Hessian is a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field. It describes the local curvature of a function of many variables. The Hessian matrix was developed in the 19th century by the German mathematician Ludwig Otto Hesse and later named after him. Hesse originally used the term \"functional determinants\".\n\nSuppose is a function taking as input a vector and outputting a scalar ; if all second partial derivatives of exist and are continuous over the domain of the function, then the Hessian matrix of is a square matrix, usually defined and arranged as follows:\n\nor, by stating an equation for the coefficients using indices i and j:\n\nThe determinant of the above matrix is also sometimes referred to as the Hessian.\n\nThe Hessian matrix can be considered related to the Jacobian matrix by .\n\nThe mixed derivatives of \"f\" are the entries off the main diagonal in the Hessian. Assuming that they are continuous, the order of differentiation does not matter (Schwarz's theorem). For example,\n\nIn a formal statement: if the second derivatives of are all continuous in a neighborhood , then the Hessian of is a symmetric matrix throughout ; see symmetry of second derivatives.\n\nThe Hessian matrix of a convex function is positive semi-definite. Refining this property allows us to test if a critical point is a local maximum, local minimum, or a saddle point, as follows:\n\nIf the Hessian is positive definite at , then attains an isolated local minimum at . If the Hessian is negative definite at , then attains an isolated local maximum at . If the Hessian has both positive and negative eigenvalues then is a saddle point for . Otherwise the test is inconclusive. This implies that, at a local minimum (respectively, a local maximum), the Hessian is positive-semi-definite (respectively, negative semi-definite).\n\nNote that for positive semidefinite and negative semidefinite Hessians the test is inconclusive (a critical point where the Hessian is semidefinite but not definite may be a local extremum or a saddle point). However, more can be said from the point of view of Morse theory.\n\nThe second derivative test for functions of one and two variables is simple. In one variable, the Hessian contains just one second derivative; if it is positive then is a local minimum, and if it is negative then is a local maximum; if it is zero then the test is inconclusive. In two variables, the determinant can be used, because the determinant is the product of the eigenvalues. If it is positive then the eigenvalues are both positive, or both negative. If it is negative then the two eigenvalues have different signs. If it is zero, then the second derivative test is inconclusive.\n\nEquivalently, the second-order conditions that are sufficient for a local minimum or maximum can be expressed in terms of the sequence of principal (upper-leftmost) minors (determinants of sub-matrices) of the Hessian; these conditions are a special case of those given in the next section for bordered Hessians for constrained optimization—the case in which the number of constraints is zero. Specifically, the sufficient condition for a minimum is that all of these principal minors be positive, while the sufficient condition for a maximum is that the minors alternate in sign with the 1×1 minor being negative.\n\nIf the gradient (the vector of the partial derivatives) of a function is zero at some point , then has a \"critical point\" (or \"stationary point\") at . The determinant of the Hessian at is then called the discriminant. If this determinant is zero then is called a \"degenerate critical point\" of , or a \"non-Morse critical point\" of . Otherwise it is non-degenerate, and called a \"Morse critical point\" of .\n\nThe Hessian matrix plays an important role in Morse theory and catastrophe theory, because its kernel and eigenvalues allow classification of the critical points.\n\nHessian matrices are used in large-scale optimization problems within Newton-type methods because they are the coefficient of the quadratic term of a local Taylor expansion of a function. That is,\n\nwhere is the gradient . Computing and storing the full Hessian matrix takes memory, which is infeasible for high-dimensional functions such as the loss functions of neural nets, conditional random fields, and other statistical models with large numbers of parameters. For such situations, truncated-Newton and quasi-Newton algorithms have been developed. The latter family of algorithms use approximations to the Hessian; one of the most popular quasi-Newton algorithms is BFGS.\n\nSuch approximations may use the fact that an optimization algorithm uses the Hessian only as a linear operator , and proceed by first noticing that the Hessian also appears in the local expansion of the gradient:\n\nLetting for some scalar , this gives\n\ni.e.,\n\nso if the gradient is already computed, the approximate Hessian can be computed by a linear (in the size of the gradient) number of scalar operations. (While simple to program, this approximation scheme is not numerically stable since has to be made small to prevent error due to the formula_8 term, but decreasing it loses precision in the first term.)\n\nThe Hessian matrix is commonly used for expressing image processing operators in image processing and computer vision (see the Laplacian of Gaussian (LoG) blob detector, the determinant of Hessian (DoH) blob detector and scale space).\n\nA bordered Hessian is used for the second-derivative test in certain constrained optimization problems. Given the function considered previously, but adding a constraint function such that , the bordered Hessian is the Hessian of the Lagrange function formula_9:\n\nIf there are, say, \"m\" constraints then the zero in the upper-left corner is an \"m\" × \"m\" block of zeroes, and there are \"m\" border rows at the top and \"m\" border columns at the left.\n\nThe above rules stating that extrema are characterized (among critical points with a non-singular Hessian) by a positive-definite or negative-definite Hessian cannot apply here since a bordered Hessian can neither be negative-definite nor positive-definite, as formula_11 if formula_12 is any vector whose sole non-zero entry is its first.\n\nThe second derivative test consists here of sign restrictions of the determinants of a certain set of \"n – m\" submatrices of the bordered Hessian. Intuitively, one can think of the \"m\" constraints as reducing the problem to one with \"n – m\" free variables. (For example, the maximization of subject to the constraint can be reduced to the maximization of without constraint.)\n\nSpecifically, sign conditions are imposed on the sequence of leading principal minors (determinants of upper-left-justified sub-matrices) of the bordered Hessian, for which the first 2\"m\" leading principal minors are neglected, the smallest minor consisting of the truncated first 2\"m\"+1 rows and columns, the next consisting of the truncated first 2\"m\"+2 rows and columns, and so on, with the last being the entire bordered Hessian; if 2\"m\"+1 is larger than n+m, then the smallest leading principal minor is the Hessian itself. There are thus \"n\"–\"m\" minors to consider, each evaluated at the specific point being considered as a candidate maximum or minimum. A sufficient condition for a local \"maximum\" is that these minors alternate in sign with the smallest one having the sign of (–1). A sufficient condition for a local \"minimum\" is that all of these minors have the sign of (–1). (In the unconstrained case of \"m\"=0 these conditions coincide with the conditions for the unbordered Hessian to be negative definite or positive definite respectively).\n\nIf is instead a vector field , i.e.\n\nthen the collection of second partial derivatives is not a matrix, but rather a third order tensor. This can be thought of as an array of Hessian matrices, one for each component of :\nThis tensor degenerates to the usual Hessian matrix when = 1.\n\nIn the context of several complex variables, the Hessian may be generalized. Suppose formula_15, and we write formula_16. Then one may generalize the Hessian to formula_17. Note that if formula_18 satisfies the n-dimensional Cauchy-Riemann conditions, then the complex Hessian matrix is identically zero.\n\nLet formula_19 be a Riemannian manifold and formula_20 its Levi-Civita connection. Let formula_21 be a smooth function. We may define the Hessian tensor\n\nwhere we have taken advantage of the first covariant derivative of a function being the same as its ordinary derivative. Choosing local coordinates formula_24 we obtain the local expression for the Hessian as\n\nwhere formula_26 are the Christoffel symbols of the connection. Other equivalent forms for the Hessian are given by\n\n\n\n"}
{"id": "21991097", "url": "https://en.wikipedia.org/wiki?curid=21991097", "title": "ICAM (Color Appearance Model)", "text": "ICAM (Color Appearance Model)\n\niCAM is a color appearance model developed by Mark D. Fairchild and Garrett M. Johnson that was published in 2002 at the IS&T/SID 10th Color Imaging Conference in Scottsdale.\n\nIt has been recognized that there are significant aspects of color appearance phenomena that are not described well, if at all, by models such as CIECAM97s or CIECAM02.\n\nThe requirements for such a model include\n\n"}
{"id": "3321765", "url": "https://en.wikipedia.org/wiki?curid=3321765", "title": "Igor Pak", "text": "Igor Pak\n\nIgor Pak () (born 1971, Moscow, Soviet Union) is a professor of mathematics at the University of California, Los Angeles, working in combinatorics and discrete probability. He formerly taught at the Massachusetts Institute of Technology, and the University of Minnesota and is best known for his bijective proof of the hook-length formula for the number of Young tableaux, and his work on random walks. He was a keynote speaker alongside George Andrews and Doron Zeilberger at the 2006 Harvey Mudd College Mathematics Conference on Enumerative Combinatorics.\n\nPak is an Associate Editor for the journal \"Discrete Mathematics\" and an Editor (responsible for combinatorics and discrete geometry) of the Transactions of the American Mathematical Society. He gave a Fejes Tóth Lecture at the University of Calgary in February 2009.\n\nPak went to Moscow High School № 57. After graduating, he worked for a year at Bank Menatep.\n\nHe did his undergraduate studies at Moscow State University. He was a Ph.D. student of Persi Diaconis at Harvard University, where he received a doctorate in Mathematics in 1997, with a thesis title \"Random Walks on Groups: Strong Uniform Time Approach\". Afterwards, he worked with László Lovász as a postdoc at Yale University. He was a fellow at the Mathematical Sciences Research Institute, and a long term visitor at the Hebrew University of Jerusalem.\n\n"}
{"id": "44460457", "url": "https://en.wikipedia.org/wiki?curid=44460457", "title": "Incidence poset", "text": "Incidence poset\n\nIn mathematics, an incidence poset or incidence order is a type of partially ordered set that represents the incidence relation between vertices and edges of an undirected graph. The incidence poset of a graph \"G\" has an element for each vertex or edge in \"G\"; in this poset, there is an order relation \"x\" ≤ \"y\" if and only if either \"x\" = \"y\" or \"x\" is a vertex, \"y\" is an edge, and \"x\" is an endpoint of \"y\".\n\nAs an example, a zigzag poset or \"fence\" with an odd number of elements, with alternating order relations \"a\" < \"b\" > \"c\" < \"d\"... is an incidence poset of a path graph.\n\nEvery incidence poset of a non-empty graph has height two. Its width equals the number of edges plus the number of acyclic connected components.\n\nIncidence posets have been particularly studied with respect to their order dimension, and its relation to the properties of the underlying graph. The incidence poset of a connected graph \"G\" has order dimension at most two if and only if \"G\" is a path graph, and has order dimension at most three if and only if \"G\" is at most planar (Schnyder's theorem). However, graphs whose incidence posets have order dimension 4 may be dense and may have unbounded chromatic number. Every complete graph on \"n\" vertices, and by extension every graph on \"n\" vertices, has an incidence poset with order dimension \"O\"(log log \"n\"). If an incidence poset has high dimension then it must contain copies of the incidence posets of all small trees either as sub-orders or as the duals of sub-orders.\n"}
{"id": "4678961", "url": "https://en.wikipedia.org/wiki?curid=4678961", "title": "Key clustering", "text": "Key clustering\n\nKey clustering, in cryptography, is two different keys that generate the same ciphertext from the same plaintext by using the same cipher algorithm. A good cipher algorithm, using different keys on the same plaintext, should generate a different ciphertext irrespective of the key length.\n\nIf there is a plaintext P, two different keys K1 and K2, and an algorithm A, the two key generate ciphertexts C1 and C2 as follows:\nP → A(K1) → C1\n\nP → A(K2) → C2\n\nKey clustering has occurred if C1 and C2 are the same, which should not occur.\n\nIf an attacker tries to break a cipher by a brute-force attack, trying all possible keys until it finds the correct key, key clustering makes it easier to attack a particular cipher text. If there are \"n\" possible keys without any key clustering, the attacker needs to try an average of \"n\"/2 keys to decrypt it and no more than \"n\" keys. If there are two keys that are clustered, the average number of keys is reduced to \"n\"/4 and the maximum is \"n\"-1 keys. If three keys cluster, the average attempt is only \"n\"/6 attempts.\n"}
{"id": "10104622", "url": "https://en.wikipedia.org/wiki?curid=10104622", "title": "Kuratowski's closure-complement problem", "text": "Kuratowski's closure-complement problem\n\nIn point-set topology, Kuratowski's closure-complement problem asks for the largest number of distinct sets obtainable by repeatedly applying the set operations of closure and complement to a given starting subset of a topological space. The answer is 14. This result was first published by Kazimierz Kuratowski in 1922. The problem gained wide exposure three decades later as an exercise in John L. Kelley's classic textbook \"General Topology\".\n\nLetting \"S\" denote an arbitrary subset of a topological space, write \"kS\" for the closure of \"S\", and \"cS\" for the complement of \"S\". The following three identities imply that no more than 14 distinct sets are obtainable:\n\nThe first two are trivial. The third follows from the identity \"kikiS\" = \"kiS\" where \"iS\" is the interior of \"S\" which is equal to the complement of the closure of the complement of \"S\", \"iS\" = \"ckcS\". (The operation \"ki\" = \"kckc\" is idempotent.)\n\nA subset realizing the maximum of 14 is called a 14-set. The space of real numbers under the usual topology contains 14-sets. Here is one example:\nwhere formula_2 denotes an open interval and formula_3 denotes a closed interval.\n\nDespite its origin within the context of a topological space, Kuratowski's closure-complement problem is actually more algebraic than topological. A surprising abundance of closely related problems and results have appeared since 1960, many of which have little or nothing to do with point-set topology.\n\nThe closure-complement operations yield a monoid which can be used to classify topological spaces.\n\n"}
{"id": "32869192", "url": "https://en.wikipedia.org/wiki?curid=32869192", "title": "List of dualities", "text": "List of dualities\n\nIn mathematics, a duality, generally speaking, translates concepts, theorems or mathematical structures into other concepts, theorems or structures, in a one-to-one fashion, often (but not always) by means of an involution operation: if the dual of \"A\" is \"B\", then the dual of \"B\" is \"A\".\n\n\n\n\n"}
{"id": "5971839", "url": "https://en.wikipedia.org/wiki?curid=5971839", "title": "List of mathematicians (W)", "text": "List of mathematicians (W)\n\n\n\n\n\n\n\n\n\n"}
{"id": "1845113", "url": "https://en.wikipedia.org/wiki?curid=1845113", "title": "Locally compact quantum group", "text": "Locally compact quantum group\n\nA locally compact quantum group is a relatively new C*-algebraic approach toward quantum groups that generalizes the Kac algebra, compact-quantum-group and Hopf-algebra approaches. Earlier attempts at a unifying definition of quantum groups using, for example, multiplicative unitaries have enjoyed some success but have also encountered several technical problems.\n\nOne of the main features distinguishing this new approach from its predecessors is the axiomatic existence of left and right invariant weights. This gives a noncommutative analogue of left and right Haar measures on a locally compact Hausdorff group.\n\nBefore we can even begin to properly define a locally compact quantum group, we first need to define a number of preliminary concepts and also state a few theorems.\n\nDefinition (weight). Let formula_1 be a C*-algebra, and let formula_2 denote the set of positive elements of formula_1. A weight on formula_1 is a function formula_5 such that\n\nSome notation for weights. Let formula_11 be a weight on a C*-algebra formula_1. We use the following notation:\n\nTypes of weights. Let formula_11 be a weight on a C*-algebra formula_1.\n\nDefinition (one-parameter group). Let formula_1 be a C*-algebra. A one-parameter group on formula_1 is a family formula_40 of *-automorphisms of formula_1 that satisfies formula_42 for all formula_43. We say that formula_44 is norm-continuous if and only if for every formula_45, the mapping formula_46 defined by formula_47 is continuous.\n\nDefinition (analytic extension of a one-parameter group). Given a norm-continuous one-parameter group formula_44 on a C*-algebra formula_1, we are going to define an analytic extension of formula_44. For each formula_51, let\nwhich is a horizontal strip in the complex plane. We call a function formula_53 norm-regular if and only if the following conditions hold:\nSuppose now that formula_61, and let\nDefine formula_63 by formula_64. The function formula_65 is uniquely determined (by the theory of complex-analytic functions), so formula_66 is well-defined indeed. The family formula_67 is then called the analytic extension of formula_44.\n\nTheorem 1. The set formula_69, called the set of analytic elements of formula_1, is a dense subset of formula_1.\n\nDefinition (K.M.S. weight). Let formula_1 be a C*-algebra and formula_5 a weight on formula_1. We say that formula_11 is a K.M.S. weight ('K.M.S.' stands for 'Kubo-Martin-Schwinger') on formula_1 if and only if formula_11 is a \"proper weight\" on formula_1 and there exists a norm-continuous one-parameter group formula_79 on formula_1 such that\n\nWe denote by formula_87 the multiplier algebra of formula_88.\n\nTheorem 2. If formula_1 and formula_90 are C*-algebras and formula_91 is a non-degenerate *-homomorphism (i.e., formula_92 is a dense subset of formula_90), then we can uniquely extend formula_94 to a *-homomorphism formula_95.\n\nTheorem 3. If formula_96 is a state (i.e., a positive linear functional of norm formula_97) on formula_1, then we can uniquely extend formula_99 to a state formula_100 on formula_101.\n\nDefinition (Locally compact quantum group). A (C*-algebraic) locally compact quantum group is an ordered pair formula_102, where formula_1 is a C*-algebra and formula_104 is a \"non-degenerate\" *-homomorphism called the co-multiplication, that satisfies the following four conditions:\n\nFrom the definition of a locally compact quantum group, it can be shown that the right-invariant K.M.S. weight formula_114 is automatically faithful. Therefore, the faithfulness of formula_114 is a redundant condition and does not need to be postulated.\n\nThe category of locally compact quantum groups allows for a dual construction with which one can prove that the bi-dual of a locally compact quantum group is isomorphic to the original one. This result gives a far-reaching generalization of Pontryagin duality for locally compact Hausdorff abelian groups.\n\nThe theory has an equivalent formulation in terms of von Neumann algebras.\n\n\n"}
{"id": "303398", "url": "https://en.wikipedia.org/wiki?curid=303398", "title": "Maximal and minimal elements", "text": "Maximal and minimal elements\n\nIn mathematics, especially in order theory, a maximal element of a subset \"S\" of some partially ordered set (poset) is an element of \"S\" that is not smaller than any other element in \"S\". A minimal element of a subset \"S\" of some partially ordered set is defined dually as an element of \"S\" that is not greater than any other element in \"S\".\n\nThe notions of maximal and minimal elements are weaker than those of greatest element and least element which are also known, respectively, as maximum and minimum. The maximum of a subset \"S\" of a partially ordered set is an element of \"S\" which is greater than or equal to any other element of \"S\", and the minimum of \"S\" is again defined dually. While a partially ordered set can have at most one each maximum and minimum it may have multiple maximal and minimal elements. For totally ordered sets, the notions of maximal element and maximum coincide, and the notions of minimal element and minimum coincide.\n\nAs an example, in the collection\n\nordered by containment, the element {\"d\", \"o\"} is minimal as it contains no sets in the collection, the element {\"g\", \"o\", \"a\", \"d\"} is maximal as there are no sets in the collection which contain it, the element {\"d\", \"o\", \"g\"} is neither, and the element {\"o\", \"a\", \"f\"} is both minimal and maximal. By contrast, neither a maximum nor a minimum exists for \"S\".\n\nZorn's lemma states that every partially ordered set for which every totally ordered subset has an upper bound contains at least one maximal element. This lemma is equivalent to the well-ordering theorem and the axiom of choice and implies major results in other mathematical areas like the Hahn–Banach theorem and Tychonoff's theorem, the existence of a Hamel basis for every vector space, and the existence of an algebraic closure for every field.\n\nLet formula_1 be a partially ordered set and formula_2. Then formula_3 is a maximal element of formula_4 if\n\nfor all formula_5, formula_6 implies formula_7\n\nThe definition for minimal elements is obtained by using ≥ instead of ≤.\n\nMaximal elements need not exist.\n\nIn general ≤ is only a partial order on \"S\". If \"m\" is a maximal element and \"s\"∈\"S\", it remains the possibility that neither \"s\"≤\"m\" nor \"m\"≤\"s\". This leaves open the possibility that there are many maximal elements.\n"}
{"id": "25358908", "url": "https://en.wikipedia.org/wiki?curid=25358908", "title": "Minimum k-cut", "text": "Minimum k-cut\n\nIn mathematics, the minimum \"k\"-cut, is a combinatorial optimization problem that requires finding a set of edges whose removal would partition the graph to at least \"k\" connected components. These edges are referred to as \"k\"-cut. The goal is to find the minimum-weight \"k\"-cut. This partitioning can have applications in VLSI design, data-mining, finite elements and communication in parallel computing.\n\nGiven an undirected graph \"G\" = (\"V\", \"E\") with an assignment of weights to the edges \"w\": \"E\" → \"N\" and an integer \"k\" ∈ {2, 3, …, |\"V\"|}, partition \"V\" into \"k\" disjoint sets \"F\" = {\"C\", \"C\", …, \"C\"} while minimizing\n\nFor a fixed \"k\", the problem is polynomial time solvable in \"O\"(|\"V\"|). However, the problem is NP-complete if \"k\" is part of the input. It is also NP-complete if we specify formula_2 vertices and ask for the minimum formula_2-cut which separates these vertices among each of the sets.\n\nSeveral approximation algorithms exist with an approximation of 2 − 2/\"k\". A simple greedy algorithm that achieves this approximation factor computes a minimum cut in each connected components and removes the lightest one. This algorithm requires a total of \"n\" − 1 max flow computations. Another algorithm achieving the same guarantee uses the Gomory–Hu tree representation of minimum cuts. Constructing the Gomory–Hu tree requires \"n\" − 1 max flow computations, but the algorithm requires an overall \"O\"(\"kn\") max flow computations. Yet, it is easier to analyze the approximation factor of the second algorithm. Moreover, under the Small Set Expansion Hypothesis (a conjecture closely related to the Unique Games Conjecture), the problem is NP-hard to approximate to within formula_4 factor for every constant formula_5, meaning that the aforementioned approximation algorithms are essentially tight for large formula_2.\n\nA variant of the problem asks for a minimum weight \"k\"-cut where the output partitions have pre-specified sizes. This problem variant is approximable to within a factor of 3 for any fixed \"k\" if one restricts the graph to a metric space, meaning a complete graph that satisfies the triangle inequality. More recently, polynomial time approximation schemes (PTAS) were discovered for those problems.\n\n\n"}
{"id": "57411", "url": "https://en.wikipedia.org/wiki?curid=57411", "title": "Multiplication algorithm", "text": "Multiplication algorithm\n\nA multiplication algorithm is an algorithm (or method) to multiply two numbers. Depending on the size of the numbers, different algorithms are in use. Efficient multiplication algorithms have existed since the advent of the decimal system.\n\nThe grid method (or box method) is an introductory method for multiple-digit multiplication that is often taught to pupils at primary school or elementary school level. It has been a standard part of the national primary-school mathematics curriculum in England and Wales since the late 1990s.\n\nBoth factors are broken up (\"partitioned\") into their hundreds, tens and units parts, and the products of the parts are then calculated explicitly in a relatively simple multiplication-only stage, before these contributions are then totalled to give the final answer in a separate addition stage.\n\nThe calculation 34 × 13, for example, could be computed using the grid:\n\nfollowed by addition to obtain 442, either in a single sum (see right), or through forming the row-by-row totals (300 + 40) + (90 + 12) = 340 + 102 = 442.\n\nThis calculation approach (though not necessarily with the explicit grid arrangement) is also known as the partial products algorithm. Its essence is the calculation of the simple multiplications separately, with all addition being left to the final gathering-up stage.\n\nThe grid method can in principle be applied to factors of any size, although the number of sub-products becomes cumbersome as the number of digits increases. Nevertheless, it is seen as a usefully explicit method to introduce the idea of multiple-digit multiplications; and, in an age when most multiplication calculations are done using a calculator or a spreadsheet, it may in practice be the only multiplication algorithm that some students will ever need.\n\nIf a positional numeral system is used, a natural way of multiplying numbers is taught in schools\nas long multiplication, sometimes called grade-school multiplication, sometimes called Standard Algorithm:\nmultiply the multiplicand by each digit of the multiplier and then add up all the properly shifted results. It requires memorization of the multiplication table for single digits.\n\nThis is the usual algorithm for multiplying larger numbers by hand in base 10. Computers initially used a very similar shift and add algorithm in base 2, but modern processors have optimized circuitry for fast multiplications using more efficient algorithms, at the price of a more complex hardware realization. A person doing long multiplication on paper will write down all the products and then add them together; an abacus-user will sum the products as soon as each one is computed.\n\nThis example uses \"long multiplication\" to multiply 23,958,233 (multiplicand) by 5,830 (multiplier) and arrives at 139,676,498,390 for the result (product).\nBelow pseudocode describes the process of above multiplication. It keeps only one row to maintain the sum which finally becomes the result. Note that the '+=' operator is used to denote sum to existing value and store operation (akin to languages such as Java and C) for compactness.\n\nLet \"n\" be the total number of digits in the two input numbers in base \"D\". If the result must be kept in memory then the space complexity is trivially Θ(\"n\"). However, in certain applications, the entire result need not be kept in memory and instead the digits of the result can be streamed out as they are computed (for example, to system console or file). In these scenarios, long multiplication has the advantage that it can easily be formulated as a log space algorithm; that is, an algorithm that only needs working space proportional to the logarithm of the number of digits in the input (Θ(log \"n\")). This is the \"double\" logarithm of the numbers being multiplied themselves (log log \"N\"). Note that operands themselves still need to be kept in memory and their Θ(\"n\") space is not considered in this analysis.\n\nThe method is based on the observation that each digit of the result can be computed from right to left with only knowing the carry from the previous step. Let \"a\" and \"b\" be the \"i\"-th digit of the operand, \"r\" be the \"i\"-th digit of the result and \"c\" be the carry generated for \"r\" (i=1 is the right most digit) then\n\nA simple inductive argument shows that the carry can never exceed \"n\" and the total sum for \"r\" can never exceed \"D\" * \"n\": the carry into the first column is zero, and for all other columns, there are at most \"n\" digits in the column, and a carry of at most \"n\" from the previous column (by the induction hypothesis). The sum is at most \"D\" * \"n\", and the carry to the next column is at most \"D\" * \"n\" / \"D\", or \"n\". Thus both these values can be stored in O(log \"n\") digits.\n\nIn pseudocode, the log-space algorithm is:\nmultiply(a[1..p], b[1..q], base) // Operands containing rightmost digits at index 1\nSome chips implement this algorithm for various integer and floating-point sizes in computer hardware or in microcode. In arbitrary-precision arithmetic, it's common to use long multiplication with the base set to 2, where \"w\" is the number of bits in a word, for multiplying relatively small numbers.\n\nTo multiply two numbers with \"n\" digits using this method, one needs about \"n\" operations. More formally: using a natural size metric of number of digits, the time complexity of multiplying two \"n\"-digit numbers using long multiplication is Θ(\"n\").\n\nWhen implemented in software, long multiplication algorithms have to deal with overflow during additions, which can be expensive. For this reason, a typical approach is to represent the number in a small base \"b\" such that, for example, 8\"b\" is a representable machine integer (for example Richard Brent used this approach in his Fortran package MP); we can then perform several additions before having to deal with overflow. When the number becomes too large, we add part of it to the result or carry and map the remaining part back to a number less than \"b\"; this process is called \"normalization\".\n\nLattice, or sieve, multiplication is algorithmically equivalent to long multiplication. It requires the preparation of a lattice (a grid drawn on paper) which guides the calculation and separates all the multiplications from the additions. It was introduced to Europe in 1202 in Fibonacci's Liber Abaci. Fibonacci described the operation as mental, using his right and left hands to carry the intermediate calculations. Matrakçı Nasuh presented 6 different variants of this method in this 16th-century book, Umdet-ul Hisab. It was widely used in Enderun schools across the Ottoman Empire. Napier's bones, or Napier's rods also used this method, as published by Napier in 1617, the year of his death.\n\nAs shown in the example, the multiplicand and multiplier are written above and to the right of a lattice, or a sieve. It is found in Muhammad ibn Musa al-Khwarizmi's \"Arithmetic\", one of Leonardo's sources mentioned by Sigler, author of \"Fibonacci's Liber Abaci\", 2002.\n\n\nThe pictures on the right show how to calculate 345 × 12 using lattice multiplication. As a more complicated example, consider the picture below displaying the computation of 23,958,233 multiplied by 5,830 (multiplier); the result is 139,676,498,390. Notice 23,958,233 is along the top of the lattice and 5,830 is along the right side. The products fill the lattice and the sum of those products (on the diagonal) are along the left and bottom sides. Then those sums are totaled as shown.\nIn base 2, long multiplication reduces to a nearly trivial operation. For each '1' bit in the multiplier, shift the multiplicand an appropriate amount and then sum the shifted values. Depending on computer processor architecture and choice of multiplier, it may be faster to code this algorithm using hardware bit shifts and adds rather than depend on multiplication instructions, when the multiplier is fixed and the number of adds required is small.\n\nThis algorithm is also known as peasant multiplication, because it has been widely used among those who are classified as peasants and thus have not memorized the multiplication tables required by long multiplication. The algorithm was also in use in ancient Egypt.\n\nOn paper, write down in one column the numbers you get when you repeatedly halve the multiplier, ignoring the remainder; in a column beside it repeatedly double the multiplicand. Cross out each row in which the last digit of the first number is even, and add the remaining numbers in the second column to obtain the product.\n\nThe main advantages of this method are that it can be taught quickly, no memorization is required, and it can be performed using tokens such as poker chips if paper and pencil are not available. It does however take more steps than long multiplication so it can be unwieldy when large numbers are involved.\n\nThis example uses peasant multiplication to multiply 11 by 3 to arrive at a result of 33.\n\nDescribing the steps explicitly:\n\n\nThe method works because multiplication is distributive, so:\n\nA more complicated example, using the figures from the earlier examples (23,958,233 and 5,830):\n\nHistorically, computers used a \"shift and add\" algorithm to multiply small integers. Both base 2 long multiplication and base 2 peasant multiplication reduce to this same algorithm.\nIn base 2, multiplying by the single digit of the multiplier reduces to a simple series of logical AND operations. Each partial product is added to a running sum as soon as each partial product is computed. Most currently available microprocessors implement this or other similar algorithms (such as Booth encoding) for various integer and floating-point sizes in hardware multipliers or in microcode.\n\nOn currently available processors, a bit-wise shift instruction is faster than a multiply instruction and can be used to multiply (shift left) and divide (shift right) by powers of two. Multiplication by a constant and division by a constant can be implemented using a sequence of shifts and adds or subtracts. For example, there are several ways to multiply by 10 using only bit-shift and addition.\n\nIn some cases such sequences of shifts and adds or subtracts will outperform hardware multipliers and especially dividers. A division by a number of the form formula_3 or formula_4 often can be converted to such a short sequence.\n\nThese types of sequences have to always be used for computers that do not have a \"multiply\" instruction, and can also be used by extension to floating point numbers if one replaces the shifts with computation of \"2*x\" as \"x+x\", as these are logically equivalent.\n\nTwo quantities can be multiplied using quarter squares by employing the following identity involving the floor function that some sources attribute to Babylonian mathematics (2000–1600 BC).\n\nIf one of and is odd, the other is odd too; this means that the fractions, if any, will cancel out, and discarding the remainders does not introduce any error. Below is a lookup table of quarter squares with the remainder discarded for the digits 0 through 18; this allows for the multiplication of numbers up to .\n\nIf, for example, you wanted to multiply 9 by 3, you observe that the sum and difference are 12 and 6 respectively. Looking both those values up on the table yields 36 and 9, the difference of which is 27, which is the product of 9 and 3.\n\nAntoine Voisin published a table of quarter squares from 1 to 1000 in 1817 as an aid in multiplication. A larger table of quarter squares from 1 to 100000 was published by Samuel Laundy in 1856, and a table from 1 to 200000 by Joseph Blater in 1888.\n\nQuarter square multipliers were used in analog computers to form an analog signal that was the product of two analog input signals. In this application, the sum and difference of two input voltages are formed using operational amplifiers. The square of each of these is approximated using piecewise linear circuits. Finally the difference of the two squares is formed and scaled by a factor of one fourth using yet another operational amplifier.\n\nIn 1980, Everett L. Johnson proposed using the quarter square method in a digital multiplier. To form the product of two 8-bit integers, for example, the digital device forms the sum and difference, looks both quantities up in a table of squares, takes the difference of the results, and divides by four by shifting two bits to the right. For 8-bit integers the table of quarter squares will have 2-1=511 entries (one entry for the full range 0..510 of possible sums, the differences using only the first 256 entries in range 0..255) or 2-1=511 entries (using for negative differences the technique of 2-complements and 9-bit masking, which avoids testing the sign of differences), each entry being 16-bit wide (the entry values are from (0²/4)=0 to (510²/4)=65025).\n\nThe Quarter square multiplier technique has also benefitted 8-bit systems that do not have any support for a hardware multiplier. Steven Judd implemented this for the 6502.\n\nSuppose we want to multiply two numbers formula_6 and formula_7 that are close to a round number formula_8. Writing formula_9 and formula_10, allows one to express the product as:\n\nExample. Suppose we want to multiply 92 by 87. We can then take formula_12 and implement the above formula as follows. We write the numbers below each other and next to them the amounts we have to add to get to 100:\nSince the numbers on the right are formula_14 and formula_15, the product is obtained by subtracting from the top left number the bottom right number (or subtract from the bottom left number the top right number), multiply that by 100 and add to that the product of the two numbers on the right. We have 87 - 8 = 79; 79*100 = 7900; 8*13 = 104; 7900+104 = 8004.\n\nThe multiplication of 8 by 13 could also have been done using the same method, by taking formula_16. The above table can then be extended to:\nThe product is then computed by evaluating the differences 87-8=79; 13-2 = 11, and the product 2*(-3) = -6. We then have 92*87 = 79*100 + 11*10 - 6 = 7900 + 104 = 8004.\n\nComplex multiplication normally involves four multiplications and two additions.\n\nOr\n\nBut there is a way of reducing the number of multiplications to three.\n\nThe product (\"a\" + \"bi\") · (\"c\" + \"di\") can be calculated in the following way.\n\nThis algorithm uses only three multiplications, rather than four, and five additions or subtractions rather than two. If a multiply is more expensive than three adds or subtracts, as when calculating by hand, then there is a gain in speed. On modern computers a multiply and an add can take about the same time so there may be no speed gain. There is a trade-off in that there may be some loss of precision when using floating point.\n\nFor fast Fourier transforms (FFTs) (or any linear transformation) the complex multiplies are by constant coefficients \"c\" + \"di\" (called twiddle factors in FFTs), in which case two of the additions (\"d\"−\"c\" and \"c\"+\"d\") can be precomputed. Hence, only three multiplies and three adds are required. However, trading off a multiplication for an addition in this way may no longer be beneficial with modern floating-point units.\n\nFor systems that need to multiply numbers in the range of several thousand digits, such as computer algebra systems and bignum libraries, long multiplication is too slow. These systems may employ Karatsuba multiplication, which was discovered in 1960 (published in 1962). The heart of Karatsuba's method lies in the observation that two-digit multiplication can be done with only three rather than the four multiplications classically required. This is an example of what is now called a \"divide and conquer algorithm\". Suppose we want to multiply two 2-digit base-\"m\" numbers: \"x\"\" m + x\" and \"y\"\" m + y\":\n\n\nTo compute these three products of \"m\"-digit numbers, we can employ the same trick again, effectively using recursion. Once the numbers are computed, we need to add them together (steps 4 and 5), which takes about \"n\" operations.\n\nKaratsuba multiplication has a time complexity of O(\"n\") ≈ O(\"n\"), making this method significantly faster than long multiplication. Because of the overhead of recursion, Karatsuba's multiplication is slower than long multiplication for small values of \"n\"; typical implementations therefore switch to long multiplication if \"n\" is below some threshold.\n\nKaratsuba's algorithm is the first known algorithm for multiplication that is asymptotically faster than long multiplication, and can thus be viewed as the starting point for the theory of fast multiplications.\n\nIn 1963, Peter Ungar suggested setting \"m\" to \"i\" to obtain a similar reduction in the complex multiplication algorithm. To multiply (\"a\" + \"b i\") · (\"c\" + \"d i\"), follow these steps:\n\nLike the algorithm in the previous section, this requires three multiplications and five additions or subtractions.\n\nAnother method of multiplication is called Toom–Cook or Toom-3. The Toom–Cook method splits each number to be multiplied into multiple parts. The Toom–Cook method is one of the generalizations of the Karatsuba method. A three-way Toom–Cook can do a size-\"3N\" multiplication for the cost of five size-\"N\" multiplications, improvement by a factor of 9/5 compared to the Karatsuba method's improvement by a factor of 4/3.\n\nAlthough using more and more parts can reduce the time spent on recursive multiplications further, the overhead from additions and digit management also grows. For this reason, the method of Fourier transforms is typically faster for numbers with several thousand digits, and asymptotically faster for even larger numbers.\n\nThe basic idea due to Strassen (1968) is to use fast polynomial multiplication to perform fast integer multiplication. The algorithm was made practical and theoretical guarantees were provided in 1971 by Schönhage and Strassen resulting in the Schönhage–Strassen algorithm. The details are the following: We choose the largest integer \"w\" that will not cause overflow during the process outlined below. Then we split the two numbers into \"m\" groups of \"w\" bits as follows\n\nWe look at these numbers as polynomials in \"x\", where \"x = 2\", to get,\n\nThen we can then say that,\n\nClearly the above setting is realized by polynomial multiplication, of two polynomials \"a\" and \"b\". The crucial step now is to use Fast Fourier multiplication of polynomials to realize the multiplications above faster than in naive \"O(m)\" time.\n\nTo remain in the modular setting of Fourier transforms, we look for a ring with a \"2m\" root of unity. Hence we do multiplication modulo \"N\" (and thus in the \"Z/NZ\" ring). Further, N must be chosen so that there is no 'wrap around', essentially, no reductions modulo N occur. Thus, the choice of N is crucial. For example, it could be done as,\n\nThe ring \"Z/NZ\" would thus have a \"2m\" root of unity, namely 8. Also, it can be checked that \"c < N\", and thus no wrap around will occur.\n\nThe algorithm has a time complexity of Θ(\"n\" log(\"n\") log(log(\"n\"))) and is used in practice for numbers with more than 10,000 to 40,000 decimal digits. In 2007 this was improved by Martin Fürer (Fürer's algorithm) to give a time complexity of \"n\" log(\"n\") 2 using Fourier transforms over complex numbers. Anindya De, Chandan Saha, Piyush Kurur and Ramprasad Saptharishi gave a similar algorithm using modular arithmetic in 2008 achieving the same running time. In context of the above material, what these latter authors have achieved is to find \"N\" much less than \"2 + 1\", so that \"Z/NZ\" has a \"2m\" root of unity. This speeds up computation and reduces the time complexity. However, these latter algorithms are only faster than Schönhage–Strassen for impractically large inputs.\n\nUsing number-theoretic transforms instead of discrete Fourier transforms avoids rounding error problems by using modular arithmetic instead of floating-point arithmetic. In order to apply the factoring which enables the FFT to work, the length of the transform must be factorable to small primes and must be a factor of \"N\"-1, where \"N\" is the field size. In particular, calculation using a Galois Field GF(\"k\"), where \"k\" is a Mersenne Prime, allows the use of a transform sized to a power of 2; e.g. \"k\" = 2-1 supports transform sizes up to 2.\n\nThere is a trivial lower bound of Ω(\"n\") for multiplying two \"n\"-bit numbers on a single processor; no matching algorithm (on conventional Turing machines) nor any better lower bound is known. Multiplication lies outside of AC[\"p\"] for any prime \"p\", meaning there is no family of constant-depth, polynomial (or even subexponential) size circuits using AND, OR, NOT, and MOD gates that can compute a product. This follows from a constant-depth reduction of MOD to multiplication. Lower bounds for multiplication are also known for some classes of branching programs.\n\nAll the above multiplication algorithms can also be expanded to multiply polynomials. For instance the Strassen algorithm may be used for polynomial multiplication\nAlternatively the Kronecker substitution technique may be used to convert the problem of multiplying polynomials into a single binary multiplication.\n\nLong multiplication methods can be generalised to allow the multiplication of algebraic formulae:\n\nAs a further example of column based multiplication, consider multiplying 23 long tons (t), 12 hundredweight (cwt) and 2 quarters (qtr) by 47. This example uses avoirdupois measures: 1 t = 20 cwt, 1 cwt = 4 qtr.\n\nFirst multiply the quarters by 2, the result 94 is written into the first workspace. Next, multiply 12 x 47 but don't add up the partial results (84, 480) yet. Likewise multiply 23 by 47. The quarters column is totaled and the result placed in the second workspace (a trivial move in this case). 94 quarters is 23 cwt and 2 qtr, so place the 2 in the answer and put the 23 in the next column left. Now add up the three entries in the cwt column giving 587. This is 29 t 7 cwt, so write the 7 into the answer and the 29 in the column to the left. Now add up the tons column. There is no adjustment to make, so the result is just copied down.\n\nThe same layout and methods can be used for any traditional measurements and non-decimal currencies such as the old British £sd system.\n\n\n\n\n"}
{"id": "682403", "url": "https://en.wikipedia.org/wiki?curid=682403", "title": "Names of large numbers", "text": "Names of large numbers\n\nThis article lists and discusses the usage and derivation of names of large numbers, together with their possible extensions.\n\nThe following table lists those names of large numbers that are found in many English dictionaries and thus have a special claim to being \"real words.\" The \"Traditional British\" values shown are unused in American English and are obsolete in British English, but their other-language variants are dominant in many non-English-speaking areas, including continental Europe and Spanish-speaking countries in Latin America; see Long and short scales.\n\nIndian English does not use millions, but has its own system of large numbers including lakhs and crores.\n\nEnglish also has many words, such as \"zillion\", used informally to mean large but unspecified amounts; see indefinite and fictitious numbers.\n\nApart from \"million\", the words in this list ending with -\"illion\" are all derived by adding prefixes (\"bi\"-, \"tri\"-, etc., derived from Latin) to the stem -\"illion\". \"Centillion\" appears to be the highest name ending in -\"illion\" that is included in these dictionaries. \"Trigintillion\", often cited as a word in discussions of names of large numbers, is not included in any of them, nor are any of the names that can easily be created by extending the naming pattern (\"unvigintillion\", \"duovigintillion\", \"duoquinquagintillion\", etc.).\n\nAll of the dictionaries included \"googol\" and \"googolplex\", generally crediting it to the Kasner and Newman book and to Kasner's nephew. None include any higher names in the googol family (googolduplex, etc.). The \"Oxford English Dictionary\" comments that \"googol\" and \"googolplex\" are \"not in formal mathematical use\".\n\nSome names of large numbers, such as \"million\", \"billion\", and \"trillion\", have real referents in human experience, and are encountered in many contexts. At times, the names of large numbers have been forced into common usage as a result of hyperinflation. The highest numerical value banknote ever printed was a note for 1 sextillion pengő (10 or 1 milliard bilpengő as printed) printed in Hungary in 1946. In 2009, Zimbabwe printed a 100 trillion (10) Zimbabwean dollar note, which at the time of printing was worth about US$30.\n\nNames of larger numbers, however, have a tenuous, artificial existence, rarely found outside definitions, lists, and discussions of the ways in which large numbers are named. Even well-established names like \"sextillion\" are rarely used, since in the context of science, including astronomy, where such large numbers often occur, they are nearly always written using scientific notation. In this notation, powers of ten are expressed as \"10\" with a numeric superscript, e.g. \"The X-ray emission of the radio galaxy is .\" When a number such as 10 needs to be referred to in words, it is simply read out as \"ten to the forty-fifth\". This is easier to say and less ambiguous than \"quattuordecillion\", which means something different in the long scale and the short scale.\n\nWhen a number represents a quantity rather than a count, SI prefixes can be used—thus \"femtosecond\", not \"one quadrillionth of a second\"—although often powers of ten are used instead of some of the very high and very low prefixes. In some cases, specialized units are used, such as the astronomer's parsec and light year or the particle physicist's barn.\n\nNevertheless, large numbers have an intellectual fascination and are of mathematical interest, and giving them names is one of the ways in which people try to conceptualize and understand them.\n\nOne of the earliest examples of this is \"The Sand Reckoner\", in which Archimedes gave a system for naming large numbers. To do this, he called the numbers up to a myriad myriad (10) \"first numbers\" and called 10 itself the \"unit of the second numbers\". Multiples of this unit then became the second numbers, up to this unit taken a myriad myriad times, 10·10=10. This became the \"unit of the third numbers\", whose multiples were the third numbers, and so on. Archimedes continued naming numbers in this way up to a myriad myriad times the unit of the 10-th numbers, i.e. formula_1 and embedded this construction within another copy of itself to produce names for numbers up to formula_2 Archimedes then estimated the number of grains of sand that would be required to fill the known universe, and found that it was no more than \"one thousand myriad of the eighth numbers\" (10).\n\nSince then, many others have engaged in the pursuit of conceptualizing and naming numbers that really have no existence outside the imagination. One motivation for such a pursuit is that attributed to the inventor of the word \"googol\", who was certain that any finite number \"had to have a name\". Another possible motivation is competition between students in computer programming courses, where a common exercise is that of writing a program to output numbers in the form of English words.\n\nMost names proposed for large numbers belong to systematic schemes which are extensible. Thus, many names for large numbers are simply the result of following a naming system to its logical conclusion—or extending it further.\n\nThe words \"bymillion\" and \"trimillion\" were first recorded in 1475 in a manuscript of Jehan Adam. Subsequently, Nicolas Chuquet wrote a book \"Triparty en la science des nombres\" which was not published during Chuquet's lifetime. However, most of it was copied by Estienne de La Roche for a portion of his 1520 book, \"L'arismetique\". Chuquet's book contains a passage in which he shows a large number marked off into groups of six digits, with the comment:\nOu qui veult le premier point peult signiffier million Le second point byllion Le tiers point tryllion Le quart quadrillion Le cinq quyllion Le six sixlion Le sept. septyllion Le huyt ottyllion Le neuf nonyllion et ainsi des ault' se plus oultre on vouloit preceder\n\n(Or if you prefer the first mark can signify million, the second mark byllion, the third mark tryllion, the fourth quadrillion, the fifth quyillion, the sixth sixlion, the seventh septyllion, the eighth ottyllion, the ninth nonyllion and so on with others as far as you wish to go).\nAdam and Chuquet used the long scale of powers of a million; that is, Adam's \"bymillion\" (Chuquet's \"byllion\") denoted 10, and Adam's \"trimillion\" (Chuquet's \"tryllion\") denoted 10.\n\nThe names \"googol\" and \"googolplex\" were invented by Edward Kasner's nephew, Milton Sirotta, and introduced in Kasner and Newman's 1940 book,\n\"Mathematics and the Imagination\",\nin the following passage:\nConway and Guy \nhave suggested that \"N-plex\" be used as a name for 10. This gives rise to the name \"googolplexplex\" for 10 = 10. This number (ten to the power of a googolplex) is also known as a googolduplex and googolplexian. Conway and Guy have proposed that \"N-minex\" be used as a name for 10, giving rise to the name \"googolminex\" for the reciprocal of a googolplex. None of these names are in wide use, nor are any currently found in dictionaries.\n\nThe names \"googol\" and \"googolplex\" inspired the name of the Internet company Google and its corporate headquarters, the Googleplex, respectively.\n\nThis section illustrates several systems for naming large numbers, and shows how they can be extended past \"vigintillion\".\n\nTraditional British usage assigned new names for each power of one million (the long scale): ; ; ; and so on. It was adapted from French usage, and is similar to the system that was documented or invented by Chuquet.\n\nTraditional American usage (which was also adapted from French usage but at a later date), Canadian, and modern British usage assign new names for each power of one thousand (the short scale.) Thus, a \"billion\" is 1000 × 1000 = 10; a \"trillion\" is 1000 × 1000 = 10; and so forth. Due to its dominance in the financial world (and by the US dollar), this was adopted for official United Nations documents.\n\nTraditional French usage has varied; in 1948, France, which had been using the short scale, reverted to the long scale.\n\nThe term \"milliard\" is unambiguous and always means 10. It is almost never seen in American usage, rarely in British usage, and frequently in European usage. The term is sometimes attributed to French mathematician Jacques Peletier du Mans circa 1550 (for this reason, the long scale is also known as the \"Chuquet-Peletier\" system), but the Oxford English Dictionary states that the term derives from post-Classical Latin term \"milliartum\", which became \"milliare\" and then \"milliart\" and finally our modern term.\n\nWith regard to names ending in -illiard for numbers 10, \"milliard\" is certainly in widespread use in languages other than English, but the degree of actual use of the larger terms is questionable. The terms \"Milliarde\" in German, \"miljard\" in Dutch, \"milyar\" in Turkish and \"миллиард\" in Russian are standard usage when discussing financial topics.\n\nFor additional details, see billion and long and short scales.\n\nThe naming procedure for large numbers is based on taking the number \"n\" occurring in 10 (short scale) or 10 (long scale) and concatenating Latin roots for its units, tens, and hundreds place, together with the suffix \"-illion\". In this way, numbers up to 10 = 10 (short scale) or 10 = 10 (long scale) may be named. The choice of roots and the concatenation procedure is that of the standard dictionary numbers if \"n\" is 20 or smaller. For larger \"n\" (between 21 and 999), prefixes can be constructed based on a system described by John Horton Conway and Richard K. Guy:\n\nSince the system of using Latin prefixes will become ambiguous for numbers with exponents of a size which the Romans rarely counted to, like 10, Conway and Guy have also proposed a consistent set of conventions which permit, in principle, the extension of this system to provide English names for any integer whatsoever.\n\nThe following table shows number names generated by the system described by Conway and Guy for the short and long scales.\n\nNames of reciprocals of large numbers are not listed, as they are regularly formed by adding -th, e.g. \"quattuordecillionth,\" \"centillionth,\" etc.\n\nThe International System of Quantities (ISQ) defines a series of prefixes denoting integer powers of 1024 between 1024 and 1024.\n\nIn 2001, Russ Rowlett, Director of the Center for Mathematics and Science Education at the University of North Carolina at Chapel Hill proposed that, to avoid confusion, the Latin-based short scale and long scale systems should be replaced by an unambiguous Greek-based system for naming large numbers that would be based on powers of one thousand.\n\n"}
{"id": "1575704", "url": "https://en.wikipedia.org/wiki?curid=1575704", "title": "Neil Robertson (mathematician)", "text": "Neil Robertson (mathematician)\n\nGeorge Neil Robertson (born November 30, 1938) is a mathematician working mainly in topological graph theory, currently a distinguished professor emeritus at the Ohio State University. He earned his B.Sc. from Brandon College in 1959, and his Ph.D. in 1969 at the University of Waterloo under his doctoral advisor William Tutte.\n\nIn 1969, Robertson joined the faculty of The Ohio State University, where he was promoted to Associate Professor in 1972 and Professor in 1984. He was a consultant with Bell Communications Research from 1984 to 1996. He has held visiting faculty positions in many institutions, most extensively at Princeton University from 1996 to 2001, and at Victoria University of Wellington, New Zealand, in 2002. He also holds an adjunct position at King Abdulaziz University in Saudi Arabia.\n\nRobertson is known for his work in graph theory, and particularly for a long series of papers co-authored with Paul Seymour and published over a span of many years, in which they proved the Robertson–Seymour theorem (formerly Wagner's Conjecture). This states that families of graphs closed under the graph minor operation may be characterized by a finite set of forbidden minors. As part of this work, Robertson and Seymour also proved the graph structure theorem describing the graphs in these families.\n\nAdditional major results in Robertson's research include the following:\n\nRobertson has won the Fulkerson Prize three times, in 1994 for his work on the Hadwiger conjecture, in 2006 for the Robertson–Seymour theorem, and in 2009 for his proof of the strong perfect graph theorem.\n\nHe also won the Pólya Prize (SIAM) in 2004, the OSU Distinguished Scholar Award in 1997, and the Waterloo Alumni Achievement Medal in 2002. In 2012 he became a fellow of the American Mathematical Society.\n\n"}
{"id": "38919712", "url": "https://en.wikipedia.org/wiki?curid=38919712", "title": "Neutron–proton ratio", "text": "Neutron–proton ratio\n\nThe neutron–proton ratio (N/Z ratio or nuclear ratio) of an atomic nucleus is the ratio of its number of neutrons to its number of protons. Among stable nuclei and naturally occurring nuclei, this ratio generally increases with increasing atomic number. This is because electrical repulsive forces between protons scale with distance differently than strong nuclear force attractions. In particular, most pairs of protons in large nuclei are not far enough apart, such that electrical repulsion dominates over the strong nuclear force, and thus proton density in stable larger nuclei must be lower than in stable smaller nuclei where more pairs of protons have appreciable short-range nuclear force attractions.\n\nFor each element with atomic number Z small enough to occupy only the first three nuclear shells, that is up to that of calcium (Z = 20), there exists a stable isotope with N/Z ratio of one, with the exception of beryllium (N/Z = 1.25) and every element with odd atomic number between 9 and 19 inclusive (N = Z+1). Hydrogen-1 (N/Z ratio = 0) and helium-3 (N/Z ratio = 0.5) are the only stable isotopes with neutron–proton ratio under one. Uranium-238 and plutonium-244 have the highest N/Z ratios of any primordial nuclide at 1.587 and 1.596, respectively, while lead-208 has the highest N/Z ratio of any known stable isotope at 1.537. Radioactive decay generally proceeds so as to change the N/Z ratio to increase stability. If the N/Z ratio is greater than 1, alpha decay increases the N/Z ratio, and hence provides a common pathway towards stability for decays involving large nuclei with too few neutrons. Positron emission and electron capture also increase the ratio, while beta decay will decrease the ratio.\n\nNuclear waste exists mainly because nuclear fuel has a higher stable N/Z ratio than the parts into which it is fissioned.\n\nFor stable nuclei, the neutron-proton ratio is such that the bonding energy is at a local minimum or close to a minimum. \n\nFrom the liquid drop model, this bonding energy is approximated by empirical Bethe–Weizsäcker formula\n\nGiven a value of formula_2 and ignoring the contributions of nucleon spin pairing (i.e. ignoring the formula_3 term), the binding energy is a quadratic expression in formula_4 that is minimized when the neutron-proton ratio is formula_5.\n\n"}
{"id": "64685", "url": "https://en.wikipedia.org/wiki?curid=64685", "title": "Post correspondence problem", "text": "Post correspondence problem\n\nThe Post correspondence problem is an undecidable decision problem that was introduced by Emil Post in 1946. Because it is simpler than the halting problem and the \"Entscheidungsproblem\" it is often used in proofs of undecidability.\n\nLet formula_1 be an alphabet with at least two symbols. The input of the problem consists of two finite lists formula_2 and formula_3 of words over formula_1. A solution to this problem is a sequence of indices formula_5 with formula_6 and formula_7 for all formula_8, such that\n\nThe decision problem then is to decide whether such a solution exists or not.\n\nThis gives rise to an equivalent alternative definition often found in the literature, according to which any two homomorphisms formula_12 with a common domain and a common codomain form an instance of the Post correspondence problem, which now asks whether there exists a nonempty word formula_13 in the domain such that\n\nConsider the following two lists:\n\nA solution to this problem would be the sequence (3, 2, 3, 1), because\n\nFurthermore, since (3, 2, 3, 1) is a solution, so are all of its \"repetitions\", such as (3, 2, 3, 1, 3, 2, 3, 1), etc.; that is, when a solution exists, there are infinitely many solutions of this repetitive kind.\n\nHowever, if the two lists had consisted of only formula_16 and formula_17 from those sets, then there would have been no solution (the last letter of any such α string is not the same as the letter before it, whereas β only constructs pairs of the same letter).\n\nA convenient way to view an instance of a Post correspondence problem is as a collection of blocks of the form\n\nthere being an unlimited supply of each type of block. Thus the above example is viewed as\n\n\"i = 1\"\n\"i = 2\"\n\"i = 3\"\n\nwhere the solver has an endless supply of each of these three block types. A solution corresponds to some way of laying blocks next to each other so that the string in the top cells corresponds to the string in the bottom cells. Then the solution to the above example corresponds to:\n\n\"i = 3\"\n\"i = 2\"\n\"i = 3\"\n\"i = 1\"\nAgain using blocks to represent an instance of the problem, the following is an example that has infinitely many solutions in addition to the kind obtained by merely \"repeating\" a solution.\n\n\"1\"\n\"2\"\n\"3\"\nIn this instance, every sequence of the form (1, 2, 2, . . ., 2, 3) is a solution (in addition to all their repetitions):\n\n\"1\"\n\"2\"\n\"2\"\n\"2\"\n\"3\"\n\nThe most common proof for the undecidability of PCP describes an instance of PCP that can simulate the computation of an arbitrary Turing machine on a particular input. A match will occur if and only if the input would be accepted by the Turing machine. Because deciding if a Turing machine will accept an input is a basic undecidable problem, PCP cannot be decidable either. The following discussion is based on Michael Sipser's textbook \"Introduction to the Theory of Computation\".\n\nIn more detail, the idea is that the string along the top and bottom will be a computation history of the Turing machine's computation. This means it will list a string describing the initial state, followed by a string describing the next state, and so on until it ends with a string describing an accepting state. The state strings are separated by some separator symbol (usually written #). According to the definition of a Turing machine, the full state of the machine consists of three parts:\nAlthough the tape has infinitely many cells, only some finite prefix of these will be non-blank. We write these down as part of our state. To describe the state of the finite control, we create new symbols, labelled \"q\" through \"q\", for each of the finite state machine's \"k\" states. We insert the correct symbol into the string describing the tape's contents at the position of the tape head, thereby indicating both the tape head's position and the current state of the finite control. For the alphabet {0,1}, a typical state might look something like:\n\n101101110\"q\"00110.\n\nA simple computation history would then look something like this:\n\n\"q\"101#1\"q\"01#11\"q\"1#1\"q\"10.\n\nWe start out with this block, where \"x\" is the input string and \"q\" is the start state:\n\nThe top starts out \"lagging\" the bottom by one state, and keeps this lag until the very end stage. Next, for each symbol \"a\" in the tape alphabet, as well as #, we have a \"copy\" block, which copies it unmodified from one state to the next:\n\nWe also have a block for each position transition the machine can make, showing how the tape head moves, how the finite state changes, and what happens to the surrounding symbols. For example, here the tape head is over a 0 in state 4, and then writes a 1 and moves right, changing to state 7:\n\nFinally, when the top reaches an accepting state, the bottom needs a chance to finally catch up to complete the match. To allow this, we extend the computation so that once an accepting state is reached, each subsequent machine step will cause a symbol near the tape head to vanish, one at a time, until none remain. If \"q\" is an accepting state, we can represent this with the following transition blocks, where \"a\" is a tape alphabet symbol:\n\nThere are a number of details to work out, such as dealing with boundaries between states, making sure that our initial tile goes first in the match, and so on, but this shows the general idea of how a static tile puzzle can simulate a Turing machine computation.\n\nThe previous example\n\n\"q\"101#1\"q\"01#11\"q\"1#1\"q\"10.\n\nis represented as the following solution to the Post correspondence problem:\n\nMany variants of PCP have been considered. One reason is that, when one tries to prove undecidability of some new problem by reducing from PCP, it often happens that the first reduction one finds is not from PCP itself but from an apparently weaker version. Also note that if one removes \"correspondence\", the problem becomes decidable.\n\n\n"}
{"id": "2058464", "url": "https://en.wikipedia.org/wiki?curid=2058464", "title": "Pre-algebra", "text": "Pre-algebra\n\nPre-algebra is a common name for a course in middle school mathematics. In the United States, pre-algebra is usually taught in the 8th grade. The objective of it is to prepare students for the study of algebra.\n\nPre-algebra includes several broad subjects:\n\nPre-algebra may include subjects from geometry, especially subjects that further understanding of algebra in applications to area and volume.\n"}
{"id": "348860", "url": "https://en.wikipedia.org/wiki?curid=348860", "title": "Projective representation", "text": "Projective representation\n\nIn the field of representation theory in mathematics, a projective representation of a group \"G\" on a vector space \"V\" over a field \"F\" is a group homomorphism from \"G\" to the projective linear group\nwhere GL(\"V\") is the general linear group of invertible linear transformations of \"V\" over \"F\", and \"F\" is the normal subgroup consisting of nonzero scalar multiples of the identity; scalar transformations).\n\nIn more concrete terms, a projective representation is a collection of operators formula_1, where it is understood that each formula_2 is only defined up to multiplication by a constant. These should satisfy the homomorphism property up to a constant:\nfor some constants formula_4.\n\nSince each formula_2 is only defined up to a constant anyway, it does not strictly speaking make sense to ask whether the constants formula_4 are equal to 1. Nevertheless, one can ask whether it is \"possible to choose\" a particular representative of each family formula_2 of operators in such a way that the formula_2's satisfy the homomorphism property on the nose, not just up to a constant. If such a choice is possible, we say that formula_9 can be \"de-projectivized,\" or that formula_9 can be \"lifted to an ordinary representation.\" This possibility is discussed further below.\n\nOne way in which a projective representation can arise is by taking a linear group representation of on and applying the quotient map\n\nwhich is the quotient by the subgroup of scalar transformations (diagonal matrices with all diagonal entries equal). The interest for algebra is in the process in the other direction: given a \"projective representation\", try to 'lift' it to an ordinary \"linear representation\". A general projective representation cannot be lifted to a linear representation , and the obstruction to this lifting can be understood via group homology, as described below. \n\nHowever, one \"can\" lift a projective representation formula_9 of to a linear representation of a different group , which will be a central extension of . The group formula_13 is the subgroup of formula_14 defined as follows:\nwhere formula_16 is the quotient map of formula_17 onto formula_18. Since formula_9 is a homomorphism, it is easy to check that formula_13 is, indeed, a subgroup of formula_14. If the original projective representation formula_9 is faithful, then formula_13 is isomorphic to the preimage in formula_17 of formula_25.\n\nWe can define a homomorphism formula_26 by setting formula_27. The kernel of formula_28 is:\nwhich is contained in the center of formula_13. It is clear also that formula_28 is surjective, so that formula_13 is a central extension of formula_33. We can also define an ordinary representation formula_34 of formula_13 by setting formula_36. The \"ordinary\" representation formula_34 of formula_13 is a lift of the \"projective\" representation formula_9 of formula_33 is the sense that:\n\nIf is a perfect group there is a single universal perfect central extension of that can be used.\n\nThe analysis of the lifting question involves group cohomology. Indeed, if one fixes for each in a lifted element in lifting from back to , the lifts then satisfy\n\nfor some scalar in . It follows that the 2-cocycle or Schur multiplier satisfies the cocycle equation\n\nfor all in . This depends on the choice of the lift ; a different choice of lift will result in a different cocycle\n\ncohomologous to . Thus defines a unique class in . This class might not be trivial. For example, in the case of the symmetric group and alternating group, Schur established that there is exactly one non-trivial class of Schur multiplier, and completely determined all the corresponding irreducible representations.\n\nIn general, a nontrivial class leads to an extension problem for . If is correctly extended we obtain a linear representation of the extended group, which induces the original projective representation when pushed back down to . The solution is always a central extension. From Schur's lemma, it follows that the irreducible representations of central extensions of , and the irreducible projective representations of , are essentially the same objects.\n\nConsider the field formula_45 of integers mod formula_46, where formula_46 is prime, and let formula_48 be the formula_46-dimensional space of functions on formula_45 with values in formula_51. For each formula_52 in formula_45, define two operators, formula_54 and formula_55 on formula_48 as follows:\nWe write the formula for formula_55 as if formula_52 and formula_61 were integers, but it is easily seen that the result only depends on the value of formula_52 and formula_61 mod formula_46. The operator formula_54 is a translation, while formula_55 is a shift in frequency space (that is, it has the effect of translating the discrete Fourier transform of formula_67).\n\nOne may easily verify that for any formula_52 and formula_61 in formula_45, the operators formula_54 and formula_72 commute up to multiplication by a constant:\n\nWe may therefore define a projective representation formula_9 of formula_75 as follows:\nwhere formula_77 denotes the image of an operator formula_78 in the quotient group formula_18. Since formula_54 and formula_72 commute up to a constant, formula_9 is easily seen to be a projective representation. On the other hand, since formula_54 and formula_72 do not actually commute—and no nonzero multiples of them will commute—formula_9 cannot be lifted to an ordinary (linear) representation of formula_75.\n\nSince the projective representation formula_9 is faithful, the central extension formula_13 of formula_75 obtained by the construction in the previous section is just the preimage in formula_17 of the image of formula_9. Explicitly, this means that formula_13 is the group of all operators of the form\nfor formula_94. This group is a discrete version of the Heisenberg group and is isomorphic to the group of matrices of the form\n\nwith formula_94.\n\nStudying projective representations of Lie groups leads one to consider true representations of their central extensions (see Group extension#Lie groups). In many cases of interest it suffices to consider representations of covering groups. Specifically, suppose formula_97 is a connected cover of a connected Lie group formula_33, so that formula_99 for a discrete central subgroup formula_100 of formula_97. (Note that formula_97 is a special sort of central extension of formula_33.) Suppose also that formula_104 is an irreducible unitary representation of formula_97 (possibly infinite dimensional). Then by Schur's lemma, the central subgroup formula_100 will act by scalar multiples of the identity. Thus, at the projective level, formula_104 will descend to formula_33. That is to say, for each formula_109, we can choose a preimage formula_110 of formula_111 in formula_110, and define a projective representation formula_9 of formula_33 by setting\nwhere formula_77 denotes the image in formula_18 of an operator formula_78. Since formula_100 is contained in the center of formula_97 and the center of formula_97 acts as scalars, the value of formula_122 does not depend on the choice of formula_110.\n\nThe preceding construction is an important source of examples of projective representations. Bargmann's theorem (discussed below) gives a criterion under which \"every\" irreducible projective unitary representation of formula_33 arises in this way.\n\nA physically important example of the above construction comes from the case of the rotation group SO(3), whose universal cover is SU(2). According to the representation theory of SU(2), there is exactly one irreducible representation of SU(2) in each dimension. When the dimension is odd (the \"integer spin\" case), the representation descends to an ordinary representation of SO(3). When the dimension is even (the \"fractional spin\" case), the representation does not descend to an ordinary representation of SO(3) but does (by the result discussed above) descend to a projective representation of SO(3). Such projective representations of SO(3) (the ones that do not come from ordinary representations) are referred to as \"spinorial representations.\"\n\nBy an argument discussed below, every finite-dimensional, irreducible \"projective\" representation of SO(3) comes from a finite-dimensional, irreducible \"ordinary\" representation of SU(2).\n\nNotable cases of covering groups giving interesting projective representations:\n\n\nIn quantum physics, symmetry of a physical system is typically implemented by means of a projective unitary representation formula_9 of a Lie group formula_33 on the quantum Hilbert space, that is, a continuous homomorphism\nwhere formula_130 is the quotient of the unitary group formula_131 by the operators of the form formula_132. The reason for taking the quotient is that physically, two vectors in the Hilbert space that differ by a constant represent the same physical state. [That is to say, the space of (pure) states is the set of equivalence classes of unit vectors that differ by a constant.] Thus, a unitary operator that is a multiple of the identity actually acts as the identity on the level of physical states. \n\nA finite-dimensional projective representation of formula_33 then gives rise to a projective unitary representation formula_134 of the Lie algebra formula_135 of formula_33. In the finite-dimensional case, it is always possible to \"de-projectivize\" the Lie-algebra representation formula_134 simply by choosing a representative for each formula_138 having trace zero. In light of the homomorphisms theorem, it is then possible to de-projectivize formula_9 itself, but at the expense of passing to the universal cover formula_140 of formula_33. That is to say, every finite-dimensional projective unitary representation of formula_33 arises from an ordinary unitary representation of formula_97 by the procedure mentioned at the beginning of this section. \n\nSpecifically, since the Lie-algebra representation was de-projectivized by choosing a trace-zero representative, every finite-dimensional projective unitary representation of formula_33 arises from a \"determinant-one\" ordinary unitary representation of formula_97 (i.e., one in which each element of formula_97 acts as an operator with determinant one). If formula_135 is semisimple, then every element of formula_135 is a linear combination of commutators, in which case \"every\" representation of formula_135 is by operators with trace zero. In the semisimple case, then, the associated linear representation of formula_97 is unique.\n\nConversely, if formula_9 is an \"irreducible\" unitary representation of the universal cover formula_140 of formula_33, then by Schur's lemma, the center of formula_140 acts as scalar multiples of the identity. Thus, at the projective level, formula_9 descends to a projective representation of the original group formula_33. Thus, there is a natural one-to-one correspondence between the irreducible projective representations of formula_33 and the irreducible, determinant-one ordinary representations of formula_97. (In the semisimple case, the qualifier \"determinant-one\" may be omitted, because in that case, every representation of formula_97 is automatically determinant one.)\n\nAn important example is the case of SO(3), whose universal cover is SU(2). Now, the Lie algebra formula_160 is semisimple. Furthermore, since SU(2) is a compact group, every finite-dimensional representation of it admits an inner product with respect to which the representation is unitary. Thus, the irreducible \"projective\" representations of SO(3) are in one-to-one correspondence with the irreducible \"ordinary\" representations of SU(2).\n\nThe results of the previous subsection do not hold in the infinite-dimensional case, simply because the trace of formula_138 is typically not well defined. Indeed, the result fails: Consider, for example, the translations in position space and in momentum space for a quantum particle moving in formula_162, acting on the Hilbert space formula_163. These operators are defined as follows:\nfor all formula_166. These operators are simply continuous versions of the operators formula_54 and formula_55 described in the \"First example\" section above. As in that section, we can then define a \"projective\" unitary representation formula_9 of formula_170:\nbecause the operators commute up to a phase factor. But no choice of the phase factors will lead to an ordinary unitary representation, since translations in position do not commute with translations in momentum (and multiplying by a nonzero constant will not change this). These operators do, however, come from an ordinary unitary representation of the Heisenberg group, which is a one-dimensional central extension of formula_170. (See also the Stone–von Neumann theorem.)\n\nOn the other hand, Bargmann's theorem states that if the two-dimensional Lie algebra cohomology formula_173 of formula_135 is trivial, then every projective unitary representation of formula_33 can be de-projectivized after passing to the universal cover. More precisely, suppose we begin with a projective unitary representation formula_9 of a Lie group formula_33. Then the theorem states that formula_9 can be lifted to an ordinary unitary representation formula_179 of the universal cover formula_97 of formula_33. This means that formula_9 maps each element of the kernel of the covering map to a scalar multiple of the identity—so that at the projective level, formula_179 descends to formula_33—and that the associated projective representation of formula_33 is equal to formula_9.\n\nThe theorem does not apply to the group formula_170—as the previous example shows—because the two-dimensional cohomology of the associated commutative Lie algebra is nontrivial. Examples where the result does apply include semisimple groups (e.g., SL(2,R)) and the Poincaré group. This last result is important for Wigner's classification of the projective unitary representations of the Poincaré group.\n\nThe proof of Bargmann's theorem goes by considering a central extension formula_13 of formula_33, constructed similarly to the section above on linear representations and projective representations, as a subgroup of the direct product group formula_190, where formula_191 is the Hilbert space on which formula_9 acts and formula_193 is the group of unitary operators on formula_191. The group formula_13 is defined as\nAs in the earlier section, the map formula_26 given by formula_198 is a surjective homomorphism whose kernel is formula_199 so that formula_13 is a central extension of formula_33. Again as in the earlier section, we can then define a linear representation formula_34 of formula_13 by setting formula_204. Then formula_34 is a lift of formula_9 in the sense that formula_207, where formula_16 is the quotient map from formula_193 to formula_210. \n\nA key technical point is to show that formula_13 is a \"Lie\" group. (This claim is not so obvious, because if formula_191 is infinite dimensional, the group formula_190 is an infinite-dimensional topological group.) Once this result is established, we see that formula_13 is a one-dimensional Lie group central extension of formula_33, so that the Lie algebra formula_216 of formula_13 is also a one-dimensional central extension of formula_135. But the cohomology group formula_173 may be identified with the space of one-dimensional central extensions of formula_135; if formula_173 is trivial then every one-dimensional central extension of formula_135 is trivial. In that case, formula_216 is just the direct sum of formula_135 with a copy of the real line. It follows that the universal cover formula_225 of formula_13 must be just a direct product of the universal cover of formula_33 with a copy of the real line. We can then lift formula_34 from formula_13 to formula_225 (by composing with the covering map) and finally restrict this lift to the universal cover of formula_33.\n\n\n"}
{"id": "1935820", "url": "https://en.wikipedia.org/wiki?curid=1935820", "title": "Proxy re-encryption", "text": "Proxy re-encryption\n\nProxy re-encryption (PRE) schemes are cryptosystems which allow third parties (proxies) to alter a ciphertext which has been encrypted for one party, so that it may be decrypted by another.\n\nA proxy re-encryption is generally used when one party, say Bob, wants to reveal the contents of messages sent to him and encrypted with his public key to a third party, Chris, without revealing his private key to Chris. Bob does not want the proxy to be able to read the contents of his messages. \nBob could designate a proxy to re-encrypt one of his messages that is to be sent to Chris. This generates a new key that Chris can use to decrypt the message. Now if Bob sends Chris a message that was encrypted under Bob's key, the proxy will alter the message, allowing Chris to decrypt it. This method allows for a number of applications such as e-mail forwarding, law-enforcement monitoring, and content distribution.\n\nA weaker re-encryption scheme is one in which the proxy possesses both parties' keys simultaneously. One key decrypts a plaintext, while the other encrypts it. Since the goal of many proxy re-encryption schemes is to avoid revealing either of the keys or the underlying plaintext to the proxy, this method is not ideal.\n\nProxy re-encryption schemes are similar to traditional symmetric or asymmetric encryption schemes, with the addition of two functions:\n\nProxy re-encryption should not be confused with proxy signatures, which is a separate construction with a different purpose.\n\n\n"}
{"id": "3084709", "url": "https://en.wikipedia.org/wiki?curid=3084709", "title": "Semi-differentiability", "text": "Semi-differentiability\n\nIn calculus, a branch of mathematics, the notions of one-sided differentiability and semi-differentiability of a real-valued function \"f\" of a real variable are weaker than differentiability. Specifically, the function \"f\" is said to be right differentiable at a point \"a\" if, roughly speaking, a derivative can be defined as the function's argument \"x\" moves to the right (increases) from the value \"a\", and left differentiable at \"a\" if the derivative can be defined as \"x\" moves to the left from \"a\".\n\nIn mathematics, a left derivative and a right derivative are derivatives (rates of change of a function) defined for movement in one direction only (left or right; that is, to lower or higher values) by the argument of a function.\n\nLet \"f\" denote a real-valued function defined on a subset \"I\" of the real numbers.\n\nIf is a limit point of   and the one-sided limit\n\nexists as a real number, then \"f\" is called right differentiable at \"a\" and the limit \"∂\"\"f\"(\"a\") is called the right derivative of \"f\" at \"a\".\n\nIf is a limit point of   and the one-sided limit\n\nexists as a real number, then \"f\" is called left differentiable at \"a\" and the limit \"∂\"\"f\"(\"a\") is called the left derivative of \"f\" at \"a\".\n\nIf is a limit point of   and and if \"f\" is left and right differentiable at \"a\", then \"f\" is called semi-differentiable at \"a\".\n\nIf the left and right derivatives are equal, then they have the same value as the usual (\"bidirectional\") derivative. One can also define a symmetric derivative, which equals the arithmetic mean of the left and right derivatives (when they both exist), so the symmetric derivative may exist when the usual derivative does not.\n\n\nIf a real-valued, differentiable function \"f\", defined on an interval \"I\" of the real line, has zero derivative everywhere, then it is constant, as an application of the mean value theorem shows. The assumption of differentiability can be weakened to continuity and one-sided differentiability of \"f\". The version for right differentiable functions is given below, the version for left differentiable functions is analogous.\n\nAnother common use is to describe derivatives treated as binary operators in infix notation, in which the derivatives is to be applied either to the left or right operands. This is useful, for example, when defining generalizations of the Poisson bracket. For a pair of functions f and g, the left and right derivatives are respectively defined as\n\nIn bra–ket notation, the derivative operator can act on the right operand as the regular derivative or on the left as the negative derivative.\n\nThis above definition can be generalized to real-valued functions \"f\" defined on subsets of R using a weaker version of the directional derivative. Let \"a\" be an interior point of the domain of \"f\". Then \"f\" is called \"semi-differentiable\" at the point \"a\" if for every direction \"u\" ∈ R the limit \n\nexists as a real number.\n\nSemi-differentiability is thus weaker than Gâteaux differentiability, for which one takes in the limit above \"h\" → 0 without restricting \"h\" to only positive values.\n\n\nInstead of real-valued functions, one can consider functions taking values in R or in a Banach space.\n\n"}
{"id": "98764", "url": "https://en.wikipedia.org/wiki?curid=98764", "title": "Serialism", "text": "Serialism\n\nIn music, serialism is a method of composition using series of pitches, rhythms, dynamics, timbres or other musical elements. Serialism began primarily with Arnold Schoenberg's twelve-tone technique, though some of his contemporaries were also working to establish serialism as a form of post-tonal thinking. Twelve-tone technique orders the twelve notes of the chromatic scale, forming a row or series and providing a unifying basis for a composition's melody, harmony, structural progressions, and variations. Other types of serialism also work with sets, collections of objects, but not necessarily with fixed-order series, and extend the technique to other musical dimensions (often called \"parameters\"), such as duration, dynamics, and timbre.\n\nThe idea of serialism is also applied in various ways in the visual arts, design, and architecture (; ), and the musical concept has also been adopted in literature (; ; ).\n\nIntegral serialism or total serialism is the use of series for aspects such as duration, dynamics, and register as well as pitch . Other terms, used especially in Europe to distinguish post–World War II serial music from twelve-tone music and its American extensions, are general serialism and multiple serialism .\n\nComposers such as Arnold Schoenberg, Anton Webern, Alban Berg, Karlheinz Stockhausen, Pierre Boulez, Luigi Nono, Milton Babbitt, Elisabeth Lutyens, Charles Wuorinen and Jean Barraqué used serial techniques of one sort or another in most of their music. Other composers such as Béla Bartók, Luciano Berio, Benjamin Britten, John Cage, Aaron Copland, Olivier Messiaen, Arvo Pärt, Walter Piston, Ned Rorem, Alfred Schnittke, Ruth Crawford Seeger, Dmitri Shostakovich, and Igor Stravinsky used serialism only for some of their compositions or only for some sections of pieces, as did some jazz composers such as Yusef Lateef and Bill Evans.\n\nSerialism is a method , \"highly specialized technique\" , or \"way\" of composition. It may also be considered, \"a philosophy of life (\"Weltanschauung\"), a way of relating the human mind to the world and creating a completeness when dealing with a subject\" .\n\nHowever, serialism is not by itself a system of composition, nor is it a style. Neither is pitch serialism necessarily incompatible with tonality, though it is most often used as a means of composing atonal music .\n\n\"Serial music\" is a problematic term because it is used differently in different languages and especially because, shortly after its coinage in French, it underwent essential alterations during its transmission to German . The use of the word \"serial\" in connection with music was first introduced in French by René , and immediately afterward by Humphrey Searle in English, as an alternative translation of the German \"Zwölftontechnik\" twelve-tone technique or \"Reihenmusik\" (row music); it was independently introduced by Herbert Eimert and Karlheinz Stockhausen into German in 1955 as \"serielle Musik\", with a different meaning , translated into English also as \"serial music\".\n\nSerialism of the first type is most specifically defined as the structural principle according to which a recurring series of ordered elements (normally a set—or row—of pitches or pitch classes) are used in order or manipulated in particular ways to give a piece unity. Serialism is often broadly applied to all music written in what Arnold Schoenberg called \"The Method of Composing with Twelve Notes related only to one another\" (; ), or dodecaphony, and methods that evolved from his methods. It is sometimes used more specifically to apply only to music where at least one element other than pitch is subjected to being treated as a row or series. In such usages \"post-Webernian serialism\" will be used to denote works that extend serial techniques to other elements of music. Other terms used to make the distinction are \"twelve-note serialism\" for the former and \"integral serialism\" for the latter.\n\nA row may be assembled pre-compositionally (perhaps to embody particular intervallic or symmetrical properties), or it may be derived from a spontaneously invented thematic or motivic idea. The structure of the row, however, does not in itself define the structure of a composition, which requires development of a comprehensive strategy. The choice of available strategies will of course depend on the relationships contained in a row class, and rows may be constructed with an eye to producing the relationships needed to form desired strategies .\n\nThe basic set may have additional restrictions, such as the requirement that it use each interval only once.\n\nRules of analysis derived from twelve-tone theory do not apply to serialism of the second type: \"in particular the ideas, one, that the series is an intervallic sequence, and two, that the rules are consistent\" . Stockhausen, for example, in early serial compositions such as \"Kreuzspiel\" and \"Formel\", \"advances in unit sections within which a preordained set of pitches is repeatedly reconfigured ... The composer's model for the distributive serial process corresponds to a development of the Zwölftonspiel of Josef Matthias Hauer\" , and Goeyvaerts, in such a work as \"Nummer 4\", provides a classic illustration of the distributive function of seriality: 4 times an equal number of elements of equal duration within an equal global time is distributed in the most equable way, unequally with regard to one another, over the temporal space: from the greatest possible coïncidence to the greatest possible dispersion. This provides an exemplary demonstration of that logical principle of seriality: \"every situation must occur once and only once\". \nFor Henri Pousseur, after an initial period working with twelve-tone technique in works like \"Sept Versets\" (1950) and \"Trois Chants sacrés\" (1951), serialismevolved away from this bond in \"Symphonies pour quinze Solistes\" [1954–55] and in the \"Quintette\" [\"à la mémoire d’Anton Webern\", 1955], and from around the time of \"Impromptu\" [1955] encounters whole new dimensions of application and new functions.\nThe twelve-tone series loses its imperative function as a prohibiting, regulating, and patterning authority; its working-out is abandoned through its own constant-frequent presence: all 66 intervallic relations among the 12 pitches being virtually present. Prohibited intervals, like the octave, and prohibited successional relations, such as premature note repetitions, frequently occur, although obscured in the dense contexture. The number twelve no longer plays any governing, defining rôle; the pitch constellations no longer hold to the limitation determined by their formation. The dodecaphonic series loses its significance as a concrete model of shape (or a well-defined collection of concrete shapes) is played out. And the chromatic total remains active only, and provisionally, as a general reference. \nIn the 1960s Pousseur took this a step further, applying a consistent set of predefined transformations to pre-existent music. One example is the large orchestral work \"Couleurs croisées\" (Crossed Colours, 1967), which performs these transformations on the protest song \"We Shall Overcome\", thereby creating a succession of different situations that are sometimes chromatic and dissonant and at other times diatonic and consonant . In his opera \"Votre Faust\" (Your Faust, 1960–68) Pousseur used a large number of different quotations, themselves arranged into a \"scale\" for serial treatment, so as to bring coherence and order to the work. This \"generalised\" serialism (in the strongest possible sense) aims not to exclude any musical phenomena, no matter how heterogenous, in order \"to control the effects of tonal determinism, dialectize its causal functions, and overcome any academic prohibitions, especially the fixing of an anti-grammar meant to replace some previous one\" .\n\nAt about the same time, Stockhausen began using serial methods to integrate a variety of musical sources from recorded examples of folk and traditional music from around the world in his electronic composition \"Telemusik\" (1966), and from national anthems in \"Hymnen\" (1966–67). He extended this serial \"polyphony of styles\" in a series of \"process-plan\" works in the late 1960s, as well as later in portions of \"Licht\", the cycle of seven operas he composed between 1977 and 2003 .\n\nIn the late 19th and early 20th century, composers began to struggle against the ordered system of chords and intervals known as \"functional tonality\". Composers such as Debussy and Strauss found differing ways of stretching the limits of the tonal system in order to accommodate their ideas. After a brief period of free atonality, Arnold Schoenberg and others began exploring tone rows, in which an ordering of the twelve pitches of the equal tempered chromatic scale is used as the source material of a composition. This ordered set, often called a row, allowed for new forms of expression and (unlike free atonality) the expansion of underlying structural organizing principles without recourse to common practice harmony .\n\nTwelve-tone serialism first appeared in the 1920s, with antecedents predating that decade (instances of twelve-note passages occur in Liszt's \"Faust Symphony\" , and in Bach , ). Schoenberg was the composer most decisively involved in devising and demonstrating the fundamentals of twelve-tone serialism, though it is clear it is not the work of just one musician .\n\nSerialism, along with John Cage's indeterminate music (music composed with the use of chance operations) and Werner Meyer-Eppler's aleatoricism, was enormously influential in post-war music. Theorists such as George Perle codified serial systems, and his 1962 text \"Serial Composition and Atonality\" became a standard work on the origins of serial composition in the work of Schoenberg, Berg, and Webern.\n\nThe serialization of rhythm, dynamics, and other elements of music was partly fostered by the work of Olivier Messiaen and his analysis students, including Karel Goeyvaerts and Boulez, in post-war Paris.\n\nSeveral of the composers associated with Darmstadt, notably Karlheinz Stockhausen, Karel Goeyvaerts, and Henri Pousseur developed a form of serialism that initially rejected the recurring rows characteristic of twelve-tone technique, in order to eradicate any lingering traces of thematicism . Instead of a recurring, referential row, \"each musical component is subjected to control by a series of numerical proportions\" . In Europe, the style of some serial and non-serial music of the early 1950s emphasized the determination of all parameters for each note independently, often resulting in widely spaced, isolated \"points\" of sound, an effect called first in German \"punktuelle Musik\" (\"pointist\" or \"punctual music\"), then in French \"musique ponctuelle\", but quickly confused with \"pointillistic\" (German \"pointillistische\", French \"pointilliste\"), the familiar term associated with the densely packed dots in paintings of Seurat, despite the fact that the conception was at the opposite extreme .\n\nPieces were structured by closed sets of proportions, a method closely related to certain works from the de Stijl and Bauhaus movements in design and architecture called \"serial art\" by some writers (; ; ; ), specifically the paintings of Piet Mondrian, Theo van Doesburg, Bart van Leck, Georg van Tongerloo, Richard Paul Lohse, and Burgoyne Diller, who had been seeking to “avoid repetition and symmetry on all structural levels and working with a limited number of elements” .\n\nStockhausen described the final synthesis in this manner:\n\nSo serial thinking is something that's come into our consciousness and will be there forever: it's relativity and nothing else. It just says: Use all the components of any given number of elements, don't leave out individual elements, use them all with equal importance and try to find an equidistant scale so that certain steps are no larger than others. It's a spiritual and democratic attitude toward the world. The stars are organized in a serial way. Whenever you look at a certain star sign you find a limited number of elements with different intervals. If we more thoroughly studied the distances and proportions of the stars we'd probably find certain relationships of multiples based on some logarithmic scale or whatever the scale may be. \n\nIgor Stravinsky's adoption of twelve-tone serial techniques offers an example of the level of influence that serialism had after the Second World War. Previously Stravinsky had used series of notes without rhythmic or harmonic implications . Because many of the basic techniques of serial composition have analogs in traditional counterpoint, uses of inversion, retrograde, and retrograde inversion from before the war are not necessarily indicative of Stravinsky adopting Schoenbergian techniques. However with his meeting Robert Craft and acquaintance with younger composers, Stravinsky began to consciously study Schoenberg's music, as well as the music of Webern and later composers, and began to use the techniques in his own work, using, for example, serial techniques applied to fewer than twelve notes. Over the course of the 1950s he used procedures related to Messiaen, Webern and Berg. While it is difficult to label each and every work as \"serial\" in the strict definition, every major work of the period has clear uses and references to its ideas.\n\nDuring this period, the concept of \"serialism\" influenced not only new compositions but also the scholarly analysis of the classical masters. Adding to their professional tools of sonata form and tonality, scholars began to analyze previous works in the light of serial techniques; for example they found the use of row technique in previous composers going back to Mozart and Beethoven (; ). In particular, the orchestral outburst that introduces the development section half-way through the last movement of Mozart's next-to-last symphony is a tone row that Mozart punctuates in a very modern and violent episode that Michael Steinberg called \"rude octaves and frozen silences\" .\n\nRuth Crawford Seeger is credited with extending serial controls to parameters other than pitch and to formal planning as early as 1930–33 .\n\nSome music theorists have criticized serialism on the basis that the compositional strategies employed are often incompatible with the way information is extracted by the human mind from a piece of music. Nicolas Ruwet (1959) was one of the first to criticise serialism through a comparison with linguistic structures, citing theoretical claims by Boulez and Henri Pousseur, and taking as specific examples bars from Stockhausen's \"Klavierstücke I & II\", and calling for a general re-examination of Webern's music. Ruwet specifically names three works as exempt from his criticism: Stockhausen's \"Zeitmaße\" and \"Gruppen\", and Boulez's \"Le marteau sans maître\" .\n\nIn response, questioned the equivalence made by Ruwet between phoneme and the single note. He also suggested that, if analysis of \"Le marteau sans maître\" and \"Zeitmaße\", \"performed with sufficient insight\", were to be made from the point of view of wave theory—taking into account the dynamic interaction of the different component phenomena, which creates \"waves\" that interact in a sort of frequency modulation—this analysis \"would accurately reflect the realities of perception\". This was because these composers had long since acknowledged the lack of differentiation found in punctual music and, becoming increasingly aware of the laws of perception and complying better with them, \"paved the way to a more effective kind of musical communication, without in the least abandoning the emancipation that they had been allowed to achieve by this 'zero state' that was punctual music\". This was achieved, amongst other things, by the development of the concept of \"groups\", which allows structural relationships to be defined not only between individual notes but also at higher and higher levels, up to the overall form of a piece. This is \"a structural method par excellence\", and a sufficiently simple conception that it remains easily perceptible . Pousseur also points out that serial composers were the first to recognize and attempt to move beyond the lack of differentiation within certain pointillist works . Pousseur later followed up his own suggestion, by developing his idea of \"wave\" analysis and applying it to Stockhausen's \"Zeitmaße\" in two essays, and .\n\nLater writers have continued both lines of reasoning. Fred Lerdahl, for example, outlines Ruwet's subject further in his essay \"Cognitive Constraints on Compositional Systems\" . Lehrdahl has in turn been criticized for excluding \"the possibility of other, non-hierarchical methods of achieving musical coherence,\" and for concentrating on the audibility of tone rows , and the portion of his essay focussing on Boulez's \"multiplication\" technique (exemplified in three movements of \"Le Marteau sans maître\") has been challenged on perceptual grounds by Stephen Heinemann (1998) and Ulrich . Ruwet's critique has also been criticised for making \"the fatal mistake of equating visual presentation (a score) with auditive presentation (the music as heard)\" .\n\nWithin the community of modern music, exactly what constituted serialism was also a matter of debate. The conventional English usage is that the word \"serial\" applies to all twelve-tone music, which is a subset of serial music, and it is this usage that is generally intended in reference works. Nevertheless, a large body of music exists that is called \"serial\" but does not employ note-rows at all, let alone twelve-tone technique, e.g., Stockhausen's \"Klavierstücke I–IV\" (the focus of Ruwet's criticism, they use permuted sets), as well as his \"Stimmung\" (with pitches from the overtone series, which also is used as the model for the rhythms) and Pousseur's \"Scambi\" (where the permuted sounds are made exclusively from filtered white noise).\n\nWhen serialism is not equated with twelve-tone technique, a contributing problem is that the word \"serial\" is seldom if ever defined. Nevertheless, there are many published analyses of individual pieces in which the term is used, though examples are merely pointed out and the actual meaning is skated around .\n\nThe vocabulary of serialism eventually became rooted in set theory, and uses a seemingly quasi-mathematical vocabulary to describe how the basic sets are manipulated to produce the final result. Musical set theory is often used to analyze and compose serial music, but may also be used to study tonal music and nonserial atonal music.\n\nThe basis for serial composition is Schoenberg's twelve-tone technique, where the twelve notes of the basic chromatic scale are organized into a row. This \"basic\" row is then used to create permutations, that is, rows derived from the basic set by reordering its elements. The row may be used to produce a set of intervals, or a composer may have wanted to use a particular succession of intervals, from which the original row was created. A row that uses all of the intervals in their ascending form once is an all-interval row. In addition to permutations, the basic row may have some set of notes derived from it, which is used to create a new row, these are \"derived sets\".\n\nBecause there are tonal chord progressions that use all twelve notes, it is possible to create pitch rows with very strong tonal implications, and even to write tonal music using twelve-tone technique. Most tone rows contain subsets that can imply a pitch center; a composer can create music centered on one or more of the row's constituent pitches by emphasizing or avoiding these subsets, respectively, as well as through other, more complex compositional devices (; ).\n\nTo serialize other elements of music, a system quantifying an identifiable element must be created or defined (this is called \"parametrization\", after the term in mathematics). For example, if duration is to be serialized, then a set of durations must be specified. If tone colour (timbre) is to be serialized, then a set of separate tone colours must be identified, and so on.\n\nThe selected set or sets, their permutations and derived sets form the basic material with which the composer works.\n\nComposition using twelve-tone serial methods focuses on each appearance of the collection of twelve chromatic notes, called an aggregate. (Sets of more or fewer pitches, or of elements other than pitch may be treated analogously.) The principle is that in a row, no element of the aggregate should be reused until all of the other members have been used, and each member must appear only in its place in the series. This rule is violated in numerous works still termed \"serial\".\n\nAn aggregate may be divided into subsets, and all the members of the aggregate not part of any one subset are said to be its \"complement\". A subset is \"self-complementing\" if it contains half of the set and its complement is also a permutation of the original subset. This is most commonly seen with \"hexachords\" or six-note segments from a basic tone row. A hexachord that is self-complementing for a particular permutation is referred to as \"prime combinatorial\". A hexachord that is self-complementing for all of the canonic operations—inversion, retrograde, and retrograde inversion—is referred to as \"all-combinatorial\".\n\nThe composer then presents the aggregate. If there are multiple serial sets, or if several parameters are associated with the same set, then a presentation will have these values calculated. Large-scale design may be achieved through the use of combinatorial devices, for example, subjecting a subset of the basic set to a series of combinatorial devices.\n\n\n\n"}
{"id": "36544526", "url": "https://en.wikipedia.org/wiki?curid=36544526", "title": "Sugeno integral", "text": "Sugeno integral\n\nIn mathematics, the Sugeno integral, named after M. Sugeno, is a type of integral with respect to a fuzzy measure.\n\nLet formula_1 be a measurable space and let formula_2 be an formula_3-measurable function.\n\nThe Sugeno integral over the crisp set formula_4 of the function formula_5 with respect to the fuzzy measure formula_6 is defined by:\nwhere formula_8.\n\nThe Sugeno integral over the fuzzy set formula_9 of the function formula_5 with respect to the fuzzy measure formula_6 is defined by:\n\nwhere formula_13 is the membership function of the fuzzy set formula_9.\n\n"}
{"id": "1063436", "url": "https://en.wikipedia.org/wiki?curid=1063436", "title": "T-schema", "text": "T-schema\n\nThe T-schema (\"truth schema\"; not to be confused with 'Convention T') is used to give an inductive definition of truth which lies at the heart of any realisation of Alfred Tarski's semantic theory of truth. Some authors refer to it as the \"Equivalence Schema\", a synonym introduced by Michael Dummett.\n\nThe T-schema is often expressed in natural language, but it can be formalized in many-sorted predicate logic or modal logic; such a formalisation is called a \"T-theory.\" T-theories form the basis of much fundamental work in philosophical logic, where they are applied in several important controversies in analytic philosophy.\n\nAs expressed in semi-natural language (where 'S' is the name of the sentence abbreviated to S):\n'S' is true if and only if S\n\nExample: 'snow is white' is true if and only if snow is white.\n\nBy using the schema one can give an inductive definition for the truth of compound sentences. Atomic sentences are assigned truth values disquotationally. For example, the sentence \"'Snow is white' is true\" becomes materially equivalent with the sentence \"snow is white\", i.e. 'snow is white' is true if and only if snow is white. The truth of more complex sentences is defined in terms of the components of the sentence:\n\nJoseph Heath points out that \"The analysis of the truth predicate provided by Tarski's Schema T is not capable of handling all occurrences of the truth predicate in natural language. In particular, Schema T treats only “freestanding” uses of the predicate—cases when it is applied to complete sentences.\" He gives as \"obvious problem\" the sentence:\nHeath argues that analyzing this sentence using T-schema generates the sentence fragment—“everything that Bill believes”—on the righthand side of the Logical biconditional.\n\n\n"}
{"id": "10711519", "url": "https://en.wikipedia.org/wiki?curid=10711519", "title": "Torkel Franzén", "text": "Torkel Franzén\n\nTorkel Franzén (1 April 1950 – 19 April 2006) was a Swedish academic. He worked at the Department of Computer Science and Electrical Engineering at Luleå University of Technology, Sweden, in the fields of mathematical logic and computer science. He was known for his work on Gödel's incompleteness theorems and for his contributions to Usenet. He was active in the online science fiction fan community, and even issued his own electronic fanzine \"Frotz\" on his fiftieth birthday. He died of bone cancer at age 56.\n\n\n\n"}
{"id": "7180468", "url": "https://en.wikipedia.org/wiki?curid=7180468", "title": "Unity amplitude", "text": "Unity amplitude\n\nA sinusoidal waveform is said to have a unity amplitude when the amplitude of the wave is equal to 1.\n\nformula_1 \n\nwhere formula_2. This terminology is most commonly used in Digital Signal Processing and is usually associated with the Fourier series and Fourier Transform sinusoids that involve a duty cycle, formula_3, and a defined fundamental period, formula_4.\n\nAnalytic signals with unit amplitude satisfy the Bedrosian Theorem .\n"}
{"id": "76174", "url": "https://en.wikipedia.org/wiki?curid=76174", "title": "Universal quantification", "text": "Universal quantification\n\nIn predicate logic, a universal quantification is a type of quantifier, a logical constant which is interpreted as \"given any\" or \"for all\". It expresses that a propositional function can be satisfied by every member of a domain of discourse. In other words, it is the predication of a property or relation to every member of the domain. It asserts that a predicate within the scope of a universal quantifier is true of every value of a predicate variable.\n\nIt is usually denoted by the turned A (∀) logical operator symbol, which, when used together with a predicate variable, is called a universal quantifier (\"∀x\", \"∀(x)\", or sometimes by alone). Universal quantification is distinct from \"existential\" quantification (\"there exists\"), which only asserts that the property or relation holds for at least one member of the domain.\n\nQuantification in general is covered in the article on quantification (logic). Symbols are encoded .\n\nSuppose it is given that\n2·0 = 0 + 0, and 2·1 = 1 + 1, and 2·2 = 2 + 2, etc.\nThis would seem to be a logical conjunction because of the repeated use of \"and\". However, the \"etc.\" cannot be interpreted as a conjunction in formal logic. Instead, the statement must be rephrased:\nFor all natural numbers \"n\", 2·\"n\" = \"n\" + \"n\".\nThis is a single statement using universal quantification.\n\nThis statement can be said to be more precise than the original one. While the \"etc.\" informally includes natural numbers, and nothing more, this was not rigorously given. In the universal quantification, on the other hand, the natural numbers are mentioned explicitly.\n\nThis particular example is true, because any natural number could be substituted for \"n\" and the statement \"2·\"n\" = \"n\" + \"n\"\" would be true. In contrast,\nFor all natural numbers \"n\", 2·\"n\" > 2 + \"n\"\nis false, because if \"n\" is substituted with, for instance, 1, the statement \"2·1 > 2 + 1\" is false. It is immaterial that \"2·\"n\" > 2 + \"n\"\" is true for \"most\" natural numbers \"n\": even the existence of a single counterexample is enough to prove the universal quantification false.\n\nOn the other hand,\nfor all composite numbers \"n\", 2·\"n\" > 2 + \"n\"\nis true, because none of the counterexamples are composite numbers. This indicates the importance of the \"domain of discourse\", which specifies which values \"n\" can take. In particular, note that if the domain of discourse is restricted to consist only of those objects that satisfy a certain predicate, then for universal quantification this requires a logical conditional. For example,\nFor all composite numbers \"n\", 2·\"n\" > 2 + \"n\"\nis logically equivalent to\nFor all natural numbers \"n\", if \"n\" is composite, then 2·\"n\" > 2 + \"n\".\nHere the \"if ... then\" construction indicates the logical conditional.\n\nIn symbolic logic, the universal quantifier symbol formula_1 (an inverted \"A\" in a sans-serif font, Unicode U+2200) is used to indicate universal quantification.\n\nFor example, if \"P\"(\"n\") is the predicate \"2·\"n\" > 2 + \"n\" and N is the set of natural numbers, then:\n\nis the (false) statement:\nFor all natural numbers \"n\", 2·\"n\" > 2 + \"n\".\n\nSimilarly, if \"Q\"(\"n\") is the predicate \"n\" is composite\", then\n\nis the (true) statement:\nFor all natural numbers \"n\", if \"n\" is composite, then 2·\"n\" > 2 + n\n\nand since \"\"n\" is composite\" implies that \"n\" must already be a natural number, we can shorten this statement to the equivalent:\n\nFor all composite numbers \"n\", 2·\"n\" > 2 + \"n\".\n\nSeveral variations in the notation for quantification (which apply to all forms) can be found in the quantification article. There is a special notation used only for universal quantification, which is given:\n\nThe parentheses indicate universal quantification by default.\n\nNote that a quantified propositional function is a statement; thus, like statements, quantified functions can be negated. The notation most mathematicians and logicians utilize to denote negation is: formula_6. However, some use the tilde (~).\n\nFor example, if P(\"x\") is the propositional function \"x is married\", then, for a universe of discourse X of all living human beings, the universal quantification\nGiven any living person \"x\", that person is married\nis given:\n\nIt can be seen that this is irrevocably false. Truthfully, it is stated that\nIt is not the case that, given any living person \"x\", that person is married\nor, symbolically:\n\nIf the statement is not true for \"every\" element of the Universe of Discourse, then, presuming the universe of discourse is non-empty, there must be at least one element for which the statement is false. That is, the negation of formula_7 is logically equivalent to \"There exists a living person \"x\" who is not married\", or:\n\nGenerally, then, the negation of a propositional function's universal quantification is an existential quantification of that propositional function's negation; symbolically,\n\nIt is erroneous to state \"all persons are not married\" (i.e. \"there exists no person who is married\") when it is meant that \"not all persons are married\" (i.e. \"there exists a person who is not married\"):\n\nThe universal (and existential) quantifier moves unchanged across the logical connectives ∧, ∨, →, and ↚, as long as the other operand is not affected; that is:\n\nformula_13\n\nConversely, for the logical connectives ↑, ↓, ↛, and ←, the quantifiers flip:\n\nformula_14\n\nA rule of inference is a rule justifying a logical step from hypothesis to conclusion. There are several rules of inference which utilize the universal quantifier.\n\n\"Universal instantiation\" concludes that, if the propositional function is known to be universally true, then it must be true for any arbitrary element of the universe of discourse. Symbolically, this is represented as\n\nwhere \"c\" is a completely arbitrary element of the universe of discourse.\n\n\"Universal generalization\" concludes the propositional function must be universally true if it is true for any arbitrary element of the universe of discourse. Symbolically, for an arbitrary \"c\",\n\nThe element \"c\" must be completely arbitrary; else, the logic does not follow: if \"c\" is not arbitrary, and is instead a specific element of the universe of discourse, then P(\"c\") only implies an existential quantification of the propositional function.\n\nBy convention, the formula formula_17 is always true, regardless of the formula \"P\"(\"x\"); see vacuous truth.\n\nThe universal closure of a formula φ is the formula with no free variables obtained by adding a universal quantifier for every free variable in φ. For example, the universal closure of\nis\n\nIn category theory and the theory of elementary topoi, the universal quantifier can be understood as the right adjoint of a functor between power sets, the inverse image functor of a function between sets; likewise, the existential quantifier is the left adjoint.\n\nFor a set formula_20, let formula_21 denote its powerset. For any function formula_22 between sets formula_20 and formula_24, there is an inverse image functor formula_25 between powersets, that takes subsets of the codomain of \"f\" back to subsets of its domain. The left adjoint of this functor is the existential quantifier formula_26 and the right adjoint is the universal quantifier formula_27.\n\nThat is, formula_28 is a functor that, for each subset formula_29, gives the subset formula_30 given by\nthose formula_32 in the image of formula_33 under formula_34. Similarly, the universal quantifier formula_35 is a functor that, for each subset formula_29, gives the subset formula_37 given by\nthose formula_32 whose preimage under formula_34 is contained in formula_33.\n\nThe more familiar form of the quantifiers as used in first-order logic is obtained by taking the function \"f\" to be the unique function formula_42 so that formula_43 is the two-element set holding the values true and false, a subset \"S\" is that subset for which the predicate formula_44 holds, and\nwhich is true if formula_33 is not empty, and\nwhich is false if S is not X.\n\nThe universal and existential quantifiers given above generalize to the presheaf category.\n\n\n"}
{"id": "140589", "url": "https://en.wikipedia.org/wiki?curid=140589", "title": "Z++", "text": "Z++\n\nZ++ (pronounced \"zee plus plus\") was an object-oriented extension to the Z specification language.\n\nZ++ is an object-oriented extension to the Z specification language, allowing for the definition of classes, and the relation of classes through inheritance, association or aggregation.\n\nThe primary construct of Z++ is a class. A Z++ class consists of a number of clauses which are optional.\n\nZ++ Class Structure:\n\n\n\n"}
