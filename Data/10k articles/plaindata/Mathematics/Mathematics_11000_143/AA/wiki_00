{"id": "191178", "url": "https://en.wikipedia.org/wiki?curid=191178", "title": "42 (number)", "text": "42 (number)\n\n42 (forty-two) is the natural number that succeeds 41 and precedes 43.\n\nForty-two (42) is a pronic number and an abundant number; its prime factorization 2 · 3 · 7 makes it the second sphenic number and also the second of the form (2 · 3 · \"r\").\n\nAdditional properties of the number 42 include:\n\n\n\n\n\n\n\nThe number 42 is, in \"The Hitchhiker's Guide to the Galaxy\" by Douglas Adams, the \"Answer to the Ultimate Question of Life, the Universe, and Everything\", calculated by an enormous supercomputer named Deep Thought over a period of 7.5 million years. Unfortunately, no one knows what the question is. Thus, to calculate the Ultimate Question, a special computer the size of a small planet was built from organic components and named \"Earth\". The Ultimate Question \"What do you get when you multiply six by nine\" was found by Arthur Dent and Ford Prefect in the second book of the series, \"The Restaurant at the End of the Universe\". This appeared first in the radio play and later in the novelization of \"The Hitchhiker's Guide to the Galaxy\". The fact that Adams named the episodes of the radio play \"fits\", the same archaic title for a chapter or section used by Lewis Carroll in \"The Hunting of the Snark\", suggests that Adams was influenced by Carroll's fascination with and frequent use of the number. The fourth book in the series, , contains 42 chapters. According to , 42 is the street address of Stavromula Beta. In 1994 Adams created the \"42 Puzzle\", a game based on the number 42.\n\nThe 2011 book \"42: Douglas Adams' Amazingly Accurate Answer to Life, the Universe and Everything\" examines Adams' choice of the number 42 and also contains a compendium of some instances of the number in science, popular culture, and humour.\n\nLewis Carroll, who was a mathematician, made repeated use of this number in his writings.\n\nExamples of Carroll's use of 42:\n\n\n\n\n\n\n\n\n"}
{"id": "4474244", "url": "https://en.wikipedia.org/wiki?curid=4474244", "title": "Actuarial reserves", "text": "Actuarial reserves\n\nAn actuarial reserve is a liability equal to the actuarial present value of the future cash flows of a contingent event. In the insurance context an actuarial reserve is the present value of the future cash flows of an insurance policy and the total liability of the insurer is the sum of the actuarial reserves for every individual policy. Regulated insurers are required to keep offsetting assets to pay off this future liability.\n\nThe loss random variable is the starting point in the determination of any type of actuarial reserve calculation. Define formula_1 to be the future state lifetime random variable of a person aged x. Then, for a death benefit of one dollar and premium formula_2, the loss random variable, formula_3, can be written in actuarial notation as a function of formula_1\n\nFrom this we can see that the present value of the loss to the insurance company now if the person dies in \"t\" years, is equal to the present value of the death benefit minus the present value of the premiums.\n\nThe loss random variable described above only defines the loss at issue. For \"K\"(\"x\") > \"t\", the loss random variable at time \"t\" can be defined as:\n\nNet level premium reserves, also called benefit reserves, only involve two cash flows and are used for some US GAAP reporting purposes. The valuation premium in an NLP reserve is a premium such that the value of the reserve at time zero is equal to zero. The net level premium reserve is found by taking the expected value of the loss random variable defined above. They can be formulated prospectively or retrospectively. The amount of prospective reserves at a point in time is derived by subtracting the actuarial present value of future valuation premiums from the actuarial present value of the future insurance benefits. Retrospective reserving subtracts accumulated value of benefits from accumulated value of valuation premiums as of a point in time. The two methods yield identical results (assuming bases are the same for both prospective and retrospective calculations).\n\nAs an example, consider a whole life insurance policy of one dollar issues on (x) with yearly premiums paid at the start of the year and death benefit paid at the end of the year. In actuarial notation, a benefit reserve is denoted as \"V\". Our objective is to find the value of the net level premium reserve at time t. First we define the loss random variable at time zero for this policy. Hence\n\nThen, taking expected values we have:\n\nSetting the reserve equal to zero and solving for P yields:\n\nFor a whole life policy as defined above the premium is denoted as formula_12 in actuarial notation. The NLP reserve at time \"t\" is the expected value of the loss random variable at time \"t\" given \"K\"(\"x\") > \"t\"\n\nwhere formula_16\n\nModified reserves are based on premiums which are not level by duration. Almost all modified reserves are intended to accumulate lower reserves in early policy years than they would under the net level premium method. This is to allow the issuer greater margins to pay for expenses which are usually very high in these years. To do this, modified reserves assume a lower premium in the first year or two than the net level premium, and later premiums are higher. The Commissioner's Reserve Valuation Method, used for statutory reserves in the United States, allows for use of modified reserves.\nA full preliminary term reserve is calculated by treating the first year of insurance as a one- year term insurance. Reserves for the remainder of the insurance are calculated as if they are for the same insurance minus the first year. This method usually decreases reserves in the first year sufficiently to allow payment of first year expenses for low-premium plans, but not high-premium plans such as limited-pay whole life.\n\nThe calculation process often involves a number of assumptions, particularly in relation to future claims experience, and investment earnings potential. Generally, the computation involves calculating the expected claims for each future time period. These expected future cash outflows are then discounted to reflect interest to the date of the expected cash flow.\n\nFor example, if we expect to pay $300,000 in Year 1, $200,000 in year 2 and $150,000 in Year 3, and we are able to invest reserves to earn 8%p.a., the respective contributions to Actuarial Reserves are:\n\n\nIf we sum the discounted expected claims over all years in which a claim could be experienced, we have completed the computation of Actuarial Reserves. In the above example, if there were no expected future claims after year 3, our computation would give Actuarial Reserves of $568,320.38.\n\n"}
{"id": "5492505", "url": "https://en.wikipedia.org/wiki?curid=5492505", "title": "Bar induction", "text": "Bar induction\n\nBar induction is a reasoning principle used in intuitionistic mathematics, introduced by L.E.J. Brouwer. Bar induction's main use is the intuitionistic derivation of the Fan Theorem, a key result used in the derivation of the Uniform Continuity Theorem.\n\nIt is also useful in giving constructive alternatives to other classical results.\n\nThe goal of the principle is to prove properties for all infinite sequences of natural numbers (called choice sequences in intuitionistic terminology), by inductively reducing them to properties of finite lists. Bar induction can also be used to prove properties about all choice sequences in a \"spread\" (a special kind of set).\n\nGiven a choice sequence formula_1, any finite sequence of elements formula_2 of this sequence is called an \"initial segment\" of this choice sequence.\n\nThere are three forms of bar induction currently in the literature, each one places certain restrictions on a pair of predicates and the key differences are highlighted using bold font.\n\nGiven two predicates formula_4 and formula_5 on finite sequences of natural numbers such that all of the following conditions hold:\n\nthen we can conclude that formula_5 holds for the empty sequence (i.e. A holds for all choice sequences starting with the empty sequence).\n\nThis principle of bar induction is favoured in the works of, A. S. Troelstra, S.C. Kleene and Dragalin.\n\nGiven two predicates formula_4 and formula_5 on finite sequences of natural numbers such that all of the following conditions hold:\n\nthen we can conclude that formula_5 holds for the empty sequence.\n\nThis principle of bar induction is favoured in the works of Joan Moschovakis and is (intuitionistically) provably equivalent to decidable bar induction.\n\nGiven two predicates formula_4 and formula_5 on finite sequences of natural numbers such that all of the following conditions hold:\n\nthen we can conclude that formula_5 holds for the empty sequence.\n\nThis principle of bar induction is used in the works of A. S. Troelstra, S.C. Kleene, Dragalin and Joan Moschovakis. It is usually derived from either thin bar induction or monotonic bar induction. (Dummett 1977)\n\nThe following results about these schemata can be intuitionistically proven (mouse over symbols for meaning):\n\n\nAn additional schemata of bar induction was originally given as a theorem by Brouwer (1975) containing no \"extra\" restriction on R under the name \"The Bar Theorem\". However, the proof for this theorem was erroneous, and unrestricted bar induction is not considered to be intuitionistically valid (see Dummett 1977 pp94-104 for a summary of why this is so). The schema of unrestricted bar induction is given below for completeness.\n\nGiven two predicates formula_4 and formula_5 on finite sequences of natural numbers such that all of the following conditions hold:\n\nthen we can conclude that formula_5 holds for the empty sequence.\n\nIn classical reverse mathematics, \"bar induction\" (formula_3) denotes the related principle stating that if a relation formula_4\" is a well-order, then we have the schema of transfinite induction over formula_4 for arbitrary formulas.\n\n"}
{"id": "11682860", "url": "https://en.wikipedia.org/wiki?curid=11682860", "title": "Barnardisation", "text": "Barnardisation\n\nBarnardisation is a method of disclosure control for tables of counts that involves randomly adding or subtracting 1 from some cells in the table.\n\nIt is named after Professor George Alfred Barnard (1915–2002), a professor of mathematics at the University of Essex.\n\nIn the United Kingdom, barnardisation is sometimes employed by public agencies in order to enable them to provide information for statistical purposes without infringing the information privacy rights of the individuals to whom the information relates. The question whether barnardisation may fall short of the complete anonymisation of data and the status of barnardised data under the complex provisions of the Data Protection Act 1998 were considered by the House of Lords in the case of \"Common Services Agency v Scottish Information Commissioner\" [2008] 1 WLR 1550, the above case is also reported at All ER 2008 (4) 851.\n\n"}
{"id": "15811316", "url": "https://en.wikipedia.org/wiki?curid=15811316", "title": "Box–Cox distribution", "text": "Box–Cox distribution\n\nIn statistics, the Box–Cox distribution (also known as the power-normal distribution) is the distribution of a random variable \"X\" for which the Box–Cox transformation on \"X\" follows a truncated normal distribution. It is a continuous probability distribution having probability density function (pdf) given by\n\nfor \"y\" > 0, where \"m\" is the location parameter of the distribution, \"s\" is the dispersion, \"ƒ\" is the family parameter, \"I\" is the indicator function, Φ is the cumulative distribution function of the standard normal distribution, and sgn is the sign function.\n\n"}
{"id": "36601188", "url": "https://en.wikipedia.org/wiki?curid=36601188", "title": "Bramble (graph theory)", "text": "Bramble (graph theory)\n\nIn graph theory, a bramble for an undirected graph \"G\" is a family of connected subgraphs of \"G\" that all touch each other: for every pair of disjoint subgraphs, there must exist an edge in \"G\" that has one endpoint in each subgraph. The \"order\" of a bramble is the smallest size of a hitting set, a set of vertices of \"G\" that has a nonempty intersection with each of the subgraphs. Brambles may be used to characterize the treewidth of \"G\".\n\nA haven of order \"k\" in a graph \"G\" is a function \"β\" that maps each set \"X\" of fewer than \"k\" vertices to a connected component of \"G\" − \"X\", in such a way that every two subsets \"β\"(\"X\") and \"β\"(\"Y\") touch each other. Thus, the set of images of \"β\" forms a bramble in \"G\", with order \"k\". Conversely, every bramble may be used to determine a haven: for each set \"X\" of size smaller than the order of the bramble, there is a unique connected component \"β\"(\"X\") that contains all of the subgraphs in the bramble that are disjoint from \"X\".\n\nAs Seymour and Thomas showed, the order of a bramble (or equivalently, of a haven) characterizes treewidth: a graph has a bramble of order \"k\" if and only if it has treewidth at least .\n\nExpander graphs of bounded degree have treewidth proportional to their number of vertices, and therefore also have brambles of linear order. However, as Grohe and Marx showed, for these graphs, a bramble of such a high order must include exponentially many sets. More strongly, for these graphs, even brambles whose order is slightly larger than the square root of the treewidth must have exponential size. However, Grohe and Marx also showed that every graph of treewidth \"k\" has a bramble of polynomial size and of order formula_1.\n\nBecause brambles may have exponential size, it is not always possible to construct them in polynomial time for graphs of unbounded treewidth. However, when the treewidth is bounded, a polynomial time construction is possible: it is possible to find a bramble of order \"k\", when one exists, in time O(\"n\") where \"n\" is the number of vertices in the given graph. Even faster algorithms are possible for graphs with few minimal separators.\n\nBodlaender, Grigoriev, and Koster studied heuristics for finding brambles of high order. Their methods do not always generate brambles of order close to the treewidth of the input graph, but for planar graphs they give a constant approximation ratio. Kreutzer and Tazari provide randomized algorithms that, on graphs of treewidth \"k\", find brambles of polynomial size and of order formula_2 within polynomial time, coming within a logarithmic factor of the order shown by to exist for polynomial size brambles.\n\nThe concept of bramble has also been defined for directed graphs. In a directed graph \"D\", a bramble is a collection of strongly connected subgraphs of \"D\" that all touch each other: for every pair of disjoint elements \"X\", \"Y\" of the bramble, there must exist an arc in \"D\" from \"X\" to \"Y\" and one from \"Y\" to \"X\". The \"order\" of a bramble is the smallest size of a hitting set, a set of vertices of \"D\" that has a nonempty intersection with each of the elements of the bramble. The \"bramble number\" of \"D\" is the maximum order of a bramble in \"D\".\nAs noted in , the bramble number of a directed graph is within a constant factor of its directed treewidth.\n"}
{"id": "21042117", "url": "https://en.wikipedia.org/wiki?curid=21042117", "title": "Brooks' theorem", "text": "Brooks' theorem\n\nIn graph theory, Brooks' theorem states a relationship between the maximum degree of a graph and its chromatic number. According to the theorem, in a connected graph in which every vertex has at most Δ neighbors, the vertices can be colored with only Δ colors, except for two cases, complete graphs and cycle graphs of odd length, which require Δ + 1 colors.\n\nThe theorem is named after R. Leonard Brooks, who published a proof of it in 1941. A coloring with the number of colors described by Brooks' theorem is sometimes called a \"Brooks coloring\" or a Δ-\"coloring\".\n\nFor any connected undirected graph \"G\" with maximum degree Δ,\nthe chromatic number of \"G\" is at most Δ unless \"G\" is a complete graph or an odd cycle, in which case the chromatic number is Δ + 1.\n\n gives a simplified proof of Brooks' theorem. If the graph is not biconnected, its biconnected components may be colored separately and then the colorings combined. If the graph has a vertex \"v\" with degree less than Δ, then a greedy coloring algorithm that colors vertices farther from \"v\" before closer ones uses at most Δ colors. Therefore, the most difficult case of the proof concerns biconnected Δ-regular graphs with Δ ≥ 3. In this case, Lovász shows that one can find a spanning tree such that two nonadjacent neighbors \"u\" and \"w\" of the root \"v\" are leaves in the tree. A greedy coloring starting from \"u\" and \"w\" and processing the remaining vertices of the spanning tree in bottom-up order, ending at \"v\", uses at most Δ colors. For, when every vertex other than \"v\" is colored, it has an uncolored parent, so its already-colored neighbors cannot use up all the free colors, while at \"v\" the two neighbors \"u\" and \"w\" have equal colors so again a free color remains for \"v\" itself.\n\nA more general version of the theorem applies to list coloring: given any connected undirected graph with maximum degree Δ that is neither a clique nor an odd cycle, and a list of Δ colors for each vertex, it is possible to choose a color for each vertex from its list so that no two adjacent vertices have the same color. In other words, the list chromatic number of a connected undirected graph G never exceeds Δ, unless G is a clique or an odd cycle. This has been proved by .\n\nFor certain graphs, even fewer than Δ colors may be needed. shows that Δ − 1 colors suffice if and only if the given graph has no Δ-clique, \"provided\" Δ is large enough. For triangle-free graphs, or more generally graphs in which the neighborhood of every vertex is sufficiently sparse, O(Δ/log Δ) colors suffice.\n\nThe degree of a graph also appears in upper bounds for other types of coloring; for edge coloring, the result that the chromatic index is at most Δ + 1 is Vizing's theorem. An extension of Brooks' theorem to total coloring, stating that the total chromatic number is at most Δ + 2, has been conjectured by Mehdi Behzad and Vizing. The Hajnal–Szemerédi theorem on equitable coloring states that any graph has a (Δ + 1)-coloring in which the sizes of any two color classes differ by at most one.\n\nA Δ-coloring, or even a Δ-list-coloring, of a degree-Δ graph may be found in linear time. Efficient algorithms are also known for finding Brooks colorings in parallel and distributed models of computation.\n\n"}
{"id": "3415428", "url": "https://en.wikipedia.org/wiki?curid=3415428", "title": "Conformally flat manifold", "text": "Conformally flat manifold\n\nA (pseudo-)Riemannian manifold is conformally flat if each point has a neighborhood that can be mapped to flat space by a conformal transformation.\n\nMore formally, let (\"M\", \"g\") be a pseudo-Riemannian manifold. Then (\"M\", \"g\") is conformally flat if for each point \"x\" in \"M\", there exists a neighborhood \"U\" of \"x\" and a smooth function \"f\" defined on \"U\" such that (\"U\", \"e\"\"g\") is flat (i.e. the curvature of \"e\"\"g\" vanishes on \"U\"). The function \"f\" need not be defined on all of \"M\".\n\nSome authors use \"locally conformally flat\" to describe the above notion and reserve \"conformally flat\" for the case in which the function \"f\" is defined on all of \"M\".\n\n\n"}
{"id": "29947659", "url": "https://en.wikipedia.org/wiki?curid=29947659", "title": "Confrontation analysis", "text": "Confrontation analysis\n\nConfrontation analysis (also known as dilemma analysis) is an operational analysis technique used to structure, understand and think through multi-party interactions such as negotiations. It is the underpinning mathematical basis of drama theory. \n\nIt is derived from game theory but considers that instead of resolving the game, the players often redefine the game when interacting. Emotions triggered from the potential interaction play a large part in this redefinition. So whereas game theory looks on an interaction as a single decision matrix and resolves that, confrontation analysis looks on the interaction as a sequence of linked interactions, where the decision matrix changes under the influence of precisely defined emotional \"dilemmas\".\n\nConfrontation analysis was devised by Professor Nigel Howard in the early 1990s drawing from his work on game theory and metagame analysis. It has been turned to defence, political, legal, financial and commercial applications.\n\nMuch of the theoretical background to General Rupert Smith's book \"The Utility of Force\" drew its inspiration from the theory of confrontation analysis.\n\nConfrontation analysis can also be used in a \"decision workshop\" as structure to support role-playing for training, analysis and decision rehearsal.\n\nConfrontation analysis looks on an interaction as a sequence of confrontations. During each confrontation the parties communicate until they have made their \"positions\" clear to one another. These positions can be expressed as a \"card table\" (also known as an options board) of yes/no decisions. For each decision each party communicates what they would like to happen (their \"position\") and what will happen if they cannot agree (the \"threatened future\"). These interactions produce \"dilemmas\" and the \"card table\" changes as players attempt to eliminate these.\n\nConsider the example on the (Initial Card Table), taken from the 1995 Bosnian Conflict. This represents an interaction between the Bosnian Serbs and the United Nations forces over the safe areas. The Bosnian Serbs had Bosniak enclaves surrounded and were threatening to attack.\n\nEach side had a position as to what they wanted to happen:\n\nThe Bosnian Serbs wanted (see 4th column):\nThe UN wanted (See 5th column):\nIf no further changes were made then what the sides were saying would happen was (see 1st column):\n\nConfrontation analysis then specifies a number of precisely defined \"dilemmas\" that occur to the parties following from the structure of the card tables. It states that motivated by the desire to eliminate these dilemmas, the parties involved will CHANGE THE CARD TABLE, to eliminate their problem.\n\nIn the situation at the start the Bosnian Serbs have no dilemmas, but the UN has four. It has three \"persuasion dilemmas\" in that the Bosnian Serbs are not going to do the three things they want them to (not attack the enclaves, withdraw the heavy weapons and not take hostages). It also has a \"rejection dilemma\" in that the Bosnian Serbs do not believe they will actually use the air strikes, as they think the UN will submit to their position, for fear of having hostages taken.\n\nFaced with these dilemmas, the UN modified the card table to eliminate its dilemmas. It took two actions:\n\nFirstly, it withdrew its forces from the positions where they were vulnerable to being taken hostage. This action eliminated the Bosnian Serbs' option (card) of taking hostages.\nSecondly, with the addition of the Rapid Reaction Force, and in particular its artillery the UN had an additional capability to engage Bosnian Serb weapons; they added the card \"Use artillery against Bosnian Serbs\". Because of this, the UN's threat of air strikes became more credible. The situation changed to that of the Second Card Table:\n\nThe Bosnian Serbs wanted (see 4th column):\nThe UN wanted (See 5th column):\nIf no further changes were made then what the sides were saying would happen was (see 1st column):\n\nFaced with this new situation, the Bosnian Serbs modified their position to accept the UN proposal. The final table was an agreement as shown in the Final Card table (see thumbnail and picture).\n\nConfrontation analysis does not necessarily produce a win-win solution (although end states are more likely to remain stable if they do); however, the word \"confrontation\" should not necessarily imply that any negotiations should be carried out in an aggressive way.\n\nThe \"card tables\" or are isomorphic to game theory models, but are not built with the aim of finding a \"solution\". Instead, the aim is to find the dilemmas facing characters and so help to predict how they will change the table itself. Such prediction requires not only analysis of the model and its dilemmas, but also exploration of the reality outside the model; without this it is impossible to decide which ways of changing the model in order to eliminate dilemmas might be rationalized by the characters.\n\nSometimes analysis of the ticks and crosses can be supported by values showing the payoff to each of the parties.\n\n"}
{"id": "5213404", "url": "https://en.wikipedia.org/wiki?curid=5213404", "title": "Craps principle", "text": "Craps principle\n\nIn probability theory, the craps principle is a theorem about event probabilities under repeated iid trials. Let formula_1 and formula_2 denote two mutually exclusive events which might occur on a given trial. Then the probability that formula_1 occurs before formula_2 equals the conditional probability that formula_1 occurs given that formula_1 or formula_2 occur on the next trial, which is\n\nThe events formula_1 and formula_2 need not be collectively exhaustive (if they are, the result is trivial).\n\nLet formula_11 be the event that formula_1 occurs before formula_2. Let formula_14 be the event that neither formula_1 nor formula_2 occurs on a given trial. Since formula_14, formula_1 and formula_2 are mutually exclusive and collectively exhaustive for the first trial, we have\n\nand formula_21. \nSince the trials are i.i.d., we have formula_22. Using formula_23 and solving the displayed equation for formula_24 gives the formula \n\nIf the trials are repetitions of a game between two players, and the events are\n\nthen the craps principle gives the respective conditional probabilities of each player winning a certain repetition, given that someone wins (i.e., given that a draw does not occur). In fact, the result is only affected by the relative marginal probabilities of winning formula_28 and formula_29 ; in particular, the probability of a draw is irrelevant.\n\nIf the game is played repeatedly until someone wins, then the conditional probability above is the probability that the player wins the game. This is illustrated below for the original game of craps, using an alternative proof.\n\nIf the game being played is craps, then this principle can greatly simplify the computation of the probability of winning in a certain scenario. Specifically, if the first roll is a 4, 5, 6, 8, 9, or 10, then the dice are repeatedly re-rolled until one of two events occurs:\n\nSince formula_1 and formula_2 are mutually exclusive, the craps principle applies. For example, if the original roll was a 4, then the probability of winning is\n\nThis avoids having to sum the infinite series corresponding to all the possible outcomes:\n\nMathematically, we can express the probability of rolling formula_36 ties followed by rolling the point:\n\nThe summation becomes an infinite geometric series:\n\nwhich agrees with the earlier result.\n"}
{"id": "56706426", "url": "https://en.wikipedia.org/wiki?curid=56706426", "title": "Ermelinda DeLaViña", "text": "Ermelinda DeLaViña\n\nErmelinda DeLaViña is a Hispanic American mathematician specializing in graph theory. One of her results in this area is related to an inequality showing that every undirected graph has an independent set that is at least as large as its radius; DeLaViña showed that the graphs with no larger independent set always contain a Hamiltonian path.\nShe is a professor in the Computer and Mathematical Sciences Department of the University of Houston–Downtown, where she is also Associate Dean of the College of Science and Technology.\n\nDeLaViña grew up in a working-class family in Texas, with roots stretching back for five generations there. Her parents came from Bishop, Texas, but raised her in Houston. Inspired by a 9th-grade algebra teacher, she aimed for a college education despite the discouragement of her school counselors. She started her undergraduate studies at the University of Houston, but dropped out after one term, and after working for two years began again at the University of Texas–Pan American, where she graduated with a bachelor's degree in mathematics and a minor in computer science in 1989, becoming the first in her family with a college degree.\nShe returned to graduate school at the University of Houston and completed a Ph.D. in mathematics there in 1997. Her doctoral supervisor was Siemion Fajtlowicz, with whom she worked on the Graffiti computer program for automatically formulating conjectures in graph theory.\n\nAfter completing her doctorate, DeLaViña became an assistant professor at the University of Houston–Downtown. She was promoted to full professor there in 2010, and became associate dean in 2012.\n\n"}
{"id": "27989949", "url": "https://en.wikipedia.org/wiki?curid=27989949", "title": "Extended static checking", "text": "Extended static checking\n\nExtended static checking (ESC) is a collective name for a range of techniques for statically checking the correctness of various program constraints. ESC can be thought of as an extended form of type checking. As with type checking, ESC is performed automatically at compile time (i.e. without human intervention). This distinguishes it from more general approaches to the formal verification of software, which typically rely on human-generated proofs. Furthermore, it promotes practicality over soundness, in that it aims to dramatically reduce the number of \"false positives\" (overestimated errors that are not real errors, that is, ESC over strictness) at the cost of introducing some \"false negatives\" (real ESC underestimation error, but that need no programmer's attention, or are not targeted by ESC). ESC can identify a range of errors which are currently outside the scope of a type checker, including division by zero, array out of bounds, integer overflow and null dereferences.\n\nThe techniques used in extended static checking come from various fields of Computer Science, including static program analysis, symbolic simulation, model checking, abstract interpretation, SAT solving and automated theorem proving and type checking. Extended static checking is generally performed only at an intraprocedural level (rather than an interprocedural one) in order to scale to large programs. Furthermore, extended static checking aims to report errors by exploiting user-supplied specifications, in the form of pre- and post-conditions, loop invariants and class invariants.\n\nExtended static checkers typically operate by propagating strongest postconditions (resp. weakest preconditions) intraprocedurally through a method starting from the precondition (resp. postcondition). At each point during this process an intermediate condition is generated that captures what is known at that program point. This is combined with the necessary conditions of the program statement at that point to form a \"verification condition\". An example of this is a statement involving a division, whose necessary condition is that the divisor be non-zero. The verification condition arising from this effectively states: \"given the intermediate condition at this point, it must follow that the divisor is non-zero\". All verification conditions must be shown to be false (hence correct by means of excluded third) in order for a method to pass extended static checking (or \"unable to find more errors\"). Typically, some form of automated theorem prover is used to discharge verification conditions.\n\nExtended Static Checking was pioneered in ESC/Modula-3 and, later, ESC/Java. Its roots originate from more simplistic static checking techniques, such as static debugging or Lint (software) and FindBugs. A number of other languages have adopted ESC, including Spec# and SPARKada and VHDL VSPEC. However, there is currently no widely used software programming language that provides extended static checking in its base development environment.\n\n\n"}
{"id": "38447249", "url": "https://en.wikipedia.org/wiki?curid=38447249", "title": "Fusion frame", "text": "Fusion frame\n\nIn mathematics, a fusion frame of a vector space is a natural extension of a frame. It is an additive construct of several, potentially \"overlapping\" frames. The motivation for this concept comes from the event that a signal can not be acquired by a single sensor alone (a constraint found by limitations of hardware or data throughput), rather the partial components of the signal must be collected via a network of sensors, and the partial signal representations are then \"fused\" into the complete signal.\n\nBy construction, fusion frames easily lend themselves to parallel or distributed processing of sensor networks consisting of arbitrary overlapping sensor fields.\n\nGiven a Hilbert space formula_1, let formula_2 be closed subspaces of formula_1, where formula_4 is an index set. Let formula_5 be a set of positive scalar weights. Then formula_6 is a fusion frame of formula_1 if there exist constants formula_8 such that for all formula_9 we have\n\nformula_10,\n\nwhere formula_11 denotes the orthogonal projection onto the subspace formula_12. The constants formula_13 and formula_14 are called lower and upper bound, respectively. When the lower and upper bounds are equal to each other, formula_6 becomes a formula_13-tight fusion frame. Furthermore, if formula_17, we can call formula_6 Parseval fusion frame.\n\nAssume formula_19 is a frame for formula_12. Then formula_21 is called a fusion frame system for formula_1.\n\nLet formula_23 be closed subspaces of formula_1 with positive weights formula_5. Suppose formula_19 is a frame for formula_12 with frame bounds formula_28 and formula_29. Let formula_30 and formula_31, which satisfy that formula_32. Then formula_6 is a fusion frame of formula_1 if and only if formula_35 is a frame of formula_1.\n\nAdditionally, if formula_21 is called a fusion frame system for formula_1 with lower and upper bounds formula_13 and formula_14, then formula_35 is a frame of formula_1 with lower and upper bounds formula_43 and formula_44. And if formula_35 is a frame of formula_1 with lower and upper bounds formula_47 and formula_48, then formula_21 is called a fusion frame system for formula_1 with lower and upper bounds formula_51 and formula_52.\n\nLet formula_53 be a closed subspace, and let formula_54 be an orthonormal basis of formula_55. Then for all formula_9, the orthogonal projection of formula_57 onto formula_55 is given by formula_59.\n\nWe can also express the orthogonal projection of formula_57 onto formula_55 in terms of given local frame formula_62 of formula_55,\n\nformula_64,\n\nwhere formula_65 is a dual frame of the local frame formula_62.\n\nLet formula_6 be a fusion frame for formula_1. Let formula_69 be representation space for projection. The analysis operator formula_70 is defined by\n\nformula_71.\n\nThen The adjoint operator formula_72, which we call the synthesis operator, is given by\n\nformula_73,\n\nwhere formula_74.\n\nThe fusion frame operator formula_75 is defined by\n\nformula_76.\n\nGiven the lower and upper bounds of the fusion frame formula_6, formula_13 and formula_14, the fusion frame operator formula_80 can be bounded by\n\nformula_81,\nwhere formula_82 is the identity operator. Therefore, the fusion frame operator formula_80 is positive and invertible.\n\nGiven a fusion frame system formula_84 for formula_1, where formula_86, and formula_87, which is a dual frame for formula_88, the fusion frame operator formula_80 can be expressed as\n\nformula_90,\n\nwhere formula_91, formula_92 are analysis operators for formula_88 and formula_94 respectively, and formula_95, formula_96 are synthesis operators for formula_88 and formula_94 respectively.\n\nFor finite frames (i.e., formula_99 and formula_100), the fusion frame operator can be constructed with a matrix. Let formula_6 be a fusion frame for formula_102, and let formula_103 be a frame for the subspace formula_12 and formula_105 an index set for each formula_106. With\n\nformula_107\n\nand\n\nformula_108\n\nwhere formula_109 is the canonical dual frame of formula_110, the fusion frame operator formula_111 is given by\n\nformula_112.\n\nThe fusion frame operator formula_113 is then given by an formula_114 matrix.\n\n\n"}
{"id": "747122", "url": "https://en.wikipedia.org/wiki?curid=747122", "title": "Generalized linear model", "text": "Generalized linear model\n\nIn statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a \"link function\" and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\n\nGeneralized linear models were formulated by John Nelder and Robert Wedderburn as a way of unifying various other statistical models, including linear regression, logistic regression and Poisson regression. They proposed an iteratively reweighted least squares method for maximum likelihood estimation of the model parameters. Maximum-likelihood estimation remains popular and is the default method on many statistical computing packages. Other approaches, including Bayesian approaches and least squares fits to variance stabilized responses, have been developed.\n\nOrdinary linear regression predicts the expected value of a given unknown quantity (the \"response variable\", a random variable) as a linear combination of a set of observed values (\"predictors\"). This implies that a constant change in a predictor leads to a constant change in the response variable (i.e. a \"linear-response model\"). This is appropriate when the response variable has a normal distribution (intuitively, when a response variable can vary essentially indefinitely in either direction with no fixed \"zero value\", or more generally for any quantity that only varies by a relatively small amount, e.g. human heights).\n\nHowever, these assumptions are inappropriate for some types of response variables. For example, in cases where the response variable is expected to be always positive and varying over a wide range, constant input changes lead to geometrically varying, rather than constantly varying, output changes. As an example, a prediction model might predict that 10 degree temperature decrease would lead to 1,000 fewer people visiting the beach is unlikely to generalize well over both small beaches (e.g. those where the expected attendance was 50 at a particular temperature) and large beaches (e.g. those where the expected attendance was 10,000 at a low temperature). The problem with this kind of prediction model would imply a temperature drop of 10 degrees would lead to 1,000 fewer people visiting the beach, a beach whose expected attendance was 50 at a higher temperature would now be predicted to have the impossible attendance value of −950. Logically, a more realistic model would instead predict a constant \"rate\" of increased beach attendance (e.g. an increase in 10 degrees leads to a doubling in beach attendance, and a drop in 10 degrees leads to a halving in attendance). Such a model is termed an \"exponential-response model\" (or \"log-linear model\", since the logarithm of the response is predicted to vary linearly).\n\nSimilarly, a model that predicts a probability of making a yes/no choice (a Bernoulli variable) is even less suitable as a linear-response model, since probabilities are bounded on both ends (they must be between 0 and 1). Imagine, for example, a model that predicts the likelihood of a given person going to the beach as a function of temperature. A reasonable model might predict, for example, that a change in 10 degrees makes a person two times more or less likely to go to the beach. But what does \"twice as likely\" mean in terms of a probability? It cannot literally mean to double the probability value (e.g. 50% becomes 100%, 75% becomes 150%, etc.). Rather, it is the \"odds\" that are doubling: from 2:1 odds, to 4:1 odds, to 8:1 odds, etc. Such a model is a \"log-odds or logistic model\".\n\nGeneralized linear models cover all these situations by allowing for response variables that have arbitrary distributions (rather than simply normal distributions), and for an arbitrary function of the response variable (the \"link function\") to vary linearly with the predicted values (rather than assuming that the response itself must vary linearly). For example, the case above of predicted number of beach attendees would typically be modeled with a Poisson distribution and a log link, while the case of predicted probability of beach attendance would typically be modeled with a Bernoulli distribution (or binomial distribution, depending on exactly how the problem is phrased) and a log-odds (or \"logit\") link function.\n\nIn a generalized linear model (GLM), each outcome Y of the dependent variables is assumed to be generated from a particular distribution in the exponential family, a large range of probability distributions that includes the normal, binomial, Poisson and gamma distributions, among others. The mean, μ, of the distribution depends on the independent variables, X, through:\n\nwhere E(Y) is the expected value of Y; X\"β is the \"linear predictor\", a linear combination of unknown parameters β\"; \"g\" is the link function.\n\nIn this framework, the variance is typically a function, V, of the mean:\n\nIt is convenient if V follows from the exponential family distribution, but it may simply be that the variance is a function of the predicted value.\n\nThe unknown parameters, β, are typically estimated with maximum likelihood, maximum quasi-likelihood, or Bayesian techniques.\n\nThe GLM consists of three elements:\n\nThe overdispersed exponential family of distributions is a generalization of the exponential family and exponential dispersion model of distributions and includes those probability distributions, parameterized by formula_3 and formula_4, whose density functions \"f\" (or probability mass function, for the case of a discrete distribution) can be expressed in the form\n\nThe \"dispersion parameter\", formula_4, typically is known and is usually related to the variance of the distribution. The functions formula_7, formula_8, formula_9, formula_10, and formula_11 are known. Many common distributions are in this family, including the normal, exponential, gamma, Poisson, Bernoulli, and (for fixed number of trials) binomial, multinomial, and negative binomial.\n\nFor scalar formula_12 and formula_13, this reduces to\n\nformula_3 is related to the mean of the distribution. If formula_8 is the identity function, then the distribution is said to be in canonical form (or \"natural form\"). Note that any distribution can be converted to canonical form by rewriting formula_3 as formula_18 and then applying the transformation formula_19. It is always possible to convert formula_10 in terms of the new parametrization, even if formula_21 is not a one-to-one function; see comments in the page on the exponential family. If, in addition, formula_9 is the identity and formula_4 is known, then formula_3 is called the \"canonical parameter\" (or \"natural parameter\") and is related to the mean through\n\nFor scalar formula_12 and formula_13, this reduces to\n\nUnder this scenario, the variance of the distribution can be shown to be\n\nFor scalar formula_12 and formula_13, this reduces to\n\nThe linear predictor is the quantity which incorporates the information about the independent variables into the model. The symbol \"η\" (Greek \"eta\") denotes a linear predictor. It is related to the expected value of the data through the link function.\n\n\"η\" is expressed as linear combinations (thus, \"linear\") of unknown parameters β. The coefficients of the linear combination are represented as the matrix of independent variables X. \"η\" can thus be expressed as\n\nThe link function provides the relationship between the linear predictor and the mean of the distribution function. There are many commonly used link functions, and their choice is informed by several considerations. There is always a well-defined \"canonical\" link function which is derived from the exponential of the response's density function. However, in some cases it makes sense to try to match the domain of the link function to the range of the distribution function's mean, or use a non-canonical link function for algorithmic purposes, for example Bayesian probit regression.\n\nWhen using a distribution function with a canonical parameter formula_13, the canonical link function is the function that expresses formula_13 in terms of formula_36, i.e. formula_37. For the most common distributions, the mean formula_36 is one of the parameters in the standard form of the distribution's density function, and then formula_39 is the function as defined above that maps the density function into its canonical form. When using the canonical link function, formula_40, which allows formula_41 to be a sufficient statistic for formula_42.\n\nFollowing is a table of several exponential-family distributions in common use and the data they are typically used for, along with the canonical link functions and their inverses (sometimes referred to as the mean function, as done here).\n\nIn the cases of the exponential and gamma distributions, the domain of the canonical link function is not the same as the permitted range of the mean. In particular, the linear predictor may be positive, which would give an impossible negative mean. When maximizing the likelihood, precautions must be taken to avoid this. An alternative is to use a noncanonical link function.\n\nNote also that in the case of the Bernoulli, binomial, categorical and multinomial distributions, the support of the distributions is not the same type of data as the parameter being predicted. In all of these cases, the predicted parameter is one or more probabilities, i.e. real numbers in the range formula_43. The resulting model is known as \"logistic regression\" (or \"multinomial logistic regression\" in the case that K-way rather than binary values are being predicted).\n\nFor the Bernoulli and binomial distributions, the parameter is a single probability, indicating the likelihood of occurrence of a single event. The Bernoulli still satisfies the basic condition of the generalized linear model in that, even though a single outcome will always be either 0 or 1, the \"expected value\" will nonetheless be a real-valued probability, i.e. the probability of occurrence of a \"yes\" (or 1) outcome. Similarly, in a binomial distribution, the expected value is \"Np\", i.e. the expected proportion of \"yes\" outcomes will be the probability to be predicted.\n\nFor categorical and multinomial distributions, the parameter to be predicted is a \"K\"-vector of probabilities, with the further restriction that all probabilities must add up to 1. Each probability indicates the likelihood of occurrence of one of the \"K\" possible values. For the multinomial distribution, and for the vector form of the categorical distribution, the expected values of the elements of the vector can be related to the predicted probabilities similarly to the binomial and Bernoulli distributions.\n\nThe maximum likelihood estimates can be found using an iteratively reweighted least squares algorithm or a Newton–Raphson method with updates of the form:\n\nwhere formula_45 is the observed information matrix (the negative of the Hessian matrix) and formula_46 is the score function; or a Fisher's scoring method:\n\nwhere formula_48 is the Fisher information matrix. Note that if the canonical link function is used, then they are the same.\n\nIn general, the posterior distribution cannot be found in closed form and so must be approximated, usually using Laplace approximations or some type of Markov chain Monte Carlo method such as Gibbs sampling.\n\nA possible point of confusion has to do with the distinction between generalized linear models and the general linear model, two broad statistical models. The general linear model may be viewed as a special case of the generalized linear model with identity link and responses normally distributed. As most exact results of interest are obtained only for the general linear model, the general linear model has undergone a somewhat longer historical development. Results for the generalized linear model with non-identity link are asymptotic (tending to work well with large samples).\n\nA simple, very important example of a generalized linear model (also an example of a general linear model) is linear regression. In linear regression, the use of the least-squares estimator is justified by the Gauss–Markov theorem, which does not assume that the distribution is normal.\n\nFrom the perspective of generalized linear models, however, it is useful to suppose that the distribution function is the normal distribution with constant variance and the link function is the identity, which is the canonical link if the variance is known.\n\nFor the normal distribution, the generalized linear model has a closed form expression for the maximum-likelihood estimates, which is convenient. Most other GLMs lack closed form estimates.\n\nWhen the response data, \"Y\", are binary (taking on only values 0 and 1), the distribution function is generally chosen to be the Bernoulli distribution and the interpretation of \"μ\" is then the probability, \"p\", of \"Y\" taking on the value one.\n\nThere are several popular link functions for binomial functions.\n\nThe most typical link function is the canonical logit link:\n\nGLMs with this setup are logistic regression models (or \"logit models\").\n\nAlternatively, the inverse of any continuous cumulative distribution function (CDF) can be used for the link since the CDF's range is formula_43, the range of the binomial mean. The normal CDF formula_51 is a popular choice and yields the probit model. Its link is\n\nThe reason for the use of the probit model is that a constant scaling of the input variable to a normal CDF (which can be absorbed through equivalent scaling of all of the parameters) yields a function that is practically identical to the logit function, but probit models are more tractable in some situations than logit models. (In a Bayesian setting in which normally distributed prior distributions are placed on the parameters, the relationship between the normal priors and the normal CDF link function means that a probit model can be computed using Gibbs sampling, while a logit model generally cannot.)\n\nThe complementary log-log function may also be used:\nThis link function is asymmetric and will often produce different results from the logit and probit link functions. The cloglog model corresponds to applications where we observe either zero events (e.g., defects) or one or more, where the number of events is assumed to follow the Poisson distribution. The Poisson assumption means that\n\nwhere \"μ\" is a positive number denoting the inverse of the expected number of events. If \"p\" represents the proportion of observations with at least one event, its complement\n\nand then\n\nA linear model requires the response variable to take values over the entire real line. Since \"μ\" must be positive, we can enforce that by taking the logarithm, and letting log(\"μ\") be a linear model. This produces the \"cloglog\" transformation\n\nThe identity link \"g(p) = p\" is also sometimes used for binomial data to yield a linear probability model. However, the identity link can predict nonsense \"probabilities\" less than zero or greater than one. This can be avoided by using a transformation like cloglog, probit or logit (or any inverse cumulative distribution function). A primary merit of the identity link is that it can be estimated using linear math—and other standard link functions are approximately linear matching the identity link near p = 0.5.\n\nThe variance function for \"quasibinomial\" data is:\n\nwhere the dispersion parameter \"τ\" is exactly 1 for the binomial distribution. Indeed, the standard binomial likelihood omits \"τ\". When it is present, the model is called \"quasibinomial\", and the modified likelihood is called a quasi-likelihood, since it is not generally the likelihood corresponding to any real probability distribution. If \"τ\" exceeds 1, the model is said to exhibit overdispersion.\n\nThe binomial case may be easily extended to allow for a multinomial distribution as the response (also, a Generalized Linear Model for counts, with a constrained total). There are two ways in which this is usually done:\n\nIf the response variable is an ordinal measurement, then one may fit a model function of the form:\n\nfor \"m\" > 2. Different links \"g\" lead to proportional odds models or ordered probit models.\n\nIf the response variable is a nominal measurement, or the data do not satisfy the assumptions of an ordered model, one may fit a model of the following form:\n\nfor \"m\" > 2. Different links \"g\" lead to multinomial logit or multinomial probit models. These are more general than the ordered response models, and more parameters are estimated. \n\nAnother example of generalized linear models includes Poisson regression which models count data using the Poisson distribution. The link is typically the logarithm, the canonical link.\n\nThe variance function is proportional to the mean\n\nwhere the dispersion parameter \"τ\" is typically fixed at exactly one. When it is not, the resulting quasi-likelihood model is often described as poisson with overdispersion or \"quasipoisson\".\n\nThe standard GLM assumes that the observations are uncorrelated. Extensions have been developed to allow for correlation between observations, as occurs for example in longitudinal studies and clustered designs:\n\nGeneralized additive models (GAMs) are another extension to GLMs in which the linear predictor \"η\" is not restricted to be linear in the covariates X but is the sum of smoothing functions applied to the \"x\"s:\n\nThe smoothing functions \"f\" are estimated from the data. In general this requires a large number of data points and is computationally intensive.\n\nThe term \"generalized linear model\", and especially its abbreviation GLM, are sometimes confused with the term \"general linear model\". Co-originator John Nelder has expressed regret over this terminology.\n\n\n\n"}
{"id": "26066377", "url": "https://en.wikipedia.org/wiki?curid=26066377", "title": "Haridatta", "text": "Haridatta\n\nHaridatta (c. 683 CE) was an astronomer-mathematician of Kerala, India, who is believed to be the promulgator of the Parahita system of astronomical computations. This system of computations is widely popular in Kerala and Tamil Nadu. According to legends, Haridatta promulgated the Parahita system on the occasion of the \"Mamankam\" held in the year 683 CE. Mamankam was a 12-yearly festival held in Thirunnavaya on the banks of the Bharathapuzha river.\n\nThe distinctive contribution of Harfidatta, apart from his resolving the Aryabhatiya calculations and using the Katapayadi system of numerals is the corrections he introduced to the values of the mean and true positions, the velocity, etc., of the moon and other planets as obtained from Aryabhata's constants. This correction is called the \"Sakabda-samskara\" since it applied from the date of Aryabhata in the Saka era 444, at which date his constants gave accurate results.\n\nThe Parahita system of computations introduced by Haridatta was a simplification of the system propounded in Aryabhatiya by Aryabhata. Haridatta introduced the following simplificattions.\nThe system was called Parahita meaning \"suitable for the common man\" because it simplified astronomical computations and made it accessible for practice even for ordinary persons.\n\n\nScholars have been able to identify only two works as authored by Haridatta. One of them, titled \"Grahacaranibandhana\", is the basic manual of computations of the Parahita system of astronomy. This was unearthed by K.V. Sarma and was published in 1954. The other work titled \"Mahamarganibandhana\" is no longer extant.\n\n"}
{"id": "39806471", "url": "https://en.wikipedia.org/wiki?curid=39806471", "title": "Ignatov's theorem", "text": "Ignatov's theorem\n\nIn probability and mathematical statistics, Ignatov's theorem is a basic result on the distribution of record values of a stochastic process.\n\nLet \"X\", \"X\", ... be an infinite sequence of independent and identically distributed random variables. The \"initial rank\" of the \"n\"th term of this sequence is the value \"r\" such that for exactly \"r\" values of \"i\" less than or equal to \"n\". Let denote the stochastic process consisting of the terms \"X\" having initial rank \"k\"; that is, \"Y\" is the \"j\"th term of the stochastic process that achieves initial rank \"k\". The sequence Y is called the sequence of \"k\"th partial records. Ignatov's theorem states that the sequences Y, Y, Y, ... are independent and identically distributed.\n\nThe theorem is named after Tzvetan Ignatov a Bulgarian professor in probability and mathematical statistics at Sofia University. Due to it and his general contributions to mathematics, Prof. Ignatov was granted a Doctor Honoris Causa degree in 2013 from Sofia University. The recognition is given on extremely rare occasions and only to scholars with internationally landmark results.\n\n"}
{"id": "24393682", "url": "https://en.wikipedia.org/wiki?curid=24393682", "title": "Jim Geelen", "text": "Jim Geelen\n\nJim Geelen is a professor at the Department of Combinatorics and Optimization in the faculty of mathematics at the University of Waterloo, where he holds the Canada Research Chair in Combinatorial optimization. He is known for his work on Matroid theory and the extension of the Graph Minors Project to representable matroids. In 2003, he won the Fulkerson Prize with his co-authors A. M. H. Gerards, and A. Kapoor for their research on Rota's excluded minors conjecture. In 2006, he won the Coxeter–James Prize presented by the Canadian Mathematical Society.\n\nHe received a Bachelor of Science degree in 1992 from Curtin University in Australia, and obtained his Ph.D. in 1996 at the University of Waterloo under the supervision of William Cunningham. After brief postdoctoral fellowships in the Netherlands, Germany, and Japan, he returned to the University of Waterloo in 1997.\n"}
{"id": "862189", "url": "https://en.wikipedia.org/wiki?curid=862189", "title": "John G. Thompson", "text": "John G. Thompson\n\nJohn Griggs Thompson (born October 13, 1932) is a mathematician at the University of Florida noted for his work in the field of finite groups. He was awarded the Fields Medal in 1970, the Wolf Prize in 1992 and the 2008 Abel Prize.\n\nHe received his B.A. from Yale University in 1955 and his doctorate from the University of Chicago in 1959 under the supervision of Saunders Mac Lane. After spending some time on the Mathematics faculty at the University of Chicago, he moved in 1970 to the Rouse Ball Professorship in Mathematics at the University of Cambridge, England, and later moved to the Mathematics Department of the University of Florida as a Graduate Research Professor. He is currently a Professor Emeritus of Pure Mathematics at the University of Cambridge, and professor of mathematics at the University of Florida. He received the Abel Prize 2008 together with Jacques Tits.\n\nThompson's doctoral thesis introduced new techniques, and included the solution of a problem in finite group theory which had stood for around sixty years, the nilpotency of Frobenius kernels. At the time, this achievement was noted in \"The New York Times\".\n\nThompson became a figure in the progress toward the classification of finite simple groups. In 1963, he and Walter Feit proved that all nonabelian finite simple groups are of even order (the \"Odd Order Paper\", filling a whole issue of the \"Pacific Journal of Mathematics\"). This work was recognised by the award of the 1965 Cole Prize in Algebra of the American Mathematical Society. His N-group papers classified all finite simple groups for which the normalizer of every non-identity solvable subgroup is solvable. This included, as a by-product, the classification of all minimal finite simple groups (simple groups for which every proper subgroup is solvable). This work had some influence on later developments in the classification of finite simple groups, and was quoted in the citation by Richard Brauer for the award of Thompson's Fields Medal in 1970 (Proceedings of the International Congress of Mathematicians, Nice, France, 1970).\n\nThe Thompson group \"Th\" is one of the 26 sporadic finite simple groups. Thompson also made major contributions to the inverse Galois problem. He found a criterion for a finite group to be a Galois group, that in particular implies that the monster simple group is a Galois group.\n\nIn 1971, Thompson was elected to the United States National Academy of Sciences. In 1982, he was awarded the Senior Berwick Prize of the London Mathematical Society, and in 1988, he received the honorary degree of Doctor of Science from the University of Oxford. Thompson was awarded the United States National Medal of Science in 2000. He is a Fellow of the Royal Society (United Kingdom), and a recipient of its Sylvester Medal in 1985. He is a member of the Norwegian Academy of Science and Letters.\n\n\n"}
{"id": "30673871", "url": "https://en.wikipedia.org/wiki?curid=30673871", "title": "Journal of the European Mathematical Society", "text": "Journal of the European Mathematical Society\n\nFounded in 1999, the journal publishes articles on pure and applied mathematics.\n\nThe journal is indexed in the following database:\n"}
{"id": "10854684", "url": "https://en.wikipedia.org/wiki?curid=10854684", "title": "Karnaugh map", "text": "Karnaugh map\n\nThe Karnaugh map (KM or K-map) is a method of simplifying Boolean algebra expressions. Maurice Karnaugh introduced it in 1953 as a refinement of Edward Veitch's 1952 Veitch chart, which actually was a rediscovery of Allan Marquand's 1881 \"logical diagram\" aka Marquand diagram but with a focus now set on its utility for switching circuits. Veitch charts are therefore also known as \"Marquand–Veitch diagrams\", and Karnaugh maps as \"Karnaugh–Veitch maps\" (\"KV maps\").\n\nThe Karnaugh map reduces the need for extensive calculations by taking advantage of humans' pattern-recognition capability. It also permits the rapid identification and elimination of potential race conditions.\n\nThe required Boolean results are transferred from a truth table onto a two-dimensional grid where, in Karnaugh maps, the cells are ordered in Gray code, and each cell position represents one combination of input conditions, while each cell value represents the corresponding output value. Optimal groups of 1s or 0s are identified, which represent the terms of a canonical form of the logic in the original truth table. These terms can be used to write a minimal Boolean expression representing the required logic.\n\nKarnaugh maps are used to simplify real-world logic requirements so that they can be implemented using a minimum number of physical logic gates. A sum-of-products expression can always be implemented using AND gates feeding into an OR gate, and a product-of-sums expression leads to OR gates feeding an AND gate. Karnaugh maps can also be used to simplify logic expressions in software design. Boolean conditions, as used for example in conditional statements, can get very complicated, which makes the code difficult to read and to maintain. Once minimised, canonical sum-of-products and product-of-sums expressions can be implemented directly using AND and OR logic operators. Diagrammatic and mechanical methods for minimizing simple logic expressions have existed since at least the medieval times. More systematic methods for minimizing complex expressions began to be developed in the early 1950s, but until the mid to late 1980's the Karnaugh map was the most common used in practice.\n\nKarnaugh maps are used to facilitate the simplification of Boolean algebra functions. For example, consider the Boolean function described by the following truth table.\n\nFollowing are two different notations describing the same function in unsimplified Boolean algebra, using the Boolean variables , , , , and their inverses.\n\n\n1)In this time the four input variables can be combined in 16 different ways, so the truth table has 16 rows, and the K-map has 16 positions. The K-map is therefore arranged in a 4 × 4 grid.\n\nThe row and column indices (shown across the top, and down the left side of the Karnaugh map) are ordered in Gray code rather than binary numerical order. Gray code ensures that only one variable changes between each pair of adjacent cells. Each cell of the completed Karnaugh map contains a binary digit representing the function's output for that combination ofÀ inputs.\n\nAfter the Karnaugh map has been constructed, it is used to find one of the simplest possible forms — a canonical form — for the information in the truth table. Adjacent 1s in the Karnaugh map represent opportunities to simplify the expression. The minterms ('minimal terms') for the final expression are found by encircling groups of 1s in the map. Minterm groups must be rectangular and must have an area that is a power of two (i.e., 1, 2, 4, 8…). Minterm rectangles should be as large as possible without containing any 0s. Groups may overlap in order to make each one larger. The optimal groupings in the example below are marked by the green, red and blue lines, and the red and green groups overlap. The red group is a 2 × 2 square, the green group is a 4 × 1 rectangle, and the overlap area is indicated in brown.\n\nThe cells are often denoted by a shorthand which describes the logical value of the inputs that the cell covers. For example, would mean a cell which covers the 2x2 area where and are true, i.e. the cells numbered 13, 9, 15, 11 in the diagram above. On the other hand, would mean the cells where is true and is false (that is, is true).\n\nThe grid is toroidally connected, which means that rectangular groups can wrap across the edges (see picture). Cells on the extreme right are actually 'adjacent' to those on the far left, in the sense that the corresponding input values only differ by one bit's; similarly, so are those at the very top and those at the bottom. Therefore, can be a valid term—it includes cells 12 and 8 at the top, and wraps to the bottom to include cells 10 and 14—as is , which includes the four corners.\n\nOnce the Karnaugh map has been constructed and the adjacent 1s linked by rectangular and square boxes, the algebraic minterms can be found by examining which variables stay the same within each box.\n\nFor the red grouping:\n\nThus the first minterm in the Boolean sum-of-products expression is .\n\nFor the green grouping, \"A\" and \"B\" maintain the same state, while \"C\" and \"D\" change. \"B\" is 0 and has to be negated before it can be included. The second term is therefore . Note that it is acceptable that the green grouping overlaps with the red one.\n\nIn the same way, the blue grouping gives the term .\n\nThe solutions of each grouping are combined: the normal form of the circuit is formula_5.\n\nThus the Karnaugh map has guided a simplification of\n\nIt would also have been possible to derive this simplification by carefully applying the axioms of boolean algebra, but the time it takes to do that grows exponentially with the number of terms.\n\nThe inverse of a function is solved in the same way by grouping the 0s instead.\n\nThe three terms to cover the inverse are all shown with grey boxes with different colored borders:\n\nThis yields the inverse:\n\nThrough the use of De Morgan's laws, the product of sums can be determined:\n\nKarnaugh maps also allow easy minimizations of functions whose truth tables include \"don't care\" conditions. A \"don't care\" condition is a combination of inputs for which the designer doesn't care what the output is. Therefore, \"don't care\" conditions can either be included in or excluded from any rectangular group, whichever makes it larger. They are usually indicated on the map with a dash or X.\n\nThe example on the right is the same as the example above but with the value of \"f\"(1,1,1,1) replaced by a \"don't care\". This allows the red term to expand all the way down and, thus, removes the green term completely.\n\nThis yields the new minimum equation:\n\nNote that the first term is just , not . In this case, the don't care has dropped a term (the green rectangle); simplified another (the red one); and removed the race hazard (removing the yellow term as shown in the following section on race hazards).\n\nThe inverse case is simplified as follows:\n\nKarnaugh maps are useful for detecting and eliminating race conditions. Race hazards are very easy to spot using a Karnaugh map, because a race condition may exist when moving between any pair of adjacent, but disjoint, regions circumscribed on the map. However, because of the nature of Gray coding, \"adjacent\" has a special definition explained above – we're in fact moving on a torus, rather than a rectangle, wrapping around the top, bottom, and the sides.\n\n\nWhether glitches will actually occur depends on the physical nature of the implementation, and whether we need to worry about it depends on the application. In clocked logic, it is enough that the logic settles on the desired value in time to meet the timing deadline. In our example, we are not considering clocked logic.\n\nIn our case, an additional term of formula_11 would eliminate the potential race hazard, bridging between the green and blue output states or blue and red output states: this is shown as the yellow region (which wraps around from the bottom to the top of the right half) in the adjacent diagram.\n\nThe term is redundant in terms of the static logic of the system, but such redundant, or consensus terms, are often needed to assure race-free dynamic performance.\n\nSimilarly, an additional term of formula_12 must be added to the inverse to eliminate another potential race hazard. Applying De Morgan's laws creates another product of sums expression for \"f\", but with a new factor of formula_13.\n\nThe following are all the possible 2-variable, 2 × 2 Karnaugh maps. Listed with each is the minterms as a function of formula_14 and the race hazard free (\"see previous section\") minimum equation. A minterm is defined as an expression that gives the most minimal form of expression of the mapped variables. All possible horizontal and vertical interconnected blocks can be formed. These blocks must be of the size of the powers of 2 (1, 2, 4, 8, 16, 32, ...). These expressions create a minimal logical mapping of the minimal logic variable expressions for the binary expressions to be mapped. Here are all the blocks with one field.\n\nA block can be continued across the bottom, top, left, or right of the chart. That can even wrap beyond the edge of the chart for variable minimization. This is because each logic variable corresponds to each vertical column and horizontal row. A visualization of the k-map can be considered cylindrical. The fields at edges on the left and right are adjacent, and the top and bottom are adjacent. K-Maps for four variables must be depicted as a donut or torus shape. The four corners of the square drawn by the k-map are adjacent. Still more complex maps are needed for 5 variables and more. \nAlternative graphical minimization methods include:\n\n\n\n"}
{"id": "55974779", "url": "https://en.wikipedia.org/wiki?curid=55974779", "title": "Leading-one detector", "text": "Leading-one detector\n\nA leading-one detector is an electronic circuit commonly found in central processing units and especially their arithmetic logic units (ALUs). It is used to detect whether the leading bit in a computer word is 1 or 0, which is used to perform basic binary tests like codice_1.\n"}
{"id": "5971846", "url": "https://en.wikipedia.org/wiki?curid=5971846", "title": "List of mathematicians (Z)", "text": "List of mathematicians (Z)\n\n\n\n\n\n"}
{"id": "517370", "url": "https://en.wikipedia.org/wiki?curid=517370", "title": "List of polygons, polyhedra and polytopes", "text": "List of polygons, polyhedra and polytopes\n\nA polytope is a geometric object with flat sides, which exists in any general number of dimensions. The following list of polygons, polyhedra and polytopes gives the names of various classes of polytopes and lists some specific examples.\n\n\n\n\n\n\n\n\n\n\n\nList of uniform tilings\nUniform tilings in hyperbolic plane\n\n\n\nRegular polyhedron\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4-polytope – general term for a four dimensional polytope\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3100179", "url": "https://en.wikipedia.org/wiki?curid=3100179", "title": "Locally nilpotent", "text": "Locally nilpotent\n\nIn the mathematical field of commutative algebra, an ideal \"I\" in a commutative ring \"A\" is locally nilpotent at a prime ideal \"p\" if \"I\", the localization of \"I\" at \"p\", is a nilpotent ideal in \"A\".\n\nIn non-commutative algebra and group theory, an algebra or group is locally nilpotent if and only if every finitely generated subalgebra or subgroup is nilpotent. The subgroup generated by the normal locally nilpotent subgroups is called the Hirsch–Plotkin radical and is the generalization of the Fitting subgroup to groups without the ascending chain condition on normal subgroups. \n\nA locally nilpotent ring is one in which every finitely generated subring is nilpotent: locally nilpotent rings form a radical class, giving rise to the Levitzki radical.\n"}
{"id": "21340642", "url": "https://en.wikipedia.org/wiki?curid=21340642", "title": "Math Horizons", "text": "Math Horizons\n\nMath Horizons is a magazine aimed at undergraduates interested in mathematics, published by the Mathematical Association of America. It publishes expository articles about \"beautiful mathematics\" as well as articles about the culture of mathematics covering mathematical people, institutions, humor, games, cartoons, and book reviews.\n\nThe MAA gives the Trevor Evans Awards annually to \"authors of exceptional articles that are accessible to undergraduates\" that are published in \"Math Horizons\".\n\n"}
{"id": "35811096", "url": "https://en.wikipedia.org/wiki?curid=35811096", "title": "Meaning and Necessity", "text": "Meaning and Necessity\n\nMeaning and Necessity: A Study in Semantics and Modal Logic is a 1947 book about logic by the philosopher Rudolf Carnap.\n\n\"Meaning and Necessity\" was the culmination of Carnap's concern with the semantics of natural and formal languages, which developed subsequent to his publication of \"The Logical Syntax of Language\" in 1934.\n\nCarnap attempts to develop a new method for analyzing the meanings of linguistic expressions as well as to lay a semantic foundation for modal logic. Carnap maintains that his new method consists in doing away with the traditional assumption that linguistic expressions name concrete or abstract entities and in replacing it with the ascription to them of intensions and extensions. He states that linguistic expressions designate their intensions and extensions: every designation refers to both an intension and an extension.\n\nThe intensional entities to which individual constants or descriptions, predicates, and declarative sentences are respectively said to refer are individual concepts, properties and propositions, the corresponding extensions being individuals, classes and truth-values. Carnap insists that intensions, including individual concepts, are objectively real, not mental concepts. However, he rejects the charge of hypostatization: individual concepts and properties and propositions must not be considered as things, but this does not prevent them from being genuine objective entities.\n\nCarnap's method is an alternative to Gottlob Frege's theory of sense and reference, which he refers to as the \"method of extension and intension\". Carnap holds that his method provides the most economical account of the logical behavior of expressions in modal contexts - for instance, the expressions '9' and '7' in the sentence '9 is necessarily greater than 7.' His criticism of Frege involves a rejection of the traditional category of names, conceived as a class of expressions each of which stands for a unique thing.\n\n\"Meaning and Necessity\" was influential, and laid the foundations of much subsequent work in the semantics of modal logic. The work is seen by the British philosopher A. J. Ayer as the most important of Carnap's three books on semantics, the other two being \"Introduction to Semantics\" (1942) and \"Formalization of Logic\" (1943). Ayer criticizes the book, on the grounds that since, according to Carnap, linguistic expressions designate their intensions and extensions, it is not clear that the difference between Carnap's view and what Carnap calls the traditional assumption that linguistic expressions name concrete or abstract entities is more than nominal.\n\n"}
{"id": "29557417", "url": "https://en.wikipedia.org/wiki?curid=29557417", "title": "Metavariable", "text": "Metavariable\n\nIn logic, a metavariable (also metalinguistic variable or syntactical variable) is a symbol or symbol string which belongs to a metalanguage and stands for elements of some object language. For instance, in the sentence\n\nthe symbols A and B are part of the metalanguage in which the statement about the object language ℒ is formulated.\n\nJohn Corcoran considers this terminology unfortunate because it obscures the use of schemata and because such \"variables\" do not actually range over a domain.\n\nThe convention is that a metavariable is to be uniformly substituted with the same instance in all its appearances in a given schema. This is in contrast with nonterminal symbols in formal grammars where the nonterminals on the right of a production can be substituted by different instances.\n\nAttempts to formalize the notion of metavariable result in some kind of type theory.\n\n\n"}
{"id": "3842768", "url": "https://en.wikipedia.org/wiki?curid=3842768", "title": "Network on a chip", "text": "Network on a chip\n\nA network on a chip or network-on-chip (, or ) is a network-based communications subsystem on an integrated circuit (\"microchip\"), most typically between modules in a system on a chip (SoC). The modules on the IC are typically semiconductor IP cores schematizing various functions of the computer system, and are designed to be modular in the sense of network science. The network on chip is a router-based packet switching network between SoC modules. \n\nNoC technology applies the theory and methods of computer networking to on-chip communication and brings notable improvements over conventional bus and crossbar communication architectures. Networks-on-chip come in many network topologies, many of which are still experimental as of 2018. \n\nNetworks-on-chip improve the scalability of systems-on-chip and the power efficiency of complex SoCs compared to other communication subsystem designs. A very common NoC used in contemporary personal computers is a graphics processing unit (GPU), which is commonly used in computer graphics, video gaming and accelerating artificial intelligence. They are an emerging technology, with projections for large growth in the near future as manycore computer architectures become more common.\n\nNetworks-on-chip can span synchronous and asynchronous clock domains, known as clock domain crossing, or use unclocked asynchronous logic. NoCs support globally asynchronous, locally synchronous electronics architectures, allowing each processor core or functional unit on the System-on-Chip to have its own clock domain.\n\nNoC architectures typically model sparse small-world networks (SWNs) and scale-free networks (SFNs) to limit the number, length, area and power consumption of interconnection wires and point-to-point connections.\n\nTraditionally, ICs have been designed with dedicated point-to-point connections, with one wire dedicated to each signal. This results in a dense network topology. For large designs, in particular, this has several limitations from a physical design viewpoint. It requires power quadratic in the number of interconnections. The wires occupy much of the area of the chip, and in nanometer CMOS technology, interconnects dominate both performance and dynamic power dissipation, as signal propagation in wires across the chip requires multiple clock cycles. This also allows more parasitic capacitance, resistance and inductance to accrue on the circuit. (See Rent's rule for a discussion of wiring requirements for point-to-point connections).\n\nSparsity and locality of interconnections in the communications subsystem yield several improvements over traditional bus-based and crossbar-based systems.\n\nThe wires in the links of the network-on-chip are shared by many signals. A high level of parallelism is achieved, because all data links in the NoC can operate simultaneously on different data packets. Therefore, as the complexity of integrated systems keeps growing, a NoC provides enhanced performance (such as throughput) and scalability in comparison with previous communication architectures (e.g., dedicated point-to-point signal wires, shared buses, or segmented buses with bridges). Of course, the algorithms must be designed in such a way that they offer large parallelism and can hence utilize the potential of NoC.\n\nSome researchers think that NoCs need to support quality of service (QoS), namely achieve the various requirements in terms of throughput, end-to-end delays, fairness, and deadlines. Real-time computation, including audio and video playback, is one reason for providing QoS support. However, current system implementations like VxWorks, RTLinux or QNX are able to achieve sub-millisecond real-time computing without special hardware. \n\nThis may indicate that for many real-time applications the service quality of existing on-chip interconnect infrastructure is sufficient, and dedicated hardware logic would be necessary to achieve microsecond precision, a degree that is rarely needed in practice for end users (sound or video jitter need only tenth of milliseconds latency guarantee). Another motivation for NoC-level quality of service (QoS) is to support multiple concurrent users sharing resources of a single chip multiprocessor in a public cloud computing infrastructure. In such instances, hardware QoS logic enables the service provider to make contractual guarantees on the level of service that a user receives, a feature that may be deemed desirable by some corporate or government clients.\n\nMany challenging research problems remain to be solved at all levels, from the physical link level through the network level, and all the way up to the system architecture and application software. The first dedicated research symposium on networks on chip was held at Princeton University, in May 2007. The second IEEE \"International Symposium on Networks-on-Chip\" was held in April 2008 at Newcastle University.\n\nResearch has been conducted on integrated optical waveguides and devices comprising an optical network on a chip (ONoC).\n\nNoC development and studies require comparing different proposals and options. NoC traffic patterns are under development to help such evaluations. Existing NoC benchmarks include NoCBench and MCSL NoC Traffic Patterns.\n\nAn \"interconnect processing unit\" (IPU) is a on-chip communication network with hardware and software components which jointly implement key functions of different system-on-chip programming models through a set of communication and synchronization primitives and provide low-level platform services to enable advanced features in modern heterogeneous applications on a single die.\n\n\n\nAdapted from Avinoam Kolodny's's column in the ACM SIGDA e-newsletter by Igor Markov <br>The original text can be found at http://www.sigda.org/newsletter/2006/060415.txt\n\n\n"}
{"id": "10843656", "url": "https://en.wikipedia.org/wiki?curid=10843656", "title": "New York Number Theory Seminar", "text": "New York Number Theory Seminar\n\nThe New York Number Theory Seminar is a research seminar devoted to the theory of numbers and related parts of mathematics and physics. In 1981, Number Theorists Harvey Cohn, David and Gregory Chudnovsky and Melvyn B. Nathanson, who were then affiliated with City College (CUNY), Columbia University, and Rutgers–Newark, and currently at Lehman College (CUNY) and the Polytechnic University of New York, began to meet regularly, usually on Thursday afternoons, during the academic year at the Graduate Center of the City University of New York. The location was convenient to all parts of the city and major transportation hubs. Harvey Cohn has retired, but Nathanson is now based in the City University and acts as the host of the seminar. The New York Number Theory Seminar also organizes an annual Workshop on Combinatorial and Additive Number Theory (CANT) at the CUNY Graduate Center. Proceedings of the seminar have been published regularly by Springer-Verlag.\n\nThe Graduate Center is currently located at 365 Fifth Avenue, between 34th and 35th Streets. The Ph.D. program in mathematics is located on the fourth floor. The seminar has met on Thursday afternoons for several years.\n\nHarald Helfgott: In 2013, he released two papers claiming to be a proof of Goldbach's weak conjecture; the claim is now broadly accepted. The problem had a history of over 250 years without a full proof.\n\nTom Sanders: Went to Oxford University.\n\nMelvyn B. Nathanson: Frequent collaborator with Paul Erdős, who has published more papers in the field of mathematics than any other person. Erdős has collaborated with over 500 people, and wrote 19 papers in number theory with Nathanson. He also organizes the Workshop on Combinatorial and Additive Number Theory, which has been held annually at the Graduate Center, CUNY since 2003. Nathanson's essays on political and social issues related to science have appeared in The New York Times, The Bulletin of the Atomic Scientists, The Mathematical Intelligencer, Notices of the American Mathematical Society, and other publications.\n\n"}
{"id": "24184879", "url": "https://en.wikipedia.org/wiki?curid=24184879", "title": "Panlogism", "text": "Panlogism\n\nIn philosophy, panlogism is a Hegelian doctrine that holds that the universe is the act or realization of Logos. According to the doctrine of panlogism, logic and ontology are the same study.\n"}
{"id": "24615296", "url": "https://en.wikipedia.org/wiki?curid=24615296", "title": "Pebble motion problems", "text": "Pebble motion problems\n\nThe pebble motion problems, or pebble motion on graphs, are a set of related problems in graph theory dealing with the movement of multiple objects (\"pebbles\") from vertex to vertex in a graph with a constraint on the number of pebbles that can occupy a vertex at any time. Pebble motion problems occur in domains such as multi-robot motion planning (in which the pebbles are robots) and network routing (in which the pebbles are packets of data). The best-known example of a pebble motion problem is the famous 15 puzzle where a disordered group of fifteen tiles must be rearranged within a 4x4 grid by sliding one tile at a time.\n\nThe general form of the pebble motion problem is Pebble Motion on Graphs formulated as follows:\n\nLet formula_1 be a graph with formula_2 vertices. Let formula_3 be a set of pebbles with formula_4. An arrangement of pebbles is a mapping formula_5 such that formula_6 for formula_7. A move formula_8 consists of transferring pebble formula_9 from vertex formula_10 to adjacent unoccupied vertex formula_11. The Pebble Motion on Graphs problem is to decide, given two arrangements formula_12 and formula_13, whether there is a sequence of moves that transforms formula_12 into formula_13.\n\nCommon variations on the problem limit the structure of the graph to be:\n\nAnother set of variations consider the case in which some or all of the pebbles are unlabeled and interchangeable.\n\nOther versions of the problem seek not only to prove reachability but to find a (potentially optimal) sequence of moves (i.e. a plan) which performs the transformation.\n\nFinding the shortest path in the pebble motion on graphs problem (with labeled pebbles) is known to be NP-hard and APX-hard. The unlabeled problem can be solved in polynomial time when using the cost metric mentioned above (minimizing the total number of moves to adjacent vertices), but is NP-hard for other natural cost metrics.\n"}
{"id": "2973987", "url": "https://en.wikipedia.org/wiki?curid=2973987", "title": "Penrose graphical notation", "text": "Penrose graphical notation\n\nIn mathematics and physics, Penrose graphical notation or tensor diagram notation is a (usually handwritten) visual depiction of multilinear functions or tensors proposed by Roger Penrose in 1971. A diagram in the notation consists of several shapes linked together by lines. The notation has been studied extensively by Predrag Cvitanović, who used it to classify the classical Lie groups. It has also been generalized using representation theory to spin networks in physics, and with the presence of matrix groups to trace diagrams in linear algebra.\n\nIn the language of multilinear algebra, each shape represents a multilinear function. The lines attached to shapes represent the inputs or outputs of a function, and attaching shapes together in some way is essentially the composition of functions.\n\nIn the language of tensor algebra, a particular tensor is associated with a particular shape with many lines projecting upwards and downwards, corresponding to abstract upper and lower indices of tensors respectively. Connecting lines between two shapes corresponds to contraction of indices. One advantage of this notation is that one does not have to invent new letters for new indices. This notation is also explicitly basis-independent.\n\nEach shape represents a matrix, and tensor multiplication is done horizontally, and matrix multiplication is done vertically.\n\nThe metric tensor is represented by a U-shaped loop or an upside-down U-shaped loop, depending on the type of tensor that is used.\nThe Levi-Civita antisymmetric tensor is represented by a thick horizontal bar with sticks pointing downwards or upwards, depending on the type of tensor that is used.\n\nThe structure constants (formula_1) of a Lie algebra are represented by a small triangle with one line pointing upwards and two lines pointing downwards.\n\nContraction of indices is represented by joining the index lines together.\n\nSymmetrization of indices is represented by a thick zig-zag or wavy bar crossing the index lines horizontally.\n\nAntisymmetrization of indices is represented by a thick straight line crossing the index lines horizontally.\n\nThe determinant is formed by applying antisymmetrization to the indices.\n\nThe covariant derivative (formula_2) is represented by a circle around the tensor(s) to be differentiated and a line joined from the circle pointing downwards to represent the lower index of the derivative.\n\nThe diagrammatic notation is useful in manipulating tensor algebra. It usually involves a few simple \"identities\" of tensor manipulations.\n\nFor example, formula_3, where \"n\" is the number of dimensions, is a common \"identity\".\n\nThe Ricci and Bianchi identities given in terms of the Riemann curvature tensor illustrate the power of the notation\n\nThe notation has been extended with support for spinors and twistors.\n\n"}
{"id": "18786361", "url": "https://en.wikipedia.org/wiki?curid=18786361", "title": "Ping-pong lemma", "text": "Ping-pong lemma\n\nIn mathematics, the ping-pong lemma, or table-tennis lemma, is any of several mathematical statements that ensure that several elements in a group acting on a set freely generates a free subgroup of that group.\n\nThe ping-pong argument goes back to late 19th century and is commonly attributed to Felix Klein who used it to study subgroups of Kleinian groups, that is, of discrete groups of isometries of the hyperbolic 3-space or, equivalently Möbius transformations of the Riemann sphere. The ping-pong lemma was a key tool used by Jacques Tits in his 1972 paper containing the proof of a famous result now known as the Tits alternative. The result states that a finitely generated linear group is either virtually solvable or contains a free subgroup of rank two. The ping-pong lemma and its variations are widely used in geometric topology and geometric group theory.\n\nModern versions of the ping-pong lemma can be found in many books such as Lyndon&Schupp, de la Harpe, Bridson&Haefliger and others.\n\nThis version of the ping-pong lemma ensures that several subgroups of a group acting on a set generate a free product. The following statement appears in, and the proof is from.\n\nLet \"G\" be a group acting on a set \"X\" and let \"H\", \"H\"..., \"H\" be nontrivial subgroups of \"G\" where \"k\"≥2, such that at least one of these subgroups has order greater than 2.\nSuppose there exist pairwise disjoint nonempty subsets \"X\", \"X\"...,\"X\" of \"X\" such that the following holds:\n\n\nThen\n\nBy the definition of free product, it suffices to check that a given reduced word is nontrivial. Let \"w\" be such a word, and let\n\nWhere \"w\"∈ \"H\" for all such \"β\", and since \"w\" is fully reduced \"α\"≠ α for any \"i\". We then let \"w\" act on an element of one of the sets \"X\". As we assume for at least one subgroup \"H\" has order at least 3, without loss of generality we may assume that \"H\" has order at least 3. We first make the assumption that α and α are both 1. From here we consider \"w\" acting on \"X\". We get the following chain of containments and note that since the \"X\" are disjoint that \"w\" acts nontrivially and is thus not the identity element.\n\nTo finish the proof we must consider the three cases: \nIn each case, \"hwh\" is a reduced word with \"α\"' and \"α\"' both 1, and thus is nontrivial. Finally, \"hwh\" is not 1, and so neither is \"w\". This proves the claim.\n\nLet \"G\" be a group acting on a set \"X\". Let \"a\"...,\"a\" be elements of \"G\" of infinite order, where \"k\" ≥ 2. Suppose there exist disjoint nonempty subsets\n\nof \"X\" with the following properties:\n\n\nThen the subgroup \"H\" = <\"a\", ..., \"a\"> ≤ \"G\" generated by \"a\", ..., \"a\" is free with free basis {\"a\", ..., \"a\"}.\n\nThis statement follows as a corollary of the version for general subgroups if we let \"X\"= \"X\"∪\"X\" and let \"H\" = ⟨\"a\"⟩.\n\nOne can use the ping-pong lemma to prove that the subgroup \"H\" = <\"A\",\"B\">≤SL(2,Z), generated by the matrices\n\nis free of rank two.\n\nIndeed, let \"H\" = <\"A\"> and \"H\" = <\"B\"> be cyclic subgroups of SL(2,Z) generated by \"A\" and \"B\" accordingly. It is not hard to check that A and B are elements of infinite order in SL(2,Z) and that\n\nand\n\nConsider the standard action of SL(2,Z) on R by linear transformations. Put\n\nand\n\nIt is not hard to check, using the above explicitly descriptions of \"H\" and \"H\" that for every nontrivial \"g\" ∈ \"H\" we have \"g\"(\"X\") ⊆ \"X\" and that for every nontrivial \"g\" ∈ \"H\" we have \"g\"(\"X\") ⊆ \"X\". Using the alternative form of the ping-pong lemma, for two subgroups, given above, we conclude that \"H\" = \"H\"∗\"H\". Since the groups \"H\" and \"H\" are infinite cyclic, it follows that \"H\" is a free group of rank two.\n\nLet \"G\" be a word-hyperbolic group which is torsion-free, that is, with no nontrivial elements of finite order. Let \"g\", \"h\" ∈ \"G\" be two non-commuting elements, that is such that \"gh\" ≠ \"hg\". Then there exists \"M\"≥1 such that for any integers \"n\" ≥ \"M\", \"m\" ≥ \"M\" the subgroup H = <\"g\", \"h\"> ≤ \"G\" is free of rank two.\n\nThe group \"G\" acts on its \"hyperbolic boundary\" ∂\"G\" by homeomorphisms. It is known that if \"a\" ∈ \"G\" is a nontrivial element then \"a\" has exactly two distinct fixed points, \"a\" and \"a\" in ∂\"G\" and that \"a\" is an attracting fixed point while \"a\" is a repelling fixed point.\n\nSince \"g\" and \"h\" do not commute, the basic facts about word-hyperbolic groups imply that \"g\", \"g\", \"h\" and \"h\" are four distinct points in ∂\"G\". Take disjoint neighborhoods \"U\", \"U\", \"V\" and \"V\" of \"g\", \"g\", \"h\" and \"h\" in ∂\"G\" respectively.\nThen the attracting/repelling properties of the fixed points of \"g\" and \"h\" imply that there exists \"M\" ≥ 1 such that for any integers \"n\" ≥ \"M\", \"m\" ≥ \"M\" we have:\n\nThe ping-pong lemma now implies that \"H\" = <\"g\", \"h\"> ≤ \"G\" is free of rank two.\n\n\n"}
{"id": "25175", "url": "https://en.wikipedia.org/wiki?curid=25175", "title": "Quadratic equation", "text": "Quadratic equation\n\nIn algebra, a quadratic equation (from the Latin for \"square\") is any equation having the form \nwhere represents an unknown, and , , and represent known numbers, with . If , then the equation is linear, not quadratic. The numbers , , and are the \"coefficients\" of the equation and may be distinguished by calling them, respectively, the \"quadratic coefficient\", the \"linear coefficient\" and the \"constant\" or \"free term\".\n\nBecause the quadratic equation involves only one unknown, it is called \"univariate\". The quadratic equation only contains powers of that are non-negative integers, and therefore it is a polynomial equation. In particular, it is a second-degree polynomial equation, since the greatest power is two.\n\nQuadratic equations can be solved by a process known in American English as factoring and in other varieties of English as \"factorising\", by completing the square, by using the quadratic formula, or by graphing. Solutions to problems equivalent to the quadratic equation were known as early as 2000 BC.\n\nA quadratic equation with real or complex coefficients has two solutions, called \"roots\". These two solutions may or may not be distinct, and they may or may not be real.\n\nIt may be possible to express a quadratic equation as a product . In some cases, it is possible, by simple inspection, to determine values of \"p\", \"q\", \"r,\" and \"s\" that make the two forms equivalent to one another. If the quadratic equation is written in the second form, then the \"Zero Factor Property\" states that the quadratic equation is satisfied if or . Solving these two linear equations provides the roots of the quadratic.\n\nFor most students, factoring by inspection is the first method of solving quadratic equations to which they are exposed. If one is given a quadratic equation in the form , the sought factorization has the form , and one has to find two numbers and that add up to and whose product is (this is sometimes called \"Vieta's rule\" and is related to Vieta's formulas). As an example, factors as . The more general case where does not equal can require a considerable effort in trial and error guess-and-check, assuming that it can be factored at all by inspection.\n\nExcept for special cases such as where or , factoring by inspection only works for quadratic equations that have rational roots. This means that the great majority of quadratic equations that arise in practical applications cannot be solved by factoring by inspection.\n\nThe process of completing the square makes use of the algebraic identity\nwhich represents a well-defined algorithm that can be used to solve any quadratic equation. Starting with a quadratic equation in standard form, \n\nWe illustrate use of this algorithm by solving \n\nThe plus-minus symbol \"±\" indicates that both and are solutions of the quadratic equation.\n\nCompleting the square can be used to derive a general formula for solving quadratic equations, called the quadratic formula. The mathematical proof will now be briefly summarized. It can easily be seen, by polynomial expansion, that the following equation is equivalent to the quadratic equation:\nTaking the square root of both sides, and isolating , gives:\n\nSome sources, particularly older ones, use alternative parameterizations of the quadratic equation such as or  , where has a magnitude one half of the more common one, possibly with opposite sign. These result in slightly different forms for the solution, but are otherwise equivalent.\n\nA number of alternative derivations can be found in the literature. These proofs are simpler than the standard completing the square method, represent interesting applications of other frequently used techniques in algebra, or offer insight into other areas of mathematics.\n\nA lesser known quadratic formula, as used in Muller's method, and which can be found from Vieta's formulas, provides the same roots via the equation:\n\nOne property of this form is that it yields one valid root when , while the other root contains division by zero, because when , the quadratic equation becomes a linear equation, which has one root. By contrast, in this case, the more common formula has a division by zero for one root and an indeterminate form for the other root. On the other hand, when , the more common formula yields two correct roots whereas this form yields the zero root and an indeterminate form .\n\nIt is sometimes convenient to reduce a quadratic equation so that its leading coefficient is one. This is done by dividing both sides by \"a\", which is always possible since \"a\" is non-zero. This produces the \"reduced quadratic equation\":\n\nwhere \"p\" = \"b\"/\"a\" and \"q\" = \"c\"/\"a\". This monic equation has the same solutions as the original.\n\nThe quadratic formula for the solutions of the reduced quadratic equation, written in terms of its coefficients, is:\nor equivalently:\n\nIn the quadratic formula, the expression underneath the square root sign is called the \"discriminant\" of the quadratic equation, and is often represented using an upper case or an upper case Greek delta:\nA quadratic equation with \"real\" coefficients can have either one or two distinct real roots, or two distinct complex roots. In this case the discriminant determines the number and nature of the roots. There are three cases:\n\n\n\n\nThus the roots are distinct if and only if the discriminant is non-zero, and the roots are real if and only if the discriminant is non-negative.\n\nThe function is the quadratic function. The graph of any quadratic function has the same general shape, which is called a parabola. The location and size of the parabola, and how it opens, depend on the values of , , and . As shown in Figure 1, if , the parabola has a minimum point and opens upward. If , the parabola has a maximum point and opens downward. The extreme point of the parabola, whether minimum or maximum, corresponds to its vertex. The \"-coordinate\" of the vertex will be located at formula_19, and the \"-coordinate\" of the vertex may be found by substituting this \"-value\" into the function. The \"-intercept\" is located at the point .\n\nThe solutions of the quadratic equation correspond to the roots of the function , since they are the values of for which . As shown in Figure 2, if , , and are real numbers and the domain of is the set of real numbers, then the roots of are exactly the -coordinates of the points where the graph touches the -axis. As shown in Figure 3, if the discriminant is positive, the graph touches the -axis at two points; if zero, the graph touches at one point; and if negative, the graph does not touch the -axis.\n\nThe term\nis a factor of the polynomial\nif and only if is a root of the quadratic equation\nIt follows from the quadratic formula that\nIn the special case where the quadratic has only one distinct root (\"i.e.\" the discriminant is zero), the quadratic polynomial can be factored as\n\nFor most of the 20th century, graphing was rarely mentioned as a method for solving quadratic equations in high school or college algebra texts. Students learned to solve quadratic equations by factoring, completing the square, and applying the quadratic formula. Recently, graphing calculators have become common in schools and graphical methods have started to appear in textbooks, but they are generally not highly emphasized.\n\nBeing able to use a graphing calculator to solve a quadratic equation requires the ability to produce a graph of , the ability to scale the graph appropriately to the dimensions of the graphing surface, and the recognition that when , is a solution to the equation. The skills required to solve a quadratic equation on a calculator are in fact applicable to finding the real roots of any arbitrary function.\n\nSince an arbitrary function may cross the -axis at multiple points, graphing calculators generally require one to identify the desired root by positioning a cursor at a \"guessed\" value for the root. (Some graphing calculators require bracketing the root on both sides of the zero.) The calculator then proceeds, by an iterative algorithm, to refine the estimated position of the root to the limit of calculator accuracy.\n\nAlthough the quadratic formula provides an exact solution, the result is not exact if real numbers are approximated during the computation, as usual in numerical analysis, where real numbers are approximated by floating point numbers (called \"reals\" in many programming languages). In this context, the quadratic formula is not completely stable.\n\nThis occurs when the roots have different order of magnitude, or, equivalently, when and are close in magnitude. In this case, the subtraction of two nearly equal numbers will cause loss of significance or catastrophic cancellation in the smaller root. To avoid this, the root that is smaller in magnitude, , can be computed as formula_25 where is the root that is bigger in magnitude.\n\nA second form of cancellation can occur between the terms and of the discriminant, that is when the two roots are very close. This can lead to loss of up to half of correct significant figures in the roots.\n\nThe golden ratio is found as the positive solution of the quadratic equation formula_26\n\nThe equations of the circle and the other conic sections—ellipses, parabolas, and hyperbolas—are quadratic equations in two variables.\n\nGiven the cosine or sine of an angle, finding the cosine or sine of the angle that is half as large involves solving a quadratic equation.\n\nThe process of simplifying expressions involving the square root of an expression involving the square root of another expression involves finding the two solutions of a quadratic equation.\n\nDescartes' theorem states that for every four kissing (mutually tangent) circles, their radii satisfy a particular quadratic equation.\n\nThe equation given by Fuss' theorem, giving the relation among the radius of a bicentric quadrilateral's inscribed circle, the radius of its circumscribed circle, and the distance between the centers of those circles, can be expressed as a quadratic equation for which the distance between the two circles' centers in terms of their radii is one of the solutions. The other solution of the same equation in terms of the relevant radii gives the distance between the circumscribed circle's center and the center of the excircle of an ex-tangential quadrilateral.\n\nBabylonian mathematicians, as early as 2000 BC (displayed on Old Babylonian clay tablets) could solve problems relating the areas and sides of rectangles. There is evidence dating this algorithm as far back as the Third Dynasty of Ur. In modern notation, the problems typically involved solving a pair of simultaneous equations of the form:\nwhich is equivalent to the statement that and are the roots of the equation:\n\nThe steps given by Babylonian scribes for solving the above rectangle problem, in terms of and , were as follows:\n\nGeometric methods were used to solve quadratic equations in Babylonia, Egypt, Greece, China, and India. The Egyptian Berlin Papyrus, dating back to the Middle Kingdom (2050 BC to 1650 BC), contains the solution to a two-term quadratic equation. Babylonian mathematicians from circa 400 BC and Chinese mathematicians from circa 200 BC used geometric methods of dissection to solve quadratic equations with positive roots. Rules for quadratic equations were given in \"The Nine Chapters on the Mathematical Art\", a Chinese treatise on mathematics. These early geometric methods do not appear to have had a general formula. Euclid, the Greek mathematician, produced a more abstract geometrical method around 300 BC. With a purely geometric approach Pythagoras and Euclid created a general procedure to find solutions of the quadratic equation. In his work \"Arithmetica\", the Greek mathematician Diophantus solved the quadratic equation, but giving only one root, even when both roots were positive.\n\nIn 628 AD, Brahmagupta, an Indian mathematician, gave the first explicit (although still not completely general) solution of the quadratic equation as follows: \"To the absolute number multiplied by four times the [coefficient of the] square, add the square of the [coefficient of the] middle term; the square root of the same, less the [coefficient of the] middle term, being divided by twice the [coefficient of the] square is the value.\" (\"Brahmasphutasiddhanta\", Colebrook translation, 1817, page 346) This is equivalent to:\nThe \"Bakhshali Manuscript\" written in India in the 7th century AD contained an algebraic formula for solving quadratic equations, as well as quadratic indeterminate equations (originally of type ). Muhammad ibn Musa al-Khwarizmi (Persia, 9th century), inspired by Brahmagupta, developed a set of formulas that worked for positive solutions. Al-Khwarizmi goes further in providing a full solution to the general quadratic equation, accepting one or two numerical answers for every quadratic equation, while providing geometric proofs in the process. He also described the method of completing the square and recognized that the discriminant must be positive, which was proven by his contemporary 'Abd al-Hamīd ibn Turk (Central Asia, 9th century) who gave geometric figures to prove that if the discriminant is negative, a quadratic equation has no solution. While al-Khwarizmi himself did not accept negative solutions, later Islamic mathematicians that succeeded him accepted negative solutions, as well as irrational numbers as solutions. Abū Kāmil Shujā ibn Aslam (Egypt, 10th century) in particular was the first to accept irrational numbers (often in the form of a square root, cube root or fourth root) as solutions to quadratic equations or as coefficients in an equation. The 9th century Indian mathematician Sridhara wrote down rules for solving quadratic equations.\n\nThe Jewish mathematician Abraham bar Hiyya Ha-Nasi (12th century, Spain) authored the first European book to include the full solution to the general quadratic equation. His solution was largely based on Al-Khwarizmi's work. The writing of the Chinese mathematician Yang Hui (1238–1298 AD) is the first known one in which quadratic equations with negative coefficients of 'x' appear, although he attributes this to the earlier Liu Yi. By 1545 Gerolamo Cardano compiled the works related to the quadratic equations. The quadratic formula covering all cases was first obtained by Simon Stevin in 1594. In 1637 René Descartes published \"La Géométrie\" containing the quadratic formula in the form we know today. The first appearance of the general solution in the modern mathematical literature appeared in an 1896 paper by Henry Heaton.\n\nVieta's formulas give a simple relation between the roots of a polynomial and its coefficients. In the case of the quadratic polynomial, they take the following form:\nand\nThese results follow immediately from the relation:\nwhich can be compared term by term with\nThe first formula above yields a convenient expression when graphing a quadratic function. Since the graph is symmetric with respect to a vertical line through the vertex, when there are two real roots the vertex's -coordinate is located at the average of the roots (or intercepts). Thus the -coordinate of the vertex is given by the expression\nThe -coordinate can be obtained by substituting the above result into the given quadratic equation, giving\n\nAs a practical matter, Vieta's formulas provide a useful method for finding the roots of a quadratic in the case where one root is much smaller than the other. If , then , and we have the estimate:\nThe second Vieta's formula then provides:\nThese formulas are much easier to evaluate than the quadratic formula under the condition of one large and one small root, because the quadratic formula evaluates the small root as the difference of two very nearly equal numbers (the case of large ), which causes round-off error in a numerical evaluation. Figure 5 shows the difference between (i) a direct evaluation using the quadratic formula (accurate when the roots are near each other in value) and (ii) an evaluation based upon the above approximation of Vieta's formulas (accurate when the roots are widely spaced). As the linear coefficient increases, initially the quadratic formula is accurate, and the approximate formula improves in accuracy, leading to a smaller difference between the methods as increases. However, at some point the quadratic formula begins to lose accuracy because of round off error, while the approximate method continues to improve. Consequently, the difference between the methods begins to increase as the quadratic formula becomes worse and worse.\n\nThis situation arises commonly in amplifier design, where widely separated roots are desired to ensure a stable operation (see step response).\n\nIn the days before calculators, people would use mathematical tables—lists of numbers showing the results of calculation with varying arguments—to simplify and speed up computation. Tables of logarithms and trigonometric functions were common in math and science textbooks. Specialized tables were published for applications such as astronomy, celestial navigation and statistics. Methods of numerical approximation existed, called prosthaphaeresis, that offered shortcuts around time-consuming operations such as multiplication and taking powers and roots. Astronomers, especially, were concerned with methods that could speed up the long series of computations involved in celestial mechanics calculations.\n\nIt is within this context that we may understand the development of means of solving quadratic equations by the aid of trigonometric substitution. Consider the following alternate form of the quadratic equation,\n\n[1]   formula_39\n\nwhere the sign of the ± symbol is chosen so that and may both be positive. By substituting\n\n[2]   formula_40\n\nand then multiplying through by , we obtain\n\n[3]   formula_41\n\nIntroducing functions of and rearranging, we obtain\n\n[4]   formula_42\n\n[5]   formula_43\n\nwhere the subscripts and correspond, respectively, to the use of a negative or positive sign in equation [1]. Substituting the two values of or found from equations [4] or [5] into [2] gives the required roots of [1]. Complex roots occur in the solution based on equation [5] if the absolute value of exceeds unity. The amount of effort involved in solving quadratic equations using this mixed trigonometric and logarithmic table look-up strategy was two-thirds the effort using logarithmic tables alone. Calculating complex roots would require using a different trigonometric form.\n\n\nIf the quadratic equation formula_52 with real coefficients has two complex roots—the case where formula_53 requiring \"a\" and \"c\" to have the same sign as each other—then the solutions for the roots can be expressed in polar form as\n\nwhere formula_55 and formula_56\n\nThe quadratic equation may be solved geometrically in a number of ways. One way is via Lill's method. The three coefficients , , are drawn with right angles between them as in SA, AB, and BC in Figure 6. A circle is drawn with the start and end point SC as a diameter. If this cuts the middle line AB of the three then the equation has a solution, and the solutions are given by negative of the distance along this line from A divided by the first coefficient or SA. If is the coefficients may be read off directly. Thus the solutions in the diagram are −AX1/SA and −AX2/SA.\nThe Carlyle circle, named after Thomas Carlyle, has the property that the solutions of the quadratic equation are the horizontal coordinates of the intersections of the circle with the horizontal axis. Carlyle circles have been used to develop ruler-and-compass constructions of regular polygons.\n\nThe general quadratic equation can be written in the standard form\nwhere and are complex numbers. Then the solutions can be written in the particularly symmetric form\nor equivalently\nThe correctness of the formula can easily be verified by inserting the expression into the equation. This formula has the advantage that it is numerically more stable than the classical quadratic formula. It appeared in. Problems originating from physics or geometry often present themselves in the homogeneous standard form on which the alternative formula is based. In particular, René Descartes' first example in \"La Géométrie\", at the very hour of birth of the quadratic formula, was geometric and of this particular homogeneous form.\n\nThe formula and its derivation remain correct if the coefficients , and are complex numbers, or more generally members of any field whose characteristic is not . (In a field of characteristic 2, the element is zero and it is impossible to divide by it.)\n\nThe symbol\nin the formula should be understood as \"either of the two elements whose square is , if such elements exist\". In some fields, some elements have no square roots and some have two; only zero has just one square root, except in fields of characteristic . Even if a field does not contain a square root of some number, there is always a quadratic extension field which does, so the quadratic formula will always make sense as a formula in that extension field.\n\nIn a field of characteristic , the quadratic formula, which relies on being a unit, does not hold. Consider the monic quadratic polynomial\nover a field of characteristic . If , then the solution reduces to extracting a square root, so the solution is\nand there is only one root since\nIn summary,\nSee quadratic residue for more information about extracting square roots in finite fields.\n\nIn the case that , there are two distinct roots, but if the polynomial is irreducible, they cannot be expressed in terms of square roots of numbers in the coefficient field. Instead, define the 2-root of to be a root of the polynomial , an element of the splitting field of that polynomial. One verifies that is also a root. In terms of the 2-root operation, the two roots of the (non-monic) quadratic are\nand\n\nFor example, let denote a multiplicative generator of the group of units of , the Galois field of order four (thus and are roots of over . Because , is the unique solution of the quadratic equation . On the other hand, the polynomial is irreducible over , but it splits over , where it has the two roots and , where is a root of in .\n\nThis is a special case of Artin–Schreier theory.\n\n"}
{"id": "25308", "url": "https://en.wikipedia.org/wiki?curid=25308", "title": "Quasispecies model", "text": "Quasispecies model\n\nThe quasispecies model is a description of the process of the Darwinian evolution of certain self-replicating entities within the framework of physical chemistry. A quasispecies is a large group or \"cloud\" of related genotypes that exist in an environment of high mutation rate (at stationary state), where a large fraction of offspring are expected to contain one or more mutations relative to the parent. This is in contrast to a species, which from an evolutionary perspective is a more-or-less stable single genotype, most of the offspring of which will be genetically accurate copies.\n\nIt is useful mainly in providing a qualitative understanding of the evolutionary processes of self-replicating macromolecules such as RNA or DNA or simple asexual organisms such as bacteria or viruses (see also viral quasispecies), and is helpful in explaining something of the early stages of the origin of life. Quantitative predictions based on this model are difficult because the parameters that serve as its input are impossible to obtain from actual biological systems. The quasispecies model was put forward by Manfred Eigen and Peter Schuster based on initial work done by Eigen.\n\nWhen evolutionary biologists describe competition between species, they generally assume that each species is a single genotype whose descendants are mostly accurate copies. (Such genotypes are said to have a high reproductive \"fidelity\".) Evolutionarily, we are interested in the behavior and fitness of that one species or genotype over time.\n\nSome organisms or genotypes, however, may exist in circumstances of low fidelity, where most descendants contain one or more mutations. A group of such genotypes is constantly changing, so discussions of which single genotype is the most fit become meaningless. Importantly, if many closely related genotypes are only one mutation away from each other, then genotypes in the group can mutate back and forth into each other. For example, with one mutation per generation, a child of the sequence AGGT could be AGTT, and a grandchild could be AGGT again. Thus we can envision a \"cloud\" of related genotypes that is rapidly mutating, with sequences going back and forth among different points in the cloud. Though the proper definition is mathematical, that cloud, roughly speaking, is a quasispecies.\n\nQuasispecies behavior exists for large numbers of individuals existing at a certain (high) range of mutation rates.\n\nIn a species, though reproduction may be mostly accurate, periodic mutations will give rise to one or more competing genotypes. If a mutation results in greater replication and survival, the mutant genotype may out-compete the parent genotype and come to dominate the species. Thus, the individual genotypes (or species) may be seen as the units on which selection acts and biologists will often speak of a single genotype's fitness.\n\nIn a quasispecies, however, mutations are ubiquitous and so the fitness of an individual genotype becomes meaningless: if one particular mutation generates a boost in reproductive success, it can't amount to much because that genotype's offspring are unlikely to be accurate copies with the same properties. Instead, what matters is the \"connectedness\" of the cloud. For example, the sequence AGGT has 12 (3+3+3+3) possible single point mutants AGGA, AGGG, and so on. If 10 of those mutants are viable genotypes that may reproduce (and some of whose offspring or grandchildren may mutate back into AGGT again), we would consider that sequence a well-connected node in the cloud. If instead only two of those mutants are viable, the rest being lethal mutations, then that sequence is poorly connected and most of its descendants will not reproduce. The analog of fitness for a quasispecies is the tendency of nearby relatives within the cloud to be well-connected, meaning that more of the mutant descendants will be viable and give rise to further descendants within the cloud.\n\nWhen the fitness of a single genotype becomes meaningless because of the high rate of mutations, the cloud as a whole or quasispecies becomes the natural unit of selection.\n\nQuasispecies represents the evolution of high-mutation-rate viruses such as HIV and sometimes single genes or molecules within the genomes of other organisms. Quasispecies models have also been proposed by Jose Fontanari and Emmanuel David Tannenbaum to model the evolution of sexual reproduction. Quasispecies was also shown in compositional replicators (based on the Gard model for abiogenesis) and was also suggested to be applicable to describe cell's replication, which amongst other things requires the maintenance and evolution of the internal composition of the parent and bud.\n\nThe model rests on four assumptions:\n\nIn the quasispecies model, mutations occur through errors made in the process of copying already existing sequences. Further, selection arises because different types of sequences tend to replicate at different rates, which leads to the suppression of sequences that replicate more slowly in favor of sequences that replicate faster. However, the quasispecies model does not predict the ultimate extinction of all but the fastest replicating sequence. Although the sequences that replicate more slowly cannot sustain their abundance level by themselves, they are constantly replenished as sequences that replicate faster mutate into them. At equilibrium, removal of slowly replicating sequences due to decay or outflow is balanced by replenishing, so that even relatively slowly replicating sequences can remain present in finite abundance.\n\nDue to the ongoing production of mutant sequences, selection does not act on single sequences, but on mutational \"clouds\" of closely related sequences, referred to as \"quasispecies\". In other words, the evolutionary success of a particular sequence depends not only on its own replication rate, but also on the replication rates of the mutant sequences it produces, and on the replication rates of the sequences of which it is a mutant. As a consequence, the sequence that replicates fastest may even disappear completely in selection-mutation equilibrium, in favor of more slowly replicating sequences that are part of a quasispecies with a higher average growth rate. Mutational clouds as predicted by the quasispecies model have been observed in RNA viruses and in \"in vitro\" RNA replication.\n\nThe mutation rate and the general fitness of the molecular sequences and their neighbors is crucial to the formation of a quasispecies. If the mutation rate is zero, there is no exchange by mutation, and each sequence is its own species. If the mutation rate is too high, exceeding what is known as the error threshold, the quasispecies will break down and be dispersed over the entire range of available sequences.\n\nA simple mathematical model for a quasispecies is as follows: let there be formula_1 possible sequences and let there be formula_2 organisms with sequence \"i\". Let's say that each of these organisms asexually gives rise to formula_3 offspring. Some are duplicates of their parent, having sequence \"i\", but some are mutant and have some other sequence. Let the mutation rate formula_4 correspond to the probability that a \"j\" type parent will produce an \"i\" type organism. Then the expected fraction of offspring generated by \"j\" type organisms that would be \"i\" type organisms is formula_5,\n\nwhere formula_6.\n\nThen the total number of \"i\"-type organisms after the first round of reproduction, given as formula_7, is\n\nSometimes a death rate term formula_9 is included so that:\n\nwhere formula_11 is equal to 1 when i=j and is zero otherwise. Note that the \"n-th\" generation can be found by just taking the \"n-th\" power of W substituting it in place of W in the above formula.\n\nThis is just a system of linear equations. The usual way to solve such a system is to first diagonalize the W matrix. Its diagonal entries will be eigenvalues corresponding to certain linear combinations of certain subsets of sequences which will be eigenvectors of the W matrix. These subsets of sequences are the quasispecies. Assuming that the matrix W is a primitive matrix (irreducible and aperiodic), then after very many generations only the eigenvector with the largest eigenvalue will prevail, and it is this quasispecies that will eventually dominate. The components of this eigenvector give the relative abundance of each sequence at equilibrium.\n\nW being primitive means that for some integer formula_12, that the formula_13 power of W is > 0, i.e. all the entries are positive. If W is primitive then each type can, through a sequence of mutations (i.e. powers of W) mutate into all the other types after some number of generations. W is not primitive if it is periodic, where the population can perpetually cycle through different disjoint sets of compositions, or if it is reducible, where the dominant species (or quasispecies) that develops can depend on the initial population, as is the case in the simple example given below.\n\nThe quasispecies formulae may be expressed as a set of linear differential equations. If we consider the difference between the new state formula_7 and the old state formula_2 to be the state change over one moment of time, then we can state that the time derivative of formula_2 is given by this difference, formula_17 we can write:\n\nThe quasispecies equations are usually expressed in terms of concentrations formula_19 where\n\nThe above equations for the quasispecies then become for the discrete version:\n\nor, for the continuum version:\n\nThe quasispecies concept can be illustrated by a simple system consisting of 4 sequences. Sequences [0,0], [0,1], [1,0], and [1,1] are numbered 1, 2, 3, and 4, respectively. Let's say the [0,0] sequence never mutates and always produces a single offspring. Let's say the other 3 sequences all produce, on average, formula_24 replicas of themselves, and formula_25 of each of the other two types, where formula_26. The W matrix is then:\n\nThe diagonalized matrix is:\n\nAnd the eigenvectors corresponding to these eigenvalues are:\n\nOnly the eigenvalue formula_29 is more than unity. For the n-th generation, the corresponding eigenvalue will be formula_30 and so will increase without bound as time goes by. This eigenvalue corresponds to the eigenvector [0,1,1,1], which represents the quasispecies consisting of sequences 2, 3, and 4, which will be present in equal numbers after a very long time. Since all population numbers must be positive, the first two quasispecies are not legitimate. The third quasispecies consists of only the non-mutating sequence 1. It's seen that even though sequence 1 is the most fit in the sense that it reproduces more of itself than any other sequence, the quasispecies consisting of the other three sequences will eventually dominate (assuming that the initial population was not homogeneous of the sequence 1 type).\n\n"}
{"id": "5173979", "url": "https://en.wikipedia.org/wiki?curid=5173979", "title": "Recursive ordinal", "text": "Recursive ordinal\n\nIn mathematics, specifically set theory, an ordinal formula_1 is said to be recursive if there is a recursive well-ordering of a subset of the natural numbers having the order type formula_1.\n\nIt is trivial to check that formula_3 is recursive, the successor of a recursive ordinal is recursive, and the set of all recursive ordinals is closed downwards. The supremum of all recursive ordinals is called the Church–Kleene ordinal and denoted by formula_4. Indeed, an ordinal is recursive if and only if it is smaller than formula_4. Since there are only countably many recursive relations, there are also only countably many recursive ordinals. Thus, formula_4 is countable.\n\nThe recursive ordinals are exactly the ordinals that have an ordinal notation in Kleene's formula_7. \n\n\n"}
{"id": "5238435", "url": "https://en.wikipedia.org/wiki?curid=5238435", "title": "Regular part", "text": "Regular part\n\nIn mathematics, the regular part of a Laurent series consists of the series of terms with positive powers. That is, if\nthen the regular part of this Laurent series is\n\nIn contrast, the series of terms with negative powers is the principal part.\n"}
{"id": "5834541", "url": "https://en.wikipedia.org/wiki?curid=5834541", "title": "Schröder's equation", "text": "Schröder's equation\n\nSchröder's equation, named after Ernst Schröder, is a functional equation with one independent variable: given the function , find the function such that:\n\nSchröder's equation is an eigenvalue equation for the composition operator , which sends a function to .\n\nIf is a fixed point of , meaning , then either (or ) or . Thus, provided\n\nFor , if is analytic on the unit disk, fixes , and , then Gabriel Koenigs showed in 1884 that there is an analytic (non-trivial) satisfying Schröder's equation. This is one of the first steps in a long line of theorems fruitful for understanding composition operators on analytic function spaces, cf. Koenigs function.\n\nEquations such as Schröder's are suitable to encoding self-similarity, and have thus been extensively utilized in studies of nonlinear dynamics (often referred to colloquially as chaos theory). It is also used in studies of turbulence, as well as the renormalization group.\n\nAn equivalent transpose form of Schröder's equation for the inverse of Schröder's conjugacy function is . The change of variables (the Abel function) further converts Schröder's equation to the older Abel equation, . Similarly, the change of variables converts Schröder's equation to Böttcher's equation, . \n\nMoreover, for the velocity, ,   \"Julia's equation\",   , holds. \n\nThe -th power of a solution of Schröder's equation provides a solution of Schröder's equation with eigenvalue , instead. In the same vein, for an invertible solution of Schröder's equation, the (non-invertible) function is also a solution, for \"any\" periodic function with period . All solutions of Schröder's equation are related in this manner.\n\nSchröder's equation was solved analytically if is an attracting (but not superattracting)\nfixed point, that is by Gabriel Koenigs (1884).\n\nIn the case of a superattracting fixed point, , Schröder's equation is unwieldy, and had best be transformed to Böttcher's equation.\n\nThere are a good number of particular solutions dating back to Schröder's original 1870 paper.\n\nThe series expansion around a fixed point and the relevant convergence properties of the solution for the resulting orbit and its analyticity properties are cogently summarized by Szekeres. Several of the solutions are furnished in terms of asymptotic series, cf. Carleman matrix.\n\nIt is used to analyse discrete dynamical systems by finding a new coordinate system in which the system (orbit) generated by \"h\"(\"x\") looks simpler, a mere dilation.\n\nMore specifically, a system for which a discrete unit time step amounts to , can have its smooth orbit (or flow) reconstructed from the solution of the above Schröder's equation, its conjugacy\n\nThat is,   .\n\nIn general, all of its functional iterates (its \"regular iteration group\", cf. iterated function) are provided by the orbit\n\nfor real — not necessarily positive or integer. (Thus a full continuous group.)\nThe set of , i.e., of all positive integer iterates of (semigroup) is called the \"splinter\" (or Picard sequence) of .\n\nHowever, all iterates (fractional, infinitesimal, or negative) of are likewise specified through the coordinate transformation determined to solve Schröder's equation: a holographic continuous interpolation of the initial discrete recursion has been constructed; in effect, the entire orbit.\n\nFor instance, the functional square root is , so that , and so on.\n\nFor example, special cases of the logistic map such as the chaotic case were already worked out by Schröder in his original paper (cf. p. 306), \n\nIn fact, this solution is seen to result as motion dictated by a sequence of switchback potentials, , a generic feature of continuous iterates effected by Schröder's equation.\n\nA nonchaotic case he also illustrated with his method, , yields \nLikewise, for the Beverton–Holt model, , one readily finds , so that\n\n"}
{"id": "25451646", "url": "https://en.wikipedia.org/wiki?curid=25451646", "title": "Septic equation", "text": "Septic equation\n\nIn algebra, a septic equation is an equation of the form\n\nwhere .\n\nA septic function is a function of the form\n\nwhere . In other words, it is a polynomial of degree seven. If , then \"f\" is a sextic function (), quintic function (), etc.\n\nThe equation may be obtained from the function by setting .\n\nThe \"coefficients\" may be either integers, rational numbers, real numbers, complex numbers or, more generally, members of any field.\n\nBecause they have an odd degree, septic functions appear similar to quintic or cubic function when graphed, except they may possess additional local maxima and local minima (up to three maxima and three minima). The derivative of a septic function is a sextic function.\n\nSome seventh degree equations can be solved by factorizing into radicals, but other septics cannot. Évariste Galois developed techniques for determining whether a given equation could be solved by radicals which gave rise to the field of Galois theory. To give an example of an irreducible but solvable septic, one can generalize the solvable de Moivre quintic to get,\n\nwhere the auxiliary equation is \n\nThis means that the septic is obtained by eliminating and between , and .\n\nIt follows that the septic's seven roots are given by\n\nwhere is any of the 7 seventh roots of unity. The Galois group of this septic is the maximal solvable group of order 42. This is easily generalized to any other degrees , not necessarily prime.\n\nAnother solvable family is,\n\nwhose members appear in Kluner's \"Database of Number Fields\". Its discriminant is\n\nThe Galois group of these septics is the dihedral group of order 14.\n\nThe general septic equation can be solved with the alternating or symmetric Galois groups or . Such equations require hyperelliptic functions and associated theta functions of genus 3 for their solution. However, these equations were not studied specifically by the nineteenth-century mathematicians studying the solutions of algebraic equations, because the sextic equations' solutions were already at the limits of their computational abilities without computers.\n\nSeptics are the lowest order equations for which it is not obvious that their solutions may be obtained by superimposing \"continuous functions\" of two variables. Hilbert's 13th problem was the conjecture this was not possible in the general case for seventh-degree equations. Vladimir Arnold solved this in 1957, demonstrating that this was always possible. However, Arnold himself considered the \"genuine\" Hilbert problem to be whether for septics their solutions may be obtained by superimposing \"algebraic functions\" of two variables (the problem still being open).\n\n\nThe square of the area of a cyclic pentagon is a root of a septic equation whose coefficients are symmetric functions of the sides of the pentagon. The same is true of the square of the area of a cyclic hexagon.\n\n"}
{"id": "50448848", "url": "https://en.wikipedia.org/wiki?curid=50448848", "title": "Short integer solution problem", "text": "Short integer solution problem\n\nShort integer solution (SIS) and ring-SIS problems are two \"average\"-case problems that are used in lattice-based cryptography constructions. Lattice-based cryptography began in 1996 from a seminal work by Ajtai who presented a family of one-way functions based on SIS problem. He showed that it is secure in an average case if formula_1 (where formula_2 for some constant formula_3) is hard in a worst-case scenario.\n\nAverage case problems are the problems that are hard to be solved for some randomly selected instances. For cryptography applications, worse case complexity is not sufficient, and we need to guarantee cryptographic construction are hard based on average case complexity.\n\nA \"full rank lattice\" formula_4 is a set of integer linear combinations of formula_5 linearly independent vectors formula_6, named \"basis\":\n\nwhere formula_8 is a matrix having basis vectors in its columns.\n\nRemark: Given formula_9 two bases for lattice formula_10, there exist unimodular matrices formula_11 such that formula_12.\n\nDefinition: Rotational shift operator on formula_13 is denoted by formula_14, and is defined as:\n\nMicciancio introduced \"cyclic lattices\" in his work in generalizing the compact knapsack problem to arbitrary rings. A cyclic lattice is a lattice that is closed under rotational shift operator. Formally, cyclic lattices are defined as follows:\n\nDefinition: A lattice formula_16 is cyclic if formula_17.\n\nExamples: \nconsider the quotient polynomial ring formula_19, and let formula_21 be some polynomial in formula_22, i.e. formula_23 where formula_24 for formula_25.\n\nDefine the embedding coefficient formula_26-module isomorphism formula_27 as:\n\nLet formula_29 be an ideal. The lattice corresponding to ideal formula_29, denoted by formula_31, is a sublattice of formula_18, and is defined as\n\nTheorem: formula_34 is cyclic if and only if formula_10 corresponds to some ideal formula_36 in the quotient polynomial ring formula_19.\n\nproof:\nformula_38 We have:\n\nLet formula_40 be an arbitrary element in formula_10. Then, define formula_42. But since formula_36 is an ideal, we have formula_44. Then, formula_45. But, formula_46. Hence, formula_10 is cyclic.\n\nformula_48\n\nLet formula_34 be a cyclic lattice. Hence formula_50.\n\nDefine the set of polynomials formula_51:\n\n\nHence, formula_29 is an ideal, and consequently, formula_59.\n\nLet formula_60 be a monic polynomial of degree formula_5. For cryptographic applications, formula_62 is usually selected to be irreducible. The ideal generated by formula_62 is:\n\nThe quotient polynomial ring formula_65 partitions formula_66 into equivalence classes of polynomials of degree at most formula_67:\n\nwhere addition and multiplication are reduced modulo formula_69.\n\nConsider the embedding coefficient formula_70-module isomorphism formula_71. Then, each ideal in formula_72 defines a sublattice of formula_73 called ideal lattice.\n\nDefinition: formula_74, the lattice corresponding to an ideal formula_75, is called ideal lattice. More precisely, consider a quotient polynomial ring formula_76, where formula_77 is the ideal generated by a degree formula_78 polynomial formula_79. formula_74, is a sublattice of formula_73, and is defined as:\n\nRemark:\n\nSIS and Ring-SIS are two \"average\" case problems that are used in lattice-based cryptography constructions. Lattice-based cryptography began in 1996 from a seminal work by Ajtai who presented a family of one-way functions based on SIS problem. He showed that it is secure in an average case if formula_90 (where formula_2 for some constant formula_3) is hard in a worst-case scenario.\n\nLet formula_93 be an formula_94 matrix with entries in formula_95 that consists of formula_96 uniformly random vectors formula_97: formula_98. Find a nonzero vector formula_99 such that:\n\nIt should be noted that a solution to SIS without the required constraint on the length of the solution (formula_102) is easy to compute by using \"Gaussian elimination\" technique. We also require formula_103, otherwise formula_104 is a trivial solution.\nIn order to guarantee formula_105 has non-trivial, short solution, we require:\n\nTheorem: For any formula_108, any formula_109, and any sufficiently large formula_110 (for any constant formula_111), solving formula_112 with nonnegligible probability is at least as hard as solving the formula_113 and formula_114 for some formula_115 with a high probability in the worst-case scenario.\n\nRing-SIS problem, a compact ring-based analogue of SIS problem, was studied in. \nThey consider quotient polynomial ring formula_116 with formula_117 and formula_118, respectively, and extend the definition of \"norm\" on vectors in formula_119 to vectors in formula_120 as follows:\n\nGiven a vector formula_121 where formula_122 are some polynomial in formula_72. Consider the embedding coefficient formula_70-module isomorphism formula_71:\n\nLet formula_127. Define norm formula_128 as:\n\nAlternatively, a better notion for norm is achieved by exploiting the \"canonical embedding\". The canonical embedding is defined as:\n\nwhere formula_131 is the formula_132 complex root of formula_69 for formula_134.\n\nGiven the quotient polynomial ring formula_116, define\n\nformula_136. Select formula_96 independent uniformly random elements formula_138. Define vector formula_139. Find a nonzero vector formula_140 such that:\n\nRecall that to guarantee existence of a solution to SIS problem, we require formula_143. However, Ring-SIS problem provide us with more compactness and efficacy: to guarantee existence of a solution to Ring-SIS problem, we require formula_144.\n\nDefinition: The \"nega-circulant matrix\" of formula_145 is defined as:\n\nWhen the quotient polynomial ring is formula_147 for formula_148, the ring multiplication formula_149 can be efficiently computed by first forming formula_150, the nega-circulant matrix of formula_151, and then multiplying formula_150 with formula_153, the embedding coefficient vector of formula_154 (or, alternatively with formula_155, the canonical coefficient vector).\nMoreover, R-SIS problem is a special case of SIS problem where the matrix formula_156 in the SIS problem is restricted to negacirculant blocks: formula_157.\n\n"}
{"id": "21769209", "url": "https://en.wikipedia.org/wiki?curid=21769209", "title": "Sievert integral", "text": "Sievert integral\n\nThe Sievert integral, named after Swedish medical physicist Rolf Sievert, is a special function commonly encountered in radiation transport calculations. \n\nIt plays a role in the sievert (symbol: Sv) unit of ionizing radiation dose in the International System of Units (SI).\n"}
{"id": "51724747", "url": "https://en.wikipedia.org/wiki?curid=51724747", "title": "Single pushout graph rewriting", "text": "Single pushout graph rewriting\n\nIn computer science, single pushout graph rewriting or SPO graph rewriting refers to a mathematical framework for graph rewriting, and is used in contrast to the double-pushout approach of graph rewriting.\n"}
{"id": "46788343", "url": "https://en.wikipedia.org/wiki?curid=46788343", "title": "Sorin Popa", "text": "Sorin Popa\n\nSorin Teodor Popa (24 March 1953) is a Romanian American mathematician working on operator algebras. He is a professor at the University of California, Los Angeles. \n\nPopa earned his Ph.D. from the University of Bucharest in 1983 under the supervision of Dan-Virgil Voiculescu. He has advised 15 doctoral students at UCLA, including Adrian Ioana.\n\nIn 1990 he was an invited speaker at the International Congress of Mathematicians (ICM) in Kyoto (on \"Subfactors and Classifications in von Neumann algebras\"). He was a Guggenheim Fellow in 1995. In 2006 he gave a plenary lecture at the ICM in Madrid (on \"Deformation and Rigidity for group actions and Von Neumann Algebras\"). In 2009 he was awarded the Ostrowski Prize. He is one of the inaugural fellows of the American Mathematical Society.\n\n\n"}
{"id": "9377661", "url": "https://en.wikipedia.org/wiki?curid=9377661", "title": "Support function", "text": "Support function\n\nIn mathematics, the support function \"h\" of a non-empty closed convex set \"A\" in formula_1\ndescribes the (signed) distances of supporting hyperplanes of \"A\" from the origin. The support function is a convex function on formula_1.\nAny non-empty closed convex set \"A\" is uniquely determined by \"h\". Furthermore, the support function, as a function of the set \"A\" is compatible with many natural geometric operations, like scaling, translation, rotation and Minkowski addition. \nDue to these properties, the support function is one of the most central basic concepts in convex geometry.\n\nThe support function formula_3 \nof a non-empty closed convex set \"A\" in formula_1 is given by \nformula_6; see\n. Its interpretation is most intuitive when \"x\" is a unit vector: \nby definition, \"A\" is contained in the closed half space \nand there is at least one point of \"A\" in the boundary\nof this half space. The hyperplane \"H\"(\"x\") is therefore called a \"supporting hyperplane\" \nwith \"exterior\" (or \"outer\") unit normal vector \"x\".\nThe word \"exterior\" is important here, as \nthe orientation of \"x\" plays a role, the set \"H\"(\"x\") is in general different from \"H\"(-\"x\").\nNow \"h\" is the (signed) distance of \"H\"(\"x\") from the origin.\n\nThe support function of a singleton \"A\"={\"a\"} is formula_9.\n\nThe support function of the Euclidean unit ball \"B\" is formula_10.\n\nIf \"A\" is a line segment through the origin with endpoints -\"a\" and \"a\" then formula_11.\n\nThe support function of a \"compact\" convex set is real valued and continuous, but if the \nset is unbounded, its support function is extended real valued (it takes the value \nformula_12). As any nonempty closed convex set is the intersection of\nits supporting half spaces, the function \"h\" determines \"A\" uniquely. \nThis can be used to describe certain geometric properties of convex sets analytically. \nFor instance, a set \"A\" is point symmetric with respect to the origin if and only \"h\"\nis an even function.\n\nIn general, the support function is not differentiable. However, directional derivatives\nexist and yield support functions of support sets. If \"A\" is \"compact\" and convex, \nand \"h\"'(\"u\";\"x\") denotes the directional derivative of\n\"h\" at \"u\" ≠ \"0\" in direction \"x\",\nwe have \nHere \"H\"(\"u\") is the supporting hyperplane of \"A\" with exterior normal vector \"u\", defined\nabove. If \"A\" ∩ \"H\"(\"u\") is a singleton {\"y\"}, say, it follows that the support function is differentiable at \n\"u\" and its gradient coincides with \"y\". Conversely, if \"h\" is differentiable at \"u\", then \"A\" ∩ \"H\"(\"u\") is a singleton. Hence \"h\" is differentiable at all points \"u\" ≠ \"0\" \nif and only if \"A\" is \"strictly convex\" (the boundary of \"A\" does not contain any line segments).\n\nIt follows directly from its definition that the support function is positive homogeneous:\nand subadditive:\nIt follows that \"h\" is a convex function. \nIt is crucial in convex geometry that these properties characterize support functions:\nAny positive homogeneous, convex, real valued function on formula_1 is the \nsupport function of a nonempty compact convex set. Several proofs are known\none is using the fact that the Legendre transform of a positive homogeneous, convex, real valued function \nis the (convex) indicator function of a compact convex set.\n\nMany authors restrict the support function to the Euclidean unit sphere \nand consider it as a function on \"S\". \nThe homogeneity property shows that this restriction determines the \nsupport function on formula_1, as defined above.\n\nThe support functions of a dilated or translated set are closely related to the original set \"A\":\nand \nThe latter generalises to \nwhere \"A\" + \"B\" denotes the Minkowski sum:\nThe Hausdorff distance \nof two nonempty compact convex sets \"A\" and \"B\" can be expressed in terms of support functions, \nwhere, on the right hand side, the uniform norm on the unit sphere is used.\n\nThe properties of the support function as a function of the set \"A\" are sometimes summarized in saying\nthat formula_23:\"A\" formula_24 \"h\" maps the family of non-empty\ncompact convex sets to the cone of all real-valued continuous functions on the sphere whose positive \nhomogeneous extension is convex. Abusing terminology slightly, formula_23 \nis sometimes called \"linear\", as it respects Minkowski addition, although it is not \ndefined on a linear space, but rather on an (abstract) convex cone of nonempty compact convex sets. \nThe mapping formula_23 is an isometry between this cone, endowed with the Hausdorff metric, and \na subcone of the family of continuous functions on \"S\" with the uniform norm.\n\nIn contrast to the above, support functions are sometimes defined on the boundary of \"A\" rather than on \n\"S\", under the assumption that there exists a unique exterior unit normal at each boundary point. \nConvexity is not needed for the definition.\nFor an oriented regular surface, \"M\", with a unit normal vector, \"N\", defined everywhere on its surface, the support function \nis then defined by\nIn other words, for any formula_28, this support function gives the \nsigned distance of the unique hyperplane that touches \"M\" in \"x\".\n\n"}
{"id": "35889821", "url": "https://en.wikipedia.org/wiki?curid=35889821", "title": "Transcendental law of homogeneity", "text": "Transcendental law of homogeneity\n\nIn mathematics, the transcendental law of homogeneity (TLH) is a heuristic principle enunciated by Gottfried Wilhelm Leibniz most clearly in a 1710 text entitled \"Symbolismus memorabilis calculi algebraici et infinitesimalis in comparatione potentiarum et differentiarum, et de lege homogeneorum transcendentali\". Henk J. M. Bos describes it as the principle to the effect that in a sum involving infinitesimals of different orders, only the lowest-order term must be retained, and the remainder discarded. Thus, if formula_1 is finite and formula_2 is infinitesimal, then one sets\n\nSimilarly,\n\nwhere the higher-order term \"du\" \"dv\" is discarded in accordance with the TLH. A recent study argues that Leibniz's TLH was a precursor of the standard part function over the hyperreals.\n\n"}
{"id": "27126714", "url": "https://en.wikipedia.org/wiki?curid=27126714", "title": "UMLsec", "text": "UMLsec\n\nUMLsec is an extension to the Unified Modelling Language for integrating security related information in UML specifications. This information can be used for model based security engineering. Most security information is added using stereotypes and cover many security properties including secure information flow, confidentiality and access control. Using an attacker model these properties can be checked on a model level.\n\nIt was first proposed by Jürjens et al. in 2002 and later revised and extended by the same author.\n\nUMLsec is defined as lightweight extension for UML. \n\nThe profile is defined through a set of prototypes with properties (tag definitions) and constraints. UMLsec defines 21 stereotypes listed below.\n\nTo ensure security it is necessary to specify what kind of attacker is assumed. In UMLsec, the attacker model is defined through the threats that it poses. The table below defines the \"default\" adversary. Other adversaries may of course be defined.\n"}
{"id": "54432", "url": "https://en.wikipedia.org/wiki?curid=54432", "title": "Unification (computer science)", "text": "Unification (computer science)\n\nIn logic and computer science, unification is an algorithmic process of solving equations between symbolic expressions.\n\nDepending on which expressions (also called \"terms\") are allowed to occur in an equation set (also called \"unification problem\"), and which expressions are considered equal, several frameworks of unification are distinguished. If higher-order variables, that is, variables representing functions, are allowed in an expression, the process is called higher-order unification, otherwise first-order unification. If a solution is required to make both sides of each equation literally equal, the process is called syntactic or free unification, otherwise semantic or equational unification, or E-unification, or unification modulo theory.\n\nA \"solution\" of a unification problem is denoted as a substitution, that is, a mapping assigning a symbolic value to each variable of the problem's expressions. A unification algorithm should compute for a given problem a \"complete\", and \"minimal\" substitution set, that is, a set covering all its solutions, and containing no redundant members. Depending on the framework, a complete and minimal substitution set may have at most one, at most finitely many, or possibly infinitely many members, or may not exist at all. In some frameworks it is generally impossible to decide whether any solution exists. For first-order syntactical unification, Martelli and Montanari gave an algorithm that reports unsolvability or computes a complete and minimal singleton substitution set containing the so-called most general unifier.\n\nFor example, using \"x\",\"y\",\"z\" as variables, the singleton equation set { \"cons\"(\"x\",\"cons\"(\"x\",\"nil\")) = \"cons\"(2,\"y\") } is a syntactic first-order unification problem that has the substitution { \"x\" ↦ 2, \"y\" ↦ \"cons\"(2,\"nil\") } as its only solution.\nThe syntactic first-order unification problem { \"y\" = \"cons\"(2,\"y\") } has no solution over the set of finite terms; however, it has the single solution { \"y\" ↦ \"cons\"(2,\"cons\"(2,\"cons\"(2...))) } over the set of infinite trees.\nThe semantic first-order unification problem { \"a\"⋅\"x\" = \"x\"⋅\"a\" } has each substitution of the form { \"x\" ↦ \"a\"⋅...⋅\"a\" } as a solution in a semigroup, i.e. if (⋅) is considered associative; the same problem, viewed in an abelian group, where (⋅) is considered also commutative, has any substitution at all as a solution.\nThe singleton set { \"a\" = \"y\"(\"x\") } is a syntactic second-order unification problem, since \"y\" is a function variable.\nOne solution is { \"x\" ↦ \"a\", \"y\" ↦ (identity function) }; another one is { \"y\" ↦ (constant function mapping each value to \"a\"), \"x\" ↦ \"(any value)\" }.\n\nThe first formal investigation of unification can be attributed to John Alan Robinson, who used first-order syntactical unification as a basic building block of his resolution procedure for first-order logic, a great step forward in automated reasoning technology, as it eliminated one source of combinatorial explosion: searching for instantiation of terms. Today, automated reasoning is still the main application area of unification.\nSyntactical first-order unification is used in logic programming and programming language type system implementation, especially in Hindley–Milner based type inference algorithms.\nSemantic unification is used in SMT solvers, term rewriting algorithms and cryptographic protocol analysis.\nHigher-order unification is used in proof assistants, for example Isabelle and Twelf, and restricted forms of higher-order unification (higher-order pattern unification) are used in some programming language implementations, such as lambdaProlog, as higher-order patterns are expressive, yet their associated unification procedure retains theoretical properties closer to first-order unification.\n\nFormally, a unification approach presupposes\n\nGiven a set formula_1 of variable symbols, a set formula_23 of constant symbols and sets formula_24 of \"n\"-ary function symbols, also called operator symbols, for each natural number formula_25, the set of (unsorted first-order) terms formula_3 is recursively defined to be the smallest set with the following properties:\nFor example, if formula_32 is a variable symbol, formula_33 is a constant symbol, and formula_34 is a binary function symbol, then formula_35, and (hence) formula_36 by the first, second, and third term building rule, respectively. The latter term is usually written as formula_37, using infix notation and the more common operator symbol + for convenience.\n\nA substitution is a mapping formula_38 from variables to terms; the notation formula_39 refers to a substitution mapping each variable formula_40 to the term formula_41, for formula_42, and every other variable to itself. Applying that substitution to a term formula_8 is written in postfix notation as formula_44; it means to (simultaneously) replace every occurrence of each variable formula_40 in the term formula_8 by formula_41. The result formula_48 of applying a substitution formula_49 to a term formula_8 is called an instance of that term formula_8.\nAs a first-order example, applying the substitution to the term \n\nIf a term formula_8 has an instance equivalent to a term formula_15, that is, if formula_54 for some substitution formula_55, then formula_8 is called more general than formula_15, and formula_15 is called more special than, or subsumed by, formula_8. For example, formula_60 is more general than formula_61 if ⊕ is commutative, since then formula_62.\n\nIf ≡ is literal (syntactic) identity of terms, a term may be both more general and more special than another one only if both terms differ just in their variable names, not in their syntactic structure; such terms are called variants, or renamings of each other.\nFor example, \nformula_63\nis a variant of \nformula_64,\nsince \n\nand \n\nHowever, formula_63 is \"not\" a variant of formula_66, since no substitution can transform the latter term into the former one.\nThe latter term is therefore properly more special than the former one.\n\nFor arbitrary formula_11, a term may be both more general and more special than a structurally different term.\nFor example, if ⊕ is idempotent, that is, if always formula_68, then the term formula_69 is more general than formula_70, and vice versa, although formula_69 and formula_70 are of different structure.\n\nA substitution formula_55 is more special than, or subsumed by, a substitution formula_49 if formula_75 is more special than formula_48 for each term formula_8. We also say that formula_49 is more general than formula_55.\nFor instance formula_80 is more special than formula_81,\nbut \nformula_82 is not,\nas formula_83 is not more special than\nformula_84.\n\nA unification problem is a finite set of potential equations, where .\nA substitution σ is a solution of that problem if for formula_85. Such a substitution is also called a unifier of the unification problem.\nFor example, if ⊕ is associative, the unification problem { \"x\" ⊕ \"a\" ≐ \"a\" ⊕ \"x\" } has the solutions {\"x\" ↦ \"a\"}, {\"x\" ↦ \"a\" ⊕ \"a\"}, {\"x\" ↦ \"a\" ⊕ \"a\" ⊕ \"a\"}, etc., while the problem { \"x\" ⊕ \"a\" ≐ \"a\" } has no solution.\n\nFor a given unification problem, a set \"S\" of unifiers is called complete if each solution substitution is subsumed by some substitution σ ∈ \"S\"; the set \"S\" is called minimal if none of its members subsumes another one.\n\n\"Syntactic unification of first-order terms\" is the most widely used unification framework.\nIt is based on \"T\" being the set of \"first-order terms\" (over some given set \"V\" of variables, \"C\" of constants and \"F\" of \"n\"-ary function symbols) and on ≡ being \"syntactic equality\".\nIn this framework, each solvable unification problem has a complete, and obviously minimal, singleton solution set .\nIts member is called the most general unifier (mgu) of the problem.\nThe terms on the left and the right hand side of each potential equation become syntactically equal when the mgu is applied i.e. .\nAny unifier of the problem is subsumed by the mgu .\nThe mgu is unique up to variants: if \"S\" and \"S\" are both complete and minimal solution sets of the same syntactical unification problem, then \"S\" = { \"σ\" } and \"S\" = { \"σ\" } for some substitutions and and is a variant of for each variable \"x\" occurring in the problem.\n\nFor example, the unification problem { \"x\" ≐ \"z\", \"y\" ≐ \"f\"(\"x\") } has a unifier { \"x\" ↦ \"z\", \"y\" ↦ \"f\"(\"z\") }, because\n\nThis is also the most general unifier.\nOther unifiers for the same problem are e.g. { \"x\" ↦ \"f\"(\"x\"), \"y\" ↦ \"f\"(\"f\"(\"x\")), \"z\" ↦ \"f\"(\"x\") }, { \"x\" ↦ \"f\"(\"f\"(\"x\")), \"y\" ↦ \"f\"(\"f\"(\"f\"(\"x\"))), \"z\" ↦ \"f\"(\"f\"(\"x\")) }, and so on; there are infinitely many similar unifiers.\n\nAs another example, the problem \"g\"(\"x\",\"x\") ≐ \"f\"(\"y\") has no solution with respect to ≡ being literal identity, since any substitution applied to the left and right hand side will keep the outermost \"g\" and \"f\", respectively, and terms with different outermost function symbols are syntactically different.\n\nThe first algorithm given by Robinson (1965) was rather inefficient; cf. box.\nThe following faster algorithm originated from Martelli, Montanari (1982).\nThis paper also lists preceding attempts to find an efficient syntactical unification algorithm, and states that linear-time algorithms were discovered independently by Martelli, Montanari (1976) and Paterson, Wegman (1978).\n\nGiven a finite set formula_86 of potential equations,\nthe algorithm applies rules to transform it to an equivalent set of equations of the form\nwhere \"x\", ..., \"x\" are distinct variables and \"u\", ..., \"u\" are terms containing none of the \"x\".\nA set of this form can be read as a substitution.\nIf there is no solution the algorithm terminates with ⊥; other authors use \"Ω\", \"{}\", or \"fail\" in that case.\nThe operation of substituting all occurrences of variable \"x\" in problem \"G\" with term \"t\" is denoted \"G\" {\"x\" ↦ \"t\"}.\nFor simplicity, constant symbols are regarded as function symbols having zero arguments.\n\nAn attempt to unify a variable \"x\" with a term containing \"x\" as a strict subterm \"x\" ≐ \"f\"(..., \"x\", ...) would lead to an infinite term as solution for \"x\", since \"x\" would occur as a subterm of itself.\nIn the set of (finite) first-order terms as defined above, the equation \"x\" ≐ \"f\"(..., \"x\", ...) has no solution; hence the \"eliminate\" rule may only be applied if \"x\" ∉ \"vars\"(\"t\").\nSince that additional check, called \"occurs check\", slows down the algorithm, it is omitted e.g. in most Prolog systems.\nFrom a theoretical point of view, omitting the check amounts to solving equations over infinite trees, see below.\n\nFor the proof of termination of the algorithm consider a triple formula_87\nwhere is the number of variables that occur more than once in the equation set, is the number of function symbols and constants\non the left hand sides of potential equations, and is the number of equations.\nWhen rule \"eliminate\" is applied, decreases, since \"x\" is eliminated from \"G\" and kept only in { \"x\" ≐ \"t\" }.\nApplying any other rule can never increase again.\nWhen rule \"decompose\", \"conflict\", or \"swap\" is applied, decreases, since at least the left hand side's outermost \"f\" disappears.\nApplying any of the remaining rules \"delete\" or \"check\" can't increase , but decreases .\nHence, any rule application decreases the triple formula_87 with respect to the lexicographical order, which is possible only a finite number of times.\n\nConor McBride observes that “by expressing the structure which unification exploits” in a dependently typed language such as Epigram, Robinson's algorithm can be made recursive on the number of variables, in which case a separate termination proof becomes unnecessary.\n\nIn the Prolog syntactical convention a symbol starting with an upper case letter is a variable name; a symbol that starts with a lowercase letter is a function symbol; the comma is used as the logical \"and\" operator.\nFor mathematical notation, \"x,y,z\" are used as variables, \"f,g\" as function symbols, and \"a,b\" as constants.\n\nThe most general unifier of a syntactic first-order unification problem of size may have a size of . For example, the problem has the most general unifier , cf. picture. In order to avoid exponential time complexity caused by such blow-up, advanced unification algorithms work on directed acyclic graphs (dags) rather than trees.\n\nThe concept of unification is one of the main ideas behind logic programming, best known through the language Prolog. It represents the mechanism of binding the contents of variables and can be viewed as a kind of one-time assignment. In Prolog, this operation is denoted by the equality symbol codice_1, but is also done when instantiating variables (see below). It is also used in other languages by the use of the equality symbol codice_1, but also in conjunction with many operations including codice_3, codice_4, codice_5, codice_6. Type inference algorithms are typically based on unification.\n\nIn Prolog:\n\nUnification is used during type inference, for instance in the functional programming language Haskell. On one hand, the programmer does not need to provide type information for every function, on the other hand it is used to detect typing errors. The Haskell expression codice_7 is not correctly typed. The list construction function codice_8 is of type codice_9, and for the first argument codice_10 the polymorphic type variable codice_11 has to be unified with codice_10's type, codice_13. The second argument, codice_14, is of type codice_15, but codice_11 cannot be both codice_13 and codice_18 at the same time.\n\nLike for Prolog, an algorithm for type inference can be given:\n\n\nDue to its declarative nature, the order in a sequence of unifications is (usually) unimportant.\n\nNote that in the terminology of first-order logic, an atom is a basic proposition and is unified similarly to a Prolog term.\n\n\"Order-sorted logic\" allows one to assign a \"sort\", or \"type\", to each term, and to declare a sort \"s\" a \"subsort\" of another sort \"s\", commonly written as \"s\" ⊆ \"s\". For example, when reаsoning about biological creatures, it is useful to declare a sort \"dog\" to be a subsort of a sort \"animal\". Wherever a term of some sort \"s\" is required, a term of any subsort of \"s\" may be supplied instead.\nFor example, assuming a function declaration \"mother\": \"animal\" → \"animal\", and a constant declaration \"lassie\": \"dog\", the term \"mother\"(\"lassie\") is perfectly valid and has the sort \"animal\". In order to supply the information that the mother of a dog is a dog in turn, another declaration \"mother\": \"dog\" → \"dog\" may be issued; this is called \"function overloading\", similar to overloading in programming languages.\n\nWalther gave a unification algorithm for terms in order-sorted logic, requiring for any two declared sorts \"s\", \"s\" their intersection \"s\" ∩ \"s\" to be declared, too: if \"x\" and \"x\" is a variable of sort \"s\" and \"s\", respectively, the equation \"x\" ≐ \"x\" has the solution { \"x\" = \"x\", \"x\" = \"x\" }, where \"x\": \"s\" ∩ \"s\".\n\nAfter incorporating this algorithm into a clause-based automated theorem prover, he could solve a benchmark problem by translating it into order-sorted logic, thereby boiling it down an order of magnitude, as many unary predicates turned into sorts.\n\nSmolka generalized order-sorted logic to allow for parametric polymorphism.\n\nIn his framework, subsort declarations are propagated to complex type expressions.\nAs a programming example, a parametric sort \"list\"(\"X\") may be declared (with \"X\" being a type parameter as in a C++ template), and from a subsort declaration \"int\" ⊆ \"float\" the relation \"list\"(\"int\") ⊆ \"list\"(\"float\") is automatically inferred, meaning that each list of integers is also a list of floats.\n\nSchmidt-Schauß generalized order-sorted logic to allow for term declarations.\n\nAs an example, assuming subsort declarations \"even\" ⊆ \"int\" and \"odd\" ⊆ \"int\", a term declaration like ∀ \"i\" : \"int\". (\"i\" + \"i\") : \"even\" allows to declare a property of integer addition that could not be expressed by ordinary overloading.\n\nBackground on infinite trees:\n\nUnification algorithm, Prolog II:\n\nApplications:\n\nE-unification is the problem of finding solutions to a given set of equations,\ntaking into account some equational background knowledge \"E\".\nThe latter is given as a set of universal equalities.\nFor some particular sets \"E\", equation solving algorithms (a.k.a. \"E-unification algorithms\") have been devised;\nfor others it has been proven that no such algorithms can exist.\n\nFor example, if and are distinct constants,\nthe equation has no solution\nwith respect to purely syntactic unification,\nwhere nothing is known about the operator .\nHowever, if the is known to be commutative,\nthen the substitution solves the above equation,\nsince\nThe background knowledge \"E\" could state the commutativity of by the universal equality\n\" for all \".\n\nIt is said that \"unification is decidable\" for a theory, if a unification algorithm has been devised for it that terminates for \"any\" input problem.\nIt is said that \"unification is semi-decidable\" for a theory, if a unification algorithm has been devised for it that terminates for any \"solvable\" input problem, but may keep searching forever for solutions of an unsolvable input problem.\n\nUnification is decidable for the following theories:\n\nUnification is semi-decidable for the following theories:\n\nIf there is a convergent term rewriting system \"R\" available for \"E\",\nthe one-sided paramodulation algorithm\ncan be used to enumerate all solutions of given equations.\n\nStarting with \"G\" being the unification problem to be solved and \"S\" being the identity substitution, rules are applied nondeterministically until the empty set appears as the actual \"G\", in which case the actual \"S\" is a unifying substitution. Depending on the order the paramodulation rules are applied, on the choice of the actual equation from \"G\", and on the choice of \"R\"’s rules in \"mutate\", different computations paths are possible. Only some lead to a solution, while others end at a \"G\" ≠ {} where no further rule is applicable (e.g. \"G\" = { \"f\"(...) ≐ \"g\"(...) }).\n\nFor an example, a term rewrite system \"R\" is used defining the \"append\" operator of lists built from \"cons\" and \"nil\"; where \"cons\"(\"x\",\"y\") is written in infix notation as \"x\".\"y\" for brevity; e.g. \"app\"(\"a\".\"b\".\"nil\",\"c\".\"d\".\"nil\") → \"a\".\"app\"(\"b\".\"nil\",\"c\".\"d\".\"nil\") → \"a\".\"b\".\"app\"(\"nil\",\"c\".\"d\".\"nil\") → \"a\".\"b\".\"c\".\"d\".\"nil\" demonstrates the concatenation of the lists \"a\".\"b\".\"nil\" and \"c\".\"d\".\"nil\", employing the rewrite rule 2,2, and 1. The equational theory \"E\" corresponding to \"R\" is the congruence closure of \"R\", both viewed as binary relations on terms.\nFor example, \"app\"(\"a\".\"b\".\"nil\",\"c\".\"d\".\"nil\") ≡ \"a\".\"b\".\"c\".\"d\".\"nil\" ≡ \"app\"(\"a\".\"b\".\"c\".\"d\".\"nil\",\"nil\"). The paramodulation algorithm enumerates solutions to equations with respect to that \"E\" when fed with the example \"R\".\n\nA successful example computation path for the unification problem { \"app\"(\"x\",\"app\"(\"y\",\"x\")) ≐ \"a\".\"a\".\"nil\" } is shown below. To avoid variable name clashes, rewrite rules are consistently renamed each time before their use by rule \"mutate\"; \"v\", \"v\", ... are computer-generated variable names for this purpose. In each line, the chosen equation from \"G\" is highlighted in red. Each time the \"mutate\" rule is applied, the chosen rewrite rule (\"1\" or \"2\") is indicated in parentheses. From the last line, the unifying substitution \"S\" = { \"y\" ↦ \"nil\", \"x\" ↦ \"a\".\"nil\" } can be obtained. In fact,\n\"app\"(\"x\",\"app\"(\"y\",\"x\")) {\"y\"↦\"nil\", \"x\"↦ \"a\".\"nil\" } = \"app\"(\"a\".\"nil\",\"app\"(\"nil\",\"a\".\"nil\")) ≡ \"app\"(\"a\".\"nil\",\"a\".\"nil\") ≡ \"a\".\"app\"(\"nil\",\"a\".\"nil\") ≡ \"a\".\"a\".\"nil\" solves the given problem.\nA second successful computation path, obtainable by choosing \"mutate(1), mutate(2), mutate(2), mutate(1)\" leads to the substitution \"S\" = { \"y\" ↦ \"a\".\"a\".\"nil\", \"x\" ↦ \"nil\" }; it is not shown here. No other path leads to a success.\n\nIf \"R\" is a convergent term rewriting system for \"E\",\nan approach alternative to the previous section consists in successive application of \"narrowing steps\";\nthis will eventually enumerate all solutions of a given equation.\nA narrowing step (cf. picture) consists in\nFormally, if is a renamed copy of a rewrite rule from \"R\", having no variables in common with a term \"s\", and the subterm is not a variable and is unifiable with via the mgu , then can be narrowed to the term , i.e. to the term , with the subterm at \"p\" replaced by . The situation that \"s\" can be narrowed to \"t\" is commonly denoted as \"s\" ~› \"t\".\nIntuitively, a sequence of narrowing steps \"t\" ~› \"t\" ~› ... ~› \"t\" can be thought of as a sequence of rewrite steps \"t\" → \"t\" → ... → \"t\", but with the initial term \"t\" being further and further instantiated, as necessary to make each of the used rules applicable.\n\nThe above example paramodulation computation corresponds to the following narrowing sequence (\"↓\" indicating instantiation here):\n\nThe last term, \"v\".\"v\".\"nil\" can be syntactically unified with the original right hand side term \"a\".\"a\".\"nil\".\n\nThe \"narrowing lemma\" ensures that whenever an instance of a term \"s\" can be rewritten to a term \"t\" by a convergent term rewriting system, then \"s\" and \"t\" can be narrowed and rewritten to a term and , respectively, such that is an instance of .\n\nFormally: whenever holds for some substitution σ, then there exist terms such that and and for some substitution τ.\n\nMany applications require one to consider the unification of typed lambda-terms instead of first-order terms. Such unification is often called \"higher-order unification\". A well studied branch of higher-order unification is the problem of unifying simply typed lambda terms modulo the equality determined by αβη conversions. Such unification problems do not have most general unifiers. While higher-order unification is undecidable, Gérard Huet gave a semi-decidable (pre-)unification algorithm that allows a systematic search of the space of unifiers (generalizing the unification algorithm of Martelli-Montanari with rules for terms containing higher-order variables) that seems to work sufficiently well in practice. Huet and Gilles Dowek have written articles surveying this topic.\n\nDale Miller has described what is now called \"higher-order pattern unification\". This subset of higher-order unification is decidable and solvable unification problems have most-general unifiers. Many computer systems that contain higher-order unification, such as the higher-order logic programming languages λProlog and Twelf, often implement only the pattern fragment and not full higher-order unification.\n\nIn computational linguistics, one of the most influential theories of ellipsis is that ellipses are represented by free variables whose values are then determined using Higher-Order Unification (HOU). For instance, the semantic representation of \"Jon likes Mary and Peter does too\" is and the value of R (the semantic representation of the ellipsis) is determined by the equation . The process of solving such equations is called Higher-Order Unification.\n\nFor example, the unification problem { \"f\"(\"a\", \"b\", \"a\") ≐ \"d\"(\"b\", \"a\", \"c\") }, where the only variable is \"f\", has the\nsolutions {\"f\" ↦ λ\"x\".λ\"y\".λ\"z\".\"d\"(\"y\", \"x\", \"c\") }, {\"f\" ↦ λ\"x\".λ\"y\".λ\"z\".\"d\"(\"y\", \"z\", \"c\") },\n\nWayne Snyder gave a generalization of both higher-order unification and E-unification, i.e. an algorithm to unify lambda-terms modulo an equational theory.\n\n\n"}
{"id": "5959843", "url": "https://en.wikipedia.org/wiki?curid=5959843", "title": "Vector notation", "text": "Vector notation\n\nVector notation is a commonly used mathematical notation for working with mathematical vectors, which may be geometric vectors or members of vector spaces.\n\nFor representing a vector, the common typographic convention is lower case, upright boldface type, as in formula_1 for a vector named ‘v’. The International Organization for Standardization (ISO) recommends either bold italic serif, as in v or a, or non-bold italic serif accented by a right arrow, as in formula_2 or formula_3. This arrow notation for vectors is commonly used in handwriting, where boldface is impractical. The arrow represents right-pointing arrow notation or harpoons. Shorthand notations include tildes and straight lines placed respectively, below or above the name of a vector.\n\nBetween 1880 and 1887, Oliver Heaviside developed operational calculus, a method of solving differential equations by transforming them into ordinary algebraic equations which caused much controversy when introduced because of the lack of rigour in its derivation. After the turn of the 20th century, Josiah Willard Gibbs would in physical chemistry supply notation for the scalar product and vector products, which was introduced in \"Vector Analysis\".\n\nA rectangular vector is a coordinate vector specified by components that define a rectangle (or rectangular prism in three dimensions, and similar shapes in greater dimensions). The starting point and terminal point of the vector lie at opposite ends of the rectangle (or prism, etc.).\n\nA rectangular vector in formula_4 can be specified using an ordered set of components, enclosed in either parentheses or angle brackets.\n\nIn a general sense, an \"n\"-dimensional vector v can be specified in either of the following forms:\n\n\nWhere \"v\", \"v\", …, \"v\", \"v\" are the components of v.\n\nA rectangular vector in formula_4 can also be specified as a row or column matrix containing the ordered set of components. A vector specified as a row matrix is known as a row vector; one specified as a column matrix is known as a column vector.\n\nAgain, an \"n\"-dimensional vector formula_1 can be specified in either of the following forms using matrices:\n\n\nWhere \"v\", \"v\", …, \"v\", \"v\" are the components of v. In some advanced contexts, a row and a column vector have different meaning; see covariance and contravariance of vectors.\n\nA rectangular vector in formula_11 (or fewer dimensions, such as formula_12 where \"v\" below is zero) can be specified as the sum of the scalar multiples of the components of the vector with the members of the standard basis in formula_11. The basis is represented with the unit vectors formula_14, formula_15, and formula_16.\n\nA three-dimensional vector v can be specified in the following form, using unit vector notation:\n\n\nWhere \"v\", \"v\", and \"v\" are the scalar components of v. Scalar components may be positive or negative; the absolute value of a scalar component is a magnitude.\n\nThe two polar coordinates of a point in a plane may be considered as a two dimensional vector. Such a \n\"polar vector\" consists of a magnitude (or length) and a direction (or angle). The magnitude, typically represented as \"r\", is the distance from a starting point, the origin, to the point which is represented. The angle, typically represented as \"θ\" (the Greek letter theta), is the angle between the a fixed direction, typically that of the \"x\"-axis, and the direction from the origin to the point. The angle is typically reduced to lie within the range formula_18 radians or formula_19.\n\nIt must be emphasized that a \"polar vector\" is not really a vector, since the addition of two polar vectors is not defined.\n\nPolar vectors can be specified using either ordered pair notation (a subset of ordered set notation using only two components) or matrix notation, as with rectangular vectors. In these forms, the first component of the vector is \"r\" (instead of \"v\") and the second component is \"θ\" (instead of \"v\"). To differentiate polar vectors from rectangular vectors, the angle may be prefixed with the angle symbol, formula_20.\n\nA two-dimensional polar vector \"v\" can be represented as any of the following, using either ordered pair or matrix notation:\n\n\nWhere \"r\" is the magnitude, \"θ\" is the angle, and the angle symbol (formula_20) is optional.\n\nPolar vectors can also be specified using simplified autonomous equations that define \"r\" and \"θ\" explicitly. This can be unwieldy, but is useful for avoiding the confusion with two-dimensional rectangular vectors that arises from using ordered pair or matrix notation.\n\nA two-dimensional vector whose magnitude is 5 units and whose direction is \"π\"/9 radians (20°) can be specified using either of the following forms:\n\n\nA cylindrical vector is an extension of the concept of polar vectors into three dimensions. It is akin to an arrow in the cylindrical coordinate system. A cylindrical vector is specified by a distance in the \"xy\"-plane, an angle, and a distance from the \"xy\"-plane (a height). The first distance, usually represented as \"r\" or \"ρ\" (the Greek letter rho), is the magnitude of the projection of the vector onto the \"xy\"-plane. The angle, usually represented as \"θ\" or \"φ\" (the Greek letter phi), is measured as the offset from the line collinear with the \"x\"-axis in the positive direction; the angle is typically reduced to lie within the range formula_18. The second distance, usually represented as \"h\" or \"z\", is the distance from the \"xy\"-plane to the endpoint of the vector.\n\nCylindrical vectors are specified like polar vectors, where the second distance component is concatenated as a third component to form ordered triplets (again, a subset of ordered set notation) and matrices. The angle may be prefixed with the angle symbol (formula_20); the distance-angle-distance combination distinguishes cylindrical vectors in this notation from spherical vectors in similar notation.\n\nA three-dimensional cylindrical vector \"v\" can be represented as any of the following, using either ordered triplet or matrix notation:\n\n\nWhere \"r\" is the magnitude of the projection of v onto the \"xy\"-plane, \"θ\" is the angle between the positive \"x\"-axis and v, and \"h\" is the height from the \"xy\"-plane to the endpoint of \"v\". Again, the angle symbol (formula_20) is optional.\n\nA cylindrical vector can also be specified directly, using simplified autonomous equations that define \"r\" (or \"ρ\"), \"θ\" (or \"φ\"), and \"h\" (or \"z\"). Consistency should be used when choosing the names to use for the variables; \"ρ\" should not be mixed with \"θ\" and so on.\n\nA three-dimensional vector, the magnitude of whose projection onto the \"xy\"-plane is 5 units, whose angle from the positive \"x\"-axis is \"π\"/9 radians (20°), and whose height from the \"xy\"-plane is 3 units can be specified in any of the following forms:\n\n\nA spherical vector is another method for extending the concept of polar vectors into three dimensions. It is akin to an arrow in the spherical coordinate system. A spherical vector is specified by a magnitude, an azimuth angle, and a zenith angle. The magnitude is usually represented as \"ρ\". The azimuth angle, usually represented as \"θ\", is the offset from the line collinear with the \"x\"-axis in the positive direction. The zenith angle, usually represented as \"φ\", is the offset from the line collinear with the \"z\"-axis in the positive direction. Both angles are typically reduced to lie within the range from zero (inclusive) to 2\"π\" (exclusive).\n\nSpherical vectors are specified like polar vectors, where the zenith angle is concatenated as a third component to form ordered triplets and matrices. The azimuth and zenith angles may be both prefixed with the angle symbol (formula_20); the prefix should be used consistently to produce the distance-angle-angle combination that distinguishes spherical vectors from cylindrical ones.\n\nA three-dimensional spherical vector \"v\" can be represented as any of the following, using either ordered triplet or matrix notation:\n\n\nWhere \"ρ\" is the magnitude, \"θ\" is the azimuth angle, and \"φ\" is the zenith angle.\n\nLike polar and cylindrical vectors, spherical vectors can be specified using simplified autonomous equations, in this case for \"ρ\", \"θ\", and \"φ\".\n\nA three-dimensional vector whose magnitude is 5 units, whose azimuth angle is \"π\"/9 radians (20°), and whose zenith angle is \"π\"/4 radians (45°) can be specified as:\n\n\nIn any given vector space, the operations of vector addition and scalar multiplication are defined. Normed vector spaces also define an operation known as the norm (or determination of magnitude). Inner product spaces also define an operation known as the inner product. In formula_4, the inner product is known as the dot product. In formula_11 and formula_48, an additional operation known as the cross product is also defined.\n\nVector addition is represented with the plus sign used as an operator between two vectors. The sum of two vectors u and v would be represented as:\n\nScalar multiplication is represented in the same manners as algebraic multiplication. A scalar beside a vector (either or both of which may be in parentheses) implies scalar multiplication. The two common operators, a dot and a rotated cross, are also acceptable (although the rotated cross is almost never used), but they risk confusion with dot products and cross products, which operate on two vectors. The product of a scalar \"c\" with a vector v can be represented in any of the following fashions:\n\n\nUsing the algebraic properties of subtraction and division, along with scalar multiplication, it is also possible to “subtract” two vectors and “divide” a vector by a scalar.\n\nVector subtraction is performed by adding the scalar multiple of −1 with the second vector operand to the first vector operand. This can be represented by the use of the minus sign as an operator. The difference between two vectors u and v can be represented in either of the following fashions:\n\n\nScalar division is performed by multiplying the vector operand with the numeric inverse of the scalar operand. This can be represented by the use of the fraction bar or division signs as operators. The quotient of a vector v and a scalar \"c\" can be represented in any of the following forms:\n\n\nThe norm of a vector is represented with double bars on both sides of the vector. The norm of a vector v can be represented as:\n\nThe norm is also sometimes represented with single bars, like formula_59, but this can be confused with absolute value (which is a type of norm).\n\nThe inner product (also known as the scalar product, not to be confused with scalar multiplication) of two vectors is represented as an ordered pair enclosed in angle brackets. The inner product of two vectors u and v would be represented as:\n\nIn formula_4, the inner product is also known as the dot product. In addition to the standard inner product notation, the dot product notation (using the dot as an operator) can also be used (and is more common). The dot product of two vectors u and v can be represented as:\n\nIn some older literature, the dot product is implied between two vectors written side-by-side. This notation can be confused with the dyadic product between two vectors.\n\nThe cross product of two vectors (in formula_11) is represented using the rotated cross as an operator. The cross product of two vectors u and v would be represented as:\n\nIn some older literature, the following notation is used for the cross product between u and v:\n\n"}
