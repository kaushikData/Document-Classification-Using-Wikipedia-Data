{"id": "4224687", "url": "https://en.wikipedia.org/wiki?curid=4224687", "title": "164 (number)", "text": "164 (number)\n\n164 (one hundred [and] sixty-four) is the natural number following 163 and preceding 165.\n\n164 is a zero of the Mertens function.\n\nIn base 10, 164 is the smallest number that can be expressed as a concatenation of two squares in two different ways: as 1 + 64 or 16 + 4.\n\n\n\n\n164 is also:\n\n\n"}
{"id": "1211056", "url": "https://en.wikipedia.org/wiki?curid=1211056", "title": "Abstract polytope", "text": "Abstract polytope\n\nIn mathematics, an abstract polytope is an algebraic partially ordered set or poset which captures the combinatorial properties of a traditional polytope, but not any purely geometric properties such as angles, edge lengths, etc.\n\nAn ordinary geometric polytope is said to be a \"realization\" in some real N-dimensional space, typically Euclidean, of the corresponding abstract polytope.\n\nThe abstract definition allows some more general combinatorial structures than traditional definitions of a polytope, thus allowing many new objects that have no counterpart in traditional theory.\n\nThe term \"polytope\" is a generalisation of polygons and polyhedra into any number of dimensions.\n\nIn Euclidean geometry, the six quadrilaterals illustrated are all different. Yet they have a common structure in the alternating chain of four vertices and four sides which gives them their name. They are said to be isomorphic or “structure preserving”.\n\nThis common structure may be represented in an underlying abstract polytope, a purely algebraic partially-ordered set which captures the pattern of connections or \"incidences\" between the various structural elements. The measurable properties of traditional polytopes such as angles, edge-lengths, skewness, straightness and convexity have no meaning for an abstract polytope.\n\nWhat is true for traditional polytopes (also called classical or geometric polytopes) may not be so for abstract ones, and vice versa. For example, a traditional polytope is regular if all its facets and vertex figures are regular, but this is not necessarily so for an abstract polytope.\n\nA traditional geometric polytope is said to be a \"realisation\" of the associated abstract polytope. A realisation is a mapping or injection of the abstract object into a real space, typically Euclidean, to construct a traditional polytope as a real geometric figure.\n\nThe six quadrilaterals shown are all distinct realisations of the abstract quadrilateral, each with different geometric properties. Some of them do not conform to traditional definitions of a quadrilateral and are said to be \"unfaithful\" realisations. A conventional polytope is a faithful realisation.\n\nIn an abstract polytope, each structural element - vertex, edge, cell, etc. is associated with a corresponding member or element of the set. The term \"face\" often refers to any such element e.g. a vertex (0-face), edge (1-face) or a general \"k\"-face, and not just a polygonal 2-face.\n\nThe faces are \"ranked\" according to their associated real dimension: vertices have rank = 0, edges rank = 1 and so on.\n\nIncident faces of different rank, for example a vertex F of an edge G, are ordered by the relation F < G. F is said to be a \"subface\" of G, or G has subface F.\n\nF, G are said to be \"incident\" if either F = G or F < G or G < F. This usage of \"incidence\" also occurs in Finite geometry, although it differs from traditional geometry and some other areas of mathematics. For example in the square \"abcd\", edges \"ab\" and \"bc\" are not abstractly incident (although they are both incident with vertex \"b\").\n\nA polytope is then defined as a set of faces P with an order relation <, and which satisfies certain additional axioms. Formally, P (with <) will be a (strict) partially ordered set, or \"poset\".\n\nJust as the number zero is necessary in mathematics, so also set theory requires an empty set which, technically, every set contains. In an abstract polytope this is known as the \"least\" or \"null\" face and is a subface of all the others. Since the least face is one level below the vertices or 0-faces, its rank is −1 and may be denoted as \"F\". It is not usually realized.\n\nThere is also a single face of which all the others are subfaces. This is called the \"greatest\" face. In an \"n\"-dimensional polytope, the greatest face has rank = \"n\" and may be denoted as \"F\". It is sometimes realized as the interior of the geometric figure.\n\nThese least and greatest faces are sometimes called \"improper\" faces, with all others being \"proper\" faces.\n\nThe faces of the abstract quadrilateral or square are shown in the table below:\n\nThe relation < comprises a set of pairs, which here include\n\nOrder relations are transitive, i.e. F < G and G < H implies that F < H. Therefore, to specify the hierarchy of faces, it is not necessary to give every case of F < H, only the pairs where one is the successor of the other, i.e. where F < H and no G satisfies F < G < H.\n\nThe edges W, X, Y and Z are sometimes written as ab, ad, bc, and cd respectively, but such notation is not always appropriate.\n\nAll four edges are structurally similar and the same is true of the vertices. The figure therefore has the symmetries of a square and is usually referred to as the square.\n\nSmaller posets, and polytopes in particular, are often best visualised in a Hasse diagram, as shown. By convention, faces of equal rank are placed on the same vertical level. Each \"line\" between faces, say F, G, indicates an ordering relation < such that F < G where F is below G in the diagram.\n\nThe Hasse diagram defines the unique poset and therefore fully captures the structure of the polytope. Isomorphic polytopes give rise to isomorphic Hasse diagrams, and vice versa. The same is not generally true for the graph representation of polytopes.\n\nThe \"rank\" of a face F is defined as (\"m\" − 2), where \"m\" is the maximum number of faces in any chain (F', F\", ... , F) satisfying F' < F\" < ... < F. F' is always the least face, F.\n\nThe \"rank\" of an abstract polytope P is the maximum rank n of any face. It is always the rank of the greatest face F.\n\nThe rank of a face or polytope usually corresponds to the \"dimension\" of its counterpart in traditional theory.\n\nFor some ranks, their face-types are named in the following table.\n† Traditionally \"face\" has meant a rank 2 face or 2-face. In abstract theory the term \"face\" denotes a face of \"any\" rank.\n\nA flag is a maximal chain of faces, i.e. a (totally) ordered set Ψ of faces, each a subface of the next (if any), and such that Ψ is not a subset of any larger chain. Given any two distinct faces F, G in a flag, either F < G or F > G.\n\nFor example, {ø, a, ab, abc} is a flag in the triangle abc.\n\nFor a given polytope, all flags contain the same number of faces. Other posets do not, in general, satisfy this requirement.\n\nAny subset P' of a poset P is a poset (with the same relation <, restricted to P').\n\nIn an abstract polytope, given any two faces \"F\", \"H\" of P with \"F\" ≤ \"H\", the set {\"G\" | \"F\" ≤ \"G\" ≤ \"H\"} is called a section of \"P\", and denoted \"H\"/\"F\". (In order theory, a section is called a closed interval of the poset and denoted [\"F\", \"H\"].\n\nFor example, in the prism abcxyz (see diagram) the section xyz/ø (highlighted green) is the triangle\n\nA \"k\"-section is a section of rank \"k\".\n\nA polytope that is the subset of another polytope is not necessarily a section. In the diagram, the square abcd is a \"subset\" of the tetrahedron abcd, but is not a \"section\" of it.\n\nP is thus a section of itself.\n\nThis concept of section \"does not\" have the same meaning as in traditional geometry.\n\nThe vertex figure at a given vertex \"V\" is the (\"n\"−1)-section \"F\"/\"V\", where \"F\" is the greatest face.\n\nFor example, in the triangle abc, the vertex figure at b, abc/b, is {b, ab, bc, abc}, which is a line segment. The vertex figures of a cube are triangles.\n\nA poset P is connected if P has rank ≤ 1, or, given any two proper faces F and G, there is a sequence of proper faces\n\nsuch that F = H, G = H, and each H, i < k, is incident with its successor.\n\nThe above condition ensures that a pair of disjoint triangles abc and xyz is \"not\" a (single) polytope.\n\nA poset P is strongly connected if every section of P (including P itself) is connected.\n\nWith this additional requirement, two pyramids that share just a vertex are also excluded. However, two square pyramids, for example, \"can\", be \"glued\" at their square faces - giving an octahedron. The \"common face\" is \"not\" then a face of the octahedron.\n\nAn abstract polytope is a partially ordered set, whose elements we call \"faces\", satisfying the 4 axioms:\n\n\nAn \"n\"-polytope is a polytope of rank \"n\".\n\nIn the case of the \"null polytope\", the least and greatest faces are the \"same single element\".\n\nAxiom 2 is equivalent to saying that the poset is a graded poset.\n\nGiven the other axioms, Axiom 3 is equivalent to \"strong flag-connectedness\", which informally means:\n\nAxiom 4 is known as the “diamond property”, since the Hasse Diagram of \"a\", \"b\", and the faces between it is diamond-shaped.\n\nIt can be shown from the axioms that every section is a polytope, and that Rank(\"G\"/\"F\") = Rank(\"G\") − Rank(\"F\") − 1.\n\nThere is just one poset for each rank −1 and 0. These are, respectively, the null face and the point. These are not always considered to be valid abstract polytopes.\n\nThere is only one polytope of rank 1, which is the line segment. It has a least face, just two 0-faces and a greatest face, for example {ø, a, b, ab}. It follows that the vertices a and b have rank 0, and that the greatest face ab, and therefore the poset, both have rank 1.\n\nFor each \"p\", 3 ≤ \"p\" < formula_1, we have the (abstract equivalent of) the traditional polygon with \"p\" vertices and \"p\" edges, or a \"p\"-gon. For p = 3, 4, 5, ... we have the triangle, square, pentagon, ...\n\nFor \"p\" = 2, we have the digon, and \"p\" = formula_1 we get the apeirogon.\n\nA digon is a polygon with just 2 edges. Unlike any other polygon, both edges have the same two vertices. For this reason, it is \"degenerate\" in the Euclidean plane.\n\nFaces are sometimes described using \"vertex notation\" - e.g. {ø, a, b, c, ab, ac, bc, abc} for the triangle abc. This method has the advantage of \"implying\" the < relation.\n\nWith the digon this vertex notation \"cannot be used\". It is necessary to give the faces individual symbols and specify the subface pairs F < G.\n\nThus a digon is defined as a set {ø, a, b, E', E\", G} with the relation < given by\n\nThe exchange maps and the flag action in particular can be used to prove that \"any\" abstract polytope is a quotient of some regular polytope.\n\nA polytope can also be represented by tabulating its incidences. \n\nThe following incidence matrix is that of a triangle:\nThe table shows a 1 wherever a face is a subface of another, \"or vice versa\" (so the table is symmetric about the diagonal)- so in fact, the table has \"redundant information\"; it would suffice to show only a 1 when the row face ≤ the column face.\n\nSince both the body and the empty set are incident with all other elements, the first row and column as well as the last row and column are trivial and can conveniently be omitted.\n\nFurther information is gained by counting each occurrence. This numerative usage enables a symmetry grouping, as in the Hasse Diagram of the square pyramid: If vertices B, C, D, and E are considered symmetrically equivalent within the abstract polytope, then edges f, g, h, and j will be grouped together, and also edges k, l, m, and n, And finally also the triangles P, Q, R, and S. Thus the corresponding incidence matrix of this abstract polytope may be shown as:\n\nIn this accumulated incidence matrix representation the diagonal entries represent the total counts of either element type.\n\nElements of different type of the same rank clearly are never incident so the value will always be 0, however to help distinguish such relationships, an asterisk (*) is used instead of 0.\n\nThe sub-diagonal entries of each row represent the incidence counts of the relevant sub-elements, while the super-diagonal entries represent the respective element counts of the vertex-, edge- or whatever -figure.\n\nAlready this simple square pyramid shows that the symmetry-accumulated incidence matrices are no longer symmetrical. But there is still a simple entity-relation (beside the generalised Euler formulae for the diagonal, respectively the sub-diagonal entities of each row, respectively the super-diagonal elements of each row - those at least whenever no holes or stars etc. are considered), as for any such incidence matrix formula_3 holds:\n\nformula_4\n\nIn the 1960s Branko Grünbaum issued a call to the geometric community to consider generalizations of the concept of regular polytopes that he called \"polystromata\". He developed a theory of polystromata, showing examples of new objects including the 11-cell.\n\nThe 11-cell is a self-dual 4-polytope whose facets are not icosahedra, but are \"hemi-icosahedra\" — that is, they are the shape one gets if one considers opposite faces of the icosahedra to be actually the \"same\" face (Grünbaum, 1977). A few years after Grünbaum's discovery of the 11-cell, H.S.M. Coxeter discovered a similar polytope, the 57-cell (Coxeter 1982, 1984), and then independently rediscovered the 11-cell.\n\nWith the earlier work by Branko Grünbaum, H. S. M. Coxeter and Jacques Tits having laid the groundwork, the basic theory of the combinatorial structures now known as abstract polytopes was first described by Egon Schulte in his 1980 PhD dissertation. In it he defined \"regular incidence complexes\" and \"regular incidence polytopes\". Subsequently, he and Peter McMullen developed the basics of the theory in a series of research articles that were later collected into a book. Numerous other researchers have since made their own contributions, and the early pioneers (including Grünbaum) have also accepted Schulte's definition as the \"correct\" one.\n\nSince then, research in the theory of abstract polytopes has focused mostly on \"regular\" polytopes, that is, those whose automorphism groups act transitively on the set of flags of the polytope.\n\n\n"}
{"id": "882332", "url": "https://en.wikipedia.org/wiki?curid=882332", "title": "Alexander Macfarlane", "text": "Alexander Macfarlane\n\nProf Alexander Macfarlane FRSE LLD (21 April 1851 – 28 August 1913) was a Scottish logician, physicist, and mathematician.\n\nMacfarlane was born in Blairgowrie, Scotland and studied at the University of Edinburgh. His doctoral thesis \"The disruptive discharge of electricity\" reported on experimental results from the laboratory of Peter Guthrie Tait.\n\nIn 1878 Macfarlane was elected a Fellow of the Royal Society of Edinburgh. His proposers were Peter Guthrie Tait, Philip Kelland, Alexander Crum Brown, and John Hutton Balfour.\n\nDuring his life, Macfarlane played a prominent role in research and education. He taught at the universities of Edinburgh and St Andrews, was physics professor at the University of Texas (1885 – 1894), professor of Advanced Electricity, and later of mathematical physics, at Lehigh University. In 1896 Macfarlane encouraged the association of quaternion students to promote the algebra. He became the Secretary of the Quaternion Society, and in 1909 its President. He edited the \"Bibliography of Quaternions\" that the Society published in 1904.\n\nMacfarlane was also the author of a popular 1916 collection of mathematical biographies (\"Ten British Mathematicians\"), a similar work on physicists (\"Lectures on Ten British Physicists of the Nineteenth Century\", 1919). Macfarlane was caught up in the revolution in geometry during his lifetime, in particular through the influence of G. B. Halsted who was mathematics professor at the University of Texas. Macfarlane originated an \"Algebra of Physics\", which was his adaptation of quaternions to physical science. His first publication on \"Space Analysis\" preceded the presentation of Minkowski Space by seventeen years.\n\nMacfarlane actively participated in several International Congresses of Mathematicians including the primordial meeting in Chicago, 1893, and the Paris meeting of 1900 where he spoke on \"Application of space analysis to curvilinear coordinates\".\n\nMacfarlane retired to Chatham, Ontario, where he died in 1913.\n\nAlexander Macfarlane stylized his work as \"Space Analysis\". In 1894 he published his five earlier papers and a book review of Alexander MacAulay's \"Utility of Quaternions in Physics\". This collection is now available on-line.\nPage numbers are carried from previous publications, and the reader is presumed familiar with quaternions. The first paper is \"Principles of the Algebra of Physics\" where he first proposes the hyperbolic quaternion algebra, since \"a student of physics finds a difficulty in principle of quaternions which makes the square of a vector negative.\" The second paper is \"The Imaginary of the Algebra\". Similar to Homersham Cox (1882/83), Macfarlane uses the hyperbolic versor as the hyperbolic quaternion corresponding to the versor of Hamilton. The presentation is encumbered by the notation\nLater he conformed to the notation exp(A α) used by Euler and Sophus Lie. The expression formula_2 is meant to emphasize that α is a \"right versor\", where π/2 is the measure of a right angle in radians. The π/2 in the exponent is, in fact, superfluous.\n\nPapers three and four are \"Fundamental Theorems of Analysis Generalized for Space\" and \"On the definition of the Trigonometric Functions\", which he had presented the previous year in Chicago at the \"Congress of Mathematicians\" held in connection with the World's Columbian Exhibition. He follows George Salmon in exhibiting the hyperbolic angle, argument of hyperbolic functions. The fifth paper is \"Elliptic and Hyperbolic Analysis\" which considers the spherical law of cosines as the fundamental theorem of the sphere, and proceeds to analogues for the ellipsoid of revolution, general ellipsoid, and equilateral hyperboloids of one and two sheets, where he provides the hyperbolic law of cosines.\n\nIn 1900 Alexander published \"Hyperbolic Quaternions\" with the Royal Society in Edinburgh, and included a sheet of nine figures, two of which display conjugate hyperbolas. Having been stung in the \"Great Vector Debate\" over the non-associativity of his Algebra of Physics, he restored associativity by reverting to biquaternions, an algebra used by students of Hamilton since 1853.\n\n\n\n"}
{"id": "16436055", "url": "https://en.wikipedia.org/wiki?curid=16436055", "title": "Array controller based encryption", "text": "Array controller based encryption\n\nWithin a storage network, encryption of data may occur at different hardware levels. Array controller based encryption describes the encryption of data occurring at the disk array controller before being sent to the disk drives. This article will provide an overview of different implementation techniques to array controller based encryption. For cryptographic and encryption theory, see disk encryption theory.\n\nThe encryption of data can take place in many points in a storage network. The point of encryption may occur on the host computer, in the SAN infrastructure, the array controller or on each of the hard disks as shown on the diagram above. Each point of encryption has different merits and costs. Within the diagram, the key server components are also shown for each configuration of encryption. Designers of SANs and SAN components must take into consideration factors such as performance, deployment complexity, key server interoperability, strength of security, and cost when choosing where to implement encryption. But since the array controller is a natural central point of all data therefore encryption at this level is inherent and also reduces deployment complexity. \n\nWith different configurations of a hardware or software array controller, there are different types of solutions for this type of encryption. Each of these solutions can be built into existing infrastructures by replacing or upgrading certain components. Basic components include an encryption key server, key management client, and commonly an encryption unit which are all implemented into a storage network.\n\nFor an internal array controller configuration, the array controller is generally a PCI bus card situated inside the host computer. As shown in the diagram, the PCI array controller would contain an encryption unit where plaintext data is encrypted into ciphertext. This separate encryption unit is utilized to prevent and minimize performance reduction and maintain data throughput. Furthermore, the Key Management Client will generally be an additional service within the host computer applications where it will authenticate all keys retrieved from the Key Server. A major disadvantage to this type of implementation would be that encryption components are required to be integrated within each host computer and therefore is redundant on large networks with many host devices.\n\nIn the case of an external array controller setup, the array controller would be an independent hardware module connected to the network. Within the hardware array controller would be an Encryption unit for data encryption as well as a Key Management Client for authentication. Generally, there are few hardware array controllers to many host devices and storage disks. Therefore, it reduces deployment complexity to implement into fewer hardware components. Moreover, the lifecycle of an array controller is generally much longer than host computers and storage disks, therefore the encryption implementation will not need to be reimplemented as often as if encryption was done at another point in the storage network.\n\nIn an external array controller, the encryption unit can either be placed either on the front-end side or the back-end side of the array controller. There are different advantages and disadvantages in placing the encryption unit either on the front-end side or the back-end side:\n\nThe placement of the encryption unit may highly impact the secureness of your controller based encryption implementation. Therefore, this issue must be taken account for when designing your implementation to mitigate all security risks.\n\nFor the software array controller encryption, a software array controller driver directs data into individual host bus adapters. In the adjacent diagram, there are multiple host bus adapters with hardware encryption units used for better performance requirements. In contrast, this type of encryption can be implemented with only 1 host bus adapter connected to a network of multiple hard drives and would still function. Performance will definitely be reduced since there will only be one encryption unit processing data. Key management will be done much like the internal array controller encryption mentioned before with the Key Management Client implemented as a service within the Host Computer.\n\n"}
{"id": "52208686", "url": "https://en.wikipedia.org/wiki?curid=52208686", "title": "Asymptotic dimension", "text": "Asymptotic dimension\n\nIn metric geometry, asymptotic dimension of a metric space is a large-scale analog of Lebesgue covering dimension. The notion of asymptotic dimension was introduced my Mikhail Gromov in his 1993 monograph \"Asymptotic invariants of infinite groups\" in the context of geometric group theory, as a quasi-isometry invariant of finitely generated groups. As shown by Guoliang Yu, finitely generated groups of finite homotopy type with finite asymptotic dimension satisfy the Novikov conjecture. Asymptotic dimension has important applications in geometric analysis and index theory.\n\nLet formula_1 be a metric space and formula_2 be an integer. We say that formula_3 if for every formula_4 there exists a uniformly bounded cover formula_5 of formula_1 such that every closed formula_7-ball in formula_1 intersects at most formula_9 subsets from formula_5. Here 'uniformly bounded' means that formula_11.\n\nWe then define the \"asymptotic dimension\" formula_12 as the smallest integer formula_2 such that formula_3, if at least one such formula_15 exists, and define formula_16 otherwise.\n\nAlso, one says that a family formula_17 of metric spaces satisfies formula_18 \"uniformly\" if for every formula_4 and every formula_20 there exists a cover formula_21 of formula_22 by sets of diameter at most formula_23 (independent of formula_24) such that every closed formula_7-ball in formula_22 intersects at most formula_9 subsets from formula_28.\n\n\n\nAsymptotic dimension achieved particular prominence in geometric group theory after a 1998 paper of Guoliang Yu\n, which proved that if formula_63 is a finitely generated group of finite homotopy type (that is with a classifying space of the homotopy type of a finite CW-complex) such that formula_64, then formula_63 satisfies the Novikov conjecture. As was subsequently shown, finitely generated groups with finite asymptotic dimension are \"topologically amenable\", i.e. satisfy Guoliang Yu's \"Property A\" introduced in and equivalent to the exactness of the reduced C*-algebra of the group.\n\n\n\n"}
{"id": "46238164", "url": "https://en.wikipedia.org/wiki?curid=46238164", "title": "Bar mitzvah attack", "text": "Bar mitzvah attack\n\nThe bar mitzvah attack is an attack on the SSL/TLS protocols that exploits the use of the RC4 cipher with weak keys for that cipher. While this affects only the first hundred or so bytes of only the very small fraction of connections that happen to use weak keys, it allows significant compromise of user security, for example by allowing the interception of password information which could then be used for long-term exploitation.\n\nThe attack uses a vulnerability in RC4 described as the \"invariance weakness\" by Fluhrer et al. in their 2001 paper on RC4 weaknesses.\n\nThe attack is named after the bar mitzvah ceremony which is held at 13 years of age, because the vulnerability exploited is 13 years old and likely inspired by the naming of the unrelated birthday attack.\n\n"}
{"id": "11127382", "url": "https://en.wikipedia.org/wiki?curid=11127382", "title": "Belt problem", "text": "Belt problem\n\nThe belt problem is a mathematics problem which requires finding the length of a crossed belt that connects two circular pulleys with radius \"r\" and \"r\" whose centers are separated by a distance \"P\". The solution of the belt problem requires trigonometry and the concepts of the bitangent line, the vertical angle, and congruent angles.\n\nClearly triangles ACO and ADO are congruent right angled triangles, as are triangles BEO and BFO. In addition, triangles ACO and BEO are similar. Therefore angles CAO, DAO, EBO and FBO are all equal. Denoting this angle by formula_1 (denominated in radians), the length of the belt is\n\nThis exploits the convenience of denominating angles in radians that the length of an arc = the radius × the measure of the angle facing the arc.\n\nTo find formula_1 we see from the similarity of triangles ACO and BEO that\n\nFor fixed \"P\" the length of the belt depends only on the sum of the radius values \"r\" + \"r\", and not on their individual values.\n\nThere are other types of problems similar to the belt problem. The pulley problem, as shown, is similar to the belt problem; however, the belt does not cross itself. In the pulley problem the length of the belt is\n\nwhere \"r\" represents the radius of the larger pulley, \"r\" represents the radius of the smaller one, and:\n\nThe belt problem is used in the design of aeroplanes, bicycle gearing, cars, and other items with pulleys or belts that cross each other. The pulley problem is also used in the design of conveyor belts found in airport luggage belts and automated factory lines.\n\n"}
{"id": "31504702", "url": "https://en.wikipedia.org/wiki?curid=31504702", "title": "Cellular algebra", "text": "Cellular algebra\n\nIn abstract algebra, a cellular algebra is a finite-dimensional associative algebra \"A\" with a distinguished cellular basis which is particularly well-adapted to studying the representation theory of \"A\".\n\nThe cellular algebras discussed in this article were introduced in a 1996 paper of Graham and Lehrer. However, the terminology had previously been used by Weisfeiler and Lehman in the Soviet Union in the 1960s, to describe what are also known as association schemes.\n\nLet formula_1 be a fixed commutative ring with unit. In most applications this is a field, but this is not needed for the definitions. Let also formula_2 be a formula_1-algebra.\n\nA \"cell datum\" for formula_2 is a tuple formula_5 consisting of\nand satisfying the following conditions:\n\nThis definition was originally given by Graham and Lehrer who invented cellular algebras.\n\nLet formula_8 be an anti automorphism of formula_1-algebras with formula_34 (just called \"involution\" from now on).\n\nA \"cell ideal\" of formula_2 w.r.t. formula_36 is a two-sided ideal formula_37 such that the following conditions hold:\n\nA \"cell chain\" for formula_2 w.r.t. formula_36 is defined as a direct decomposition\ninto free formula_1-submodules such that\n\nNow formula_56 is called a cellular algebra if it has a cell chain. One can show that the two definitions are equivalent. Every basis gives rise to cell chains (one for each topological ordering of formula_6) and choosing a basis of every left ideal formula_58 one can construct a corresponding cell basis for formula_2.\n\nformula_60 is cellular. A cell datum is given by formula_61 and\n\nA cell-chain in the sense of the second, abstract definition is given by\n\nformula_66 is cellular. A cell datum is given by formula_67 and\n\nA cell-chain (and in fact the only cell chain) is given by\n\nIn some sense all cellular algebras \"interpolate\" between these two extremes by arranging matrix-algebra-like pieces according to the poset formula_6.\n\nModulo minor technicalities all Iwahori–Hecke algebras of finite type are cellular w.r.t. to the involution that maps the standard basis as formula_74. This includes for example the integral group algebra of the symmetric groups as well as all other finite Weyl groups.\n\nA basic Brauer tree algebra over a field is cellular if and only if the Brauer tree is a straight line (with arbitrary number of exceptional vertices).\n\nFurther examples include q-Schur algebras, the Brauer algebra, the Temperley–Lieb algebra, the Birman–Murakami–Wenzl algebra, the blocks of the Bernstein–Gelfand–Gelfand category formula_75 of a semisimple Lie algebra.\n\nAssume formula_2 is cellular and formula_5 is a cell datum for formula_2. Then one defines the \"cell module\" formula_79 as the free formula_1-module with basis formula_81 and multiplication\nwhere the coefficients formula_83 are the same as above. Then formula_79 becomes an formula_2-left module.\n\nThese modules generalize the Specht modules for the symmetric group and the Hecke-algebras of type A.\n\nThere is a canonical bilinear form formula_86 which satisfies\nfor all indices formula_88.\n\nOne can check that formula_89 is symmetric in the sense that\nfor all formula_91 and also formula_2-invariant in the sense that\nfor all formula_22,formula_91.\n\nAssume for the rest of this section that the ring formula_1 is a field. With the information contained in the invariant bilinear forms one can easily list all simple formula_2-modules:\n\nLet formula_98 and define formula_99 for all formula_100. Then all formula_101 are absolute simple formula_2-modules and every simple formula_2-module is one of these.\n\nThese theorems appear already in the original paper by Graham and Lehrer.\n\n\nIf formula_1 is an integral domain then there is a converse to this last point:\n\nIf one further assumes formula_1 to be a local domain, then additionally the following holds:\n\nAssuming that formula_1 is a field (though a lot of this can be generalized to arbitrary rings, integral domains, local rings or at least discrete valuation rings) and formula_2 is cellular w.r.t. to the involution formula_36. Then the following hold \n"}
{"id": "5007616", "url": "https://en.wikipedia.org/wiki?curid=5007616", "title": "Conical coordinates", "text": "Conical coordinates\n\nConical coordinates are a three-dimensional orthogonal coordinate system consisting of \nconcentric spheres (described by their radius ) and by two families of perpendicular cones, aligned along the - and -axes, respectively.\n\nThe conical coordinates formula_1 are defined by\n\nwith the following limitations on the coordinates\n\nSurfaces of constant are spheres of that radius centered on the origin\n\nwhereas surfaces of constant formula_7 and formula_8 are mutually perpendicular cones\n\nand\n\nIn this coordinate system, both Laplace's equation and the Helmholtz equation are separable.\n\nThe scale factor for the radius is one (), as in spherical coordinates. The scale factors for the two conical coordinates are \n\nand\n\nAn alternative set of (non-orthogonal) conical coordinates have been derived\n\nformula_13 \n\nwhere formula_14 are spherical polar coordinates. The corresponding inverse relations are\n\nformula_15\n\nThe infinitesimal Euclidean distance between two points in these coordinates \n\nformula_17 and formula_18 are orthogonal coordinates on the surface of the cone given by formula_19.\nIf the path between any two points is constrained to this surface, then the geodesic distance between any two points\n\nformula_20 and formula_21 is \n\nformula_22\n\n\n"}
{"id": "50319816", "url": "https://en.wikipedia.org/wiki?curid=50319816", "title": "Continuous module", "text": "Continuous module\n\nIn mathematics, a continuous module is a module \"M\" such that every submodule of \"M\" is essential in a direct summand and every submodule of \"M\" isomorphic to a direct summand is itself a direct summand. The endomorphism ring of a continuous module is a clean ring.\n"}
{"id": "9638015", "url": "https://en.wikipedia.org/wiki?curid=9638015", "title": "David Fowler (mathematician)", "text": "David Fowler (mathematician)\n\nDavid Herbert Fowler (28 April 1937 – 13 April 2004) was a historian of Greek mathematics who published work on pre-Eudoxian ratio theory (using the process he called anthyphairesis). He disputed the standard story of\nGreek mathematical discovery, in which the discovery of the phenomenon of incommensurability came as a shock.\n\nFowler was also the translator of René Thom's book \"Structural Stability and Morphogenesis\" from French (\"Stabilité strukturelle et morphogénèse\") into English. \n\n\n"}
{"id": "34709316", "url": "https://en.wikipedia.org/wiki?curid=34709316", "title": "Denjoy–Young–Saks theorem", "text": "Denjoy–Young–Saks theorem\n\nIn mathematics, the Denjoy–Young–Saks theorem gives some possibilities for the Dini derivatives of a function that hold almost everywhere.\n\nIf \"f\" is a real valued function defined on an interval, then with the possible exception of a set of measure 0 on the interval, the Dini derivatives of \"f\" satisfy one of the following four conditions at each point:\n\n\n"}
{"id": "16187387", "url": "https://en.wikipedia.org/wiki?curid=16187387", "title": "Derivation of the Routh array", "text": "Derivation of the Routh array\n\nThe Routh array is a tabular method permitting one to establish the stability of a system using only the coefficients of the characteristic polynomial. Central to the field of control systems design, the Routh–Hurwitz theorem and Routh array emerge by using the Euclidean algorithm and Sturm's theorem in evaluating Cauchy indices. \n\nGiven the system:\nAssuming no roots of formula_2 lie on the imaginary axis, and letting\nthen we have\nExpressing formula_8 in polar form, we have\nwhere\nand \nfrom (2) note that\nwhere\nNow if the i root of formula_2 has a positive real part, then ()\nand\nSimilarly, if the i root of formula_17 has a negative real part,\nand\nTherefore, formula_20 when the i root of formula_8 has a positive real part, and formula_22 when the i root of formula_8 has a negative real part. Alternatively,\nand\nSo, if we define\nthen we have the relationship\nand combining (3) and (16) gives us\nTherefore, given an equation of formula_8 of degree formula_31 we need only evaluate this function formula_32 to determine formula_3, the number of roots with negative real parts and formula_5, the number of roots with positive real parts.\n\nEquations (13) and (14) show that at formula_35, formula_36 is an integer multiple of formula_37. Note now, in accordance with (6) and Figure 1, the graph of formula_38 vs formula_39, that varying formula_40 over an interval (a,b) where formula_41 and formula_42 are integer multiples of formula_43, this variation causing the function formula_44 to have increased by formula_43, indicates that in the course of travelling from point a to point b, formula_39 has \"jumped\" from formula_47 to formula_48 one more time than it has jumped from formula_48 to formula_47. Similarly, if we vary formula_40 over an interval (a,b) this variation causing formula_44 to have decreased by formula_43, where again formula_39 is a multiple of formula_43 at both formula_56 and formula_57, implies that formula_58 has jumped from formula_48 to formula_47 one more time than it has jumped from formula_47 to formula_48 as formula_40 was varied over the said interval.\nThus, formula_64 is formula_43 times the difference between the number of points at which formula_66 jumps from formula_48 to formula_47 and the number of points at which formula_66 jumps from formula_47 to formula_48 as formula_40 ranges over the interval formula_73 provided that at formula_74, formula_75 is defined.\n\nIn the case where the starting point is on an incongruity (i.e. formula_76, \"i\" = 0, 1, 2, ...) the ending point will be on an incongruity as well, by equation (16) (since formula_3 is an integer and formula_5 is an integer, formula_32 will be an integer). In this case, we can achieve this same index (difference in positive and negative jumps) by shifting the axes of the tangent function by formula_37, through adding formula_37 to formula_39. Thus, our index is now fully defined for any combination of coefficients in formula_8 by evaluating formula_84 over the interval (a,b) = formula_85 when our starting (and thus ending) point is not an incongruity, and by evaluating\nover said interval when our starting point is at an incongruity.\nThis difference, formula_32, of negative and positive jumping incongruities encountered while traversing formula_40 from formula_89 to formula_90 is called the Cauchy Index of the tangent of the phase angle, the phase angle being formula_44 or formula_92, depending as formula_93 is an integer multiple of formula_43 or not.\n\nTo derive Routh's criterion, first we'll use a different notation to differentiate between the even and odd terms of formula_8:\nNow we have: \nTherefore, if formula_31 is even, \nand if formula_31 is odd:\nNow observe that if formula_31 is an odd integer, then by (3) formula_103 is odd. If formula_103 is an odd integer, then formula_105 is odd as well. Similarly, this same argument shows that when formula_31 is even, formula_105 will be even. Equation (13) shows that if formula_105 is even, formula_39 is an integer multiple of formula_43. Therefore, formula_38 is defined for formula_31 even, and is thus the proper index to use when n is even, and similarly formula_113 is defined for formula_31 odd, making it the proper index in this latter case.\nThus, from (6) and (22), for formula_31 even:\nand from (18) and (23), for formula_31 odd:\nLo and behold we are evaluating the same Cauchy index for both:\nformula_119\n\nSturm gives us a method for evaluating formula_120. His theorem states as follows:\nGiven a sequence of polynomials formula_121 where:\n1) If formula_122 then formula_123, formula_124, and formula_125\n2) formula_126 for formula_127\nand we define formula_128 as the number of changes of sign in the sequence formula_121 for a fixed value of formula_40, then:\nA sequence satisfying these requirements is obtained using the Euclidean algorithm, which is as follows:\nStarting with formula_132 and formula_133, and denoting the remainder of formula_134 by formula_135 and similarly denoting the remainder of formula_136 by formula_137, and so on, we obtain the relationships:\nor in general \nwhere the last non-zero remainder, formula_140 will therefore be the highest common factor of formula_141. It can be observed that the sequence so constructed will satisfy the conditions of Sturm's theorem, and thus an algorithm for determining the stated index has been developed.\nIt is in applying Sturm's theorem (28) to (26), through the use of the Euclidean algorithm above that the Routh matrix is formed.\nWe get\nand identifying the coefficients of this remainder by formula_143, formula_144, formula_145, formula_146, and so forth, makes our formed remainder \nwhere\nContinuing with the Euclidean algorithm on these new coefficients gives us\nwhere we again denote the coefficients of the remainder formula_150 by formula_151, formula_152, formula_153, formula_154,\nmaking our formed remainder \nand giving us\nThe rows of the Routh array are determined exactly by this algorithm when applied to the coefficients of (19). An observation worthy of note is that in the regular case the polynomials formula_157 and formula_158 have as the highest common factor formula_159 and thus there will be formula_31 polynomials in the chain formula_121.\nNote now, that in determining the signs of the members of the sequence of polynomials formula_162 that at formula_163 the dominating power of formula_164 will be the first term of each of these polynomials, and thus only these coefficients corresponding to the highest powers of formula_164 in formula_166, and formula_140, which are formula_168, formula_169, formula_143, formula_151, ... determine the signs of formula_132, formula_133, ..., formula_140 at formula_175.\nSo we get formula_176 that is, formula_177 is the number of changes of sign in the sequence formula_178, formula_179, formula_180, ... which is the number of sign changes in the sequence formula_168, formula_169, formula_143, formula_151, ... and formula_185; that is formula_186 is the number of changes of sign in the sequence formula_187, formula_188, formula_189, ... which is the number of sign changes in the sequence formula_168, formula_191, formula_143, formula_193, ... \nSince our chain formula_168, formula_169, formula_143, formula_151, ... will have formula_31 members it is clear that formula_199 since within formula_200 if going from formula_168 to formula_169 a sign change has not occurred, within \nformula_203 going from formula_168 to formula_191 one has, and likewise for all formula_31 transitions (there will be no terms equal to zero) giving us formula_31 total sign changes.\nAs formula_208 and formula_209, and from (17) formula_210, we have that formula_211 and have derived Routh's theorem -\nThe number of roots of a real polynomial formula_212 which lie in the right half plane formula_213 is equal to the number of changes of sign in the first column of the Routh scheme.\nAnd for the stable case where formula_214 then formula_215 by which we have Routh's famous criterion:\nIn order for all the roots of the polynomial formula_212 to have negative real parts, it is necessary and sufficient that all of the elements in the first column of the Routh scheme be different from zero and of the same sign.\n\n"}
{"id": "49447339", "url": "https://en.wikipedia.org/wiki?curid=49447339", "title": "Effect algebra", "text": "Effect algebra\n\nEffect algebras are algebraic structures of a kind introduced by D. Foulis and M. Bennett to serve as a framework for unsharp measurements in quantum mechanics.\n\nAn effect algebra consists of an underlying set \"A\" equipped with a partial binary operation ⊞, a unary operation (−), and two special elements 0, 1 such that the following relationships hold:\n\n\nEvery effect algebra carries a natural order: define \"a\" ≤ \"b\" if and only if there exists an element \"c\" such that exists and is equal to \"b\". The defining axioms of effect algebras guarantee that ≤ is a partial order.\n"}
{"id": "17327394", "url": "https://en.wikipedia.org/wiki?curid=17327394", "title": "Emmy Noether bibliography", "text": "Emmy Noether bibliography\n\nEmmy Noether was a German mathematician. This article lists the publications upon which her reputation is built (in part).\n\nIn the second epoch, Noether turned her attention to the theory of rings. With her paper \"Moduln in nichtkommutativen Bereichen, insbesondere aus Differential- und Differenzenausdrücken\", Hermann Weyl states, \"It is here for the first time that the Emmy Noether appears whom we all know, and who changed the face of algebra by her work.\"\n\nIn the third epoch, Emmy Noether focused on non-commutative algebras, and unified much earlier work on the representation theory of groups.\n\n\n"}
{"id": "31819967", "url": "https://en.wikipedia.org/wiki?curid=31819967", "title": "Garnir relations", "text": "Garnir relations\n\nIn mathematics, the Garnir relations give a way of expressing a basis of the Specht modules \"V\" in terms of standard polytabloids.\n\nGiven a partition λ of \"n\", one has the Specht module \"V\". In characteristic 0, this is an irreducible representation of the symmetric group \"S\". One can construct \"V\" explicitly in terms of polytabloids as follows:\n\nThe above construction gives an explicit description of the Specht module \"V\". However, the polytabloids associated to different Young tableaux are not necessarily linearly independent, indeed, the dimension of \"V\" is exactly the number of standard Young tableaux of shape λ. In fact, the polytabloids associated to standard Young tableaux span \"V\"; to express other polytabloids in terms of them, one uses a straightening algorithm.\n\nGiven a Young tableau \"S\", we construct the polytabloid \"e\" as above. Without loss of generality, all columns of \"S\" are increasing, otherwise we could instead start with the modified Young tableau with increasing columns, whose polytabloid will differ at most by a sign. \"S\" is then said to not have any \"column descents\". We want to express \"e\" as a linear combination of standard polytabloids, i.e. polytabloids associated to standard Young tableaux. To do this, we would like permutations π such that in all tableaux \"S\"π, a row descent has been eliminated, with formula_6. This then expresses \"S\" in terms of polytabloids that are closer to being standard. The permutations that achieve this are the Garnir elements.\n\nSuppose we want to eliminate a row descent in the Young tableau \"T\". We pick two subsets \"A\" and \"B\" of the boxes of \"T\" as in the following diagram:\n\nThen the Garnir element formula_7 is defined to be formula_8, where the π are the permutations of the entries of the boxes of \"A\" and \"B\" that keep both subsets \"A\" and \"B\" without column descents.\n\nConsider the following Young tableau:\n\nThere is a row descent in the second row, so we choose the subsets \"A\" and \"B\" as indicated, which gives us the following:\n\nThis gives us the Garnir element formula_9. This allows us to remove the row descent in the second row, but this has also introduced other descents in other places. But there is a way in which all tableaux obtained like this are closer to being standard, this is measured by a \"dominance order\" on polytabloids. Therefore, one can repeatedly apply this procedure to \"straighten\" a polytabloid, eventually writing it as a linear combination of standard polytabloids, showing that the Specht module is spanned by the standard polytabloids. As they are also linearly independent, they form a basis of this module.\n\nThere is a similar description for the irreducible representations of \"GL\". In that case, one can consider the Weyl modules associated to a partition λ, which can be described in terms of bideterminants. One has a similar straightening algorithm, but this time in terms of semistandard Young tableaux.\n\n"}
{"id": "4695978", "url": "https://en.wikipedia.org/wiki?curid=4695978", "title": "George Wollaston", "text": "George Wollaston\n\nGeorge Wollaston (1738–1826) was an English Anglican priest. He was elected a Fellow of the Royal Society in 1763.\n\nHe was the son of Francis Wollaston (1694-1774). He was educated at Charterhouse School and Sidney Sussex College, Cambridge, where he graduated second wrangler in 1758. He married in 1765 Elizabeth Palmer of Thurnscoe Hall and they had a single daughter Elizabeth Palmer Wollaston who died in infancy (17 April 1766).\n"}
{"id": "202693", "url": "https://en.wikipedia.org/wiki?curid=202693", "title": "Gongsun Long", "text": "Gongsun Long\n\nGongsun Long (, BC) was a member of the School of Names (Logicians) of ancient Chinese philosophy. He also ran a school and enjoyed the support of rulers, and advocated peaceful means of resolving disputes in contrast to the wars which were common in the Warring States period. However, little is known about the particulars of his life, and furthermore many of his writings have been lost. All of his essays — fourteen originally but only six extant — are included in the anthology the Gongsun Longzi ().\n\nIn Book 17 of the \"Zhuangzi\" anthology, Gongsun thus speaks of himself:\n\nWhen young, I studied the way of the former kings. When I grew up, I understood the practice of kindness and duty. I united the same and different, separated hard from white, made so the not-so and admissible the inadmissible. I confounded the wits of the hundred schools and exhausted the eloquence of countless speakers. I took myself to have reached the ultimate.\n\nHe is best known for a series of paradoxes in the tradition of Hui Shi, including \"White horses are not horses,\" \"When no thing is not the pointed-out, to point out is not to point out,\" and \"There is no 1 in 2.\" These paradoxes seem to suggest a similarity to the discovery in Greek philosophy that pure logic may lead to apparently absurd conclusions.\n\nIn the \"White Horse Dialogue\" (), one interlocutor (sometimes called the \"sophist\") defends the truth of the statement \"White horses are not horses,\" while the other interlocutor (sometimes called the \"objector\") disputes the truth of this statement. This has been interpreted in a number of ways.\n\nPossibly the simplest interpretation is to see it as based on a confusion of class and identity. The argument, by this interpretation, plays upon an ambiguity in Chinese that doesn't exist in English. The expression \"X is not Y\" (X非Y) can mean either\n\nThe sentence \"White horses are not horses\" would normally be taken to assert the obviously false claim that white horses are not part of the group of horses. However, the \"sophist\" in the dialogue defends the statement under the interpretation, \"Not all horses are white horses\". The latter statement is actually true, since — as the \"sophist\" explains — \"horses\" includes horses that are white, yellow, brown, etc., while \"white horses\" includes only white horses, and excludes the others. A.C. Graham proposed this interpretation and illustrated it with an analogy. The \"Objector\" assumes that \"a white horse is not a horse\" is parallel to \"a sword is not a weapon,\" but the \"Sophist\" is treating the statement as parallel to \"a sword is not a blade.\" Other interpretations have been put forward by Fung Yu-lan and Chad Hansen among others.\n\nThis work has been viewed by some as a serious logical discourse, by others as a facetious work of sophistry, and finally by some as a combination of the two.\n\nHe was also responsible for several other essays (), as short as 300 characters.\n\n\n"}
{"id": "4572220", "url": "https://en.wikipedia.org/wiki?curid=4572220", "title": "Grant Olney", "text": "Grant Olney\n\nGrant Olney Passmore (born October 18, 1983) is a singer/songwriter who has recorded on the Asian Man Records label. He is considered part of the New Weird America movement along with David Dondero, Devendra Banhart, Bright Eyes, and CocoRosie. His latest full-length album, Hypnosis for Happiness, was released in July 2013 on the Friendly Police UK label. His previous full-length album, Brokedown Gospel, was released on the Asian Man Records label in July 2004. He also releases music under the pseudonym Scout You Devil and as part of the songwriting duo Olney Clark.\n\nAlongside his music, Passmore is also a mathematician and theoretical computer scientist, formerly a student at the University of Texas at Austin, the Mathematical Research Institute in the Netherlands, and the University of Edinburgh, where he earned his Ph.D. He is a Life Member of Clare Hall, University of Cambridge and is cofounder of the artificial intelligence company Aesthetic Integration which produces technology for the formal verification of algorithms. He was paired with artist Hito Steyerl in the 2016 Rhizome Seven on Seven.\n\nAs a young child and early teenager, Passmore was involved in the development of the online Bulletin Board system scene, and under the name skaboy he was the author of many applications of importance to the Bulletin Board System community, including the Infusion Bulletin Board System, Empathy Image Editor, Avenger Packer Pro, and Impulse Tracker Tosser. Passmore was head programmer for ACiD Productions while working on many of these applications.\n\nPassmore married Barbara Galletly in 2014. They have two children.\n\nAlbums\n\nCompilations\n\n"}
{"id": "52651586", "url": "https://en.wikipedia.org/wiki?curid=52651586", "title": "Growth function", "text": "Growth function\n\nThe growth function, also called the shatter coefficient or the shattering number, measures the richness of a set family. It is especially used in the context of statistical learning theory, where it measures the complexity of a hypothesis class.\nThe term 'growth function' was coined by Vapnik and Chervonenkis in their 1968 paper, where they also proved many of its properties.\nIt is a basic concept in machine learning.\nLet formula_1 be a set family (a set of sets) and formula_2 a set. Their \"intersection\" is defined as the following set-family:\nThe \"intersection-size\" (also called the \"index\") of formula_1 with respect to formula_2\" is formula_6. Obviously, if a set formula_7 has formula_8 elements then the index is at most formula_9.\n\nThe growth function measures the size of formula_10 as a function of formula_11. Formally:\n\nEquivalently, let formula_1 be a hypothesis-class (a set of binary functions) and formula_2 a set with formula_8 elements. The \"restriction\" of formula_1 to formula_2 is the set of binary functions on formula_2 that can be derived from formula_1:\nThe growth function measures the size of formula_21 as a function of formula_11:\n\n1. The domain is the real line formula_24. \nThe set-family formula_1 contains all the half-lines (rays) from a given number to positive infinity, i.e., all sets of the form formula_26 for some formula_27. \nFor any set formula_2 of formula_8 real numbers, the intersection formula_10 contains formula_31 sets: the empty set, the set containing the largest element of formula_2, the set containing the two largest elements of formula_2, and so on. Therefore: formula_34. The same is true whether formula_1 contains open half-lines, closed half-lines, or both.\n\n2. The domain is the segment formula_36. \nThe set-family formula_1 contains all the open sets. \nFor any set formula_2 of formula_8 real numbers, the intersection formula_10 contains all possible subsets of formula_2. There are formula_9 such subsets, so formula_43.\n3. The domain is the Euclidean space formula_44. \nThe set-family formula_1 contains all the half-spaces of the form: formula_46, where formula_47 is a fixed vector.\nThen formula_48,\nwhere Comp is the number of number of components in a partitioning of an n-dimensional space by m hyperplanes.\n\n4. The domain is the real line formula_24. The set-family formula_1 contains all the real intervals, i.e., all sets of the form formula_51 for some formula_52. For any set formula_2 of formula_8 real numbers, the intersection formula_10 contains all runs of between 0 and formula_8 consecutive elements of formula_2. The number of such runs is formula_58, so formula_59.\n\nThe main property that makes the growth function interesting is that it can be either polynomial or exponential - nothing in-between. \n\nThe following is a property of the intersection-size:\n\nThis implies the following property of the Growth function.\nFor every family formula_1 there are two cases:\n\nFor any finite formula_1:\nsince for every formula_2, the number of elements in formula_10 is at most formula_77. Therefore, the growth function is mainly interesting when formula_1 is infinite.\n\nFor any nonempty formula_1:\nI.e, the growth function has an exponential upper-bound.\n\nWe say that a set-family formula_1 shatters a set formula_2 if their intersection contains all possible subsets of formula_2, i.e. formula_84.\nIf formula_1 shatters formula_2 of size formula_8, then formula_88, which is the upper bound.\n\nDefine the Cartesian intersection of two set-families as:\nThen:\n\nFor every two set-families:\n\nThe VC dimension of formula_1 is defined according to these two cases:\n\nSo formula_97 if-and-only-if formula_98.\n\nThe growth function can be regarded as a refinement of the concept of VC dimension. The VC dimension only tells us whether formula_99 is equal to or smaller than formula_100, while the growth function tells us exactly how formula_69 changes as a function of formula_8.\n\nAnother connection between the growth function and the VC dimension is given by the Sauer–Shelah lemma:\nIn particular, \nThis upper bound is tight, i.e, for all formula_109 there exists formula_1 with VC dimension formula_94 such that:\n\nWhile the growth-function is related to the \"maximum\" intersection-size,\nthe entropy is related to the \"average\" intersection size:\nThe intersection-size has the following property. For every set-family formula_1:\nHence:\nMoreover, the sequence formula_117 converges to a constant formula_118 when formula_119.\n\nMoreover, the random-variable formula_120 is concentrated near formula_121.\n\nLet formula_122 be a set on which a probability measure formula_123 is defined. \nLet formula_1 be family of subsets of formula_122 (= a family of events).\n\nSuppose we choose a set formula_7 that contains formula_8 elements of formula_122,\nwhere each element is chosen at random according to the probability measure formula_129, independently of the others (i.e, with replacements). For each event formula_130, we compare the following two quantities:\nWe are interested in the difference, formula_134. This difference satisfies the following upper bound:\nwhich is equivalent to:\nIn words: the probability that for \"all\" events in formula_1, the relative-frequency is near the probability, is lower-bounded by an expression that depends on the growth-function of formula_1.\n\nA corollary of this is that, iff the growth function is polynomial in formula_8 (i.e, there exists some formula_65 such that formula_141), then the above probability approaches 1 as formula_119. I.e, the family formula_1 enjoys uniform convergence in probability.\n"}
{"id": "30890995", "url": "https://en.wikipedia.org/wiki?curid=30890995", "title": "History of numerical weather prediction", "text": "History of numerical weather prediction\n\nThe history of numerical weather prediction considers how current weather conditions as input into mathematical models of the atmosphere and oceans to predict the weather and future sea state (the process of numerical weather prediction) has changed over the years. Though first attempted manually in the 1920s, it was not until the advent of the computer and computer simulation that computation time was reduced to less than the forecast period itself. ENIAC was used to create the first forecasts via computer in 1950, and over the years more powerful computers have been used to increase the size of initial datasets as well as include more complicated versions of the equations of motion. The development of global forecasting models led to the first climate models. The development of limited area (regional) models facilitated advances in forecasting the tracks of tropical cyclone as well as air quality in the 1970s and 1980s.\n\nBecause the output of forecast models based on atmospheric dynamics requires corrections near ground level, model output statistics (MOS) were developed in the 1970s and 1980s for individual \"forecast points\" (locations). The MOS apply statistical techniques to post-process the output of dynamical models with the most recent surface observations and the forecast point's climatology. This technique can correct for model resolution as well as model biases. Even with the increasing power of supercomputers, the forecast skill of numerical weather models only extends to about two weeks into the future, since the density and quality of observations—together with the chaotic nature of the partial differential equations used to calculate the forecast—introduce errors which double every five days. The use of model ensemble forecasts since the 1990s helps to define the forecast uncertainty and extend weather forecasting farther into the future than otherwise possible.\n\nUntil the end of the 19th century, weather prediction was entirely subjective and based on empirical rules, with only limited understanding of the physical mechanisms behind weather processes. In 1901 Cleveland Abbe, founder of the United States Weather Bureau, proposed that the atmosphere is governed by the same principles of thermodynamics and hydrodynamics that were studied in the previous century. In 1904, Vilhelm Bjerknes derived a two-step procedure for model-based weather forecasting. First, a \"diagnostic step\" is used to process data to generate initial conditions, which are then advanced in time by a \"prognostic step\" that solves the initial value problem. He also identified seven variables that defined the state of the atmosphere at a given point: pressure, temperature, density, humidity, and the three components of the flow velocity vector. Bjerknes pointed out that equations based on mass continuity, conservation of momentum, the first and second laws of thermodynamics, and the ideal gas law could be used to estimate the state of the atmosphere in the future through numerical methods. With the exception of the second law of thermodynamics, these equations form the basis of the primitive equations used in present-day weather models.\n\nIn 1922, Lewis Fry Richardson published the first attempt at forecasting the weather numerically. Using a hydrostatic variation of Bjerknes's primitive equations, Richardson produced by hand a 6-hour forecast for the state of the atmosphere over two points in central Europe, taking at least six weeks to do so. His forecast calculated that the change in surface pressure would be , an unrealistic value incorrect by two orders of magnitude. The large error was caused by an imbalance in the pressure and wind velocity fields used as the initial conditions in his analysis.\n\nThe first successful numerical prediction was performed using the ENIAC digital computer in 1950 by a team composed of American meteorologists Jule Charney, Philip Thompson, Larry Gates, and Norwegian meteorologist Ragnar Fjørtoft, applied mathematician John von Neumann, and computer programmer Klara Dan von Neumann. They used a simplified form of atmospheric dynamics based on solving the barotropic vorticity equation over a single layer of the atmosphere, by computing the geopotential height of the atmosphere's pressure surface. This simplification greatly reduced demands on computer time and memory, so the computations could be performed on the relatively primitive computers of the day. When news of the first weather forecast by ENIAC was received by Richardson in 1950, he remarked that the results were an \"enormous scientific advance.\" The first calculations for a 24‑hour forecast took ENIAC nearly 24 hours to produce, but Charney's group noted that most of that time was spent in \"manual operations\", and expressed hope that forecasts of the weather before it occurs would soon be realized.\nIn September 1954, Carl-Gustav Rossby assembled an international group of meteorologists in Stockholm and produced the first operational forecast (i.e. routine predictions for practical use) based on the barotropic equation. Operational numerical weather prediction in the United States began in 1955 under the Joint Numerical Weather Prediction Unit (JNWPU), a joint project by the U.S. Air Force, Navy, and Weather Bureau. The JNWPU model was originally a three-layer barotropic model, also developed by Charney. It only modeled the atmosphere in the Northern Hemisphere. In 1956, the JNWPU switched to a two-layer thermotropic model developed by Thompson and Gates. The main assumption made by the thermotropic model is that while the magnitude of the thermal wind may change, its direction does not change with respect to height, and thus the baroclinicity in the atmosphere can be simulated using the and geopotential height surfaces and the average thermal wind between them. However, due to the low skill showed by the thermotropic model, the JNWPU reverted to the single-layer barotropic model in 1958. The Japanese Meteorological Agency became the third organization to initiate operational numerical weather prediction in 1959. The first real-time forecasts made by Australia's Bureau of Meteorology in 1969 for portions of the Southern Hemisphere were also based on the single-layer barotropic model.\n\nLater models used more complete equations for atmospheric dynamics and thermodynamics. In 1959, Karl-Heinz Hinkelmann produced the first reasonable primitive equation forecast, 37 years after Richardson's failed attempt. Hinkelmann did so by removing small oscillations from the numerical model during initialization. In 1966, West Germany and the United States began producing operational forecasts based on primitive-equation models, followed by the United Kingdom in 1972 and Australia in 1977. Later additions to primitive equation models allowed additional insight into different weather phenomena. In the United States, solar radiation effects were added to the primitive equation model in 1967; moisture effects and latent heat were added in 1968; and feedback effects from rain on convection were incorporated in 1971. Three years later, the first global forecast model was introduced. Sea ice began to be initialized in forecast models in 1971. Efforts to involve sea surface temperature in model initialization began in 1972 due to its role in modulating weather in higher latitudes of the Pacific.\n\nA global forecast model is a weather forecasting model which initializes and forecasts the weather throughout the Earth's troposphere. It is a computer program that produces meteorological information for future times at given locations and altitudes. Within any modern model is a set of equations, known as the primitive equations, used to predict the future state of the atmosphere. These equations—along with the ideal gas law—are used to evolve the density, pressure, and potential temperature scalar fields and the flow velocity vector field of the atmosphere through time. Additional transport equations for pollutants and other aerosols are included in some primitive-equation high-resolution models as well. The equations used are nonlinear partial differential equations which are impossible to solve exactly through analytical methods, with the exception of a few idealized cases. Therefore, numerical methods obtain approximate solutions. Different models use different solution methods: some global models and almost all regional models use finite difference methods for all three spatial dimensions, while other global models and a few regional models use spectral methods for the horizontal dimensions and finite-difference methods in the vertical.\n\nThe National Meteorological Center's Global Spectral Model was introduced during August 1980. The European Centre for Medium-Range Weather Forecasts model debuted on May 1, 1985. The United Kingdom Met Office has been running their global model since the late 1980s, adding a 3D-Var data assimilation scheme in mid-1999. The Canadian Meteorological Centre has been running a global model since 1991. The United States ran the Nested Grid Model (NGM) from 1987 to 2000, with some features lasting as late as 2009. Between 2000 and 2002, the Environmental Modeling Center ran the Aviation (AVN) model for shorter range forecasts and the Medium Range Forecast (MRF) model at longer time ranges. During this time, the AVN model was extended to the end of the forecast period, eliminating the need of the MRF and thereby replacing it. In late 2002, the AVN model was renamed the Global Forecast System (GFS). The German Weather Service has been running their global hydrostatic model, the GME, using a hexagonal icosahedral grid since 2002. The GFS is slated to eventually be supplanted by the Flow-following, finite-volume Icosahedral Model (FIM), which like the GME is gridded on a truncated icosahedron, in the mid-2010s.\n\nIn 1956, Norman A. Phillips developed a mathematical model which could realistically depict monthly and seasonal patterns in the troposphere, which became the first successful climate model. Following Phillips's work, several groups began working to create general circulation models. The first general circulation climate model that combined both oceanic and atmospheric processes was developed in the late 1960s at the NOAA Geophysical Fluid Dynamics Laboratory. By the early 1980s, the United States' National Center for Atmospheric Research had developed the Community Atmosphere Model; this model has been continuously refined into the 2000s. In 1986, efforts began to initialize and model soil and vegetation types, which led to more realistic forecasts. For example, the Center for Ocean-Land Atmosphere Studies (COLA) model showed a warm temperature bias of 2-4 °C (4-7 °F) and a low precipitation bias due to incorrect parameterization of crop and vegetation type across the central United States. Coupled ocean-atmosphere climate models such as the Hadley Centre for Climate Prediction and Research's HadCM3 model are currently being used as inputs for climate change studies. The importance of gravity waves was neglected within these models until the mid-1980s. Now, gravity waves are required within global climate models in order to properly simulate regional and global scale circulations, though their broad spectrum makes their incorporation complicated. The Climate System Model (CSM) was developed at the National Center for Atmospheric Research in January 1994.\n\nThe horizontal domain of a model is either \"global\", covering the entire Earth, or \"regional\", covering only part of the Earth. Regional models (also known as \"limited-area\" models, or LAMs) allow for the use of finer (or smaller) grid spacing than global models. The available computational resources are focused on a specific area instead of being spread over the globe. This allows regional models to resolve explicitly smaller-scale meteorological phenomena that cannot be represented on the coarser grid of a global model. Regional models use a global model for initial conditions of the edge of their domain in order to allow systems from outside the regional model domain to move into its area. Uncertainty and errors within regional models are introduced by the global model used for the boundary conditions of the edge of the regional model, as well as errors attributable to the regional model itself.\n\nIn the United States, the first operational regional model, the limited-area fine-mesh (LFM) model, was introduced in 1971. Its development was halted, or frozen, in 1986. The NGM debuted in 1987 and was also used to create model output statistics for the United States. Its development was frozen in 1991. The ETA model was implemented for the United States in 1993 and in turn was upgraded to the NAM in 2006. The U.S. also offers the Rapid Refresh (which replaced the RUC in 2012) for short-range and high-resolution applications; both the Rapid Refresh and NAM are built on the same framework, the WRF. Metéo France has been running their Action de Recherche Petite Échelle Grande Échelle (ALADIN) mesoscale model for France, based upon the ECMWF global model, since 1995. In July 1996, the Bureau of Meteorology implemented the Limited Area Prediction System (LAPS). The Canadian Regional Finite-Elements model (RFE) went into operational use on April 22, 1986. It was followed by the Canadian Global Environmental Multiscale Model (GEM) mesoscale model on February 24, 1997.\n\nThe German Weather Service developed the High Resolution Regional Model (HRM) in 1999, which is widely run within the operational and research meteorological communities and run with hydrostatic assumptions. The Antarctic Mesoscale Prediction System (AMPS) was developed for the southernmost continent in 2000 by the United States Antarctic Program. The German non-hydrostatic Lokal-Modell for Europe (LME) has been run since 2002, and an increase in areal domain became operational on September 28, 2005. The Japanese Meteorological Agency has run a high-resolution, non-hydrostatic mesoscale model since September 2004.\n\nThe technical literature on air pollution dispersion is quite extensive and dates back to the 1930s and earlier. One of the early air pollutant plume dispersion equations was derived by Bosanquet and Pearson. Their equation did not assume Gaussian distribution nor did it include the effect of ground reflection of the pollutant plume. Sir Graham Sutton derived an air pollutant plume dispersion equation in 1947 which did include the assumption of Gaussian distribution for the vertical and crosswind dispersion of the plume and also included the effect of ground reflection of the plume. Under the stimulus provided by the advent of stringent environmental control regulations, there was an immense growth in the use of air pollutant plume dispersion calculations between the late 1960s and today. A great many computer programs for calculating the dispersion of air pollutant emissions were developed during that period of time and they were called \"air dispersion models\". The basis for most of those models was the Complete Equation For Gaussian Dispersion Modeling Of Continuous, Buoyant Air Pollution Plumes The Gaussian air pollutant dispersion equation requires the input of \"H\" which is the pollutant plume's centerline height above ground level—and H is the sum of \"H\" (the actual physical height of the pollutant plume's emission source point) plus Δ\"H\" (the plume rise due the plume's buoyancy).\n\nTo determine Δ\"H\", many if not most of the air dispersion models developed between the late 1960s and the early 2000s used what are known as \"the Briggs equations.\" G. A. Briggs first published his plume rise observations and comparisons in 1965. In 1968, at a symposium sponsored by Conservation of Clean Air and Water in Europe, he compared many of the plume rise models then available in the literature. In that same year, Briggs also wrote the section of the publication edited by Slade dealing with the comparative analyses of plume rise models. That was followed in 1969 by his classical critical review of the entire plume rise literature, in which he proposed a set of plume rise equations which have become widely known as \"the Briggs equations\". Subsequently, Briggs modified his 1969 plume rise equations in 1971 and in 1972.\n\nThe Urban Airshed Model, a regional forecast model for the effects of air pollution and acid rain, was developed by a private company in the USA in 1970. Development of this model was taken over by the Environmental Protection Agency and improved in the mid to late 1970s using results from a regional air pollution study. While developed in California, this model was later used in other areas of North America, Europe and Asia during the 1980s. The Community Multiscale Air Quality model (CMAQ) is an open source air quality model run within the United States in conjunction with the NAM mesoscale model since 2004. The first operational air quality model in Canada, Canadian Hemispheric and Regional Ozone and NOx System (CHRONOS), began to be run in 2001. It was replaced with the Global Environmental Multiscale model - Modelling Air quality and Chemistry (GEM-MACH) model in November 2009.\n\nDuring 1972, the first model to forecast storm surge along the continental shelf was developed, known as the Special Program to List the Amplitude of Surges from Hurricanes (SPLASH). In 1978, the first hurricane-tracking model based on atmospheric dynamics – the movable fine-mesh (MFM) model – began operating. Within the field of tropical cyclone track forecasting, despite the ever-improving dynamical model guidance which occurred with increased computational power, it was not until the decade of the 1980s when numerical weather prediction showed skill, and until the 1990s when it consistently outperformed statistical or simple dynamical models. In the early 1980s, the assimilation of satellite-derived winds from water vapor, infrared, and visible satellite imagery was found to improve tropical cyclones track forecasting. The Geophysical Fluid Dynamics Laboratory (GFDL) hurricane model was used for research purposes between 1973 and the mid-1980s. Once it was determined that it could show skill in hurricane prediction, a multi-year transition transformed the research model into an operational model which could be used by the National Weather Service in 1995.\n\nThe Hurricane Weather Research and Forecasting (HWRF) model is a specialized version of the Weather Research and Forecasting (WRF) model and is used to forecast the track and intensity of tropical cyclones. The model was developed by the National Oceanic and Atmospheric Administration (NOAA), the U.S. Naval Research Laboratory, the University of Rhode Island, and Florida State University. It became operational in 2007. Despite improvements in track forecasting, predictions of the intensity of a tropical cyclone based on numerical weather prediction continue to be a challenge, since statiscal methods continue to show higher skill over dynamical guidance.\n\nThe first ocean wave models were developed in the 1960s and 1970s. These models had the tendency to overestimate the role of wind in wave development and underplayed wave interactions. A lack of knowledge concerning how waves interacted among each other, assumptions regarding a maximum wave height, and deficiencies in computer power limited the performance of the models. After experiments were performed in 1968, 1969, and 1973, wind input from the Earth's atmosphere was weighted more accurately in the predictions. A second generation of models was developed in the 1980s, but they could not realistically model swell nor depict wind-driven waves (also known as wind waves) caused by rapidly changing wind fields, such as those within tropical cyclones. This caused the development of a third generation of wave models from 1988 onward.\n\nWithin this third generation of models, the spectral wave transport equation is used to describe the change in wave spectrum over changing topography. It simulates wave generation, wave movement (propagation within a fluid), wave shoaling, refraction, energy transfer between waves, and wave dissipation. Since surface winds are the primary forcing mechanism in the spectral wave transport equation, ocean wave models use information produced by numerical weather prediction models as inputs to determine how much energy is transferred from the atmosphere into the layer at the surface of the ocean. Along with dissipation of energy through whitecaps and resonance between waves, surface winds from numerical weather models allow for more accurate predictions of the state of the sea surface.\n\nBecause forecast models based upon the equations for atmospheric dynamics do not perfectly determine weather conditions near the ground, statistical corrections were developed to attempt to resolve this problem. Statistical models were created based upon the three-dimensional fields produced by numerical weather models, surface observations, and the climatological conditions for specific locations. These statistical models are collectively referred to as model output statistics (MOS), and were developed by the National Weather Service for their suite of weather forecasting models by 1976. The United States Air Force developed its own set of MOS based upon their dynamical weather model by 1983.\n\nAs proposed by Edward Lorenz in 1963, it is impossible for long-range forecasts—those made more than two weeks in advance—to predict the state of the atmosphere with any degree of skill, owing to the chaotic nature of the fluid dynamics equations involved. Extremely small errors in temperature, winds, or other initial inputs given to numerical models will amplify and double every five days. Furthermore, existing observation networks have limited spatial and temporal resolution (for example, over large bodies of water such as the Pacific Ocean), which introduces uncertainty into the true initial state of the atmosphere. While a set of equations, known as the Liouville equations, exists to determine the initial uncertainty in the model initialization, the equations are too complex to run in real-time, even with the use of supercomputers. These uncertainties limit forecast model accuracy to about six days into the future.\n\nEdward Epstein recognized in 1969 that the atmosphere could not be completely described with a single forecast run due to inherent uncertainty, and proposed a stochastic dynamic model that produced means and variances for the state of the atmosphere. While these Monte Carlo simulations showed skill, in 1974 Cecil Leith revealed that they produced adequate forecasts only when the ensemble probability distribution was a representative sample of the probability distribution in the atmosphere. It was not until 1992 that ensemble forecasts began being prepared by the European Centre for Medium-Range Weather Forecasts, the Canadian Meteorological Centre, and the National Centers for Environmental Prediction. The ECMWF model, the Ensemble Prediction System, uses singular vectors to simulate the initial probability density, while the NCEP ensemble, the Global Ensemble Forecasting System, uses a technique known as vector breeding.\n\n"}
{"id": "253859", "url": "https://en.wikipedia.org/wiki?curid=253859", "title": "History of topos theory", "text": "History of topos theory\n\nThis page gives some very general background to the mathematical idea of topos. This is an aspect of category theory, and has a reputation for being abstruse. The level of abstraction involved cannot be reduced beyond a certain point; but on the other hand context can be given. This is partly in terms of historical development, but also to some extent an explanation of differing attitudes to category theory. \n\nDuring the latter part of the 1950s, the foundations of algebraic geometry were being rewritten; and it is here that the origins of the topos concept are to be found. At that time the Weil conjectures were an outstanding motivation to research. As we now know, the route towards their proof, and other advances, lay in the construction of étale cohomology.\n\nWith the benefit of hindsight, it can be said that algebraic geometry had been wrestling with two problems for a long time. The first was to do with its points: back in the days of projective geometry it was clear that the absence of 'enough' points on an algebraic variety was a barrier to having a good geometric theory (in which it was somewhat like a compact manifold). There was also the difficulty, that was clear as soon as topology took form in the first half of the twentieth century, that the topology of algebraic varieties had 'too few' open sets. \n\nThe question of points was close to resolution by 1950; Alexander Grothendieck took a sweeping step (invoking the Yoneda lemma) that disposed of it — naturally at a cost, that every variety or more general \"scheme\" should become a functor. It wasn't possible to \"add\" open sets, though. The way forward was otherwise.\n\nThe topos definition first appeared somewhat obliquely, in or about 1960. General problems of so-called 'descent' in algebraic geometry were considered, at the same period when the fundamental group was generalised to the algebraic geometry setting (as a pro-finite group). In the light of later work (c. 1970), 'descent' is part of the theory of comonads; here we can see one way in which the Grothendieck school bifurcates in its approach from the 'pure' category theorists, a theme that is important for the understanding of how the topos concept was later treated.\n\nThere was perhaps a more direct route available: the abelian category concept had been introduced by Grothendieck in his foundational work on homological algebra, to unify categories of sheaves of abelian groups, and of modules. An abelian category is supposed to be closed under certain category-theoretic operations — by using this kind of definition one can focus entirely on structure, saying nothing at all about the nature of the objects involved. This type of definition can be traced back, in one line, to the lattice concept of the 1930s. It was a possible question to ask, around 1957, for a purely category-theoretic characterisation of categories of sheaves of \"sets\", the case of sheaves of abelian groups having been subsumed by Grothendieck's work (the \"Tohoku\" paper).\n\nSuch a definition of a topos was eventually given five years later, around 1962, by Grothendieck and Verdier (see Verdier's Bourbaki seminar \"Analysis Situs\"). The characterisation was by means of categories 'with enough colimits', and applied to what is now called a Grothendieck topos. The theory was rounded out by establishing that a Grothendieck topos was a category of sheaves, where now the word \"sheaf\" had acquired an extended meaning, since it involved a Grothendieck topology.\n\nThe idea of a Grothendieck topology (also known as a \"site\") has been characterised by John Tate as a bold pun on the two senses of Riemann surface. Technically speaking it enabled the construction of the sought-after étale cohomology (as well as other refined theories such as flat cohomology and crystalline cohomology). At this point — about 1964 — the developments powered by algebraic geometry had largely run their course. The 'open set' discussion had effectively been summed up in the conclusion that varieties had a rich enough \"site\" of open sets in unramified covers of their (ordinary) Zariski-open sets.\n\nThe current definition of topos goes back to William Lawvere and Myles Tierney. While the timing follows closely on from that described above, as a matter of history, the attitude is different, and the definition is more inclusive. That is, there are examples of toposes that are not Grothendieck topos. What is more, these may be of interest for a number of logical disciplines.\n\nLawvere and Tierney's definition picks out the central role in topos theory of the sub-object classifier. In the usual category of sets, this is the two-element set of Boolean truth-values, true and false. It is almost tautologous to say that the subsets of a given set \"X\" are \"the same as\" (just as good as) the functions on \"X\" to any such given two-element set: fix the 'first' element and make a subset \"Y\" correspond to the function sending \"Y\" there and its complement in \"X\" to the other element.\n\nNow sub-object classifiers can be found in sheaf theory. Still tautologously, though certainly more abstractly, for a topological space \"X\" there is a direct description of a sheaf on \"X\" that plays the role with respect to all sheaves of sets on \"X\". Its set of sections over an open set \"U\" of \"X\" is just the set of open subsets of \"U\". The space associated with a sheaf, for it, is more difficult to describe.\n\nLawvere and Tierney therefore formulated axioms for a topos that assumed a sub-object classifier, and some limit conditions (to make a cartesian-closed category, at least). For a while this notion of topos was called 'elementary topos'.\n\nOnce the idea of a connection with logic was formulated, there were several developments 'testing' the new theory:\n\n\nThere was some irony that in the pushing through of David Hilbert's long-range programme a natural home for intuitionistic logic's central ideas was found: Hilbert had detested the school of L. E. J. Brouwer. Existence as 'local' existence in the sheaf-theoretic sense, now going by the name of Kripke–Joyal semantics, is a good match. On the other hand Brouwer's long efforts on 'species', as he called the intuitionistic theory of reals, are presumably in some way subsumed and deprived of status beyond the historical. There is a theory of the real numbers in each topos, and so no one master intuitionist theory.\n\nThe later work on étale cohomology has tended to suggest that the full, general topos theory isn't required. On the other hand, other sites are used, and the Grothendieck topos has taken its place within homological algebra.\n\nThe Lawvere programme was to write higher-order logic in terms of category theory. That this can be done cleanly is shown by the book treatment by Joachim Lambek and P. J. Scott. What results is essentially an intuitionistic (i.e. constructive logic) theory, its content being clarified by the existence of a \"free topos\". That is a set theory, in a broad sense, but also something belonging to the realm of pure syntax. The structure on its sub-object classifier is that of a Heyting algebra. To get a more classical set theory one can look at toposes in which it is moreover a Boolean algebra, or specialising even further, at those with just two truth-values. In that book, the talk is about constructive mathematics; but in fact this can be read as foundational computer science (which is not mentioned). If one wants to discuss set-theoretic operations, such as the formation of the image (range) of a function, a topos is guaranteed to be able to express this, entirely constructively.\n\nIt also produced a more accessible spin-off in pointless topology, where the \"locale\" concept isolates some of more accessible insights found by treating \"topos\" as a significant development of \"topological space\". The slogan is 'points come later': this brings discussion full circle on this page. The point of view is written up in Peter Johnstone's \"Stone Spaces\", which has been called by a leader in the field of computer science 'a treatise on extensionality'. The extensional is treated in mathematics as ambient - it is not something about which mathematicians really expect to have a theory. Perhaps this is why topos theory has been treated as an oddity; it goes beyond what the traditionally geometric way of thinking allows. The needs of thoroughly intensional theories such as untyped lambda calculus have been met in denotational semantics. Topos theory has long looked like a possible 'master theory' in this area.\n\nThe \"topos\" concept arose in algebraic geometry, as a consequence of combining the concept of \"sheaf\" and \"closure under categorical operations\". It plays a certain definite role in cohomology theories.\n\nThe subsequent developments associated with logic are more interdisciplinary. They include examples drawing on homotopy theory (classifying toposes). They involve links between category theory and mathematical logic, and also (as a high-level, organisational discussion) between category theory and theoretical computer science based on type theory. Granted the general view of Saunders Mac Lane about \"ubiquity\" of concepts, this gives them a definite status. A 'killer application' is étale cohomology.\n\n"}
{"id": "34197327", "url": "https://en.wikipedia.org/wiki?curid=34197327", "title": "Hodge–Tate module", "text": "Hodge–Tate module\n\nIn mathematics, a Hodge–Tate module is an analogue of a Hodge structure over p-adic fields. introduced and named Hodge–Tate structures using the results of on p-divisible groups.\n\nSuppose that \"G\" is the absolute Galois group of a \"p\"-adic field \"K\". Then \"G\" has a canonical cyclotomic character χ given by its action on the \"p\"th power roots of unity. Let \"C\" be the completion of the algebraic closure of \"K\". Then a finite-dimensional vector space over \"C\" with a semi-linear action of the Galois group \"G\" is said to be of Hodge–Tate type if it is generated by the eigenvectors of integral powers of χ.\n\n\n"}
{"id": "18819745", "url": "https://en.wikipedia.org/wiki?curid=18819745", "title": "International Workshop on First-Order Theorem Proving", "text": "International Workshop on First-Order Theorem Proving\n\nThe International Workshop on First-Order Theorem Proving (FTP) is a scientific meeting of researchers interested in automated theorem proving for first-order logic and related fields. FTP workshops are less formal than many conferences, but more formal than most workshops. While FTP proceedings are published informally, most FTP workshops have resulted in a special issue of a recognized peer-reviewed academic journal.\n\nFTP is one of the constituent meetings of the International Joint Conference on Automated Reasoning, and is merged with this conference in years where it takes place.\n\n\n"}
{"id": "1637995", "url": "https://en.wikipedia.org/wiki?curid=1637995", "title": "Karl Mahlburg", "text": "Karl Mahlburg\n\nKarl Mahlburg is an American mathematician whose research interests lie in the areas of modular forms, partitions, combinatorics and number theory.\n\nHe submitted a paper to Proceedings of the National Academy of Sciences (PNAS) entitled Partition Congruences and the Andrews-Garvan-Dyson Crank in 2005, and the paper won the PNAS first Paper of the Year prize.\n\nThe paper extends a result first conjectured by Srinivasa Ramanujan and later detailed by Freeman Dyson, George Andrews, Frank Garvan, and Mahlburg's advisor Ken Ono called the crank having to do with congruence patterns in partitions. Until recently such congruence patterns were only known to occur for 5, 7, and 11. Mahlburg's result extends this to all prime numbers.\n\nMahlburg was an undergraduate at Harvey Mudd College, where he graduated with highest distinction in 2001 with a B.S. in Mathematics. In 2006, he graduated from the University of Wisconsin–Madison with a Ph.D. in Mathematics. He is currently on the mathematics faculty at LSU.\n\n"}
{"id": "12988767", "url": "https://en.wikipedia.org/wiki?curid=12988767", "title": "Metadata modeling", "text": "Metadata modeling\n\nMetadata modeling is a type of metamodeling used in software engineering and systems engineering for the analysis and construction of models applicable to and useful for some predefined class of problems.\n\nMeta-modeling is the analysis, construction and development of the frames, rules, constraints, models and theories applicable and useful for the modeling in a predefined class of problems. \n\nThe meta-data side of the diagram consists of a concept diagram. This is basically an adjusted class diagram as described in Booch, Rumbaugh and Jacobson (1999). Important notions are concept, generalization, association, multiplicity and aggregation.\n\nFirst of all, a concept is a simple version of a Unified Modeling Language (UML) class. The class definition is adopted to define a concept, namely: a set of objects that share the same attributes, operations, relations, and semantics.\n\nThe following concept types are specified:\n\n\nIn Figure 1 the three concept types that are used in the modeling technique are illustrated. Concepts are always capitalized, not only in the diagram, but also when referring to them outside the diagram.\n\nIn Figure 2 all three concept types are exemplified. Part of the process-data diagram of the requirements workflow in the Unified Process is illustrated. The USE CASE MODEL is an open concept and consists of one or more ACTORS and one or more USE CASES. ACTOR is a standard concept, it contains no further sub-concepts. USE CASE, however, is a closed concept. A USE CASE consists of a description, a flow of events, conditions, special requirements, etc. Because in this case it is unnecessary to reveal that information, the USE CASE is illustrated with a closed concept.\n\nGeneralization is a way to express a relationship between a general concept and a more specific concept. Also, if necessary, one can indicate whether the groups of concepts that are identified are overlapping or disjoint, complete or incomplete. Generalization is visualized by a solid arrow with an open arrowhead, pointing to the parent, as is illustrated in Figure 3.\n\nIn Figure 4 generalization is exemplified by showing the relationships between the different concepts described in the preceding paragraph. STANDARD CONCEPT and COMPLEX CONCEPT are both a specific kind of CONCEPT. Subsequently, a COMPLEX CONCEPT can be specified into an OPEN CONCEPT and a CLOSED CONCEPT.\n\nAn association is a structural relationship that specifies how concepts are connected to another. It can connect two concepts (binary association) or more than two concepts (n-ary association). An association is represented with an undirected solid line. To give a meaning to the association, a name and name direction can be provided. The name is in the form of an active verb and the name direction is represented by a triangle that points in the direction one needs to read. Association with a name and name direction is illustrated in Figure 5.\n\nIn Figure 6 (removed) an example of association is illustrated. The example is a fragment of the process-data diagram of the requirements analysis in the Unified Process. Because both concepts are not expanded any further, although several sub concepts exist, the concepts are illustrated as closed concepts. The figure reads as “SURVEY DESCRIPTION describes USE CASE MODEL”.\n\nExcept name and name direction, an association can have more characteristics. With multiplicity one can state how many objects of a certain concept can be connected across an instance of an association. Multiplicity is visualized by using the following expressions: (1) for exactly one, (0..1) for one or zero, (0..*) for zero or more, (1..*) for one or more, or for example (5) for an exact number. In Figure 7 association with multiplicity is illustrated.\n\nAn example of multiplicity is represented in Figure 8. It is the same example as in Figure 6, only the multiplicity values are added. The figure reads as ‘exactly one SURVEY DESCRIPTION describes exactly one USE CASE MODEL’. This implies that a SURVEY DESCRIPTION cannot describe zero or more than one USE CASE MODEL and a USE CASE MODEL cannot be described by zero or more than one SURVEY DESCRIPTIONS.\n\nA special type of association is aggregation. Aggregation represents the relation between a concept (as a whole) containing other concepts (as parts). It can also be described as a ‘has-a’ relationship. In Figure 9 an aggregation relationship between OPEN CONCEPT and STANDARD CONCEPT is illustrated. An OPEN CONCEPT consists of one or more STANDARD CONCEPTS and a STANDARD CONCEPT is part of zero or more OPEN CONCEPT.\n\nIn Figure 10 aggregation is exemplified by a fragment of the requirements capture workflow in UML-Based Web Engineering. A USE CASE MODEL consists of one or more ACTORS and USE CASES.\n\nSometimes the needs exist to assign properties to concepts. Properties are written in lower case, under the concept name, as is illustrated in Figure 11.\n\nIn Figure 12 an example of a concept with properties is visualized. The concept FEATURE has four properties, respectively: priority, type, risk and status.\n\nIn Table 1 a list presented Each CONCEPT requires a proper definition which is preferably copied from a standard glossary. All CONCEPT names in the text are with capital characters.\n\n\n"}
{"id": "601025", "url": "https://en.wikipedia.org/wiki?curid=601025", "title": "Minkowski–Bouligand dimension", "text": "Minkowski–Bouligand dimension\n\nIn fractal geometry, the Minkowski–Bouligand dimension, also known as Minkowski dimension or box-counting dimension, is a way of determining the fractal dimension of a set \"S\" in a Euclidean space R, or more generally in a metric space (\"X\", \"d\"). It is named after the German mathematician Hermann Minkowski and the French mathematician Georges Bouligand.\n\nTo calculate this dimension for a fractal \"S\", imagine this fractal lying on an evenly spaced grid, and count how many boxes are required to cover the set. The box-counting dimension is calculated by seeing how this number changes as we make the grid finer by applying a box-counting algorithm.\n\nSuppose that \"N\"(\"ε\") is the number of boxes of side length ε required to cover the set. Then the box-counting dimension is defined as:\n\nRoughly speaking, this means the dimension is the exponent \"d\" such that \"N\"(1/\"n\") ≈ \"C n\", which is what one would expect in the trivial case where \"S\" is a smooth space (a manifold) of integer dimension d.\n\nIf the above limit does not exist, one may still take the limit superior and limit inferior, which respectively define the upper box dimension and lower box dimension. The upper box dimension is sometimes called the entropy dimension, Kolmogorov dimension, Kolmogorov capacity, limit capacity or upper Minkowski dimension, while the lower box dimension is also called the lower Minkowski dimension.\n\nThe upper and lower box dimensions are strongly related to the more popular Hausdorff dimension. Only in very special applications is it important to distinguish between the three (see below). Yet another measure of fractal dimension is the correlation dimension.\n\nIt is possible to define the box dimensions using balls, with either the covering number or the packing number. The covering number formula_2 is the \"minimal\" number of open balls of radius ε required to cover the fractal, or in other words, such that their union contains the fractal. We can also\nconsider the intrinsic covering number formula_3, which is defined the same way but with the additional requirement that the centers of the open balls lie inside the set \"S\". The packing number formula_4 is the \"maximal\" number of disjoint open balls of radius ε one can situate such that their centers would be inside the fractal. While \"N\", \"N\", \"N\"' and \"N\" are not exactly identical, they are closely related, and give rise to identical definitions of the upper and lower box dimensions. This is easy to prove once the following inequalities are proven:\n\nThese, in turn, follow with a little effort from the triangle inequality.\n\nThe advantage of using balls rather than squares is that this definition generalizes to any metric space. In other words, the box definition is extrinsic — one assumes the fractal space \"S\" is contained in a Euclidean space, and defines boxes according to the external geometry of the containing space. However, the dimension of \"S\" should be intrinsic, independent of the environment into which \"S\" is placed, and the ball definition can be formulated intrinsically. One defines an internal ball as all points of \"S\" within a certain distance of a chosen center, and one counts such balls to get the dimension. (More precisely, the \"N\" definition is extrinsic, but the other two are intrinsic.)\n\nThe advantage of using boxes is that in many cases \"N\"(\"ε\") may be easily calculated explicitly, and that for boxes the covering and packing numbers (defined in an equivalent way) are equal.\n\nThe logarithm of the packing and covering numbers are sometimes referred to as \"entropy numbers\", and are somewhat analogous to the concepts of thermodynamic entropy and information-theoretic entropy, in that they measure the amount of \"disorder\" in the metric space or fractal at scale \"ε\", and also measure how many bits or digits one would need to specify a point of the space to accuracy \"ε\".\n\nAnother equivalent (extrinsic) definition for the box-counting dimension, is given by the formula:\n\nwhere for each \"r\" > 0, the set formula_7 is defined to be the \"r\"-neighborhood of \"S\", i.e. the set of all points in formula_8 which are at distance less than \"r\" from \"S\" (or equivalently, formula_7 is the union of all the open balls of radius \"r\" which are centered at a point in \"S\").\n\nBoth box dimensions are finitely additive, i.e. if { \"A\", ... \"A\" } is a finite collection of sets then\n\nHowever, they are not countably additive, i.e. this equality does not hold for an \"infinite\" sequence of sets. For example, the box dimension of a single point is 0, but the box dimension of the collection of rational numbers in the interval [0, 1] has dimension 1. The Hausdorff measure by comparison, is countably additive.\n\nAn interesting property of the upper box dimension not shared with either the lower box dimension or the Hausdorff dimension is the connection to set addition. If \"A\" and \"B\" are two sets in a Euclidean space then \"A\" + \"B\" is formed by taking all the pairs of points \"a,b\" where \"a\" is from \"A\" and \"b\" is from \"B\" and adding \"a+b\". One has\n\nThe box-counting dimension is one of a number of definitions for dimension that can be applied to fractals. For many well behaved fractals all these dimensions are equal; in particular, these dimensions coincide whenever the fractal satisfies the open set condition (OSC). For example, the Hausdorff dimension, lower box dimension, and upper box dimension of the Cantor set are all equal to log(2)/log(3). However, the definitions are not equivalent.\n\nThe box dimensions and the Hausdorff dimension are related by the inequality\n\nIn general both inequalities may be strict. The upper box dimension may be bigger than the lower box dimension if the fractal has different behaviour in different scales. For example, examine the set of numbers in the interval [0,1] satisfying the condition\n\nThe digits in the \"odd place-intervals\", i.e. between digits 2 and 2 − 1 are not restricted and may take any value. This fractal has upper box dimension 2/3 and lower box dimension 1/3, a fact which may be easily verified by calculating \"N\"(\"ε\") for formula_13 and noting that their values behave differently for \"n\" even and odd.\n\nMore examples: The set of rational numbers formula_14, a countable set with formula_15, has formula_16 because its closure, formula_17, has dimension 1. In fact, \n\nThese examples show that adding a countable set can change box-dimension, showing a kind of instability of this dimension.\n\n\n\n"}
{"id": "38713429", "url": "https://en.wikipedia.org/wiki?curid=38713429", "title": "Mixed graph", "text": "Mixed graph\n\nA mixed graph \"G\" = (\"V\", \"E\", \"A\") is a mathematical object consisting of a set of vertices (or nodes) \"V\", a set of (undirected) edges \"E\", and a set of directed edges (or arcs) \"A\".\n\nConsider adjacent vertices formula_1. A directed edge, called an arc, is an edge with an orientation and can be denoted as formula_2 or formula_3 (note that formula_4 is the tail and formula_5 is the head of the arc). Also, an undirected edge, or edge, is an edge with no orientation and can be denoted as formula_6 or formula_7. \nFor the purpose of our application example we will not be considering loops or multiple edges of mixed graphs. \n\nA cycle of a mixed graph, or mixed cycle, is formed if the directed edges of the mixed graph form a cycle. An orientation of a mixed graph is considered acyclic if cycles cannot be formed from the directed edges. We call a mixed graph acyclic if all of its orientations are acyclic.\n\nMixed graph coloring can be thought of as a labeling or an assignment of different colors (where is a positive integer) to the vertices of a mixed graph. Different colors must be assigned to vertices that are connected by an edge. The colors may be represented by the numbers from to , and for a directed arc, the tail of the arc must be colored by a smaller number than the head of the arc.\n\nFor example, consider the figure to the right. Our available k-colors to color our mixed graph are formula_8. Since formula_4 and formula_5 are connected by an edge, they must receive different colors or labelings (formula_4 and formula_5 are labelled 1 and 2, respectively). We also have an arc from formula_5 to formula_14. Since orientation assigns an ordering, we must label the tail (formula_5) with a smaller color (or integer from our set) than the head (formula_14) of our arc.\n\nA (strong) proper \"k\"-coloring of a mixed graph is a function\n\nformula_17 where formula_18 such that formula_19 if formula_20 and formula_21 if formula_22. \n\nA weaker condition on our arcs can be applied and we can consider a weak proper \"k\"-coloring of a mixed graph to be a function\n\nformula_17 where formula_18 such that formula_19 if formula_20 and formula_27 if formula_22. \nReferring back to our example, this means that we can label both the head and tail of formula_29 with the positive integer 2.\n\nA coloring may or may not exist for a mixed graph. In order for a mixed graph to have a k-coloring, the graph cannot contain any directed cycles. If such a k-coloring exists, then we refer to the smallest k needed in order to properly color our graph as the chromatic number, denoted formula_30. We can count the number of proper k-colorings as a polynomial function of k. This is called the chromatic polynomial of our graph G (by analogy with the chromatic polynomial of undirected graphs) and can be denoted as formula_31.\n\nThe deletion–contraction method can be used to compute weak chromatic polynomials of mixed graphs. This method involves deleting (or removing) an edge or arc and contracting (or joining) the remaining vertices incident to that edge (or arc) to form one vertex. After deleting an edge, formula_32, from a mixed graph formula_33 we obtain the mixed graph formula_34. We can denote this deletion of the edge, formula_32, as formula_36. Similarly, by deleting an arc, formula_37, from a mixed graph, we obtain formula_38 where we can denote the deletion of formula_37 as formula_40. Also, we can denote the contraction of formula_32 and formula_37 as formula_43 and formula_44, respectively. From Propositions given in, we obtain the following equations to compute the chromatic polynomial of a mixed graph:\n\nMixed graphs may be used to model job shop scheduling problems in which a collection of tasks is to be performed, subject to certain timing constraints. In this sort of problem, undirected edges may be used to model a constraint that two tasks are incompatible (they cannot be performed simultaneously). Directed edges may be used to model precedence constraints, in which one task must be performed before another. A graph defined in this way from a scheduling problem is called a disjunctive graph. The mixed graph coloring problem can be used to find a schedule of minimum length for performing all the tasks.\n\nMixed graphs are also used as graphical models for Bayesian inference. In this context, an acyclic mixed graph (one with no cycles of directed edges) is also called a chain graph. The directed edges of these graphs are used to indicate a causal connection between two events, in which the outcome of the first event influences the probability of the second event. Undirected edges, instead, indicate a non-causal correlation between two events. A connected component of the undirected subgraph of a chain graph is called a chain. A chain graph may be transformed into an undirected graph by constructing its moral graph, an undirected graph formed from the chain graph by adding undirected edges between pairs of vertices that have outgoing edges to the same chain, and then forgetting the orientations of the directed edges.\n\n"}
{"id": "56024347", "url": "https://en.wikipedia.org/wiki?curid=56024347", "title": "National Prize for Exact Sciences (Chile)", "text": "National Prize for Exact Sciences (Chile)\n\nThe National Prize for Exact Sciences () was created in 1992 as one of the replacements for the National Prize for Sciences under Law 19169. The other two prizes in this same area are for Natural Sciences and Applied Sciences and Technologies.\n\nIt is part of the National Prize of Chile.\n\nThe jury is made up of the Minister of Education, who calls it, the Rector of the University of Chile, the President of the , a representative of the Council of Rectors, and the last recipient of the prize.\n\n\n"}
{"id": "16252996", "url": "https://en.wikipedia.org/wiki?curid=16252996", "title": "Neumann–Dirichlet method", "text": "Neumann–Dirichlet method\n\nIn mathematics, the Neumann–Dirichlet method is a domain decomposition preconditioner which involves solving Neumann boundary value problem on one subdomain and Dirichlet boundary value problem on another, adjacent across the interface between the subdomains. On a problem with many subdomains organized in a rectangular mesh, the subdomains are assigned Neumann or Dirichlet problems in a checkerboard fashion.\n\n"}
{"id": "452638", "url": "https://en.wikipedia.org/wiki?curid=452638", "title": "Perlin noise", "text": "Perlin noise\n\nPerlin noise is a type of gradient noise developed by Ken Perlin in 1983 as a result of his frustration with the \"machine-like\" look of computer graphics at the time. He formally described his findings in a SIGGRAPH paper in 1985 called \"An image Synthesizer\". In 1997, Perlin was awarded an Academy Award for Technical Achievement for creating the algorithm:\n\nPerlin did not apply for any patents on the algorithm, but in 2001 he was granted a patent for the use of 3D+ implementations of simplex noise for texture synthesis. Simplex noise has the same purpose, but uses a simpler space-filling grid. Simplex noise alleviates some of the problems with Perlin's \"classic noise\", among them computational complexity and visually-significant directional artifacts.\n\nPerlin noise is a procedural texture primitive, a type of gradient noise used by visual effects artists to increase the appearance of realism in computer graphics. The function has a pseudo-random appearance, yet all of its visual details are the same size. This property allows it to be readily controllable; multiple scaled copies of Perlin noise can be inserted into mathematical expressions to create a great variety of procedural textures. Synthetic textures using Perlin noise are often used in CGI to make computer-generated visual elements – such as object surfaces, fire, smoke, or clouds – appear more natural, by imitating the controlled random appearance of textures in nature.\n\nIt is also frequently used to generate textures when memory is extremely limited, such as in demos, and is increasingly finding use in graphics processing units for real-time graphics in computer games.\n\nPerlin noise resulted from the work of Ken Perlin, who developed it at Mathematical Applications Group, Inc. (MAGI) for Disney's computer animated sci-fi motion picture \"Tron\" (1982). In 1997, he won an Academy Award for Technical Achievement from the Academy of Motion Picture Arts and Sciences for this contribution to CGI.\n\nPerlin noise is most commonly implemented as a two-, three- or four-dimensional function, but can be defined for any number of dimensions. An implementation typically involves three steps: grid definition with random gradient vectors, computation of the dot product between the distance-gradient vectors and interpolation between these values.\n\nDefine an \"n\"-dimensional grid where each point has a random \"n\"-dimensional unit-length gradient vector, except in the one dimensional case where the gradients are random scalars between -1 and 1.\n\nAssigning the random gradients in one and two dimensions is trivial using a random number generator. For higher dimensions a Monte Carlo approach can be used where random Cartesian coordinates are chosen in a unit cube, points falling outside the unit ball are discarded, and the remaining points are normalized to lie on the unit sphere. The process is continued until the required number of random gradients are obtained.\n\nIn order to negate the expensive process of computing new gradients for each grid node, some implementations use a hash and lookup table for a finite number of precomputed gradient vectors. The use of a hash also permits the inclusion of a random seed where multiple instances of Perlin noise are required.\n\nGiven an \"n\"-dimensional argument for the noise function, the next step in the algorithm is to determine into which grid cell the given point falls. For each corner node of that cell, the distance vector between the point and the node is determined. The dot product between the gradient vector at the node and the distance vector is then computed.\n\nFor a point in a two-dimensional grid, this will require the computation of 4 distance vectors and dot products, while in three dimensions 8 distance vectors and 8 dot products are needed. This leads to the formula_1 complexity scaling.\n\nThe final step is interpolation between the formula_2 dot products computed at the nodes of the cell containing the argument point. This has the consequence that the noise function returns 0 when evaluated at the grid nodes themselves.\n\nInterpolation is performed using a function that has zero first derivative (and possibly also second derivative) at the formula_2 grid nodes. This has the effect that the gradient of the resulting noise function at each grid node coincides with the precomputed random gradient vector there. If formula_4, an example of a function that interpolates between value formula_5 at grid node 0 and value formula_6 at grid node 1 is\n\nwhere the smoothstep function was used.\n\nNoise functions for use in computer graphics typically produce values in the range [-1.0,1.0]. In order to produce Perlin noise in this range, the interpolated value may need to be scaled by some scaling factor.\n\nThe following is a two-dimensional implementation of Classical Perlin Noise, written in C++.\n// Function to linearly interpolate between a0 and a1\n// Weight w should be in the range [0.0, 1.0]\nfloat lerp(float a0, float a1, float w) {\n\n// Computes the dot product of the distance and gradient vectors.\nfloat dotGridGradient(int ix, int iy, float x, float y) {\n\n// Compute Perlin noise at coordinates x, y\nfloat perlin(float x, float y) {\n\nFor each evaluation of the noise function, the dot product of the position and gradient vectors must be evaluated at each node of the containing grid cell. Perlin noise therefore scales with complexity formula_1 for formula_9 dimensions. Alternatives to Perlin noise producing similar results with improved complexity scaling include simplex noise and OpenSimplex noise.\n\n\n"}
{"id": "13945845", "url": "https://en.wikipedia.org/wiki?curid=13945845", "title": "Philippe G. Ciarlet", "text": "Philippe G. Ciarlet\n\nPhilippe G. Ciarlet (born October 14, 1938, Paris) is a French mathematician, known particularly for his work on mathematical analysis of the finite element method especially applied to elasticity. He has contributed also to analysis of partial differential equations and differential geometry.\n\nAlumnus of the École polytechnique and École Nationale des Ponts et Chaussées in Paris, Ciarlet earned a Ph.D. from Case Institute of Technology in Cleveland, Ohio in 1966 for a thesis entitled \"Variational Methods for Non-Linear Boundary-Value Problems\" under Richard S. Varga.\n\nReturning to Paris, Ciarlet was awarded a higher doctorate under the supervision of Jacques-Louis Lions. From 1974 to 2002, Ciarlet held the post of Professor at the Pierre and Marie Curie University, Paris. He currently holds a chair in the Department of Mathematics at the City University of Hong Kong.\n\nCiarlet has been awarded the Grand Prize (Prix Jaffé) of the French Academy of Sciences in 1989, membership of the French Academy of Sciences and the Légion d'honneur, both in 1999. He is a member of the Academies of Science of several other countries. In 2012 he became a fellow of the American Mathematical Society.\n\n"}
{"id": "461942", "url": "https://en.wikipedia.org/wiki?curid=461942", "title": "Poincaré half-plane model", "text": "Poincaré half-plane model\n\nIn non-Euclidean geometry, the Poincaré half-plane model is the upper half-plane, denoted below as H formula_1 , together with a metric, the Poincaré metric, that makes it a model of two-dimensional hyperbolic geometry.\n\nEquivalently the Poincaré half-plane model is sometimes described as a complex plane where the imaginary part (the \"y\" coordinate mentioned above) is positive.\n\nThe Poincaré half-plane model is named after Henri Poincaré, but it originated with Eugenio Beltrami, who used it, along with the Klein model and the Poincaré disk model (due to Bernhard Riemann), to show that hyperbolic geometry was equiconsistent with Euclidean geometry.\n\nThis model is conformal which means that the angles measured at a point are the same in the model as they are in the actual hyperbolic plane.\n\nThe Cayley transform provides an isometry between the half-plane model and the Poincaré disk model.\n\nThis model can be generalized to model an formula_2 dimensional hyperbolic space by replacing the real number \"x\" by a vector in an \"n\" dimensional Euclidean vector space.\n\nThe metric of the model on the half-plane, formula_3 is: \n\nwhere \"s\" measures the length along a (possibly curved) line.\nThe \"straight lines\" in the hyperbolic plane (geodesics for this metric tensor, i.e., curves which minimize the distance) are represented in this model by circular arcs perpendicular to the \"x\"-axis (half-circles whose origin is on the \"x\"-axis) and straight vertical rays perpendicular to the \"x\"-axis.\n\nIn general, the \"distance\" between two points measured in this metric along such a geodesic is:\n\nwhere \"arcosh\" and \"arsinh\" are inverse hyperbolic functions\n\nSome special cases can be simplified:\n\nAnother way to calculate the distance between two points that are on a (Euclidean) half circle is:\n\nwhere formula_11 are the points where the halfcircles meet the boundary line and formula_12is the euclidean length of the line segment connecting the points \"P\" and \"Q\" in the model.\n\n\n\n\n\n\nA Euclidean circle with center formula_19 and radius formula_20 represents: \n\nHere is how one can use compass and straightedge constructions in the model to achieve the effect of the basic constructions in the hyperbolic plane.\nFor example, how to construct the half-circle in the Euclidean half-plane which models a line on the hyperbolic plane through two given points.\n\nDraw the line segment between the two points. Construct the perpendicular bisector of the line segment. Find its intersection with the \"x\"-axis. Draw the circle around the intersection which passes through the given points. Erase the part which is on or below the \"x\"-axis.\n\nOr in the special case where the two given points lie on a vertical line, draw that vertical line through the two points and erase the part which is on or below the \"x\"-axis.\n\n\nDraw the radial \"line\" (half-circle) between the two given points as in the previous case. Construct a tangent to that line at the non-central point. Drop a perpendicular from the given center point to the \"x\"-axis. Find the intersection of these two lines to get the center of the model circle. Draw the model circle around that new center and passing through the given non-central point.\n\n\nDraw a circle around the intersection of the vertical line and the \"x\"-axis which passes through the given central point.\nDraw a horizontal line through the non-central point.\nConstruct the tangent to the circle at its intersection with that horizontal line.\n\nThe midpoint between the intersection of the tangent with the vertical line and the given non-central point is the center of the model circle.\nDraw the model circle around that new center and passing through the given non-central point.\n\n\nDraw a circle around the intersection of the vertical line and the \"x\"-axis which passes through the given central point.\nDraw a line tangent to the circle which passes through the given non-central point.\nDraw a horizontal line through that point of tangency and find its intersection with the vertical line.\n\nThe midpoint between that intersection and the given non-central point is the center of the model circle.\nDraw the model circle around that new center and passing through the given non-central point.\n\nDrop a perpendicular \"p\" from the Euclidean center of the circle to the \"x\"-axis.\n\nLet point \"q\" be the intersection of this line and the \"x\"- axis.\n\nDraw a line tangent to the circle going through \"q\".\n\nDraw the half circle \"h\" with center \"q\" going through the point where the tangent and the circle meet.\n\nThe (hyperbolic) center is the point where \"h\" and \"p\" intersect.\n\nFind the intersection of the two given semicircles (or vertical lines).\n\nFind the intersection of the given semicircle (or vertical line) with the given circle.\n\nFind the intersection of the two given circles.\n\nThe projective linear group PGL(2,C) acts on the Riemann sphere by the Möbius transformations. The subgroup that maps the upper half-plane, H, onto itself is PSL(2,R), the transforms with real coefficients, and these act transitively and isometrically on the upper half-plane, making it a homogeneous space.\n\nThere are four closely related Lie groups that act on the upper half-plane by fractional linear transformations and preserve the hyperbolic distance.\n\n\nThe relationship of these groups to the Poincaré model is as follows:\n\nImportant subgroups of the isometry group are the Fuchsian groups.\n\nOne also frequently sees the modular group SL(2,Z). This group is important in two ways. First, it is a symmetry group of the square 2x2 lattice of points. Thus, functions that are periodic on a square grid, such as modular forms and elliptic functions, will thus inherit an SL(2,Z) symmetry from the grid. Second, SL(2,Z) is of course a subgroup of SL(2,R), and thus has a hyperbolic behavior embedded in it. In particular, SL(2,Z) can be used to tessellate the hyperbolic plane into cells of equal (Poincaré) area.\n\nThe group action of the projective special linear group formula_26 on formula_27 is defined by\n\nNote that the action is transitive: for any formula_29, there exists a formula_30 such that formula_31. It is also faithful, in that if formula_32 for all formula_33 then \"g\" = \"e\".\n\nThe stabilizer or \"isotropy subgroup\" of an element formula_34 is the set of formula_35 which leave \"z\" unchanged: \"gz\" = \"z\". The stabilizer of \"i\" is the rotation group\n\nSince any element formula_34 is mapped to \"i\" by some element of formula_38, this means that the isotropy subgroup of any \"z\" is isomorphic to SO(2). Thus, formula_39. Alternatively, the bundle of unit-length tangent vectors on the upper half-plane, called the unit tangent bundle, is isomorphic to formula_38.\n\nThe upper half-plane is tessellated into free regular sets by the modular group formula_41\n\nThe geodesics for this metric tensor are circular arcs perpendicular to the real axis (half-circles whose origin is on the real axis) and straight vertical lines ending on the real axis.\n\nThe unit-speed geodesic going up vertically, through the point \"i\" is given by\n\nBecause PSL(2,R) acts transitively by isometries of the upper half-plane, this geodesic is mapped into the other geodesics through the action of PSL(2,R). Thus, the general unit-speed geodesic is given by\n\nThis provides the complete description of the geodesic flow on the unit-length tangent bundle (complex line bundle) on the upper half-plane.\n\nThe metric of the model on the half- space\n\nis given by\n\nwhere \"s\" measures length along a possibly curved line.\nThe \"straight lines\" in the hyperbolic space (geodesics for this metric tensor, i.e. curves which minimize the distance) are represented in this model by circular arcs normal to the \"z = 0\"-plane (half-circles whose origin is on the \"z = 0\"-plane) and straight vertical rays normal to the \"z = 0\"-plane.\n\nThe \"distance\" between two points measured in this metric along such a geodesic is:\n"}
{"id": "2094044", "url": "https://en.wikipedia.org/wiki?curid=2094044", "title": "Power of a point", "text": "Power of a point\n\nIn elementary plane geometry, the power of a point is a real number \"h\" that reflects the relative distance of a given point from a given circle. Specifically, the power of a point P with respect to a circle \"O\" of radius \"r\" is defined by (Figure 1).\n\nwhere \"s\" is the distance between P and the center O of the circle. By this definition, points inside the circle have negative power, points outside have positive power, and points on the circle have zero power. For external points, the power equals the square of the length of a tangent from the point to the circle. The power of a point is also known as the point's circle power or the power of a circle with respect to the point.\n\nThe power of point P \"(see in Figure 1)\" can be defined equivalently as the product of distances from the point P to the two intersection points of any ray emanating from P. For example, in Figure 1, a ray emanating from P intersects the circle in two points, M and N, whereas a tangent ray intersects the circle in one point T; the horizontal ray from P intersects the circle at A and B, the endpoints of the diameter. Their respective products of distances are equal to each other and to the power of point P in that circle\n\nThis equality is sometimes known as the \"secant-tangent theorem\", \"intersecting chords theorem\", or the \"power-of-a-point theorem\".\n\nThe power of a point is used in many geometrical definitions and proofs. For example, the radical axis of two given circles is the straight line consisting of points that have equal power to both circles. For each point on this line, there is a unique circle centered on that point that intersects both given circles orthogonally; equivalently, tangents of equal length can be drawn from that point to both given circles. Similarly, the radical center of three circles is the unique point with equal power to all three circles. There exists a unique circle, centered on the radical center, that intersects all three given circles orthogonally, equivalently, tangents drawn from the radical center to all three circles have equal length. The power diagram of a set of circles divides the plane into regions within which the circle minimizing the power is constant.\n\nMore generally, French mathematician Edmond Laguerre defined the power of a point with respect to any algebraic curve in a similar way.\n\nFor a point P outside the circle, the power \"h\" equals \"R\", the square of the radius \"R\" of a new circle centered on P that intersects the given circle at right angles, i.e., orthogonally (Figure 2). If the two circles meet at right angles at a point T, then radii drawn to T from P and from O, the center of the given circle, likewise meet at right angles (blue line segments in Figure 2). Therefore, the radius line segment of each circle is tangent to the other circle. These line segments form a right triangle with the line segment connecting O and P. Therefore, by the Pythagorean theorem,\n\nwhere \"s\" is again the distance from the point P to the center O of the given circle (solid black in Figure 2).\n\nThis construction of an orthogonal circle is useful in understanding the radical axis of two circles, and the radical center of three circles. The point T can be constructed—and, thereby, the radius \"R\" and the power \"h\" found geometrically—by finding the intersection of the given circle with a semicircle (red in Figure 2) centered on the midpoint of O and P and passing through both points. It can also be shown that the point Q is the inverse of P with respect to the given circle.\n\nThe \"power of a point theorem\", due to Jakob Steiner, states that for any line through \"A\" intersecting a circle \"c\" in points \"P\" and \"Q\", the power of the point with respect to the circle \"c\" is given up to a sign by the product\n\nof the lengths of the segments from \"A\" to \"P\" and \"A\" to \"Q\", with a positive sign if \"A\" is outside the circle and a negative sign otherwise: if \"A\" is on the circle, the product is zero. In the limiting case, when the line is tangent to the circle, \"P\" = \"Q\", and the result is immediate from the Pythagorean theorem.\n\nIn the other two cases, when \"A\" is inside the circle, or \"A\" is outside the circle, the power of a point theorem has two corollaries.\n\n\nThe power of a point is a special case of the Darboux product between two circles, which is given by\nwhere \"A\" and \"A\" are the centers of the two circles and \"r\" and \"r\" are their radii. The power of a point arises in the special case that one of the radii is zero.\n\nIf the two circles are orthogonal, the Darboux product vanishes.\n\nIf the two circles intersect, then their Darboux product is\n\nwhere \"φ\" is the angle of intersection.\n\nLaguerre defined the power of a point \"P\" with respect to an algebraic curve of degree \"n\" to be the product of the distances from the point to the intersections of a circle through the point with the curve, divided by the \"n\"th power of the diameter \"d\". Laguerre showed that this number is independent of the diameter.\n\nIn the case when the algebraic curve is a circle this is not quite the same as the power of a point with respect to a circle defined in the rest of this article, but differs from it by a factor of \"d\".\n\n\n\n"}
{"id": "201718", "url": "https://en.wikipedia.org/wiki?curid=201718", "title": "Principle of maximum entropy", "text": "Principle of maximum entropy\n\nThe principle of maximum entropy states that the probability distribution which best represents the current state of knowledge is the one with largest entropy, in the context of precisely stated prior data (such as a proposition that expresses testable information).\n\nAnother way of stating this: Take precisely stated prior data or testable information about a probability distribution function. Consider the set of all trial probability distributions that would encode the prior data. According to this principle, the distribution with maximal information entropy is the best choice.\n\nThe principle was first expounded by E. T. Jaynes in two papers in 1957 where he emphasized a natural correspondence between statistical mechanics and information theory. In particular, Jaynes offered a new and very general rationale why the Gibbsian method of statistical mechanics works. He argued that the entropy of statistical mechanics and the information entropy of information theory are basically the same thing. Consequently, statistical mechanics should be seen just as a particular application of a general tool of logical inference and information theory.\n\nIn most practical cases, the stated prior data or testable information is given by a set of conserved quantities (average values of some moment functions), associated with the probability distribution in question. This is the way the maximum entropy principle is most often used in statistical thermodynamics. Another possibility is to prescribe some symmetries of the probability distribution. The equivalence between conserved quantities and corresponding symmetry groups implies a similar equivalence for these two ways of specifying the testable information in the maximum entropy method.\n\nThe maximum entropy principle is also needed to guarantee the uniqueness and consistency of probability assignments obtained by different methods, statistical mechanics and logical inference in particular.\n\nThe maximum entropy principle makes explicit our freedom in using different forms of prior data. As a special case, a uniform prior probability density (Laplace's principle of indifference, sometimes called the principle of insufficient reason), may be adopted. Thus, the maximum entropy principle is not merely an alternative way to view the usual methods of inference of classical statistics, but represents a significant conceptual generalization of those methods.\n\nHowever these statements do not imply that thermodynamical systems need not be shown to be ergodic to justify treatment as a statistical ensemble.\n\nIn ordinary language, the principle of maximum entropy can be said to express a claim of epistemic modesty, or of maximum ignorance. The selected distribution is the one that makes the least claim to being informed beyond the stated prior data, that is to say the one that admits the most ignorance beyond the stated prior data.\n\nThe principle of maximum entropy is useful explicitly only when applied to \"testable information\". Testable information is a statement about a probability distribution whose truth or falsity is well-defined. For example, the statements\n\nand\n\n(where \"p\" + \"p\" are probabilities of events) are statements of testable information.\n\nGiven testable information, the maximum entropy procedure consists of seeking the probability distribution which maximizes information entropy, subject to the constraints of the information. This constrained optimization problem is typically solved using the method of Lagrange multipliers.\n\nEntropy maximization with no testable information respects the universal \"constraint\" that the sum of the probabilities is one. Under this constraint, the maximum entropy discrete probability distribution is the uniform distribution,\n\nThe principle of maximum entropy is commonly applied in two ways to inferential problems:\n\nThe principle of maximum entropy is often used to obtain prior probability distributions for Bayesian inference. Jaynes was a strong advocate of this approach, claiming the maximum entropy distribution represented the least informative distribution.\nA large amount of literature is now dedicated to the elicitation of maximum entropy priors and links with channel coding.\n\nMaximum entropy is a sufficient updating rule for radical probabilism. Richard Jeffrey's probability kinematics is a special case of maximum entropy inference. However, maximum entropy is not a generalisation of all such sufficient updating rules.\n\nAlternatively, the principle is often invoked for model specification: in this case the observed data itself is assumed to be the testable information. Such models are widely used in natural language processing. An example of such a model is logistic regression, which corresponds to the maximum entropy classifier for independent observations.\n\nOne of the main applications of the maximum entropy principle is in discrete and continuous density estimation.\nSimilar to support vector machine estimators, \nthe maximum entropy principle may require the solution to a quadratic programming and thus provide \na sparse mixture model as the optimal density estimator. One important advantage of the method is able to incorporate prior information in the density estimation.\n\nWe have some testable information \"I\" about a quantity \"x\" taking values in {\"x\", \"x\"..., \"x\"}. We assume this information has the form of \"m\" constraints on the expectations of the functions \"f\"; that is, we require our probability distribution to satisfy the moment inequality/equality constraints:\n\nFurthermore, the probabilities must sum to one, giving the constraint\n\nThe probability distribution with maximum information entropy subject to these equality constraints is:\n\nIt is sometimes called the Gibbs distribution. The normalization constant is determined by:\n\nand is conventionally called the partition function. (The Pitman–Koopman theorem states that the necessary and sufficient condition for a sampling distribution to admit sufficient statistics of bounded dimension is that it have the general form of a maximum entropy distribution.)\n\nThe λ parameters are Lagrange multipliers. In the case of equality constraints their values are determined from the solution of the nonlinear equations\n\nIn the case of inequality constraints, the Lagrange multipliers are determined from the solution of a convex optimization program with linear constraints. \nIn both cases, there is no closed form solution, and the computation of the Lagrange multipliers usually requires numerical methods.\n\nFor continuous distributions, the Shannon entropy cannot be used, as it is only defined for discrete probability spaces. Instead Edwin Jaynes (1963, 1968, 2003) gave the following formula, which is closely related to the relative entropy (see also differential entropy).\n\nwhere \"m\"(\"x\"), which Jaynes called the \"invariant measure\", is proportional to the limiting density of discrete points. For now, we shall assume that \"m\" is known; we will discuss it further after the solution equations are given.\n\nA closely related quantity, the relative entropy, is usually defined as the Kullback–Leibler divergence of \"m\" from \"p\" (although it is sometimes, confusingly, defined as the negative of this). The inference principle of minimizing this, due to Kullback, is known as the Principle of Minimum Discrimination Information.\n\nWe have some testable information \"I\" about a quantity \"x\" which takes values in some interval of the real numbers (all integrals below are over this interval). We assume this information has the form of \"m\" constraints on the expectations of the functions \"f\", i.e. we require our probability density function to satisfy the inequality (or purely equality) moment constraints:\n\nAnd of course, the probability density must integrate to one, giving the constraint\n\nThe probability density function with maximum \"H\" subject to these constraints is: \n\nwith the partition function determined by\n\nAs in the discrete case, in the case where all moment constraints are equalities, the values of the formula_12 parameters are determined by the system of nonlinear equations:\n\nIn the case with inequality moment constraints the Lagrange multipliers are determined from the solution of a convex optimization program. \n\nThe invariant measure function \"m\"(\"x\") can be best understood by supposing that \"x\" is known to take values only in the bounded interval (\"a\", \"b\"), and that no other information is given. Then the maximum entropy probability density function is\n\nwhere \"A\" is a normalization constant. The invariant measure function is actually the prior density function encoding 'lack of relevant information'. It cannot be determined by the principle of maximum entropy, and must be determined by some other logical method, such as the principle of transformation groups or marginalization theory.\n\nFor several examples of maximum entropy distributions, see the article on maximum entropy probability distributions.\n\nProponents of the principle of maximum entropy justify its use in assigning probabilities in several ways, including the following two arguments. These arguments take the use of Bayesian probability as given, and are thus subject to the same postulates.\n\nConsider a discrete probability distribution among \"m\" mutually exclusive propositions. The most informative distribution would occur when one of the propositions was known to be true. In that case, the information entropy would be equal to zero. The least informative distribution would occur when there is no reason to favor any one of the propositions over the others. In that case, the only reasonable probability distribution would be uniform, and then the information entropy would be equal to its maximum possible value, log \"m\". The information entropy can therefore be seen as a numerical measure which describes how uninformative a particular probability distribution is, ranging from zero (completely informative) to log \"m\" (completely uninformative).\n\nBy choosing to use the distribution with the maximum entropy allowed by our information, the argument goes, we are choosing the most uninformative distribution possible. To choose a distribution with lower entropy would be to assume information we do not possess. Thus the maximum entropy distribution is the only reasonable distribution.\n\nThe following argument is the result of a suggestion made by Graham Wallis to E. T. Jaynes in 1962. It is essentially the same mathematical argument used for the Maxwell–Boltzmann statistics in statistical mechanics, although the conceptual emphasis is quite different. It has the advantage of being strictly combinatorial in nature, making no reference to information entropy as a measure of 'uncertainty', 'uninformativeness', or any other imprecisely defined concept. The information entropy function is not assumed \"a priori\", but rather is found in the course of the argument; and the argument leads naturally to the procedure of maximizing the information entropy, rather than treating it in some other way.\n\nSuppose an individual wishes to make a probability assignment among \"m\" mutually exclusive propositions. She has some testable information, but is not sure how to go about including this information in her probability assessment. She therefore conceives of the following random experiment. She will distribute \"N\" quanta of probability (each worth 1/\"N\") at random among the \"m\" possibilities. (One might imagine that she will throw \"N\" balls into \"m\" buckets while blindfolded. In order to be as fair as possible, each throw is to be independent of any other, and every bucket is to be the same size.) Once the experiment is done, she will check if the probability assignment thus obtained is consistent with her information. (For this step to be successful, the information must be a constraint given by an open set in the space of probability measures). If it is inconsistent, she will reject it and try again. If it is consistent, her assessment will be\n\nwhere \"p\" is the probability of the \"i\" proposition, while \"n\" is the number of quanta that were assigned to the \"i\" proposition (i.e. the number of balls that ended up in bucket \"i\").\n\nNow, in order to reduce the 'graininess' of the probability assignment, it will be necessary to use quite a large number of quanta of probability. Rather than actually carry out, and possibly have to repeat, the rather long random experiment, the protagonist decides to simply calculate and use the most probable result. The probability of any particular result is the multinomial distribution,\n\nwhere\n\nis sometimes known as the multiplicity of the outcome.\n\nThe most probable result is the one which maximizes the multiplicity \"W\". Rather than maximizing \"W\" directly, the protagonist could equivalently maximize any monotonic increasing function of \"W\". She decides to maximize\n\nAt this point, in order to simplify the expression, the protagonist takes the limit as formula_19, i.e. as the probability levels go from grainy discrete values to smooth continuous values. Using Stirling's approximation, she finds\n\nAll that remains for the protagonist to do is to maximize entropy under the constraints of her testable information. She has found that the maximum entropy distribution is the most probable of all \"fair\" random distributions, in the limit as the probability levels go from discrete to continuous.\n\nGiffin and Caticha (2007) state that Bayes' theorem and the principle of maximum entropy are completely compatible and can be seen as special cases of the \"method of maximum relative entropy\". They state that this method reproduces every aspect of orthodox Bayesian inference methods. In addition this new method opens the door to tackling problems that could not be addressed by either the maximal entropy principle or orthodox Bayesian methods individually. Moreover, recent contributions (Lazar 2003, and Schennach 2005) show that frequentist relative-entropy-based inference approaches (such as empirical likelihood and exponentially tilted empirical likelihood – see e.g. Owen 2001 and Kitamura 2006) can be combined with prior information to perform Bayesian posterior analysis.\n\nJaynes stated Bayes' theorem was a way to calculate a probability, while maximum entropy was a way to assign a prior probability distribution.\n\nIt is however, possible in concept to solve for a posterior distribution directly from a stated prior distribution using the principle of minimum cross entropy (or the Principle of Maximum Entropy being a special case of using a uniform distribution as the given prior), independently of any Bayesian considerations by treating the problem formally as a constrained optimisation problem, the Entropy functional being the objective function. For the case of given average values as testable information (averaged over the sought after probability distribution), the sought after distribution is formally the Gibbs (or Boltzmann) distribution the parameters of which must be solved for in order to achieve minimum cross entropy and satisfy the given testable information.\n\nThe principle of maximum entropy bears a relation to a key assumption of kinetic theory of gases known as molecular chaos or \"Stosszahlansatz\". This asserts that the distribution function characterizing particles entering a collision can be factorized. Though this statement can be understood as a strictly physical hypothesis, it can also be interpreted as a heuristic hypothesis regarding the most probable configuration of particles before colliding .\n\n\n\n"}
{"id": "28408840", "url": "https://en.wikipedia.org/wiki?curid=28408840", "title": "Pytkeev space", "text": "Pytkeev space\n\nIn mathematics, and especially topology, a Pytkeev space is a topological space that satisfies qualities more subtle than a convergence of a sequence. They are named after E. G. Pytkeev, who proved in 1983 that sequential spaces have this property.\n\nLet \"X\" be a topological space. For a subset \"S\" of \"X\" let \"S\" denote the closure of \"S\". Then a point \"x\" is called a \"Pytkeev point\" if for every set A with , there is a countable formula_1-net of infinite subsets of \"A\". A \"Pytkeev space\" is a space in which every point is a Pytkeev point.\n\n"}
{"id": "55351593", "url": "https://en.wikipedia.org/wiki?curid=55351593", "title": "Quantum image", "text": "Quantum image\n\nQuantum computation, which exploits quantum parallelism, is in principle faster than classical computer for certain problems.\nQuantum image is encoding the image information in quantum-mechanical systems instead of classical ones and replacing classical with quantum information processing may alleviate some of these challenges\n\nHumans obtain most of their information through their eyes. Accordingly, the analysis of visual data is one of the most important functions of our brain and it has evolved high efficiency in processing visual data. Currently, visual information like images and videos constitutes the largest part of data traffic in the internet. Processing of this information requires ever larger computational power. \n\nThe laws of quantum mechanics allow one to reduce the required resources for some tasks by many orders of magnitude if the image data are encoded in the quantum state of a suitable physical system. The researchers discuss a suitable method for encoding image data, and develop a new quantum algorithm that can detect boundaries among parts of an image with a single logical operation. This edge-detection operation is independent of the size of the image. Several other algorithms are also discussed. It is theoretically and experimentally demonstrated that they work in practice. This is the first experiment to demonstrate practical quantum image processing. It contributes a substantial progress towards both theoretical and experimental quantum computing for image processing， it will stimulate future studies in the field of quantum information processing of visual data.\n"}
{"id": "36333997", "url": "https://en.wikipedia.org/wiki?curid=36333997", "title": "Rising moving average", "text": "Rising moving average\n\nThe rising moving average is a technical indicator used in stock market trading. Most commonly found visually, the pattern is spotted with a moving average overlay on a stock chart or price series. When the moving average has been rising consecutively for a number of days, this is used as a buy signal, to indicate a rising trend forming.\n\nWhile the rising moving average indicator is commonly used by investors without realising, there has been significant backtesting on historic stock data to calculate the performance of the rising moving average. Simulations have found that shorter rising averages, within the 3- to 10-day period, are more profitable overall than longer rising averages (e.g. 20 days). These have only been tested on US equity stocks however.\n"}
{"id": "31965557", "url": "https://en.wikipedia.org/wiki?curid=31965557", "title": "SIAM Journal on Numerical Analysis", "text": "SIAM Journal on Numerical Analysis\n\nThe SIAM Journal on Numerical Analysis (SINUM; until 1965: \"Journal of the Society for Industrial & Applied Mathematics, Series B: Numerical Analysis\") is a peer-reviewed mathematical journal published by the Society for Industrial and Applied Mathematics that covers research on the analysis of numerical methods. The journal was established in 1964 and appears bimonthly. The editor-in-chief is Pavel Bochev.\n"}
{"id": "917633", "url": "https://en.wikipedia.org/wiki?curid=917633", "title": "Sahlqvist formula", "text": "Sahlqvist formula\n\nIn modal logic, Sahlqvist formulas are a certain kind of modal formula with remarkable properties. The Sahlqvist correspondence theorem states that every Sahlqvist formula is canonical, and corresponds to a first-order definable class of Kripke frames. \n\nSahlqvist's definition characterizes a decidable set of modal formulas with first-order correspondents. Since it is undecidable, by Chagrova's theorem, whether an arbitrary modal formula has a first-order correspondent, there are formulas with first-order frame conditions that are not Sahlqvist [Chagrova 1991] (see the examples below). Hence Sahlqvist formulas define only a (decidable) subset of modal formulas with first-order correspondents.\n\nSahlqvist formulas are built up from implications, where the consequent is \"positive\" and the antecedent is of a restricted form.\n\n\n\nWhen a Sahlqvist formula is used as an axiom in a normal modal logic, the logic is guaranteed to be complete with respect to the elementary class of frames the axiom defines. This result comes from the Sahlqvist completeness theorem [Modal Logic, Blackburn \"et al.\", Theorem 4.42]. But there is also a converse theorem, namely a theorem that states which first-order conditions are the correspondents of Sahlqvist formulas. Kracht's theorem states that \"any Sahlqvist formula locally corresponds to a Kracht formula; and conversely, every Kracht formula is a local first-order correspondent of some Sahlqvist formula which can be effectively obtained from the Kracht formula\" [Modal Logic, Blackburn \"et al.\", Theorem 3.59].\n\n"}
{"id": "41038739", "url": "https://en.wikipedia.org/wiki?curid=41038739", "title": "Spherical category", "text": "Spherical category\n\nIn category theory, a branch of mathematics, a spherical category is a pivotal category (a monoidal category with traces) in which left and right traces coincide.\nSpherical fusion categories give rise to a family of three-dimensional topological state sum models (a particular formulation of a topological quantum field theory), the Turaev-Viro model, or rather Turaev-Viro-Barrett-Westbury model.\n"}
{"id": "12742593", "url": "https://en.wikipedia.org/wiki?curid=12742593", "title": "Spijker's lemma", "text": "Spijker's lemma\n\nIn mathematics, Spijker's lemma is a result in the theory of rational mappings of the Riemann sphere. It states that the image of a circle under a complex rational map with numerator and denominator having degree at most \"n\" has length at most 2\"nπ\".\n\n"}
{"id": "38624698", "url": "https://en.wikipedia.org/wiki?curid=38624698", "title": "Stochastic quantization", "text": "Stochastic quantization\n\nIn physics, stochastic quantization is a method for modelling quantum mechanics, introduced by Edward Nelson in 1966, and streamlined by Parisi and Wu.\n\nStochastic quantization serves to quantize Euclidean field theories, and is used for numerical applications, such as numerical simulations of gauge theories with fermions. This serves to address the problem of fermion doubling that usually occurs in these numerical calculations.\n\nStochastic quantization takes advantage of the fact that a Euclidean quantum field theory can be modeled as the equilibrium limit of a statistical mechanical system coupled to a heat bath. In particular, in the path integral representation of a Euclidean quantum field theory, the path integral measure is closely related to the Boltzmann distribution of a statistical mechanical system in equilibrium. In this relation, Euclidean Green's functions become correlation functions in the statistical mechanical system. A statistical mechanical system in equilibrium can be modeled, via the ergodic hypothesis, as the stationary distribution of a stochastic process. Then the Euclidean path integral measure can also be thought of as the stationary distribution of a stochastic process; hence the name stochastic quantization.\n"}
{"id": "1440207", "url": "https://en.wikipedia.org/wiki?curid=1440207", "title": "Symmetric polynomial", "text": "Symmetric polynomial\n\nIn mathematics, a symmetric polynomial is a polynomial in variables, such that if any of the variables are interchanged, one obtains the same polynomial. Formally, is a \"symmetric polynomial\" if for any permutation of the subscripts one has .\n\nSymmetric polynomials arise naturally in the study of the relation between the roots of a polynomial in one variable and its coefficients, since the coefficients can be given by polynomial expressions in the roots, and all roots play a similar role in this setting. From this point of view the elementary symmetric polynomials are the most fundamental symmetric polynomials. A theorem states that any symmetric polynomial can be expressed in terms of elementary symmetric polynomials, which implies that every \"symmetric\" polynomial expression in the roots of a monic polynomial can alternatively be given as a polynomial expression in the coefficients of the polynomial.\n\nSymmetric polynomials also form an interesting structure by themselves, independently of any relation to the roots of a polynomial. In this context other collections of specific symmetric polynomials, such as complete homogeneous, power sum, and Schur polynomials play important roles alongside the elementary ones. The resulting structures, and in particular the ring of symmetric functions, are of great importance in combinatorics and in representation theory.\n\nSymmetric polynomials in two variables \"X\", \"X\":\nand in three variables \"X\", \"X\", \"X\":\nThere are many ways to make specific symmetric polynomials in any number of variables, see the various types below. An example of a somewhat different flavor is\nwhere first a polynomial is constructed that changes sign under every exchange of variables, and taking the square renders it completely symmetric (if the variables represent the roots of a monic polynomial, this polynomial gives its discriminant).\n\nOn the other hand, the polynomial in two variables\nis not symmetric, since if one exchanges formula_6 and formula_7 one gets a different polynomial, formula_8. Similarly in three variables\nhas only symmetry under cyclic permutations of the three variables, which is not sufficient to be a symmetric polynomial. However, the following is symmetric:\n\nOne context in which symmetric polynomial functions occur is in the study of monic univariate polynomials of degree \"n\" having \"n\" roots in a given field. These \"n\" roots determine the polynomial, and when they are considered as independent variables, the coefficients of the polynomial are symmetric polynomial functions of the roots. Moreover the fundamental theorem of symmetric polynomials implies that a polynomial function \"f\" of the \"n\" roots can be expressed as (another) polynomial function of the coefficients of the polynomial determined by the roots if and only if \"f\" is given by a symmetric polynomial.\n\nThis yields the approach to solving polynomial equations by inverting this map, \"breaking\" the symmetry – given the coefficients of the polynomial (the elementary symmetric polynomials in the roots), how can one recover the roots?\nThis leads to studying solutions of polynomials using the permutation group of the roots, originally in the form of Lagrange resolvents, later developed in Galois theory.\n\nConsider a monic polynomial in \"t\" of degree \"n\"\n\nwith coefficients \"a\" in some field \"k\". There exist \"n\" roots \"x\",…,\"x\" of \"P\" in some possibly larger field (for instance if \"k\" is the field of real numbers, the roots will exist in the field of complex numbers); some of the roots might be equal, but the fact that one has \"all\" roots is expressed by the relation\n\nBy comparison of the coefficients one finds that\n\nThese are in fact just instances of Viète's formulas. They show that all coefficients of the polynomial are given in terms of the roots by a symmetric polynomial expression: although for a given polynomial \"P\" there may be qualitative differences between the roots (like lying in the base field \"k\" or not, being simple or multiple roots), none of this affects the way the roots occur in these expressions.\n\nNow one may change the point of view, by taking the roots rather than the coefficients as basic parameters for describing \"P\", and considering them as indeterminates rather than as constants in an appropriate field; the coefficients \"a\" then become just the particular symmetric polynomials given by the above equations. Those polynomials, without the sign formula_14, are known as the elementary symmetric polynomials in \"x\",…,\"x\". A basic fact, known as the fundamental theorem of symmetric polynomials states that \"any\" symmetric polynomial in \"n\" variables can be given by a polynomial expression in terms of these elementary symmetric polynomials. It follows that any symmetric polynomial expression in the roots of a monic polynomial can be expressed as a polynomial in the \"coefficients\" of the polynomial, and in particular that its value lies in the base field \"k\" that contains those coefficients. Thus, when working only with such symmetric polynomial expressions in the roots, it is unnecessary to know anything particular about those roots, or to compute in any larger field than \"k\" in which those roots may lie. In fact the values of the roots themselves become rather irrelevant, and the necessary relations between coefficients and symmetric polynomial expressions can be found by computations in terms of symmetric polynomials only. An example of such relations are Newton's identities, which express the sum of any fixed power of the roots in terms of the elementary symmetric polynomials.\n\nThere are a few types of symmetric polynomials in the variables \"X\", \"X\", …, \"X\" that are fundamental.\n\nFor each nonnegative integer \"k\", the elementary symmetric polynomial \"e\"(\"X\", …, \"X\") is the sum of all distinct products of \"k\" distinct variables. (Some authors denote it by σ instead.) For \"k\" = 0 there is only the empty product so \"e\"(\"X\", …, \"X\") = 1, while for \"k\" > \"n\", no products at all can be formed, so \"e\"(\"X\", \"X\", …, \"X\") = 0 in these cases. The remaining \"n\" elementary symmetric polynomials are building blocks for all symmetric polynomials in these variables: as mentioned above, any symmetric polynomial in the variables considered can be obtained from these elementary symmetric polynomials using multiplications and additions only. In fact one has the following more detailed facts:\nFor example, for \"n\" = 2, the relevant elementary symmetric polynomials are \"e\"(\"X\", \"X\") = \"X\"+\"X\", and \"e\"(\"X\", \"X\") = \"X\"\"X\". The first polynomial in the list of examples above can then be written as\n(for a proof that this is always possible see the fundamental theorem of symmetric polynomials).\n\nPowers and products of elementary symmetric polynomials work out to rather complicated expressions. If one seeks basic \"additive\" building blocks for symmetric polynomials, a more natural choice is to take those symmetric polynomials that contain only one type of monomial, with only those copies required to obtain symmetry. Any monomial in \"X\", …, \"X\" can be written as \"X\"…\"X\" where the exponents α are natural numbers (possibly zero); writing α = (α,…,α) this can be abbreviated to \"X\". The monomial symmetric polynomial \"m\"(\"X\", …, \"X\") is defined as the sum of all monomials \"x\" where β ranges over all \"distinct\" permutations of (α,…,α). For instance one has\nClearly \"m\" = \"m\" when β is a permutation of α, so one usually considers only those \"m\" for which α ≥ α ≥ … ≥ α, in other words for which α is a partition of an integer.\nThese monomial symmetric polynomials form a vector space basis: every symmetric polynomial \"P\" can be written as a linear combination of the monomial symmetric polynomials. To do this it suffices to separate the different types of monomial occurring in \"P\". In particular if \"P\" has integer coefficients, then so will the linear combination.\n\nThe elementary symmetric polynomials are particular cases of monomial symmetric polynomials: for 0 ≤ \"k\" ≤ \"n\" one has\n\nFor each integer \"k\" ≥ 1, the monomial symmetric polynomial \"m\"(\"X\", …, \"X\") is of special interest. It is the power sum symmetric polynomial, defined as\nAll symmetric polynomials can be obtained from the first \"n\" power sum symmetric polynomials by additions and multiplications, possibly involving rational coefficients. More precisely,\nIn particular, the remaining power sum polynomials \"p\"(\"X\", …, \"X\") for \"k\" > \"n\" can be so expressed in the first \"n\" power sum polynomials; for example\n\nIn contrast to the situation for the elementary and complete homogeneous polynomials, a symmetric polynomial in \"n\" variables with \"integral\" coefficients need not be a polynomial function with integral coefficients of the power sum symmetric polynomials.\nFor an example, for \"n\" = 2, the symmetric polynomial\nhas the expression\nUsing three variables one gets a different expression\nThe corresponding expression was valid for two variables as well (it suffices to set \"X\" to zero), but since it involves \"p\", it could not be used to illustrate the statement for \"n\" = 2. The example shows that whether or not the expression for a given monomial symmetric polynomial in terms of the first \"n\" power sum polynomials involves rational coefficients may depend on \"n\". But rational coefficients are \"always\" needed to express elementary symmetric polynomials (except the constant ones, and \"e\" which coincides with the first power sum) in terms of power sum polynomials. The Newton identities provide an explicit method to do this; it involves division by integers up to \"n\", which explains the rational coefficients. Because of these divisions, the mentioned statement fails in general when coefficients are taken in a field of finite characteristic; however it is valid with coefficients in any ring containing the rational numbers.\n\nFor each nonnegative integer \"k\", the complete homogeneous symmetric polynomial \"h\"(\"X\", …, \"X\") is the sum of all distinct monomials of degree \"k\" in the variables \"X\", …, \"X\". For instance\nThe polynomial \"h\"(\"X\", …, \"X\") is also the sum of all distinct monomial symmetric polynomials of degree \"k\" in \"X\", …, \"X\", for instance for the given example\n\nAll symmetric polynomials in these variables can be built up from complete homogeneous ones: any symmetric polynomial in \"X\", …, \"X\" can be obtained from the complete homogeneous symmetric polynomials \"h\"(\"X\", …, \"X\"), …, \"h\"(\"X\", …, \"X\") via multiplications and additions. More precisely:\nFor example, for \"n\" = 2, the relevant complete homogeneous symmetric polynomials are and . The first polynomial in the list of examples above can then be written as\nAs in the case of power sums, the given statement applies in particular to the complete homogeneous symmetric polynomials beyond \"h\"(\"X\", …, \"X\"), allowing them to be expressed in terms of the ones up to that point; again the resulting identities become invalid when the number of variables is increased.\n\nAn important aspect of complete homogeneous symmetric polynomials is their relation to elementary symmetric polynomials, which can be expressed as the identities\nSince \"e\"(\"X\", …, \"X\") and \"h\"(\"X\", …, \"X\") are both equal to 1, one can isolate either the first or the last term of these summations; the former gives a set of equations that allows one to recursively express the successive complete homogeneous symmetric polynomials in terms of the elementary symmetric polynomials, and the latter gives a set of equations that allows doing the inverse. This implicitly shows that any symmetric polynomial can be expressed in terms of the \"h\"(\"X\", …, \"X\") with 1 ≤ \"k\" ≤ \"n\": one first expresses the symmetric polynomial in terms of the elementary symmetric polynomials, and then expresses those in terms of the mentioned complete homogeneous ones.\n\nAnother class of symmetric polynomials is that of the Schur polynomials, which are of fundamental importance in the applications of symmetric polynomials to representation theory. They are however not as easy to describe as the other kinds of special symmetric polynomials; see the main article for details.\n\nSymmetric polynomials are important to linear algebra, representation theory, and Galois theory. They are also important in combinatorics, where they are mostly studied through the ring of symmetric functions, which avoids having to carry around a fixed number of variables all the time.\n\nAnalogous to symmetric polynomials are alternating polynomials: polynomials that, rather than being \"invariant\" under permutation of the entries, change according to the sign of the permutation.\n\nThese are all products of the Vandermonde polynomial and a symmetric polynomial, and form a quadratic extension of the ring of symmetric polynomials: the Vandermonde polynomial is a square root of the discriminant.\n\n\n"}
{"id": "18305460", "url": "https://en.wikipedia.org/wiki?curid=18305460", "title": "Tutte matrix", "text": "Tutte matrix\n\nIn graph theory, the Tutte matrix \"A\" of a graph \"G\" = (\"V\", \"E\") is a matrix used to determine the existence of a perfect matching: that is, a set of edges which is incident with each vertex exactly once.\n\nIf the set of vertices \"V\" has \"n\" elements then the Tutte matrix is an \"n\" × \"n\" matrix A with entries\n\nwhere the \"x\" are indeterminates. The determinant of this skew-symmetric matrix is then a polynomial (in the variables \"x\", \"i < j\" ): this coincides with the square of the pfaffian of the matrix \"A\" and is non-zero (as a polynomial) if and only if a perfect matching exists. (This polynomial is not the Tutte polynomial of \"G\".)\n\nThe Tutte matrix is named after W. T. Tutte, and is a generalisation of the Edmonds matrix for a balanced bipartite graph.\n\n"}
{"id": "20714332", "url": "https://en.wikipedia.org/wiki?curid=20714332", "title": "Vandermonde polynomial", "text": "Vandermonde polynomial\n\nIn algebra, the Vandermonde polynomial of an ordered set of \"n\" variables formula_1, named after Alexandre-Théophile Vandermonde, is the polynomial:\n\nIt is also called the Vandermonde determinant, as it is the determinant of the Vandermonde matrix.\n\nThe value depends on the order of the terms: it is an alternating polynomial, not a symmetric polynomial.\n\nThe defining property of the Vandermonde polynomial is that it is \"alternating\" in the entries, meaning that permuting the formula_5 by an odd permutation changes the sign, while permuting them by an even permutation does not change the value of the polynomial – in fact, it is the basic alternating polynomial, as will be made precise below.\n\nIt thus depends on the order, and is zero if two entries are equal – this also follows from the formula, but is also consequence of being alternating: if two variables are equal, then switching them both does not change the value and inverts the value, yielding formula_6 and thus formula_7 (assuming the characteristic is not 2, otherwise being alternating is equivalent to being symmetric).\n\nConversely, the Vandermonde polynomial is a factor of every alternating polynomial: as shown above, an alternating polynomial vanishes if any two variables are equal, and thus must have formula_8 as a factor for all formula_9.\n\nThus, the Vandermonde polynomial (together with the symmetric polynomials) generates the alternating polynomials.\n\nIts square is widely called the discriminant, though some sources call the Vandermonde polynomial itself the discriminant.\n\nThe discriminant (the square of the Vandermonde polynomial: formula_10) does not depend on the order of terms, as formula_11, and is thus an invariant of the \"unordered\" set of points.\n\nIf one adjoins the Vandermonde polynomial to the ring of symmetric polynomials in \"n\" variables formula_12, one obtains the quadratic extension formula_13, which is the ring of alternating polynomials.\n\nGiven a polynomial, the Vandermonde polynomial of its roots is defined over the splitting field; for a non-monic polynomial, with leading coefficient \"a\", one may define the Vandermonde polynomial as\n(multiplying with a leading term) to accord with the discriminant.\n\nOver arbitrary rings, one instead uses a different polynomial to generate the alternating polynomials – see (Romagny, 2005).\n\n (a vast generalization)\n\nThe Vandermonde polynomial can be considered a special case of the Weyl character formula, specifically the Weyl denominator formula (the case of the trivial representation) of the special unitary group formula_15.\n\n\n"}
{"id": "15507968", "url": "https://en.wikipedia.org/wiki?curid=15507968", "title": "Vector field reconstruction", "text": "Vector field reconstruction\n\nVector field reconstruction is a method of creating a vector field from experimental or computer generated data, usually with the goal of finding a differential equation model of the system.\n\nA differential equation model is one that describes the value of dependent variables as they evolve in time or space by giving equations involving those variables and their derivatives with respect to some independent variables, usually time and/or space. An ordinary differential equation is one in which the system's dependent variables are functions of only one independent variable. Many physical, chemical, biological and electrical systems are well described by ordinary differential equations. Frequently we assume a system is governed by differential equations, but we do not have exact knowledge of the influence of various factors on the state of the system. For instance, we may have an electrical circuit that in theory is described by a system of ordinary differential equations, but due to the tolerance of resistors, variations of the supply voltage or interference from outside influences we do not know the exact parameters of the system. For some systems, especially those that support chaos, a small change in parameter values can cause a large change in the behavior of the system, so an accurate model is extremely important. Therefore, it may be necessary to construct more exact differential equations by building them up based on the actual system performance rather than a theoretical model. Ideally, one would measure all the dynamical variables involved over an extended period of time, using many different initial conditions, then build or fine tune a differential equation model based on these measurements.\n\nIn some cases we may not even know enough about the processes involved in a system to even formulate a model. In other cases, we may have access to only one dynamical variable for our measurements, i.e., we have a scalar time series. If we only have a scalar time series, we need to use the method of time delay embedding or derivative coordinates to get a large enough set of dynamical variables to describe the system.\n\nIn a nutshell, once we have a set of measurements of the system state over some period of time, we find the derivatives of these measurements, which gives us a local vector field, then determine a global vector field consistent with this local field. This is usually done by a least squares fit to the derivative data.\n\nIn the best possible case, one has data streams of measurements of all the system variables, equally spaced in time, say\n\nfor\n\nbeginning at several different initial conditions. Then the task of finding a vector field, and thus a differential equation model consists of fitting functions, for instance, a cubic spline, to the data to obtain a set of continuous time functions\n\ncomputing time derivatives dx/dt, dx/dt...,dx/dt of the functions, then making a least squares fit using some sort of orthogonal basis functions (orthogonal polynomials, radial basis functions, etc.) to each component of the tangent vectors to find a global vector field. A differential equation then can be read off the global vector field.\n\nThere are various methods of creating the basis functions for the least squares fit. The most common method is the Gram–Schmidt process. Which creates a set of orthogonal basis vectors, which can then easily be normalized. This method begins by first selecting any standard basis β={v, v...,v}. Next, set the first vector v=u. Then, we set u=v-projv. This process is repeated to for k vectors, with the final vector being u= v-∑projv. This then creates a set of orthogonal standard basis vectors.\n\nThe reason for using a standard orthogonal basis rather than a standard basis arises from the creation of the least squares fitting done next. Creating a least-squares fit begins by assuming some function, in the case of the reconstruction an n degree polynomial, and fitting the curve to the data using constants. The accuracy of the fit can be increased by increasing the degree of the polynomial being used to fit the data. If a set of non-orthogonal standard basis functions was used, it becomes necessary to recalculate the constant coefficients of the function describing the fit. However, by using the orthogonal set of basis functions, it is not necessary to recalculate the constant coefficients.\n\nVector field reconstruction has several applications, and many different approaches. Some mathematicians have not only used radial basis functions and polynomials to reconstruct a vector field, but they have used Lyapunov exponents and singular value decomposition. Gouesbet and Letellier used a multivariate polynomial approximation and least squares to reconstruct their vector field. This method was applied to the Rössler system, and the Lorenz system, as well as thermal lens oscillations.\n\nThe Rossler system, Lorenz system and Thermal lens oscillation follows the differential equations in standard system as\n\nwhere F(X,Y,Z) is known as the standard function.\n\nIn some situation the model is not very efficient and difficulties can arise if the model has a large number of coefficients and demonstrates a divergent solution. For example, nonautonomous differential equations give the previously described results. In this case the modification of the standard approach in application gives a better way of further development of global vector reconstruction.\n\nUsually the system being modeled in this way is a chaotic dynamical system, because chaotic systems explore a large part of the phase space and the estimate of the global dynamics based on the local dynamics will be better than with a system exploring only a small part of the space.\n\nFrequently, one has only a single scalar time series measurement from a system known to have more than one degree of freedom. The time series may not even be from a system variable, but may be instead of a function of all the variables, such as temperature in a stirred tank reactor using several chemical species. In this case, one must use the technique of delay coordinate embedding, where a state vector consisting of the data at time t and several delayed versions of the data is constructed.\n\nA comprehensive review of the topic is available from \n"}
{"id": "3545503", "url": "https://en.wikipedia.org/wiki?curid=3545503", "title": "VisSim", "text": "VisSim\n\nVisSim is a visual block diagram program for simulation of dynamical systems and model based design of embedded systems, with its own visual language. It is developed by Visual Solutions of Westford, Massachusetts. Visual Solutions, has been acquired by Altair in August 2015 and its products have been rebranded as solidThinking Embed as a part of solidThinking's Model Based Development Suite. With solidThinking Embed, you can develop virtual prototypes of dynamic systems. Models are built by sliding blocks into the work area and wiring them together with the mouse. Embed automatically converts the control diagrams into C-code ready to be downloaded to the target hardware.\n\nVisSim or now solidThinking Embed uses a graphical data flow paradigm to implement dynamic systems based on differential equations. Version 8 adds interactive UML OMG 2 compliant state chart graphs that are placed in VisSim diagrams. This allows the modeling of state based systems such as startup sequencing of process plants or serial protocol decoding.\n\nVisSim/solidThinking Embed is used in control system design and digital signal processing for multidomain simulation and design. It includes blocks for arithmetic, Boolean, and transcendental functions, as well as digital filters, transfer functions, numerical integration and interactive plotting. The most commonly modeled systems are aeronautical, biological/medical, digital power, electric motor, electrical, hydraulic, mechanical, process, thermal/HVAC and econometric.\n\nA read-only version of the software, VisSim Viewer, is available free of charge and provides a way for people not licensed to use VisSim to run VisSim models. This program is intended to allow models to be more widely shared while preserving the model in its published form. The viewer will execute any VisSim model, and only allows changes to block and simulation parameters to illustrate different design scenarios. Sliders and buttons may be activated if included in the model.\n\nThe \"VisSim/C-Code\" add-on generates ANSI C code for the model, and generates target specific code for on-chip devices like PWM, ADC, encoder, GPIO, I2C etc. This is useful for development of embedded systems. After the behavior of the controller has been simulated, C-code can be generated, compiled and run on the target. For debugging, VisSim supports an interactive JTAG linkage, called \"Hotlink\", that allows interactive gain change and plotting of on-target variables. The VisSim generated code has been called efficient and readable, making it well suited for development of embedded systems. VisSim's author served on the X3J11 ANSI C committee and wrote several C compilers, in addition to co-authoring a book on C. This deep understanding of ANSI C, and the nature of the resulting machine code when compiled, is the key to the code generator's efficiency. VisSim can target small 16-bit fixed point systems like the Texas Instruments MSP430, using only 740 bytes flash and 64 bytes of RAM for a small closed-loop Pulse-width modulation (PWM) actuated system, as well as allowing very high control sample rates over 500 kHz on larger 32-bit floating point processors like the Texas Instruments 150 MHz F28335.\n\nThe technique of simulating system performance off-line, and then generating code from the simulation is known as \"model-based development\". Model-based development for embedded systems is becoming widely adopted for production systems because it shortens development cycles for hardware development in the same way that Model-driven architecture shortens production cycles for software development.\n\nModel building is a visual way of describing a situation. In an engineering context, instead of writing and solving a system of equations, model building involves using visual \"blocks\" to solve the problem. The advantage of using models is that in some cases problems which appear difficult if expressed mathematically may be easier to understand when represented pictorially.\n\nVisSim uses a hierarchical composition to create nested block diagrams. A typical model would consist of \"virtual plants\" composed of various VisSim \"layers\", combined if necessary with custom blocks written in C or FORTRAN. A virtual controller can be added and tuned to give desired overall system response. Graphical control element such as sliders and buttons allow control of what-if analysis for operator training or controller tuning.\n\nAlthough VisSim was originally designed for use by control engineers, it can be used for any type of mathematical model.\n\n\n\n\n"}
{"id": "356161", "url": "https://en.wikipedia.org/wiki?curid=356161", "title": "Émile Borel", "text": "Émile Borel\n\nFélix Édouard Justin Émile Borel (; 7 January 1871 – 3 February 1956) was a French mathematician and politician. As a mathematician, he was known for his founding work in the areas of measure theory and probability.\n\nBorel was born in Saint-Affrique, Aveyron, the son of a Protestant pastor. He studied at the Collège Sainte-Barbe and Lycée Louis-le-Grand before applying to both the École normale supérieure and the École Polytechnique. Although he qualified for both, he chose to attend the former institution in 1889. That year he won the concours général, an annual national mathematics competition. After graduating in 1892, he placed first in the agrégation, a competitive civil service examination leading to the position of professeur agrégé. His thesis, published in 1893, was titled \"Sur quelques points de la théorie des fonctions\" (\"On some points in the theory of functions\"). That year, Borel started a four-year stint as a lecturer at the University of Lille, during which time he published 22 research papers. He returned to the École normale in 1897, and was appointed to the chair of theory of function, which he held until 1941.\n\nIn 1901, Borel married 17-year-old Marguerite, the daughter of colleague Paul Émile Appel; she later wrote more than 30 novels under the pseudonym Camille Marbo. Émile Borel died in Paris on 3 February 1956.\n\nAlong with René-Louis Baire and Henri Lebesgue, Émile Borel was among the pioneers of measure theory and its application to probability theory. The concept of a Borel set is named in his honor. One of his books on probability introduced the amusing thought experiment that entered popular culture under the name infinite monkey theorem or the like. He also published a series of papers (1921–27) that first defined games of strategy.\n\nIn 1913 and 1914 he bridged the gap between hyperbolic geometry and special relativity with expository work. For instance, his book \"Introduction Geometrique à quelques Théories Physiques\" described hyperbolic rotations as transformations that leave a hyperbola stable just as a circle around a rotational center is stable.\n\nIn 1928 he co-founded Institut Henri Poincaré in Paris.\n\nIn the 1920s, 1930s, and 1940s, he was active in politics. In 1922, he founded Paris Institute of Statistics, the oldest French school for statistics. From 1924 to 1936, he was a member of the French National Assembly. In 1925, he was Minister of Marine in the cabinet of fellow mathematician Paul Painlevé. During the Second World War, he was a member of the French Resistance.\n\nBesides the \"Centre Émile Borel\" at the Institut Henri Poincaré in Paris\nand a crater on the Moon, the following mathematical notions are named after him:\n\nBorel also described a poker model which he coins \"La Relance\" in his 1938 book \"Applications de la théorie des probabilités aux Jeux de Hasard\".\n\nBorel was awarded the Resistance Medal in 1950.\n\n\n\n 8. Michel Pinault, Emile Borel, une carrière intellectuelle sous la 3ème République, Paris, L'Harmattan, 2017. Voir : michel-pinault.over-blog.com\n\n \n"}
