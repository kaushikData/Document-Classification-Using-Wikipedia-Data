{"id": "690769", "url": "https://en.wikipedia.org/wiki?curid=690769", "title": "Acyclic coloring", "text": "Acyclic coloring\n\nIn graph theory, an acyclic coloring is a (proper) vertex coloring in which every 2-chromatic subgraph is acyclic.\nThe acyclic chromatic number A(\"G\") of a graph \"G\" is the least number of colors needed in any acyclic coloring of \"G\".\n\nAcyclic coloring is often associated with graphs embedded on non-plane surfaces.\n\nA(\"G\") ≤ 2 if and only if \"G\" is acyclic.\n\nBounds on A(\"G\") in terms of the maximum degree Δ(\"G\") of \"G\" include the following:\n\nA milestone in the study of acyclic coloring is the following affirmative answer to a conjecture of \nGrünbaum:\n\nTheorem. \n\nBorodin's proof involved several years of painstaking inspection of 450 reducible configurations.\nOne consequence of this theorem is that every planar graph can be decomposed into an independent set and two induced forests. \n\nIt is NP-complete to determine whether A(\"G\") ≤ 3. \n\nSince chordal graphs can be optimally colored in \"O(n+m)\" time, the same is also true for acyclic coloring on that class of graphs.\n\nA linear-time algorithm to acyclically color a graph of maximum degree ≤ 3 using 4 colors or fewer was given by .\n\n\n\n"}
{"id": "42691004", "url": "https://en.wikipedia.org/wiki?curid=42691004", "title": "Arthur Hobbs (mathematician)", "text": "Arthur Hobbs (mathematician)\n\nArthur Hobbs (born 1940) is an American mathematician specializing in graph theory. He spent his teaching career at Texas A&M University.\n\nArthur Hobbs was born on June 19, 1940, in Washington, D.C. He is the eldest child of his family, having two younger brothers. His father was an engineer and later became an attorney. The family moved in 1941 to Pennsylvania, and again after World War II to South Bend, Indiana, where Arthur Hobbs grew up. He married his wife Barbara in 1964; they have two daughters and five grandchildren.\n\nAfter graduating in 1958 from John Adams High School, Hobbs studied mathematics at the University of Michigan, graduating in 1962. He then served in the US Army in Washington, D.C., for approximately two years, and then from 1965 to 1968 worked for the National Bureau of Standards.\n\nHe received his Ph.D. from the University of Waterloo in Ontario, Canada, in 1971. His research focused on Hamiltonian cycles, particularly concentrating in squares and higher powers of graphs, and his thesis adviser was the graph theorist William Thomas Tutte.\n\nAfter receiving his Ph.D., Hobbs began teaching as a mathematics professor at Texas A&M University in 1971, where he worked until his retirement in 2008. He was the faculty senator for twelve years, and also taught various mathematics courses including, but not limited to calculus, combinatorics, discrete mathematics, graph theory, and number theory. Hobbs and his colleague taught a course in the intersection of graph theory and number theory, he explains:\n\nHobbs' research before entering graduate school was on thickness of graphs. Later, in graduate school and for ten years following, he concentrated on Hamiltonian cycles, particularly in squares and higher powers of graphs. He then spent a couple of years working on the Gyarfas and Lehel conjecture that any family of trees T1; T2; : : : Tn, with 1; 2; : : : ; \"n\" vertices respectively, can be packed in an edge-disjoint manner into the complete graph on \"n\" vertices. This conjecture is still open. Hobbs has also worked with packings of graphs with trees and coverings by trees, which he worked on with several co-authors, including Paul A. Catlin, Jerrold W. Grossman, Lavanya Kannan, and Hong-Jian Lai.\n\nThey defined the fractional arboricity of a graph as\n\nwhere \"ω\"(\"H\" is the number of components of H and the maximum is taken over all subgraphs H for which the denominator is not zero. They also defined the strength of a graph as\n\nwhere the maximum is taken over all subsets \"S\" of \"E\"(\"G\") for which the denominator is not zero. Additionally, they characterized uniformly dense graphs, and have found several classes of uniformly dense graphs and several ways of constructing such graphs.\n\nHobbs has also done research in matroid theory.\n\nDr. Hobbs has 40 publications in graph theory, and in 1989 co-authored the book \"Elementary Linear Algebra.\" He has also written an essay on how to read research papers. A few publications are listed below:\n\n\n"}
{"id": "5377011", "url": "https://en.wikipedia.org/wiki?curid=5377011", "title": "Bangladesh Mathematical Olympiad", "text": "Bangladesh Mathematical Olympiad\n\nThe Bangladesh Mathematical Olympiad (BdMO) is an annual mathematical competition arranged for school and college students to nourish their interest and capabilities for mathematics. It has been regularly organized by the Bangladesh Math Olympiad Committee (BdMOC) since 2001. The first Math Olympiad was held in Shahjalal University of Science and Technology. Mohammad Kaykobad, Muhammad Zafar Iqbal and Munir Hasan were instrumental in establishing the Mathematics Olympiad in Bangladesh.\n\nWith the dedicated endeavor of the members of the committee, the daily newspaper \"Prothom Alo\" and the Dutch Bangla Bank Limitedf, BdMOC has already achieved its primary goal – to send a team to the International Mathematical Olympiad (IMO). Bangladeshi students have participated in the International Mathematical Olympiad since 2005.\n\nBesides arranging Divisional and National Math Olympiads, BdMOC always extends its cooperation to all interested groups and individuals who want to arrange a Mathematics Olympiad. The Bangladesh Math Olympiad and the selection of the Bangladeshi national team for the IMO is bounded by rules set by the BdMOC, called BdMO Regulations. The Bangladesh Mathematical Olympiad is open for school and college students from the country. The competitions usually take place around December–January–February. In IMO 2014, the Bangladesh team achieved one silver, one bronze and four honorable mentions, placing the country at 53 among 101 participating countries. In IMO 2015, the Bangladesh team achieved one silver, four bronze and one honorable mention, finishing in 33rd place. In IMO 2018 Bangladesh won a gold medal for the first time. \n\nThe students are divided into four academic categories:\n\nThe country is divided in 24 regions for the Regional Olympiad. In each division except Dhaka, nearly 60 students among 1000 participants are selected for the National Olympiad. In Dhaka, the number of participants is more than 3000 and 100–150 are selected for the National Olympiad. In all of the problems in the Regional Olympiad, only the final answers are necessary.\nIn the National Olympiad, the top 71 participants are given prizes. The time given for solving the problems depends on the category: 2 hours for the Primary category, 3 hours for the Junior category, and 4 hours for the Secondary and Higher Secondary category.\n\nA group for the National Math Camp is selected from the winners of the National Olympiad.\n\nFor every year since 2005, Bangladeshi students have participated in the International Mathematical Olympiad.\nSamin Riasat and Nazia Chowdhury won bronze medals for Bangladesh in 2009. Dhananjoy Biswas won the first silver medal for Bangladesh in 2012.\n\nThe following is the full list of medal winners from Bangladesh:\n"}
{"id": "34248077", "url": "https://en.wikipedia.org/wiki?curid=34248077", "title": "Bel–Robinson tensor", "text": "Bel–Robinson tensor\n\nIn general relativity and differential geometry, the Bel–Robinson tensor is a tensor defined in the abstract index notation by:\nAlternatively,\nwhere formula_3 is the Weyl tensor. It was introduced by Lluís Bel in 1959. The Bel–Robinson tensor is constructed from the Weyl tensor in a manner analogous to the way the electromagnetic stress–energy tensor is built from the electromagnetic tensor. Like the electromagnetic stress–energy tensor, the Bel–Robinson tensor is totally symmetric and traceless:\nIn general relativity, there is no unique definition of the local energy of the gravitational field. The Bel–Robinson tensor is a possible definition for local energy, since it can be shown that whenever the Ricci tensor vanishes (i.e. in vacuum), the Bel–Robinson tensor is divergence-free:\n"}
{"id": "56558", "url": "https://en.wikipedia.org/wiki?curid=56558", "title": "Blood pressure", "text": "Blood pressure\n\nBlood pressure (BP) is the pressure of circulating blood on the walls of blood vessels. Used without further specification, \"blood pressure\" usually refers to the pressure in large arteries of the systemic circulation. Blood pressure is usually expressed in terms of the systolic pressure (maximum during one heart beat) over diastolic pressure (minimum in between two heart beats) and is measured in millimeters of mercury (mmHg), above the surrounding atmospheric pressure.\n\nBlood pressure is one of the vital signs, along with respiratory rate, heart rate, oxygen saturation, and body temperature. Normal resting blood pressure in an adult is approximately systolic, and diastolic, abbreviated \"120/80 mmHg\".\n\nTraditionally, blood pressure was measured non-invasively using a mercury-tube sphygmomanometer, or an aneroid gauge, which is still generally considered to be the gold standard of accuracy for auscultatory readings. More recently other semi-automated methods have become common, largely due to concerns about potential mercury toxicity, although cost and ease of use have also influenced this trend. Early automated alternatives to mercury-tube sphygmomanometers were often seriously inaccurate, but validated devices allow for an average difference between two standardized reading methods of 5 mm Hg or less and a standard deviation of less than 8 mm Hg.\n\nBlood pressure is influenced by cardiac output, total peripheral resistance and arterial stiffness and varies depending on situation, emotional state, activity, and relative health/disease states. In the short term, blood pressure is regulated by baroreceptors which act via the brain to influence nervous and endocrine systems.\n\nBlood pressure that is low is called hypotension, and pressure that is consistently high is hypertension. Both have many causes and may be of sudden onset or of long duration. Long-term hypertension is a risk factor for many diseases, including heart disease, stroke and kidney failure. Long-term hypertension is more common than long-term hypotension, which often goes undetected because of infrequent monitoring and the absence of symptoms.\n\nThe risk of cardiovascular disease increases progressively above 115/75 mmHg. In practice blood pressure is considered too low only if noticeable symptoms are present.\n\nObservational studies demonstrate that people who maintain arterial pressures at the low end of these pressure ranges have much better long term cardiovascular health. There is an ongoing medical debate over what is the optimal level of blood pressure to target when using drugs to lower blood pressure with hypertension, particularly in older people.\n\nThe table shows the classification of blood pressure adopted by the American Heart Association for adults who are 18 years and older. It assumes the values are a result of averaging resting blood pressure readings measured at two or more visits to the doctor.\n\nIn November 2017 the American Heart Association announced revised definitions for blood pressure categories that increased the number of people considered to have high blood pressure.\n\nIn the UK, clinic blood pressures are usually categorized into three groups; low (90/60 or lower), normal (between 90/60 and 139/89), and high (140/90 or higher).\n\nBlood pressure fluctuates from minute to minute and normally shows a circadian rhythm over a 24-hour period, with highest readings in the early morning and evenings and lowest readings at night. Loss of the normal fall in blood pressure at night is associated with a greater future risk of cardiovascular disease and there is evidence that night-time blood pressure is a stronger predictor of cardiovascular events than day-time blood pressure. Also, an individual's blood pressure varies with exercise, emotional reactions, sleep, digestion and time of day (circadian rhythm).\n\nVarious other factors, such as age and sex, also influence a person's blood pressure. In children, the normal ranges are lower than for adults and depend on height. Reference blood pressure values have been developed for children in different countries, based on the distribution of blood pressure in children of these countries. As adults age, systolic pressure tends to rise and diastolic pressure tends to fall. Consequently, in the elderly, systolic blood pressure often exceeds the normal adult range, this is thought to be due to increased stiffness of the arteries.\n\nDifferences between left and right arm blood pressure measurements tend to be small. However, occasionally there is a consistent difference greater than 10 mmHg which may need further investigation, e.g. for obstructive arterial disease.\n\nThe mean arterial pressure (MAP) is the average over a cardiac cycle and is determined by the cardiac output (CO, systemic vascular resistance (SVR), and central venous pressure (CVP):\n\nIn practice the contribution of CVP (which is small) is generally ignored and so\n\nMAP can be estimated from measurements of the systolic pressure formula_3  and the diastolic pressure formula_4 \n\nThe pulse pressure is the difference between the measured systolic and diastolic pressures,\n\nThe up and down fluctuation of the arterial pressure results from the pulsatile nature of the cardiac output, i.e. the heartbeat. Pulse pressure is determined by the interaction of the stroke volume of the heart, the compliance (ability to expand) of the arterial system—largely attributable to the aorta and large elastic arteries—and the resistance to flow in the arterial tree. By expanding under pressure, the aorta absorbs some of the force of the blood surge from the heart during a heartbeat. In this way, the pulse pressure is reduced from what it would be if the aorta were not compliant. The loss of arterial compliance that occurs with aging explains the elevated pulse pressures found in elderly patients.\n\nBlood pressure generally refers to the arterial pressure in the systemic circulation. However, measurement of pressures in the venous system and the pulmonary vessels plays an important role in intensive care medicine but requires invasive measurement of pressure using a catheter.\n\nVenous pressure is the vascular pressure in a vein or in the atria of the heart. It is much less than arterial pressure, with common values of 5 mmHg in the right atrium and 8 mmHg in the left atrium.\n\nVariants of venous pressure include:\n\nNormally, the pressure in the pulmonary artery is about 15 mmHg at rest.\n\nIncreased blood pressure in the capillaries of the lung causes pulmonary hypertension, leading to interstitial edema if the pressure increases to above 20 mmHg, and to pulmonary edema at pressures above 25 mmHg.\n\nDisorders of blood pressure control include high blood pressure, low blood pressure, and blood pressure that shows excessive or maladaptive fluctuation.\n\nArterial hypertension can be an indicator of other problems and may have long-term adverse effects. Sometimes it can be an acute problem, for example hypertensive emergency.\n\nLevels of arterial pressure put mechanical stress on the arterial walls. Higher pressures increase heart workload and progression of unhealthy tissue growth (atheroma) that develops within the walls of arteries. The higher the pressure, the more stress that is present and the more atheroma tend to progress and the heart muscle tends to thicken, enlarge and become weaker over time.\n\nPersistent hypertension is one of the risk factors for strokes, heart attacks, heart failure and arterial aneurysms, and is the leading cause of chronic kidney failure. Even moderate elevation of arterial pressure leads to shortened life expectancy. At severely high pressures, mean arterial pressures 50% or more above average, a person can expect to live no more than a few years unless appropriately treated.\n\nIn the past, most attention was paid to diastolic pressure; but nowadays it is recognized that both high systolic pressure and high pulse pressure (the numerical difference between systolic and diastolic pressures) are also risk factors. In some cases, it appears that a decrease in excessive diastolic pressure can actually increase risk, due probably to the increased difference between systolic and diastolic pressures (see the article on pulse pressure). If systolic blood pressure is elevated (>140 mmHg) with a normal diastolic blood pressure (<90 mmHg), it is called \"isolated systolic hypertension\" and may present a health concern.\n\nFor those with heart valve regurgitation, a change in its severity may be associated with a change in diastolic pressure. In a study of people with heart valve regurgitation that compared measurements 2 weeks apart for each person, there was an increased severity of aortic and mitral regurgitation when diastolic blood pressure increased, whereas when diastolic blood pressure decreased, there was a decreased severity.\n\nBlood pressure that is too low is known as hypotension. This is a medical concern if it causes signs or symptoms, such as dizziness, fainting, or in extreme cases, shock.\n\nWhen arterial pressure and blood flow decrease beyond a certain point, the perfusion of the brain becomes critically decreased (i.e., the blood supply is not sufficient), causing lightheadedness, dizziness, weakness or fainting.\n\nSometimes the arterial pressure drops significantly when a patient stands up from sitting. This is known as orthostatic hypotension (postural hypotension); gravity reduces the rate of blood return from the body veins below the heart back to the heart, thus reducing stroke volume and cardiac output.\n\nWhen people are healthy, the veins below their heart quickly constrict and the heart rate increases to minimize and compensate for the gravity effect. This is carried out involuntarily by the autonomic nervous system. The system usually requires a few seconds to fully adjust and if the compensations are too slow or inadequate, the individual will suffer reduced blood flow to the brain, dizziness and potential blackout. Increases in G-loading, such as routinely experienced by aerobatic or combat pilots 'pulling Gs', greatly increases this effect. Repositioning the body horizontally largely eliminates the problem.\n\nOther causes of low arterial pressure include:\n\nShock is a complex condition which leads to critically decreased perfusion. The usual mechanisms are loss of blood volume, pooling of blood within the veins reducing adequate return to the heart and/or low effective heart pumping. Low arterial pressure, especially low pulse pressure, is a sign of shock and contributes to and reflects decreased perfusion.\n\nIf there is a significant difference in the pressure from one arm to the other, that may indicate a narrowing (for example, due to aortic coarctation, aortic dissection, thrombosis or embolism) of an artery.\n\nNormal fluctuation in blood pressure is adaptive and necessary. Fluctuations in pressure that are significantly greater than the norm are associated with greater white matter hyperintensity, a finding consistent with reduced local cerebral blood flow and a heightened risk of cerebrovascular disease. Within both high and low blood pressure groups, a greater degree of fluctuation was found to correlate with an increase in cerebrovascular disease compared to those with less variability, suggesting the consideration of the clinical management of blood pressure fluctuations, even among normotensive older adults. Older individuals and those who had received blood pressure medications were more likely to exhibit larger fluctuations in pressure.\n\nDuring each heartbeat, blood pressure varies between a maximum (systolic) and a minimum (diastolic) pressure. The blood pressure in the circulation is principally due to the pumping action of the heart. Differences in mean blood pressure are responsible for blood flow from one location to another in the circulation. The rate of mean blood flow depends on both blood pressure and the resistance to flow presented by the blood vessels. Mean blood pressure decreases as the circulating blood moves away from the heart through arteries and capillaries due to viscous losses of energy. Mean blood pressure drops over the whole circulation, although most of the fall occurs along the small arteries and arterioles. Gravity affects blood pressure via hydrostatic forces (e.g., during standing), and valves in veins, breathing, and pumping from contraction of skeletal muscles also influence blood pressure in veins.\n\nMost influences on blood pressure can be understood in terms of their effect on cardiac output and resistance (the determinants of mean arterial pressure).\n\nSome factors are:\n\nIn practice, each individual's autonomic nervous system and other systems regulating blood pressure respond to and regulate all these factors so that, although the above issues are important, they rarely act in isolation and the actual arterial pressure response of a given individual can vary widely in the short and long term.\n\nThe endogenous regulation of arterial pressure is not completely understood, but the following mechanisms of regulating arterial pressure have been well-characterized:\n\nThese different mechanisms are not necessarily independent of each other, as indicated by the link between the RAS and aldosterone release. When blood pressure falls many physiological cascades commence in order to return the blood pressure to a more appropriate level.\n\n\nCurrently, the RAS is targeted pharmacologically by ACE inhibitors and angiotensin II receptor antagonists, also known as angiotensin receptor blockers (ARBs). The aldosterone system is directly targeted by spironolactone, an aldosterone antagonist. The fluid retention may be targeted by diuretics; the antihypertensive effect of diuretics is due to its effect on blood volume. Generally, the baroreceptor reflex is not targeted in hypertension because if blocked, individuals may suffer from orthostatic hypotension and fainting.\n\nArterial pressure is most commonly measured via a sphygmomanometer, which uses the height of a column of mercury, or an aneroid gauge, to reflect the blood pressure by auscultation. The most common automated blood pressure measurement technique is based on the so-called \"oscillometric\" method. Blood pressure values are generally reported in millimetres of mercury (mmHg), though aneroid and electronic devices do not contain mercury.\n\nFor each heartbeat, blood pressure varies between systolic and diastolic pressures. Systolic pressure is peak pressure in the arteries, which occurs near the end of the cardiac cycle when the ventricles are contracting. Diastolic pressure is minimum pressure in the arteries, which occurs near the beginning of the cardiac cycle when the ventricles are filled with blood. An example of normal measured values for a resting, healthy adult human is 120 mmHg systolic and 80 mmHg diastolic (written as 120/80 mmHg, and spoken as \"one-twenty over eighty\").\n\nSystolic and diastolic arterial blood pressures are not static but undergo natural variations from one heartbeat to another and throughout the day (in a circadian rhythm). They also change in response to stress, nutritional factors, drugs, disease, exercise, and momentarily from standing up. Sometimes the variations are large. Hypertension refers to arterial pressure being abnormally high, as opposed to hypotension, when it is abnormally low. Along with body temperature, respiratory rate, and pulse rate, blood pressure is one of the four main vital signs routinely monitored by medical professionals and healthcare providers.\n\nMeasuring pressure invasively, by penetrating the arterial wall to take the measurement, is much less common and usually restricted to a hospital setting.\n\nIn pregnancy, it is the fetal heart and not the mother's heart that builds up the fetal blood pressure to drive blood through the fetal circulation. The blood pressure in the fetal aorta is approximately 30 mmHg at 20 weeks of gestation, and increases to approximately 45 mmHg at 40 weeks of gestation.\n\nThe average blood pressure for full-term infants:\n\nBlood pressure in non-human mammals is similar to human blood pressure. In contrast, heart rate differs markedly, largely depending on the size of the animal (larger animals have slower heart rates). As in humans, blood pressure in animals differs by age, sex, time of day and circumstances: measurements made in laboratories or anesthesia may not be repesentative of values under free-living conditions. Rats, mice, dogs and rabbits have been used extensively to study the causes of high blood pressure.\nHypertension in cats and dogs is diagnosed if the blood pressure is greater than 150 mm Hg (systolic) and/or 95 mm Hg (diastolic).\n\n"}
{"id": "3986044", "url": "https://en.wikipedia.org/wiki?curid=3986044", "title": "Cofunction", "text": "Cofunction\n\nIn mathematics, a function \"f\" is cofunction of a function \"g\" if \"f\"(\"A\") = \"g\"(\"B\") whenever \"A\" and \"B\" are complementary angles. This definition typically applies to trigonometric functions. The prefix \"co-\" can be found already in Edmund Gunter's \"Canon triangulorum\" (1620).\n\nFor example, sine (Latin: \"sinus\") and cosine (Latin: \"cosinus\", \"sinus complementi\") are cofunctions of each other (hence the \"co\" in \"cosine\"):\n\nThe same is true of secant (Latin: \"secans\") and cosecant (Latin: \"cosecans\", \"secans complementi\") as well as of tangent (Latin: \"tangens\") and cotangent (Latin: \"cotangens\", \"tangens complementi\"):\n\nThese equations are also known as the cofunction identities.\n\nThis also holds true for the versine (versed sine, ver) and coversine (coversed sine, cvs), the vercosine (versed cosine, vcs) and covercosine (coversed cosine, cvc), the haversine (half-versed sine, hav) and hacoversine (half-coversed sine, hcv), the havercosine (half-versed cosine, hvc) and hacovercosine (half-coversed cosine, hcc), as well as the exsecant (external secant, exs) and excosecant (external cosecant, exc):\n\n"}
{"id": "3086963", "url": "https://en.wikipedia.org/wiki?curid=3086963", "title": "Computation in the limit", "text": "Computation in the limit\n\nIn computability theory, a function is called limit computable if it is the limit of a uniformly computable sequence of functions. The terms computable in the limit, limit recursive and recursively approximable are also used. One can think of limit computable functions as those admitting an eventually correct computable guessing procedure at their true value. A set is limit computable just when its characteristic function is limit computable.\n\nIf the sequence is uniformly computable relative to \"D\", then the function is limit computable in \"D\".\n\nA total function formula_1 is limit computable if there is a total computable function formula_2 such that\n\nThe total function formula_1 is limit computable in \"D\" if there is a total function formula_2 computable in \"D\" also satisfying\n\nA set of natural numbers is defined to be computable in the limit if and only if its characteristic function is computable in the limit. In contrast, the set is computable if and only if it is computable in the limit by a function formula_7 and there is a second computable function that takes input \"i\" and returns a value of \"t\" large enough that the formula_7 has stabilized.\n\nThe limit lemma states that a set of natural numbers is limit computable if and only if the set is computable from formula_9 (the Turing jump of the empty set). The relativized limit lemma states that a set is limit computable in formula_10 if and only if it is computable from formula_11.\nMoreover, the limit lemma (and its relativization) hold uniformly. Thus one can go from an index for the function formula_2 to an index for formula_13 relative to formula_9. One can also go from an index for formula_13 relative to formula_9 to an index for some formula_2 that has limit formula_13.\n\nAs formula_9 is a [computably enumerable] set, it must be computable in the limit itself as the computable function can be defined\nwhose limit formula_1 as formula_22 goes to infinity is the characteristic function of formula_9.\n\nIt therefore suffices to show that if limit computability is preserved by Turing reduction, as this will show that all sets computable from formula_9 are limit computable. Fix sets formula_25 which are identified with their characteristic functions and a computable function formula_26 with limit formula_27. Suppose that formula_28 for some Turing reduction formula_29 and define a computable function formula_30 as follows\n\nNow suppose that the computation formula_32 converges in formula_22 steps and only looks at the first formula_22 bits of formula_27. Now pick formula_36 such that for all formula_37 formula_38. If formula_39 then the computation formula_40 converges in at most formula_41 steps to formula_32. Hence formula_43 has a limit of formula_44, so formula_45 is limit computable.\n\nAs the formula_46 sets are just the sets computable from formula_9 by Post's theorem, the limit lemma also entails that the limit computable sets are the formula_46 sets.\n\nA real number \"x\" is computable in the limit if there is a computable sequence formula_49 of rational numbers (or, which is equivalent, computable real numbers) which converges to \"x\". In contrast, a real number is computable if and only if there is a sequence of rational numbers which converges to it and which has a computable modulus of convergence.\n\nWhen a real number is viewed as a sequence of bits, the following equivalent definition holds. An infinite sequence formula_50 of binary digits is computable in the limit if and only if there is a total computable function formula_7 taking values in the set formula_52 such that for each \"i\" the limit formula_53 exists and equals formula_54. Thus for each \"i\", as \"t\" increases the value of formula_7 eventually becomes constant and equals formula_54. As with the case of computable real numbers, it is not possible to effectively move between the two representations of limit computable reals.\n\n\n\n"}
{"id": "17910805", "url": "https://en.wikipedia.org/wiki?curid=17910805", "title": "Counting board", "text": "Counting board\n\nThe counting board is the precursor of the abacus, and the earliest known form of a counting device (excluding fingers and other very simple methods). Counting boards were made of stone or wood, and the counting was done on the board with beads, or pebbles etc. Not many boards survive because of the perishable materials used in their construction. \n\nThe oldest known counting board, the Salamis Tablet (\"c.\" 300 BC) was discovered on the Greek island of Salamis in 1899. It is thought to have been used by the Babylonians in about 300 BC and is more of a gaming board than a calculating device. It is marble, about 150 x 75 x 4.5 cm, and is in the Greek National museum in Athens. It has carved Greek letters and parallel grooves. \n\nThe German mathematicican Adam Ries described the use of counting boards in \"Rechenbuch auf Linien und Ziphren in allerlei Handthierung / geschäfften und Kaufmanschafft\". In the novel \"Wolf Hall\", Hilary Mantel refers to Thomas Cromwell using a counting board in 16th-century England.\n\n"}
{"id": "2992310", "url": "https://en.wikipedia.org/wiki?curid=2992310", "title": "Difference set", "text": "Difference set\n\nIn combinatorics, a formula_1 difference set is a subset formula_2 of size formula_3 of a group formula_4 of order formula_5 such that every nonidentity element of formula_4 can be expressed as a product formula_7 of elements of formula_2 in exactly formula_9 ways. A difference set formula_2 is said to be \"cyclic\", \"abelian\", \"non-abelian\", etc., if the group formula_4 has the corresponding property. A difference set with formula_12 is sometimes called \"planar\" or \"simple\". If formula_4 is an abelian group written in additive notation, the defining condition is that every nonzero element of formula_4 can be written as a \"difference\" of elements of formula_2 in exactly formula_9 ways. The term \"difference set\" arises in this way.\n\n\nTwo difference sets formula_40 in group formula_41 and formula_42 in group formula_43 are equivalent if there is a group isomorphism formula_44 between formula_41 and formula_43 such that formula_47 for some formula_48. The two difference sets are isomorphic if the designs formula_49 and formula_50 are isomorphic as block designs.\n\nEquivalent difference sets are isomorphic, but there exist examples of isomorphic difference sets which are not equivalent. In the cyclic difference set case, all known isomorphic difference sets are equivalent.\n\nA multiplier of a difference set formula_2 in group formula_4 is a group automorphism formula_53 of formula_4 such that formula_55 for some formula_56. If formula_4 is abelian and formula_53 is the automorphism that maps formula_59, then formula_60 is called a \"numerical\" or \"Hall\" multiplier.\n\nIt has been conjectured that if \"p\" is a prime dividing formula_61 and not dividing \"v\", then the group automorphism defined by formula_62 fixes some translate of \"D\" (this is equivalent to being a multiplier). It is known to be true for formula_63 when formula_4 is an abelian group, and this is known as the First Multiplier Theorem. A more general known result, the Second Multiplier Theorem, says that if formula_2 is a formula_1-difference set in an abelian group formula_4 of exponent formula_68 (the least common multiple of the orders of every element), let formula_60 be an integer coprime to formula_5. If there exists a divisor formula_71 of formula_61 such that for every prime \"p\" dividing \"m\", there exists an integer \"i\" with formula_73, then \"t\" is a numerical divisor.\n\nFor example, 2 is a multiplier of the (7,3,1)-difference set mentioned above.\n\nIt has been mentioned that a numerical multiplier of a difference set formula_2 in an abelian group formula_4 fixes a translate of formula_2, but it can also be shown that there is a translate of formula_2 which is fixed by all numerical multipliers of formula_2.\n\nThe known difference sets or their complements have one of the following parameter sets:\n\n\nIn many constructions of difference sets the groups that are used are related to the additive and multiplicative groups of finite fields. The notation used to denote these fields differs according to discipline. In this section, formula_94 is the Galois field of order formula_80, where formula_80 is a prime or prime power. The group under addition is denoted by formula_97, while formula_98 is the multiplicative group of non-zero elements.\n\n\n\n\nThe systematic use of cyclic difference sets and methods for the construction of symmetric block designs dates back to R. C. Bose and a seminal paper of his in 1939. However, various examples appeared earlier than this, such as the \"Paley Difference Sets\" which date back to 1933. The generalization of the cyclic difference set concept to more general groups is due to R.H. Bruck in 1955. Multipliers were introduced by Marshall Hall Jr. in 1947.\n\nIt is found by Xia, Zhou and Giannakis that difference sets can be used to construct a complex vector codebook that achieves the difficult Welch bound on maximum cross correlation amplitude. The so-constructed codebook also forms the so-called Grassmannian manifold.\n\nA formula_114 difference family is a set of subsets formula_115 of a group formula_4 such that the order of formula_4 is formula_5, the size of formula_119 is formula_3 for all formula_121, and every nonidentity element of formula_4 can be expressed as a product formula_7 of elements of formula_119 for some formula_121 (i.e. both formula_126 come from the same formula_119) in exactly formula_9 ways.\n\nA difference set is a difference family with formula_129. The parameter equation above generalises to formula_130. \nThe development formula_131 of a difference family is a 2-design.\nEvery 2-design with a regular automorphism group is formula_132 for some difference family formula_133.\n\n\n\n"}
{"id": "31881212", "url": "https://en.wikipedia.org/wiki?curid=31881212", "title": "Differential poset", "text": "Differential poset\n\nIn mathematics, a differential poset is a partially ordered set (or \"poset\" for short) satisfying certain local properties. (The formal definition is given below.) This family of posets was introduced by as a generalization of Young's lattice (the poset of integer partitions ordered by inclusion), many of whose combinatorial properties are shared by all differential posets. In addition to Young's lattice, the other most significant example of a differential poset is the Young–Fibonacci lattice.\n\nA poset \"P\" is said to be a differential poset, and in particular to be \"r\"-differential (where \"r\" is a positive integer), if it satisfies the following conditions:\n\nThese basic properties may be restated in various ways. For example, Stanley shows that the number of elements covering two distinct elements \"x\" and \"y\" of a differential poset is always either 0 or 1, so the second defining property could be altered accordingly.\n\nThe defining properties may also be restated in the following linear algebraic setting: taking the elements of the poset \"P\" to be formal basis vectors of an (infinite dimensional) vector space, let \"D\" and \"U\" be the operators defined so that \"D\" \"x\" is equal to the sum of the elements covered by \"x\", and \"U\" \"x\" is equal to the sum of the elements covering \"x\". (The operators \"D\" and \"U\" are called the \"down\" and \"up operator\", for obvious reasons.) Then the second and third conditions may be replaced by the statement that \"DU\" – \"UD\" = \"rI\" (where \"I\" is the identity).\n\nThis latter reformulation makes a differential poset into a combinatorial realization of a Weyl algebra, and in particular explains the name \"differential\": the operators \"\"d\"/\"dx\"\" and \"multiplication by \"x\"\" on the vector space of polynomials obey the same commutation relation as \"U\" and \"D\"/\"r\".\n\nThe canonical examples of differential posets are Young's lattice, the poset of integer partitions ordered by inclusion, and the Young–Fibonacci lattice. Stanley's initial paper established that Young's lattice is the only 1-differential distributive lattice, while showed that these are the only 1-differential lattices.\n\nThere is a canonical construction (called \"reflection\") of a differential poset given a finite poset that obeys all of the defining axioms below its top rank. (The Young–Fibonacci lattice is the poset that arises by applying this construction beginning with a single point.) This can be used to show that there are infinitely many differential posets. includes a remark that \"[David] Wagner described a very general method for constructing differential posets which make it unlikely that [they can be classified].\" This is made precise in , where it is shown that there are uncountably many 1-differential posets. On the other hand, explicit examples of differential posets are rare; gives a convoluted description of a differential poset other than the Young and Young–Fibonacci lattices.\n\nThe Young-Fibonacci lattice has a natural \"r\"-differential analogue for every positive integer \"r\". These posets are lattices, and can be constructed by a variation of the reflection construction. In addition, the product of an \"r\"-differential and \"s\"-differential poset is always an (\"r\" + \"s\")-differential poset. This construction also preserves the lattice property. It is not known for any \"r\" > 1 whether there are any \"r\"-differential lattices other than those that arise by taking products of the Young–Fibonacci lattices and Young's lattice.\n\nIn addition to the question of whether there are other differential lattices, there are several long-standing open problems relating to the rank growth of differential posets. It was conjectured in that if \"P\" is a differential poset with vertices at rank \"n\", then\n\nwhere \"p\"(\"n\") is the number of integer partitions of \"n\" and is the \"n\"th Fibonacci number. In other words, the conjecture states that at every rank, every differential poset has a number of vertices lying between the numbers for Young's lattice and the Young-Fibonacci lattice. The upper bound was proved in . The lower bound remains open. proved an asymptotic version of the lower bound, showing that\nfor every differential poset and some constant \"a\". By comparison, the partition function has asymptotics\n\nAll known bounds on rank sizes of differential posets are quickly growing functions. In the original paper of Stanley, it was shown (using eigenvalues of the operator \"DU\") that the rank sizes are weakly increasing. However, it took 25 years before showed that the rank sizes of an \"r\"-differential poset strictly increase (except trivially between ranks 0 and 1 when \"r\" = 1).\n\nEvery differential poset \"P\" shares a large number of combinatorial properties. A few of these include:\n\nIn a differential poset, the same set of edges is used to compute the up and down operators \"U\" and \"D\". If one permits different sets of up edges and down edges (sharing the same vertex sets, and satisfying the same relation), the resulting concept is the \"dual graded graph\", initially defined by . One recovers differential posets as the case that the two sets of edges coincide.\n\nMuch of the interest in differential posets is inspired by their connections to representation theory. The elements of Young's lattice are integer partitions, which encode the representations of the symmetric groups, and are connected to the ring of symmetric functions; defined algebras whose representation is encoded instead by the Young–Fibonacci lattice, and allow for analogous constructions such as a Fibonacci version of symmetric functions. It is not known whether similar algebras exist for every differential poset. In another direction, defined dual graded graphs corresponding to any Kac–Moody algebra.\n\nOther variations are possible; defined versions in which the number \"r\" in the definition varies from rank to rank, while defined a signed analogue of differential posets in which cover relations may be assigned a \"weight\" of −1.\n\n"}
{"id": "487859", "url": "https://en.wikipedia.org/wiki?curid=487859", "title": "Directional derivative", "text": "Directional derivative\n\nIn mathematics, the directional derivative of a multivariate differentiable function along a given vector v at a given point x intuitively represents the instantaneous rate of change of the function, moving through x with a velocity specified by v. It therefore generalizes the notion of a partial derivative, in which the rate of change is taken along one of the curvilinear coordinate curves, all other coordinates being constant.\n\nThe directional derivative is a special case of the Gâteaux derivative.\n\nLet \"τ\" be a curve whose tangent vector at some chosen point is v. The directional derivative of a function \"f\" with respect to v may be denoted by any of the following:\n\nThe \"directional derivative\" of a scalar function \nalong a vector\nis the function formula_11 defined by the limit\n\nThis definition is valid in a broad range of contexts, for example where the norm of a vector (and hence a unit vector) is undefined.\n\nIf the function \"f\" is differentiable at x, then the directional derivative exists along any vector v, and one has\n\nwhere the formula_14 on the right denotes the gradient and formula_15 is the dot product. This follows from defining a path formula_16 and using the definition of the derivative as a limit which can be calculated along this path to get:\n\nIntuitively, the directional derivative of \"f\" at a point x represents the rate of change of \"f\", in the direction of v with respect to time, when moving past x.\n\nIn a Euclidean space, some authors define the directional derivative to be with respect to an arbitrary nonzero vector v after normalization, thus being independent of its magnitude and depending only on its direction.\n\nThis definition gives the rate of increase of \"f\" per unit of distance moved in the direction given by v. In this case, one has\nor in case \"f\" is differentiable at x,\n\nIn the context of a function on a Euclidean space, some texts restrict the vector v to being a unit vector. With this restriction, both the above definitions are equivalent.\n\nMany of the familiar properties of the ordinary derivative hold for the directional derivative. These include, for any functions \"f\" and \"g\" defined in a neighborhood of, and differentiable at, p: \n\nLet be a differentiable manifold and a point of . Suppose that is a function defined in a neighborhood of , and differentiable at . If is a tangent vector to at , then the directional derivative of along , denoted variously as (see Exterior derivative), formula_23 (see Covariant derivative), formula_24 (see Lie derivative), or formula_25 (see ), can be defined as follows. Let be a differentiable curve with and . Then the directional derivative is defined by\nThis definition can be proven independent of the choice of , provided is selected in the prescribed manner so that .\n\nThe Lie derivative of a vector field formula_27 along a vector field formula_28 is given by the difference of two directional derivatives (with vanishing torsion):\nIn particular, for a scalar field formula_30, the Lie derivative reduces to the standard directional derivative:\n\nDirectional derivatives are often used in introductory derivations of the Riemann curvature tensor. Consider a curved rectangle with an infinitesimal vector \"δ\" along one edge and \"δ\"′ along the other. We translate a covector \"S\" along \"δ\" then \"δ\"′ and then subtract the translation along \"δ\"′ and then \"δ\". Instead of building the directional derivative using partial derivatives, we use the covariant derivative. The translation operator for \"δ\" is thus\nand for \"δ\"′,\nThe difference between the two paths is then\nIt can be argued that the noncommutativity of the covariant derivatives measures the curvature of the manifold:\nwhere \"R\" is the Riemann curvature tensor and the sign depends on the sign convention of the author.\n\nIn the Poincaré algebra, we can define an infinitesimal translation operator P as\n(the i ensures that P is a self-adjoint operator) For a finite displacement λ, the unitary Hilbert space representation for translations is \nBy using the above definition of the infinitesimal translation operator, we see that the finite translation operator is an exponentiated directional derivative:\nThis is a translation operator in the sense that it acts on multivariable functions f(x) as\n\nThe rotation operator also contains a directional derivative. The rotation operator for an angle θ, i.e. by an amount θ=|θ| about an axis parallel to formula_40=θ/θ is\nHere L is the vector operator that generates SO(3):\nIt may be shown geometrically that an infinitesimal right-handed rotation changes the position vector x by\nSo we would expect under infinitesimal rotation:\nIt follows that \nFollowing the same exponentiation procedure as above, we arrive at the rotation operator in the position basis, which is an exponentiated directional derivative:\n\nA normal derivative is a directional derivative taken in the direction normal (that is, orthogonal) to some surface in space, or more generally along a normal vector field orthogonal to some hypersurface. See for example Neumann boundary condition. If the normal direction is denoted by formula_47, then the directional derivative of a function \"f\" is sometimes denoted as formula_48. In other notations,\n\nSeveral important results in continuum mechanics require the derivatives of vectors with respect to vectors and of tensors with respect to vectors and tensors. The directional directive provides a systematic way of finding these derivatives.\n\nThe definitions of directional derivatives for various situations are given below. It is assumed that the functions are sufficiently smooth that derivatives can be taken.\n\nLet formula_50 be a real-valued function of the vector formula_51. Then the derivative of formula_50 with respect to formula_51 (or at formula_51) in the direction formula_55 is defined as\nfor all vectors formula_55.\n\n\"Properties:\"\n\\cdot\\mathbf{u} = \\left(\\frac{\\partial f_1}{\\partial \\mathbf{v}} + \\frac{\\partial f_2}{\\partial \\mathbf{v}}\\right)\\cdot\\mathbf{u}.\n\nLet formula_62 be a vector-valued function of the vector formula_51. Then the derivative of formula_62 with respect to formula_51 (or at formula_51) in the direction formula_55 is the second-order tensor defined as\nfor all vectors formula_55.\n\n\"Properties:\"\n\nLet formula_74 be a real-valued function of the second order tensor formula_75. Then the derivative of formula_74 with respect to formula_75 (or at formula_75) in the direction\nformula_79 is the second order tensor defined as\nfor all second order tensors formula_79.\n\n\"Properties:\"\n\nLet formula_86 be a second order tensor-valued function of the second order tensor formula_75. Then the derivative of formula_86 with respect to formula_75 \n(or at formula_75) in the direction formula_79 is the fourth order tensor defined as\nfor all second order tensors formula_79.\n\n\"Properties:\"\n\n\n\n"}
{"id": "6053993", "url": "https://en.wikipedia.org/wiki?curid=6053993", "title": "Distributive category", "text": "Distributive category\n\nIn mathematics, a category is distributive if it has finite products and finite coproducts such that for every choice of objects formula_1, the canonical map\n\nis an isomorphism, and for all objects formula_3, the canonical map formula_4 is an isomorphism (where 0 denotes the initial object). Equivalently. if for every object formula_3 the endofunctor formula_6 defined by formula_7 preserves coproducts up to isomorphisms formula_8. It follows that formula_8 and aforementioned canonical maps are equal for each choice of objects. \n\nIn particular, if the functor formula_6 has a right adjoint (i.e., if the category is cartesian closed), it necessarily preserves all colimits, and thus any cartesian closed category with finite coproducts (i.e., any bicartesian closed category) is distributive.\n\nThe category of sets is distributive. Let , , and be sets. Then\nwhere formula_12 denotes the coproduct in Set, namely the disjoint union, and formula_13 denotes a bijection. In the case where , , and are finite sets, this result reflects the distributive property: the above sets each have cardinality formula_14.\n\nThe category Grp is not distributive, even though it has both products and coproducts.\n\nAn even simpler category that has both products and coproducts but is not distributive is the category of pointed sets.\n\n"}
{"id": "22762908", "url": "https://en.wikipedia.org/wiki?curid=22762908", "title": "Dual of BCH is an independent source", "text": "Dual of BCH is an independent source\n\nA certain family of BCH codes have a particularly useful property, which is that\ntreated as linear operators, their dual operators turns their input into an formula_1-wise independent source. That is, the set of vectors from the input vector space are mapped to an formula_1-wise independent source. The proof of this fact below as the following Lemma and Corollary is useful in derandomizing the algorithm for a formula_3-approximation to MAXEkSAT.\n\nLet formula_4 be a linear code such that formula_5 has distance greater than formula_6. Then formula_7 is an formula_1-wise independent source.\n\nIt is sufficient to show that given any formula_9 matrix \"M\", where \"k\" is greater than or equal to \"l\", such that the rank of \"M\" is \"l\", for all formula_10, formula_11 takes every value in formula_12 the same number of times.\n\nSince \"M\" has rank \"l\", we can write \"M\" as two matrices of the same size, formula_13 and formula_14, where formula_13 has rank equal to \"l\". This means that formula_11 can be rewritten as formula_17 for some formula_18 and formula_19.\n\nIf we consider \"M\" written with respect to a basis where the first \"l\" rows are the identity matrix, then formula_18 has zeros wherever formula_14 has nonzero rows, and formula_19 has zeros wherever formula_13 has nonzero rows.\n\nNow any value \"y\", where formula_24, can be written as formula_25 for some vectors formula_26.\n\nWe can rewrite this as:\n\nformula_27\n\nFixing the value of the last formula_28 coordinates of\nformula_29 (note that there are exactly formula_30\nsuch choices), we can rewrite this equation again as:\n\nformula_31 for some \"b\".\n\nSince formula_13 has rank equal to \"l\", there\nis exactly one solution formula_18, so the total number of solutions is exactly formula_30, proving the lemma.\n\nRecall that BCH is an formula_35 linear code.\n\nLet formula_36 be BCH. Then formula_7 is an formula_1-wise independent source of size formula_39.\n\nThe dimension \"d\" of \"C\" is just formula_40. So formula_41.\n\nSo the cardinality of formula_7 considered as a set is just\nformula_43, proving the Corollary.\n\nCoding Theory notes at University at Buffalo\n\nCoding Theory notes at MIT\n"}
{"id": "51761", "url": "https://en.wikipedia.org/wiki?curid=51761", "title": "Elias gamma coding", "text": "Elias gamma coding\n\nElias γ code or Elias gamma code is a universal code encoding positive integers developed by Peter Elias. It is used most commonly when coding integers whose upper-bound cannot be determined beforehand.\n\nTo code a number \"x\" ≥ 1:\n\nAn equivalent way to express the same process:\n\nTo represent a number formula_1, Elias gamma (γ) uses formula_2 bits.\n\nThe code begins (the implied probability distribution for the code is added for clarity):\n\nTo decode an Elias gamma-coded integer:\n\nGamma coding is used in applications where the largest encoded value is not known ahead of time, or to compress data in which small values are much more frequent than large values.\n\nGamma coding is a building block in the Elias delta code.\n\nGamma coding does not code zero or negative integers.\nOne way of handling zero is to add 1 before coding and then subtract 1 after decoding.\nAnother way is to prefix each nonzero code with a 1 and then code zero as a single 0.\n\nOne way to code all integers is to set up a bijection, mapping integers (0, −1, 1, −2, 2, −3, 3, ...) to (1, 2, 3, 4, 5, 6, 7, ...) before coding. In software, this is most easily done by mapping non-negative inputs to odd outputs, and negative inputs to even outputs, so the least-significant bit becomes an inverted sign bit:\nformula_3\n\nExponential-Golomb coding generalizes the gamma code to integers with a \"flatter\" power-law distribution, just as Golomb coding generalizes the unary code.\nIt involves dividing the number by a positive divisor, commonly a power of 2, writing the gamma code for one more than the quotient, and writing out the remainder in an ordinary binary code.\n\n"}
{"id": "11643809", "url": "https://en.wikipedia.org/wiki?curid=11643809", "title": "Enigform", "text": "Enigform\n\nEnigform is a Mozilla Firefox extension authored by Arturo 'Buanzo' Busleiman which uses GnuPG to implement OpenPGP-signed HTTP requests. OpenPGP encryption began to be implemented in 2007. Some people believe it to be an alternative for the Secure Sockets Layer method for encrypting Hypertext Transfer Protocol (or \"HTTP\") connections. However, the author never had such an intention in his mind. Guaranteeing the identity of a requester and the integrity of the request is Enigform's primary goal, through the use of digital signatures. In this instance, requests are form submissions to web servers. Apache HTTP Server support via the mod_openpgp module currently supports request verification.\n\nThe project got its initial funding from OWASP in 2007. A secure instant messaging system based on Enigform and HTTPS has been announced during the OWASP Ibero-American Web-Application Security Conference was announced in 2010.\n\nEnigform was granted the Trusted status on the Mozilla Add-ons website in 2008.\n\nOn April 23, 2009, Enigform was declared a Finalist in the Security category of Les Trophées du Libre, and was awarded the second prize.\n\nIn April 2012, Enigform is now available for testing in the Firefox 12+ series.\n\nCurrently, the author has not received the prize money from the Les Trophees du Libre parent company, Cetril, and it seems that the company has vanished.\n\n"}
{"id": "25568464", "url": "https://en.wikipedia.org/wiki?curid=25568464", "title": "Entanglement-assisted stabilizer formalism", "text": "Entanglement-assisted stabilizer formalism\n\nIn the theory of quantum communication, the entanglement-assisted stabilizer formalism is a method for protecting quantum information with the help of entanglement shared between a sender and receiver before they transmit quantum data over a quantum communication channel. It extends the standard stabilizer formalism\nby including shared entanglement (Brun \"et al.\" 2006).\nThe advantage of entanglement-assisted stabilizer codes is that the sender can\nexploit the error-correcting properties of an arbitrary set of Pauli operators.\nThe sender's Pauli operators do not necessarily have to form an\nAbelian subgroup of the Pauli group formula_1 over formula_2 qubits.\nThe sender can make clever use of her shared\nebits so that the global stabilizer is Abelian and thus forms a valid\nquantum error-correcting code.\n\nWe review the construction of an entanglement-assisted code (Brun \"et al.\" 2006). Suppose that\nthere is a nonabelian subgroup formula_3 of size formula_4.\nApplication of the fundamental theorem of symplectic geometry (Lemma 1 in the first external reference)\nstates that there exists a minimal set of independent generators\nformula_5\nfor formula_6 with the following commutation relations:\nThe decomposition of formula_6 into the above minimal generating set\ndetermines that the code requires formula_12 ancilla qubits and formula_13 ebits. The code\nrequires an ebit for every anticommuting pair in the minimal generating set.\nThe simple reason for this requirement is that an ebit is a simultaneous\nformula_14-eigenstate of the Pauli operators formula_15. The second qubit\nin the ebit transforms the anticommuting pair formula_16 into a\ncommuting pair formula_15. The above decomposition also\nminimizes the number of ebits required for the code---it is an optimal decomposition.\n\nWe can partition the nonabelian group formula_6 into two subgroups: the\nisotropic subgroup formula_19 and the entanglement subgroup\nformula_20. The isotropic subgroup formula_19 is a commuting\nsubgroup of formula_6 and thus corresponds to ancilla\nqubits:\n\nThe elements of the entanglement subgroup formula_20 come in\nanticommuting pairs and thus correspond to ebits:\n\nThe two subgroups formula_19 and formula_20 play a role in the\nerror-correcting conditions for the entanglement-assisted stabilizer\nformalism. An entanglement-assisted code corrects errors in a set\nformula_28 if for all formula_29,\n\nThe operation of an entanglement-assisted code is as follows. The sender\nperforms an encoding unitary on her unprotected qubits, ancilla qubits, and\nher half of the ebits. The unencoded state is a simultaneous +1-eigenstate of\nthe following Pauli operators:\n\nThe Pauli operators to the right of the vertical bars indicate the receiver's half\nof the shared ebits. The encoding unitary transforms the unencoded Pauli operators\nto the following encoded Pauli operators:\nThe sender transmits all of her qubits over the noisy quantum channel. The\nreceiver then possesses the transmitted qubits and his half of the ebits. He\nmeasures the above encoded operators to diagnose the error. The last step is\nto correct the error.\n\nWe can interpret the rate of an entanglement-assisted code\nin three different ways (Wilde and Brun 2007b).\nSuppose that an entanglement-assisted quantum code encodes formula_33 information\nqubits into formula_2 physical qubits with the help of formula_13 ebits.\n\n\nWhich interpretation is most reasonable depends on the context in which we use\nthe code. In any case, the parameters formula_2, formula_33, and formula_13 ultimately govern\nperformance, regardless of which definition of the rate we use to interpret\nthat performance.\n\nWe present an example of an entanglement-assisted code\nthat corrects an arbitrary single-qubit error (Brun \"et al.\" 2006). Suppose\nthe sender wants to use the quantum error-correcting properties of the\nfollowing nonabelian subgroup of formula_46:\nThe first two generators anticommute. We obtain a modified third generator by\nmultiplying the third generator by the second. We then multiply the last\ngenerator by the first, second, and modified third generators. The\nerror-correcting properties of the generators are invariant under these\noperations. The modified generators are as follows:\nThe above set of generators have the commutation relations given by the\nfundamental theorem of symplectic geometry:\nThe above set of generators is unitarily equivalent to the following canonical\ngenerators:\nWe can add one ebit to resolve the anticommutativity of the first two\ngenerators and obtain the canonical stabilizer:\nThe receiver Bob possesses the qubit on the left and the sender Alice\npossesses the four qubits on the right. The following state is an eigenstate\nof the above stabilizer\nwhere formula_53 is a qubit that the sender wants to\nencode. The encoding unitary then rotates the canonical stabilizer to the following set of globally commuting\ngenerators:\nThe receiver measures the above generators upon receipt of all qubits to\ndetect and correct errors.\n\nWe continue with the previous example. We\ndetail an algorithm for determining an encoding circuit and the optimal number\nof ebits for the entanglement-assisted code---this algorithm first appeared in the appendix of (Wilde and Brun 2007a) and later in the appendix of (Shaw \"et al.\" 2008). The operators in\nthe above example have the following representation as a binary\nmatrix (See the stabilizer code article):\nCall the matrix to the left of the vertical bar the \"formula_56\nmatrix\" and the matrix to the right of the vertical bar the\n\"formula_57 matrix.\"\n\nThe algorithm consists of row and column operations on the above matrix. Row\noperations do not affect the error-correcting properties of the code but are\ncrucial for arriving at the optimal decomposition from the fundamental theorem\nof symplectic geometry. The operations available for manipulating columns of\nthe above matrix are Clifford operations. Clifford\noperations preserve the Pauli group formula_1 under conjugation. The\nCNOT gate, the Hadamard gate, and the Phase gate generate the Clifford group.\nA CNOT gate from qubit formula_59 to qubit formula_60 adds column formula_59 to column formula_60 in the\nformula_57 matrix and adds column formula_60 to column formula_59 in the formula_56 matrix. A Hadamard\ngate on qubit formula_59 swaps column formula_59 in the formula_56 matrix with column formula_59 in the\nformula_57 matrix and vice versa. A phase gate on qubit formula_59 adds column formula_59 in the\nformula_57 matrix to column formula_59 in the formula_56 matrix. Three CNOT gates implement a\nqubit swap operation. The effect of a swap on qubits\nformula_59 and formula_60 is to swap columns formula_59 and formula_60 in both the formula_57 and formula_56 matrix.\n\nThe algorithm begins by computing the symplectic product between the first row\nand all other rows. We emphasize that the symplectic product here is the\nstandard symplectic product. Leave the matrix as it is if the first row is not\nsymplectically orthogonal to the second row or if the first row is\nsymplectically orthogonal to all other rows. Otherwise, swap the second row\nwith the first available row that is not symplectically orthogonal to the\nfirst row. In our example, the first row is not symplectically orthogonal to\nthe second so we leave all rows as they are.\n\nArrange the first row so that the top left entry in the formula_57 matrix is one. A\nCNOT, swap, Hadamard, or combinations of these operations can achieve this\nresult. We can have this result in our example by swapping qubits one and two.\nThe matrix becomes\nPerform CNOTs to clear the entries in the formula_57 matrix in the top row to the\nright of the leftmost entry. These entries are already zero in this example so\nwe need not do anything. Proceed to the clear the entries in the first row of\nthe formula_56 matrix. Perform a phase gate to clear the leftmost entry in the first\nrow of the formula_56 matrix if it is equal to one. It is equal to zero in this case\nso we need not do anything. We then use Hadamards and CNOTs to clear the other\nentries in the first row of the formula_56 matrix.\n\nWe perform the above operations for our example. Perform a Hadamard on qubits\ntwo and three. The matrix becomes\nPerform a CNOT from qubit one to qubit two and from qubit one to qubit three.\nThe matrix becomes\nThe first row is complete. We now proceed to clear the entries in the second\nrow. Perform a Hadamard on qubits one and four. The matrix becomes\nPerform a CNOT from qubit one to qubit two and from qubit one to qubit four.\nThe matrix becomes\nThe first two rows are now complete. They need one ebit to compensate for\ntheir anticommutativity or their nonorthogonality with respect to the\nsymplectic product.\n\nNow we perform a \"Gram-Schmidt\northogonalization\" with respect to the symplectic product.\nAdd row one to any other row that has one as the leftmost entry in its formula_56\nmatrix. Add row two to any other row that has one as the leftmost entry in its\nformula_57 matrix. For our example, we add row one to row four and we add row two to\nrows three and four. The matrix becomes\nThe first two rows are now symplectically orthogonal to all other rows per the\nfundamental theorem of symplectic geometry.\nWe proceed with the same algorithm on the next two rows. The next two rows are\nsymplectically orthogonal to each other so we can deal with them individually.\nPerform a Hadamard on qubit two. The matrix becomes\nPerform a CNOT from qubit two to qubit three and from qubit two to qubit\nfour. The matrix becomes\nPerform a phase gate on qubit two:\nPerform a Hadamard on qubit three followed by a CNOT from qubit two to qubit\nthree:\nAdd row three to row four and perform a Hadamard on qubit two:\nPerform a Hadamard on qubit four followed by a CNOT from qubit three to qubit\nfour. End by performing a Hadamard on qubit three:\nThe above matrix now corresponds to the canonical Pauli operators. Adding one half of an ebit to the receiver's side\ngives the canonical stabilizer whose\nsimultaneous +1-eigenstate is the above state.\nThe above operations in reverse order\ntake the canonical stabilizer to the encoded\nstabilizer.\n\n"}
{"id": "33473957", "url": "https://en.wikipedia.org/wiki?curid=33473957", "title": "Francis Allotey", "text": "Francis Allotey\n\nFrancis Kofi Ampenyin Allotey (9 August 1932 – 2 November 2017) was a Ghanaian mathematical physicist. \n\nAllotey was born on 9 August 1932 in the fishing town of Saltpond in the Central Region of Ghana to Joseph Kofi Allotey, a general commodities merchant of the Royal Sempe Mankrado We, Accra and Alice Esi Nyena Allotey, a dressmaker from the Royal Dehyena family of Enyan Owomase and Ekumfi Edumafa, in the Central Region of Ghana. His father owned a bookstore. During his childhood, Allotey spent his free time in his father's bookstore reading the biographies of famous scientists which piqued his interest in science. He was raised a Roman Catholic. He had his primary education at the St. John the Baptist Catholic School, Saltpond and was among the pioneer batch of Ghana National College when the school was founded in July 1948 by Kwame Nkrumah. After secondary school, he attended the University Tutorial College in Ghana and the London Borough Polytechnic. He held master's and doctorate degrees from Princeton University and the Diploma of Imperial College, obtained in 1960. During his time at Princeton, he was mentored by many physicists such as Robert Dicke, Val Fitch, Robert Oppenheimer, Paul A.M. Dirac and C.N. Yang.\n\nHe was known for the \"Allotey Formalism\" which arose from his work on soft X-ray spectroscopy. He was the 1973 recipient of the \"UK Prince Philip Golden Award\" for his work in this area. A founding fellow of the African Academy of Sciences, in 1974 he became the first Ghanaian full professor of mathematics and head of the Department of Mathematics and later Dean of the Faculty of Science at the Kwame Nkrumah University of Science and Technology. He was also the founding director of the KNUST Computer Centre before he assumed his position as the Pro-Vice-Chancellor of the university. \n\nAllotey was the President of the Ghana Academy of Arts and Sciences and a member of a number of international scientific organizations including the Abdus Salam International Centre for Theoretical Physics Scientific Council since 1996. He was also the President of the Ghana Institute of Physics and the founding President of the African Physical Society. He was instrumental in getting Ghana to join the International Union of Pure and Applied Physics, making it one of the first few African countries to join the Union. He collaborated with the IUPAP and ICTP to encourage physics education in developing countries through workshops and conferences in order to create awareness on the continent.\n\nAllotey was the Chairman of Board of Trustees of the Accra Institute of Technology, the President of the African Institute for Mathematical Sciences, Ghana. He was an honorary fellow of the Institute of Physics. He was an honorary Fellow of the Nigerian Mathematical Society among others. He consulted for many international institutions such as the UNESCO, IAEA and UNIDO. He was also the Vice president, 7th General Assembly of Intergovernmental Bureau of Informatics (IBI). He was also instrumental in the advancement of computer education in Africa and worked closely with organisations such as the IBM International and the International Federation for Information Processing. In 2004, he was the only African among the 100 most eminent physicists and mathematicians in the world to be cited in a book titled, \"\"One hundred reasons to be a scientist\".\"\n\nThe Professor Francis Allotey Graduate School was established in 2009 at the Accra Institute of Technology. The institute provides master's degrees in Business Administration and Software Engineering and doctoral programmes in Information Technology and Philosophy. The Government of Ghana awarded him the Millennium Excellence Award in 2005, and dedicated a postage stamp in his honour. In 2009 he received the Order of the Volta and was posthumously awarded the \"Osagyefo Kwame Nkrumah African Genius Award\" in 2017. He helped establish the African Institute of Mathematical Sciences in Ghana in 2012.\n\nAllotey first married Edoris Enid Chandler from Barbados who he met while they were both studying in London. They had two children, Francis Kojo Enu Allotey and Joseph Kobina Nyansa Allotey. Chandler died in November 1981. He then remarried to Ruby Asie Mirekuwa Akuamoah. Together they raised her two children, Cilinnie and Kay. Akuamoah died in October, 2011. Overall, Allotey had four children and 20 grandchildren.\n\nFrancis Allotey died of natural causes on 2 November 2017. The Ghanaian government accorded him a state funeral in recognition of his contributions to the advancement of science and technology in Ghana. His body was interred in his hometown, Saltpond. \n\n"}
{"id": "53625492", "url": "https://en.wikipedia.org/wiki?curid=53625492", "title": "Gröbner fan", "text": "Gröbner fan\n\nIn computer algebra, the Gröbner fan of an ideal in the ring of polynomials is a concept in the theory of Gröbner bases. It is defined to be a fan consisting of cones that correspond to different monomial orders on that ideal. The concept has been introduced by Mora and Robbiano in 1988. \nThe result is a weaker version of the result presented in the same issue of the journal by Bayer and Morrison. Gröbner fan is a base for the nowadays active field of tropical geometry.\nOne implementation of the Gröbner fan is called Gfan, based on an article of Fukuda, et. al. which is included in some computer algebra systems such as Singular and Macaulay2.\n\n"}
{"id": "12471936", "url": "https://en.wikipedia.org/wiki?curid=12471936", "title": "Hadamard manifold", "text": "Hadamard manifold\n\nIn mathematics, a Hadamard manifold, named after Jacques Hadamard — sometimes called a Cartan–Hadamard manifold, after Élie Cartan — is a Riemannian manifold (\"M\", \"g\") that is complete and simply connected and has everywhere non-positive sectional curvature.\n\n\n"}
{"id": "41227", "url": "https://en.wikipedia.org/wiki?curid=41227", "title": "Hamming distance", "text": "Hamming distance\n\nIn information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of \"substitutions\" required to change one string into the other, or the minimum number of \"errors\" that could have transformed one string into the other. In a more general context, the Hamming distance is one of several string metrics for measuring the edit distance between two sequences. It is named after the American mathematician Richard Hamming (1915-1998).\n\nA major application is in coding theory, more specifically to block codes, in which the equal-length strings are vectors over a finite field.\n\nThe Hamming distance between:\n\nFor a fixed length \"n\", the Hamming distance is a metric on the set of the words of length n (also known as a Hamming space), as it fulfills the conditions of non-negativity, identity of indiscernibles and symmetry, and it can be shown by complete induction that it satisfies the triangle inequality as well. The Hamming distance between two words \"a\" and \"b\" can also be seen as the Hamming weight of \"a\" − \"b\" for an appropriate choice of the − operator, much as the difference between two integers can be seen as a distance from zero on the number line.\n\nFor binary strings \"a\" and \"b\" the Hamming distance is equal to the number of ones (population count) in \"a\" XOR \"b\". The metric space of length-\"n\" binary strings, with the Hamming distance, is known as the \"Hamming cube\"; it is equivalent as a metric space to the set of distances between vertices in a hypercube graph. One can also view a binary string of length \"n\" as a vector in formula_1 by treating each symbol in the string as a real coordinate; with this embedding, the strings form the vertices of an \"n\"-dimensional hypercube, and the Hamming distance of the strings is equivalent to the Manhattan distance between the vertices.\n\nThe minimum Hamming distance is used to define some essential notions in coding theory, such as error detecting and error correcting codes. In particular, a code \"C\" is said to be \"k\" error detecting if, and only if, the minimum Hamming distance between any two of its codewords is at least \"k\"+1.\n\nA code \"C\" is said to be \"k-errors correcting\" if, for every word \"w\" in the underlying Hamming space \"H\", there exists at most one codeword \"c\" (from \"C\") such that the Hamming distance between \"w\" and \"c\" is at most \"k\". In other words, a code is \"k\"-errors correcting if, and only if, the minimum Hamming distance between any two of its codewords is at least 2\"k\"+1. This is more easily understood geometrically as any closed balls of radius \"k\" centered on distinct codewords being disjoint. These balls are also called \"Hamming spheres\" in this context.\n\nThus a code with minimum Hamming distance \"d\" between its codewords can detect at most \"d\"-1 errors and can correct ⌊(\"d\"-1)/2⌋ errors. The latter number is also called the \"packing radius\" or the \"error-correcting capability\" of the code.\n\nThe Hamming distance is named after Richard Hamming, who introduced the concept in his fundamental paper on Hamming codes \"Error detecting and error correcting codes\" in 1950. Hamming weight analysis of bits is used in several disciplines including information theory, coding theory, and cryptography.\n\nIt is used in telecommunication to count the number of flipped bits in a fixed-length binary word as an estimate of error, and therefore is sometimes called the signal distance. For \"q\"-ary strings over an alphabet of size \"q\" ≥ 2 the Hamming distance is applied in case of the q-ary symmetric channel, while the Lee distance is used for phase-shift keying or more generally channels susceptible to synchronization errors because the Lee distance accounts for errors of ±1. If formula_2 or formula_3 both distances coincide because any pair of elements from formula_4 or formula_5 differ by 1, but the distances are different for larger formula_6.\n\nThe Hamming distance is also used in systematics as a measure of genetic distance.\n\nHowever, for comparing strings of different lengths, or strings where not just substitutions but also insertions or deletions have to be expected, a more sophisticated metric like the Levenshtein distance is more appropriate.\n\nIn processor interconnects, the dynamic energy consumption depends on the number of transitions. With level-signaling scheme, the number of transitions depends on Hamming distance between consecutively transmitted buses. Hence, by reducing this Hamming distance, the data-movement energy can be reduced. \n\nThe function codice_1, implemented in Python 2.3+, computes the Hamming distance between\ntwo strings (or other iterable objects) of equal length by creating a sequence of Boolean values indicating mismatches and matches between corresponding positions in the two inputs and then summing the sequence with False and True values being interpreted as zero and one.\n\ndef hamming_distance(s1, s2):\n\nwhere the zip() function merges two equal-length collections in pairs.\nThe following C function will compute the Hamming distance of two integers (considered as binary values, that is, as sequences of bits). The running time of this procedure is proportional to the Hamming distance rather than to the number of bits in the inputs. It computes the bitwise exclusive or of the two inputs, and then finds the Hamming weight of the result (the number of nonzero bits) using an algorithm of that repeatedly finds and clears the lowest-order nonzero bit. Some compilers support the __builtin_popcount function which can calculate this using specialized processor hardware where available.\nint hamming_distance(unsigned x, unsigned y)\n\nOr, a much faster hardware alternative (for compilers that support builtins) is to use popcount like so.\nint hamming_distance(unsigned x, unsigned y)\n//if your compiler supports 64-bit integers\nint hamming_distance(unsigned long long x, unsigned long long y)\n\n\n"}
{"id": "537969", "url": "https://en.wikipedia.org/wiki?curid=537969", "title": "IBM hexadecimal floating point", "text": "IBM hexadecimal floating point\n\nIBM System/360 computers, and subsequent machines based on that architecture (mainframes), support a hexadecimal floating-point format (HFP).\n\nIn comparison to IEEE 754 floating-point, the IBM floating-point format has a longer significand, and a shorter exponent. All IBM floating-point formats have 7 bits of exponent with a bias of 64. The normalized range of representable numbers is from 16 to 16 (approx. 5.39761 × 10 to 7.237005 × 10).\n\nThe number is represented as the following formula: (−1) × 0. × 16.\n\nA single-precision binary floating-point number (called \"short\" by IBM) is stored in a 32-bit word:\n\nIn this format the initial bit is not suppressed, and the\nradix point is set to the left of the mantissa in increments of 4 bits.\n\nSince the base is 16, the exponent in this form is about twice as large as the equivalent in IEEE 754, in order to have similar exponent range in binary, 9 exponent bits would be required.\n\nConsider encoding the value −118.625 as an IBM single-precision floating-point value.\n\nThe value is negative, so the sign bit is 1.\n\nThe value 118.625 in binary is 1110110.101. This value is normalized by moving the radix point left four bits (one hexadecimal digit) at a time until the leftmost digit is zero, yielding 0.01110110101. The remaining rightmost digits are padded with zeros, yielding a 24-bit fraction of .0111 0110 1010 0000 0000 0000.\n\nThe normalized value moved the radix point two digits to the left, yielding a multiplier and exponent of 16. A bias of +64 is added to the exponent (+2), yielding +66, which is 100 0010.\n\nCombining the sign, exponent plus bias, and normalized fraction produces this encoding:\n\nIn other words, the number represented is −0.76A000 × 16 = −0.4633789… × 16 = −118.625\n\nThe number represented is +0.FFFFFF × 16 = (1 − 16) × 16 ≈ +7.2370051 × 10\n\nThe number represented is +0.1 × 16 = 16 × 16 ≈ +5.397605 × 10\n\nZero (0.0) is represented in normalized form as all zero bits, which is arithmetically the value +0.0 × 16 = +0 × 16 ≈ +0.000000 × 10 = 0. Given a significand of all-bits zero, any combination of positive or negative sign bit and a non-zero biased exponent will yield a value arithmetically equal to zero. However, the normalized form generated for zero by CPU hardware is all-bits zero. This is true for all three floating-point precision formats.\n\nSince the base is 16, there can be up to three leading zero bits in the binary significand. That means when the number is converted into binary, there can be as few as 21 bits of precision. Because of the \"wobbling precision\" effect, this can cause some calculations to be very inaccurate.\n\nA good example of the inaccuracy is representation of decimal value 0.1. It has no exact binary or hexadecimal representation. In hexadecimal format, it is represented as 0.19999999... or 0.0001 1001 1001 1001 1001 1001 1001..., that is:\n\nThis has only 21 bits, whereas the binary version has 24 bits of precision.\n\nSix hexadecimal digits of precision is roughly equivalent to six decimal digits (i.e. (6 − 1) log(16) ≈ 6.02). A conversion of single precision hexadecimal float to decimal string would require at least 9 significant digits (i.e. 6 log(16) + 1 ≈ 8.22) in order to convert back to the same hexadecimal float value.\n\nThe double-precision floating-point format (called \"long\" by IBM) is the same as the \"short\" format except that the mantissa (fraction) field is wider and the double-precision number is stored in a double word (8 bytes):\n\nThe exponent for this format covers only about a quarter of the range as the corresponding IEEE binary format.\n\n14 hexadecimal digits of precision is roughly equivalent to 17 decimal digits. A conversion of double precision hexadecimal float to decimal string would require at least 18 significant digits in order to convert back to the same hexadecimal float value.\n\nCalled extended-precision by IBM, a quadruple-precision floating-point format was added to the System/370 series and was available on some S/360 models (S/360-85, -195, and others by special request or simulated by OS software). The extended-precision mantissa (fraction) field is wider, and the extended-precision number is stored as two double words (16 bytes):\n\n28 hexadecimal digits of precision is roughly equivalent to 32 decimal digits. A conversion of extended precision hexadecimal float to decimal string would require at least 35 significant digits in order to convert back to the same hexadecimal float value.\n\nMost arithmetic operations truncate like simple pocket calculators. Therefore, 1 − 16 = 1. In this case, the result is rounded away from zero.\n\nStarting with the S/390 G5 in 1998, IBM mainframes have also included IEEE binary floating-point units which conform to the IEEE 754 Standard for Floating-Point Arithmetic. IEEE decimal floating-point was added to IBM System z9 GA2 in 2007 using millicode and in 2008 to the IBM System z10 in hardware.\n\nModern IBM mainframes support three floating-point radices with 3 hexadecimal (HFP) formats, 3 binary (BFP) formats, and 3 decimal (DFP) formats. There are two floating-point units per core; one supporting HFP and BFP, and one supporting DFP; there is one register file, FPRs, which holds all 3 formats.\n\nThe IBM floating-point format is used in:\n\nAs IBM is the only remaining provider of hardware (and only in their mainframes) using their non-standard floating-point format, no popular file format requires it; Except the FDA requires the SAS file format and \"All floating-point numbers in the file are stored using the IBM mainframe representation. [..] Most platforms use the IEEE representation for floating-point numbers. [..] To assist you in reading and/or writing transport files, we are providing routines to convert from IEEE representation (either big endian or little endian) to transport representation and back again.\" Code for IBM's format is also available under LGPLv2.1.\n\n\n\n"}
{"id": "3100378", "url": "https://en.wikipedia.org/wiki?curid=3100378", "title": "Ian G. Macdonald", "text": "Ian G. Macdonald\n\nIan Grant Macdonald (born 11 October 1928 in London, England) is a British mathematician known for his contributions to symmetric functions, special functions, Lie algebra theory and other aspects of algebra, algebraic combinatorics, and combinatorics.\n\nHe was educated at Winchester College and Trinity College, Cambridge, graduating in 1952. He then spent five years as a civil servant. He was offered a position at Manchester University in 1957 by Max Newman, on the basis of work he had done while outside academia. In 1960 he moved to the University of Exeter, and in 1963 became a Fellow of Magdalen College, Oxford. He became Fielden Professor at Manchester in 1972, and professor at Queen Mary College, University of London, in 1976.\n\nHe worked on symmetric products of algebraic curves, Jordan algebras and the representation theory of groups over local fields. In 1972 he proved the Macdonald identities, after a pattern known to Freeman Dyson. His 1979 book \"Symmetric Functions and Hall Polynomials\" has become a classic. Symmetric functions are an old theory, part of the theory of equations, to which both K-theory and representation theory lead. His was the first text to integrate much classical theory, such as Hall polynomials, Schur functions, the Littlewood–Richardson rule, with the abstract algebra approach. It was both an expository work and, in part, a research monograph, and had a major impact in the field. The Macdonald polynomials are now named after him. The Macdonald conjectures from 1982 also proved most influential.\n\nMacdonald was elected a Fellow of the Royal Society in 1979. In 1991 he received the Pólya Prize of the London Mathematical Society. He was awarded the 2009 Steele Prize for Mathematical Exposition. In 2012 he became a fellow of the American Mathematical Society.\n\n\n"}
{"id": "46721841", "url": "https://en.wikipedia.org/wiki?curid=46721841", "title": "Jon T. Pitts", "text": "Jon T. Pitts\n\nJon T. Pitts (born 1948) is an American mathematician working on geometric analysis and variational calculus. He is a professor at Texas A&M University.\n\nPitts obtained his Ph.D. from Princeton University in 1974 under the supervision of Frederick Almgren, Jr., with the thesis \"Every Compact Three-Dimensional Manifold Contains Two-Dimensional Minimal Submanifolds\".\n\nHe received a Sloan Fellowship in 1981.\n\nThe Almgren–Pitts min-max theory is named after his teacher and him.\n\n\n"}
{"id": "49589765", "url": "https://en.wikipedia.org/wiki?curid=49589765", "title": "Kunstweg", "text": "Kunstweg\n\nBürgi's Kunstweg is a set of algorithms invented by Jost Bürgi at the end of the 16th century. They can be used for the calculation of sines to an arbitrary precision. Bürgi used these algorithms to calculate a Canon Sinuum, a table of sines in steps of 2 arc seconds. It is thought that this table had 8 sexagesimal places. Some authors have speculated that this table only covered the range from 0 to 45 degrees, but nothing seems to support this claim. Such tables were extremely important for navigation at sea. Johannes Kepler called the Canon Sinuum the most precise known table of sines (reference?). Bürgi explained his algorithms in his work Fundamentum Astronomiae which he presented to Emperor Rudolf II. in 1592.\n\nThe principles of iterative sine table calculation through the Kunstweg are as follows: cells in a column sum up the values of the two previous cells in the same column. The final cell's value is divided by two, and the next iteration starts. Finally, the values of the last column get normalized. Rather accurate approximations of sines are obtained after few iterations.\n\nAs recently as 2015, Folkerts et al. showed that this simple process converges indeed towards the true sines. According to Folkerts, this was the first step towards difference calculus.\n"}
{"id": "22874054", "url": "https://en.wikipedia.org/wiki?curid=22874054", "title": "Lajos Pósa (mathematician)", "text": "Lajos Pósa (mathematician)\n\nLajos Pósa (born December 9, 1947 in Budapest) is a Hungarian mathematician working in the topic of combinatorics, and one of the most prominent mathematics educators of Hungary, best known of his mathematics camps for gifted students. Winner of the Széchenyi Prize.\nPaul Erdős's favorite \"child\", he discovered theorems at the age of 16. Since 2002 he works at the Rényi Institute of the Hungarian Academy of Sciences; earlier he was at the Eötvös Loránd University, at the Departments of Mathematical Analysis, Computer Science.\n\nHe was born in Budapest, Hungary on December 9, 1947. His father was a chemist, his mother a mathematics teacher. He was a child prodigy. While still in elementary school, the educator Rózsa Péter, friend of his mother introduced him to Paul Erdős, who invited him\nfor lunch in a restaurant, and bombarded him with mathematical questions. Pósa finished the problems sooner than his soup, which impressed Erdős, who himself had been a child prodigy, and who supported young talents with much care and competence. That is how Pósa’s first paper was born, co-authored with Erdős (hence his Erdős number is 1).\n\nHe went to the first special mathematics class of the country at Fazekas Mihály Secondary School from 1962 to 1966, where his classmates included Miklós Laczkovich, László Lovász, József Pelikán, Zsolt Baranyai, István Berkes, Katalin Vesztergombi, Péter Major. He won the first prize on the International Mathematical Olympiad in 1966 (Bulgaria) and second prize in 1965 (Germany).\n\nHe started his Mathematics studies at ELTE University in 1966, and graduated in 1971. From 1971 to 1982 he worked at the Department of Mathematical Analysis at ELTE University, and he obtained a doctorate in 1983 with his dissertation about Hamiltonian circuits of random graphs. From 1984 to 2002 he worked at the Department of Computer Science at ELTE University, and since 2002 he has been a member of the Rényi Mathematical Institute.\n\nDespite his significant results in mathematical research, he stopped research and devoted himself fully to Mathematics Education. Erdős, who preferred him among all his protégés, expressed his regret that Pósa had stopped research with the typical Erdős style phrase “Pósa is dead.”\n\nHe started teaching mathematics very early. He tutored his secondary school classmates, and during his first year at university he started teaching extracurricular courses at his former secondary school. His students at that time included: László Babai, György Elekes, Péter Komjáth, Imre Z. Ruzsa.\n\nAt the beginning of the 1970s he got involved with the school reform movement called complex teaching of mathematics led by Tamás Varga. Pósa worked on the reform of secondary mathematics teaching, while he taught at Radnóti Miklós Secondary School from 1976 to 1980. From 1982 to 1989 he was a member of the Research Group on Mathematics Education led by János Surányi. From 1982 to 1991 he had two experimental\nclasses at Eötvös József Secondary School. His teaching materials written at that time were tested in several classes, and based on these he and colleagues have written a textbook series for the four years of secondary school.\n\nNevertheless, Pósa is best known for finding and teaching gifted students. Since 1988 he has been organizing his own week-end maths camps. There are several groups of 20-35 students, and each group has two or three camps a year. In a camp, students mostly work in groups of\n2-4, but there are also plenary sessions where they discuss solutions, and sum up important thoughts. Students work on problems carefully built on each other. There are several topics running parallelly, and one topic spans several camps. The emphasis is on thinking, proving, and the connection between seemingly distant ideas. The camps are also an important scene of teacher training, as prospective teachers observe and help in the camps.\n\n\n\n\n\n"}
{"id": "2854390", "url": "https://en.wikipedia.org/wiki?curid=2854390", "title": "Leray cover", "text": "Leray cover\n\nIn mathematics, a Leray cover(ing) is a cover of a topological space which allows for easy calculation of its cohomology. Such covers are named after Jean Leray. \n\nSheaf cohomology measures the extent to which a locally exact sequence on a fixed topological space, for instance the de Rham sequence, fails to be globally exact. Its definition, using derived functors, is reasonably natural, if technical. Moreover, important properties, such as the existence of a long exact sequence in cohomology corresponding to any short exact sequence of sheaves, follow directly from the definition. However, it is virtually impossible to calculate from the definition. On the other hand, Čech cohomology with respect to an open cover is well-suited to calculation, but of limited usefulness because it depends on the open cover chosen, not only on the sheaves and the space. By taking a direct limit of Čech cohomology over arbitrarily fine covers, we obtain a Čech cohomology theory that does not depend on the open cover chosen. In reasonable circumstances (for instance, if the topological space is paracompact), the derived-functor cohomology agrees with this Čech cohomology obtained by direct limits. However, like the derived functor cohomology, this cover-independent Čech cohomology is virtually impossible to calculate from the definition. The Leray condition on an open cover ensures that the cover in question is already \"fine enough.\" The derived functor cohomology agrees with the Čech cohomology with respect to any Leray cover.\n\nLet formula_1 be an open cover of the topological space formula_2, and formula_3 a sheaf on X. We say that formula_4 is a Leray cover with respect to formula_3 if, for every nonempty finite set formula_6 of indices, and for all formula_7, we have that formula_8, in the derived functor cohomology. For example, if formula_2 is a separated scheme, and formula_3 is quasicoherent, then any cover of formula_2 by open affine subschemes is a Leray cover.\n"}
{"id": "23256783", "url": "https://en.wikipedia.org/wiki?curid=23256783", "title": "List of PPAD-complete problems", "text": "List of PPAD-complete problems\n\nThis is a list of PPAD-complete problems.\n\n\n\n\n\n\n"}
{"id": "20935181", "url": "https://en.wikipedia.org/wiki?curid=20935181", "title": "MacVector", "text": "MacVector\n\nMacVector is a commercial sequence analysis application for Apple Macintosh computers running Mac OS X. It is intended to be used by molecular biologists to help analyze, design, research and document their experiments in the laboratory.\n\nMacVector is a collection of sequence analysis algorithms linked to various sequence editors, including a single sequence editor, a multiple sequence alignment editor and a contig editor. MacVector tries to use a minimum of windows and steps to access all the functionality. Functions include:\n\n\nMacVector has a contig assembly plugin called Assembler that uses phred, phrap, Bowtie, SPAdes, Velvet and cross match.\n\nAs of version 13.0.1 MacVector uses Sparkle for updating between releases.\n\nMacVector was originally developed by IBI in 1994. It was acquired by Kodak, and subsequently Oxford Molecular in 1996. Oxford Molecular was merged into Accelrys in 2001. It was acquired by MacVector, Inc on 1 January 2007.\n\n\n"}
{"id": "27857702", "url": "https://en.wikipedia.org/wiki?curid=27857702", "title": "Mandelbox", "text": "Mandelbox\n\nIn mathematics, the mandelbox is a fractal with a boxlike shape found by Tom Lowe in 2010. It is defined in a similar way to the famous Mandelbrot set as the values of a parameter such that the origin does not escape to infinity under iteration of certain geometrical transformations. The mandelbox is defined as a map of continuous Julia sets, but, unlike the Mandelbrot set, can be defined in any number of dimensions.. It is typically drawn in three dimensions for illustrative purposes.\n\nThe iteration applies to vector \"z\" as follows:\n\nHere, \"c\" is the constant being tested, and \"scale\" is a real number.\n\nA notable property of the mandelbox, particularly for scale -1.5, is that it contains approximations of many well known fractals within it.\n\nFor 1<|scale|<2 the mandelbox contains a solid core. Consequently its fractal dimension is 3, or n when generalised to n dimensions.\n\nFor scale < -1 the mandelbox sides have length 4 and for 1 < scale <= 4+1 they have length 4(scale+1)/(scale-1)\n\n\n"}
{"id": "488896", "url": "https://en.wikipedia.org/wiki?curid=488896", "title": "Mathematical Tables Project", "text": "Mathematical Tables Project\n\nThe Mathematical Tables Project was one of the largest and most sophisticated computing organizations that operated prior to the invention of the digital electronic computer. Begun in the United States in 1938 as a project of the Works Progress Administration (WPA), it employed 450 unemployed clerks to tabulate higher mathematical functions, such as exponential functions, logarithms, and trigonometric functions. These tables were eventually published in a 28 volume set by Columbia University Press.\n\nThe group was led by a group of mathematicians and physicists, most of whom had been unable to find professional work during the Great Depression. The mathematical leader was Gertrude Blanch, who had just finished her doctorate in mathematics at Cornell University. She had been unable to find a university position and was working at a photographic company before joining the project.\n\nThe administrative director was Arnold Lowan, who had a degree in physics from Columbia University and had spent a year at the Institute for Advanced Study in Princeton University before returning to New York without a job. Perhaps the most accomplished mathematician to be associated with the group was Cornelius Lanczos, who had once served as an assistant to Albert Einstein. He spent a year with the project and organized seminars on computation and applied mathematics at the project's office in Lower Manhattan.\n\nIn addition to computing tables of mathematical functions, the project did large computations for sciences, including the physicist Hans Bethe and did calculations for a variety of war projects including tables for the LORAN navigation system, tables for microwave radar, bombing tables, and shock wave propagation tables.\n\nThe Mathematical Tables Project survived the termination of the WPA in 1943 and continued to operate in New York until 1948. At that point, roughly 25 members of the group moved to Washington, DC to become the Computation Laboratory of the National Bureau of Standards, now the National Institute of Standards and Technology. Blanch moved to Los Angeles to lead the computing office of the Institute for Numerical Analysis at UCLA and Arnold Lowan joined the faculty of Yeshiva University in New York. The greatest legacy of the project is the \"Handbook of Mathematical Functions\", which was published 16 years after the group disbanded. Edited by two veterans of the project, Milton Abramowitz and Irene Stegun, it became a widely circulated mathematical and scientific reference.\n\n"}
{"id": "18837", "url": "https://en.wikipedia.org/wiki?curid=18837", "title": "Median", "text": "Median\n\nThe median is the value separating the higher half from the lower half of a data sample (a population or a probability distribution). For a data set, it may be thought of as the \"middle\" value. For example, in the data set {1, 3, 3, 6, 7, 8, 9}, the median is 6, the fourth largest, and also the fourth smallest, number in the sample. For a continuous probability distribution, the median is the value such that a number is equally likely to fall above or below it.\n\nThe median is a commonly used measure of the properties of a data set in statistics and probability theory. The basic advantage of the median in describing data compared to the mean (often simply described as the \"average\") is that it is not skewed so much by extremely large or small values, and so it may give a better idea of a \"typical\" value. For example, in understanding statistics like household income or assets which vary greatly, a mean may be skewed by a small number of extremely high or low values. Median income, for example, may be a better way to suggest what a \"typical\" income is.\n\nBecause of this, the median is of central importance in robust statistics, as it is the most resistant statistic, having a breakdown point of 50%: so long as no more than half the data are contaminated, the median will not give an arbitrarily large or small result.\n\nThe median of a finite list of numbers can be found by arranging all the numbers from smallest to greatest.\n\nIf there is an odd number of numbers, the middle one is picked. For example, consider the list of numbers\n\nThis list contains seven numbers. The median is the fourth of them, which is 6.\n\nIf there is an even number of observations, then there is no single middle value; the median is then usually defined to be the mean of the two middle values. For example, in the data set\n\nthe median is the mean of the middle two numbers: this is formula_1, which is formula_2. (In more technical terms, this interprets the median as the fully trimmed mid-range).\n\nThe formula used to find the index of the middle number of a data set of \"n\" numerically ordered numbers is formula_3. This either gives the middle number (for an odd number of values) or the halfway point between the two middle values. For example, with 14 values, the formula will give an index of 7.5, and the median will be taken by averaging the seventh (the floor of this index) and eighth (the ceiling of this index) values. So the median can be represented by the following formula:\n\nOne can find the median using the Stem-and-Leaf Plot.\n\nThere is no widely accepted standard notation for the median, but some authors represent the median of a variable \"x\" either as \"x͂\" or as \"μ\" sometimes also \"M\". In any of these cases, the use of these or other symbols for the median needs to be explicitly defined when they are introduced.\n\nThe median is used primarily for skewed distributions, which it summarizes differently from the arithmetic mean. Consider the multiset { 1, 2, 2, 2, 3, 14 }. The median is 2 in this case, (as is the mode), and it might be seen as a better indication of central tendency (less susceptible to the exceptionally large value in data) than the arithmetic mean of 4.\n\nThe median is a popular summary statistic used in descriptive statistics, since it is simple to understand and easy to calculate, while also giving a measure that is more robust in the presence of outlier values than is the mean. The widely cited empirical relationship between the relative locations of the mean and the median for skewed distributions is, however, not generally true. There are, however, various relationships for the \"absolute\" difference between them; see below.\n\nWith an even number of observations (as shown above) no value need be exactly at the value of the median. Nonetheless, the value of the median is uniquely determined with the usual definition. A related concept, in which the outcome is forced to correspond to a member of the sample, is the medoid.\n\nIn a population, at most half have values strictly less than the median and at most half have values strictly greater than it. If each group contains less than half the population, then some of the population is exactly equal to the median. For example, if \"a\" < \"b\" < \"c\", then the median of the list {\"a\", \"b\", \"c\"} is \"b\", and, if \"a\" < \"b\" < \"c\" < \"d\", then the median of the list {\"a\", \"b\", \"c\", \"d\"} is the mean of \"b\" and \"c\"; i.e., it is (\"b\" + \"c\")/2. Indeed, as it is based on the middle data in a group, it is not necessary to even know the value of extreme results in order to calculate a median. For example, in a psychology test investigating the time needed to solve a problem, if a small number of people failed to solve the problem at all in the given time a median can still be calculated.\n\nThe median can be used as a measure of location when a distribution is skewed, when end-values are not known, or when one requires reduced importance to be attached to outliers, e.g., because they may be measurement errors.\n\nA median is only defined on ordered one-dimensional data, and is independent of any distance metric. A geometric median, on the other hand, is defined in any number of dimensions.\n\nThe median is one of a number of ways of summarising the typical values associated with members of a statistical population; thus, it is a possible location parameter. The median is the 2nd quartile, 5th decile, and 50th percentile. Since the median is the same as the \"second quartile\", its calculation is illustrated in the article on quartiles. A median can be worked out for ranked but not numerical classes (e.g. working out a median grade when students are graded from A to F), although the result might be halfway between grades if there is an even number of cases.\n\nWhen the median is used as a location parameter in descriptive statistics, there are several choices for a measure of variability: the range, the interquartile range, the mean absolute deviation, and the median absolute deviation.\n\nFor practical purposes, different measures of location and dispersion are often compared on the basis of how well the corresponding population values can be estimated from a sample of data. The median, estimated using the sample median, has good properties in this regard. While it is not usually optimal if a given population distribution is assumed, its properties are always reasonably good. For example, a comparison of the efficiency of candidate estimators shows that the sample mean is more statistically efficient than the sample median when data are uncontaminated by data from heavy-tailed distributions or from mixtures of distributions, but less efficient otherwise, and that the efficiency of the sample median is higher than that for a wide range of distributions. More specifically, the median has a 64% efficiency compared to the minimum-variance mean (for large normal samples), which is to say the variance of the median will be ~50% greater than the variance of the mean—see asymptotic efficiency and references therein.\n\nFor any probability distribution on the real line R with cumulative distribution function \"F\", regardless of whether it is any kind of continuous probability distribution, in particular an absolutely continuous distribution (which has a probability density function), or a discrete probability distribution, a median is by definition any real number \"m\" that satisfies the inequalities\n\nor, equivalently, the inequalities\n\nin which a Lebesgue–Stieltjes integral is used. For an absolutely continuous probability distribution with probability density function \"ƒ\", the median satisfies\n\nAny probability distribution on R has at least one median, but in specific cases there may be more than one median. Specifically, if a probability density is zero on an interval [\"a\", \"b\"], and the cumulative distribution function at \"a\" is 1/2, any value between \"a\" and \"b\" will also be a median.\n\nThe medians of certain types of distributions can be easily calculated from their parameters; furthermore, they exist even for some distributions lacking a well-defined mean, such as the Cauchy distribution:\n\nThe \"mean absolute error\" of a real variable \"c\" with respect to the random variable \"X\" is\nProvided that the probability distribution of \"X\" is such that the above expectation exists, then \"m\" is a median of \"X\" if and only if \"m\" is a minimizer of the mean absolute error with respect to \"X\". In particular, \"m\" is a sample median if and only if \"m\" minimizes the arithmetic mean of the absolute deviations.\n\nMore generally, a median is defined as a minimum of\nas discussed below in the section on multivariate medians (specifically, the spatial median).\n\nThis optimization-based definition of the median is useful in statistical data-analysis, for example, in \"k\"-medians clustering.\n\nIt can be shown for a unimodal distribution that the median formula_10 and the mean formula_11 lie within (3/5) ≈ 0.7746 standard deviations of each other. In symbols,\n\nwhere |·| is the absolute value.\n\nA similar relation holds between the median and the mode: they lie within 3 ≈ 1.732 standard deviations of each other:\n\nIf the distribution has finite variance, then the distance between the median and the mean is bounded by one standard deviation.\n\nThis bound was proved by Mallows, who used Jensen's inequality twice, as follows. We have\n\nThe first and third inequalities come from Jensen's inequality applied to the absolute-value function and the square function, which are each convex. The second inequality comes from the fact that a median minimizes the absolute deviation function\n\nThis proof also follows directly from Cantelli's inequality.\nThe result can be generalized to obtain a multivariate version of the inequality, as follows:\n\nwhere \"m\" is a spatial median, that is, a minimizer of the function\nformula_17 The spatial median is unique when the data-set's dimension is two or more. An alternative proof uses the one-sided Chebyshev inequality; it appears in .\n\nJensen's inequality states that for any random variable \"x\" with a finite expectation \"E\"(\"x\") and for any convex function \"f\"\n\nIt has been shown that if \"x\" is a real variable with a unique median \"m\" and \"f\" is a C function then\n\nA C function is a real valued function, defined on the set of real numbers \"R\", with the property that for any real \"t\"\n\nis a closed interval, a singleton or an empty set.\n\nEven though comparison-sorting \"n\" items requires operations, selection algorithms can compute the 'th-smallest of items with only operations. This includes the median, which is the 'th order statistic (or for an even number of samples, the arithmetic mean of the two middle order statistics).\n\nSelection algorithms still have the downside of requiring memory, that is, they need to have the full sample (or a linear-sized portion of it) in memory. Because this, as well as the linear time requirement, can be prohibitive, several estimation procedures for the median have been developed. A simple one is the median of three rule, which estimates the median as the median of a three-element subsample; this is commonly used as a subroutine in the quicksort sorting algorithm, which uses an estimate of its input's median. A more robust estimator is Tukey's \"ninther\", which is the median of three rule applied with limited recursion: if is the sample laid out as an array, and\n\nthen\n\nThe \"remedian\" is an estimator for the median that requires linear time but sub-linear memory, operating in a single pass over the sample.\n\nIn individual series (if number of observation is very low) first one must arrange all the observations in order. Then count(\"n\") is the total number of observation in given data.\n\nIf \"n\" is odd then Median (\"M\") = value of ((\"n\" + 1)/2)th item term.\n\nIf \"n\" is even then Median (\"M\") = value of [(\"n\"/2)th item term + (\"n\"/2 + 1)th item term]/2\n\n\nAs an example, we will calculate the sample median for the following set of observations: 1, 5, 2, 8, 7.\n\nStart by sorting the values: 1, 2, 5, 7, 8.\n\nIn this case, the median is 5 since it is the middle observation in the ordered list.\n\nThe median is the ((\"n\" + 1)/2)th item, where \"n\" is the number of values. For example, for the list {1, 2, 5, 7, 8}, we have \"n\" = 5, so the median is the ((5 + 1)/2)th item.\n\n\nAs an example, we will calculate the sample median for the following set of observations: 1, 6, 2, 8, 7, 2.\n\nStart by sorting the values: 1, 2, 2, 6, 7, 8.\n\nIn this case, the arithmetic mean of the two middlemost terms is (2 + 6)/2 = 4. Therefore, the median is 4 since it is the arithmetic mean of the middle observations in the ordered list.\n\nThe distributions of both the sample mean and the sample median were determined by Laplace. The distribution of the sample median from a population with a density function formula_21 is asymptotically normal with mean formula_22 and variance\n\nwhere formula_22 is the median of formula_21 and formula_26 is the sample size.\n\nThese results have also been extended. It is now known for the formula_27-th quantile that the distribution of the sample formula_27-th quantile is asymptotically normal around the formula_27-th quantile with variance equal to\nwhere formula_31 is the value of the distribution density at the formula_27-th quantile.\n\nIn the case of a discrete variable, the sampling distribution of the median for small-samples can be investigated as follows. We take the sample size to be an odd number formula_33. If a given value formula_34 is to be the median of the sample then two conditions must be satisfied. The first is that at most formula_35 observations can have a value of formula_36 or less. The second is that at most formula_35 observations can have a value of formula_38 or more. Let formula_39 be the number of observations which have a value of formula_36 or less and let formula_41 be the number of observations which have a value of formula_38 or more. Then formula_39 and formula_41 both have a minimum value of 0 and a maximum of formula_35. If an observation has a value below formula_34, it is not relevant how far below formula_34 it is and conversely, if an observation has a value above formula_34, it is not relevant how far above formula_34 it is. We can therefore represent the observations as following a trinomial distribution with probabilities formula_50, formula_51 and formula_52. The probability that the median formula_53 will have a value formula_34 is then given by\n\nSumming this over all values of formula_34 defines a proper distribution and gives a unit sum. In practice, the function formula_51 will often not be known but it can be estimated from an observed frequency distribution. An example is given in the following table where the actual distribution is not known but a sample of 3,800 observations allows a sufficiently accurate assessment of formula_51.\n\nUsing these data it is possible to investigate the effect of sample size on the standard errors of the mean and median. The observed mean is 3.16, the observed raw median is 3 and the observed interpolated median is 3.174. The following table gives some comparison statistics. The standard error of the median is given both from the above expression for formula_59 and from the asymptotic approximation given earlier.\nThe expected value of the median falls slightly as sample size increases while, as would be expected, the standard errors of both the median and the mean are proportionate to the inverse square root of the sample size. The asymptotic approximation errs on the side of caution by overestimating the standard error.\n\nIn the case of a continuous variable, the following argument can be used. If a given value formula_34 is to be the median, then one observation must take the value formula_34. The elemental probability of this is formula_62. Then, of the remaining formula_63 observations, exactly formula_35 of them must be above formula_34 and the remaining formula_35 below. The probability of this is the formula_35th term of a binomial distribution with parameters formula_68 and formula_63. Finally we multiply by formula_70 since any of the observations in the sample can be the median observation. Hence the elemental probability of the median at the point formula_34 is given by\n\nNow we introduce the beta function. For integer arguments formula_73 and formula_74, this can be expressed as formula_75. Also, we note that formula_76. Using these relationships and setting both formula_73 and formula_74 equal to formula_79 allows the last expression to be written as\n\nHence the density function of the median is a symmetric beta distribution over the unit interval which supports formula_68. Its mean, as we would expect, is 0.5 and its variance is formula_82. The corresponding variance of the sample median is\n\nHowever this finding can only be used if the density function formula_51 is known or can be assumed. As this will not always be the case, the median variance has to be estimated sometimes from the sample data.\n\n\nThe value of formula_85—the asymptotic value of formula_86 where formula_87 is the population median—has been studied by several authors. The standard \"delete one\" jackknife method produces inconsistent results. An alternative—the \"delete k\" method—where formula_88 grows with the sample size has been shown to be asymptotically consistent. This method may be computationally expensive for large data sets. A bootstrap estimate is known to be consistent, but converges very slowly (order of formula_89). Other methods have been proposed but their behavior may differ between large and small samples.\n\n\nThe efficiency of the sample median, measured as the ratio of the variance of the mean to the variance of the median, depends on the sample size and on the underlying population distribution. For a sample of size formula_90 from the normal distribution, the efficiency for large N is\n\nThe efficiency tends to formula_92 as formula_93 tends to infinity.\n\nFor univariate distributions that are \"symmetric\" about one median, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population median.\n\nIf data are represented by a statistical model specifying a particular family of probability distributions, then estimates of the median can be obtained by fitting that family of probability distributions to the data and calculating the theoretical median of the fitted distribution. Pareto interpolation is an application of this when the population is assumed to have a Pareto distribution.\n\nThe coefficient of dispersion (CD) is defined as the ratio of the average absolute deviation from the median to the median of the data. It is a statistical measure used by the states of Iowa, New York and South Dakota in estimating dues taxes. In symbols\n\nwhere \"n\" is the sample size, \"m\" is the sample median and \"x\" is a variate. The sum is taken over the whole sample.\n\nConfidence intervals for a two-sample test in which the sample sizes are large have been derived by Bonett and Seier This test assumes that both samples have the same median but differ in the dispersion around it. The confidence interval (CI) is bounded inferiorly by\n\nwhere \"t\" is the mean absolute deviation of the \"j\" sample, var() is the variance and \"z\" is the value from the normal distribution for the chosen value of \"α\": for \"α\" = 0.05, \"z\" = 1.96. The following formulae are used in the derivation of these confidence intervals\n\nwhere \"r\" is the Pearson correlation coefficient between the squared deviation scores\n\n\"a\" and \"b\" here are constants equal to 1 and 2, \"x\" is a variate and \"s\" is the standard deviation of the sample.\n\nPreviously, this article discussed the univariate median, when the sample or population had one-dimension. When the dimension is two or higher, there are multiple concepts that extend the definition of the univariate median; each such multivariate median agrees with the univariate median when the dimension is exactly one.\n\nLet be a set of points in a space with a distance function formula_100. Medoid is defined as\n\nformula_101\n\nThe medoid is often used in clustering using the k-medoid algorithm.\n\nThe marginal median is defined for vectors defined with respect to a fixed set of coordinates. A marginal median is defined to be the vector whose components are univariate medians. The marginal median is easy to compute, and its properties were studied by Puri and Sen.\n\nFor \"N\" vectors in a normed vector space, a spatial median minimizes the average distance\n\nwhere \"x\" and \"a\" are vectors. The spatial median is unique when the data-set's dimension is two or more and the norm is the Euclidean norm (or another strictly convex norm). The spatial median is also called the L1 median, even when the norm is Euclidean. Other names are used especially for finite sets of points: geometric median, Fermat point (in mechanics), or Weber or Fermat-Weber point (in geographical location theory). In the special case where the norm is an L1-norm, then the spatial median and the marginal median are the same.\n\nMore generally, a spatial median is defined as a minimizer of\n\nthis general definition is convenient for defining a spatial median of a population in a finite-dimensional normed space, for example, for distributions without a finite mean. Spatial medians are defined for random vectors with values in a Banach space.\n\nThe spatial median is a robust and highly efficient estimator of a central tendency of a population.\n\nAn alternative generalization of the spatial median in higher dimensions that does not relate to a particular metric is the centerpoint.\n\nWhen dealing with a discrete variable, it is sometimes useful to regard the observed values as being midpoints of underlying continuous intervals. An example of this is a Likert scale, on which opinions or preferences are expressed on a scale with a set number of possible responses. If the scale consists of the positive integers, an observation of 3 might be regarded as representing the interval from 2.50 to 3.50. It is possible to estimate the median of the underlying variable. If, say, 22% of the observations are of value 2 or below and 55.0% are of 3 or below (so 33% have the value 3), then the median formula_53 is 3 since the median is the smallest value of formula_105 for which formula_106 is greater than a half. But the interpolated median is somewhere between 2.50 and 3.50. First we add half of the interval width formula_107 to the median to get the upper bound of the median interval. Then we subtract that proportion of the interval width which equals the proportion of the 33% which lies above the 50% mark. In other words, we split up the interval width pro rata to the numbers of observations. In this case, the 33% is split into 28% below the median and 5% above it so we subtract 5/33 of the interval width from the upper bound of 3.50 to give an interpolated median of 3.35. More formally, if the values formula_108 are known, the interpolated median can be calculated from\n\nAlternatively, if in an observed sample there are formula_41 scores above the median category, formula_111 scores in it and formula_39 scores below it then the interpolated median is given by\n\nFor univariate distributions that are \"symmetric\" about one median, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population median; for non-symmetric distributions, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population \"pseudo-median\", which is the median of a symmetrized distribution and which is close to the population median. The Hodges–Lehmann estimator has been generalized to multivariate distributions.\n\nThe Theil–Sen estimator is a method for robust linear regression based on finding medians of slopes.\n\nIn the context of image processing of monochrome raster images there is a type of noise, known as the salt and pepper noise, when each pixel independently becomes black (with some small probability) or white (with some small probability), and is unchanged otherwise (with the probability close to 1). An image constructed of median values of neighborhoods (like 3×3 square) can effectively reduce noise in this case.\n\nIn cluster analysis, the k-medians clustering algorithm provides a way of defining clusters, in which the criterion of maximising the distance between cluster-means that is used in k-means clustering, is replaced by maximising the distance between cluster-medians.\n\nThis is a method of robust regression. The idea dates back to Wald in 1940 who suggested dividing a set of bivariate data into two halves depending on the value of the independent parameter formula_114: a left half with values less than the median and a right half with values greater than the median. He suggested taking the means of the dependent formula_115 and independent formula_114 variables of the left and the right halves and estimating the slope of the line joining these two points. The line could then be adjusted to fit the majority of the points in the data set.\n\nNair and Shrivastava in 1942 suggested a similar idea but instead advocated dividing the sample into three equal parts before calculating the means of the subsamples. Brown and Mood in 1951 proposed the idea of using the medians of two subsamples rather the means. Tukey combined these ideas and recommended dividing the sample into three equal size subsamples and estimating the line based on the medians of the subsamples.\n\nAny \"mean\"-unbiased estimator minimizes the risk (expected loss) with respect to the squared-error loss function, as observed by Gauss. A \"median\"-unbiased estimator minimizes the risk with respect to the absolute-deviation loss function, as observed by Laplace. Other loss functions are used in statistical theory, particularly in robust statistics.\n\nThe theory of median-unbiased estimators was revived by George W. Brown in 1947:\n\nFurther properties of median-unbiased estimators have been reported. Median-unbiased estimators are invariant under one-to-one transformations.\n\nThere are methods of construction median-unbiased estimators that are optimal (in a sense analogous to minimum-variance property considered for mean-unbiased estimators). Such constructions exist for probability distributions having monotone likelihood-functions. One such procedure is an analogue of the Rao–Blackwell procedure for mean-unbiased estimators: The procedure holds for a smaller class of probability distributions than does the Rao—Blackwell procedure but for a larger class of loss functions.\n\nThe idea of the median appeared in the 13th century in the Talmud (further for possible older mentions)\n\nThe idea of the median also appeared later in Edward Wright's book on navigation (\"Certaine Errors in Navigation\") in 1599 in a section concerning the determination of location with a compass. Wright felt that this value was the most likely to be the correct value in a series of observations.\n\nIn 1757, Roger Joseph Boscovich developed a regression method based on the L1 norm and therefore implicitly on the median.\n\nIn 1774, Laplace suggested the median be used as the standard estimator of the value of a posterior pdf. The specific criterion was to minimize the expected magnitude of the error; formula_117 where formula_118 is the estimate and formula_119 is the true value. Laplaces's criterion was generally rejected for 150 years in favor of the least squares method of Gauss and Legendre which minimizes formula_120 to obtain the mean. The distribution of both the sample mean and the sample median were determined by Laplace in the early 1800s.\n\nAntoine Augustin Cournot in 1843 was the first to use the term \"median\" (\"valeur médiane\") for the value that divides a probability distribution into two equal halves. Gustav Theodor Fechner used the median (\"Centralwerth\") in sociological and psychological phenomena. It had earlier been used only in astronomy and related fields. Gustav Fechner popularized the median into the formal analysis of data, although it had been used previously by Laplace.\n\nFrancis Galton used the English term \"median\" in 1881, having earlier used the terms \"middle-most value\" in 1869, and the \"medium\" in 1880.\n\n\n"}
{"id": "4500167", "url": "https://en.wikipedia.org/wiki?curid=4500167", "title": "Minkowski plane", "text": "Minkowski plane\n\nIn mathematics, a Minkowski plane (named after Hermann Minkowski) is one of the Benz planes: Möbius plane, Laguerre plane and Minkowski plane.\n\nApplying the pseudo-euclidean distance formula_1 on two points formula_2 (instead of the euclidean distance) we get the geometry of \"hyperbolas\", because a pseudo-euclidean circle formula_3 is a hyperbola with midpoint formula_4. \n\nBy a transformation of coordinates formula_5, formula_6, the pseudo-euclidean distance can be rewritten as formula_7. The hyperbolas then have asymptotes parallel to the non-primed coordinate axes.\n\nThe following completion (see Möbius and Laguerre planes) \"homogenizes\" the geometry of hyperbolas:\n\nThe incidence structure formula_11 is called the classical real Minkowski plane.\n\nThe set of points consists of formula_12, two copies of formula_13 and the point formula_14.\n\nAny line formula_15 is completed by point formula_14, any hyperbola\nformula_17 by the two points formula_18 (see figure).\n\nTwo points formula_19 can not be connected by a cycle if and only if\nformula_20 or formula_21. \n\nWe define:\nTwo points formula_22 are (+)-parallel (formula_23) if formula_20 and (−)-parallel (formula_25) if formula_21. \nBoth these relations are equivalence relations on the set of points.\n\nTwo points formula_22 are called parallel (formula_28) if\nformula_23 or formula_25.\n\nFrom the definition above we find:\n\nLemma:\n\nLike the classical Möbius and Laguerre planes Minkowski planes can be described as the geometry of plane sections of a suitable quadric. But in this case the quadric lives in projective 3-space: The classical real Minkowski plane is isomorphic to the geometry of plane sections of a hyperboloid of one sheet (not degenerated quadric of index 2).\n\nLet formula_51 be an incidence structure with the set formula_52 of points, the set formula_53 of cycles and two equivalence relations formula_54 ((+)-parallel) and formula_55 ((−)-parallel) on set formula_52. For formula_57 we define:\nformula_58 and\nformula_59.\nAn equivalence class formula_60 or formula_61 is called (+)-generator\nand (−)-generator, respectively. (For the space model of the classical Minkowski plane a generator is a line on the hyperboloid.)\nTwo points formula_62 are called parallel (formula_63) if formula_64 or formula_65.\n\nAn incidence structure formula_66 is called Minkowski plane if the following axioms hold:\n\nFor investigations the following statements on parallel classes (equivalent to C1, C2 respectively) are advantageous.\n\nFirst consequences of the axioms are\n\nLemma: For a Minkowski plane formula_94 the following is true\n\nAnalogously to Möbius and Laguerre planes we get the connection to the linear\ngeometry via the residues.\n\nFor a Minkowski plane formula_95 and formula_96 we define the local structure\nand call it the residue at point P.\n\nFor the classical Minkowski plane formula_98 is the real affine plane formula_12.\n\nAn immediate consequence of axioms C1 to C4 and C1′, C2′ are the following two theorems.\n\nTheorem: For a Minkowski plane formula_100 any residue is an affine plane.\n\nTheorem:\nLet be formula_95 an incidence structure with two equivalence relations formula_54 and formula_55 on the set formula_52 of points (see above).\n\nThe minimal model of a Minkowski plane can be established over the set\nformula_108 of three elements:\n\nformula_109\n\nformula_110 \n\nformula_111\n\nformula_112 \nformula_113 \nformula_114 \nformula_115 \nformula_116 \nformula_117 \n\nParallel points:\n\nformula_118 if and only if formula_119 \n\nformula_120 if and only if formula_121.\n\nHence: formula_122\nand formula_123.\n\nFor finite Minkowski-planes we get from C1′, C2′:\n\nLemma:\nLet be formula_95 a finite Minkowski plane, i.e. formula_125. For any pair of cycles formula_126 and any pair of generators formula_127 we have:\nformula_128.\n\nThis gives rise of the definition:\nFor a finite Minkowski plane formula_94 and a cycle formula_35 of formula_94 we call the integer formula_132 the order of formula_94.\n\nSimple combinatorial considerations yield\n\nLemma:\nFor a finite Minkowski plane formula_134 the following is true:\n\nWe get the most important examples of Minkowski planes by generalizing the classical real model: Just replace formula_13 by an arbitrary field formula_139 then we get \"in any case\" a Minkowski plane formula_140.\n\nAnalogously to Möbius and Laguerre planes the Theorem of Miquel is a characteristic property of a Minkowski plane formula_141.\nTheorem (Miquel): For the Minkowski plane formula_141 the following is true:\n\nTheorem (Chen): Only a Minkowski plane formula_141 satisfies the theorem of Miquel.\n\nBecause of the last theorem formula_145 is called a miquelian Minkowski plane.\n\nRemark: The minimal model of a Minkowski plane is miquelian.\n\nAn astonishing result is\n\nTheorem (Heise): Any Minkowski plane of \"even\" order is miquelian.\n\nRemark: A suitable stereographic projection shows: formula_145 is isomorphic\nto the geometry of the plane sections on a hyperboloid of one sheet (quadric of index 2) in projective 3-space over field formula_150.\n\nRemark: There are a lot of Minkowski planes that are not miquelian (s. weblink below). But there are no \"ovoidal Minkowski\" planes, in difference to Möbius and Laguerre planes. Because any quadratic set of index 2 in projective 3-space is a quadric (see quadratic set).\n\n\n\n"}
{"id": "32147219", "url": "https://en.wikipedia.org/wiki?curid=32147219", "title": "Mnemonics in trigonometry", "text": "Mnemonics in trigonometry\n\nIn trigonometry, it is common to use mnemonics to help remember trigonometric identities and the relationships between the various trigonometric functions. \n\nThe \"sine\", \"cosine\", and \"tangent\" ratios in a right triangle can be remembered by representing them as strings of letters, for instance SOH-CAH-TOA in English:\n\nOne way to remember the letters is to sound them out phonetically (i.e. ). \n\nAnother method is to expand the letters into a sentence, such as \"Some Old Hags Can't Always Hide Their Old Age\", \"Some Old Hippy Caught Another Hippy Trippin' On Acid\", or sentences that might be more appropriate for younger people is \"Some Old Horse Came A'Hopping Through Our Alley\" and \"Sitting On Hard Concrete Always Hurts Those Outer Areas\".\n\nCommunities in Chinese circles may choose to remember it as TOA-CAH-SOH, which also means 'big-footed woman' () in Hokkien.\n\nAn alternate way to remember the letters for Sin, Cos, and Tan is to memorize the nonsense syllables Oh, Ah, Oh-Ah (i.e. ) for O/H, A/H, O/A. Or, to remember all six functions, Sin, Cos, Tan, Cot, Sec, and Csc, memorize the syllables O/H, A/H, Oh/Ah, Ah/Oh, H/A, H/O (i.e. ). \n\nAll Students Take Calculus is a mnemonic for the sign of each trigonometric functions in each quadrant of the plane. The letters ASTC signify which of the trigonometric functions are positive, starting in the top right 1st quadrant and moving counterclockwise through quadrants 2 to 4.\n\nOther mnemonics include:\n\nOther easy to remember mnemonics are the ACTS and CAST laws. These have the disadvantages of not going sequentially from quadrants 1 to 4 and not reinforcing the numbering convention of the quadrants. \n\nAnother mnemonic permits all of the basic identities to be read off quickly. Although the word part of the mnemonic used to build the chart does not hold in English, the chart itself is fairly easy to reconstruct with a little thought. Functions without \"co\" appear on the left, co-functions on the right, a 1 goes in the middle, triangles point down, and the entire drawing looks like a fallout shelter trefoil.\n\nStarting at any corner of the hexagon:\n\nAside from the last bullet, the specific values for each identity are summarized in this table:\n"}
{"id": "36713242", "url": "https://en.wikipedia.org/wiki?curid=36713242", "title": "Partial differential algebraic equation", "text": "Partial differential algebraic equation\n\nIn mathematics a partial differential algebraic equation (PDAE) set is an incomplete system of partial differential equations that is closed with a set of algebraic equations.\n\nA general PDAE is defined as:\n\nwhere:\n\n\nThe relationship between a PDAE and a partial differential equation (PDE) is analogous to the relationship between an ordinary differential equation (ODE) and a differential algebraic equation (DAE).\n\nPDAEs of this general form are challenging to solve. Simplified forms are studied in more detail in the literature. Even as recently as 2000, the term \"PDAE\" has been handled as unfamiliar by those in related fields.\n\nSemi-discretization is a common method for solving PDAEs whose independent variables are those of time and space, and has been used for decades. This method involves removing the spatial variables using a discretization method, such as the finite volume method, and incorporating the resulting linear equations as part of the algebraic relations. This reduces the system to a DAE, for which conventional solution methods can be employed.\n"}
{"id": "54037964", "url": "https://en.wikipedia.org/wiki?curid=54037964", "title": "Philippe Di Francesco", "text": "Philippe Di Francesco\n\nPhilippe Di Francesco is a French-American mathematician, focusing in mathematical physics, physical combinatorics and integrable systems, currently the Morris and Gertrude Fine Distinguished Professor of Mathematics at University of Illinois.\n"}
{"id": "12518245", "url": "https://en.wikipedia.org/wiki?curid=12518245", "title": "Pinhole camera model", "text": "Pinhole camera model\n\nThe pinhole camera model describes the mathematical relationship between the coordinates of a point in three-dimensional space and its projection onto the image plane of an \"ideal\" pinhole camera, where the camera aperture is described as a point and no lenses are used to focus light. The model does not include, for example, geometric distortions or blurring of unfocused objects caused by lenses and finite sized apertures. It also does not take into account that most practical cameras have only discrete image coordinates. This means that the pinhole camera model can only be used as a first order approximation of the mapping from a 3D scene to a 2D image. Its validity depends on the quality of the camera and, in general, decreases from the center of the image to the edges as lens distortion effects increase.\n\nSome of the effects that the pinhole camera model does not take into account can be compensated, for example by applying suitable coordinate transformations on the image coordinates; other effects are sufficiently small to be neglected if a high quality camera is used. This means that the pinhole camera model often can be used as a reasonable description of how a camera depicts a 3D scene, for example in computer vision and computer graphics.\n\n\" NOTE: The xxx coordinate system in the figure is left-handed, that is the direction of the OZ axis is in reverse to the system the reader may be used to.\"\n\nThe geometry related to the mapping of a pinhole camera is illustrated in the figure. The figure contains the following basic objects:\n\n\nThe \"pinhole\" aperture of the camera, through which all projection lines must pass, is assumed to be infinitely small, a point. In the literature this point in 3D space is referred to as the \"optical (or lens or camera) center\".\n\nNext we want to understand how the coordinates formula_4 of point Q depend on the coordinates formula_2 of point P. This can be done with the help of the following figure which shows the same scene as the previous figure but now from above, looking down in the negative direction of the X2 axis.\n\nIn this figure we see two similar triangles, both having parts of the projection line (green) as their hypotenuses. The catheti of the left triangle are formula_7 and \"f\" and the catheti of the right triangle are formula_8 and formula_9. Since the two triangles are similar it follows that\n\nA similar investigation, looking in the negative direction of the X1 axis gives\n\nThis can be summarized as\n\nwhich is an expression that describes the relation between the 3D coordinates formula_15 of point P and its image coordinates formula_16 given by point Q in the image plane.\n\nThe mapping from 3D to 2D coordinates described by a pinhole camera is a perspective projection followed by a 180° rotation in the image plane. This corresponds to how a real pinhole camera operates; the resulting image is rotated 180° and the relative size of projected objects depends on their distance to the focal point and the overall size of the image depends on the distance \"f\" between the image plane and the focal point. In order to produce an unrotated image, which is what we expect from a camera, there are two possibilities:\n\n\nIn both cases, the resulting mapping from 3D coordinates to 2D image coordinates is given by the expression above, but without the negation, thus\n\nThe mapping from 3D coordinates of points in space to 2D image coordinates can also be represented in homogeneous coordinates. Let formula_18 be a representation of a 3D point in homogeneous coordinates (a 4-dimensional vector), and let formula_19 be a representation of the image of this point in the pinhole camera (a 3-dimensional vector). Then the following relation holds\n\nwhere formula_21 is the formula_22 camera matrix and the formula_23 means equality between elements of projective spaces. This implies that the left and right hand sides are equal up to a non-zero scalar multiplication. A consequence of this relation is that also formula_21 can be seen as an element of a projective space; two camera matrices are equivalent if they are equal up to a scalar multiplication. This description of the pinhole camera mapping, as a linear transformation formula_21 instead of as a fraction of two linear expressions, makes it possible to simplify many derivations of relations between 3D and 2D coordinates.\n\n\n"}
{"id": "403139", "url": "https://en.wikipedia.org/wiki?curid=403139", "title": "Point at infinity", "text": "Point at infinity\n\nIn geometry, a point at infinity or ideal point is an idealized limiting point at the \"end\" of each line.\n\nIn the case of an affine plane (including the Euclidean plane), there is one ideal point for each pencil of parallel lines of the plane. Adjoining these points produces a projective plane, in which no point can be distinguished, if we \"forget\" which points were added. This holds for a geometry over any field, and more generally over any division ring.\n\nIn the real case, a point at infinity completes a line into a topologically closed curve. In higher dimensions, all the points at infinity form a projective subspace of one dimension less than that of the whole projective space to which they belong. A point at infinity can also be added to the complex line (which may be thought of as the complex plane), thereby turning it into a closed surface known as the complex projective line, CP, also called the Riemann sphere (when complex numbers are mapped to each point).\n\nIn the case of a hyperbolic space, each line has two distinct ideal points. Here, the set of ideal points takes the form of a quadric.\n\nIn an affine or Euclidean space of higher dimension, the points at infinity are the points which are added to the space to get the projective completion. The set of the points at infinity is called, depending on the dimension of the space, the line at infinity, the plane at infinity or the hyperplane at infinity, in all cases a projective space of one less dimension.\n\nAs a projective space over a field is a smooth algebraic variety, the same is true for the set of points at infinity. Similarly, if the ground field is the real or the complex field, the set of points at infinity is a manifold.\n\nIn artistic drawing and technical perspective, the projection on the picture plane of the point at infinity of a class of parallel lines is called their vanishing point.\n\nIn hyperbolic geometry, points at infinity are typically named ideal points. Unlike Euclidean and elliptic geometries, each line has two points at infinity: given a line \"l\" and a point \"P\" not on \"l\", the right- and left-limiting parallels converge asymptotically to different points at infinity.\n\nAll points at infinity together form the Cayley absolute or boundary of a hyperbolic plane.\n\nThis construction can be generalized to topological spaces. Different compactifications may exist for a given space, but arbitrary topological space admits Alexandroff extension, also called the \"one-point compactification\" when the original space is not itself compact. Projective line (over arbitrary field) is the Alexandroff extension of the corresponding field. Thus the circle is the one-point compactification of the real line, and the sphere is the one-point compactification of the plane. Projective spaces P for  > 1 are not \"one-point\" compactifications of corresponding affine spaces for the reason mentioned above under , and completions of hyperbolic spaces with ideal points are also not one-point compactifications.\n\n"}
{"id": "40102361", "url": "https://en.wikipedia.org/wiki?curid=40102361", "title": "Primecoin", "text": "Primecoin\n\nPrimecoin (sign: Ψ; code: XPM) is a cryptocurrency that implements a proof-of-work system that searches for chains of prime numbers.\n\nLaunched on July 7, 2013 by anonymous hacker and peercoin founder Sunny King, Primecoin was the first cryptocurrency to have a proof-of-work system with a practical use. Earlier cryptocurrencies, such as Bitcoin, were mined using algorithms that solved arbitrary mathematical problems, the results of which had no value or use outside of mining the cryptocurrency itself. Primecoin's algorithm, however, computed chains of prime numbers (Cunningham and bi-twin chains), the results of which were published on its blockchain's public ledger, available for use by scientists, mathematicians, and anyone else. Use of a proof-of-work system to calculate chains of prime numbers was an innovation that produced useful results while also meeting the criteria for a proof-of-work system: it involved a calculation that was difficult to perform but easy to verify, and the difficulty was adjustable.\n\nShortly after its launch, some trade journals reported that the rush of over 18,000 new users seeking to mine Primecoin overwhelmed providers of dedicated servers.\n\nUnlike Bitcoin, Primecoin targets a block generation period of one minute rather than every ten minutes, changes difficulty every block rather than every 2016 blocks, and has a block reward that is a function of the difficulty (blockreward = 999/difficulty) rather than fixed. Primecoin transactions are confirmed approximately 8–10 times as fast as Bitcoin transactions.\n"}
{"id": "61476", "url": "https://en.wikipedia.org/wiki?curid=61476", "title": "Radius of convergence", "text": "Radius of convergence\n\nIn mathematics, the radius of convergence of a power series is the radius of the largest disk in which the series converges. It is either a non-negative real number or formula_1. When it is positive, the power series converges absolutely and uniformly on compact sets inside the open disk of radius equal to the radius of convergence, and it is the Taylor series of the analytic function to which it converges.\n\nFor a power series \"ƒ\" defined as:\n\nwhere\n\nThe radius of convergence \"r\" is a nonnegative real number or formula_1 such that the series converges if\n\nand diverges if\n\nSome may prefer an alternative definition, as existence is obvious:\n\nOn the boundary, that is, where |\"z\" − \"a\"| = \"r\", the behavior of the power series may be complicated, and the series may converge for some values of \"z\" and diverge for others. The radius of convergence is infinite if the series converges for all complex numbers \"z\".\n\nTwo cases arise. The first case is theoretical: when you know all the coefficients formula_7 then you take certain limits and find the precise radius of convergence. The second case is practical: when you construct a power series solution of a difficult problem you typically will only know a finite number of terms in a power series, anywhere from a couple of terms to a hundred terms. In this second case, extrapolating a plot estimates the radius of convergence.\n\nThe radius of convergence can be found by applying the root test to the terms of the series. The root test uses the number\n\n\"lim sup\" denotes the limit superior. The root test states that the series converges if \"C\" < 1 and diverges if \"C\" > 1. It follows that the power series converges if the distance from \"z\" to the center \"a\" is less than\n\nand diverges if the distance exceeds that number; this statement is the Cauchy–Hadamard theorem. Note that \"r\" = 1/0 is interpreted as an infinite radius, meaning that \"ƒ\" is an entire function.\n\nThe limit involved in the ratio test is usually easier to compute, and when that limit exists, it shows that the radius of convergence is finite.\nThis is shown as follows. The ratio test says the series converges if\n\nThat is equivalent to\n\nUsually, in scientific applications, only a finite number of coefficients formula_7 are known. Typically, as formula_14 increases, these coefficients settle into a regular behavior determined by the nearest radius-limiting singularity. In this case, two main techniques have been developed, based on the fact that the coefficients of a Taylor series are roughly exponential with ratio formula_15 where r is the radius of convergence.\n\n\nThe intercept with formula_21 estimates the reciprocal of the radius of convergence, formula_15.\n\nA power series with a positive radius of convergence can be made into a holomorphic function by taking its argument to be a complex variable. The radius of convergence can be characterized by the following theorem:\n\nThe set of all points whose distance to \"a\" is strictly less than the radius of convergence is called the \"disk of convergence\".\n\n\"The nearest point\" means the nearest point in the complex plane, not necessarily on the real line, even if the center and all coefficients are real. For example, the function\n\nhas no singularities on the real line, since formula_39 has no real roots. Its Taylor series about 0 is given by\n\nThe root test shows that its radius of convergence is 1. In accordance with this, the function \"ƒ\"(\"z\") has singularities at ±\"i\", which are at a distance 1 from 0.\n\nFor a proof of this theorem, see analyticity of holomorphic functions.\n\nThe arctangent function of trigonometry can be expanded in a power series familiar to calculus students:\n\nIt is easy to apply the root test in this case to find that the radius of convergence is 1.\n\nConsider this power series:\n\nwhere the rational numbers \"B\" are the Bernoulli numbers. It may be cumbersome to try to apply the ratio test to find the radius of convergence of this series. But the theorem of complex analysis stated above quickly solves the problem. At \"z\" = 0, there is in effect no singularity since the singularity is removable. The only non-removable singularities are therefore located at the \"other\" points where the denominator is zero. We solve\n\nby recalling that if \"z\" = \"x\" + \"iy\" and \"e\" = cos(\"y\") + \"i\" sin(\"y\") then\n\nand then take \"x\" and \"y\" to be real. Since \"y\" is real, the absolute value of cos(\"y\") + \"i\" sin(\"y\") is necessarily 1. Therefore, the absolute value of \"e\" can be 1 only if \"e\" is 1; since \"x\" is real, that happens only if \"x\" = 0. Therefore \"z\" is pure imaginary and cos(\"y\") + \"i\" sin(\"y\") = 1. Since \"y\" is real, that happens only if cos(\"y\") = 1 and sin(\"y\") = 0, so that \"y\" is an integer multiple of 2. Consequently the singular points of this function occur at\n\nThe singularities nearest 0, which is the center of the power series expansion, are at ±2\"i\". The distance from the center to either of those points is 2, so the radius of convergence is 2.\n\nIf the power series is expanded around the point \"a\" and the radius of convergence is , then the set of all points such that is a circle called the \"boundary\" of the disk of convergence. A power series may diverge at every point on the boundary, or diverge on some points and converge at other points, or converge at all the points on the boundary. Furthermore, even if the series converges everywhere on the boundary (even uniformly), it does not necessarily converge absolutely.\n\nExample 1: The power series for the function , expanded around , which is simply\nhas radius of convergence 1 and diverges at every point on the boundary.\n\nExample 2: The power series for , expanded around , which is\nhas radius of convergence 1, and diverges for but converges for all other points on the boundary. The function of Example 1 is the derivative of .\n\nExample 3: The power series\nhas radius of convergence 1 and converges everywhere on the boundary absolutely. If is the function represented by this series on the unit disk, then the derivative of \"h\"(\"z\") is equal to \"g\"(\"z\")/\"z\" with \"g\" of Example 2. It turns out that is the dilogarithm function.\n\nExample 4: The power series\nhas radius of convergence 1 and converges uniformly on the entire boundary {|\"z\"| = 1}, but does not converge absolutely on the boundary.\n\nIf we expand the function\n\naround the point \"x\" = 0, we find out that the radius of convergence of this series is formula_50 meaning that this series converges for all complex numbers. However, in applications, one is often interested in the precision of a numerical answer. Both the number of terms and the value at which the series is to be evaluated affect the accuracy of the answer. For example, if we want to calculate accurate up to five decimal places, we only need the first two terms of the series. However, if we want the same precision for we must evaluate and sum the first five terms of the series. For , one requires the first 18 terms of the series, and for we need to evaluate the first 141 terms.\n\nSo the fastest convergence of a power series expansion is at the center, and as one moves away from the center of convergence, the rate of convergence slows down until you reach the boundary (if it exists) and cross over, in which case the series will diverge.\n\nAn analogous concept is the abscissa of convergence of a Dirichlet series\n\nSuch a series converges if the real part of \"s\" is greater than a particular number depending on the coefficients \"a\": the abscissa of convergence.\n\n\n"}
{"id": "2227485", "url": "https://en.wikipedia.org/wiki?curid=2227485", "title": "Recursive data type", "text": "Recursive data type\n\nIn computer programming languages, a recursive data type (also known as a recursively-defined, inductively-defined or inductive data type) is a data type for values that may contain other values of the same type. Data of recursive types are usually viewed as directed graphs.\n\nAn important application of recursion in computer science is in defining dynamic data structures such as Lists and Trees. Recursive data structures can dynamically grow to an arbitrarily large size in response to runtime requirements; in contrast, a static array's size requirements must be set at compile time.\n\nSometimes the term \"inductive data type\" is used for algebraic data types which are not necessarily recursive.\n\nAn example is the list type, in Haskell:\n\nThis indicates that a list of a's is either an empty list or a cons cell containing an 'a' (the \"head\" of the list) and another list (the \"tail\").\n\nAnother example is a similar singly linked type in Java:\n\nThis indicates that non-empty list of type E contains a data member of type E, and a reference to another List object for the rest of the list (or a null reference to indicate that this is the end of the list).\n\nData types can also be defined by mutual recursion. The most important basic example of this is a tree, which can be defined mutually recursively in terms of a forest (a list of trees). Symbolically:\nA forest \"f\" consists of a list of trees, while a tree \"t\" consists of a pair of a value \"v\" and a forest \"f\" (its children). This definition is elegant and easy to work with abstractly (such as when proving theorems about properties of trees), as it expresses a tree in simple terms: a list of one type, and a pair of two types.\n\nThis mutually recursive definition can be converted to a singly recursive definition by inlining the definition of a forest:\nA tree \"t\" consists of a pair of a value \"v\" and a list of trees (its children). This definition is more compact, but somewhat messier: a tree consists of a pair of one type and a list another, which require disentangling to prove results about.\n\nIn Standard ML, the tree and forest data types can be mutually recursively defined as follows, allowing empty trees:\nIn type theory, a recursive type has the general form μα.T where the type variable α may appear in the type T and stands for the entire type itself.\n\nFor example, the natural numbers (see Peano arithmetic) may be defined by the Haskell datatype:\n\nIn type theory, we would say: formula_1 where the two arms of the sum type represent the Zero and Succ data constructors. Zero takes no arguments (thus represented by the unit type) and Succ takes another Nat (thus another element of formula_2).\n\nThere are two forms of recursive types: the so-called isorecursive types, and equirecursive types. The two forms differ in how terms of a recursive type are introduced and eliminated.\n\nWith isorecursive types, the recursive type formula_3 and its expansion (or \"unrolling\") formula_4 (Where the notationformula_5 indicates that all instances of Z are replaced with Y in X) are distinct (and disjoint) types with special term constructs, usually called \"roll\" and \"unroll\", that form an isomorphism between them. To be precise: formula_6 and formula_7, and these two are inverse functions.\n\nUnder equirecursive rules, a recursive type formula_3 and its unrolling formula_9 are \"equal\" -- that is, those two type expressions are understood to denote the same type. In fact, most theories of equirecursive types go further and essentially stipulate that any two type expressions with the same \"infinite expansion\" are equivalent. As a result of these rules, equirecursive types contribute significantly more complexity to a type system than isorecursive types do. Algorithmic problems such as type checking and type inference are more difficult for equirecursive types as well. Since direct comparison does not make sense on an equirecursive type, they can be converted into a canonical form in O(n log n) time, which can easily be compared.\n\nEquirecursive types capture the form of self-referential (or mutually referential) type definitions seen in procedural and object-oriented programming languages, and also arise in type-theoretic semantics of objects and classes.\nIn functional programming languages, isorecursive types (in the guise of datatypes) are far more common.\n\nRecursion is not allowed in type synonyms in Miranda, OCaml (unless -rectypes flag is used or it's a record or variant), and Haskell; so for example the following Haskell types are illegal:\n\nInstead, you must wrap it inside an algebraic data type (even if it only has one constructor):\n\nThis is because type synonyms, like typedefs in C, are replaced with their definition at compile time. (Type synonyms are not \"real\" types; they are just \"aliases\" for convenience of the programmer.) But if you try to do this with a recursive type, it will loop infinitely because no matter how many times you substitute it, it still refers to itself, e.g. \"Bad\" will grow indefinitely: (Int, (Int, (Int, ... .\n\nAnother way to see it is that a level of indirection (the algebraic data type) is required to allow the isorecursive type system to figure out when to \"roll\" and \"unroll\".\n\n"}
{"id": "1390873", "url": "https://en.wikipedia.org/wiki?curid=1390873", "title": "Reflection symmetry", "text": "Reflection symmetry\n\nReflection symmetry, line symmetry, mirror symmetry, mirror-image symmetry, is symmetry with respect to reflection. That is, a figure which does not change upon undergoing a reflection has reflectional symmetry.\n\nIn 2D there is a line/axis of symmetry, in 3D a plane of symmetry. An object or figure which is indistinguishable from its transformed image is called mirror symmetric.\n\nIn formal terms, a mathematical object is symmetric with respect to a given operation such as reflection, rotation or translation, if, when applied to the object, this operation preserves some property of the object. The set of operations that preserve a given property of the object form a group. Two objects are symmetric to each other with respect to a given group of operations if one is obtained from the other by some of the operations (and vice versa).\n\nThe symmetric function of a two-dimensional figure is a line such that, for each perpendicular constructed, if the perpendicular intersects the figure at a distance 'd' from the axis along the perpendicular, then there exists another intersection of the shape and the perpendicular, at the same distance 'd' from the axis, in the opposite direction along the perpendicular.\n\nAnother way to think about the symmetric function is that if the shape were to be folded in half over the axis, the two halves would be identical: the two halves are each other's mirror images.\n\nThus a square has four axes of symmetry, because there are four different ways to fold it and have the edges all match. A circle has infinitely many axes of symmetry.\nTriangles with reflection symmetry are isosceles. Quadrilaterals with reflection symmetry are kites, (concave) deltoids, rhombuses, and isosceles trapezoids. All even-sided polygons have two simple reflective forms, one with lines of reflections through vertices, and one through edges.\n\nFor an arbitrary shape, the axiality of the shape measures how close it is to being bilaterally symmetric. It equals 1 for shapes with reflection symmetry, and between 2/3 and 1 for any convex shape.\n\nFor each line or plane of reflection, the symmetry group is isomorphic with \"C\" (see point groups in three dimensions), one of the three types of order two (involutions), hence algebraically \"C\". The fundamental domain is a half-plane or half-space.\n\nIn certain contexts there is rotational as well as reflection symmetry. Then mirror-image symmetry is equivalent to inversion symmetry; in such contexts in modern physics the term parity or P-symmetry is used for both.\n\nFor more general types of reflection there are correspondingly more general types of reflection symmetry. For example:\n\n\nAnimals that are bilaterally symmetric have reflection symmetry in the sagittal plane, which divides the body vertically into left and right halves, with one of each sense organ and limb pair on either side. Most animals are bilaterally symmetric, likely because this supports forward movement and streamlining.\n\nMirror symmetry is often used in architecture, as in the facade of Santa Maria Novella, Venice. It is also found in the design of ancient structures such as Stonehenge. Symmetry was a core element in some styles of architecture, such as Palladianism.\n\n\n\n"}
{"id": "1461209", "url": "https://en.wikipedia.org/wiki?curid=1461209", "title": "Riemann–Lebesgue lemma", "text": "Riemann–Lebesgue lemma\n\nIn mathematics, the Riemann–Lebesgue lemma, named after Bernhard Riemann and Henri Lebesgue, is of importance in harmonic analysis and asymptotic analysis.\n\nThe lemma says that the Fourier transform or Laplace transform of an \"L\" function vanishes at infinity.\n\nIf \"ƒ\" is \"L\" integrable on R, that is to say, if the Lebesgue integral of |\"ƒ\"| is finite, then the Fourier transform of \"ƒ\" satisfies\n\nFirst suppose that formula_2, the characteristic function of an open interval.\n\nThen:formula_3 as formula_4By additivity of limits, the same holds for an arbitrary simple function.\n\nThat is, for any function formula_5 of the form:formula_6We have that:formula_7\n\nIf \"ƒ\" is an arbitrary integrable function, it may be approximated in the \"L\" norm by a compactly supported smooth function \"g\". Pick such a \"g\" so that ||\"ƒ\" − \"g\"|| < \"ε\". Then\n\nand since this holds for any \"ε\" > 0, the theorem follows.\n\n"}
{"id": "9258361", "url": "https://en.wikipedia.org/wiki?curid=9258361", "title": "Ruppeiner geometry", "text": "Ruppeiner geometry\n\nRuppeiner geometry is thermodynamic geometry (a type of information geometry) using the language of Riemannian geometry to study thermodynamics. George Ruppeiner proposed it in 1979. He claimed that thermodynamic systems can be represented by Riemannian geometry, and that statistical properties can be derived from the model.\n\nThis geometrical model is based on the inclusion of the theory of fluctuations into the axioms of equilibrium thermodynamics, namely, there exist equilibrium states which can be represented by points on two-dimensional surface (manifold) and the distance between these equilibrium states is related to the fluctuation between them. This concept is associated to probabilities, i.e. the less probable a fluctuation between states, the further apart they are. This can be recognized if one considers the metric tensor g in the distance formula (line element) between the two equilibrium states\n\nwhere the matrix of coefficients \"g\" is the symmetric metric tensor which is called a Ruppeiner metric, defined as a negative Hessian of the entropy function\n\nwhere U is the internal energy (mass) of the system and N refers to the extensive parameters of the system. Mathematically, the Ruppeiner geometry is one particular type of information geometry and it is similar to the Fisher-Rao metric used in mathematical statistics.\n\nThe Ruppeiner metric can be understood as the thermodynamic limit (large systems limit) of the more general Fisher information metric. For small systems (systems where fluctuations are large), the Ruppeiner metric may not exist, as second derivatives of the entropy are not guaranteed to be non-negative.\n\nThe Ruppeiner metric is conformally related to the Weinhold metric via\n\nwhere T is the temperature of the system under consideration. Proof of the conformal relation can be easily done when one writes down the first law of thermodynamics (dU=TdS+...) in differential form with a few manipulations. The Weinhold geometry is also considered as a thermodynamic geometry. It is defined as a Hessian of the internal energy with respect to entropy and other extensive parameters.\n\nIt has long been observed that the Ruppeiner metric is flat for systems with noninteracting underlying statistical mechanics such as the ideal gas. Curvature singularities signal critical behaviors. In addition, it has been applied to a number of statistical systems including Van de Waals gas. Recently the anyon gas has been studied using this approach.\n\nIn the last five years or so, this geometry has been applied to black hole thermodynamics, with some physically relevant results. The most physically significant case is for the Kerr black hole in higher dimensions, where the curvature singularity signals thermodynamic instability, as found earlier by conventional methods.\n\nThe entropy of a black hole is given by the well-known Bekenstein-Hawking formula\n\nwhere formula_6 is Boltzmann's constant, formula_7 the speed of light, formula_8 Newton's constant and formula_9 is the area of the event horizon of the black hole. Calculating the Ruppeiner geometry of the black hole's entropy is, in principle, straightforward, but it is important that the entropy should be written in terms of extensive parameters, \nwhere formula_11 is ADM mass of the black hole and formula_12 are the conserved charges and formula_13 runs from 1 to n. The signature of the metric reflects the sign of the hole's specific heat. For a Reissner-Nordström black hole, the Ruppeiner metric has a Lorentzian signature which corresponds to the negative heat capacity it possess, while for the BTZ black hole, we have a Euclidean signature. This calculation cannot be done for the Schwarzschild black hole, because its entropy is \nwhich renders the metric degenerate.\n\n"}
{"id": "57577573", "url": "https://en.wikipedia.org/wiki?curid=57577573", "title": "Simon problems", "text": "Simon problems\n\nIn mathematics, the Simon problems (or Simon's problems) are a series of fifteen questions posed in the year 2000 by Barry Simon, an American mathematical physicist. Inspired by other collections of mathematical problems and open conjectures, such as the famous list by David Hilbert, the Simon problems concern quantum operators. In 2014, Artur Avila won a Fields Medal for work including the solution of three Simon problems. Among these was the problem of proving that the set of energy levels of one particular abstract quantum system was in fact the Cantor set, a challenge known as the \"Ten Martini Problem\" after the reward that Mark Kac offered for solving it. Eight of the problems pertain to anomalous spectral behavior of Schrödinger operators, and five concern operators that incorporate the Coulomb potential.\n\nThe 2000 list was a refinement of a similar set of problems that Simon had posed in 1984.\n\n"}
{"id": "19133673", "url": "https://en.wikipedia.org/wiki?curid=19133673", "title": "Simulations and games in economics education", "text": "Simulations and games in economics education\n\nA simulation game is \"a game that contains a mixture of skill, chance, and strategy to simulate an aspect of reality, such as a stock exchange\". Similarly, Finnish author Virpi Ruohomäki states that \"a simulation game combines the features of a game (competition, cooperation, rules, participants, roles) with those of a simulation (incorporation of critical features of reality). A game is a simulation game if its rules refer to an empirical model of reality.\" A properly built simulation game used to teach or learn economics would closely follow the assumptions and rules of the theoretical models within this discipline.\n\nEconomics education studies recommend the adoption of more active and collaborative learning methodologies (Greenlaw, 1999). Simkins (1999) stated \"… \"teaching practices, which rely heavily on the lecture format, are not doing enough to develop students' cognitive learning skills, attract good students to economics, and motivate them to continue coursework in the discipline.\"\" (p. 278). This is consistent with the results of a survey published in the American Economic Review by Allgood (2004) that shows that students \"rarely take economics as a free elective – especially beyond principles.\" (p. 5). More is needed to be done in the classroom to excite students about economics education.\n\nSimulations supplement the standard lecture. Both computerized and non-computer based simulation and games show significant levels of growth in education (see Lean, Moizer, Towler, and Abbey, 2006; Dobbins, Boehlje, Erickson and Taylor, 1995; Gentry, 1990;.\n\nThrough a simulation game, students may participate directly in a market by managing a simulated firm and making decisions on price and production to maximize profits. An excellent review of the use of a successful market simulation is given by Motahar (1994) in the Journal of Economics Education.\n\nA monopolistic competition simulation game can be used as an example in the standard economics classroom or for experimental economics. Economic experiments using monopolistic competition simulations can create real-world incentives that may be used in the teaching and learning of economics to help students better understand why markets and other exchange systems work the way they do. An explanation of experimental economics is given by Roth (1995).\nAssumptions of monopolistic competition\n\nA simulation game in monopolistic competition needs to incorporate the standard theoretical assumptions of this market structure, including: \n\nIn a simulation of monopolistic competition, each firm must be small in size, and should not be able to influence the direction of the overall market. Yet each firm has some control over price owing to product differentiation. To be consistent with economic theory, the simulation model should allow entry of new firms to occur as long as profits are greater than normal, and economic profits exist. The entry of new firms will decrease the market price, and eventually cause economic profits to return to zero (see Baye, 2009).\n\nControllable decisions in monopolistic competition\n\nTo simulate monopolistic competition, the controllable firm decisions of the participants (students) must include, at a minimum, those specified in the standard theoretical model, including (see Baye, 2009): \n\nSimulation game experience\n\nFrom an educational point of view, students will have an \"opportunity\" to learn by their own observations and experience through participation in a simulation game (see Schmidt, 2003). Consistent with the theoretical model of monopolistic competition (see Baye, 2009), student participants would observe and experience that their pricing decisions are controlled by the market. They would \"experience\" that in the simulation they would have to lower their firm's price to be competitive as new firms entered the market. In the long-run, they would see the impact of changing plant size. They would observe that the successful firms would take advantage of economies of scale, but would also be careful not to incur diseconomies of scale in the long-run. Students would experience that economic profits cannot be maintained in the long-run. They would see, first hand, that their accounting profits will inevitably decline and move closer to normal profits. This experience provides students an opportunity to learn (as a supplement to the lecture and readings) the economic messages of monopolistic competition.\n\nIn 2018, Harvard Business Publishing published \"Macroeconomics Simulation: Econland\". This 30-minute simulation brings economic policymaking to life by allowing students to make monetary and fiscal policy decisions and consider their impact on the economy of a fictional country. Students manage the economy through a 7-year business cycle in an effort to maximize the approval rating from their population. Exploring the trade-offs of economic policy decision-making and the effects of the global economic environment on a country, students consolidate their understanding of core macroeconomic concepts, including GDP, unemployment, inflation, and budget deficit. At a deeper level, students develop critical thinking skills and learn about economic modeling and system dynamics. The simulation won a Silver Medal at the International Serious Play Awards.\n\n\n\n"}
{"id": "13633935", "url": "https://en.wikipedia.org/wiki?curid=13633935", "title": "Steric number", "text": "Steric number\n\nThe steric number of a molecule is the number of atoms bonded to the central atom of a molecule (σ sigma bond) plus the number of lone pairs on the central atom. It is often used in VSEPR theory (valence shell electron-pair repulsion) in order to determine the particular shape, or molecular geometry, that will be formed.\n\nCalculating the steric number of a molecule's central atom is a vital step in predicting its geometry by VSEPR theory. On the molecule SF, for example, the central sulfur atom has four ligands about it, calculated by considering sulfur's coordination number. In addition to the four ligands, sulfur also has one remaining lone pair. Thus, the steric number is 5. The central atom's steric number together with the number of lone pairs allows anyone to predict the geometry of that central atom, using the table of molecular geometries for the VSEPR theory.\n\n"}
{"id": "2229292", "url": "https://en.wikipedia.org/wiki?curid=2229292", "title": "Stirling numbers of the second kind", "text": "Stirling numbers of the second kind\n\nIn mathematics, particularly in combinatorics, a Stirling number of the second kind (or Stirling partition number) is the number of ways to partition a set of \"n\" objects into \"k\" non-empty subsets and is denoted by formula_1 or formula_2. Stirling numbers of the second kind occur in the field of mathematics called combinatorics and the study of partitions.\n\nStirling numbers of the second kind are one of two kinds of Stirling numbers, the other kind being called Stirling numbers of the first kind (or Stirling cycle numbers). Mutually inverse (finite or infinite) triangular matrices can be formed from the Stirling numbers of each kind according to the parameters \"n\", \"k\".\n\nThe Stirling numbers of the second kind, written formula_1 or formula_4 or with other notations, count the number of ways to partition a set of formula_5 labelled objects into formula_6 nonempty unlabelled subsets. Equivalently, they count the number of different equivalence relations with precisely formula_6 equivalence classes that can be defined on an formula_5 element set. In fact, there is a bijection between the set of partitions and the set of equivalence relations on a given set. Obviously, \nas the only way to partition an \"n\"-element set into \"n\" parts is to put each element of the set into its own part, and the only way to partition a nonempty set into one part is to put all of the elements in the same part.\nThey can be calculated using the following explicit formula:\n\nThe Stirling numbers of the second kind may also be characterized as the numbers that arise when one expresses powers of an indeterminate \"x\" in terms of the falling factorials\n\n(In particular, (\"x\") = 1 because it is an empty product.) In particular, one has\n\nVarious notations have been used for Stirling numbers of the second kind. The brace notation formula_2 was used by Imanuel Marx and Antonio Salmeri in 1962 for variants of these numbers. This led Knuth to use it, as shown here, in the first volume of \"The Art of Computer Programming\" (1968). However, according to the third edition of \"The Art of Computer Programming\", this notation was also used earlier by Jovan Karamata in 1935. The notation \"S\"(\"n\", \"k\") was used by Richard Stanley in his book \"Enumerative Combinatorics\".\n\nSince the Stirling number formula_15 counts set partitions of an \"n\"-element set into \"k\" parts, the sum \n\nover all values of \"k\" is the total number of partitions of a set with \"n\" members. This number is known as the \"n\"th Bell number. \n\nAnalogously, the ordered Bell numbers can be computed from the Stirling numbers of the second kind via\n\nBelow is a triangular array of values for the Stirling numbers of the second kind :\nAs with the binomial coefficients, this table could be extended to , but those entries would all be 0.\n\nStirling numbers of the second kind obey the recurrence relation\n"}
{"id": "58688880", "url": "https://en.wikipedia.org/wiki?curid=58688880", "title": "Ulrica Wilson", "text": "Ulrica Wilson\n\nUlrica Wilson is a mathematician specializing in the theory of noncommutative rings and in the combinatorics of matrices. She is an associate professor at Morehouse College, associate director of diversity and outreach at the Institute for Computational and Experimental Research in Mathematics (ICERM), and vice president of the National Association of Mathematicians.\n\nWilson is African-American, and originally from Massachusetts, but grew up in Birmingham, Alabama.\nShe is a 1992 graduate of Spelman College,\nand completed her Ph.D. at Emory University in 2004. Her dissertation, \"Cyclicity of Division Algebras over an Arithmetically Nice Field\", was supervised by Eric Brussel.\n\nAfter two stints as a postdoctoral researcher, she joined the Morehouse College faculty in 2007, and became associate director at ICERM in 2013.\n\nWilson was the Morehouse College Vulcan Teaching Excellence Award winner for 2016–2017.\nIn 2018, she won the Presidential Award for Excellence in Science, Mathematics, and Engineering Mentoring.\nShe was included in the 2019 class of fellows of the Association for Women in Mathematics \"for her many years of supporting the professional development of women in their pursuit of graduate degrees in mathematics, most visibly through mentoring, teaching and program administration within the EDGE Program, and as associate director of diversity and outreach at ICERM\".\n"}
{"id": "2310296", "url": "https://en.wikipedia.org/wiki?curid=2310296", "title": "United States Army Training and Doctrine Command Analysis Center", "text": "United States Army Training and Doctrine Command Analysis Center\n\nThe United States Army Training and Doctrine Command Analysis Center (TRAC) is an analysis agency of the United States Army. TRAC conducts research on potential military operations worldwide to inform decisions about the most challenging issues facing the Army and the Department of Defense (DoD). TRAC relies upon the intellectual capital of a highly skilled workforce of military and civilian personnel to execute its mission. \n\nTRAC conducts operations research (OR) on a wide range of military topics, some contemporary but most often set 5 to 15 years in the future. How should Army units be organized? What new systems should be procured? How should soldiers and commanders be trained? What are the costs and benefits of competing options? What are the potential risks and rewards of a planned military course of action? TRAC directly supports the mission of the Army's Training and Doctrine Command (TRADOC), to develop future concepts and requirements while also serving the decision needs of many military clients.\n\nTo produce relevant and credible operations analysis to inform decisions. TRAC serves many clients and has many stakeholders, but has only one shareholder: the American Soldier.\n\nTRAC is led by a civilian SES director, subordinate to the Commanding General of the US Army Training and Doctrine Command, and comprises four centers:\n\nEach center director is subordinate to the TRAC director.\n\nTRAC headquarters has two components:\n\nThe discipline of operations research is built upon the collaboration of interdisciplinary team members who have mutually supporting knowledge, skills and experiences pertinent to the study problem. The TRAC building blocks of military operations analysis are future scenarios; leading edge models and simulations; realistic data about systems, forces, and behavior; and skilled operations analysts. Leading a core team of analysts, a TRAC Study Director may receive support from other TRADOC and Army agencies, and from other government agencies and industry as well. TRAC adheres to the proven principles of scientific inquiry and applies the problem solving model to perform its analysis.\n\nThe TRAC program of operations research and analysis is forward-looking and addresses a wide range of military topics. The analysis is conducted within a joint framework of combined arms operations across a full spectrum of missions and environments. TRAC leads TRADOC's major studies of new warfighting operations and organization (O&O) concepts and requirements. TRAC leads the Army's analysis of Advanced Warfighting Experiments (AWEs), and the Army's Analysis of Alternatives (AoA). The analysis topics span doctrine, training, leader development, organization, materiel, and soldier support.\n\nScenarios are used by the U.S. Army for education, training and force development. Director, TRAC is the TRADOC executive agent for development of scenarios for use in studies and analysis. TRAC develops scenarios of potential military operations set in the future for use in modeling and analysis. TRAC relies upon input and assistance from many Army and DoD agencies, other Services and the Combatant Commanders to develop and apply a family of scenarios depicting joint operations of corps and divisions, and brigades and battalions. The family of scenarios undergoes continual review and change in anticipation of emerging threats and new operational environments around the world based on intelligence estimates.\n\nMilitary operations are highly complex processes and typically must be modeled in order to be analyzed. The analytic tools may take the form of:\nTRAC develops and maintains a class of warfighting M&S referred to as force-on-force, ranging from individual objects (e.g., soldier, weapon, terrain feature) to aggregated objects (e.g., battalions) at corps level. TRAC M&S represent the Army's de facto standards for force-on-force M&S and are widely used by military, industry and allies. TRAC is a significant contributor to advanced M&S research and improved modeling methodologies in the military.\n\n"}
{"id": "39583234", "url": "https://en.wikipedia.org/wiki?curid=39583234", "title": "Étale homotopy type", "text": "Étale homotopy type\n\nIn mathematics, especially in algebraic geometry, the étale homotopy type is an analogue of the homotopy type of topological spaces for algebraic varieties.\n\nRoughly speaking, for a variety or scheme \"X\", the idea is to consider étale coverings formula_1 and to replace each connected component of \"U\" and the higher \"intersections\", i.e., fiber products, formula_2 (\"n\"+1 copies of \"U\", formula_3) by a single point. This gives a simplicial set which captures some information related to \"X\" and the étale topology of it.\n\nSlightly more precisely, it is in general necessary to work with étale hypercovers formula_4 instead of the above simplicial scheme determined by a usual étale cover. Taking finer and finer hypercoverings (which is technically accomplished by working with the pro-object in simplicial sets determined by taking all hypercoverings), the resulting object is the étale homotopy type of \"X\". Similarly to classical topology, it is able to recover much of the usual data related to the étale topology, in particular the étale fundamental group of the scheme and the étale cohomology of locally constant étale sheaves.\n\n\n"}
