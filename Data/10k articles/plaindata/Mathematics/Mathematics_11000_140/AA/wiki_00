{"id": "2992000", "url": "https://en.wikipedia.org/wiki?curid=2992000", "title": "269 (number)", "text": "269 (number)\n\n269 (two hundred [and] sixty-nine) is the natural number between 268 and 270. It is also a prime number.\n\n269 is a twin prime,\nand a Ramanujan prime.\nIt is the largest prime factor of 9! + 1 = 362881,\nand the smallest natural number that cannot be represented as the determinant of a 10 × 10 (0,1)-matrix.\n"}
{"id": "5827091", "url": "https://en.wikipedia.org/wiki?curid=5827091", "title": "Alexander Ostrowski", "text": "Alexander Ostrowski\n\nAlexander Markowich Ostrowski (; ; 25 September 1893, in Kiev, Russian Empire – 20 November 1986, in Montagnola, Lugano, Switzerland) was a mathematician.\n\nHis father Mark having been a merchant, Alexander Ostrowski attended the Kiev College of Commerce, not a high school, and thus had an insufficient qualification to be admitted to university. However, his talent did not remain undetected: Ostrowski's mentor, Dmitry Grave, wrote to Landau and Hensel for help.\n\nSubsequently, Ostrowski began to study mathematics at Marburg University under Hensel's supervision in 1912. During the World War I he was interned, but thanks to the intervention of Hensel, the restrictions on his movements were eased somewhat, and he was allowed to use the university library.\n\nAfter the war had ended Ostrowski moved to Göttingen where he wrote his doctoral dissertation and was influenced by Hilbert, Klein and Landau. In 1920, after having obtained his doctorate, Ostrowski moved to Hamburg where he worked as Hecke's assistant and finished his habilitation in 1922. In 1923 he returned to Göttingen, and in 1928 became Professor of Mathematics at Basel, until retirement in 1958. In 1950 Ostrowski obtained Swiss citizenship. After retirement he still published scientific papers until his late eighties.\n\n\n"}
{"id": "31251396", "url": "https://en.wikipedia.org/wiki?curid=31251396", "title": "Amorphous set", "text": "Amorphous set\n\nIn set theory, an amorphous set is an infinite set which is not the disjoint union of two infinite subsets.\n\nAmorphous sets cannot exist if the axiom of choice is assumed. Fraenkel constructed a permutation model of Zermelo–Fraenkel with Atoms in which the set of atoms is an amorphous set. After Cohen's initial work on forcing in 1963, proofs of the consistency of amorphous sets with Zermelo–Fraenkel were obtained.\n\nEvery amorphous set is Dedekind-finite, meaning that it has no bijection to a proper subset of itself. To see this, suppose that \"S\" is a set that does have a bijection \"f\" to a proper subset. For each \"i\" ≥ 0\ndefine \"S\" to be the set of elements that belong to the image of the \"i\"-fold composition of \"f\" with itself but not to the image of the (\"i\" + 1)-fold composition.\nThen each \"S\" is non-empty, so the union of the sets \"S\" with even indices would be an infinite set whose complement is also infinite, showing that \"S\" cannot be amorphous. However, the converse is not necessarily true: it is consistent for there to exist Dedekind-finite sets that are not amorphous.\n\nNo amorphous set can be linearly ordered. Because the image of an amorphous set is itself either amorphous or finite, it follows that every function from an amorphous set to a linearly ordered set has only a finite image.\n\nThe cofinite filter on an amorphous set is an ultrafilter. This is because the complement of each infinite subset must not be infinite, so every subset is either finite or cofinite.\n\nIf π is a partition of an amorphous set into finite subsets, then there must be exactly one integer \"n\"(π) such that π has infinitely many subsets of size \"n\"; for, if every size was used finitely many times, or if more than one size was used infinitely many times, this information could be used to coarsen the partition and split π into two infinite subsets. If an amorphous set has the additional property that, for every partition π, \"n\"(π) = 1, then it is called strictly amorphous or strongly amorphous, and if there is a finite upper bound on \"n\"(π) then the set is called bounded amorphous. It is consistent with ZF that amorphous sets exist and are all bounded, or that they exist and are all unbounded.\n"}
{"id": "612", "url": "https://en.wikipedia.org/wiki?curid=612", "title": "Arithmetic mean", "text": "Arithmetic mean\n\nIn mathematics and statistics, the arithmetic mean (, stress on third syllable of \"arithmetic\"), or simply the mean or average when the context is clear, is the sum of a collection of numbers divided by the count of numbers in the collection. The collection is often a set of results of an experiment or an observational study, or frequently a set of results from a survey. The term \"arithmetic mean\" is preferred in some contexts in mathematics and statistics because it helps distinguish it from other means, such as the geometric mean and the harmonic mean. \n\nIn addition to mathematics and statistics, the arithmetic mean is used frequently in many diverse fields such as economics, anthropology, and history, and it is used in almost every academic field to some extent. For example, per capita income is the arithmetic average income of a nation's population.\n\nWhile the arithmetic mean is often used to report central tendencies, it is not a robust statistic, meaning that it is greatly influenced by outliers (values that are very much larger or smaller than most of the values). Notably, for skewed distributions, such as the distribution of income for which a few people's incomes are substantially greater than most people's, the arithmetic mean may not coincide with one's notion of \"middle\", and robust statistics, such as the median, may be a better description of central tendency.\n\nThe arithmetic mean (or mean or average), formula_1 (read formula_2 \"bar\"), is the mean of the formula_3 values formula_4.\n\nThe arithmetic mean is the most commonly used and readily understood measure of central tendency in a data set. In statistics, the term average refers to any of the measures of central tendency. The arithmetic mean of a set of observed data is defined as being equal to the sum of the numerical values of each and every observation divided by the total number of observations. Symbolically, if we have a data set consisting of the values formula_5, then the arithmetic mean formula_6 is defined by the formula:\n\n(See summation for an explanation of the summation operator).\n\nFor example, let us consider the monthly salary of 10 employees of a firm: 2500, 2700, 2400, 2300, 2550, 2650, 2750, 2450, 2600, 2400. The arithmetic mean is\n\nIf the data set is a statistical population (i.e., consists of every possible observation and not just a subset of them), then the mean of that population is called the population mean. If the data set is a statistical sample (a subset of the population), we call the statistic resulting from this calculation a sample mean.\n\nThe arithmetic mean has several properties that make it useful, especially as a measure of central tendency. These include:\n\n\nThe arithmetic mean may be contrasted with the median. The median is defined such that no more than half the values are larger than, and no more than half are smaller than, the median. If elements in the data increase arithmetically, when placed in some order, then the median and arithmetic average are equal. For example, consider the data sample formula_15. The average is formula_16, as is the median. However, when we consider a sample that cannot be arranged so as to increase arithmetically, such as formula_17, the median and arithmetic average can differ significantly. In this case, the arithmetic average is 6.2 and the median is 4. In general, the average value can vary significantly from most values in the sample, and can be larger or smaller than most of them.\n\nThere are applications of this phenomenon in many fields. For example, since the 1980s, the median income in the United States has increased more slowly than the arithmetic average of income.\n\nA weighted average, or weighted mean, is an average in which some data points count more heavily than others, in that they are given more weight in the calculation. For example, the arithmetic mean of formula_18 and formula_19 is formula_20, or equivalently formula_21. In contrast, a \"weighted\" mean in which the first number receives, for example, twice as much weight as the second (perhaps because it is assumed to appear twice as often in the general population from which these numbers were sampled) would be calculated as formula_22. Here the weights, which necessarily sum to the value one, are formula_23 and formula_24, the former being twice the latter. Note that the arithmetic mean (sometimes called the \"unweighted average\" or \"equally weighted average\") can be interpreted as a special case of a weighted average in which all the weights are equal to each other (equal to formula_25 in the above example, and equal to formula_26 in a situation with formula_3 numbers being averaged).\n\nIf a numerical property, and any sample of data from it, could take on any value from a continuous range, instead of, for example, just integers, then the probability of a number falling into some range of possible values can be described by integrating a continuous probability distribution across this range, even when the naive probability for a sample number taking one certain value from infinitely many is zero. The analog of a weighted average in this context, in which there are an infinite number of possibilities for the precise value of the variable in each range, is called the \"mean of the probability distribution\". A most widely encountered probability distribution is called the normal distribution; it has the property that all measures of its central tendency, including not just the mean but also the aforementioned median and the mode (the three M's), are equal to each other. This equality does not hold for other probability distributions, as illustrated for the lognormal distribution here.\n\nParticular care must be taken when using cyclic data, such as phases or angles. Naïvely taking the arithmetic mean of 1° and 359° yields a result of 180°.\nThis is incorrect for two reasons:\n\nIn general application, such an oversight will lead to the average value artificially moving towards the middle of the numerical range. A solution to this problem is to use the optimization formulation (viz., define the mean as the central point: the point about which one has the lowest dispersion), and redefine the difference as a modular distance (i.e., the distance on the circle: so the modular distance between 1° and 359° is 2°, not 358°).\n\nThe arithmetic mean is often denoted by a bar, for example as in formula_1 (read formula_2 \"bar\").\n\nSome software (text processors, web browsers) may not display the x̄ symbol properly. For example, the x̄ symbol in HTML is actually a combination of two codes - the base letter x plus a code for the line above (&#772: or ¯).\n\nIn some texts, such as pdfs, the x̄ symbol may be replaced by a cent (¢) symbol (Unicode &#162) when copied to text processor such as Microsoft Word.\n\n"}
{"id": "7719010", "url": "https://en.wikipedia.org/wiki?curid=7719010", "title": "Armitage–Doll multistage model of carcinogenesis", "text": "Armitage–Doll multistage model of carcinogenesis\n\nThe Armitage–Doll model is a statistical model of carcinogenesis, proposed in 1954 by Peter Armitage and Richard Doll, which suggested that a sequence of multiple distinct genetic events preceded the onset of cancer. The original paper has recently been reprinted with a set of commentary articles.\n\n"}
{"id": "1266713", "url": "https://en.wikipedia.org/wiki?curid=1266713", "title": "Cayley's formula", "text": "Cayley's formula\n\nIn mathematics, Cayley's formula is a result in graph theory named after Arthur Cayley. It states that for every positive integer \"n\", the number of trees on \"n\" labeled vertices is formula_1.\n\nThe formula equivalently counts the number of spanning trees of a complete graph with labeled vertices\n\nMany proofs of Cayley's tree formula are known.\nOne classical proof of the formula uses Kirchhoff's matrix tree theorem, a formula for the number of spanning trees in an arbitrary graph involving the determinant of a matrix. Prüfer sequences yield a bijective proof of Cayley's formula. Another bijective proof, by André Joyal, finds a one-to-one transformation between \"n\"-node trees with two distinguished nodes and maximal directed pseudoforests.\nA proof by double counting due to Jim Pitman counts in two different ways the number of different sequences of directed edges that can be added to an empty graph on n vertices to form from it a rooted tree; see Double counting (proof technique)#Counting trees.\n\nThe formula was first discovered by Carl Wilhelm Borchardt in 1860, and proved via a determinant. In a short 1889 note, Cayley extended the formula in several directions, by taking into account the degrees of the vertices. Although he referred to Borchardt's original paper, the name \"Cayley's formula\" became standard in the field.\n\nCayley's formula immediately gives the number of labelled rooted forests on \"n\" vertices, namely (\"n\" + 1).\nEach labelled rooted forest can be turned into a labelled tree with one extra vertex, by adding a vertex with label \"n\" + 1 and connecting it to all roots of the trees in the forest.\n\nThere is a close connection with rooted forests and parking functions, since the number of parking functions on \"n\" cars is also (\"n\" + 1). A bijection between rooted forests and parking functions was given by M. P. Schützenberger in 1968.\n\nThe following generalizes Cayley's formula to labelled forests:\nLet \"T\" be the number of labelled forests on \"n\" vertices,\nsuch that vertices 1, 2, ..., \"k\" all belong to different trees.\nThen .\n"}
{"id": "32212762", "url": "https://en.wikipedia.org/wiki?curid=32212762", "title": "Critical pair (order theory)", "text": "Critical pair (order theory)\n\nIn order theory, a discipline within mathematics, a critical pair is a pair of elements in a partially ordered set that are incomparable but that could be made comparable without requiring any other changes to the partial order.\n\nFormally, let be a partially ordered set. Then a critical pair is an ordered pair of elements of with the following three properties:\n\nIf is a critical pair, then the binary relation obtained from by adding the single relationship is also a partial order. The properties required of critical pairs ensure that, when the relationship is added, the addition does not cause any violations of the transitive property.\n\nA set of linear extensions of is said to \"reverse\" a critical pair in if there exists a linear extension in for which occurs earlier than . This property may be used to characterize realizers of finite partial orders: A nonempty set of linear extensions is a realizer if and only if it reverses every critical pair.\n"}
{"id": "7223383", "url": "https://en.wikipedia.org/wiki?curid=7223383", "title": "Cryptographic Module Testing Laboratory", "text": "Cryptographic Module Testing Laboratory\n\nA Cryptographic Module Testing Laboratory (CMTL) is an information technology (IT) computer security testing laboratory that is accredited to conduct cryptographic module evaluations for conformance to the FIPS 140-2 U.S. Government standard.\n\nThe National Institute of Standards and Technology (NIST) National Voluntary Laboratory Accreditation Program (NVLAP) accredits CMTLs to meet Cryptographic Module Validation Program (CMVP) standards and procedures.\n\nThis has been replaced by FIPS 140-2 and the Cryptographic Module Validation Program (CMVP).\n\nThese laboratories must meet the following requirements:\n\n\nA CMTL can also be a Common Criteria (CC) Testing Laboratory (CCTL). The CC and FIPS 140-2 are different in the abstractness and focus of tests. FIPS 140-2 testing is against a defined cryptographic module and provides a suite of conformance tests to four FIPS 140 security levels. FIPS 140-2 describes the requirements for cryptographic modules and includes such areas as physical security, key management, self tests, roles and services, etc. The standard was initially developed in 1994 - prior to the development of the CC. The CC is an evaluation against a Protection Profile (PP), usually created by the user, or security target (ST). Typically, a PP covers a broad range of products.\n\n\nIf the operational environment is a modifiable operational environment, the operating system requirements of the Common Criteria are applicable at FIPS Security Levels 2 and above.\n\n\n"}
{"id": "4720804", "url": "https://en.wikipedia.org/wiki?curid=4720804", "title": "Cycle index", "text": "Cycle index\n\nIn combinatorial mathematics a cycle index is a polynomial in several variables which is structured in such a way that information about how a group of permutations acts on a set can be simply read off from the coefficients and exponents. This compact way of storing information in an algebraic form is frequently used in combinatorial enumeration.\n\nEach permutation π of a finite set of objects partitions that set into cycles; the cycle index monomial of π is a monomial in variables \"a\", \"a\", … that describes the type of this partition (the cycle type of π): the exponent of \"a\" is the number of cycles of π of size \"i\". The cycle index polynomial of a permutation group is the average of the cycle index monomials of its elements. The phrase cycle indicator is also sometimes used in place of \"cycle index\".\n\nKnowing the cycle index polynomial of a permutation group, one can enumerate equivalence classes due to the group's action. This is the main ingredient in the Pólya enumeration theorem. Performing formal algebraic and differential operations on these polynomials and then interpreting the results combinatorially lies at the core of species theory.\n\nLet \"X\" be a set. A bijective map from \"X\" onto itself is called a permutation and the set of all permutations of \"X\" forms a group under the composition of mappings, called the Symmetric group of \"X\", Sym(\"X\"). Every subgroup of Sym(\"X\") is called a permutation group of \"degree\" |\"X\"|. Let \"G\" be an abstract group with a group homomorphism,φ, from \"G\" into Sym(\"X\"). The image, φ(\"G\"), is a permutation group. The group homomorphism can be thought of as a means for permitting the group \"G\" to \"act\" on the set \"X\" (using the permutations associated with the elements of \"G\"). Such a group homomorphism is formally called a group action and the image of the homomorphism is a \"permutation representation\" of \"G\". A given group can have many different permutation representations, corresponding to different actions.\n\nSuppose that group \"G\" \"acts\" on set \"X\" (that is, a group action exists). In combinatorial applications the interest is in the set \"X\"; for instance, counting things in \"X\" and knowing what structures might be left invariant by \"G\". Little is lost by working with permutation groups in such a setting, so in these applications, when a group is considered, it is a permutation representation of the group which will be worked with, and thus, a group action must be specified. Algebraists, on the other hand, are more interested in the groups themselves and would be more concerned with the kernels of the group actions, which measure how much is lost in passing from the group to its permutation representation.\n\nFinite permutations are most often represented as group actions on the set \"X\" = {1,2, ..., n}. A permutation in this setting can be represented by a two line notation. Thus,\ncorresponds to a bijection on \"X\" = {1, 2, 3, 4, 5} which sends 1 → 2, 2 → 3, 3 → 4, 4 → 5 and 5 → 1. This can be read off from the columns of the notation. When the top row is understood to be the elements of \"X\" in an appropriate order, only the second row need be written. In this one line notation, our example would be [2 3 4 5 1]. This example is known as a \"cyclic permutation\" because it \"cycles\" the numbers around and a third notation for it would be (1 2 3 4 5). This \"cycle notation\" is to be read as: each element is sent to the element on its right, but the last element is sent to the first one (it \"cycles\" to the beginning). With cycle notation, it does not matter where a cycle starts, so (1 2 3 4 5) and (3 4 5 1 2) and (5 1 2 3 4) all represent the same permutation. The \"length of a cycle\" is the number of elements in the cycle.\n\nNot all permutations are cyclic permutations, but every permutation can be written as a product of disjoint (having no common element) cycles in essentially one way. As a permutation may have \"fixed points\" (elements that are unchanged by the permutation), these will be represented by cycles of length one. For example:\nThis permutation is the product of three cycles, one of length two, one of length three and a fixed point. The elements in these cycles are disjoint subsets of \"X\" and form a partition of \"X\".\n\nThe cycle structure of a permutation can be coded as an algebraic monomial in several (dummy) variables in the following way: a variable is needed for each distinct cycle length of the cycles that appear in the cycle decomposition of the permutation. In the previous example there were three different cycle lengths, so we will use three variables, \"a\", \"a\" and \"a\" (in general, use the variable \"a\" to correspond to length \"k\" cycles). The variable \"a\" will be raised to the \"j\"(\"g\") power where \"j\"(\"g\") is the number of cycles of length \"i\" in the cycle decomposition of permutation \"g\". We can then associate the \"cycle index monomial\"\nto the permutation \"g\". The cycle index monomial of our example would be \"a\"\"a\"\"a\", while the cycle index monomial of the permutation (1 2)(3 4)(5)(6 7 8 9)(10 11 12 13)(14)(15) would be \"a\"\"a\"\"a\".\n\nThe cycle index of a permutation group \"G\" is the average of the cycle index monomials of all the permutations \"g\" in \"G\".\n\nMore formally, let \"G\" be a permutation group of order \"m\" and degree \"n\".\nEvery permutation \"g\" in \"G\" has a unique decomposition into disjoint cycles, say\n\"c\" \"c\" \"c\" ... .\nLet the length of a cycle \"c\" be denoted by |\"c\"|.\n\nNow let \"j\"(g) be the number of cycles of \"g\" of length \"k\", where\n\nWe associate to \"g\" the monomial\nin the variables \"a\", \"a\", ..., \"a\".\n\nThen the cycle index \"Z\"(\"G\") of \"G\" is given by\n\nConsider the group \"G\" of rotational symmetries of a square in the Euclidean plane. Such symmetries are completely determined by the images of just the corners of the square. By labeling these corners 1, 2, 3 and 4 (consecutively going clockwise) we can represent the elements of \"G\" as permutations of the set \"X\" = {1,2,3,4}. The permutation representation of \"G\" consists of the four permutations (1 4 3 2), (1 3)(2 4), (1 2 3 4) and e = (1)(2)(3)(4) which represent the counter-clockwise rotations by 90°, 180°, 270° and 360° respectively. Notice that the identity permutation e is the only permutation with fixed points in this representation of \"G\". As an abstract group, \"G\" is known as the cyclic group \"C\", and this permutation representation of it is its \"regular representation\". The cycle index monomials are \"a\", \"a\", \"a\", and \"a\" respectively. Thus, the cycle index of this permutation group is:\n\nThe group \"C\" also acts on the unordered pairs of elements of \"X\" in a natural way. Any permutation \"g\" would send {\"x\",\"y\"} → {\"x\", \"y\"} (where \"x\" is the image of the element \"x\" under the permutation \"g\"). The set \"X\" is now {\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"} where \"A\" = {1,2}, \"B\" = {2,3}, \"C\" = {3,4}, \"D\" = {1,4}, \"E\" = {1,3} and \"F\" = {2,4}. These elements can be thought of as the sides and diagonals of the square or, in a completely different setting, as the edges of the complete graph \"K\". Acting on this new set, the four group elements are now represented by (\"A\" \"D\" \"C\" \"B\")(\"E\" \"F\"), (\"A C\")(\"B D\")(\"E\")(\"F\"), (\"A B C D\")(\"E F\") and e = (\"A\")(\"B\")(\"C\")(\"D\")(\"E\")(\"F\") and the cycle index of this action is:\n\nThe group \"C\" can also act on the ordered pairs of elements of \"X\" in the same natural way. Any permutation \"g\" would send (\"x\",\"y\") → (\"x\", \"y\") (in this case we would also have ordered pairs of the form (\"x\", \"x\")). The elements of \"X\" could be thought of as the arcs of the complete digraph \"D\" (with loops at each vertex). The cycle index in this case would be:\n\nAs the above example shows, the cycle index depends on the group action and not on the abstract group. Since there are many permutation representations of an abstract group, it is useful to have some terminology to distinguish them.\n\nWhen an abstract group is defined in terms of permutations, it is a permutation group and the group action is the identity homomorphism. This is referred to as the \"natural action\".\n\nThe symmetric group \"S\" in its natural action has the elements\nand so, its cycle index is:\n\nA permutation group \"G\" on the set \"X\" is \"transitive\" if for every pair of elements \"x\" and \"y\" in \"X\" there is at least one \"g\" in \"G\" such that \"y\" = \"x\". A transitive permutation group is \"regular\" (or sometimes referred to as \"sharply transitive\") if the only permutation in the group that has fixed points is the identity permutation.\n\nA finite transitive permutation group \"G\" on the set \"X\" is regular if and only if |\"G\"| = |\"X\"|. Cayley's theorem states that every abstract group has a regular permutation representation given by the group acting on itself (as a set) by (right) multiplication. This is called the \"regular representation\" of the group.\n\nThe cyclic group \"C\" in its regular representation contains the six permutations (one-line form of the permutation is given first):\n\nThus its cycle index is:\n\nOften, when an author does not wish to use the group action terminology, the permutation group involved is given a name which implies what the action is. The following three examples illustrate this point.\n\nWe will identify the complete graph \"K\" with an equilateral triangle in the Euclidean plane. This permits us to use geometric language to describe the permutations involved as symmetries of the equilateral triangle. Every permutation in the group \"S\" of \"vertex permutations\" (\"S\" in its natural action, given above) induces an edge permutation. These are the permutations:\n\nThe cycle index of the group \"G\" of edge permutations induced by vertex permutations from \"S\" is\n\nIt happens that the complete graph \"K\" is isomorphic to its own line graph (vertex-edge dual) and hence the edge permutation group induced by the vertex permutation group is the same as the vertex permutation group, namely \"S\" and the cycle index is \"Z\"(\"S\"). This is not the case for complete graphs on more than three vertices, since these have strictly more edges (formula_17) than vertices (\"n\").\n\nThis is entirely analogous to the three-vertex case. These are the vertex permutations (\"S\" in its natural action) and the edge permutations (\"S\" acting on unordered pairs) that they induce:\n\n\nWe may visualize the types of permutations geometrically as symmetries of a regular tetrahedron. This yields the following description of the permutation types.\n\nThe cycle index of the edge permutation group \"G\" of \"K\" is:\n\nConsider an ordinary cube in three-space and its group of symmetries (automorphisms), call it \"C\". It permutes the six faces of the cube.\nThere are twenty-four automorphisms.\n\n\nThe conclusion is that the cycle index of the group \"C\" is\n\nThis group contains one permutation that fixes every element (this must be a natural action).\n\nA Cyclic group, \"C\" is the group of rotations of a regular \"n\"-gon, that is, \"n\" elements equally spaced around a circle. This group has φ(\"d\") elements of order \"d\" for each divisor \"d\" of \"n\", where φ(\"d\") is the Euler φ-function, giving the number of natural numbers less than \"d\" which are relatively prime to \"d\". In the regular representation of \"C\", a permutation of order \"d\" has \"n\"/\"d\" cycles of length \"d\", thus:\n\nThe Dihedral group is like the cyclic group, but also includes reflections. In its natural action,\n\nThe cycle index of the alternating group in its natural action as a permutation group is\n\nThe numerator is 2 for the even permutations, and 0 for the odd permutations. The 2 is needed because\nformula_34.\n\nThe cycle index of the symmetric group \"S\" in its natural action is given by the formula:\nthat can be also stated in terms of complete Bell polynomials:\n\nThis formula is obtained by counting how many times a given permutation shape can occur. There are three steps: first partition the set of \"n\" labels into subsets, where there are formula_37 subsets of size \"k\". Every such subset generates formula_38 cycles of length \"k\". But we do not distinguish between cycles of the same size, i.e. they are permuted by formula_39. This yields\n\nThere is a useful recursive formula for the cycle index of the symmetric group.\nSet formula_41 and consider the size \"l\" of the cycle that contains \"n\",\nwhere formula_42\nThere are formula_43 ways to choose the remaining formula_44 elements of the cycle and every such choice generates\nformula_45 different cycles.\n\nThis yields the recurrence\nor\n\nThroughout this section we will modify the notation for cycle indices slightly by explicitly including the names of the variables. Thus, for the permutation group \"G\" we will now write:\n\nLet \"G\" be a group acting on the set \"X\". \"G\" also induces an action on the \"k\"-subsets of \"X\" and on the \"k\"-tuples of distinct elements of \"X\" (see #Example for the case \"k\" = 2), for 1 ≤ \"k\" ≤ \"n\". Let \"f\" and \"F\" denote the number of orbits of \"G\" in these actions respectively. By convention we set \"f\" = \"F\" = 1. We have:\n\na) The ordinary generating function for \"f\" is given by:\nb) The exponential generating function for \"F\" is given by:\n\nLet \"G\" be a group acting on the set \"X\" and \"h\" a function from \"X\" to \"Y\". For any \"g\" in \"G\", \"h\"(\"x\") is also a function from \"X\" to \"Y\". Thus, \"G\" induces an action on the set \"Y\" of all functions from \"X\" to \"Y\". The number of orbits of this action is Z(\"G\"; \"b\", \"b\", ...,\"b\") where \"b\" = |\"Y\"|.\n\nThis result follows from the orbit counting lemma (also known as the Not Burnside's lemma, but traditionally called Burnside's lemma) and the weighted version of the result is Pólya's enumeration theorem.\n\nThe cycle index is a polynomial in several variables and the above results show that certain evaluations of this polynomial give combinatorially significant results. As polynomials they may also be formally added, subtracted, differentiated and integrated. The area of symbolic combinatorics provides combinatorial interpretations of the results of these formal operations.\n\nThe question of what the cycle structure of a random permutation looks like is an important question in the analysis of algorithms. An overview of the most important results may be found at random permutation statistics.\n\n\n"}
{"id": "6833695", "url": "https://en.wikipedia.org/wiki?curid=6833695", "title": "Demihypercube", "text": "Demihypercube\n\nIn geometry, demihypercubes (also called \"n-demicubes\", \"n-hemicubes\", and \"half measure polytopes\") are a class of n-polytopes constructed from alternation of an n-hypercube, labeled as \"hγ\" for being \"half\" of the hypercube family, \"γ\". Half of the vertices are deleted and new facets are formed. The \"2n\" facets become \"2n\" (n-1)-demicubes, and 2 (n-1)-simplex facets are formed in place of the deleted vertices.\n\nThey have been named with a \"demi-\" prefix to each hypercube name: demicube, demitesseract, etc. The demicube is identical to the regular tetrahedron, and the demitesseract is identical to the regular 16-cell. The demipenteract is considered \"semiregular\" for having only regular facets. Higher forms don't have all regular facets but are all uniform polytopes.\n\nThe vertices and edges of a demihypercube form two copies of the halved cube graph.\n\nThorold Gosset described the demipenteract in his 1900 publication listing all of the regular and semiregular figures in n-dimensions above 3. He called it a \"5-ic semi-regular\". It also exists within the semiregular k polytope family.\n\nThe demihypercubes can be represented by extended Schläfli symbols of the form h{4,3...,3} as half the vertices of {4,3...,3}. The vertex figures of demihypercubes are rectified n-simplexes.\n\nThey are represented by Coxeter-Dynkin diagrams of three constructive forms: \n\nH.S.M. Coxeter also labeled the third bifurcating diagrams as 1 representing the lengths of the 3 branches and led by the ringed branch.\n\nAn \"n-demicube\", \"n\" greater than 2, has \"n*(n-1)/2\" edges meeting at each vertex. The graphs below show less edges at each vertex due to overlapping edges in the symmetry projection.\n\nIn general, a demicube's elements can be determined from the original n-cube: (With C = \"m\"-face count in n-cube = 2*n!/(m!*(n-m)!))\n\nThe symmetry group of the demihypercube is the Coxeter group formula_1 [3] has order formula_2 and is an index 2 subgroup of the hyperoctahedral group (which is the Coxeter group formula_3 [4,3]). It is generated by permutations of the coordinate axes and reflections along \"pairs\" of coordinate axes.\n\nConstructions as alternated orthotopes have the same topology, but can be stretched with different lengths in \"n\"-axes of symmetry.\n\nThe rhombic disphenoid is the three-dimensional example as alternated cuboid. It has three sets of edge lengths, and scalene triangle faces.\n\n\n"}
{"id": "48901154", "url": "https://en.wikipedia.org/wiki?curid=48901154", "title": "Double Fourier sphere method", "text": "Double Fourier sphere method\n\nIn mathematics, the double Fourier sphere (DFS) method is a simple technique that transforms a function defined on the surface of the sphere to a function defined on a rectangular domain while preserving periodicity in both the longitude and latitude directions.\n\nFirst, a function formula_1 on the sphere is written as formula_2 using spherical coordinates, i.e.,\n\nThe function formula_4 is formula_5-periodic in formula_6, but not periodic in formula_7. The periodicity in the latitude direction has been lost. To recover it, the function is \"doubled up” and a related function on formula_8 is defined as\n\nwhere formula_10 and formula_11 for formula_12. The new function formula_13 is formula_5-periodic in formula_6 and formula_7, and is constant along the lines formula_17 and formula_18, corresponding to the poles.\n\nThe function formula_13 can be expanded into a double Fourier series\n\nThe DFS method was proposed by Merilees and developed further by Steven Orszag. The DFS method has been the subject of relatively few investigations since (a notable exception is Fornberg's work), perhaps due to the dominance of spherical harmonics expansions. Over the last fifteen years it has begun to be used for the computation of gravitational fields near black holes and to novel space-time spectral analysis.\n"}
{"id": "22931897", "url": "https://en.wikipedia.org/wiki?curid=22931897", "title": "Eri Jabotinsky", "text": "Eri Jabotinsky\n\nEri Jabotinsky (, also transliterated Ari, – ) was a Revisionist Zionist activist, Israeli politician and academic mathematician. He was the son of Ze'ev Jabotinsky, the founder of the opposition movement within Zionism at the time, and later served in the Knesset between 1949 and 1951, as a member of the opposition Herut party of Menachem Begin. Following his break with the party, he pursued his academic career.\n\nBorn in Odessa, the Jabotinsky family made \"aliyah\" to Mandatory Palestine in 1919. Following the arrest of his father the following year, he left the country and moved to France, where he attended high school in Paris, and later earned a degree in electrical engineering. Between 1933 and 1935 he worked as an engineer in an aircraft factory. In 1935 he returned to Palestine, and worked as an engineer at the Naharayim power station in the Jordan Valley.\n\nA long-term member of Revisionism's Betar youth movement, he became one of its representative leaders in 1936, and joined its worldwide board two years later. With Betar and the party's military wing, Irgun, he helped coordinate illegal Jewish immigration into Palestine; he was temporarily arrested by the British authorities in 1940, and upon his release he moved to the United States, where his father died suddenly about the same time. There, together with Hillel Kook's 'Bergson Boys', Aryeh Ben-Eliezer, Shmuel Merlin and Yitzhak Ben-Ami, they founded the Emergency Committee to Save European Jewry, among others. He briefly returned to Palestine, but was again arrested by the British and expelled for illegal activities in 1944.\n\nFollowing Israel's independence, he returned in 1948, and was elected to the first Knesset the following year, as a member of the Herut party's list of candidates. On 20 February 1951 however, Jabotinsky and Hillel Kook left Herut, and sat as independents for the rest of the term, although the move was not recognised by the house committee; this followed on-going disagreements over the party’s direction and its new leadership by Menachem Begin.\n\nAfter leaving the Knesset, he was awarded a PhD in mathematics in 1957 from the Hebrew University of Jerusalem. He also lectured on electricity theory at the Technion between 1955 and his death in 1969.\n\nHe was the father-in-law of Anatoly Rubin\n\n"}
{"id": "54463708", "url": "https://en.wikipedia.org/wiki?curid=54463708", "title": "Ethash", "text": "Ethash\n\nEthash is the proof-of-work function in Ethereum-based blockchain currencies. It uses Keccak, a hash function eventually standardized to SHA-3. These two are different, and should not be confused. Since version 1.0, Ethash has been designed to be ASIC-resistant via memory-hardness (harder to implement in special ASIC chips) and easily verifiable. It also uses a slightly modified version of earlier Dagger and Hashimoto hashes to remove computational overhead. Previously referred to as Dagger-Hashimoto, the Ethash function has evolved over time. Ethash uses an initial 1 GB dataset known as the Ethash DAG and a 16 MB cache for light clients to hold. These are regenerated every 30,000 blocks, known as an epoch. Miners grab slices of the DAG to generate mix-hashes using transaction and receipt data, along with a cryptographic nonce to generate a hash below a dynamic target difficulty.\n\nIn April 2018, the first ASIC miners for Ethash, the ASIC-resistant hash, were announced by Bitmain. Fear of over-influence from Bitmain and 51% attacks prompted discussions of bricking the devices, forcing ASIC miners into hard-mode mining, or continuing or expediting development and eventual release of Casper. It is thought that ASIC miners are not a threat to Ethereum. It was decided that Ethereum would switch from its pure proof of work to a hybrid proof of work and proof of stake scheme.\n"}
{"id": "3133127", "url": "https://en.wikipedia.org/wiki?curid=3133127", "title": "Fary–Milnor theorem", "text": "Fary–Milnor theorem\n\nIn the mathematical theory of knots, the Fary–Milnor theorem, named after István Fáry and John Milnor, states that three-dimensional smooth curves with small total curvature must be unknotted. The theorem was proved independently by Fáry in 1949 and Milnor in 1950. It was later shown to follow from the existence of quadrisecants .\n\nIf \"K\" is any closed curve in Euclidean space that is sufficiently smooth to define the curvature κ at each of its points, and if the total absolute curvature is less than or equal to 4π, then \"K\" is an unknot, i.e.:\n\nThe contrapositive tells us that if \"K\" is not an unknot, i.e. \"K\" is not isotopic to the circle, then the total curvature will be strictly greater than 4π. Notice that having the total curvature less than or equal to 4 is merely a sufficient condition for \"K\" to be an unknot; it is not a necessary condition. In other words, although all knots with total curvature less than or equal to 4π are the unknot, there exist unknots with curvature strictly greater than 4π.\n\nFor closed polygonal chains the same result holds with the integral of curvature replaced by the sum of angles between adjacent segments of the chain. By approximating arbitrary curves by polygonal chains one may extend the definition of total curvature to larger classes of curves, within which the Fary–Milnor theorem also holds (, ).\n\n\n"}
{"id": "43166060", "url": "https://en.wikipedia.org/wiki?curid=43166060", "title": "Fixed-point subgroup", "text": "Fixed-point subgroup\n\nIn algebra, the fixed-point subgroup formula_1 of an automorphism \"f\" of a group \"G\" is the subgroup of \"G\":\nMore generally, if \"S\" is a set of automorphisms of \"G\" (i.e., a subset of the automorphism group of \"G\"), then the set of the elements of \"G\" that are left fixed by every automorphism in \"S\" is a subgroup of \"G\", denoted by \"G\".\n\nFor example, take \"G\" to be the group of invertible \"n\"-by-\"n\" real matrices and formula_3 (called the Cartan involution). Then formula_1 is the group formula_5 of \"n\"-by-\"n\" orthogonal matrices.\n\nTo give an abstract example, let \"S\" be a subset of a group \"G\". Then each element of \"S\" can be thought of as an automorphism through conjugation. Then\nthat is, the centralizer of \"S\".\n"}
{"id": "2897576", "url": "https://en.wikipedia.org/wiki?curid=2897576", "title": "Fluent (artificial intelligence)", "text": "Fluent (artificial intelligence)\n\nIn artificial intelligence, a fluent is a condition that can change over time. In logical approaches to reasoning about actions, fluents can be represented in first-order logic by predicates having an argument that depends on time. For example, the condition “the box is on the table”, if it can change over time, cannot be represented by formula_1; a third argument is necessary to the predicate formula_2 to specify the time: formula_3 means that the box is on the table at time formula_4. This representation of fluents is modified in the situation calculus by using the sequence of the past actions in place of the current time.\nA fluent can also be represented by a function, dropping the time argument. For example, that the box is on the table can be represented by formula_5, where formula_6 is a function and not a predicate. In first order logic, converting predicates to functions is called reification; for this reason, fluents represented by functions are said to be reified. When using reified fluents, a separate predicate is necessary to tell when a fluent is actually true or not. For example, formula_7 means that the box is actually on the table at time formula_4, where the predicate formula_9 is the one that tells when fluents are true. This representation of fluents is used in the event calculus, in the fluent calculus, and in the features and fluents logics.\nSome fluents can be represented as functions in a different way. For example, the position of a box can be represented by a function formula_10 whose value is the object the box is standing on at time formula_4. Conditions that can be represented in this way are called \"functional fluents\". Statements about the values of such functions can be given in first order logic with equality using literals such as formula_12. Some fluents are represented this way in the situation calculus.\n\n"}
{"id": "7134874", "url": "https://en.wikipedia.org/wiki?curid=7134874", "title": "Fondements de la Géometrie Algébrique", "text": "Fondements de la Géometrie Algébrique\n\nFondements de la Géometrie Algébrique (FGA) is a book that collected together seminar notes of Alexander Grothendieck. It is an \nimportant source for his pioneering work on scheme theory, which laid foundations for algebraic geometry in its modern technical developments. \nThe title is a translation of the title of Weil's book \"Foundations of Algebraic Geometry.\"\nIt contained material on descent theory, and existence theorems including that for the Hilbert scheme. The \"Technique de descente et théorèmes d'existence en géometrie algébrique\" is one series of seminars within \"FGA\".\n\nLike the bulk of Grothendieck's work of the IHÉS period, duplicated notes were circulated, but the publication was not as a conventional book.\n\nThese are Séminaire Bourbaki notes, by number, from the years 1957 to 1962.\n\n\n"}
{"id": "35736414", "url": "https://en.wikipedia.org/wiki?curid=35736414", "title": "Fréchet inequalities", "text": "Fréchet inequalities\n\nIn probabilistic logic, the Fréchet inequalities, also known as the Boole–Fréchet inequalities, are rules implicit in the work of George Boole and explicitly derived by Maurice Fréchet that govern the combination of probabilities about logical propositions or events logically linked together in conjunctions (AND operations) or disjunctions (OR operations) as in Boolean expressions or fault or event trees common in risk assessments, engineering design and artificial intelligence. These inequalities can be considered rules about how to bound calculations involving probabilities without assuming independence or, indeed, without making any dependence assumptions whatsoever. The Fréchet inequalities are closely related to the Boole–Bonferroni–Fréchet inequalities, and to Fréchet bounds.\n\nIf \"A\" are logical propositions or events, the Fréchet inequalities are\n\nwhere P( ) denotes the probability of an event or proposition. In the case where there are only two events, say \"A\" and \"B\", the inequalities reduce to\n\nThe inequalities bound the probabilities of the two kinds of joint events given the probabilities of the individual events. For example, if A is \"has lung cancer\", and B is \"has mesothelioma\", then A & B is \"has both lung cancer and mesothelioma\", and A ∨ B is \"has lung cancer or mesothelioma or both diseases\", and the inequalities relate the risks of these events.\n\nNote that logical conjunctions are denoted in various ways in different fields, including AND, &, ∧ and graphical AND-gates. Logical disjunctions are likewise denoted in various ways, including OR, |, ∨, and graphical OR-gates. If events are taken to be sets rather than logical propositions, the set-theoretic versions of the Fréchet inequalities are\n\nIf the probability of an event A is P(A) = \"a\" = 0.7, and the probability of the event B is P(B) = \"b\" = 0.8, then the probability of the conjunction. i.e., the joint event A & B, is surely in the interval\nLikewise, the probability of the disjunction A ∨ B is surely in the interval\n\nThese intervals are contrasted with the results obtained from the rules of probability assuming independence, where the probability of the conjunction is P(A & B) = \"a\" × \"b\" = 0.7 × 0.8 = 0.56, and the probability of the disjunction is P(A ∨ B) = \"a\" + \"b\" − \"a\" × \"b\" = 0.94.\n\nWhen the marginal probabilities are very small (or large), the \nFréchet intervals are strongly asymmetric about the analogous results under independence. For example, suppose P(A) = 0.000002 = 2×10 and P(B) = 0.000003 = 3×10. Then the Fréchet inequalities say P(A & B) is in the interval [0, 2×10], and P(A ∨ B) is in the interval [3×10, 5×10]. If A and B are independent, however, the probability of A & B is 6×10 which is, comparatively, very close to the lower limit (zero) of the Fréchet interval. Similarly, the probability of A ∨ B is 4.999994×10, which is very close to the upper limit of the Fréchet interval. This is what justifies the rare-event approximation often used in reliability theory.\n\nThe proofs are elementary. Recall that P(\"A\" ∨ \"B\") = P(\"A\") + P(\"B\") − P(\"A\" & \"B\"), which implies P(\"A\") + P(\"B\") − P(\"A\" ∨ \"B\") = P(\"A\" & \"B\"). Because all probabilities are no bigger than 1, we know P(\"A\" ∨ \"B\") ≤ 1, which implies that P(\"A\") + P(\"B\") − 1 ≤ P(\"A\" & \"B\"). Because all probabilities are also positive we can similarly say 0 ≤ P(\"A\" & \"B\"), so max(0, P(\"A\") + P(\"B\") − 1) ≤ P(\"A\" & \"B\"). This gives the lower bound on the conjunction.\n\nTo get the upper bound, recall that P(\"A\" & \"B\") = P(\"A\"|\"B\") P(\"B\") = P(\"B\"|\"A\") P(\"A\"). Because P(\"A\"|\"B\") ≤ 1 and P(\"B\"|\"A\") ≤ 1, we know P(\"A\" & \"B\") ≤ P(\"A\") and P(\"A\" & \"B\") ≤ P(\"B\"). Therefore, P(\"A\" & \"B\") ≤ min(P(\"A\"), P(\"B\")), which is the upper bound.\n\nThe best-possible nature of these bounds follows from observing that they are realized by some dependency between the events A and B. Comparable bounds on the disjunction are similarly derived.\n\nWhen the input probabilities are themselves interval ranges, the Fréchet formulas still work as a probability bounds analysis.\nHailperin considered the problem of evaluating probabilistic Boolean expressions involving many events in complex conjunctions and disjunctions.\nSome have suggested using the inequalities in various applications of artificial intelligence and have extended the rules to account for various assumptions about the dependence among the events. The inequalities can also be generalized to other logical operations, including even modus ponens. When the input probabilities are characterized by probability distributions, analogous operations that generalize logical and arithmetic convolutions without assumptions about the dependence between the inputs can be defined based on the related notion of Fréchet bounds.\n\nIt is interesting that similar bounds hold also in Quantum Mechanics in the case of separable quantum systems and that entangled states violate these bounds. Let us consider a composite quantum system. In particular, we focus on a composite quantum system \"AB\" made by two finite subsystems denoted as \"A\" and \"B\". Assume that we know the density matrix of the subsystem \"A\", i.e., formula_1 that is a trace-one positive definite matrix in formula_2 (the space of Hermitian matrices of dimension formula_3), and the density matrix of subsystem \"B\" denoted as formula_4 We can think of formula_1 and formula_6 as the \"marginals\" of the subsystems \"A\" and \"B\". From the knowledge of these marginals, we want to infer something about the \"joint\" formula_7 in formula_8 We restrict our attention to \"joint\" formula_7 that are separable. A density matrix on a composite system is separable if there exist formula_10 and formula_11 which are mixed states of the respective subsystems such that\n\nwhere\n\nOtherwise formula_7 is called an entangled state.\n\nFor separable density matrices formula_15 in formula_16 the following Fréchet like bounds hold:\n\nThe inequalities are matrix inequalities, formula_18 denotes the tensor product and formula_19 the identity matrix of dimension formula_20. It is evident that structurally the above inequalities are analogues of the classical Fréchet bounds for the logical conjunction. It is also worth to notice that when the matrices formula_21 and formula_7 are restricted to be diagonal, we obtain the classical Fréchet bounds.\n\nThe upper bound is known in Quantum Mechanics as reduction criterion for density matrices; it was first proven by and independently formulated by. The lower bound has been obtained in that provides a Bayesian interpretation of these bounds.\n\nWe have observed when the matrices formula_21 and formula_7 are all diagonal, we obtain the classical Fréchet bounds. To show that, let us consider again the previous numerical example:\n\nthen we have:\n\nwhich means:\n\nIt is worth to point out that entangled states violate the above Fréchet bounds. Consider for instance the entangled density matrix (which is not separable):\n\nwhich has marginal\n\nEntangled states are not separable and it can easily be verified that\n\nsince the resulting matrices have one negative eigenvalue.\n\nAnother example of violation of probabilistic bounds is provided by the famous Bell's inequality: entangled states exhibit a form of \"stochastic\" dependence stronger than the strongest classical dependence: and in fact they violate Fréchet like bounds.\n\n"}
{"id": "27043777", "url": "https://en.wikipedia.org/wiki?curid=27043777", "title": "Govinda Bhattathiri", "text": "Govinda Bhattathiri\n\nGovinda Bhaṭṭathiri (also known as Govinda Bhattathiri of Thalakkulam or Thalkkulathur) ( 1237 – 1295) was an Indian astrologer and astronomer who flourished in Kerala during the thirteenth century CE. His major work was Dasadhyayi, a commentary on the first ten chapters of the astrological text Brihat Jataka composed by Varāhamihira (505 – 587 CE). This is considered to be the most important of the 70 known commentaries on this text. Bhaṭṭathiri had also authored another important work in astrology titled \"Muhūrttaratnaṃ\". Paramesvara (ca.1380–1460), an astronomer of the Kerala school of astronomy and mathematics known for the introduction of the \"Dṛggaṇita\" system of astronomical computations, had composed an extensive commentary on this work. In this commentary Paramesvara had indicated that he was a grandson of a disciple of the author of \"Muhūrttaratnaṃ\".\n\nGovinda Bhaṭṭatiri was born in the Nambudiri family known by the name Thalakkulathur in the village of Alathiyur, Tirur in Kerala. He was traditionally considered to be the progenitor of the Pazhur Kaniyar family of astrologers. He is a legendary figure in the Kerala astrological traditions.\n"}
{"id": "3013586", "url": "https://en.wikipedia.org/wiki?curid=3013586", "title": "Helge Tverberg", "text": "Helge Tverberg\n\nHelge Arnulf Tverberg (born 6 March 1935) is a Norwegian mathematician. He was a professor in the Mathematics Department at the University of Bergen, his speciality being combinatorics; he retired at the mandatory age of seventy.\n\nHe was born in Bergen. He took the cand.real. degree at the University of Bergen in 1958, and the dr.philos. degree in 1968. He was a lecturer from 1958 to 1971 and professor from 1971 to his retirement in 2005. He was a visiting scholar at the University of Reading in 1966 and at the Australian National University, in Canberra, from 1980 to 1981, 1987 to 1988 and in 2004. He is a member of the Norwegian Academy of Science and Letters.\n\nTverberg, in 1965, proved a result on intersection patterns of partitions of point configurations that has come to be known as Tverberg's partition theorem. It inaugurated a new branch of combinatorial geometry, with many variations and applications. An account by Günter M. Ziegler of Tverberg's work in this direction appeared in the issue of the Notices of the American Mathematical Society for April, 2011.\n"}
{"id": "2545815", "url": "https://en.wikipedia.org/wiki?curid=2545815", "title": "Heyting arithmetic", "text": "Heyting arithmetic\n\nIn mathematical logic, Heyting arithmetic (sometimes abbreviated HA) is an axiomatization of arithmetic in accordance with the philosophy of intuitionism (Troelstra 1973:18). It is named after Arend Heyting, who first proposed it.\n\nHeyting arithmetic adopts the axioms of Peano arithmetic (PA), but uses intuitionistic logic as its rules of inference. In particular, the law of the excluded middle does not hold in general, though the induction axiom can be used to prove many specific cases. For instance, one can prove that is a theorem (any two natural numbers are either equal to each other, or not equal to each other). In fact, since \"=\" is the only predicate symbol in Heyting arithmetic, it then follows that, for any quantifier-free formula \"p\", is a theorem (where \"x\", \"y\", \"z\"… are the free variables in \"p\").\n\nKurt Gödel studied the relationship between Heyting arithmetic and Peano arithmetic. He used the Gödel–Gentzen negative translation to prove in 1933 that if HA is consistent, then PA is also consistent.\n\nHeyting arithmetic should not be confused with Heyting algebras, which are the intuitionistic analogue of Boolean algebras.\n\n\n\n"}
{"id": "1688550", "url": "https://en.wikipedia.org/wiki?curid=1688550", "title": "Hyperrectangle", "text": "Hyperrectangle\n\nIn geometry, an \"n\"-orthotope (also called a hyperrectangle or a box) is the generalization of a rectangle for higher dimensions, formally defined as the Cartesian product of intervals.\n\nA three-dimensional \"orthotope\" is also called a right rectangular prism, rectangular cuboid, or rectangular parallelepiped.\n\nA special case of an \"n-orthotope\", where all edges are equal length, is the \"n-cube\".\n\nBy analogy, the term \"hyperrectangle\" or \"box\" refers to Cartesian products of orthogonal intervals of other kinds, such as ranges of keys in database theory or ranges of integers, rather than real numbers.\n\nThe dual polytope of an \"n\"-orthotope has been variously called a rectangular n-orthoplex, rhombic \"n\"-fusil, or \"n\"-lozenge. It is constructed by 2\"n\" points located in the center of the orthotope rectangular faces.\n\nAn n-fusil's Schläfli symbol can be represented by a sum of \"n\" orthogonal line segments: { } + { } + ... + { }.\n\nA 1-fusil is a line segment. A 2-fusil is a rhombus. Its plane cross selections in all pairs of axes are rhombi.\n\n\n"}
{"id": "14922", "url": "https://en.wikipedia.org/wiki?curid=14922", "title": "If and only if", "text": "If and only if\n\nIn logic and related fields such as mathematics and philosophy, if and only if (shortened iff) is a biconditional logical connective between statements.\n\nIn that it is biconditional (a statement of material equivalence), the connective can be likened to the standard material conditional (\"only if\", equal to \"if ... then\") combined with its reverse (\"if\"); hence the name. The result is that the truth of either one of the connected statements requires the truth of the other (i.e. either both statements are true, or both are false). It is controversial whether the connective thus defined is properly rendered by the English \"if and only if\", with its pre-existing meaning.\n\nIn writing, phrases commonly used, with debatable propriety, as alternatives to P \"if and only if\" Q include: \"Q is necessary and sufficient for P\", \"P is equivalent (or materially equivalent) to Q\" (compare material implication), \"P precisely if Q\", \"P precisely (or exactly) when Q\", \"P exactly in case Q\", and \"P just in case Q\". Some authors regard \"iff\" as unsuitable in formal writing; others use it freely.\n\nIn logical formulae, logical symbols are used instead of these phrases; see the discussion of notation.\n\nThe truth table of \"P\" formula_1 \"Q\" is as follows:\nIt is equivalent to that produced by the XNOR gate, and opposite to that produced by the XOR gate.\n\nThe corresponding logical symbols are \"↔\", \"formula_1\", and \"≡\", and sometimes \"iff\". These are usually treated as equivalent. However, some texts of mathematical logic (particularly those on first-order logic, rather than propositional logic) make a distinction between these, in which the first, ↔, is used as a symbol in logic formulas, while ⇔ is used in reasoning about those logic formulas (e.g., in metalogic). In Łukasiewicz's notation, it is the prefix symbol 'E'.\n\nAnother term for this logical connective is exclusive nor.\n\nIn TeX \"if and only if\" is shown as a long double arrow: formula_3 via command \\iff.\n\nIn most logical systems, one proves a statement of the form \"P iff Q\" by proving \"if P, then Q\" and \"if Q, then P\". Proving this pair of statements sometimes leads to a more natural proof since there are not obvious conditions in which one would infer a biconditional directly. An alternative is to prove the disjunction \"(P and Q) or (not-P and not-Q)\", which itself can be inferred directly from either of its disjuncts—that is, because \"iff\" is truth-functional, \"P iff Q\" follows if P and Q have both been shown true, or both false.\n\nUsage of the abbreviation \"iff\" first appeared in print in John L. Kelley's 1955 book \"General Topology\".\nIts invention is often credited to Paul Halmos, who wrote \"I invented 'iff,' for 'if and only if'—but I could never believe I was really its first inventor.\" \n\nIt is somewhat unclear how \"iff\" was meant to be pronounced. In current practice, the single 'word' \"iff\" is almost always read as the four words \"if and only if\". However, in the preface of \"General Topology\", Kelley suggests that it should be read differently: \"In some cases where mathematical content requires 'if and only if' and euphony demands something less I use Halmos' 'iff'\". The authors of one discrete mathematics textbook suggest: \"Should you need to pronounce iff, really hang on to the 'ff' so that people hear the difference from 'if'\", implying that \"iff\" could be pronounced as .\n\nTechnically, definitions are always \"if and only if\" statements; many texts such as Kelley's \"General Topology\" follow the strict demands of logic, and use \"if and only if\" or \"iff\" in definitions of new terms (for instance, from \"General Topology\", p. 25: \"A set is countable iff it is finite or countably infinite\" [boldface in original]). However, this usage of \"if and only if\" is not universal; often, mathematical definitions follow the special convention that \"if\" is interpreted to mean \"if and only if\" (for example, one might say, \"A topological space is compact if every open cover has a finite subcover\").\n\n\nSufficiency is the converse of necessity. That is to say, given \"P\"→\"Q\" (i.e. if \"P\" then \"Q\"), \"P\" would be a sufficient condition for \"Q\", and \"Q\" would be a necessary condition for \"P\". Also, given \"P\"→\"Q\", it is true that \"¬Q\"→\"¬P\" (where ¬ is the negation operator, i.e. \"not\"). This means that the relationship between \"P\" and \"Q\", established by \"P\"→\"Q\", can be expressed in the following, all equivalent, ways:\nAs an example, take (1), above, which states \"P\"→\"Q\", where \"P\" is \"the fruit in question is an apple\" and \"Q\" is \"Madison will eat the fruit in question\". The following are four equivalent ways of expressing this very relationship:\nSo we see that (2), above, can be restated in the form of \"if...then\" as \"If Madison will eat the fruit in question, then it is an apple\"; taking this in conjunction with (1), we find that (3) can be stated as \"If the fruit in question is an apple, then Madison will eat it; \"and\" if Madison will eat the fruit, then it is an apple\".\n\nEuler diagrams show logical relationships among events, properties, and so forth. \"P only if Q\", \"if P then Q\", and \"P→Q\" all mean that P is a subset, either proper or improper, of Q. \"P if Q\", \"if Q then P\", and Q→P all mean that Q is a proper or improper subset of P. \"P if and only if Q\" and \"Q if and only if P\" both mean that the sets P and Q are identical to each other.\n\nIff is used outside the field of logic. Wherever logic is applied, especially in mathematical discussions, it has the same meaning as above: it is an abbreviation for \"if and only if\", indicating that one statement is both necessary and sufficient for the other. This is an example of mathematical jargon. (However, as noted above, \"if\", rather than \"iff\", is more often used in statements of definition.)\n\nThe elements of \"X\" are \"all and only\" the elements of \"Y\" is used to mean: \"for any \"z\" in the domain of discourse, \"z\" is in \"X\" if and only if \"z\" is in \"Y\".\"\n\n\n"}
{"id": "28163606", "url": "https://en.wikipedia.org/wiki?curid=28163606", "title": "Jorge Arturo Mendoza Huertas", "text": "Jorge Arturo Mendoza Huertas\n\nJorge Arturo Mendoza Huertas (born 4 July 1971) is a Peruvian mental calculator.\n\nIn 2014 he set a new Guinness World Record by mentally adding a hundred single-digit numbers together in 18.23 seconds. At the 2006 Mental Calculation World Cup in Giessen, Germany, he finished in first position in the category 'Addition'.\n\n\n\n"}
{"id": "41995729", "url": "https://en.wikipedia.org/wiki?curid=41995729", "title": "Keith Martin Ball", "text": "Keith Martin Ball\n\nKeith Martin Ball FRS FRSE (born 26 December 1960) is a mathematician and professor at the University of Warwick. He was scientific director of the International Centre for Mathematical Sciences (ICMS) from 2010 to 2014.<ref name=\"doi10.1007/BF01231769|noedit\"></ref>\n\nBall was educated at Berkhamsted School and Trinity College, Cambridge where he studied the Cambridge Mathematical Tripos and was awarded a Bachelor of Arts degree in Mathematics in 1982 and a PhD in 1987 for research supervised by Béla Bollobás.\n\nKeith Ball's research is in the fields of functional analysis, high-dimensional and discrete geometry and information theory. He is the author of \"Strange Curves, Counting Rabbits, & Other Mathematical Explorations\".\n\nBall was elected a Fellow of the American Mathematical Society (AMS) in 2012 and a Fellow of the Royal Society (FRS) in 2013. His Royal Society citation reads \n"}
{"id": "47682758", "url": "https://en.wikipedia.org/wiki?curid=47682758", "title": "Ken-ichi Kawarabayashi", "text": "Ken-ichi Kawarabayashi\n\nKen-ichi Kawarabayashi (, born 1975) is a Japanese graph theorist who works as a professor at the National Institute of Informatics and is known for his research on graph theory (particularly the theory of graph minors) and graph algorithms.\n\nKawarabayashi was born on May 22, 1975 in Tokyo.\nHe earned a bachelor's degree in mathematics from Keio University in 1998, a master's degree from Keio in 2000, and a PhD from Keio in 2001, researching the Lovasz–Woodall conjecture under the supervision of Katsuhiro Ota. After temporary positions at Vanderbilt University and under the supervision of Paul Seymour at Princeton University, he became an assistant professor at Tohoku University in 2003, and moved to the National Institute of Informatics in 2006.\n\nIn 2003, Kawarabayashi was one of three winners of the Kirkman Medal of the Institute of Combinatorics and its Applications, an award given by them annually to researchers within four years of their PhD. In 2015, he won the Spring Prize of the Mathematical Society of Japan, its highest honor. He was a keynote speaker at the 2015 International Colloquium on Automata, Languages and Programming.\n\n"}
{"id": "1790247", "url": "https://en.wikipedia.org/wiki?curid=1790247", "title": "Leonard Eugene Dickson", "text": "Leonard Eugene Dickson\n\nLeonard Eugene Dickson (January 22, 1874 – January 17, 1954) was an American mathematician. He was one of the first American researchers in abstract algebra, in particular the theory of finite fields and classical groups, and is also remembered for a three-volume history of number theory, \"History of the Theory of Numbers\".\n\nDickson considered himself a Texan by virtue of having grown up in Cleburne, where his father was a banker, merchant, and real estate investor. He attended the University of Texas at Austin, where George Bruce Halsted encouraged his study of mathematics. Dickson earned a B.S. in 1893 and an M.S. in 1894, under Halsted's supervision. Dickson first specialised in Halsted's own specialty, geometry.\n\nBoth the University of Chicago and Harvard University welcomed Dickson as a Ph.D. student, and Dickson initially accepted Harvard's offer, but chose to attend Chicago instead. In 1896, when he was only 22 years of age, he was awarded Chicago's first doctorate in mathematics, for a dissertation titled \"The Analytic Representation of Substitutions on a Power of a Prime Number of Letters with a Discussion of the Linear Group\", supervised by E. H. Moore.\n\nDickson then went to Leipzig and Paris to study under Sophus Lie and Camille Jordan, respectively. On returning to the USA, he became an instructor at the University of California. In 1899 and at the extraordinarily young age of 25, Dickson was appointed associate professor at the University of Texas. Chicago countered by offering him a position in 1900, and he spent the balance of his career there. At Chicago, he supervised 53 Ph.D. theses; his most accomplished student was probably A. A. Albert. He was a visiting professor at the University of California in 1914, 1918, and 1922. In 1939, he returned to Texas to retire.\n\nDickson married Susan McLeod Davis in 1902; they had two children, Campbell and Eleanor.\n\nDickson was elected to the National Academy of Sciences in 1913, and was also a member of the American Philosophical Society, the American Academy of Arts and Sciences, the London Mathematical Society, the French Academy of Sciences and the Union of Czech Mathematicians and Physicists. Dickson was the first recipient of a prize created in 1924 by The American Association for the Advancement of Science, for his work on the arithmetics of algebras. Harvard (1936) and Princeton (1941) awarded him honorary doctorates.\n\nDickson presided over the American Mathematical Society in 1917–1918. His December 1918 presidential address, titled \"Mathematics in War Perspective,\" criticized American mathematics for falling short of those of Britain, France, and Germany:\n\nIn 1928, he was also the first recipient of the Cole Prize for algebra, awarded annually by the AMS, for his book \"Algebren und ihre Zahlentheorie\".\n\nIt appears that Dickson was a hard man:\n\nDickson had a major impact on American mathematics, especially abstract algebra. His mathematical output consists of 18 books and more than 250 papers. The \"Collected Mathematical Papers of Leonard Eugene Dickson\" fill six large volumes.\n\nIn 1901, Dickson published his first book \"Linear groups with an exposition of the Galois field theory\", a revision and expansion of his Ph.D. thesis. Teubner in Leipzig published the book, as there was no well-established American scientific publisher at the time. Dickson had already published 43 research papers in the preceding five years; all but seven on finite linear groups. Parshall (1991) described the book as follows:\n\nAn appendix in this book lists the non-abelian simple groups then known having order less than 1 billion. He listed 53 of the 56 having order less than 1 million. The remaining three were found in 1960, 1965, and 1967.\n\nDickson worked on finite fields and extended the theory of linear associative algebras initiated by Joseph Wedderburn and Cartan.\n\nHe started the study of modular invariants of a group.\n\nIn 1905, Wedderburn, then at Chicago on a Carnegie Fellowship, published a paper that included three claimed proofs of a theorem stating that all finite division algebras were commutative, now known as Wedderburn's theorem. The proofs all made clever use of the interplay between the additive group of a finite division algebra \"A\", and the multiplicative group \"A\"* = \"A\" − {0}. Karen Parshall noted that the first of these three proofs had a gap not noticed at the time. Dickson also found a proof of this result but, believing Wedderburn's first proof to be correct, Dickson acknowledged Wedderburn's priority. But Dickson also noted that Wedderburn constructed his second and third proofs only after having seen Dickson's proof. She concluded that Dickson should be credited with the first correct proof.\n\nDickson's search for a counterexample to Wedderburn's theorem led him to investigate nonassociative algebras, and in a series of papers he found all possible three and four-dimensional (nonassociative) division algebras over a field.\n\nIn 1919 Dickson constructed Cayley numbers by a doubling process starting with quaternions ℍ. His method was extended to a doubling of ℝ to produce ℂ, and of ℂ to produce ℍ by A. A. Albert in 1922, and the procedure is known now as the Cayley–Dickson construction of composition algebras.\n\nDickson proved many interesting results in number theory, using results of Vinogradov to deduce the ideal Waring theorem in his investigations of additive number theory. He proved the Waring's problem for formula_1 under the further condition of\n\nindependently of Subbayya Sivasankaranarayana Pillai who proved it for formula_3 ahead of him.\n\nThe three-volume \"History of the Theory of Numbers\" (1919–23) is still much consulted today, covering divisibility and primality, Diophantine analysis, and quadratic and higher forms. The work contains little interpretation and makes no attempt to contextualize the results being described, yet it contains essentially every significant number theoretic idea from the dawn of mathematics up to the 1920s except for quadratic reciprocity and higher reciprocity laws. A planned fourth volume on these topics was never written. A. A. Albert remarked that this three volume work \"would be a life's work by itself for a more ordinary man.\"\n\n\n"}
{"id": "37848137", "url": "https://en.wikipedia.org/wiki?curid=37848137", "title": "Lulu smoothing", "text": "Lulu smoothing\n\nIn signal processing, Lulu smoothing is a nonlinear mathematical technique for removing impulsive noise from a data sequence such as a time series. It is a nonlinear equivalent to taking a moving average (or other smoothing technique) of a time series, and is similar to other nonlinear smoothing techniques, such as Tukey or median smoothing. LULU smoothers are compared in detail to median smoothers by Jankowitz and found to be superior in some aspects, particularly in mathematical properties like idempotence.\n\nLulu operators have a number of attractive mathematical properties, among them idempotence – meaning that repeated application of the operator yields the same result as a single application – and co-idempotence. \nAn interpretation of idempotence is that: 'Idempotence means that there is no “noise” left in the smoothed data and co-idempotence means that there is no “signal” left in the residual.'\n\nWhen studying smoothers there are four properties that are useful to optimize:\n\n\nThe operators can also be used to decompose a signal into various subcomponents similar to wavelet or Fourier decomposition.\n\nLulu smoothers were discovered by C. H. Rohwer and have been studied for the last 30 years. Their exact and asymptotic distributions have been derived.\n\nApplying a Lulu smoother consists of repeated applications of the min and max operators over a given subinterval of the data.\nAs with other smoothers, a width or interval must be specified. The Lulu smoothers are composed of repeated applications of the \"L\" (lower) and \"U\" (Upper) operators, which are defined as follows:\n\nFor an L operator of width \"n\" over an infinite sequence of \"x\"s (..., \"x\", \"x\"...), the operation on \"x\" is calculated as follows:\n\n\nThus for width 2, the \"L\" operator is:\n\nThis is identical to the L operator, except that the order of Min and Max is reversed, i.e. for width 2:\n\nExamples of the \"U\" and \"L\" operators, as well as combined \"UL\" and \"LU\" operators on a sample data set are shown in the following figures.\n\nIt can be seen that the results of the \"UL\" and \"LU\" operators can be different. The combined operators are very effective at removing impulsive noise, the only cases where the noise is not removed effectively is where we get multiple noise signals very close together, in which case the filter 'sees' the multiple noises as part of the signal.\n"}
{"id": "546969", "url": "https://en.wikipedia.org/wiki?curid=546969", "title": "Magic constant", "text": "Magic constant\n\nThe magic constant or magic sum of a magic square is the sum of numbers in any row, column, or diagonal of the magic square. For example, the magic square shown below has a magic constant of 15. In general formula_1 where formula_2 is the side length of the square.\n\nThe term magic constant or magic sum is similarly applied to other \"magic\" figures such as magic stars and magic cubes. Number shapes on a triangular grid divided into equal polyiamond areas containing equal sums give a polyiamond magic constant.\n\nThe magic constant of an \"n\"-pointed normal magic star is formula_3.\n\nIn 2013 Dirk Kinnaes found the magic series polytope. The number of unique sequences that form the magic constant is now known up to formula_4.\n\nIn the mass model the value in each cell specifies the mass for that cell. This model has two notable properties. First it demonstrates the balanced nature of all magic squares. If such a model is suspended from the central cell the structure balances. ( consider the magic sums of the rows/columns .. equal mass at an equal distance balance). The second property that can be calculated is the moment of inertia. Summing the individual moments of inertia ( distance squared from the center x the cell value) gives the moment of inertia for the magic square. \"This is the only property of magic squares, aside from the line sums, which is solely dependent on the order of the square\" \n\n\n"}
{"id": "28126761", "url": "https://en.wikipedia.org/wiki?curid=28126761", "title": "Martin Dyer", "text": "Martin Dyer\n\nMartin Edward Dyer (born 16 July 1946 in Ryde, Isle of Wight, England) is a professor in the School of Computing at the University of Leeds, Leeds, England. He graduated from the University of Leeds in 1967, obtained his MSc from Imperial College London in 1968 and his PhD from the University of Leeds in 1979. His research interests lie in theoretical computer science, discrete optimization and combinatorics. Currently, he focuses on the complexity of counting and the efficiency of Markov chain algorithms for approximate counting.\n\nFour key contributions made by Martin Dyer are:\n\n\nIn 1991, Professor Dyer received the Fulkerson Prize in Discrete Mathematics (Jointly with Alan Frieze and Ravi Kannan for the paper \"A random polynomial time algorithm for approximating the volume of convex bodies\" in the Journal of the Association for Computing Machinery) awarded by the American Mathematical Society and the Mathematical Programming Society.\n\nIn 2013, the EATCS Awards Committee consisting of Leslie Ann Goldberg, Vladimiro Sassone and Friedhelm Meyer auf der Heide (chair), has unanimously decided to give the EATCS Award to Professor Martin Dyer.\n\nMartin Dyer is married to Alison. They have two adult children.\n\n"}
{"id": "53776585", "url": "https://en.wikipedia.org/wiki?curid=53776585", "title": "MiMa Mineralogy and Mathematics Museum", "text": "MiMa Mineralogy and Mathematics Museum\n\nMiMa is a museum of mineralogy and mathematics in Oberwolfach, in the central Black Forest in southern Germany. The museum was opened on 30 January 2010 on the site of the mineral museum after a two-year conversion and expansion phase. It is operated jointly by the municipality of Oberwolfach, the Oberwolfach Society of the Friends of Minerals and Mining and the Mathematics Research Institute, Oberwolfach.\n\nThe idea is to unite the two distinct features of the region in one interactive museum: the Black Forest minerals of the Society of Friends of Minerals and Mining, Oberwolfach, the knowledge of the Mathematics Research Institute, Oberwolfach.\n\nThe museum not only displays minerals from the Black Forest and insights into mathematics, but also the links between these two fields. Interactive installations help to elucidate the themes of symmetry and crystallography.\n\nThe MiMa houses minerals and mining artefacts from the whole of the Black Forest with an emphasis on minerals from the nearby Clara Pit, using films and information. A scale model shows the interior of the interior of the mine with mining galleries and shafts down to sea level. There is information on the ores extracted and there technical usage.\n\nOther exhibits:\n\n\nIn the field of mathematics, the subject areas of crystallography and symmetry are explained using interactive programmes. In addition, there is a selection of exhibits from the prize-winning exhibition IMAGINARY (Land of Ideas Prize, 2009) and an installation with historical information about the Oberwolfach Mathematical Research Institute.\n\nOther exhibits:\n\n\nIn a film room, films from the world of minerals and mathematics are shown. The museum shop sells minerals and books. Special exhibitions and cultural events fill out the programme.\n\n"}
{"id": "54225729", "url": "https://en.wikipedia.org/wiki?curid=54225729", "title": "Moschovakis coding lemma", "text": "Moschovakis coding lemma\n\nThe Moschovakis coding lemma is a lemma from descriptive set theory involving sets of real numbers in the axiom of determinacy (the principle that every two-player integer game is determined). The lemma was developed and named after the mathematician, Yiannis N. Moschovakis.\n\nThe lemma may be expressed generally as follows:\n\nLet Γ be a non-selfdual point-class closed under ∃ ω ω and ∧ , and ≺ a Γ well-founded relation on ω ω of rank θ ∈ ON. Let R ⊆ dom( ≺ ) × ω ω be such that ∀ x ∈ dom( ≺ ) ∃ y R ( x,y ) . Then there is a Γ set A ⊆ dom( ≺ ) × ω ω which is a choice set for R , that is: 1. ∀ α < θ ∃ x ∈ dom( ≺ ) ∃ y [ | x | ≺ = α ∧ A ( x,y )] . 2. ∀ x ∀ y A ( x,y ) → [ x ∈ dom( ≺ ) ∧ R ( x,y )] . Proof. We may assume θ is minimal so that the theorem fails, and fix ≺ , R , and a good universal set U ⊆ ( ω ω ) 3 for the Γ subsets of ( ω ω ) 2 . Easily, θ is a limit ordinal. For δ < θ , say u ∈ ω ω codes a δ -choice set provided (1) holds for α ≤ δ using A = U u , and (2) holds for A = U u where we replace x ∈ dom( ≺ ) with x ∈ dom( ≺ ) ∧| x | ≺ ≤ δ . By minimality of θ , for all δ < θ there are δ -choice sets. Play the game where I, II play out u,v ∈ ω ω , and II wins provided that if u codes a δ 1 -choice set for some δ 1 < θ , then v codes a δ 2 -choice set for some δ 2 > δ 1 . If I has a winning strategy, we get a Σ 1 1 set B of reals coding δ -choice sets for arbitrarily large δ < θ . Define then A ( x,y ) ↔∃ w ∈ B U ( w,x,y ), which easily works. Suppose now τ is a winning strategy for II. From the s - m - n theorem, let s : ( ω ω ) 2 → ω ω be continuous such that for all ,x,t,w, U ( s (,x ) ,t,w ) ↔ ∃ y ∃ z [ y ≺ x ∧ U (,y,z ) ∧ U ( z,t,w )]. By the recursion theorem, let 0 be such that U (0 ,x,z ) ↔ z = τ ( s (0 ,x )). A straightforward induction on | x | ≺ for x ∈ dom( ≺ ) shows that ∀ x ∈ dom( ≺ ) ∃ ! z U ( 0 ,x,z ), and ∀ x ∈ dom( ≺ ) ∀ z [ U ( 0 ,x,z ) → z codes a ≥| x | ≺ -choice set]. Let A ( x,y ) ↔∃ z ∈ dom( ≺ ) ∃ w [ U ( 0 ,z,w ) ∧ U ( w,x,y )]., \n\n"}
{"id": "1236569", "url": "https://en.wikipedia.org/wiki?curid=1236569", "title": "Nagata–Smirnov metrization theorem", "text": "Nagata–Smirnov metrization theorem\n\nThe Nagata–Smirnov metrization theorem in topology characterizes when a topological space is metrizable. The theorem states that a topological space formula_1 is metrizable if and only if it is regular, Hausdorff and has a countably locally finite (i.e., σ-locally finite) basis.\n\nA topological space X is called a regular space if every non-empty closed subset C of X and a point p not contained in C admit non-overlapping open neighborhoods.\nA collection in a space X is countably locally finite (or σ-locally finite) if it is the union of a countable family of locally finite collections of subsets of X.\n\nUnlike Urysohn's metrization theorem, which provides only a sufficient condition for metrizability, this theorem provides both a necessary and sufficient condition for a topological space to be metrizable. The theorem is named after Junichi Nagata and Yuriĭ Mikhaĭlovich Smirnov.\n\n\n"}
{"id": "3009815", "url": "https://en.wikipedia.org/wiki?curid=3009815", "title": "Null (mathematics)", "text": "Null (mathematics)\n\nIn mathematics, the word null (from meaning \"zero\", which is from meaning \"none\") means of or related to having zero members in a set or a value of zero. Sometimes the symbol ∅ is used to distinguish \"null\" from 0.\n\nIn a normed vector space the null vector is the zero vector; in a seminormed vector space such as Minkowski space, null vectors are, in general, non-zero. In set theory, the null set is the set with zero elements; and in measure theory, a null set is a set with zero measure.\n\nA null space of a mapping is the part of the domain that is mapped into the null element of the image (the inverse image of the null element).\n\nIn statistics, a null hypothesis is a proposition presumed true unless statistical evidence indicates otherwise.\n"}
{"id": "1193984", "url": "https://en.wikipedia.org/wiki?curid=1193984", "title": "Ott-Heinrich Keller", "text": "Ott-Heinrich Keller\n\nEduard Ott-Heinrich Keller (22 June 1906 in Frankfurt (Main), Germany\n– 1990 in Halle, Germany) was a German mathematician who worked in the fields of geometry, topology and algebraic geometry. He formulated the celebrated problem which is now called the Jacobian conjecture in 1939.\n\nHe was born in Frankfurt–am-Main, and studied at the universities of Frankfurt, Vienna, Berlin and Göttingen. As a student of Max Dehn he wrote a dissertation on the tiling of space with cubes. This led to another 'Keller conjecture': the Keller cube-tiling conjecture from 1930.\n\nSubsequently he worked with Georg Hamel in Berlin, habilitating in 1933 with a thesis on Cremona transformations. The Jacobian conjecture is quite naturally posed in that setting. The motivation for looking at rather general polynomial transformations, say of the projective plane, came from the singularity theory for algebraic curves.\n\nDuring World War II he taught in a naval college in Flensburg. After the war he had several positions, and was appointed a professor at Martin Luther University of Halle-Wittenberg in 1952, as successor of H. W. E. Jung.\n\n"}
{"id": "10160606", "url": "https://en.wikipedia.org/wiki?curid=10160606", "title": "Overdetermined system", "text": "Overdetermined system\n\nIn mathematics, a system of equations is considered overdetermined if there are more equations than unknowns. An overdetermined system is almost always inconsistent (it has no solution) when constructed with random coefficients. However, an overdetermined system will have solutions in some cases, for example if some equation occurs several times in the system, or if some equations are linear combinations of the others.\n\nThe terminology can be described in terms of the concept of constraint counting. Each unknown can be seen as an available degree of freedom. Each equation introduced into the system can be viewed as a constraint that restricts one degree of freedom. \nTherefore, the critical case occurs when the number of equations and the number of free variables are equal. For every variable giving a degree of freedom, there exists a corresponding constraint. The \"overdetermined\" case occurs when the system has been overconstrained — that is, when the equations outnumber the unknowns. In contrast, the \"underdetermined\" case occurs when the system has been underconstrained — that is, when the number of equations is fewer than the number of unknowns. Such systems usually have an infinite number of solutions.\n\nConsider the system of 3 equations and 2 unknowns ( and ), which is overdetermined because 3>2, and which corresponds to Diagram #1:\n\nThere is one solution for each pair of linear equations: for the first and second equations (0.2, −1.4), for the first and third (−2/3, 1/3), and for the second and third (1.5, 2.5). However, there is no solution that satisfies all three simultaneously. Diagrams #2 and 3 show other configurations that are inconsistent because no point is on all of the lines. Systems of this variety are deemed inconsistent.\n\nThe only cases where the overdetermined system does in fact have a solution are demonstrated in Diagrams #4, 5, and 6. These exceptions can occur only when the overdetermined system contains enough linearly dependent equations that the number of independent equations does not exceed the number of unknowns. Linear dependence means that some equations can be obtained from linearly combining other equations. For example, \"Y\" = \"X\" + 1 and 2\"Y\" = 2\"X\" + 2 are linearly dependent equations because the second one can be obtained by taking twice the first one.\n\nAny system of linear equations can be written as a matrix equation.\nThe previous system of equations (in Diagram #1) can be written as follows:\nNotice that the rows of the coefficient matrix (corresponding to equations) outnumber the columns (corresponding to unknowns), meaning that the system is overdetermined. The rank of this matrix is 2, which corresponds to the number of dependent variables in the system. A linear system is consistent if and only if the coefficient matrix has the same rank as its augmented matrix (the coefficient matrix with an extra column added, that column being the column vector of constants). The augmented matrix has rank 3, so the system is inconsistent. The nullity is 0, which means that the null space contains only the zero vector and thus has no basis.\n\nIn linear algebra the concepts of row space, column space and null space are important for determining the properties of matrices. The informal discussion of constraints and degrees of freedom above relates directly to these more formal concepts.\nThe homogeneous case (in which all constant terms are zero) is always consistent (because there is a trivial, all-zero solution). There are two cases, depending on the number of linearly dependent equations: either there is just the trivial solution, or there is the trivial solution plus an infinite set of other solutions.\n\nConsider the system of linear equations: \"L\" = 0 for 1 ≤ \"i\" ≤ \"M\", and variables \"X\", \"X\", ..., \"X\", where each \"L\" is a weighted sum of the \"X\"s. Then \"X\" = \"X\" = ... = \"X\" = 0 is always a solution. When \"M\" < \"N\" the system is \"underdetermined\" and there are always an infinitude of further solutions. In fact the dimension of the space of solutions is always at least \"N\" − \"M\".\n\nFor \"M\" ≥ \"N\", there may be no solution other than all values being 0. There will be an infinitude of other solutions only when the system of equations has enough dependencies (linearly dependent equations) that the number of independent equations is at most \"N\" − 1. But with \"M\" ≥ \"N\" the number of independent equations could be as high as \"N\", in which case the trivial solution is the only one.\n\nIn systems of linear equations, \"L\"=\"c\" for 1 ≤ \"i\" ≤ \"M\", in variables \"X\", \"X\", ..., \"X\" the equations are sometimes linearly dependent; in fact the number of linearly independent equations cannot exceed \"N\"+1. We have the following possible cases for an overdetermined system with \"N\" unknowns and \"M\" equations (\"M\">\"N\").\n\nThese results may be easier to understand by putting the augmented matrix of the coefficients of the system in row echelon form by using Gaussian elimination. This row echelon form is the augmented matrix of a system of equations that is equivalent to the given system (it has exactly the same solutions). The number of independent equations in the original system is the number of non-zero rows in the echelon form. The system is inconsistent (no solution) if and only if the last non-zero row in echelon form has only one non-zero entry that is in the last column (giving an equation 0 = c where c is a non-zero constant). Otherwise, there is exactly one solution when the number of non-zero rows in echelon form is equal to the number of unknowns, and there are infinitely many solutions when the number of non-zero rows is lower than the number of variables.\n\nPutting it another way, according to the Rouché–Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has \"k\" free parameters where \"k\" is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions.\n\nAll exact solutions can be obtained, or it can be shown that none exist, using matrix algebra. See System of linear equations#Matrix solution.\n\nThe method of ordinary least squares can be used to find an approximate solution to overdetermined systems. For the system formula_3 the least squares formula is obtained from the problem\n\nthe solution of which can be written with the normal equations,\n\nwhere formula_6 indicates a matrix transpose, \"provided\" formula_7 exists (that is, provided \"A\" has full column rank). With this formula an approximate solution is found when no exact solution exists, and it gives an exact solution when one does exist.\nHowever, to achieve good numerical accuracy, using the QR factorization of \"A\" to solve the least squares problem is preferred.\n\nThe concept can also be applied to more general systems of equations, such as systems of polynomial equations or partial differential equations. In the case of the systems of polynomial equations, it may happen that an overdetermined system has a solution, but that no one equation is a consequence of the others and that, when removing any equation, the new system has more solutions. For example, formula_8 has the single solution formula_9 but each equation by itself has two solutions.\n\n"}
{"id": "22049756", "url": "https://en.wikipedia.org/wiki?curid=22049756", "title": "Overlap coefficient", "text": "Overlap coefficient\n\nThe overlap coefficient, or Szymkiewicz–Simpson coefficient, is a similarity measure that measures the overlap between two finite sets. It is related to the Jaccard index and is defined as the size of the intersection divided by the smaller of the size of the two sets:\n\nIf set \"X\" is a subset of \"Y\" or the converse then the overlap coefficient is equal to 1.\n"}
{"id": "22358224", "url": "https://en.wikipedia.org/wiki?curid=22358224", "title": "Picture (mathematics)", "text": "Picture (mathematics)\n\nIn combinatorial mathematics, a picture is a bijection between skew diagrams satisfying certain properties, introduced by in a generalization of the Robinson–Schensted correspondence and the Littlewood–Richardson rule.\n\n"}
{"id": "37161661", "url": "https://en.wikipedia.org/wiki?curid=37161661", "title": "Pierre Conner", "text": "Pierre Conner\n\nPierre Euclide Conner (27 June 1932, Houston, Texas – 3 February 2018, New Orleans, Louisiana) was an American mathematician, who worked on algebraic topology and differential topology (especially cobordism theory).\n\nIn 1955 Conner received his Ph.D from Princeton University under Donald Spencer with thesis \"The Green's and Neumann's Problems for Differential Forms on Riemannian Manifolds\". He was a post-doctoral fellow from 1955 to 1957 (and again in 1961–1962) at the Institute for Advanced Study. He was in the 1960s a professor at the University of Virginia, where he collaborated with his colleague Edwin E. Floyd, and then in the 1970s a professor at Louisiana State University.\n\nIn 2012 he became a fellow of the American Mathematical Society.\n\n\n"}
{"id": "206886", "url": "https://en.wikipedia.org/wiki?curid=206886", "title": "Program evaluation and review technique", "text": "Program evaluation and review technique\n\nThe program (or project) evaluation and review technique (PERT) is a statistical tool used in project management, which was designed to analyze and represent the tasks involved in completing a given project. \n\nFirst developed by the United States Navy in the 1950s, it is commonly used in conjunction with the critical path method (CPM).\n\nProject Evaluation and Review Techniques is commonly abbreviated to PERT.\nPERT is a method of analyzing the tasks involved in completing a given project, especially the time needed to complete each task, and to identify the minimum time needed to complete the total project. It incorporates uncertainty by making it possible to schedule a project while not knowing precisely the details and durations of all the activities. It is more of an event-oriented technique rather than start- and completion-oriented, and is used more in projects where time is the major factor rather than cost. It is applied to very large-scale, one-time, complex, non-routine infrastructure and Research and Development projects.\n\nProgram Evaluation Review Technique (PERT) offers a management tool, which relies \"on arrow and node diagrams of \"activities\" and \"events\": arrows represent the \"activities\" or work necessary to reach the \"events\" or nodes that indicate each completed phase of the total project.\"\n\nPERT and CPM are complementary tools, because \"CPM employs one time estimate and one cost estimate for each activity; PERT may utilize three time estimates (optimistic, expected, and pessimistic) and no costs for each activity. Although these are distinct differences, the term PERT is applied increasingly to all critical path scheduling.\"\n\n\"PERT\" was developed primarily to simplify the planning and scheduling of large and complex projects. It was developed for the U.S. Navy Special Projects Office in 1957 to support the U.S. Navy's Polaris nuclear submarine project. It found applications all over industry. An early example was it was used for the 1968 Winter Olympics in Grenoble which applied PERT from 1965 until the opening of the 1968 Games. This project model was the first of its kind, a revival for scientific management, founded by Frederick Taylor (Taylorism) and later refined by Henry Ford (Fordism). DuPont's critical path method was invented at roughly the same time as PERT.\nInitially PERT stood for \"Program Evaluation Research Task,\" but by 1959 was already renamed. It had been made public in 1958 in two publications of the U.S. Department of the Navy, entitled \"Program Evaluation Research Task, Summary Report, Phase 1.\" and \"Phase 2.\" In a 1959 article in \"The American Statistician\" the main Willard Fazar, Head of the Program Evaluation Branch, Special Projects Office, U.S. Navy, gave a detailed description of the main concepts of the PERT. He explained:\n\nTen years after the introduction of PERT in 1958 the American librarian Maribeth Brennan published a selected bibliography with about 150 publications on PERT and CPM, which had been published between 1958 and 1968. The origin and development was summarized as follows:\nFor the subdivision of work units in PERT another tool was developed: the Work Breakdown Structure. The Work Breakdown Structure provides \"a framework for complete networking, the Work Breakdown Structure was formally introduced as the first item of analysis in carrying out basic PERT/COST.\"\n\nIn a PERT diagram, the main building block is the \"event\", with connections to its known predecessor events and successor events.\n\nBesides events, PERT also knows activities and sub-activities:\n\nPERT has defined four types of time required to accomplish an activity:\n\nPERT supplies a number of tools for management with determination of concepts, such as:\n\nThe first step to scheduling the project is to determine the tasks that the project requires and the order in which they must be completed. The order may be easy to record for some tasks e.g.When building a house, the land must be graded before the foundation can be laid) while difficult for others (there are two areas that need to be graded, but there are only enough bulldozers to do one). Additionally, the time estimates usually reflect the normal, non-rushed time. Many times, the time required to execute the task can be reduced for an additional cost or a reduction in the quality.\n\nIn the following example there are seven tasks, labeled \"A\" through \"G\". Some tasks can be done concurrently (\"A\" and \"B\") while others cannot be done until their predecessor task is complete (\"C\" cannot begin until \"A\" is complete). Additionally, each task has three time estimates: the optimistic time estimate (\"o\"), the most likely or normal time estimate (\"m\"), and the pessimistic time estimate (\"p\"). The expected time (\"te\") is computed using the formula (\"o\" + 4\"m\" + \"p\") ÷ 6.\n\nOnce this step is complete, one can draw a Gantt chart or a network diagram.\n\nA network diagram can be created by hand or by using diagram software. There are two types of network diagrams, activity on arrow (AOA) and activity on node (AON). Activity on node diagrams are generally easier to create and interpret. To create an AON diagram, it is recommended (but not required) to start with a node named \"start\". This <nowiki>\"activity\"</nowiki> has a duration of zero (0). Then you draw each activity that does not have a predecessor activity (\"a\" and \"b\" in this example) and connect them with an arrow from start to each node. Next, since both \"c\" and \"d\" list \"a\" as a predecessor activity, their nodes are drawn with arrows coming from \"a\". Activity \"e\" is listed with \"b\" and \"c\" as predecessor activities, so node \"e\" is drawn with arrows coming from both \"b\" and \"c\", signifying that \"e\" cannot begin until both \"b\" and \"c\" have been completed. Activity \"f\" has \"d\" as a predecessor activity, so an arrow is drawn connecting the activities. Likewise, an arrow is drawn from \"e\" to \"g\". Since there are no activities that come after \"f\" or \"g\", it is recommended (but again not required) to connect them to a node labeled \"finish\".\n\nBy itself, the network diagram pictured above does not give much more information than a Gantt chart; however, it can be expanded to display more information. The most common information shown is:\n\nIn order to determine this information it is assumed that the activities and normal duration times are given. The first step is to determine the ES and EF. The ES is defined as the maximum EF of all predecessor activities, unless the activity in question is the first activity, for which the ES is zero (0). The EF is the ES plus the task duration (EF = ES + duration).\n\n\nBarring any unforeseen events, the project should take 19.51 work days to complete. The next step is to determine the late start (LS) and late finish (LF) of each activity. This will eventually show if there are activities that have slack. The LF is defined as the minimum LS of all successor activities, unless the activity is the last activity, for which the LF equals the EF. The LS is the LF minus the task duration (LS = LF − duration).\n\n\nThe next step is to determine the critical path and if any activities have slack. The critical path is the path that takes the longest to complete. To determine the path times, add the task durations for all available paths. Activities that have slack can be delayed without changing the overall time of the project. Slack is computed in one of two ways, slack = LF − EF \"or\" slack = LS − ES. Activities that are on the critical path have a slack of zero (0).\n\n\nThe critical path is \"aceg\" and the critical time is 19.51 work days. It is important to note that there can be more than one critical path (in a project more complex than this example) or that the critical path can change. For example, let's say that activities \"d\" and \"f\" take their pessimistic (b) times to complete instead of their expected (T) times. The critical path is now \"adf\" and the critical time is 22 work days. On the other hand, if activity \"c\" can be reduced to one work day, the path time for \"aceg\" is reduced to 15.34 work days, which is slightly less than the time of the new critical path, \"beg\" (15.67 work days).\n\nAssuming these scenarios do not happen, the slack for each activity can now be determined.\n\n\nTherefore, activity \"b\" can be delayed almost 4 work days without delaying the project. Likewise, activity \"d\" or activity \"f\" can be delayed 4.68 work days without delaying the project (alternatively, \"d\" and \"f\" can be delayed 2.34 work days each).\n\n\n\nDuring project execution, however, a real-life project will never execute exactly as it was planned due to uncertainty. This can be due to ambiguity resulting from subjective estimates that are prone to human errors or can be the result of variability arising from unexpected events or risks. The main reason that PERT may provide inaccurate information about the project completion time is due to this schedule uncertainty. This inaccuracy may be large enough to render such estimates as not helpful.\n\nOne possible method to maximize solution robustness is to include safety in the baseline schedule in order to absorb the anticipated disruptions. This is called \"proactive scheduling\". A pure proactive scheduling is a utopia; incorporating safety in a baseline schedule which allows for every possible disruption would lead to a baseline schedule with a very large make-span. A second approach, termed \"reactive scheduling\", consists of defining a procedure to react to disruptions that cannot be absorbed by the baseline schedule.\n"}
{"id": "1560204", "url": "https://en.wikipedia.org/wiki?curid=1560204", "title": "Proof-of-work system", "text": "Proof-of-work system\n\nA Proof-of-Work (PoW) system (or protocol, or function) is an economic measure to deter denial of service attacks and other service abuses such as spam on a network by requiring some work from the service requester, usually meaning processing time by a computer. The concept was invented by Cynthia Dwork and Moni Naor as presented in a 1993 journal article. The term \"Proof of Work\" or PoW was first coined and formalized in a 1999 paper by Markus Jakobsson and Ari Juels. An early example of the proof-of-work system used to give value to a currency is the shell money of the Solomon Islands.\n\nA key feature of these schemes is their asymmetry: the work must be moderately hard (but feasible) on the requester side but easy to check for the service provider. This idea is also known as a CPU cost function, client puzzle, computational puzzle or CPU pricing function. It is distinct from a CAPTCHA, which is intended for a human to solve quickly, rather than a computer. Proof of space (PoSpace) proposals apply the same principle by proving a dedicated amount of memory or disk space instead of CPU time. Proof-of-Stake, Proof of bandwidth, and other approaches have been discussed in the context of cryptocurrency. Proof of ownership aims at proving that specific data are held by the prover.\n\nOne popular system, used in Hashcash, uses partial hash inversions to prove that work was done, as a good-will token to send an e-mail. For instance the following header represents about 2 hash computations to send a message to codice_1 on January 19, 2038:\n\nIt is verified with a single computation by checking that the SHA-1 hash of the stamp (omit the header name codice_2 including the colon and any amount of whitespace following it up to the digit '1') begins with 52 binary zeros, that is 13 hexadecimal zeros:\n\nWhether PoW systems can actually solve a particular denial-of-service issue such as the spam problem is subject to debate;\nthe system must make sending spam emails obtrusively unproductive for the spammer, but should also not prevent legitimate users from sending their messages. In other words, a genuine user should not encounter any difficulties when sending an email, but an email spammer would have to expend a considerable amount of computing power to send out many emails at once. Proof-of-work systems are being used as a primitive by other more complex cryptographic systems such as bitcoin which uses a system similar to Hashcash.\n\nThere are two classes of proof-of-work protocols.\n\n\n\nKnown-solution protocols tend to have slightly lower variance than unbounded probabilistic protocols because the variance of a rectangular distribution is lower than the variance of a Poisson distribution (with the same mean). A generic technique for reducing variance is to use multiple independent sub-challenges, as the average of multiple samples will have a lower variance.\n\nThere are also fixed-cost functions such as the time-lock puzzle.\n\nMoreover, the underlying functions used by these schemes may be:\n\nFinally, some PoW systems offer shortcut computations that allow participants who know a secret, typically a private key, to generate cheap POWs. The rationale is that mailing-list holders may generate stamps for every recipient without incurring a high cost. Whether such a feature is desirable depends on the usage scenario.\n\nHere is a list of known proof-of-work functions:\n\n\nComputer scientist Hal Finney built on the proof-of-work idea, yielding a system that exploited reusable proof of work (\"RPOW\").\nThe idea of making proofs-of-work reusable for some practical purpose had already been established in 1999. Finney's purpose for RPoW was as token money. Just as a gold coin's value is thought to be underpinned by the value of the raw gold needed to make it, the value of an RPoW token is guaranteed by the value of the real-world resources required to 'mint' a PoW token. In Finney's version of RPoW, the PoW token is a piece of Hashcash.\n\nA website can demand a PoW token in exchange for service. Requiring a PoW token from users would inhibit frivolous or excessive use of the service, sparing the service's underlying resources, such as bandwidth to the Internet, computation, disk space, electricity and administrative overhead.\n\nFinney's RPoW system differed from a PoW system in permitting the random exchange of tokens without repeating the work required to generate them. After someone had \"spent\" a PoW token at a website, the website's operator could exchange that \"spent\" PoW token for a new, unspent RPoW token, which could then be spent at some third-party website similarly equipped to accept RPoW tokens. This would save the resources otherwise needed to 'mint' a PoW token. The anti-counterfeit property of the RPoW token was guaranteed by remote attestation. The RPoW server that exchanges a used PoW or RPoW token for a new one of equal value uses remote attestation to allow any interested party to verify what software is running on the RPoW server. Since the source code for Finney's RPoW software was published (under a BSD-like license), any sufficiently knowledgeable programmer could, by inspecting the code, verify that the software (and, by extension, the RPoW server) never issued a new token except in exchange for a spent token of equal value.\n\nUntil 2009, Finney's system was the only RPoW system to have been implemented; it never saw economically significant use.\n\nRPoW is protected by the private keys stored in the trusted platform module (TPM) hardware and manufacturers holding TPM private keys. Stealing a TPM manufacturer's key or obtaining the key by examining the TPM chip itself would subvert that assurance.\n\nIn 2009, the Bitcoin network went online. Bitcoin is a proof-of-work cryptocurrency that, like Finney's RPoW, is also based on the Hashcash PoW. But in Bitcoin double-spend protection is provided by a decentralized P2P protocol for tracking transfers of coins, rather than the hardware trusted computing function used by RPoW. Bitcoin has better trustworthiness because it is protected by computation. Bitcoins are \"mined\" using the Hashcash proof-of-work function by individual miners and verified by the decentralized nodes in the P2P bitcoin network.\n\nThe difficulty is periodically adjusted to keep the block time around a target time.\n\nWithin the Bitcoin community there are groups working together in mining pools. Some miners use ASICs (Application-Specific Integrated Circuit) for PoW.\n\n\n"}
{"id": "8088939", "url": "https://en.wikipedia.org/wiki?curid=8088939", "title": "Real point", "text": "Real point\n\nIn geometry, a real point is a point in the complex projective plane with homogeneous coordinates for which there exists a nonzero complex number such that , , and are all real numbers.\n\nThis definition can be widened to a complex projective space of arbitrary finite dimension as follows:\n\nare the homogeneous coordinates of a real point if there exists a nonzero complex number such that the coordinates of\n\nare all real.\n\nGeometries that are specializations of real projective geometry, such as Euclidean geometry, elliptic geometry or conformal geometry may be complexified, thus embedding the points of the geometry in a complex projective space, but retaining the identity of the original real space as special. Lines, planes etc. are expanded to the lines, etc. of the complex projective space. As with the inclusion of points at infinity and complexification of real polynomials, this allows some theorems to be stated more simply without exceptions and for a more regular algebraic analysis of the geometry.\n\nViewed in terms of homogeneous coordinates, a real vector space of homogeneous coordinates of the original geometry is complexified. A point of the original geometric space is defined by an equivalence class of homogeneous vectors of the form , where is an nonzero complex value and is a real vector. A point of this form (and hence belongs to the original real space) is called a \"real point\", whereas a point that has been added through the complexification and thus does not have this form is called an \"imaginary point\".\n\n"}
{"id": "20048578", "url": "https://en.wikipedia.org/wiki?curid=20048578", "title": "Regiomontanus' angle maximization problem", "text": "Regiomontanus' angle maximization problem\n\nIn mathematics, the Regiomontanus's angle maximization problem, is a famous optimization problem posed by the 15th-century German mathematician Johannes Müller (also known as Regiomontanus). The problem is as follows:\n\nIf the viewer stands too close to the wall or too far from the wall, the angle is small; somewhere in between it is as large as possible.\n\nThe same approach applies to finding the optimal place from which to kick a ball in rugby. For that matter, it is not necessary that the alignment of the picture be at right angles: we might be looking at a window of the Leaning Tower of Pisa or a realtor showing off the advantages of a sky-light in a sloping attic roof.\n\nThere is a unique circle passing through the top and bottom of the painting and tangent to the eye-level line. By elementary geometry, if the viewer's position were to move along the circle, the angle subtended by the painting would remain constant. All positions on the eye-level line except the point of tangency are outside of the circle, and therefore the angle subtended by the painting from those points is smaller.\n\nBy \"Elements\" III.36 (alternatively the power-of-a-point theorem), the distance from the wall to the point of tangency is the geometric mean of the heights of the top and bottom of the painting. This means, in turn, that if we reflect the bottom of the picture in the line at eye-level and draw the circle with the segment between the top of the picture and this reflected point as diameter, the circle intersects the line at eye-level in the required position (by Elements II.14).\n\nIn the present day, this problem is widely known because it appears as an exercise in many first-year calculus textbooks (for example that of Stewart ).\n\nLet\n\nThe angle we seek to maximize is \"β − α\". The tangent of the angle increases as the angle increases; therefore it suffices to maximize\n\nSince \"b\" − \"a\" is a positive constant, we only need to maximize the fraction that follows it. Differentiating, we get\n\nTherefore the angle increases as \"x\" goes from 0 to and decreases as \"x\" increases from . The angle is therefore as large as possible precisely when \"x\" = , the geometric mean of \"a\" and \"b\".\n\nWe have seen that it suffices to maximize\n\nThis is equivalent to \"minimizing\" the reciprocal:\n\nObserve that this last quantity is equal to\n\nThus we have\n\nThis is as small as possible precisely when the square is 0, and that happens when \"x\" = . Alternatively, we might cite this as an instance of the inequality between the arithmetic and geometric means.\n"}
{"id": "367621", "url": "https://en.wikipedia.org/wiki?curid=367621", "title": "Section (fiber bundle)", "text": "Section (fiber bundle)\n\nIn the mathematical field of topology, a section (or cross section) of a fiber bundle formula_1 is a continuous right inverse of the projection function formula_2. In other words, if formula_1 is a fiber bundle over a base space, formula_4:\n\nthen a section of that fiber bundle is a continuous map,\n\nsuch that\n\nA section is an abstract characterization of what it means to be a graph. The graph of a function formula_9 can be identified with a function taking its values in the Cartesian product formula_10, of formula_11 and formula_12:\n\nLet formula_14 be the projection onto the first factor: formula_15. Then a graph is any function formula_16 for which formula_7.\n\nThe language of fibre bundles allows this notion of a section to be generalized to the case when formula_1 is not necessarily a Cartesian product. If formula_14 is a fibre bundle, then a section is a choice of point formula_20 in each of the fibres. The condition formula_7 simply means that the section at a point formula_22 must lie over formula_22. (See image.)\n\nFor example, when formula_1 is a vector bundle a section of formula_1 is an element of the vector space formula_26 lying over each point formula_27. In particular, a vector field on a smooth manifold formula_28 is a choice of tangent vector at each point of formula_28: this is a \"section\" of the tangent bundle of formula_28. Likewise, a 1-form on formula_28 is a section of the cotangent bundle.\n\nSections, particularly of principal bundles and vector bundles, are also very important tools in differential geometry. In this setting, the base space formula_4 is a smooth manifold formula_28, and formula_1 is assumed to be a smooth fiber bundle over formula_28 (i.e., formula_1 is a smooth manifold and formula_37 is a smooth map). In this case, one considers the space of smooth sections of formula_1 over an open set formula_39, denoted formula_40. It is also useful in geometric analysis to consider spaces of sections with intermediate regularity (e.g., formula_41 sections, or sections with regularity in the sense of Hölder conditions or Sobolev spaces).\n\nFiber bundles do not in general have such \"global\" sections (consider, for example, the fiber bundle over formula_42 with fiber formula_43 obtained by taking the Möbius bundle and removing the zero section), so it is also useful to define sections only locally. A local section of a fiber bundle is a continuous map formula_44 where formula_39 is an open set in formula_4 and formula_47 for all formula_48 in formula_39. If formula_50 is a local trivialization of formula_1, where formula_52 is a homeomorphism from formula_53 to formula_54 (where formula_55 is the fiber), then local sections always exist over formula_39 in bijective correspondence with continuous maps from formula_39 to formula_55. The (local) sections form a sheaf over formula_4 called the sheaf of sections of formula_1.\n\nThe space of continuous sections of a fiber bundle formula_1 over formula_39 is sometimes denoted formula_63, while the space of global sections of formula_1 is often denoted formula_65 or formula_66.\n\nSections are studied in homotopy theory and algebraic topology, where one of the main goals is to account for the existence or non-existence of global sections. An obstruction denies the existence of global sections since the space is too \"twisted\". More precisely, obstructions \"obstruct\" the possibility of extending a local section to a global section due to the space's \"twistedness\". Obstructions are indicated by particular characteristic classes, which are cohomological classes. For example, a principal bundle has a global section if and only if it is trivial. On the other hand, a vector bundle always has a global section, namely the zero section. However, it only admits a nowhere vanishing section if its Euler class is zero.\n\nObstructions to extending local sections may be generalized in the following manner: take a topological space and form a category whose objects are open subsets, and morphisms are inclusions. Thus we use a category to generalize a topological space. We generalize the notion of a \"local section\" using sheaves of abelian groups, which assigns to each object an abelian group (analogous to local sections).\n\nThere is an important distinction here: intuitively, local sections are like \"vector fields\" on an open subset of a topological space. So at each point, an element of a \"fixed\" vector space is assigned. However, sheaves can \"continuously change\" the vector space (or more generally abelian group).\n\nThis entire process is really the global section functor, which assigns to each sheaf its global section. Then sheaf cohomology enables us to consider a similar extension problem while \"continuously varying\" the abelian group. The theory of characteristic classes generalizes the idea of obstructions to our extensions.\n\n\n\n"}
{"id": "16576796", "url": "https://en.wikipedia.org/wiki?curid=16576796", "title": "Shape analysis (digital geometry)", "text": "Shape analysis (digital geometry)\n\nThis article describes shape analysis to analyze and process geometric shapes. \nThe shape analysis described here is related to the statistical analysis of geometric shapes, to shape matching and shape recognition. It applies purely to the geometry of an object, not to the structural analysis that deals with predicted behaviour of mechanical parts.\n\n\"Shape analysis\" is the (mostly) automatic analysis of geometric shapes, for example using a computer to detect similarly shaped objects in a database or parts that fit together. For a computer to automatically analyze and process geometric shapes, the objects have to be represented in a digital form. Most commonly a boundary representation is used to describe the object with its boundary (usually the outer shell, see also 3D model). However, other volume based representations (e.g. constructive solid geometry) or point based representations (point clouds) can be used to represent shape.\n\nOnce the objects are given, either by modeling (computer-aided design), by scanning (3D scanner) or by extracting shape from 2D or 3D images, they have to be simplified before a comparison can be achieved. The simplified representation is often called a \"shape descriptor\" (or fingerprint, signature). These simplified representations try to carry most of the important information, while being easier to handle, to store and to compare than the shapes directly.\nA \"complete shape descriptor\" is a representation that can be used to completely reconstruct the original object (for example the medial axis transform).\n\nShape analysis is used in many application fields:\n\nShape descriptors can be classified by their invariance with respect to the transformations allowed in the associated shape definition. Many descriptors are invariant with respect to \"congruency\", meaning that congruent shapes (shapes that could be translated, rotated and mirrored) will have the same descriptor (for example moment or spherical harmonic based descriptors or Procrustes analysis operating on point clouds).\n\nAnother class of shape descriptors (called \"intrinsic\" shape descriptors) is invariant with respect to isometry. These descriptors do not change with different isometric embeddings of the shape. Their advantage is that they can be applied nicely to deformable objects (e.g. a person in different body postures) as these deformations do not involve much stretching but are in fact near-isometric. Such descriptors are commonly based on geodesic distances measures along the surface of an object or on other isometry invariant characteristics such as the Laplace-Beltrami spectrum (see also spectral shape analysis).\n\nThere are other shape descriptors, such as \"graph-based\" descriptors like the medial axis or the Reeb graph that capture geometric and/or topological information and simplify the shape representation but can not be as easily compared as descriptors that represent shape as a vector of numbers.\n\nFrom this discussion it becomes clear, that different shape descriptors target different aspects of shape and can be used for a specific application. Therefore, depending on the application, it is necessary to analyze how well a descriptor captures the features of interest.\n\n\n\n"}
{"id": "48591344", "url": "https://en.wikipedia.org/wiki?curid=48591344", "title": "Svitlana Mayboroda", "text": "Svitlana Mayboroda\n\nSvitlana Mayboroda (born 1981) is a Ukrainian mathematician who works as a professor of mathematics at the University of Minnesota. Her research concerns harmonic analysis and partial differential equations, including boundary value problems for elliptic partial differential equations. Her work has provided a mathematical explanation for Anderson localization, a phenomenon in physics in which waves are confined to a local region rather than propagating throughout a medium, and with this explanation she can predict the regions in which waves will be confined.\n\nMayboroda was born on June 2, 1981 in Kharkiv. She earned the Ukrainian equivalent of two master's degrees, one in finance and one in applied mathematics, from the University of Kharkiv in 2001, and completed her Ph.D. in 2005 from the University of Missouri under the supervision of Marius Mitrea. After visiting positions at the Australian National University, Ohio State University, and Brown University, she joined the Purdue University faculty in 2008, and moved to the University of Minnesota in 2011.\n\nMayboroda was a Sloan Research Fellow for 2010–2015.\nIn 2013, she became the inaugural winner of the Sadosky Research Prize in Analysis of the Association for Women in Mathematics.\nIn 2015 she was elected as a fellow of the American Mathematical Society. In 2016, she was awarded the first Northrop Professorship at the University of Minnesota.\nShe is an invited speaker at the 2018 International Congress of Mathematicians, speaking in the section on Analysis and Operator Algebras.\n\n \n"}
{"id": "2300687", "url": "https://en.wikipedia.org/wiki?curid=2300687", "title": "Viscosity solution", "text": "Viscosity solution\n\nIn mathematics, the viscosity solution concept was introduced in the early 1980s by Pierre-Louis Lions and Michael G. Crandall as a generalization of the classical concept of what is meant by a 'solution' to a partial differential equation (PDE). It has been found that the viscosity solution is the natural solution concept to use in many applications of PDE's, including for example first order equations arising in optimal control (the Hamilton–Jacobi equation), differential games (the Isaacs equation) or front evolution problems, as well as second-order equations such as the ones arising in stochastic optimal control or stochastic differential games.\n\nThe classical concept was that a PDE\nover a domain formula_2 has a solution if we can find a function \"u\"(\"x\") continuous and differentiable over the entire domain such that formula_3, formula_4, formula_5, formula_6 satisfy the above equation at every point.\n\nIf a scalar equation is degenerate elliptic (defined below), one can define a type of weak solution called \"viscosity solution\".\nUnder the viscosity solution concept, \"u\" need not be everywhere differentiable. There may be points where either formula_5 or formula_6 does not exist and yet \"u\" satisfies the equation in an appropriate generalized sense. The definition allows only for certain kind of singularities, so that existence, uniqueness, and stability under uniform limits, hold for a large class of equations.\n\nThere are several equivalent ways to phrase the definition of viscosity solutions. See for example the section II.4 of Fleming and Soner's book or the definition using semi-jets in the Users Guide.\n\n\n\n\n\nConsider the boundary value problem formula_39, or formula_40, on formula_41 with boundary conditions formula_42. The function formula_43 is the unique viscosity solution. To see this, note that the boundary conditions are satisfied, and formula_39 is well-defined on the interior except at formula_45. Thus, it remains to show that the conditions for subsolution and supersolution hold at formula_46.\n\nFirst, suppose that formula_47 is any function differentiable at formula_46 with formula_49 and formula_50 near formula_46. From these assumptions, it follows that formula_52. For positive formula_3, this inequality implies formula_54, using that formula_55 for formula_56. On the other hand, for formula_57, we have that formula_58. Because formula_25 is differentiable, the left and right limits agree and are equal to formula_60, and we therefore conclude that formula_61, i.e., formula_62. Thus, formula_4 is a subsolution. Moreover, the fact that formula_4 is a supersolution holds vacuously, since there is no function formula_47 differentiable at formula_46 with formula_49 and formula_68 near formula_46. This implies that formula_4 is a viscosity solution.\n\nThe previous boundary value problem is an Eikonal equation in a single spatial dimension with formula_71, where the solution is known to be the signed distance function to the boundary of the domain. Note also in the previous example, the importance of the sign of formula_72. In particular, the viscosity solution to the PDE formula_73 with the same boundary conditions is formula_74. This can be explained by observing that the solution formula_43 is the limiting solution of the vanishing viscosity problem formula_76 as formula_77 goes to zero, while formula_74 is the limit solution of the vanishing viscosity problem formula_79 . One can readily confirm that formula_80 solves the PDE formula_76 for each epsilon. Further, the family of solutions formula_82 converge toward the solution formula_83 as formula_77 vanishes (see Figure).\n\nThe three basic properties of viscosity solutions are \"existence\", \"uniqueness\" and \"stability\".\n\nThe term \"viscosity solutions\" first appear in the work of Michael G. Crandall and Pierre-Louis Lions in 1983 regarding the Hamilton–Jacobi equation. The name is justified by the fact that the existence of solutions was obtained by the vanishing viscosity method. The definition of solution had actually been given earlier by Lawrence C. Evans in 1980. Subsequently the definition and properties of viscosity solutions for the Hamilton–Jacobi equation were refined in a joint work by Crandall, Evans and Lions in 1984.\n\nFor a few years the work on viscosity solutions concentrated on first order equations because it was not known whether second order elliptic equations would have a unique viscosity solution except in very particular cases. The breakthrough result came with the method introduced by Robert Jensen in 1988 to prove the comparison principle using a regularized approximation of the solution which has a second derivative almost everywhere (in modern versions of the proof this is achieved with sup-convolutions and Alexandrov theorem).\n\nIn subsequent years the concept of viscosity solution has become increasingly prevalent in analysis of degenerate elliptic PDE. Based on their stability properties, Barles and Souganidis obtained a very simple and general proof of convergence of finite difference schemes. Further regularity properties of viscosity solutions were obtained, especially in the uniformly elliptic case with the work of Luis Caffarelli. Viscosity solutions have become a central concept in the study of elliptic PDE.\n\nIn the modern approach, the existence of solutions is obtained most often through the Perron method. The vanishing viscosity method is not practical for second order equations in general since the addition of artificial viscosity does not guarantee the existence of a classical solution. Moreover, the definition of \"viscosity solutions\" does not involve any viscosity of any kind. The theory of viscosity solutions is completely unrelated to viscous fluids. Thus, it has been suggested that the name \"viscosity solution\" does not represent the concept appropriately. Yet, the name persists because of the history of the subject. Other names that were suggested were \"Crandall–Lions solutions\", in honor to their pioneers, \"formula_92-weak solutions\", referring to their stability properties, or \"comparison solutions\", referring to their most characteristic property.\n\n"}
{"id": "625507", "url": "https://en.wikipedia.org/wiki?curid=625507", "title": "Windows Calculator", "text": "Windows Calculator\n\nWindows Calculator is a software calculator included in all versions of Windows.\n\nA simple arithmetic calculator was first included with Windows 1.0.\n\nIn Windows 3.0, a scientific mode was added, which included exponents and roots, logarithms, factorial-based functions, trigonometry (supports radian, degree and gradians angles), base conversions (2, 8, 10, 16), logic operations, statistical functions such as single variable statistics and linear regression.\n\nIn Windows 98 and later, it uses an arbitrary-precision arithmetic library, replacing the standard IEEE floating point library. It offers bignum precision for basic operations (addition, subtraction, multiplication, division) and 32 digits of precision for advanced operations (square root, transcendental functions). The largest value that can be represented on the Windows Calculator is currently and the smallest is . (Also ! calculates Gamma function not just factorial so one can get 4.7! ).\n\nIn Windows 2000, digit grouping is added. Degree and base settings are added to menu bar.\n\nIn Windows 7, separate programmer, statistics, unit conversion, date calculation and worksheets modes were added. Tooltips were removed. Furthermore, Calculator's interface was revamped for the first time since its introduction. The base conversion functions were moved to the programmer mode and statistics functions were moved to the statistics mode. Switching between modes does not preserve the current number, clearing it to 0.\n\nIn every mode except programmer mode, one can see the history of calculations. The app was redesigned to accommodate multi-touch. Standard mode behaves as a simple checkbook calculator; entering the sequence formula_1 gives the answer 25. In scientific mode, order of operations is followed while doing calculations (multiplication and division are done before addition and subtraction), which means formula_2.\n\nIn programmer mode, inputting a number in decimal has a lower and upper limit, depending on the data type, and must always be an integer. Data type of number in decimal mode is signed n-bit integer when converting from number in hexadecimal, octal, or binary mode.\nOn the right of the main Calculator, one can add a panel with date calculation, unit conversion and worksheets. Worksheets allow one to calculate a result of a chosen field based on the values of other fields. Pre-defined templates include calculating a car's fuel economy (mpg and L/100 km), a vehicle lease, and a mortgage. In pre-beta versions of Windows 7, Calculator also provided a Wages template.\n\nWhile the traditional Calculator is still included with Windows 8.1, a Metro-style Calculator is also present, featuring a full-screen interface as well as normal, scientific, and conversion modes.\n\nThe Calculator in non-LTSB editions of Windows 10 is a Universal Windows Platform app. In contrast, Windows 10 LTSC (which does not include universal Windows apps) includes the traditional calculator, but which is now named . Both calculators provide the features of the traditional calculator included with Windows 7, such as a unit conversions for volume, length, weight, temperature, energy, area, speed, time, power, data, pressure and angle, and the history list which the user can clear. \n\nBoth the universal Windows app and LTSB's register themselves with the system as handlers of a \" pseudo-protocol. This registration is similar to that performed by any other well-behaved application when it registers itself as a handler for a filetype (e.g. ) or protocol (e.g. ).\n\nAll Windows 10 editions (both LTSB and non-LTSB) continue to have a , which however is just a stub that launches (via ShellExecute) the handler that is associated with the \" pseudo-protocol. As with any other protocol or filetype, when there are multiple handlers to choose from, users are free to choose which handler they prefer either via the classic control panel ('Default programs' settings) or the immersive UI settings ('Default Apps' settings) or from the command prompt via .\n\nBy default, Calculator runs in standard mode, which resembles a four-function calculator. More advanced functions are available in scientific mode, including logarithms, numerical base conversions, some logical operators, operator precedence, radian, degree and gradians support as well as simple single-variable statistical functions. It does not provide support for user-defined functions, complex numbers, storage variables for intermediate results (other than the classic accumulator memory of pocket calculators), automated polar-cartesian coordinates conversion, or support for two-variables statistics.\n\nCalculator supports keyboard shortcuts; all Calculator features have an associated keyboard shortcut.\n\nCalculator in hexadecimal mode cannot accept or display a hexadecimal number larger than 16 hex digits. The largest number it can handle is therefore 0xFFFFFFFFFFFFFFFF (decimal 18,446,744,073,709,551,615). Any calculations in hex mode which exceed this limit will display a result of zero, even if those calculations would succeed in other modes. In particular, scientific notation is not available in this mode.\n\nCalculator Plus is a separate application for Windows XP and Windows Server 2003 users that adds a 'Conversion' mode over the Windows XP version of the Calculator. The 'Conversion' mode supports unit conversion and currency conversion. Currency exchange rates can be updated using the built-in update feature, which downloads exchange rates from the European Central Bank.\n\n\n"}
{"id": "19309289", "url": "https://en.wikipedia.org/wiki?curid=19309289", "title": "Zlil Sela", "text": "Zlil Sela\n\nZlil Sela is an Israeli mathematician working in the area of geometric group theory.\nHe is a Professor of Mathematics at the Hebrew University of Jerusalem. Sela is known for the solution of the isomorphism problem for torsion-free word-hyperbolic groups and for the solution of the Tarski conjecture about equivalence of first order theories of finitely generated non-abelian free groups.\n\nSela received his Ph.D. in 1991 from the Hebrew University of Jerusalem, where his doctoral advisor was Eliyahu Rips.\nPrior to his current appointment at the Hebrew University, he held an Associate Professor position at Columbia University in New York. While at Columbia, Sela won the Sloan Fellowship from the Sloan Foundation.\n\nSela gave an Invited Address at the 2002 International Congress of Mathematicians in Beijing. He gave a plenary talk at the 2002 annual meeting of the Association for Symbolic Logic,\nand he delivered an AMS Invited Address at the October 2003 meeting of the American Mathematical Society and the 2005 Tarski Lectures at the University of California at Berkeley.\nHe was also awarded the 2003 Erdős Prize from the Israel Mathematical Union.\nSela also received the 2008 Carol Karp Prize from the Association for Symbolic Logic for his work on the Tarski conjecture and on discovering and developing new connections between model theory and geometric group theory.\n\nSela's early important work was his solution in mid-1990s of the isomorphism problem for torsion-free word-hyperbolic groups. The machinery of group actions on real trees, developed by Eliyahu Rips, played a key role in Sela's approach. The solution of the isomorphism problem also relied on the notion of \"canonical representatives\" for elements of hyperbolic groups, introduced by Rips and Sela in a joint 1995 paper. The machinery of the canonical representatives allowed Rips and Sela to prove algorithmic solvability of finite systems of equations in torsion-free hyperbolic groups, by reducing the problem to solving equations in free groups where the Makanin-Razborov algorithm can be applied. The technique of canonical representatives was later generalized by Dahmani to the case of relatively hyperbolic groups and played a key role in the solution of the isomorphism problem for \"toral\" relatively hyprbolic groups.\n\nIn his work on the isomorphism problem Sela also introduced and developed the notion of a JSJ-decomposition for word-hyperbolic groups, motivated by the notion of a JSJ decomposition for 3-manifolds. A JSJ-decomposition is a representation of a word-hyperbolic group as the fundamental group of a graph of groups which encodes in a canonical way all possible splittings over infinite cyclic subgroups. The idea of JSJ-decomposition was later extended by Rips and Sela to torsion-free finitely presented groups and this work gave rise a systematic development of the JSJ-decomposition theory with many further extensions and generalizations by other mathematicians. Sela applied a combination of his JSJ-decomposition and real tree techniques to prove that torsion-free word-hyperbolic groups are Hopfian. This result and Sela's approach were later generalized by others to finitely generated subgroups of hyperbolic groups and to the setting of relatively hyperbolic groups.\n\nSela's most important work came in early 2000s when he produced a solution to a famous Tarski conjecture. Namely, in a long series of papers, he proved that any two non-abelian finitely generated free groups have the same first-order theory. Sela's work relied on applying his earlier JSJ-decomposition and real tree techniques as well as developing new ideas and machinery of \"algebraic geometry\" over free groups.\n\nSela pushed this work further to study first-order theory of arbitrary torsion-free word-hyperbolic groups and to characterize all groups that are elementarily equivalent to (that is, have the same first order theory as) a given torsion-free word-hyperbolic group. In particular, his work implies that if a finitely generated group \"G\" is elementarily equivalent to a word-hyperbolic group then \"G\" is word-hyperbolic as well.\n\nSela also proved that the first order theory of a finitely generated free group is stable in the model-theoretic sense, providing a brand-new and qualitatively different source of examples for the stability theory.\n\nAn alternative solution for the Tarski conjecture has been presented by Olga Kharlampovich and Alexei Myasnikov.\n\nThe work of Sela on first-order theory of free and word-hyperbolic groups substantially influenced the development of geometric group theory, in particular by stimulating the development and the study of the notion of limit groups and of relatively hyperbolic groups.\n\n\n\n"}
{"id": "31708940", "url": "https://en.wikipedia.org/wiki?curid=31708940", "title": "Zyablov bound", "text": "Zyablov bound\n\nIn coding theory, the Zyablov bound is a lower bound on the rate formula_1 and relative distance formula_2 of concatenated codes.\n\nLet formula_1 be the rate of the outer code formula_4 and formula_2 be the relative distance, then the rate of the concatenated codes satisfies the following bound.\n\nwhere formula_7 is the rate of the inner code formula_8.\n\nLet formula_4 be the outer code, formula_8 be the inner code.\n\nConsider formula_4 meets the Singleton bound with rate of formula_1, i.e. formula_4 has relative distance formula_14 In order for formula_15 to be an asymptotically good code, formula_8 also needs to be an asymptotically good code which means, formula_8 needs to have rate formula_18 and relative distance formula_19.\n\nSuppose formula_8 meets the Gilbert-Varshamov bound with rate of formula_7 and thus with relative distance\n\nthen formula_15 has rate of formula_24 and formula_25\n\nExpressing formula_1 as a function of formula_27\n\nThen optimizing over the choice of r, we get that rate of the Concatenated error correction code satisfies,\n\nThis lower bound is called Zyablov bound (the bound of formula_30 is necessary to ensure formula_31). See Figure 2 for a plot of this bound.\n\nNote that the Zyablov bound implies that for every formula_32, there exists a (concatenated) code with rate formula_33\n\nWe can construct a code that achieves the Zyablov bound in polynomial time. In particular, we can construct explicit asymptotically good code (over some alphabets) in polynomial time.\n\nLinear Codes will help us complete the proof of the above statement since linear codes have polynomial representation. Let Cout be an formula_34 Reed-Solomon error correction code where formula_35 (evaluation points being formula_36 with formula_37, then formula_38.\n\nWe need to construct the Inner code that lies on Gilbert-Varshamov bound. This can be done in two ways\n\n\nThus we can construct a code that achieves the Zyablov bound in polynomial time.\n\n\n"}
