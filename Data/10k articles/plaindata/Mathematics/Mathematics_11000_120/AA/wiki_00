{"id": "56963959", "url": "https://en.wikipedia.org/wiki?curid=56963959", "title": "A Primer of Real Functions", "text": "A Primer of Real Functions\n\nA Primer of Real Functions is a revised edition of a classic Carus Monograph on the theory of functions of a real variable. It is authored by R. P. Boas, Jr and updated by his son Harold P. Boas.\n\n"}
{"id": "57921059", "url": "https://en.wikipedia.org/wiki?curid=57921059", "title": "Abraham Joseph Menz", "text": "Abraham Joseph Menz\n\nAbraham Joseph ben Simon Wolf Menz was an eighteenth century rabbi and mathematician at Frankfurt.\n\nHe wrote an elementary textbook on mathematics entitled \"Reshit Limmudim\", in three parts: \"Kelale Handasah,\" the general rules of algebra; \"Yesodot ha-Gemaṭriot,\" the elements of geometry; and \"Yesod ha-Tekunah,\" on astronomy.\n\n"}
{"id": "2402", "url": "https://en.wikipedia.org/wiki?curid=2402", "title": "Albrecht Dürer", "text": "Albrecht Dürer\n\nAlbrecht Dürer (; ; 21 May 1471 – 6 April 1528) was a painter, printmaker, and theorist of the German Renaissance. Born in Nuremberg, Dürer established his reputation and influence across Europe when he was still in his twenties due to his high-quality woodcut prints.\nHe was in communication with the major Italian artists of his time, including Raphael, Giovanni Bellini and Leonardo da Vinci, and from 1512 he was patronized by Emperor Maximilian I. Dürer is commemorated by both the Lutheran and Episcopal Churches.\n\nDürer's vast body of work includes engravings, his preferred technique in his later prints, altarpieces, portraits and self-portraits, watercolours and books. The woodcuts, such as the \"Apocalypse\" series (1498), are more Gothic than the rest of his work. \nHis well-known engravings include the \"Knight, Death, and the Devil\" (1513), \"Saint Jerome in his Study\" (1514) and \"Melencolia I\" (1514), which has been the subject of extensive analysis and interpretation. His watercolours also mark him as one of the first European landscape artists, while his ambitious woodcuts revolutionized the potential of that medium.\n\nDürer's introduction of classical motifs into Northern art, through his knowledge of Italian artists and German humanists, has secured his reputation as one of the most important figures of the Northern Renaissance. This is reinforced by his theoretical treatises, which involve principles of mathematics, perspective, and ideal proportions.\n\nDürer was born on 21 May 1471, third child and second son of his parents, who had at least fourteen and possibly as many as eighteen children. His father, Albrecht Dürer the Elder (originally Albrecht Ajtósi), was a successful goldsmith who in 1455 had moved to Nuremberg from , near Gyula in Hungary. One of Albrecht's brothers, Hans Dürer, was also a painter and trained under him. Another of Albrecht's brothers, Endres Dürer, took over their father's business and was a master goldsmith. The German name \"Dürer\" is a translation from the Hungarian, \"Ajtósi\". Initially, it was \"Türer\", meaning doormaker, which is \"ajtós\" in Hungarian (from \"ajtó\", meaning door). A door is featured in the coat-of-arms the family acquired. Albrecht Dürer the Younger later changed \"Türer\", his father's diction of the family's surname, to \"Dürer\", to adapt to the local Nuremberg dialect. Dürer the Elder married Barbara Holper, daughter of his master when he himself qualified as a master in 1467.\n\nDürer's godfather was Anton Koberger, who left goldsmithing to become a printer and publisher in the year of Dürer's birth, and became the most successful publisher in Germany, eventually owning twenty-four printing-presses and built a number of offices in Germany and abroad. Koberger's most famous publication was the \"Nuremberg Chronicle\", published in 1493 in German and Latin editions. It contained an unprecedented 1,809 woodcut illustrations (albeit with many repeated uses of the same block) by the Wolgemut workshop. Dürer may have worked on some of these, as the work on the project began while he was with Wolgemut.\n\nBecause Dürer left autobiographical writings and became very famous by his mid-twenties, his life is well documented by several sources. After a few years of school, Dürer started to learn the basics of goldsmithing and drawing from his father. Though his father wanted him to continue his training as a goldsmith, he showed such a precocious talent in drawing that he started as an apprentice to Michael Wolgemut at the age of fifteen in 1486. A self-portrait, a drawing in silverpoint, is dated 1484 (Albertina, Vienna) \"when I was a child\", as his later inscription says. Wolgemut was the leading artist in Nuremberg at the time, with a large workshop producing a variety of works of art, in particular woodcuts for books. Nuremberg was then an important and prosperous city, a centre for publishing and many luxury trades. It had strong links with Italy, especially Venice, a relatively short distance across the Alps.\n\nAfter completing his apprenticeship, Dürer followed the common German custom of taking \"Wanderjahre\"—in effect gap years—in which the apprentice learned skills from artists in other areas; Dürer was to spend about four years away. He left in 1490, possibly to work under Martin Schongauer, the leading engraver of Northern Europe, but who died shortly before Dürer's arrival at Colmar in 1492. It is unclear where Dürer travelled in the intervening period, though it is likely that he went to Frankfurt and the Netherlands. In Colmar, Dürer was welcomed by Schongauer's brothers, the goldsmiths Caspar and Paul and the painter Ludwig. In 1493 Dürer went to Strasbourg, where he would have experienced the sculpture of Nikolaus Gerhaert. Dürer's first painted self-portrait (now in the Louvre) was painted at this time, probably to be sent back to his fiancée in Nuremberg.\n\nIn early 1492 Dürer travelled to Basel to stay with another brother of Martin Schongauer, the goldsmith Georg. Very soon after his return to Nuremberg, on 7 July 1494, at the age of 23, Dürer was married to Agnes Frey following an arrangement made during his absence. Agnes was the daughter of a prominent brass worker (and amateur harpist) in the city. However, no children resulted from the marriage, and with Albrecht the Dürer name died out. The marriage between Agnes and Albrecht was not a generally happy one, as indicated by the letters of Dürer in which he quipped to Willibald Pirckheimer in an extremely rough tone about his wife. He called her an \"old crow\" and made other vulgar remarks. Pirckheimer also made no secret of his antipathy towards Agnes, describing her as a miserly shrew with a bitter tongue, who helped cause Dürer's death at a young age. It is speculated by many scholars Albrecht was bisexual, if not homosexual, due to several of his works containing themes of homosexual desire, as well as the intimate nature of his correspondence with certain very close male friends.\n\nWithin three months of his marriage, Dürer left for Italy, alone, perhaps stimulated by an outbreak of plague in Nuremberg. He made watercolour sketches as he traveled over the Alps. Some have survived and others may be deduced from accurate landscapes of real places in his later work, for example his engraving \"Nemesis\".\n\nIn Italy, he went to Venice to study its more advanced artistic world. Through Wolgemut's tutelage, Dürer had learned how to make prints in drypoint and design woodcuts in the German style, based on the works of Martin Schongauer and the Housebook Master. He also would have had access to some Italian works in Germany, but the two visits he made to Italy had an enormous influence on him. He wrote that Giovanni Bellini was the oldest and still the best of the artists in Venice. His drawings and engravings show the influence of others, notably Antonio Pollaiuolo, with his interest in the proportions of the body; Lorenzo di Credi; and Andrea Mantegna, whose work he produced copies of while training. Dürer probably also visited Padua and Mantua on this trip.\n\nOn his return to Nuremberg in 1495, Dürer opened his own workshop (being married was a requirement for this). Over the next five years his style increasingly integrated Italian influences into underlying Northern forms. Dürer's father died in 1502, and his mother died in 1514. Arguably his best works in the first years of the workshop were his woodcut prints, mostly religious, but including secular scenes such as \"The Men's Bath House\" (ca. 1496). These were larger and more finely cut than the great majority of German woodcuts hitherto, and far more complex and balanced in composition.\n\nIt is now thought unlikely that Dürer cut any of the woodblocks himself; this task would have been performed by a specialist craftsman. However, his training in Wolgemut's studio, which made many carved and painted altarpieces and both designed and cut woodblocks for woodcut, evidently gave him great understanding of what the technique could be made to produce, and how to work with block cutters. Dürer either drew his design directly onto the woodblock itself, or glued a paper drawing to the block. Either way, his drawings were destroyed during the cutting of the block.\n\nHis famous series of sixteen great designs for the \"Apocalypse\" is dated 1498, as is his engraving of\" St. Michael Fighting the Dragon\". He made the first seven scenes of the \"Great Passion\" in the same year, and a little later, a series of eleven on the Holy Family and saints. The \"Seven Sorrows Polyptych\", commissioned by Frederick III of Saxony in 1496, was executed by Dürer and his assistants c. 1500. Around 1503–1505 he produced the first seventeen of a set illustrating the \"Life of the Virgin\", which he did not finish for some years. Neither these, nor the \"Great Passion,\" were published as sets until several years later, but prints were sold individually in considerable numbers.\n\nDuring the same period Dürer trained himself in the difficult art of using the burin to make engravings. It is possible he had begun learning this skill during his early training with his father, as it was also an essential skill of the goldsmith. In 1496 he executed the \"Prodigal Son\", which the Italian Renaissance art historian Giorgio Vasari singled out for praise some decades later, noting its Germanic quality. He was soon producing some spectacular and original images, notably \"Nemesis\" (1502), \"The Sea Monster\" (1498), and \"Saint Eustace\" (c. 1501), with a highly detailed landscape background and animals. His landscapes of this period, such as \"Pond in the Woods\" and \"Willow Mill\", are quite different from his earlier watercolours. There is a much greater emphasis on capturing atmosphere, rather than depicting topography. He made a number of Madonnas, single religious figures, and small scenes with comic peasant figures. Prints are highly portable and these works made Dürer famous throughout the main artistic centres of Europe within a very few years.\n\nThe Venetian artist Jacopo de' Barbari, whom Dürer had met in Venice, visited Nuremberg in 1500, and Dürer said that he learned much about the new developments in perspective, anatomy, and proportion from him. De' Barbari was unwilling to explain everything he knew, so Dürer began his own studies, which would become a lifelong preoccupation. A series of extant drawings show Dürer's experiments in human proportion, leading to the famous engraving of \"Adam and Eve\" (1504), which shows his subtlety while using the burin in the texturing of flesh surfaces. This is the only existing engraving signed with his full name.\n\nDürer created large numbers of preparatory drawings, especially for his paintings and engravings, and many survive, most famously the \"Betende Hände\" (\"Praying Hands\") from circa 1508, a study for an apostle in the Heller altarpiece. He continued to make images in watercolour and bodycolour (usually combined), including a number of still lifes of meadow sections or animals, including his \"Young Hare\" (1502) and the \"Great Piece of Turf\" (1503).\n\nIn Italy, he returned to painting, at first producing a series of works executed in tempera on linen. These include portraits and altarpieces, notably, the Paumgartner altarpiece and the \"Adoration of the Magi\". In early 1506, he returned to Venice and stayed there until the spring of 1507. By this time Dürer's engravings had attained great popularity and were being copied. In Venice he was given a valuable commission from the emigrant German community for the church of San Bartolomeo. This was the altar-piece known as the \"Adoration of the Virgin\" or the \"Feast of Rose Garlands\". It includes portraits of members of Venice's German community, but shows a strong Italian influence. It was subsequently acquired by the Emperor Rudolf II and taken to Prague. Other paintings Dürer produced in Venice include \"The Virgin and Child with the Goldfinch\", \"Christ among the Doctors\" (supposedly produced in a mere five days), and a number of smaller works.\n\nDespite the regard in which he was held by the Venetians, Dürer returned to Nuremberg by mid-1507, remaining in Germany until 1520. His reputation had spread throughout Europe and he was on friendly terms and in communication with most of the major artists including Raphael, Giovanni Bellini and—mainly through Lorenzo di Credi—Leonardo da Vinci.\n\nBetween 1507 and 1511 Dürer worked on some of his most celebrated paintings: \"Adam and Eve\" (1507), \"The Martyrdom of the Ten Thousand\" (1508, for Frederick of Saxony), \"Virgin with the Iris\" (1508), the altarpiece \"Assumption of the Virgin\" (1509, for Jacob Heller of Frankfurt), and \"Adoration of the Trinity\" (1511, for Matthaeus Landauer). During this period he also completed two woodcut series, the Great Passion and the Life of the Virgin, both published in 1511 together with a second edition of the Apocalypse series. The post-Venetian woodcuts show Dürer's development of chiaroscuro modelling effects, creating a mid-tone throughout the print to which the highlights and shadows can be contrasted.\n\nOther works from this period include the thirty-seven woodcut subjects of the Little Passion, published first in 1511, and a set of fifteen small engravings on the same theme in 1512. Indeed, complaining that painting did not make enough money to justify the time spent when compared to his prints, he produced no paintings from 1513 to 1516. However, in 1513 and 1514 Dürer created his three most famous engravings: \"Knight, Death, and the Devil\" (1513, probably based on Erasmus's treatise \"Enchiridion militis Christiani\"), \"St. Jerome in his Study\", and the much-debated \"Melencolia I\" (both 1514). Further outstanding pen and ink drawings of Dürer´s period of art work of 1513 were drafts for his friend Willibald Prickheimer. These drafts were later used to design the famous chandeliers lusterweibchen.\n\nIn 1515, he created his \"woodcut of a Rhinoceros\" which had arrived in Lisbon from a written description and sketch by another artist, without ever seeing the animal himself. An image of the Indian rhinoceros, the image has such force that it remains one of his best-known and was still used in some German school science text-books as late as last century. In the years leading to 1520 he produced a wide range of works, including the woodblocks for the first western printed star charts in 1515 and portraits in tempera on linen in 1516. His only experiments with etching came in this period, producing five 1515–1516 and a sixth 1518; a technique he may have abandoned as unsuited to his aesthetic of methodical, classical form.\n\nFrom 1512, Maximilian I became Dürer's major patron. His commissions included \"The Triumphal Arch\", a vast work printed from 192 separate blocks, the symbolism of which is partly informed by Pirckheimer's translation of Horapollo's \"Hieroglyphica\". The design program and explanations were devised by Johannes Stabius, the architectural design by the master builder and court-painter Jörg Kölderer and the woodcutting itself by Hieronymous Andreae, with Dürer as designer-in-chief. \"The Arch\" was followed by \"The Triumphal Procession\", the program of which was worked out in 1512 by and includes woodcuts by Albrecht Altdorfer and Hans Springinklee, as well as Dürer.\n\nDürer worked with pen on the marginal images for an edition of the Emperor's printed Prayer-Book; these were quite unknown until facsimiles were published in 1808 as part of the first book published in lithography. Dürer's work on the book was halted for an unknown reason, and the decoration was continued by artists including Lucas Cranach the Elder and Hans Baldung. Dürer also made several portraits of the Emperor, including one shortly before Maximilian's death in 1519.\n\nMaximilian's death came at a time when Dürer was concerned he was losing \"my sight and freedom of hand\" (perhaps caused by arthritis) and increasingly affected by the writings of Martin Luther. In July 1520 Dürer made his fourth and last major journey, to renew the Imperial pension Maximilian had given him and to secure the patronage of the new emperor, Charles V, who was to be crowned at Aachen. Dürer journeyed with his wife and her maid via the Rhine to Cologne and then to Antwerp, where he was well received and produced numerous drawings in silverpoint, chalk and charcoal. In addition to going to the coronation, he made excursions to Cologne (where he admired the painting of Stefan Lochner), Nijmegen, 's-Hertogenbosch, Bruges (where he saw Michelangelo's Madonna of Bruges), Ghent (where he admired van Eyck's altarpiece), and Zeeland.\n\nDürer took a large stock of prints with him and wrote in his diary to whom he gave, exchanged or sold them, and for how much. This provides rare information of the monetary value placed on prints at this time. Unlike paintings, their sale was very rarely documented. While providing valuable documentary evidence, Dürer's Netherlandish diary also reveals that the trip was not a profitable one. For example, Dürer offered his last portrait of Maximilian to his daughter, Margaret of Austria, but eventually traded the picture for some white cloth after Margaret disliked the portrait and declined to accept it. During this trip he also met Bernard van Orley, Jan Provoost, Gerard Horenbout, Jean Mone, Joachim Patinir and Tommaso Vincidor, though he did not, it seems, meet Quentin Matsys.\n\nAt the request of Christian II of Denmark, Dürer went to Brussels to paint the King's portrait. There he saw \"the things which have been sent to the king from the golden land\"—the Aztec treasure that Hernán Cortés had sent home to Holy Roman Emperor Charles V following the fall of Mexico. Dürer wrote that this treasure \"was much more beautiful to me than miracles. These things are so precious that they have been valued at 100,000 florins\". Dürer also appears to have been collecting for his own cabinet of curiosities, and he sent back to Nuremberg various animal horns, a piece of coral, some large fish fins, and a wooden weapon from the East Indies.\n\nHaving secured his pension, Dürer finally returned home in July 1521, having caught an undetermined illness—perhaps malaria—which afflicted him for the rest of his life, and greatly reduced his rate of work.\n\nAlbrecht Dürer besides his enormous contribution to painting and science, had also contributed to the development of the German school of fencing. In 1512 he authored “the Book of Fencing”, and being a great painter himself, he expressed himself not in a textual form, but in drawings.\n\nOn his return to Nuremberg, Dürer worked on a number of grand projects with religious themes, including a crucifixion scene and a Sacra conversazione, though neither was completed. This may have been due in part to his declining health, but perhaps also because of the time he gave to the preparation of his theoretical works on geometry and perspective, the proportions of men and horses, and fortification.\n\nHowever, one consequence of this shift in emphasis was that during the last years of his life, Dürer produced comparatively little as an artist. In painting, there was only a portrait of , a , , and two panels showing St. John with St. Peter in and St. Paul with St. Mark in the . This last great work, the Four Apostles, was given by Dürer to the City of Nuremberg—although he was given 100 guilders in return.\n\nAs for engravings, Dürer's work was restricted to portraits and illustrations for his treatise. The portraits include Cardinal-Elector Albert of Mainz; Frederick the Wise, elector of Saxony; the humanist scholar Willibald Pirckheimer; Philipp Melanchthon, and Erasmus of Rotterdam. For those of the Cardinal, Melanchthon, and Dürer's final major work, a drawn portrait of the Nuremberg patrician Ulrich Starck, Dürer depicted the sitters in profile, perhaps reflecting a more mathematical approach.\n\nDespite complaining of his lack of a formal classical education, Dürer was greatly interested in intellectual matters and learned much from his boyhood friend Willibald Pirckheimer, whom he no doubt consulted on the content of many of his images. He also derived great satisfaction from his friendships and correspondence with Erasmus and other scholars. Dürer succeeded in producing two books during his lifetime. \"The Four Books on Measurement\" were published at Nuremberg in 1525 and was the first book for adults on mathematics in German, as well as being cited later by Galileo and Kepler. The other, a work on city fortifications, was published in 1527. \"The Four Books on Human Proportion\" were published posthumously, shortly after his death in 1528.\n\nDürer died in Nuremberg at the age of 56, leaving an estate valued at 6,874 florins – a considerable sum. He is buried in the \"Johannisfriedhof\" cemetery. His large house (purchased in 1509 from the heirs of the astronomer Bernhard Walther), where his workshop was located and where his widow lived until her death in 1539, remains a prominent Nuremberg landmark. It is now a museum.\n\nDürer's writings suggest that he may have been sympathetic to Martin Luther's ideas, though it is unclear if he ever left the Catholic Church. Dürer wrote of his desire to draw Luther in his diary in 1520: \"And God help me that I may go to Dr. Martin Luther; thus I intend to make a portrait of him with great care and engrave him on a copper plate to create a lasting memorial of the Christian man who helped me overcome so many difficulties.\" In a letter to Nicholas Kratzer in 1524, Dürer wrote \"because of our Christian faith we have to stand in scorn and danger, for we are reviled and called heretics\". Most tellingly, Pirckheimer wrote in a letter to Johann Tscherte in 1530: \"I confess that in the beginning I believed in Luther, like our Albert of blessed memory ... but as anyone can see, the situation has become worse.\" Dürer may even have contributed to the Nuremberg City Council's mandating Lutheran sermons and services in March 1525. Notably, Dürer had contacts with various reformers, such as Zwingli, Andreas Karlstadt, Melanchthon, Erasmus and Cornelius Grapheus from whom Dürer received Luther's \"Babylonian Captivity\" in 1520.\n\nDürer's later works have also been claimed to show Protestant sympathies. For example, his woodcut of \"The Last Supper\" of 1523 has often been understood to have an evangelical theme, focussing as it does on Christ espousing the Gospel, as well the inclusion of the Eucharistic cup, an expression of Protestant utraquism, although this interpretation has been questioned. The delaying of the engraving of St Philip, completed in 1523 but not distributed until 1526, may have been due to Dürer's uneasiness with images of Saints; even if Dürer was not an iconoclast, in his last years he evaluated and questioned the role of art in religion.\n\nDürer exerted a huge influence on the artists of succeeding generations, especially in printmaking, the medium through which his contemporaries mostly experienced his art, as his paintings were predominantly in private collections located in only a few cities. His success in spreading his reputation across Europe through prints was undoubtedly an inspiration for major artists such as Raphael, Titian, and Parmigianino, all of whom collaborated with printmakers in order to promote and distribute their work.\n\nHis work in engraving seems to have had an intimidating effect upon his German successors, the \"Little Masters\" who attempted few large engravings but continued Dürer's themes in small, rather cramped compositions. Lucas van Leyden was the only Northern European engraver to successfully continue to produce large engravings in the first third of the 16th century. The generation of Italian engravers who trained in the shadow of Dürer all either directly copied parts of his landscape backgrounds (Giulio Campagnola, Giovanni Battista Palumba, Benedetto Montagna and Cristofano Robetta), or whole prints (Marcantonio Raimondi and Agostino Veneziano). However, Dürer's influence became less dominant after 1515, when Marcantonio perfected his new engraving style, which in turn travelled over the Alps to dominate Northern engraving also.\n\nIn painting, Dürer had relatively little influence in Italy, where probably only his altarpiece in Venice was seen, and his German successors were less effective in blending German and Italian styles. His intense and self-dramatizing self-portraits have continued to have a strong influence up to the present, especially on painters in the 19th and 20th century who desired a more dramatic portrait style. Dürer has never fallen from critical favour, and there have been significant revivals of interest in his works in Germany in the \"Dürer Renaissance\" of about 1570 to 1630, in the early nineteenth century, and in German nationalism from 1870 to 1945.\n\nDürer's study of human proportions and the use of transformations to a coordinate grid to demonstrate facial variation inspired similar work by D'Arcy Thompson in his book \"On Growth and Form\".\n\nThe Lutheran Church remembers Dürer as a great Christian annually on 6 April, along with Lucas Cranach the Elder and Hans Burgkmair. The liturgical calendar of the Episcopal Church (United States) remembers him, Cranach and Matthias Grünewald on 5 August.\n\nIn all his theoretical works, in order to communicate his theories in the German language rather than in Latin, Dürer used graphic expressions based on a vernacular, craftsmen's language. For example, \"Schneckenlinie\" (\"snail-line\") was his term for a spiral form. Thus, Dürer contributed to the expansion in German prose which Martin Luther had begun with his translation of the Bible.\n\nDürer's work on geometry is called the \"Four Books on Measurement\" (\"Underweysung der Messung mit dem Zirckel und Richtscheyt\" or \"Instructions for Measuring with Compass and Ruler\"). The first book focuses on linear geometry. Dürer's geometric constructions include helices, conchoids and epicycloids. He also draws on Apollonius, and Johannes Werner's 'Libellus super viginti duobus elementis conicis' of 1522.\n\nThe second book moves onto two dimensional geometry, i.e. the construction of regular polygons. Here Dürer favours the methods of Ptolemy over Euclid. The third book applies these principles of geometry to architecture, engineering and typography.\n\nIn architecture Dürer cites Vitruvius but elaborates his own classical designs and columns. In typography, Dürer depicts the geometric construction of the Latin alphabet, relying on Italian precedent. However, his construction of the Gothic alphabet is based upon an entirely different modular system. The fourth book completes the progression of the first and second by moving to three-dimensional forms and the construction of polyhedra. Here Dürer discusses the five Platonic solids, as well as seven Archimedean semi-regular solids, as well as several of his own invention.\n\nIn all these, Dürer shows the objects as nets. Finally, Dürer discusses the Delian Problem and moves on to the 'construzione legittima', a method of depicting a cube in two dimensions through linear perspective. It was in Bologna that Dürer was taught (possibly by Luca Pacioli or Bramante) the principles of linear perspective, and evidently became familiar with the 'costruzione legittima' in a written description of these principles found only, at this time, in the unpublished treatise of Piero della Francesca. He was also familiar with the 'abbreviated construction' as described by Alberti and the geometrical construction of shadows, a technique of Leonardo da Vinci. Although Dürer made no innovations in these areas, he is notable as the first Northern European to treat matters of visual representation in a scientific way, and with understanding of Euclidean principles. In addition to these geometrical constructions, Dürer discusses in this last book of \"Underweysung der Messung\" an assortment of mechanisms for drawing in perspective from models and provides woodcut illustrations of these methods that are often reproduced in discussions of perspective.\n\nDürer's work on human proportions is called the \"Four Books on Human Proportion\" (\"Vier Bücher von Menschlicher Proportion\") of 1528. The first book was mainly composed by 1512/13 and completed by 1523, showing five differently constructed types of both male and female figures, all parts of the body expressed in fractions of the total height. Dürer based these constructions on both Vitruvius and empirical observations of, \"two to three hundred living persons\", in his own words. The second book includes eight further types, broken down not into fractions but an Albertian system, which Dürer probably learned from Francesco di Giorgio's 'De harmonica mundi totius' of 1525. In the third book, Dürer gives principles by which the proportions of the figures can be modified, including the mathematical simulation of convex and concave mirrors; here Dürer also deals with human physiognomy. The fourth book is devoted to the theory of movement.\n\nAppended to the last book, however, is a self-contained essay on aesthetics, which Dürer worked on between 1512 and 1528, and it is here that we learn of his theories concerning 'ideal beauty'. Dürer rejected Alberti's concept of an objective beauty, proposing a relativist notion of beauty based on variety. Nonetheless, Dürer still believed that truth was hidden within nature, and that there were rules which ordered beauty, even though he found it difficult to define the criteria for such a code. In 1512/13 his three criteria were function ('Nutz'), naïve approval ('Wohlgefallen') and the happy medium ('Mittelmass'). However, unlike Alberti and Leonardo, Dürer was most troubled by understanding not just the abstract notions of beauty but also as to how an artist can create beautiful images. Between 1512 and the final draft in 1528, Dürer's belief developed from an understanding of human creativity as spontaneous or inspired to a concept of 'selective inward synthesis'. In other words, that an artist builds on a wealth of visual experiences in order to imagine beautiful things. Dürer's belief in the abilities of a single artist over inspiration prompted him to assert that \"one man may sketch something with his pen on half a sheet of paper in one day, or may cut it into a tiny piece of wood with his little iron, and it turns out to be better and more artistic than another's work at which its author labours with the utmost diligence for a whole year\".\n\nFor lists of Albrecht Dürer's works, see:\n\n\n"}
{"id": "3087414", "url": "https://en.wikipedia.org/wiki?curid=3087414", "title": "American Mathematical Association of Two-Year Colleges", "text": "American Mathematical Association of Two-Year Colleges\n\nThe American Mathematical Association of Two-Year Colleges (AMATYC) is an organization dedicated to the improvement of education in the first two years of college mathematics in the United States and Canada. AMATYC hosts an annual conference, summer institutes, workshops and mentoring for teachers in and outside math, and a semiannual math competition. AMATYC publishes one refereed journal, \"MathAMATYC Educator,\" and issues position statements on matters of mathematics education.\n\nThe math competition is held in spring and fall semester each year and is limited to problems in precalculus. Only students enrolled in two-year colleges are eligible to participate. Only students who haven't received any degree/diploma, including within or outside of the U.S, can enter the competition.\n\nAMATYC was founded in 1974. Its office is at Southwest Tennessee Community College in Memphis, Tennessee.\n\nAMATYC is divided into eight regions: Central, Mid-Atlantic, Midwest, Northeast, Northwest, Southeast, Southwest, and West. A vice president is assigned to each region.\n\n\n"}
{"id": "20847694", "url": "https://en.wikipedia.org/wiki?curid=20847694", "title": "Amnestic functor", "text": "Amnestic functor\n\nIn the mathematical field of category theory, an amnestic functor \"F\" : \"A\" → \"B\" is a functor for which an \"A\"-isomorphism \"ƒ\" is an identity whenever \"Fƒ\" is an identity.\n\nAn example of a functor which is \"not\" amnestic is the forgetful functor Met→Top from the category of metric spaces with continuous functions for morphisms to the category of topological spaces. If formula_1 and formula_2 are equivalent metrics on a space formula_3 then formula_4 is a isomorphism that covers the identity, but is not an identity morphism (its domain and codomain are not equal). \n\n"}
{"id": "41275177", "url": "https://en.wikipedia.org/wiki?curid=41275177", "title": "Ante Graovac", "text": "Ante Graovac\n\nAnte Graovac is a Croatian scientist (born July 15, 1945 in Split, died November 13, 2012 in Zagreb) known for his contribution to chemical graph theory. He was director of 26 successful annual meetings MATH/CHEM/COMP held in Dubrovnik. He was Secretary of the International Academy of Mathematical Chemistry.\n"}
{"id": "5440367", "url": "https://en.wikipedia.org/wiki?curid=5440367", "title": "Baire one star function", "text": "Baire one star function\n\nBaire one star function is a term from real analysis. A function formula_1 is in class Baire* one, written formula_2, and is called a Baire one star function, if for each perfect set formula_3, there is an open interval formula_4, such that formula_5 is nonempty, and the restriction formula_6 is continuous. The notion seems to have originated with B. Kerchheim in an article titled 'Baire one star functions' (Real Anal. Exch. 18 (1992/93), 385-399).\nThe terminology is actually due to Richard O'Malley, 'Baire* 1, Darboux functions' Proc. Amer. Math. Soc. 60 (1976) 187-192. The concept itself (under a different name) goes back at least to 1951. See H. W. Ellis, 'Darboux properties and applications to nonabsolutely convergent integrals' Canad. Math. J., 3 (1951), 471-484, where the same concept is labelled as [CG] (for generalized continuity).\n"}
{"id": "1482061", "url": "https://en.wikipedia.org/wiki?curid=1482061", "title": "Bekenstein bound", "text": "Bekenstein bound\n\nIn physics, the Bekenstein bound is an upper limit on the entropy \"S\", or information \"I\", that can be contained within a given finite region of space which has a finite amount of energy—or conversely, the maximum amount of information required to perfectly describe a given physical system down to the quantum level. It implies that the information of a physical system, or the information necessary to perfectly describe that system, must be finite if the region of space and the energy is finite. In computer science, this implies that there is a maximum information-processing rate (Bremermann's limit) for a physical system that has a finite size and energy, and that a Turing machine with finite physical dimensions and unbounded memory is not physically possible.\n\nUpon exceeding the Bekenstein bound a storage medium would collapse into a black hole.\n\nThe universal form of the bound was originally found by Jacob Bekenstein as the inequality\n\nwhere \"S\" is the entropy, \"k\" is Boltzmann's constant, \"R\" is the radius of a sphere that can enclose the given system, \"E\" is the total mass–energy including any rest masses, \"ħ\" is the reduced Planck constant, and \"c\" is the speed of light. Note that while gravity plays a significant role in its enforcement, the expression for the bound does not contain the gravitational constant \"G\".\n\nIn informational terms, the bound is given by\n\nwhere \"I\" is the information expressed in number of bits contained in the quantum states in the sphere. The ln 2 factor comes from defining the information as the logarithm to the base 2 of the number of quantum states. Using mass energy equivalence, the informational limit may be reformulated as\n\nwhere formula_4 is the mass of the system in kilograms, and the radius formula_5 is expressed in meters.\n\nBekenstein derived the bound from heuristic arguments involving black holes. If a system exists that violates the bound, i.e., by having too much entropy, Bekenstein argued that it would be possible to violate the second law of thermodynamics by lowering it into a black hole. In 1995, Ted Jacobson demonstrated that the Einstein field equations (i.e., general relativity) can be derived by assuming that the Bekenstein bound and the laws of thermodynamics are true. However, while a number of arguments have been devised which show that some form of the bound must exist in order for the laws of thermodynamics and general relativity to be mutually consistent, the precise formulation of the bound has been a matter of debate.\n\nIt happens that the Bekenstein-Hawking Boundary Entropy of three-dimensional black holes exactly saturates the bound\n\nwhere formula_10 is Boltzmann's constant, \"A\" is the two-dimensional area of the black hole's event horizon in units of the Planck area, formula_8.\n\nThe bound is closely associated with black hole thermodynamics, the holographic principle and the covariant entropy bound of quantum gravity, and can be derived from a conjectured strong form of the latter.\n\nAn average human brain has a mass of 1.5 kg and a volume of 1260 cm. If the brain is approximated by a sphere, then the radius will be 6.7 cm.\n\nThe informational Bekenstein bound will be formula_12 bits and represents the maximum information needed to perfectly recreate an average human brain down to the quantum level. This means that the number formula_13 of states of the human brain must be less than formula_14.\n\n\n\n"}
{"id": "15231265", "url": "https://en.wikipedia.org/wiki?curid=15231265", "title": "Bühlmann model", "text": "Bühlmann model\n\nIn credibility theory, a branch of study in actuarial science, the Bühlmann model is a random effects model (or \"variance components model\" or hierarchical linear model) used in to determine the appropriate premium for a group of insurance contracts. The model is named after Hans Bühlmann who first published a description in 1967.\n\nConsider \"i\" risks which generate random losses for which historical data of \"m\" recent claims are available (indexed by \"j\"). A premium for the \"i\"th risk is to be determined based on the expected value of claims. A linear estimator which minimizes the mean square error is sought. Write\n\n\nNote: formula_9 and formula_10 are functions of random parameter formula_11 \n\nThe Bühlmann model is the solution for the problem: \n\nwhere formula_13 is the estimator of premium formula_14 and arg min represents the parameter values which minimize the expression.\n\nThe solution for the problem is: \n\nwhere: \n\nWe can give this result the interpretation, that Z part of the premium is based on the information that we have about the specific risk, and (1-Z) part is based on the information that we have about the whole population.\n\nThe following proof is slightly different from the one in the original paper. It is also more general, because it considers all linear estimators, while original proof considers only estimators based on average claim.\n\nProof:\n\nThe last equation follows from the fact that\n\nWe are using here the law of total expectation and the fact, that formula_20\n\nIn our previous equation, we decompose minimized function in the sum of two expressions. The second expression does not depend on parameters used in minimization. Therefore, minimizing the function is the same as minimizing the first part of the sum.\n\nLet us find critical points of the function\n\nFor formula_23 we have:\n\nWe can simplify derivative, noting that:\n\nTaking above equations and inserting into derivative, we have:\n\nRight side doesn't depend on \"k\". Therefore all formula_28 are constant\n\nFrom the solution for formula_30 we have\n\nFinally, the best estimator is\n\n \n"}
{"id": "612143", "url": "https://en.wikipedia.org/wiki?curid=612143", "title": "CAST-256", "text": "CAST-256\n\nIn cryptography, CAST-256 (or CAST6) is a symmetric-key block cipher published in June 1998. It was submitted as a candidate for the Advanced Encryption Standard (AES); however, it was not among the five AES finalists. It is an extension of an earlier cipher, CAST-128; both were designed according to the \"CAST\" design methodology invented by Carlisle Adams and Stafford Tavares. Howard Heys and Michael Wiener also contributed to the design. \n\nCAST-256 uses the same elements as CAST-128, including S-boxes, but is adapted for a block size of 128 bits – twice the size of its 64-bit predecessor. (A similar construction occurred in the evolution of RC5 into RC6). Acceptable key sizes are 128, 160, 192, 224 or 256 bits. CAST-256 is composed of 48 rounds, sometimes described as 12 \"quad-rounds\", arranged in a generalized Feistel network.\n\nIn RFC 2612, the authors state that, \"The CAST-256 cipher described in this document is available worldwide on a royalty-free and licence-free basis for commercial and non-commercial uses.\"\n\nCurrently, the best public cryptanalysis of CAST-256 in the standard single secret key setting that works for all keys is the zero-correlation cryptanalysis breaking 28 rounds with 2 time and 2 data.\n\n\n"}
{"id": "1751201", "url": "https://en.wikipedia.org/wiki?curid=1751201", "title": "Charlie Eppes", "text": "Charlie Eppes\n\nProf. Charles Edward Eppes, Ph.D., is a fictional character and one of the protagonists of the CBS crime drama \"Numb3rs\". He is portrayed by David Krumholtz.\n\nDr. Charles Eppes is portrayed as a young mathematical genius and professor of applied mathematics at the fictional \"California Institute of Science\", \"CalSci\" (primarily based on Caltech, where some filming and mathematics consulting is done). As a world-class mathematician, Charlie helps his brother Don Eppes solve many of his perplexing FBI cases, sometimes with the help of his best friend, mentor and colleague Larry Fleinhardt and his on-again off-again girlfriend, former student and now wife, Amita Ramanujan, who further refines Charlie's approach and helps him stay focused. Charlie has consulted for the National Security Agency (NSA), in part as a cryptanalyst, for nearly five years, having attained TS/SCI security clearance. It was revoked at the end of season four after he transmitted information to Pakistan, but has recently been re-instated.\n\nAccording to Eppes' father, he could multiply four-digit numbers mentally at age three and at the age of four required special teachers. In the second grade, he attempted to find a 70-digit narcissistic number in base 12—Eppes has described himself as \"quixotic\" in elementary school. A prodigy, he attended Princeton University at the age of 13 after graduating from high school at the same time as his brother who is five years his senior (\"Soft Target\"), and took Professor Lawrence Fleinhardt's quantum physics course in his first year. They became fast friends, with Fleinhardt establishing his academic connections. Eppes published his first mathematical treatise at the age of 14 (in the \"American Journal of Mathematics\") and graduated at the age of 16. In fact, he was the youngest person to ever write a paper of importance.\n\nIt was his paper on the Eppes Convergence, which concerned asymptotics of Hermitian random matrices, that made him a star in his field. Following a seminar that heavily criticized this seminal piece many years after its initial publication, Charlie realized that his work with the FBI has prevented him from doing research significant to other mathematicians and now hopes to spend decades on cognitive emergence theory (\"the mathematics of the brain\") to rectify this certain inequity, which has delighted Fleinhardt. Although he was a child prodigy, Charlie now laments the fact that his best years in his research will never come ahead of schedule again.\n\nEppes is a multiple Ph.D. (\"Decoy Effect\"), a recipient of the Milton Prize and a nominee for the Fields Medal. Following his five-year research on random matrices, Charlie worked on sequences with orthogonal symmetry. He has also provided insights for possibly solving the P vs. NP problem and published works on H-infinity control of nonlinear systems and computational fluid dynamics, while his current research is in cognitive emergence theory. He has presented seminars on harmonic analysis and the zeros of random orthogonal polynomials and given lectures on group theory and Kac–Moody algebras. Eppes has taught courses on calculus, chaos theory, fluid dynamics, game theory and probability at CalSci in addition to giving guest lectures on applied probability. The lecture in which he converted the classroom into a miniature casino for analyzing probabilities is considered an \"Eppes Classic\". Also, Eppes has taken over Fleinhardt's computational physics class when he was asked to do so, and has given a joint lecture on circular motion and the Coriolis effect with Fleinhardt. Professor Otto Bahnoff took over Eppes' mathematical physics grad seminar on the day he got married.\n\nCharlie has wild curly hair, is wary of people, and frequently enthralled with objects and patterns. According to Krumholtz, Charlie wants to understand how the world works. His father has said Charlie is easily fascinated, possesses a big heart and is thorough, but he misses certain things completely. Meanwhile, Larry observed that he is \"a talented theoretician with an ego problem\" and a student once described him as fast-talking and disorganized, to the agreement of Larry. Fleinhardt also accurately noticed that his colleague has a high standard of guilt and is a pragmatist. Charlie is rather fond of providing excessive explanations (e.g., his discussion of Occam's razor) and reducing complex phenomena to intuitively obvious situations through practical analogies that are quite unlike Larry's metaphysical musings and cosmic metaphors. (These analogies are known as \"audience visions\" or, as friend Megan Reeves calls them, \"those cute little analogies.\") While contemplating, his antics and mannerisms may even disconcert more conventional thinkers. Wearing his headset, Dr. Eppes has an intense focusing ability as he voraciously writes equations, often covering several chalkboards with a staccato clacking and the aid of a red chalk holder. Nevertheless, if his line of thought is interrupted during a tense moment, as one of restricted foresight, he can become very disgruntled. Further, when deeply concentrating on a particular problem, it seems Charlie is unable to provide insights to other topics for the sake that they are simply needed or wanted—he has to write what is in his head. Like his brother Don, he is characteristically stubborn and obsessive (\"one part exuberance, two parts obsession\"), especially when it comes to work, but he's rather naïve when it comes to human behavior. The latter often interferes with his FBI work and, thus, is the cause of much distress for him at times.\n\nHe is extremely talented in chess, as it requires both his father and brother to play against him and a distraction to defeat him. Charlie also has a vast understanding of theoretical physics, often assisting Larry with his multi-dimensional supergravity theory and papers on gravity waves, and biology, extending to knowledge of ciliate protozoa and the spread of infectious diseases. While brilliant in some areas, he is lacking in others. Dr. Fleinhardt has stated that it is a good thing he went into applied mathematics as opposed to engineering, as machines malfunction in his presence, though he was able to take apart and rebuild his father's cell phone to analyze its GPS transceiver. He is apparently a bad speller (e.g., he misspells \"anomaly\" and \"conceited\") and does not know the meaning of \"defenestration\" (for which Larry chastises him stating that the idealization is to be a Renaissance man and that even math and physics majors had to have a course on English). Thus, his father likes playing Scrabble with him.\n\nCharlie is a rationalist. He is skeptical of UFOs, psychic ability, and gematria—all considered to be pseudosciences. He also does not like illusions. He, however, is also fairly open-minded in terms of faith and religion.\n\nThe episode \"Uncertainty Principle\" is significant for the backstory it gives on the familial relationships, particularly Charlie's difficulty in dealing with his mother's death from cancer. While it's unclear how Don reacted, Charlie spent the last three months of his mother's life isolated in the garage, incessantly working on one of the Millennium Prize Problems, specifically P vs NP; it's a point of contention between the brothers. Also, Charlie doesn't think Don understood what he went through during their school years, especially how he was treated as \"Don's brainiac little brother\" by his peers in high school, and how he often left him to his own resources as a child, though he was not as inept as Don had reasoned. However, Charlie's relationship with Don remains strong, as he has begun to increasingly fear for his brother's safety on the job and still looks to his older brother for acceptance. Charlie and his father worry about Don committing to relationships, and for a time, fear that he had been cheating on Robin Brooks. Ironically, Charlie has similar problems with women himself. After a couple dozen murder cases, Charlie is somewhat jaded and world-weary like his brother. FBI agent David Sinclair of Don's team even comments that he has never seen two brothers so similar and yet so different. In \"The Janus List\", the brothers seem to take on each other's tendencies a bit. Though, in season five, Charlie is dismayed that Don does not reveal to him his newfound religious faith.\n\nIn \"Prime Suspect,\" Charlie purchases the beautiful Craftsman family home from his father, who continues to live with him. Now 30 years old, Charlie wants to be responsible and take care of his father but still believes that much of the pressures involving their dad has been put on his shoulders as Don doesn't seem to have enough time. Realizing this, Alan sets his sights on moving out to accomplish things on his own, with Charlie seemingly supporting the idea, but has since chosen to stay as he favors his son's company. Recently, Charlie is bothered by his father trying to impose his will on him with maintaining the house, as he is a full tenured professor at one of the most prestigious institutions in the country working on \"life-altering\" mathematics, i.e., in solving crimes. Alan just wants him to be responsible and not end up like Larry, though Mr. Eppes respects Fleinhardt. Even he has thought of this possibility and decides to do more around the home. After Charlie compares his own situation to that of Einstein his father looks into information on the physicist and understands his son's predicament. Charlie also feels guilty about the amount of time his parents, particularly his mother, who always was attuned to his way of thinking, spent with him as a child; he even asks his mother (JoBeth Williams) in a dream if she regrets the time away from Don and Alan because of the special attention he needed growing up.\n\nCharlie's research often interferes with his relationships: as with Amita on their first date, for all they could talk about is mathematics; Fleinhardt says that it is a common interest and they should not struggle to avoid the subject. Charlie and Amita had several false starts. Charlie has also spent some time with his ex-girlfriend, Susan Berry (Sonya Walger), an attractive neuroscientist from London. He had lived with Susan for two years, and Larry described this as his very own Berry's phase. However, she later reveals that she is currently involved with someone else and has to return to England. Charlie attempts to start a relationship with Amita once more, though her job offer at Harvard University strains this possibility and makes him distraught for some time. Eventually, she decides to take the alternative offer at CalSci with the hope to begin a romantic relationship with him, though his fear of possible failure causes him to question whether he wants this second chance; Don cautions him about such an attitude. Amita notices his ambivalence and isn't certain if she wants to back out of the relationship, but he then pushes it forward. Pressures from their colleagues over the inappropriateness of the relationship nearly cost them, but by the middle of the third season their romantic involvement stabilizes and they have grown considerably closer. He feels rejected when Amita does not want him to meet her father, due to his expectations about the men she dates. Initially, he wonders if it is based on him being Jewish, but the problem is that he is not Indian-born. Alan explains that Mr. Ramanujan would like him after meeting him. Charlie and Amita state that they love each other, and have even decided to move in together, though, the actual living arrangement has not been determined. When Amita's parents finally meet him, they are rude, but warm to him later. Alan says that Charlie will have to marry her.\n\nUnforeseen complications with his work emerge as the new Chair of the CalSci Physics, Mathematics, and Astronomy Division, Dr. Mildred Finch, in the episode \"Waste Not\", makes Charlie head of the Ph.D. admissions committee against his wishes, bogging him down with more work, and gives him pressures about using the school's supercomputer for FBI work or missing classes for such. This leads to Charlie being confrontational, but she calms him when she says she just wants him to be \"the Sean Connery of the mathematics department.\" In the episode \"Take Out\", Charlie and Millie attend a black tie reception for CalSci fundraising and bond. Soon after, Charlie is asked by Millie to meet with Macmillan Pharmaceuticals, which according to Amita has a reputation for exploiting third-world countries. Amita serves as his conscience in this matter, and he goes to see Dr. Finch about it. With \"Pythagorean wit,\" he dazzles Macmillan and is to serve as a mathematical consultant for the pharmaco-kinetic modelling project, only under the provision that CalSci will administer the trials and monitor the drug at every step, all computational analyses are conducted by Charlie and his team, and 5% of the gross will go to third-world AIDS organizations chosen by Professors Eppes and Ramanujan . In \"Democracy\", as part of his duties, he recruits the young fantasy baseball and sabermetrics fanatic Oswald Kittner (Jay Baruchel), who shows great promise, to attend CalSci.\n\nWhen Larry announces his leave of absence to board the International Space Station (in the episode \"Brutus\"), Charlie is shocked and upset. He is in denial about Larry's ambitions and he thinks reason of the risks involved will assert itself firmly in Larry's mind. He reasons that apparently Larry wouldn't do such a thing. Larry remands his few prized possessions to him, and he is grateful for the gesture. When he tells him his concerns, Larry is angered. Amita says to Larry, \"[Charlie] has never dreamt of something he couldn't reach, so he has no idea what it is like to want something he is not able to get. So how could he understand how much [Larry] you would give up when this chance comes along?\" Larry understands and decides to ignore Charlie's protestations. Though neither apologize, they are on good terms. Charlie says to Amita that he does not know what he would do without him, as he peruses through Larry's precious items that were bestowed upon him. In \"Killer Chat\", he says that he was glad that Larry's dream could come true, but feels conflicted about being relieved when he discovers that Larry might have been scrapped from the mission when NASA learned of his eccentric indulgences such as sleeping in the campus steam tunnels. However, Charlie agrees with Megan in wanting to help Larry and personally vouches for him. Eppes' words and affiliation with the NSA gets him back on the mission, thus, repaying a debt as Larry helped launch him into the academic firmament. Earlier, he had given him back his lucky T-shirt.\n\nIn \"The Art of Reckoning\", Charlie is initially overjoyed to see his friend Larry return, but Charlie is dismayed with Larry's lack of enthusiasm in life, so Charlie doesn't object to him readjusting at a monastery. His concern is apparent, and he says he just wants his friend back.\n\nSince then, in \"Trust Metric\", Charlie is pleased to see his friend doing well, feeling that he needed a friend, and is glad to have focused on teaching while Don did not include him with FBI work; though, on previous occasions, he expressed the desire to be involved. Granger's escape from the prison bus causes him to become active in helping his brother once more, allowing him a chance to use set covering deployment.\n\nDr. Finch tells him to publish, and so he has renewed interest in old research, having decided to publish one of his eleventh-grade papers he started at nine years old, \"The Mathematics of Friendship,\" with an addendum. A publishing company has turned his work into a classic book for those not mathematically inclined, while opting for a title with more pizazz, \"Friendship, As Easy as Pi.\" Charlie takes joy in the belief that this book will allow his thoughts to reach a much wider audience than before. By the episode \"In Security,\" the published book appears with the title \"The Attraction Equation\" and a dapper photo on the back cover of him holding a sculpture of a stellated icosidodecahedron with bevelled edges. A decision theoretic approach to relationships is covered in the book. His proud father hands copies to friends and Larry sells signed copies on eBay. He apparently has some fans and gives into a televised interview.\n\nIn \"Checkmate\", Charlie is training in weapons and tactics in LA's FBI school, where he is shown to have a high skill in marksmanship at the range, impressing many of his FBI training peers as well as his instructor, which he credits to following Wyatt Earp's preference for careful accuracy instead of rapid firing. He is given a certificate of marksmanship by the FBI. In the episode \"Pay to Play\", Charlie convinces Don to let him join the team when they go to arrest a suspect, mentioning that he passed the FBI course.\n\nIn the season four finale \"When Worlds Collide\", Charlie helps an innocent colleague accused of terrorism by sending genetic research to scientists at Pakistani universities which is prohibited, with full cognizance of the consequences. As a result, Charlie is arrested, loses his security clearance and ultimately loses his ability to help Don on FBI cases. Once again, Charlie holds strong to his ideals. In the previous season's \"Burn Rate\", Charlie has strong opinions concerning genetic engineering, believing bomb suspect, fellow prodigy, and former Feynman student Emmett Glaser's ideas about genetic predeterminism are rational, not incendiary, and helps to clear his name.\n\nThe charges against Dr. Eppes are dropped. For a time, Don Eppes' team is attempting to make do with Fleinhardt's and Amita's expertise, but Charles' assistance is sorely missed to the point of consulting him secretly. For his part, Charles has been convinced by his lawyer and father to attempt to have his clearance restored. Even Don supports the idea and stands up to security clearance investigator Carl McGowan (Keith Carradine), stating to go after him, not his brother. Temporarily, Charlie works as a LAPD consultant until the fiasco with his security clearance can be resolved. He later gets his clearance back and is working with his brother and the FBI again. He is trying to assert himself in the methodology used to solve a crime, having struggled with not being included, but some friction arises with Amita and Larry.\n\nCharlie works once again with rival Marshall Penfield, as they settle their differences (\"Frienemies\"). Also, he is chosen to be the head of the think tank model comprising himself, Larry, Alan, and Amita (\"Jacked\"). As Dr. Eppes applies the Turing Test to a seemingly unique artificial intelligence, he is tricked only to come to the realization that the computer only uses a recursive algorithm to apply the most human responses, while simultaneously being tempted by an offer to work for DARPA. Head of DARPA special projects Jane Karellen (Nancy Travis) knows that he has a limited window to use his genius and tells Charlie that he is one of the top five minds on the planet. Amita's life is even threatened by the advanced computer (\"First Law\").\nLate in season five, Charlie moves into a new office. While moving, he gets some inspiration for his cognitive emergence theory, which causes him to momentarily set aside his work on a series of home invasions for Don. Don is stabbed while attempting to arrest the home invaders (\"The Fifth Man\"). Charlie blames himself for Don being stabbed and throws himself into his FBI consultation work as a result (\"Disturbed\"). This worries everyone, especially Don, who tells him to \"do whatever you want to do\" (\"Greatest Hits\") while visiting Charlie in his new office. Around this time, Charlie is also presented with a series of letters from previous successive holders of this prestigious office, wherein each celebrated mathematician writes of accomplishments they intend to achieve, passing down the torch to the next in line to do the same. At first Charlie is hesitant, until the very determined Amita convinces him to write the corresponding letter and eases his burdens, as Charlie fears he cannot meet his destiny with so much weight to succeed being placed on his shoulders his entire life.\n\nAt the end of the season, while leaving for dinner, Charlie is attacked, and Amita is kidnapped. Charlie is so emotionally distraught that he cannot think of the math needed to find Amita. With the help of Don, Alan, and David, Charlie snaps out of it to locate Amita. Amita is rescued. He realizes that he did not want to lose Amita, and he proposes to her (\"Angels and Devils\").\n\nAt the end the premiere episode of the program's sixth season, it was revealed that Amita has accepted Charlie's proposal. The issue was skirted throughout the episode, as the two were waiting until Amita officially received permission from her parents. He and Amita discuss the number of children that they want to have, and they both decide that they may need to participate in the Big Brother/Big Sister program for some practical experience before having children. He and Amita attempt to find a wedding date that is suited to their and their families' schedules; Alan suggests that they should take his and Margaret's anniversary date as Charlie and Amita's date. Charlie and Amita have since pushed the date forward as his academic fantasy has come to light, being a visiting professor at Cambridge University. In the season finale, after the wedding ceremony presided over by Larry, he is happy but worries about disconnecting with Don and offers to have the garage renovated into a guest house for his father. He toasts his friends and family and amazes at the prospect of staying in England with his wife.\n\nLong intrigued by mathematicians and scientists, creators Cheryl Heuton and Nick Falacci created a show with one as the protagonist. Inspiration for Charlie came specifically from Richard Feynman. Finding the actor who would portray Charlie would be a challenge. Over one hundred actors auditioned for the role of Charlie Eppes. One of the actors was David Krumholtz, who later admitted in an interview with TVGuide.com and in an interview with USA Weekend's Lorrie Lynch to failing math in high school. Krumholtz was cast as Charlie because of his ability to make math sound natural.\n\nTo prepare for his role of Charlie Eppes, Krumholtz spent some time at Caltech talking to professors and walking the Caltech campus, attempting to understand both the basics of the math and the mathematician's mind. He even spoke with Dr. Tony Chan of UCLA about mathematicians’ work while filming the first pilot. Math consultants helped Krumholtz understand the basics of the equations on the show. Early on, Professor Rick Wilson’s graduate student, David Grynkiewicz, showed Krumholtz how to write his own equations and even filled in for his hand in several episodes. Krumholtz now frequently writes the equations himself.\n\nKrumholtz memorizes the pages of mathematical equations and concepts that are a part of the script. When doing scenes involving audience-visions, Krumholtz prefers to recite his lines as the cameras are rolling; producers went along with this because they reasoned that if the cameras weren't rolling, the lines wouldn't make it into the show. (Audience visions are Charlie's visually-aided explanations of the mathematics involved in a case.)\n\nEarly reception of Charlie varied according to the audience in question. Due to television production's traditional approach of utilizing only two worlds for filming, production staff initially opposed the idea of Charlie being a college professor. Some even unsuccessfully suggested to Heuton and Falacci that Charlie should be working with the FBI full-time as an employee.\n\nWhen the pilot was previewed, the reception was more positive. CBS executive Nina Tassler liked Charlie. The focus group that watched the original pilot for \"Numb3rs\" loved him.\n\nWhen \"Numb3rs\" was previewed for the TV critics, the reception was different. Melanie McFarland, TV critic for the \"Seattle Post-Intelligencer\", stated that Charlie was not original as of the Pilot. According to Lauren Aaronson of \"Popular Science\", Charlie’s expertise seems a little bit incredible. Robert Bianco of \"USA Today\", however, called Krumholtz, as Charlie, \"appealing\". Toni Fitzgerald of \"Media Life Magazine\" stated that Krumholtz, as Charlie, \"stands out\".\n\nSince the early days of the series, the character of Charlie Eppes has become more widely accepted. Krumholtz appeared at the 2005 National Council of Teachers of Mathematics (NCTM) convention in Anaheim. Since then, Krumholtz receives cheers when he attends math conventions. In a public service announcement, Krumholtz congratulated the Federal Bureau of Investigation on their 100th anniversary. Charlie was one of the first geeks on primetime television who paved the way for other shows starring geeks such as \"Bones\", \"Chuck\" and \"The Big Bang Theory\". Charlie was a runner-up in the category of \"Sexiest Brainiac\" in TV Guide's poll in 2007.\n\n\"CharlieVision\" (as labeled by the show's creators) is the mode in which Charlie's insights are displayed on-screen. It consists of fast-paced visions or cutscenes often characterized by false-color images that integrate his analogies and mathematical models, usually followed by him rushing off to tell Don about his new insights. 'CharlieVision' is not to be confused with \"audience visions,\" in which Charlie's voice uses an analogy to simplify a mathematical concept while corresponding images are flashed on screen. Ridley Scott and Tony Scott, executive producers for Numb3rs, designed the specifics of the Charlie-visions, such as the ash yellow color that appears on-screen when Charlie suddenly becomes inspired.\n"}
{"id": "50534018", "url": "https://en.wikipedia.org/wiki?curid=50534018", "title": "Chow group of a stack", "text": "Chow group of a stack\n\nIn algebraic geometry, the Chow group of a stack is a generalization of the Chow group of a variety or scheme to stacks. For a quotient stack formula_1, the Chow group of \"X\" is the same as the \"G\"-equivariant Chow group of \"Y\".\n\nA key difference from the theory of Chow groups of a variety is that a cycle is allowed to carry non-trivial automorphisms and consequently intersection-theoretic operations must take this into account. For example, the degree of a 0-cycle on a stack need not be an integer but is a rational number (due to non-trivial stabilizers).\n\n develops the basic theory (mostly over Q) for the Chow group of a (separated) Deligne–Mumford stack. There, the Chow group is defined exactly as in the classical case: it is the free abelian group generated by integral closed substacks modulo rational equivalence.\n\nIf a stack \"X\" can be written as the quotient stack formula_1 for some quasi-projective variety \"Y\" with a linearized action of a linear algebraic group \"G\", then the Chow group of \"X\" is defined as the \"G\"-equivariant Chow group of \"Y\". This approach is introduced and developed by Edidin-Graham and Totaro. later extended the theory to a stack admitting a stratification by quotient stacks.\n\nFor higher Chow groups (precursor of motivic homologies) of algebraic stacks, see Roy Joshua's Intersection Theory on Stacks:I and II. \n\nThe calculations depend on definitions. Thus, here, we proceed somehow axiomatically. Specifically, we assume: given an algebraic stack \"X\" locally of finite type over a base field \"k\",\nThese properties are valid if \"X\" is Deligne–Mumford and are expected to hold for any other reasonable theory.\n\nWe take \"X\" to be the classifying stack formula_5, the stack of principal \"G\"-bundles for a smooth linear algebraic group \"G\". By definition, it is the quotient stack formula_6, where * is viewed as the stack associated to * = Spec \"k\". We approximate it as follows. Given an integer \"p\", choose a representation formula_7 such that there is a \"G\"-invariant open subset \"U\" of \"V\" on which \"G\" acts freely and the complement formula_8 has codimension formula_9. Let formula_10 be the quotient of formula_11 by the action formula_12. Note the action is free and so formula_10 is a vector bundle over formula_14. By Property 1 applied to this vector bundle,\nThen, since formula_16, by Property 2,\nsince formula_18.\n\nAs a concrete example, let formula_19 and let it act on formula_20 by scaling. Then formula_21 acts freely on formula_22. By the above calculation, for each pair of integers \"n\", \"p\" such that formula_23,\nIn particular, for every integer \"p\" ≥ 0, formula_25. In general, formula_26 for the hyperplane class \"h\", formula_27 \"k\"-times self-intersection and formula_28 for negative \"k\" and so\nwhere the right-hand side is independent of models used in the calculation (since different \"h\"s correspond under the projections between projective spaces.) For formula_30, the class formula_31, any \"n\", may be thought of as the fundamental class of formula_32.\n\nSimilarly, we have\nwhere formula_34 is the first Chern class of \"h\" (and \"c\" and \"h\" are identified when Chow groups and Chow rings of projective spaces are identified). Since formula_35, we have that formula_36 is the free formula_37-module generated by formula_38.\n\nThe notion originates in the Kuranishi theory in symplectic geometry.\n\nIn § 2. of , given a DM stack \"X\" and \"C\" the intrinsic normal cone to \"X\", K. Behrend defines the virtual fundamental class of \"X\" as\nwhere \"s\" is the zero-section of the cone determined by the perfect obstruction theory and \"s\" is the refined Gysin homomorphism defined just as in Fulton's \"Intersection theory\". The same paper shows that the degree of this class, morally the integration over it, is equal to the weighted Euler characteristic of the Behrend function of \"X\".\n\nMore recent (circa 2017) approaches do this type of construction in the context of derived algebraic geometry.\n\n\n"}
{"id": "249254", "url": "https://en.wikipedia.org/wiki?curid=249254", "title": "Clique problem", "text": "Clique problem\n\nIn computer science, the clique problem is the computational problem of finding cliques (subsets of vertices, all adjacent to each other, also called complete subgraphs) in a graph. It has several different formulations depending on which cliques, and what information about the cliques, should be found. Common formulations of the clique problem include finding a maximum clique (a clique with the largest possible number of vertices), finding a maximum weight clique in a weighted graph, listing all maximal cliques (cliques that cannot be enlarged), and solving the decision problem of testing whether a graph contains a clique larger than a given size.\n\nThe clique problem arises in the following real-world setting. Consider a social network, where the graph's vertices represent people, and the graph's edges represent mutual acquaintance. Then a clique represents a subset of people who all know each other, and algorithms for finding cliques can be used to discover these groups of mutual friends. Along with its applications in social networks, the clique problem also has many applications in bioinformatics and computational chemistry.\n\nMost versions of the clique problem are hard. The clique decision problem is NP-complete (one of Karp's 21 NP-complete problems). The problem of finding the maximum clique is both fixed-parameter intractable and hard to approximate. And, listing all maximal cliques may require exponential time as there exist graphs with exponentially many maximal cliques. Therefore, much of the theory about the clique problem is devoted to identifying special types of graph that admit more efficient algorithms, or to establishing the computational difficulty of the general problem in various models of computation.\n\nTo find a maximum clique, one can systematically inspect all subsets, but this sort of brute-force search is too time-consuming to be practical for networks comprising more than a few dozen vertices.\nAlthough no polynomial time algorithm is known for this problem, more efficient algorithms than the brute-force search are known. For instance, the Bron–Kerbosch algorithm can be used to list all maximal cliques in worst-case optimal time, and it is also possible to list them in polynomial time per clique.\n\nThe study of complete subgraphs in mathematics predates the \"clique\" terminology. For instance, complete subgraphs make an early appearance in the mathematical literature in the graph-theoretic reformulation of Ramsey theory by . But the term \"clique\" and the problem of algorithmically listing cliques both come from the social sciences, where complete subgraphs are used to model social cliques, groups of people who all know each other. used graphs to model social networks, and adapted the social science terminology to graph theory. They were the first to call complete subgraphs \"cliques\". The first algorithm for solving the clique problem is that of , who were motivated by the sociological application.\nSocial science researchers have also defined various other types of cliques and maximal cliques in social network, \"cohesive subgroups\" of people or actors in the network all of whom share one of several different kinds of connectivity relation. Many of these generalized notions of cliques can also be found by constructing an undirected graph whose edges represent related pairs of actors from the social network, and then applying an algorithm for the clique problem to this graph.\n\nSince the work of Harary and Ross, many others have devised algorithms for various versions of the clique problem. In the 1970s, researchers began studying these algorithms from the point of view of worst-case analysis. See, for instance, , an early work on the worst-case complexity of the maximum clique problem. Also in the 1970s, beginning with the work of and , researchers began using the theory of NP-completeness and related intractability results to provide a mathematical explanation for the perceived difficulty of the clique problem. In the 1990s, a breakthrough series of papers beginning with and reported in \"The New York Times\", showed that (assuming P ≠ NP) it is not even possible to approximate the problem accurately and efficiently.\n\nClique-finding algorithms have been used in chemistry, to find chemicals that match a target structure and to model molecular docking and the binding sites of chemical reactions. They can also be used to find similar structures within different molecules.\nIn these applications, one forms a graph in which each vertex represents a matched pair of atoms, one from each of two molecules. Two vertices are connected by an edge if the matches that they represent are compatible with each other. Being compatible may mean, for instance, that the distances between the atoms within the two molecules are approximately equal, to within some given tolerance. A clique in this graph represents a set of matched pairs of atoms in which all the matches are compatible with each other. A special case of this method is the use of the modular product of graphs to reduce the problem of finding the maximum common induced subgraph of two graphs to the problem of finding a maximum clique in their product.\n\nIn automatic test pattern generation, finding cliques can help to bound the size of a test set. In bioinformatics, clique-finding algorithms have been used to infer evolutionary trees, predict protein structures, and find closely interacting clusters of proteins. Listing the cliques in a dependency graph is an important step in the analysis of certain random processes. In mathematics, Keller's conjecture on face-to-face tiling of hypercubes was disproved by , who used a clique-finding algorithm on an associated graph to find a counterexample.\n\nAn undirected graph is formed by a finite set of vertices and a set of unordered pairs of vertices, which are called edges. By convention, in algorithm analysis, the number of vertices in the graph is denoted by and the number of edges is denoted by . A clique in a graph is a complete subgraph of . That is, it is a subset of the vertices such that every two vertices in are the two endpoints of an edge in . A maximal clique is a clique to which no more vertices can be added. For each vertex that is not part of a maximal clique, there must be another vertex that is in the clique and non-adjacent to , preventing from being added to the clique. A maximum clique is a clique that includes the largest possible number of vertices. The clique number is the number of vertices in a maximum clique of .\n\nSeveral closely related clique-finding problems have been studied.\nThe first four of these problems are all important in practical applications. The clique decision problem is not of practical importance; it is formulated in this way in order to apply the theory of NP-completeness to clique-finding problems.\n\nThe clique problem and the independent set problem are complementary: a clique in is an independent set in the complement graph of and vice versa. Therefore, many computational results may be applied equally well to either problem, and some research papers do not clearly distinguish between the two problems. However, the two problems have different properties when applied to restricted families of graphs. For instance, the clique problem may be solved in polynomial time for planar graphs while the independent set problem remains NP-hard on planar graphs.\n\nA maximal clique, sometimes called inclusion-maximal, is a clique that is not included in a larger clique. Therefore, every clique is contained in a maximal clique.\nMaximal cliques can be very small. A graph may contain a non-maximal clique with many vertices and a separate clique of size 2 which is maximal. While a maximum (i.e., largest) clique is necessarily maximal, the converse does not hold. There are some types of graphs in which every maximal clique is maximum; these are the complements of the well-covered graphs, in which every maximal independent set is maximum.\nHowever, other graphs have maximal cliques that are not maximum.\n\nA single maximal clique can be found by a straightforward greedy algorithm. Starting with an arbitrary clique (for instance, any single vertex or even the empty set), grow the current clique one vertex at a time by looping through the graph's remaining vertices. For each vertex that this loop examines, add to the clique if it is adjacent to every vertex that is already in the clique, and discard otherwise. This algorithm runs in linear time. \nBecause of the ease of finding maximal cliques, and their potential small size, more attention has been given to the much harder algorithmic problem of finding a maximum or otherwise large clique than has been given to the problem of finding a single maximal clique.\nHowever, some research in parallel algorithms has studied the problem of finding a maximal clique. In particular, the problem of finding the lexicographically first maximal clique (the one found by the algorithm above) has been shown to be complete for the class of polynomial-time functions. This result implies that the problem is unlikely to be solvable within the parallel complexity class NC.\n\nOne can test whether a graph contains a -vertex clique, and find any such clique that it contains, using a brute force algorithm. This algorithm examines each subgraph with vertices and checks to see whether it forms a clique. It takes time , as expressed using big O notation.\nThis is because there are subgraphs to check, each of which has edges whose presence in needs to be checked. Thus, the problem may be solved in polynomial time whenever is a fixed constant. However, when does not have a fixed value, but instead may vary as part of the input to the problem, the time is exponential.\n\nThe simplest nontrivial case of the clique-finding problem is finding a triangle in a graph, or equivalently determining whether the graph is triangle-free.\nIn a graph with edges, there may be at most triangles (using big theta notation to indicate that this bound is tight). The worst case for this formula occurs when is itself a clique. Therefore, algorithms for listing all triangles must take at least time in the worst case (using big omega notation), and algorithms are known that match this time bound. For instance, describe an algorithm that sorts the vertices in order from highest degree to lowest and then iterates through each vertex in the sorted list, looking for triangles that include and do not include any previous vertex in the list. To do so the algorithm marks all neighbors of , searches through all edges incident to a neighbor of outputting a triangle for every edge that has two marked endpoints, and then removes the marks and deletes from the graph. As the authors show, the time for this algorithm is proportional to the arboricity of the graph (denoted ) multiplied by the number of edges, which is . Since the arboricity is at most , this algorithm runs in time . More generally, all -vertex cliques can be listed by a similar algorithm that takes time proportional to the number of edges multiplied by the arboricity to the power . For graphs of constant arboricity, such as planar graphs (or in general graphs from any non-trivial minor-closed graph family), this algorithm takes time, which is optimal since it is linear in the size of the input.\n\nIf one desires only a single triangle, or an assurance that the graph is triangle-free, faster algorithms are possible. As observe, the graph contains a triangle if and only if its adjacency matrix and the square of the adjacency matrix contain nonzero entries in the same cell. Therefore, fast matrix multiplication techniques such as the Coppersmith–Winograd algorithm can be applied to find triangles in time . used fast matrix multiplication to improve the algorithm for finding triangles to . These algorithms based on fast matrix multiplication have also been extended to problems of finding -cliques for larger values of .\n\nBy a result of , every -vertex graph has at most maximal cliques. They can be listed by the Bron–Kerbosch algorithm, a recursive backtracking procedure of . The main recursive subroutine of this procedure has three arguments: a partially constructed (non-maximal) clique, a set of candidate vertices that could be added to the clique, and another set of vertices that should not be added (because doing so would lead to a clique that has already been found). The algorithm tries adding the candidate vertices one by one to the partial clique, making a recursive call for each one. After trying each of these vertices, it moves it to the set of vertices that should not be added again. Variants of this algorithm can be shown to have worst-case running time , matching the number of cliques that might need to be listed. Therefore, this provides a worst-case-optimal solution to the problem of listing all maximal cliques. Further, the Bron–Kerbosch algorithm has been widely reported as being faster in practice than its alternatives.\n\nHowever, when the number of cliques is significantly smaller than its worst case, other algorithms might be preferable. As showed, it is also possible to list all maximal cliques in a graph in an amount of time that is polynomial per generated clique. An algorithm such as theirs in which the running time depends on the output size is known as an output-sensitive algorithm. Their algorithm is based on the following two observations, relating the maximal cliques of the given graph to the maximal cliques of a graph formed by removing an arbitrary vertex from :\nUsing these observations they can generate all maximal cliques in by a recursive algorithm that chooses a vertex arbitrarily and then, for each maximal clique in , outputs both and the clique formed by adding to and removing the non-neighbors of . However, some cliques of may be generated in this way from more than one parent clique of , so they eliminate duplicates by outputting a clique in only when its parent in is lexicographically maximum among all possible parent cliques. On the basis of this principle, they show that all maximal cliques in may be generated in time per clique, where is the number of edges in and is the number of vertices. improve this to per clique, where is the arboricity of the given graph. provide an alternative output-sensitive algorithm based on fast matrix multiplication. show that it is even possible to list all maximal cliques in lexicographic order with polynomial delay per clique. However, the choice of ordering is important for the efficiency of this algorithm: for the reverse of this order,\nthere is no polynomial-delay algorithm unless P = NP.\n\nOn the basis of this result, it is possible to list all maximal cliques in polynomial time, for families of graphs in which the number of cliques is polynomially bounded. These families include chordal graphs, complete graphs, triangle-free graphs, interval graphs, graphs of bounded boxicity, and planar graphs. In particular, the planar graphs have cliques, of at most constant size, that can be listed in linear time. The same is true for any family of graphs that is both sparse (having a number of edges at most a constant times the number of vertices) and closed under the operation of taking subgraphs.\n\nIt is possible to find the maximum clique, or the clique number, of an arbitrary \"n\"-vertex graph in time by using one of the algorithms described above to list all maximal cliques in the graph and returning the largest one. However, for this variant of the clique problem better worst-case time bounds are possible. The algorithm of solves this problem in time . It is a recursive backtracking scheme similar to that of the Bron–Kerbosch algorithm, but is able to eliminate some recursive calls when it can be shown that the cliques found within the call will be suboptimal. improved the time to , and improved it to time, at the expense of greater space usage. Robson's algorithm combines a similar backtracking scheme (with a more complicated case analysis) and a dynamic programming technique in which the optimal solution is precomputed for all small connected subgraphs of the complement graph. These partial solutions are used to shortcut the backtracking recursion. The fastest algorithm known today is a refined version of this method by which runs in time .\n\nThere has also been extensive research on heuristic algorithms for solving maximum clique problems without worst-case runtime guarantees, based on methods including branch and bound, local search, greedy algorithms, and constraint programming. Non-standard computing methodologies that have been suggested for finding cliques include DNA computing and adiabatic quantum computation. The maximum clique problem was the subject of an implementation challenge sponsored by DIMACS in 1992–1993, and a collection of graphs used as benchmarks for the challenge, which is publicly available.\n\nPlanar graphs, and other families of sparse graphs, have been discussed above: they have linearly many maximal cliques, of bounded size, that can be listed in linear time. In particular, for planar graphs, any clique can have at most four vertices, by Kuratowski's theorem.\n\nPerfect graphs are defined by the properties that their clique number equals their chromatic number, and that this equality holds also in each of their induced subgraphs. For perfect graphs, it is possible to find a maximum clique in polynomial time, using an algorithm based on semidefinite programming.\nHowever, this method is complex and non-combinatorial, and specialized clique-finding algorithms have been developed for many subclasses of perfect graphs. In the complement graphs of bipartite graphs, Kőnig's theorem allows the maximum clique problem to be solved using techniques for matching. In another class of perfect graphs, the permutation graphs, a maximum clique is a longest decreasing subsequence of the permutation defining the graph and can be found using known algorithms for the longest decreasing subsequence problem. Conversely, every instance of the longest decreasing subsequence problem can be described equivalently as a problem of finding a maximum clique in a permutation graph. provide an alternative quadratic-time algorithm for maximum cliques in comparability graphs, a broader class of perfect graphs that includes the permutation graphs as a special case. In chordal graphs, the maximal cliques can be found by listing the vertices in an elimination ordering, and checking the clique neighborhoods of each vertex in this ordering.\n\nIn some cases, these algorithms can be extended to other, non-perfect, classes of graphs as well. For instance, in a circle graph, the neighborhood of each vertex is a permutation graph, so a maximum clique in a circle graph can be found by applying the permutation graph algorithm to each neighborhood. Similarly, in a unit disk graph (with a known geometric representation), there is a polynomial time algorithm for maximum cliques based on applying the algorithm for complements of bipartite graphs to shared neighborhoods of pairs of vertices.\n\nThe algorithmic problem of finding a maximum clique in a random graph drawn from the Erdős–Rényi model (in which each edge appears with probability , independently from the other edges) was suggested by . Because the maximum clique in a random graph has logarithmic size with high probability, it\ncan be found by a brute force search in expected time . This is a quasi-polynomial time bound. Although the clique number of such graphs is usually very close to , simple greedy algorithms as well as more sophisticated randomized approximation techniques only find cliques with size , half as big. The number of maximal cliques in such graphs is with high probability exponential in , which prevents methods that list all maximal cliques from running in polynomial time. Because of the difficulty of this problem, several authors have investigated the planted clique problem, the clique problem on random graphs that have been augmented by adding large cliques. While spectral methods and semidefinite programming can detect hidden cliques of size , no polynomial-time algorithms are currently known to detect those of size (expressed using little-o notation).\n\nSeveral authors have considered approximation algorithms that attempt to find a clique or independent set that, although not maximum, has size as close to the maximum as can be found in polynomial time.\nAlthough much of this work has focused on independent sets in sparse graphs, a case that does not make sense for the complementary clique problem, there has also been work on approximation algorithms that do not use such sparsity assumptions.\n\nThe clique decision problem is NP-complete. It was one of Richard Karp's original 21 problems shown NP-complete in his 1972 paper \"Reducibility Among Combinatorial Problems\". This problem was also mentioned in Stephen Cook's paper introducing the theory of NP-complete problems. Because of the hardness of the decision problem, the problem of finding a maximum clique is also NP-hard. If one could solve it, one could also solve the decision problem, by comparing the size of the maximum clique to the size parameter given as input in the decision problem.\n\nKarp's NP-completeness proof is a many-one reduction from the Boolean satisfiability problem.\nIt describes how to translate Boolean formulas in conjunctive normal form (CNF) into equivalent instances of the maximum clique problem.\nSatisfiability, in turn, was proved NP-complete in the Cook–Levin theorem. From a given CNF formula, Karp forms a graph that has a vertex for every pair , where is a variable or its negation and is a clause in the formula that contains . Two of these vertices are connected by an edge if they represent compatible variable assignments for different clauses. That is, there is an edge from to whenever and and are not each other's negations. If denotes the number of clauses in the CNF formula, then the -vertex cliques in this graph represent consistent ways of assigning truth values to some of its variables in order to satisfy the formula. Therefore, the formula is satisfiable if and only if a -vertex clique exists.\n\nSome NP-complete problems (such as the travelling salesman problem in planar graphs) may be solved in time that is exponential in a sublinear function of the input size parameter ,\nsignificantly faster than a brute-force search.\nHowever, it is unlikely that such a subexponential time bound is possible for the clique problem in arbitrary graphs, as it would imply similarly subexponential bounds for many other standard NP-complete problems.\n\nThe computational difficulty of the clique problem has led it to be used to prove several lower bounds in circuit complexity. The existence of a clique of a given size is a monotone graph property, meaning that, if a clique exists in a given graph, it will exist in any supergraph. Because this property is monotone, there must exist a monotone circuit, using only and gates and or gates, to solve the clique decision problem for a given fixed clique size. However, the size of these circuits can be proven to be a super-polynomial function of the number of vertices and the clique size, exponential in the cube root of the number of vertices. Even if a small number of NOT gates are allowed, the complexity remains superpolynomial. Additionally, the depth of a monotone circuit for the clique problem using gates of bounded fan-in must be at least a polynomial in the clique size.\n\nThe (deterministic) decision tree complexity of determining a graph property is the number of questions of the form \"Is there an edge between vertex and vertex ?\" that have to be answered in the worst case to determine whether a graph has a particular property. That is, it is the minimum height of a boolean decision tree for the problem. There are possible questions to be asked. Therefore, any graph property can be determined with at most questions. It is also possible to define random and quantum decision tree complexity of a property, the expected number of questions (for a worst case input) that a randomized or quantum algorithm needs to have answered in order to correctly determine whether the given graph has the property.\n\nBecause the property of containing a clique is monotone, it is covered by the Aanderaa–Karp–Rosenberg conjecture, which states that the deterministic decision tree complexity of determining any non-trivial monotone graph property is exactly . For arbitrary monotone graph properties, this conjecture remains unproven. However, for deterministic decision trees, and for any in the range , the property of containing a -clique was shown to have decision tree complexity exactly by . Deterministic decision trees also require exponential size to detect cliques, or large polynomial size to detect cliques of bounded size.\n\nThe Aanderaa–Karp–Rosenberg conjecture also states that the randomized decision tree complexity of non-trivial monotone functions is . The conjecture again remains unproven, but has been resolved for the property of containing a clique for . This property is known to have randomized decision tree complexity . For quantum decision trees, the best known lower bound is , but no matching algorithm is known for the case of .\n\nParameterized complexity is the complexity-theoretic study of problems that are naturally equipped with a small integer parameter and for which the problem becomes more difficult as increases, such as finding -cliques in graphs. A problem is said to be fixed-parameter tractable if there is an algorithm for solving it on inputs of size , and a function , such that the algorithm runs in time . That is, it is fixed-parameter tractable if it can be solved in polynomial time for any fixed value of and moreover if the exponent of the polynomial does not depend on .\n\nFor finding -vertex cliques, the brute force search algorithm has running time . Because the exponent of depends on , this algorithm is not fixed-parameter tractable.\nAlthough it can be improved by fast matrix multiplication the running time still has an exponent that is linear in Thus, although the running time of known algorithms for the clique problem is polynomial for any fixed these algorithms do not suffice for fixed-parameter tractability. defined a hierarchy of parametrized problems, the W hierarchy, that they conjectured did not have fixed-parameter tractable algorithms. They proved that independent set (or, equivalently, clique) is hard for the first level of this hierarchy, W[1]. Thus, according to their conjecture, clique has no fixed-parameter tractable algorithm. Moreover, this result provides the basis for proofs of W[1]-hardness of many other problems, and thus serves as an analogue of the Cook–Levin theorem for parameterized complexity.\n\nAlthough the problems of listing maximal cliques or finding maximum cliques are unlikely to be fixed-parameter tractable with the parameter , they may be fixed-parameter tractable for other parameters of instance complexity. For instance, both problems are known to be fixed-parameter tractable when parametrized by the degeneracy of the input graph.\n\nWeak results hinting that the clique problem might be hard to approximate have been known for a long time. observed that, because of the fact that the clique number takes on small integer values and is NP-hard to compute, it cannot have a fully polynomial-time approximation scheme. If too accurate an approximation were available, rounding its value to an integer would give the exact clique number. However, little more was known until the early 1990s, when several authors began to make connections between the approximation of maximum cliques and probabilistically checkable proofs. They used these connections to prove hardness of approximation results for the maximum clique problem.\nAfter many improvements to these results it is now known that, for every real number , there can be no polynomial time algorithm that approximates the maximum clique to within a factor better than , unless P = NP.\n\nThe rough idea of these inapproximability results is to form a graph that represents a probabilistically checkable proof system for an NP-complete problem such as the Boolean satisfiability problem. In a probabilistically checkable proof system, a proof is represented as a sequence of bits. An instance of the satisfiability problem should have a valid proof if and only if it is satisfiable. The proof is checked by an algorithm that, after a polynomial-time computation on the input to the satisfiability problem, chooses to examine a small number of randomly chosen positions of the proof string. Depending on what values are found at that sample of bits, the checker will either accept or reject the proof, without looking at the rest of the bits. False negatives are not allowed: a valid proof must always be accepted. However, an invalid proof may sometimes mistakenly be accepted. For every invalid proof, the probability that the checker will accept it must be low.\n\nTo transform a probabilistically checkable proof system of this type into a clique problem, one forms a graph with a vertex for each possible accepting run of the proof checker. That is, a vertex is defined by one of the possible random choices of sets of positions to examine, and by bit values for those positions that would cause the checker to accept the proof. Two vertices are adjacent, in this graph, if the corresponding two accepting runs see the same bit values at every position they both examine. Each (valid or invalid) proof string corresponds to a clique, the set of accepting runs that see that proof string, and all maximal cliques arise in this way. One of these cliques is large if and only if it corresponds to a proof string that many proof checkers accept. If the original satisfiability instance is satisfiable, it will have a valid proof string, one that is accepted by all runs of the checker, and this string will correspond to a large clique in the graph. However, if the original instance is not satisfiable, then all proof strings are invalid, each proof string has only a small number of checker runs that mistakenly accept it, and all cliques are small. Therefore, if one could distinguish in polynomial time between graphs that have large cliques and graphs in which all cliques are small, or if one could accurately approximate the clique problem, then applying this approximation to the graphs generated from satisfiability instances would allow satisfiable instances to be distinguished from unsatisfiable instances. However, this is not possible unless P = NP.\n\n"}
{"id": "5235067", "url": "https://en.wikipedia.org/wiki?curid=5235067", "title": "Coherent risk measure", "text": "Coherent risk measure\n\nIn the fields of actuarial science and financial economics there are a number of ways that risk can be defined; to clarify the concept theoreticians have described a number of properties that a risk measure might or might not have. A coherent risk measure is a function formula_1 that satisfies properties of monotonicity, sub-additivity, homogeneity, and translational invariance.\n\nConsider a random outcome formula_2 viewed as an element of a linear space formula_3 of measurable functions, defined on an appropriate probability space. A functional formula_4 → formula_5 is said to be coherent risk measure for formula_3 if it satisfies the following properties:\n\nThat is, the risk of holding no assets is zero.\n\nThat is, if portfolio formula_9 always has better values than portfolio formula_10 under almost all scenarios then the risk of formula_9 should be less than the risk of formula_10. E.g. If formula_10 is an in the money call option (or otherwise) on a stock, and formula_9 is also an in the money call option with a lower strike price. \nIn financial risk management, monotonicity implies a portfolio with greater future returns has less risk.\n\nIndeed, the risk of two portfolios together cannot get any worse than adding the two risks separately: this is the diversification principle.\nIn financial risk management, sub-additivity implies diversification is beneficial. \n\nLoosely speaking, if you double your portfolio then you double your risk.\nIn financial risk management, positive homogeneity implies the risk of a position is proportional to its size.\n\nIf formula_17 is a deterministic portfolio with guaranteed return formula_18 and formula_19 then\nThe portfolio formula_17 is just adding cash formula_22 to your portfolio formula_23. In particular, if formula_24 then formula_25. \nIn financial risk management, translation invariance implies that the addition of a sure amount of capital reduces the risk by the same amount.\n\nThe notion of coherence has been subsequently relaxed. Indeed, the notions of Sub-additivity and Positive Homogeneity can be replaced by the notion of convexity:\n\nIt is well known that value at risk is not a coherent risk measure as it does not respect the sub-additivity property. An immediate consequence is that value at risk might discourage diversification.\nValue at risk is, however, coherent, under the assumption of elliptically distributed losses (e.g. normally distributed) when the portfolio value is a linear function of the asset prices. However, in this case the value at risk becomes equivalent to a mean-variance approach where the risk of a portfolio is measured by the variance of the portfolio's return.\n\nThe Wang transform function (distortion function) for the Value at Risk is formula_27. The non-concavity of formula_28 proves the non coherence of this risk measure.\n\n\nAs a simple example to demonstrate the non-coherence of value-at-risk consider looking at the VaR of a portfolio at 95% confidence over the next year of two default-able zero coupon bonds that mature in 1 years time denominated in our numeraire currency.\n\nAssume the following:\n\nUnder these conditions the 95% VaR for holding either of the bonds is 0 since the probability of default is less than 5%. However if we held a portfolio that consisted of 50% of each bond by value then the 95% VaR is 35% (= 0.5*0.7 + 0.5*0) since the probability of at least one of the bonds defaulting is 7.84% which exceeds 5%. This violates the sub-additivity property showing that VaR is not a coherent risk measure.\n\nThe average value at risk (sometimes called expected shortfall or conditional value-at-risk or formula_29) is a coherent risk measure, even though it is derived from Value at Risk which is not. The domain can be extended for more general Orlitz Hearts from the more typical Lp spaces.\n\nThe entropic value at risk is a coherent risk measure.\n\nThe tail value at risk (or tail conditional expectation) is a coherent risk measure only when the underlying distribution is continuous.\n\nThe Wang transform function (distortion function) for the tail value at risk is formula_30. The concavity of formula_28 proves the coherence of this risk measure in the case of continuous distribution.\n\nThe PH risk measure (or Proportional Hazard Risk measure) transforms the hazard rates formula_32 using a coefficient formula_33.\n\nThe Wang transform function (distortion function) for the PH risk measure is formula_34. The concavity of formula_28 if formula_36 proves the coherence of this risk measure.\ng-entropic risk measures are a class of information-theoretic coherent risk measures that involve some important cases such as CVaR and EVaR.\n\nThe Wang risk measure is defined by the following Wang transform function (distortion function) formula_37. The coherence of this risk measure is a consequence of the concavity of formula_28.\n\nThe entropic risk measure is a convex risk measure which is not coherent. It is related to the exponential utility.\n\nThe superhedging price is a coherent risk measure.\n\nIn a situation with formula_39-valued portfolios such that risk can be measured in formula_40 of the assets, then a set of portfolios is the proper way to depict risk. Set-valued risk measures are useful for markets with transaction costs.\n\nA set-valued coherent risk measure is a function formula_41, where formula_42 and formula_43 where formula_44 is a constant solvency cone and formula_45 is the set of portfolios of the formula_46 reference assets. formula_47 must have the following properties:\n\n\n\n\n\n\nA Wang transform of the cumulative distribution function is an increasing function formula_51 where formula_52 and formula_53. This function is called \"distortion function\" or Wang transform function.\n\nThe \"dual distortion function\" is formula_54. \nGiven a probability space formula_55, then for any random variable formula_56 and any distortion function formula_57 we can define a new probability measure formula_58 such that for any formula_59 it follows that\nformula_60 \n\n\nFor any increasing concave Wang transform function, we could define a corresponding premium principle :\nformula_61\n\n\nA coherent risk measure could be defined by a Wang transform of the cumulative distribution function formula_57 if and only if formula_57 is concave.\n\nIf instead of the sublinear property,\"R\" is convex, then \"R\" is a set-valued convex risk measure.\n\nA lower semi-continuous convex risk measure formula_1 can be represented as\nsuch that formula_66 is a penalty function and formula_67 is the set of probability measures absolutely continuous with respect to \"P\" (the \"real world\" probability measure), i.e. formula_68. The dual characterization is tied to formula_69 spaces, Orlitz hearts, and their dual spaces.\n\nA lower semi-continuous risk measure is coherent if and only if it can be represented as\nsuch that formula_71.\n\n"}
{"id": "303500", "url": "https://en.wikipedia.org/wiki?curid=303500", "title": "Completing the square", "text": "Completing the square\n\nIn elementary algebra, completing the square is a technique for converting a quadratic polynomial of the form\n\nto the form\n\nfor some values of \"h\" and \"k\".\n\nCompleting the square is used in\n\nIn mathematics, completing the square is often applied in any computation involving quadratic polynomials. Completing the square is also used to derive the quadratic formula.\n\nThere is a simple formula in elementary algebra for computing the square of a binomial:\n\nFor example:\n\nIn any perfect square, the coefficient of \"x\" is twice the number \"p\", and the constant term is equal to \"p\".\n\nConsider the following quadratic polynomial:\n\nThis quadratic is not a perfect square, since 28 is not the square of 5:\n\nHowever, it is possible to write the original quadratic as the sum of this square and a constant:\n\nThis is called completing the square.\n\nGiven any monic quadratic\n\nit is possible to form a square that has the same first two terms:\n\nThis square differs from the original quadratic only in the value of the constant\nterm. Therefore, we can write\n\nwhere formula_11. This operation is known as completing the square.\nFor example:\n\nGiven a quadratic polynomial of the form\nit is possible to factor out the coefficient \"a\", and then complete the square for the resulting monic polynomial.\n\nExample:\nThis allows us to write any quadratic polynomial in the form\n\nThe result of completing the square may be written as a formula. For the general case:\n\nSpecifically, when \"a\"=1:\n\nThe matrix case looks very similar:\n\nwhere formula_19 has to be symmetric.\n\nIf formula_19 is not symmetric the formulae for formula_21 and formula_22 have\nto be generalized to:\n\nIn analytic geometry, the graph of any quadratic function is a parabola in the \"xy\"-plane. Given a quadratic polynomial of the form\n\nthe numbers \"h\" and \"k\" may be interpreted as the Cartesian coordinates of the vertex (or stationary point) of the parabola. That is, \"h\" is the \"x\"-coordinate of the axis of symmetry (i.e. the axis of symmetry has equation \"x=h\"), and \"k\" is the minimum value (or maximum value, if \"a\" < 0) of the quadratic function.\n\nOne way to see this is to note that the graph of the function \"ƒ\"(\"x\") = \"x\" is a parabola whose vertex is at the origin (0, 0). Therefore, the graph of the function \"ƒ\"(\"x\" − \"h\") = (\"x\" − \"h\") is a parabola shifted to the right by \"h\" whose vertex is at (\"h\", 0), as shown in the top figure. In contrast, the graph of the function \"ƒ\"(\"x\") + \"k\" = \"x\" + \"k\" is a parabola shifted upward by \"k\" whose vertex is at (0, \"k\"), as shown in the center figure. Combining both horizontal and vertical shifts yields \"ƒ\"(\"x\" − \"h\") + \"k\" = (\"x\" − \"h\") + \"k\" is a parabola shifted to the right by \"h\" and upward by \"k\" whose vertex is at (\"h\", \"k\"), as shown in the bottom figure.\n\nCompleting the square may be used to solve any quadratic equation. For example:\n\nThe first step is to complete the square:\n\nNext we solve for the squared term:\n\nThen either\n\nand therefore\n\nThis can be applied to any quadratic equation. When the \"x\" has a coefficient other than 1, the first step is to divide out the equation by this coefficient: for an example see the non-monic case below.\n\nUnlike methods involving factoring the equation, which is reliable only if the roots are rational, completing the square will find the roots of a quadratic equation even when those roots are irrational or complex. For example, consider the equation\n\nCompleting the square gives\n\nso\n\nThen either\n\nIn terser language:\n\nso\n\nEquations with complex roots can be handled in the same way. For example:\n\nFor an equation involving a non-monic quadratic, the first step to solving them is to divide through by the coefficient of \"x\". For example:\n\nApplying this procedure to the general form of a quadratic equation leads to the quadratic formula.\n\nCompleting the square may be used to evaluate any integral of the form\n\nusing the basic integrals\n\nFor example, consider the integral\n\nCompleting the square in the denominator gives:\n\nThis can now be evaluated by using the substitution\n\"u\" = \"x\" + 3, which yields\n\nConsider the expression\n\nwhere \"z\" and \"b\" are complex numbers, \"z\" and \"b\" are the complex conjugates of \"z\" and \"b\", respectively, and \"c\" is a real number. Using the identity |\"u\"| = \"uu\" we can rewrite this as\n\nwhich is clearly a real quantity. This is because\n\nAs another example, the expression\n\nwhere \"a\", \"b\", \"c\", \"x\", and \"y\" are real numbers, with \"a\" > 0 and \"b\" > 0, may be expressed in terms of the square of the absolute value of a complex number. Define\n\nThen\n\nso\n\nA matrix \"M\" is idempotent when \"M\"  = \"M\". Idempotent matrices generalize the idempotent properties of 0 and 1. The completion of the square method of addressing the equation \nshows that some idempotent 2 × 2 matrices are parametrized by a circle in the (\"a,b\")-plane:\n\nThe matrix formula_51 will be idempotent provided formula_50 which, upon completing the square, becomes\nIn the (\"a,b\")-plane, this is the equation of a circle with center (1/2, 0) and radius 1/2.\n\nConsider completing the square for the equation\n\nSince \"x\" represents the area of a square with side of length \"x\", and \"bx\" represents the area of a rectangle with sides \"b\" and \"x\", the process of completing the square can be viewed as visual manipulation of rectangles.\n\nSimple attempts to combine the \"x\" and the \"bx\" rectangles into a larger square result in a missing corner. The term (\"b\"/2) added to each side of the above equation is precisely the area of the missing corner, whence derives the terminology \"completing the square\".\n\nAs conventionally taught, completing the square consists of adding the third term, \"v\" to\n\nto get a square. There are also cases in which one can add the middle term, either 2\"uv\" or −2\"uv\", to\n\nto get a square.\n\nBy writing\n\nwe show that the sum of a positive number \"x\" and its reciprocal is always greater than or equal to 2. The square of a real expression is always greater than or equal to zero, which gives the stated bound; and here we achieve 2 just when \"x\" is 1, causing the square to vanish.\n\nConsider the problem of factoring the polynomial\n\nThis is\n\nso the middle term is 2(\"x\")(18) = 36\"x\". Thus we get\n\n(the last line being added merely to follow the convention of decreasing degrees of terms).\n\n\n"}
{"id": "1639442", "url": "https://en.wikipedia.org/wiki?curid=1639442", "title": "Controlled Cryptographic Item", "text": "Controlled Cryptographic Item\n\nA Controlled Cryptographic Item (CCI) is a U.S. National Security Agency term for secure telecommunications or information handling equipment, associated cryptographic component or other hardware item which performs a critical communications security (COMSEC) function. Items so designated may be unclassified but are subject to special accounting controls and required markings.\n\nPart of the physical security protection given to COMSEC equipment and material is afforded by its special handling and accounting. CCI equipment must be controlled in a manner that affords protection at least equal to that normally provided other high value equipment, such as money, computers, and Privacy Act-controlled. There are two separate channels used for the handling of such equipment and materials: \"the COMSEC channel\" and \"the administrative channel.\" The COMSEC channel, called the COMSEC Material Control System, is used to distribute accountable COMSEC items such as classified and CCI equipment, keying material, and maintenance manuals. Some military departments have been authorized to distribute CCI equipment through their standard logistics system.\n\nThe COMSEC channel is composed of a series of COMSEC accounts, each of which has an appointed COMSEC Custodian who is personally responsible and accountable for all COMSEC materials charged to his/her account. The COMSEC Custodian assumes accountability for the equipment or material upon receipt, then controls its dissemination to authorized individuals on job requirements and a need-to-know basis. The administrative channel is used to distribute COMSEC information other than that which is accountable in the COMSEC Material Control System.\n\nPersons with access to COMSEC materials are asked, among other restrictions, to avoid unapproved travel to any countries which are adversaries of the United States, or their establishments or facilities within the U.S.\n\n"}
{"id": "50510967", "url": "https://en.wikipedia.org/wiki?curid=50510967", "title": "Daniel Bump", "text": "Daniel Bump\n\nDaniel Willis Bump (born 1952) is a mathematician who is a professor at Stanford University. He is a fellow of the American Mathematical Society since 2015, for \"contributions to number theory, representation theory, combinatorics, and random matrix theory, as well as mathematical exposition\".\n\nHe obtained his Ph.D. from the University of Chicago in 1982 under the supervision of Walter Lewis Baily, Jr. Among his students is president of the National Association of Mathematicians Edray Goins.\n\n\n"}
{"id": "1156527", "url": "https://en.wikipedia.org/wiki?curid=1156527", "title": "Detection theory", "text": "Detection theory\n\nDetection theory or signal detection theory is a means to measure the ability to differentiate between information-bearing patterns (called stimulus in living organisms, signal in machines) and random patterns that distract from the information (called noise, consisting of background stimuli and random activity of the detection machine and of the nervous system of the operator). In the field of electronics, the separation of such patterns from a disguising background is referred to as signal recovery.\n\nAccording to the theory, there are a number of determiners of how a detecting system will detect a signal, and where its threshold levels will be. The theory can explain how changing the threshold will affect the ability to discern, often exposing how adapted the system is to the task, purpose or goal at which it is aimed.\n\nAnother field which is closely related to signal detection theory is called compressed sensing (or compressive sensing). The objective of compressed sensing is to recover high dimensional but with low complexity entities from only a few measurements. Thus, one of the most important applications of compressed sensing is in the recovery of high dimensional signals which are known to be sparse (or nearly sparse) with only a few linear measurements. The number of measurements needed in the recovery of signals is by far smaller than what Nyquist sampling theorem requires provided that the signal is sparse, meaning that it only contains a few non-zero elements. There are different methods of signal recovery in compressed sensing including basis pursuit , expander recovery algorithm, CoSaMP and also fast non-iterative algorithm. In all of the recovery methods mentioned above, choosing an appropriate measurement matrix using probabilistic constructions or deterministic constructions, is of great importance. In other words, measurement matrices must satisfy certain specific conditions such as RIP (Restricted Isometry Property) or Null-Space property in order to achieve robust sparse recovery.\n\nBack to the detecting theory, when the detecting system is a human being, characteristics such as experience, expectations, physiological state (e.g., fatigue) and other factors can affect the threshold applied. For instance, a sentry in wartime might be likely to detect fainter stimuli than the same sentry in peacetime due to a lower criterion, however they might also be more likely to treat innocuous stimuli as a threat.\n\nMuch of the early work in detection theory was done by radar researchers. By 1954, the theory was fully developed on the theoretical side as described by Peterson, Birdsall and Fox and the foundation for the psychological theory was made by Wilson P. Tanner, David M. Green, and John A. Swets, also in 1954.\nDetection theory was used in 1966 by John A. Swets and David M. Green for psychophysics. Green and Swets criticized the traditional methods of psychophysics for their inability to discriminate between the real sensitivity of subjects and their (potential) response biases.\n\nDetection theory has applications in many fields such as diagnostics of any kind, quality control, telecommunications, and psychology. The concept is similar to the signal to noise ratio used in the sciences and confusion matrices used in artificial intelligence. It is also usable in alarm management, where it is important to separate important events from background noise.\n\nSignal detection theory (SDT) is used when psychologists want to measure the way we make decisions under conditions of uncertainty, such as how we would perceive distances in foggy conditions or during eyewitness identification. SDT assumes that the decision maker is not a passive receiver of information, but an active decision-maker who makes difficult perceptual judgments under conditions of uncertainty. In foggy circumstances, we are forced to decide how far away from us an object is, based solely upon visual stimulus which is impaired by the fog. Since the brightness of the object, such as a traffic light, is used by the brain to discriminate the distance of an object, and the fog reduces the brightness of objects, we perceive the object to be much farther away than it actually is (see also decision theory). According to SDT, during eyewitness identifications, witnesses base their decision as to whether a suspect is the culprit or not based on their perceived level of familiarity with the suspect.\n\nTo apply signal detection theory to a data set where stimuli were either present or absent, and the observer categorized each trial as having the stimulus present or absent, the trials are sorted into one of four categories:\n\nBased on the proportions of these types of trials, numerical estimates of sensitivity can be obtained with statistics like the sensitivity index \"d\"' and A', and response bias can be estimated with statistics like c and β.\n\nSignal detection theory can also be applied to memory experiments, where items are presented on a study list for later testing. A test list is created by combining these 'old' items with novel, 'new' items that did not appear on the study list. On each test trial the subject will respond 'yes, this was on the study list' or 'no, this was not on the study list'. Items presented on the study list are called Targets, and new items are called Distractors. Saying 'Yes' to a target constitutes a Hit, while saying 'Yes' to a distractor constitutes a False Alarm.\n\nSignal Detection Theory has wide application, both in humans and animals. Topics include memory, stimulus characterists of schedules of reinforcement, etc.\n\nConceptually, sensitivity refers to how hard or easy it is to detect that a target stimulus is present from background events. For example, in a recognition memory paradigm, having longer to study to-be-remembered words makes it easier to recognize previously seen or heard words. In contrast, having to remember 30 words rather than 5 makes the discrimination harder. One of the most commonly used statistics for computing sensitivity is the so-called sensitivity index or \"d\"'. There are also non-parametric measures, such as the area under the ROC-curve.\n\nBias is the extent to which one response is more probable than another. That is, a receiver may be more likely to respond that a stimulus is present or more likely to respond that a stimulus is not present. Bias is independent of sensitivity. For example, if there is a penalty for either false alarms or misses, this may influence bias. If the stimulus is a bomber, then a miss (failing to detect the plane) may increase deaths, so a liberal bias is likely. In contrast, crying wolf (a false alarm) too often may make people less likely to respond, grounds for a conservative bias.\n\nIn the case of making a decision between two hypotheses, \"H1\", absent, and \"H2\", present, in the event of a particular observation, \"y\", a classical approach is to choose \"H1\" when \"p(H1|y) > p(H2|y)\" and \"H2\" in the reverse case. In the event that the two \"a posteriori\" probabilities are equal, one might choose to default to a single choice (either always choose \"H1\" or always choose \"H2\"), or might randomly select either \"H1\" or \"H2\". The \"a priori\" probabilities of \"H1\" and \"H2\" can guide this choice, e.g. by always choosing the hypothesis with the higher \"a priori\" probability.\n\nWhen taking this approach, usually what one knows are the conditional probabilities, \"p(y|H1)\" and \"p(y|H2)\", and the \"a priori\" probabilities formula_1 and formula_2. In this case,\n\nformula_3,\n\nformula_4\n\nwhere \"p(y)\" is the total probability of event \"y\",\n\nformula_5.\n\n\"H2\" is chosen in case\n\nformula_6\n\nformula_7\n\nand \"H1\" otherwise.\n\nOften, the ratio formula_8 is called formula_9 and formula_10 is called formula_11, the \"likelihood ratio\".\n\nUsing this terminology, \"H2\" is chosen in case formula_12. This is called MAP testing, where MAP stands for \"maximum \"a posteriori\"\").\n\nTaking this approach minimizes the expected number of errors one will make.\n\nIn some cases, it is far more important to respond appropriately to \"H1\" than it is to respond appropriately to \"H2\". For example, if an alarm goes off, indicating H1 (an incoming bomber is carrying a nuclear weapon), it is much more important to shoot down the bomber if H1 = TRUE, than it is to avoid sending a fighter squadron to inspect a false alarm (i.e., H1 = FALSE, H2 = TRUE) (assuming a large supply of fighter squadrons). The Bayes criterion is an approach suitable for such cases.\n\nHere a utility is associated with each of four situations:\n\nAs is shown below, what is important are the differences, formula_17 and formula_18.\n\nSimilarly, there are four probabilities, formula_19, formula_20, etc., for each of the cases (which are dependent on one's decision strategy).\n\nThe Bayes criterion approach is to maximize the expected utility:\n\nformula_21\n\nformula_22\n\nformula_23\n\nEffectively, one may maximize the sum,\n\nformula_24,\n\nand make the following substitutions:\n\nformula_25\n\nformula_26\n\nwhere formula_27 and formula_28 are the \"a priori\" probabilities, formula_29 and formula_30, and formula_31 is the region of observation events, \"y\", that are responded to as though \"H1\" is true.\n\nformula_32\n\nformula_33 and thus formula_34 are maximized by extending formula_31 over the region where\n\nformula_36\n\nThis is accomplished by deciding H2 in case\n\nformula_37\n\nformula_38\n\nand H1 otherwise, where \"L(y)\" is the so-defined \"likelihood ratio\".\n\n\n"}
{"id": "47021614", "url": "https://en.wikipedia.org/wiki?curid=47021614", "title": "Dualizing sheaf", "text": "Dualizing sheaf\n\nIn algebraic geometry, the dualizing sheaf on a proper scheme \"X\" of dimension \"n\" over a field \"k\" is a coherent sheaf formula_1 together with a linear functional\nthat induces a natural isomorphism of vector spaces\nfor each coherent sheaf \"F\" on \"X\" (the superscript * refers to a dual vector space). The linear functional formula_4 is called a trace morphism.\n\nA pair formula_5, if it is exists, is unique up to a natural isomorphism. In fact, in the language of category theory, formula_1 is an object representing the contravariant functor formula_7 from the category of coherent sheaves on \"X\" to the category of \"k\"-vector spaces.\n\nFor a normal projective variety \"X\", the dualizing sheaf exists and it is in fact the canonical sheaf: formula_8 where formula_9 is a canonical divisor. More generally, the dualuzing sheaf exists for any projective scheme.\n\nThere is the following variant of Serre's duality theorem: for a projective scheme \"X\" of pure dimension \"n\" and a Cohen–Macaulay sheaf \"F\" on \"X\" such that formula_10 is of pure dimension \"n\", there is a natural isomorphism\nIn particular, if \"X\" itself is a Cohen–Macaulay scheme, then the above duality holds for any locally free sheaf.\n\n\n\n"}
{"id": "33946193", "url": "https://en.wikipedia.org/wiki?curid=33946193", "title": "Edward Bierstone", "text": "Edward Bierstone\n\nEdward Bierstone is a Canadian mathematician at the University of Toronto who specializes in singularity theory, analytic geometry, and differential analysis.\n\nHe got his B.Sc. from the University of Toronto and his Ph.D. at Brandeis University in 1972. He was a visiting scholar at the Institute for Advanced Study in the summer of 1973. He served as the Director of the Fields Institute from 2009 to 2013.\n\nBierstone was elected a member of the Royal Society of Canada in 1992 and, together with Pierre Milman, received the Jefferey Williams Prize in 2005. In 2012 he became a fellow of the American Mathematical Society.\n\n"}
{"id": "50531340", "url": "https://en.wikipedia.org/wiki?curid=50531340", "title": "Epi-convergence", "text": "Epi-convergence\n\nIn mathematical analysis, epi-convergence is a type of convergence for real-valued and extended real-valued functions. \n\nEpi-convergence is important because it is the appropriate notion of convergence with which to approximate minimization problems in the field of mathematical optimization. The symmetric notion of hypo-convergence is appropriate for maximization problems. Mosco convergence is a generalization of epi-convergence to infinite dimensional spaces.\n\nLet formula_1 be a metric space, and formula_2 a real-valued function for each natural number formula_3. We say that the sequence formula_4 epi-converges to a function formula_5 if for each formula_6\n\nThe following extension allows epi-convergence to be applied to a sequence of functions with non-constant domain.\n\nDenote by formula_8 the extended real numbers. Let formula_9 be a function formula_10 for each formula_11. The sequence formula_12 epi-converges to formula_13 if for each formula_6\n\nEpi-convergence is the appropriate topology with which to approximate minimization problems. For maximization problems one uses the symmetric notion of hypo-convergence. formula_12 hypo-converges to formula_17 if\n\nand\n\nAssume we have a difficult minimization problem \n\nwhere formula_21 and formula_22. We can attempt to approximate this problem by a sequence of easier problems\n\nfor functions formula_24 and sets formula_25.\n\nEpi-convergence provides an answer to the question: In what sense should the approximations converge to the original problem in order to guarantee that approximate solutions converge to a solution of the original?\n\nWe can embed these optimization problems into the epi-convergence framework by defining extended real-valued functions\n\nSo that the problems formula_27 and formula_28 are equivalent to the original and approximate problems, respectively.\n\nIf formula_12 epi-converges to formula_17, then formula_31. Furthermore, if formula_32 is a limit point of minimizers of formula_9, then formula_32 is a minimizer of formula_17. In this sense,\n\nEpi-convergence is the weakest notion of convergence for which this result holds.\n\n\n"}
{"id": "1153192", "url": "https://en.wikipedia.org/wiki?curid=1153192", "title": "Expectiminimax", "text": "Expectiminimax\n\nThe expectiminimax algorithm is a variation of the minimax algorithm, for use in artificial intelligence systems that play two-player zero-sum games, such as backgammon, in which the outcome depends on a combination of the player's skill and chance elements such as dice rolls. In addition to \"min\" and \"max\" nodes of the traditional minimax tree, this variant has \"chance\" (\"move by nature\") nodes, which take the expected value of a random event occurring. In game theory terms, an expectiminimax tree is the game tree of an extensive-form game of perfect, but incomplete information.\n\nIn the traditional minimax method, the levels of the tree alternate from max to min until the depth limit of the tree has been reached. In an expectiminimax tree, the \"chance\" nodes are interleaved with the max and min nodes. Instead of taking the max or min of the utility values of their children, chance nodes take a weighted average, with the weight being the probability that child is reached.\n\nThe interleaving depends on the game. Each \"turn\" of the game is evaluated as a \"max\" node (representing the AI player's turn), a \"min\" node (representing a potentially-optimal opponent's turn), or a \"chance\" node (representing a random effect or player).\n\nFor example, consider a game in which each round consists of a single dice throw, and then decisions made by first the AI player, and then another intelligent opponent. The order of nodes in this game would alternate between \"chance\", \"max\" and then \"min\".\n\nThe expectiminimax algorithm is a variant of the minimax algorithm and was first proposed by Donald Michie in 1966.\nIts pseudocode is given below. \n\nNote that for random nodes, there must be a known probability of reaching each child. (For most games of chance, child nodes will be equally-weighted, which means the return value can simply be the average of all child values.)\n\n"}
{"id": "56780028", "url": "https://en.wikipedia.org/wiki?curid=56780028", "title": "Francis Bonahon", "text": "Francis Bonahon\n\nFrancis Bonahon (9 September 1955, Tarbes) is a French mathematician, specializing in low-dimensional topology.\n\nBonahon received in 1972 his \"baccalauréat\", and was accepted in 1974 into the École Normale Supérieure. He received in 1975 his \"maîtrise\" in mathematics from the University of Paris VII, and in 1979 his doctorate from the University of Paris XI under Laurence Siebenmann with thesis \"Involutions et fibrés de Seifert dans les variétés de dimension 3\". As a postdoc he was for the academic year 1979/80 a \"Procter Fellow\" at Princeton University. In 1980 he became an \"attaché de recherche\" and in 1983 a \"chargé de recherche\" of the CNRS. In 1985 he received his habilitation from the University of Paris XI under Siebenmann with thesis \"Geometric structures on 3-manifolds and applications\". Bonahon became in 1986 an assistant professor, in 1988 an associate professor, and in 1989 a full professor at the University of Southern California in Los Angeles.\n\nHe was a visiting professor in 1990 at the University of California, Davis, in 1996 at the Centre Émile Borel and at the IHES, in 1997 at Caltech, in 2000 at IHES, and in 2015 at the MSRI.\n\nBonahon's research deals with three-dimensional topology, knot theory, surface diffeomorphisms, hyperbolic geometry, and Kleinian groups.\n\nHe received in 1985 a bronze medal from CNRS and from 1989 to 1994 a Presidential Young Investigator Award. From 1987 to 1989 he was a Sloan Research Fellow. In 1990 he was an Invited Speaker with talk \"Ensembles limites et applications\" at the ICM in Kyoto. He was elected a Fellow of American Mathematical Society in 2012.\n\nHis doctoral students include Frédéric Paulin.\n\n\n"}
{"id": "14201348", "url": "https://en.wikipedia.org/wiki?curid=14201348", "title": "Geometry and topology", "text": "Geometry and topology\n\nIn mathematics, geometry and topology is an umbrella term for the historically distinct disciplines of geometry and topology, as general frameworks allow both disciplines to be manipulated uniformly, most visibly in local to global theorems in Riemannian geometry, and results like the Gauss–Bonnet theorem and Chern–Weil theory.\n\nSharp distinctions between geometry and topology can be drawn, however, as discussed below.\n\nIt is also the title of a journal \"Geometry & Topology\" that covers these topics.\n\nIt is distinct from \"geometric topology\", which more narrowly involves applications of topology to geometry.\n\nIt includes:\n\nIt does not include such parts of algebraic topology as homotopy theory, but some areas of geometry and topology (such as surgery theory, particularly algebraic surgery theory) are heavily algebraic.\n\nGeometry has \"local\" structure (or infinitesimal), while topology only has \"global\" structure. Alternatively, geometry has \"continuous\" moduli, while topology has \"discrete\" moduli.\n\nBy examples, an example of geometry is Riemannian geometry, while an example of topology is homotopy theory. The study of metric spaces is geometry, the study of topological spaces is topology.\n\nThe terms are not used completely consistently: symplectic manifolds are a boundary case, and coarse geometry is global, not local.\n\nBy definition, differentiable manifolds of a fixed dimension are all locally diffeomorphic to Euclidean space, so aside from dimension, there are no local invariants. Thus, differentiable structures on a manifold are topological in nature.\n\nBy contrast, the curvature of a Riemannian manifold is a local (indeed, infinitesimal) invariant (and is the only local invariant under isometry).\n\nIf a structure has a discrete moduli (if it has no deformations, or if a deformation of a structure is isomorphic to the original structure), the structure is said to be rigid, and its study (if it is a geometric or topological structure) is topology. If it has non-trivial deformations, the structure is said to be flexible, and its study is geometry.\n\nThe space of homotopy classes of maps is discrete, so studying maps up to homotopy is topology.\nSimilarly, differentiable structures on a manifold is usually a discrete space, and hence an example of topology, but exotic Rs have continuous moduli of differentiable structures.\n\nAlgebraic varieties have continuous moduli spaces, hence their study is algebraic geometry. These are finite-dimensional moduli spaces.\n\nThe space of Riemannian metrics on a given differentiable manifold is an infinite-dimensional space.\n\nSymplectic manifolds are a boundary case, and parts of their study are called symplectic topology and symplectic geometry.\n\nBy Darboux's theorem, a symplectic manifold has no local structure, which suggests that their study be called topology.\n\nBy contrast, the space of symplectic structures on a manifold form a continuous moduli, which suggests that their study be called geometry.\n\nHowever, up to isotopy, the space of symplectic structures is discrete (any family of symplectic structures are isotopic).\n"}
{"id": "3136832", "url": "https://en.wikipedia.org/wiki?curid=3136832", "title": "Grammar-based code", "text": "Grammar-based code\n\nGrammar-based codes or Grammar-based compression are compression algorithms based on the idea of constructing a context-free grammar (CFG) for the string to be compressed. Examples include universal lossless data compression algorithms. To compress a data sequence formula_1, a grammar-based code transforms formula_2 into a context-free grammar formula_3.\nThe problem of finding a smallest grammar for an input sequence is known to be NP-hard, so many grammar-transform algorithms are proposed from theoretical and practical viewpoints.\nGenerally, the produced grammar formula_3 is further compressed by statistical encoders like arithmetic coding.\n\nThe class of grammar-based codes is very broad. It includes block codes, variations of the incremental parsing Lempel-Ziv code, the multilevel pattern matching (MPM) algorithm, and many other new universal lossless compression algorithms.\nGrammar-based codes are universal in the sense that they can achieve asymptotically the entropy rate of any stationary, ergodic source with a finite alphabet.\n\nThe compression programs of the following are available from external links.\n\n\n\n"}
{"id": "10447275", "url": "https://en.wikipedia.org/wiki?curid=10447275", "title": "Graph center", "text": "Graph center\n\nThe center (or Jordan center) of a graph is the set of all vertices of minimum eccentricity, that is, the set of all vertices \"u\" where the greatest distance \"d\"(\"u\",\"v\") to other vertices \"v\" is minimal. Equivalently, it is the set of vertices with eccentricity equal to the graph's radius. Thus vertices in the center (central points) minimize the maximal distance from other points in the graph.\n\nFinding the center of a graph is useful in facility location problems where the goal is to minimize the worst-case distance to the facility. For example, placing a hospital at a central point reduces the longest distance the ambulance has to travel.\n\nThe concept of the center of a graph is related to the closeness centrality measure in social network analysis, which is the reciprocal of the mean of the distances \"d\"(\"A\",\"B\").\n"}
{"id": "374002", "url": "https://en.wikipedia.org/wiki?curid=374002", "title": "Limit cardinal", "text": "Limit cardinal\n\nIn mathematics, limit cardinals are certain cardinal numbers. A cardinal number λ is a weak limit cardinal if λ is neither a successor cardinal nor zero. This means that one cannot \"reach\" λ from another cardinal by repeated successor operations. These cardinals are sometimes called simply \"limit cardinals\" when the context is clear.\n\nA cardinal λ is a strong limit cardinal if λ cannot be reached by repeated powerset operations. This means that λ is nonzero and, for all κ < λ, 2 < λ. Every strong limit cardinal is also a weak limit cardinal, because κ ≤ 2 for every cardinal κ, where κ denotes the successor cardinal of κ.\n\nThe first infinite cardinal, formula_1 (aleph-naught), is a strong limit cardinal, and hence also a weak limit cardinal.\n\nOne way to construct limit cardinals is via the union operation: formula_2 is a weak limit cardinal, defined as the union of all the alephs before it; and in general formula_3 for any limit ordinal λ is a weak limit cardinal.\n\nThe ב operation can be used to obtain strong limit cardinals. This operation is a map from ordinals to cardinals defined as\nThe cardinal\nis a strong limit cardinal of cofinality ω. More generally, given any ordinal α, the cardinal\nis a strong limit cardinal. Thus there are arbitrarily large strong limit cardinals.\n\nIf the axiom of choice holds, every cardinal number has an initial ordinal. If that initial ordinal is formula_9 then the cardinal number is of the form formula_10 for the same ordinal subscript λ. The ordinal λ determines whether formula_10 is a weak limit cardinal. Because formula_12 if λ is a successor ordinal then formula_10 is not a weak limit. Conversely, if a cardinal κ is a successor cardinal, say formula_14 then formula_15 Thus, in general, formula_10 is a weak limit cardinal if and only if λ is zero or a limit ordinal.\n\nAlthough the ordinal subscript tells whether a cardinal is a weak limit, it does not tell whether a cardinal is a strong limit. For example, ZFC proves that formula_17 is a weak limit cardinal, but neither proves nor disproves that formula_17 is a strong limit cardinal (Hrbacek and Jech 1999:168). The generalized continuum hypothesis states that formula_19 for every infinite cardinal κ. Under this hypothesis, the notions of weak and strong limit cardinals coincide.\n\nThe preceding defines a notion of \"inaccessibility\": we are dealing with cases where it is no longer enough to do finitely many iterations of the successor and powerset operations; hence the phrase \"cannot be reached\" in both of the intuitive definitions above. But the \"union operation\" always provides another way of \"accessing\" these cardinals (and indeed, such is the case of limit ordinals as well). Stronger notions of inaccessibility can be defined using cofinality. For a weak (resp. strong) limit cardinal κ the requirement that cf(κ) = κ (i.e. κ be regular) so that κ cannot be expressed as a sum (union) of fewer than κ smaller cardinals. Such a cardinal is called a weakly (resp. strongly) inaccessible cardinal. The preceding examples both are singular cardinals of cofinality ω and hence they are not inaccessible.\n\nformula_1 would be an inaccessible cardinal of both \"strengths\" except that the definition of inaccessible requires that they be uncountable. Standard Zermelo-Fraenkel set theory with the Axiom of Choice (ZFC) cannot even prove the consistency of the existence of an inaccessible cardinal of either kind above formula_1, due to Gödel's Incompleteness Theorem. More specifically, if formula_22 is weakly inaccessible then formula_23. These form the first in a hierarchy of large cardinals.\n\n\n\n"}
{"id": "452950", "url": "https://en.wikipedia.org/wiki?curid=452950", "title": "Limit cycle", "text": "Limit cycle\n\nIn mathematics, in the study of dynamical systems with two-dimensional phase space, a limit cycle is a closed trajectory in phase space having the property that at least one other trajectory spirals into it either as time approaches infinity or as time approaches negative infinity. Such behavior is exhibited in some nonlinear systems. Limit cycles have been used to model the behavior of a great many real world oscillatory systems. The study of limit cycles was initiated by Henri Poincaré (1854–1912).\n\nWe consider a two-dimensional dynamical system of the form\nwhere\nis a smooth function. A \"trajectory\" of this system is some smooth function formula_3 with values in formula_4 which satisfies this differential equation. Such a trajectory is called \"closed\" (or \"periodic\") if it is not constant but returns to its starting point, i.e. if there exists some formula_5 such that formula_6 for all formula_7. An orbit is the image of a trajectory, a subset of formula_4. A \"closed orbit\", or \"cycle\", is the image of a closed trajectory. A \"limit cycle\" is a cycle which is the limit set of some other trajectory.\n\nBy the Jordan curve theorem, every closed trajectory divides the plane into two regions, the interior and the exterior of the curve.\n\nGiven a limit cycle and a trajectory in its interior that approaches the limit cycle for time approaching formula_9, then there is a neighborhood around the limit cycle such that \"all\" trajectories in the interior that start in the neighborhood approach the limit cycle for time approaching formula_10. The corresponding statement holds for a trajectory in the interior that approaches the limit cycle for time approaching formula_11, and also for trajectories in the exterior approaching the limit cycle.\n\nIn the case where all the neighbouring trajectories approach the limit cycle as time approaches infinity, it is called a \"stable\" or \"attractive\" limit cycle (ω-limit cycle). If instead all neighbouring trajectories approach it as time approaches negative infinity, then it is an \"unstable\" limit cycle (α-limit cycle). If there is a neighbouring trajectory which spirals into the limit cycle as time approaches infinity, and another one which spirals into it as time approaches negative infinity, then it is a \"semi-stable\" limit cycle. There are also limit cycles which are neither stable, unstable nor semi-stable: for instance, a neighboring trajectory may approach the limit cycle from the outside, but the inside of the limit cycle is approached by a family of other cycles (which wouldn't be limit cycles).\n\nStable limit cycles are examples of attractors. They imply self-sustained oscillations: the closed trajectory describes perfect periodic behavior of the system, and any small perturbation from this closed trajectory causes the system to return to it, making the system stick to the limit cycle.\n\nEvery closed trajectory contains within its interior a stationary point of the system, i.e. a point formula_12 where formula_13. The Bendixson–Dulac theorem and the Poincaré–Bendixson theorem predict the absence or existence, respectively, of limit cycles of two-dimensional nonlinear dynamical systems.\n\nFinding limit cycles in general is a very difficult problem. The number of limit cycles of a polynomial differential equation in the plane is the main object of the second part of Hilbert's sixteenth problem. It is unknown, for instance, whether there is any system formula_14 in the plane where both components of formula_15 are quadratic polynomials of the two variables, such that the system has more than 4 limit cycles.\n\n\n"}
{"id": "44300155", "url": "https://en.wikipedia.org/wiki?curid=44300155", "title": "Localization formula for equivariant cohomology", "text": "Localization formula for equivariant cohomology\n\nIn differential geometry, the localization formula states: for an equivariantly closed equivariant differential form formula_1 on an orbifold \"M\" with a torus action and for a sufficient small formula_2 in the Lie algebra of the torus \"T\",\nwhere the sum runs over all connected components \"F\" of the set of fixed points formula_4, formula_5 is the orbifold multiplicity of \"M\" (which is one if \"M\" is a manifold) and formula_6 is the equivariant Euler form of the normal bundle of \"F\".\n\nThe formula allows one to compute the equivariant cohomology ring of the orbifold \"M\" (a particular kind of differentiable stack) from the equivariant cohomology of its fixed point components, up to multiplicities and Euler forms. No analog of such results holds in the non-equivariant cohomology.\n\nOne important consequence of the formula is the Duistermaat–Heckman theorem, which states: supposing there is a Hamiltonian circle action (for simplicity) on a compact symplectic manifold \"M\" of dimension 2\"n\",\nwhere \"H\" is Hamiltonian for the circle action, the sum is over points fixed by the circle action and formula_8 are eigenvalues on the tangent space at \"p\" (cf. Lie group action.)\n\nThe localization formula can also computes the Fourier transform of (Kostant's symplectic form on) coadjoint orbit, yielding the Harish-Chandra's integration formula, which in turns gives Kirillov's character formula.\n\nThe localization theorem for equivariant cohomology in non-rational coefficients is discussed in Daniel Quillen's papers.\n\nThe localization theorem states that the equivariant cohomology can be recovered, up to torsion elements, from the equivariant cohomology of the fixed point subset. This does not extend, in verbatim, to the non-abelian action. But there is still a version of the localization theorem for non-abelian actions.\n\n"}
{"id": "2172840", "url": "https://en.wikipedia.org/wiki?curid=2172840", "title": "Luby transform code", "text": "Luby transform code\n\nIn computer science, Luby transform codes (LT codes) are the first class of practical fountain codes that are near-optimal erasure correcting codes. They were invented by Michael Luby in 1998 and published in 2002. Like some other fountain codes, LT codes depend on sparse bipartite graphs to trade reception overhead for encoding and decoding speed. The distinguishing characteristic of LT codes is in employing a particularly simple algorithm based on the exclusive or operation (formula_1) to encode and decode the message.\n\nLT codes are \"rateless\" because the encoding algorithm can in principle produce an infinite number of message packets (i.e., the percentage of packets that must be received to decode the message can be arbitrarily small). They are \"erasure correcting codes\" because they can be used to transmit digital data reliably on an erasure channel.\n\nThe next generation beyond LT codes are raptor codes (see for example IETF RFC 5053 or IETF RFC 6330), which have linear time encoding and decoding. Raptor codes use two encoding stages for encoding, where the second stage is an LT encoding.\n\nThe traditional scheme for transferring data across an erasure channel depends on continuous two-way communication.\n\nCertain networks, such as ones used for cellular wireless broadcasting, do not have a feedback channel. Applications on these networks still require reliability. Fountain codes in general, and LT codes in particular, get around this problem by adopting an essentially one-way communication protocol.\n\nThe encoding process begins by dividing the uncoded message into \"n\" blocks of roughly equal length. Encoded packets are then produced with the help of a pseudorandom number generator.\n\n\nThis process continues until the receiver signals that the message has been received and successfully decoded.\n\nThe decoding process uses the \"exclusive or\" operation to retrieve the encoded message.\n\nThis decoding procedure works because \"A\" formula_1 \"A\" = 0 for any bit string \"A\". After \"d\" − 1 distinct blocks have been exclusive-ored into a packet of degree \"d\", the original unencoded content of the unmatched block is all that remains. In symbols we have\n\nSeveral variations of the encoding and decoding processes described above are possible. For instance, instead of prefixing each packet with a list of the actual message block indices {\"i\", \"i\", …, \"i\"}, the encoder might simply send a short \"key\" which served as the seed for the pseudorandom number generator (PRNG) or index table used to construct the list of indices. Since the receiver equipped with the same RNG or index table can reliably recreate the \"random\" list of indices from this seed, the decoding process can be completed successfully. Alternatively, by combining a simple LT code of low average degree with a robust error-correcting code, a raptor code can be constructed that will outperform an optimized LT code in practice.\n\nThere is only one parameter that can be used to optimize a straight LT code: the degree distribution function (described as a pseudorandom number generator for the degree \"d\" in the LT encoding section above). In practice the other \"random\" numbers (the list of indices { \"i\", \"i\", …, \"i\" } ) are invariably taken from a uniform distribution on [0, \"n\"), where \"n\" is the number of blocks into which the message has been divided.\n\nLuby himself discussed the \"ideal soliton distribution\" defined by\n\nThis degree distribution theoretically minimizes the expected number of redundant code words that will be sent before the decoding process can be completed. However the ideal soliton distribution does not work well in practice because any fluctuation around the expected behavior makes it likely that at some step in the decoding process there will be no available packet of (reduced) degree 1 so decoding will fail. Furthermore, some of the original blocks will not be xor-ed into any of the transmission packets. Therefore, in practice, a modified distribution, the \"robust soliton distribution\", is substituted for the ideal distribution. The effect of the modification is, generally, to produce more packets of very small degree (around 1) and fewer packets of degree greater than 1, except for a spike of packets at a fairly large quantity chosen to ensure that all original blocks will be included in some packet.\n\n\n"}
{"id": "37589990", "url": "https://en.wikipedia.org/wiki?curid=37589990", "title": "Matsushima's formula", "text": "Matsushima's formula\n\nIn mathematics, Matsushima’s formula, introduced by , is a formula for the Betti numbers of a quotient of a symmetric space \"G\"/\"H\" by a discrete group, in terms of unitary representations of the group \"G\".\n"}
{"id": "321801", "url": "https://en.wikipedia.org/wiki?curid=321801", "title": "Multiply perfect number", "text": "Multiply perfect number\n\nIn mathematics, a multiply perfect number (also called multiperfect number or pluperfect number) is a generalization of a perfect number.\n\nFor a given natural number \"k\", a number \"n\" is called \"k\"-perfect (or \"k\"-fold perfect) if and only if the sum of all positive divisors of \"n\" (the divisor function, \"σ\"(\"n\")) is equal to \"kn\"; a number is thus perfect if and only if it is 2-perfect. A number that is \"k\"-perfect for a certain \"k\" is called a multiply perfect number. As of 2014, \"k\"-perfect numbers are known for each value of \"k\" up to 11.\n\nIt can be proven that:\n\n\nAn open question is whether every \"k\"-perfect numbers are divisible by \"k\"!, where \"!\" is the factorial.\n\nThe following table gives an overview of the smallest \"k\"-perfect numbers for \"k\" ≤ 11 :\n\nFor example, 120 is 3-perfect because the sum of the divisors of 120 is\n1+2+3+4+5+6+8+10+12+15+20+24+30+40+60+120 = 360 = 3 × 120.\n\n\nA number \"n\" with σ(\"n\") = 2\"n\" is perfect.\n\nA number \"n\" with σ(\"n\") = 3\"n\" is triperfect. An odd triperfect number must exceed 10, have at least 12 distinct prime factors, the largest exceeding 10.\n\n\n"}
{"id": "21474", "url": "https://en.wikipedia.org/wiki?curid=21474", "title": "Natural number", "text": "Natural number\n\nIn mathematics, the natural numbers are those used for counting (as in \"there are \"six\" coins on the table\") and ordering (as in \"this is the \"third\" largest city in the country\"). In common mathematical terminology, words colloquially used for counting are \"cardinal numbers\" and words connected to ordering represent \"ordinal numbers\".\n\nSome definitions, including the standard ISO 80000-2, begin the natural numbers with , corresponding to the non-negative integers , whereas others start with 1, corresponding to the positive integers . Texts that exclude zero from the natural numbers sometimes refer to the natural numbers together with zero as the whole numbers, but in other writings, that term is used instead for the integers (including negative integers).\n\nThe natural numbers are a basis from which many other number sets may be built by extension: the integers (Grothendieck group), by including (if not yet in) the neutral element 0 and an additive inverse (−\"n\") for each nonzero natural number \"n\"; the rational numbers, by including a multiplicative inverse (1/\"n\") for each nonzero integer \"n\" (and also the product of these inverses by integers); the real numbers by including with the rationals the limits of (converging) Cauchy sequences of rationals; the complex numbers, by including with the real numbers the unresolved square root of minus one (and also the sums and products thereof); and so on. These chains of extensions make the natural numbers canonically embedded (identified) in the other number systems.\n\nProperties of the natural numbers, such as divisibility and the distribution of prime numbers, are studied in number theory. Problems concerning counting and ordering, such as partitioning and enumerations, are studied in combinatorics.\n\nIn common language, for example in primary school, natural numbers may be called counting numbers both to intuitively exclude the negative integers and zero, and also to contrast the discreteness of counting to the continuity of measurement, established by the real numbers.\n\nThe natural numbers can, at times, appear as a convenient set of names (labels), that is, as what linguists call nominal numbers, foregoing many or all of the properties of being a number in a mathematical sense.\n\nThe most primitive method of representing a natural number is to put down a mark for each object. Later, a set of objects could be tested for equality, excess or shortage, by striking out a mark and removing an object from the set.\n\nThe first major advance in abstraction was the use of numerals to represent numbers. This allowed systems to be developed for recording large numbers. The ancient Egyptians developed a powerful system of numerals with distinct hieroglyphs for 1, 10, and all the powers of 10 up to over 1 million. A stone carving from Karnak, dating from around 1500 BC and now at the Louvre in Paris, depicts 276 as 2 hundreds, 7 tens, and 6 ones; and similarly for the number 4,622. The Babylonians had a place-value system based essentially on the numerals for 1 and 10, using base sixty, so that the symbol for sixty was the same as the symbol for one, its value being determined from context.\n\nA much later advance was the development of the idea that  can be considered as a number, with its own numeral. The use of a 0 digit in place-value notation (within other numbers) dates back as early as 700 BC by the Babylonians, but they omitted such a digit when it would have been the last symbol in the number. The Olmec and Maya civilizations used 0 as a separate number as early as the , but this usage did not spread beyond Mesoamerica. The use of a numeral 0 in modern times originated with the Indian mathematician Brahmagupta in 628. However, 0 had been used as a number in the medieval computus (the calculation of the date of Easter), beginning with Dionysius Exiguus in 525, without being denoted by a numeral (standard Roman numerals do not have a symbol for 0); instead \"nulla\" (or the genitive form \"nullae\") from \"nullus\", the Latin word for \"none\", was employed to denote a 0 value.\n\nThe first systematic study of numbers as abstractions is usually credited to the Greek philosophers Pythagoras and Archimedes. Some Greek mathematicians treated the number 1 differently than larger numbers, sometimes even not as a number at all.\n\nIndependent studies also occurred at around the same time in India, China, and Mesoamerica.\n\nIn 19th century Europe, there was mathematical and philosophical discussion about the exact nature of the natural numbers. A school of Naturalism stated that the natural numbers were a direct consequence of the human psyche. Henri Poincaré was one of its advocates, as was Leopold Kronecker who summarized \"God made the integers, all else is the work of man\".\n\nIn opposition to the Naturalists, the constructivists saw a need to improve the logical rigor in the foundations of mathematics. In the 1860s, Hermann Grassmann suggested a recursive definition for natural numbers thus stating they were not really natural but a consequence of definitions. Later, two classes of such formal definitions were constructed; later, they were shown to be equivalent in most practical applications.\n\nSet-theoretical definitions of natural numbers were initiated by Frege and he initially defined a natural number as the class of all sets that are in one-to-one correspondence with a particular set, but this definition turned out to lead to paradoxes including Russell's paradox. Therefore, this formalism was modified so that a natural number is defined as a particular set, and any set that can be put into one-to-one correspondence with that set is said to have that number of elements.\n\nThe second class of definitions was introduced by Charles Sanders Peirce, refined by Richard Dedekind, and further explored by Giuseppe Peano; this approach is now called Peano arithmetic. It is based on an axiomatization of the properties of ordinal numbers: each natural number has a successor and every non-zero natural number has a unique predecessor. Peano arithmetic is equiconsistent with several weak systems of set theory. One such system is ZFC with the axiom of infinity replaced by its negation. Theorems that can be proved in ZFC but cannot be proved using the Peano Axioms include Goodstein's theorem.\n\nWith all these definitions it is convenient to include 0 (corresponding to the empty set) as a natural number. Including 0 is now the common convention among set theorists and logicians. Other mathematicians also include 0, e.g., Computer languages often start from zero when enumerating items like loop counters and string- or array-elements. Many mathematicians have kept the older tradition and take 1 to be the first natural number.\n\nSince different properties are customarily associated to the tokens and , e.g., neutral elements for addition and multiplications, respectively, it is important to know which version of \"natural numbers\", generically denoted by formula_1 is employed in the case under consideration. This can be done by explanation in prose, by explicitly writing down the set, or by qualifying the generic identifier with a super- or subscript (see also in #Notation), e.g., like this:\n\n\nMathematicians use N or (an N in blackboard bold) to refer to the set of all natural numbers. Older texts have also occasionally employed \"J\" as the symbol for this set.\n\nTo be unambiguous about whether 0 is included or not, sometimes a subscript (or superscript) \"0\" is added in the former case, and a superscript \" or subscript \" is added in the latter case:\n\nAlternatively, since natural numbers naturally embed in the integers, they may be referred to as the positive, or the non-negative integers, respectively.\n\nThe set of natural numbers is an infinite set. This kind of infinity is, by definition, called countable infinity. All sets that can be put into a bijective relation to the natural numbers are said to have this kind of infinity. This is also expressed by saying that the cardinal number of the set is aleph-naught ().\n\nOne can recursively define an addition operator on the natural numbers by setting and for all , . Here should be read as \"successor\". This turns the natural numbers into a commutative monoid with identity element 0, the so-called free object with one generator. This monoid satisfies the cancellation property and can be embedded in a group (in the mathematical sense of the word \"group\"). The smallest group containing the natural numbers is the integers.\n\nIf 1 is defined as , then . That is, is simply the successor of .\n\nAnalogously, given that addition has been defined, a multiplication operator × can be defined via and . This turns into a free commutative monoid with identity element 1; a generator set for this monoid is the set of prime numbers.\n\nAddition and multiplication are compatible, which is expressed in the distribution law: . These properties of addition and multiplication make the natural numbers an instance of a commutative semiring. Semirings are an algebraic generalization of the natural numbers where multiplication is not necessarily commutative. The lack of additive inverses, which is equivalent to the fact that is not closed under subtraction (i.e., subtracting one natural from another does not always result in another natural), means that is \"not\" a ring; instead it is a semiring (also known as a \"rig\").\n\nIf the natural numbers are taken as \"excluding 0\", and \"starting at 1\", the definitions of + and × are as above, except that they begin with and .\n\nIn this section, juxtaposed variables such as indicate the product , and the standard order of operations is assumed.\n\nA total order on the natural numbers is defined by letting if and only if there exists another natural number where . This order is compatible with the arithmetical operations in the following sense: if , and are natural numbers and , then and .\n\nAn important property of the natural numbers is that they are well-ordered: every non-empty set of natural numbers has a least element. The rank among well-ordered sets is expressed by an ordinal number; for the natural numbers, this is denoted as (omega).\n\nIn this section, juxtaposed variables such as indicate the product , and the standard order of operations is assumed.\n\nWhile it is in general not possible to divide one natural number by another and get a natural number as result, the procedure of \"division with remainder\" is available as a substitute: for any two natural numbers and with there are natural numbers and such that\n\nThe number is called the \"quotient\" and is called the \"remainder\" of the division of by . The numbers and are uniquely determined by and . This Euclidean division is key to several other properties (divisibility), algorithms (such as the Euclidean algorithm), and ideas in number theory.\n\nThe addition (+) and multiplication (×) operations on natural numbers as defined above have several algebraic properties:\n\nTwo important generalizations of natural numbers arise from the two uses of counting and ordering: cardinal numbers and ordinal numbers.\n\nMany well-ordered sets with cardinal number have an ordinal number greater than (the latter is the lowest possible). The least ordinal of cardinality (i.e., the initial ordinal) is .\n\nFor finite well-ordered sets, there is a one-to-one correspondence between ordinal and cardinal numbers; therefore they can both be expressed by the same natural number, the number of elements of the set. This number can also be used to describe the position of an element in a larger finite, or an infinite, sequence.\n\nA countable non-standard model of arithmetic satisfying the Peano Arithmetic (i.e., the first-order Peano axioms) was developed by Skolem in 1933. The hypernatural numbers are an uncountable model that can be constructed from the ordinary natural numbers via the ultrapower construction.\n\nGeorges Reeb used to claim provocatively that \"The naïve integers don't fill up\" . Other generalizations are discussed in the article on numbers.\n\nMany properties of the natural numbers can be derived from the five Peano axioms:\n\n\nThese are not the original axioms published by Peano, but are named in his honor. Some forms of the Peano axioms have 1 in place of 0. In ordinary arithmetic, the successor of formula_8 is formula_11. Replacing Axiom Five by an axiom schema one obtains a (weaker) first-order theory called \"Peano Arithmetic.\"\n\nIn the area of mathematics called set theory, a specific construction due to John von Neumann defines the natural numbers as follows:\n\nWith this definition, a natural number is a particular set with elements, and if and only if is a subset of . The standard definition, now called definition of von Neumann ordinals, is: \"each ordinal is the well-ordered set of all smaller ordinals.\"\n\nAlso, with this definition, different possible interpretations of notations like (-tuples versus mappings of into ) coincide.\n\nEven if one does not accept the axiom of infinity and therefore cannot accept that the set of all natural numbers exists, it is still possible to define any one of these sets.\n\nAlthough the standard construction is useful, it is not the only possible construction. Ernst Zermelo's construction goes as follows:\n\n\n"}
{"id": "43672852", "url": "https://en.wikipedia.org/wiki?curid=43672852", "title": "Normal form (dynamical systems)", "text": "Normal form (dynamical systems)\n\nIn mathematics, the normal form of a dynamical system is a simplified form that can be useful in determining the system's behavior.\n\nNormal forms are often used for determining local bifurcations in a system. All systems exhibiting a certain type of bifurcation are locally (around the equilibrium) topologically equivalent to the normal form of the bifurcation. For example, the normal form of a saddle-node bifurcation is formula_1 where formula_2 is the bifurcation parameter. The transcritical bifurcation formula_3 near formula_4 can be converted to the normal form formula_5 with the transformation formula_6.\n\n"}
{"id": "614998", "url": "https://en.wikipedia.org/wiki?curid=614998", "title": "Pál Turán", "text": "Pál Turán\n\nPál Turán (; 18 August 1910 – 26 September 1976) also known as Paul Turán, was a Hungarian mathematician who worked primarily in number theory. He had a long collaboration with fellow Hungarian mathematician Paul Erdős, lasting 46 years and resulting in 28 joint papers.\n\nTurán was born into a Jewish family in Budapest on 18 August 1910.At the same period of time, Turán and Erdős were famous answerers in the journal \"KöMaL\". He received a teaching degree at the University of Budapest in 1933 and the Ph.D. degree under Lipót Fejér in 1935 at Eötvös Loránd University.\n\nAs a Jew, he fell victim to numerus clausus, and could not get a university job for several years. He was sent to labour service at various times from 1940-44. He is said to have been recognized and perhaps protected by a fascist guard, who, as a mathematics student, had admired Turán's work.\n\nTurán became associate professor at the University of Budapest in 1945 and full professor in 1949. Turán married Edit (Klein) Kóbor in 1939; they had one son, Róbert. Turán married, secondly, to Vera Sós, a mathematician, in 1952; they had two children, György and Tamás.\n\nTurán died in Budapest on 26 September 1976 of leukemia, aged 66.\n\nTurán worked primarily in number theory, but also did much work in analysis and graph theory.\n\nIn 1934, Turán used the Turán sieve to give a new and very simple proof of a 1917 result of G. H. Hardy and Ramanujan on the normal order of the number of distinct prime divisors of a number \"n\", namely that it is very close to formula_1. In probabilistic terms he estimated the variance from formula_1. Halász says \"Its true significance lies in the fact that it was the starting point of probabilistic number theory\". The Turán–Kubilius inequality is a generalization of this work. \n\nTurán was very interested in the distribution of primes in arithmetic progressions, and he coined the term \"prime number race\" for irregularities in the distribution of prime numbers among residue classes. With his coauthor Knapowski he proved results concerning Chebyshev's bias. The Erdős–Turán conjecture makes a statement about primes in arithmetic progression. Much of Turán's number theory work dealt with the Riemann hypothesis and he developed the power sum method (see below) to help with this. Erdős said \"Turán was an 'unbeliever,' in fact, a 'pagan': he did not believe in the truth of Riemann's hypothesis.\"\n\nMuch of Turán's work in analysis was tied to his number theory work. Outside of this he proved Turán's inequalities relating the values of the Legendre polynomials for different indices, and, together with Paul Erdős, the Erdős–Turán equidistribution inequality.\n\nErdős wrote of Turán, \"In 1940–1941 he created the area of extremal problems in graph theory which is now one of the fastest-growing subjects in combinatorics.\"Peter Frankl said of Turán, \"He fell victim to Numerus clausus. Mathematicians have only paper and pen, he doesn't have anything in camp. So he created combinatorics which is not needed both thing.\"\n\nThe field is known more briefly today as extremal graph theory. Turán's best-known result in this area is Turán's Graph Theorem, that gives an upper bound on the number of edges in a graph that does not contain the complete graph \"K\" as a subgraph. He invented the Turán graph, a generalization of the complete bipartite graph, to prove his theorem. He is also known for the Kővári–Sós–Turán theorem bounding the number of edges that can exist in a bipartite graph with certain forbidden subgraphs, and for raising Turán's brick factory problem, namely of determining the crossing number of a complete bipartite graph.\n\nTurán developed the power sum method to work on the Riemann hypothesis. The method deals with inequalities giving lower bounds for sums of the form\n\nAside from its applications in analytic number theory, it has been used in complex analysis, numerical analysis, differential equations, transcendental number theory, and estimating the number of zeroes of a function in a disk.\n\n\n\n"}
{"id": "5811855", "url": "https://en.wikipedia.org/wiki?curid=5811855", "title": "Riesz mean", "text": "Riesz mean\n\nIn mathematics, the Riesz mean is a certain mean of the terms in a series. They were introduced by Marcel Riesz in 1911 as an improvement over the Cesàro mean. The Riesz mean should not be confused with the Bochner–Riesz mean or the Strong–Riesz mean.\n\nGiven a series formula_1, the Riesz mean of the series is defined by\n\nSometimes, a generalized Riesz mean is defined as\n\nHere, the formula_4 are sequence with formula_5 and with formula_6 as formula_7. Other than this, the formula_4 are otherwise taken as arbitrary.\n\nRiesz means are often used to explore the summability of sequences; typical summability theorems discuss the case of formula_9 for some sequence formula_10. Typically, a sequence is summable when the limit formula_11 exists, or the limit formula_12 exists, although the precise summability theorems in question often impose additional conditions.\n\nLet formula_13 for all formula_14. Then\n\nHere, one must take formula_16; formula_17 is the Gamma function and formula_18 is the Riemann zeta function. The power series\n\ncan be shown to be convergent for formula_20. Note that the integral is of the form of an inverse Mellin transform.\n\nAnother interesting case connected with number theory arises by taking formula_21 where formula_22 is the Von Mangoldt function. Then\n\nAgain, one must take \"c\" > 1. The sum over \"ρ\" is the sum over the zeroes of the Riemann zeta function, and\n\nis convergent for \"λ\" > 1.\n\nThe integrals that occur here are similar to the Nörlund–Rice integral; very roughly, they can be connected to that integral via Perron's formula.\n\n"}
{"id": "360380", "url": "https://en.wikipedia.org/wiki?curid=360380", "title": "Sarah Flannery", "text": "Sarah Flannery\n\nSarah Flannery (born 1982, County Cork, Ireland) was, at sixteen years old, the winner of the 1999 Esat Young Scientist Exhibition for development of the Cayley–Purser algorithm, based on work she had done with researchers at Baltimore Technologies during a brief internship there. The project, entitled \"Cryptography – A new algorithm versus the RSA\", also won her the EU Young Scientist of the Year Award for 1999. Her book \"In Code\" (2001), co-written with her father, mathematician David Flannery, retells the story of the making and breaking of the algorithm, as well as the enjoyment she got from solving mathematical puzzles while growing up. She dedicates many of her accomplishments in the fields of mathematics and cryptography to her father's efforts during her childhood. \n\nShe studied computer science at Peterhouse, a college of the University of Cambridge, graduating in 2003, and, as of 2006, worked for Electronic Arts as a software engineer. She worked at TirNua as a \"Chief Scientist\".She developed the virtual economy in the game and the back-end web services that powered the game features. She has also worked at RockYou, and several other institutions involved in software development and computer science. With time, her interests and achievements in cryptography as a teenager had manifested into a dedication to science in technology. \n\nBefore working at TirNua, Flannery was software engineer working directly with then Electronic Arts Worldwide Chief Technology Officer, Scott Cronce, and, later, with many fellow Tirnua founders on her first virtual world.\n\nAt EA, she successfully set up the EA Open Source program using the Essential Project. Flannery created data visualizations on software architecture and game content creation which were used to directly impact the quality of both. She also successfully ran and turned around the virtual economy within EA-Land ().\n\nPreviously, she worked on the technical and scientific computing software product Mathematica for Wolfram Research.\n\nThe lights on St. Patrick's Street, one of the main thoroughfares of Flannery's home city of Cork, are named after her.\n\nSarah Flannery is the daughter of David and Elaine Flannery. She has four younger brothers: Mick, Brian, David and Eamonn. Her oldest brother is the singer-songwriter Mick Flannery. Her education included a primary all-girls school and a secondary co-ed school. \n\nIn 2014, Flannery and her French-American husband, Luc Barthelet, welcomed a child. She also has four stepchildren.\n\n\n\n"}
{"id": "1504486", "url": "https://en.wikipedia.org/wiki?curid=1504486", "title": "Scientific formalism", "text": "Scientific formalism\n\nScientific formalism is a family of approaches to the presentation of science. It is viewed as an important part of the scientific method, especially in the physical sciences.\n\nThere are multiple levels of scientific formalism possible. At the lowest level, scientific formalism deals with the symbolic manner in which the information is presented. To achieve formalism in a scientific theory at this level, one starts with a well defined set of axioms, and from this follows a formal system.\n\nHowever, at a higher level, scientific formalism also involves consideration of the axioms themselves. These can be viewed as questions of ontology. For example, one can, at the lower level of formalism, define a property called 'existence'. However, at the higher level, the question of whether an electron exists in the same sense that a bacterium exists still needs to be resolved.\n\nSome actual formal theories on facts have been proposed.\n\nThe scientific climate of the twentieth century revived these questions. From about the time of Isaac Newton to that of James Clerk Maxwell they had been dormant, in the sense that the physical sciences could rely on the status of the real numbers as a description of the continuum, and an agnostic view of atoms and their structure. Quantum mechanics, the dominant physical theory after about 1925, was formulated in a way which raised questions of both types.\n\nIn the Newtonian framework there was indeed a degree of comfort in the answers one could give. Consider for example the question of whether the Earth really goes round the Sun. In a frame of reference adapted to calculating the Earth's orbit, this is a mathematical but also tautological statement. Newtonian mechanics can answer the question, whether it is not equally the case that the Sun goes round the Earth, as it indeed appears to Earth-based astronomers. In Newton's theory there is a basic, fixed frame of reference that is inertial. The 'correct answer' is that the point of view of an observer in an inertial frame of reference is privileged: other observers see artifacts of their acceleration relative to an inertial frame (the inertial forces). Before Newton, Galileo would draw the consequences, from the Copernican heliocentric model. He was, however, constrained to call his work (in effect) scientific formalism, under the old 'description' saving the phenomena. To avoid going against authority, the elliptic orbits of the heliocentric model could be labelled as a more convenient device for calculations, rather than an actual description of reality.\n\nIn general relativity, Newton's inertial frames are no longer privileged. In quantum mechanics, Paul Dirac argued that physical models were not there to provide semantic constructs allowing us to \"understand\" microscopic physics in language comparable to that we use on the familiar scale of everyday objects. His attitude, adopted by many theoretical physicists, is that a good model is judged by our capacity to use it to calculate physical quantities that can be tested experimentally. Dirac's view is close to what Bas van Fraassen calls constructive empiricism.\n\nA physicist who took the issues involved seriously was Pierre Duhem, writing at the beginning of the twentieth century. He wrote an extended analysis of the approach he saw as characteristically British, in requiring field theories of theoretical physics to have a mechanical-physical interpretation. That was an accurate characterisation of what Dirac (himself British) would later argue against. The national characteristics specified by Duhem do not need to be taken too seriously, since he also claimed that the use of abstract algebra, namely quaternions, was also characteristically British (as opposed to French or German); as if the use of classical analysis methods alone was important one way or the other.\n\nDuhem also wrote on saving the phenomena. In addition to the Copernican revolution debate of \"saving the phenomena\" (Greek: σῴζειν τὰ φαινόμενα, \"sozein ta phainomena\") versus offering explanations that inspired Duhem was Thomas Aquinas, who wrote, regarding eccentrics and epicycles, that\n\nReason may be employed in two ways to establish a point: firstly, for the purpose of furnishing sufficient proof of some principle [...]. Reason is employed in another way, not as furnishing a sufficient proof of a principle, but as confirming an already established principle, by showing the congruity of its results, as in astronomy the theory of eccentrics and epicycles is considered as established, because thereby the sensible appearances of the heavenly movements can be explained (\"possunt salvari apparentia sensibilia\"); not, however, as if this proof were sufficient, forasmuch as some other theory might explain them. [...]\n\nThe idea that a physical interpretation—in common language or classical ideas and physical entities, though of or examined in an ontological or quasi-ontological sense—of a phenomenon in physics is not an ultimate or necessary condition for its understanding or validity, also appears in modern structural realist views on science.\n\nRobert Bellarmine wrote to heliocentrist Paolo Antonio Foscarini:Nor is it the same to demonstrate that by assuming the sun to be at the center and the earth in heaven one can save the appearances, and to demonstrate that in truth the sun is at the center and the earth in heaven; for I believe the first demonstration may be available, but I have very great doubts about the second…\n\nModern physicist Pierre Duhem \"suggests that in one respect, at least, Bellarmine had shown himself a better scientist than Galileo by disallowing the possibility of a 'strict proof of the earth's motion,' on the grounds that an astronomical theory merely 'saves the appearances' without necessarily revealing what 'really happens.'\"\n\n"}
{"id": "71085", "url": "https://en.wikipedia.org/wiki?curid=71085", "title": "Shannon–Hartley theorem", "text": "Shannon–Hartley theorem\n\nIn information theory, the Shannon–Hartley theorem tells the maximum rate at which information can be transmitted over a communications channel of a specified bandwidth in the presence of noise. It is an application of the noisy-channel coding theorem to the archetypal case of a continuous-time analog communications channel subject to Gaussian noise. The theorem establishes Shannon's channel capacity for such a communication link, a bound on the maximum amount of error-free information per time unit that can be transmitted with a specified bandwidth in the presence of the noise interference, assuming that the signal power is bounded, and that the Gaussian noise process is characterized by a known power or power spectral density. The law is named after Claude Shannon and Ralph Hartley.\n\nThe Shannon–Hartley theorem states the channel capacity \"C\", meaning the theoretical tightest upper bound on the information rate of data that can be communicated at an arbitrarily low error rate using an average received signal power \"S\" through an analog communication channel subject to additive white Gaussian noise of power \"N\":\n\nwhere\n\nDuring the late 1920s, Harry Nyquist and Ralph Hartley developed a handful of fundamental ideas related to the transmission of information, particularly in the context of the telegraph as a communications system. At the time, these concepts were powerful breakthroughs individually, but they were not part of a comprehensive theory. In the 1940s, Claude Shannon developed the concept of channel capacity, based in part on the ideas of Nyquist and Hartley, and then formulated a complete theory of information and its transmission.\n\nIn 1927, Nyquist determined that the number of independent pulses that could be put through a telegraph channel per unit time is limited to twice the bandwidth of the channel. In symbols,\n\nwhere \"f\" is the pulse frequency (in pulses per second) and \"B\" is the bandwidth (in hertz). The quantity 2\"B\" later came to be called the \"Nyquist rate\", and transmitting at the limiting pulse rate of 2\"B\" pulses per second as \"signalling at the Nyquist rate\". Nyquist published his results in 1928 as part of his paper \"Certain topics in Telegraph Transmission Theory.\"\n\nDuring 1928, Hartley formulated a way to quantify information and its line rate (also known as data signalling rate \"R\" bits per second). This method, later known as Hartley's law, became an important precursor for Shannon's more sophisticated notion of channel capacity.\n\nHartley argued that the maximum number of distinguishable pulse levels that can be transmitted and received reliably over a communications channel is limited by the dynamic range of the signal amplitude and the precision with which the receiver can distinguish amplitude levels. Specifically, if the amplitude of the transmitted signal is restricted to the range of [−\"A\" ... +\"A\"] volts, and the precision of the receiver is ±Δ\"V\" volts, then the maximum number of distinct pulses \"M\" is given by\n\nBy taking information per pulse in bit/pulse to be the base-2-logarithm of the number of distinct messages \"M\" that could be sent, Hartley constructed a measure of the line rate \"R\" as:\n\nwhere \"f\" is the pulse rate, also known as the symbol rate, in symbols/second or baud.\n\nHartley then combined the above quantification with Nyquist's observation that the number of independent pulses that could be put through a channel of bandwidth \"B\" hertz was 2\"B\" pulses per second, to arrive at his quantitative measure for achievable line rate.\n\nHartley's law is sometimes quoted as just a proportionality between the analog bandwidth, \"B\", in Hertz and what today is called the digital bandwidth, \"R\", in bit/s.\nOther times it is quoted in this more quantitative form, as an achievable line rate of \"R\" bits per second:\n\nHartley did not work out exactly how the number \"M\" should depend on the noise statistics of the channel, or how the communication could be made reliable even when individual symbol pulses could not be reliably distinguished to \"M\" levels; with Gaussian noise statistics, system designers had to choose a very conservative value of \"M\" to achieve a low error rate.\n\nThe concept of an error-free capacity awaited Claude Shannon, who built on Hartley's observations about a logarithmic measure of information and Nyquist's observations about the effect of bandwidth limitations.\n\nHartley's rate result can be viewed as the capacity of an errorless \"M\"-ary channel of 2\"B\" symbols per second. Some authors refer to it as a capacity. But such an errorless channel is an idealization, and if M is chosen small enough to make the noisy channel nearly errorless, the result is necessarily less than the Shannon capacity of the noisy channel of bandwidth \"B\", which is the Hartley–Shannon result that followed later.\n\nClaude Shannon's development of information theory during World War II provided the next big step in understanding how much information could be reliably communicated through noisy channels. Building on Hartley's foundation, Shannon's noisy channel coding theorem (1948) describes the maximum possible efficiency of error-correcting methods versus levels of noise interference and data corruption. The proof of the theorem shows that a randomly constructed error-correcting code is essentially as good as the best possible code; the theorem is proved through the statistics of such random codes.\n\nShannon's theorem shows how to compute a channel capacity from a statistical description of a channel, and establishes that given a noisy channel with capacity C and information transmitted at a line rate \"R\", then if\n\nthere exists a coding technique which allows the probability of error at the receiver to be made arbitrarily small. This means that theoretically, it is possible to transmit information nearly without error up to nearly a limit of C bits per second.\n\nThe converse is also important. If\n\nthe probability of error at the receiver increases without bound as the rate is increased. So no useful information can be transmitted beyond the channel capacity. The theorem does not address the rare situation in which rate and capacity are equal.\n\nThe Shannon–Hartley theorem establishes what that channel capacity is for a finite-bandwidth continuous-time channel subject to Gaussian noise. It connects Hartley's result with Shannon's channel capacity theorem in a form that is equivalent to specifying the \"M\" in Hartley's line rate formula in terms of a signal-to-noise ratio, but achieving reliability through error-correction coding rather than through reliably distinguishable pulse levels.\n\nIf there were such a thing as a noise-free analog channel, one could transmit unlimited amounts of error-free data over it per unit of time (Note: An infinite-bandwidth analog channel can't transmit unlimited amounts of error-free data, without infinite signal power). Real channels, however, are subject to limitations imposed by both finite bandwidth and nonzero noise.\n\nBandwidth and noise affect the rate at which information can be transmitted over an analog channel. Bandwidth limitations alone do not impose a cap on the maximum information rate because it is still possible for the signal to take on an indefinitely large number of different voltage levels on each symbol pulse, with each slightly different level being assigned a different meaning or bit sequence. Taking into account both noise and bandwidth limitations, however, there is a limit to the amount of information that can be transferred by a signal of a bounded power, even when sophisticated multi-level encoding techniques are used.\n\nIn the channel considered by the Shannon–Hartley theorem, noise and signal are combined by addition. That is, the receiver measures a signal that is equal to the sum of the signal encoding the desired information and a continuous random variable that represents the noise. This addition creates uncertainty as to the original signal's value. If the receiver has some information about the random process that generates the noise, one can in principle recover the information in the original signal by considering all possible states of the noise process. In the case of the Shannon–Hartley theorem, the noise is assumed to be generated by a Gaussian process with a known variance. Since the variance of a Gaussian process is equivalent to its power, it is conventional to call this variance the noise power.\n\nSuch a channel is called the Additive White Gaussian Noise channel, because Gaussian noise is added to the signal; \"white\" means equal amounts of noise at all frequencies within the channel bandwidth. Such noise can arise both from random sources of energy and also from coding and measurement error at the sender and receiver respectively. Since sums of independent Gaussian random variables are themselves Gaussian random variables, this conveniently simplifies analysis, if one assumes that such error sources are also Gaussian and independent.\n\nComparing the channel capacity to the information rate from Hartley's law, we can find the effective number of distinguishable levels \"M\":\n\nThe square root effectively converts the power ratio back to a voltage ratio, so the number of levels is approximately proportional to the ratio of signal RMS amplitude to noise standard deviation.\n\nThis similarity in form between Shannon's capacity and Hartley's law should not be interpreted to mean that \"M\" pulse levels can be literally sent without any confusion. More levels are needed to allow for redundant coding and error correction, but the net data rate that can be approached with coding is equivalent to using that \"M\" in Hartley's law.\n\nIn the simple version above, the signal and noise are fully uncorrelated, in which case \"S\" + \"N\" is the total power of the received signal and noise together. A generalization of the above equation for the case where the additive noise is not white (or that the is not constant with frequency over the bandwidth) is obtained by treating the channel as many narrow, independent Gaussian channels in parallel:\n\nwhere\n\nNote: the theorem only applies to Gaussian stationary process noise. This formula's way of introducing frequency-dependent noise cannot describe all continuous-time noise processes. For example, consider a noise process consisting of adding a random wave whose amplitude is 1 or −1 at any point in time, and a channel that adds such a wave to the source signal. Such a wave's frequency components are highly dependent. Though such a noise may have a high power, it is fairly easy to transmit a continuous signal with much less power than one would need if the underlying noise was a sum of independent noises in each frequency band.\n\nFor large or small and constant signal-to-noise ratios, the capacity formula can be approximated:\n\n\n\n\n\n\n"}
{"id": "63763", "url": "https://en.wikipedia.org/wiki?curid=63763", "title": "Solved game", "text": "Solved game\n\nA solved game is a game whose outcome (win, lose or draw) can be correctly predicted from any position, assuming that both players play perfectly.\nThis concept is usually applied to abstract strategy games, and especially to games with full information and no element of chance;\nsolving such a game may use combinatorial game theory and/or computer assistance.\n\nA two-player game can be solved on several levels:\n\n\n\n\nDespite their name, many game theorists believe that \"ultra-weak\" proofs are the deepest, most interesting and valuable. \"Ultra-weak\" proofs require a scholar to reason about the abstract properties of the game, and show how these properties lead to certain outcomes if perfect play is realized.\n\nBy contrast, \"strong\" proofs often proceed by brute force—using a computer to exhaustively search a game tree to figure out what would happen if perfect play were realized. The resulting proof gives an optimal strategy for every possible position on the board. However, these proofs are not as helpful in understanding deeper reasons why some games are solvable as a draw, and other, seemingly very similar games are solvable as a win.\n\nGiven the rules of any two-person game with a finite number of positions, one can always trivially construct a minimax algorithm that would exhaustively traverse the game tree. However, since for many non-trivial games such an algorithm would require an infeasible amount of time to generate a move in a given position, a game is not considered to be solved weakly or strongly unless the algorithm can be run by existing hardware in a reasonable time. Many algorithms rely on a huge pre-generated database, and are effectively nothing more.\n\nAs an example of a strong solution, the game of tic-tac-toe is solvable as a draw for both players with perfect play (a result even manually determinable by schoolchildren). Games like nim also admit a rigorous analysis using combinatorial game theory.\n\nWhether a game is solved is not necessarily the same as whether it remains interesting for humans to play. Even a strongly solved game can still be interesting if its solution is too complex to be memorized; conversely, a weakly solved game may lose its attraction if the winning strategy is simple enough to remember (e.g. Maharajah and the Sepoys). An ultra-weak solution (e.g. Chomp or Hex on a sufficiently large board) generally does not affect playability.\n\nMoreover, even if the game is not solved, it is possible that an algorithm yields a good approximate solution: for instance, an article in \"Science\" from January 2015 claims that their heads up limit Texas hold 'em poker bot Cepheus guarantees that a human lifetime of play is not sufficient to establish with statistical significance that its strategy is not an exact solution.\n\nIn game theory, perfect play is the behavior or strategy of a player that leads to the best possible outcome for that player regardless of the response by the opponent. Perfect play for a game is known when the game is solved. Based on the rules of a game, every possible final position can be evaluated (as a win, loss or draw). By backward reasoning, one can recursively evaluate a non-final position as identical to the position that is one move away and best valued for the player whose move it is. Thus a transition between positions can never result in a better evaluation for the moving player, and a perfect move in a position would be a transition between positions that are equally evaluated. As an example, a perfect player in a drawn position would always get a draw or win, never a loss. If there are multiple options with the same outcome, perfect play is sometimes considered the fastest method leading to a good result, or the slowest method leading to a bad result.\n\nPerfect play can be generalized to non-perfect information games, as the strategy that would guarantee the highest minimal expected outcome regardless of the strategy of the opponent. As an example, the perfect strategy for rock–paper–scissors would be to randomly choose each of the options with equal (1/3) probability. The disadvantage in this example is that this strategy will never exploit non-optimal strategies of the opponent, so the expected outcome of this strategy versus any strategy will always be equal to the minimal expected outcome.\n\nAlthough the optimal strategy of a game may not (yet) be known, a game-playing computer might still benefit from solutions of the game from certain endgame positions (in the form of endgame tablebases), which will allow it to play perfectly after some point in the game. Computer chess programs are well known for doing this.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "203056", "url": "https://en.wikipedia.org/wiki?curid=203056", "title": "Spherical harmonics", "text": "Spherical harmonics\n\nIn mathematics and physical science, spherical harmonics are special functions defined on the surface of a sphere. They are often employed in solving partial differential equations that commonly occur in science. The spherical harmonics are a complete set of orthogonal functions on the sphere, and thus may be used to represent functions defined on the surface of a sphere, just as circular functions (sines and cosines) are used to represent functions on a circle via Fourier series. Like the sines and cosines in Fourier series, the spherical harmonics may be organized by (spatial) angular frequency, as seen in the rows of functions in the illustration on the right. Further, spherical harmonics are basis functions for SO(3), the group of rotations in three dimensions, and thus play a central role in the group theoretic discussion of SO(3).\n\nDespite their name, spherical harmonics take their simplest form in Cartesian coordinates, where they can be defined as homogeneous polynomials of degree formula_1 in formula_2 that obey Laplace's equation. Functions that satisfy Laplace's equation are often said to be harmonic, hence the name spherical harmonics. The connection with spherical coordinates arises immediately if one uses the homogeneity to extract a factor of formula_3 from the above-mentioned polynomial of degree formula_1; the remaining factor can be regarded as a function of the spherical angular coordinates formula_5 and formula_6 only, or equivalently of the orientational unit vector formula_7 specified by these angles. In this setting, they may be viewed as the angular portion of a set of solutions to Laplace's equation in three dimensions, and this viewpoint is often taken as an alternative definition.\n\nA specific set of spherical harmonics, denoted formula_8 or formula_9, are called Laplace's spherical harmonics, as they were first introduced by Pierre Simon de Laplace in 1782. These functions\nform an orthogonal system, and are thus basic to the expansion of a general function on the sphere as alluded to above.\n\nSpherical harmonics are important in many theoretical and practical applications, e.g., the representation of multipole electrostatic and electromagnetic fields, computation of atomic orbital electron configurations, representation of gravitational fields, geoids, fiber reconstruction for estimation of the path and location of neural axons based on the properties of water diffusion from diffusion-weighted MRI imaging for streamline tractography, and the magnetic fields of planetary bodies and stars, and characterization of the cosmic microwave background radiation. In 3D computer graphics, spherical harmonics play a role in a wide variety of topics including indirect lighting (ambient occlusion, global illumination, precomputed radiance transfer, etc.) and modelling of 3D shapes.\n\nSpherical harmonics were first investigated in connection with the Newtonian potential of Newton's law of universal gravitation in three dimensions. In 1782, Pierre-Simon de Laplace had, in his \"Mécanique Céleste\", determined that the gravitational potential at a point x associated with a set of point masses \"m\" located at points x was given by\n\nEach term in the above summation is an individual Newtonian potential for a point mass. Just prior to that time, Adrien-Marie Legendre had investigated the expansion of the Newtonian potential in powers of \"r\" = |x| and \"r\" = |x|. He discovered that if \"r\" ≤ \"r\" then\n\nwhere γ is the angle between the vectors x and x. The functions \"P\" are the Legendre polynomials, and they are a special case of spherical harmonics. Subsequently, in his 1782 memoire, Laplace investigated these coefficients using spherical coordinates to represent the angle γ between x and x. (See Applications of Legendre polynomials in physics for a more detailed analysis.)\n\nIn 1867, William Thomson (Lord Kelvin) and Peter Guthrie Tait introduced the solid spherical harmonics in their \"Treatise on Natural Philosophy\", and also first introduced the name of \"spherical harmonics\" for these functions. The solid harmonics were homogeneous polynomial solutions of Laplace's equation\nBy examining Laplace's equation in spherical coordinates, Thomson and Tait recovered Laplace's spherical harmonics. (See the section below, \"Harmonic polynomial representation\".) The term \"Laplace's coefficients\" was employed by William Whewell to describe the particular system of solutions introduced along these lines, whereas others reserved this designation for the zonal spherical harmonics that had properly been introduced by Laplace and Legendre.\n\nThe 19th century development of Fourier series made possible the solution of a wide variety of physical problems in rectangular domains, such as the solution of the heat equation and wave equation. This could be achieved by expansion of functions in series of trigonometric functions. Whereas the trigonometric functions in a Fourier series represent the fundamental modes of vibration in a string, the spherical harmonics represent the fundamental modes of vibration of a sphere in much the same way. Many aspects of the theory of Fourier series could be generalized by taking expansions in spherical harmonics rather than trigonometric functions. This was a boon for problems possessing spherical symmetry, such as those of celestial mechanics originally studied by Laplace and Legendre.\n\nThe prevalence of spherical harmonics already in physics set the stage for their later importance in the 20th century birth of quantum mechanics. The spherical harmonics are eigenfunctions of the square of the orbital angular momentum operator\nand therefore they represent the different quantized configurations of atomic orbitals.\n\nLaplace's equation imposes that the divergence of the gradient of a scalar field is zero. In spherical coordinates this is:\n\nConsider the problem of finding solutions of the form . By separation of variables, two differential equations result by imposing Laplace's equation:\nThe second equation can be simplified under the assumption that has the form . Applying separation of variables again to the second equation gives way to the pair of differential equations\n\nfor some number . A priori, is a complex constant, but because must be a periodic function whose period evenly divides , is necessarily an integer and is a linear combination of the complex exponentials . The solution function is regular at the poles of the sphere, where . Imposing this regularity in the solution of the second equation at the boundary points of the domain is a Sturm–Liouville problem that forces the parameter to be of the form for some non-negative integer with ; this is also explained below in terms of the orbital angular momentum. Furthermore, a change of variables transforms this equation into the Legendre equation, whose solution is a multiple of the associated Legendre polynomial . Finally, the equation for has solutions of the form ; requiring the solution to be regular throughout forces .\n\nHere the solution was assumed to have the special form . For a given value of , there are independent solutions of this form, one for each integer with . These angular solutions are a product of trigonometric functions, here represented as a complex exponential, and associated Legendre polynomials:\n\nwhich fulfill\n\nHere is called a spherical harmonic function of degree and order , is an associated Legendre polynomial, is a normalization constant, and and represent colatitude and longitude, respectively. In particular, the colatitude , or polar angle, ranges from at the North Pole, to at the Equator, to at the South Pole, and the longitude , or azimuth, may assume all values with . For a fixed integer , every solution of the eigenvalue problem\nis a linear combination of . In fact, for any such solution, is the expression in spherical coordinates of a homogeneous polynomial that is harmonic (see below), and so counting dimensions shows that there are linearly independent such polynomials.\n\nThe general solution to Laplace's equation in a ball centered at the origin is a linear combination of the spherical harmonic functions multiplied by the appropriate scale factor ,\n\nwhere the are constants and the factors are known as solid harmonics. Such an expansion is valid in the ball\n\nIn quantum mechanics, Laplace's spherical harmonics are understood in terms of the orbital angular momentum\nThe is conventional in quantum mechanics; it is convenient to work in units in which . The spherical harmonics are eigenfunctions of the square of the orbital angular momentum\nLaplace's spherical harmonics are the joint eigenfunctions of the square of the orbital angular momentum and the generator of rotations about the azimuthal axis:\n\nThese operators commute, and are densely defined self-adjoint operators on the Hilbert space of functions \"f\" square-integrable with respect to the normal distribution on R:\nFurthermore, L is a positive operator.\n\nIf \"Y\" is a joint eigenfunction of L and \"L\", then by definition\nfor some real numbers \"m\" and λ. Here \"m\" must in fact be an integer, for \"Y\" must be periodic in the coordinate φ with period a number that evenly divides 2π. Furthermore, since\nand each of \"L\", \"L\", \"L\" are self-adjoint, it follows that λ ≥ \"m\".\n\nDenote this joint eigenspace by \"E\", and define the raising and lowering operators by\nThen \"L\" and \"L\" commute with L, and the Lie algebra generated by \"L\", \"L\", \"L\" is the special linear Lie algebra of order 2, formula_30, with commutation relations\nThus (it is a \"raising operator\") and (it is a \"lowering operator\"). In particular, must be zero for \"k\" sufficiently large, because the inequality λ ≥ \"m\" must hold in each of the nontrivial joint eigenspaces. Let \"Y\" ∈ \"E\" be a nonzero joint eigenfunction, and let \"k\" be the least integer such that\nThen, since\nit follows that\nThus λ = ℓ(ℓ+1) for the positive integer .\n\nSee also the section below on spherical harmonics in higher dimensions.\n\nThe spherical harmonics can be expressed as the restriction to the unit sphere of certain polynomial functions on formula_35. Specifically, we say that a polynomial function formula_36 on formula_35 is \"homogeneous of degree\" formula_1 if \nfor all real numbers formula_40 and all formula_41. We say that formula_36 is \"harmonic\" if\nwhere formula_44 is the Laplacian. Then for each formula_1, we define\n\nFor example, when formula_47, formula_48 is just the 3-dimensional space of all linear functions, since any such function is automatically harmonic. Meanwhile, when formula_49, we have a 5-dimensional space:\n\nFor any formula_1, the space of spherical harmonics of degree formula_1 is just the space of restrictions to the sphere of the elements of formula_53. As suggested in the introduction, this perspective is presumably the origin of the term \"spherical harmonic\" (i.e., the restriction to the sphere of a harmonic function).\n\nFor example, the formula\ndefines a homogeneous polynomial of degree formula_1 on formula_35, which happens to be independent of formula_57. This polynomial is easily seen to be harmonic. If we write formula_36 in spherical coordinates formula_59 and then restrict to formula_60, we obtain\nwhich can be rewritten as\nAfter using the formula for the associated Legendre polynomial formula_63, we may recognize this as the formula for the spherical harmonic formula_64 (See the section below on special cases of the spherical harmonics.)\n\nSeveral different normalizations are in common use for the Laplace spherical harmonic functions. Throughout the section, we use the standard convention that (see associated Legendre polynomials)\n\nwhich is the natural normalization given by Rodrigues' formula.\n\nIn acoustics, the Laplace spherical harmonics are generally defined as (this is the convention used in this article)\n\nwhile in quantum mechanics: \n\nwhere formula_68 are associated Legendre polynomials without the Condon–Shortley phase (to avoid counting the phase twice).\n\nIn both definitions, the spherical harmonics are orthonormal\n\nwhere δ is the Kronecker delta and \"d\"Ω = sinθ \"d\"φ \"d\"θ. This normalization is used in quantum mechanics because it ensures that probability is normalized, i.e.\n\nThe Condon-Shortley phase convention is used here for consistency. The corresponding inverse equations are\n\nThe real spherical harmonics are sometimes known as \"tesseral spherical harmonics\". These functions have the same orthonormality properties as the complex ones above.\nThe harmonics with \"m\" > 0 are said to be of cosine type, and those with \"m\" < 0 of sine type. The reason for this can be seen by writing the functions in terms of the Legendre polynomials as\n\nformula_72\n\nThe same sine and cosine factors can be also seen in the following subsection that deals with the cartesian representation.\n\nSee here for a list of real spherical harmonics up to and including formula_73, which can be seen to be consistent with the output of the equations above.\n\nAs is known from the analytic solutions for the hydrogen atom, the eigenfunctions of the angular part of the wave function are spherical harmonics.\nHowever, the solutions of the non-relativistic Schrödinger equation without magnetic terms can be made real.\nThis is why the real forms are extensively used in basis functions for quantum chemistry, as the programs don't then need to use complex algebra. Here, it is important to note that the real functions span the same space as the complex ones would.\n\nFor example, as can be seen from the table of spherical harmonics, the usual \"p\" functions (formula_74) are complex and mix axis directions, but the real versions are essentially just \"x\", \"y\" and \"z\".\n\nIf the quantum mechanical convention is adopted for the formula_75, then,\n\nHere, formula_7 is the vector with components formula_2, and\n\nis a vector with complex coefficients. It suffices to take formula_80 as a real parameter.\nThe essential property of formula_81 is that it is null:\n\nIn naming this generating function after Herglotz, we follow , who credit unpublished notes by him for its discovery.\n\nEssentially all the properties of the spherical harmonics can be derived from this generating function. An immediate benefit of this definition is that if the c-number vector\nformula_7 is replaced by the quantum mechanical spin vector operator formula_84, one obtains a generating function for a standardized set of spherical tensor operators,\nformula_85:\n\nThe parallelism of the two definitions ensures that the formula_87's transform under rotations (see below) in the same way as the formula_75's, which in turn guarantees that they are spherical tensor operators, formula_89, with formula_90 and formula_91, obeying all the properties of such operators, such as the Clebsch-Gordan composition theorem, and the Wigner-Eckart theorem. They are, moreover, a standardized set with a fixed scale or normalization.\n\nThe Herglotzian definition yields polynomials which may, if one wishes, be further factorized into a polynomial of formula_92 and another of formula_93 and formula_94, as follows (Condon-Shortley phase):\nand for \"m\" = 0:\nHere\n\nand\nFor formula_100 this reduces to\n\nThe factor formula_102 is essentially the associated Legendre polynomial formula_103, and the factors formula_104 are essentially formula_105.\n\nUsing the expressions for formula_106, formula_107, and formula_108 listed explicitly above we obtain: \nIt may be verified that this agrees with the function listed here and here.\n\nUsing the equations above to form the real spherical harmonics, it is seen that for formula_111 only the formula_112 terms (cosines) are included, and for formula_113 only the formula_114 terms (sines) are included:\n\nand for \"m\" = 0:\n\n1. When formula_100, the spherical harmonics reduce to the ordinary Legendre polynomials:\n\n2. When formula_119,\n\nor more simply in Cartesian coordinates,\n\n3. At the north pole, where formula_122, and formula_6 is undefined,\nall spherical harmonics except those with formula_124 vanish:\n\nThe spherical harmonics have deep and consequential properties under the operations of spatial inversion (parity) and rotation.\n\nThe spherical harmonics have definite parity. That is, they are either even or odd with respect to inversion about the origin. Inversion is represented by the operator formula_126. Then, as can be seen in many ways (perhaps most simply from the Herglotz generating function), with\nformula_127 being a unit vector,\n\nIn terms of the spherical angles, parity transforms a point with coordinates formula_129 to formula_130. The statement of the parity of spherical harmonics is then\n\nParity continues to hold for real spherical harmonics, and for spherical harmonics in higher dimensions: applying a point reflection to a spherical harmonic of degree ℓ changes the sign by a factor of (−1).\n\nConsider a rotation formula_132 about the origin that sends the unit vector formula_127 to formula_134. Under this operation, a spherical harmonic of degree formula_1 and order formula_136 transforms into a linear combination of spherical harmonics of the same degree. That is,\nwhere formula_138 is a matrix of order formula_139 that depends on the\nrotation formula_132. However, this is not the standard way of expressing this property. In the standard way one writes,\n\nwhere formula_142 is the complex conjugate of an element of the Wigner D-matrix.\n\nThe rotational behavior of the spherical harmonics is perhaps their quintessential feature from the viewpoint of group theory. The formula_143's of degree formula_1 provide a basis set of functions for the irreducible representation of the group SO(3) of dimension formula_139. Many facts about spherical harmonics (such as the addition theorem) that are proved laboriously using the methods of analysis acquire simpler proofs and deeper significance using the methods of symmetry.\n\nThe Laplace spherical harmonics form a complete set of orthonormal functions and thus form an orthonormal basis of the Hilbert space of square-integrable functions. On the unit sphere, any square-integrable function can thus be expanded as a linear combination of these:\n\nThis expansion holds in the sense of mean-square convergence — convergence in L of the sphere — which is to say that\n\nThe expansion coefficients are the analogs of Fourier coefficients, and can be obtained by multiplying the above equation by the complex conjugate of a spherical harmonic, integrating over the solid angle Ω, and utilizing the above orthogonality relationships. This is justified rigorously by basic Hilbert space theory. For the case of orthonormalized harmonics, this gives:\n\nIf the coefficients decay in ℓ sufficiently rapidly — for instance, exponentially — then the series also converges uniformly to \"f\".\n\nA square-integrable function \"f\" can also be expanded in terms of the real harmonics \"Y\" above as a sum\n\nThe convergence of the series holds again in the same sense, but the benefit of the real expansion is that for real functions \"f\" the expansion coefficients become real.\n\nThe total power of a function \"f\" is defined in the signal processing literature as the integral of the function squared, divided by the area of its domain. Using the orthonormality properties of the real unit-power spherical harmonic functions, it is straightforward to verify that the total power of a function defined on the unit sphere is related to its spectral coefficients by a generalization of Parseval's theorem (here, the theorem is stated for Schmidt semi-normalized harmonics, the relationship is slightly different for orthonormal harmonics):\n\nwhere\n\nis defined as the angular power spectrum (for Schmidt semi-normalized harmonics). In a similar manner, one can define the cross-power of two functions as\n\nwhere\n\nis defined as the cross-power spectrum. If the functions \"f\" and \"g\" have a zero mean (i.e., the spectral coefficients \"f\" and \"g\" are zero), then \"S\"(ℓ) and \"S\"(ℓ) represent the contributions to the function's variance and covariance for degree ℓ, respectively. It is common that the (cross-)power spectrum is well approximated by a power law of the form\n\nWhen β = 0, the spectrum is \"white\" as each degree possesses equal power. When β < 0, the spectrum is termed \"red\" as there is more power at the low degrees with long wavelengths than higher degrees. Finally, when β > 0, the spectrum is termed \"blue\". The condition on the order of growth of \"S\"(ℓ) is related to the order of differentiability of \"f\" in the next section.\n\nOne can also understand the differentiability properties of the original function \"f\" in terms of the asymptotics of \"S\"(ℓ). In particular, if \"S\"(ℓ) decays faster than any rational function of ℓ as ℓ → ∞, then \"f\" is infinitely differentiable. If, furthermore, \"S\"(ℓ) decays exponentially, then \"f\" is actually real analytic on the sphere.\n\nThe general technique is to use the theory of Sobolev spaces. Statements relating the growth of the \"S\"(ℓ) to differentiability are then similar to analogous results on the growth of the coefficients of Fourier series. Specifically, if\nthen \"f\" is in the Sobolev space \"H\"(\"S\"). In particular, the Sobolev embedding theorem implies that \"f\" is infinitely differentiable provided that\nfor all \"s\".\n\nA mathematical result of considerable interest and use is called the \"addition theorem\" for spherical harmonics. This is a generalization of the trigonometric identity\nin which the role of the trigonometric functions appearing on the right-hand side is played by the spherical harmonics and that of the left-hand side is played by the Legendre polynomials.\n\nThe \"addition theorem\" states\n\nwhere \"P\" is the Legendre polynomial of degree ℓ. This expression is valid for both real and complex harmonics. The result can be proven analytically, using the properties of the Poisson kernel in the unit ball, or geometrically by applying a rotation to the vector y so that it points along the \"z\"-axis, and then directly calculating the right-hand side.\n\nIn particular, when x = y, this gives Unsöld's theorem\nwhich generalizes the identity cosθ + sinθ = 1 to two dimensions.\n\nIn the expansion (), the left-hand side \"P\"(x·y) is a constant multiple of the degree ℓ zonal spherical harmonic. From this perspective, one has the following generalization to higher dimensions. Let \"Y\" be an arbitrary orthonormal basis of the space H of degree ℓ spherical harmonics on the \"n\"-sphere. Then formula_159, the degree ℓ zonal harmonic corresponding to the unit vector \"x\", decomposes as\n\nFurthermore, the zonal harmonic formula_160 is given as a constant multiple of the appropriate Gegenbauer polynomial:\nCombining () and () gives () in dimension \"n\" = 2 when x and y are represented in spherical coordinates. Finally, evaluating at x = y gives the functional identity\nwhere ω is the volume of the (\"n\"−1)-sphere.\n\nAnother useful identity expresses the product of two spherical harmonics as a sum over spherical harmonics \nwhere the values of formula_163 and formula_164 are determined by the selection rules for the 3j-symbols.\n\nThe Clebsch–Gordan coefficients are the coefficients appearing in the expansion of the product of two spherical harmonics in terms of spherical harmonics themselves. A variety of techniques are available for doing essentially the same calculation, including the Wigner 3-jm symbol, the Racah coefficients, and the Slater integrals. Abstractly, the Clebsch–Gordan coefficients express the tensor product of two irreducible representations of the rotation group as a sum of irreducible representations: suitably normalized, the coefficients are then the multiplicities.\n\nThe Laplace spherical harmonics formula_143 can be visualized by considering their \"nodal lines\", that is, the set of points on the sphere where formula_166, or alternatively where formula_167. Nodal lines of formula_143 are composed of ℓ circles: there are |\"m\"| circles along longitudes and ℓ−|\"m\"| circles along latitudes. One can determine the number of nodal lines of each type by counting the number of zeros of formula_143 in the latitudinal and longitudinal directions independently. For the latitudinal direction, the real and imaginary components of the associated Legendre polynomials each possess ℓ−|\"m\"| zeros, whereas for the longitudinal direction, the trigonometric sin and cos functions possess 2|\"m\"| zeros.\n\nWhen the spherical harmonic order \"m\" is zero (upper-left in the figure), the spherical harmonic functions do not depend upon longitude, and are referred to as zonal. Such spherical harmonics are a special case of zonal spherical functions. When ℓ = |\"m\"| (bottom-right in the figure), there are no zero crossings in latitude, and the functions are referred to as sectoral. For the other cases, the functions checker the sphere, and they are referred to as tesseral.\n\nMore general spherical harmonics of degree ℓ are not necessarily those of the Laplace basis formula_143, and their nodal sets can be of a fairly general kind.\n\nAnalytic expressions for the first few orthonormalized Laplace spherical harmonics that use the Condon-Shortley phase convention:\n\nThe classical spherical harmonics are defined as functions on the unit sphere \"S\" inside three-dimensional Euclidean space. Spherical harmonics can be generalized to higher-dimensional Euclidean space R as follows. Let P denote the space of homogeneous polynomials of degree ℓ in \"n\" variables. That is, a polynomial \"P\" is in P provided that\n\nLet A denote the subspace of P consisting of all harmonic polynomials; these are the solid spherical harmonics. Let H denote the space of functions on the unit sphere\nobtained by restriction from A.\n\nThe following properties hold:\n\n\n\n\nAn orthogonal basis of spherical harmonics in higher dimensions can be constructed inductively by the method of separation of variables, by solving the Sturm-Liouville problem for the spherical Laplacian\nwhere φ is the axial coordinate in a spherical coordinate system on \"S\". The end result of such a procedure is\nwhere the indices satisfy |ℓ| ≤ ℓ ≤ ... ≤ ℓ and the eigenvalue is −ℓ(ℓ + \"n\"−2). The functions in the product are defined in terms of the Legendre function\n\nThe space H of spherical harmonics of degree ℓ is a representation of the symmetry group of rotations around a point (SO(3)) and its double-cover SU(2). Indeed, rotations act on the two-dimensional sphere, and thus also on H by function composition\nfor ψ a spherical harmonic and ρ a rotation. The representation H is an irreducible representation of SO(3).\n\nThe elements of H arise as the restrictions to the sphere of elements of A: harmonic polynomials homogeneous of degree ℓ on three-dimensional Euclidean space R. By polarization of ψ ∈ A, there are coefficients formula_192 symmetric on the indices, uniquely determined by the requirement\nThe condition that ψ be harmonic is equivalent to the assertion that the tensor formula_192 must be trace free on every pair of indices. Thus as an irreducible representation of SO(3), H is isomorphic to the space of traceless symmetric tensors of degree ℓ.\n\nMore generally, the analogous statements hold in higher dimensions: the space H of spherical harmonics on the \"n\"-sphere is the irreducible representation of SO(\"n\"+1) corresponding to the traceless symmetric ℓ-tensors. However, whereas every irreducible tensor representation of SO(2) and SO(3) is of this kind, the special orthogonal groups in higher dimensions have additional irreducible representations that do not arise in this manner.\n\nThe special orthogonal groups have additional spin representations that are not tensor representations, and are \"typically\" not spherical harmonics. An exception are the spin representation of SO(3): strictly speaking these are representations of the double cover SU(2) of SO(3). In turn, SU(2) is identified with the group of unit quaternions, and so coincides with the 3-sphere. The spaces of spherical harmonics on the 3-sphere are certain spin representations of SO(3), with respect to the action by quaternionic multiplication.\n\nThe angle-preserving symmetries of the two-sphere are described by the group of Möbius transformations PSL(2,C). With respect to this group, the sphere is equivalent to the usual Riemann sphere. The group PSL(2,C) is isomorphic to the (proper) Lorentz group, and its action on the two-sphere agrees with the action of the Lorentz group on the celestial sphere in Minkowski space. The analog of the spherical harmonics for the Lorentz group is given by the hypergeometric series; furthermore, the spherical harmonics can be re-expressed in terms of the hypergeometric series, as SO(3) = PSU(2) is a subgroup of PSL(2,C).\n\nMore generally, hypergeometric series can be generalized to describe the symmetries of any symmetric space; in particular, hypergeometric series can be developed for any Lie group.\n\n\n\n"}
{"id": "380219", "url": "https://en.wikipedia.org/wiki?curid=380219", "title": "Stone space", "text": "Stone space\n\nIn topology, and related areas of mathematics, a Stone space is a non-empty compact totally disconnected Hausdorff space. Such spaces are also called \"profinite\" spaces. They are named after Marshall Harvey Stone.\n\nA form of Stone's representation theorem for Boolean algebras states that every Boolean algebra is isomorphic to the Boolean algebra of clopen sets of a Stone space. This isomorphism forms a category-theoretic duality between the categories of Boolean algebras and Stone spaces.\n\nEquivalently, Stone space is a topological space such that:\n"}
{"id": "3237359", "url": "https://en.wikipedia.org/wiki?curid=3237359", "title": "Subharmonic function", "text": "Subharmonic function\n\nIn mathematics, subharmonic and superharmonic functions are important classes of functions used extensively in partial differential equations, complex analysis and potential theory.\n\nIntuitively, subharmonic functions are related to convex functions of one variable as follows. If the graph of a convex function and a line intersect at two points, then the graph of the convex function is \"below\" the line between those points. In the same way, if the values of a subharmonic function are no larger than the values of a harmonic function on the \"boundary\" of a ball, then the values of the subharmonic function are no larger than the values of the harmonic function also \"inside\" the ball.\n\n\"Superharmonic\" functions can be defined by the same description, only replacing \"no larger\" with \"no smaller\". Alternatively, a superharmonic function is just the negative of a subharmonic function, and for this reason any property of subharmonic functions can be easily transferred to superharmonic functions.\n\nFormally, the definition can be stated as follows. Let formula_1 be a subset of the Euclidean space formula_2 and let\n\nbe an upper semi-continuous function. Then, formula_4 is called \"subharmonic\" if for any closed ball formula_5 of center formula_6 and radius formula_7 contained in formula_1 and every real-valued continuous function formula_9 on formula_5 that is harmonic in formula_11 and satisfies formula_12 for all formula_13 on the boundary formula_14 of formula_11 we have formula_12 for all formula_17\n\nNote that by the above, the function which is identically −∞ is subharmonic, but some authors exclude this function by definition.\n\nA function formula_18 is called \"superharmonic\" if formula_19 is subharmonic.\n\n\nIf formula_28 is analytic then formula_29 is subharmonic. More examples can be constructed by using the properties listed above,\nby taking maxima, convex combinations and limits. In dimension 1, all subharmonic functions can be obtained in this way.\n\nIf formula_18 is subharmonic in a region formula_31, in Euclidean space of dimension formula_32, formula_33 is harmonic in formula_31, and formula_35, then formula_33\nis called a harmonic majorant of formula_18. If a harmonic majorant exists, then there exists the least harmonic majorant, and\nwhile in dimension 2,\nwhere formula_33 is the least harmonic majorant, and formula_41 is a Borel measure in formula_31.\nThis is called the Riesz representation theorem.\n\nSubharmonic functions are of a particular importance in complex analysis, where they are intimately connected to holomorphic functions.\n\nOne can show that a real-valued, continuous function formula_43 of a complex variable (that is, of two real variables) defined on a set formula_44 is subharmonic if and only if for any closed disc formula_45 of center formula_46 and radius formula_7 one has\n\nIntuitively, this means that a subharmonic function is at any point no greater than the average of the values in a circle around that point, a fact which can be used to derive the maximum principle.\n\nIf formula_28 is a holomorphic function, then \nis a subharmonic function if we define the value of formula_51 at the zeros of formula_28 to be −∞. It follows that\nis subharmonic for every \"α\" > 0. This observation plays a role in the theory of Hardy spaces, especially for the study of \"H\" when 0 < \"p\" < 1.\n\nIn the context of the complex plane, the connection to the convex functions can be realized as well by the fact that a subharmonic function formula_28 on a domain formula_55 that is constant in the imaginary direction is convex in the real direction and vice versa.\n\nIf formula_18 is subharmonic in a region formula_57 of the complex plane, and formula_9 is harmonic on formula_57, then formula_9 is a harmonic majorant of formula_18 in formula_57 if formula_18≤formula_9 in formula_57. Such an inequality can be viewed as a growth condition on formula_18.\n\nLet \"φ\" be subharmonic, continuous and non-negative in an open subset \"Ω\" of the complex plane containing the closed unit disc \"D\"(0, 1). The \"radial maximal function\" for the function \"φ\" (restricted to the unit disc) is defined on the unit circle by\nIf \"P\" denotes the Poisson kernel, it follows from the subharmonicity that\nIt can be shown that the last integral is less than the value at e of the Hardy–Littlewood maximal function \"φ\" of the restriction of \"φ\" to the unit circle T,\nso that 0 ≤ \"M\" \"φ\" ≤ \"φ\". It is known that the Hardy–Littlewood operator is bounded on \"L\"(T) when 1 < \"p\" < ∞.\nIt follows that for some universal constant \"C\",\n\nIf \"f\" is a function holomorphic in \"Ω\" and 0 < \"p\" < ∞, then the preceding inequality applies to \"φ\" = |\"f\"|. It can be deduced from these facts that any function \"F\" in the classical Hardy space \"H\" satisfies\nWith more work, it can be shown that \"F\" has radial limits \"F\"(e) almost everywhere on the unit circle, and (by the dominated convergence theorem) that \"F\", defined by \"F\"(e) = \"F\"(\"r\"e) tends to \"F\" in \"L\"(T).\n\nSubharmonic functions can be defined on an arbitrary Riemannian manifold.\n\n\"Definition:\" Let \"M\" be a Riemannian manifold, and formula_72 an upper semicontinuous function. Assume that for any open subset formula_73, and any harmonic function \"f\" on \"U\", such that formula_74 on the boundary of \"U\", the inequality formula_74 holds on all \"U\". Then \"f\" is called \"subharmonic\".\n\nThis definition is equivalent to one given above. Also, for twice differentiable functions, subharmonicity is equivalent to the inequality formula_76, where formula_26 is the usual Laplacian.\n\n\n"}
{"id": "18063628", "url": "https://en.wikipedia.org/wiki?curid=18063628", "title": "The Principles of Mathematics", "text": "The Principles of Mathematics\n\nThe Principles of Mathematics (PoM) is a book written by Bertrand Russell in 1903. In it he presented his famous paradox and argued his thesis that mathematics and logic are identical.\nThe book presents a view of the foundations of mathematics and has become a classic reference. It reported on developments by Giuseppe Peano, Mario Pieri, Richard Dedekind, Georg Cantor, and others.\n\nIn 1905 Louis Couturat published a partial French translation that expanded the book's readership. In 1937 Russell prepared a new introduction saying, \"Such interest as the book now possesses is historical, and consists in the fact that it represents a certain stage in the development of its subject.\" Further editions were printed in 1938, 1951, 1996, and 2009.\n\n\"The Principles of Mathematics\" consists of 59 chapters divided into seven parts: indefinables in mathematics, number, quantity, order, infinity and continuity, space, matter and motion.\n\nIn chapter one, \"Definition of Pure Mathematics\", Russell asserts that : \n\nThere is an anticipation of relativity physics in the final part as the last three chapters consider Newton's laws of motion, absolute and relative motion, and Hertz's dynamics. However, Russell rejects what he calls \"the relational theory\", and says on page 489 : \nIn his review, G. H. Hardy says \"Mr. Russell is a firm believer in absolute position in space and time, a view as much out of fashion nowadays that Chapter [58: Absolute and Relative Motion] will be read with peculiar interest.\"\n\nReviews were prepared by G. E. Moore and Charles Sanders Peirce, but Moore's was never published and that of Peirce was brief and somewhat dismissive. He indicated that he thought it unoriginal, saying that the book \"can hardly be called literature\" and \"Whoever wishes a convenient introduction to the remarkable researches into the logic of mathematics that have been made during the last sixty years [...] will do well to take up this book.\"\n\nG. H. Hardy wrote a favorable review expecting the book to appeal more to philosophers than mathematicians. But he says : \nIn 1904 another review appeared in Bulletin of the American Mathematical Society (11(2):74–93) written by Edwin Bidwell Wilson. He says \"The delicacy of the question is such that even the greatest mathematicians and philosophers of to-day have made what seem to be substantial slips of judgement and have shown on occasions an astounding ignorance of the essence of the problem which they were discussing. ... all too frequently it has been the result of a wholly unpardonable disregard of the work already accomplished by others.\" Wilson recounts the developments of Peano that Russell reports, and takes the occasion to correct Henri Poincaré who had ascribed them to David Hilbert. In praise of Russell, Wilson says \"Surely the present work is a monument to patience, perseverance, and thoroughness.\" (page 88)\n\nIn 1938 the book was re-issued with a new preface by Russell. This preface was interpreted as a retreat from the realism of the first edition and a turn toward nominalist philosophy of symbolic logic. James Feibleman, an admirer of the book, thought Russell’s new preface went too far into nominalism so he wrote a rebuttal to this introduction. Feibleman says, \"It is the first comprehensive treatise on symbolic logic to be written in English; and it gives to that system of logic a realistic interpretation.\"\n\nIn 1959 Russell wrote \"My Philosophical Development\", in which he recalled the impetus to write the \"Principles\":\nRecalling the book after his later work, he provides this evaluation:\n\nSuch self-deprecation from the author after half a century of philosophical growth is understandable. On the other hand, Jules Vuillemin wrote in 1968: \n\nWhen W. V. O. Quine penned his autobiography, he wrote:\n\n\"The Principles\" was an early expression of analytic philosophy and thus has come under close examination. Peter Hylton wrote, \"The book has an air of excitement and novelty to it ... The salient characteristic of \"Principles\" is ... the way in which the technical work is integrated into metaphysical argument.\"\n\nIvor Grattan-Guinness made an in-depth study of \"Principles\". First he published \"Dear Russell – Dear Jourdain\" (1977) which included correspondence with Philip Jourdain who promulgated some of the book’s ideas. Then in 2000 Grattan-Guinness published \"The Search for Mathematical Roots 1870 – 1940\" which considered the author’s circumstances, the book’s composition and its shortcomings.\n\nIn 2006, Philip Ehrlich challenged the validity of Russell's analysis of infinitesimals in the Leibniz tradition.\nA recent study documents the non-sequiturs in Russell's critique of the infinitesimals of Gottfried Leibniz and Hermann Cohen.\n\n\n\n"}
{"id": "7896038", "url": "https://en.wikipedia.org/wiki?curid=7896038", "title": "Thomas Jones (mathematician)", "text": "Thomas Jones (mathematician)\n\nThomas Jones (23 June 1756 – 18 July 1807) was Head Tutor at Trinity College, Cambridge for twenty years and an outstanding teacher of mathematics. He is notable as a mentor of Adam Sedgwick.\n\nJones was born at Berriew, Montgomeryshire, in Wales.\n\nOn completing his studies at Shrewsbury School, Jones was admitted to St John's College, Cambridge on 28 May 1774, as a 'pensioner' (i.e. a fee-paying student, as opposed to a scholar or sizar). He was believed to be an illegitimate son of Mr Owen Owen, of Tyncoed, and his housekeeper, who afterwards married a Mr Jones, of Traffin, County Kerry, Thomas then being brought up as his son.\n\nOn 27 June 1776, Jones migrated from St John's College to Trinity College. He became a scholar in 1777 and obtained his BA in 1779, winning the First Smith's Prize and becoming Senior Wrangler. In 1782, he obtained his MA and became a Fellow of Trinity College in 1781. He became a Junior Dean, 1787–1789 and a Tutor, 1787–1807. He was ordained a deacon at the Peterborough parish on 18 June 1780. Then he was ordained priest, at the Ely parish on 6 June 1784, canon of Fen Ditton, Cambridgeshire, in 1784, and then canon of Swaffham Prior, also 1784. On 11 December 1791, he preached before the University, at Great St Mary's, a sermon against duelling (from Exodus XX. 13), which was prompted by a duel that had lately taken place near Newmarket between Henry Applewhaite and Richard Ryecroft, undergraduates of Pembroke, in which the latter was fatally wounded. Jones died on 18 July 1807, in lodgings in Edgware Road, London. He is buried in the cemetery of Dulwich College. A bust and a memorial tablet are in the ante-chapel of Trinity College.\n\nHis academic mentor was John Cranke (1746–1816). His Cambridge tutor was Thomas Postlethwaite.\n\n"}
{"id": "45084321", "url": "https://en.wikipedia.org/wiki?curid=45084321", "title": "Timeline of class field theory", "text": "Timeline of class field theory\n\nIn mathematics, class field theory is the study of abelian extensions of local and global fields. \n\n\n"}
{"id": "32307688", "url": "https://en.wikipedia.org/wiki?curid=32307688", "title": "Unipotent representation", "text": "Unipotent representation\n\nIn mathematics, a unipotent representation of a reductive group is a representation that has some similarities with unipotent conjugacy classes of groups. \n\nInformally, Langlands philosophy suggests that there should be a correspondence between representations of a reductive group and conjugacy classes a Langlands dual group, and the unipotent representations should be roughly the ones corresponding to unipotent classes in the dual group.\n\nUnipotent representations are supposed to be the basic \"building blocks\" out of which one can construct all other representations in the following sense. \nUnipotent representations should form a small (preferably finite) set of irreducible representations for each reductive group, such that all irreducible representations can be obtained from unipotent representations of possibly smaller groups by some sort of systematic process, such as (cohomological or parabolic) induction.\n\nOver finite fields, the unipotent representations are those that occur in the decomposition of the Deligne–Lusztig characters \"R\" of the trivial representation 1 of a torus \"T\" . They were classified by .\nSome examples of unipotent representations over finite fields are the trivial 1-dimensional representation, the Steinberg representation, and θ.\n\n classified the unipotent characters over non-archimedean local fields.\n\n discusses several different possible definitions of unipotent representations of real Lie groups.\n\n\n"}
{"id": "2275071", "url": "https://en.wikipedia.org/wiki?curid=2275071", "title": "W. Hugh Woodin", "text": "W. Hugh Woodin\n\nWilliam Hugh Woodin (born April 23, 1955) is an American mathematician and set theorist at Harvard University. He has made many notable contributions to the theory of inner models and determinacy. A type of large cardinal, the Woodin cardinal, bears his name.\n\nBorn in Tucson, Arizona, Woodin earned his Ph.D. from the University of California, Berkeley in 1984 under Robert M. Solovay. His dissertation title was \"Discontinuous Homomorphisms of C\"(\"Omega\") \"and Set Theory\". He served as chair of the Berkeley mathematics department for the 2002–2003 academic year. Woodin is a managing editor of the Journal of Mathematical Logic. He was elected a Fellow of the American Academy of Arts and Sciences in 2000.\n\nHe is the great-grandson of William Hartman Woodin, former Secretary of the Treasury.\n\nHe has done work on the theory of generic multiverses and the related concept of Ω-logic, which suggested an argument that the continuum hypothesis is either undecidable or false in the sense of mathematical platonism. Woodin criticizes this view arguing that it leads to a counterintuitive reduction in which all truths in the set theoretical universe can be decided from a small part of it. He claims that these and related mathematical results lead (intuitively) to the conclusion that Continuum Hypothesis has a truth value and the Platonistic approach is reasonable.\n\nWoodin now predicts that there should be a way of constructing an inner model for almost all known large cardinals, which he calls the Ultimate L and which would have similar properties as Gödel's constructible universe. In particular, the Continuum Hypothesis would be true in this universe.\n\n\n"}
{"id": "268145", "url": "https://en.wikipedia.org/wiki?curid=268145", "title": "Y-Δ transform", "text": "Y-Δ transform\n\nThe Y-Δ transform, also written wye-delta and also known by many other names, is a mathematical technique to simplify the analysis of an electrical network. The name derives from the shapes of the circuit diagrams, which look respectively like the letter Y and the Greek capital letter Δ. This circuit transformation theory was published by Arthur Edwin Kennelly in 1899. It is widely used in analysis of three-phase electric power circuits. \n\nThe Y-Δ transform can be considered a special case of the star-mesh transform for three resistors. In mathematics, the Y-Δ transform plays an important role in theory of circular planar graphs.\n\nThe Y-Δ transform is known by a variety of other names, mostly based upon the two shapes involved, listed in either order. The Y, spelled out as wye, can also be called T or star; the Δ, spelled out as delta, can also be called triangle, Π (spelled out as pi), or mesh. Thus, common names for the transformation include wye-delta or delta-wye, star-delta, star-mesh, or T-Π. \n\nThe transformation is used to establish equivalence for networks with three terminals. Where three elements terminate at a common node and none are sources, the node is eliminated by transforming the impedances. For equivalence, the impedance between any pair of terminals must be the same for both networks. The equations given here are valid for complex as well as real impedances\n\nThe general idea is to compute the impedance formula_1 at a terminal node of the Y circuit with impedances formula_2, formula_3 to adjacent nodes in the Δ circuit by\n\nwhere formula_5 are all impedances in the Δ circuit. This yields the specific formulae\n\nThe general idea is to compute an impedance formula_5 in the Δ circuit by\n\nwhere formula_9 is the sum of the products of all pairs of impedances in the Y circuit and formula_10 is the impedance of the node in the Y circuit which is opposite the edge with formula_5. The formula for the individual edges are thus\n\nOr, if using admittance instead of resistance:\n\nNote that the general formula in Y to Δ using admittance is similar to Δ to Y using resistance.\n\nThe feasibility of the transformation can be shown as a consequence of the superposition theorem for electric circuits. A short proof, rather than one derived as a corollary of the more general star-mesh transform, can be given as follows. The equivalence lies in the statement that for any external voltages (formula_14 and formula_15) applying at the three nodes (formula_16 and formula_17), the corresponding currents (formula_18 and formula_19) are exactly the same for both the Y and Δ circuit, and vice versa. In this proof, we start with given external currents at the nodes. According to the superposition theorem, the voltages can be obtained by studying the superposition of the resulting voltages at the nodes of the following three problems applied at the three nodes with current:\n\n\nThe equivalence can be readily shown by using Kirchhoff's circuit laws that formula_23. Now each problem is relatively simple, since it involves only one single ideal current source. To obtain exactly the same outcome voltages at the nodes for each problem, the equivalent resistances in the two circuits must be the same, this can be easily found by using the basic rules of series and parallel circuits:\n\nThough usually six equations are more than enough to express three variables (formula_25) in term of the other three variables(formula_26), here it is straightforward to show that these equations indeed lead to the above designed expressions.\n\nIn fact, the superposition theorem establishes the relation between the values of the resistances, the uniqueness theorem guarantees the uniqueness of such solution.\n\nResistive networks between two terminals can theoretically be simplified to a single equivalent resistor (more generally, the same is true of impedance). Series and parallel transforms are basic tools for doing so, but for complex networks such as the bridge illustrated here, they do not suffice. \n\nThe Y-Δ transform can be used to eliminate one node at a time and produce a network that can be further simplified, as shown. \n\nThe reverse transformation, Δ-Y, which adds a node, is often handy to pave the way for further simplification as well.\n\nEvery two-terminal network represented by a planar graph can be reduced to a single equivalent resistor by a sequence of series, parallel, Y-Δ, and Δ-Y transformations. However, there are non-planar networks that cannot be simplified using these transformations, such as a regular square grid wrapped around a torus, or any member of the Petersen family.\n\nIn graph theory, the Y-Δ transform means replacing a Y subgraph of a graph with the equivalent Δ subgraph. The transform preserves the number of edges in a graph, but not the number of vertices or the number of cycles. Two graphs are said to be Y-Δ equivalent if one can be obtained from the other by a series of Y-Δ transforms in either direction. For example, the Petersen family is a Y-Δ equivalence class.\n\nTo relate formula_27 from Δ to formula_28 from Y, the impedance between two corresponding nodes is compared. The impedance in either configuration is determined as if one of the nodes is disconnected from the circuit.\nThe impedance between \"N\" and \"N\" with \"N\" disconnected in Δ:\n\nTo simplify, let formula_30 be the sum of formula_27.\n\nThus,\n\nThe corresponding impedance between N and N in Y is simple:\n\nhence:\n\nRepeating for formula_36:\n\nand for formula_38:\n\nFrom here, the values of formula_28 can be determined by linear combination (addition and/or subtraction).\n\nFor example, adding (1) and (3), then subtracting (2) yields\n\nFor completeness:\n\nLet \n\nWe can write the Δ to Y equations as\n\nMultiplying the pairs of equations yields\n\nand the sum of these equations is\n\nFactor formula_53 from the right side, leaving formula_30 in the numerator, canceling with an formula_30 in the denominator.\n\nDivide (8) by (1)\n\nwhich is the equation for formula_58. Dividing (8) by (2) or (3) (expressions for formula_59 or formula_60) gives the remaining equations.\n\n\n\n"}
{"id": "890891", "url": "https://en.wikipedia.org/wiki?curid=890891", "title": "Zero–one law", "text": "Zero–one law\n\nIn probability theory, a zero–one law is a result that states that an event must have probability 0 or 1 and no intermediate value. Sometimes, the statement is that the limit of certain probabilities must be 0 or 1.\n\nIt may refer to: \n"}
