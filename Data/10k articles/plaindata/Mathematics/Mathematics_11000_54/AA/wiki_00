{"id": "54570072", "url": "https://en.wikipedia.org/wiki?curid=54570072", "title": "Agnes E. Wells", "text": "Agnes E. Wells\n\nAgnes Ermina Wells, Ph.D. (January 4, 1876, Saginaw, Michigan – July 6, 1959) was an American educator and a women's equal rights movement activist. She was Dean of Women at Indiana University and professor of mathematics and astronomy there.\n\nWells was born in Saginaw, Michigan on January 4, 1876. She had a sister Florence and a brother Ben.\n\nShe attended the Arthur Hill High School and she then spent one year at the Saginaw County Training School for Teachers and another in Dresden, Germany, where she studied the German language and music. She studied at Bryn Mawr College before transferring to the University of Michigan, where she studied mathematics and graduated in 1903. In 1916, she earned her Master of Arts degree from Carleton College in Minnesota, where her field of study was astronomy. After completing her dissertation on \"A Study of the Relative Proper Motions and Radial Velocities of Stars in the Pleiades Group\", she received her Ph.D. in astronomy from the University of Michigan in 1924.\n\nWells first worked as an educator in Crystal Falls in the Upper Peninsula of Michigan, where she was a high school principal for the 1904 to 1905 school year. She then worked at Duluth High School in Minnesota as a mathematics teacher. From 1907 to 1914, she was the head of the mathematics department. While working on her master's degree, she was an instructor at Carleton College.\n\nIn 1917, she was a faculty member and during the summers she was dean of women at the University of Michigan in Ann Arbor. At the Helen Newberry Residence, she was the social director. She then went to Indiana University and taught mathematics and was the dean of women beginning in 1919. Wells provided guidance to female students and assisted with them housing, as well as being credited with establishing the dormitory system at the school. In 1924, she became a member of the Indiana Academy of Science, and that year also began to teach astronomy courses. She retired as the dean of women in 1938, and she taught mathematics and astronomy at the university from that point until 1944. The Agnes E. Wells quadrangle at Indiana University comprises four buildings: Morrison Hall, Sycamore Hall, Memorial Hall, and Goodbody Hall, all built between 1925 and 1940.\n\nFor the American Association of University Women, she established a fellowship fund in the amount of $1 million.\n\nWells was active in many clubs and organizations. She helped found chapters of the Mortar Board for senior women at both University of Michigan and Indiana University. She was a member American Association of Deans of Women, Michigan State Society, National Education Association, Daughters of the American Revolution and Phi Beta Kappa.\n\nShe was a member of the National Woman's Party, and became its chair in 1949. The organization worked for the right for women to vote via the Nineteenth Amendment to the Constitution and for the Equal Rights Amendment, which Wells spoke about to the subcommittee on Constitutional Amendments of Congress in 1945.\n\nShe lived with a woman named Lydia Woodbridge, a teacher at Indiana University, who was identified as Well's partner in Bloomington, Indiana. Woodbridge was assistant dean of woman and a professor of French. When Wells retired as dean, Woodbridge also stepped down as assistant dean and devoted her efforts towards teaching French. Woodbridge died on July 28, 1946 in Bloomington, Indiana at the age of 70. Soon after her death, Wells wrote in a letter to Anita Pollitzer, an acquaintance in the Party, that her “friend of 41 years and house-companion for 28 years” had just died.\n\nIn her later years, she lived with her sister Florence Wells in Saginaw, Michigan. She died there on July 7, 1959. In 1971, she was inducted into the Saginaw Hall of Fame.\n\nWells, Agnes E. \"A Study of the Relative Proper Motions and Radial Velocities of Stars in the Pleiades Group\", University of Michigan, 1924.\n\n\n"}
{"id": "48207020", "url": "https://en.wikipedia.org/wiki?curid=48207020", "title": "Alexander Christison", "text": "Alexander Christison\n\nProf Alexander Christison FRSE (1753-1820) was a Scottish educator and mathematician of influence during the Scottish Enlightenment.\n\nHe was born in 1753, at Redpath House, Longformacus, Berwickshire. After a local education he began employment as the local schoolteacher before being inspired to attend the University of Edinburgh to study Classics, graduating in 1775. This background gave him access to teach at a higher calibre of school and he taught both at George Watson’s College, Dalkeith Grammar School and the High School in Edinburgh. In the 1780s he lived at Alexander's Land in the Bristo area.\n\nHe was elected a Fellow of the Royal Society of Edinburgh in 1800 his main proposer being the physician, James Gregory. The University of Edinburgh granted him a further degree (MA) in 1806, and from that date he served as Professor of Humanity at the University, a role he continued until death.\n\nAt the end of his life he is listed as living at 4 Argyll Square with his son James Christison, advocate. Argyll Square was demolished in the mid 19th century to build the Royal Scottish Museum.\n\nHe died in Edinburgh on 25 June 1820 and is buried in Greyfriars Kirkyard in the city centre. He is buried in the plot of Professor George Dunbar at the north-west section of the western extension. He is also memorialised on Robert Christison's grave at New Calton.\n\nChristison’s son was the noted Scottish toxicologist Sir Robert Christison.\n"}
{"id": "8689526", "url": "https://en.wikipedia.org/wiki?curid=8689526", "title": "Baldassarre Boncompagni", "text": "Baldassarre Boncompagni\n\nPrince Baldassarre Boncompagni-Ludovisi (10 May 1821 – 13 April 1894), was an Italian historian of mathematics and aristocrat.\n\nBoncompagni was born in Rome, into an ancient noble and wealthy Roman family, the Ludovisi-Boncompagni, as the third son of Prince Luigi Boncompagni Ludovisi and Princess Maria Maddalena Odescalchi. He studied under the mathematician Barnabas Dotterel and astronomer Ignazio Calandrelli, developing an interest in the history of science. In 1847 Pope Pius IX appointed him a member of the Accademia dei Lincei. Between 1850-1862 he produced studies on mathematicians of the Middle Ages and in 1868 founded the \"Bullettino di bibliografia e di storia delle scienze matematiche e fisiche\". After the annexation of the Papal States into the Kingdom of Italy (1870), he refused further participation in the new Academy of the Lincei, and did not accept the appointment as Senator of the Kingdom offered by Quintino Sella. He did, however, serve as a member of several other Italian and foreign academies.\n\nBoncompagni edited \"Bullettino di bibliografia e di storia delle scienze matematiche e fisiche\" (\"The bulletin of bibliography and history of mathematical and physical sciences\") (1868–1887), the first Italian periodical entirely dedicated to the history of mathematics. He edited every article that appeared in the journal. He also prepared and published the first modern edition of Fibonacci's \"Liber Abaci\".\n\n\n"}
{"id": "44807434", "url": "https://en.wikipedia.org/wiki?curid=44807434", "title": "Barbara L. Osofsky", "text": "Barbara L. Osofsky\n\nBarbara L. Osofsky (born 1937) is a retired professor of mathematics at Rutgers University. Her research concerns abstract algebra. Osofsky's contributions to mathematics include her characterization of semisimple rings in terms of properties of cyclic modules. Osofsky also established a logical equivalence between the continuum hypothesis and statements about the global dimension of associative rings.\n\nOsofsky received her Ph.D. from Rutgers University in 1964. She then worked at Rutgers until 2004, when she retired. She served as acting chair of the Rutgers mathematics department in 1978.\n\nIn 1973, Osofsky addressed a national meeting of the AMS. She was the first woman in 50 years to do so. She became the first female editor of an AMS journal in 1974 when she became the editor of Proceedings of the American Mathematical Society.\n\nFrom 2000 to 2002, Osofsky served as First Vice-President of the Mathematical Association of America. In 2005, she was awarded the MAA meritorious service award.\n\nIn 2012, Osofsky became a fellow of the American Mathematical Society.\n\n"}
{"id": "16553854", "url": "https://en.wikipedia.org/wiki?curid=16553854", "title": "Cavity quantum electrodynamics", "text": "Cavity quantum electrodynamics\n\nCavity quantum electrodynamics (cavity QED) is the study of the interaction between light confined in a reflective cavity and atoms or other particles, under conditions where the quantum nature of light photons is significant. It could in principle be used to construct a quantum computer.\n\nThe case of a single 2-level atom in the cavity is mathematically described by the Jaynes-Cummings model, and undergoes vacuum Rabi oscillations formula_1, that is between an excited atom and n-1 photons, and a ground state atom and n photons.\n\nIf the cavity is in resonance with the atomic transition, a half-cycle of oscillation starting with no photons coherently swaps the atom qubit's state onto the cavity field's, formula_2, and can be repeated to swap it back again; this could be used as a single photon source (starting with an excited atom), or as an interface between an atom or trapped ion quantum computer and optical quantum communication.\n\nOther interaction durations create entanglement between the atom and cavity field; for example, a quarter-cycle on resonance starting from formula_3 gives the maximally entangled state (a Bell state) formula_4. This can in principle be used as a quantum computer, mathematically equivalent to a trapped ion quantum computer with cavity photons replacing phonons.\n\nThe 2012 Nobel Prize for Physics was awarded to Serge Haroche and David Wineland for their work on controlling quantum systems.\n\nHaroche was born 1944 in Casablanca, Morocco, and in 1971 gained a PhD from Université Pierre et Marie Curie in Paris. He shares half of the prize for developing a new field called cavity quantum electrodynamics (CQED) – whereby the properties of an atom are controlled by placing it in an optical or microwave cavity. Haroche focused on microwave experiments and turned the technique on its head – using CQED to control the properties of individual photons. \n\nIn a series of ground-breaking experiments, Haroche used CQED to realize Schrödinger's famous cat experiment in which a system is in a superposition of two very different quantum states until a measurement is made on the system. Such states are extremely fragile, and the techniques developed to create and measure CQED states are now being applied to the development of quantum computers.\n\n"}
{"id": "26347082", "url": "https://en.wikipedia.org/wiki?curid=26347082", "title": "Chern Medal", "text": "Chern Medal\n\nThe Chern Medal is an international award recognizing outstanding lifelong achievement of the highest level in the field of mathematics. The prize is given at the International Congress of Mathematicians (ICM), which is held every four years.\n\nIt is named in honor of the late Chinese mathematician Shiing-Shen Chern. The award is a joint effort of the International Mathematical Union (IMU) and the Chern Medal Foundation (CMF) to be bestowed in the same fashion as the IMU's other three awards (the Fields Medal, the Nevanlinna Prize, and the Gauss Prize), i.e. at the opening ceremony of the International Congress of Mathematicians (ICM), which is held every four years. The first such occasion was at the 2010 ICM in Hyderabad, India.\n\nEach recipient receives a medal decorated with Chern's likeness, a cash prize of $250,000 (USD), and the opportunity to direct $250,000 of charitable donations to one or more organizations for the purpose of supporting research, education, or outreach in mathematics.\n\n\n\n"}
{"id": "2840305", "url": "https://en.wikipedia.org/wiki?curid=2840305", "title": "Computer-assisted proof", "text": "Computer-assisted proof\n\nA computer-assisted proof is a mathematical proof that has been at least partially generated by computer.\n\nMost computer-aided proofs to date have been implementations of large proofs-by-exhaustion of a mathematical theorem. The idea is to use a computer program to perform lengthy computations, and to provide a proof that the result of these computations implies the given theorem. In 1976, the four color theorem was the first major theorem to be verified using a computer program.\n\nAttempts have also been made in the area of artificial intelligence research to create smaller, explicit, new proofs of mathematical theorems from the bottom up using machine reasoning techniques such as heuristic search. Such automated theorem provers have proved a number of new results and found new proofs for known theorems. Additionally, interactive proof assistants allow mathematicians to develop human-readable proofs which are nonetheless formally verified for correctness. Since these proofs are generally human-surveyable (albeit with difficulty, as with the proof of the Robbins conjecture) they do not share the controversial implications of computer-aided proofs-by-exhaustion.\n\nOne method for using computers in mathematical proofs is by means of so-called validated numerics or rigorous numerics. This means computing numerically yet with mathematical rigour. One uses set-valued arithmetic and inclusion principle in order to ensure that the set-valued output of a numerical program encloses the solution of the original mathematical problem. This is done by controlling, enclosing and propagating round-off and truncation errors using for example interval arithmetic. More precisely, one reduces the computation to a sequence of elementary operations, say formula_1. In a computer, the result of each elementary operation is rounded off by the computer precision. However, one can construct an interval provided by upper and lower bounds on the result of an elementary operation. Then one proceeds by replacing numbers with intervals and performing elementary operations between such intervals of representable numbers.\n\nComputer-assisted proofs are the subject of some controversy in the mathematical world, with Thomas Tymoczko first to articulate objections. Those who adhere to Tymoczko's arguments believe that lengthy computer-assisted proofs are not, in some sense, 'real' mathematical proofs because they involve so many logical steps that they are not practically verifiable by human beings, and that mathematicians are effectively being asked to replace logical deduction from assumed axioms with trust in an empirical computational process, which is potentially affected by errors in the computer program, as well as defects in the runtime environment and hardware.\n\nOther mathematicians believe that lengthy computer-assisted proofs should be regarded as \"calculations\", rather than \"proofs\": the proof algorithm itself should be proved valid, so that its use can then be regarded as a mere \"verification\". Arguments that computer-assisted proofs are subject to errors in their source programs, compilers, and hardware can be resolved by providing a formal proof of correctness for the computer program (an approach which was successfully applied to the four-color theorem in 2005) as well as replicating the result using different programming languages, different compilers, and different computer hardware.\n\nAnother possible way of verifying computer-aided proofs is to generate their reasoning steps in a machine-readable form, and then use an automated theorem prover to demonstrate their correctness. This approach of using a computer program to prove another program correct does not appeal to computer proof skeptics, who see it as adding another layer of complexity without addressing the perceived need for human understanding.\n\nAnother argument against computer-aided proofs is that they lack mathematical elegance—that they provide no insights or new and useful concepts. In fact, this is an argument that could be advanced against any lengthy proof by exhaustion.\n\nAn additional philosophical issue raised by computer-aided proofs is whether they make mathematics into a quasi-empirical science, where the scientific method becomes more important than the application of pure reason in the area of abstract mathematical concepts. This directly relates to the argument within mathematics as to whether mathematics is based on ideas, or \"merely\" an exercise in formal symbol manipulation. It also raises the question whether, if according to the Platonist view, all possible mathematical objects in some sense \"already exist\", whether computer-aided mathematics is an observational science like astronomy, rather than an experimental one like physics or chemistry. This controversy within mathematics is occurring at the same time as questions are being asked in the physics community about whether twenty-first century theoretical physics is becoming too mathematical, and leaving behind its experimental roots.\n\nThe emerging field of experimental mathematics is confronting this debate head-on by focusing on numerical experiments as its main tool for mathematical exploration.\n\nIn 2010, academics at The University of Edinburgh offered people the chance to \"buy their own theorem\" created through a computer-assisted proof. This new theorem would be named after the purchaser.\n\nInclusion in this list does not imply that a formal computer-checked proof exists, but rather, that a computer program has been involved in some way. See the main articles for details.\n\n\n\n"}
{"id": "8811", "url": "https://en.wikipedia.org/wiki?curid=8811", "title": "Discrete Fourier transform", "text": "Discrete Fourier transform\n\nIn mathematics, the discrete Fourier transform (DFT) converts a finite sequence of equally-spaced samples of a function into a same-length sequence of equally-spaced samples of the discrete-time Fourier transform (DTFT), which is a complex-valued function of frequency. The interval at which the DTFT is sampled is the reciprocal of the duration of the input sequence. An inverse DFT is a Fourier series, using the DTFT samples as coefficients of complex sinusoids at the corresponding DTFT frequencies. It has the same sample-values as the original input sequence. The DFT is therefore said to be a frequency domain representation of the original input sequence. If the original sequence spans all the non-zero values of a function, its DTFT is continuous (and periodic), and the DFT provides discrete samples of one cycle. If the original sequence is one cycle of a periodic function, the DFT provides all the non-zero values of one DTFT cycle.\n\nThe DFT is the most important discrete transform, used to perform Fourier analysis in many practical applications. In digital signal processing, the function is any quantity or signal that varies over time, such as the pressure of a sound wave, a radio signal, or daily temperature readings, sampled over a finite time interval (often defined by a window function). In image processing, the samples can be the values of pixels along a row or column of a raster image. The DFT is also used to efficiently solve partial differential equations, and to perform other operations such as convolutions or multiplying large integers.\n\nSince it deals with a finite amount of data, it can be implemented in computers by numerical algorithms or even dedicated hardware. These implementations usually employ efficient fast Fourier transform (FFT) algorithms; so much so that the terms \"FFT\" and \"DFT\" are often used interchangeably. Prior to its current usage, the \"FFT\" initialism may have also been used for the ambiguous term \"finite Fourier transform\".\n\nThe \"discrete Fourier transform\" transforms a sequence of \"N\" complex numbers formula_1 into another sequence of complex numbers, formula_2 which is defined by\nwhere the last expression follows from the first one by Euler's formula.\n\nThe transform is sometimes denoted by the symbol formula_3, as in formula_4 or formula_5 or formula_6.\n\n can also be evaluated outside the domain formula_7, and that extended sequence is formula_8-periodic. Accordingly, other sequences of formula_8 indices are sometimes used, such as formula_10 (if formula_8 is even) and formula_12 (if formula_8 is odd), which amounts to swapping the left and right halves of the result of the transform.\n\nThe normalization factor multiplying the DFT and IDFT (here 1 and 1/\"N\") and the signs of the exponents are merely conventions, and differ in some treatments. The only requirements of these conventions are that the DFT and IDFT have opposite-sign exponents and that the product of their normalization factors be formula_24.  A normalization of formula_25 for both the DFT and IDFT, for instance, makes the transforms unitary. A discrete impulse, formula_15 = 1 at n = 0 and 0 otherwise; might transform to formula_18 = 1 for all k (use normalization factors 1 for DFT and 1/\"N\" for IDFT). A DC signal, formula_18 = 1 at k = 0 and 0 otherwise; might inversely transform to formula_29 for all formula_30 (use formula_24 for DFT and 1 for IDFT) which is consistent with viewing DC as the mean average of the signal.\n\nLet formula_32 and\nformula_33\n\nHere we demonstrate how to calculate the DFT of formula_34 using :\n\nformula_35\n\nformula_36\n\nformula_37\n\nformula_38\n\nformula_39\n\nThe discrete Fourier transform is an invertible, linear transformation\n\nwith formula_41 denoting the set of complex numbers. In other words, for any formula_42, an \"N\"-dimensional complex vector has a DFT and an IDFT which are in turn \"N\"-dimensional complex vectors.\n\nThe inverse transform is given by:\n\nThe vectors formula_44 \nform an orthogonal basis over the set of \"N\"-dimensional complex vectors:\n\nwhere formula_46 is the Kronecker delta. (In the last step, the summation is trivial if formula_47, where it is 1+1+⋅⋅⋅=\"N\", and otherwise is a geometric series that can be explicitly summed to obtain zero.) This orthogonality condition can be used to derive the formula for the IDFT from the definition of the DFT, and is equivalent to the unitarity property below.\n\nIf formula_18 and formula_49 are the DFTs of formula_15 and formula_51 respectively then the Parseval's theorem states:\n\nwhere the star denotes complex conjugation. Plancherel theorem is a special case of the Parseval's theorem and states:\n\nThese theorems are also equivalent to the unitary condition below.\n\nThe periodicity can be shown directly from the definition:\n\nSimilarly, it can be shown that the IDFT formula leads to a periodic extension.\n\nMultiplying formula_15 by a \"linear phase\" formula_56 for some integer \"m\" corresponds to a \"circular shift\" of the output formula_18: formula_18 is replaced by formula_59, where the subscript is interpreted modulo \"N\" (i.e., periodically). Similarly, a circular shift of the input formula_15 corresponds to multiplying the output formula_18 by a linear phase. Mathematically, if formula_62 represents the vector x then\n\nThe convolution theorem for the discrete-time Fourier transform indicates that a convolution of two infinite sequences can be obtained as the inverse transform of the product of the individual transforms. An important simplification occurs when the sequences are of finite length, formula_8. In terms of the DFT and inverse DFT, it can be written as follows:\n\nwhich is the convolution of the formula_34 sequence with a formula_69 sequence extended by periodic summation:\n\nSimilarly, the cross-correlation of  formula_34  and  formula_72  is given by:\n\nWhen either sequence contains a string of zeros, of length formula_74,  formula_75 of the circular convolution outputs are equivalent to values of  formula_76  Methods have also been developed to use this property as part of an efficient process that constructs  formula_77  with an formula_34 or formula_69 sequence potentially much longer than the practical transform size (formula_8). Two such methods are called overlap-save and overlap-add. The efficiency results from the fact that a direct evaluation of either summation (above) requires formula_81 operations for an output sequence of length formula_8.  An indirect method, using transforms, can take advantage of the formula_83 efficiency of the fast Fourier transform (FFT) to achieve much better performance. Furthermore, convolutions can be used to efficiently compute DFTs via Rader's FFT algorithm and Bluestein's FFT algorithm.\n\nIt can also be shown that:\n\nThe trigonometric interpolation polynomial\nwhere the coefficients \"X\" are given by the DFT of \"x\" above, satisfies the interpolation property formula_88 for formula_89.\n\nFor even \"N\", notice that the Nyquist component formula_90 is handled specially.\n\nThis interpolation is \"not unique\": aliasing implies that one could add \"N\" to any of the complex-sinusoid frequencies (e.g. changing formula_91 to formula_92 ) without changing the interpolation property, but giving \"different\" values in between the formula_15 points. The choice above, however, is typical because it has two useful properties. First, it consists of sinusoids whose frequencies have the smallest possible magnitudes: the interpolation is bandlimited. Second, if the formula_15 are real numbers, then formula_95 is real as well.\n\nIn contrast, the most obvious trigonometric interpolation polynomial is the one in which the frequencies range from 0 to formula_96 (instead of roughly formula_97 to formula_98 as above), similar to the inverse DFT formula. This interpolation does \"not\" minimize the slope, and is \"not\" generally real-valued for real formula_15; its use is a common mistake.\n\nAnother way of looking at the DFT is to note that in the above discussion, the DFT can be expressed as the DFT matrix, a Vandermonde matrix, \nwhere\nis a primitive Nth root of unity.\n\nThe inverse transform is then given by the inverse of the above matrix,\n\nWith unitary normalization constants formula_103, the DFT becomes a unitary transformation, defined by a unitary matrix:\n\nwhere formula_107 is the determinant function. The determinant is the product of the eigenvalues, which are always formula_108 or formula_109 as described below. In a real vector space, a unitary transformation can be thought of as simply a rigid rotation of the coordinate system, and all of the properties of a rigid rotation can be found in the unitary DFT.\n\nThe orthogonality of the DFT is now expressed as an orthonormality condition (which arises in many areas of mathematics as described in root of unity):\n\nIf X is defined as the unitary DFT of the vector x, then\n\nand the Plancherel theorem is expressed as\n\nIf we view the DFT as just a coordinate transformation which simply specifies the components of a vector in a new coordinate system, then the above is just the statement that the dot product of two vectors is preserved under a unitary DFT transformation. For the special case formula_113, this implies that the length of a vector is preserved as well—this is just Parseval's theorem,\n\nA consequence of the circular convolution theorem is that the DFT matrix diagonalizes any circulant matrix.\n\nA useful property of the DFT is that the inverse DFT can be easily expressed in terms of the (forward) DFT, via several well-known \"tricks\". (For example, in computations, it is often convenient to only implement a fast Fourier transform corresponding to one transform direction and then to get the other transform direction from the first.)\n\nFirst, we can compute the inverse DFT by reversing all but one of the inputs (Duhamel \"et al.\", 1988):\n\nSecond, one can also conjugate the inputs and outputs:\n\nThird, a variant of this conjugation trick, which is sometimes preferable because it requires no modification of the data values, involves swapping real and imaginary parts (which can be done on a computer simply by modifying pointers). Define swap(formula_15) as formula_15 with its real and imaginary parts swapped—that is, if formula_121 then swap(formula_15) is formula_123. Equivalently, swap(formula_15) equals formula_125. Then\n\nThat is, the inverse transform is the same as the forward transform with the real and imaginary parts swapped for both input and output, up to a normalization (Duhamel \"et al.\", 1988).\n\nThe conjugation trick can also be used to define a new transform, closely related to the DFT, that is involutory—that is, which is its own inverse. In particular, formula_127 is clearly its own inverse: formula_128. A closely related involutory transformation (by a factor of (1+\"i\") /) is formula_129, since the formula_130 factors in formula_131 cancel the 2. For real inputs formula_34, the real part of formula_133 is none other than the discrete Hartley transform, which is also involutory.\n\nThe eigenvalues of the DFT matrix are simple and well-known, whereas the eigenvectors are complicated, not unique, and are the subject of ongoing research.\n\nConsider the unitary form formula_134 defined above for the DFT of length \"N\", where\nThis matrix satisfies the matrix polynomial equation:\nThis can be seen from the inverse properties above: operating formula_134 twice gives the original data in reverse order, so operating formula_134 four times gives back the original data and is thus the identity matrix. This means that the eigenvalues formula_139 satisfy the equation:\nTherefore, the eigenvalues of formula_134 are the fourth roots of unity: formula_139 is +1, −1, +\"i\", or −\"i\".\n\nSince there are only four distinct eigenvalues for this formula_143 matrix, they have some multiplicity. The multiplicity gives the number of linearly independent eigenvectors corresponding to each eigenvalue. (Note that there are \"N\" independent eigenvectors; a unitary matrix is never defective.)\n\nThe problem of their multiplicity was solved by McClellan and Parks (1972), although it was later shown to have been equivalent to a problem solved by Gauss (Dickinson and Steiglitz, 1982). The multiplicity depends on the value of \"N\" modulo 4, and is given by the following table:\n\nOtherwise stated, the characteristic polynomial of formula_134 is:\n\nNo simple analytical formula for general eigenvectors is known. Moreover, the eigenvectors are not unique because any linear combination of eigenvectors for the same eigenvalue is also an eigenvector for that eigenvalue. Various researchers have proposed different choices of eigenvectors, selected to satisfy useful properties like orthogonality and to have \"simple\" forms (e.g., McClellan and Parks, 1972; Dickinson and Steiglitz, 1982; Grünbaum, 1982; Atakishiyev and Wolf, 1997; Candan \"et al.\", 2000; Hanna \"et al.\", 2004; Gurevich and Hadani, 2008).\n\nA straightforward approach is to discretize an eigenfunction of the continuous Fourier transform,\nof which the most famous is the Gaussian function.\nSince periodic summation of the function means discretizing its frequency spectrum\nand discretization means periodic summation of the spectrum,\nthe discretized and periodically summed Gaussian function yields an eigenvector of the discrete transform:\n\nThe closed form expression for the series can be expressed by\nJacobi theta functions as\n\n\nTwo other simple closed-form analytical eigenvectors for special DFT period \"N\" were found (Kong, 2008):\n\nFor DFT period \"N\" = 2\"L\" + 1 = 4\"K\" +1, where \"K\" is an integer, the following is an eigenvector of DFT:\n\nFor DFT period \"N\" = 2\"L\" = 4\"K\", where \"K\" is an integer, the following is an eigenvector of DFT:\n\nThe choice of eigenvectors of the DFT matrix has become important in recent years in order to define a discrete analogue of the fractional Fourier transform—the DFT matrix can be taken to fractional powers by exponentiating the eigenvalues (e.g., Rubio and Santhanam, 2005). For the continuous Fourier transform, the natural orthogonal eigenfunctions are the Hermite functions, so various discrete analogues of these have been employed as the eigenvectors of the DFT, such as the Kravchuk polynomials (Atakishiyev and Wolf, 1997). The \"best\" choice of eigenvectors to define a fractional discrete Fourier transform remains an open question, however.\n\nIf the random variable is constrained by\nthen \nmay be considered to represent a discrete probability mass function of , with an associated probability mass function constructed from the transformed variable,\n\nFor the case of continuous functions formula_153 and formula_154, the Heisenberg uncertainty principle states that\nwhere formula_156 and formula_157 are the variances of formula_158 and formula_159 respectively, with the equality attained in the case of a suitably normalized Gaussian distribution. Although the variances may be analogously defined for the DFT, an analogous uncertainty principle is not useful, because the uncertainty will not be shift-invariant. Still, a meaningful uncertainty principle has been introduced by Massar and Spindel.\n\nHowever, the Hirschman entropic uncertainty will have a useful analog for the case of the DFT. The Hirschman uncertainty principle is expressed in terms of the Shannon entropy of the two probability functions.\n\nIn the discrete case, the Shannon entropies are defined as\nand\nand the entropic uncertainty principle becomes\n\nThe equality is obtained for formula_163 equal to translations and modulations of a suitably normalized Kronecker comb of period formula_164 where formula_164 is any exact integer divisor of formula_8. The probability mass function formula_167 will then be proportional to a suitably translated Kronecker comb of period formula_168.\n\nThere is also a well-known deterministic uncertainty principle that uses signal sparsity (or the number of non-zero coefficients). Let formula_169 and formula_170 be the number of non-zero elements of the time and frequency sequences formula_171 and formula_172, respectively. Then, \nAs an immediate consequence of the inequality of arithmetic and geometric means, one also has formula_174. Both uncertainty principles were shown to be tight for specifically-chosen \"picket-fence\" sequences (discrete impulse trains), and find practical use for signal recovery applications.\n\n\nIt follows that for even formula_8 formula_180 and formula_181 are real-valued, and the remainder of the DFT is completely specified by just formula_182 complex numbers.\n\n\nIt is possible to shift the transform sampling in time and/or frequency domain by some real shifts \"a\" and \"b\", respectively. This is sometimes known as a generalized DFT (or GDFT), also called the shifted DFT or offset DFT, and has analogous properties to the ordinary DFT:\n\nMost often, shifts of formula_188 (half a sample) are used.\nWhile the ordinary DFT corresponds to a periodic signal in both time and frequency domains, formula_189 produces a signal that is anti-periodic in frequency domain (formula_190) and vice versa for formula_191.\nThus, the specific case of formula_192 is known as an \"odd-time odd-frequency\" discrete Fourier transform (or O DFT).\nSuch shifted transforms are most often used for symmetric data, to represent different boundary symmetries, and for real-symmetric data they correspond to different forms of the discrete cosine and sine transforms.\n\nAnother interesting choice is formula_193, which is called the centered DFT (or CDFT). The centered DFT has the useful property that, when \"N\" is a multiple of four, all four of its eigenvalues (see above) have equal multiplicities (Rubio and Santhanam, 2005)\n\nThe term GDFT is also used for the non-linear phase extensions of DFT. Hence, GDFT method provides a generalization for constant amplitude orthogonal block transforms including linear and non-linear phase types. GDFT is a framework \nto improve time and frequency domain properties of the traditional DFT, e.g. auto/cross-correlations, by the addition of the properly designed phase shaping function (non-linear, in general) to the original linear phase functions (Akansu and Agirman-Tosun, 2010).\n\nThe discrete Fourier transform can be viewed as a special case of the z-transform, evaluated on the unit circle in the complex plane; more general z-transforms correspond to \"complex\" shifts \"a\" and \"b\" above.\n\nThe ordinary DFT transforms a one-dimensional sequence or array formula_15 that is a function of exactly one discrete variable \"n\". The multidimensional DFT of a multidimensional array formula_195 that is a function of \"d\" discrete variables formula_196 for formula_197 in formula_198 is defined by:\n\nwhere formula_200 as above and the \"d\" output indices run from formula_201. This is more compactly expressed in vector notation, where we define formula_202 and formula_203 as \"d\"-dimensional vectors of indices from 0 to formula_204, which we define as formula_205:\n\nwhere the division formula_207 is defined as formula_208 to be performed element-wise, and the sum denotes the set of nested summations above.\n\nThe inverse of the multi-dimensional DFT is, analogous to the one-dimensional case, given by:\n\nAs the one-dimensional DFT expresses the input formula_15 as a superposition of sinusoids, the multidimensional DFT expresses the input as a superposition of plane waves, or multidimensional sinusoids. The direction of oscillation in space is formula_211. The amplitudes are formula_212. This decomposition is of great importance for everything from digital image processing (two-dimensional) to solving partial differential equations. The solution is broken up into plane waves.\n\nThe multidimensional DFT can be computed by the composition of a sequence of one-dimensional DFTs along each dimension. In the two-dimensional case formula_213 the formula_214 independent DFTs of the rows (i.e., along formula_215) are computed first to form a new array formula_216. Then the formula_217 independent DFTs of \"y\" along the columns (along formula_218) are computed to form the final result formula_219. Alternatively the columns can be computed first and then the rows. The order is immaterial because the nested summations above commute.\n\nAn algorithm to compute a one-dimensional DFT is thus sufficient to efficiently compute a multidimensional DFT. This approach is known as the \"row-column\" algorithm. There are also intrinsically multidimensional FFT algorithms.\n\nFor input data formula_195 consisting of real numbers, the DFT outputs have a conjugate symmetry similar to the one-dimensional case above:\n\nwhere the star again denotes complex conjugation and the formula_197-th subscript is again interpreted modulo formula_223 (for formula_224).\n\nThe DFT has seen wide usage across a large number of fields; we only sketch a few examples below (see also the references at the end). All applications of the DFT depend crucially on the availability of a fast algorithm to compute discrete Fourier transforms and their inverses, a fast Fourier transform.\n\nWhen the DFT is used for signal spectral analysis, the formula_225 sequence usually represents a finite set of uniformly spaced time-samples of some signal formula_226, where formula_227 represents time. The conversion from continuous time to samples (discrete-time) changes the underlying Fourier transform of formula_228 into a discrete-time Fourier transform (DTFT), which generally entails a type of distortion called aliasing. Choice of an appropriate sample-rate (see \"Nyquist rate\") is the key to minimizing that distortion. Similarly, the conversion from a very long (or infinite) sequence to a manageable size entails a type of distortion called \"leakage\", which is manifested as a loss of detail (a.k.a. resolution) in the DTFT. Choice of an appropriate sub-sequence length is the primary key to minimizing that effect. When the available data (and time to process it) is more than the amount needed to attain the desired frequency resolution, a standard technique is to perform multiple DFTs, for example to create a spectrogram. If the desired result is a power spectrum and noise or randomness is present in the data, averaging the magnitude components of the multiple DFTs is a useful procedure to reduce the variance of the spectrum (also called a periodogram in this context); two examples of such techniques are the Welch method and the Bartlett method; the general subject of estimating the power spectrum of a noisy signal is called spectral estimation.\n\nA final source of distortion (or perhaps \"illusion\") is the DFT itself, because it is just a discrete sampling of the DTFT, which is a function of a continuous frequency domain. That can be mitigated by increasing the resolution of the DFT. That procedure is illustrated at Sampling the DTFT.\n\nSee FFT filter banks and Sampling the DTFT.\n\nThe field of digital signal processing relies heavily on operations in the frequency domain (i.e. on the Fourier transform). For example, several lossy image and sound compression methods employ the discrete Fourier transform: the signal is cut into short segments, each is transformed, and then the Fourier coefficients of high frequencies, which are assumed to be unnoticeable, are discarded. The decompressor computes the inverse transform based on this reduced number of Fourier coefficients. (Compression applications often use a specialized form of the DFT, the discrete cosine transform or sometimes the modified discrete cosine transform.)\nSome relatively recent compression algorithms, however, use wavelet transforms, which give a more uniform compromise between time and frequency domain than obtained by chopping data into segments and transforming each segment. In the case of JPEG2000, this avoids the spurious image features that appear when images are highly compressed with the original JPEG.\n\nDiscrete Fourier transforms are often used to solve partial differential equations, where again the DFT is used as an approximation for the Fourier series (which is recovered in the limit of infinite \"N\"). The advantage of this approach is that it expands the signal in complex exponentials formula_229, which are eigenfunctions of differentiation: formula_230. Thus, in the Fourier representation, differentiation is simple—we just multiply by formula_231. (Note, however, that the choice of formula_30 is not unique due to aliasing; for the method to be convergent, a choice similar to that in the trigonometric interpolation section above should be used.) A linear differential equation with constant coefficients is transformed into an easily solvable algebraic equation. One then uses the inverse DFT to transform the result back into the ordinary spatial representation. Such an approach is called a spectral method.\n\nSuppose we wish to compute the polynomial product \"c\"(\"x\") = \"a\"(\"x\") · \"b\"(\"x\"). The ordinary product expression for the coefficients of \"c\" involves a linear (acyclic) convolution, where indices do not \"wrap around.\" This can be rewritten as a cyclic convolution by taking the coefficient vectors for \"a\"(\"x\") and \"b\"(\"x\") with constant term first, then appending zeros so that the resultant coefficient vectors a and b have dimension \"d\" > deg(\"a\"(\"x\")) + deg(\"b\"(\"x\")). Then,\n\nWhere c is the vector of coefficients for \"c\"(\"x\"), and the convolution operator formula_234 is defined so\n\nBut convolution becomes multiplication under the DFT:\n\nHere the vector product is taken elementwise. Thus the coefficients of the product polynomial \"c\"(\"x\") are just the terms 0, ..., deg(\"a\"(\"x\")) + deg(\"b\"(\"x\")) of the coefficient vector\n\nWith a fast Fourier transform, the resulting algorithm takes O (\"N\" log \"N\") arithmetic operations. Due to its simplicity and speed, the Cooley–Tukey FFT algorithm, which is limited to composite sizes, is often chosen for the transform operation. In this case, \"d\" should be chosen as the smallest integer greater than the sum of the input polynomial degrees that is factorizable into small prime factors (e.g. 2, 3, and 5, depending upon the FFT implementation).\n\nThe fastest known algorithms for the multiplication of very large integers use the polynomial multiplication method outlined above. Integers can be treated as the value of a polynomial evaluated specifically at the number base, with the coefficients of the polynomial corresponding to the digits in that base. After polynomial multiplication, a relatively low-complexity carry-propagation step completes the multiplication.\n\nWhen data is convolved with a function with wide support, such as for downsampling by a large sampling ratio, because of the Convolution theorem and the FFT algorithm, it may be faster to transform it, multiply pointwise by the transform of the filter and then reverse transform it. Alternatively, a good filter is obtained by simply truncating the transformed data and re-transforming the shortened data set.\n\nThe DFT can be interpreted as the complex-valued representation theory of the finite cyclic group. In other words, a sequence of formula_30 complex numbers can be thought of as an element of formula_30-dimensional complex space formula_240 or equivalently a function formula_241 from the finite cyclic group of order formula_30 to the complex numbers, formula_243. So formula_241 is a class function on the finite cyclic group, and thus can be expressed as a linear combination of the irreducible characters of this group, which are the roots of unity.\n\nFrom this point of view, one may generalize the DFT to representation theory generally, or more narrowly to the representation theory of finite groups.\n\nMore narrowly still, one may generalize the DFT by either changing the target (taking values in a field other than the complex numbers), or the domain (a group other than a finite cyclic group), as detailed in the sequel.\n\nMany of the properties of the DFT only depend on the fact that formula_245 is a primitive root of unity, sometimes denoted formula_246 or formula_247 (so that formula_248). Such properties include the completeness, orthogonality, Plancherel/Parseval, periodicity, shift, convolution, and unitarity properties above, as well as many FFT algorithms. For this reason, the discrete Fourier transform can be defined by using roots of unity in fields other than the complex numbers, and such generalizations are commonly called \"number-theoretic transforms\" (NTTs) in the case of finite fields. For more information, see number-theoretic transform and discrete Fourier transform (general).\n\nThe standard DFT acts on a sequence \"x\", \"x\", …, \"x\" of complex numbers, which can be viewed as a function {0, 1, …, \"N\" − 1} → C. The multidimensional DFT acts on multidimensional sequences, which can be viewed as functions\nThis suggests the generalization to Fourier transforms on arbitrary finite groups, which act on functions \"G\" → C where \"G\" is a finite group. In this framework, the standard DFT is seen as the Fourier transform on a cyclic group, while the multidimensional DFT is a Fourier transform on a direct sum of cyclic groups.\n\nFurther, Fourier transform can be on cosets of a group.\n\n There are various alternatives to the DFT for various applications, prominent among which are wavelets. The analog of the DFT is the discrete wavelet transform (DWT). From the point of view of time–frequency analysis, a key limitation of the Fourier transform is that it does not include \"location\" information, only \"frequency\" information, and thus has difficulty in representing transients. As wavelets have location as well as frequency, they are better able to represent location, at the expense of greater difficulty representing frequency. For details, see comparison of the discrete wavelet transform with the discrete Fourier transform.\n\n\n\n"}
{"id": "26599926", "url": "https://en.wikipedia.org/wiki?curid=26599926", "title": "Duality theory for distributive lattices", "text": "Duality theory for distributive lattices\n\nIn mathematics, duality theory for distributive lattices provides three different (but closely related) representations of bounded distributive lattices via Priestley spaces, spectral spaces, and pairwise Stone spaces. This generalizes the well-known Stone duality between Stone spaces and Boolean algebras.\n\nLet be a bounded distributive lattice, and let denote the set of prime filters of . For each , let . Then is a spectral space, where the topology on is generated by }. The spectral space is called the \"prime spectrum\" of .\n\nThe map is a lattice isomorphism from onto the lattice of all compact open subsets of . In fact, each spectral space is homeomorphic to the prime spectrum of some bounded distributive lattice.\n\nSimilarly, if and denotes the topology generated by }, then is also a spectral space. Moreover, is a pairwise Stone space. The pairwise Stone space is called the \"bitopological dual\" of . Each pairwise Stone space is bi-homeomorphic to the bitopological dual of some bounded distributive lattice.\n\nFinally, let be set-theoretic inclusion on the set of prime filters of and let . Then is a Priestley space. Moreover, is a lattice isomorphism from onto the lattice of all clopen up-sets of . The Priestley space is called the \"Priestley dual\" of . Each Priestley space is isomorphic to the Priestley dual of some bounded distributive lattice.\n\nLet Dist denote the category of bounded distributive lattices and bounded lattice homomorphisms. Then the above three representations of bounded distributive lattices can be extended to dual equivalence between Dist and the categories Spec, PStone, and Pries of spectral spaces with spectral maps, of pairwise Stone spaces with bi-continuous maps, and of Priestley spaces with Priestley morphisms, respectively:\n\nThus, there are three equivalent ways of representing bounded distributive lattices. Each one has its own motivation and advantages, but ultimately they all serve the same purpose of providing better understanding of bounded distributive lattices.\n\n\n"}
{"id": "1077330", "url": "https://en.wikipedia.org/wiki?curid=1077330", "title": "E. T. Whittaker", "text": "E. T. Whittaker\n\nEdmund Taylor Whittaker FRS FRSE (24 October 1873 – 24 March 1956) was a British mathematician who contributed widely to applied mathematics, mathematical physics, and the theory of special functions. He had a particular interest in numerical analysis, but also worked on celestial mechanics and the history of physics. Near the end of his career he received the Copley Medal, the most prestigious honorary award in British science. The School of Mathematics of the University of Edinburgh holds The Whittaker Colloquium, a yearly lecture in his honour.\n\nWhittaker was born in Southport, in Lancashire. He was educated at Manchester Grammar School and Trinity College, Cambridge from 1892. He graduated as Second Wrangler in the examination in 1895 and also received the Tyson Medal for Mathematics and Astronomy. In 1896, Whittaker was elected as a Fellow of Trinity College, Cambridge, and remained at Cambridge as a teacher until 1906. Between 1906 and 1911, he was the Royal Astronomer of Ireland and professor of astronomy at Trinity College Dublin where he taught mathematical physics. In 1911, Whittaker became professor at the University of Edinburgh and remained there for the rest of his career.\n\nShortly after coming to Edinburgh, Whittaker established a Mathematical Laboratory, the first enterprise of this kind in Great Britain, and probably one of the first attempts to make numerical analysis an integral part of the university curriculum. It is probable that in starting this new venture, Whittaker was influenced by his work in astronomy, and by his friendship with the great actuaries of the period. To the knowledge of the present writer, he regarded the introduction of the Mathematical Laboratory course as his most notable contribution to mathematical education. He and his associates, among whom the most outstanding was A. C. Aitken, researched on curve fitting, numerical solution of integral equations, and similar topics. His thorough knowledge of the history and practice of numerical analysis found expression in his \"Calculus of Observations\" (written in collaboration with G. Robinson) which today, more than thirty years after its appearance, and after two revolutions in numerical computing, is still one of the most important works on the subject.\n\nA classmate at Manchester Grammar School, Ernest Barker, with whom Edmund shared the office of prefect, later recalled his personality:\n\nWhittaker was a Christian and became a convert to the Roman Catholic Church (1930). In relation to that he was a member of the Pontifical Academy of Sciences from 1936 onward and was president of a Newman Society. Earlier at Cambridge in 1901 he married the daughter of a Presbyterian minister. They had five children, including the mathematician John Macnaghten Whittaker (1905-1984). His elder daughter, Beatrice, married E.T. Copson, who would later become Professor of Mathematics at St. Andrews University.\n\nWhittaker wrote the biography of a famous Italian mathematician, Vito Volterra for the Royal Society in 1941. In 1954, he was awarded the Copley Medal by the Royal Society, their highest award, \"for his distinguished contributions to both pure and applied mathematics and to theoretical physics\". Back in 1931 Whittaker had received the Royal Society's Sylvester Medal \"for his original contributions to both pure and applied mathematics\". Whittaker died in Edinburgh, Scotland.\n\nWhittaker is remembered as the author of the book \"A Course of Modern Analysis\", first published in 1902. The book's later editions were in collaboration with George Neville Watson, and so the book became known as \"Whittaker & Watson\". The book is one of the handful of mathematics texts of its era that was considered indispensable. It has remained in print continuously for over a century.\n\nWhittaker is the eponym of the Whittaker function or Whittaker integral, in the theory of confluent hypergeometric functions. This makes him also the eponym of the Whittaker model in the local theory of automorphic representations. He published also on algebraic functions and automorphic functions. He gave expressions for the Bessel functions as integrals involving Legendre functions.\n\nIn the theory of partial differential equations, Whittaker developed a general solution of the Laplace equation in three dimensions and the solution of the wave equation. He developed the electrical potential field as a directional flow of energy (sometimes referred to as alternating currents). Whittaker's pair of papers in 1903 and 1904 indicated that any potential can be analysed by a Fourier-like series of waves, such as a planet's gravitational field point-charge. The superpositions of inward and outward wave pairs produce the \"static\" fields (or scalar potential). These were harmonically-related. By this conception, the structure of electric potential is created from two opposite, though balanced, parts. Whittaker suggested that gravity possessed a wavelike \"undulatory\" character.\n\nIn 1910, Whittaker wrote \"A History of the Theories of Aether and Electricity\", which gave a very detailed account of the aether theories from René Descartes to Hendrik Lorentz and Albert Einstein, including the contributions of Hermann Minkowski, and which made Whittaker a respected historian of science.\n\nIn 1951 (Vol. 1) and 1953 (Vol. 2), he published an extended and revised edition of his book in two volumes. The second volume contains some interesting historical remarks. For example, it contains a chapter named \"The Relativity Theory of Poincaré and Lorentz\", where Whittaker credited Henri Poincaré and Lorentz for developing special relativity, and especially alluded to Lorentz's 1904 paper (dated by Whittaker as 1903), Poincaré's St. Louis speech () of September 1904, and Poincaré's June 1905 paper. He attributed to Einstein's special relativity paper only little importance, which he said \"set forth the relativity theory of Poincaré and Lorentz with some amplifications, and which attracted much attention”, and he credited Einstein only with being the first to publish the correct relativistic formulas for relativistic aberration and the Doppler effect. He also attributed the formula formula_1 to Poincaré. In 1984 Clifford Truesdell wrote that Whittaker \"aroused colossal antagonism by trying to set the record straight on the basis of print and record rather than recollection and folklore and professional propaganda...\" On the other hand, Abraham Pais wrote that \"Whittaker's treatment of special relativity shows how well the author's lack of physical insight matches his ignorance of the literature\". According to Roberto Torretti, \"Whittaker's views on the origin of special relativity have been rejected by the great majority of scholars\", and he cites Max Born (1956), Gerald Holton (1960,1964), Schribner (1964), Goldberg (1967), Zahar (1973), Hirosige (1976), Schaffner (1976), and Arthur I. Miller (1981).\n\nNevertheless, Whittaker gave a concise statement of relativity of simultaneity by referring to conjugate diameters of hyperbolas. He wrote, \"[the] hyperbola is unaltered when any pair of conjugate diameters are taken as new axes, and a new unit of length is taken proportional to the length of either of these diameters.\" His observation means that the simultaneous events for a worldline are hyperbolic-orthogonal to the worldline.\n\n\n"}
{"id": "24662110", "url": "https://en.wikipedia.org/wiki?curid=24662110", "title": "Effective Polish space", "text": "Effective Polish space\n\nIn mathematical logic, an effective Polish space is a complete separable metric space that has a computable presentation. Such spaces are studied in effective descriptive set theory and in constructive analysis. In particular, standard examples of Polish spaces such as the real line, the Cantor set and the Baire space are all effective Polish spaces.\n\nAn effective Polish space is a complete separable metric space \"X\" with metric \"d\" such that there is a countable dense set \"C\" = (\"c\", \"c\"...) that makes the following two relations on formula_1 computable (Moschovakis 2009:96-7):\n\n"}
{"id": "23030862", "url": "https://en.wikipedia.org/wiki?curid=23030862", "title": "Electrical drawing", "text": "Electrical drawing\n\nAn electrical drawing, is a type of technical drawing that shows information about power, lighting, and communication for an engineering or architectural project. Any electrical working drawing consists of \"lines, symbols, dimensions, and notations to accurately convey an engineering's design to the workers, who install the electrical system on the job\". \n\nA complete set of working drawings for the average electrical system in large projects usually consists of:\n\nElectrical drafters prepare wiring and layout diagrams used by workers who erect, install, and repair electrical equipment and wiring in communication centers, power plants, electrical distribution systems, and buildings.\n\n"}
{"id": "374404", "url": "https://en.wikipedia.org/wiki?curid=374404", "title": "Emanuel Sperner", "text": "Emanuel Sperner\n\nEmanuel Sperner (9 December 1905 – 31 January 1980) was a German mathematician, best known for two theorems. He was born in Waltdorf (near Neiße, Upper Silesia, now Nysa, Poland), and died in Sulzburg-Laufen, West Germany. He was a student at Carolinum in Nysa and then Hamburg University where his advisor was Wilhelm Blaschke. He was appointed Professor in Königsberg in 1934, and subsequently held posts in a number of universities until 1974. \n\nSperner's theorem, from 1928, says that the size of an antichain in the power set of an \"n\"-set (a Sperner family) is at most the middle binomial coefficient(s). It has several proofs and numerous generalizations, including the Sperner property of a partially ordered set.\n\nSperner's lemma, from 1928, states that every Sperner coloring of a triangulation of an \"n\"-dimensional simplex contains a cell colored with a complete set of colors. It was proven by Sperner to provide an alternate proof of a theorem of Lebesgue characterizing dimensionality of Euclidean spaces. It was later noticed that this lemma provides a direct proof of the Brouwer fixed-point theorem without explicit use of homology.\n\nSperner's students included Kurt Leichtweiss and Gerhard Ringel.\n\n"}
{"id": "181289", "url": "https://en.wikipedia.org/wiki?curid=181289", "title": "Empty product", "text": "Empty product\n\nIn mathematics, an empty product, or nullary product, is the result of multiplying no factors. It is by convention equal to the multiplicative identity 1 (assuming there is an identity for the multiplication operation in question), just as the empty sum—the result of adding no numbers—is by convention zero, or the additive identity.\n\nThe term \"empty product\" is most often used in the above sense when discussing arithmetic operations. However, the term is sometimes employed when discussing set-theoretic intersections, categorical products, and products in computer programming; these are discussed below.\n\nLet \"a\", \"a\", \"a\", … be a sequence of numbers, and let\n\nbe the product of the first \"m\" elements of the sequence. Then\n\nfor all \"m\" = 1, 2, … provided that we use the following conventions: formula_3 and formula_4. In other words, a \"product\" formula_5 with only one factor evaluates to that factor, while a \"product\" formula_6 with no factors at all evaluates to 1. Allowing a \"product\" with only one or zero factors reduces the number of cases to be considered in many mathematical formulas. Such \"products\" are natural starting points in induction proofs, as well as in algorithms. For these reasons, the \"empty product is one\" convention is common practice in mathematics and computer programming.\n\nThe notion of an empty product is useful for the same reason that the number zero and the empty set are useful: while they seem to represent quite uninteresting notions, their existence allows for a much shorter mathematical presentation of many subjects.\n\nFor example, the empty products 0! = 1 and \"x\" = 1 shorten Taylor series notation (see zero to the power of zero for a discussion when \"x\" = 0). Likewise, if \"M\" is an \"n\" × \"n\" matrix then \"M\" is the \"n\" × \"n\" identity matrix, reflecting the fact that applying a linear map zero times has the same effect as applying the identity map.\n\nAs another example, the fundamental theorem of arithmetic says that every positive integer can be written uniquely as a product of primes. However, if we do not allow products with only 0 or 1 factors, then the theorem (and its proof) become longer.\n\nMore examples of the use of the empty product in mathematics may be found in the binomial theorem (which assumes and implies that \"x\" = 1 for all \"x\"), Stirling number, König's theorem, binomial type, binomial series, difference operator and Pochhammer symbol.\n\nSince logarithms turn products into sums, they should map an empty product to an empty sum. So if we define the empty product to be 1, then the empty sum should be formula_7. Conversely, the exponential function turns sums into products, so if we define the empty sum to be 0, then the empty product should be formula_8.\n\nConsider the general definition of the Cartesian product:\n\nIf \"I\" is empty, the only such \"g\" is the empty function formula_11, which is the unique subset of formula_12 that is a function formula_13, namely the empty subset formula_14 (the only subset that formula_15 has):\n\nThus, the cardinality of the Cartesian product of no sets is 1.\n\nUnder the perhaps more familiar \"n\"-tuple interpretation,\n\nthat is, the singleton set containing the empty tuple. Note that in both representations the empty product has cardinality 1.\n\nIn any category, the product of an empty family is a terminal object of that category. This can be demonstrated by using the limit definition of the product. An \"n\"-fold categorical product can be defined as the limit with respect to a diagram given by the discrete category with \"n\" objects. An empty product is then given by the limit with respect to the empty category, which is the terminal object of the category if it exists. This definition specializes to give results as above. For example, in the category of sets the categorical product is the usual Cartesian product, and the terminal object is a singleton set. In the category of groups the categorical product is the Cartesian product of groups, and the terminal object is a trivial group with one element. To obtain the usual arithmetic definition of the empty product we must take the decategorification of the empty product in the category of finite sets.\n\nDually, the coproduct of an empty family is an initial object.\nNullary categorical products or coproducts may not exist in a given category; e.g. in the category of fields, neither exists.\n\nClassical logic defines the operation of conjunction, which is generalized to universal quantification in and predicate calculus, and is widely known as logical multiplication because we intuitively identify true with 1 and false with 0 and our conjunction behaves as ordinary multiplier. Multipliers can have arbitrary number of inputs. In case of 0 inputs, we have empty conjunction, which is identically equal to true.\n\nThis is related to another concept in logic, vacuous truth, which tells us that empty set of objects can have any property. It can be explained the way that the conjunction (as part of logic in general) deals with values less or equal 1. This means that longer is the conjunction, the higher is probability to end up with 0. Conjunction merely checks the propositions and returns 0 (or false) as soon as one of propositions evaluates to false. Reducing the number of conjoined propositions increases the chance to pass the check and stay with 1. Particularly, if there are 0 tests or members to check, none can fail so, by default, we must always succeed regardless of which propositions or member properties had to be tested.\n\nMany programming languages, such as Python, allow the direct expression of lists of numbers, and even functions that allow an arbitrary number of parameters. If such a language has a function that returns the product of all the numbers in a list, it usually works like this:\n\nThis convention helps avoid having to code special cases like \"if length of list is 1\" or \"if length of list is zero\" as special cases.\n\nMultiplication is an infix operator and therefore a binary operator, complicating the notation of an empty product. Some programming languages handle this by implementing variadic functions. For example, the fully parenthesized prefix notation of Lisp languages gives rise to a natural notation for nullary functions:\n\n\n"}
{"id": "3435716", "url": "https://en.wikipedia.org/wiki?curid=3435716", "title": "Equiconsistency", "text": "Equiconsistency\n\nIn mathematical logic, two theories are equiconsistent if the consistency of one theory implies the consistency of the other theory, and vice versa. In this case, they are, roughly speaking, \"as consistent as each other\".\n\nIn general, it is not possible to prove the absolute consistency of a theory \"T\". Instead we usually take a theory \"S\", believed to be consistent, and try to prove the weaker statement that if \"S\" is consistent then \"T\" must also be consistent—if we can do this we say that \"T\" is \"consistent relative to S\". If \"S\" is also consistent relative to \"T\" then we say that \"S\" and \"T\" are equiconsistent.\n\nIn mathematical logic, formal theories are studied as mathematical objects. Since some theories are powerful enough to model different mathematical objects, it is natural to wonder about their own consistency.\n\nHilbert proposed a program at the beginning of the 20th century whose ultimate goal was to show, using mathematical methods, the consistency of mathematics. Since most mathematical disciplines can be reduced to arithmetic, the program quickly became the establishment of the consistency of arithmetic by methods formalizable within arithmetic itself. \n\nGödel's incompleteness theorems show that Hilbert's program cannot be realized: if a consistent recursively enumerable theory is strong enough to formalize its own metamathematics (whether something is a proof or not), i.e. strong enough to model a weak fragment of arithmetic (Robinson arithmetic suffices), then the theory cannot prove its own consistency. There are some technical caveats as to what requirements the formal statement representing the metamathematical statement \"The theory is consistent\" needs to satisfy, but the outcome is that if a (sufficiently strong) theory can prove its own consistency then either there is no computable way of identifying whether a statement is even an axiom of the theory or not, or else the theory itself is inconsistent (in which case it can prove anything, including false statements such as its own consistency).\n\nGiven this, instead of outright consistency, one usually considers relative consistency: Let \"S\" and \"T\" be formal theories. Assume that \"S\" is a consistent theory. Does it follow that \"T\" is consistent? If so, then \"T is consistent relative to S\". Two theories are equiconsistent if each one is consistent relative to the other.\n\nIf \"T\" is consistent relative to \"S\", but \"S\" is not known to be consistent relative to \"T\", then we say that \"S\" has greater consistency strength than \"T\". When discussing these issues of consistency strength the metatheory in which the discussion takes places needs to be carefully addressed. For theories at the level of second-order arithmetic, the reverse mathematics program has much to say. Consistency strength issues are a usual part of set theory, since this is a recursive theory that can certainly model most of mathematics. The most widely used set of axioms of set theory is called ZFC. When a set-theoretic statement is said to be equiconsistent to another , what is being claimed is that in the metatheory (Peano Arithmetic in this case) it can be proven that the theories ZFC+ and ZFC+ are equiconsistent. Usually, primitive recursive arithmetic can be adopted as the metatheory in question, but even if the metatheory is ZFC or an extension of it, the notion is meaningful. The method of forcing allows one to show that the theories ZFC, ZFC+CH and ZFC+¬CH are all equiconsistent (where CH denotes the continuum hypothesis).\n\nWhen discussing fragments of ZFC or their extensions (for example, ZF, set theory without the axiom of choice, or ZF+AD, set theory with the axiom of determinacy), the notions described above are adapted accordingly. Thus, ZF is equiconsistent with ZFC, as shown by Gödel.\n\nThe consistency strength of numerous combinatorial statements can be calibrated by large cardinals. For example, the negation of Kurepa's hypothesis is equiconsistent with an inaccessible cardinal, the non-existence of special formula_1-Aronszajn trees is equiconsistent with a Mahlo cardinal, and the non-existence of formula_1-Aronszajn trees is equiconsistent with a weakly compact cardinal.\n\n\n"}
{"id": "13378267", "url": "https://en.wikipedia.org/wiki?curid=13378267", "title": "European Journal of Combinatorics", "text": "European Journal of Combinatorics\n\nThe European Journal of Combinatorics is a peer-reviewed scientific journal for combinatorics. It is an international, bimonthly journal of discrete mathematics, specializing in theories arising from combinatorial problems. The journal is primarily open to papers dealing with mathematical structures within combinatorics and/or establishing direct links between combinatorics and the theories of computing. The journal includes full-length research papers, short notes, and research problems on several topics.\nThis journal has been founded by Michel Deza, Michel Las Vergnas and Pierre Rosenstiehl.\nThe current editors-in-chief are Patrice Ossona de Mendez and Pierre Rosenstiehl.\n\nThe impact factor for the \"European Journal of Combinatorics\" in 2012 was 0.658.\n\n"}
{"id": "3313244", "url": "https://en.wikipedia.org/wiki?curid=3313244", "title": "Exponential factorial", "text": "Exponential factorial\n\nThe exponential factorial of a positive integer \"n\", denoted by \"n\"$, is \"n\" raised to the power of \"n\" − 1, which in turn is raised to the power of \"n\" − 2, and so on and so forth, that is,\n\nThe exponential factorial can also be defined with the recurrence relation\n\nThe first few exponential factorials are 1, 1, 2, 9, 262144, etc. . So, for example, 262144 is an exponential factorial since\n\nThe exponential factorials grow much more quickly than regular factorials or even hyperfactorials. The exponential factorial of 5 is 5 which is approximately 6.206069878660874 × 10.\n\nThe sum of the reciprocals of the exponential factorials from 1 onwards is the transcendental number 1.6111149258083767361111... .\n\nLike tetration, there is currently no accepted method of extension of the exponential factorial function to real and complex values of its argument, unlike the factorial function, for which such an extension is provided by the gamma function.\n\n"}
{"id": "1797141", "url": "https://en.wikipedia.org/wiki?curid=1797141", "title": "FOIL method", "text": "FOIL method\n\nIn elementary algebra, FOIL is a mnemonic for the standard method of multiplying two binomials—hence the method may be referred to as the FOIL method. The word \"FOIL\" is an acronym for the four terms of the product:\nThe general form is:\nNote that is both a \"first\" term and an \"outer\" term; is both a \"last\" and \"inner\" term, and so forth. The order of the four terms in the sum is not important, and need not match the order of the letters in the word FOIL.\n\nThe FOIL method is a special case of a more general method for multiplying algebraic expressions using the distributive law. The word \"FOIL\" was originally intended solely as a mnemonic for high-school students learning algebra. The term appears in William Betz's 1929 text, \"Algebra for Today\", where he states:\n... first terms, outer terms, inner terms, last terms. (The rule stated above may also be remembered by the word FOIL, suggested by the first letters of the words first, outer, inner, last.)\nWilliam Betz was active in the mathematics reform movement in the United States at that time, had written many texts on elementary mathematics topics and had \"devoted his life to the improvement of mathematics education.\" \n\nMany students and educators in the United States now use the word \"FOIL\" as a verb meaning \"to expand the product of two binomials\". This neologism has not gained widespread acceptance in the mathematical community.\n\nThe method is most commonly used to multiply linear binomials. For example,\nIf either binomial involves subtraction, the corresponding terms must be negated. For example,\n\nThe FOIL method is equivalent to a two-step process involving the distributive law:\nIn the first step, the () is distributed over the addition in first binomial. In the second step, the distributive law is used to simplify each of the two terms. Note that this process involves a total of three applications of the distributive property. In contrast to the FOIL method, the method using distributive can be applied easily to products with more terms such as trinomials and higher.\n\nThe FOIL rule converts a product of two binomials into a sum of four (or fewer, if like terms are then combined) monomials. The reverse process is called \"factoring\" or \"factorization\". In particular, if the proof above is read in reverse it illustrates the technique called factoring by grouping.\n\nA visual memory tool can replace the FOIL mnemonic for a pair of polynomials with any number of terms. Make a table with the terms of the first polynomial on the left edge and the terms of the second on the top edge, then fill in the table with products. The table equivalent to the FOIL rule looks like this.\nIn the case that these are polynomials, , the terms of a given degree are found by adding along the antidiagonals\nso formula_7\n\nTo multiply , the table would be as follows.\nThe sum of the table entries is the product of the polynomials. Thus\nSimilarly, to multiply , one writes the same table\nand sums along antidiagonals:\n\nThe FOIL rule cannot be directly applied to expanding products with more than two multiplicands, or multiplicands with more than two summands. However, applying the associative law and recursive foiling allows one to expand such products. For instance,\nAlternate methods based on distributing forgo the use of the FOIL rule, but may be easier to remember and apply. For example,\n\n"}
{"id": "23822989", "url": "https://en.wikipedia.org/wiki?curid=23822989", "title": "General Dirichlet series", "text": "General Dirichlet series\n\nIn the field of mathematical analysis, a general Dirichlet series is an infinite series that takes the form of\n\nwhere formula_2, formula_3 are complex numbers and formula_4 is a strictly increasing sequence of nonnegative real numbers that tends to infinity.\n\nA simple observation shows that an 'ordinary' Dirichlet series\n\nis obtained by substituting formula_6 while a power series\n\nis obtained when formula_8.\n\nIf a Dirichlet series is convergent at formula_9, then it is uniformly convergent in the domain\n\nand convergent for any formula_11 where formula_12.\n\nThere are now three possibilities regarding the convergence of a Dirichlet series, i.e. it may converge for all, for none or for some values of \"s\". In the latter case, there exist a formula_13 such that the series is convergent for formula_14 and divergent for formula_15. By convention, formula_16 if the series converges nowhere and formula_17 if the series converges everywhere on the complex plane.\n\nThe abscissa of convergence of a Dirichlet series can be defined as formula_13 above. Another equivalent definition is\n\nThe line formula_20 is called the line of convergence. The half-plane of convergence is defined as\n\nThe abscissa, line and half-plane of convergence of a Dirichlet series are analogous to radius, boundary and disk of convergence of a power series.\n\nOn the line of convergence, the question of convergence remains open as in the case of power series. However, if a Dirichlet series converges and diverges at different points on the same vertical line, then this line must be the line of convergence. The proof is implicit in the definition of abscissa of convergence. An example would be the series\n\nwhich converges at formula_23 (alternating harmonic series) and diverges at formula_24 (harmonic series). Thus, formula_25 is the line of convergence.\n\nSuppose that a Dirichlet series does not converge at formula_24, then it is clear that formula_27 and formula_28 diverges. On the other hand, if a Dirichlet series converges at formula_24, then formula_30 and formula_28 converges. Thus, there are two formulas to compute formula_13, depending on the convergence of formula_28 which can be determined by various convergence tests. These formulas are similar to the Cauchy–Hadamard theorem for the radius of convergence of a power series.\n\nIf formula_34 is divergent, i.e. formula_27, then formula_13 is given by\n\nIf formula_34 is convergent, i.e. formula_30, then formula_13 is given by\n\nA Dirichlet series is absolutely convergent if the series\n\nis convergent. As usual, an absolutely convergent Dirichlet series is convergent, but the converse is not always true.\n\nIf a Dirichlet series is absolutely convergent at formula_43, then it is absolutely convergent for all \"s\" where formula_44. A Dirichlet series may converge absolutely for all, for no or for some values of \"s\". In the latter case, there exist a formula_45 such that the series converges absolutely for formula_46 and converges non-absolutely for formula_47.\n\nThe abscissa of absolute convergence can be defined as formula_45 above, or equivalently as\n\nThe line and half-plane of absolute convergence can be defined similarly. There are also two formulas to compute formula_45.\n\nIf formula_51 is divergent, then formula_45 is given by\n\nIf formula_51 is convergent, then formula_45 is given by\n\nIn general, the abscissa of convergence does not coincide with abscissa of absolute convergence. Thus, there might be a strip between the line of convergence and absolute convergence where a Dirichlet series is conditionally convergent. The width of this strip is given by\n\nIn the case where \"L\" = 0, then\n\nAll the formulas provided so far still hold true for 'ordinary' Dirichlet series by substituting formula_59.\n\nA function represented by a Dirichlet series\n\nis analytic on the half-plane of convergence. Moreover, for formula_61\n\nA Dirichlet series can be further generalized to the multi-variable case where formula_63, \"k\" = 2, 3, 4..., or complex variable case where formula_64, \"m\" = 1, 2, 3...\n\n\n"}
{"id": "1146267", "url": "https://en.wikipedia.org/wiki?curid=1146267", "title": "Geometric quantization", "text": "Geometric quantization\n\nIn mathematical physics, geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization, for which there is in general no exact recipe, in such a way that certain analogies between the classical theory and the quantum theory remain manifest. For example, the similarity between the Heisenberg equation in the Heisenberg picture of quantum mechanics and the Hamilton equation in classical physics should be built in.\n\nOne of the earliest attempts at a natural quantization was Weyl quantization, proposed by Hermann Weyl in 1927. Here, an attempt is made to associate a quantum-mechanical observable (a self-adjoint operator on a Hilbert space) with a real-valued function on classical phase space. The position and momentum in this phase space are mapped to the generators of the Heisenberg group, and the Hilbert space appears as a group representation of the Heisenberg group. In 1946, H. J. Groenewold considered the product of a pair of such observables and asked what the corresponding function would be on the classical phase space. This led him to discover the phase-space star-product of a pair of functions.\n\nThe modern theory of geometric quantization was developed by Bertram Kostant and Jean-Marie Souriau in the 1970s. One of the motivations of the theory was to understand and generalize Kirillov's orbit method in representation theory.\n\nMore generally, this technique leads to deformation quantization, where the ★-product is taken to be a deformation of the algebra of functions on a symplectic manifold or Poisson manifold. However, as a natural quantization scheme (a functor), Weyl's map is not satisfactory. For example, the Weyl map of the classical angular-momentum-squared is not just the quantum angular momentum squared operator, but it further contains a constant term 3ħ/2. (This extra term is actually physically significant, since it accounts for the nonvanishing angular momentum of the ground-state Bohr orbit in the hydrogen atom. As a mere representation change, however, Weyl's map underlies the alternate Phase space formulation of conventional quantum mechanics.\n\nThe geometric quantization procedure falls into the following three steps: prequantization, polarization, and metaplectic correction. Prequantization produces a natural Hilbert space together with a quantization procedure for observables that exactly transforms Poisson brackets on the classical side into commutators on the quantum side. Nevertheless, the prequantum Hilbert space is generally understood to be \"too big\". The idea is that one should then select a Poisson-commuting set of \"n\" variables on the 2\"n\"-dimensional phase space and consider functions (or, more properly, sections) that depend only on these \"n\" variables. The \"n\" variables can be either real-valued, resulting in a position-style Hilbert space, or complex-valued, producing something like the Segal–Bargmann space. A polarization is a coordinate-independent description of such a choice of \"n\" Poisson-commuting functions. The metaplectic correction (also known as the half-form correction) is a technical modification of the above procedure that is necessary in the case of real polarizations and often convenient for complex polarizations.\n\nSuppose formula_1 is a symplectic manifold with symplectic form formula_2. Suppose at first that formula_2 is exact, meaning that there is a globally defined \"symplectic potential\" formula_4 with formula_5. We can consider the \"prequantum Hilbert space\" of square-integrable functions on formula_6 (with respect to the Liouville volume measure). For each smooth function formula_7 on formula_6, we can define the Kostant–Souriau prequantum operator\nwhere formula_10 is the Hamiltonian vector field associated to formula_7.\n\nMore generally, suppose formula_1 has the property that the integral of formula_13 over any closed surface is an integer. Then we can construct a line bundle formula_14 with connection whose curvature 2-form is formula_15. In that case, the prequantum Hilbert space is the space of square-integrable sections of formula_14, and we replace the formula for formula_17 above with\nwith formula_19 the connection.\nThe prequantum operators satisfy \nfor all smooth functions formula_7 and formula_22.\n\nThe construction of the preceding Hilbert space and the operators formula_17 is known as \"prequantization\".\n\nThe next step in the process of geometric quantization is the choice of a polarization. A polarization is a choice at each point in formula_6 a Lagrangian subspace of the complexified tangent space of formula_6. The subspaces should form an integrable distribution, meaning that the commutator of two vector fields lying in the subspace at each point should also lie in the vector field at each point. The \"quantum\" (as opposed to prequantum) Hilbert space is the space of sections of formula_14 that are covariantly constant in the direction of the polarization. The idea is that in the quantum Hilbert space, the sections should be functions of only formula_27 variables on the formula_28-dimensional classical phase space.\n\nIf formula_7 is a function for which the associated Hamiltonian flow preserves the polarization, then formula_17 will preserve the quantum Hilbert space. The assumption that the flow of formula_7 preserve the polarization is a strong one. Typically not very many functions will satisfy this assumption.\n\nThe half-form correction—also known as the metaplectic correction—is a technical modification to the above procedure that is necessary in the case of real polarizations to obtain a nonzero quantum Hilbert space; it is also often useful in the complex case. The line bundle formula_14 is replaced by the tensor product of formula_14 with the square root of the canonical bundle of formula_14. In the case of the vertical polarization, for example, instead of considering functions formula_35 of formula_36 that are independent of formula_37, one considers objects of the form formula_38. The formula for formula_17 must then be supplemented by an additional Lie derivative term. In the case of a complex polarization on the plane, for example, the half-form correction allows the quantization of the harmonic oscillator to reproduce the standard quantum mechanical formula for the energies, formula_40, with the \"formula_41\" coming courtesy of the half-forms.\n\nGeometric quantization of Poisson manifolds and symplectic foliations also is developed. For instance, this is the case of partially integrable and superintegrable Hamiltonian systems and non-autonomous mechanics.\n\nIn the case that the symplectic manifold is the 2-sphere, it can be realized as a coadjoint orbit in formula_42. Assuming that the area of the sphere is an integer multiple of formula_43, we can perform geometric quantization\nand the resulting Hilbert space carries an irreducible representation of SU(2). In the case that the area of the sphere is formula_43, we obtain the two-dimensional spin 1/2 representation.\n\n\n\n"}
{"id": "16640557", "url": "https://en.wikipedia.org/wiki?curid=16640557", "title": "Hand compass", "text": "Hand compass\n\nA hand compass (also hand bearing compass or sighting compass) is a compact magnetic compass capable of one-hand use and fitted with a sighting device to record a precise bearing or azimuth to a given target or to determine a location. Hand or sighting compasses include instruments with simple notch-and-post alignment (\"gunsights\"), prismatic sights, direct or lensatic sights, and mirror/vee (reflected-image) sights. With the additional precision offered by the sighting arrangement, and depending upon construction, sighting compasses provide increased accuracy when measuring precise bearings to an objective.\n\nThe term \"hand compass\" is used by some in the forestry and surveying professions to refer to a certain type of hand compass optimized for use in those fields, also known as a forester or cruiser compass. A \"hand compass\" may also include the various one-hand or 'pocket' versions of the surveyor's or geologist's transit.\n\nWhile small portable compasses fitted with mechanical sighting devices have existed for a few hundred years, the first one-hand compass with a sighting device appeared around 1885. These soon evolved into more elaborate and specialized models such as the Brunton \"Pocket Transit\" patented in 1894. Hand compasses were soon widely employed in the practice of forestry, geology, archaeology, speleology, preliminary cartography and land surveying.\nIn the United States, the hand compass became very popular among foresters seeking a compass to plot and estimate stands of timber. While the \"Pocket Transit\" was more than adequate for such work, it was relatively expensive. Consequently, a new type of hand compass was introduced: the \"forester\" or \"cruiser compass\". Traditionally, \"cruiser\" compasses featured a sighting notch, a mechanically-damped or \"dry\" needle, adjustable declination and a large dial marked in individual degrees using counterclockwise calibration (reversed east and west positions). A screw base for a tripod or \"jacob staff\" (monopod) was often fitted as well.\n\nBy the late 1960s many foresters had begun using more modern liquid-damped compass designs, including mirror-sight protractor models such as the \"Silva Type 15 \"Ranger\"\" or the \"Suunto MC-1\" (later, the \"MC-2\"). These compasses were fast to use, particularly along straight cruise lines and were sufficiently accurate for most forestry applications. On the other hand, geologists, speleologists, archaeologists, ornithologists, and foresters engaged in precision survey work often used direct-reading models such as the \"Suunto KB-14\", prismatic compasses such as Suunto \"KB-77\" or the traditional Brunton \"Pocket Transit\". Many models featured an optional quadrant (0-90-0 degree) scale instead of an azimuthal (0-360 degree) system.\n\nBy using a hand compass in combination with aerial photographs and maps a person can determine his/her location in the field, determine direction to landmarks or destinations, estimate distance, estimate area, and find points of interest (marked boundary lines, USGS marker, plot centers). For increased accuracy, many professional hand compasses continue to be fitted with tripod mounts. While the hand compass continues to be widely employed in such work, it has been increasingly supplanted in recent years by use of the GPS, or Global Positioning System receiver.\n\nThe marine hand compass, or hand-bearing compass as it is termed in nautical use, has been used by small-boat or inshore sailors since at least the 1920s to keep a running course or to record precise bearings to landmarks on shore in order to determine position via the resection technique. Instead of a magnetized needle or disc, most hand bearing compasses feature liquid damping with a \"floating card\" design (a magnetized, degreed float or dial atop a jeweled pivot bearing). Equipped with a viewing prism, the hand bearing compass allows instant reading of forward bearings from the user to an object or vessel, and some provide the reciprocal bearing as well. Modern examples of marine hand bearing compasses include the \"Suunto KB-14\" and \"KB-77\", and the \"Plastimo Iris 50\". These compasses frequently have battery-illuminated or photoluminescent degree dials for use in low light or darkness.\n\n"}
{"id": "51077606", "url": "https://en.wikipedia.org/wiki?curid=51077606", "title": "How Not to Be Wrong", "text": "How Not to Be Wrong\n\nHow Not to Be Wrong: The Power of Mathematical Thinking, written by Jordan Ellenberg, is a \"New York Times Best Selling\" book that connects various economic and societal philosophies with basic mathematics and statistical principles.\n\n\"How Not to Be Wrong\" explains the mathematics behind some of simplest day-to-day thinking. It then goes into more complex decisions people make. For example, Ellenberg explains many misconceptions about lotteries and whether or not they can be mathematically beaten.\n\nEllenberg uses mathematics to examine real-world issues ranging from the fetishizing of straight lines in the reporting of obesity to the game theory of missing flights, from the relevance to digestion of regression to the mean to the counter-intuitive Berkson's paradox.\n\nCHAPTER 1, LESS LIKE SWEDEN: Ellenberg encourages his readers to think nonlinearly, and know that “where you should go depends on where you are”. To develop his thought, he relates this to Voodoo economics and the Laffer curve of taxation. Although there are little to no numbers in this chapter, the point is that the overall concept still ties back to mathematical thinking.\n\nCHAPTER 2, STRAIGHT LOCALLY, CURVED GLOBALLY: This chapter puts an emphasis on recognizing that “not every curve is a straight line”, and makes reference to multiple mathematical concepts including the Pythagorean theorem, the derivation of Pi, Zeno’s paradox, and non-standard analysis.\n\nCHAPTER 3, EVERYONE IS OBESE: Here, Ellenberg dissects some common statistics about Obesity trends in the United States. He ties it into linear regression, and points out basic contradictions made by the original arguments presented. He uses many examples to make his point, including the correlation between SAT scores and tuition rates, as well as the trajectory of missiles.\n\nCHAPTER 4, HOW MANY IS THAT IN DEAD AMERICANS: Ellenberg analyzes statistics about the number of casualties around the world in different countries resulting from war. He notes that although proportion in these cases matters, it doesn’t always necessarily make sense when relating them to American deaths. He uses examples of deaths due to brain cancer, the Binomial Theorem, and voting polls to reinforce his point.\n\nCHAPTER 5, MORE PIE THAN PLATE: This chapter goes in depth with number percentages relating to employment rates, and references political allegations. He emphasizes that “actual numbers in these cases aren’t important, but knowing what to divide by what is mathematics in its truest form”, noting that mathematics in itself is in everything.\n\nCHAPTER 6, THE BALTIMORE STOCKBROKER AND THE BIBLE CODE: Ellenberg tries to get across that mathematics is in every single thing that we do. To support this, he uses examples about hidden codes in the Torah determined by ELS, Equidistant Letter Sequence, a Stockbroker parable, noting that “Improbable things happen”, and wiggle room attributes to that.\n\nCHAPTER 7, DEAD FISH DON’T READ MINDS: This chapter touches on a lot of things. The basis for this chapter are stories about a dead salmons MRI, trial and error in algebra, and birth control statistics as well as basketball statistics. He also notes that poetry can be compared to mathematics in that it’s “trained by exposure to stimuli, and manipulable in the lab”. Additionally, he writes of a few other mathematical concepts, including Null hypothesis and the Quantic Formula.\n\nCHAPTER 8, REDUCTIO AD UNLIKELY: This chapter focuses on the works and theorems/concepts of many famous mathematicians and philosophers. These include but aren’t limited to the Reductio Ad Absurdum by Aristotle, a look into the constellation Taurus by John Mitchell, and Yitang “Tom” Zhangs “bounded gaps” conjecture. He also delves into explaining rational numbers, the prime number theorem, and makes up his own word, “flogarithms”.\n\nCHAPTER 9, THE INTERNATIONAL JOURNAL OF HARUSPICY: Ellenberg relates the practice of Haruspicy, genes that affect Schizophrenia, and the accuracy of published papers as well as other things to the “P value” or Statistical Significance. He also notes at the end that Jerzy Neyman and Egon Pearson claimed that Statistics is about doing, not interpreting, and then relates this to other real-world examples.\n\nCHAPTER 10, ARE YOU THERE GOD? IT’S ME, BAYESIAN INFERENCE: This chapter relates Algorithms to things ranging from God, to Netflix movie recommendations, and to terrorism on Facebook. Ellenberg goes through quite a few mathematical concepts in this chapter, which include Conditional probabilities relating back to “P value”, Posterior Possibilities, Bayesian Inference, and Bayes Theorem as they correlate to Radio Psychics and Probability. Additionally, he uses Punnett Squares and other methods to explore the probability of Gods existence.\n\nCHAPTER 11, WHAT TO EXPECT WHEN YOU’RE EXPECTING TO WIN THE LOTTERY: This chapter discusses the different probabilities of winning the lottery and expected value as it relates to lottery tickets, including the story of how MIT students managed to “win” the lottery every time in their town. Ellenberg also talks about the Law of Large numbers again, as well as introducing the Additivity of expected value and the games of Franc-Carreau or the “noodle/needle problem”. Many mathematicians and other famous people are mentioned in this chapter, including Georges-Louis LeClerc, Comte de Buffon, and James Harvey.\n\nCHAPTER 12, MISS MORE PLANES: The mathematical concepts in this chapter include Utility and Utils, and the Laffer curve again. This chapter discusses the amount of time spent in the airport as it relates to flights being missed, Daniel Ellsberg, Blaise Pascal’s Pense’s, the probability of God once more, and the St. Petersburg Paradox.\n\nCHAPTER 13, WHERE THE TRAIN TRACKS MEET: This chapter includes discussions about the Lottery again, and Geometry in Renaissance paintings. It introduces some things about coding, including Error Correcting Code, Hamming Code, and code words. It also mentions Hamming distance at it relates to language. The mathematical concepts included in this chapter are Variance, the Projective Plane, the Fano plane, and the Face centered cubic lattice.\n\nCHAPTER 14, THE TRIUMPH OF MEDIOCRITY: This chapter discusses Mediocrity in everyday business according to Horace Secrist. It also includes discussions about Francis Galton’s “Hereditary Genius”, and baseball statistics about home runs.CHAPTER 15, GALTONS ELLIPSE: This chapter focuses on Sir Francis Galton, and his work on scatter plots, as well as the ellipses formed by them, correlation and causation, and the development from linear systems to quadratics. This chapter also addressed conditional and unconditional expectation, regression to the mean, eccentricity, Bivariate normal distribution, and dimensions in geometry.\n\nCHAPTER 16, DOES LUNG CANCER MAKE YOU SMOKE CIGARETTES: This chapter explores the correlation between smoking cigarettes and lung cancer, using work from R.A Fisher. It also goes into Berkson’s Fallacy, and uses the attractiveness of men to develop the thought, and talks about common effect at the end.\n\nCHAPTER 17, THERE IS NO SUCH THING AS PUBLIC OPINION: This chapter delves into the workings of a majority rules system, and points out the contradictions and confusion of it all, ultimately stating that public opinion doesn’t exist. It uses many examples to make its point, including different election statistics, the death sentence of a mentally retarded person, and a case with Justice Antonin Scalia. It also includes mathematical terms/concepts such as Independence of irrelevant alternatives, Asymmetric Domination effect, and Condorcet paradoxes.\n\nCHAPTER 18, “OUT OF NOTHING, I HAVE CREATED A STRANGE NEW UNIVERSE”: This chapter talks about János Bolyais, and his work on the parallel postulate. Others mentioned in this chapter include David Hilbert, and Gottlob Frege. It also explored points and lines, Formalism, and what the author calls a “Genius” mentality.\n\nThis last chapter introduces one last concept, \"ex falso quodlibet,\" and mentions Theodore Roosevelt, as well as the election between Obama and Romney. The author ends the novel with encouraging statements, noting that it’s okay to not know everything, and that we all learn from failure. He ends saying that to love math is to be “touched by fire and bound by reason”, and that we should all use it well.\n\nBill Gates endorsed \"How Not to Be Wrong\" and included it in his 2016 \"5 Books to Read This Summer\" list.\n\n\"The Washington Post\" reported that the book is “brilliantly engaging… part of the sheer intellectual joy of the book is watching the author leap nimbly from topic to topic, comparing slime molds to the Bush–Gore Florida vote, criminology to Beethoven’s Ninth Symphony. The final effect is of one enormous mosaic unified by mathematics.”\n\n\"The Wall Street Journal\" said, “Mr. Ellenberg writes, a kind of 'X-ray specs that reveal hidden structures underneath the messy and chaotic surface of the world.” \"The Guardian\" wrote, “Ellenberg's prose is a delight – informal and robust, irreverent yet serious.”\n\nBusiness insider said it's \"A collection of fascinating examples of math and its surprising applications...\"How Not To Be Wrong\" is full of interesting and weird mathematical tools and observations\".\n\nPublishers weekly writes \"Wry, accessible, and entertaining...Ellenberg finds the common-sense math at work in the every day world, and his vivid examples and clear descriptions show how 'math is woven into the way we reason'\".\n\nTimes Higher Education notes \"\"How Not To Be Wrong\" is beautifully written, holding the reader's attention throughout with well-chosen material, illuminating exposition, wit, and helpful examples...Ellenberg shares Gardner's remarkable ability to write clearly and entertainingly, bringing in deep mathematical ideas without the reader registering their difficulty\".\n\nSalon describes the book as \"A poet-mathematician offers an empowering and entertaining primer for the age of Big Data...A rewarding popular math book for just about anyone\".\n\n"}
{"id": "39197406", "url": "https://en.wikipedia.org/wiki?curid=39197406", "title": "Igor Shelushkov", "text": "Igor Shelushkov\n\nIgor Alexeyevich Shelushkov ( – ?) was a Soviet mental calculator. He mentally extracted roots of large numbers and calculated the number of syllables and characters in a given verse by listening. Shelushkov was featured in the 1968 Soviet popular science film \"Seven Steps Beyond the Horizon\", where he mentally extracted the sixth root of a 12-digit number and the 77th root of another multi-digit number. Shelushkov also competed with the Soviet third generation computer Mir. He extracted the 77th root of a 148-digit number in 18 seconds, while it took about 10 minutes to program the related operation for computer.\n\nShelushkov was a postgraduate at Gorki Polytechnic Institute (now Nizhny Novgorod State Technical University). According to Shelushkov, he used the memorized logarithmic table for calculations. His abilities were mentioned by Russian mathematician Vladimir Tvorogov, who attended one of his performances, and by psychologist Artur Petrovsky. Shelushkov's subsequent fate is unclear.\n"}
{"id": "32162058", "url": "https://en.wikipedia.org/wiki?curid=32162058", "title": "Jacquet module", "text": "Jacquet module\n\nIn mathematics, the Jacquet module \"J\"(\"V\") of a linear representation \"V\" of a group \"N\" is the space of co-invariants of \"N\"; or in other words the largest quotient of \"V\" on which \"N\" acts trivially, or the zeroth homology group H(\"N\",\"V\"). \n\nThe Jacquet functor \"J\" is the functor taking \"V\" to its Jacquet module \"J\"(\"V\"). Use of the phrase \"Jacquet module\" often implies that \"V\" is an admissible representation of a reductive algebraic group \"G\" over a local field, and \"N\" is the unipotent radical of a parabolic subgroup of \"G\". In the case of \"p\"-adic groups they were studied by .\n\n"}
{"id": "53788372", "url": "https://en.wikipedia.org/wiki?curid=53788372", "title": "Kuramoto–Sivashinsky equation", "text": "Kuramoto–Sivashinsky equation\n\nIn mathematics, the Kuramoto–Sivashinsky equation is a fourth-order nonlinear partial differential equation, named after Yoshiki Kuramoto and Gregory Sivashinsky, who derived the equation to model the diffusive instabilities in a laminar flame front in the late 1970s.\n\nwhere formula_2 is the Laplace operator and its square, formula_3 is the biharmonic operator. The Kuramoto–Sivashinsky equation is known for its chaotic behavior.\n\n"}
{"id": "1057456", "url": "https://en.wikipedia.org/wiki?curid=1057456", "title": "List of pre-modern Iranian scientists and scholars", "text": "List of pre-modern Iranian scientists and scholars\n\nThe following is a non-comprehensive list of Iranian scientists and engineers who lived from antiquity up until the beginning of the modern age. For the modern era, see List of contemporary Iranian scientists, scholars, and engineers. For mathematicians of any era, see List of Iranian mathematicians. (A person may appear on two lists, e.g. Abū Ja'far al-Khāzin.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "19357321", "url": "https://en.wikipedia.org/wiki?curid=19357321", "title": "Locally finite poset", "text": "Locally finite poset\n\nIn mathematics, a locally finite poset is a partially ordered set \"P\" such that for all \"x\", \"y\" ∈ \"P\", the interval [\"x\", \"y\"] consists of finitely many elements.\n\nGiven a locally finite poset \"P\" we can define its \"incidence algebra\". Elements of the incidence algebra are functions \"ƒ\" that assign to each interval [\"x\", \"y\"] of \"P\" a real number \"ƒ\"(\"x\", \"y\"). These functions form an associative algebra with a product defined by\n\nThere is also a definition of \"incidence coalgebra\".\n\nIn theoretical physics a locally finite poset is also called a causal set and has been used as a model for spacetime.\n\nStanley, Richard P. Enumerative Combinatorics, Volume I. Cambridge University Press, 1997. Pages 98, 113—116.\n"}
{"id": "5265043", "url": "https://en.wikipedia.org/wiki?curid=5265043", "title": "Mark Steiner", "text": "Mark Steiner\n\nMark Steiner (born May 6, 1942) is a professor of philosophy at the Hebrew University of Jerusalem, where he specializes in the philosophy of mathematics and physics. He is best known for his book \"The Applicability of Mathematics as a Philosophical Problem\", in which he attempted to explain the historical utility of mathematics in physics. The book may be considered an extended meditation on the issues raised by Eugene Wigner's article \"The Unreasonable Effectiveness of Mathematics in the Natural Sciences\" . The publisher writes, \"Steiner argues that, on the contrary, these laws were discovered, using manmade mathematical analogies, resulting in an anthropocentric picture of the universe as \"user friendly\" to human cognition—a challenge to the entrenched dogma of naturalism.\" Steiner is also the author of the book \"Mathematical Knowledge\".\n\nHe earned his Ph.D. at Princeton University in 1972.\n\nHis brother is Richard C. Steiner, Professor of Semitics at Yeshiva University.\n\n"}
{"id": "18910", "url": "https://en.wikipedia.org/wiki?curid=18910", "title": "Markup language", "text": "Markup language\n\nIn computer text processing, a markup language is a system for annotating a document in a way that is syntactically distinguishable from the text. The idea and terminology evolved from the \"marking up\" of paper manuscripts, i.e., the revision instructions by editors, traditionally written with a blue pencil on authors' manuscripts. In digital media, this \"blue pencil instruction text\" was replaced by tags, that is, instructions are expressed directly by tags or \"instruction text encapsulated by tags.\" However the whole idea of a mark up language is to avoid the formatting work for the text, as the tags in the mark up language serve the purpose to format the appropriate text (like a header or the beginning of a paragraph etc.). Every tag used in a Markup language has a property to format the text we write.\n\nExamples include typesetting instructions such as those found in troff, TeX and LaTeX, or structural markers such as XML tags. Markup instructs the software that displays the text to carry out appropriate actions, but is omitted from the version of the text that users see.\n\nSome markup languages, such as the widely used HTML, have pre-defined presentation semantics—meaning that their specification prescribes how to present the structured data. Others, such as XML, do not have them and are general purpose.\n\nHyperText Markup Language (HTML), one of the document formats of the World Wide Web, is an instance of Standard Generalized Markup Language or SGML, and follows many of the markup conventions used in the publishing industry in the communication of printed work between authors, editors, and printers.\n\nThe term \"markup\" is derived from the traditional publishing practice of \"marking up\" a manuscript, which involves adding handwritten annotations in the form of conventional symbolic printer's instructions in the margins and text of a paper manuscript or printed. It is computer jargon used in coding proof. For centuries, this task was done primarily by skilled typographers known as \"markup men\" or \"d markers\" who marked up text to indicate what typeface, style, and size should be applied to each part, and then passed the manuscript to others for typesetting by hand. Markup was also commonly applied by editors, proofreaders, publishers, and graphic designers, and indeed by document authors.\n\nThere are three main general categories of electronic markup:\n\n\n\n\nThere is considerable blurring of the lines between the types of markup. In modern word-processing systems, presentational markup is often saved in descriptive-markup-oriented systems such as XML, and then processed procedurally by implementations. The programming in procedural-markup systems such as TeX may be used to create higher-level markup systems that are more descriptive, such as LaTeX.\n\nIn recent years, a number of small and largely unstandardized markup languages have been developed to allow authors to create formatted text via web browsers, for use in wikis and web forums. These are sometimes called lightweight markup languages. Markdown or the markup language used by Wikipedia are examples of such wiki markup.\n\nThe first well-known public presentation of markup languages in computer text processing was made by William W. Tunnicliffe at a conference in 1967, although he preferred to call it \"generic coding.\" It can be seen as a response to the emergence of programs such as RUNOFF that each used their own control notations, often specific to the target typesetting device. In the 1970s, Tunnicliffe led the development of a standard called GenCode for the publishing industry and later was the first chair of the International Organization for Standardization committee that created SGML, the first standard descriptive markup language. Book designer Stanley Rice published speculation along similar lines in 1970. Brian Reid, in his 1980 dissertation at Carnegie Mellon University, developed the theory and a working implementation of descriptive markup in actual use.\n\nHowever, IBM researcher Charles Goldfarb is more commonly seen today as the \"father\" of markup languages. Goldfarb hit upon the basic idea while working on a primitive document management system intended for law firms in 1969, and helped invent IBM GML later that same year. GML was first publicly disclosed in 1973.\n\nIn 1975, Goldfarb moved from Cambridge, Massachusetts to Silicon Valley and became a product planner at the IBM Almaden Research Center. There, he convinced IBM's executives to deploy GML commercially in 1978 as part of IBM's Document Composition Facility product, and it was widely used in business within a few years.\n\nSGML, which was based on both GML and GenCode, was developed by Goldfarb in 1974. Goldfarb eventually became chair of the SGML committee. SGML was first released by ISO as the ISO 8879 standard in October 1986.\n\nSome early examples of computer markup languages available outside the publishing industry can be found in typesetting tools on Unix systems such as troff and nroff. In these systems, formatting commands were inserted into the document text so that typesetting software could format the text according to the editor's specifications. It was a trial and error iterative process to get a document printed correctly. Availability of WYSIWYG (\"what you see is what you get\") publishing software supplanted much use of these languages among casual users, though serious publishing work still uses markup to specify the non-visual structure of texts, and WYSIWYG editors now usually save documents in a markup-language-based format.\n\nAnother major publishing standard is TeX, created and refined by Donald Knuth in the 1970s and '80s. TeX concentrated on detailed layout of text and font descriptions to typeset mathematical books. This required Knuth to spend considerable time investigating the art of typesetting. TeX is mainly used in academia, where it is a \"de facto\" standard in many scientific disciplines. A TeX macro package known as LaTeX provides a descriptive markup system on top of TeX, and is widely used.\n\nThe first language to make a clean distinction between structure and presentation was Scribe, developed by Brian Reid and described in his doctoral thesis in 1980. Scribe was revolutionary in a number of ways, not least that it introduced the idea of styles separated from the marked up document, and of a grammar controlling the usage of descriptive elements. Scribe influenced the development of Generalized Markup Language (later SGML) and is a direct ancestor to HTML and LaTeX .\n\nIn the early 1980s, the idea that markup should be focused on the structural aspects of a document and leave the visual presentation of that structure to the interpreter led to the creation of SGML. The language was developed by a committee chaired by Goldfarb. It incorporated ideas from many different sources, including Tunnicliffe's project, GenCode. Sharon Adler, Anders Berglund, and James A. Marke were also key members of the SGML committee.\n\nSGML specified a syntax for including the markup in documents, as well as one for separately describing \"what\" tags were allowed, and \"where\" (the Document Type Definition (DTD) or schema). This allowed authors to create and use any markup they wished, selecting tags that made the most sense to them and were named in their own natural languages. Thus, SGML is properly a meta-language, and many particular markup languages are derived from it. From the late '80s on, most substantial new markup languages have been based on SGML system, including for example TEI and DocBook. SGML was promulgated as an International Standard by International Organization for Standardization, ISO 8879, in 1986.\n\nSGML found wide acceptance and use in fields with very large-scale documentation requirements. However, many found it cumbersome and difficult to learn—a side effect of its design attempting to do too much and be too flexible. For example, SGML made end tags (or start-tags, or even both) optional in certain contexts, because its developers thought markup would be done manually by overworked support staff who would appreciate saving keystrokes.\n\nIn 1989, computer scientist Sir Tim Berners-Lee wrote a memo proposing an Internet-based hypertext system, then specified HTML and wrote the browser and server software in the last part of 1990. The first publicly available description of HTML was a document called \"HTML Tags\", first mentioned on the Internet by Berners-Lee in late 1991. It describes 18 elements comprising the initial, relatively simple design of HTML. Except for the hyperlink tag, these were strongly influenced by SGMLguid, an in-house SGML-based documentation format at CERN. Eleven of these elements still exist in HTML 4.\n\nBerners-Lee considered HTML an SGML application. The Internet Engineering Task Force (IETF) formally defined it as such with the mid-1993 publication of the first proposal for an HTML specification: \"Hypertext Markup Language (HTML)\" Internet-Draft by Berners-Lee and Dan Connolly, which included an SGML Document Type Definition to define the grammar. Many of the HTML text elements are found in the 1988 ISO technical report TR 9537 \"Techniques for using SGML\", which in turn covers the features of early text formatting languages such as that used by the RUNOFF command developed in the early 1960s for the CTSS (Compatible Time-Sharing System) operating system. These formatting commands were derived from those used by typesetters to manually format documents. Steven DeRose argues that HTML's use of descriptive markup (and influence of SGML in particular) was a major factor in the success of the Web, because of the flexibility and extensibility that it enabled. HTML became the main markup language for creating web pages and other information that can be displayed in a web browser, and is quite likely the most used markup language in the world today.\n\nXML (Extensible Markup Language) is a meta markup language that is now widely used. XML was developed by the World Wide Web Consortium, in a committee created and chaired by Jon Bosak. The main purpose of XML was to simplify SGML by focusing on a particular problem—documents on the Internet. XML remains a meta-language like SGML, allowing users to create any tags needed (hence \"extensible\") and then describing those tags and their permitted uses.\n\nXML adoption was helped because every XML document can be written in such a way that it is also an SGML document, and existing SGML users and software could switch to XML fairly easily. However, XML eliminated many of the more complex and human-oriented features of SGML to simplify implementation environments such as documents and publications. However, it appeared to strike a happy medium between simplicity and flexibility, and was rapidly adopted for many other uses. XML is now widely used for communicating data between applications.\n\nSince January 2000, all W3C Recommendations for HTML have been based on XML rather than SGML, using the abbreviation XHTML (Extensible HyperText Markup Language). The language specification requires that XHTML Web documents must be \"well-formed\" XML documents. This allows for more rigorous and robust documents while using tags familiar from HTML.\n\nOne of the most noticeable differences between HTML and XHTML is the rule that \"all tags must be closed\": empty HTML tags such as codice_2 must either be \"closed\" with a regular end-tag, or replaced by a special form: (the space before the 'codice_3' on the end tag is optional, but frequently used because it enables some pre-XML Web browsers, and SGML parsers, to accept the tag). Another is that all attribute values in tags must be quoted. Finally, all tag and attribute names within the XHTML namespace must be lowercase to be valid. HTML, on the other hand, was case-insensitive.\n\nMany XML-based applications now exist, including the Resource Description Framework as RDF/XML, XForms, DocBook, SOAP, and the Web Ontology Language (OWL). For a partial list of these, see List of XML markup languages.\n\nA common feature of many markup languages is that they intermix the text of a document with markup instructions in the same data stream or file. This is not necessary; it is possible to isolate markup from text content, using pointers, offsets, IDs, or other methods to co-ordinate the two. Such \"standoff markup\" is typical for the internal representations that programs use to work with marked-up documents. However, embedded or \"inline\" markup is much more common elsewhere. Here, for example, is a small section of text marked up in HTML:\n\nThe codes enclosed in angle-brackets codice_4 are markup instructions (known as tags), while the text between these instructions is the actual text of the document. The codes codice_5, codice_6, and codice_7 are examples of \"semantic\" markup, in that they describe the intended purpose or meaning of the text they include. Specifically, codice_5 means \"this is a first-level heading\", codice_6 means \"this is a paragraph\", and codice_7 means \"this is an emphasized word or phrase\". A program interpreting such structural markup may apply its own rules or styles for presenting the various pieces of text, using different typefaces, boldness, font size, indentation, colour, or other styles, as desired.\nA tag such as \"h1\" (header level 1) might be presented in a large bold sans-serif typeface, for example, or in a monospaced (typewriter-style) document it might be underscored – or it might not change the presentation at all.\n\nIn contrast, the codice_11 tag in HTML is an example of \"presentational\" markup; it is generally used to specify a particular characteristic of the text (in this case, the use of an italic typeface) without specifying the reason for that appearance.\n\nThe Text Encoding Initiative (TEI) has published extensive guidelines for how to encode texts of interest in the humanities and social sciences, developed through years of international cooperative work. These guidelines are used by projects encoding historical documents, the works of particular scholars, periods, or genres, and so on.\n\nWhile the idea of markup language originated with text documents, there is increasing use of markup languages in the presentation of other types of information, including playlists, vector graphics, web services, content syndication, and user interfaces. Most of these are XML applications, because XML is a well-defined and extensible language.\n\nThe use of XML has also led to the possibility of combining multiple markup languages into a single profile, like XHTML+SMIL and XHTML+MathML+SVG.\n\nBecause markup languages, and more generally data description languages (not necessarily textual markup), are not programming languages (they are data without instructions), they are more easily manipulated than programming languages—for example, web pages are presented as HTML documents, not C code, and thus can be embedded within other web pages, displayed when only partially received, and so forth. This leads to the web design principle of the rule of least power, which advocates using the \"least\" (computationally) powerful language that satisfies a task to facilitate such manipulation and reuse.\n\n"}
{"id": "1529094", "url": "https://en.wikipedia.org/wiki?curid=1529094", "title": "Mary Cartwright", "text": "Mary Cartwright\n\nDame Mary Lucy Cartwright, (17 December 1900 – 3 April 1998) was a British mathematician.\n\nWith J. E. Littlewood she was one of the first mathematicians to study what would later become known as chaos theory.\n\nCartwright was born in Aynho, Northamptonshire, where her father, William Digby Cartwright, was vicar. Through her grandmother Jane Holbech she was descended from the poet John Donne and William Mompesson, the Vicar of Eyam. She had four siblings, two older and two younger: John (born 1896), Nigel (born 1898), Jane (born 1905), and William (born 1907).\n\nEarly education was at Leamington High School (1912–1915) then Gravely Manor School in Boscombe (1915–1916) before completion in Godolphin School in Salisbury (1916–1919).\n\nShe studied mathematics at St Hugh's College, Oxford, graduating in 1923 with a first class degree. She was the first woman to attain the final degree lectures and to obtain a first. She then taught at Alice Ottley School in Worcester and Wycombe Abbey School in Buckinghamshire before returning to Oxford in 1928 to read for her D.Phil.\n\nShe was supervised by G. H. Hardy in her doctoral studies. During the academic year 1928–9 Hardy was at Princeton, so it was E. C. Titchmarsh who took over the duties as a supervisor. Her thesis \"The Zeros of Integral Functions of Special Types\" was examined by J. E. Littlewood whom she met for the first time as an external examiner in her oral examination for this 1930 D.Phil. She would later establish an enduring collaboration with Littlewood.\n\nIn 1930, Cartwright was awarded a Yarrow Research Fellowship and she went to Girton College, Cambridge, to continue working on the topic of her doctoral thesis. Attending Littlewood's lectures, she solved one of the open problems which he posed. Her mathematical theorem, now known as Cartwright's theorem, gives an estimate for the maximum modulus of an analytic function that takes the same value no more than \"p\" times in the unit disc. To prove the theorem she used a new approach, applying a technique introduced by Lars Ahlfors for conformal mappings.\n\nIn 1936, Cartwright became director of studies in mathematics at Girton College, and in 1938 she began work on a new project which had a major impact on the direction of her research. The Radio Research Board of the Department of Scientific and Industrial Research produced a memorandum regarding certain differential equations which came out of modelling radio and radar work. They asked the London Mathematical Society if they could help find a mathematician who could work on these problems and Cartwright became interested in this memorandum. The dynamics lying behind the problems were unfamiliar to Cartwright so she approached Littlewood for help with this aspect. They began to collaborate studying the equations. Littlewood wrote:\n\nThe fine structure which Littlewood describes here is today seen to be a typical instance of the butterfly effect. The collaboration led to important results, and these have greatly influenced the direction that the modern theory of dynamical systems has taken.\n\nIn 1945, she simplified Hermite's elementary proof of the irrationality of. Her version of the proof was published in an appendix to Sir Harold Jeffreys' book \"Scientific Inference\". In 1947, she was elected to be a Fellow of the Royal Society and, although she was not the first woman to be elected to that Society, she was the first female mathematician.\n\nCartwright was appointed Mistress of Girton in 1948 then, in addition, a Reader in the Theory of Functions in Cambridge in 1959, holding this appointment until 1968. From 1957 to 1960 she was president of the Cambridge Association of University Women.\n\nAfter retiring from Girton, she was a visiting professor at Brown University from 1968-1969 and at Claremont Graduate School from 1969-1970.\n\nShe died in Midfield Lodge Nursing Home in Cambridge in 1998.\n\nCartwright was the first woman:\nShe also received the De Morgan Medal of the Society in 1968. In 1968 she was elected an Honorary Fellow of The Royal Society of Edinburgh (HonFRSE). In 1969 she received the distinction of being honoured by the Queen, becoming Dame Mary Cartwright, Dame Commander of the Order of the British Empire.\n\n\n"}
{"id": "1186588", "url": "https://en.wikipedia.org/wiki?curid=1186588", "title": "Mathematische Arbeitstagung", "text": "Mathematische Arbeitstagung\n\nThe Mathematische Arbeitstagung taking place annually in Bonn since 1957, and founded by Friedrich Hirzebruch, was an international meeting of mathematicians intended to act in clearing-house fashion, by disseminating current research ideas; and, at the same time, to bring mathematics in West Germany back into its place in European trends. It proved highly successful in attracting the cream of younger mathematicians, partly because its structure was not that of the conventional international conference. The programme of talks was decided 'in real time' only, rather than in advance.\n\nFor example, in 1962 the meeting was dominated by talks on K-theory, at that time the breaking news. The early participants included Jean-Pierre Serre, Michael Atiyah and Frank Adams.\n\nThe institutional structure was reinforced from 1969 by the \"Sonderforschungsbereich Theoretische Mathematik\" programme, and from 1980 by the founding of the Max Planck Institute for Mathematics in Bonn.\n\n"}
{"id": "12559595", "url": "https://en.wikipedia.org/wiki?curid=12559595", "title": "Midhinge", "text": "Midhinge\n\nIn statistics, the midhinge is the average of the first and third quartiles and is thus a measure of location.\nEquivalently, it is the 25% trimmed mid-range or 25% midsummary; it is an L-estimator.\n\nThe midhinge is complemented by the H-spread, or interquartile range, which is the difference of the third and first quartiles and which is a measure of statistical dispersion, in sense that if one knows the midhinge and the interquartile range, one can find the first and third quartiles.\n\nThe use of the term \"hinge\" for the lower or upper quartiles derives from John Tukey's work on exploratory data analysis, and \"midhinge\" is a fairly modern term dating from around that time. The midhinge is slightly simpler to calculate than the trimean, which originated in the same context and equals the average of the median and the midhinge.\n\n\n"}
{"id": "305303", "url": "https://en.wikipedia.org/wiki?curid=305303", "title": "Multiset", "text": "Multiset\n\nIn mathematics, a multiset (aka bag or mset) is a modification of the concept of a set that, unlike a set, allows for multiple instances for each of its elements. The positive integer number of instances, given for each element is called the multiplicity of this element in the multiset. As a consequence, an infinite number of multisets exist, which contain only elements and , but vary by the multiplicity of their elements:\n\nThese objects are all different, when viewed as multisets, although they are the same set, since they all consist of the same elements. As with sets, and in contrast to tuples, order does not matter in discriminating multisets, so and denote the same multiset. To distinguish between sets and multisets, a notation that incorporates square brackets is sometimes used: the multiset can be denoted as .\n\nThe cardinality of a multiset is constructed by summing up the multiplicities of all its elements. For example, in the multiset the multiplicities of the members , , and are respectively 2, 3, and 1, and therefore the cardinality of this multiset is 6.\n\nNicolaas Govert de Bruijn coined the word \"multiset\" in the 1970s, according to Donald Knuth.\nHowever, the use of the concept for multisets predates the coinage of word \"multiset\" by many centuries. Knuth himself attributes the first study of multisets to the Indian mathematician Bhāskarāchārya, who described permutations of multisets around 1150. Knuth also lists other names that were proposed or used for this concept, including \"list\", \"bunch\", \"bag\", \"heap\", \"sample\", \"weighted set\", \"collection\", and \"suite\".\n\nWayne Blizard traced multisets back to the very origin of numbers, arguing that “in ancient times, the number \"n\" was often represented by a collection of \"n\" strokes, tally marks, or units.” These and similar collections of objects are multisets, because strokes, tally marks, or units are considered indistinguishable. This shows that people implicitly used multisets even before mathematics emerged.\n\nPractical needs for this structure have caused multisets to be rediscovered several times, appearing in literature under different names. For instance, they were important in early AI languages, such as QA4, where they were referred to as \"bags,\" a term attributed to Peter Deutsch. A multiset has been also called an aggregate, heap, bunch, sample, weighted set, occurrence set, and fireset (finitely repeated element set).\n\nAlthough multisets were used implicitly from ancient times, their explicit exploration happened much later. The first known study of multisets is attributed to the Indian mathematician Bhāskarāchārya circa 1150, who described permutations of multisets. The work of Marius Nizolius (1498–1576) contains another early reference to the concept of multisets. Athanasius Kircher found the number of multiset permutations when one element can be repeated. Jean Prestet published a general rule for multiset permutations in 1675. John Wallis explained this rule in more detail in 1685.\n\nMultisets appeared explicitly in the work of Richard Dedekind.\n\nOther mathematicians formalized multisets and began to study them as precise mathematical structures in the 20th century. For example, Whitney (1933) described \"generalized sets\" (\"sets\" whose characteristic functions may take any integer value - positive, negative or zero). Monro (1987) investigated the category Mul of multisets and their morphisms, defining a \"multiset\" as a set with an equivalence relation between elements \"of the same \"sort\"\", and a \"morphism\" between multisets as a function which respects \"sorts\". He also introduced a \"multinumber\": a function \"f(x)\" from a multiset to the natural numbers, giving the \"multiplicity\" of element \"x\" in the multiset. Monro argued that the concepts of multiset and multinumber are often mixed indiscriminately, though both are useful.\n\nOne of the simplest and most natural examples is the multiset of prime factors of a number . Here the underlying set of elements is the set of prime divisors of . For example, the number 120 has the prime factorization\nwhich gives the multiset .\n\nA related example is the multiset of solutions of an algebraic equation. A quadratic equation, for example, has two solutions. However, in some cases they are both the same number. Thus the multiset of solutions of the equation could be , or it could be . In the latter case it has a solution of multiplicity 2. More generally, the fundamental theorem of algebra asserts that the complex solutions of a polynomial equation of degree always form a multiset of cardinality .\n\nA special case of the above are the eigenvalues of a matrix, if these are defined as the multiset of roots of its characteristic polynomial. However a choice is made here: the (usual) definition of eigenvalues does not refer to the characteristic polynomial, and this give rise to another notion of multiplicity, and therefore to another multiset. The geometric multiplicity of an eigenvalue of a matrix is the dimension of the kernel of ; the geometric multiplicity of an eigenvalue is not larger and often smaller than its \"algebraic multiplicity\", which is the multiplicity as a root of the characteristic polynomial. The geometric multiplicity of an eigenvalue is its multiplicity as a root of the minimal polynomial of . Therefore, the multiset of eigenvalues with geometric multiplicities is the multiset of roots of the minimal polynomial.\n\nA multiset may be formally defined as a 2-tuple where is the \"underlying set\" of the multiset, formed from its distinct elements, and formula_2 is a function from to the set of the positive integers, giving the \"multiplicity\", that is, the number of occurrences, of the element in the multiset as the number .\n\nRepresenting the function by its graph, that is the set of ordered pairs formula_3 allows for writing the multiset as , and the multiset as . This notation is however not commonly used and more compact notations are employed.\n\nIf formula_4 is a finite set, the multiset is often represented as \n\nwhere upper indices equal to 1 are omitted. For example, the multiset may be written formula_7 or formula_8 If the elements of the multiset are numbers, a confusion is possible with ordinary arithmetic operations, those normally can be excluded from the context. On the other hand, the latter notation is coherent with the fact that the prime factorization of a positive integer is a uniquely defined multiset, as asserted by the fundamental theorem of arithmetic. Also, a monomial is a multiset of indeterminates.\n\nA multiset corresponds to an ordinary set if the multiplicity of every element is one (as opposed to some larger natural number). An indexed family, , where varies over some index-set \"I\", may define a multiset, sometimes written . In this view the underlying set of the multiset is given by the image of the family, and the multiplicity of any element is the number of index values such that formula_9. In this article the multiplicities are considered to be finite, i.e. no element occurs infinitely many times in the family: even in an infinite multiset, the multiplicities are finite numbers.\n\nIt is possible to extend the definition of a multiset by allowing multiplicities of individual elements to be infinite cardinals instead of natural numbers, but not all properties carry over to this generalization.\n\nElements of a multiset are generally taken in a fixed set , sometimes called a \"universe\", which is typically the set of natural numbers. An element of that does not belong to a given multiset is said to have a multiplicity 0 in this multiset. This extends the multiplicity function of the multiset to a function from to the set formula_10 of nonnegative integers. This defines a one to one correspondence between these functions and the multisets that have their elements in .\n\nThis extended multiplicity function is commonly called simply the multiplicity function, and suffices for defining multisets, when the universe containing the elements has been fixed. This multiplicity function is a generalization of the indicator function of a subset, and shares some properties with it.\n\nThe support of a multiset formula_11 in a universe is the underlying set of the multiset. Using the multiplicity function formula_12, it is characterized as \n\nA multiset is \"finite\" if its support is finite, or, equivalently, if its cardinality\nis finite. The \"empty multiset\" is the unique multiset with an empty support (underlying set), and thus a cardinality 0.\n\nThe usual operations of sets may be extended to multisets by using the multiplicity function, in a similar way as using the indicator function for subsets. In the following, and are multisets in a given universe , with multiplicity functions formula_15 and formula_16\n\n\nTwo multisets are \"disjoint\" if their supports are disjoint sets. This is equivalent to saying that their intersection is the empty multiset or that their sum equals their union.\n\nThe number of multisets of cardinality , with elements taken from a finite set of cardinality , is called the multiset coefficient or multiset number. This number is written by some authors as formula_21, a notation that is meant to resemble that of binomial coefficients; it is used for instance in (Stanley, 1997), and could be pronounced \" multichoose \" to resemble \" choose \" for formula_22. Unlike for binomial coefficients, there is no \"multiset theorem\" in which multiset coefficients would occur, and they should not be confused with the unrelated multinomial coefficients that occur in the multinomial theorem.\n\nThe value of multiset coefficients can be given explicitly as\nwhere the second expression is as a binomial coefficient; many authors in fact avoid separate notation and just write binomial coefficients. So, the number of such multisets is the same as the number of subsets of cardinality in a set of cardinality . The analogy with binomial coefficients can be stressed by writing the numerator in the above expression as a rising factorial power\nto match the expression of binomial coefficients using a falling factorial power:\n\nThere are for example 4 multisets of cardinality 3 with elements taken from the set of cardinality 2 ( = 2, = 3), namely , , , . There are also 4 subsets of cardinality 3 in the set of cardinality 4 (), namely , , , .\n\nOne simple way to prove the equality of multiset coefficients and binomial coefficients given above, involves representing multisets in the following way. First, consider the notation for multisets that would represent (6 s, 2 s, 3 s, 7 s) in this form:\n\nThis is a multiset of cardinality = 18 made of elements of a set of cardinality = 4. The number of characters including both dots and vertical lines used in this notation is 18 + 4 − 1. The number of vertical lines is 4 − 1. The number of multisets of cardinality 18 is then the number of ways to arrange the 4 − 1 vertical lines among the 18 + 4 − 1 characters, and is thus the number of subsets of cardinality 4 − 1 in a set of cardinality 18 + 4 − 1. Equivalently, it is the number of ways to arrange the 18 dots among the 18 + 4 − 1 characters, which is the number of subsets of cardinality 18 of a set of cardinality 18 + 4 − 1. This is\nthus is the value of the multiset coefficient and its equivalencies:\n\nOne may define a generalized binomial coefficient\nin which is not required to be a nonnegative integer, but may be negative or a non-integer, or a non-real complex number. (If  = 0, then the value of this coefficient is 1 because it is the empty product.) Then the number of multisets of cardinality in a set of cardinality is\n\nA recurrence relation for multiset coefficients may be given as\nwith\n\nThe above recurrence may be interpreted as follows.\nLet  := formula_36 be the source set. There is always exactly one (empty) multiset of size 0, and if  = 0 there are no larger multisets, which gives the initial conditions.\n\nNow, consider the case in which  > 0. A multiset of cardinality with elements from might or might not contain any instance of the final element . If it does appear, then by removing once, one is left with a multiset of cardinality  − 1 of elements from , and every such multiset can arise, which gives a total of\n\nIf does not appear, then our original multiset is equal to a multiset of cardinality with elements from , of which there are\n\nThus,\n\nThe generating function of the multiset coefficients is very simple, being\nAs multisets are in one to one correspondence with monomials, formula_41 is also the number of monomials of degree in indeterminates. Thus, above series is also the Hilbert series of the polynomial ring formula_42\n\nAs formula_41 is a polynomial in , it is defined for any complex value of .\n\nThe multiplicative formula allows the definition of multiset coefficients to be extended by replacing \"n\" by an arbitrary number \"α\" (negative, real, complex):\n\nWith this definition one has a generalization of the negative binomial formula (with one of the variables set to 1), which justifies calling the formula_45 negative binomial coefficients:\n\nThis Taylor series formula is valid for all complex numbers \"α\" and \"X\" with  < 1. It can also be interpreted as an identity of formal power series in \"X\", where it actually can serve as definition of arbitrary powers of series with constant coefficient equal to 1; the point is that with this definition all identities hold that one expects for exponentiation, notably\n\nand formulas such as these can be used to prove identities for the multiset coefficients.\n\nIf \"α\" is a nonpositive integer \"n\", then all terms with \"k\" > −\"n\" are zero, and the infinite series becomes a finite sum. However, for other values of \"α\", including positive integers and rational numbers, the series is infinite.\n\nMultisets have various applications. They are becoming fundamental in combinatorics. Multisets have become an important tool in database theory, which often uses the synonym \"bag\". For instance, multisets are often used to implement relations in database systems. Multisets also play an important role in computer science.\n\nThere are also other applications. For instance, Richard Rado used multisets as a device to investigate the properties of families of sets. He wrote, \"The notion of a set takes no account of multiple occurrence of any one of its members, and yet it is just this kind of information which is frequently of importance. We need only think of the set of roots of a polynomial f(x) or the spectrum of a linear operator.\"\n\nDifferent generalizations of multisets have been introduced, studied and applied to solving problems.\n\n"}
{"id": "21987680", "url": "https://en.wikipedia.org/wiki?curid=21987680", "title": "Non-sampling error", "text": "Non-sampling error\n\nIn statistics, non-sampling error is a catch-all term for the deviations of estimates from their true values that are not a function of the sample chosen, including various systematic errors and random errors that are not due to sampling. Non-sampling errors are much harder to quantify than sampling errors.\n\nNon-sampling errors in survey estimates can arise from:\n\n\nAn excellent discussion of issues pertaining to non-sampling error can be found in several sources such as Kalton (1983) and Salant and Dillman (1995),\n\n"}
{"id": "166589", "url": "https://en.wikipedia.org/wiki?curid=166589", "title": "On Numbers and Games", "text": "On Numbers and Games\n\nOn Numbers and Games is a mathematics book by John Horton Conway first published in 1976. The book is written by a pre-eminent mathematician, and is directed at other mathematicians. The material is, however, developed in a playful and unpretentious manner and many chapters are accessible to non-mathematicians. Martin Gardner discussed the book at length, particularly Conway's construction of Surreal numbers, in his Mathematical Games column in \"Scientific American\" in September 1976.\n\nThe book is roughly divided into two sections: the first half (or \"Zeroth Part\"), on numbers, the second half (or \"First Part\"), on games. In the first section, Conway provides an axiomatic construction of numbers and ordinal arithmetic, namely, the integers, reals, the countable infinity, and entire towers of infinite ordinals, using a notation that is essentially an almost trite (but critically important) variation of the Dedekind cut. As such, the construction is rooted in axiomatic set theory, and is closely related to the Zermelo–Fraenkel axioms. The section also covers what Conway (adopting Knuth's nomenclature) termed the \"surreal numbers\".\n\nConway then notes that, in this notation, the numbers in fact belong to a larger class, the class of all two-player games. The axioms for greater than and less than are seen to be a natural ordering on games, corresponding to which of the two players may win. The remainder of the book is devoted to exploring a number of different (non-traditional, mathematically inspired) two-player games, such as nim, hackenbush, and the map-coloring games col and snort. The development includes their scoring, a review of Sprague–Grundy theorem, and the inter-relationships to numbers, including their relationship to infinitesimals.\n\nThe book was first published by Academic Press Inc in 1976, , and re-released by AK Peters in 2000 ().\n\nA game in the sense of Conway is a position in a contest between two players, Left and Right. Each player has a set of games called \"options\" to choose from in turn. Games are written {L|R} where L is the set of Left's options and R is the set of Right's options. At the start there are no games at all, so the empty set (i.e., the set with no members) is the only set of options we can provide to the players. This defines the game is called 1, and the game {|0} is called -1. The game {0|0} is called * (star), and is the first game we find that is not a number.\n\nAll numbers are positive, negative, or zero, and we say that a game is positive if Left will win, negative if Right will win, or zero if the second player will win. Games that are not numbers have a fourth possibility: they may be fuzzy, meaning that the first player will win. * is a fuzzy game.\n\n"}
{"id": "194467", "url": "https://en.wikipedia.org/wiki?curid=194467", "title": "Parity bit", "text": "Parity bit\n\nA parity bit, or check bit, is a bit added to a string of binary code to ensure that the total number of 1-bits in the string is even or odd. Parity bits are used as the simplest form of error detecting code.\n\nThere are two variants of parity bits: even parity bit and odd parity bit. \n\nIn the case of even parity, for a given set of bits, the occurrences of bits whose value is 1 is counted. If that count is odd, the parity bit value is set to 1, making the total count of occurrences of 1s in the whole set (including the parity bit) an even number. If the count of 1s in a given set of bits is already even, the parity bit's value is 0. \n\nIn the case of odd parity, the coding is reversed. For a given set of bits, if the count of bits with a value of 1 is even, the parity bit value is set to 1 making the total count of 1s in the whole set (including the parity bit) an odd number. If the count of bits with a value of 1 is odd, the count is already odd so the parity bit's value is 0.\n\nEven parity is a special case of a cyclic redundancy check (CRC), where the 1-bit CRC is generated by the polynomial \"x\"+1.\n\nIf a bit is present at a point otherwise dedicated to a parity bit, but is not used for parity, it may be referred to as a mark parity bit if the parity bit is always 1, or a space parity bit if the bit is always 0. In such cases where the value of the bit is constant, it may be called a stick parity bit even though its function has nothing to do with parity. The function of such bits varies with the system design, but examples of functions for such bits include timing management, or identification of a packet as being of data or address significance. If its actual bit value is irrelevant to its function, the bit amounts to a don't-care term.\n\nParity bits are generally applied to the smallest units of a communication protocol, typically 8-bit octets (bytes), although they can also be applied separately to an entire message string of bits.\n\nIn mathematics, parity refers to the evenness or oddness of an integer, which for a binary number is determined only by the least significant bit. In telecommunications and computing, parity refers to the evenness or oddness of the number of bits with value one within a given set of bits, and is thus determined by the value of all the bits. It can be calculated via a XOR sum of the bits, yielding 0 for even parity and 1 for odd parity. This property of being dependent upon all the bits and changing value if any one bit changes allows for its use in error detection schemes.\n\nIf an odd number of bits (including the parity bit) are transmitted incorrectly, the parity bit will be incorrect, thus indicating that a parity error occurred in the transmission. The parity bit is only suitable for detecting errors; it cannot correct any errors, as there is no way to determine which particular bit is corrupted. The data must be discarded entirely, and re-transmitted from scratch. On a noisy transmission medium, successful transmission can therefore take a long time, or even never occur. However, parity has the advantage that it uses only a single bit and requires only a number of XOR gates to generate. See Hamming code for an example of an error-correcting code.\n\nParity bit checking is used occasionally for transmitting ASCII characters, which have 7 bits, leaving the 8th bit as a parity bit.\n\nFor example, the parity bit can be computed as follows. Assume Alice and Bob are communicating and Alice wants to send Bob the simple 4-bit message 1001.\nThis mechanism enables the detection of single bit errors, because if one bit gets flipped due to line noise, there will be an incorrect number of ones in the received data. In the two examples above, Bob's calculated parity value matches the parity bit in its received value, indicating there are no single bit errors. Consider the following example with a transmission error in the second bit using XOR:\n\nThere is a limitation to parity schemes. A parity bit is only guaranteed to detect an odd number of bit errors. If an even number of bits have errors, the parity bit records the correct number of ones, even though the data is corrupt. (See also error detection and correction.) Consider the same example as before with an even number of corrupted bits:\nBob observes even parity, as expected, thereby failing to catch the two bit errors.\n\nBecause of its simplicity, parity is used in many hardware applications where an operation can be repeated in case of difficulty, or where simply detecting the error is helpful. For example, the SCSI and PCI buses use parity to detect transmission errors, and many microprocessor instruction caches include parity protection. Because the I-cache data is just a copy of main memory, it can be disregarded and re-fetched if it is found to be corrupted.\n\nIn serial data transmission, a common format is 7 data bits, an even parity bit, and one or two stop bits. This format neatly accommodates all the 7-bit ASCII characters in a convenient 8-bit byte. Other formats are possible; 8 bits of data plus a parity bit can convey all 8-bit byte values. \n\nIn serial communication contexts, parity is usually generated and checked by interface hardware (e.g., a UART) and, on reception, the result made available to a processor such as the CPU (and so too, for instance, the operating system) via a status bit in a hardware register in the interface hardware. Recovery from the error is usually done by retransmitting the data, the details of which are usually handled by software (e.g., the operating system I/O routines).\n\nWhen the total number of transmitted bits, including the parity bit, is even, odd parity has the advantage that the all-zeros and all-ones patterns are both detected as errors. If the total number of bits is odd, only one of the patterns is detected as an error, and the choice can be made based on which is expected to be the more common error.\n\nParity data is used by some redundant array of independent disks (RAID) levels to achieve redundancy. If a drive in the array fails, remaining data on the other drives can be combined with the parity data (using the Boolean XOR function) to reconstruct the missing data.\n\nFor example, suppose two drives in a three-drive RAID 5 array contained the following data:\n\nDrive 1: 01101101Drive 2: 11010100\n\nTo calculate parity data for the two drives, an XOR is performed on their data:\n\n         01101101XOR 11010100 _____________        10111001\n\nThe resulting parity data, 10111001, is then stored on Drive 3.\n\nShould any of the three drives fail, the contents of the failed drive can be reconstructed on a replacement drive by subjecting the data from the remaining drives to the same XOR operation. If Drive 2 were to fail, its data could be rebuilt using the XOR results of the contents of the two remaining drives, Drive 1 and Drive 3:\n\nDrive 1: 01101101Drive 3: 10111001\n\nas follows:\n\n         10111001XOR 01101101 _____________        11010100\n\nThe result of that XOR calculation yields Drive 2's contents. 11010100 is then stored on Drive 2, fully repairing the array. This same XOR concept applies similarly to larger arrays, using any number of disks. In the case of a RAID 3 array of 12 drives, 11 drives participate in the XOR calculation shown above and yield a value that is then stored on the dedicated parity drive.\n\nA \"parity track\" was present on the first magnetic tape data storage in 1951. Parity in this form, applied across multiple parallel signals, is known as a transverse redundancy check. This can be combined with parity computed over multiple bits sent on a single signal, a longitudinal redundancy check. In a parallel bus, there is one longitudinal redundancy check bit per parallel signal.\n\nParity was also used on at least some paper-tape (punched tape) data entry systems (which preceded magnetic tape systems). On the systems sold by British company ICL (formerly ICT) the paper tape had 8 hole positions running across it, with the 8th being for parity. 7 positions were used for the data, e.g., 7-bit ASCII. The 8th position had a hole punched in it depending on the number of data holes punched.\n\n\n"}
{"id": "45674569", "url": "https://en.wikipedia.org/wiki?curid=45674569", "title": "Petrie dual", "text": "Petrie dual\n\nIn topological graph theory, the Petrie dual of a embedded graph (on a 2-manifold with all faces disks) is another embedded graph that has the Petrie polygons of the first embedding as its faces.\n\nThe Petrie dual is also called the Petrial, and the Petrie dual of an embedded graph formula_1 may be denoted formula_2.\nIt can be obtained from a signed rotation system or ribbon graph representation of the embedding by twisting every edge of the embedding.\n\nLike the usual dual graph, repeating the Petrie dual operation twice returns to the original surface embedding.\nUnlike the usual dual graph (which is an embedding of a generally different graph in the same surface) the Petrie dual is an embedding of the same graph in a generally different surface.\n\nSurface duality and Petrie duality are two of the six Wilson operations, and together generate the group of these operations.\n\nAppying the Petrie dual to a regular polyhedron produces a regular map.\n\nFor example, the Petrie dual of a cube (a bipartite graph with eight vertices and twelve edges, embedded onto a sphere with six square faces)\nhas four hexagonal faces, the equators of the cube. Topologically, it forms an embedding of the same graph onto a torus.\n\nThe regular maps obtained in this way are as follows.\n"}
{"id": "25267", "url": "https://en.wikipedia.org/wiki?curid=25267", "title": "Quantum field theory", "text": "Quantum field theory\n\nIn theoretical physics, quantum field theory (QFT) is a theoretical framework that combines classical field theory, special relativity, and quantum mechanics and is used to construct physical models of subatomic particles (in particle physics) and quasiparticles (in condensed matter physics). \n\nQFT treats particles as excited states (also called quanta) of their underlying fields, which are—in a sense—more fundamental than the basic particles. Interactions between particles are described by interaction terms in the Lagrangian involving their corresponding fields. Each interaction can be visually represented by Feynman diagrams, which are formal computational tools, in the process of relativistic perturbation theory.\n\nAs a successful theoretical framework today, quantum field theory emerged from the work of generations of theoretical physicists spanning much of the 20th century. Its development began in the 1920s with the description of interactions between light and electrons, culminating in the first quantum field theory — quantum electrodynamics. A major theoretical obstacle soon followed with the appearance and persistence of various infinities in perturbative calculations, a problem only resolved in the 1950s with the invention of the renormalization procedure. A second major barrier came with QFT's apparent inability to describe the weak and strong interactions, to the point where some theorists called for the abandonment of the field theoretic approach. The development of gauge theory and the completion of the Standard Model in the 1970s led to a renaissance of quantum field theory.\n\nQuantum field theory is the result of the combination of classical field theory, quantum mechanics, and special relativity. A brief overview of these theoretical precursors is in order.\n\nThe earliest successful classical field theory is one that emerged from Newton's law of universal gravitation, despite the complete absence of the concept of fields from his 1687 treatise \"Philosophiæ Naturalis Principia Mathematica\". The force of gravity as described by Newton is an \"action at a distance\" — its effects on faraway objects are instantaneous, no matter the distance. In an exchange of letters with Richard Bentley, however, Newton stated that \"it is inconceivable that inanimate brute matter should, without the mediation of something else which is not material, operate upon and affect other matter without mutual contact.\" It was not until the 18th century that mathematical physicists discovered a convenient description of gravity based on fields — a numerical quantity (a vector (mathematics and physics)) assigned to every point in space indicating the action of gravity on any particle at that point. However, this was considered merely a mathematical trick.\n\nFields began to take on an existence of their own with the development of electromagnetism in the 19th century. Michael Faraday coined the English term \"field\" in 1845. He introduced fields as properties of space (even when it is devoid of matter) having physical effects. He argued against \"action at a distance\", and proposed that interactions between objects occur via space-filling \"lines of force\". This description of fields remains to this day.\n\nThe theory of classical electromagnetism was completed in 1862 with Maxwell's equations, which described the relationship between the electric field, the magnetic field, electric current, and electric charge. Maxwell's equations implied the existence of electromagnetic waves, a phenomenon whereby electric and magnetic fields propagate from one spatial point to another at a finite speed, which turns out to be the speed of light. Action-at-a-distance was thus conclusively refuted. \n\nDespite the enormous success of classical electromagnetism, it was unable to account for the discrete lines in atomic spectra, nor for the distribution of blackbody radiation in different wavelengths. Max Planck's study of blackbody radiation marked the beginning of quantum mechanics. He treated atoms, which absorb and emit electromagnetic radiation, as tiny oscillators with the crucial property that their energies can only take on a series of discrete, rather than continuous, values. These are known as quantum harmonic oscillators. This process of restricting energies to discrete values is called quantization. Building on this idea, Albert Einstein proposed in 1905 an explanation for the photoelectric effect, that light is composed of individual packets of energy called photons (the quanta of light). This implied that the electromagnetic radiation, while being waves in the classical electromagnetic field, also exists in the form of particles.\n\nIn 1913, Niels Bohr introduced the Bohr model of atomic structure, wherein electrons within atoms can only take on a series of discrete, rather than continuous, energies. This is another example of quantization. The Bohr model successfully explained the discrete nature of atomic spectral lines. In 1924, Louis de Broglie proposed the hypothesis of wave-particle duality, that microscopic particles exhibit both wave-like and particle-like properties under different circumstances. Uniting these scattered ideas, a coherent discipline, quantum mechanics, was formulated between 1925 and 1926, with important contributions from de Broglie, Werner Heisenberg, Max Born, Erwin Schrödinger, Paul Dirac, and Wolfgang Pauli.\n\nIn the same year as his paper on the photoelectric effect, Einstein published his theory of special relativity, built on Maxwell's electromagnetism. New rules, called Lorentz transformation, were given for the way time and space coordinates of an event change under changes in the observer's velocity, and the distinction between time and space was blurred. It was proposed that all physical laws must be the same for observers at different velocities, \"i.e.\" that physical laws be invariant under Lorentz transformations.\n\nTwo difficulties remained. Observationally, the Schrödinger equation underlying quantum mechanics could explain the stimulated emission of radiation from atoms, where an electron emits a new photon under the action of an external electromagnetic field, but it was unable to explain spontaneous emission, where an electron spontaneously decreases in energy and emits a photon even without the action of an external electromagnetic field. Theoretically, the Schrödinger equation could not describe photons and was inconsistent with the principles of special relativity — it treats time as an ordinary number while promoting spatial coordinates to linear operators.\n\nQuantum field theory naturally began with the study of electromagnetic interactions, as the electromagnetic field was the only known classical field as of the 1920s.\n\nThrough the works of Born, Heisenberg, and Pascual Jordan in 1925-1926, a quantum theory of the free electromagnetic field (one with no interactions with matter) was developed via canonical quantization by treating the electromagnetic field as a set of quantum harmonic oscillators. With the exclusion of interactions, however, such a theory was yet incapable of making quantitative predictions about the real world.\n\nIn his seminal 1927 paper \"The quantum theory of the emission and absorption of radiation\", Dirac coined the term quantum electrodynamics (QED), a theory that adds upon the terms describing the free electromagnetic field an additional interaction term between electric current density and the electromagnetic vector potential. Using first-order perturbation theory, he successfully explained the phenomenon of spontaneous emission. According to the uncertainty principle in quantum mechanics, quantum harmonic oscillators cannot remain stationary, but they have a non-zero minimum energy and must always be oscillating, even in the lowest energy state (the ground state). Therefore, even in a perfect vacuum, there remains an oscillating electromagnetic field having zero-point energy. It is this quantum fluctuation of electromagnetic fields in the vacuum that \"stimulates\" the spontaneous emission of radiation by electrons in atoms. Dirac's theory was hugely successful in explaining both the emission and absorption of radiation by atoms; by applying second-order perturbation theory, it was able to account for the scattering of photons, resonance fluorescence, as well as non-relativistic Compton scattering. Nonetheless, the application of higher-order perturbation theory was plagued with problematic infinities in calculations.\n\nIn 1928, Dirac wrote down a wave equation that described relativistic electrons — the Dirac equation. It had the following important consequences: the spin of an electron is 1/2; the electron \"g\"-factor is 2; it led to the correct Sommerfeld formula for the fine structure of the hydrogen atom; and it could be used to derive the Klein-Nishina formula for relativistic Compton scattering. Although the results were fruitful, the theory also apparently implied the existence of negative energy states, which would cause atoms to be unstable, since they could always decay to lower energy states by the emission of radiation.\n\nThe prevailing view at the time was that the world was composed of two very different ingredients: material particles (such as electrons) and quantum fields (such as photons). Material particles were considered to be eternal, with their physical state described by the probabilities of finding each particle in any given region of space or range of velocities. On the other hand photons were considered merely the excited states of the underlying quantized electromagnetic field, and could be freely created or destroyed. It was between 1928 and 1930 that Jordan, Eugene Wigner, Heisenberg, Pauli, and Enrico Fermi discovered that material particles could also be seen as excited states of quantum fields. Just as photons are excited states of the quantized electromagnetic field, so each type of particle had its corresponding quantum field: an electron field, a proton field, etc. Given enough energy, it would now be possible to create material particles. Building on this idea, Fermi proposed in 1932 an explanation for \"β\" decay known as Fermi's interaction. Atomic nuclei do not contain electrons \"per se\", but in the process of decay, an electron is created out of the surrounding electron field, analogous to the photon created from the surrounding electromagnetic field in the radiative decay of an excited atom.\n\nIt was realized in 1929 by Dirac and others that negative energy states implied by the Dirac equation could be removed by assuming the existence of particles with the same mass as electrons but opposite electric charge. This not only ensured the stability of atoms, but it was also the first proposal of the existence of antimatter. Indeed, the evidence for positrons was discovered in 1932 by Carl David Anderson in cosmic rays. With enough energy, such as by absorbing a photon, an electron-positron pair could be created, a process called pair production; the reverse process, annihilation, could also occur with the emission of a photon. This showed that particle numbers need not be fixed during an interaction. Historically, however, positrons were at first thought of as \"holes\" in an infinite electron sea, rather than a new kind of particle, and this theory was referred to as the Dirac hole theory. QFT naturally incorporated antiparticles in its formalism.\n\nRobert Oppenheimer showed in 1930 that higher-order perturbative calculations in QED always resulted in infinite quantities, such as the electron self-energy and the vacuum zero-point energy of the electron and photon fields, suggesting that the computational methods at the time could not properly deal with interactions involving photons with extremely high momenta. It was not until 20 years later that a systematic approach to remove such infinities was developed.\n\nA series of papers were published between 1934 and 1938 by Ernst Stueckelberg that established a relativistically invariant formulation of QFT. In 1947, Stueckelberg also independently developed a complete renormalization procedure. Unfortunately, such achievements were not understood and recognized by the theoretical community.\n\nFaced with these infinities, John Archibald Wheeler and Heisenberg proposed, in 1937 and 1943 respectively, to supplant the problematic QFT with the so-called S-matrix theory. Since the specific details of microscopic interactions are inaccessible to observations, the theory should only attempt to describe the relationships between a small number of observables (\"e.g.\" the energy of an atom) in an interaction, rather than be concerned with the microscopic minutiae of the interaction. In 1945, Richard Feynman and Wheeler daringly suggested abandoning QFT altogether and proposed action-at-a-distance as the mechanism of particle interactions.\n\nIn 1947, Willis Lamb and Robert Retherford measured the minute difference in the \"S\" and \"P\" energy levels of the hydrogen atom, also called the Lamb shift. By ignoring the contribution of photons whose energy exceeds the electron mass, Hans Bethe successfully estimated the numerical value of the Lamb shift. Subsequently, Norman Myles Kroll, Lamb, James Bruce French, and Victor Weisskopf again confirmed this value using an approach in which infinities cancelled other infinities to result in finite quantities. However, this method was clumsy and unreliable and could not be generalized to other calculations.\n\nThe breakthrough eventually came around 1950 when a more robust method for eliminating infinities was developed Julian Schwinger, Feynman, Freeman Dyson, and Shinichiro Tomonaga. The main idea is to replace the initial, so-called \"bare\", parameters (mass, electric charge, etc.), which have no physical meaning, by their finite measured values. To cancel the apparently infinite parameters, one has to introduce additional, infinite, \"counterterms\" into the Lagrangian. This systematic computational procedure is known as renormalization and can be applied to arbitrary order in perturbation theory.\n\nBy applying the renormalization procedure, calculations were finally made to explain the electron's anomalous magnetic moment (the deviation of the electron \"g\"-factor from 2) and vacuum polarisation. These results agreed with experimental measurements to a remarkable degree, thus marking the end of a \"war against infinities\".\n\nAt the same time, Feynman introduced the path integral formulation of quantum mechanics and Feynman diagrams. The latter can be used to visually and intuitively organise and to help compute terms in the perturbative expansion. Each diagram can be interpreted as paths of particles in an interaction, with each vertex and line having a corresponding mathematical expression, and the product of these expressions gives the scattering amplitude of the interaction represented by the diagram.\n\nIt was with the invention of the renormalization procedure and Feynman diagrams that QFT finally arose as a complete theoretical framework.\n\nGiven the tremendous success of QED, many theorists believed, in the few years after 1949, that QFT could soon provide an understanding of all microscopic phenomena, not only the interactions between photons, electrons, and positrons. Contrary to this optimism, QFT entered yet another period of depression that lasted for almost two decades.\n\nThe first obstacle was the limited applicability of the renormalization procedure. In perturbative calculations in QED, all infinite quantities could be eliminated by redefining a small (finite) number of physical quantities (namely the mass and charge of the electron). Dyson proved in 1949 that this is only possible for a small class of theories called \"renormalizable theories\", of which QED is an example. However, most theories, including the Fermi theory of the weak interaction, are \"non-renormalizable\". Any perturbative calculation in these theories beyond the first order would result in infinities that could not be removed by redefining a finite number of physical quantities.\n\nThe second major problem stemmed from the limited validity of the Feynman diagram method, which are based on a series expansion in perturbation theory. In order for the series to converge and low-order calculations to be a good approximation, the coupling constant, in which the series is expanded, must be a sufficiently small number. The coupling constant in QED is the fine-structure constant , which is small enough that only the simplest, lowest order, Feynman diagrams need to be considered in realistic calculations. In contrast, the coupling constant in the strong interaction is roughly of the order of one, making complicated, higher order, Feynman diagrams just as important as simple ones. There was thus no way of deriving reliable quantitative predictions for the strong interaction using perturbative QFT methods.\n\nWith these difficulties looming, many theorists began to turn away from QFT. Some focused on symmetry principles and conservation laws, while others picked up the old S-matrix theory of Wheeler and Heisenberg. QFT was used heuristically as guiding principles, but not as a basis for quantitative calculations.\n\nIn 1954, Yang Chen-Ning and Robert Mills generalised the local symmetry of QED, leading to non-Abelian gauge theories (also known as Yang-Mills theories), which are based on more complicated local symmetry groups. In QED, (electrically) charged particles interact via the exchange of photons, while in non-Abelian gauge theory, particles carrying a new type of \"charge\" interact via the exchange of massless gauge bosons. Unlike photons, these gauge bosons themselves carry charge.\n\nSheldon Glashow developed a non-Abelian gauge theory that unified the electromagnetic and weak interactions in 1960. In 1964, Abdul Salam and John Clive Ward arrived at the same theory through a different path. This theory, nevertheless, was non-renormalizable.\n\nPeter Higgs, Robert Brout, and François Englert proposed in 1964 that the gauge symmetry in Yang-Mills theories could be broken by a mechanism called spontaneous symmetry breaking, through which originally massless gauge bosons could acquire mass.\n\nBy combining the earlier theory of Glashow, Salam, and Ward with the idea of spontaneous symmetry breaking, Steven Weinberg wrote down in 1967 a theory describing electroweak interactions between all leptons and the effects of the Higgs boson. His theory was at first mostly ignored, until it was brought back to light in 1971 by Gerard 't Hooft's proof that non-Abelian gauge theories are renormalizable. The electroweak theory of Weinberg and Salam was extended from leptons to quarks in 1970 by Glashow, John Iliopoulos, and Luciano Maiani, marking its completion.\n\nHarald Fritzsch, Murray Gell-Mann, and Heinrich Leutwyler discovered in 1971 that certain phenomena involving the strong interaction could also be explained by non-Abelian gauge theory. Quantum chromodynamics (QCD) was born. In 1973, David Gross, Frank Wilczek, and Hugh David Politzer showed that non-Abelian gauge theories are \"asymptotically free\", meaning that under renormalization, the coupling constant of the strong interaction decreases as the interaction energy increases. (Similar discoveries had been made numerous times prior, but they had been largely ignored.) Therefore, at least in high-energy interactions, the coupling constant in QCD becomes sufficiently small to warrant a perturbative series expansion, making quantitative predictions for the strong interaction possible.\n\nThese theoretical breakthroughs brought about a renaissance in QFT. The full theory, which includes the electroweak theory and chromodynamics, is referred to today as the Standard Model of elementary particles. The Standard Model successfully describes all fundamental interactions except gravity, and its many predictions have been met with remarkable experimental confirmation in subsequent decades. The Higgs boson, central to the mechanism of spontaneous symmetry breaking, was finally detected in 2012 at CERN, marking the complete verification of the existence of all constituents of the Standard Model.\n\nThe 1970s saw the development of non-perturbative methods in non-Abelian gauge theories. The 't Hooft-Polyakov monopole was discovered by 't Hooft and Alexander Polyakov, flux tubes by Holger Bech Nielsen and Poul Olesen, and instantons by Polyakov \"et al.\". These objects are inaccessible through perturbation theory.\n\nSupersymmetry also appeared in the same period. The first supersymmetric QFT in four dimensions was built by Yuri Golfand and Evgeny Likhtman in 1970, but their result failed to garner widespread interest due to the Iron Curtain. Supersymmetry only took off in the theoretical community after the work of Julius Wess and Bruno Zumino in 1973.\n\nAmong the four fundamental interactions, gravity remains the only one that lacks a consistent QFT description. Various attempts at a theory of quantum gravity led to the development of string theory, itself a type of two-dimensional QFT with conformal symmetry. Joël Scherk and John Schwarz first proposed in 1974 that string theory could be \"the\" quantum theory of gravity.\n\nAlthough quantum field theory arose from the study of interactions between elementary particles, it has been successfully applied to other physical systems, particularly to many-body systems in condensed matter physics.\n\nHistorically, the Higgs mechanism of spontaneous symmetry breaking was a result of Yoichiro Nambu's application of superconductor theory to elementary particles, while the concept of renormalization came out of the study of second-order phase transitions in matter.\n\nSoon after the introduction of photons, Einstein performed the quantization procedure on vibrations in a crystal, leading to the first quasiparticle — phonons. Lev Landau claimed that low-energy excitations in many condensed matter systems could be described in terms of interactions between a set of quasiparticles. The Feynman diagram method of QFT was naturally well suited to the analysis of various phenomena in condensed matter systems.\n\nGauge theory is used to describe the quantization of magnetic flux in superconductors, the resistivity in the quantum Hall effect, as well as the relation between frequency and voltage in the AC Josephson effect.\n\nFor simplicity, natural units are used in the following sections, in which the reduced Planck constant and the speed of light are both set to one.\n\nA classical field is a function of spatial and time coordinates. Examples include the gravitational field in Newtonian gravity and the electric field and magnetic field in classical electromagnetism. A classical field can be thought of as a numerical quantity assigned to every point in space that changes in time. Hence, it has infinite degrees of freedom.\n\nMany phenomena exhibiting quantum mechanical properties cannot be explained by classical fields alone. Phenomena such as the photoelectric effect are best explained by discrete particles (photons), rather than a spatially continuous field. The goal of quantum field theory is to describe various quantum mechanical phenomena using a modified concept of fields.\n\nCanonical quantisation and path integrals are two common formulations of QFT. To motivate the fundamentals of QFT, an overview of classical field theory is in order.\n\nThe simplest classical field is a real scalar field — a real number at every point in space that changes in time. It is denoted as , where is the position vector, and is the time. Suppose the Lagrangian of the field is\nwhere formula_2 is the time-derivative of the field, is the divergence operator, and is a real parameter (the \"mass\" of the field). Applying the Euler–Lagrange equation on the Lagrangian:\nwe obtain the equations of motion for the field, which describe the way it varies in time and space:\nThis is known as the Klein–Gordon equation.\n\nThe Klein–Gordon equation is a wave equation, so its solutions can be expressed as a sum of normal modes (obtained via Fourier transform) as follows:\nwhere is a complex number (normalised by convention), denotes complex conjugation, and is the frequency of the normal mode:\nwhere , and denotes the covariant derivative. The Lagrangian of a QFT, hence its calculational results and physical predictions, depends on the geometry of the spacetime background.\n\nThe correlation functions and physical predictions of a QFT depend on the spacetime metric . For a special class of QFTs called topological quantum field theories (TQFTs), all correlation functions are independent of continuous changes in the spacetime metric. QFTs in curved spacetime generally change according to the \"geometry\" (local structure) of the spacetime background, while TQFTs are invariant under spacetime diffeomorphisms but are sensitive to the \"topology\" (global structure) of spacetime. This means that all calculational results of TQFTs are topological invariants of the underlying spacetime. Chern–Simons theory is an example of TQFT. Applications of TQFT include the fractional quantum Hall effect and topological quantum computers.\n\nUsing perturbation theory, the total effect of a small interaction term can be approximated order by order by a series expansion in the number of virtual particles participating in the interaction. Every term in the expansion may be understood as one possible way for (physical) particles to interact with each other via virtual particles, expressed visually using a Feynman diagram. The electromagnetic force between two electrons in QED is represented (to first order in perturbation theory) by the propagation of a virtual photon. In a similar manner, the W and Z bosons carry the weak interaction, while gluons carry the strong interaction. The interpretation of an interaction as a sum of intermediate states involving the exchange of various virtual particles only makes sense in the framework of perturbation theory. In contrast, non-perturbative methods in QFT treat the interacting Lagrangian as a whole without any series expansion. Instead of particles that carry interactions, these methods have spawned such concepts as 't Hooft–Polyakov monopole, domain wall, flux tube, and instanton.\n\nIn spite of its overwhelming success in particle physics and condensed matter physics, QFT itself lacks a formal mathematical foundation. For example, according to Haag's theorem, there does not exist a well-defined interaction picture for QFT, which implies that perturbation theory of QFT, which underlies the entire Feynman diagram method, is fundamentally not rigorous.\n\nSince the 1950s, theoretical physicists and mathematicians have attempted to organise all QFTs into a set of axioms, in order to establish the existence of concrete models of relativistic QFT in a mathematically rigorous way and to study their properties. This line of study is called constructive quantum field theory, a subfield of mathematical physics, which has led to such results as CPT theorem, spin-statistics theorem, and Goldstone's theorem.\n\nCompared to ordinary QFT, topological quantum field theory and conformal field theory are better supported mathematically — both can be classified in the framework of representations of cobordisms.\n\nAlgebraic quantum field theory is another approach to the axiomatisation of QFT, in which the fundamental objects are local operators and the algebraic relations between them. Axiomatic systems following this approach include Wightman axioms and Haag-Kastler axioms. One way to construct theories satisfying Wightman axioms is to use Osterwalder-Schrader axioms, which give the necessary and sufficient conditions for a real time theory to be obtained from an imaginary time theory by analytic continuation (Wick rotation).\n\nYang-Mills existence and mass gap, one of the Millenium Prize Problems, concerns the well-defined existence of Yang-Mills theories as set out by the above axioms. The full problem statement is as follows.\n\n\n\n\n"}
{"id": "849412", "url": "https://en.wikipedia.org/wiki?curid=849412", "title": "Ramanujan graph", "text": "Ramanujan graph\n\nIn spectral graph theory, a Ramanujan graph, named after Srinivasa Ramanujan, is a regular graph whose spectral gap is almost as large as possible (see extremal graph theory). Such graphs are excellent spectral expanders.\n\nExamples of Ramanujan graphs include the clique, the biclique formula_1, and the Petersen graph. As Murty's survey paper notes, Ramanujan graphs \"fuse diverse branches of pure mathematics, namely, number theory, representation theory, and algebraic geometry\". As observed by Toshikazu Sunada, a regular graph is Ramanujan if and only if its Ihara zeta function satisfies an analog of the Riemann hypothesis.\n\nLet formula_2 be a connected formula_3-regular graph with formula_4 vertices, and let formula_5 be the eigenvalues of the adjacency matrix of formula_2. Because formula_2 is connected and formula_3-regular, its eigenvalues satisfy formula_9 formula_10. Whenever there exists formula_11 with formula_12, define\n\nA formula_3-regular graph formula_2 is a Ramanujan graph if formula_16.\n\nA Ramanujan graph is characterized as a regular graph whose Ihara zeta function satisfies an analogue of the Riemann Hypothesis.\n\nFor a fixed formula_3 and large formula_4, the formula_3-regular, formula_4-vertex Ramanujan graphs nearly minimize formula_21. If formula_2 is a formula_3-regular graph with diameter formula_24, a theorem due to Noga Alon states\n\nWhenever formula_2 is formula_3-regular and connected on at least three vertices, formula_28, and therefore formula_29. Let formula_30 be the set of all connected formula_3-regular graphs formula_2 with at least formula_4 vertices. Because the minimum diameter of graphs in formula_30 approaches infinity for fixed formula_3 and increasing formula_4, this theorem implies an earlier theorem of Alon and Boppana which states\n\nA slightly stronger bound is\n\nwhere formula_39. The outline of Friedman's proof is the following. Take formula_40. Let formula_41 be the formula_3-regular tree of height formula_43 and let formula_44 be its adjacency matrix. We want to prove that formula_45, for some formula_46 depending only on formula_24. Define a function formula_48 by formula_49, where formula_50 is the distance from formula_51 to the root of formula_41. Choosing a formula_46 close to formula_54 it can be proved that formula_55. Now let formula_56 and formula_57 be a pair of vertices at distance formula_24 and define\n\nwhere formula_60 is a vertex in formula_41 which distance to the root is equal to the distance from formula_62 to formula_56 and the symmetric for formula_64. By choosing properly formula_65 we get formula_66, formula_67 for formula_62 close to formula_56 and formula_70 for formula_62 close to formula_57. Then by the min-max theorem formula_73.\n\nConstructions of Ramanujan graphs are often algebraic.\n\n\n\n"}
{"id": "25657", "url": "https://en.wikipedia.org/wiki?curid=25657", "title": "Roman numerals", "text": "Roman numerals\n\nThe numeric system represented by Roman numerals originated in ancient Rome and remained the usual way of writing numbers throughout Europe well into the Late Middle Ages. Numbers in this system are represented by combinations of letters from the Latin alphabet. Roman numerals, as used today, employ seven symbols, each with a fixed integer value, as follows:\n\nThe use of Roman numerals continued long after the decline of the Roman Empire. From the 14th century on, Roman numerals began to be replaced in most contexts by the more convenient Arabic numerals; however, this process was gradual, and the use of Roman numerals persists in some minor applications to this day.\n\nThe original pattern for Roman numerals used the symbols I, V, and X (1, 5, and 10) as simple tally marks. Each marker for 1 () added a unit value up to 5 (), and was then added to () to make the numbers from 6 to 9:\nThe numerals for 4 () and 9 () proved problematic (among other things, they are easily confused with and ), and are generally replaced with (one less than 5) and (one less than 10). This feature of Roman numerals is called subtractive notation.\n\nThe numbers from 1 to 10 (including subtractive notation for 4 and 9) are expressed in Roman numerals as follows:\nThe system being basically decimal, \"tens and hundreds follow the same pattern\":\n\nThus 10 to 100 (counting in tens, with taking the place of , taking the place of and taking the place of ):\nNote that 40 () and 90 () follow the same subtractive pattern as 4 and 9.\n\nSimilarly, 100 to 1000 (counting in hundreds):\nAgain - 400 () and 900 () follow the standard subtractive pattern.\n\nIn the absence of a standard symbols for 5,000 and 10,000 the pattern breaks down at this point - in modern usage is repeated up to three times. The Romans had several different methods for indicating larger numbers, but for practical purposes Roman Numerals for numbers larger than 3,000 are seldom if ever used nowadays, and this suffices.\n\nMany numbers include hundreds, units and tens. The Roman numeral system being basically decimal, each power of ten is added in descending sequence from left to right, as with Arabic numerals. For example:\n\n\nAs each power of ten (or \"place\") has its own notation there is no need for place keeping zeros, so \"missing places\" are ignored, as in Latin (and English) speech, thus:\n\n\nRoman numerals for large numbers are nowadays seen mainly in the form of year numbers (other uses are detailed later in this article), as in these examples:\n\nThe \"standard\" forms described above reflect typical modern usage rather than an unchanging and universally accepted convention. Usage in ancient Rome varied greatly and remained inconsistent in medieval times. There is still no official \"binding\" standard, which makes the elaborate \"rules\" used in some sources to distinguish between \"correct\" and \"incorrect\" forms highly problematic\n\n\nAlthough Roman numerals came to be written with letters of the Roman alphabet, they were originally independent symbols. The Etruscans, for example, used , , , , , and for , , , , , and , of which only and happened to be letters in their alphabet.\n\nOne hypothesis is that the Etrusco-Roman numerals actually derive from notches on tally sticks, which continued to be used by Italian and Dalmatian shepherds into the 19th century.\n\nThus, descends not from the letter but from a notch scored across the stick. Every fifth notch was double cut i.e. , , , , \"etc.\"), and every tenth was cross cut (), , much like European tally marks today. This produced a positional system: \"Eight\" on a counting stick was eight tallies, , or the eighth of a longer series of tallies; either way, it could be abbreviated (or ), as the existence of a implies four prior notches. By extension, \"eighteen\" was the eighth tally after the first ten, which could be abbreviated , and so was . Likewise, number \"four\" on the stick was the I-notch that could be felt just before the cut of the (), so it could be written as either or (). Thus the system was neither additive nor subtractive in its conception, but \"ordinal\". When the tallies were transferred to writing, the marks were easily identified with the existing Roman letters , and .\nThe tenth or along the stick received an extra stroke. Thus 50 was written variously as , , , , , \"etc.\", but perhaps most often as a chicken-track shape like a superimposed and : . This had flattened to (an inverted T) by the time of Augustus, and soon thereafter became identified with the graphically similar letter . Likewise, 100 was variously , , , , or as any of the symbols for 50 above plus an extra stroke. The form (that is, a superimposed and ) came to predominate. It was written variously as or , was then abbreviated to or , with variant finally winning out because, as a letter, it stood for \"centum\", Latin for \"hundred\".\n\nThe hundredth or was marked with a box or circle. Thus 500 was like a superimposed on a or — that is, like a with a cross bar,— becoming or by the time of Augustus, under the graphic influence of the letter . It was later identified \"as\" the letter D; an alternative symbol for \"thousand\" was a bracketed (or ), and half of a thousand or \"five hundred\" is the right half of the symbol, (or ), and this may have been converted into . This at least was the etymology given to it later on.\n\nMeanwhile, 1000 was a circled or boxed : , , , and by Augustinian times was partially identified with the Greek letter \"phi\". Over time, the symbol changed to and . The latter symbol further evolved into , then , and eventually changed to under the influence of the Latin word \"mille\" \"thousand\".\n\nAlfred Hooper has an alternative hypothesis for the origin of the Roman numeral system, for small numbers. Hooper contends that the digits are related to hand gestures for counting. For example, the numbers , , , correspond to the number of fingers held up for another to see. , then represents that hand upright with fingers together and thumb apart. Numbers 6–10, are represented with two hands as follows (left hand, right hand) 6=(,), 7=(,), 8=(,), 9=(,), 10=(,) and results from either crossing of the thumbs, or holding both hands up in a cross.\n\nAnother possibility is that each represents a finger and represents the thumb of one hand. This way the numbers between 1–10 can be counted on one hand using the order: =P, =PR, =PRM, =IT, =T, =TP, =TPR, =TPRM, =IN, =N (P=pinky, R=ring, M=middle, I=index, T=thumb N=no fingers/other hand). This pattern can also be continued using the other hand with the fingers representing and the thumb .\n\nA third hypothesis about the origins states that the basic ciphers were , , and (or ) and that the intermediary ones were derived from taking half of those (half an is , half a is and half a is ). The was later replaced with , the initial of \"mille\" (the Latin word for \"thousand\").\n\nLower case, minuscule, letters were developed in the Middle Ages, well after the demise of the Western Roman Empire, and since that time lower-case versions of Roman numbers have also been commonly used: , , , , and so on.\n\nSince the Middle Ages, a \" has sometimes been substituted for the final \" of a \"lower-case\" Roman numeral, such as \" for 3 or \" for 7. This \" can be considered a swash variant of \". The use of a final \"\" is still used in medical prescriptions to prevent tampering with or misinterpretation of a number after it is written.\n\nNumerals in documents and inscriptions from the Middle Ages sometimes include additional symbols, which today are called \"medieval Roman numerals\". Some simply substitute another letter for the standard one (such as \" for \", or \" for \"), while others serve as abbreviations for compound numerals (\" for \", or \" for \"). Although they are still listed today in some dictionaries, they are long out of use.\n\nChronograms, messages with dates encoded into them, were popular during the Renaissance era. The chronogram would be a phrase containing the letters , , , , , , and . By putting these letters together, the reader would obtain a number, usually indicating a particular year.\n\nBy the 11th century, Arabic numerals had been introduced into Europe from al-Andalus, by way of Arab traders and arithmetic treatises. Roman numerals, however, proved very persistent, remaining in common use in the West well into the 14th and 15th centuries, even in accounting and other business records (where the actual calculations would have been made using an abacus). Replacement by their more convenient \"Arabic\" equivalents was quite gradual, and Roman numerals are still used today in certain contexts. A few examples of their current use are:\n\nIn astronomy, the natural satellites or \"moons\" of the planets are traditionally designated by capital Roman numerals appended to the planet's name. For example, Titan's designation is Saturn .\n\nIn chemistry, Roman numerals are often used to denote the groups of the periodic table.\nThey are also used in the IUPAC nomenclature of inorganic chemistry, for the oxidation number of cations which can take on several different positive charges. They are also used for naming phases of polymorphic crystals, such as ice.\n\nIn education, school grades (in the sense of year-groups rather than test scores) are sometimes referred to by a Roman numeral; for example, \"grade IX\" is sometimes seen for \"grade 9\".\n\nIn entomology, the broods of the thirteen and seventeen year periodical cicadas are identified by Roman numerals.\n\nIn advanced mathematics (including trigonometry, statistics, and calculus), when a graph includes negative numbers, its quadrants are named using , , , and . These quadrant names signify positive numbers on both axes, negative numbers on the X axis, negative numbers on both axes, and negative numbers on the Y axis, respectively. The use of Roman numerals to designate quadrants avoids confusion, since Arabic numerals are used for the actual data represented in the graph.\n\nIn military unit designation, Roman numerals are often used to distinguish between units at different levels. This reduces possible confusion, especially when viewing operational or strategic level maps. In particular, army corps are often numbered using Roman numerals (for example the American XVIII Airborne Corps or the WW2-era German III Panzerkorps) with Arabic numerals being used for divisions and armies.\n\nIn music, Roman numerals are used in several contexts:\n\nIn pharmacy, Roman numerals are used in some contexts, including to denote \"one half\" and to mean \"nothing\". (See the sections below on \"zero\" and \"fractions\".)\n\nIn photography, Roman numerals (with zero) are used to denote varying levels of brightness when using the Zone System.\n\nIn seismology, Roman numerals are used to designate degrees of the Mercalli intensity scale of earthquakes.\n\nIn sport the team containing the \"top\" players and representing a club or a school at the highest level in (say) rugby union is often called the \"1st XV\", while a cricket or American football team for younger or less experienced players might be the \"3rd XI\".\n\nIn tarot, Roman numerals (with zero) are used to denote the cards of the Major Arcana.\n\nIn theology and biblical scholarship, the Septuagint is often referred to as \"\", as this translation of the Old Testament into Greek is named for the legendary number of its translators (\"septuaginta\" being Latin for \"seventy\").\n\nSome uses that are rare or never seen in English speaking countries may be relatively common in parts of continental Europe. For instance:\n\nCapital or small capital Roman numerals are widely used in Romance languages to denote , e.g. the French ' and the Spanish ' mean \"18th century\". Slavic languages in and adjacent to Russia similarly favour Roman numerals (). On the other hand, in Slavic languages in Central Europe, like most Germanic languages, one writes \"18.\" (with a period) before the local word for \"century\".\nMixed Roman and Arabic numerals are sometimes used in numeric representations of dates (especially in formal letters and official documents, but also on tombstones). The is written in Roman numerals, while the day is in Arabic numerals: \"14..1789\" and \".14.1789\" both refer unambiguously to 14 June 1789.\nRoman numerals are sometimes used to represent the in hours-of-operation signs displayed in windows or on doors of businesses, and also sometimes in railway and bus timetables. Monday, taken as the first day of the week, is represented by . Sunday is represented by . The hours of operation signs are tables composed of two columns where the left column is the day of the week in Roman numerals and the right column is a range of hours of operation from starting time to closing time. In the example case (left), the business opens from 10 am to 7 pm on weekdays, 10 AM to 5 pm on Saturdays and is closed on Sundays. Note that the listing uses 24-hour time.\nRoman numerals may also be used for floor numbering. For instance, apartments in central Amsterdam are indicated as 138-, with both an Arabic numeral (number of the block or house) and a Roman numeral (floor number). The apartment on the ground floor is indicated as .\n\nIn Italy, where roads outside built-up areas have kilometre signs, major roads and motorways also mark 100-metre subdivisionals, using Roman numerals from to for the smaller intervals. The sign \" | 17\" thus marks kilometre 17.9.\n\nA notable exception to the use of Roman numerals in Europe is in Greece, where Greek numerals (based on the Greek alphabet) are generally used in contexts where Roman numerals would be used elsewhere.\n\nThe number zero does not have its own Roman numeral, but the word \"nulla\" (the Latin word meaning \"none\") was used by medieval scholars in lieu of 0. Dionysius Exiguus was known to use \"nulla\" alongside Roman numerals in 525. About 725, Bede or one of his colleagues used the letter N, the initial of \"nulla\" or of \"nihil\" (the Latin word for \"nothing\"), in a table of epacts, all written in Roman numerals.\n\nThough the Romans used a decimal system for whole numbers, reflecting how they counted in Latin, they used a duodecimal system for fractions, because the divisibility of twelve makes it easier to handle the common fractions of 1/3 and 1/4 than does a system based on ten On coins, many of which had values that were duodecimal fractions of the unit , they used a tally-like notational system based on twelfths and halves. A dot (•) indicated an \"twelfth\", the source of the English words \"inch\" and \"ounce\"; dots were repeated for fractions up to five twelfths. Six twelfths (one half) was abbreviated as the letter \"S\" for \"half\". \"Uncia\" dots were added to \"S\" for fractions from seven to eleven twelfths, just as tallies were added to for whole numbers from six to nine.\n\nEach fraction from 1/12 to 12/12 had a name in Roman times; these corresponded to the names of the related coins:\n\nThe arrangement of the dots was variable and not necessarily linear. Five dots arranged like (⁙) (as on the face of a die) are known as a quincunx, from the name of the Roman fraction/coin. The Latin words ' and ' are the source of the English words \"sextant\" and \"quadrant\".\n\nOther Roman fractional notations included the following:\n\nA number of systems were developed for the expression of larger numbers that cannot be conveniently expressed using the normal seven letter symbols of conventional Roman numerals.\n\nOne of these was the \"apostrophus\", in which 500 (usually written as \") was written as |, while 1,000, was written as | instead of \". This is a system of encasing numbers to denote thousands (imagine the s and s as parentheses), which has its origins in Etruscan numeral usage. The and used to represent 500 and 1,000 were most likely derived from | and |, respectively, and subsequently influenced the adoptions of \" and \" in conventional Roman numerals.\n\nIn this system, an extra denoted 500, and multiple extra s are used to denote 5,000, 50,000, etc. For example:\n\nSometimes | was reduced to ↀ for 1,000. John Wallis is often credited for introducing the symbol for infinity (modern ∞), and one conjecture is that he based it on this usage, since 1,000 was hyperbolically used to represent very large numbers. Similarly, | for 5,000 was reduced to ↁ; | for 10,000 to ↂ; | for 50,000 to ↇ; and | for 100,000 to ↈ.\n\nAnother system is the \"vinculum\", where a conventional Roman numeral is multiplied by 1,000 by adding an overline. Although mathematical historian David Eugene Smith disputes that this was part of ancient Roman usage, the notation was certainly in use in the Middle Ages, and is at least a convenient and \"conventional\" way to express numbers greater than 3,999 in Roman numerals.\n\nFor instance:\n\nAdding \"vertical\" lines (or brackets) before and after the numeral seems to have been used (at least by late medieval times) to multiply a Roman numeral by 10: thus for 10,000 rather than . In combination with the overline the bracketed forms might be used to raise the multiplier to (say) ten (or one hundred) thousand, thus:\nThis needs to be distinguished from the custom of adding both underline and overline to a Roman numeral, simply to make it clear that it \"is\" a number, e.g. .\n\n"}
{"id": "28733878", "url": "https://en.wikipedia.org/wiki?curid=28733878", "title": "Spectrahedron", "text": "Spectrahedron\n\nIn convex geometry, a spectrahedron is a shape that can be represented as a linear matrix inequality. Alternatively, the set of positive semidefinite matrices forms a convex cone in , and a spectrahedron is a shape that can be formed by intersecting this cone with a linear affine subspace.\n\nSpectrahedra are the solution spaces of semidefinite programs. Every spectrahedron is a convex set and also semialgebraic, but the reverse (conjectured to be true until 2017) is false.\n\nAn example of a spectrahedron is the spectraplex, defined as\n\nformula_1\n\nwhere formula_2is the set of positive semidefinite matrices and formula_3 is the trace of the matrix formula_4. The spectraplex is a compact set, and can be thought of as the \"semidefinite\" analog of the simplex.\n"}
{"id": "2292730", "url": "https://en.wikipedia.org/wiki?curid=2292730", "title": "Square principle", "text": "Square principle\n\nIn mathematical set theory, the global square principle is a combinatorial principle introduced by Ronald Jensen in his analysis of the fine structure of the constructible universe L.\n\nDefine Sing to be the class of all limit ordinals which are not regular. \"Global square\" states that there is a system formula_1 satisfying:\n\n\nJensen introduced also a local version of the principle. If \nformula_9 is an uncountable cardinal, \nthen formula_10 asserts that there is a sequence formula_11 satisfying:\n\n\nJensen proved that this principle holds in the constructible universe for any uncountable cardinal κ.\n"}
{"id": "1180641", "url": "https://en.wikipedia.org/wiki?curid=1180641", "title": "Stochastic gradient descent", "text": "Stochastic gradient descent\n\nStochastic gradient descent (often shortened to SGD), also known as incremental gradient descent, is an iterative method for optimizing a differentiable objective function, a stochastic approximation of gradient descent optimization. A recent article implicitly credits Herbert Robbins and Sutton Monro for developing SGD in their 1951 article titled \"A Stochastic Approximation Method\"; see Stochastic approximation for more information. It is called stochastic because samples are selected randomly (or shuffled) instead of as a single group (as in standard gradient descent) or in the order they appear in the training set.\n\nBoth statistical estimation and machine learning consider the problem of minimizing an objective function that has the form of a sum:\nwhere the parameter formula_2 which minimizes formula_3 is to be estimated. Each summand function formula_4 is typically associated with the formula_5-th observation in the data set (used for training).\n\nIn classical statistics, sum-minimization problems arise in least squares and in maximum-likelihood estimation (for independent observations). The general class of estimators that arise as minimizers of sums are called M-estimators. However, in statistics, it has been long recognized that requiring even local minimization is too restrictive for some problems of maximum-likelihood estimation. Therefore, contemporary statistical theorists often consider stationary points of the likelihood function (or zeros of its derivative, the score function, and other estimating equations).\n\nThe sum-minimization problem also arises for empirical risk minimization. In this case, formula_6 is the value of the loss function at formula_5-th example, and formula_3 is the empirical risk.\n\nWhen used to minimize the above function, a standard (or \"batch\") gradient descent method would perform the following iterations :\nwhere formula_10 is a step size (sometimes called the \"learning rate\" in machine learning).\n\nIn many cases, the summand functions have a simple form that enables inexpensive evaluations of the sum-function and the sum gradient. For example, in statistics, one-parameter exponential families allow economical function-evaluations and gradient-evaluations.\n\nHowever, in other cases, evaluating the sum-gradient may require expensive evaluations of the gradients from all summand functions. When the training set is enormous and no simple formulas exist, evaluating the sums of gradients becomes very expensive, because evaluating the gradient requires evaluating all the summand functions' gradients. To economize on the computational cost at every iteration, stochastic gradient descent samples a subset of summand functions at every step. This is very\neffective in the case of large-scale machine learning problems.\n\nIn stochastic (or \"on-line\") gradient descent, the true gradient of formula_3 is approximated by a gradient at a single example:\nAs the algorithm sweeps through the training set, it performs the above update for each training example. Several passes can be made over the training set until the algorithm converges. If this is done, the data can be shuffled for each pass to prevent cycles. Typical implementations may use an adaptive learning rate so that the algorithm converges.\n\nIn pseudocode, stochastic gradient descent can be presented as follows:\nA compromise between computing the true gradient and the gradient at a single example is to compute the gradient against more than one training example (called a \"mini-batch\") at each step. This can perform significantly better than \"true\" stochastic gradient descent described, because the code can make use of vectorization libraries rather than computing each step separately. It may also result in smoother convergence, as the gradient computed at each step is averaged over more training examples.\n\nThe convergence of stochastic gradient descent has been analyzed using the theories of convex minimization and of stochastic approximation. Briefly, when the learning rates formula_10 decrease with an appropriate rate,\nand subject to relatively mild assumptions, stochastic gradient descent converges almost surely to a global minimum \nwhen the objective function is convex or pseudoconvex, \nand otherwise converges almost surely to a local minimum.\nThis is in fact a consequence of the Robbins-Siegmund theorem.\n\nLet's suppose we want to fit a straight line formula_14 to a training set with observationsformula_15 and corresponding estimated responses formula_16 using least squares. The objective function to be minimized is:\n\nThe last line in the above pseudocode for this specific problem will become:\n\nThe key difference compared to standard (Batch) Gradient Descent is that only one piece of data from the dataset is used to calculate the step, and the piece of data is picked randomly at each step.\n\nStochastic gradient descent is a popular algorithm for training a wide range of models in machine learning, including (linear) support vector machines, logistic regression (see, e.g., Vowpal Wabbit) and graphical models. When combined with the backpropagation algorithm, it is the \"de facto\" standard algorithm for training artificial neural networks. Its use has been also reported in the Geophysics community, specifically to applications of Full Waveform Inversion (FWI).\n\nStochastic gradient descent competes with the L-BFGS algorithm, which is also widely used. Stochastic gradient descent has been used since at least 1960 for training linear regression models, originally under the name ADALINE.\n\nAnother popular stochastic gradient descent algorithm is the least mean squares (LMS) adaptive filter.\n\nMany improvements on the basic stochastic gradient descent algorithm have been proposed and used. In particular, in machine learning, the need to set a learning rate (step size) has been recognized as problematic. Setting this parameter too high can cause the algorithm to diverge; setting it too low makes it slow to converge. A conceptually simple extension of stochastic gradient descent makes the learning rate a decreasing function of the iteration number , giving a \"learning rate schedule\", so that the first iterations cause large changes in the parameters, while the later ones do only fine-tuning. Such schedules have been known since the work of MacQueen on -means clustering.\n\nAs mentioned earlier, classical stochastic gradient descent is generally sensitive to learning rate . Fast convergence requires large learning rates but this may induce numerical instability. The problem can be largely solved by considering \"implicit updates\" whereby the stochastic gradient is evaluated at the next iterate rather than the current one:\n\nThis equation is implicit since formula_20 appears on both sides of the equation. It is a stochastic form of the proximal gradient method since the update\ncan also be written as:\n\nAs an example, \nconsider least squares with features formula_22 and observations\nformula_23. We wish to solve:\nNote that formula_25 could have \"1\" as the first element to include an intercept. Classical stochastic gradient descent proceeds as follows:\n\nwhere formula_5 is uniformly sampled between 1 and formula_28. Although theoretical convergence of this procedure happens under relatively mild assumptions, in practice the procedure can be quite unstable. In particular, when formula_10 is misspecified so that formula_30 has large absolute eigenvalues with high probability, the procedure may diverge numerically within a few iterations. In contrast, \"implicit stochastic gradient descent\" (shortened as ISGD) can be solved in closed-form as:\n\nThis procedure will remain numerically stable virtually for all formula_10 as the learning rate is now normalized. Such comparison between classical and implicit stochastic gradient descent in the least squares problem is very similar to the comparison between least mean squares (LMS) and \nnormalized least mean squares filter (NLMS).\n\nEven though a closed-form solution for ISGD is only possible in least squares, the procedure can be efficiently implemented in a wide range of models. Specifically, suppose that formula_6 depends on formula_2 only through a linear combination with features formula_35, so that we can write formula_36, where \nformula_37 may depend on formula_38 as well but not on formula_2 except through formula_40. Least squares obeys this rule, and so does logistic regression, and most generalized linear models. For instance, in least squares, formula_41, and in logistic regression formula_42, where formula_43 is the logistic function. In Poisson regression, formula_44, and so on.\n\nIn such settings, ISGD is simply implemented as follows:\n\nThe scaling factor formula_46 can be found through bisection method since \nin most regular models, such as the aforementioned generalized linear models, function formula_47 is decreasing, \nand thus the search bounds for formula_48 are \nformula_49, where formula_50.\n\nFurther proposals include the \"momentum method\", which appeared in Rumelhart, Hinton and Williams' seminal paper on backpropagation learning. Stochastic gradient descent with momentum remembers the update at each iteration, and determines the next update as a linear combination of the gradient and the previous update:\nthat leads to:\n\nwhere the parameter formula_2 which minimizes formula_3 is to be estimated, and formula_10 is a step size (sometimes called the \"learning rate\" in machine learning).\n\nThe name momentum stems from an analogy to momentum in physics: the weight vector formula_2, thought of as a particle traveling through parameter space, incurs acceleration from the gradient of the loss (\"force\"). Unlike in classical stochastic gradient descent, it tends to keep traveling in the same direction, preventing oscillations. Momentum has been used successfully by computer scientists in the training of artificial neural networks for several decades.\n\n\"Averaged stochastic gradient descent\", invented independently by Ruppert and Polyak in the late 1980s, is ordinary stochastic gradient descent that records an average of its parameter vector over time. That is, the update is the same as for ordinary stochastic gradient descent, but the algorithm also keeps track of\n\nWhen optimization is done, this averaged parameter vector takes the place of .\n\n\"AdaGrad\" (for adaptive gradient algorithm) is a modified stochastic gradient descent with per-parameter learning rate, first published in 2011. Informally, this increases the learning rate for more sparse parameters and decreases the learning rate for less sparse ones. This strategy often improves convergence performance over standard stochastic gradient descent in settings where data is sparse and sparse parameters are more informative. Examples of such applications include natural language processing and image recognition. It still has a base learning rate , but this is multiplied with the elements of a vector which is the diagonal of the outer product matrix\n\nwhere formula_60, the gradient, at iteration . The diagonal is given by\n\nThis vector is updated after every iteration. The formula for an update is now\n\nor, written as per-parameter updates,\n\nEach gives rise to a scaling factor for the learning rate that applies to a single parameter . Since the denominator in this factor, formula_64 is the \"ℓ\" norm of previous derivatives, extreme parameter updates get dampened, while parameters that get few or small updates receive higher learning rates.\n\nWhile designed for convex problems, AdaGrad has been successfully applied to non-convex optimization.\n\n\"RMSProp\" (for Root Mean Square Propagation) is also a method in which the learning rate is adapted for each of the parameters. The idea is to divide the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight. So, first the running average is calculated in terms of means square,\n\nwhere, formula_66 is the forgetting factor.\n\nAnd the parameters are updated as,\n\nRMSProp has shown excellent adaptation of learning rate in different applications. RMSProp can be seen as a generalization of Rprop and is capable to work with mini-batches as well opposed to only full-batches.\n\n\"Adam\" (short for Adaptive Moment Estimation) is an update to the \"RMSProp\" optimizer. In this optimization algorithm, running averages of both the gradients and the second moments of the gradients are used. Given parameters formula_68 and a loss function formula_69, where formula_70 indexes the current training iteration (indexed at formula_71), Adam's parameter update is given by:\n\nwhere formula_77 is a small scalar used to prevent division by 0, and formula_78 and formula_79 are the forgetting factors for gradients and second moments of gradients, respectively. Squaring and square-rooting is done elementwise.\n\nKalman-based Stochastic Gradient Descent (kSGD) is an online and offline algorithm for learning parameters from statistical problems from quasi-likelihood models, which include linear models, non-linear models, generalized linear models, and neural networks with squared error loss as special cases. For online learning problems, kSGD is a special case of the Kalman Filter for linear regression problems, a special case of the Extended Kalman Filter for non-linear regression problems, and can be viewed as an incremental Gauss-Newton method. Moreover, because of kSGD's relationship to the Kalman Filter and natural gradient descent's relationship to the Kalman Filter, kSGD is a rigorous improvement over the popular natural gradient descent method.\n\nThe benefits of kSGD, in comparison to other methods, are (1) it is not sensitive to the condition number of the problem ,p\\sigma ^{2}</math> with probability converging to 1 at a rate depending on formula_80, where formula_81 is the variance of the residuals. Moreover, for specific choices of formula_82, kSGD's objective function bias at iteration formula_83 can be shown to be formula_84 with probability converging to 1 at a rate depending on formula_80, where formula_86 is the optimal parameter.\n}} (2) it has a robust choice of hyperparameters, and (3) it has a stopping condition. The drawbacks of kSGD is that the algorithm requires storing a dense covariance matrix between iterations, and requires a matrix-vector product at each iteration.\n\nTo describe the algorithm, suppose formula_6, where formula_88 is defined by an example formula_89 such that\n\nwhere formula_91 is mean function (i.e. the expected value of formula_92 given formula_93), and formula_94 is the variance function (i.e. the variance of formula_92 given formula_93). Then, the parameter update, formula_97, and covariance matrix update, formula_98 are given by the following\n\nwhere formula_105 are hyperparameters. The formula_106 update can result in the covariance matrix becoming indefinite, which can be avoided at the cost of a matrix-matrix multiplication. formula_107 can be any positive definite symmetric matrix, but is typically taken to be the identity. As noted by Patel, for all problems besides linear regression, restarts are required to ensure convergence of the algorithm, but no theoretical or implementation details were given. In a closely related, off-line, mini-batch method for non-linear regression analyzed by Bertsekas, a forgetting factor was used in the covariance matrix update to prove convergence.\n\n\n\n\n\n"}
{"id": "5740025", "url": "https://en.wikipedia.org/wiki?curid=5740025", "title": "Stochastic volatility", "text": "Stochastic volatility\n\nIn statistics, stochastic volatility models are those in which the variance of a stochastic process is itself randomly distributed. They are used in the field of mathematical finance to evaluate derivative securities, such as options. The name derives from the models' treatment of the underlying security's volatility as a random process, governed by state variables such as the price level of the underlying security, the tendency of volatility to revert to some long-run mean value, and the variance of the volatility process itself, among others.\n\nStochastic volatility models are one approach to resolve a shortcoming of the Black–Scholes model. In particular, models based on Black-Scholes assume that the underlying volatility is constant over the life of the derivative, and unaffected by the changes in the price level of the underlying security. However, these models cannot explain long-observed features of the implied volatility surface such as volatility smile and skew, which indicate that implied volatility does tend to vary with respect to strike price and expiry. By assuming that the volatility of the underlying price is a stochastic process rather than a constant, it becomes possible to model derivatives more accurately.\n\nStarting from a constant volatility approach, assume that the derivative's underlying asset price follows a standard model for geometric Brownian motion:\n\nwhere formula_2 is the constant drift (i.e. expected return) of the security price formula_3, formula_4 is the constant volatility, and formula_5 is a standard Wiener process with zero mean and unit rate of variance. The explicit solution of this stochastic differential equation is\n\nThe maximum likelihood estimator to estimate the constant volatility formula_4 for given stock prices formula_3 at different times formula_9 is\n\nits expected value is formula_11\n\nThis basic model with constant volatility formula_4 is the starting point for non-stochastic volatility models such as Black–Scholes model and Cox–Ross–Rubinstein model.\n\nFor a stochastic volatility model, replace the constant volatility formula_4 with a function formula_14, that models the variance of formula_3. This variance function is also modeled as Brownian motion, and the form of formula_14 depends on the particular SV model under study. \n\nwhere formula_19 and formula_20 are some functions of formula_21, and formula_22 is another standard gaussian that is correlated with formula_5 with constant correlation factor formula_24.\n\nThe popular Heston model is a commonly used SV model, in which the randomness of the variance process varies as the square root of variance. In this case, the differential equation for variance takes the form:\n\nwhere formula_26 is the mean long-term volatility, formula_27 is the rate at which the volatility reverts toward its long-term mean, formula_28 is the volatility of the volatility process, and formula_29 is, like formula_30, a gaussian with zero mean and formula_31 standard deviation. However, formula_30 and formula_29 are correlated with the constant correlation value formula_34.\n\nIn other words, the Heston SV model assumes that the variance is a random process that\n\nSome parametrisation of the volatility surface, such as 'SVI', are based on the Heston model.\n\nThe CEV model describes the relationship between volatility and price, introducing stochastic volatility:\n\nConceptually, in some markets volatility rises when prices rise (e.g. commodities), so formula_39. In other markets, volatility tends to rise as prices fall, modelled with formula_40.\n\nSome argue that because the CEV model does not incorporate its own stochastic process for volatility, it is not truly a stochastic volatility model. Instead, they call it a local volatility model.\n\nThe SABR model (Stochastic Alpha, Beta, Rho), introduced by Hagan et al. describes a single forward formula_41 (related to any asset e.g. an index, interest rate, bond, currency or equity) under stochastic volatility formula_42:\n\nThe initial values formula_45 and formula_46 are the current forward price and volatility, whereas formula_47 and formula_48 are two correlated Wiener processes (i.e. Brownian motions) with correlation coefficient formula_49. The constant parameters formula_50 are such that formula_51.\n\nThe main feature of the SABR model is to be able to reproduce the smile effect of the volatility smile.\n\nThe Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model is another popular model for estimating stochastic volatility. It assumes that the randomness of the variance process varies with the variance, as opposed to the square root of the variance as in the Heston model. The standard GARCH(1,1) model has the following form for the variance differential:\n\nThe GARCH model has been extended via numerous variants, including the NGARCH, TGARCH, IGARCH, LGARCH, EGARCH, GJR-GARCH, etc. Strictly, however, the conditional volatilities from GARCH models are not stochastic since at time \"t\" the volatility is completely pre-determined (deterministic) given previous values.\n\nThe 3/2 model is similar to the Heston model, but assumes that the randomness of the variance process varies with formula_53. The form of the variance differential is:\n\nHowever the meaning of the parameters is different from Heston model. In this model both, mean reverting and volatility of variance parameters, are stochastic quantities given by formula_55 and formula_56 respectively.\n\nOnce a particular SV model is chosen, it must be calibrated against existing market data. Calibration is the process of identifying the set of model parameters that are most likely given the observed data. One popular technique is to use maximum likelihood estimation (MLE). For instance, in the Heston model, the set of model parameters formula_57 can be estimated applying an MLE algorithm such as the Powell Directed Set method to observations of historic underlying security prices.\n\nIn this case, you start with an estimate for formula_58, compute the residual errors when applying the historic price data to the resulting model, and then adjust formula_59 to try to minimize these errors. Once the calibration has been performed, it is standard practice to re-calibrate the model periodically.\n\nAn alternative to calibration is statistical estimation, thereby accounting for parameter uncertainty. Many frequentist and Bayesian methods have been proposed and implemented, typically for a subset of the abovementioned models. The following list contains extension packages for the open source statistical software R that have been specifically designed for heteroskedasticity estimation. The first three cater for GARCH-type models with deterministic volatilities; the fourth deals with stochastic volatility estimation.\n\nThere are also alternate statistical estimation libraries in other languages such as Python:\n\n\n\n"}
{"id": "1639294", "url": "https://en.wikipedia.org/wiki?curid=1639294", "title": "Subquotient", "text": "Subquotient\n\nIn the mathematical fields of category theory and abstract algebra, a subquotient is a quotient object of a subobject. Subquotients are particularly important in abelian categories, and in group theory, where they are also known as sections, though this conflicts with a different meaning in category theory.\n\nFor example, of the 26 sporadic groups, 20 are subquotients of the monster group, and are referred to as the \"Happy Family\", while the other 6 are pariah groups.\n\nA quotient of a subrepresentation of a representation (of, say, a group) might be called a subquotient representation; e.g., Harish-Chandra's subquotient theorem.\n\nIn constructive set theory, where the law of excluded middle does not necessarily hold, one can consider the relation 'subquotient of' as replacing the usual order relation(s) on cardinals. When one has the law of the excluded middle, then a subquotient formula_1 of formula_2 is either the empty set or there is an onto function formula_3. This order relation is traditionally denoted formula_4. If additionally the axiom of choice holds, then formula_1 has a one-to-one function to formula_2 and this order relation is the usual formula_7 on corresponding cardinals.\n\nThe relation »is subquotient of« is transitive.\nLet formula_8 groups and formula_9 and formula_10 be group homomorphisms, then also the composition\nis a homomorphism.\n\nIf formula_12 is a subgroup of formula_13 and formula_14 a subgroup of formula_15, then formula_16 is a subgroup of formula_17. We have formula_18, indeed formula_19, because every formula_20 has a preimage in formula_12. Thus formula_22. This means that the image, say formula_23, of a subgroup, say formula_14, of formula_25 is also the image of a subgroup, namely formula_26 under formula_27, of formula_13.\n\nIn other words: If formula_23 is a subquotient of formula_15 and formula_15 is subquotient of formula_13 then formula_23 is subquotient of formula_13.  ■\n\n"}
{"id": "11905171", "url": "https://en.wikipedia.org/wiki?curid=11905171", "title": "Tonelli's theorem (functional analysis)", "text": "Tonelli's theorem (functional analysis)\n\nIn mathematics, Tonelli's theorem in functional analysis is a fundamental result on the weak lower semicontinuity of nonlinear functionals on \"L\" spaces. As such, it has major implications for functional analysis and the calculus of variations. Roughly, it shows that weak lower semicontinuity for integral functionals is equivalent to convexity of the integral kernel. The result is attributed to the Italian mathematician Leonida Tonelli.\n\nLet Ω be a bounded domain in \"n\"-dimensional Euclidean space R and let \"f\" : R → R ∪ {±∞} be a continuous extended real-valued function. Define a nonlinear functional \"F\" on functions \"u\" : Ω → R by\n\nThen \"F\" is sequentially weakly lower semicontinuous on the \"L\" space \"L\"(Ω; R) for 1 < \"p\" < +∞ and weakly-∗ lower semicontinuous on \"L\"(Ω; R) if and only if the function \"f\"\n\nis convex.\n\n"}
{"id": "3837790", "url": "https://en.wikipedia.org/wiki?curid=3837790", "title": "Trilinear coordinates", "text": "Trilinear coordinates\n\nIn geometry, the trilinear coordinates \"x:y:z\" of a point relative to a given triangle describe the relative directed distances from the three sidelines of the triangle. Trilinear coordinates are an example of homogeneous coordinates. The ratio \"x:y\" is the ratio of the perpendicular distances from the point to the sides (extended if necessary) opposite vertices \"A\" and \"B\" respectively; the ratio \"y:z\" is the ratio of the perpendicular distances from the point to the sidelines opposite vertices \"B\" and \"C\" respectively; and likewise for \"z:x\" and vertices \"C\" and \"A\".\n\nIn the diagram at right, the trilinear coordinates of the indicated interior point are the actual distances (\"a' \", \"b' \", \"c' \"), or equivalently in ratio form, \"ka' \":\"kb' \":\"kc' \" for any positive constant \"k\". If a point is on a sideline of the reference triangle, its corresponding trilinear coordinate is 0. If an exterior point is on the opposite side of a sideline from the interior of the triangle, its trilinear coordinate associated with that sideline is negative. It is impossible for all three trilinear coordinates to be non-positive.\n\nThe name \"trilinear coordinates\" is sometimes abbreviated to \"trilinears\". \n\nThe ratio notation \"x\":\"y\":\"z\" for trilinear coordinates is different from the ordered triple notation (\"a' \", \"b' \", \"c' \") for actual directed distances. Here each of \"x\", \"y\", and \"z\" has no meaning by itself; its ratio to one of the others \"does\" have meaning. Thus \"comma notation\" for trilinear coordinates should be avoided, because the notation (\"x\", \"y\", \"z\"), which means an ordered triple, does not allow, for example, (\"x\", \"y\", \"z\") = (2\"x\", 2\"y\", 2\"z\"), whereas the \"colon notation\" does allow \"x\" : \"y\" : \"z\" = 2\"x\" : 2\"y\" : 2\"z\".\n\nThe trilinear coordinates of the incenter of a triangle \"ABC\" are 1 : 1 : 1; that is, the (directed) distances from the incenter to the sidelines \"BC\", \"CA\", \"AB\" are proportional to the actual distances denoted by (\"r\", \"r\", \"r\"), where \"r\" is the inradius of triangle \"ABC\". Given side lengths \"a, b, c\" we have:\n\nNote that, in general, the incenter is not the same as the centroid; the centroid has barycentric coordinates 1 : 1 : 1 (these being proportional to actual signed areas of the triangles \"BGC\", \"CGA\", \"AGB\", where \"G\" = centroid.)\n\nThe midpoint of, for example, side \"BC\" has trilinear coordinates in actual sideline distances formula_1 for triangle area formula_2, which in arbitrarily specified relative distances simplifies to formula_3 The coordinates in actual sideline distances of the foot of the altitude from \"A\" to \"BC\" are formula_4 which in purely relative distances simplifies to formula_5\n\nTrilinear coordinates enable many algebraic methods in triangle geometry. For example, three points\n\nare collinear if and only if the determinant\n\nequals zero. Thus if \"x:y:z\" is a variable point, the equation of a line through the points \"P\" and \"U\" is \"D\" = 0. From this, every straight line has a linear equation homogeneous in \"x, y, z\". Every equation of the form \"lx+my+nz\" = 0 in real coefficients is a real straight line of finite points unless \"l : m: n\" is proportional to \"a : b : c\", the side lengths, in which case we have the locus of points at infinity.\n\nThe dual of this proposition is that the lines\n\nconcur in a point (α, β, γ) if and only if \"D\" = 0.\n\nAlso, if the actual directed distances are used when evaluating the determinant of \"D\", then the area of triangle \"PUX\" is \"KD\", where \"K\" = \"abc/8∆\" (and where \"∆\" is the area of triangle \"ABC\", as above) if triangle \"PUX\" has the same orientation (clockwise or counterclockwise) as triangle \"ABC\", and \"K\" = \"–abc/8∆\" otherwise.\n\nTwo lines with trilinear equations formula_7 and formula_8 are parallel if and only if\n\nwhere \"a, b, c\" are the side lengths.\n\nThe tangents of the angles between two lines with trilinear equations formula_7 and formula_8 are given by\n\nThus two lines with trilinear equations formula_7 and formula_8 are perpendicular if and only if\n\nThe equation of the altitude from vertex \"A\" to side \"BC\" is\n\nThe equation of a line with variable distances \"p, q, r\" from the vertices \"A\", \"B\", \"C\" whose opposite sides are \"a, b, c\" is\n\nThe trilinears with the coordinate values \"a', b', c' \" being the actual perpendicular distances to the sides satisfy\n\nfor triangle sides \"a, b, c\" and area formula_2. This can be seen in the figure at the top of this article, with interior point \"P\" partitioning triangle \"ABC\" into three triangles \"PBC\", \"PCA\", and \"PAB\" with respective areas (1/2)\"aa' \", (1/2)\"bb' \", and (1/2)\"cc' \".\n\nThe distance \"d\" between two points with actual-distance trilinears \"a' \" : \"b' \" : \"c' \" is given by\n\nThe distance \"d\" from a point \"a' \" : \"b' \" : \"c' \", in trilinear coordinates of actual distances, to a straight line \"lx + my + nz\" = 0 is\n\nThe equation of a conic section in the variable trilinear point \"x\" : \"y\" : \"z\" is\n\nIt has no linear terms and no constant term.\n\nThe equation of a circle of radius \"r\" having center at actual-distance coordinates (\"a', b', c' \") is\n\nThe equation in trilinear coordinates \"x, y, z\" of any circumconic of a triangle is\n\nIf the parameters \"l, m, n\" respectively equal the side lengths \"a, b, c\" (or the sines of the angles opposite them) then the equation gives the circumcircle.\n\nEach distinct circumconic has a center unique to itself. The equation in trilinear coordinates of the circumconic with center \"x' : y' : z' \" is\n\nEvery conic section inscribed in a triangle has an equation in trilinear coordinates\n\nwith exactly one or three of the unspecified signs being negative.\n\nThe equation of the incircle can be simplified to\n\nwhile the equation for, for example, the excircle adjacent to the side segment opposite vertex \"A\" can be written as\n\nMany cubic curves are easily represented using trilinear coordinates. For example, the pivotal self-isoconjugate cubic \"Z(U,P)\", as the locus of a point \"X\" such that the \"P\"-isoconjugate of \"X\" is on the line \"UX\" is given by the determinant equation\n\nAmong named cubics \"Z(U,P)\" are the following:\n\nFor any choice of trilinear coordinates \"x:y:z\" to locate a point, the actual distances of the point from the sidelines are given by \"a' = kx\", \"b' = ky\", \"c' = kz\" where \"k\" can be determined by the formula formula_30 in which \"a\", \"b\", \"c\" are the respective sidelengths \"BC\", \"CA\", \"AB\", and ∆ is the area of \"ABC\".\n\nA point with trilinear coordinates \"x\" : \"y\" : \"z\" has barycentric coordinates \"ax\" : \"by\" : \"cz\" where \"a\", \"b\", \"c\" are the sidelengths of the triangle. Conversely, a point with barycentrics \"α\" : \"β\" : \"γ\" has trilinear coordinates \"α/a\" : \"β/b\" : \"γ/c\".\n\nGiven a reference triangle \"ABC\", express the position of the vertex \"B\" in terms of an ordered pair of Cartesian coordinates and represent this algebraically as a vector \"B\", using vertex \"C\" as the origin. Similarly define the position vector of vertex \"A\" as \"A\". Then any point \"P\" associated with the reference triangle \"ABC\" can be defined in a Cartesian system as a vector \"P\" = \"k\"\"A\" + \"k\"\"B\". If this point \"P\" has trilinear coordinates \"x : y : z\" then the conversion formula from the coefficients \"k\" and \"k\" in the Cartesian representation to the trilinear coordinates is, for side lengths \"a\", \"b\", \"c\" opposite vertices \"A\", \"B\", \"C\",\n\nand the conversion formula from the trilinear coordinates to the coefficients in the Cartesian representation is\n\nMore generally, if an arbitrary origin is chosen where the Cartesian coordinates of the vertices are known and represented by the vectors \"A\", \"B\" and \"C\" and if the point \"P\" has trilinear coordinates \"x\" : \"y\" : \"z\", then the Cartesian coordinates of \"P\" are the weighted average of the Cartesian coordinates of these vertices using the barycentric coordinates \"ax\", \"by\" and \"cz\" as the weights. Hence the conversion formula from the trilinear coordinates \"x, y, z\" to the vector of Cartesian coordinates \"P\" of the point is given by\n\nwhere the side lengths are |\"C\" − \"B\"| = \"a\", |\"A\" − \"C\"| = \"b\" and |\"B\" − \"A\"| = \"c\".\n\n\n"}
{"id": "1411087", "url": "https://en.wikipedia.org/wiki?curid=1411087", "title": "Weak derivative", "text": "Weak derivative\n\nIn mathematics, a weak derivative is a generalization of the concept of the derivative of a function (\"strong derivative\") for functions not assumed differentiable, but only integrable, i.e., to lie in the L space formula_1. See distributions for a more general definition.\n\nLet formula_2 be a function in the Lebesgue space formula_3. We say that formula_4 in formula_3 is a \"weak derivative\" of formula_2 if,\n\nfor all infinitely differentiable functions formula_8 with formula_9. This definition is motivated by the integration technique of Integration by parts.\n\nGeneralizing to formula_10 dimensions, if formula_2 and formula_4 are in the space formula_13 of locally integrable functions for some open set formula_14, and if formula_15 is a multi-index, we say that formula_4 is the formula_17-weak derivative of formula_2 if\n\nfor all formula_20, that is, for all infinitely differentiable functions formula_21 with compact support in formula_22. Here formula_23 is defined as\n\nIf formula_2 has a weak derivative, it is often written formula_26 since weak derivatives are unique (at least, up to a set of measure zero, see below).\n\n\n\n\nIf two functions are weak derivatives of the same function, they are equal except on a set with Lebesgue measure zero, i.e., they are equal almost everywhere. If we consider equivalence classes of functions such that two functions are equivalent if they are equal almost everywhere, then the weak derivative is unique.\n\nAlso, if \"u\" is differentiable in the conventional sense then its weak derivative is identical (in the sense given above) to its conventional (strong) derivative. Thus the weak derivative is a generalization of the strong one. Furthermore, the classical rules for derivatives of sums and products of functions also hold for the weak derivative.\n\nThis concept gives rise to the definition of weak solutions in Sobolev spaces, which are useful for problems of differential equations and in functional analysis.\n\n\n"}
{"id": "10777748", "url": "https://en.wikipedia.org/wiki?curid=10777748", "title": "Weitzenböck's inequality", "text": "Weitzenböck's inequality\n\nIn mathematics, Weitzenböck's inequality, named after Roland Weitzenböck, states that for a triangle of side lengths formula_1, formula_2, formula_3, and area formula_4, the following inequality holds:\n\nEquality occurs if and only if the triangle is equilateral. Pedoe's inequality is a generalization of Weitzenböck's inequality. The Hadwiger-Finsler inequality is a strengthened version of Weitzenböck's inequality.\n\nRewriting the inequality above allows for a more concrete geometric interpretation, which in turn provides an immediate proof.\n\nNow the summands on the left side are the areas of equilateral triangles erected over the sides of the original triangle and hence the equations states that the sum of areas of the equilateral triangles is always greater or equal than the threefold area of the original triangle.\n\nThis can now can be shown by replicating area of the triangle three times within the equilateral triangles. To achieve that the Fermat point is used to partition the triangle into three obtuse subtriangles with a formula_8 angle and each of those subtriangles is replicated three times within the equilateral triangle next to it. This only works if every angle of the triangle is smaller than formula_8, since otherwise the Fermat point is not located in the interior of the triangle and becomes a vertex instead. However if one angle is greater or equal to formula_8 it is possible to replicate the whole triangle three times within the largest equilateral triangle, so the sum of areas of all equilateral triangles stays greater than the threefold area of the triangle anyhow.\n\nThe proof of this inequality was set as a question in the International Mathematical Olympiad of 1961. Even so, the result is not too difficult to derive using Heron's formula for the area of a triangle:\n\nIt can be shown that the area of the inner Napoleon's triangle, which must be nonnegative, is\n\nso the expression in parentheses must be greater than or equal to 0.\n\nThis method assumes no knowledge of inequalities except that all squares are nonnegative.\n\nand the result follows immediately by taking the positive square root of both sides. From the first inequality we can also see that equality occurs only when formula_14 and the triangle is equilateral.\n\nThis proof assumes knowledge of the AM–GM inequality.\n\nAs we have used the arithmetic-geometric mean inequality, equality only occurs when formula_14 and the triangle is equilateral.\n\n\n\n"}
{"id": "84400", "url": "https://en.wikipedia.org/wiki?curid=84400", "title": "Zero-point energy", "text": "Zero-point energy\n\nZero-point energy (ZPE) is the difference between the lowest possible energy that a quantum mechanical system may have, and the classical minimum energy of the system. Unlike in classical mechanics, quantum systems constantly fluctuate in their lowest energy state due to the Heisenberg uncertainty principle. As well as atoms and molecules, the empty space of the vacuum has these properties. According to quantum field theory, the universe can be thought of not as isolated particles but continuous fluctuating fields: matter fields, whose quanta are fermions (i.e. leptons and quarks), and force fields, whose quanta are bosons (e.g. photons and gluons). All these fields have zero-point energy. These fluctuating zero-point fields lead to a kind of reintroduction of an aether in physics, since some systems can detect the existence of this energy. However this aether cannot be thought of as a physical medium if it is to be Lorentz invariant such that there is no contradiction with Einstein's theory of special relativity.\n\nPhysics currently lacks a full theoretical model for understanding zero-point energy; in particular the discrepancy between theorized and observed vacuum energy is a source of major contention. Physicists Richard Feynman and John Wheeler calculated the zero-point radiation of the vacuum to be an order of magnitude greater than nuclear energy, with a single light bulb containing enough energy to boil all the world's oceans. Yet according to Einstein's theory of general relativity any such energy would gravitate and the experimental evidence from both the expansion of the universe, dark energy and the Casimir effect show any such energy to be exceptionally weak. A popular proposal that attempts to address this issue is to say that the fermion field has a negative zero-point energy while the boson field has positive zero-point energy and thus these energies somehow cancel each other out. This idea would be true if supersymmetry were an exact symmetry of nature. However, the LHC at CERN has so far found no evidence to support supersymmetry. Moreover, it is known that if supersymmetry is valid at all, it is at most a broken symmetry, only true at very high energies, and no one has been able to show a theory where zero-point cancellations occur in the low energy universe we observe today. This discrepancy is known as the cosmological constant problem and it is one of the greatest unsolved mysteries in physics. Many physicists believe that \"the vacuum holds the key to a full understanding of nature\".\n\nThe term zero-point energy (ZPE) is a translation from the German Nullpunktsenergie.\nThe terms zero-point radiation or ground state energy are also sometimes used interchangeably. The term zero-point field (ZPF) can be used when referring to a specific vacuum field, for instance the QED vacuum which specifically deals with quantum electrodynamics (e.g. electromagnetic interactions between photons, electrons and the vacuum) or the QCD vacuum which deals with quantum chromodynamics (e.g. color charge interactions between quarks, gluons and the vacuum). A vacuum can be viewed not as empty space but as the combination of all zero-point fields. In quantum field theory this combination of fields is called the vacuum state, its associated zero-point energy is called the vacuum energy and the average energy value is called the vacuum expectation value (VEV) also called its condensate.\n\nIn classical mechanics all particles can be thought of as having some energy made up of their potential energy and kinetic energy. Temperature, for example, arises from the intensity of random particle motion caused by kinetic energy (known as brownian motion). As temperature is reduced to absolute zero, it might be thought that all motion ceases and particles come completely to rest. In fact, however, kinetic energy is retained by particles even at the lowest possible temperature. The random motion corresponding to this zero-point energy never vanishes as a consequence of the uncertainty principle of quantum mechanics.\nThe uncertainty principle states that no object can ever have precise values of position and velocity simultaneously. The total energy of a quantum mechanical object (potential and kinetic) is described by its Hamiltonian which also describes the system as a harmonic oscillator, or wave function, that fluctuates between various energy states (see wave-particle duality). All quantum mechanical systems undergo fluctuations even in their ground state, a consequence of their wave-like nature. The uncertainty principle requires every quantum mechanical system to have a fluctuating zero-point energy greater than the minimum of its classical potential well. This results in motion even at absolute zero. For example, liquid helium does not freeze under atmospheric pressure regardless of temperature due to its zero-point energy.\n\nGiven the equivalence of mass and energy expressed by Einstein's , any point in space that contains energy can be thought of as having mass to create particles. Virtual particles spontaneously flash into existence at every point in space due to the energy of quantum fluctuations caused by the uncertainty principle. Modern physics has developed quantum field theory (QFT) to understand the fundamental interactions between matter and forces, it treats every single point of space as a quantum harmonic oscillator. According to QFT the universe is made up of matter fields, whose quanta are fermions (i.e. leptons and quarks), and force fields, whose quanta are bosons (e.g. photons and gluons). All these fields have zero-point energy. Recent experiments advocate the idea that particles themselves can be thought of as excited states of the underlying quantum vacuum, and that all properties of matter are merely vacuum fluctuations arising from interactions of the zero-point field.\n\nThe idea that \"empty\" space can have an intrinsic energy associated to it, and that there is no such thing as a \"true vacuum\" is seemingly unintuitive. It is often argued that the entire universe is completely bathed in the zero-point radiation, and as such it can add only some constant amount to calculations. Physical measurements will therefore reveal only deviations from this value. For many practical calculations zero-point energy is dismissed by fiat in the mathematical model as a term that has no physical effect. Such treatment causes problems however, as in Einstein's theory of general relativity the absolute energy value of space is not an arbitrary constant and gives rise to the cosmological constant. For decades most physicists assumed that there was some undiscovered fundamental principle that will remove the infinite zero-point energy and make it completely vanish. If the vacuum has no intrinsic, absolute value of energy it will not gravitate. It was believed that as the universe expands from the aftermath of the big bang, the energy contained in any unit of empty space will decrease as the total energy spreads out to fill the volume of the universe; galaxies and all matter in the universe should begin to decelerate. This possibility was ruled out in 1998 by the discovery that the expansion of the universe is not slowing down but is in fact accelerating, meaning empty space does indeed have some intrinsic energy. The discovery of dark energy is best explained by zero-point energy, though it still remains a mystery as to why the value appears to be so small compared to huge value obtained through theory - the cosmological constant problem.\n\nMany physical effects attributed to zero-point energy have been experimentally verified, such as spontaneous emission, Casimir force, Lamb shift, magnetic moment of the electron and Delbrück scattering, these effects are usually called \"radiative corrections\". In more complex nonlinear theories (e.g. QCD) zero-point energy can give rise to a variety of complex phenomena such as multiple stable states, symmetry breaking, chaos and emergence. Many physicists believe that \"the vacuum holds the key to a full understanding of nature\" and that studying it is critical in the search for the theory of everything. Active areas of research include the effects of virtual particles, quantum entanglement, the difference (if any) between inertial and gravitational mass, variation in the speed of light, a reason for the observed value of the cosmological constant and the nature of dark energy.\n\nZero-point energy evolved from historical ideas about the vacuum. To Aristotle the vacuum was , \"the empty\"; space independent of body. He believed this concept violated basic physical principles and asserted that the elements of fire, air, earth, and water were not made of atoms, but were continuous. To the atomists the concept of emptiness had absolute character: it was the distinction between existence and nonexistence. Debate about the characteristics of the vacuum were largely confined to the realm of philosophy, it was not until much later on with the beginning of the renaissance, that Otto von Guericke invented the first vacuum pump and the first testable scientific ideas began to emerge. It was thought that a totally empty volume of space could be created by simply removing all gases. This was the first generally accepted concept of the vacuum.\n\nLate in the 19th century, however, it became apparent that the evacuated region still contained thermal radiation. The existence of the aether as a substitute for a true void was the most prevalent theory of the time. According to the successful electromagnetic aether theory based upon Maxwell's electrodynamics, this all-encompassing aether was endowed with energy and hence very different from nothingness. The fact that electromagnetic and gravitational phenomena were easily transmitted in empty space indicated that their associated aethers were part of the fabric of space itself. Maxwell himself noted that:\n\nHowever, the results of the Michelson–Morley experiment in 1887 were the first strong evidence that the then-prevalent aether theories were seriously flawed, and initiated a line of research that eventually led to special relativity, which ruled out the idea of a stationary aether altogether. To scientists of the period, it seemed that a true vacuum in space might be completely eliminated by cooling thus eliminating all radiation or energy. From this idea evolved the second concept of achieving a real vacuum: cool it down to absolute zero temperature after evacuation. Absolute zero was technically impossible to achieve in the 19th century, so the debate remained unsolved.\n\nIn 1900, Max Planck derived the average energy of a single \"energy radiator\", e.g., a vibrating atomic unit, as a function of absolute temperature:\n\nwhere is Planck's constant, is the frequency, is Boltzmann's constant, and is the absolute temperature. The zero-point energy makes no contribution to Planck's original law, as its existence was unknown to Planck in 1900.\n\nThe concept of zero-point energy was developed by Max Planck in Germany in 1911 as a corrective term added to a zero-grounded formula developed in his original quantum theory in 1900.\n\nIn 1912, Max Planck published the first journal article to describe the discontinuous emission of radiation, based on the discrete quanta of energy. In Planck's \"second quantum theory\" resonators absorbed energy continuously, but emitted energy in discrete energy quanta only when they reached the boundaries of finite cells in phase space, where their energies became integer multiples of . This theory led Planck to his new radiation law, but in this version energy resonators possessed a zero-point energy, the smallest average energy a resonator could take on. Planck's radiation equation contained a residual energy factor, one , as an additional term dependent on the frequency , which was greater than zero (where is Planck's constant). It is therefore widely agreed that \"Planck's equation marked the birth of the concept of zero-point energy.\" In a series of papers from 1911 to 1913, Planck found that the average energy of an oscillator to be:\n\nSoon, the idea of zero-point energy attracted the attention of Albert Einstein and his assistant Otto Stern. In 1913 they published a paper that attempted to prove the existence of zero-point energy by calculating the specific heat of hydrogen gas and compared it with the experimental data. However, after assuming they had succeeded, they retracted support for the idea shortly after publication because they found Planck's second theory may not apply to their example. In a letter to Paul Ehrenfest of the same year Einstein declared zero-point energy “dead as a doornail” Zero-point energy was also invoked by Peter Debye, who noted that zero-point energy of the atoms of a crystal lattice would cause a reduction in the intensity of the diffracted radiation in X-ray diffraction even as the temperature approached absolute zero. In 1916 Walther Nernst proposed that empty space was filled with zero-point electromagnetic radiation. With the development of general relativity Einstein found the energy density of the vacuum to contribute towards a cosmological constant in order to obtain static solutions to his field equations; the idea that empty space, or the vacuum, could have some intrinsic energy associated to it had returned, with Einstein stating in 1920:\n\nIn 1913 Niels Bohr had proposed what is now called the Bohr model of the atom, but despite this it remained a mystery as to why electrons do not fall into their nuclei. According to classical ideas, the fact that an accelerating charge loses energy by radiating implied that an electron should spiral into the nucleus and that atoms should not be stable. This problem of classical mechanics was nicely summarized by James Hopwood Jeans in 1915: \"There would be a very real difficulty in supposing that the (force) law held down to the zero values of . For the forces between two charges at zero distance would be infinite; we should have charges of opposite sign continually rushing together and, when once together, no force would tend to shrink into nothing or to diminish indefinitely in size\" This resolution to this puzzle came in 1926 with Schrodinger's famous equation. This equation explained the new, non-classical, fact that as an electron moves close to a nucleus its kinetic energy necessarily increases in such a way that the minimum total energy (kinetic plus potential) occurs at some positive separation rather than at zero separation; in other words, that zero-point energy is essential for atomic stability.\n\nIn 1926 Pascual Jordan published the first attempt to quantize the electromagnetic field. In a joint paper with Max Born and Werner Heisenberg he considered the field inside a cavity as a superposition of quantum harmonic oscillators. In his calculation he found that in addition to the \"thermal energy\" of the oscillators there also had to exist infinite zero-point energy term. He was able to obtain the same fluctuation formula that Einstein had obtained in 1909. However, Jordan did not think that his infinite zero-point energy term was \"real\", writing to Einstein that \"it is just a quantity of the calculation having no direct physical meaning\" Jordan found a way to get rid of the infinite term, publishing a joint work with Pauli in 1928, performing what has been called \"the first infinite subtraction, or renormalisation, in quantum field theory\"\nBuilding on the work of Heisenberg and others Paul Dirac's theory of emission and absorption (1927) was the first application of the quantum theory of radiation. Dirac's work was seen as crucially important to the emerging field of quantum mechanics; it dealt directly with the process in which \"particles\" are actually created: spontaneous emission. Dirac described the quantization of the electromagnetic field as an ensemble of harmonic oscillators with the introduction of the concept of creation and annihilation operators of particles. The theory showed that spontaneous emission depends upon the zero-point energy fluctuations of the electromagnetic field in order to get started. In a process in which a photon is annihilated (absorbed), the photon can be thought of as making a transition into the vacuum state. Similarly, when a photon is created (emitted), it is occasionally useful to imagine that the photon has made a transition out of the vacuum state. In the words of Dirac:\n\nContemporary physicists, when asked to give a physical explanation for spontaneous emission, generally invoke the zero-point energy of the electromagnetic field. This view was popularized by Victor Weisskopf who in 1935 wrote:\n\nThis view was also later supported by (1948), who argued that spontaneous emission \"can be thought of as forced emission taking place under the action of the fluctuating field.\" This new theory, which Dirac coined quantum electrodynamics (QED) predicted a fluctuating zero-point or \"vacuum\" field existing even in the absence of sources.\n\nThroughout the 1940s improvements in microwave technology made it possible to take more precise measurements of the shift of the levels of a hydrogen atom, now known as the Lamb shift, and measurement of the magnetic moment of the electron. Discrepancies between these experiments and Dirac's theory led to the idea of incorporating renormalisation into QED to deal with zero-point infinities. Renormalization was originally developed by Hans Kramers and also Victor Weisskopf(1936), and first successfully applied to calculate a finite value for the Lamb shift by Hans Bethe (1947). As per spontaneous emission, these effects can in part be understood with interactions with the zero-point field. But in light of renormalisation being able to remove some zero-point infinities from calculations, not all physicists were comfortable attributing zero-point energy any physical meaning, viewing it instead as a mathematical artifact that might one day be fully eliminated. In Wolfgang Pauli's 1945 Nobel lecture he made clear his opposition to the idea of zero-point energy stating \"It is clear that this zero-point energy has no physical reality\".\nIn 1948 Hendrik Casimir showed that one consequence of the zero-point field is an attractive force between two uncharged, perfectly conducting parallel plates, the so-called Casimir effect. At the time, Casimir was studying the properties of \"colloidal solutions\". These are viscous materials, such as paint and mayonnaise, that contain micron-sized particles in a liquid matrix. The properties of such solutions are determined by van der Waals forces – long-range, attractive forces that exist between neutral atoms and molecules. One of Casimir's colleagues, Theo Overbeek, realized that the theory that was used at the time to explain van der Waals forces, which had been developed by Fritz London in 1930, did not properly explain the experimental measurements on colloids. Overbeek therefore asked Casimir to investigate the problem. Working with Dirk Polder, Casimir discovered that the interaction between two neutral molecules could be correctly described only if the fact that light travels at a finite speed was taken into account. Soon afterwards after a conversation with Bohr about zero-point energy, Casimir noticed that this result could be interpreted in terms of vacuum fluctuations. He then asked himself what would happen if there were two mirrors – rather than two molecules – facing each other in a vacuum. It was this work that led to his famous prediction of an attractive force between reflecting plates. The work by Casimir and Polder opened up the way to a unified theory of van der Waals and Casimir forces and a smooth continuum between the two phenomena. This was done by Lifshitz (1956) in the case of plane parallel dielectric plates. The generic name for both van der Waals and Casimir forces is dispersion forces, because both of them are caused by dispersions of the operator of the dipole moment. The role of relativistic forces becomes dominant at orders of a hundred nanometers.\n\nIn 1951 Herbert Callen and Theodore Welton proved the quantum fluctuation-dissipation theorem (FDT) which was originally formulated in classical form by Nyquist (1928) as an explanation for observed Johnson noise in electric circuits. Fluctuation-dissipation theorem showed that when something dissipates energy, in an effectively irreversible way, a connected heat bath must also fluctuate. The fluctuations and the dissipation go hand in hand; it is impossible to have one without the other. The implication of FDT being that the vacuum could be treated as a heat bath coupled to a dissipative force and as such energy could, in part, be extracted from the vacuum for potentially useful work. FDT has been shown to be true experimentally under certain quantum, non-classical, conditions.\n\nIn 1963 the Jaynes–Cummings model was developed describing the system of a two-level atom interacting with a quantized field mode (i.e. the vacuum) within an optical cavity. It gave nonintuitive predictions such as that an atom's spontaneous emission could be driven by field of effectively constant frequency (Rabi frequency). In the 1970s experiments were being performed to test aspects of quantum optics and showed that the rate of spontaneous emission of an atom could be controlled using reflecting surfaces. These results were at first regarded with suspicion in some quarters: it was argued that no modification of a spontaneous emission rate would be possible, after all, how can the emission of a photon be affected by an atom's environment when the atom can only \"see\" its environment by emitting a photon in the first place? These experiments gave rise to cavity quantum electrodynamics (CQED), the study of effects of mirrors and cavities on radiative corrections. Spontaneous emission can be suppressed (or \"inhibited\") or amplified. Amplification was first predicted by Purcell in 1946 (the Purcell effect) and has been experimentally verified. This phenomenon can be understood, partly, in terms of the action of the vacuum field on the atom.\n\nZero-point energy is fundamentally related to the Heisenberg uncertainty principle. Roughly speaking, the uncertainty principle states that complementary variables (such as a particle's position and momentum, or a field's value and derivative at a point in space) cannot simultaneously be specified precisely by any given quantum state. In particular, there cannot exist a state in which the system simply sits motionless at the bottom of its potential well: for, then, its position and momentum would both be completely determined to arbitrarily great precision. Therefore, instead, the lowest-energy state (the ground state) of the system must have a distribution in position and momentum that satisfies the uncertainty principle−−which implies its energy must be greater than the minimum of the potential well.\n\nNear the bottom of a potential well, the Hamiltonian of a general system (the quantum-mechanical operator giving its energy) can be approximated as a quantum harmonic oscillator,\nwhere is the minimum of the classical potential well.\n\nThe uncertainty principle tells us that\nmaking the expectation values of the kinetic and potential terms above satisfy\n\nThe expectation value of the energy must therefore be at least\nwhere is the angular frequency at which the system oscillates.\n\nA more thorough treatment, showing that the energy of the ground state actually saturates this bound and is exactly , requires solving for the ground state of the system.\n\nThe idea of a quantum harmonic oscillator and its associated energy can apply to either an atom or subatomic particle. In ordinary atomic physics, the zero-point energy is the energy associated with the ground state of the system. The professional physics literature tends to measure frequency, as denoted by above, using angular frequency, denoted with and defined by . This leads to a convention of writing Planck's constant with a bar through its top () to denote the quantity . In these terms, the most famous such example of zero-point energy is the above associated with the ground state of the quantum harmonic oscillator. In quantum mechanical terms, the zero-point energy is the expectation value of the Hamiltonian of the system in the ground state.\n\nIf more than one ground state exists, they are said to be degenerate. Many systems have degenerate ground states. Degeneracy occurs whenever there exists a unitary operator which acts non-trivially on a ground state and commutes with the Hamiltonian of the system.\n\nAccording to the third law of thermodynamics, a system at absolute zero temperature exists in its ground state; thus, its entropy is determined by the degeneracy of the ground state. Many systems, such as a perfect crystal lattice, have a unique ground state and therefore have zero entropy at absolute zero. It is also possible for the highest excited state to have absolute zero temperature for systems that exhibit negative temperature.\n\nThe wave function of the ground state of a particle in a one-dimensional well is a half-period sine wave which goes to zero at the two edges of the well. The energy of the particle is given by:\n\nwhere is the Planck constant, is the mass of the particle, is the energy state ( corresponds to the ground-state energy), and is the width of the well.\n\nIn quantum field theory (QFT), the fabric of \"empty\" space is visualized as consisting of fields, with the field at every point in space and time being a quantum harmonic oscillator, with neighboring oscillators interacting with each other. According to QFT the universe is made up of matter fields whose quanta are fermions (e.g. electrons and quarks) and force fields, whose quanta are bosons (i.e. photons and gluons). All these fields have zero-point energy. A related term is \"zero-point field\" (ZPF), which is the lowest energy state of a particular field. The vacuum can be viewed, not as empty space, but the combination of all zero-point fields.\n\nIn QFT this combination of fields is called the vacuum state, its associated zero-point energy is called the vacuum energy and the average expectation value of the Hamiltonian is called the vacuum expectation value (also called condensate or simply VEV). The QED vacuum is a part of the vacuum state which specifically deals with quantum electrodynamics (e.g. electromagnetic interactions between photons, electrons and the vacuum) and the QCD vacuum deals with quantum chromodynamics (e.g. color charge interactions between quarks, gluons and the vacuum). Recent experiments advocate the idea that particles themselves can be thought of as excited states of the underlying quantum vacuum, and that all properties of matter are merely vacuum fluctuations arising from interactions with the zero-point field.\n\nEach point in space makes a contribution of , resulting in a calculation of infinite zero-point energy in any finite volume; this is one reason renormalization is needed to make sense of quantum field theories. In cosmology, the vacuum energy is one possible explanation for the cosmological constant and the source of dark energy. \nScientists are not in agreement about how much energy is contained in the vacuum. Quantum mechanics requires the energy to be large as Paul Dirac claimed it is, like a sea of energy. Other scientists specializing in General Relativity require the energy to be small enough for curvature of space to agree with observed astronomy. The Heisenberg uncertainty principle allows the energy to be as large as needed to promote quantum actions for a brief moment of time, even if the average energy is small enough to satisfy relativity and flat space. To cope with disagreements, the vacuum energy is described as a virtual energy potential of positive and negative energy.\n\nIn quantum perturbation theory, it is sometimes said that the contribution of one-loop and multi-loop Feynman diagrams to elementary particle propagators are the contribution of vacuum fluctuations, or the zero-point energy to the particle masses.\n\nThe oldest and best known quantized force field is the electromagnetic field. Maxwell's equations have been superseded by quantum electrodynamics (QED). By considering the zero-point energy that arises from QED it is possible to gain a characteristic understanding of zero-point energy that arises not just through electromagnetic interactions but in all quantum field theories.\n\nIn the quantum theory of the electromagnetic field, classical wave amplitudes and are replaced by operators and that satisfy:\n\nThe classical quantity appearing in the classical expression for the energy of a field mode is replaced in quantum theory by the photon number operator . The fact that:\n\nimplies that quantum theory does not allow states of the radiation field for which the photon number and a field amplitude can be precisely defined, i.e., we cannot have simultaneous eigenstates for and . The reconciliation of wave and particle attributes of the field is accomplished via the association of a probability amplitude with a classical mode pattern. The calculation of field modes is entirely classical problem, while the quantum properties of the field are carried by the mode \"amplitudes\" and associated with these classical modes.\n\nThe zero-point energy of the field arises formally from the non-commutativity of and . This is true for any harmonic oscillator: the zero-point energy appears when we write the Hamiltonian:\n\nIt is often argued that the entire universe is completed bathed in the zero-point electromagnetic field, and as such it can add only some constant amount to expectation values. Physical measurements will therefore reveal only deviations from the vacuum state. Thus the zero-point energy can be dropped from the Hamiltonian by redefining the zero of energy, or by arguing that it is a constant and therefore has no effect on Heisenberg equations of motion. Thus we can choose to declare by fiat that the ground state has zero energy and a field Hamiltonian, for example, can be replaced by:\n\nwithout affecting any physical predictions of the theory. The new Hamiltonian is said to be normally ordered (or Wick ordered) and is denoted by a double-dot symbol. The normally ordered Hamiltonian is denoted , i.e.:\n\nIn other words, within the normal ordering symbol we can commute and . Since zero-point energy is intimately connected to the non-commutativity of and , the normal ordering procedure eliminates any contribution from the zero-point field. This is especially reasonable in the case of the field Hamiltonian, since the zero-point term merely adds a constant energy which can be eliminated by a simple redefinition for the zero of energy. Moreover, this constant energy in the Hamiltonian obviously commutes with and and so cannot have any effect on the quantum dynamics described by the Heisenberg equations of motion.\n\nHowever, things are not quite that simple. The zero-point energy cannot be eliminated by dropping its energy from the Hamiltonian: When we do this and solve the Heisenberg equation for a field operator, we must include the vacuum field, which is the homogeneous part of the solution for the field operator. In fact we can show that the vacuum field is essential for the preservation of the commutators and the formal consistent of QED. When we calculate the field energy we obtain not only a contribution from particles and forces that may be present but also a contribution from the vacuum field itself i.e. the zero-point field energy. In other words, the zero-point energy reappears even though we may have deleted it from the Hamiltonian.\n\nFrom Maxwell's equations, the electromagnetic energy of a \"free\" field i.e. one with no sources, is described by:\n\nWe introduce the \"mode function\" that satisfies the Helmholtz equation:\n\nwhere and assume it is normalized such that:\n\nWe wish to \"quantize\" the electromagnetic energy of free space for a multimode field. The field intensity of free space should be independent of position such that should be independent of for each mode of the field. The mode function satisfying these conditions is:\n\nwhere in order to have the transversality condition satisfied for the Coulomb gauge in which we are working.\n\nTo achieve the desired normalization we pretend space is divided into cubes of volume and impose on the field the periodic boundary condition:\n\nor equivalently\n\nwhere can assume any integer value. This allows us to consider the field in any one of the imaginary cubes and to define the mode function:\n\nwhich satisfies the Helmholtz equation, transversality, and the \"box normalization\":\n\nwhere is chosen to be a unit vector which specifies the polarization of the field mode. The condition means that there are two independent choices of , which we call and where and . Thus we define the mode functions:\n\nin terms of which the vector potential becomes:\n\nor:\n\nwhere and , are photon annihilation and creation operators for the mode with wave vector and polarization . This gives the vector potential for a plane wave mode of the field. The condition for shows that there are infinitely many such modes. The linearity of Maxwell's equations allows us to write:\n\nfor the total vector potential in free space. Using the fact that:\n\nwe find the field Hamiltonian is:\n\nThis is the Hamiltonian for an infinite number of uncoupled harmonic oscillators. Thus different modes of the field are independent and satisfy the commutation relations:\n\nClearly the least eigenvalue for is:\n\nThis state describes the zero-point energy of the vacuum. It appears that this sum is divergent – in fact highly divergent, as putting in the density factor\n\nshows. The summation becomes approximately the integral:\n\nfor high values of . It diverges proportional to for large .\n\nThere are two separate questions to consider. First, is the divergence a real one such that the zero-point energy really is infinite? If we consider the volume is contained by perfectly conducting walls, very high frequencies can only be contained by taking more and more perfect conduction. No actual method of containing the high frequencies is possible. Such modes will not be stationary in our box and thus not countable in the stationary energy content. So from this physical point of view the above sum should only extend to those frequencies which are countable; a cut-off energy is thus eminently reasonable. However, on the scale of a \"universe\" questions of general relativity must be included. Suppose even the boxes could be reproduced, fit together and closed nicely by curving spacetime. Then exact conditions for running waves may be possible. However the very high frequency quanta will still not be contained. As per John Wheeler's \"geons\" these will leak out of the system. So again a cut-off is permissible, almost necessary. The question here becomes one of consistency since the very high energy quanta will act as a mass source and start curving the geometry.\n\nThis leads to the second question. Divergent or not, finite or infinite, is the zero-point energy of any physical significance? The ignoring of the whole zero-point energy is often encouraged for all practical calculations. The reason for this is that energies are not typically defined by an arbitrary data point, but rather changes in data points, so adding or subtracting a constant (even if infinite) should to be allowed. However this is not the whole story, in reality energy is not so arbitrarily defined: in general relativity the seat of the curvature of spacetime is the energy content and there the absolute amount of energy has real physical meaning. There is no such thing as an arbitrary additive constant with density of field energy. Energy density curves space, and an increase in energy density produces an increase of curvature. Furthermore, the zero-point energy density has other physical consequences e.g. the Casimir effect, contribution to the Lamb shift, or anomalous magnetic moment of the electron, it is clear it is not just a mathematical constant or artifact that can be cancelled out.\n\nThe vacuum state of the \"free\" electromagnetic field (that with no sources) is defined as the ground state in which for all modes . The vacuum state, like all stationary states of the field, is an eigenstate of the Hamiltonian but not the electric and magnetic field operators. In the vacuum state, therefore, the electric and magnetic fields do not have definite values. We can imagine them to be fluctuating about their mean value of zero.\n\nIn a process in which a photon is annihilated (absorbed), we can think of the photon as making a transition into the vacuum state. Similarly, when a photon is created (emitted), it is occasionally useful to imagine that the photon has made a transition out of the vacuum state. An atom, for instance, can be considered to be \"dressed\" by emission and reabsorption of \"virtual photons\" from the vacuum. The vacuum state energy described by is infinite. We can make the replacement:\n\nthe zero-point energy density is:\n\nor in other words the spectral energy density of the vacuum field:\n\nThe zero-point energy density in the frequency range from to is therefore:\n\nThis can be large even in relatively narrow \"low frequency\" regions of the spectrum. In the optical region from 400 to 700 nm, for instance, the above equation yields around 220 erg/cm.\n\nWe showed in the above section that the zero-point energy can be eliminated from the Hamiltonian by the normal ordering prescription. However, this elimination does not mean that the vacuum field has been rendered unimportant or without physical consequences. To illustrate this point we consider a linear dipole oscillator in the vacuum. The Hamiltonian for the oscillator plus the field with which it interacts is:\n\nThis has the same form as the corresponding classical Hamiltonian and the Heisenberg equations of motion for the oscillator and the field are formally the same as their classical counterparts. For instance the Heisenberg equations for the coordinate and the canonical momentum of the oscillator are:\n\nor:\n\nsince the rate of change of the vector potential in the frame of the moving charge is given by the convective derivative\n\nFor nonrelativistic motion we may neglect the magnetic force and replace the expression for by:\n\nAbove we have made the electric dipole approximation in which the spatial dependence of the field is neglected. The Heisenberg equation for is found similarly from the Hamiltonian to be:\n\nIn the electric dipole approximation.\n\nIn deriving these equations for , , and we have used the fact that equal-time particle and field operators commute. This follows from the assumption that particle and field operators commute at some time (say, ) when the matter-field interpretation is presumed to begin, together with the fact that a Heisenberg-picture operator evolves in time as , where is the time evolution operator satisfying\n\nAlternatively, we can argue that these operators must commute if we are to obtain the correct equations of motion from the Hamiltonian, just as the corresponding Poisson brackets in classical theory must vanish in order to generate the correct Hamilton equations. The formal solution of the field equation is:\n\nand therefore the equation for may be written:\n\nwhere:\n\nand:\n\nIt can be shown that in the radiation reaction field, if the mass is regarded as the \"observed\" mass then we can take:\n\nThe total field acting on the dipole has two parts, and . is the free or zero-point field acting on the dipole. It is the homogeneous solution of the Maxwell equation for the field acting on the dipole, i.e., the solution, at the position of the dipole, of the wave equation\n\nsatisfied by the field in the (source free) vacuum. For this reason is often referred to as the \"vacuum field\", although it is of course a Heisenberg-picture operator acting on whatever state of the field happens to be appropriate at . is the source field, the field generated by the dipole and acting on the dipole.\n\nUsing the above equation for we obtain an equation for the Heisenberg-picture operator formula_48 that is formally the same as the classical equation for a linear dipole oscillator:\n\nwhere . in this instance we have considered a dipole in the vacuum, without any \"external\" field acting on it. the role of the external field in the above equation is played by the vacuum electric field acting on the dipole.\n\nClassically, a dipole in the vacuum is not acted upon by any \"external\" field: if there are no sources other than the dipole itself, then the only field acting on the dipole is its own radiation reaction field. In quantum theory however there is always an \"external\" field, namely the source-free or vacuum field .\n\nAccording to our earlier equation for the free field is the only field in existence at as the time at which the interaction between the dipole and the field is \"switched on\". The state vector of the dipole-field system at is therefore of the form\n\nwhere is the vacuum state of the field and is the initial state of the dipole oscillator. The expectation value of the free field is therefore at all times equal to zero:\n\nsince . however, the energy density associated with the free field is infinite:\n\nThe important point of this is that the zero-point field energy does not affect the Heisenberg equation for since it is a c-number or constant (i.e. an ordinary number rather than an operator) and commutes with . We can therefore drop the zero-point field energy from the Hamiltonian, as is usually done. But the zero-point field re-emerges as the homogeneous solution for the field equation. A charged particle in the vacuum will therefore always see a zero-point field of infinite density. This is the origin of one of the infinities of quantum electrodynamics, and it cannot be eliminated by the trivial expedient dropping of the term in the field Hamiltonian.\n\nThe free field is in fact necessary for the formal consistency of the theory. In particular, it is necessary for the preservation of the commutation relations, which is required by the unitary of time evolution in quantum theory:\n\nWe can calculate from the formal solution of the operator equation of motion\n\nUsing the fact that\n\nand that equal-time particle and field operators commute, we obtain:\n\nFor the dipole oscillator under consideration it can be assumed that the radiative damping rate is small compared with the natural oscillation frequency, i.e., . Then the integrand above is sharply peaked at and:\n\nthe necessity of the vacuum field can also be appreciated by making the small damping approximation in\n\nand\n\nWithout the free field in this equation the operator would be exponentially dampened, and commutators like would approach zero for . With the vacuum field included, however, the commutator is at all times, as required by unitarity, and as we have just shown. A similar result is easily worked out for the case of a free particle instead of a dipole oscillator.\n\nWhat we have here is an example of a \"fluctuation-dissipation elation\". Generally speaking if a system is coupled to a bath that can take energy from the system in an effectively irreversible way, then the bath must also cause fluctuations. The fluctuations and the dissipation go hand in hand we cannot have one without the other. In the current example the coupling of a dipole oscillator to the electromagnetic field has a dissipative component, in the form of the zero-point (vacuum) field; given the existence of radiation reaction, the vacuum field must also exist in order to preserve the canonical commutation rule and all it entails.\n\nThe spectral density of the vacuum field is fixed by the form of the radiation reaction field, or vice versa: because the radiation reaction field varies with the third derivative of , the spectral energy density of the vacuum field must be proportional to the third power of in order for to hold. In the case of a dissipative force proportional to , by contrast, the fluctuation force must be proportional to formula_60 in order to maintain the canonical commutation relation. This relation between the form of the dissipation and the spectral density of the fluctuation is the essence of the fluctuation-dissipation theorem.\n\nThe fact that the canonical commutation relation for a harmonic oscillator coupled to the vacuum field is preserved implies that the zero-point energy of the oscillator is preserved. it is easy to show that after a few damping times the zero-point motion of the oscillator is in fact sustained by the driving zero-point field.\n\nThe QCD vacuum is the vacuum state of quantum chromodynamics (QCD). It is an example of a \"non-perturbative\" vacuum state, characterized by a non-vanishing condensates such as the gluon condensate and the quark condensate in the complete theory which includes quarks. The presence of these condensates characterizes the confined phase of quark matter. In technical terms, gluons are vector gauge bosons that mediate strong interactions of quarks in quantum chromodynamics (QCD). Gluons themselves carry the color charge of the strong interaction. This is unlike the photon, which mediates the electromagnetic interaction but lacks an electric charge. Gluons therefore participate in the strong interaction in addition to mediating it, making QCD significantly harder to analyze than QED (quantum electrodynamics) as it deals with nonlinear equations to characterize such interactions.\n\nThe Standard Model hypothesises a field called the Higgs field (symbol: ), which has the unusual property of a non-zero amplitude in its ground state (zero-point) energy after renormalization; i.e., a non-zero vacuum expectation value. It can have this effect because of its unusual \"Mexican hat\" shaped potential whose lowest \"point\" is not at its \"centre\". Below a certain extremely high energy level the existence of this non-zero vacuum expectation spontaneously breaks electroweak gauge symmetry which in turn gives rise to the Higgs mechanism and triggers the acquisition of mass by those particles interacting with the field. The Higgs mechanism occurs whenever a charged field has a vacuum expectation value. This effect occurs because scalar field components of the Higgs field are \"absorbed\" by the massive bosons as degrees of freedom, and couple to the fermions via Yukawa coupling, thereby producing the expected mass terms. The expectation value of in the ground state (the vacuum expectation value or VEV) is then , where . The measured value of this parameter is approximately . It has units of mass, and is the only free parameter of the Standard Model that is not a dimensionless number.\n\nThe Higgs mechanism is a type of superconductivity which occurs in the vacuum. It occurs when all of space is filled with a sea of particles which are charged and thus the field has a nonzero vacuum expectation value. Interaction with the vacuum energy filling the space prevents certain forces from propagating over long distances (as it does in a superconducting medium; e.g., in the Ginzburg–Landau theory).\n\nZero-point energy has many observed physical consequences. It is important to note that zero-point energy is not merely an artefact of mathematical formalism that can, for instance, be dropped from a Hamiltonian by redefining the zero of energy, or by arguing that it is a constant and therefore has no effect on Heisenberg equations of motion without latter consequence. Indeed, such treatment could create a problem at a deeper, as of yet undiscovered, theory. For instance, in general relativity the zero of energy (i.e. the energy density of the vacuum) contributes to a cosmological constant of the type introduced by Einstein in order to obtain static solutions to his field equations. The zero-point energy density of the vacuum, due to all quantum fields, is extremely large, even when we cut off the largest allowable frequencies based on plausible physical arguments. It implies a cosmological constant larger than the limits imposed by observation by about 120 orders of magnitude. This \"cosmological constant problem\" remains one of the greatest unsolved mysteries of physics.\n\nA phenomenon that is commonly presented as evidence for the existence of zero-point energy in vacuum is the Casimir effect, proposed in 1948 by Dutch physicist Hendrik Casimir, who considered the quantized electromagnetic field between a pair of grounded, neutral metal plates. The vacuum energy contains contributions from all wavelengths, except those excluded by the spacing between plates. As the plates draw together, more wavelengths are excluded and the vacuum energy decreases. The decrease in energy means there must be a force doing work on the plates as they move.\n\nEarly experimental tests from the 1950s onwards gave positive results showing the force was real, but other external factors could not be ruled out as the primary cause, with the range of experimental error sometimes being nearly 100%. That changed in 1997 with Lamoreaux conclusively showing that the Casimir force was real. Results have been repeatedly replicated since then.\n\nIn 2009 Munday et al. published experimental proof that (as predicted in 1961) the Casimir force could also be repulsive as well as being attractive. Repulsive Casimir forces could allow quantum levitation of objects in a fluid and lead to a new class of switchable nanoscale devices with ultra-low static friction\n\nAn interesting theoretical side effect of the Casimir effect is the Scharnhorst effect, a hypothetical phenomenon in which light signals travel slightly faster than between two closely spaced conducting plates.\n\nThe quantum fluctuations of the electromagnetic field have important physical consequences. In addition to the Casimir effect, they also lead to a splitting between the two energy levels and (in term symbol notation) of the hydrogen atom which was not predicted by the Dirac equation, according to which these states should have the same energy. Charged particles can interact with the fluctuations of the quantized vacuum field, leading to slight shifts in energy, this effect is called the Lamb shift. The shift of about is roughly of the difference between the energies of the 1s and 2s levels, and amounts to 1,058 MHz in frequency units. A small part of this shift (27 MHz ≈ 3%) arises not from fluctuations of the electromagnetic field, but from fluctuations of the electron–positron field. The creation of (virtual) electron–positron pairs has the effect of screening the Coulomb field and acts as a vacuum dielectric constant. This effect is much more important in muonic atoms.\n\nTaking (Planck's constant divided by ), (the speed of light), and (the electromagnetic coupling constant i.e. a measure of the strength of the electromagnetic force (where is the absolute value of the electronic charge and formula_61 is the vacuum permittivity)) we can form a dimensionless quantity called the fine-structure constant:\n\nThe fine-structure constant is the coupling constant of quantum electrodynamics (QED) determining the strength of the interaction between electrons and photons. It turns out that the fine structure constant is not really a constant at all owing to the zero-point energy fluctuations of the electron-positron field. The quantum fluctuations caused by zero-point energy have the effect of screening electric charges: owing to (virtual) electron-positron pair production, the charge of the particle measured far from the particle is far smaller than the charge measured when close to it.\n\nThe Heisenberg inequality where , and , are the standard deviations of position and momentum states that:\n\nIt means that a short distance implies large momentum and therefore high energy i.e. particles of high energy must be used to explore short distances. QED concludes that the fine structure constant is an increasing function of energy. It has been shown that at energies of the order of the Z boson rest energy, 90 GeV, that:\n\nrather than the low-energy . The renormalization procedure of eliminating zero-point energy infinities allows the choice of an arbitrary energy (or distance) scale for defining . All in all, depends on the energy scale characteristic of the process under study, and also on details of the renormalization procedure. The energy dependence of has been observed for several years now in precision experiment in high-energy physics.\n\nIn the presence of strong electrostatic fields it is predicted that virtual particles become separated from the vacuum state and form real matter. The fact that electromagnetic radiation can be transformed into matter and vice versa leads to fundamentally new features in quantum electrodynamics. One of the most important consequences is that, even in the vacuum, the Maxwell equations have to be exchanged by more complicated formulas. In general, it will be not possible to separate processes in the vacuum from the processes involving matter since electromagnetic fields can create matter if the field fluctuations are strong enough. This leads to highly complex nonlinear interaction - gravity will have an effect on the light at the same time the light has an effect on gravity. These effects were first predicted by Werner Heisenberg and Hans Heinrich Euler in 1936 and independently the same year by Victor Weisskopf who stated: \"The physical properties of the vacuum originate in the “zero-point energy” of matter, which also depends on absent particles through the external field strengths and therefore contributes an additional term to the purely Maxwellian field energy\". Thus strong magnetic fields vary the energy contained in the vacuum. The scale above which the electromagnetic field is expected to become nonlinear is known as the Schwinger limit. At this point the vacuum has all the properties of a birefringent medium, thus in principle a rotation of the polarization frame (the Faraday effect) can be observed in empty space.\nBoth Einstein's theory of special and general relativity state that light should pass freely through a vacuum without being altered, a principle known as Lorentz invariance. Yet, in theory, large nonlinear self-interaction of light due to quantum fluctuations should lead to this principle being measurably violated if the interactions are strong enough. Nearly all theories of quantum gravity predict that that Lorentz invariance is not an exact symmetry of nature. It is predicted the speed at which light travels through the vacuum depends on its direction, polarization and the local strength of the magnetic field. There have been a number of inconclusive results which claim to show evidence of a Lorentz violation by finding a rotation of the polarization plane of light coming from distant galaxies. The first concrete evidence for vacuum birefringence was published in 2017 when a team of astronomers looked at the light coming from the star RX J1856.5-3754, the closest discovered neutron star to Earth.\n\nRoberto Mignani at the National Institute for Astrophysics in Milan who led the team of astronomers has commented that \"“When Einstein came up with the theory of general relativity 100 years ago, he had no idea that it would be used for navigational systems. The consequences of this discovery probably will also have to be realised on a longer timescale.” The team found that visible light from the star had undergone linear polarisation of around 16%. If the birefringence had been caused by light passing through interstellar gas or plasma, the effect should have been no more than 1%. Definitive proof would require repeating the observation at other wavelengths and on other neutron stars. At X-ray wavelengths the polarization from the quantum fluctuations should be near 100%. Although no telescope currently exists that can make such measurements, there are several proposed X-ray telescopes that may soon be able to verify the result conclusively such as China's Hard X-ray Modulation Telescope (HXMT) and NASA's Imaging X-ray Polarimetry Explorer (IXPE).\n\nIn the late 1990s it was discovered that very distant supernova were dimmer than expected suggesting that the universe's expansion was accelerating rather than slowing down. This revived discussion that Einstein's cosmological constant, long disregarded by physicists as being equal to zero, was in fact some small positive value. This would indicate empty space exerted some form of negative pressure or energy.\n\nThere is no natural candidate for what might cause what has been called dark energy but the current best guess is that it is the zero-point energy of the vacuum. One difficulty with this assumption is that the zero-point energy of the vacuum is absurdly large compared to the observed cosmological constant. In general relativity, mass and energy are equivalent; both produce a gravitational field and therefore the theorized vacuum energy of quantum field theory should have led the universe ripping itself to pieces. This obviously has not happened and this issue, called the cosmological constant problem, is one of the greatest unsolved mysteries in physics.\n\nThe European Space Agency is building the Euclid telescope. Due to launch in 2020 it will map galaxies up to 10 billion light years away. By seeing how dark energy influences their arrangement and shape, the mission will allow scientists to see if the strength of dark energy has changed. If dark energy is found to vary throughout time it would indicate it is due to quintessence, where observed acceleration is due to the energy of a scalar field, rather than the cosmological constant. No evidence of quintessence is yet available, but it has not been ruled out either. It generally predicts a slightly slower acceleration of the expansion of the universe than the cosmological constant. Some scientists think that the best evidence for quintessence would come from violations of Einstein's equivalence principle and variation of the fundamental constants in space or time. Scalar fields are predicted by the \"Standard Model of particle physics\" and string theory, but an analogous problem to the cosmological constant problem (or the problem of constructing models of cosmological inflation) occurs: renormalization theory predicts that scalar fields should acquire large masses again due to zero-point energy.\n\nCosmic inflation is a faster-than-light expansion of space just after the Big Bang. It explains the origin of the large-scale structure of the cosmos. It is believed quantum vacuum fluctuations caused by zero-point energy arising in the microscopic inflationary period, later became magnified to a cosmic size, becoming the gravitational seeds for galaxies and structure in the Universe (see galaxy formation and evolution and structure formation). Many physicists also believe that inflation explains why the Universe appears to be the same in all directions (isotropic), why the cosmic microwave background radiation is distributed evenly, why the Universe is flat, and why no magnetic monopoles have been observed.\n\nThe mechanism for inflation is unclear, it is similar in effect to dark energy but is a far more energetic and short lived process. As with dark energy the best explanation is some form of vacuum energy arising from quantum fluctuations. It may be that inflation caused baryogenesis, the hypothetical physical processes that produced an asymmetry (imbalance) between baryons and antibaryons produced in the very early universe, but this is far from certain.\n\nThere has been a long debate over the question of whether zero-point fluctuations of quantized vacuum fields are “real” i.e. do they have physical effects that cannot be interpreted by an equally valid alternative theory? Schwinger, in particular, attempted to formulate QED without reference to zero-point fluctuations via his \"source theory\". From such an approach it is possible to derive the Casimir Effect without reference to a fluctuating field. Such a derivation was first given by Schwinger (1975) for a scalar field, and then generalized to the electromagnetic case by Schwinger, DeRaad, and Milton (1978). in which they state \"the vacuum is regarded as truly a state with all physical properties equal to zero\". More recently Jaffe (2005) has highlighted a similar approach in deriving the Casimir effect stating \"the concept of zero-point fluctuations is a heuristic and calculational aid in the description of the Casimir effect, but not a necessity in QED.\"\n\nNevertheless, as Jaffe himself notes in his paper, \"no one has shown that source theory or another S-matrix based approach can provide a complete description of QED to all orders.\" Furthermore, Milonni has shown the necessity of the vacuum field for the formal consistency of QED. In QCD, color confinement has led physicists to abandon the source theory or S-matrix based approach for the strong interactions. The Higgs mechanism, Hawking Radiation and the Unruh effect are also theorized to be dependent on zero-point vacuum fluctuations, the field contribution being an inseparable parts of these theories. Jaffe continues \"Even if one could argue away zero-point contributions to the quantum vacuum energy, the problem of spontaneous symmetry breaking remains: condensates [ground state vacua] that carry energy appear at many energy scales in the Standard Model. So there is good reason to be skeptical of attempts to avoid the standard formulation of quantum field theory and the zero-point energies it brings with it.\" It is difficult to judge the physical reality of infinite zero-point energies that are inherent in field theories, but modern physics does not know any better way to construct gauge-invariant, renormalizable theories than with zero-point energy and they would seem to be a necessity for any attempt at a unified theory.\n\nThe mathematical models used in classical electromagnetism, quantum electrodynamics (QED) and the standard model all view the electromagnetic vacuum as a linear system with no overall observable consequence (e.g. in the case of the Casimir effect, Lamb shift, and so on) these phenomena can be explained by alternative mechanisms other than action of the vacuum by arbitrary changes to the normal ordering of field operators. See alternative theories section). This is a consequence of viewing electromagnetism as a U(1) gauge theory, which topologically does not allow the complex interaction of a field with and on itself. In higher symmetry groups and in reality, the vacuum is not a calm, randomly fluctuating, largely immaterial and passive substance, but at times can be viewed as a turbulent virtual plasma that can have complex vortices (i.e. solitons vis-à-vis particles), entangled states and a rich nonlinear structure. There are many observed nonlinear physical electromagnetic phenomena such as Aharonov–Bohm (AB) and Altshuler–Aronov–Spivak (AAS) effects, Berry, Aharonov–Anandan, Pancharatnam and Chiao–Wu phase rotation effects, Josephson effect,\n\nWhat are called Maxwell's equations today, are in fact a simplified version of the original equations reformulated by Heaviside, FitzGerald, Lodge and Hertz. The original equations used Hamilton's more expressive quaternion notation, a kind of Clifford algebra, which fully subsumes the standard Maxwell vectorial equations largely used today. In the late 1880s there was a debate over the relative merits of vector analysis and quaternions. According to Heaviside the electromagnetic potential field was purely metaphysical, an arbitrary mathematical fiction, that needed to be \"murdered\". It was concluded that there was no need for the greater physical insights provided by the quaternions if the theory was purely local in nature. Local vector analysis has become the dominant way of using Maxwell's equations ever since. However, this strictly vectorial approach has led to a restrictive topological understanding in some areas of electromagnetism, for example, a full understanding of the energy transfer dynamics in Tesla's oscillator-shuttle-circuit can only be achieved in quaternionic algebra or higher SU(2) symmetries. It has often been argued that quaternions are not compatible with special relativity, but multiple papers have shown ways of incorporating relativity.\n\nA good example of nonlinear electromagnetics is in high energy dense plasmas, where vortical phenomena occur which seemingly violate the second law of thermodynamics by increasing the energy gradient within the electromagnetic field and violate Maxwell's laws by creating ion currents which capture and concentrate their own and surrounding magnetic fields. In particular Lorentz force law, which elaborates Maxwell's equations is violated by these force free vortices. These apparent violations are due to the fact that the traditional conservation laws in classical and quantum electrodynamics (QED) only display linear U(1) symmetry (in particular, by the extended Noether theorem, conservation laws such as the laws of thermodynamics need not always apply to dissipative systems, which are expressed in gauges of higher symmetry). The second law of thermodynamics states that in a closed linear system entropy flow can only be positive (or exactly zero at the end of a cycle). However, negative entropy (i.e. increased order, structure or self-organisation) can spontaneously appear in an open nonlinear thermodynamic system that is far from equilibrium, so long as this emergent order accelerates the overall flow of entropy in the total system. The 1977 Nobel Prize in Chemistry was awarded to thermodynamicist Ilya Prigogine for his theory of dissipative systems that described this notion. Prigogine described the principle as \"order through fluctuations\" or \"order out of chaos\". It has been argued by some that all emergent order in the universe from galaxies, solar systems, planets, weather, complex chemistry, evolutionary biology to even consciousness, technology and civilizations are themselves examples of thermodynamic dissipative systems; nature having naturally selected these structures to accelerate entropy flow within the universe to an ever-increasing degree. For example, it has been estimated that human body is 10,000 times more effective at dissipating energy per unit of mass than the sun.\n\nOne may query what this has to do with zero-point energy. Given the complex and adaptive behaviour that arises from nonlinear systems considerable attention in recent years has gone into studying a new class of phase transitions which occur at absolute zero temperature. These are quantum phase transitions which are driven by EM field fluctuations as a consequence of zero-point energy. A good example of a spontaneous phase transition that are attributed to zero-point fluctuations can be found in superconductors. Superconductivity is one of the best known empirically quantified macroscopic electromagnetic phenomena whose basis is recognised to be quantum mechanical in origin. The behaviour of the electric and magnetic fields under superconductivity is governed by the London equations. However, it has been questioned in a series of journal articles whether the quantum mechanically canonised London equations can be given a purely classical derivation. Bostick, for instance, has claimed to show that the London equations do indeed have a classical origin that applies to superconductors and to some collisionless plasmas as well. In particular it has been asserted that the Beltrami vortices in the plasma focus display the same paired flux-tube morphology as Type II superconductors. Others have also pointed out this connection, Fröhlich has shown that the hydrodynamic equations of compressible fluids, together with the London equations, lead to a macroscopic parameter (formula_65 = electric charge density / mass density), without involving either quantum phase factors or Planck's constant. In essence, it has been asserted that Beltrami plasma vortex structures are able to at least simulate the morphology of Type I and Type II superconductors. This occurs because the \"organised\" dissipative energy of the vortex configuration comprising the ions and electrons far exceeds the \"disorganised\" dissipative random thermal energy. The transition from disorganised fluctuations to organised helical structures is a phase transition involving a change in the condensate's energy (i.e. the ground state or zero-point energy) but \"without any associated rise in temperature\". This is an example of zero-point energy having multiple stable states (see Quantum phase transition, Quantum critical point, Topological degeneracy, Topological order) and where the overall system structure is independent of a reductionist or deterministic view, that \"classical\" macroscopic order can also causally affect quantum phenomena. Furthermore, the pair production of Beltrami vortices has been compared to the morphology of pair production of virtual particles in the vacuum.\n\nThe idea that the vacuum energy can have multiple stable energy states is a leading hypothesis for the cause of cosmic inflation. In fact, it has been argued that these early vacuum fluctuations led to the expansion of the universe and in turn have guaranteed the non-equilibrium conditions necessary to drive order from chaos, as without such expansion the universe would have reached thermal equilibrium and no complexity could have existed. With the continued accelerated expansion of the universe, the cosmos generates an energy gradient that increases the \"free energy\" (i.e. the available, usable or potential energy for useful work) which the universe is able to utilize to create ever more complex forms of order. The only reason Earth's environment does not decay into an equilibrium state is that it receives a daily dose of sunshine and that, in turn, is due to the sun \"polluting\" interstellar space with decreasing entropy. The sun's fusion power is only possible due to the gravitational disequilibrium of matter that arose from cosmic expansion. In this essence, the vacuum energy can be viewed as the key cause of the negative entropy (i.e. structure) throughout the universe. That humanity might alter the morphology of the vacuum energy to create an energy gradient for useful work is the subject of much controversy.\n\nPhysicists overwhelmingly reject any possibility that the zero-point energy field can be exploited to obtain useful energy (work) or uncompensated momentum; such efforts are seen as tantamount to perpetual motion machines.\n\nNevertheless, the allure of free energy has motivated such research, usually falling in the category of fringe science. As long ago as 1889 (before quantum theory or discovery of the zero point energy) Nikola Tesla proposed that useful energy could be obtained from free space, or what was assumed at that time to be an all-pervasive aether. Others have since claimed to exploit zero-point or vacuum energy with a large amount of pseudoscientific literature causing ridicule around the subject. Despite rejection by the scientific community, harnessing zero-point energy remains an interest of research by non-scientific entities, particularly in the US where it has attracted the attention of major aerospace/defence contractors and the U.S. Department of Defence as well as in China, Germany, Russia and Brazil.\n\nA common assumption is that the Casimir force is of little practical use; the argument is made that the only way to actually gain energy from the two plates is to allow them to come together (getting them apart again would then require more energy), and therefore it is a one-use-only tiny force in nature. In 1984 Robert Forward published work showing how a \"vacuum-fluctuation battery\" could be constructed. The battery can be recharged by making the electrical forces slightly stronger than the Casimir force to reexpand the plates.\n\nIn 1995 and 1998 Maclay et al. published the first models of a microelectromechanical system (MEMS) with Casimir forces. While not exploiting the Casimir force for useful work, the papers drew attention from the MEMS community due to the revelation that Casimir effect needs to be considered as a vital factor in the future design of MEMS. In particular, Casimir effect might be the critical factor in the stiction failure of MEMS.\n\nIn 1999 Pinto, a former scientist at NASA's Jet Propulsion Laboratory at Caltech in Pasadena, published in \"Physical Review\" his Gedankenexperiment for a \"Casimir engine\". The paper showed that continuous positive net exchange of energy from the Casimir effect was possible, even stating in the abstract \"In the event of no other alternative explanations, one should conclude that major technological advances in the area of endless, by-product free-energy production could be achieved.\" In 2001 Capasso et al. showed how the force can be used to control the mechanical motion of a MEMS device, The researchers suspended a polysilicon plate from a torsional rod – a twisting horizontal bar just a few microns in diameter. When they brought a metallized sphere close up to the plate, the attractive Casimir force between the two objects made the plate rotate. They also studied the dynamical behaviour of the MEMS device by making the plate oscillate. The Casimir force reduced the rate of oscillation and led to nonlinear phenomena, such as hysteresis and bistability in the frequency response of the oscillator. According to the team, the system’s behaviour agreed well with theoretical calculations.\n\nDespite this and several similar peer reviewed papers, there is not a consensus as to whether such devices can produce a continuous output of work. Garret Moddel at University of Colorado has highlighted that he believes such devices hinge on the assumption that the Casimir force is a nonconservative force, he argues that there is sufficient evidence (e.g. analysis by Scandurra (2001)) to say that the Casimir effect is a conservative force and therefore even though such an engine can exploit the Casimir force for useful work it cannot produce more output energy than has been input into the system.\n\nIn 2008 DARPA solicited research proposals in the area of Casimir Effect Enhancement (CEE). The goal of the program is to develop new methods to control and manipulate attractive and repulsive forces at surfaces based on engineering of the Casimir Force.\n\nA 2008 patent by Haisch and Moddel details a device that is able to extract power from zero-point fluctuations using a gas that circulates through a Casimir cavity. As gas atoms circulate around the system they enter the cavity. Upon entering the electrons spin down to release energy via electromagnetic radiation. This radiation is then extracted by an absorber. On exiting the cavity the ambient vacuum fluctuations (i.e. the zero-point field) impart energy on the electrons to return the orbitals to previous energy levels, as predicted by Senitzky (1960). The gas then goes through a pump and flows through the system again. A published test of this concept by Moddel was performed in 2012 and seemed to give excess energy that could not be attributed to another source. However it has not been conclusively shown to be from zero-point energy and the theory requires further investigation.\n\nIn 1951 Callen and Welton proved the quantum fluctuation-dissipation theorem (FDT) which was originally formulated in classical form by Nyquist (1928) as an explanation for observed Johnson noise in electric circuits. Fluctuation-dissipation theorem showed that when something dissipates energy, in an effectively irreversible way, a connected heat bath must also fluctuate. The fluctuations and the dissipation go hand in hand; it is impossible to have one without the other. The implication of FDT being that the vacuum could be treated as a heat bath coupled to a dissipative force and as such energy could, in part, be extracted from the vacuum for potentially useful work. Such a theory has met with resistance: Macdonald (1962) and Harris (1971) claimed that extracting power from the zero-point energy to be impossible, so FDT could not be true. Grau and Kleen (1982) and Kleen (1986), argued that the Johnson noise of a resistor connected to an antenna must satisfy Planck's thermal radiation formula, thus the noise must be zero at zero temperature and FDT must be invalid. Kiss (1988) pointed out that the existence of the zero-point term may indicate that there is a renormalization problem—i.e., a mathematical artifact—producing an unphysical term that is not actually present in measurements (in analogy with renormalization problems of ground states in quantum electrodynamics). Later, Abbott et al. (1996) arrived at a different but unclear conclusion that \"zero-point energy is infinite thus it should be renormalized but not the ‘zero-point fluctuations’\". Despite such criticism, FDT has been shown to be true experimentally under certain quantum, non-classical conditions. Zero-point fluctuations can, and do, contribute towards systems which dissipate energy. A paper by Armen Allahverdyan and Theo Nieuwenhuizen in 2000 showed the feasibility of extracting zero-point energy for useful work from a single bath, without contradicting the laws of thermodynamics, by exploiting certain quantum mechanical properties.\n\nThere have been a growing number of papers showing that in some instances the classical laws of thermodynamics, such as limits on the Carnot efficiency, can be violated by exploiting negative entropy of quantum fluctuations.\n\nDespite efforts to reconcile quantum mechanics and thermodynamics over the years, their compatibility is still an open fundamental problem. The full extent that quantum properties can alter classical thermodynamic bounds is unknown\n\nThe use of zero-point energy for space travel is highly speculative. A complete quantum theory of gravitation (that would deal with the role of quantum phenomena like zero-point energy) does not yet exist. Speculative papers explaining a relationship between zero-point energy and gravitational shielding effects have been proposed, but the interaction (if any) is not yet fully understood. Most serious scientific research in this area depends on the theorized anti-gravitational properties of antimatter (currently being tested at the alpha experiment at CERN) and/or the effects of non-Newtonian forces such as the gravitomagnetic field under specific quantum conditions. According to the general theory of relativity, rotating matter can generate a new force of nature, known as the gravitomagnetic interaction, whose intensity is proportional to the rate of spin. In certain conditions the gravitomagnetic field can be repulsive. In neutrons stars for example it can produce a gravitational analogue of the Meissner effect, but the force produced in such an example is theorized to be exceedingly weak.\n\nIn 1963 Robert Forward, a physicist and aerospace engineer at Hughes Research Laboratories, published a paper showing how within the framework of general relativity \"anti-gravitational\" effects might be achieved. Since all atoms have spin, gravitational permeability may be able to differ from material to material. A strong toroidal gravitational field that acts against the force of gravity could be generated by materials that have nonlinear properties that enhance time-varying gravitational fields. Such an effect would be analogous to the nonlinear electromagnetic permeability of iron making it an effective core (i.e. the doughnut of iron) in a transformer, whose properties are dependent on magnetic permeability. In 1966 Dewitt was first to identify the significance of gravitational effects in superconductors. Dewitt demonstrated that a magnetic-type gravitational field must result in the presence of fluxoid quantization. In 1983, Dewitt's work was substantially expanded by Ross.\n\nFrom 1971 to 1974 Henry William Wallace, a scientist at GE Aerospace was issued with three patents. Wallace used Dewitt's theory to develop an experimental apparatus for generating and detecting a secondary gravitational field, which he named the kinemassic field (now better known as the gravitomagnetic field). In his three patents, Wallace describes three different methods used for detection of the gravitomagnetic field – change in the motion of a body on a pivot, detection of a transverse voltage in a semiconductor crystal, and a change in the specific heat of a crystal material having spin-aligned nuclei. There are no publicly available independent tests verifying Wallace's devices. Such an effect if any would be small. Referring to Wallace's patents, a New Scientist article in 1980 stated \"Although the Wallace patents were initially ignored as cranky, observers believe that his invention is now under serious but secret investigation by the military authorities in the USA. The military may now regret that the patents have already been granted and so are available for anyone to read.\" A further reference to Wallace's patents occur in an electric propulsion study prepared for the Astronautics Laboratory at Edwards Air Force Base which states: \"The patents are written in a very believable style which include part numbers, sources for some components, and diagrams of data. Attempts were made to contact Wallace using patent addresses and other sources but he was not located nor is there a trace of what became of his work. The concept can be somewhat justified on general relativistic grounds since rotating frames of time varying fields are expected to emit gravitational waves.\"\n\nIn 1986 the U.S. Air Force's then Rocket Propulsion Laboratory (RPL) at Edwards Air Force Base solicited \"Non Conventional Propulsion Concepts\" under a small business research and innovation program. One of the six areas of interest was \"Esoteric energy sources for propulsion, including the quantum dynamic energy of vacuum space...\" In the same year BAE Systems launched \"Project Greenglow\" to provide a \"focus for research into novel propulsion systems and the means to power them\"\n\nIn 1988 Kip Thorne et al. published work showing how traversable wormholes can exist in spacetime only if they are threaded by quantum fields generated by some form of exotic matter that has negative energy. In 1993 Scharnhorst and Barton showed that the speed of a photon will be increased if it travels between two Casimir plates, an example of negative energy. In the most general sense, the exotic matter needed to create wormholes would share the repulsive properties of the inflationary energy, dark energy or zero-point radiation of the vacuum. Building on the work of Thorne, in 1994 Miguel Alcubierre proposed a method for changing the geometry of space by creating a wave that would cause the fabric of space ahead of a spacecraft to contract and the space behind it to expand (see Alcubierre drive). The ship would then ride this wave inside a region of flat space, known as a \"warp bubble\" and would not move within this bubble but instead be carried along as the region itself moves due to the actions of the drive.\n\nIn 1992 Evgeny Podkletnov published a heavily debated journal article claiming a specific type of rotating superconductor could shield gravitational force. Independently of this, from 1991 to 1993 Ning Li and Douglas Torr published a number of articles about gravitational effects in superconductors. One finding they derived is the source of gravitomagnetic flux in a type II superconductor material is due to spin alignment of the lattice ions. Quoting from their third paper: \"It is shown that the coherent alignment of lattice ion spins will generate a detectable gravitomagnetic field, and in the presence of a time-dependent applied magnetic vector potential field, a detectable gravitoelectric field.\" The claimed size of the generated force has been disputed by some but defended by others. In 1997 Li published a paper attempting to replicate Podkletnov's results and showed the effect was very small, if it existed at all. Li is reported to have left the University of Alabama in 1999 to found the company \"AC Gravity LLC\". AC Gravity was awarded a U.S. DOD grant for $448,970 in 2001 to continue anti-gravity research. The grant period ended in 2002 but no results from this research were ever made public.\n\nIn 2002 Phantom Works, Boeing's advanced research and development facility in Seattle, approached Evgeny Podkletnov directly. Phantom Works was blocked by Russian technology transfer controls. At this time Lieutenant General George Muellner, the outgoing head of the Boeing Phantom Works, confirmed that attempts by Boeing to work with Podkletnov had been blocked by Moscow, also commenting that \"The physical principles – and Podkletnov's device is not the only one – appear to be valid... There is basic science there. They're not breaking the laws of physics. The issue is whether the science can be engineered into something workable\"\n\nFroning and Roach (2002) put forward a paper that builds on the work of Puthoff, Haisch and Alcubierre. They used fluid dynamic simulations to model the interaction of a vehicle (like that proposed by Alcubierre) with the zero-point field. Vacuum field perturbations are simulated by fluid field perturbations and the aerodynamic resistance of viscous drag exerted on the interior of the vehicle is compared to the Lorentz force exerted by the zero-point field (a Casimir-like force is exerted on the exterior by unbalanced zero-point radiation pressures). They find that the optimized negative energy required for an Alcubierre drive is where it is a saucer-shaped vehicle with toroidal electromagnetic fields. The EM fields distort the vacuum field perturbations surrounding the craft sufficiently to affect the permeability and permittivity of space.\n\nIn 2014 NASA's Eagleworks Laboratories announced that they had successfully validated the use of a Quantum Vacuum Plasma Thruster which makes use of the Casimir effect for propulsion. In 2016 a scientific paper by the team of NASA scientists passed peer review for the first time. The paper suggests that the zero-point field acts as pilot-wave and that the thrust may be due to particles pushing off the quantum vacuum. While peer review doesn’t guarantee that a finding or observation is valid, it does indicate that independent scientists looked over the experimental setup, results, and interpretation and that they could not find any obvious errors in the methodology and that they found the results reasonable. In the paper, the authors identify and discuss nine potential sources of experimental errors, including rogue air currents, leaky electromagnetic radiation, and magnetic interactions. Not all of them could be completely ruled out, and further peer reviewed experimentation is needed in order to rule these potential errors out.\n\n\n"}
