{"id": "1107647", "url": "https://en.wikipedia.org/wiki?curid=1107647", "title": "Alexander polynomial", "text": "Alexander polynomial\n\nIn mathematics, the Alexander polynomial is a knot invariant which assigns a polynomial with integer coefficients to each knot type. James Waddell Alexander II discovered this, the first knot polynomial, in 1923. In 1969, John Conway showed a version of this polynomial, now called the Alexander–Conway polynomial, could be computed using a skein relation, although its significance was not realized until the discovery of the Jones polynomial in 1984. Soon after Conway's reworking of the Alexander polynomial, it was realized that a similar skein relation was exhibited in Alexander's paper on his polynomial.\n\nLet \"K\" be a knot in the 3-sphere. Let \"X\" be the infinite cyclic cover of the knot complement of \"K\". This covering can be obtained by cutting the knot complement along a Seifert surface of \"K\" and gluing together infinitely many copies of the resulting manifold with boundary in a cyclic manner. There is a covering transformation \"t\" acting on \"X\". Consider the first homology (with integer coefficients) of \"X\", denoted formula_1. The transformation \"t\" acts on the homology and so we can consider formula_1 a module over the ring of Laurent polynomials formula_3. This is called the Alexander invariant or Alexander module.\n\nThe module is finitely presentable; a presentation matrix for this module is called the Alexander matrix. If the number of generators, \"r\", is less than or equal to the number of relations, \"s\", then we consider the ideal generated by all \"r\" by \"r\" minors of the matrix; this is the zero'th Fitting ideal or Alexander ideal and does not depend on choice of presentation matrix. If \"r > s\", set the ideal equal to 0. If the Alexander ideal is principal, take a generator; this is called an Alexander polynomial of the knot. Since this is only unique up to multiplication by the Laurent monomial formula_4, one often fixes a particular unique form. Alexander's choice of normalization is to make the polynomial have a positive constant term.\n\nAlexander proved that the Alexander ideal is nonzero and always principal. Thus an Alexander polynomial always exists, and is clearly a knot invariant, denoted formula_5. The Alexander polynomial for the knot configured by only one string is a polynomial of t and then it is the same polynomial for the mirror image knot. Namely, it can not distinguish between the knot and one for its mirror image.\n\nThe following procedure for computing the Alexander polynomial was given by J. W. Alexander in his paper.\n\nTake an oriented diagram of the knot with \"n\" crossings; there are \"n\" + 2 regions of the knot diagram. To work out the Alexander polynomial, first one must create an incidence matrix of size (\"n\", \"n\" + 2). The \"n\" rows correspond to the \"n\" crossings, and the \"n\" + 2 columns to the regions. The values for the matrix entries are either 0, 1, −1, \"t\", −\"t\".\n\nConsider the entry corresponding to a particular region and crossing. If the region is not adjacent to the crossing, the entry is 0. If the region is adjacent to the crossing, the entry depends on its location. The following table gives the entry, determined by the location of the region at the crossing from the perspective of the incoming undercrossing line.\n\nRemove two columns corresponding to adjacent regions from the matrix, and work out the determinant of the new \"n\" by \"n\" matrix. Depending on the columns removed, the answer will differ by multiplication by formula_4. To resolve this ambiguity, divide out the largest possible power of \"t\" and multiply by −1 if necessary, so that the constant term is positive. This gives the Alexander polynomial.\n\nThe Alexander polynomial can also be computed from the Seifert matrix.\n\nAfter the work of J. W. Alexander, Ralph Fox considered a copresentation of the knot group formula_7, and introduced non-commutative differential calculus , which also permits one to compute formula_5. Detailed exposition of this approach about higher Alexander polynomials can be found in the book .\n\nThe Alexander polynomial is symmetric: formula_9 for all knots K.\n\nFurthermore, the Alexander polynomial evaluates to a unit on 1: formula_21.\n\nIt is known that every integral Laurent polynomial which is both symmetric and evaluates to a unit at 1 is the Alexander polynomial of a knot (Kawauchi 1996).\n\nSince the Alexander ideal is principal, formula_28 if and only if the commutator subgroup of the knot group is perfect (i.e. equal to its own commutator subgroup).\n\nFor a topologically slice knot, the Alexander polynomial satisfies the Fox–Milnor condition formula_29 where formula_30 is some other integral Laurent polynomial.\n\nTwice the knot genus is bounded below by the degree of the Alexander polynomial.\n\nMichael Freedman proved that a knot in the 3-sphere is topologically slice; i.e., bounds a \"locally-flat\" topological disc in the 4-ball, if the Alexander polynomial of the knot is trivial (Freedman and Quinn, 1990).\n\nThere are other relations with surfaces and smooth 4-dimensional topology. For example, under certain assumptions, there is a way of modifying a smooth 4-manifold by performing a surgery that consists of removing a neighborhood of a two-dimensional torus and replacing it with a knot complement crossed with \"S\". The result is a smooth 4-manifold homeomorphic to the original, though now the Seiberg–Witten invariant has been modified by multiplication with the Alexander polynomial of the knot.\n\nKnots with symmetries are known to have restricted Alexander polynomials. See the symmetry section in (Kawauchi 1996). Nonetheless, the Alexander polynomial can fail to detect some symmetries, such as strong invertibility.\n\nIf the knot complement fibers over the circle, then the Alexander polynomial of the knot is known to be \"monic\" (the coefficients of the highest and lowest order terms are equal to formula_31). In fact, if formula_32 is a fiber bundle where formula_33 is the knot complement, let formula_34 represent the monodromy, then formula_35 where formula_36 is the induced map on homology.\n\nIf a knot formula_37 is a satellite knot with pattern knot formula_38 (there exists an embedding formula_39 such that formula_40, where formula_41 is an unknotted solid torus containing formula_38), then formula_43, where formula_44 is the integer that represents formula_45 in formula_46.\n\nExamples: For a connect-sum formula_47. If formula_37 is an untwisted Whitehead double, then formula_49.\n\nAlexander proved the Alexander polynomial satisfies a skein relation. John Conway later rediscovered this in a different form and showed that the skein relation together with a choice of value on the unknot was enough to determine the polynomial. Conway's version is a polynomial in \"z\" with integer coefficients, denoted formula_50 and called the Alexander–Conway polynomial (also known as Conway polynomial or Conway–Alexander polynomial).\n\nSuppose we are given an oriented link diagram, where formula_51 are link diagrams resulting from crossing and smoothing changes on a local region of a specified crossing of the diagram, as indicated in the figure. \n\nHere are Conway's skein relations:\n\n\nThe relationship to the standard Alexander polynomial is given by formula_54. Here formula_55 must be properly normalized (by multiplication of formula_56) to satisfy the skein relation formula_57. Note that this relation gives a Laurent polynomial in \"t\".\n\nSee knot theory for an example computing the Conway polynomial of the trefoil.\n\nUsing pseudo-holomorphic curves, and associated a bigraded abelian group, called knot Floer homology, to each isotopy class of knots. The graded Euler characteristic of knot Floer homology is the Alexander polynomial. While the Alexander polynomial gives a lower bound on the genus of a knot, showed that knot Floer homology detects the genus. Similarly, while the Alexander polynomial gives an obstruction to a knot complement fibering over the circle, showed that knot Floer homology completely determines when a knot complement fibers over the circle. The knot Floer homology groups are part of the Heegaard Floer homology family of invariants; see Floer homology for further discussion.\n\n\n"}
{"id": "3724336", "url": "https://en.wikipedia.org/wiki?curid=3724336", "title": "Annuity (European)", "text": "Annuity (European)\n\nUnder European Union law, an annuity is a financial contract which provides an income stream in return for an initial payment with specific parameters. It is the opposite of a settlement funding. A Swiss annuity is not considered a European annuity for tax reasons.\n\nAn immediate annuity is an annuity for which the time between the contract date and the date of the first payment is not longer than the time interval between payments. A common use for an immediate annuity is to provide a pension to a retired person or persons.\n\nIt is a financial contract which makes a series of payments with certain characteristics:\n\nAn annuity certain pays the annuitant for a number of years designated. This option is not suitable for retirement income, as the person may outlive the number of years the annuity will pay.\n\nA life annuity or lifetime immediate annuity is most often used to provide an income in old age (i.e., a pension). This type of annuity may be purchased from an insurance (Ireland and the UK, Life Assurance) company.\n\nThis annuity can be compared to a loan which is made by the purchaser to the issuing company, who then pay back the original capital with interest to the \"annuitant\" on whose life the annuity is based. The assumed period of the loan is based on the life expectancy of the annuitant but life annuities are payable until the death of the last surviving annuitant. In order to guarantee that the income continues for life, the investment relies on cross-subsidy. Because an \"annuity population\" can be expected to have a distribution of lifespans around the population's mean (average) age, those dying earlier will support those living longer (longevity insurance).\n\nCross-subsidy remains one of the most effective ways of spreading a given amount of capital and investment return over a lifetime without the risk of funds running out.\n\nAlthough this will reduce the available payments, an annuity can be arranged to continue until the death of the last survivor of two or more people. For example, many annuities continue to pay out (perhaps at a reduced rate) to the spouse of the main annuitant after his or her death, for as long as the spouse survives. The annuity paid to the spouse is called a reversionary annuity or survivorship annuity. However, if the annuitant is in good health, it may be more beneficial to select the higher payout option on their life only and purchase a life insurance policy that would pay income to the survivor.\n\nOther features such as a minimum guaranteed payment period irrespective of death, known as life with period certain, or \"escalation\" where the payment rises by inflation or a fixed rate annually can also be purchased.\n\nAnnuities with guaranteed periods are available from most providers. In such a product, if death takes place within the guaranteed period, payments continue to be made to a nominated beneficiary.\n\nImpaired life annuities for smokers or those with a particular illness are also available from some insurance companies. Since the life expectancy is reduced, the annuity rate is better (i.e. a higher annuity for the same initial payment). This can have the unfortunate appearance of one \"betting against\" the nominee.\n\nLife annuities are priced based on the probability of the nominee surviving to receive the payments. Longevity insurance is a form of annuity that defers commencement of the payments until very late in life. A common longevity contract would be purchased at or before retirement but would not commence payments until 20 years after retirement. If the nominee dies before payments commence there is no payable benefit. This drastically reduces the cost of the annuity while still providing protection against outliving one's resources.\n\nThe second usage for the term \"annuity\" came into its own during the 1970s. This is a \"deferred annuity\" and is a vehicle for accumulating savings, and eventually distributing them either as an immediate annuity or as a lump-sum payment. Note that this is different from an immediate annuity.\n\nUnder the heading of deferred annuities, there are contracts which may be similar to \nContracts may also be linked to other investments such as property (real estate) or government bonds, or any combination of the above selected by the investor or his advisors.\nAll varieties of deferred annuities owned by individuals have one thing in common in many jurisdictions: any increase in account values is \"not\" taxed until those gains are withdrawn. This is also known as tax-deferred growth.\n\nTo complete the definitions here, a deferred annuity where the benefits are fixed at outside, either in terms of a lump sum or an annuity, can be called a \"fixed deferred annuity\". A deferred annuity that permits allocations to stock or bond funds and for which the account value is not guaranteed to stay above the initial amount invested is called a \"variable annuity\". \n\nBy law, an annuity contract can only be issued by an insurance company. They are distributed by, and available for purchase from, duly licensed bank, stock brokerage, and insurance company representatives. Some annuities may also be purchased directly from the issuer, i.e., the insurance company writing the contract.\n\nIn a typical immediate annuity contract, an individual would pay a lump sum or a series of payments (sometimes called \"annuity considerations\") to an insurance company, and in return pay the annuitant a series of periodic payments for the rest of their life. The exact terms of an annuity product are set out in the contract.\n\nIn common with other types of insurance contract, both immediate and deferred annuities will typically pay commission to the sales person (or \"advisor\").\nA wide variety of features have been developed by annuity companies in order to make their products more attractive. These include death benefit options and living benefit options.\n\nBecause immediate annuities generally provide a series of guaranteed payments, the annuity company normally matches its liabilities with government bonds and other high grade bonds, and the market yield available on these bonds largely determines the retail pricing of the annuities. (The companies are usually required by law to invest their funds in this way, to reduce the risk of default.)\n\nThese investments are generally regarded as less risky than other investments, such as those linked to the stock market, and probably offer a lower expected return. However fixed annuities do not protect the purchaser against the effects of inflation, which is a material risk.\n\nFor many elderly people, the financial risk of living longer than expected and running out of money is a bigger risk than investment risks such as exposure to a falling stock market. Immediate annuities protect against this risk.\n\nDeferred pensions are often used as a savings vehicle by higher rate taxpayers, as in some jurisdictions they get higher rate tax relief on their pension contributions and their fund accumulates without investment returns being subject to tax. The proceeds will be taxed when they are taken as benefits, but maybe at a lower rate. Those in lower tax brackets may be told to avoid deferred pensions because they may not be able to recoup the charges made by the annuity company. (In some jurisdictions, some or all of the proceeds must by law be applied to purchase a pension.)\n\nActuarial formulae are used to model annuities and determine their price.\n\nIn technical language an annuity is said to be payable for an assigned \"status\", this being a general word chosen in preference to such words as \"time\", \"term\" or \"period\", because it may include more readily either a term of years certain, or a life or combination of lives. The \"magnitude\" of the annuity is the sum to be paid (and received) in the course of each year. Thus, if £100 is to be received each year by a person, he is said to have \"an annuity of £100\". If the payments are made half-yearly, it is sometimes said that he has \"a half-yearly annuity of £100\"; but to avoid ambiguity, it is more commonly said he has \"an annuity of £100, payable by half-yearly instalments\". An annuity is considered as accruing during each instant of the status for which it is enjoyed, although it is only payable at fixed intervals. If the enjoyment of an annuity is postponed until after the lapse of a certain number of years, the annuity is said to be \"deferred\". If an annuity, instead of being payable at the end of each year, half-year, etc., is payable in advance, it is called an \"annuity-due\". The holder of an annuity is called an \"annuitant\", and the person on whose life the annuity depends is called the \"nominee\".\n\nUpon immediate annuitization, a wide variety of options are available in the way the stream of payments is paid. If the annuity is paid over a fixed period independent of any contingency, it is known as an \"annuity with period certain\", or just \"annuity certain\"; if it is to continue for ever, it is called a \"perpetuity\"; and if in the latter case it is not to commence until after a term of years, it is called a \"deferred perpetuity\". An annuity depending on the continuance of an assigned life or lives would commonly be called a \"life annuity\", but also known as a \"life-contingent annuity\" or simply \"lifetime annuity\"; but more commonly the simple term \"annuity\" is understood to mean a life annuity, unless the contrary is stated. The payments can also be paid over the lifetime of the nominee(s) or for a fixed period, whichever is longer. This is known as \"life with period certain\".\n\nA hybrid of these is when the payments stop at death, but also after a predetermined number of payments, if this is earlier: known as a \"temporary life annuity\". The difference with the period certain annuity is that the period certain annuity will keep paying after the death of the nominee until the period is completed.\n\nIf not otherwise stated, it is always understood that an annuity is payable yearly, and that the annual payment (or rent, as it is sometimes called) is a single currency unit.\n\nInstances of perpetuities are the dividends upon the public stocks in England, France and some other countries. Thus, although it is usual to speak of £100 consols, the reality is the yearly dividend which the government pays by quarterly instalments. The practice of the French in this is arguably more logical. In speaking of their public funds (\"rentes\") they do not mention the ideal capital sum, but speak of the annuity or annual payment that is received by the public creditor. Other instances of perpetuities are the incomes derived from the debenture stocks of railway companies, also the feu-duties commonly payable on house property in Scotland. The number of years' purchase which the perpetual annuities granted by a government or a railway company realize in the open market, forms a very simple test of the credit of the various governments or railways.\n\nIn the United Kingdom, the income from \"compulsory purchase annuities\" purchased with pension funds or by an employer immediately on retirement (a \"Hancock\" annuity) is treated as taxable income. The income from \"purchased life annuities\", bought by any other means, has an element which is considered return of capital, and only the excess over this is considered a gain that is subject to income tax. The element considered capital return is based on life expectancy and will therefore increase with age.\n\nBecause of cross-subsidy and the guarantees an annuity can give against running out of income and becoming dependent on state welfare in old age, annuities often have a favourable tax treatment, which may affect how attractive they are relative to other investments.\n\nImmediate annuities are a compulsory feature of certain pension saving schemes in some countries, where the government grants tax deductions, provided that savings are paid into a fund which can only (or mainly) be withdrawn as an annuity. The Netherlands has such schemes and United Kingdom used to until A day. From 2003 the tax deduction in the Netherlands is only allowed if, without additional savings, the old age income would be less than 70% of the current income. The French government currently honors a very unusual debt contract: an annuity that was issued in 1738 and currently yields €1.20 per year.\n\nIn the United Kingdom contributions into pension savings are generally net of income tax (i.e. tax relief is available), up to certain limits. On retirement if an annuity is not purchased, retirement income up until the age of 75 can be drawn from the pension fund by using \"pension income withdrawal\" commonly known as \"income drawdown\". This is an unsecured pension as opposed to an annuity which is a secured backed pension. Unsecured pensions operate under age-related income limits calculated by the Government Actuarial Department to prevent the fund being eroded too fast. Before A-day, individuals could vary withdrawals between 35% and 100% of a maximum limit, recalculated every three years at what was known as the \"triennial review\". Following changes introduced by HMRC as part of the A-day legislation, individuals can now draw an income between zero and 120% of the \"GAD rate\". On reaching 75, the individual must then secure their pension fund by the purchase of an annuity, except that up to 25% of the fund can be taken as tax-free cash, also known as the pension commencement lump sum, or enter into an alternatively secured pension (ASP). Under an ASP arrangement the rate of income must fall between 55% and 90% of the GAD rate for a 75-year-old. The GAD rates are subject to periodic review and are based on the return of a level, single life annuity paid monthly in arrears without any guarantee or value protection for an individual in good health. These rates are in turn largely dependent on long-term gilt yields and mortality data.\n\nUnsecured or alternatively secured pensions carry both the investment risk of the invested pension fund and mortality drag that occurs from the loss of cross-subsidy and advancing average age expectancy that occurs in the time over which annuity purchase is delayed.\n\nTerminable annuities are employed in the system of British public finance as a means of reducing the National Debt. This result is attained by substituting for a perpetual annual charge (or one lasting until the capital which it represents can be paid off \"en bloc\"), an annual charge of a larger amount, but lasting for a short term. The latter is so calculated as to pay off, during its existence, the capital which it replaces, with interest at an assumed or agreed rate, and under specified conditions. The practical effect of the substitution of a terminable annuity for an obligation of longer currency is to bind the present generation of citizens to increase its own obligations in the present and near future in order to diminish those of its successors. This end might be attained in other ways; for instance, by setting aside out of revenue a fixed annual sum for the purchase and cancellation of debt (Pitt's method, in intention), or by fixing the annual debt charge at a figure sufficient to provide a margin for reduction of the principal of the debt beyond the amount required for interest (Sir Stafford Northcote's method), or by providing an annual surplus of revenue over expenditure (the \"Old Sinking Fund\"), available for the same purpose. All these methods have been tried in the course of British financial history, and the second and third of them are still employed; but on the whole the method of terminable annuities has been the one preferred by chancellors of the exchequer and by parliament.\n\nTerminable annuities, as employed by the British government, fall under two heads:\nThe important difference between these two classes is that an annuity under (1), once created, cannot be modified except with the holder's consent, i.e. is practically unalterable without a breach of public faith; whereas an annuity under (2) can, if necessary, be altered by interdepartmental arrangement under the authority of parliament. Thus annuities of class (1) fulfil most perfectly the object of the system as explained above; while those of class (2) have the advantage that in times of emergency their operation can be suspended without any inconvenience or breach of faith, with the result that the resources of government can on such occasions be materially increased, apart from any additional taxation. For this purpose it is only necessary to retain as a charge on the income of the year a sum equal to the (smaller) perpetual charge which was originally replaced by the (larger) terminable charge, whereupon the difference between the two amounts is temporarily released, while ultimately the increased charge is extended for a period equal to that for which it is suspended.\n\nAnnuities of class (1) were first instituted in 1808, but were later regulated by an act of 1829. They may be granted either for a specified life, or two lives, or for an arbitrary term of years; and the consideration for them may take the form either of cash or of government stock, the latter being cancelled when the annuity is set up.\n\nAnnuities (2) held by government departments date from 1863. They were created in exchange for permanent debt surrendered for cancellation, the principal operations having been effected in 1863, 1867, 1870, 1874, 1883 and 1899.\n\nAnnuities of this class do not affect the public at all, except of course in their effect on the market for government securities. They are merely financial operations between the government, in its capacity as the banker of savings banks and other funds, and itself, in the capacity of custodian of the national finances. Savings bank depositors are not concerned with the manner in which government invests their money, their rights being confined to the receipt of interest and the repayment of deposits upon specified conditions. The case is, however, different as regards forty millions of consols (included in the above figures), belonging to suitors in chancery, which were cancelled and replaced by a terminable annuity in 1883. As the liability to the suitors in that case was for a specified amount of stock, special arrangements were made to ensure the ultimate replacement of the precise amount of stock cancelled.\n\nThe mathematical theory of life annuities is based upon a knowledge of the rate of mortality among mankind in general, or among the particular class of persons on whose lives the annuities depend, see actuarial present value. In practice simply tables may be used, which vary in different places, but which are easily accessible.\n\nAbraham Demoivre, in his \"Annuities on Lives\", put forth a very simple law of mortality: out of 86 children born alive, 1 will die every year until the last dies between the ages of 85 and 86. This law agreed sufficiently well at the Middle Ages of life with the mortality deduced from the best observations of his time; but, as observations became more exact, the approximation was found to be not sufficiently close. This was particularly the case when it was desired to obtain the value of joint life, contingent or other complicated benefits. Therefore, Demoivre's law is devoid of practical utility. No simple formula has sufficient accuracy.\n\nThe rate of mortality at each age is, therefore, in practice usually determined by a series of figures deduced from observation; and the value of an annuity at any age is found from these numbers by means of a series of arithmetical calculations.\n\nThe first writer who is known to have attempted to obtain, on correct mathematical principles, the value of a life annuity, was Jan De Witt, grand pensionary of Holland and West Friesland. Our knowledge of his writings on the subject is derived from two papers contributed by Frederick Hendriks to the \"Assurance Magazine\", vol. ii. (1852) p. 222, and vol. in. p. 93. The former of these contains a translation of De Witt's report upon the value of life annuities, which was prepared in consequence of the resolution passed by the states-general, on 25 April 1671, to negotiate funds by life annuities, and which was distributed to the members on 30 July 1671. The latter contains the translation of a number of letters addressed by De Witt to Burgomaster Johan Hudde, bearing dates from September 1670 to October 1671. The existence of De Witt's report was well known among his contemporaries, and Hendriks collected a number of extracts from various authors referring to it; but the report is not contained in any collection of his works extant, and had been entirely lost for 180 years, until Hendriks discovered it among the state archives of Holland in company with the letters to Hudde. It was the very first document on the subject that was ever written.\n\nIt appears that it had long been the practice in Holland for life annuities to be granted to nominees of any age, in the constant proportion of double the rate of interest allowed on stock; that is to say, if the towns were borrowing money at 6%, they would be willing to grant a life annuity at 12%, and so on. De Witt states that \"annuities have been sold, even in the present century, first at six years' purchase, then at seven and eight; and that the majority of all life annuities now current at the country's expense were obtained at nine years' purchase\"; but that the price had been increased in the course of a few years from eleven years' purchase to twelve, and from twelve to fourteen. He also states that the rate of interest had been successively reduced from 6-¼% to 5%, and then to 4%. The principal object of his report is to prove that, taking interest at 4%, a life annuity was worth at least sixteen years' purchase; and, in fact, that an annuitant purchasing an annuity for the life of a young and healthy nominee at sixteen years' purchase, made an excellent bargain.\n\nHe argues that it is more to the advantage, both of the country and of the private investor, that the public loans should be raised by way of grant of life annuities rather than perpetual annuities. It appears from De Witt's correspondence with Hudde, that the rate of mortality assumed was deduced from the mortality that had actually prevailed among the nominees on whose lives annuities had been granted in former years. De Witt appears to have come to the conclusion that the probability of death is the same in any half-year from the age of 3 to 53 inclusive; that in the next ten years, from 53 to 63, the probability is greater in the ratio of 3 to 2; that in the next ten years, from 63 to 73, it is greater in the ratio of 2 to 1; and in the next seven years, from 73 to 80, it is greater in the ratio of 3 to 1; and he places the limit of human life at 80. If a mortality table of the usual form is deduced from these suppositions, out of 212 persons alive at the age of 3, 2 will die every year up to 53, 3 in each of the ten years from 53 to 63, 4 in each of the next ten years from 63 to 73, and 6 in each of the next seven years from 73 to 80, when all will be dead.\n\nDe Witt calculates the value of an annuity in the following way. Assume that annuities on 10,000 lives each ten years of age, which satisfy the Hm mortality table, have been purchased. Of these nominees 79 will die before attaining the age of 11, and no annuity payment will be made in respect of them; none will die between the ages of 11 and 12, so that annuities will be paid for one year on 9921 lives; 40 attain the age of 12 and die before 13, so that two payments will be made with respect to these lives. Reasoning in this way we see that the annuities on 35 of the nominees will be payable for three years; on 40 for four years, and so on. Proceeding thus to the end of the table, 15 nominees attain the age of 95, 5 of whom die before the age of 96, so that 85 payments will be paid in respect of these 5 lives. Of the survivors all die before attaining the age of 97, so that the annuities on these lives will be payable for 86 years. Having previously calculated a table of the values of annuities certain for every number of years up to 86, the value of all the annuities on the 10,000 nominees will be found by taking 40 times the value of an annuity for 2 years, 35 times the value of an annuity for 3 years, and so on—the last term being the value of 10 annuities for 86 years—and adding them together; and the value of an annuity on one of the nominees will then be found by dividing by 10,000. Before leaving the subject of De Witt, we may mention that we find in the correspondence a distinct suggestion of the law of mortality that bears the name of Demoivre. In De Witt's letter, dated 27 October 1671 (\"Ass. Mag\". vol. iii. p. 107), he speaks of a \"provisional hypothesis\" suggested by Hudde, that out of 80 young lives (who, from the context, may be taken as of the age 6) about 1 dies annually. In strictness, therefore, the law in question might be more correctly termed Hudde's than Demoivre's.\n\nDe Witt's report being thus of the nature of an unpublished state paper, although it contributed to its author's reputation, did not contribute to advance the exact knowledge of the subject; and the author to whom the credit must be given of first showing how to calculate the value of an annuity on correct principles is Edmund Halley. He gave the first approximately correct mortality table (deduced from the records of the numbers of deaths and baptisms in the city of Breslau), and showed how it might be employed to calculate the value of an annuity on the life of a nominee of any age.\n\nPreviously to Halley's time, and apparently for many years subsequently, all dealings with life annuities were based upon mere conjectural estimates. The earliest known reference to any estimate of the value of life annuities rose out of the requirements of the Falcidian law, which in 40 B.C. was adopted in the Roman Empire, and which declared that a testator should not give more than three-fourths of his property in legacies, so that at least one-fourth must go to his legal representatives. It is easy to see how it would occasionally become necessary, while this law was in force, to value life annuities charged upon a testator's estate. Aemilius Macer (A.D. 230) states that the method which had been in common use at that time was as follows:--From the earliest age until 30 take 30 years' purchase, and for each age after 30 deduct 1 year. It is obvious that no consideration of compound interest can have entered into this estimate; and it is easy to see that it is equivalent to assuming that all persons who attain the age of 30 will certainly live to the age of 60, and then certainly die. Compared with this estimate, that which was propounded by the praetorian prefect Ulpian was a great improvement. His table is as follows:\n\nHere also we have no reason to suppose that the element of interest was taken into consideration; and the assumption, that between the ages of 40 and 50 each addition of a year to the nominee's age diminishes the value of the annuity by one year's purchase, is equivalent to assuming that there is no probability of the nominee dying between the ages of 40 and 50. Considered, however, simply as a table of the average duration of life, the values are fairly accurate. At all events, no more correct estimate appears to have been arrived at until the close of the 17th century.\n\nThe mathematics of annuities has been very fully treated in Demoivre's \"Treatise on Annuities\" (1725); Simpson's \"Doctrine of Annuities and Reversions\" (1742); P. Gray, \"Tables and Formulae\"; Baily's \"Doctrine of Life Annuities\"; there are also innumerable compilations of \"Valuation Tables\" and \"Interest Tables\", by means of which the value of an annuity at any age and any rate of interest may be found. See also the article interest, and especially that on insurance.\n\n\"Commutation tables\", aptly so named in 1840 by Augustus De Morgan (see his paper \"On the Calculation of Single Life Contingencies,\" \"Assurance Magazine\", xii. 328), show the proportion in which a benefit due at one age ought to be changed, so as to retain the same value and be due at another age. The earliest known specimen of a commutation table is contained in William Dale's \"Introduction to the Study of the Doctrine of Annuities\", published in 1772. A full account of this work is given by F. Hendriks in the second number of the \"Assurance Magazine\", pp. 15–17. William Morgan's \"Treatise on Assurances\", 1779, also contains a commutation table. Morgan gives the table as furnishing a convenient means of checking the correctness of the values of annuities found by the ordinary process. It may be assumed that he was aware that the table might be used for the direct calculation of annuities; but he appears to have been ignorant of its other uses.\n\nThe first author who fully developed the powers of the table was John Nicholas Tetens, a native of Schleswig, who in 1785, while professor of philosophy and mathematics at Kiel, published in the German language an \"Introduction to the Calculation of Life Annuities and Assurances\". This work appears to have been quite unknown in England until F. Hendriks gave, in the first number of the \"Assurance Magazine\", pp. 1–20 (Sept. 1850), an account of it, with a translation of the passages describing the construction and use of the commutation table, and a sketch of the author's life and writings, to which we refer the reader who desires fuller information. It may be mentioned here that Tetens also gave only a specimen table, apparently not imagining that persons using his work would find it extremely useful to have a series of commutation tables, calculated and printed ready for use.\n\nThe use of the commutation table was independently developed in England-apparently between the years 1788 and 1811—by George Barrett, of Petworth, Sussex, who was the son of a yeoman farmer, and was himself a village schoolmaster, and afterwards farm steward or bailiff. It has been usual to consider Barrett as the originator in England of the method of calculating the values of annuities by means of a commutation table, and this method is accordingly sometimes called Barrett's method. (It is also called the commutation method and the columnar method.) Barrett's method of calculating annuities was explained by him to Francis Baily in 1811, and was first made known to the world in a paper written by the latter and read before the Royal Society in 1812.\n\nBy what has been universally considered an unfortunate error of judgment, this paper was not recommended by the council of the Royal Society to be printed, but it was given by Baily as an appendix to the second issue (in 1813) of his work on life annuities and assurances. Barrett had calculated extensive tables, and with Baily's aid attempted to get them published by subscription, but without success; and the only printed tables calculated according to his manner, besides the specimen tables given by Baily, are the tables contained in Babbage's \"Comparative View of the various Institutions for the Assurance of Lives\", 1826.\n\nIn 1825 Griffith Davies published his \"Tables of Life Contingencies\", a work which contains, among others, two tables, which are confessedly derived from Baily's explanation of Barrett's tables.\n\nThose who desire to pursue the subject further can refer to the appendix to Baily's \"Life Annuities and Assurances\", De Morgan's paper \"On the Calculation of Single Life Contingencies,\" \"Assurance Magazine\", xii. 348-349; Gray's \"Tables and Formulae\" chap. viii.; the preface to Davies's \"Treatise on Annuities\"; also Hendriks's papers in the \"Assurance Magazine\", No. 1, p. 1, and No. 2, p. 12; and in particular De Morgan's \"Account of a Correspondence between Mr George Barrett and Mr Francis Baily,\" in the \"Assurance Magazine\", vol. iv. p. 185.\n\nThe principal commutation tables published in England are contained in the following works:--David Jones, \"Value of Annuities and Reversionary Payments\", issued in parts by the Useful Knowledge Society, completed in 1843; Jenkin Jones, \"New Rate of Mortality\", 1843; G. Davies, \"Treatise on Annuities\", 1825 (issued 1855); David Chisholm, \"Commutation Tables\", 1858; Nelson's \"Contributions to Vital Statistics\", 1857; Jardine Henry, \"Government Life Annuity Commutation Tables\", 1866 and 1873; \"Institute of Actuaries Life Tables\", 1872; R. P. Hardy, \"Valuation Tables\", 1873; and Dr William Farr's contributions to the sixth (1844), twelfth (1849), and twentieth (1857) \"Reports\" of the Registrar General in England (English Tables, I. 2), and to the \"English Life Table\", 1864.\n\nThe theory of annuities may be further studied in the discussions in the English \"Journal of the Institute of Actuaries\". The institute was founded in 1848, the first sessional meeting being held in January 1849. Its establishment has contributed in various ways to promote the study of the theory of life contingencies. Among these may be specified the following:--Before it was formed, students of the subject worked for the most part alone, and without any concert; and when any person had made an improvement in the theory, it had little chance of becoming publicly known unless he wrote a formal treatise on the whole subject. But the formation of the institute led to much greater interchange of opinion among actuaries, and afforded them a ready means of making known to their professional associates any improvements, real or supposed, that they thought they had made. Again, the discussions which follow the reading of papers before the institute have often served, first, to bring out into bold relief differences of opinion that were previously unsuspected, and afterwards to soften down those differences,--to correct extreme opinions in every direction, and to bring about a greater agreement of opinion on many important subjects. In no way, probably, have the objects of the institute been so effectually advanced as by the publication of its \"Journal\". The first number of this work, which was originally called the \"Assurance Magazine\", appeared in September 1850, and it has been continued quarterly down to the present time. It was originated by the public spirit of two well-known actuaries (Mr Charles Jellicoe and Mr Samuel Brown), and was adopted as the organ of the Institute of Actuaries in 1852, and called the \"Assurance Magazine and Journal of the Institute of Actuaries\", Mr Jellicoe continuing to be the editor,--a post he held until the year 1867, when he was succeeded by Mr T. B. Sprague (who contributed to the 9th edition of this Encyclopaedia an elaborate article on \"Annuities,\" on which the above account is based). The name was again changed in 1866, the words \"Assurance Magazine\" being dropped; but in the following year it was considered desirable to resume these, for the purpose of showing the continuity of the publication, and it is now called the \"Journal of the Institute of Actuaries and Assurance Magazine\". This work contains not only the papers read before the institute (to which have been appended of late years short abstracts of the discussions on them), and many original papers which were unsuitable for reading, together with correspondence, but also reprints of many papers published elsewhere, which from various causes had become difficult of access to the ordinary reader, among which may be specified various papers which originally appeared in the \"Philosophical Transactions\", the \"Philosophical Magazine\", the \"Mechanics' Magazine\", and the \"Companion to the Almanac\"; also translations of various papers from the French, German, and Danish. Among the useful objects which the continuous publication of the \"Journal\" of the institute has served, we may specify in particular two:--that any supposed improvement in the theory was effectually submitted to the criticisms of the whole actuarial profession, and its real value speedily discovered; and that any real improvement, whether great or small, being placed on record, successive writers have been able, one after the other, to take it up and develop it, each commencing where the previous one had left off.\n"}
{"id": "1734238", "url": "https://en.wikipedia.org/wiki?curid=1734238", "title": "Balanced set", "text": "Balanced set\n\nIn linear algebra and related areas of mathematics a balanced set, circled set or disk in a vector space (over a field \"K\" with an absolute value function formula_1) is a set \"S\" such that for all scalars formula_2 with formula_3\n\nwhere\n\nThe balanced hull or balanced envelope for a set \"S\" is the smallest balanced set containing \"S\". It can be constructed as the intersection of all balanced sets containing \"S\".\n\n\n\n\n"}
{"id": "24277294", "url": "https://en.wikipedia.org/wiki?curid=24277294", "title": "Blossom algorithm", "text": "Blossom algorithm\n\nThe blossom algorithm is an algorithm in graph theory for constructing maximum matchings on graphs. The algorithm was developed by Jack Edmonds in 1961, and published in 1965. Given a general graph \"G\" = (\"V\", \"E\"), the algorithm finds a matching \"M\" such that each vertex in \"V\" is incident with at most one edge in \"M\" and |\"M\"| is maximized. The matching is constructed by iteratively improving an initial empty matching along augmenting paths in the graph. Unlike bipartite matching, the key new idea is that an odd-length cycle in the graph (blossom) is contracted to a single vertex, with the search continuing iteratively in the contracted graph.\n\nA major reason that the blossom algorithm is important is that it gave the first proof that a maximum-size matching could be found using a polynomial amount of computation time. Another reason is that it led to a linear programming polyhedral description of the matching polytope, yielding an algorithm for min-\"weight\" matching. \nAs elaborated by Alexander Schrijver, further significance of the result comes from the fact that this was the first polytope whose proof of integrality \"does not simply follow just from total unimodularity, and its description was a breakthrough in polyhedral combinatorics.\"\n\nGiven \"G\" = (\"V\", \"E\") and a matching \"M\" of \"G\", a vertex \"v\" is exposed if no edge of \"M\" is incident with \"v\". A path in \"G\" is an alternating path, if its edges are alternately not in \"M\" and in \"M\" (or in \"M\" and not in \"M\"). An augmenting path \"P\" is an alternating path that starts and ends at two distinct exposed vertices. Note that the number of unmatched edges in an augmenting path is greater by one than the number of matched edges, and hence the total number of edges in an augmenting path is odd. A matching augmentation along an augmenting path \"P\" is the operation of replacing \"M\" with a new matching formula_1.\n\nBy Berge's lemma, matching \"M\" is maximum if and only if there is no \"M\"-augmenting path in \"G\". Hence, either a matching is maximum, or it can be augmented. Thus, starting from an initial matching, we can compute a maximum matching by augmenting the current matching with augmenting paths as long as we can find them, and return whenever no augmenting paths are left. We can formalize the algorithm as follows:\n\nWe still have to describe how augmenting paths can be found efficiently. The subroutine to find them uses blossoms and contractions.\n\nGiven \"G\" = (\"V\", \"E\") and a matching \"M\" of \"G\", a \"blossom\" \"B\" is a cycle in \"G\" consisting of \"2k + 1\" edges of which exactly \"k\" belong to \"M\", and where one of the vertices \"v\" of the cycle (the \"base\") is such that there exists an alternating path of even length (the \"stem\") from \"v\" to an exposed vertex \"w\".\n\nFinding Blossoms:\n\nDefine the contracted graph \"G’\" as the graph obtained from \"G\" by contracting every edge of \"B\", and define the contracted matching \"M’\" as the matching of \"G’\" corresponding to \"M\".\n\n\"G’\" has an \"M’\"-augmenting path if and only if \"G\" has an \"M\"-augmenting path, and that any \"M’\"-augmenting path \"P’\" in \"G’\" can be lifted to an \"M\"-augmenting path in \"G\" by undoing the contraction by \"B\" so that the segment of \"P’\" (if any) traversing through \"v\" is replaced by an appropriate segment traversing through \"B\". In more detail:\n\n\n\nThus blossoms can be contracted and search performed in the contracted graphs. This reduction is at the heart of Edmonds' algorithm.\n\nThe search for an augmenting path uses an auxiliary data structure consisting of a forest \"F\" whose individual trees correspond to specific portions of the graph \"G\". In fact, the forest \"F\" is the same that would be used to find maximum matchings in bipartite graphs (without need for shrinking blossoms).\nIn each iteration the algorithm either (1) finds an augmenting path, (2) finds a blossom and recurses onto the corresponding contracted graph, or (3) concludes there are no augmenting paths. The auxiliary structure is built by an incremental procedure discussed next.\n\nThe construction procedure considers vertices \"v\" and edges \"e\" in \"G\" and incrementally updates \"F\" as appropriate. If \"v\" is in a tree \"T\" of the forest, we let \"root(v)\" denote the root of \"T\". If both \"u\" and \"v\" are in the same tree \"T\" in \"F\", we let \"distance(u,v)\" denote the length of the unique path from \"u\" to \"v\" in \"T\".\n\nThe following four figures illustrate the execution of the algorithm. Dashed lines indicate edges that are currently not present in the forest. First, the algorithm processes an out-of-forest edge that causes the expansion of the current forest (lines B10 – B12).\n\nNext, it detects a blossom and contracts the graph (lines B20 – B21).\n\nFinally, it locates an augmenting path P′ in the contracted graph (line B22) and lifts it to the original graph (line B23). Note that the ability of the algorithm to contract blossoms is crucial here; the algorithm cannot find \"P\" in the original graph directly because only out-of-forest edges between vertices at even distances from the roots are considered on line B17 of the algorithm.\n\nThe forest \"F\" constructed by the \"find_augmenting_path()\" function is an alternating forest.\n\nEach iteration of the loop starting at line B09 either adds to a tree \"T\" in \"F\" (line B10) or finds an augmenting path (line B17) or finds a blossom (line B20). It is easy to see that the running time is formula_6. Micali and Vazirani show an algorithm that constructs maximum matching in formula_7 time.\n\nThe algorithm reduces to the standard algorithm for matching in bipartite graphs when \"G\" is bipartite. As there are no odd cycles in \"G\" in that case, blossoms will never be found and one can simply remove lines B20 – B24 of the algorithm.\n\nThe matching problem can be generalized by assigning weights to edges in \"G\" and asking for a set \"M\" that produces a matching of maximum (minimum) total weight. The weighted matching problem can be solved by a combinatorial algorithm that uses the unweighted Edmonds's algorithm as a subroutine. Kolmogorov provides an efficient C++ implementation of this.\n"}
{"id": "1644938", "url": "https://en.wikipedia.org/wiki?curid=1644938", "title": "C0-semigroup", "text": "C0-semigroup\n\nIn mathematics, a \"C\"-semigroup, also known as a strongly continuous one-parameter semigroup, is a generalization of the exponential function. Just as exponential functions provide solutions of scalar linear constant coefficient ordinary differential equations, strongly continuous semigroups provide solutions of linear constant coefficient ordinary differential equations in Banach spaces. Such differential equations in Banach spaces arise from e.g. delay differential equations and partial differential equations.\n\nFormally, a strongly continuous semigroup is a representation of the semigroup (R,+) on some Banach space \"X\" that is continuous in the strong operator topology. Thus, strictly speaking, a strongly continuous semigroup is not a semigroup, but rather a continuous representation of a very particular semigroup.\n\nA strongly continuous semigroup on a Banach space formula_1 is a map\nformula_2\nsuch that\nThe first two axioms are algebraic, and state that formula_8 is a representation of the semigroup formula_9; the last is topological, and states that the map formula_8 is continuous in the strong operator topology.\n\nThe infinitesimal generator \"A\" of a strongly continuous semigroup \"T\" is defined by\n\nwhenever the limit exists. The domain of \"A\", \"D\"(\"A\"), is the set of \"x∈X\" for which this limit does exist; \"D\"(\"A\") is a linear subspace and \"A\" is linear on this domain. The operator \"A\" is closed, although not necessarily bounded, and the domain is dense in \"X\".\n\nThe strongly continuous semigroup \"T\" with generator \"A\" is often denoted by the symbol \"e\". This notation is compatible with the notation for matrix exponentials, and for functions of an operator defined via functional calculus (for example, via the spectral theorem).\n\nA uniformly continuous semigroup is a strongly continuous semigroup \"T\" such that\n\nholds. In this case, the infinitesimal generator \"A\" of \"T\" is bounded and we have\n\nand\n\nConversely, any bounded operator\n\nis the infinitesimal generator of a uniformly continuous semigroup given by\n\nThus, a linear operator \"A\" is the infinitesimal generator of a uniformly continuous semigroup if and only if \"A\" is a bounded linear operator. If \"X\" is a finite-dimensional Banach space, then any strongly continuous semigroup is a uniformly continuous semigroup. For a strongly continuous semigroup which is not a uniformly continuous semigroup the infinitesimal generator \"A\" is not bounded. In this case, formula_17 does not need to converge.\n\nConsider the abstract Cauchy problem:\n\nwhere \"A\" is a closed operator on a Banach space \"X\" and \"x∈X\". There are two concepts of solution of this problem:\n\nAny classical solution is a mild solution. A mild solution is a classical solution if and only if it is continuously differentiable.\n\nThe following theorem connects abstract Cauchy problems and strongly continuous semigroups.\n\nTheorem Let \"A\" be a closed operator on a Banach space \"X\". The following assertions are equivalent:\nWhen these assertions hold, the solution of the Cauchy problem is given by \"u\"(\"t\") = \"T\"(\"t\")\"x\" with \"T\" the strongly continuous semigroup generated by \"A\".\n\nIn connection with Cauchy problems, usually a linear operator \"A\" is given and the question is whether this is the generator of a strongly continuous semigroup. Theorems which answer this question are called generation theorems. A complete characterization of operators that generate strongly continuous semigroups is given by the Hille-Yosida theorem. Of more practical importance are however the much easier to verify conditions given by the Lumer-Phillips theorem.\n\nThe strongly continuous semigroup \"T\" is called uniformly continuous if the map \"t\" → \"T\"(\"t\") is continuous from [0, ∞) to \"L\"(\"X\").\n\nThe generator of a uniformly continuous semigroup is a bounded operator.\n\nA strongly continuous semigroup \"T\" is called eventually differentiable if there exists a such that (equivalently: for all and \"T\" is immediately differentiable if for all .\n\nEvery analytic semigroup is immediately differentiable.\n\nAn equivalent characterization in terms of Cauchy problems is the following: the strongly continuous semigroup generated by \"A\" is eventually differentiable if and only if there exists a such that for all the solution \"u\" of the abstract Cauchy problem is differentiable on . The semigroup is immediately differentiable if \"t\" can be chosen to be zero.\n\nA strongly continuous semigroup \"T\" is called eventually compact if there exists a \"t\" > 0 such that \"T\"(\"t\") is a compact operator (equivalently if \"T\"(\"t\") is a compact operator for all \"t\" ≥ \"t\") . The semigroup is called immediately compact if \"T\"(\"t\") is a compact operator for all \"t\" > 0.\n\nA strongly continuous semigroup is called eventually norm continuous if there exists a \"t\" ≥ 0 such that the map \"t\" → \"T\"(\"t\") is continuous from (\"t\", ∞) to \"L\"(\"X\"). The semigroup is called immediately norm continuous if \"t\" can be chosen to be zero.\n\nNote that for an immediately norm continuous semigroup the map \"t\" → \"T\"(\"t\") may not be continuous in \"t\" = 0 (that would make the semigroup uniformly continuous).\n\nAnalytic semigroups, (eventually) differentiable semigroups and (eventually) compact semigroups are all eventually norm continuous.\n\nThe growth bound of a semigroup \"T\" is the constant\n\nIt is so called as this number is also the infimum of all real numbers \"ω\" such that there exists a constant \"M\" (≥ 1) with\n\nfor all \"t\" ≥ 0.\n\nThe following are equivalent:\n\nA semigroup that satisfies these equivalent conditions is called exponentially stable or uniformly stable (either of the first three of the above statements is taken as the definition in certain parts of the literature). That the \"L\" conditions are equivalent to exponential stability is called the Datko-Pazy theorem.\n\nIn case \"X\" is a Hilbert space there is another condition that is equivalent to exponential stability in terms of the resolvent operator of the generator: all \"λ\" with positive real part belong to the resolvent set of \"A\" and the resolvent operator is uniformly bounded on the right half plane, i.e. (\"λI\" − \"A\") belongs to the Hardy space formula_27. This is called the Gearhart-Pruss theorem.\n\nThe spectral bound of an operator \"A\" is the constant\n"}
{"id": "56260115", "url": "https://en.wikipedia.org/wiki?curid=56260115", "title": "Clearing denominators", "text": "Clearing denominators\n\nIn mathematics, the method of clearing denominators, also called clearing fractions, is a technique for simplifying an equation equating two expressions that each are a sum of rational expressions – which includes simple fractions.\n\nConsider the equation\n\nThe smallest common multiple of the two denominators 6 and 15\"z\" is 30\"z\", so one multiplies both sides by 30\"z\":\n\nThe result is an equation with no fractions.\n\nThe simplified equation is not entirely equivalent to the original. For when we substitute and in the last equation, both sides simplify to 0, so we get , a mathematical truth. But the same substitution applied to the original equation results in , which is mathematically meaningless.\n\nWithout loss of generality, we may assume that the right-hand side of the equation is 0, since an equation may equivalently be rewritten in the form .\n\nSo let the equation have the form\n\nThe first step is to determine a common denominator of these fractions – preferably the least common denominator, which is the least common multiple of the .\n\nThis means that each is a factor of , so for some expression that is not a fraction. Then\n\nprovided that does not assume the value 0 – in which case also equals 0.\n\nSo we have now\n\nProvided that does not assume the value 0, the latter equation is equivalent with\n\nin which the denominators have vanished.\n\nAs shown by the provisos, care has to be taken not to introduce zeros of – viewed as a function of the unknowns of the equation – as spurious solutions.\n\nConsider the equation\n\nThe least common denominator is .\n\nFollowing the method as described above results in\n\nSimplifying this further gives us the solution .\n\nIt is easily checked that none of the zeros of – namely , , and – is a solution of the final equation, so no spurious solutions were introduced.\n"}
{"id": "621215", "url": "https://en.wikipedia.org/wiki?curid=621215", "title": "Cointerpretability", "text": "Cointerpretability\n\nIn mathematical logic, cointerpretability is a binary relation on formal theories: a formal theory \"T\" is cointerpretable in another such theory \"S\", when the language of \"S\" can be translated into the language of \"T\" in such a way that \"S\" proves every formula whose translation is a theorem of \"T\". The \"translation\" here is required to preserve the logical structure of formulas. \n\nThis concept, in a sense dual to interpretability, was introduced by , who also proved that, for theories of Peano arithmetic and any stronger theories with effective axiomatizations, cointerpretability is equivalent to formula_1-conservativity.\n\n\n"}
{"id": "1411533", "url": "https://en.wikipedia.org/wiki?curid=1411533", "title": "Connected category", "text": "Connected category\n\nIn category theory, a branch of mathematics, a connected category is a category in which, for every two objects \"X\" and \"Y\" there is a finite sequence of objects\nwith morphisms\nor\nfor each 0 ≤ \"i\" < \"n\" (both directions are allowed in the same sequence). Equivalently, a category \"J\" is connected if each functor from \"J\" to a discrete category is constant. In some cases it is convenient to not consider the empty category to be connected.\n\nA stronger notion of connectivity would be to require at least one morphism \"f\" between any pair of objects \"X\" and \"Y\". Any category with this property is connected in the above sense.\n\nA small category is connected if and only if its underlying graph is weakly connected, meaning that it is connected if one disregard the direction of the arrows.\n\nEach category \"J\" can be written as a disjoint union (or coproduct) of a collection of connected categories, which are called the connected components of \"J\". Each connected component is a full subcategory of \"J\".\n"}
{"id": "17579850", "url": "https://en.wikipedia.org/wiki?curid=17579850", "title": "Covering code", "text": "Covering code\n\nIn coding theory, a covering code is a set of elements (called \"codewords\") in a space, with the property that every element of the space is within a fixed distance of some codeword. \n\nLet formula_1, formula_2, formula_3 be integers.\nA code formula_4 over an alphabet \"Q\" of size |\"Q\"| = \"q\" is called\n\"q\"-ary \"R\"-covering code of length \"n\"\nif for every word formula_5 there is a codeword formula_6\nsuch that the Hamming distance formula_7.\nIn other words, the spheres (or balls or rook-domains) of radius \"R\"\nwith respect to the Hamming metric around the codewords of \"C\" have to exhaust\nthe metric space formula_8.\nThe covering radius of a code \"C\" is the smallest \"R\" such that \"C\" is \"R\"-covering.\nEvery perfect code is a covering code of minimal size.\n\n\"C\" = {0134,0223,1402,1431,1444,2123,2234,3002,3310,4010,4341} is a 5-ary 2-covering code of length 4.\n\nThe determination of the minimal size formula_9 of a \"q\"-ary \"R\"-covering code of length \"n\" is a very hard problem. In many cases, only upper and lower bounds are known with a large gap between them.\nEvery construction of a covering code gives an upper bound on \"K\"(\"n\", \"R\").\nLower bounds include the sphere covering bound and \nRodemich’s bounds formula_10 and formula_11.\nThe covering problem is closely related to the packing problem in formula_8, i.e. the determination of the maximal size of a \"q\"-ary \"e\"-error correcting code of length \"n\".\n\nA particular case is the football pools problem, based on football pool betting, where the aim is to predict the results of \"n\" football matches as a home win, draw or away win, or to at least predict of them with multiple bets. Thus a ternary covering, \"K\"(\"n\",1), is sought. \n\nIf formula_13 then 3 are needed, so for \"n\" = 4, \"k\" = 2, 9 are needed; for \"n\" = 13, \"k\" = 3, 59049 are needed. The best bounds known as of 2011 are \n\nThe standard work on covering codes lists the following applications.\n\n\n"}
{"id": "14736250", "url": "https://en.wikipedia.org/wiki?curid=14736250", "title": "Delannoy number", "text": "Delannoy number\n\nIn mathematics, a Delannoy number formula_1 describes the number of paths from the southwest corner (0, 0) of a rectangular grid to the northeast corner (\"m\", \"n\"), using only single steps north, northeast, or east. The Delannoy numbers are named after French army officer and amateur mathematician Henri Delannoy.\n\nThe Delannoy number formula_2 also counts the number of global alignments of two sequences of lengths formula_3 and formula_4, the number of points in an \"m\"-dimensional integer lattice that are at most \"n\" steps from the origin, and, in cellular automata, the number of cells in an \"m\"-dimensional von Neumann neighborhood of radius \"n\" while the number of cells on a surface of an \"m\"-dimensional von Neumann neighborhood of radius \"n\" is given with .\n\nThe Delannoy number \"D\"(3,3) equals 63. The following figure illustrates the 63 Delannoy paths through a 3 × 3 grid:\n\nThe subset of paths that do not rise above the SW–NE diagonal are counted by a related family of numbers, the Schröder numbers.\n\nThe Delannoy array is an infinite matrix of the Delannoy numbers:\n\nIn this array, the numbers in the first row are all one, the numbers in the second row are the odd numbers, the numbers in the third row are the centered square numbers, and the numbers in the fourth row are the centered octahedral numbers. Alternatively, the same numbers can be arranged in a triangular array resembling Pascal's triangle, also called the tribonacci triangle, in which each number is the sum of the three numbers above it:\n\nThe central Delannoy numbers \"D\"(\"n\") = \"D\"(\"n\",\"n\") are the numbers for a square \"n\" × \"n\" grid. The first few central Delannoy numbers (starting with \"n\"=0) are:\n\nFor formula_5 diagonal (i.e. northeast) steps, there must be formula_6 steps in the formula_7 direction and formula_8 steps in the formula_9 direction in order to reach the point formula_10; as these steps can be performed in any order, the number of such paths is given by the multinomial coefficient\nformula_11. Hence, one gets the closed-form expression\n\nAn alternative expression is given by\n\nAnd also\n\nwhere formula_15 is given with .\n\nThe basic recurrence relation for the Delannoy numbers is easily seen to be\n\nThis recurrence relation also leads directly to the generating function\n\nSubstituting formula_18 in the first closed form expression above, replacing formula_19, and a little algebra, gives\n\nwhile the second expression above yields\n\nThe central Delannoy numbers satisfy also a three-term recurrence relationship among themselves,\n\nand have a generating function\n\nThe leading asymptotic behavior of the central Delannoy numbers is given by\n\nwhere \nformula_25\nand \nformula_26.\n\n"}
{"id": "4474775", "url": "https://en.wikipedia.org/wiki?curid=4474775", "title": "Duhamel's principle", "text": "Duhamel's principle\n\nIn mathematics, and more specifically in partial differential equations, Duhamel's principle is a general method for obtaining solutions to inhomogeneous linear evolution equations like the heat equation, wave equation, and vibrating plate equation. It is named after Jean-Marie Duhamel who first applied the principle to the inhomogeneous heat equation that models, for instance, the distribution of heat in a thin plate which is heated from beneath. For linear evolution equations without spatial dependency, such as a harmonic oscillator, Duhamel's principle reduces to the method of variation of parameters technique for solving linear inhomogeneous ordinary differential equations.\n\nThe philosophy underlying Duhamel's principle is that it is possible to go from solutions of the Cauchy problem (or initial value problem) to solutions of the inhomogeneous problem. Consider, for instance, the example of the heat equation modeling the distribution of heat energy \"u\" in R. The initial value problem is\nwhere \"g\" is the initial heat distribution. By contrast, the inhomogeneous problem for the heat equation is\ncorresponds to adding an external heat energy \"ƒ\"(\"x\",\"t\")\"dt\" at each point. Intuitively, one can think of the inhomogeneous problem as a set of homogeneous problems each starting afresh at a different time slice \"t\" = \"t\". By linearity, one can add up (integrate) the resulting solutions through time \"t\" and obtain the solution for the inhomogeneous problem. This is the essence of Duhamel's principle.\n\nFormally, consider a linear inhomogeneous evolution equation for a function \nwith spatial domain \"D\" in R, of the form\nwhere \"L\" is a linear differential operator that involves no time derivatives.\n\nDuhamel's principle is, formally, that the solution to this problem is\nwhere \"P\"\"ƒ\" is the solution of the problem\nThe integrand is the retarded solution formula_7, evaluated at time \"t\", representing the effect, at the later time \"t\", of an infinitesimal force formula_8 applied at time \"s\".\n\nDuhamel's principle also holds for linear systems (with vector-valued functions \"u\"), and this in turn furnishes a generalization to higher \"t\" derivatives, such as those appearing in the wave equation (see below). Validity of the principle depends on being able to solve the homogeneous problem in an appropriate function space and that the solution should exhibit reasonable dependence on parameters so that the integral is well-defined. Precise analytic conditions on \"u\" and \"f\" depend on the particular application.\n\nThe linear wave equation models the displacement \"u\" of an idealized dispersionless one-dimensional string, in terms of derivatives with respect to time \"t\" and space \"x\":\n\nThe function \"f\"(\"x\",\"t\"), in natural units, represents an external force applied to string at the position (\"x\",\"t\"). In order to be a suitable physical model for nature, it should be possible to solve it for any initial state that the string is in, specified by its initial displacement and velocity:\n\nMore generally, we should be able to solve the equation with data specified on any \"t\" = \"constant\" slice:\n\nTo evolve a solution from any given time slice \"T\" to \"T\"+\"dT\", the contribution of the force must be added to the solution. That contribution comes from changing the velocity of the string by \"f\"(\"x\",\"T\")\"dT\". That is, to get the solution at time \"T\"+\"dT\" from the solution at time \"T\", we must add to it a new (forward) solution of the \"homogeneous\" (no external forces) wave equation\n\nwith the initial conditions\n\nA solution to this equation is achieved by straightforward integration:\n\n(The expression in parenthesis is just formula_15 in the notation of the general method above.) So a solution of the original initial value problem is obtained by starting with a solution to the problem with the same prescribed initial values problem but with \"zero\" external force, and adding to that (integrating) the contributions from the added force in the time intervals from \"T\" to \"T\"+\"dT\":\n\nDuhamel's principle is the result that the solution to an inhomogeneous, linear, partial differential equation can be solved by first finding the solution for a step input, and then superposing using Duhamel's integral.\nSuppose we have a constant coefficient, m order inhomogeneous ordinary differential equation.\n\nwhere\n\nWe can reduce this to the solution of a homogeneous ODE using the following method. All steps are done formally, ignoring necessary requirements for the solution to be well defined.\n\nFirst let \"G\" solve\n\nDefine formula_21, with formula_22 being the characteristic function of the interval formula_23. Then we have\n\nin the sense of distributions. Therefore\n\nsolves the ODE.\n\nMore generally, suppose we have a constant coefficient inhomogeneous partial differential equation\n\nwhere\n\nWe can reduce this to the solution of a homogeneous ODE using the following method. All steps are done formally, ignoring necessary requirements for the solution to be well defined.\n\nFirst, taking the Fourier transform in \"x\" we have\n\nAssume that formula_31 is an m order ODE in \"t\". Let formula_32 be the coefficient of the highest order term of formula_31.\nNow for every formula_34 let formula_35 solve\n\nDefine formula_37. We then have\n\nin the sense of distributions. Therefore\n\nsolves the PDE (after transforming back to \"x\").\n\n"}
{"id": "59112216", "url": "https://en.wikipedia.org/wiki?curid=59112216", "title": "ENO methods", "text": "ENO methods\n\nIn numerical solution of differential equations, ENO (essentially non-oscillatory) methods are classes of high-resolution schemes. The first ENO scheme is developed by Harten, Engquist, Osher and Chakravarthy in 1987. In 1994, the first weighted version of ENO is developed.\n\n"}
{"id": "682629", "url": "https://en.wikipedia.org/wiki?curid=682629", "title": "Element (mathematics)", "text": "Element (mathematics)\n\nIn mathematics, an element, or member, of a set is any one of the distinct objects that make up that set.\n\nWriting formula_1 means that the elements of the set are the numbers 1, 2, 3 and 4. Sets of elements of , for example formula_2, are subsets of .\n\nSets can themselves be elements. For example, consider the set formula_3. The elements of are \"not\" 1, 2, 3, and 4. Rather, there are only three elements of , namely the numbers 1 and 2, and the set formula_4.\n\nThe elements of a set can be anything. For example, formula_5, is the set whose elements are the colors , and .\n\nThe relation \"is an element of\", also called set membership, is denoted by the symbol \"formula_6\". Writing\n\nmeans that \"\"x\" is an element of \"A\". Equivalent expressions are \"x\" is a member of \"A\", \"x\" belongs to \"A\", \"x\" is in \"A\" and \"x\" lies in \"A\". The expressions \"A\" includes \"x\" and \"A\" contains \"x\" are also used to mean set membership, however some authors use them to mean instead \"x\" is a subset of \"A\"\". Logician George Boolos strongly urged that \"contains\" be used for membership only and \"includes\" for the subset relation only.\n\nFor the relation ϵ , the converse relation ϵ may be written\n\nThe negation of set membership is denoted by the symbol \"∉\". Writing\n\nThe symbol ϵ was first used by Giuseppe Peano 1889 in his work . Here he wrote on page X:\n\nwhich means\n\nThe symbol ϵ means \"is\". So a ϵ b is read as a \"is a\" b; ...\n\nThe symbol itself is a stylized lowercase Greek letter epsilon (\"ε\"), the first letter of the word , which means \"is\".\n\nThe Unicode characters for these symbols are U+2208 ('element of'), U+2209 ('not an element of'), U+220B ('contains as member') and U+220C ('does not contain as member'). The equivalent LaTeX commands are \"\\in\", \"\\notin\", \"\\ni\" and \"\\not\\ni\". Mathematica has commands \"\\[Element]\", \"\\[NotElement]\", \"\\[ReverseElement]\" and \"\\[NotReverseElement]\".\nEvery relation \"R\" : \"U\" → \"V\" is subject to two involutions: complementation \"R\" → formula_10 and conversion \"R\":\"V\" → \"U\".\nThe relation ∈ has for its domain a universal set \"U\", and has the power set P(\"U\") for its codomain or range. The complementary relation formula_11 expresses the opposite of ∈. An element \"x\" ∈ \"U\" may have \"x\" ∉ \"A\", in which case \"x\" ∈ \"U\" \\ \"A\", the complement of \"A\" in \"U\".\n\nThe converse relation formula_12 swaps the domain and range with ∈. For any \"A\" in P(\"U\"), formula_13 is true when \"x\" ∈ \"A\".\n\nThe number of elements in a particular set is a property known as cardinality; informally, this is the size of a set. In the above examples the cardinality of the set \"A\" is 4, while the cardinality of either of the sets \"B\" and \"C\" is 3. An infinite set is a set with an infinite number of elements, while a finite set is a set with a finite number of elements. The above examples are examples of finite sets. An example of an infinite set is the set of positive integers = { 1, 2, 3, 4, ... }.\n\nUsing the sets defined above, namely \"A\" = {1, 2, 3, 4 }, \"B\" = {1, 2, {3, 4}} and \"C\" = { red, green, blue }:\n\n"}
{"id": "59028633", "url": "https://en.wikipedia.org/wiki?curid=59028633", "title": "Elena Marchisotto", "text": "Elena Marchisotto\n\nElena Anne Corie Marchisotto (born 1945) is a mathematician, mathematics educator, and historian of mathematics. She is a professor emeritus of mathematics at California State University, Northridge.\n\nMarchisotto graduated from Manhattanville College in 1967 and earned a master's degree from California State University, Northridge in 1977. She completed a Ph.D. in 1990 from New York University. Her dissertation, \"The contributions of Mario Pieri to mathematics and mathematics education\", was jointly supervised by Kenneth P. Goldberg and Anneli Cahn Lax.\n\nShe joined the California State University, Northridge faculty in 1983.\nAt Northridge, she directed the developmental mathematics program.\n\nWith James T. Smith, Marchisotto wrote a book on Mario Pieri, \"The legacy of Mario Pieri in geometry and arithmetic\" (Birkhäuser, 2007).\n\nIn 1995, Marchisotto became a co-author of an updated edition of \"The Mathematical Experience\" (a book originally published in 1981 by Reuben Hersh and Philip J. Davis), after having read the book and used it for her teaching from the early 1980s.\nShe also co-authored an English translation of a history of mathematics by , \"Hilbert's Flute: The History of Modern Mathematics\" (with Bottazzini and Patricia Miller, Springer, 2016).\n\nMarchisotto's textbooks include:\n"}
{"id": "28830309", "url": "https://en.wikipedia.org/wiki?curid=28830309", "title": "Eugene Lawler", "text": "Eugene Lawler\n\nEugene Leighton (Gene) Lawler (1933 – September 2, 1994) was an American computer scientist, a professor of computer science at the University of California, Berkeley.\n\nLawler came to Harvard as a graduate student in 1954, after a three-year undergraduate B.S. program in Mathematics at Florida State University. He received a master's degree in 1957, and took a hiatus in his studies, during which he briefly went to law school and worked in the U.S. Army, at a grinding wheel company, and as an electrical engineer at Sylvania from 1959 to 1961. He returned to Harvard in 1958, and completed his Ph.D. in 1962 under the supervision of Anthony G. Oettinger with a dissertation entitled \"Some Aspects of Discrete Mathematical Programming\". He then became a faculty member at the University of Michigan until 1971, when he moved to Berkeley. He retired in 1994, shortly before his death.\n\nAt Berkeley, Lawler's doctoral students included Marshall Bern, Chip Martel, Arvind Raghunathan, Arnie Rosenthal, Huzur Saran, David Shmoys, and Tandy Warnow.\n\nLawler was an expert on combinatorial optimization and a founder of the field, the author of the widely used textbook \"Combinatorial Optimization: Networks and Matroids\" and coauthor of \"The Traveling Salesman Problem: a guided tour of combinatorial optimization\". He played a central role in rescuing the ellipsoid method for linear programming from obscurity in the West. He also wrote (with D. E. Wood) a heavily cited 1966 survey on branch and bound algorithms, selected as a citation classic in 1987,\nand another influential early paper on dynamic programming with J. M. Moore. Lawler was also the first to observe that matroid intersection can be solved in polynomial time.\n\nThe NP-completeness proofs for two of Karp's 21 NP-complete problems, directed Hamiltonian cycle and 3-dimensional matching, were credited by Karp to Lawler. The NP-completeness of 3-dimensional matching is an example of one of Lawler's favorite observations, the \"mystical power of twoness\": for many combinatorial optimization problems that can be parametrized by an integer, the problem can be solved in polynomial time when the parameter is two but becomes NP-complete when the parameter is three. For 3-dimensional matching, the solvable parameter-2 version of the problem is graph matching; the same phenomenon arises in the complexities of 2-coloring and 3-coloring for graphs, in the matroid intersection problem for intersections of two or three matroids, and in 2-SAT and 3-SAT for satisfiability problems. Lenstra writes that \"Gene would invariably comment that this is why a world with two sexes has been devised.\"\n\nDuring the 1970s, Lawler made great headway in systematizing algorithms for job shop scheduling. His 1979 survey on the subject introduced the three-field notation for theoretic scheduling problems, which (despite the existence of earlier notations) became standard in the study of scheduling algorithms. Another later survey is also highly cited (over 1000 citations each in Google scholar).\n\nIn the late 1980s, Lawler shifted his research focus to problems of computational biology, including the reconstruction of evolutionary trees and several works on sequence alignment.\n\nIn Spring 1969, while on sabbatical in Berkeley, Lawler took part in a protest against the Vietnam War that led to the arrests of 483 protesters, including Lawler; Richard Karp bailed him out.\nKarp recalls Lawler as \"the social conscience of the CS Division, always looking out for the welfare of students and especially concerned for women, minorities and handicapped students\".\n\nA special issue of the journal \"Mathematical Programming\" (vol. 82, issues 1–2) was dedicated in Lawler's honor in 1998.\n\nThe ACM Eugene L. Lawler Award is given by the Association for Computing Machinery every two years for \"humanitarian contributions within computer science and informatics\".\n\n\n"}
{"id": "1881195", "url": "https://en.wikipedia.org/wiki?curid=1881195", "title": "Factor theorem", "text": "Factor theorem\n\nIn algebra, the factor theorem is a theorem linking factors and zeros of a polynomial. It is a special case of the polynomial remainder theorem.\n\nThe factor theorem states that a polynomial formula_1 has a factor formula_2 if and only if formula_3 (i.e. formula_4 is a root).\n\nTwo problems where the factor theorem is commonly applied are those of factoring a polynomial and finding the roots of a polynomial equation; it is a direct consequence of the theorem that these problems are essentially equivalent.\n\nThe factor theorem is also used to remove known zeros from a polynomial while leaving all unknown zeros intact, thus producing a lower degree polynomial whose zeros may be easier to find. Abstractly, the method is as follows:\n\nFind the factors of\n\nTo do this one would use trial and error (or the rational root theorem) to find the first x value that causes the expression to equal zero. To find out if formula_17 is a factor, substitute formula_18 into the polynomial above:\n\nAs this is equal to 18 and not 0 this means formula_17 is not a factor of formula_23. So, we next try formula_24 (substituting formula_25 into the polynomial):\n\nThis is equal to formula_27. Therefore formula_28, which is to say formula_29, is a factor, and formula_30 is a root of formula_16\n\nThe next two roots can be found by algebraically dividing formula_23 by formula_33 to get a quadratic:\n\nand therefore formula_33 and formula_36 are factors of formula_16 Of these the quadratic factor can be further factored using the quadratic formula, which gives as roots of the quadratic formula_38 Thus the three irreducible factors of the original polynomial are formula_39 formula_40 and formula_41\n"}
{"id": "28096905", "url": "https://en.wikipedia.org/wiki?curid=28096905", "title": "Fannes–Audenaert inequality", "text": "Fannes–Audenaert inequality\n\nThe Fannes–Audenaert inequality is a mathematical bound on the difference between the von Neumann entropies of two density matrices as a function of their trace distance. It was proved by Koenraad M. R. Audenaert in 2007 as an optimal refinement of Mark Fannes' original inequality, which was published in 1973. Mark Fannes is a Belgian physicist specialised in mathematical quantum mechanics. He works at the KU Leuven. Koenraad M. R. Audenaert is a Belgian physicist and civil engineer. He currently works at Royal Holloway, University of London.\n\nFor any two density matrices formula_1 and formula_2 of dimensions formula_3,\n\nwhere\n\nis the (Shannon) entropy of the probability distribution formula_6,\n\nis the (von Neumann) entropy of a matrix formula_1 with eigenvalues formula_9, and\n\nis the trace distance between the two matrices. Note that the base for the logarithm is arbitrary, so long as the same base is used on both sides of the inequality.\n\nAudenaert also proved that—given only the trace distance \"T\" and the dimension \"d\"—this is the \"optimal\" bound. He did this by directly exhibiting a pair of matrices which saturate the bound for any values of \"T\" and \"d\". The matrices (which are diagonal in the same basis, i.e. they commute) are\n\nThe original inequality proved by Fannes was\n\nwhen formula_14. He also proved the weaker inequality\n\nwhich can be used for larger \"T\".\n\nFannes proved this inequality as a means to prove the continuity of the von Neumann entropy, which did not require an optimal bound. The proof is very compact, and can be found in the textbook by Nielsen and Chuang. Audenaert's proof of the optimal inequality, on the other hand, is significantly more complicated.\n"}
{"id": "1850216", "url": "https://en.wikipedia.org/wiki?curid=1850216", "title": "Fermat's theorem on sums of two squares", "text": "Fermat's theorem on sums of two squares\n\nIn additive number theory, Fermat's theorem on sums of two squares states that an odd prime \"p\" can be expressed as:\n\nformula_1\n\nwith \"x\" and \"y\" integers, if and only if\n\nThe prime numbers for which this is true are called Pythagorean primes.\nFor example, the primes 5, 13, 17, 29, 37 and 41 are all congruent to 1 modulo 4, and they can be expressed as sums of two squares in the following ways:\n\nOn the other hand, the primes 3, 7, 11, 19, 23 and 31 are all congruent to 3 modulo 4, and none of them can be expressed as the sum of two squares. This is the easier part of the theorem, and follows immediately from the observation that all squares are congruent to 0 or 1 modulo 4.\n\nAlbert Girard was the first to make the observation, describing all positive integral numbers (not necessarily primes) expressible as the sum of two squares of positive integers; this was published in 1625. The statement that every prime \"p\" of the form \"4n+1\" is the sum of two squares is sometimes called \"Girard's theorem\". For his part, Fermat wrote an elaborate version of the statement (in which he also gave the number of possible expressions of the powers of \"p\" as a sum of two squares) in a letter to Marin Mersenne dated December 25, 1640: for this reason this version of the theorem is sometimes called \"Fermat's Christmas theorem.\" \n\nSince the Diophantus identity implies that the product of two integers each of which can be written as the sum of two squares is itself expressible as the sum of two squares, by applying Fermat's theorem to the prime factorization of any positive integer \"n\", we see that if all the prime factors of \"n\" congruent to 3 modulo 4 occur to an even exponent, then \"n\" is expressible as a sum of two squares. The converse also holds. This equivalence provides the characterization Girard guessed.\n\nFermat usually did not write down proofs of his claims, and he did not provide a proof of this statement. The first proof was found by Euler after much effort and is based on infinite descent. He announced it in two letters to Goldbach, on May 6, 1747 and on April 12, 1749; he published the detailed proof in two articles (between 1752 and 1755). Lagrange gave a proof in 1775 that was based on his study of quadratic forms. This proof was simplified by Gauss in his \"Disquisitiones Arithmeticae\" (art. 182). Dedekind gave at least two proofs based on the arithmetic of the Gaussian integers. There is an elegant proof using Minkowski's theorem about convex sets. Simplifying an earlier short proof due to Heath-Brown (who was inspired by Liouville's idea), Zagier presented a one-sentence proof of Fermat's assertion.\nAnd more recently Christopher gave a partition-theoretic proof.\n\nFermat announced two related results fourteen years later. In a letter to Blaise Pascal dated September 25, 1654 he announced the following two results for odd primes formula_4:\n\n\nHe also wrote:\n\nIn other words, if \"p, q\" are of the form 20\"k\" + 3 or 20\"k\" + 7, then \"pq\" = \"x\" + 5\"y\". Euler later extended this to the conjecture that\n\nBoth Fermat's assertion and Euler's conjecture were established by Lagrange.\n\nA generalization of Fermat's theorem, the sum of two squares theorem, characterizes the integers (not necessarily prime) that can be expressed as the sum of two squares. They are exactly the integers in which each prime that is congruent to 3 mod 4 appears with an even exponent in the prime factorization of the number.\n\n\n"}
{"id": "75233", "url": "https://en.wikipedia.org/wiki?curid=75233", "title": "Hilary Putnam", "text": "Hilary Putnam\n\nHilary Whitehall Putnam (; July 31, 1926 – March 13, 2016) was an American philosopher, mathematician, and computer scientist, and a major figure in analytic philosophy in the second half of the 20th century. He made significant contributions to philosophy of mind, philosophy of language, philosophy of mathematics, and philosophy of science. At the time of his death, Putnam was Cogan University Professor Emeritus at Harvard University.\n\nHe was known for his willingness to apply an equal degree of scrutiny to his own philosophical positions as to those of others, subjecting each position to rigorous analysis until he exposed its flaws. As a result, he acquired a reputation for frequently changing his own position.\n\nIn philosophy of mind, Putnam is known for his argument against the type-identity of mental and physical states based on his hypothesis of the multiple realizability of the mental, and for the concept of functionalism, an influential theory regarding the mind–body problem. In philosophy of language, along with Saul Kripke and others, he developed the causal theory of reference, and formulated an original theory of meaning, introducing the notion of semantic externalism based on a famous thought experiment called Twin Earth.\n\nIn philosophy of mathematics, he and his mentor W. V. O. Quine developed the \"Quine–Putnam indispensability thesis\", an argument for the reality of mathematical entities, later espousing the view that mathematics is not purely logical, but \"quasi-empirical\". In the field of epistemology, he is known for his critique of the well known \"brain in a vat\" thought experiment. This thought experiment appears to provide a powerful argument for epistemological skepticism, but Putnam challenges its coherence.\n\nIn metaphysics, he originally espoused a position called metaphysical realism, but eventually became one of its most outspoken critics, first adopting a view he called \"internal realism\", which he later abandoned. Despite these changes of view, throughout his career he remained committed to scientific realism, roughly the view that mature scientific theories are approximately true descriptions of ways things are.\n\nIn the philosophy of perception Putnam came to endorse direct realism, according to which perceptual experiences directly present one with the external world. In the past, he further held that there are no mental representations, sense data, or other intermediaries that stand between the mind and the world. By 2012, however, he rejected this further commitment, in favor of \"transactionalism\", a view that accepts both that perceptual experiences are world-involving transactions, and that these transactions are functionally describable (provided that worldly items and intentional states may be referred to in the specification of the function). Such transactions can further involve qualia.\n\nIn his later work, Putnam became increasingly interested in American pragmatism, Jewish philosophy, and ethics, thus engaging with a wider array of philosophical traditions. He also displayed an interest in metaphilosophy, seeking to \"renew philosophy\" from what he identifies as narrow and inflated concerns.\n\nOutside philosophy, Putnam contributed to mathematics and computer science. Together with Martin Davis he developed the Davis–Putnam algorithm for the Boolean satisfiability problem and he helped demonstrate the unsolvability of Hilbert's tenth problem. He was at times a politically controversial figure, especially for his involvement with the Progressive Labor Party in the late 1960s and early 1970s.\n\nPutnam was born in Chicago, Illinois, in 1926. His father, Samuel Putnam, was a scholar of Romance languages, columnist, and translator who wrote for the \"Daily Worker\", a publication of the American Communist Party, from 1936 to 1946 (when he became disillusioned with communism). As a result of his father's commitment to communism, Putnam had a secular upbringing, although his mother, Riva, was Jewish. The family lived in France until 1934, when they returned to the United States, settling in Philadelphia. Putnam attended Central High School; there he met Noam Chomsky, who was a year behind him. The two remained friends—and often intellectual opponents—for the rest of Putnam's life. Putnam studied philosophy at the University of Pennsylvania, receiving his B.A. degree and becoming a member of the Philomathean Society, the oldest continually-existing collegiate literary society in the United States. He went on to do graduate work in philosophy at Harvard University, and later at UCLA's Philosophy Department, where he received his Ph.D. in 1951 for a dissertation entitled \"The Meaning of the Concept of Probability in Application to Finite Sequences\". Putnam's teacher Hans Reichenbach (his dissertation supervisor) was a leading figure in logical positivism, the dominant school of philosophy of the day; one of Putnam's most consistent positions has been his rejection of logical positivism as self-defeating.\n\nAfter teaching at Northwestern (1951–52), Princeton (1953–61), and MIT (1961–65), he moved to Harvard in 1965. His wife, the philosopher Ruth Anna Putnam, took a teaching position in philosophy at Wellesley College. Hilary and Ruth Anna were married on August 11, 1962. Ruth Anna, descendant of a family with a long scholarly tradition in Gotha (her ancestor was the German classical scholar Christian Friedrich Wilhelm Jacobs), was born in Berlin, Germany, in 1927 to anti-Nazi political-activist parents and, like Putnam himself, she was raised an atheist (her mother was Jewish and her father had been from a Christian background). Putnam was also an atheist. The Putnams, rebelling against the antisemitism that they had experienced during their youth, decided to establish a traditional Jewish home for their children. Since they had no experience with the rituals of Judaism, they sought out invitations to other Jews' homes for Seder. They had \"no idea how to do it [themselves]\", in the words of Ruth Anna. They therefore began to study Jewish ritual and Hebrew, and became more Jewishly interested, identified, and active. In 1994, Hilary Putnam celebrated a belated Bar Mitzvah service. His wife had a Bat Mitzvah service four years later.\n\nHilary was a popular teacher at Harvard. In keeping with the family tradition, he was politically active. In the 1960s and early 1970s, he was an active supporter of the American Civil Rights Movement and an opponent of American military intervention in Vietnam. In 1963, he organized one of the first faculty and student committees at MIT against the war. Putnam was disturbed when he learned from reading the reports of David Halberstam that the U.S. was \"defending\" South Vietnamese peasants from the Vietcong by poisoning their rice crops. After moving to Harvard in 1965, he organized campus protests and began teaching courses on Marxism. Hilary became an official faculty advisor to the Students for a Democratic Society and, in 1968, became a member of the Progressive Labor Party (PLP).\n\nHe was elected a Fellow of the American Academy of Arts and Sciences in 1965. After 1968, his political activities were centered on the PLP. The Harvard administration considered these activities disruptive and attempted to censure Putnam, but two other faculty members criticized the procedures. Putnam permanently severed his ties with the PLP in 1972. In 1997, at a meeting of former draft resistance activists at Arlington Street Church in Boston, Putnam described his involvement with the PLP as a mistake. He said that he had been impressed at first with the PLP's commitment to alliance-building and its willingness to attempt to organize from within the armed forces.\n\nIn 1976, he was elected President of the American Philosophical Association. The following year, he was selected as Walter Beverly Pearson Professor of Mathematical Logic, in recognition of his contributions to the philosophy of logic and mathematics. While breaking with his radical past, Putnam never abandoned his belief that academics have a particular social and ethical responsibility toward society. He continued to be forthright and progressive in his political views, as expressed in the articles \"How Not to Solve Ethical Problems\" (1983) and \"Education for Democracy\" (1993).\n\nPutnam was a Corresponding Fellow of the British Academy. He retired from teaching in June 2000, but, as of 2009, he continued to give a seminar almost yearly at Tel Aviv University. He also held the Spinoza Chair of Philosophy at the University of Amsterdam in 2001. He was the Cogan University Professor Emeritus at Harvard University and a founding patron of the small liberal arts college Ralston College. His corpus includes five volumes of collected works, seven books, and more than 200 articles. Putnam's renewed interest in Judaism inspired him to publish several books and essays on the topic. With his wife, he has co-authored several books and essays on the late-19th-century American pragmatist movement. He began a blog in May 2014.\n\nFor his contributions in philosophy and logic, he was awarded the Rolf Schock Prize in Logic and Philosophy in 2011 and the Nicholas Rescher Prize for Systematic Philosophy in 2015.\n\nHe delivered his last Skype talk, entitled \"Thought and Language\", at an international conference on \"The Philosophy of Hilary Putnam\" held at the Indian Institute of Technology, Bombay, on October 3, 2015 and organized by his student Sanjit Chakraborty.\n\nPutnam's best-known work concerns philosophy of mind. His most noted original contributions to that field came in several key papers published in the late 1960s that set out the hypothesis of multiple realizability. In these papers, Putnam argues that, contrary to the famous claim of the type-identity theory, it is not necessarily true that \"Pain is identical to C-fibre firing.\" Pain, according to Putnam's papers, may correspond to utterly different physical states of the nervous system in different organisms, and yet they all experience the same mental state of \"being in pain\".\n\nPutnam cited examples from the animal kingdom to illustrate his thesis. He asked whether it was likely that the brain structures of diverse types of animals realize pain, or other mental states, the same way. If they do not share the same brain structures, they cannot share the same mental states and properties. The answer to this puzzle had to be that mental states were realized by different physical states in different species. Putnam then took his argument a step further, asking about such things as the nervous systems of alien beings, artificially intelligent robots and other silicon-based life forms. These hypothetical entities, he contended, should not be considered incapable of experiencing pain just because they lack the same neurochemistry as humans. Putnam concluded that type-identity theorists had been making an \"ambitious\" and \"highly implausible\" conjecture which could be disproven with one example of multiple realizability. This argument is sometimes referred to as the \"likelihood argument\".\n\nPutnam formulated a complementary argument based on what he called \"functional isomorphism\". He defined the concept in these terms: \"Two systems are functionally isomorphic if 'there is a correspondence between the states of one and the states of the other that preserves functional relations'.\" In the case of computers, two machines are functionally isomorphic if and only if the sequential relations among states in the first are exactly mirrored by the sequential relations among states in the other. Therefore, a computer made out of silicon chips and a computer made out of cogs and wheels can be functionally isomorphic but constitutionally diverse. Functional isomorphism implies multiple realizability. This argument is sometimes referred to as an \"\"a priori\" argument\".\n\nJerry Fodor, Putnam, and others noted that, along with being an effective argument against type-identity theories, multiple realizability implies that any low-level explanation of higher-level mental phenomena is insufficiently abstract and general. Functionalism, which identifies mental kinds with functional kinds that are characterized exclusively in terms of causes and effects, abstracts from the level of microphysics, and therefore seemed to be a better explanation of the relation between mind and body. In fact, there are many functional kinds, such as mousetraps, software and bookshelves, which are multiply realized at the physical level.\n\nThe first formulation of such a functionalist theory was put forth by Putnam himself. This formulation, which is now called \"machine-state functionalism\", was inspired by analogies noted by Putnam and others between the mind and Turing machines. The point, for functionalism is the nature of the states of the Turing machine. Each state can be defined in terms of its relations to the other states and to the inputs and outputs, and the details of how it accomplishes what it accomplishes and of its material constitution are completely irrelevant. According to machine-state functionalism, the nature of a mental state is just like the nature of a Turing machine state. Just as \"state one\" simply is the state in which, given a particular input, such-and-such happens, so being in pain is the state which disposes one to cry \"ouch\", become distracted, wonder what the cause is, and so forth.\n\nIn the late 1980s, Putnam abandoned his adherence to functionalism and other computational theories of mind. His change of mind was primarily due to the difficulties that computational theories have in explaining certain intuitions with respect to the externalism of mental content. This is illustrated by Putnam's own Twin Earth thought experiment (see \"Philosophy of language\"). He also developed a separate argument against functionalism in 1988, based on Fodor's generalized version of multiple realizability. Asserting that functionalism is really a watered-down identity theory in which mental kinds are identified with functional kinds, Putnam argued that mental kinds may be multiply realizable over functional kinds. The argument for functionalism is that the same mental state could be implemented by the different states of a universal Turing machine.\n\nDespite Putnam's rejection of functionalism, it has continued to flourish and has been developed into numerous versions by thinkers as diverse as David Marr, Daniel Dennett, Jerry Fodor, and David Lewis. Functionalism helped lay the foundations for modern cognitive science and is the dominant theory of mind in philosophy today.\n\nBy 2012 Putnam accepted a modification of functionalism called \"liberal functionalism\". The view holds that \"what matters for consciousness and for mental properties generally is the right sort of functional capacities and not the particular matter that subserves those capacities\". The specification of these capacities (1) may refer to what goes on outside the organism's \"brain\", (2) may include intentional idioms, and (3) need not describe a capacity to compute something or other.\n\nOne of Putnam's contributions to philosophy of language is his claim that \"meaning just ain't in the head\". His views on meaning, first laid out in \"Meaning and Reference\" (1973), then in \" The Meaning of 'Meaning' \" (1975), use his famous \"Twin Earth\" thought experiment to illustrate that the meaning of terms are determined by factors outside the mind.\n\nTwin Earth shows this, according to Putnam, since on Twin Earth everything is identical to Earth, except that its lakes, rivers and oceans are filled with XYZ whereas those of earth are filled with HO. Consequently, when an earthling, Fredrick, uses the Earth-English word \"water\", it has a different meaning from the Twin Earth-English word \"water\" when used by his physically identical twin, Frodrick, on Twin Earth. Since Fredrick and Frodrick are physically indistinguishable when they utter their respective words, and since their words have different meanings, meaning cannot be determined solely by what is in their heads. This led Putnam to adopt a version of semantic externalism with regard to meaning and mental content.\nThe late philosopher of mind and language Donald Davidson, despite his many differences of opinion with Putnam, wrote that semantic externalism constituted an \"anti-subjectivist revolution\" in philosophers' way of seeing the world. Since the time of Descartes, philosophers had been concerned with proving knowledge from the basis of subjective experience. Thanks to Saul Kripke, Putnam, Tyler Burge and others, Davidson said, philosophy could now take the objective realm for granted and start questioning the alleged \"truths\" of subjective experience.\n\nPutnam, along with Saul Kripke, Keith Donnellan, and others, contributed to what is known as the causal theory of reference. In particular, Putnam maintained in \"The Meaning of \"Meaning\"\" that the objects referred to by natural kind terms—such as tiger, water, and tree—are the principal elements of the meaning of such terms. There is a linguistic division of labor, analogous to Adam Smith's economic division of labor, according to which such terms have their references fixed by the \"experts\" in the particular field of science to which the terms belong. So, for example, the reference of the term \"lion\" is fixed by the community of zoologists, the reference of the term \"elm tree\" is fixed by the community of botanists, and the reference of the term \"table salt\" is fixed as \"NaCl\" by chemists. These referents are considered rigid designators in the Kripkean sense and are disseminated outward to the linguistic community.\n\nPutnam specifies a finite sequence of elements (a vector) for the description of the meaning of every term in the language. Such a vector consists of four components:\n\nSuch a \"meaning-vector\" provides a description of the reference and use of an expression within a particular linguistic community. It provides the conditions for its correct usage and makes it possible to judge whether a single speaker attributes the appropriate meaning to that expression or whether its use has changed enough to cause a difference in its meaning. According to Putnam, it is legitimate to speak of a change in the meaning of an expression only if the reference of the term, and not its stereotype, has changed. However, since there is no possible algorithm that can determine which aspect—the stereotype or the reference—has changed in a particular case, it is necessary to consider the usage of other expressions of the language. Since there is no limit to the number of such expressions which must be considered, Putnam embraced a form of semantic holism.\n\nPutnam made a significant contribution to philosophy of mathematics in the Quine–Putnam \"indispensability argument\" for mathematical realism. This argument is considered by Stephen Yablo to be one of the most challenging arguments in favor of the acceptance of the existence of abstract mathematical entities, such as numbers and sets. The form of the argument is as follows.\n\n\nThe justification for the first premise is the most controversial. Both Putnam and Quine invoke naturalism to justify the exclusion of all non-scientific entities, and hence to defend the \"only\" part of \"all and only\". The assertion that \"all\" entities postulated in scientific theories, including numbers, should be accepted as real is justified by confirmation holism. Since theories are not confirmed in a piecemeal fashion, but as a whole, there is no justification for excluding any of the entities referred to in well-confirmed theories. This puts the nominalist who wishes to exclude the existence of sets and non-Euclidean geometry, but to include the existence of quarks and other undetectable entities of physics, for example, in a difficult position.\n\nPutnam holds the view that mathematics, like physics and other empirical sciences, uses both strict logical proofs and \"quasi-empirical\" methods. For example, Fermat's last theorem states that for no integer formula_1 are there positive integer values of \"x\", \"y\", and \"z\" such that formula_2. Before this was proven for all formula_1 in 1995 by Andrew Wiles, it had been proven for many values of \"n\". These proofs inspired further research in the area, and formed a quasi-empirical consensus for the theorem. Even though such knowledge is more conjectural than a strictly proven theorem, it was still used in developing other mathematical ideas.\n\nPutnam has contributed to scientific fields not directly related to his work in philosophy. As a mathematician, Putnam contributed to the resolution of Hilbert's tenth problem in mathematics. This problem was settled by Yuri Matiyasevich in 1970, with a proof that relied heavily on previous research by Putnam, Julia Robinson and Martin Davis.\n\nIn computability theory, Putnam investigated the structure of the ramified analytical hierarchy, its connection with the constructible hierarchy and its Turing degrees. He showed that there exist many levels of the constructible hierarchy which do not add any subsets of the integers and later, with his student George Boolos, that the first such \"non-index\" is the ordinal formula_4 of ramified analysis (this is the smallest formula_5 such that formula_6 is a model of full second-order comprehension), and also, together with a separate paper with Richard Boyd (another of Putnam's students) and Gustav Hensel, how the Davis–Mostowski–Kleene hyperarithmetical hierarchy of arithmetical degrees can be naturally extended up to formula_4.\n\nIn computer science, Putnam is known for the Davis–Putnam algorithm for the Boolean satisfiability problem (SAT), developed with Martin Davis in 1960. The algorithm finds if there is a set of true or false values that satisfies a given Boolean expression so that the entire expression becomes true. In 1962, they further refined the algorithm with the help of George Logemann and Donald W. Loveland. It became known as the DPLL algorithm. This algorithm is efficient and still forms the basis of most complete SAT solvers.\n\nIn the field of epistemology, Putnam is known for his \"brain in a vat\" thought experiment (a modernized version of Descartes's evil demon hypothesis). The argument is that one cannot coherently state that one is a disembodied \"brain in a vat\" placed there by some \"mad scientist\".\n\nThis follows from the causal theory of reference. Words always refer to the kinds of things they were coined to refer to, thus the kinds of things their user, or the user's ancestors, experienced. So, if some person, Mary, were a \"brain in a vat\", whose every experience is received through wiring and other gadgetry created by the \"mad scientist\", then Mary's idea of a \"brain\" would not refer to a \"real\" brain, since she and her linguistic community have never seen such a thing. Rather, she saw something that looked like a brain, but was actually an image fed to her through the wiring. Similarly, her idea of a \"vat\" would not refer to a \"real\" vat. So, if, as a brain in a vat, she were to say \"I'm a brain in a vat\", she would actually be saying \"I'm a brain-image in a vat-image\", which is incoherent. On the other hand, if she is not a brain in a vat, then saying that she is a brain in a vat is still incoherent, but now because she actually means the opposite. This is a form of epistemological externalism: knowledge or justification depends on factors outside the mind and is not solely determined internally.\n\nPutnam has clarified that his real target in this argument was never skepticism, but metaphysical realism. Since realism of this kind assumes the existence of a gap between how man conceives the world and the way the world really is, skeptical scenarios such as this one (or Descartes' evil demon) present a formidable challenge. Putnam, by arguing that such a scenario is impossible, attempts to show that this notion of a gap between man's concept of the world and the way it is, is in itself absurd. Man cannot have a \"God's eye\" view of reality. He is limited to his conceptual schemes. Metaphysical realism is therefore false, according to Putnam.\n\nIn the late 1970s and the 1980s, stimulated by results from mathematical logic and by some ideas of Quine, Putnam abandoned his long-standing defence of metaphysical realism—the view that the categories and structures of the external world are both causally and ontologically independent of the conceptualizations of the human mind. He adopted a rather different view, which he called \"internal realism\".\n\nInternal realism was the view that, although the world may be \"causally\" independent of the human mind, the structure of the world—its division into kinds, individuals and categories—is a function of the human mind, and hence the world is not \"ontologically\" independent. The general idea is influenced by Kant's idea of the dependence of our knowledge of the world on the categories of thought.\n\nThe problem with metaphysical realism, according to Putnam, was that it fails to explain the possibility of reference and truth. According to the metaphysical realist, our concepts and categories refer because they match up in some mysterious manner with the pre-structured categories, kinds and individuals that are inherent in the external world. But how is it possible that the world \"carves up\" into certain structures and categories, the mind carves up the world into its own categories and structures, and the two \"carvings\" perfectly coincide? The answer must be that the world does not come pre-structured but that structure must be imposed on it by the human mind and its conceptual schemes. In \"Reason, Truth, and History\", Putnam identified truth with what he termed \"idealized rational acceptability.\" The theory, which owes something to C. S. Peirce, is that a belief is true if it would be accepted by anyone under ideal epistemic conditions.\n\nNelson Goodman had formulated a similar notion in \"Fact, Fiction and Forecast\" in 1956. In that work, Goodman went as far as to suggest that there is \"no one world, but many worlds, each created by the human mind.\" Putnam rejected this form of social constructivism, but retained the idea that there can be many correct descriptions of reality. No one of these descriptions can be scientifically proven to be the \"one, true\" description of the world. This does not imply relativism, for Putnam, because not all descriptions are equally correct and the ones that are correct are not determined subjectively.\n\nPutnam renounced internal realism in his reply to Simon Blackburn in the volume \"Reading Putnam\". The reasons he gave up his \"antirealism\" are stated in the first three of his replies in \"The Philosophy of Hilary Putnam\", an issue of the journal \"Philosophical Topics\", where he gives a history of his use(s) of the term \"internal realism\", and, at more length, in his \"The Threefold Cord: Mind, Body and World\" (1999).\n\nAlthough he abandoned internal realism, Putnam still resists the idea that any given thing or system of things can only be described in exactly one complete and correct way. He thus accepts \"conceptual relativity\" – the view that it may be a matter of choice or convention, e.g., whether mereological sums exist, or whether space-time points are individuals or mere limits. In other words, having abandoned internal realism Putnam came to accept metaphysical realism in simply the broad sense of rejecting all forms of verificationism and all talk of our 'making' the world.\n\nUnder the influence of C. S. Peirce and William James, Putnam also became convinced that there is no fact–value dichotomy; that is, normative (e.g., ethical and aesthetic) judgments often have a factual basis, while scientific judgments have a normative element.\n\nAt the end of the 1980s, Putnam became increasingly disillusioned with what he perceived as the \"scientism\" and the rejection of history that characterize modern analytic philosophy. He rejected internal realism because it assumed a \"cognitive interface\" model of the relation between the mind and the world. Putnam claimed that the very notion of truth would have to be abandoned by a consistent eliminative materialist. Under the increasing influence of James and the pragmatists, he adopted a direct realist view of this relation. For a time, under the influence of Ludwig Wittgenstein, he adopted a pluralist view of philosophy itself and came to view most philosophical problems as nothing more than conceptual or linguistic confusions created by philosophers by using ordinary language out of its original context. In 2017 a book collecting articles on pragmatism by both Ruth Anna and Hilary Putnam was published, \"Pragmatism as a Way of Life: The Lasting Legacy of William James and John Dewey\" (Harvard UP, 2017, ).\n\nMany of Putnam's most recent works have addressed the concerns of ordinary people, particularly their concerns about social problems. For example, he has written about the nature of democracy, social justice and religion. He has discussed the ideas of the continental philosopher, Jürgen Habermas, and has written articles influenced by \"continental\" ideas.\n\nPutnam died of mesothelioma, a rare form of lung cancer, at his home in Arlington, Massachusetts during the early hours of March 13, 2016.\n\nPutnam himself may be his own most formidable philosophical adversary. His frequent changes of mind have led him to attack his previous positions. However, many significant criticisms of his views have come from other philosophers and scientists. For example, multiple realizability has been criticized on the grounds that, if it were true, research and experimentation in the neurosciences would be impossible. According to William Bechtel and Jennifer Mundale, to be able to conduct such research in the neurosciences, universal consistencies must either exist or be assumed to exist in brain structures. It is the similarity (or homology) of brain structures that allows us to generalize across species. If multiple realizability were an empirical fact, results from experiments conducted on one species of animal (or one organism) would not be meaningful when generalized to explain the behavior of another species (or organism of the same species). Other criticisms of metaphysical realism have been proposed by Jaegwon Kim, David Lewis, Robert Richardson and Patricia Churchland.\n\nOne of the main arguments against functionalism was formulated by Putnam himself: the Twin Earth thought experiment. However, there have been other criticisms. The Chinese room argument by John Searle (1980) is a direct attack on the claim that thought can be represented as a set of functions. The thought experiment is designed to show that it is possible to mimic intelligent action, without any interpretation or understanding, through the use of a purely functional system. In short, Searle describes a situation in which a person who speaks only English is locked in a room with Chinese symbols in baskets and a rule book in English for moving the symbols around. The person is instructed, by people outside the room, to follow the rule book for sending certain symbols out of the room when given certain symbols. Further, suppose that the people outside the room are Chinese speakers and are communicating with the person inside via the Chinese symbols. According to Searle, it would be absurd to claim that the English speaker inside \"knows\" Chinese based on these syntactic processes alone. This thought experiment attempts to show that systems that operate merely on syntactic processes cannot realize any semantics (meaning) or intentionality (aboutness). Thus, Searle attacks the idea that thought can be equated with the following of a set of syntactic rules. Thus, functionalism is an inadequate theory of the mind. Several other arguments against functionalism have been advanced by Ned Block.\n\nPutnam has consistently adhered to the idea of semantic holism, in spite of the many changes in his other positions. The problems with this position have been described by Michael Dummett, Jerry Fodor, Ernest Lepore, and others. In the first place, they suggest that, if semantic holism is true, it is impossible to understand how a speaker of a language can learn the meaning of an expression, for any expression of the language. Given the limits of our cognitive abilities, we will never be able to master the whole of the English (or any other) language, even based on the (false) assumption that languages are static and immutable entities. Thus, if one must understand all of a natural language to understand a single word or expression, language learning is simply impossible. Semantic holism also fails to explain how two speakers can mean the same thing when using the same linguistic expression, and therefore how any communication at all is possible between them. Given a sentence \"P\", since Fred and Mary have each mastered different parts of the English language and \"P\" is related in different ways to the sentences in each part, the result is that \"P\" means one thing for Fred and something else for Mary. Moreover, if a sentence \"P\" derives its meaning from its relations with all of the sentences of a language, as soon as the vocabulary of an individual changes by the addition or elimination of a sentence, the totality of relations changes, and therefore also the meaning of \"P\". As this is a common phenomenon, the result is that \"P\" has two different meanings in two different moments in the life of the same person. Consequently, if I accept the truth of a sentence and then reject it later on, the meaning of that which I rejected and that which I accepted are completely different and therefore I cannot change my opinions with regard to the same sentences.\n\nThe brain in a vat argument has also been subject to criticism. Crispin Wright argues that Putnam's formulation of the brain-in-a-vat scenario is too narrow to refute global skepticism. The possibility that one is a recently disembodied brain in a vat is not undermined by semantic externalism. If a person has lived her entire life outside the vat—speaking the English language and interacting normally with the outside world—prior to her \"envatment\" by a mad scientist, when she wakes up inside the vat, her words and thoughts (e.g., \"tree\" and \"grass\") will still refer to the objects or events in the external world that they referred to before her envatment. In another scenario, a brain in a vat may be hooked up to a supercomputer that randomly generates perceptual experiences. In this case, one's words and thoughts would not refer to anything, and would therefore be devoid of content. Semantics would no longer exist and the argument would be meaningless.\n\nIn philosophy of mathematics, Stephen Yablo has argued that the Quine–Putnam indispensability thesis does not demonstrate that mathematical entities are truly indispensable. The argumentation is sophisticated, but the upshot is that one can achieve the same logical results by simply adding to any statement about an abstract object the assumption \"so-and-so is assumed (or hypothesized) to exist\". For example, one can take the argument for indispensability described above and adjust it as follows:\n\nFinally, Putnam's internal realism has been accused by Curtis Brown of being a disguised form of subjective idealism. If this is the case, it is subject to the traditional arguments against that position. In particular, it falls into the trap of solipsism. That is, if existence depends on experience, as subjective idealism maintains, and if one's consciousness were to stop existing, then the rest of the universe would stop existing as well.\n\nThere is a detailed bibliography of Hilary Putnam's writings (with 16 books and 198 articles) compiled by Vincent C. Müller and published in 1993. Online in PhilPapers. A later version of this is on Harvard's Servers.\n\n\n\n\n"}
{"id": "13050911", "url": "https://en.wikipedia.org/wiki?curid=13050911", "title": "History of the Church–Turing thesis", "text": "History of the Church–Turing thesis\n\nThe history of the Church–Turing thesis (\"thesis\") involves the history of the development of the study of the nature of functions whose values are effectively calculable; or, in more modern terms, functions whose values are algorithmically computable. It is an important topic in modern mathematical theory and computer science, particularly associated with the work of Alonzo Church and Alan Turing.\n\nThe debate and discovery of the meaning of \"computation\" and \"recursion\" has been long and contentious. This article provides detail of that debate and discovery from Peano's axioms in 1889 through recent discussion of the meaning of \"axiom\".\n\nIn 1889, Giuseppe Peano presented his \"The principles of arithmetic, presented by a new method\", based on the work of Dedekind. Soare proposes that the origination of \"primitive recursion\" began formally with the axioms of Peano, although\n\nObserve that \"in fact\" Peano's axioms are \"9\" in number and axiom \"9\" is the recursion/induction axiom.\n\nAt the International Congress of Mathematicians (ICM) in 1900 in Paris the famous mathematician David Hilbert posed a set of problems – now known as Hilbert's problems – his beacon illuminating the way for mathematicians of the twentieth century. Hilbert's 2nd and 10th problems introduced the Entscheidungsproblem (the \"decision problem\"). In his 2nd problem he asked for a proof that \"arithmetic\" is \"consistent\". Kurt Gödel would prove in 1931 that, within what he called \"P\" (nowadays called Peano Arithmetic), \"there exist undecidable sentences [propositions]\". Because of this, \"the consistency of P is unprovable in P, provided P is consistent\". While Gödel’s proof would display the tools necessary for Alonzo Church and Alan Turing to resolve the Entscheidungsproblem, he himself would not answer it.\n\nIt is within Hilbert's 10th problem where the question of an \"Entscheidungsproblem\" actually appears. The heart of matter was the following question: \"What do we mean when we say that a function is 'effectively calculable'\"? The answer would be something to this effect: \"When the function is calculated by a \"mechanical\" procedure (process, method).\" Although stated easily nowadays, the question (and answer) would float about for almost 30 years before it was framed precisely.\n\nHilbert's original description of problem 10 begins as follows:\n\nBy 1922, the specific question of an \"Entscheidungsproblem\" applied to Diophantine equations had developed into the more general question about a \"decision method\" for \"any\" mathematical formula. \nMartin Davis explains it this way: Suppose we are given a \"calculational procedure\" that consists of (1) a set of axioms and (2) a logical conclusion written in first-order logic, that is—written in what Davis calls \"Frege's rules of deduction\" (or the modern equivalent of Boolean logic). Gödel’s doctoral dissertation proved that Frege's rules were \"complete\" \"... in the sense that every valid formula is provable\". Given that encouraging fact, could there be a generalized \"calculational procedure\" that would tell us whether a conclusion can be derived from its premises? Davis calls such calculational procedures \"algorithms\". The Entscheidungsproblem would be an algorithm as well. \"In principle, an algorithm for [the] Entscheidungsproblem would have reduced all human deductive reasoning to brute calculation\".\n\nIn other words: Is there an \"algorithm\" that can tell us if \"any\" formula is \"true\" (i.e. an algorithm that always correctly yields a judgment \"truth\" or \"falsehood\"?)\n\nIndeed: What about our Entscheidungsproblem algorithm itself? Can it determine, in a finite number of steps, whether it, itself, is “successful” and \"truthful\" (that is, it does not get hung up in an endless \"circle\" or \"loop\", and it correctly yields a judgment \"truth\" or \"falsehood\" about its own behavior and results)?\n\nAt the 1928 Congress [in Bologna, Italy] Hilbert refines the question very carefully into three parts. The following is Stephen Hawking's summary:\n\nGabriel Sudan (1927) and Wilhelm Ackermann (1928) display recursive functions that are not \"primitive\" recursive:\n\nIn subsequent years Kleene observes that Rózsa Péter (1935) simplified Ackermann's example (\"cf. also Hilbert-Bernays 1934\") and Raphael Robinson (1948). Péter exhibited another example (1935) that employed Cantor's diagonal argument. Péter (1950) and Ackermann (1940) also displayed \"transfinite recursions\", and this led Kleene to wonder:\n\nKleene concludes that all \"recursions\" involve (i) the formal analysis he presents in his §54 \"Formal calculations of primitive recursive functions\" and, (ii) the use of mathematical induction. He immediately goes on to state that indeed the Gödel-Herbrand definition does indeed \"characterize all recursive functions\" – see the quote in 1934, below.\n\nIn 1930, mathematicians gathered for a mathematics meeting and retirement event for Hilbert. As luck would have it,\n\nHe announced that the answer to the first two of Hilbert's three questions of 1928 was NO.\n\nSubsequently in 1931 Gödel published his famous paper \"On Formally Undecidable Propositions of Principia Mathematica and Related \" In his preface to this paper Martin Davis delivers a caution:\n\nTo quote Kleene (1952), \"The characterization of all \"recursive functions\" was accomplished in the definition of 'general recursive function' by Gödel 1934, who built on a suggestion of Herbrand\" (Kleene 1952:274). Gödel delivered a series of lectures at the Institute for Advanced Study (IAS), Princeton NJ. In a preface written by Martin Davis Davis observes that\n\nDawson states that these lectures were meant to clarify concerns that the \"incompleteness theorems were somehow dependent on the particularities of formalization\":\n\nKleene and Rosser transcribed Gödel's 1934 lectures in Princeton. In his paper \"General Recursive Functions of Natural Numbers\" Kleene states:\n\nChurch's paper \"An Unsolvable Problem of Elementary Number Theory\" (1936) proved that the Entscheidungsproblem was undecidable within the λ-calculus and Gödel-Herbrand's general recursion; moreover Church cites two theorems of Kleene's that proved that the functions defined in the λ-calculus are identical to the functions defined by general recursion:\n\nThe paper opens with a very long footnote, 3. Another footnote, 9, is also of interest. Martin Davis states that \"This paper is principally important for its explicit statement (since known as Church's thesis) that the functions which can be computed by a finite algorithm are precisely the recursive functions, and for the consequence that an explicit unsolvable problem can be given\":\n\nFootnote 9 is in section \"§4 Recursive functions\":\n\nSome time prior to Church's paper \"An Unsolvable Problem of Elementary Number Theory\" (1936) a dialog occurred between Gödel and Church as to whether or not λ-definability was sufficient for the definition of the notion of \"algorithm\" and \"effective calculability\".\n\nIn Church (1936) we see, under the chapter §7 \"The notion of effective calculability\", a footnote 18 which states the following:\n\nBy \"identifying\" Church means – not \"establishing the identity of\" – but rather \"to cause to be or become identical\", \"to conceive as united\" (as in spirit, outlook or principle) (vt form), and (vi form) as \"to be or become the same\".\n\nPost's doubts as to whether or not recursion was an adequate definition of \"effective calculability\", plus the publishing of Church's paper, encouraged him in the fall of 1936 to propose a \"formulation\" with \"psychological fidelity\": A worker moves through \"a sequence of spaces or boxes\" performing machine-like \"primitive acts\" on a sheet of paper in each box. The worker is equipped with \"a fixed ualterable set of directions\". Each instruction consists of three or four symbols: (1) an identifying label/number, (2) an operation, (3) next instruction j; however, if the instruction is of type (e) and the determination is \"yes\" THEN instruction j' ELSE if it is \"no\" instruction j. The \"primitive acts\" are of only 1 of 5 types: (a) mark the paper in the box he's in (or over-mark a mark already there), (b) erase the mark (or over-erase), (c) move one room to the right, (d) move one room to the left, (e) determine if the paper is marked or blank. The worker starts at step 1 in the starting-room, and does what the instructions instruct them to do. (See more at Post–Turing machine.)\n\nThis matter, mentioned in the introduction about \"intuitive theories\" caused Post to take a potent poke at Church:\n\nIn other words Post is saying \"Just because you \"defined\" it so doesn't \"make\" it truly so; your definition is based on no more than an intuition.\" Post was searching for more than a definition: \"The success of the above program would, for us, change this hypothesis not so much to a definition or to an axiom but to a \"natural law\". Only so, it seems to the writer, can Gödel's theorem ... and Church's results ... be transformed into conclusions concerning all symbolic logics and all methods of solvability.\"\n\nThis contentious stance finds grumpy expression in Alan Turing 1939, and it will reappear with Gödel, Gandy, and Sieg.\n\nA. M. Turing's paper \"On Computable Numbers, With an Application to the Entscheidungsproblem\" was delivered to the London Mathematical Society in November 1936. Again the reader must bear in mind a caution: as used by Turing, the word \"computer\" is a human being, and the action of a \"computer\" he calls \"computing\"; for example, he states \"Computing is normally done by writing certain symbols on paper\" (p. 135). \"But\" he uses the word \"computation\" in the context of his machine-definition, and his definition of \"computable\" numbers is as follows:\n\nWhat is Turing's definition of his \"machine?\" Turing gives two definitions, the first a summary in \"§1 Computing machines\" and another very similar in §9.I derived from his more detailed analysis of the actions a human \"computer\". With regards to his definition §1 he says that \"justification lies in the fact that the human memory is necessarily limited\", and he concludes §1 with the bald assertion of his proposed machine with his use of the word \"all\"\n\nThe emphasis of the word one in the above brackets is intentional. With regards to §9.I he allows the machine to examine \"more\" squares; it is this more-square sort of behavior that he claims typifies the actions of a computer (person):\n\nTuring goes on to define a \"computing machine\" in §2 is (i) \"a-machine\" (\"automatic machine\") as defined in §1 with the added restriction (ii): (ii) It prints two kinds of symbols – figures 0 and 1 – and other symbols. The figures 0 and 1 will represent \"the sequence computed by the machine\".\n\nFurthermore, to define the if the \"number\" is to be considered \"computable\", the machine must print an infinite number of 0's and 1's; if not it is considered to be \"circular\"; otherwise it is considered to be \"circle-free\":\n\nAlthough he doesn't call it his \"thesis\", Turing proposes a proof that his \"computability\" is equivalent to Church's \"effective calculability\":\n\nThe \"Appendix: Computability and effective calculability\" begins in the following manner; observe that he does \"not\" mention \"recursion\" here, and in fact his proof-sketch has his machine munch strings of symbols in the λ-calculus and the calculus munch \"complete configurations\" of his machine, and nowhere is recursion mentioned. The proof of the equivalence of machine-computability and recursion must wait for Kleene 1943 and 1952:\n\nGandy (1960) seems to confuse this bold proof-sketch with Church's Thesis; see 1960 and 1995 below. Moreover a careful reading of Turing's definitions leads the reader to observe that Turing was asserting that the \"operations\" of his proposed machine in §1 are \"sufficient\" to compute \"any\" computable number, and the machine that imitates the action of a human \"computer\" as presented in §9.I is a variety of this proposed machine. This point will be reiterated by Turing in 1939.\n\nAlan Turing's massive Princeton PhD thesis (under Alonzo Church) appears as \"Systems of Logic Based on Ordinals\". In it he summarizes the quest for a definition of \"effectively calculable\". He proposes a definition as shown in the boldface type that specifically identifies (renders identical) the notions of \"machine computation\" and \"effectively calculable\".\n\nThis is a powerful expression. because \"identicality\" is actually an unequivocal statement of necessary and sufficient conditions, in other words there are no other contingencies to the identification\" except what interpretation is given to the words \"function\", \"machine\", \"computable\", and \"effectively calculable\":\n\nJ. B. Rosser's paper \"An Informal Exposition of Proofs of Gödel's Theorem and Church's Theorem\" states the following:\n\nKleene defines \"general recursive\" functions and \"partial recursive functions\" in his paper \"Recursive Predicates and Quantifiers\". The representing function, mu-operator, etc make their appearance. He goes on in §12 \"Algorithm theories\" to state his famous Thesis I, what he would come to call Church's Thesis in 1952:\n\nIn his chapter §60, Kleene defines the \"Church's thesis\" as follows:\n\nOn page 317 he \"explicitly\" calls the above thesis \"Church's thesis\":\n\nAbout Turing's \"formulation\", Kleene says:\n\nKleene proposes that what Turing showed: \"Turing's computable functions (1936-1937) are those which can be computed by a machine of a kind which is designed, according to his analysis, to reproduce all the sorts of operations which a human computer could perform, working according to preassigned instructions.\" \n\nThis interpretation of Turing plays into Gandy's concern that a machine specification may not explicitly \"reproduce all the sorts of operations which a human computer could perform\" – i.e. his two examples are (i) massively symbol-parallel computation and two-dimensional computation e.g. Conway's \"game of life\". Therefore there may be processes that can \"compute more\" than a Turing machine can. See 1980 below.\n\nKleene defines Turing's Thesis as follows:\n\nIndeed immediately before this statement, Kleene states the Theorem XXX:\n\nTo his 1931 paper \"On Formally Undecidable Propositions\", Gödel added a \"Note added 28 August 1963\" which clarifies his opinion of the alternative forms/expression of \"a formal system\". He reiterates his opinions even more clearly in 1964 (see below):\n\nGödel 1964 – In Gödel's \"Postscriptum\" to his lecture's notes of 1934 at the IAS at Princeton, he repeats, but reiterates in even more bold terms, his less-than-glowing opinion about the efficacy of computability as defined by Church's λ-definability and recursion (we have to infer that both are denigrated because of his use of the plural \"definitions\" in the following). This was in a letter to Martin Davis (presumably as he was assembling \"The Undecidable\"). The repeat of some of the phrasing is striking:\n\nFootnote 3 is in the body of the 1934 lecture notes:\n\nDavis does observe that \"in fact the equivalence between his [Gödel's] definition [of recursion] and Kleene's [1936] is not quite trivial. So, despite appearances to the contrary, footnote 3 of these lectures is not a statement of Church's thesis.\"\n\nRobin Gandy's influential paper titled \"Church's Thesis and Principles for Mechanisms\" appears in Barwise et al. Gandy starts off with an unlikely expression of Church's Thesis, framed as follows:\n\nRobert Soare (1995, see below) had issues with this framing, considering Church's paper (1936) published prior to Turing's \"Appendix proof\" (1937).\n\nGandy attempts to \"analyze mechanical processes and so to provide arguments for the following:\n\nGandy \"exclude[s] from consideration devices which are essentially analogue machines ... .The only physical presuppositions made about mechanical devices (Cf Principle IV below) are that there is a lower bound on the linear dimensions of every atomic part of the device and that there is an upper bound (the velocity of light) on the speed of propagation of change\". But then he restricts his machines even more:\n\nHe in fact makes an argument for this \"Thesis M\" that he calls his \"Theorem\", the most important \"Principle\" of which is \"Principle IV: Principle of local causation\":\n\nSoare's thorough examination of \"Computability and Recursion\" appears. He quotes Gödel's 1964 opinion (above) with respect to the \"much less suitable\" definition of computability, and goes on to add:\n\nSoare's footnote 7 (1995) also catches Gandy's \"confusion\", but apparently it continues into Gandy (1988). This confusion represents a serious error of research and/or thought and remains a cloud hovering over his whole program:\n\nBreger points out a problem when one is approaching a notion \"axiomatically\", that is, an \"axiomatic system\" may have imbedded in it one or more \"tacit\" axioms that are unspoken when the axiom-set is presented.\n\nFor example, an active agent with knowledge (and capability) may be a (potential) fundamental axiom in any axiomatic system: \"the know-how of a human being is necessary – a know-how which is not formalized in the axioms. ¶ ... Mathematics as a purely formal system of symbols without a human being possessing the know-how with the symbols is impossible ...\"\n\nHe quotes Hilbert:\n\nBreger further supports his argument with examples from Giuseppe Veronese (1891) and Hermann Weyl (1930-1). He goes on to discuss the problem of then expression of an axiom-set in a particular language: i.e. a language known by the agent, e.g. German.\n\nSee more about this at Algorithm characterizations, in particular Searle's opinion that outside any computation there must be an observer that gives meaning to the symbols used.\n\nAt the \"Feferfest\" – Solomon Feferman's 70th birthday – Wilfried Sieg first presents a paper written two years earlier titled \"Calculations By Man and Machine: Conceptual Analysis\", reprinted in (Sieg et al. 2002:390–409). Earlier Sieg published \"Mechanical Procedures and Mathematical Experience\" (in George 1994, p. 71ff) presenting a history of \"calculability\" beginning with Richard Dedekind and ending in the 1950s with the later papers of Alan Turing and Stephen Cole Kleene. The Feferfest paper distills the prior paper to its major points and dwells primarily on Robin Gandy's paper of 1980. Sieg extends Turing's \"computability by string machine\" (human \"computor\") as reduced to mechanism \"computability by letter machine\" to the \"parallel\" machines of Gandy.\n\nSieg cites more recent work including \"Kolmogorov and Uspensky's work on algorithms\" and (De Pisapia 2000), in particular, the KU-pointer machine-model), and artificial neural networks and asserts:\n\nHe claims to \"step toward a more satisfactory stance ... [by] abstracting further away from particular types of configurations and operations ...\"\n\nWhether the above statement is true or not is left to the reader to ponder. Sieg goes on to describe Gandy's analysis (see above 1980). In doing so he attempts to formalize what he calls \"Gandy machines\" (with a detailed analysis in an Appendix). About the Gandy machines:\n\n\n"}
{"id": "95489", "url": "https://en.wikipedia.org/wiki?curid=95489", "title": "James Stirling (mathematician)", "text": "James Stirling (mathematician)\n\nJames Stirling (May 1692, Garden, Stirlingshire – 5 December 1770, Edinburgh) was a Scottish mathematician. He was nicknamed \"The Venetian\".\n\nThe Stirling numbers, Stirling permutations, and Stirling's approximation are named after him. He also proved the correctness of Isaac Newton's classification of cubics.\n\nStirling was born on 11 May 1692 at Garden House near Stirling, the third son of Archibald Stirling, Lord Garden.\n\nAt 18 years of age he went to Balliol College, Oxford, where, chiefly through the influence of the Earl of Mar, he was nominated in 1711 to be one of Bishop Warner's exhibitioners (or Snell exhibitioner) at Balliol. In 1715 he was expelled on account of his correspondence with his cousins, who were members of the Keir and Garden families, who were noted Jacobites, and had been accessory to the \"Gathering of the Brig o' Turk\" in 1708.\n\nFrom Oxford he made his way to Venice, where he occupied himself as a professor of mathematics. In 1717 appeared his \"Lineae tertii ordinis Newtonianae, sive . . .\" (8vo, Oxford). While in Venice, also, he communicated, through Isaac Newton, to the Royal Society a paper entitled \"Methodus differentialis Newtoniana illustrata\" (\"Phil. Trans.\", 1718). Fearing assassination on account of having discovered a trade secret of the glassmakers of Venice, he returned with Newton's help to London about the year 1725.\n\nIn London he remained for ten years, being most part of the time connected with an academy in Tower Street, and devoting his leisure to mathematics and correspondence with eminent mathematicians. In 1730 his most important work was published, the \"Methodus differentialis, sive tractatus de summatione et interpolatione serierum infinitarum\" (4to, London), which is something more than an expansion of the paper of 1718. In 1735, he communicated to the Royal Society a paper \"On the Figure of the Earth, and on the Variation of the Force of Gravity at its Surface.\"\n\nIn the same year he was appointed manager for the Scots Mining Company at Leadhills, where the Scots Mining Company House was built for him in 1736. His next paper to the Royal Society was concerned, not with pure, but with applied sciences; specifically, a trompe, i.e., a water-powered air compressor that was used by a Scottish lead mine. His name is also connected with another practical undertaking, since grown to vast dimensions. The accounts of the city of Glasgow for 1752 show that the very first instalment of ten millions sterling spent in making Glasgow a seaport, viz. a sum of £28, 4s. 4d., was for a silver tea-kettle to be presented to \"James Stirling, mathematician, for his service, pains, and trouble in surveying the river towards deepening it by locks.\"\n\nAnother edition of the \"Lineae tertii ordinis\" was published in Paris in 1797; another edition of the \"Methodus differentialis\" in London in 1764; and a translation of the latter into English by Halliday in London in 1749. A considerable collection of literary remains, consisting of papers, letters and two manuscript volumes of a treatise on weights and measures, are still preserved at Garden.\n\n"}
{"id": "32823409", "url": "https://en.wikipedia.org/wiki?curid=32823409", "title": "Jordan's theorem (symmetric group)", "text": "Jordan's theorem (symmetric group)\n\nJordan's theorem is a statement in finite group theory. It states that if a primitive permutation group \"G\" is a subgroup of the symmetric group \"S\" and contains a \"p\"-cycle for some prime number \"p\" < \"n\" − 2, then \"G\" is either the whole symmetric group \"S\" or the alternating group \"A\".\n\nThe statement can be generalized to the case that \"p\" is a prime power.\n\n\n"}
{"id": "590428", "url": "https://en.wikipedia.org/wiki?curid=590428", "title": "Journal of Theoretical Biology", "text": "Journal of Theoretical Biology\n\nThe Journal of Theoretical Biology is a biweekly peer-reviewed scientific journal covering theoretical biology, as well as mathematical, computational and statistical aspects of biology. Some research areas covered by the journal include cell biology, evolutionary biology, population genetics, morphogenesis, and immunology. It is often referred to informally by the initials 'JTB'.\n\nThe journal was established in 1961. Its founding editor-in-chief was English biologist James F. Danielli, who remained editor until his death in 1984. Other notable founding members of the Editorial Board included Melvin Calvin, Barry Commoner, Christian De Duve, Daniel Mazia, Jacques Monod, Conrad Waddington, and John Zachary Young. The \"Journal of Theoretical Biology\" is published by Elsevier and, , the editors-in-chief are Denise Kirschner (University of Michigan Medical School), Yoh Iwasa (Kyushu University), and Lewis Wolpert (University College London). According to the \"Journal Citation Reports\", the journal has a 2015 impact factor of 2.049.\n\nSome of the most highly cited articles that have been published in the journal include:\n"}
{"id": "953148", "url": "https://en.wikipedia.org/wiki?curid=953148", "title": "Lagrange reversion theorem", "text": "Lagrange reversion theorem\n\nIn mathematics, the Lagrange reversion theorem gives series or formal power series expansions of certain implicitly defined functions; indeed, of compositions with such functions.\n\nLet \"v\" be a function of \"x\" and \"y\" in terms of another function \"f\" such that\nThen for any function \"g\", for small enough \"y\":\nIf \"g\" is the identity, this becomes\n\nIn 1770, Joseph Louis Lagrange (1736–1813) published his power series solution of the implicit equation for \"v\" mentioned above. However, his solution used cumbersome series expansions of logarithms. In 1780, Pierre-Simon Laplace (1749–1827) published a simpler proof of the theorem, which was based on relations between partial derivatives with respect to the variable x and the parameter y. Charles Hermite (1822–1901) presented the most straightforward proof of the theorem by using contour integration.\n\nLagrange's reversion theorem is used to obtain numerical solutions to Kepler's equation.\n\nWe start by writing:\n\nWriting the delta-function as an integral we have:\n\nThe integral over \"k\" then gives formula_6 and we have:\n\nRearranging the sum and cancelling then gives the result:\n\n"}
{"id": "39180704", "url": "https://en.wikipedia.org/wiki?curid=39180704", "title": "Lev M. Bregman", "text": "Lev M. Bregman\n\nLev M. Bregman (born 31 January 1941 in Leningrad) is a Soviet and Israeli mathematician, most known for the Bregman divergence named after him.\n\nBregman received his M. Sc. in mathematics in 1963 at Leningrad University and his Ph.D. in mathematics in 1966 at the same institution, under the direction of his advisor Prof. J. V. Romanovsky, for his thesis about relaxation methods for finding a common point of convex sets, which led to one of his most well-known publications.\n\nBregman's Theorem, proving a 1963 conjecture of Henryk Minc, gives an upper bound on the permanent of a 0-1 matrix.\n\nBregman is currently employed at the Institute for Industrial Mathematics, Beer-Sheva, Israel, after having spent one year at Ben-Gurion University of the Negev, Beer-Sheva. Formerly, during 1966-1991, he was senior researcher at University of Leningrad.\n\nBregman is author of several text books and dozens of publications in international journals.\n\n\n"}
{"id": "5102088", "url": "https://en.wikipedia.org/wiki?curid=5102088", "title": "Manjul Bhargava", "text": "Manjul Bhargava\n\nManjul Bhargava (born 8 August 1974) is a Canadian-American mathematician. He is the R. Brandon Fradd Professor of Mathematics at Princeton University, the Stieltjes Professor of Number Theory at Leiden University, and also holds Adjunct Professorships at the Tata Institute of Fundamental Research, the Indian Institute of Technology Bombay, and the University of Hyderabad. He is known primarily for his contributions to number theory.\n\nBhargava was awarded the Fields Medal in 2014. According to the International Mathematical Union citation, he was awarded the prize \"for developing powerful new methods in the geometry of numbers, which he applied to count rings of small rank and to bound the average rank of elliptic curves\".\n\nBhargava was born in Hamilton, Ontario, Canada to immigrant parents from India and he grew up primarily in Long Island, New York. His mother Mira Bhargava, a mathematician at Hofstra University, was his first mathematics teacher. He completed all of his high school math and computer science courses by age 14. He attended Plainedge High School in North Massapequa, and graduated in 1992 as the class valedictorian. He obtained his B.A. from Harvard University in 1996. For his research as an undergraduate, he was awarded the 1996 Morgan Prize. Bhargava went on to receive his doctorate from Princeton in 2001, supervised by Andrew Wiles and funded by a Hertz Fellowship. He was a visiting scholar at the Institute for Advanced Study in 2001-02, and at Harvard University in 2002-03. Princeton appointed him as a tenured Full Professor in 2003. He was appointed to the Stieltjes Chair in Leiden University in 2010.\n\nBhargava is also an accomplished tabla player, having studied under gurus such as Zakir Hussain. He also studied Sanskrit from his grandfather Purushottam Lal Bhargava, a well-known scholar of Sanskrit and ancient Indian history. He is an admirer of Sanskrit poetry.\n\nHis PhD thesis generalized Gauss's classical law for composition of binary quadratic forms to many other situations. One major use of his results is the parametrization of quartic and quintic orders in number fields, thus allowing the study of asymptotic behavior of arithmetic properties of these orders and fields.\n\nHis research also includes fundamental contributions to the representation theory of quadratic forms, to interpolation problems and p-adic analysis, to the study of ideal class groups of algebraic number fields, and to the arithmetic theory of elliptic curves. A short list of his specific mathematical contributions are:\n\n\nIn 2015 Manjul Bhargava and Arul Shankar proved the Birch and Swinnerton-Dyer conjecture for a positive proportion of elliptic curves.\n\nBhargava has won several awards for his research, the most prestigious being the Fields Medal, the highest award in the field of mathematics, which he won in 2014.\n\nBhargava is the third youngest full professor in Princeton University's history, after Charles Fefferman and John Pardon.\n\nIn addition, he won the Morgan Prize and Hertz Fellowship in 1996, a Clay 5-year Research Fellowship, the Merten M. Hasse Prize from the MAA in 2003, the Clay Research Award in 2005, and the Leonard M. and Eleanor B. Blumenthal Award for the Advancement of Research in Pure Mathematics in 2005.\n\nPeter Sarnak of Princeton University has said of Bhargava:\n\nHe was named one of Popular Science Magazine's \"Brilliant 10\" in November 2002. He won the $10,000 SASTRA Ramanujan Prize, shared with Kannan Soundararajan, awarded by SASTRA in 2005 at Thanjavur, India, for his outstanding contributions to number theory.\n\nIn 2008, Bhargava was awarded the American Mathematical Society's Cole Prize. The citation reads:\n\nIn 2009, he was awarded the Face of the Future award at the India Abroad Person of the Year ceremony in New York City. In 2014, the same publication, the most prestigious and most widely read of the diaspora publications, gave him a second prize, India Abroad Publisher's Prize for Special Excellence.\n\nIn 2011, he was awarded the Fermat Prize for \"various generalizations of the Davenport-Heilbronn estimates and for his startling recent results (with Arul Shankar) on the average rank of elliptic curves\".\n\nIn 2011, he delivered the Hedrick lectures of the MAA in Lexington, Kentucky. He was also the 2011 Simons Lecturer at MIT.\n\nIn 2012, Bhargava was named an inaugural recipient of the Simons Investigator Award, and became a fellow of the American Mathematical Society in its inaugural class of fellows.\n\nHe was awarded the 2012 Infosys Prize in mathematics for his \"extraordinarily original work in algebraic number theory, which has revolutionized the way in which number fields and elliptic curves are counted\".\n\nIn 2013, he was elected to the National Academy of Sciences.\n\nIn 2014, Bhargava was awarded the Fields Medal at the International Congress of Mathematicians in Seoul for \"developing powerful new methods in the geometry of numbers, which he applied to count rings of small rank and to bound the average rank of elliptic curves\".\n\nIn 2015, he was awarded the Padma Bhushan, the third highest civilian award of India.\n\nIn 2018 Bhargava was named as the inaugural occupant of The Distinguished Chair for the Public Dissemination of Mathematics at The National Museum of Mathematics (MoMath). This is the first visiting professorship in the United States dedicated exclusively to raising public awareness of mathematics.\n\n\n\n"}
{"id": "22999330", "url": "https://en.wikipedia.org/wiki?curid=22999330", "title": "Markov switching multifractal", "text": "Markov switching multifractal\n\nIn financial econometrics, the Markov-switching multifractal (MSM) is a model of asset returns developed by Laurent E. Calvet and Adlai J. Fisher that incorporates stochastic volatility components of heterogeneous durations. MSM captures the outliers, log-memory-like volatility persistence and power variation of financial returns. In currency and equity series, MSM compares favorably with standard volatility models such as GARCH(1,1) and FIGARCH both in- and out-of-sample. MSM is used by practitioners in the financial industry to forecast volatility, compute value-at-risk, and price derivatives.\n\nThe MSM model can be specified in both discrete time and continuous time.\n\nLet formula_1 denote the price of a financial asset, and let formula_2 denote the return over two consecutive periods. In MSM, returns are specified as\n\nwhere formula_4 and formula_5 are constants and {formula_6} are independent standard Gaussians. Volatility is driven by the first-order latent Markov state vector:\n\nGiven the volatility state formula_8, the next-period multiplier formula_9 is drawn from a fixed distribution with probability formula_10, and is otherwise left unchanged.\nThe transition probabilities are specified by\n\nThe sequence formula_10 is approximately geometric formula_13 at low frequency. The marginal distribution has a unit mean, has a positive support, and is independent of .\n\nIn empirical applications, the distribution is often a discrete distribution that can take the values formula_14 or formula_15 with equal probability. The return process formula_16 is then specified by the parameters formula_17. Note that the number of parameters is the same for all formula_18.\n\nMSM is similarly defined in continuous time. The price process follows the diffusion:\n\nwhere formula_20, formula_21 is a standard Brownian motion, and formula_4 and formula_23 are constants. Each component follows the dynamics:\n\nThe intensities vary geometrically with :\n\nWhen the number of components formula_25 goes to infinity, continuous-time MSM converges to a multifractal diffusion, whose sample paths take a continuum of local Hölder exponents on any finite time interval.\n\nWhen formula_26 has a discrete distribution, the Markov state vector formula_8 takes finitely many values formula_28. For instance, there are formula_29 possible states in binomial MSM. The Markov dynamics are characterized by the transition matrix formula_30 with components formula_31.\nConditional on the volatility state, the return formula_16 has Gaussian density\n\nThe log likelihood function has the following analytical expression:\n\nMaximum likelihood provides reasonably precise estimates in finite samples.\n\nWhen formula_26 has a continuous distribution, estimation can proceed by simulated method of moments, or simulated likelihood via a particle filter.\n\nGiven formula_36, the conditional distribution of the latent state vector at date formula_37 is given by:\n\nMSM often provides better volatility forecasts than some of the best traditional models both in and out of sample. Calvet and Fisher report considerable gains in exchange rate volatility forecasts at horizons of 10 to 50 days as compared with GARCH(1,1), Markov-Switching GARCH, and Fractionally Integrated GARCH. Lux obtains similar results using linear predictions.\n\nExtensions of MSM to multiple assets provide reliable estimates of the value-at-risk in a portfolio of securities.\n\nIn financial economics, MSM has been used to analyze the pricing implications of multifrequency risk. The models have had some success in explaining the excess volatility of stock returns compared to fundamentals and the negative skewness of equity returns. They have also been used to generate multifractal jump-diffusions.\n\nMSM is a stochastic volatility model with arbitrarily many frequencies. MSM builds on the convenience of regime-switching models, which were advanced in economics and finance by James D. Hamilton. \nMSM is closely related to the Multifractal Model of Asset Returns. MSM improves on the MMAR’s combinatorial construction by randomizing arrival times, guaranteeing a strictly stationary process. \nMSM provides a pure regime-switching formulation of multifractal measures, which were pioneered by Benoit Mandelbrot.\n\n\n"}
{"id": "27742079", "url": "https://en.wikipedia.org/wiki?curid=27742079", "title": "Mathnasium", "text": "Mathnasium\n\nMathnasium (also Mathnasium Learning Center) is an education brand and supplemental learning franchise consisting of over 900 learning centers in North America, South America, Europe, the Middle East, and Asia that provides instruction in mathematics to students in pre-kindergarten through high school. The curriculum employs the Mathnasium Method, a proprietary system that was developed over 35 years by co-founder Lawrence Martinek. \nMartinek, a Los Angeles-based mathematics educator, began developing the Mathnasium Method early in his teaching career by writing his own supplemental materials to school curricula. In 1985, Martinek published \"Math Tips for Parents\", a guide for parents and teachers based on his own experiences and his work with his son. In the book, Martinek argues that students build confidence and mastery in mathematics through successful encounter and interaction with carefully selected materials. Martinek based the Mathnasium Method on the premise that a student's dislike of math stems from the frustration and embarrassment of not understanding math the way it is taught in the classroom. He emphasizes that an approach combining oral, visual, mental, and written modalities helps children to develop number sense.\n\nMathnasium was founded in 2002 by Larry Martinek, David Ullendorf, and Peter Markovitz. The first center opened in Westwood, Los Angeles, California, and continues to be one of a small number of centers owned by Mathnasium, LLC.\nIn June 2010, Mathnasium announced the opening of the 200th U.S. location. In February 2012, Mathnasium announced the 300th U.S. location. The 400th location opened in September 2013. There are currently Mathnasium centers in 16 countries. As of March 11, 2017, there are 800 Mathnasium locations within the United States.\n\nThe Mathnasium approach first revolves around an assessment to determine what a student does and does not know. Next, a personalized and prescriptive learning program is made. Each student follows the program with the help of specially trained Mathnasium math instructors who provide instruction and encouragement. For proof of progress, Mathnasium relies on the students' report cards, independent tests, and parent testimony to measure the speed and magnitude of improvement in math skills, numerical thinking, and attitude. Mathnasium separates levels of instruction into early education, elementary school, middle school, and high school. Students advance to a new level of study when they demonstrate mastery of the subject through completing mastery checks, where students have to answer a worksheet of problems correctly without help from instructors. One instructor typically works with four students at a time, and students are encouraged to bring in homework and get help on it as well. To encourage students, work they complete counts towards prizes, and they also can have chances to be entered into drawings for bigger prizes, such as gift cards. Instructors also grade student work and make sure that each worksheet is correct, which reinforces the Mathnasium method of focusing on getting the process correct.\n\nIn 2011, Mathnasium hosted a national TriMathlon, a math competition held at 160 participating Mathnasium locations. Over 4000 students competed in three challenges: The Counting Game, Magic Squares, and the Mental Math Workout. Along with medals and prizes for local 1st, 2nd, and 3rd place winners, each participant received a certificate for achievement for their involvement. National TriMathlon winners were selected from the top three scores in each grade level from all participating Mathnasium locations. The 12 national winners received honors and more than $10,000 in cash prizes.\n\nIn 2012, Entrepreneur ranked Mathnasium #112 in the Franchise 500®; in 2011, Mathnasium was ranked #138; in 2010, #176; in 2009, #173; and in 2008, #197. In 2012, Entrepreneur ranked Mathnasium #44 in their list of the Fastest-Growing franchises; in 2011, Mathnasium was ranked #79; and in 2010, #77. In 2012, Entrepreneur ranked Mathnasium #94 in America's Top Global franchises; in 2011, Mathnasium ranked #114; in 2010, #138; in 2009, #135; in 2008, #98; and in 2017, #170.\n\n• Mathnasium <br>\n• globalww.mathnasiumtriathlon.com/ Mathnasium TriMathlon\n"}
{"id": "358754", "url": "https://en.wikipedia.org/wiki?curid=358754", "title": "Method of complements", "text": "Method of complements\n\nIn mathematics and computing, the method of complements is a technique used to subtract one number from another using only addition of positive numbers. This method was commonly used in mechanical calculators and is still used in modern computers.\n\nThe \"nines' complement\" of a number is formed by replacing each digit with nine minus that digit. To subtract a decimal number \"y\" (the subtrahend) from another number \"x\" (the minuend) two methods may be used:\n\nIn the first method the nines' complement of \"x\" is added to \"y\". Then the nines' complement of the result obtained is formed to produce the desired result.\nIn the second method the nines' complement of \"y\" is added to \"x\" and one is added to the sum. The leading digit '1' of the result is then discarded. Discarding the initial '1' is especially convenient on calculators or computers that use a fixed number of digits: there is nowhere for it to go so it is simply lost during the calculation. The nines' complement plus one is known as the \"ten's complement.\"\n\nThe method of complements can be extended to other number bases (radices); in particular, it is used on most digital computers to perform subtraction, represent negative numbers in base 2 or binary arithmetic and test underflow and overflow in calculation.\n\nThe radix complement of an \"n\" digit number \"y\" in radix \"b\" is, by definition, formula_1. The radix complement is most easily obtained by adding 1 to the diminished radix complement, which is formula_2. Since formula_3 is the digit formula_4 repeated \"n\" times (because formula_5; see also Geometric series Formula), the diminished radix complement of a number is found by complementing each digit with respect to formula_4 (that is, subtracting each digit in \"y\" from formula_4).\n\nThe subtraction of \"y\" from \"x\" may be performed as follows. \nAdding the diminished radix complement of \"x\" to \"y\" results in the value formula_8 or formula_9 which is the diminished radix complement of formula_10, except for possible padding digits formula_4. The diminished radix complement of this is the value formula_10. Alternatively, adding the radix complement of \"y\" to \"x\" results in the value formula_13 or formula_14. Assuming \"y ≤ x\" , the result will always be greater or equal to formula_15 and dropping the initial '1' is the same as subtracting formula_15, making the result formula_17 or just formula_10, the desired result.\n\nIn the decimal numbering system, the radix complement is called the \"ten's complement\" and the diminished radix complement the \"nines' complement\". In binary, the radix complement is called the \"two's complement\" and the diminished radix complement the \"ones' complement\". The naming of complements in other bases is similar. Some people, notably Donald Knuth, recommend using the placement of the apostrophe to distinguish between the radix complement and the diminished radix complement. In this usage, the \"four's complement\" refers to the radix complement of a number in base four while \"fours' complement\" is the diminished radix complement of a number in base 5. However, the distinction is not important when the radix is apparent (nearly always), and the subtle difference in apostrophe placement is not common practice. Most writers use \"one's\" and \"nine's complement\", and many style manuals leave out the apostrophe, recommending \"ones\" and \"nines complement\".\n\nThe nines' complement of a decimal digit is the number that must be added to it to produce 9; the complement of 3 is 6, the complement of 7 is 2, and so on, see table. To form the nines' complement of a larger number, each digit is replaced by its nines' complement.\n\nConsider the following subtraction problem:\n\nWe compute the nines' complement of the minuend, 873. Add that to the subtrahend 218, then calculate the nines' complement of the result.\n\nNow calculate the nines' complement of the result\n\nWe compute the nines' complement of 218, which is 781. Because 218 is three digits long, this is the same as subtracting 218 from 999.\n\nNext, the sum of \"x\" and the nines' complement of \"y\" is taken:\n\nThe leading \"1\" digit is then dropped, giving 654.\n\nThis is not yet correct. We have essentially added 999 to the equation in the first step. Then we removed 1000 when we dropped the leading 1 in the result 1654 above. This will thus make the answer we get (654) one less than the correct answer formula_19. To fix this, we must add 1 to our answer:\n\nAdding a 1 gives 655, the correct answer to our original subtraction problem.\n\nIn the following example the result of the subtraction has fewer digits than \"x\":\n\nUsing the first method the sum of the nines' complement of \"x\" and \"y\" is\n\nThe nines' complement of 999990 is 000009. Removing the leading zeros gives 9 the desired result.\n\nIf the subtrahend, \"y\", has fewer digits than the minuend, \"x\", leading zeros must be added in the second method. These zeros become leading nines when the complement is taken. For example:\n\ncan be rewritten\n\nReplacing 00391 with its nines' complement and adding 1 produces the sum:\n\nDropping the leading\"1\" gives the correct answer: 47641.\n\nThe method of complements is especially useful in binary (radix 2) since the ones' complement is very easily obtained by inverting each bit (changing '0' to '1' and vice versa). Adding 1 to get the two's complement can be done by simulating a carry into the least significant bit. For example:\n\nbecomes the sum:\n\nDropping the initial \"1\" gives the answer: 01001110 (equals decimal 78)\n\nThe method of complements normally assumes that the operands are positive and that \"y\" ≤ \"x\", logical constraints given that adding and subtracting arbitrary integers is normally done by comparing signs, adding the two or subtracting the smaller from the larger, and giving the result the correct sign.\n\nLet's see what happens if \"x\" < \"y\". In that case, there will not be a \"1\" digit to cross out after the addition since formula_14 will be less than formula_15. For example, (in decimal):\n\nComplementing \"y\" and adding gives:\n\nThis is obviously the wrong answer; the expected answer is -144. But it isn't as far off as it seems; 856 happens to be the ten's complement of 144. This issue can be addressed in three ways:\n\nThe method of complements was used in many mechanical calculators as an alternative to running the gears backwards. For example:\n\n\nUse of the method of complements is ubiquitous in digital computers, regardless of the representation used for signed numbers. However, the circuitry required depends on the representation:\n\n\nThe method of complements was used to correct errors when accounting books were written by hand. To remove an entry from a column of numbers, the accountant could add a new entry with the ten's complement of the number to subtract. A bar was added over the digits of this entry to denote its special status. It was then possible to add the whole column of figures to obtain the corrected result.\n\nComplementing the sum is handy for cashiers making change for a purchase from currency in a single denomination of 1 raised to an integer power of the currency's base. For decimal currencies that would be 10, 100, 1,000, etc., e.g. a $10.00 bill.\n\nIn grade schools, students are sometimes taught the method of complements as a shortcut useful in mental arithmetic. Subtraction is done by adding the ten's complement of the subtrahend, which is the nines' complement plus 1. The result of this addition used when it is clear that the difference will be positive, otherwise the ten's complement of the addition's result is used with it marked as negative. The same technique works for subtracting on an adding machine.\n\n"}
{"id": "23024252", "url": "https://en.wikipedia.org/wiki?curid=23024252", "title": "Morse–Smale system", "text": "Morse–Smale system\n\nIn dynamical systems theory, an area of pure mathematics, a Morse–Smale system is a smooth dynamical system whose non-wandering set consists of finitely many hyperbolic equilibrium points and hyperbolic periodic orbits and satisfying a transversality condition on the stable and unstable manifolds. Morse–Smale systems are structurally stable and form one of the simplest and best studied classes of smooth dynamical systems. They are named after Marston Morse, the creator of the Morse theory, and Stephen Smale, who emphasized their importance for smooth dynamics and algebraic topology.\n\nFor Morse–Smale systems on 2D-sphere all equilibrium points and periodical orbits are hyperbolic; there are no separatrice loops.\n\nGradient-like dynamical systems are particular case of Morse–Smale systems.\n\nTheorem (Peixoto). The vector field on 2D manifold is structurally stable if and only if this field is Morse-Smale.\n\nAny Morse function \"f\" on a compact Riemannian manifold \"M\" defines a gradient vector field. If one imposes the condition that the unstable and stable manifolds of the critical points intersect transversely, then the gradient vector field and the corresponding smooth flow form a Morse–Smale system. The finite set of critical points of \"f\" forms the non-wandering set, which consists entirely of fixed points.\n\n"}
{"id": "48805512", "url": "https://en.wikipedia.org/wiki?curid=48805512", "title": "Oblivious ram", "text": "Oblivious ram\n\nAn Oblivious RAM (ORAM) simulator is a compiler that transforms algorithms in such a way that the resulting algorithms preserve the input-output behavior of the original algorithm but the distribution of memory access pattern of the transformed algorithm is independent of the memory access pattern of the original algorithm. The definition of ORAMs is motivated by the fact that an adversary can obtain nontrivial information about the execution of a program and the nature of the data that it is dealing with, just by observing the pattern in which various locations of memory are accessed during its execution. An adversary can get this information even if the data values are all encrypted. The definition suits equally well to the settings of protected programs running on unprotected shared memory as well as a client running a program on its system by accessing previously stored data on a remote server. The concept was formulated by Oded Goldreich in 1987.\n\nA Turing machine (TM), the mathematical abstraction of a real computer (program), is said to be oblivious if for any two inputs of the same length, the motions of the tape heads remain the same. Pippenger and Fischer proved that every TM with running time formula_1 can be made oblivious and that the running time of the oblivious TM is formula_2. A more realistic model of computation is the RAM model. In the RAM model of computation, there is a CPU that can execute the basic mathematical, logical and control instructions. The CPU is also associated with a few registers and a physical random access memory, where it stores the operands of its instructions. The CPU in addition has instructions to read the contents of a memory cell and write a specific value to a memory cell. The definition of ORAMs capture a similar notion of obliviousness memory accesses in this model.\n\nInformally, an ORAM is an algorithm at the interface of a protected CPU and the physical RAM such that it acts like a RAM to the CPU by querying the physical RAM for the CPU while hiding information about the actual memory access pattern of the CPU from the physical RAM. In other words, the distribution of memory accesses of two programs that make the same number of memory accesses to the RAM are indistinguishable from each other. This description will still make sense if the CPU is replaced by a client with a small storage and the physical RAM is replaced with a remote server with a large storage capacity, where the data of the client resides.\n\nThe following is a formal definition of ORAMs. Let formula_3 denote a program requiring a memory of size formula_4 when executing on an input formula_5. Suppose that formula_3 has instructions for basic mathematical and control operations in addition to two special instructions formula_7 and formula_8, where formula_7 reads the value at location formula_10 and formula_8 writes the value formula_12 to formula_10. The sequence of memory cell accessed by a program formula_3 during its execution is called its memory access pattern and is denoted by formula_15.\n\nA polynomial-time algorithm, formula_16 is an Oblivious RAM (ORAM) compiler with computational overhead formula_17 and memory overhead formula_18, if formula_16 given formula_20 and a deterministic RAM program formula_3 with memory-size formula_4 outputs a program formula_23 with memory-size formula_24 such that for any input formula_5, the running-time of formula_26 is bounded by formula_27 where formula_28 is the running-time of formula_29, and there exists a negligible function formula_30 such that the following properties hold:\n\nNote that the above definition uses the notion of statistical security. One can also have a similar definition for the notion of computational security.\n\nORAMs were introduced by Goldreich and Ostrovsky where in the key motivation was stated as software protection from an adversary who can observe the memory access pattern (but not the contents of the memory).\n\nThe main result in this work is that there exists an ORAM compiler that uses formula_44 server space and incurs a running time overhead of formula_45 when making a program that uses formula_4 memory cells oblivious. This work initiated a series of works in the construction of oblivious RAMs that is going on till date. There are several attributes that need to be considered when we compare various ORAM constructions. The most important parameters of an ORAM construction are the amounts of client storage, the amount of server storage and the time overhead in making one memory access. Based on these attributes, the construction of Kushilevitz et al. is the best known ORAM construction. It achieves formula_47 client storage, formula_44 server storage and formula_49 access overhead.\n\nAnother important attribute of an ORAM construction is whether the access overhead is amortized or worst-case. Several of the earlier ORAM constructions have good amortized access overhead guarantees, but have formula_50 worst-case access overheads. Some of the ORAM constructions with polylogarithmic worst-case computational overheads are. The constructions of were for the random oracle model, where the client assumes access to an oracle that behaves like a random function and returns consistent answers for repeated queries. They also noted that this oracle could be replaced by a pseudorandom function whose seed is a secret key stored by the client, if one assumes the existence of one-way functions. The papers were aimed at removing this assumption completely. The authors of also achieve an access overhead of formula_51, which is just a log-factor away from the best known ORAM access overhead.\n\nWhile most of the earlier works focus on proving security computationally, there are more recent works that use the stronger statistical notion of security.\n\nOne of the only known lower bounds on the access overhead of ORAMs is due to Goldreich et al. They show a formula_52 lower bound for ORAM access overhead, where formula_4 is the data size. There is also a conditional lower bound on the access overhead of ORAMs due to Boyle et al. that relates this quantity with that of the size of sorting networks.\n\nA trivial ORAM simulator construction, for each read or write operation, reads from and writes to every single element in the array, only performing a meaningful action for the address specified in that single operation. The trivial solution thus, scans through the entire memory for each operation. This scheme incurs a time overhead of formula_54 for each memory operation, where is the size of the memory.\n\nA simple version of a statistically secure ORAM compiler constructed by Chung and Pass is described in the following along with an overview of the proof of its correctness. The compiler on input and a program with its memory requirement , outputs an equivalent oblivious program .\n\nIf the input program uses registers, the output program will need formula_55 registers, where formula_56 is a parameter of the construction. uses formula_57 memory and its (worst-case) access overhead is formula_58.\n\nThe ORAM compiler is very simple to describe. Suppose that the original program has instructions for basic mathematical and control operations in addition to two special instructions formula_7 and formula_8, where formula_7 reads the value at location and formula_8 writes the value to . The ORAM compiler, when constructing , simply replaces each and instructions with subroutines and and keeps the rest of the program the same. It may be noted that this construction can be made to work even for memory requests coming in an online fashion.\n\nThe program stores a complete binary tree of depth formula_63 in its memory. Each node in is represented by a binary string of length at most . The root is the empty string, denoted by . The left and right children of a node represented by the string formula_64 are formula_65 and formula_66 respectively. The program thinks of the memory of as being partitioned into blocks, where each block is a contiguous sequence of memory cells of size . Thus, there are at most formula_67 blocks in total. In other words, the memory cell corresponds to block formula_68.\n\nAt any point of time, there is an association between the blocks and the leaves in .\nTo keep track of this association, also stores a data structure called position map, denoted by formula_69, using formula_70 registers. This data structure, for each block , stores the leaf of associated with in formula_71.\n\nEach node in contains an array with at most triples. Each triple is of the form formula_72, where is a block identifier and is the contents of the block. Here, is a security parameter and is formula_73.\n\nThe program starts by initializing its memory as well as registers to . Describing the procedures and is enough to complete the description of . The sub-routine is given below. The inputs to the sub-routine are a memory location formula_74 and the value to be stored at the location . It has three main phases, namely FETCH, PUT_BACK and FLUSH.\n\nThe task of the FETCH phase is to look for the location in the tree . Suppose formula_80 is the leaf associated with the block containing location . For each node in on the path from root to formula_80, this procedure goes over all triples in and looks for the triple corresponding to the block containing . If it finds that triple in , it removes the triple from and writes back the updated state of . Otherwise, it simply writes back the whole node .\n\nIn the next phase, it updates the block containing with the new value , associates that block with a freshly sampled uniformly random leaf of the tree, writes back the updated triple to the root of .\n\nThe last phase, which is called FLUSH, is an additional operation to release the memory cells in the root and other higher internal nodes. Specifically, the algorithm chooses a uniformly random leaf formula_93 and then tries to push down every node as much as possible along the path from root to formula_93. It aborts outputting an overflow if at any point some bucket is about to overflow its capacity.\n\nThe sub-routine is similar to . For the sub-routine, the input is just a memory location formula_74 and it is almost the same as . In the FETCH stage, if it does not find a triple corresponding to the location , it returns as the value at location . In the PUT_BACK phase, it will write back the same block that it read to the root, after associating it with a freshly sampled uniformly random leaf.\n\nLet stand for the ORAM compiler that was described above. Given a program , let denote formula_103. Let formula_104 denote the execution of the program on an input using memory cells. Also, let formula_15 denote the memory access pattern of formula_104. Let denote a function such that for any formula_31, for any program and for any input formula_108, the probability that formula_109 outputs an overflow is at most formula_110. The following lemma is easy to see from the description of .\n\n\nIt is easy to see that each and operation traverses root to leaf paths in chosen uniformly and independently at random. \nThis fact implies that the distribution of memory access patterns of any two programs that make the same number of memory accesses are indistinguishable, if they both do not overflow.\n\n\nThe following lemma completes the proof of correctness of the ORAM scheme.\n\n\nDuring each and operation, two random root-to-leaf paths of are fully explored by . This takes formula_123 time. This is the same as the computational overhead, and is formula_58 since is formula_58.\n\nThe total memory used up by is equal to the size of . Each triple stored in the tree has formula_126 words in it and thus there are formula_127 words per node of the tree. Since the total number of nodes in the tree is formula_70, the total memory size is formula_129 words, which is formula_130. Hence, the memory overhead of the construction is formula_58.\n\n\n"}
{"id": "5074937", "url": "https://en.wikipedia.org/wiki?curid=5074937", "title": "Orthocompact space", "text": "Orthocompact space\n\nIn mathematics, in the field of general topology, a topological space is said to be orthocompact if every open cover has an interior preserving open refinement. That is, given an open cover of the topological space, there is a refinement which is also an open cover, with the further property that at any point, the intersection of all open sets in the refinement containing that point, is also open.\n\nIf the number of open sets containing the point is finite, then their intersection is clearly open. That is, every point finite open cover is interior preserving. Hence, we have the following: every metacompact space, and in particular, every paracompact space, is orthocompact.\n\nUseful theorems:\n\n"}
{"id": "48583545", "url": "https://en.wikipedia.org/wiki?curid=48583545", "title": "Patricia Hersh", "text": "Patricia Hersh\n\nPatricia Lynn Hersh (born 1973) is an American mathematician who works as a professor of mathematics at North Carolina State University. Her research concerns algebraic combinatorics, topological combinatorics, and the connections between combinatorics and other fields of mathematics.\nHersh graduated \"magna cum laude\" with an A.B. in mathematics and computer science from Harvard University in 1995, with a senior thesis supervised by Persi Diaconis. \nShe completed her Ph.D. in 1999 at the Massachusetts Institute of Technology, under the supervision of Richard P. Stanley. \nAfter postdoctoral positions at the University of Washington, University of Michigan, and Mathematical Sciences Research Institute in Berkeley, California, she joined the faculty at Indiana University Bloomington in 2004, and moved to North Carolina State University in 2008.\n\nIn 2010, Hersh won the Ruth I. Michler Memorial Prize of the Association for Women in Mathematics, funding a visiting position for her at Cornell University. \nIn 2015 she was elected as a fellow of the American Mathematical Society \"for contributions to algebraic and topological combinatorics, and for service to the mathematical community\".\n\n"}
{"id": "601284", "url": "https://en.wikipedia.org/wiki?curid=601284", "title": "Paul Erdős", "text": "Paul Erdős\n\nPaul Erdős ( ; 26 March 1913 – 20 September 1996) was a Hungarian mathematician. He was one of the most prolific mathematicians of the 20th century. He was known both for his social practice of mathematics (he engaged more than 500 collaborators) and for his eccentric lifestyle (\"Time\" magazine called him \"The Oddball's Oddball\"). He devoted his waking hours to mathematics, even into his later years—indeed, his death came only hours after he solved a geometry problem at a conference in Warsaw.\n\nErdős published around 1,500 mathematical papers during his lifetime, a figure that remains unsurpassed. He firmly believed mathematics to be a social activity, living an itinerant lifestyle with the sole purpose of writing mathematical papers with other mathematicians. Erdős's prolific output with co-authors prompted the creation of the Erdős number, the number of steps in the shortest path between a mathematician and Erdős in terms of co-authorships.\n\nPaul Erdős was born in Budapest, Austria-Hungary, on 26 March 1913. He was the only surviving child of Anna (née Wilhelm) and Lajos Erdős (born as Louis Engländer). His two sisters, aged 3 and 5, both died of scarlet fever a few days before he was born. His parents were both Jewish mathematics teachers. His fascination with mathematics developed early—he was often left home by himself because his father was held captive in Siberia as an Austro-Hungarian POW during 1914–1920, causing his mother to have to work long hours to support their household. He taught himself to read through mathematics texts that his parents left around their home. By the age of four, given a person's age, he could calculate in his head how many seconds they had lived. Due to his sisters' deaths, he had a close relationship with his mother, with the two of them allegedly sharing the same bed until he left for college.\n\nErdős entered the University of Budapest at the age of 17. By the time he was 20, he had found a proof for Chebyshev's Theorem. Erdős later published several articles in the monthly about problems in elementary plane geometry. In 1934, at the age of 21, he was awarded a doctorate in mathematics. Erdős's thesis advisor was Lipót Fejér, who was also the thesis advisor for John von Neumann, George Pólya, and Paul (Pál) Turán. Because he was a Jew, Erdős decided Hungary was dangerous and relocated to the United States. Many members of Erdős' family, including two of his aunts, two of his uncles, and his father, died in Budapest during the Holocaust. His mother survived in hiding. He was living in America and working at the Princeton Institute for Advanced Study at the time.\n\nDescribed by his biographer, Paul Hoffman, as \"probably the most eccentric mathematician in the world,\" Erdős spent most of his adult life living out of a suitcase. Except for some years in the 1950s, when he was not allowed to enter the United States based on the pretense that he was a Communist sympathizer, his life was a continuous series of going from one meeting or seminar to another. During his visits, Erdős expected his hosts to lodge him, feed him, and do his laundry, along with anything else he needed, as well as arrange for him to get to his next destination.\n\nOn 20 September 1996, at the age of 83, he had a heart attack and died while attending a conference in Warsaw. These circumstances were close to the way he wanted to die. He once said,\n\nErdős never married and had no children. He is buried next to his mother and father in grave 17A-6-29 at Kozma Utcai Temető in Budapest. For his epitaph, he suggested \"I've finally stopped getting dumber.\" (Hungarian: \"Végre nem butulok tovább\"). His life was documented in the film \"\", made while he was still alive, and posthumously in the book \"The Man Who Loved Only Numbers\" (1998).\n\nErdős' name contains the Hungarian letter \"ő\" (\"o\" with double acute accent), but is often incorrectly written as \"Erdos\" or \"Erdös\" either \"by mistake or out of typographical necessity\".\n\nPossessions meant little to Erdős; most of his belongings would fit in a suitcase, as dictated by his itinerant lifestyle. Awards and other earnings were generally donated to people in need and various worthy causes. He spent most of his life traveling between scientific conferences, universities and the homes of colleagues all over the world. He earned enough in stipends from universities as a guest lecturer, and from various mathematical awards, to fund his travels and basic needs; money left over he used to fund cash prizes for proofs of \"Erdős problems\" (see below). He would typically show up at a colleague's doorstep and announce \"my brain is open\", staying long enough to collaborate on a few papers before moving on a few days later. In many cases, he would ask the current collaborator about whom to visit next.\n\nHis colleague Alfréd Rényi said, \"a mathematician is a machine for turning coffee into theorems\", and Erdős drank copious quantities (this quotation is often attributed incorrectly to Erdős, but Erdős himself ascribed it to Rényi). After 1971 he also took amphetamines, despite the concern of his friends, one of whom (Ron Graham) bet him $500 that he could not stop taking the drug for a month. Erdős won the bet, but complained that during his abstinence, mathematics had been set back by a month: \"Before, when I looked at a piece of blank paper my mind was filled with ideas. Now all I see is a blank piece of paper.\" After he won the bet, he promptly resumed his amphetamine use.\n\nHe had his own idiosyncratic vocabulary: although an agnostic atheist, he spoke of \"The Book\", a visualization of a book in which God had written down the best and most elegant proofs for mathematical theorems. Lecturing in 1985 he said, \"You don't have to believe in God, but you should believe in \"The Book\".\" He himself doubted the existence of God, whom he called the \"Supreme Fascist\" (SF). He accused SF of hiding his socks and Hungarian passports, and of keeping the most elegant mathematical proofs to Himself. When he saw a particularly beautiful mathematical proof he would exclaim, \"This one's from \"The Book\"!\" This later inspired a book titled \"Proofs from the Book\".\n\nOther idiosyncratic elements of Erdős's vocabulary include:\n\nHe gave nicknames to many countries, examples being: the U.S. was \"samland\" (after Uncle Sam), the Soviet Union was \"joedom\" (after Joseph Stalin), and Israel was \"\".\n\nIn 1934, he moved to Manchester, England, to be a guest lecturer. In 1938, he accepted his first American position as a scholarship holder at Princeton University. At this time, he began to develop the habit of traveling from campus to campus. He would not stay long in one place and traveled back and forth among mathematical institutions until his death.\n\nIn 1954, the United States Citizenship and Immigration Services denied Erdős, a Hungarian citizen, a re-entry visa into the United States, for reasons that have never been fully explained. Teaching at the University of Notre Dame at the time, Erdős could have chosen to remain in the country. Instead, he packed up and left, albeit requesting reconsideration from the U.S. Immigration Services at periodic intervals. \n\nHungary at the time was under the Warsaw Pact with the Soviet Union. Although Hungary limited the freedom of its own citizens to enter and exit the country, it gave Erdős the exclusive privilege of being allowed to enter and exit the country as he pleased in 1956.\n\nThe U.S. Immigration Services later granted a visa in 1963 to Erdős and he resumed including American universities in his teaching and travels. Ten years later, in 1973, the 60-year-old Erdős voluntarily left Hungary.\n\nDuring the last decades of his life, Erdős received at least fifteen honorary doctorates. He became a member of the scientific academies of eight countries, including the U.S. National Academy of Sciences and the UK Royal Society. Shortly before his death, he renounced his honorary degree from the University of Waterloo over what he considered to be unfair treatment of colleague Adrian Bondy.\n\nErdős was one of the most prolific publishers of papers in mathematical history, comparable only with Leonhard Euler; Erdős published more papers, mostly in collaboration with other mathematicians, while Euler published more pages, mostly by himself. Erdős wrote around 1,525 mathematical articles in his lifetime, mostly with co-authors. He strongly believed in and practiced mathematics as a social activity, having 511 different collaborators in his lifetime.\n\nIn his mathematical style, Erdős was much more of a \"problem solver\" than a \"theory developer\" (see \"The Two Cultures of Mathematics\" by Timothy Gowers for an in-depth discussion of the two styles, and why problem solvers are perhaps less appreciated). Joel Spencer states that \"his place in the 20th-century mathematical pantheon is a matter of some controversy because he resolutely concentrated on particular theorems and conjectures throughout his illustrious career.\" Erdős never won the highest mathematical prize, the Fields Medal, nor did he coauthor a paper with anyone who did, a pattern that extends to other prizes. He did win the Wolf Prize, where his contribution is described as \"for his numerous contributions to number theory, combinatorics, probability, set theory and mathematical analysis, and for personally stimulating mathematicians the world over\". In contrast, the works of the three winners after were recognized as \"outstanding\", \"classic\", and \"profound\", and the three before as \"fundamental\" or \"seminal\".\n\nOf his contributions, the development of Ramsey theory and the application of the probabilistic method especially stand out. Extremal combinatorics owes to him a whole approach, derived in part from the tradition of analytic number theory. Erdős found a proof for Bertrand's postulate which proved to be far neater than Chebyshev's original one. He also discovered the first elementary proof for the prime number theorem, along with Atle Selberg. However, the circumstances leading up to the proofs, as well as publication disagreements, led to a bitter dispute between Erdős and Selberg. Erdős also contributed to fields in which he had little real interest, such as topology, where he is credited as the first person to give an example of a totally disconnected topological space that is not zero-dimensional.\n\nThroughout his career, Erdős would offer payments for solutions to unresolved problems. These ranged from $25 for problems that he felt were just out of the reach of the current mathematical thinking (both his and others), to several thousand dollars for problems that were both difficult to attack and mathematically significant. There are thought to be at least a thousand remaining unsolved problems, though there is no official or comprehensive list. The offers remain active despite Erdős's death; Ronald Graham is the (informal) administrator of solutions. A solver can get either an original check signed by Erdős before his death (for memento only, cannot be cashed) or a cashable check from Graham.\n\nPerhaps the most mathematically notable of these problems is the Erdős conjecture on arithmetic progressions:\n\nIf true, it would solve several other open problems in number theory (although one main implication of the conjecture, that the prime numbers contain arbitrarily long arithmetic progressions, has since been proved independently as the Green–Tao theorem). The payment for the solution of the problem is currently worth US $5,000.\n\nThe most familiar problem with an Erdős prize is likely the Collatz conjecture, also called the 3\"N\" + 1 problem. Erdős offered $500 for a solution.\n\nHis most frequent collaborators include Hungarian mathematicians András Sárközy (62 papers) and András Hajnal (56 papers), and American mathematician Ralph Faudree (50 papers). Other frequent collaborators were\n\nFor other co-authors of Erdős, see the list of people with Erdős number 1 in List of people by Erdős number.\n\nBecause of his prolific output, friends created the Erdős number as a tribute. An Erdős number describes a person's degree of separation from Erdős himself, based on their collaboration with him, or with another who has their own Erdős number. Erdős alone was assigned the Erdős number of 0 (for being himself), while his immediate collaborators could claim an Erdős number of 1, their collaborators have Erdős number at most 2, and so on. Approximately 200,000 mathematicians have an assigned Erdős number,\nand some have estimated that 90 percent of the world's active mathematicians have an Erdős number smaller than 8 (not surprising in light of the small world phenomenon). Due to collaborations with mathematicians, many scientists in fields such as physics, engineering, biology, and economics have Erdős numbers as well.\n\nSeveral studies have shown that leading mathematicians tend to have particularly low Erdős numbers. For example, the roughly 268,000 mathematicians with a known Erdős number have a median value of 5. In contrast, the median Erdős number of Fields Medalists is 3. As of 2015, approximately 11,000 mathematicians have an Erdős number of 2 or less. Collaboration distances will necessarily increase over long time scales, as mathematicians with low Erdős numbers die and become unavailable for collaboration. The American Mathematical Society provides a free online tool to determine the Erdős number of every mathematical author listed in the Mathematical Reviews catalogue.\n\nThe Erdős number was most likely first defined by Casper Goffman, an analyst whose own Erdős number is 2. Goffman published his observations about Erdős's prolific collaboration in a 1969 article titled \"And what is your Erdős number?\"\n\nJerry Grossman has written that it could be argued that Baseball Hall of Famer Hank Aaron can be considered to have an Erdős number of 1 because they both autographed the same baseball (for Carl Pomerance) when Emory University awarded them honorary degrees on the same day. Erdős numbers have also been proposed for an infant, a horse, and several actors.\n\nErdős signed his name \"Paul Erdos P.G.O.M.\" When he became 60, he added \"L.D.\", at 65 \"A.D.\", at 70 \"L.D.\" (again), and at 75 \"C.D.\"\n\nErdős is the subject of at least three books: two biographies (Hoffman's \"The Man Who Loved Only Numbers\" and Schechter's \"My Brain is Open\", both published in 1998) and a 2013 children's picture book by Deborah Heiligman (\"The Boy Who Loved Math: The Improbable Life of Paul Erdős\").\n\n\n\n"}
{"id": "1757077", "url": "https://en.wikipedia.org/wiki?curid=1757077", "title": "Pelorus (instrument)", "text": "Pelorus (instrument)\n\nIn marine navigation, a pelorus is a reference tool for maintaining bearing of a vessel at sea. It is a \"dumb compass\" without a directive element, suitably mounted and provided with vanes to permit observation of relative bearings.\nIn appearance and use, a pelorus resembles a compass or compass repeater, with sighting vanes or a sighting telescope attached, but it has no directive properties. That is, it remains at any \"relative\" direction to which it is set. It is generally used by setting 000° at the lubber's line. Relative bearings are then observed. They can be converted to bearings true, magnetic, grid, etc., by \"adding\" the appropriate heading. The direct use of relative bearings is sometimes of value. A pelorus is useful, for instance, in determining the moment at which an aid to navigation is broad on the beam. It is also useful in measuring pairs of relative bearings which can be used to determine distance off and distance abeam of a navigational aid.\n\nIf the true heading is set at the lubber's line, true bearings are observed directly. Similarly, compass bearings can be observed if the compass heading is set at the lubber's line, etc. However, the vessel must be on the heading to which the pelorus is set if accurate results are to be obtained, or else a correction must be applied to the observed results. Perhaps the easiest way of avoiding error is to have the steersman indicate when the vessel is on course. This is usually done by calling out \"mark, mark, mark\" as long as the vessel is within a specified fraction of a degree of the desired heading. The observer, who is watching a distant object across the pelorus, selects an instant when the vessel is steady and is on course. An alternative method is to have the observer call out \"mark\" when the relative bearing is steady, and the steersman note the heading. If the compass is swinging at the moment of observation, the observation should be rejected. The number of degrees between the desired and actual headings is \"added\" if the vessel is to the \"right\" of the course, and \"subtracted\" if to the \"left\". Thus, if the course is 060° and the heading is 062° at the moment of observation, a correction of 2° is added to the bearing.\n\nThe instrument was named for one Pelorus, said to have been the pilot for Hannibal, circa 203 BC.\n\nHarold Gatty described the use of a pelorus by Polynesians before the use of a compass. In equatorial waters the nightly course of stars overhead is nearly uniform during the year. This regularity simplified navigation for the Polynesians using a pelorus, or dummy compass:\n\n<poem>[Arabs] divided the horizon into 32 points. These points were derived from fifteen stars which rose at approximately equally spaced points of the eastern horizon. The setting points of these stars on the western horizon gave them another fifteen points and north and south brought the total to thirty two.\n\nThese accomplished navigators [Polynesians] had names for one hundred and fifty stars. They knew the point on the horizon where each of these rose, and the time at which it did so. They knew the islands which each passed over.\n\nMany of the peoples in the Pacific used the thirty two point dummy compass…I consider this…proof of the Indo-Malayan origin of the Polynesians.</poem>\n\nReading from North to South, in their rising and setting positions, these stars are:\nThe true position of these stars is only approximate to their theoretical equidistant rhumbs on the sidereal compass. Over time, the elaboration of the pelorus points led to the modern compass rose.\n\n\n\n"}
{"id": "20871106", "url": "https://en.wikipedia.org/wiki?curid=20871106", "title": "Pseudorandom generators for polynomials", "text": "Pseudorandom generators for polynomials\n\nIn theoretical computer science, a pseudorandom generator for low-degree polynomials is an efficient procedure that maps a short truly random seed to a longer pseudorandom string in such a way that low-degree polynomials cannot distinguish the output distribution of the generator from the truly random distribution. That is, evaluating any low-degree polynomial at a point determined by the pseudorandom string is statistically close to evaluating the same polynomial at a point that is chosen uniformly at random.\n\nPseudorandom generators for low-degree polynomials are a particular instance of pseudorandom generators for statistical tests, where the statistical tests considered are evaluations of low-degree polynomials.\n\nA pseudorandom generator formula_1 for polynomials of degree formula_2 over a finite field formula_3 is an efficient procedure that maps a sequence of formula_4 field elements to a sequence of formula_5 field elements such that any formula_5-variate polynomial over formula_3 of degree formula_2 is fooled by the output distribution of formula_9.\nIn other words, for every such polynomial formula_10, the statistical distance between the distributions formula_11 and formula_12 is at most a small formula_13, where formula_14 is the uniform distribution over formula_15.\n\nThe case formula_16 corresponds to pseudorandom generators for linear functions and is solved by small-bias generators.\nFor example, the construction of achieves a seed length of formula_17, which is optimal up to constant factors.\n\nThe analysis of gives a seed length of formula_22.\n\n"}
{"id": "1895800", "url": "https://en.wikipedia.org/wiki?curid=1895800", "title": "Riemann series theorem", "text": "Riemann series theorem\n\nIn mathematics, the Riemann series theorem (also called the Riemann rearrangement theorem), named after 19th-century German mathematician Bernhard Riemann, says that if an infinite series of real numbers is conditionally convergent, then its terms can be arranged in a permutation so that the new series converges to an arbitrary real number, or diverges.\n\nAs an example, the series 1 – 1 + 1/2 – 1/2 + 1/3 – 1/3 + ... converges to 0 (for a sufficiently large number of terms, the partial sum gets arbitrarily near to 0); but replacing all terms with their absolute values gives 1 + 1 + 1/2 + 1/2 + 1/3 + 1/3 + ... , which sums to infinity. Thus the original series is conditionally convergent, and can be rearranged (by taking the first two positive terms followed by the first negative term, followed by the next two positive terms and then the next negative term, etc.) to give a series that converges to a different sum: 1 + 1/2 – 1 + 1/3 + 1/4 – 1/2 + ... = ln 2. More generally, using this procedure with \"p\" positives followed by \"q\" negatives gives the sum ln(\"p\"/\"q\"). Other rearrangements give other finite sums or do not converge to any sum.\n\nA series formula_1 converges if there exists a value formula_2 such that the sequence of the partial sums\n\nconverges to formula_2. That is, for any \"ε\" > 0, there exists an integer \"N\" such that if \"n\" ≥ \"N\", then\n\nA series converges conditionally if the series formula_1 converges but the series formula_7 diverges.\n\nA permutation is simply a bijection from the set of positive integers to itself. This means that if formula_8 is a permutation, then for any positive integer formula_9 there exists exactly one positive integer formula_10 such that formula_11 In particular, if formula_12, then formula_13.\n\nSuppose that formula_14 is a sequence of real numbers, and that formula_15 is conditionally convergent. Let formula_16 be a real number. Then there exists a permutation formula_8 such that\n\nThere also exists a permutation formula_8 such that\n\nThe sum can also be rearranged to diverge to formula_21 or to fail to approach any limit, finite or infinite.\n\nThe alternating harmonic series is a classic example of a conditionally convergent series:\n\nis convergent, while\n\nis the ordinary harmonic series, which diverges. Although in standard presentation the alternating harmonic series converges to ln(2), its terms can be arranged to converge to any number, or even to diverge. One instance of this is as follows. Begin with the series written in the usual order,\n\nand rearrange the terms:\n\nwhere the pattern is: the first two terms are 1 and −1/2, whose sum is 1/2. The next term is −1/4. The next two terms are 1/3 and −1/6, whose sum is 1/6. The next term is −1/8. The next two terms are 1/5 and −1/10, whose sum is 1/10. In general, the sum is composed of blocks of three:\n\nThis is indeed a rearrangement of the alternating harmonic series: every odd integer occurs once positively, and the even integers occur once each, negatively (half of them as multiples of 4, the other half as twice odd integers). Since\n\nthis series can in fact be written:\n\nwhich is half the usual sum.\n\nAn efficient way to recover and generalize the result of the previous section is to use the fact that\n\nwhere \"γ\" is the Euler–Mascheroni constant, and where the notation o(1) denotes a quantity that depends upon the current variable (here, the variable is \"n\") in such a way that this quantity goes to 0 when the variable tends to infinity.\n\nIt follows that the sum of \"q\" even terms satisfies\n\nand by taking the difference, one sees that the sum of \"p\" odd terms satisfies\n\nSuppose that two positive integers \"a\" and \"b\" are given, and that a rearrangement of the alternating harmonic series is formed by taking, in order, \"a\" positive terms from the alternating harmonic series, followed by \"b\" negative terms, and repeating this pattern at infinity (the alternating series itself corresponds to , the example in the preceding section corresponds to \"a\" = 1, \"b\" = 2):\n\nThen the partial sum of order (\"a\"+\"b\")\"n\" of this rearranged series contains positive odd terms and negative even terms, hence\n\nIt follows that the sum of this rearranged series is\n\nSuppose now that, more generally, a rearranged series of the alternating harmonic series is organized in such a way that the ratio between the number of positive and negative terms in the partial sum of order \"n\" tends to a positive limit \"r\". Then, the sum of such a rearrangement will be\n\nand this explains that any real number \"x\" can be obtained as sum of a rearranged series of the alternating harmonic series: it suffices to form a rearrangement for which the limit \"r\" is equal .\n\nFor simplicity, this proof assumes first that \"a\" ≠ 0 for every \"n\". The general case requires a simple modification, given below. Recall that a conditionally convergent series of real terms has both infinitely many negative terms and infinitely many positive terms. First, define two quantities, formula_37 and formula_38 by:\n\nThat is, the series formula_40 includes all \"a\" positive, with all negative terms replaced by zeroes, and the series formula_41 includes all \"a\" negative, with all positive terms replaced by zeroes. Since formula_1 is conditionally convergent, both the positive and the negative series diverge. Let \"M\" be a positive real number. Take, in order, just enough positive terms formula_43 so that their sum exceeds \"M\". Suppose we require \"p\" terms – then the following statement is true:\n\nThis is possible for any \"M\" > 0 because the partial sums of formula_43 tend to formula_46. Discarding the zero terms one may write\n\nNow we add just enough negative terms formula_48, say \"q\" of them, so that the resulting sum is less than \"M\". This is always possible because the partial sums of formula_48 tend to formula_21. Now we have:\n\nAgain, one may write\n\nwith\n\nNote that \"σ\" is injective, and that 1 belongs to the range of \"σ\", either as image of 1 (if \"a\" > 0), or as image of (if \"a\" < 0). Now repeat the process of adding just enough positive terms to exceed \"M\", starting with , and then adding just enough negative terms to be less than \"M\", starting with . Extend \"σ\" in an injective manner, in order to cover all terms selected so far, and observe that must have been selected now or before, thus 2 belongs to the range of this extension. The process will have infinitely many such \"changes of direction\". One eventually obtains a rearrangement  . After the first change of direction, each partial sum of  differs from \"M\" by at most the absolute value formula_54 or formula_55 of the term that appeared at the latest change of direction. But converges, so as \"n\" tends to infinity, each of \"a\", formula_54 and formula_57 go to 0. Thus, the partial sums of  tend to \"M\", so the following is true:\n\nThe same method can be used to show convergence to \"M\" negative or zero.\n\nOne can now give a formal inductive definition of the rearrangement \"σ\", that works in general. For every integer \"k\" ≥ 0, a finite set \"A\" of integers and a real number \"S\" are defined. For every \"k\" > 0, the induction defines the value \"σ\"(\"k\"), the set \"A\" consists of the values \"σ\"(\"j\") for \"j\" ≤ \"k\" and \"S\" is the partial sum of the rearranged series. The definition is as follows:\n\n\nIt can be proved, using the reasonings above, that \"σ\" is a permutation of the integers and that the permuted series converges to the given real number \"M\".\n\nLet formula_60 be a conditionally convergent series. The following is a proof that there exists a rearrangement of this series that tends to formula_61 (a similar argument can be used to show that formula_21 can also be attained).\n\nLet formula_63 be the sequence of indexes such that each formula_64 is positive, and define formula_65 to be the indexes such that each formula_66 is negative (again assuming that formula_67 is never 0). Each natural number will appear in exactly one of the sequences formula_68 and formula_69\n\nLet formula_70 be the smallest natural number such that\n\nSuch a value must exist since formula_72 the subsequence of positive terms of formula_73 diverges. Similarly, let formula_74 be the smallest natural number such that:\n\nand so on. This leads to the permutation\n\nAnd the rearranged series, formula_77 then diverges to formula_61.\n\nFrom the way the formula_79 were chosen, it follows that the sum of the first formula_80 terms of the rearranged series is at least 1 and that no partial sum in this group is less than 0. Likewise, the sum of the next formula_81 terms is also at least 1, and no partial sum in this group is less than 0 either. Continuing, this suffices to prove that this rearranged sum does indeed tend to formula_82\n\nIn Riemann's theorem, the permutation used for rearranging a conditionally convergent series to obtain a given value in formula_83 may have arbitrarily many non-fixed points, i.e. all the indexes of the terms of the series may be rearranged. \nOne may ask if it is possible to rearrange only the indexes in a smaller set so that a conditionally convergent series converges to an arbitrarily chosen real number or diverges to (positive or negative) infinity. The answer of this question is positive: Sierpiński proved that is sufficient to rearrange only some strictly positive terms or only some strictly negative terms.\n\nThis question has also been explored using the notion of ideals: for instance, Wilczyński proved that is sufficient to rearrange only the indexes in the ideal of sets of asymptotic density zero. Filipów and Szuca proved that other ideals also have this property.\n\nGiven a converging series of complex numbers, several cases can occur when considering the set of possible sums for all series obtained by rearranging (permuting) the terms of that series:\n\n\nMore generally, given a converging series of vectors in a finite-dimensional real vector space \"E\", the set of sums of converging rearranged series is an affine subspace of \"E\".\n\n\n"}
{"id": "14738183", "url": "https://en.wikipedia.org/wiki?curid=14738183", "title": "Schmidt net", "text": "Schmidt net\n\nThe Schmidt net is a manual drafting method for the Lambert azimuthal equal-area projection using graph paper. It results in one lateral hemisphere of the Earth with the grid of parallels and meridians. The method is common in the geophysical sciences.\n\nIn the figure, the area-preserving property of the projection can be seen by comparing a grid sector near the center of the net with one at the far right of the net. The two sectors have the same area on the sphere and the same area on the disk. The angle-distorting property can be seen by examining the grid lines; most of them do not intersect at right angles on the Schmidt net. A single Schmidt net can only represent one hemisphere of the earth; typically a pair of Schmidt nets is used to represent both sides of the globe.\n\nIt is relatively simple to re-plot a gridded map of the world onto a Schmidt net if the azimuth is chosen to be the junction of the equator with any particular meridian from the world-map's grid. Each grid square surrounding this chosen longitude is simply re-plotted into the corresponding distorted grid-square in the Schmidt net. Points of latitude-longitude can be plotted relative to the azimuth's longitude, interpolating between grid lines in the Schmidt net. For greater accuracy, it is helpful to have a net with finer spacing than 10°; spacings of 2° are common.\nThe Schmidt net is not an appropriate grid for representing the Earth's northern or southern hemisphere (because the lines would not correspond to meridians or parallels in such a projection). However, it can be used as a scalar measuring device for projecting latitude-longitude points onto a blank circle of the same size, to produce a Lambert equal-area projection with the azimuth at the north or south pole. The intersection of the parallels with the outer circle can be used as a \"de facto\" protractor for plotting a point's longitude as the angle in the polar projection. The Schmidt net's horizontal axis can then be used as a scalar measuring device to convert the point's latitude (relative to the pole) into a radial distance from the centre of the circle. Alternatively, the Schmidt net could be replaced entirely with a correctly projected polar grid, and grid squares from a map re-drawn into this disc.\n\nResearchers in structural geology use the Lambert azimuthal projection to plot lineation and foliation in rocks, slickensides in faults, and other linear and planar features. In this context the projection is called the equal-area hemispherical projection. The Schmidt net is often used to sketch out the Lambert azimuthal projection for these purposes. Conversely, the Wulff net (\"equal-angle projection\") is used to plot crystallographic axes and faces.\n\n"}
{"id": "30876162", "url": "https://en.wikipedia.org/wiki?curid=30876162", "title": "Shreeram Shankar Abhyankar", "text": "Shreeram Shankar Abhyankar\n\nShreeram Shankar Abhyankar (22 July 1930 – 2 November 2012) was an Indian American mathematician known for his contributions to algebraic geometry. He, at the time of his death, held the Marshall distinguished professor of mathematics chair at Purdue University, and was also a professor of computer science and industrial engineering. He is known for Abhyankar's conjecture of finite group theory.\n\nHis latest research was in the area of computational and algorithmic algebraic geometry.\n\nAbhyankar was born in Ujjain, Madhya Pradesh, India. He earned his B.Sc. from Royal Institute of Science of University of Mumbai in 1951, his A.M. at Harvard University in 1952, and his Ph.D. at Harvard in 1955. His thesis, written under the direction of Oscar Zariski, was titled \"Local uniformization on algebraic surfaces over modular ground fields\". Before going to Purdue, he was an associate professor of mathematics at Cornell University and Johns Hopkins University.\n\nAbhyankar was appointed the Marshall Distinguished Professor of Mathematics at Purdue in 1967. His research topics include algebraic geometry (particularly resolution of singularities, a field in which he made significant progress over fields of finite characteristic), commutative algebra, local algebra, valuation theory, theory of functions of several complex variables, quantum electrodynamics, circuit theory, invariant theory, combinatorics, computer-aided design, and robotics. He popularized the Jacobian conjecture.\n\nAbhyankar died of a heart condition on 2 November 2012 at his residence near Purdue University.\n\n\nAbhyankar has won numerous awards and honours.\n\n\n"}
{"id": "1542238", "url": "https://en.wikipedia.org/wiki?curid=1542238", "title": "Smooth structure", "text": "Smooth structure\n\nIn mathematics, a smooth structure on a manifold allows for an unambiguous notion of smooth function. In particular, a smooth structure allows one to perform mathematical analysis on the manifold.\n\nA smooth structure on a manifold \"M\" is a collection of smoothly equivalent smooth atlases. Here, a smooth atlas for a topological manifold \"M\" is an atlas for \"M\" such that each transition function is a smooth map, and two smooth atlases for \"M\" are smoothly equivalent provided their union is again a smooth atlas for \"M\". This gives a natural equivalence relation on the set of smooth atlases.\n\nA smooth manifold is a topological manifold \"M\"\ntogether with a smooth structure on \"M\".\n\nBy taking the union of all atlases belonging to a smooth structure, we obtain a maximal smooth atlas. This atlas contains every chart that is compatible with the smooth structure. There is a natural one-to-one correspondence between smooth structures and maximal smooth atlases. \nThus, we may regard a smooth structure as a maximal atlas and vice versa.\n\nIn general, computations with the maximal atlas of a manifold are rather unwieldy. For most applications, it suffices to choose a smaller atlas. \nFor example, if the manifold is compact, then one can find an atlas with only finitely many charts.\n\nLet formula_1 and formula_2 be two \nmaximal atlases on \"M\". \nThe two smooth structures associated to \nformula_1 and formula_2 \nare said to be equivalent if there is a homeomorphism\nformula_5 \nsuch that formula_6.\n\nJohn Milnor showed in 1956 that the 7-dimensional sphere admits a smooth structure that is not equivalent to the standard smooth structure. A sphere equipped with a nonstandard smooth structure is called an exotic sphere.\n\nThe E8 manifold is an example of a topological manifold that does not admit a smooth structure. This essentially demonstrates that Rokhlin's theorem holds only for smooth structures, and not topological manifolds in general.\n\nThe smoothness requirements on the transition functions can be weakened, so that we only require the transition maps to be \"k\"-times continuously differentiable; or strengthened, so that we require the transition maps to be real-analytic. Accordingly, this gives a formula_7 or (real-)analytic structure on the manifold rather than a smooth one. Similarly, we can define a complex structure by requiring the transition maps to be holomorphic.\n\n\n"}
{"id": "16481236", "url": "https://en.wikipedia.org/wiki?curid=16481236", "title": "Starmad", "text": "Starmad\n\nSTARMAD (space tool for advanced and rapid mission analysis and design) deals with the latest trend in the space industry is towards space missions, spacecraft, systems and products, which require quick solutions for system design and software development.\n\nFundamental aspects are: the capability of minimising the number of steps to perform a complete Space Mission Analysis and Design; the ability to evaluate and display results instantaneously; the possibility to control all complex Space Mission subjects in a concurrent manner.\n\nSTARMAD aims to achieve cost reduction and quality improvements by streamlining the design process through improving engineer involvement, and hence his understanding and efficiency in designing a space mission.\n\nSTARMAD is a Space Mission Analysis and Design tool, intended to enable users to quickly and easily perform the following tasks:\n\n1. Preliminary Orbit Analysis, in terms of Dynamics, Geometry, Manoeuvre and Maintenance, Interplanetary Transfer, and Delta-V Budget. \n2. Observation Payload Analysis, in terms of Electromagnetic Spectrum, Optics and Sizing.\n3. Spacecraft Subsystems Design, considering Attitude Control, Communications, Power System, Propulsion System, Structural Analysis and Thermal Control.\n4. Launch and Transfer Vehicle Information.\n5. Mission Operation Complexity, from the point of view of Mission Design and Planning, Flight System Design, Operational Risk Avoidance, Ground Systems.\n\nIts main features are:\n\nSTARMAD is a tool allowing user to perform a Space Mission Analysis and Design in a complete, simple and fast way.\n\nIt can be compared to an electronic handbook where you have just to insert the required inputs, press enter and see the results. \nThe user does not need a large quantity of literature to analyse a space mission subject. Based on the task, STARMAD uses suitable formulas to find the solution.\n\nStarting from the requirements, the engineer can carry out fundamental Space Mission Analyses, not only in terms of engineering parameters but also in terms of Mission Operations Complexity. In addition, configuring STARMAD with an existing space mission and satellite, it is possible to test critical applied modifications.\n\nFurthermore, it offers the possibility to work in a concurrent as well as in an independent way.\n\nThe \"System Algorithms technique\" is used to compute system performance. It applies the basic physical or geometric formulas associated with a particular system or process, such as those for determining resolution, size of an antenna, link budget, geometric coverage. System Algorithms provide the best method for computing performance, providing clear traceability and establishing the relationship between design parameters and performance characteristics. \nSTARMAD computations are based on \"System Algorithms technique\", additionally implementing all the design parameters interdependencies and automatically exchanging results throughout its sections. This allows to simplify and streamline the overall design process.\nThis method is powerful, showing how performance varies with key parameters. Limitation is the assumption of having correctly identified what limits system performance, such as optical quality, pointing stability, etc. Although this limitation, System Algorithms technique is ideal for preliminary assessment on space missions.\n\nIn automatically linking all Space Mission aspects with their associated interdependencies, STARMAD is able to simplify an otherwise complex problem.\nIt facilitates a fast and effective interaction of all disciplines involved, ensuring consistent, high-quality results. The software is an efficient working tool to ensure consistent end-to-end design of a space mission.\nThe concurrent approach will improve engineer involvement, and hence his/her understanding and efficiency in designing a space mission.\n\nThe spacecraft design process is based on mathematical models, which are implemented inside STARMAD. By this means, a consistent set of design parameters can be automatically defined and exchanged throughout the software sections and subsections. And any change, which may affect other disciplines, can immediately be identified and assessed. In this way, a number of design iterations can be performed, and different design options can easily be analysed and compared.\nIn such a way, via STARMAD, it will be possible to streamline the design process achieving cost reduction and quality improvements.\n\nSTARMAD is principally divided into five primary sections, each of which contain several subsections.\n\nThrough the Graphical User Interface (GUI), the user can define the type of problem. The Main User Interface is composed of 30 subsections (called pressing relative buttons). Each subsection can be configured with the required inputs and run independently from the others.\n\nGoing back to the main GUI, the user can solve several different concurrent space mission tasks calling other subsections and performing analyses in parallel taking under control the complexity of the problem. All the involved sections will take care of the performed evaluations and will automatically set their inputs based on the obtained results. This process will allow the user to concurrently design and analyse space mission subjects.\n\nEvery subsection has its own Output Section showing results, data and design summary when the simulation is performed. All results can be saved, stored, or re-loaded for modifications.\n\nThe content of each of the five primary sections is described below.\n\nIt is composed of the following sub-divisions:\n\nIt is subdivided in the sections:\n\n\nIt is composed of all main subsystems required to build a satellite.\n- Torque estimates: where orbit characteristics, environmental torques and slew characteristics are calculated.\n- Attitude control sizing: evaluating main parameters of Momentum wheel, Reaction wheel, Thrusters and Magnetic Torquer.\n- Uplink: setting ground transmitter parameters, this section evaluates outputs for Ground and spacecraft transmitter, Geometry and Atmosphere perturbations, Link budget in terms of EIRP, space loss, atmospheric attenuation, rain attenuation, G/T, Antenna pointing losses, Eb/No, C/No, Margin.\n- Downlink: setting the Spacecraft Transmitter parameters, this section evaluates outputs for Ground and Spacecraft transmitter, Geometry and Atmosphere perturbations, Link budget in terms of EIRP, space loss, atmospheric attenuation, rain attenuation, G/T, Antenna pointing losses, Eb/No, C/No, Margin.\n- Solar array;\n- Secondary battery;\n- Other primary sources\nto calculate solar array mass and power budgets, battery capacity and mass, power and mass for fuel cells, solar thermal dynamics, radioisotope, nuclear reactor (if on board).\n- Sizing, which principally calculates mass, power, mass flow rate, thrust for both chemical and electric propulsion system;\n- Thermodynamics, evaluating specific impulses, combustion chamber and nozzle characteristics. Additionally, it performs a complete sizing for the liquid propulsion system;\n- Storage and Feed, where oxidiser and fuel characteristics plus bulk density and volume are determined.\n- Monocoque Structure;\n- Semi-Monocoque Structure\nto calculate loads, axial and lateral deflections, stress, bending moment, margins of safety.\n\nThis software section gives an overview of the principal characteristics for existing launch and transfer vehicles. Possibility to implement a user-defined vehicle is incorporated in both sections (launchers and transfers).\n\nThis section summarises the results of the Mission Operations Complexity investigation using NASA’s JPL model as described in the text “Cost Effective Space Mission Operations”.\n\nIt is divided into four subsections as follows:\nIn each of these subsections, a level of complexity (high, medium, or low) is given for all the events related to the particular mission subject. The level of the total mission complexity and the predicted full-time equivalent manpower for efficient operations is given as a final output.\n\nSmartphones and tablets usage are widely growing in all the technological fields, helping users in performing the most different activities just ‘on the go’. The increasing computational power of such devices can actually allow also to perform complex analysis and therefore, they can be conceptually used in Space industry in helping users to build space missions, spacecraft, systems and products requiring quick solutions for system design and software development point of view.\n\"iStarmad\" is the iDevice extension of STARMAD, and it is an app to perform preliminary end-to-end space mission analysis and spacecraft subsystems design using iPhone/iPad devices.\n\nIn 2007, STARMAD was created by Davide Starnone. For the following years, it was promoted all around the world through several International Conferences, such as the \"IAC\". Then the software has been sold around the world for 7 years. In 2014 \"SSBV\", a Dutch-led technology driven company active in the domains of (aero)Space and Defence & Security, acquired STARMAD that now is officially included in their portfolio.\n\n"}
{"id": "34896576", "url": "https://en.wikipedia.org/wiki?curid=34896576", "title": "Tautology (rule of inference)", "text": "Tautology (rule of inference)\n\nIn propositional logic, tautology is one of two commonly used rules of replacement. The rules are used to eliminate redundancy in disjunctions and conjunctions when they occur in logical proofs. They are:\n\nThe principle of idempotency of disjunction:\n\nand the principle of idempotency of conjunction:\n\nWhere \"formula_3\" is a metalogical symbol representing \"can be replaced in a logical proof with.\"\n\nTheorems are those logical formulas formula_4 where formula_5 is the conclusion of a valid proof, while the equivalent semantic consequence formula_6 indicates a tautology.\n\nThe \"tautology\" rule may be expressed as a sequent:\n\nand\n\nwhere formula_9 is a metalogical symbol meaning that formula_10 is a syntactic consequence of formula_11, in the one case, formula_12 in the other, in some logical system;\n\nor as a rule of inference:\n\nand\n\nwhere the rule is that wherever an instance of \"formula_11\" or \"formula_12\" appears on a line of a proof, it can be replaced with \"formula_10\";\n\nor as the statement of a truth-functional tautology or theorem of propositional logic. The principle was stated as a theorem of propositional logic by Russell and Whitehead in \"Principia Mathematica\" as:\n\nand\n\nwhere formula_10 is a proposition expressed in some formal system.\n"}
{"id": "31831187", "url": "https://en.wikipedia.org/wiki?curid=31831187", "title": "The Mathematics Enthusiast", "text": "The Mathematics Enthusiast\n\nThe Mathematics Enthusiast is a triannual peer-reviewed open access academic journal covering undergraduate mathematics, mathematics education, including historical, philosophical, and cross-cultural perspectives on mathematics. It is hosted by ScholarWorks at the University of Montana. The journal was established in 2004 and its founding editor-in-chief is Bharath Sriraman. The journal exists as an independent entity in order to give authors full copyright over their articles, and is not affiliated with any commercial publishing companies. \n\nThe journal is abstracted and indexed in Academic Search Complete, Emerging Sources Citation Index, PsycINFO, and Scopus.\n"}
{"id": "9830383", "url": "https://en.wikipedia.org/wiki?curid=9830383", "title": "The Story of 1", "text": "The Story of 1\n\nThe Story of 1 is a BBC documentary about the history of numbers, and in particular, the number 1. It was presented by former Monty Python member Terry Jones. It was released in 2005.\n\nTerry Jones first journeys to Africa, where bones have been discovered with notches in them. However, there is no way of knowing if they were used for counting.\n\nJones then discusses the Ishango bone, which must have been used for counting, because there are 60 scratches on each side of the bone. Jones declares this \"the birth of one\"; a defining moment in history of mathematics.\n\nHe then journeys to Sumer. Shortly after farming had been invented and humans were starting to build houses, they started to represent 1 with a token. With this, it was possible for the first time in history to do arithmetic. The Sumerians would enclose a certain number of tokens in a clay envelope and imprint the number of tokens on the outside. However, it was realized that one could simply write the number on a clay tablet.\n\nTo explore why the development of numbers occurred there and not some other place, Jones travels to Australia and meets a tribe called the Warlpiri. In their language, there are no words for numbers. When an individual is asked how many grandchildren he has, he simply replies he has \"many\", while he in fact has four.\n\nIn Egypt, the numeral system provides a glimpse of Egyptian society, as larger numbers seem more applicable to higher strata of society. It went something like this: One was a line, ten was a rope, a hundred a coil of rope (three symbols for smaller numbers, probably applicable to the average Egyptian), a thousand a lotus (a symbol of pleasure), ten thousand was a commanding finger, and a million – a number the Sumerians would never have dreamed of – was the symbol of a prisoner begging for forgiveness.\n\nThe Egyptians had a standard unit, the cubit, which was instrumental for building wonders such as the pyramids.\n\nJones then journeys to Greece to cover the time of Pythagoras. Jones discusses with mathematician Marcus de Sautoy Pythagoras' obsession with numbers, his secret society, his dedication to numbers, the Pythagorean theorem, and his flawed belief that all things could be measured in units (brought down by the attempt to measure the hypotenuse of an isosceles right triangle, in units relative to the two legs).\n\nArchimedes was also in love with numbers. He tried to see what would happen if one took a sphere and turned it into a cylinder. This concept would later be applied to map making. Archimedes lived in Syracuse which at the time was at war with Rome. Archimedes was killed by a Roman soldier while working on a mathematical problem. The Romans were not interested in mathematics for its own sake, and as a result mathematics declined. The Roman numeral system was clumsy and inefficient. One reason that Terry Jones theorizes might be was that the numerals that the Romans used were basically the old-fashioned lines of the Ishango bone.\n\nJones discusses India's invention of a more efficient numeral system, including the invention of the concept of zero. He explains how the concept traveled west to the Caliphate. Then it arrived in Italy where it met fierce resistance. The reason for this was that most people were familiar only with the Roman numerals and not the superior Indian numerals. Eventually, the Hindu-Arabic numerals displaced the Roman ones.\n\nJones discusses finally how Gottfried Leibniz invented the binary system, which is the foundation for modern digital computers. He planned on building a mechanical computer to use this system, but never followed through with the plan. Leibniz was convinced that 1 and 0 were the only numbers anyone really needed. In 1944, a computer called Colossus was used to crack enemy codes during World War II. Computers like Colossus evolved into modern computers, which are used for every type of number calculation.\n"}
{"id": "13295107", "url": "https://en.wikipedia.org/wiki?curid=13295107", "title": "Transversal (geometry)", "text": "Transversal (geometry)\n\nIn geometry, a transversal is a line that passes through two lines in the same plane at two distinct points. Transversals play a role in establishing whether two other lines in the Euclidean plane are parallel. The intersections of a transversal with two lines create various types of pairs of angles: consecutive interior angles, corresponding angles, and alternate angles. By Euclid's parallel postulate, if the two lines are parallel, consecutive interior angles are supplementary, corresponding angles are equal, and alternate angles are equal.\nA transversal produces 8 angles, as shown in the graph at the above left:\n\nA transversal that cuts two parallel lines at right angles is called a perpendicular transversal. In this case, all 8 angles are right angles \n\nWhen the lines are parallel, a case that is often considered, a transversal produces several congruent and several supplementary angles. Some of these angle pairs have specific names and are discussed below:corresponding angles, alternate angles, and consecutive angles.\n\nCorresponding angles are the four pairs of angles that:\n\nTwo lines are parallel if and only if the two angles of any pair of corresponding angles of any transversal are congruent (equal in measure). \n\nNote: This follows directly from Euclid's parallel postulate. Further, if the angles of one pair are congruent, then the angles of each of the other pairs are also congruent. In our images with parallel lines, corresponding angle pairs are: α=α1, β=β1, γ=γ1 and δ=δ1.\n\nAlternate angles are the four pairs of angles that:\n\nIf the two angles of one pair are congruent (equal in measure), then the angles of each of the other pairs are also congruent.\n\nProposition 1.27 of Euclid's elements, a theorem of absolute geometry (hence valid in both hyperbolic and Euclidean Geometry), proves that if the angles of a pair of alternate angles of a transversal are congruent then the two lines are parallel (non-intersecting) .\n\nIt follows from Euclid's parallel postulate that if the two lines are parallel, then the angles of a pair of alternate angles of a transversal are congruent (Proposition 1.29 of Euclid's elements).\n\nConsecutive interior angles are the two pairs of angles that:\n\n\nTwo lines are parallel if and only if the two angles of any pair of consecutive interior angles of any transversal are supplementary (sum to 180°).\n\nBy the definition of a straight line and the properties of vertical angles, if one pair is supplementary, the other pair is also supplementary.\n\nIf three lines in general position form a triangle are then cut by a transversal, the lengths of the six resulting segments satisfy Menelaus' theorem.\n\nEuclid's formulation of the parallel postulate may be stated in terms of a transversal. Specifically, if the interior angles on the same side of the transversal are less than two right angles then lines must intersect. In fact, Euclid uses the same phrase in Greek that is usually translated as \"transversal\".\n\nEuclid's Proposition 27 states that if a transversal intersects two lines so that alternate interior angles are congruent, then the lines are parallel. Euclid proves this by contradiction: If the lines are not parallel then they must intersect and a triangle is formed. Then one of the alternate angles is an exterior angle equal to the other angle which is an opposite interior angle in the triangle. This contradicts Proposition 16 which states that an exterior angle of a triangle is always greater than the opposite interior angles.\n\nEuclid's Proposition 28 extends this result in two ways. First, if a transversal intersects two lines so that corresponding angles are congruent, then the lines are parallel. Second, if a transversal intersects two lines so that interior angles on the same side of the transversal are supplementary, then the lines are parallel. These follow from the previous proposition by applying the fact that opposite angles of intersecting lines are equal (Prop. 15) and that adjacent angles on a line are supplementary (Prop. 13). As noted by Proclus, Euclid gives only three of a possible six such criteria for parallel lines.\n\nEuclid's Proposition 29 is a converse to the previous two. First, if a transversal intersects two parallel lines, then the alternate interior angles are congruent. If not, then one is greater than the other, which implies its supplement is less than the supplement of the other angle. This implies that there are interior angles on the same side of the transversal which are less than two right angles, contradicting the fifth postulate. The proposition continues by stating that on a transversal of two parallel lines, corresponding angles are congruent and the interior angles on the same side are equal to two right angles. These statements follow in the same way that Prop. 28 follows from Prop. 27.\n\nEuclid's proof makes essential use of the fifth postulate, however, modern treatments of geometry use Playfair's axiom instead. To prove proposition 29 assuming Playfair's axiom, let a transversal cross two parallel lines and suppose that the alternate interior angles are not equal. Draw a third line through the point where the transversal crosses the first line, but with an angle equal to the angle the transversal makes with the second line. This produces two different lines through a point, both parallel to another line, contradicting the axiom.\n\n"}
{"id": "40476033", "url": "https://en.wikipedia.org/wiki?curid=40476033", "title": "Uwe Schöning", "text": "Uwe Schöning\n\nUwe Schöning (born 28 December 1955) is a German computer scientist, known for his research in computational complexity theory.\n\nSchöning earned his Ph.D. from the University of Stuttgart in 1981, under the supervision of Wolfram Schwabhäuser.\nHe is a professor in the Institute for Theoretical Informatics at the University of Ulm.\n\nSchöning introduced the low and high hierarchies to structural complexity theory in 1983. As Schöning later showed in a 1988 paper, these hierarchies play an important role in the complexity of the graph isomorphism problem, which Schöning further developed in a 1993 monograph with Köbler and Toran.\n\nIn a 1999 FOCS paper, Schöning showed that WalkSAT, a randomized algorithm previously analyzed for 2-satisfiability by Papadimitriou, had good expected time complexity (although still exponential) when applied to worst-case instances of 3-satisfiability and other NP-complete constraint satisfaction problems. At the time this was the fastest guaranteed time bound for 3SAT; subsequent research has built on this idea to develop even faster algorithms.\n\nSchöning is also the inventor of the pedagogical programming languages LOOP, GOTO, and WHILE, which he described in his textbook \"Theoretische Informatik - kurz gefasst\".\n\nSchöning is the author or editor of many books in computer science, including\n\nHis research articles include:\n\n"}
{"id": "5811728", "url": "https://en.wikipedia.org/wiki?curid=5811728", "title": "Volterra series", "text": "Volterra series\n\nThe Volterra series is a model for non-linear behavior similar to the Taylor series. It differs from the Taylor series in its ability to capture 'memory' effects. The Taylor series can be used for approximating the response of a nonlinear system to a given input if the output of this system depends strictly on the input at that particular time. In the Volterra series the output of the nonlinear system depends on the input to the system at \"all\" other times. This provides the ability to capture the 'memory' effect of devices like capacitors and inductors.\n\nIt has been applied in the fields of medicine (biomedical engineering) and biology, especially neuroscience. It is also used in electrical engineering to model intermodulation distortion in many devices including power amplifiers and frequency mixers. Its main advantage lies in its generality: it can represent a wide range of systems. Thus it is sometimes considered a non-parametric model. \nIn mathematics, a Volterra series denotes a functional expansion of a dynamic, nonlinear, time-invariant functional. Volterra series are frequently used in system identification. The Volterra series, which is used to prove the Volterra theorem, is an infinite sum of multidimensional convolutional integrals.\n\nThe Volterra series is a modernized version of the theory of analytic functionals due to the Italian mathematician Vito Volterra in work dating from 1887. Norbert Wiener became interested in this theory in the 1920s from contact with Volterra's student Paul Lévy. He applied his theory of Brownian motion to the integration of Volterra analytic functionals. \nThe use of Volterra series for system analysis originated from a restricted 1942 wartime report of Wiener, then professor of mathematics at MIT. It used the series to make an approximate analysis of the effect of radar noise in a nonlinear receiver circuit. The report became public after the war. As a general method of analysis of nonlinear systems, Volterra series came into use after about 1957 as the result of a series of reports, at first privately circulated, from MIT and elsewhere. The name \"Volterra series\" came into use a few years later.\n\nThe theory of Volterra series can be viewed from two different perspectives: either one considers an operator mapping between two real (or complex) function spaces or a functional mapping from a real (or complex) function space into the real (or complex) numbers. The latter, functional perspective is in more frequent use, due to the assumed time-invariance of the system.\n\nA continuous time-invariant system with \"x\"(\"t\") as input and \"y\"(\"t\") as output can be expanded in Volterra series as:\n\nformula_1\n\nHere the constant term formula_2 on the right hand side is usually taken to be zero by suitable choice of output level formula_3.\nThe function formula_4 is called the \"n\"-th order Volterra kernel. It can be regarded as a higher-order impulse response of the system. For the representation to be unique the kernels must be symmetrical in the variables.\n\nIf \"N\" is finite, the series is said to be \"truncated\". If \"a,b\", and \"N\" are finite, the series is called \"doubly finite\".\n\nSometimes the \"n\"-th order term is divided by n!, a convention which is convenient when taking the output of one Volterra system as the input of another ('cascading').\n\n\"The causality condition\": Since in any physically realizable system the output can only depend on previous values of the input, the kernels formula_5 will be zero if any of the variables formula_6 are negative. The integrals may then be written over the half range from zero to infinity.\nSo if the operator is causal, formula_7.\n\n\"Fréchet's approximation theorem\": The use of the Volterra series to represent a time-invariant functional relation is often justified by appealing to a theorem due to Fréchet. This theorem states that a time-invariant functional relation (satisfying certain very general conditions) can be approximated uniformly and to an arbitrary degree of precision by a sufficiently high finite order Volterra series. Among other conditions, the set of admissible input functions formula_8 for which the approximation will hold is required to be compact. It is usually taken to be an equicontinuous, uniformly bounded set of functions, which is compact by the Arzelà–Ascoli theorem. In many physical situations, this assumption about the input set is a reasonable one. The theorem, however, gives no indication as to how many terms are needed for a good approximation, which is an essential question in applications.\n\nThis is similar to the continuous-time case:\n\nformula_9\n\nformula_10 are called discrete-time Volterra kernels.\n\nIf \"P\" is finite, the series operator is said to be truncated. If \"a,b\" and \"P\" are finite the series operator is called doubly finite Volterra series. If formula_7 the operator is said to be causal.\n\nWe can always consider, without loss of the generality, the kernel formula_10 as symmetrical. In fact, for the commutativity of the multiplication it is always possible to symmetrize it by forming a new kernel taken as the average of the kernels for all permutations of the variables formula_13.\n\nFor a causal system with symmetrical kernels we can rewrite the nth term approximately in triangular form\nformula_14\n\nEstimating the Volterra coefficients individually is complicated since the basis functionals of the Volterra series are correlated. This leads to the problem of simultaneously solving a set of integral-equations for the coefficients. Hence, estimation of Volterra coefficients is generally performed by estimating the coefficients of an orthogonalized series, e.g. the Wiener series, and then recomputing the coefficients of the original Volterra series. The Volterra series main appeal over the orthogonalized series lies in its intuitive, canonical structure, i.e. all interactions of the input have one fixed degree. The orthogonalized basis functionals will generally be quite complicated.\n\nAn important aspect, with respect to which the following methods differ is whether the orthogonalization of the basis functionals is to be performed over the idealized specification of the input signal (e.g. gaussian, white noise) or over the actual realization of the input (i.e. the pseudo-random, bounded, almost-white version of gaussian white noise, or any other stimulus). The latter methods, despite their lack of mathematical elegance, have been shown to be more flexible (as arbitrary inputs can be easily accommodated) and precise (due to the effect that the idealized version of the input signal is not always realizable).\n\nThis method, developed by Lee & Schetzen, orthogonalizes with respect to the actual mathematical description of the signal, i.e. the projection onto the new basis functionals is based on the knowledge of the moments of the random signal.\n\nWe can write the Volterra series in terms of homogeneous operators, as\n\nformula_15\n\nwhere\n\nformula_16\n\nTo allow identification orthogonalization, Volterra series must be rearranged in terms of orthogonal non-homogeneous \"G\" operators (Wiener series):\n\nformula_17\n\nThe \"G\" operators can be defined by the following\n\nformula_18\n\nformula_19\n\nwhenever formula_20 is arbitrary homogeneous Volterra, \"x(n)\" is some stationary white noise (SWN) with zero mean and variance \"A\".\n\nRecalling that every Volterra functional is orthogonal to all Wiener functional of greater order, and considering the following Volterra functional:\n\nformula_21\n\nwe can write\n\nformula_22\n\nIf \"x\" is SWN, formula_23 and by letting formula_24, we have:\n\nformula_25\n\nSo if we exclude the diagonal elements, formula_26, it is\n\nformula_27\n\nIf we want to consider the diagonal elements, the solution proposed by Lee and Schetzen is:\n\nformula_28\n\nThe main drawback of this technique is that the estimation errors, made on all elements of lower-order kernels, will affect each diagonal element of order p by means of the summation formula_29, conceived as the solution for the estimation of the diagonal elements themselves. \nEfficient formulas to avoid this drawback and references for diagonal kernel element estimation can be found in\n\nOnce the Wiener kernels were identified, Volterra kernels can be obtained by using Wiener to Volterra formulas, in the following reported for a fifth order Volterra series:\n\nformula_30\n\nformula_31\n\nformula_32\n\nformula_33\n\nformula_34\n\nformula_35\n\nIn the traditional orthogonal algorithm, using inputs with high formula_36 has the advantage of stimulating high order nonlinearity, so as to achieve more accurate high order kernel identification.\nAs a drawback, the use of high formula_37 values causes high identification error in lower order kernels, as shown in \nmainly due to nonideality of the input and truncation errors.\n\nOn the contrary the use of lower formula_37 in the identification process can lead to a better estimation of lower order kernel, but can be insufficient to stimulate high order nonlinearity.\n\nThis phenomenon, that can be called \"locality\" of truncated Volterra series, can be revealed by \ncalculating the output error of a series as a function of different variances of input.\nThis test can be repeated with series identified with different input variances, obtaining different curves, each with a minimum in correspondence of the variance used in the identification.\n\nTo overcome this limitation, a low formula_37 value should be used for the lower order kernel and gradually increased for higher order kernels.\nThis is not a theoretical problem in Wiener kernel identification, since the Wiener functional are orthogonal to each other, but an appropriate normalization is needed in Wiener to Volterra conversion formulas for taking into account the use of different variances.\nFurthermore, new Wiener to Volterra conversion formulas are needed.\n\nThe traditional Wiener kernel identification should be changed as follows:\n\nformula_40\n\nformula_41\n\nformula_42\n\nformula_43\n\nIn the above formulas the impulse functions are introduced for the identification of diagonal kernel points.\nIf the Wiener kernels are extracted with the new formulas, the following Wiener to Volterra formulas (explicited up the fifth order) are needed:\n\nformula_44\n\nformula_45\n\nformula_46\n\nformula_47\n\nformula_48\n\nformula_49\n\nAs can be seen, the drawback with respect to the formula proposed in is that for the identification of the n-order kernel, all lower kernels must be identified again with the higher variance.\nHowever an outstanding improvement in the output MSE will be obtained if the Wiener and Volterra kernels are obtained with the new formulas, as can be seen in.\n\nThis method and its more efficient version (Fast Orthogonal Algorithm) were invented by Korenberg \nIn this method the orthogonalization is performed empirically over the actual input. It has been shown to perform more precisely than the Crosscorrelation method. Another advantage is that arbitrary inputs can be used for the orthogonalization and that fewer data-points suffice to reach a desired level of accuracy. Also, estimation can be performed incrementally until some criterion is fulfilled.\n\nLinear regression is a standard tool from linear analysis. Hence, one of its main advantages is the widespread existence of standard tools for solving linear regressions efficiently. It has some educational value, since it highlights the basic property of Volterra series: linear combination of non-linear basis-functionals. For estimation the order of the original should be known, since the Volterra basis-functionals are not orthogonal and thus estimation cannot be performed incrementally.\n\nThis method was invented by Franz & Schölkopf and is based on statistical learning theory. Consequently, this approach is also based on minimizing the empirical error (often called empirical risk minimization). Franz and Schölkopf proposed that the kernel method could essentially replace the Volterra series representation, although noting that the latter is more intuitive.\n\nThis method was developed by van Hemmen and coworkers and utilizes Dirac delta functions to sample the Volterra coefficients.\n\n\n"}
{"id": "46586008", "url": "https://en.wikipedia.org/wiki?curid=46586008", "title": "Weyl's tile argument", "text": "Weyl's tile argument\n\nIn philosophy, the Weyl's tile argument (named after Hermann Weyl) is an argument against the notion that physical space is discrete, or composed of a number of finite sized units (or tiles). The argument purports to show a distance function approximating Pythagoras' theorem on a discrete space cannot be defined and, since the Pythagorean theorem has been confirmed to be approximately true in nature, physical space is not discrete. While academic debate continues, counterarguments have been proposed in the literature.\n\nA demonstration of Weyl's argument proceeds by constructing a rectangular tiling of the plane representing a discrete space. A discretized triangle, n units tall and n units long, can be constructed on the tiling. The hypotenuse of the resulting triangle will be n tiles long. However, by the pythagorean theorem, a corresponding triangle in a continuous space—a triangle whose height and length are n -- will have a hypotenuse measuring n√2 units long. To show that the former result does not converge to the latter for arbitrary values of n, one can examine the percent difference between the two results: = 1-. Since n cancels out, the two results never converge, even in the limit of large n. The argument can be constructed for more general triangles, but, in each case, the result is the same. Thus, a discrete space does not even approximate the pythagorean theorem.\n\nIn response, Kris McDaniel has argued the Weyl Tile argument depends on accepting the Size Thesis, according to which the distance between two points is given by the number of tiles between the two points. However, as McDaniel points out, the size thesis is not accepted for continuous spaces. Thus, we might have reason not to accept the size thesis for discrete spaces.\n\nNonetheless, if a discrete space is constructed by a rectangular tiling of the plane and the Size Thesis is accepted, the Euclidean metric will be inappropriate for measuring distances on the resulting space. Instead, the so-called Hamming metric should be utilized. Computer scientists interested in the distance between two strings and mathematical biologists interested in the distance between two genetic sequences employ the versions of the Hamming metric in each of their respective disciplines.\n"}
