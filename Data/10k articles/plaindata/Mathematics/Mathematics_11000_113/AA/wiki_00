{"id": "43072622", "url": "https://en.wikipedia.org/wiki?curid=43072622", "title": "1QBit", "text": "1QBit\n\n1QB Information Technologies, Inc. (1QBit) is a quantum computing software company, based in Vancouver, British Columbia. 1QBit was founded on December 1, 2012 and a longstanding partnership with D-Wave Systems was formally announced on June 9, 2014. While 1QBit develops general purpose algorithms for quantum computing hardware, the organization is primarily focused on computational finance, the energy industry, and the life sciences.\n\n1QBit's software reformulates optimization problems into the quadratic unconstrained binary optimization (QUBO) format necessary to compute with D-Wave's quantum annealing processors.\n\n1QBit was founded as the first dedicated quantum computing software company in 2012. In 2013, 1QBit raised seed funding from US and Canadian angel investors, before closing a Series A financing round led by the Chicago Mercantile Exchange in 2014.\nOn August 5, 2015 the World Economic Forum announced 1QBit as a recipient of the 2015 Technology Pioneer Award recognizing 1QBit as a leader among the world's most promising technology companies.\n\n1QBit is headquartered in Vancouver, British Columbia, Canada. In early 2014, 1QBit was invited to join the OneEleven data community located in Toronto, Ontario, Canada. This second location serves as the data science and software production arm of the organization.\n\n"}
{"id": "25350167", "url": "https://en.wikipedia.org/wiki?curid=25350167", "title": "Adjacency algebra", "text": "Adjacency algebra\n\nIn algebraic graph theory, the adjacency algebra of a graph \"G\" is the algebra of polynomials in the adjacency matrix \"A\"(\"G\") of the graph. It is an example of a matrix algebra and is the set of the linear combinations of powers of \"A\".\n\nSome other similar mathematical objects are also called \"adjacency algebra\".\n\nProperties of the adjacency algebra of \"G\" are associated with various spectral, adjacency and connectivity properties of \"G\".\n\n\"Statement\". The number of walks of length \"d\" between vertices \"i\" and \"j\" is equal to the (\"i\", \"j\")-th element of \"A\".\n\n\"Statement\". The dimension of the adjacency algebra of a connected graph of diameter \"d\" is at least \"d\" + 1.\n\n\"Corollary\". A connected graph of diameter \"d\" has at least \"d\" + 1 distinct eigenvalues.\n"}
{"id": "30852785", "url": "https://en.wikipedia.org/wiki?curid=30852785", "title": "Arkiv för Matematik", "text": "Arkiv för Matematik\n\nThe journal is indexed by \"Mathematical Reviews\" and Zentralblatt MATH.\nIts 2009 MCQ was 0.47, and its 2009 impact factor was 0.455.\n"}
{"id": "43717327", "url": "https://en.wikipedia.org/wiki?curid=43717327", "title": "Circuit topology", "text": "Circuit topology\n\nThe circuit topology of a linear polymer refers to arrangement of its intra-molecular contacts. Examples of linear polymers with intra-molecular contacts are nucleic acids and proteins. For defining the circuit topology, contacts are defined depending on the context. For proteins with disulfide bonds, these bonds could be considered as contacts. In a context where beta-beta interactions in proteins are more relevant, these interactions are used to define the circuit topology. As such, circuit topology framework can be applied to a wide range of applications including protein folding and analysis of genome architecture. In particular, data from Hi-C and related technologies can be readily analysed using circuit topology framework. \n\nFor a chain with two binary contacts, three arrangements are available: parallel, series and crossed. For a chain with n contacts, the topology can be described by an n by n matrix in which each element illustrates the relation between a pair of contacts and may take one of the three states, P, S and X. \n\nCircuit topology has implications for folding kinetics and molecular evolution. Circuit topology along with contact order and size are determinants of folding rate of linear polymers. The topology of the cellular proteome and natural RNA reflect evolutionary constraints on biomolecular structures. Topology landscape of biomolecules can be characterized and evolution of molecules can be studied as transition pathways within the landscape.\n\n"}
{"id": "320861", "url": "https://en.wikipedia.org/wiki?curid=320861", "title": "Constant function", "text": "Constant function\n\nIn mathematics, a constant function is a function whose (output) value is the same for every input value. For example, the function formula_1 is a constant function because the value of  formula_2  is 4 regardless of the input value formula_3 (see image).\n\nAs a real-valued function of a real-valued argument, a constant function has the general form  formula_4  or just  formula_5  .\n\nThe graph of the constant function formula_5 is a horizontal line in the plane that passes through the point formula_10.\n\nIn the context of a polynomial in one variable \"x\", the non-zero constant function is a polynomial of degree 0 and its general form is formula_11 . This function has no intersection point with the \"x\"-axis, that is, it has no root (zero). On the other hand, the polynomial  formula_12  is the identically zero function. It is the (trivial) constant function and every \"x\" is a root. Its graph is the \"x\"-axis in the plane.\n\nA constant function is an even function, i.e. the graph of a constant function is symmetric with respect to the \"y\"-axis.\n\nIn the context where it is defined, the derivative of a function is a measure of the rate of change of function values with respect to change in input values. Because a constant function does not change, its derivative is 0. This is often written:  formula_13 . The converse is also true. Namely, if \"y\"'(\"x\")=0 for all real numbers \"x\", then \"y\"(\"x\") is a constant function.\n\nFor functions between preordered sets, constant functions are both order-preserving and order-reversing; conversely, if \"f\" is both order-preserving and order-reversing, and if the domain of \"f\" is a lattice, then \"f\" must be constant.\n\n\nA function on a connected set is locally constant if and only if it is constant.\n\n\n"}
{"id": "4487600", "url": "https://en.wikipedia.org/wiki?curid=4487600", "title": "Constraint inference", "text": "Constraint inference\n\nIn constraint satisfaction, constraint inference is a relationship between constraints and their consequences. A set of constraints formula_1 entails a constraint formula_2 if every solution to formula_1 is also a solution to formula_2. In other words, if formula_5 is a valuation of the variables in the scopes of the constraints in formula_1 and all constraints in formula_1 are satisfied by formula_5, then formula_5 also satisfies the constraint formula_2.\n\nSome operations on constraints produce a new constraint that is a consequence of them. Constraint composition operates on a pair of binary constraints formula_11 and formula_12 with a common variable. The composition of such two constraints is the constraint formula_13 that is satisfied by every evaluation of the two non-shared variables for which there exists a value of the shared variable formula_14 such that the evaluation of these three variables satisfies the two original constraints formula_11 and formula_12.\n\nConstraint projection restricts the effects of a constraint to some of its variables. Given a constraint formula_17 its projection to a subset formula_18 of its variables is the constraint formula_19 that is satisfied by an evaluation if this evaluation can be extended to the other variables in such a way the original constraint formula_17 is satisfied.\n\nExtended composition is similar in principle to composition, but allows for an arbitrary number of possibly non-binary constraints; the generated constraint is on an arbitrary subset of the variables of the original constraints. Given constraints formula_21 and a list formula_22 of their variables, the extended composition of them is the constraint formula_23 where an evaluation of formula_22 satisfies this constraint if it can be extended to the other variables so that formula_21 are all satisfied.\n\n\n"}
{"id": "5347320", "url": "https://en.wikipedia.org/wiki?curid=5347320", "title": "Controllability Gramian", "text": "Controllability Gramian\n\nIn control theory, we may need to find out whether or not a system such as \n\nformula_1\n\nis controllable or not, where formula_2, formula_3, formula_4 and formula_5 are, respectively, formula_6, formula_7, formula_8 and formula_9 matrices. \n\nOne of the many ways one can achieve such goal is by the use of the Controllability Gramian.\nLinear Time Invariant (LTI) Systems are those systems in which the parameters formula_2, formula_3, formula_4 and formula_5 are invariant with respect to time.\n\nOne can observe if the LTI system is or is not controllable simply by looking at the pair formula_14. Then, we can say that the following statements are equivalent:\n\n1. The pair formula_14 is controllable.\n\n2. The formula_6 matrix \n\nformula_17\n\nis nonsingular for any formula_18.\n\n3. The formula_19 controllability matrix\n\nformula_20\n\nhas rank n.\n\n4. The formula_21 matrix \n\nformula_22\n\nhas full row rank at every eigenvalue formula_23 of formula_2.\n\n5. If, in addition, all eigenvalues of formula_2 have negative real parts (formula_2 is stable), then the unique solution of\n\nformula_27\n\nis positive definite. The solution is called the Controllability Gramian and can be expressed as\n\nformula_28\n\nIn the following section we are going to take a closer look at the Controllability Gramian.\n\nThe controllability Gramian can be found as the solution of the Lyapunov equation given by\n\nformula_27\n\nIn fact, we can see that if we take\n\nformula_28\n\nas a solution, we are going to find that:\n\nformula_31\n\nWhere we used the fact that formula_32 at formula_33 for stable formula_2 (all its eigenvalues have negative real part). This shows us that formula_35 is indeed the solution for the Lyapunov equation under analysis.\n\nWe can see that formula_36 is a symmetric matrix, therefore, so is formula_35. \n\nWe can use again the fact that, if formula_2 is stable (all its eigenvalues have negative real part) to show that formula_35 is unique. In order to prove so, suppose we have two different solutions for \n\nformula_27\n\nand they are given by formula_41 and formula_42. Then we have:\n\nformula_43\n\nMultiplying by formula_44 by the left and by formula_45 by the right, would lead us to\n\nformula_46\n\nIntegrating from formula_47 to formula_48:\n\nformula_49\n\nusing the fact that formula_50 as formula_51:\n\nformula_52\n\nIn other words, formula_35 has to be unique.\n\nAlso, we can see that\n\nformula_54\n\nis positive for any t, and that makes formula_35 a positive definite matrix. \n\nMore properties of controllable systems can be found in , as well as the proof for the other equivalent statements of “The pair formula_14 is controllable” presented in section Controllability in LTI Systems.\n\nFor discrete time systems as\n\nformula_57\n\nOne can check that there are equivalences for the statement “The pair formula_14 is controllable” (the equivalences are much alike for the continuous time case). \n\nWe are interested in the equivalence that claims that, if “The pair formula_14 is controllable” and all the eigenvalues of formula_2 have magnitude less than formula_61 (formula_2 is stable), then the unique solution of\n\nformula_63\n\nis positive definite and given by\n\nformula_64\n\nThat is called the discrete Controllability Gramian. We can easily see the correspondence between discrete time and the continuous time case, that is, if we can check that formula_65 is positive definite, and all eigenvalues of formula_2 have magnitude less than formula_61, the system formula_14 is controllable. More properties and proofs can be found in .\n\nLinear time variant (LTV) systems are those in the form:\n\nformula_69\n\nThat is, the matrices formula_2, formula_3 and formula_4 have entries that varies with time. Again, as well as in the continuous time case and in the discrete time case, one may be interested in discovering if the system given by the pair formula_73 is controllable or not. This can be done in a very similar way of the preceding cases.\n\nThe system formula_73 is controllable at time formula_75 if and only if there exists a finite formula_76 such that the formula_6 matrix also called the Controllability Gramian is given by\n\nformula_78\n\nwhere formula_79 is the state transition matrix of formula_80 is nonsingular.\n\nAgain, we have a similar method to determine if a system is or not a controllable system.\n\nWe have that the Controllability Gramian formula_81 have the following property:\n\nformula_83\n\nthat can easily be seen by the definition of formula_81 and by the property of the state transition matrix that claims that:\n\nformula_85\n\nMore about the Controllability Gramian can be found in .\n\n\n"}
{"id": "31889797", "url": "https://en.wikipedia.org/wiki?curid=31889797", "title": "Cycles of Time", "text": "Cycles of Time\n\nCycles of Time: An Extraordinary New View of the Universe is a science book by mathematical physicist Roger Penrose published by The Bodley Head in 2010. The book outlines Penrose's Conformal Cyclic Cosmology (CCC) model, which is an extension of general relativity but opposed to the widely supported multidimensional string theories and cosmological inflation following the Big Bang.\n\nPenrose examines implications of the Second Law of Thermodynamics and its inevitable march toward a maximum entropy state of the universe. Penrose illustrates entropy in terms of information state phase space (with 1 dimension for every degree of freedom) where particles end up moving through ever larger grains of this phase space from smaller grains over time due to random motion. He disagrees with Stephen Hawking's back-track over whether information is destroyed when matter enters black holes. Such information loss would non-trivially lower total entropy in the universe as the black holes wither away due to Hawking radiation, resulting in a loss in phase space degrees of freedom.\n\nPenrose goes on further to state that over enormous scales of time (beyond 10 years), distance ceases to be meaningful as all mass breaks down into extremely red-shifted photon energy, whereupon time has no influence, and the universe continues to expand without event . This period from Big Bang to infinite expansion Penrose defines as an aeon. The smooth “hairless” infinite oblivion of the previous aeon becomes the low-entropy Big Bang state of the next aeon cycle. Conformal geometry preserves the angles but not the distances of the previous aeon, allowing the new aeon universe to appear quite small at its inception as its phase space starts anew.\n\nPenrose cites concentric rings found in the WMAP cosmic microwave background survey as preliminary evidence for his model, as he predicted black hole collisions from the previous aeon would leave such structures due to ripples of gravitational waves.\n\nMost nonexpert critics (nonscientists) have found the book a challenge to fully comprehend; a few such as \"Kirkus Reviews\" and Doug Johnstone for \"The Scotsman\" appreciate the against the grain innovative ideas Penrose puts forth. Manjit Kumar reviewing for \"The Guardian\" admires the Russian doll geometry play of the CCC concept, framing it as an idea of which M. C. Escher \"would have approved\". Graham Storrs for the \"New York Journal of Books\" concedes that this is not the book that an unambitious lay person should plunge into. The American fiction writer Anthony Doerr in \"The Boston Globe\" writes \"Penrose has never shied away from including mathematics in his texts, and kudos to his publisher for honoring that wish. That said, the second half of \"Cycles of Time\" offers some seriously hard sledding\"; \"If you'll forgive a skiing metaphor, \"Cycles of Time\" is a black diamond of a book.\"\n"}
{"id": "31623814", "url": "https://en.wikipedia.org/wiki?curid=31623814", "title": "Dual total correlation", "text": "Dual total correlation\n\nIn information theory, dual total correlation (Han 1978), excess entropy (Olbrich 2008), or binding information (Abdallah and Plumbley 2010) is one of the two known non-negative generalizations of mutual information. While total correlation is bounded by the sum entropies of the \"n\" elements, the dual total correlation is bounded by the joint-entropy of the \"n\" elements. Although well behaved, dual total correlation has received much less attention than the total correlation. A measure known as \"TSE-complexity\" defines a continuum between the total correlation and dual total correlation (Ay 2001).\n\nFor a set of \"n\" random variables formula_1, the dual total correlation formula_2 is given by\n\nwhere formula_4 is the joint entropy of the variable set formula_5 and formula_6 is the conditional entropy of variable formula_7, given the rest.\n\nThe dual total correlation normalized between [0,1] is simply the dual total correlation divided by its maximum value formula_8,\n\nDual total correlation is non-negative and bounded above by the joint entropy formula_10.\n\nSecondly, Dual total correlation has a close relationship with total correlation, formula_12. In particular,\n\nIn measure theoretic terms, by the definition of dual total correlation:\n\nwhich is equal to the union of the pairwise mutual informations:\n\nHan (1978) originally defined the dual total correlation as,\nHowever Abdallah and Plumbley (2010) showed its equivalence to the easier-to-understand form of the joint entropy minus the sum of conditional entropies via the following:\n\n\n"}
{"id": "20425855", "url": "https://en.wikipedia.org/wiki?curid=20425855", "title": "Elementary Calculus: An Infinitesimal Approach", "text": "Elementary Calculus: An Infinitesimal Approach\n\nElementary Calculus: An Infinitesimal approach is a textbook by H. Jerome Keisler. The subtitle alludes to the infinitesimal numbers of the hyperreal number system of Abraham Robinson and is sometimes given as An approach using infinitesimals. The book is available freely online and is currently published by Dover.\n\nKeisler's textbook is based on Robinson's construction of the hyperreal numbers. Keisler also published a companion book, \"Foundations of Infinitesimal Calculus\", for instructors which covers the foundational material in more depth.\n\nKeisler defines all basic notions of the calculus such as continuity, derivative, and integral using infinitesimals. The usual definitions in terms of ε-δ techniques are provided at the end of Chapter 5 to enable a transition to a standard sequence.\n\nIn his textbook, Keisler used the pedagogical technique of an infinite-magnification microscope, so as to represent graphically, distinct hyperreal numbers infinitely close to each other. Similarly, an infinite-resolution telescope is used to represent infinite numbers.\n\nWhen one examines a curve, say the graph of \"ƒ\", under a magnifying glass, its curvature decreases proportionally to the magnification power of the lens. Similarly, an infinite-magnification microscope will transform an infinitesimal arc of a graph of \"ƒ\", into a straight line, up to an infinitesimal error (only visible by applying a higher-magnification \"microscope\"). The derivative of \"ƒ\" is then the (standard part of the) slope of that line (see figure). \nThus the microscope is used as a device in explaining the derivative.\n\nThe book was first reviewed by Errett Bishop, noted for his work in constructive mathematics. Bishop's review was harshly critical; see Criticism of non-standard analysis. Shortly after, Martin Davis and Hausner published a detailed favorable review, as did Andreas Blass and Keith Stroyan. Keisler's student K. Sullivan, as part of her Ph.D. thesis, performed a controlled experiment involving 5 schools which found \"Elementary Calculus\" to have advantages over the standard method of teaching calculus. Despite the benefits described by Sullivan, the vast majority of mathematicians have not adopted infinitesimal methods in their teaching. Recently, Katz & Katz give a positive account of a calculus course based on Keisler's book. O'Donovan also described his experience teaching calculus using infinitesimals. His initial point of view was positive, but later he found pedagogical difficulties with approach to non-standard calculus taken by this text and others.\n\nG. R. Blackley remarked in a letter to Prindle, Weber & Schmidt, concerning \"Elementary Calculus: An Approach Using Infinitesimals\", \"Such problems as might arise with the book will be political. It is revolutionary. Revolutions are seldom welcomed by the established party, although revolutionaries often are.\"\n\nHrbacek writes that the definitions of \"continuity\", \"derivative\", and \"integral\" implicitly must be grounded in the ε-δ method in Robinson's theoretical framework, in order to extend definitions to include non-standard values of the inputs, claiming that the hope that non-standard calculus could be done without ε-δ methods could not be realized in full. Błaszczyk et al. detail the usefulness of microcontinuity in developing a transparent definition of uniform continuity, and characterize Hrbacek's criticism as a \"dubious lament\".\n\nBetween the first and second edition of the \"Elementary Calculus\", much of the theoretical material that was in the first chapter was moved to the epilogue at the end of the book, including the theoretical groundwork of non-standard analysis.\n\nIn the second edition Keisler introduces the extension principle and the transfer principle in the following form:\n\nKeisler then gives a few examples of \"real statements\" to which the principle applies:\n\n\n\n"}
{"id": "291182", "url": "https://en.wikipedia.org/wiki?curid=291182", "title": "Eleusis (card game)", "text": "Eleusis (card game)\n\nEleusis is a multi-genre card game where one player chooses a secret rule to determine which cards can be played on top of others, and the other players attempt to determine the rule using inductive logic.\n\nThe game was invented by Robert Abbott in 1956, and was first published by Martin Gardner in his Mathematical Games column in \"Scientific American\" magazine in June 1959. A revised version appeared in Gardner's July 1977 column.\n\nEleusis is sometimes considered an analogy to the problems of scientific method. It can be compared with the card game Mao, which also has secret rules that can be learned inductively. The games of Penultima and Zendo also feature players attempting to discover inductively a secret rule or rules thought of by a \"Master\" or \"Spectators\" who declare plays legal or illegal on the basis of the rules.\n\nThe formalization of Eleusis+Nobel inspired new modes of communication by exchange of logical notes.\n\nIn 2006, John Golden developed a streamlined version of the game, intended to assist elementary school teachers in explaining the scientific method to students. Abbott himself considers the variant a \"great game\", and refers to it as \"Eleusis Express\".\n\nTo play Eleusis Express, each player is dealt 12 cards, and the dealer decides on a rule on how a card can correctly be played (such as \"alternate red then black cards\" or \"alternate cards with a closed loop (e.g. 4, 6, 8, 9, Q, A) and those without\"). The object of the game is to empty the player's hand. Once a player thinks they have figured out the rule then they can declare themselves a \"prophet\" and make the good/bad card calls for the dealer.\n\nAn incorrect card goes below the line of cards starting a sideline. Players making an incorrect play draw another card.\n\nIf a player thinks he cannot play a legitimate card, he may declare a no play, and show his hand to everybody. If incorrect the dealer plays the correct card and gives the player another card. If the player is right the dealer replaces the player's hand with a smaller hand (with one fewer card) from the deck.\n\nThe round ends with a player running out of cards or a player correctly guessing the rule. At the end of the round a player scores 12 points minus the total of cards he has in his hand. The full game ends once everyone has had a chance to be dealer.\n\n"}
{"id": "10475", "url": "https://en.wikipedia.org/wiki?curid=10475", "title": "Enrico Bombieri", "text": "Enrico Bombieri\n\nEnrico Bombieri (born 26 November 1940 in Milan) is an Italian mathematician, known for his work in analytic number theory, algebraic geometry, univalent functions, theory of several complex variables, partial differential equations of minimal surfaces, and the theory of finite groups. He won a Fields Medal in 1974.\n\nBombieri published his first mathematical paper in 1957 when he was 16 years old. In 1963 at age 22 he earned his first degree (Laurea) in mathematics from the Università degli Studi di Milano under the supervision of Giovanni Ricci and then studied at Trinity College, Cambridge with Harold Davenport.\n\nBombieri was an assistant professor (1963–1965) and then a full professor (1965–1966) at the Università di Cagliari, at the Università di Pisa in 1966–1974, and then at the Scuola Normale Superiore di Pisa in 1974–1977. From Pisa he emigrated in 1977 to the USA, where he became a professor at the \"School of Mathematics\" at the Institute for Advanced Study in Princeton, New Jersey. In 2011 he became professor emeritus.\n\nBombieri's research in number theory, algebraic geometry, and mathematical analysis have earned him many international prizes — a Fields Medal in 1974 and the Balzan Prize in 1980. In 2010 he received the King Faisal International Prize (jointly with Terence Tao). He was a plenary speaker at the International Congress of Mathematicians in 1974 at Vancouver. He is a member, or foreign member, of several learned academies, including the French Academy of Sciences (elected 1984), the United States National Academy of Sciences (elected 1996), and the Accademia Nazionale dei Lincei (elected 1976).\nIn 2002 he was made \"Cavaliere di Gran Croce al Merito della Repubblica Italiana\".\n\nThe Bombieri–Vinogradov theorem is one of the major applications of the large sieve method. It improves Dirichlet's theorem on prime numbers in arithmetic progressions, by showing that by averaging over the modulus over a range, the mean error is much less than can be proved in a given case. This result can sometimes substitute for the still-unproved generalized Riemann hypothesis.\n\nIn 1969 Bombieri, De Giorgi, and Giusti solved Bernstein's problem.\n\nIn 1976, Bombieri developed the technique known as the \"asymptotic sieve\". In 1980 he supplied the completion of the proof of the uniqueness of finite groups of Ree type in characteristic 3; at the time of its publication it was one of the missing steps in the classification of finite simple groups.\n\nBombieri is also known for his pro bono service on behalf of the mathematics profession, e.g. for serving on external review boards and for peer-reviewing extraordinarily complicated manuscripts (like the paper of Per Enflo on the invariant subspace problem).\n\nBombieri, accomplished also in the arts, explored for wild orchids and other plants as a hobby in the Alps when a young man.\n\n\n\n"}
{"id": "5744061", "url": "https://en.wikipedia.org/wiki?curid=5744061", "title": "Epigraph (mathematics)", "text": "Epigraph (mathematics)\n\nIn mathematics, the epigraph or supergraph of a function \"f\" : R→R is the set of points lying on or above its graph:\n\nThe strict epigraph is the epigraph with the graph itself removed:\n\nThe same definitions are valid for a function that takes values in . In this case, the epigraph is empty if and only if \"f\" is identically equal to infinity.\n\nThe domain (rather than the co-domain) of the function is not particularly important for this definition; it can be any linear space or even an arbitrary set instead of formula_3.\n\nSimilarly, the set of points on or below the function is its hypograph.\n\nThe epigraph can often be used to give geometrical interpretations of the properties of convex functions or to prove these properties.\n\nA function is convex if and only if its epigraph is a convex set. The epigraph of a real affine function \"g\" : R→R is a halfspace in R.\n\nA function is lower semicontinuous if and only if its epigraph is closed.\n\n\n"}
{"id": "19688963", "url": "https://en.wikipedia.org/wiki?curid=19688963", "title": "Fast Fourier Transform Telescope", "text": "Fast Fourier Transform Telescope\n\nFast Fourier Transform Telescope is Tegmark and Zaldarriaga's name for a design for an all-digital synthetic-aperture telescope. It is a type of interferometer designed to be cheaper than standard telescope interferometers currently in use.\n\nIn 1868, Hippolyte Fizeau realized that the lenses and mirrors in a telescope perform a physical approximation of a Fourier transform. He noted that by using an array of small instruments it would be possible to measure the diameter of a star with the same precision as a single telescope which was as large as the whole array — a technique which later became known as astronomical interferometry. See History of astronomical interferometry.\n\nIn a 2008 paper, Tegmark and Zaldarriaga proposed a telescope design that dispenses altogether with the lenses and mirrors, relying instead on computers fast enough to perform all the necessary transforms. The concept is an all-digital telescope with an antenna consisting of a rectangular grid. Building radio telescopes this way should become feasible within a few years if Moore's law continues to hold. Eventually optical telescopes could also be built this way. This technique is already being used in radar applications.\n\nThis paper refers to an earlier telescope design from 1993 which took direct images of the Crab nebula at radio wavelengths using an eight-by-eight-pixel two-dimensional spatial FFT processor.\n\n\n"}
{"id": "23253927", "url": "https://en.wikipedia.org/wiki?curid=23253927", "title": "Funk transform", "text": "Funk transform\n\nIn the mathematical field of integral geometry, the Funk transform (also known as Minkowski–Funk transform, Funk–Radon transform or spherical Radon transform) is an integral transform defined by integrating a function on great circles of the sphere. It was introduced by Paul Funk in 1911, based on the work of . It is closely related to the Radon transform. The original motivation for studying the Funk transform was to describe Zoll metrics on the sphere.\n\nThe Funk transform is defined as follows. Let \"ƒ\" be a continuous function on the 2-sphere S in R. Then, for a unit vector x, let\n\nwhere the integral is carried out with respect to the arclength \"ds\" of the great circle \"C\"(x) consisting of all unit vectors perpendicular to x:\n\nThe Funk transform annihilates all odd functions, and so it is natural to confine attention to the case when \"ƒ\" is even. In that case, the Funk transform takes even (continuous) functions to even continuous functions, and is furthermore invertible. \n\nEvery square-integrable function formula_3 on the sphere can be decomposed into spherical harmonics formula_4\n\nThen the Funk transform of \"f\" reads\n\nwhere formula_7 for odd values and \nfor even values. This result was shown by .\n\nAnother inversion formula is due to .\nAs with the Radon transform, the inversion formula relies on the dual transform \"F\"* defined by\n\nThis is the average value of the circle function \"ƒ\" over circles of arc distance \"p\" from the point x. The inverse transform is given by\n\nThe classical formulation is invariant under the rotation group SO(3). It is also possible to formulate the Funk transform in a manner that makes it invariant under the special linear group SL(3,R), due to . Suppose that \"ƒ\" is a homogeneous function of degree −2 on R. Then, for linearly independent vectors x and y, define a function φ by the line integral\n\ntaken over a simple closed curve encircling the origin once. The differential form\n\nis closed, which follows by the homogeneity of \"ƒ\". By a change of variables, φ satisfies\n\nand so gives a homogeneous function of degree −1 on the exterior square of R,\n\nThe function \"Fƒ\" : ΛR → R agrees with the Funk transform when \"ƒ\" is the degree −2 homogeneous extension of a function on the sphere and the projective space associated to ΛR is identified with the space of all circles on the sphere. Alternatively, ΛR can be identified with R in an SL(3,R)-invariant manner, and so the Funk transform \"F\" maps smooth even homogeneous functions of degree −2 on R\\{0} to smooth even homogeneous functions of degree −1 on R\\{0}.\n\nThe Funk-Radon transform is used in the Q-Ball method for Diffusion MRI introduced in .\nIt is also related to intersection bodies in convex geometry. \nLet formula_15 be a star body with radial function formula_16 formula_17.\nThen the intersection body \"IK\" of \"K\" has the radial function formula_18, see .\n\n\n"}
{"id": "8909414", "url": "https://en.wikipedia.org/wiki?curid=8909414", "title": "Gadget (computer science)", "text": "Gadget (computer science)\n\nIn computational complexity theory, a gadget is a subset of a problem instance that simulates the behavior of one of the fundamental units of a different computational problem. Gadgets are typically used to construct reductions from one computational problem to another, as part of proofs of NP-completeness or other types of computational hardness. The component design technique is a method for constructing reductions by using gadgets.\n\nMany NP-completeness proofs are based on many-one reductions from 3-satisfiability, the problem of finding a satisfying assignment to a Boolean formula that is a conjunction (Boolean and) of clauses, each clause being the disjunction (Boolean or) of three terms, and each term being a Boolean variable or its negation. A reduction from this problem to a hard problem on undirected graphs, such as the Hamiltonian cycle problem or graph coloring, would typically be based on gadgets in the form of subgraphs that simulate the behavior of the variables and clauses of a given 3-satisfiability instance. These gadgets would then be glued together to form a single graph, a hard instance for the graph problem in consideration.\n\nFor instance, the problem of testing 3-colorability of graphs may be proven NP-complete by a reduction from 3-satisfiability of this type. The reduction uses two special graph vertices, labeled as \"Ground\" and \"False\", that are not part of any gadget. As shown in the figure, the gadget for a variable \"x\" consists of two vertices connected in a triangle with the ground vertex; one of the two vertices of the gadget is labeled with \"x\" and the other is labeled with the negation of \"x\". The gadget for a clause consists of six vertices, connected to each other, to the vertices representing the terms \"t\", \"t\", and \"t\", and to the ground and false vertices by the edges shown. Any 3-CNF formula may be converted into a graph by constructing a separate gadget for each of its variables and clauses and connecting them as shown.\n\nIn any 3-coloring of the resulting graph, one may designate the three colors as being true, false, or ground, where false and ground are the colors given to the false and ground vertices (necessarily different, as these vertices are made adjacent by the construction) and true is the remaining color not used by either of these vertices. Within a variable gadget, only two colorings are possible: the vertex labeled with the variable must be colored either true or false, and the vertex labeled with the variable's negation must correspondingly be colored either false or true. In this way, valid assignments of colors to the variable gadgets correspond one-for-one with truth assignments to the variables: the behavior of the gadget with respect to coloring simulates the behavior of a variable with respect to truth assignment.\nEach clause assignment has a valid 3-coloring if at least one of its adjacent term vertices is colored true, and cannot be 3-colored if all of its adjacent term vertices are colored false. In this way, the clause gadget can be colored if and only if the corresponding truth assignment satisfies the clause, so again the behavior of the gadget simulates the behavior of a clause.\n\n considered what they called \"a radically simple form of gadget reduction\", in which each bit describing part of a gadget may depend only on a bounded number of bits of the input, and used these reductions to prove an analogue of the Berman–Hartmanis conjecture stating that all NP-complete sets are polynomial-time isomorphic.\n\nThe standard definition of NP-completeness involves polynomial time many-one reductions: a problem in NP is by definition NP-complete if every other problem in NP has a reduction of this type to it, and the standard way of proving that a problem in NP is NP-complete is to find a polynomial time many-one reduction from a known NP-complete problem to it. But (in what Agrawal et al. called \"a curious, often observed fact\") all sets known to be NP-complete at that time could be proved complete using the stronger notion of AC many-one reductions: that is, reductions that can be computed by circuits of polynomial size, constant depth, and unbounded fan-in. Agrawal et al. proved that every set that is NP-complete under AC reductions is complete under an even more restricted type of reduction, NC many-one reductions, using circuits of polynomial size, constant depth, and bounded fan-in. In an NC reduction, each output bit of the reduction can depend only on a constant number of input bits,\n\nThe Berman–Hartmanis conjecture is an unsolved problem in computational complexity theory stating that all NP-complete problem classes are polynomial-time isomorphic. That is, if \"A\" and \"B\" are two NP-complete problem classes, there is a polynomial-time one-to-one reduction from \"A\" to \"B\" whose inverse is also computable in polynomial time. Agrawal et al. used their equivalence between AC reductions and NC reductions to show that all sets complete for NP under AC reductions are AC-isomorphic.\n\nOne application of gadgets is in proving hardness of approximation results, by reducing a problem that is known to be hard to approximate to another problem whose hardness is to be proven. In this application, one typically has a family of instances of the first problem in which there is a gap in the objective function values, and in which it is hard to determine whether a given instance has an objective function that is on the low side or on the high side of the gap. The reductions used in these proofs, and the gadgets used in the reductions, must preserve the existence of this gap, and the strength of the inapproximability result derived from the reduction will depend on how well the gap is preserved.\n\nTrevisan et al. show that, in many cases of the constraint satisfaction problems they study, the gadgets leading to the strongest possible inapproximability results may be constructed automatically, as the solution to a linear programming problem. The same gadget-based reductions may also be used in the other direction, to transfer approximation algorithms from easier problems to harder problems. For instance, Trevisan et al. provide an optimal gadget for reducing 3-SAT to a weighted variant of 2-SAT (consisting of seven weighted 2-SAT clauses) that is stronger than the one by ; using it, together with known semidefinite programming approximation algorithms for MAX 2-SAT, they provide an approximation algorithm for MAX 3-SAT with approximation ratio 0.801, better than previously known algorithms.\n"}
{"id": "30001045", "url": "https://en.wikipedia.org/wiki?curid=30001045", "title": "Highest-weight category", "text": "Highest-weight category\n\nIn the mathematical field of representation theory, a highest-weight category is a \"k\"-linear category C (here \"k\" is a field) that\nand such that there is a locally finite poset Λ (whose elements are called the weights of C) that satisfies the following conditions:\n\n\n\n"}
{"id": "658068", "url": "https://en.wikipedia.org/wiki?curid=658068", "title": "Infinitesimal transformation", "text": "Infinitesimal transformation\n\nIn mathematics, an infinitesimal transformation is a limiting form of \"small\" transformation. For example one may talk about an infinitesimal rotation of a rigid body, in three-dimensional space. This is conventionally represented by a 3×3 skew-symmetric matrix \"A\". It is not the matrix of an actual rotation in space; but for small real values of a parameter ε the transformation\n\nis a small rotation, up to quantities of order ε.\n\nA comprehensive theory of infinitesimal transformations was first given by Sophus Lie. This was at the heart of his work, on what are now called Lie groups and their accompanying Lie algebras; and the identification of their role in geometry and especially the theory of differential equations. The properties of an abstract Lie algebra are exactly those definitive of infinitesimal transformations, just as the axioms of group theory embody symmetry. The term \"Lie algebra\" was introduced in 1934 by Hermann Weyl, for what had until then been known as the \"algebra of infinitesimal transformations\" of a Lie group.\nFor example, in the case of infinitesimal rotations, the Lie algebra structure is that provided by the cross product, once a skew-symmetric matrix has been identified with a 3-vector. This amounts to choosing an axis vector for the rotations; the defining Jacobi identity is a well-known property of cross products.\n\nThe earliest example of an infinitesimal transformation that may have been recognised as such was in Euler's theorem on homogeneous functions. Here it is stated that a function \"F\" of \"n\" variables \"x\", ..., \"x\" that is homogeneous of degree \"r\", satisfies\n\nwith\n\nthe Theta operator. That is, from the property\n\nit is possible to differentiate with respect to λ and then set λ equal to 1. This then becomes a necessary condition on a smooth function \"F\" to have the homogeneity property; it is also sufficient (by using Schwartz distributions one can reduce the mathematical analysis considerations here). This setting is typical, in that there is a one-parameter group of scalings operating; and the information is coded in an infinitesimal transformation that is a first-order differential operator.\n\nThe operator equation\n\nwhere \n\nis an operator version of Taylor's theorem — and is therefore only valid under \"caveats\" about \"f\" being an analytic function. Concentrating on the operator part, it shows that \"D\" is an infinitesimal transformation, generating translations of the real line via the exponential. In Lie's theory, this is generalised a long way. Any connected Lie group can be built up by means of its infinitesimal generators (a basis for the Lie algebra of the group); with explicit if not always useful information given in the Baker–Campbell–Hausdorff formula.\n\n"}
{"id": "2573213", "url": "https://en.wikipedia.org/wiki?curid=2573213", "title": "Integrable system", "text": "Integrable system\n\nIn the context of differential equations to \"integrate\" an equation means to solve it from initial conditions. Accordingly, an integrable system is a system of differential equations whose behavior is determined by initial conditions and which can be \"integrated\" from those initial conditions. \n\nMany systems of differential equations arising in physics are integrable. A standard example is the motion of a rigid body about its center of mass. This system gives rise to a number of conserved quantities: the angular momenta. Conserved quantities such as these are also known as the first integrals of the system. Roughly speaking, if there are enough first integrals to give a coordinate system on the set of solutions, then it is possible to reduce the original system of differential equations to an equation that can be solved by computing an explicit integral. Other examples giving rise to integrable systems in physics are some models of shallow water waves (Korteweg–de Vries equation), the nonlinear Schrödinger equation, and the Toda lattice in statistical mechanics.\n\nWhile the presence of many conserved quantities is generally a fairly clear-cut criterion for integrability, there are other ways in which integrability can appear. It is famously difficult to be precise about what the term means. Nigel Hitchin draws a comparison to a quotation by jazz musician Louis Armstrong, who when asked what jazz was, is rumored to have replied \"If you gotta ask, you'll never know.\" Hitchin identifies three generally recognizable features of integrable systems: \n\nA differential system is said to be \"completely integrable\" in the Frobenius sense if the space on which it is defined has a foliation by maximal integral manifolds. The Frobenius theorem states that a system is completely integrable if and only if it generates an ideal that is closed under exterior differentiation. (See the article on integrability conditions for differential systems for a detailed discussion of foliations by maximal integral manifolds.)\n\nIn the context of differentiable dynamical systems, the notion of integrability refers to the existence of invariant, regular foliations; i.e., ones whose leaves are embedded submanifolds of the smallest possible dimension that are invariant under the flow. There is thus a variable notion of the degree of integrability, depending on the dimension of the leaves of the invariant foliation.\nThis concept has a refinement in the case of Hamiltonian systems, known as complete integrability in the sense of Liouville (see below), which is what is most frequently referred to in this context.\n\nAn extension of the notion of integrability is also applicable to discrete systems such as lattices.\nThis definition can be adapted to describe evolution equations that either are systems of\ndifferential equations or finite difference equations.\n\nThe distinction between integrable and nonintegrable dynamical systems thus has the qualitative\nimplication of regular motion vs. chaotic motion and hence is an intrinsic property, not just a matter of whether\na system can be explicitly integrated in exact form.\n\nIn the special setting of Hamiltonian systems, we have the notion of integrability in the Liouville sense, see the Liouville-Arnold theorem.\nLiouville integrability means that there exists a regular foliation of the phase space by invariant manifolds such that the Hamiltonian vector fields\nassociated to the invariants of the foliation span the tangent distribution. Another way to state this is that there exists\na maximal set of Poisson commuting invariants (i.e., functions on the phase space whose Poisson brackets with the Hamiltonian of the system,\nand with each other, vanish).\n\nIn finite dimensions, if the phase space is symplectic (i.e., the center of the Poisson algebra consists only of constants), then it must have\neven dimension formula_1, and the maximal number of independent Poisson commuting invariants (including the Hamiltonian itself) is\nformula_2. The leaves of the foliation are totally isotropic with respect to the symplectic form and such a maximal isotropic foliation is\ncalled Lagrangian. All \"autonomous\" Hamiltonian systems (i.e. those for which the Hamiltonian and Poisson brackets are not explicitly time dependent)\nhave at least one invariant; namely, the Hamiltonian itself, whose value along the flow is the energy. If the energy level sets are compact, the\nleaves of the Lagrangian foliation are tori, and the natural linear coordinates on these are called \"angle\" variables. The cycles of the canonical formula_3-form\nare called the action variables, and the resulting canonical coordinates are called action-angle variables (see below).\n\nThere is also a distinction between complete integrability, in the Liouville sense, and partial integrability, as well as\na notion of superintegrability and maximal superintegrability. Essentially, these distinctions correspond to the dimensions of the leaves of the foliation.\nWhen the number of independent Poisson commuting invariants is less than maximal (but, in the case of\nautonomous systems, more than one), we say the system is partially integrable.\nWhen there exist further functionally independent invariants, beyond the maximal number that\ncan be Poisson commuting, and hence the dimension of the leaves of the invariant foliation is\nless than n, we say the system is superintegrable. If there is a regular foliation with one-dimensional\nleaves (curves), this is called maximally superintegrable.\n\nWhen a finite-dimensional Hamiltonian system is completely integrable in the Liouville sense,\nand the energy level sets are compact, the flows are complete, and the leaves of the invariant foliation are tori.\nThere then exist, as mentioned above, special sets of canonical coordinates on the phase space known as action-angle variables,\nsuch that the invariant tori are the joint level sets of the action variables. These thus provide a complete set of invariants\nof the Hamiltonian flow (constants of motion), and the angle variables are the natural periodic coordinates on the torus. The motion on the\ninvariant tori, expressed in terms of these canonical coordinates, is linear in the angle variables.\n\nIn canonical transformation theory, there is the Hamilton–Jacobi method, in which solutions to Hamilton's equations are sought by first finding a complete solution of the associated Hamilton–Jacobi equation. In classical terminology, this is described as determining a transformation to a canonical set of coordinates consisting of completely ignorable variables; i.e., those in which there is no dependence of the Hamiltonian on a complete set of canonical \"position\" coordinates, and hence the corresponding canonically conjugate momenta are all conserved quantities. In the case of compact energy level sets, this is the first step towards determining the action-angle variables. In the general theory of partial differential equations of Hamilton–Jacobi type, a complete solution (i.e. one that depends on \"n\" independent constants of integration, where \"n\" is the dimension of the configuration space), exists in very general cases, but only in the local sense. Therefore, the existence of a complete solution of the Hamilton–Jacobi equation is by no means a characterization of complete integrability in the Liouville sense. Most cases that can be \"explicitly integrated\" involve a complete separation of variables, in which the separation constants provide the complete set of integration constants that are required. Only when these constants can be reinterpreted, within the full phase space setting, as the values of a complete set of Poisson commuting functions restricted to the leaves of a Lagrangian foliation, can the system be regarded as completely integrable in the Liouville sense.\n\nA resurgence of interest in classical integrable systems came with the discovery, in the late 1960s, that solitons, which are strongly stable, localized solutions of partial differential equations like the Korteweg–de Vries equation (which describes 1-dimensional non-dissipative fluid dynamics in shallow basins), could be understood by viewing these equations as infinite-dimensional integrable\nHamiltonian systems. Their study leads to a very fruitful approach for \"integrating\" such systems, the inverse scattering transform and more general inverse spectral methods (often reducible to Riemann–Hilbert problems),\nwhich generalize local linear methods like Fourier analysis to nonlocal linearization, through the solution of associated integral equations.\n\nThe basic idea of this method is to introduce a linear operator that is determined by the position in phase space and which evolves under the dynamics of the system in question in such a way that its \"spectrum\" (in a suitably generalized sense) is invariant under the evolution, cf. Lax pair. This provides, in certain cases, enough invariants, or \"integrals of motion\" to make the system completely integrable. In the case of systems having an infinite number of degrees of freedom, such as the KdV equation, this is not sufficient to make precise the property of Liouville integrability. However, for suitably defined boundary conditions, the spectral transform can, in fact, be interpreted as a transformation to completely ignorable coordinates, in which the conserved quantities form half of a doubly infinite set of canonical coordinates, and the flow linearizes in these. In some cases, this may even be seen as a transformation to action-angle variables, although typically only a finite number of the \"position\" variables are actually angle coordinates, and the rest are noncompact.\n\nThere is also a notion of quantum integrable systems.\n\nIn the quantum setting, functions on phase space must be replaced by self-adjoint operators on a Hilbert space, and the notion\nof Poisson commuting functions replaced by commuting operators.\n\nTo explain quantum integrability, it is helpful to consider the free particle setting. Here all dynamics are one-body reducible. A quantum\nsystem is said to be integrable if the dynamics are two-body reducible. The Yang-Baxter equation is a consequence of this reducibility and leads to trace identities which provide an infinite set of conserved quantities. All of these ideas are incorporated into the quantum inverse scattering method where the algebraic Bethe Ansatz can be used to obtain explicit solutions. Examples of quantum integrable models are the Lieb-Liniger Model, the Hubbard model and several variations on the Heisenberg model. Some other types of quantum integrability are known in explicitly time-dependent quantum problems, such as the driven Tavis-Cummings model .\n\nIn physics, completely integrable systems, especially in the infinite-dimensional setting, are often referred to as exactly solvable models. This obscures the distinction between integrability in the Hamiltonian sense, and the more general dynamical systems sense.\n\nThere are also exactly solvable models in statistical mechanics, which are more closely related to quantum integrable systems than classical ones. Two closely related methods: the Bethe ansatz approach, in its modern sense, based on the Yang–Baxter equations and the quantum inverse scattering method provide quantum analogs of the inverse spectral methods. These are equally important in the study of solvable models in statistical mechanics.\n\nAn imprecise notion of \"exact solvability\" as meaning: \"The solutions can be expressed explicitly in terms of some previously known functions\" is also sometimes used, as though this were an intrinsic property of the system itself, rather than the purely calculational feature that we happen to have some \"known\" functions available, in terms of which the solutions may be expressed. This notion has no intrinsic meaning, since what is meant by \"known\" functions very often is defined precisely by the fact that they satisfy certain given equations, and the list of such \"known functions\" is constantly growing. Although such a characterization of \"integrability\" has no intrinsic validity, it often implies the sort of regularity that is to be expected in integrable systems. \n\n1. Classical mechanical systems (finite-dimensional phase space):\n\n\n2. Integrable lattice models\n\n\n3. Integrable systems of PDEs in 1 + 1 dimension\n\n\n4. Integrable PDEs in 2 + 1 dimensions\n\n\n5. Other integrable systems of PDEs in higher dimensions\n\n\n\n\n"}
{"id": "48651594", "url": "https://en.wikipedia.org/wiki?curid=48651594", "title": "John Pardon", "text": "John Pardon\n\nJohn Vincent Pardon (born June 1989) is an American mathematician who works on geometry and topology. He is primarily known for having solved Gromov's problem on distortion of knots, for which he was awarded the 2012 Morgan Prize. He is currently a full professor of mathematics at Princeton University.\n\nPardon's father, William Pardon, is a mathematics professor at Duke University, and when Pardon was a high school student at the Durham Academy he also took classes at Duke. He was a three-time gold medalist at the International Olympiad in Informatics, in 2005, 2006, and 2007. In 2007, Pardon placed second in the Intel Science Talent Search competition, with a generalization to rectifiable curves of the carpenter's rule problem for polygons. In this project, he showed that every rectifiable Jordan curve in the plane can be continuously deformed into a convex curve without changing its length and without ever allowing any two points of the curve to get closer to each other. He published this research in the \"Transactions of the American Mathematical Society\" in 2009.\n\nPardon then went to Princeton University, where after his sophomore year he primarily took graduate-level mathematics classes.\nAt Princeton, Pardon solved a problem in knot theory posed by Mikhail Gromov in 1983 about whether every knot can be embedded into three-dimensional space with bounded stretch factor. Pardon showed that, on the contrary, the stretch factor of certain torus knots could be arbitrarily large.\nHis proof was published in the \"Annals of Mathematics\" in 2011, and earned him the Morgan Prize of 2012. Pardon also took part in a Chinese-language immersion program at Princeton, and was part of Princeton's team at an international debate competition in Singapore, broadcast on Chinese television. As a cello player he was a two-time winner of the Princeton Sinfonia concerto competition. He graduated in 2011, as Princeton's valedictorian.\n\nHe went to Stanford University for his graduate studies, where his accomplishments included solving the three-dimensional case of the Hilbert–Smith conjecture. He completed his Ph.D. in 2015, under the supervision of Yakov Eliashberg,\nand continued at Stanford as an assistant professor. In 2015, he was also appointed to a five-year term as a Clay Research Fellow.\n\nSince fall 2016, he has been a full professor of mathematics at Princeton University.\n\nIn 2017, Pardon received National Science Foundation Alan T. Waterman Award for his contributions to geometry and topology.\n\nHe was elected to the 2018 class of fellows of the American Mathematical Society. Also in 2018 he was an invited speaker at the International Congress of Mathematicians in Rio de Janeiro.\n\n\n"}
{"id": "1068664", "url": "https://en.wikipedia.org/wiki?curid=1068664", "title": "KOV-14", "text": "KOV-14\n\nThe KOV-14 Fortezza Plus is a US National Security Agency-approved PC card which provides encryption functions and key storage to the Secure Terminal Equipment and other devices. It is a tamper-resistant module based on the Mykotronx Krypton chip, including all of the cryptographic functionality of the original Fortezza card plus the Type 1 algorithms/protocols BATON and Firefly, the SDNS signature algorithm, and the STU-III protocol. It was developed by Mykotronx as part of the NSA's MISSI program. As of 2008, the KOV-14 is beginning to be phased out and replaced by the backwards compatible KSV-21 PC card. \n"}
{"id": "52098902", "url": "https://en.wikipedia.org/wiki?curid=52098902", "title": "Karl Schröter", "text": "Karl Schröter\n\nKarl Walter Schröter (* 7 September 1905 in Biebrich near Wiesbaden, † 22 August 1977 in Berlin) was a German mathematician and logician. Later on, after the war, he made important contributions concerning semantic consequences () and provability logic (). He worked as a mathematical theoretician and cryptanalyst for the civilian Pers Z S, the cipher bureau of the Foreign Office (), from Spring 1941 to the end of World War II.\n\nFrom 1928 to 1936, Schröter studied mathematics, physics, philosophy, and psychology at the Universities of Göttingen, Heidelberg and Frankfurt am Main. Due to family reasons he had to interrupt his studies several times. He then worked in the mathematical logic group at the University of Münster lead by Heinrich Scholz. From April 1, 1939, he was a research assistant at the Department of Philosophy at the University of Münster. On 20 December 1941, he took his examination for promotion of Dr. phil under the logician Heinrich Scholz studying mathematics, logic, and calculus with a thesis titled \"Ein allgemeiner Kalkülbegriff\" (). On April 1, 1941, he took a leave of absence to join Pers Z S, the Foreign Office civilian cipher bureau, working as a mathematician. However, even during this time he continued to work on problems of basic mathematical research.\n\nOn 19 March 1943 he presented the application to the Faculty of Philosophy and Natural Sciences, with the publication \"Axiomatisierung der Fregeschen Aussagenkalküle\" (), to be admitted to his Habilitation. On the basis of the positive opinions of Heinrich Scholz and Adolf Kratzer, the degree of Doctor \"rerum naturalium habilitatus\" was awarded to him by certificate of May 22, 1943. On June 9, 1943, the report on the completed Habilitation was given to the Reich Minister. On July 1, 2 and 3, 1943, he held a public trial lecture on the topic \"Der Nutzen der mathematischen Logik für die Mathematik\" () as a prerequisite for a civil service position as a lecturer. On August 18, 1943, he was appointed lecturer with the authorisation to teach \"Mathematical Logic and Fundamental Research\", all the while still being employed in Berlin as a \"scientific assistant worker\" at Pers Z S.\n\nKarl Schröter remained at the University of Münster from 31 December 1943 until his contract was up on 31 April 1945, when he was taken prisoner by the Counterintelligence Corps (CIC). From early May to the September 30, 1945, he appeared before a joint Anglo-American Commission, first in London, then in Marburg, concerning his work at Pers Z S during the war. After completion, he was dismissed in Marburg after the CIC had determined his political attitude. In the winter semester of 1945/46 he was affiliated as a lecturer in Münster.\n\nFrom May 1946 until his appointment to Berlin, Schröter was working at Münster as a substitute for the Review Committee of the Denazification Main Committee at the Westfälische Landesuniversität. In 1948, Karl Schröter was appointed Professor Extraordinarius for mathematical logic at Humboldt University in Berlin.\n\nIn 1967, he became director of the Institute for pure mathematics of the German Academy of Sciences at Berlin.\n\nHe was elected corresponding member in 1962 and two years later ordinary member of the German Academy of Sciences at Berlin.\n\nIn 1955, Schröter together with Günter Asser founded the \"Zeitschrift für Mathematische Logik und Grundlagen der Mathematik\" () which since 1991 is known as \"Mathematical Logic Quarterly\".\n\n\n"}
{"id": "24557812", "url": "https://en.wikipedia.org/wiki?curid=24557812", "title": "Kathrin Bringmann", "text": "Kathrin Bringmann\n\nKathrin Bringmann (born 8 May 1977) is a German number theorist in the University of Cologne, Germany, who has made fundamental contributions to the theory of mock theta functions.\n\nKathrin Bringmann was born on 8 May 1977, in Muenster, Germany. She passed the State Examinations in Mathematics and Theology at the University of Würzburg, Germany, in 2002, and obtained a Diploma in Mathematics at Würzburg in 2003. She received PhD in 2004 from University of Heidelberg under the supervision of .\n\nDuring 2004–07, she was Edward Burr Van Vleck Assistant Professor with the University of Wisconsin where she began her collaboration with Ken Ono. After briefly serving as an Assistant Professor at the University of Minnesota, she joined the University of Cologne, Germany, as Professor.\n\nBringmann has been awarded the Alfried Krupp-Förderpreis for Young Professors, a one-million-Euro prize instituted by the Alfried Krupp von Bohlen und Halbach Foundation. She is the third mathematician to win this prize. She has also been awarded the SASTRA Ramanujan Prize in 2009 for her contributions to \"areas of mathematics influenced by the genius Srinivasa Ramanujan.\"\n\nShe was the Emmy Noether Lecturer of the German Mathematical Society in 2015.\n"}
{"id": "1879769", "url": "https://en.wikipedia.org/wiki?curid=1879769", "title": "Lagrangian foliation", "text": "Lagrangian foliation\n\nIn mathematics, a Lagrangian foliation or polarization is a foliation of a symplectic manifold, whose leaves are Lagrangian submanifolds. It is one of the steps involved in the geometric quantization of a square-integrable functions on a symplectic manifold.\n\n"}
{"id": "16703846", "url": "https://en.wikipedia.org/wiki?curid=16703846", "title": "Lin–Tsien equation", "text": "Lin–Tsien equation\n\nThe Lin–Tsien equation (named after C. C. Lin and H. S. Tsien) is an integrable partial differential equation\n\nIntegrability of this equation follows from its being, modulo an appropriate linear change of dependent and independent variables, a potential form of the dispersionless KP equation. Namely, if formula_2 satisfies the Lin–Tsien equation, then formula_3 satisfies, modulo the said change of variables, the dispersionless KP equation. The Lin-Tsien equation admits a (3+1)-dimensional integrable generalization, see. \n\n"}
{"id": "39479069", "url": "https://en.wikipedia.org/wiki?curid=39479069", "title": "List of things named after Ferdinand Georg Frobenius", "text": "List of things named after Ferdinand Georg Frobenius\n\nThese are things named after Ferdinand Georg Frobenius, a German mathematician.\n"}
{"id": "10465223", "url": "https://en.wikipedia.org/wiki?curid=10465223", "title": "L² cohomology", "text": "L² cohomology\n\nIn mathematics, L cohomology is a cohomology theory for smooth non-compact manifolds \"M\" with Riemannian metric. It is defined in the same way as de Rham cohomology except that one uses square-integrable differential forms. The notion of square-integrability makes sense because the metric on \"M\" gives rise to a norm on differential forms and a volume form.\n\nL cohomology, which grew in part out of L d-bar estimates from the 1960s, was studied cohomologically, independently by Steven Zucker (1978) and Jeff Cheeger (1979). It is closely related to intersection cohomology; indeed, the results in the preceding cited works can be expressed in terms of intersection cohomology.\n\nAnother such result is the Zucker conjecture, which states that for a Hermitian locally symmetric variety the L cohomology is isomorphic to the intersection cohomology (with the middle perversity) of its Baily–Borel compactification (Zucker 1982). This was proved in different ways by Eduard Looijenga (1988) and by Leslie Saper and Mark Stern (1990).\n\n"}
{"id": "7655721", "url": "https://en.wikipedia.org/wiki?curid=7655721", "title": "MASH-1", "text": "MASH-1\n\nMASH-1 (Modular Arithmetic Secure Hash) is a hash function based on modular arithmetic.\n\nDespite many proposals, few hash functions based on modular arithmetic have withstood attack, and most that have tend to be relatively inefficient. MASH-1 evolved from a long line of related proposals successively broken and repaired.\n\nCommittee Draft ISO/IEC 10118-4 (Nov 95)\n\nMASH-1 involves use of an RSA-like modulus M, whose bitlength affects the security. M should be difficult to factor, and for M of unknown factorization, the security is based in part on the difficulty of extracting modular roots. M also determines the block size for processing messages.\n\nThere is a new version of the algorithm called MASH-2 with a different exponent. The original formula_1 is replaced by formula_2.\n\n"}
{"id": "14160015", "url": "https://en.wikipedia.org/wiki?curid=14160015", "title": "Malgrange–Ehrenpreis theorem", "text": "Malgrange–Ehrenpreis theorem\n\nIn mathematics, the Malgrange–Ehrenpreis theorem states that every non-zero linear differential operator with constant coefficients has a Green's function. It was first proved independently by and\n\nThis means that the differential equation\n\nwhere \"P\" is a polynomial in several variables and δ is the Dirac delta function, has a distributional solution \"u\". It can be used to show that\n\nhas a solution for any compactly supported distribution \"f\". The solution is not unique in general.\n\nThe analogue for differential operators whose coefficients are polynomials (rather than constants) is false: see Lewy's example.\n\nThe original proofs of Malgrange and Ehrenpreis were non-constructive as they used the Hahn–Banach theorem. Since then several constructive proofs have been found.\n\nThere is a very short proof using the Fourier transform and the Bernstein–Sato polynomial, as follows. By taking Fourier transforms the Malgrange–Ehrenpreis theorem is equivalent to the fact that every non-zero polynomial \"P\" has a distributional inverse. By replacing \"P\" by the product with its complex conjugate, one can also assume that \"P\" is non-negative. For non-negative polynomials \"P\" the existence of a distributional inverse follows from the existence of the Bernstein–Sato polynomial, which implies that \"P\" can be analytically continued as a meromorphic distribution-valued function of the complex variable \"s\"; the constant term of the Laurent expansion of \"P\" at \"s\" = −1 is then a distributional inverse of \"P\".\n\nOther proofs, often giving better bounds on the growth of a solution, are given in , and .\n\nA short constructive proof was presented in :\n\nis a fundamental solution of \"P\"(∂), i.e., \"P\"(∂)\"E\" = δ, if \"P\" is the principal part of \"P\", η ∈ R with \"P\"(η) ≠ 0, the real numbers λ, ..., λ are pairwise different, and\n\n"}
{"id": "27606747", "url": "https://en.wikipedia.org/wiki?curid=27606747", "title": "Mathematical Methods in the Physical Sciences", "text": "Mathematical Methods in the Physical Sciences\n\nMathematical Methods in the Physical Sciences is a 1966 textbook by mathematician Mary L. Boas intended to develop skills in mathematical problem solving needed for junior to senior-graduate courses in engineering, physics, and chemistry. The book provides a comprehensive survey of analytic techniques and provides careful statements of important theorems while omitting most detailed proofs. Each section contains a large number of problems, with selected answers. Numerical computational approaches using computers are outside the scope of the book.\n\nThe book, now in its third edition, was still widely used in university classrooms as of 1999\nand is frequently cited in other textbooks and scientific papers.\n\n\n\n"}
{"id": "4040947", "url": "https://en.wikipedia.org/wiki?curid=4040947", "title": "Maupertuis's principle", "text": "Maupertuis's principle\n\nIn classical mechanics, Maupertuis's principle (named after Pierre Louis Maupertuis), states that the path followed by a physical system is the one of least length (with a suitable interpretation of \"path\" and \"length\"). It is a special case of the more generally stated principle of least action. Using the calculus of variations, it results in an integral equation formulation of the equations of motion for the system.\n\nMaupertuis's principle states that the true path of a system described by formula_1 generalized coordinates formula_2 between two specified states formula_3 and formula_4 is an extremum (i.e., a stationary point, a minimum, maximum or saddle point) of the abbreviated action functional\n\nwhere formula_6 are the conjugate momenta of the generalized coordinates, defined by the equation \n\nwhere formula_8 is the Lagrangian function for the system. In other words, any \"first-order\" perturbation of the path results in (at most) \"second-order\" changes in formula_9. Note that the abbreviated action formula_9 is a functional (i.e. a function from a vector space into its underlying scalar field), which in this case takes as its input a function (i.e. the pathes between the two specified states).\n\nFor many systems, the kinetic energy formula_11 is quadratic in the generalized velocities formula_12\n\nalthough the mass tensor formula_14 may be a complicated function of the generalized coordinates formula_15. For such systems, a simple relation relates the kinetic energy, the generalized momenta and the generalized velocities\n\nprovided that the potential energy formula_17 does not involve the generalized velocities. By defining a normalized distance or metric formula_18 in the space of generalized coordinates\n\none may immediately recognize the mass tensor as a metric tensor. The kinetic energy may be written in a massless form\n\nor, equivalently,\n\nHence, the abbreviated action can be written\n\nsince the kinetic energy formula_23 equals the (constant) total energy formula_24 minus the potential energy formula_17. In particular, if the potential energy is a constant, then Jacobi's principle reduces to minimizing the path length formula_26 in the space of the generalized coordinates, which is equivalent to Hertz's principle of least curvature.\n\nHamilton's principle and Maupertuis's principle are occasionally confused and both have been called the principle of least action. They differ from each other in three important ways: \n\n\n\nMaupertuis was the first to publish a \"principle of least action\", where he defined \"action\" as formula_41, which was to be minimized over all paths connecting two specified points. However, Maupertuis applied the principle only to light, not matter (see ). He arrived at the principle by considering Snell's law for the refraction of light, which Fermat had explained by Fermat's principle, that light follows the path of shortest \"time\", not distance. This troubled Maupertuis, since he felt that time and distance should be on an equal footing: \"why should light prefer the path of shortest time over that of distance?\" Accordingly, Maupertuis asserts with no further justification the principle of least action as equivalent but more fundamental than Fermat's principle, and uses it to derive Snell's law. Maupertuis specifically states that light does not follow the same laws as material objects.\n\nA few months later, well before Maupertuis's work appeared in print, Leonhard Euler independently defined action in its modern abbreviated form formula_42 and applied it to the motion of a particle, but not to light (see ). Euler also recognized that the principle only held when the speed was a function only of position, i.e., when the total energy was conserved. (The mass factor in the action and the requirement for energy conservation were not relevant to Maupertuis, who was concerned only with light.) Euler used this principle to derive the equations of motion of a particle in uniform motion, in a uniform and non-uniform force field, and in a central force field. Euler's approach is entirely consistent with the modern understanding of Maupertuis's principle described above, except that he insisted that the action should always be a minimum, rather than a stationary point.\n\nTwo years later, Maupertuis cites Euler's 1744 work as a \"beautiful application of my principle to the motion of the planets\" and goes on to apply the principle of least action to the lever problem in mechanical equilibrium and to perfectly elastic and perfectly inelastic collisions (see ). Thus, Maupertuis takes credit for conceiving the principle of least action as a \"general\" principle applicable to all physical systems (not merely to light), whereas the historical evidence suggests that Euler was the one to make this intuitive leap. Notably, Maupertuis's definitions of the action and protocols for minimizing it in this paper are inconsistent with the modern approach described above. Thus, Maupertuis's published work does not contain a single example in which he used Maupertuis's principle (as presently understood).\n\nIn 1751, Maupertuis's priority for the principle of least action was challenged in print (\"Nova Acta Eruditorum\" of Leipzig) by an old acquaintance, Johann Samuel Koenig, who quoted a 1707 letter purportedly from Leibniz that described results similar to those derived by Euler in 1744. However, Maupertuis and others demanded that Koenig produce the original of the letter to authenticate its having been written by Leibniz. Koenig only had a copy and no clue as to the whereabouts of the original. Consequently, and that its President, Maupertuis, could continue to claim priority for having invented the principle. Koenig continued to fight for Leibniz's priority and soon luminaries such as Voltaire and the King of Prussia, Frederick II were engaged in the quarrel. However, no progress was made until the turn of the twentieth century, when other independent copies of Leibniz's letter were discovered. \n\n"}
{"id": "200307", "url": "https://en.wikipedia.org/wiki?curid=200307", "title": "Michel Rolle", "text": "Michel Rolle\n\nMichel Rolle (21 April 1652 – 8 November 1719) was a French mathematician. He is best known for Rolle's theorem (1691). He is also the co-inventor in Europe of Gaussian elimination (1690).\n\nRolle was born in Ambert, Basse-Auvergne. Rolle, the son of a shopkeeper, received only an elementary education. He married early and as a young man struggled to support his family on the meager wages of a transcriber for notaries and attorney. In spite of his financial problems and minimal education, Rolle studied algebra and Diophantine analysis (a branch of number theory) on this own. He moved from Ambert to Paris in 1675.\n\nRolle’s fortune changed dramatically in 1682 when he published an elegant solution of a difficult, unsolved problem in Diophantine analysis. The public recognition of his achievement led to a patronage under minister Louvois, a job as an elementary mathematics teacher, and eventually to a short-termed administrative post in the Ministry of War. In 1685 he joined the Académie des Sciences in a very low-level position for which he received no regular salary until 1699. Rolle was promoted to a salaried position in the Academy, a \"pensionnaire géometre,\". This was a distinguished post because of the 70 members of the Academy, only 20 were paid. He had then already been given a pension by Jean-Baptiste Colbert after he solved one of Jacques Ozanam's problems. He remained there until he died of apoplexy in 1719.\n\nWhile Rolle’s forté was always Diophantine analysis, his most important work was a book on the algebra of equations, called \"Traité d'algèbre\", published in 1690. In that book Rolle firmly established the notation for the \"n\"th root of a polynomial, and proved a polynomial version of the theorem that today bears his name. (Rolle's theorem was named by Giusto Bellavitis in 1846.)\n\nRolle was one of the most vocal early antagonists of calculus – ironically so, because Rolle's theorem is essential for basic proofs in calculus. He strove intently to demonstrate that it gave erroneous results and was based on unsound reasoning. He quarreled so vehemently on the subject that the Académie des Sciences was forced to intervene on several occasions.\n\nAmong his several achievements, Rolle helped advance the currently accepted size order for negative numbers. Descartes, for example, viewed –2 as smaller than –5. Rolle preceded most of his contemporaries by adopting the current convention in 1691. \n\nRolle died in Paris. No contemporary portrait of him is known.\n\nRolle was an early critic of infinitesimal calculus, arguing that it was inaccurate, based upon unsound reasoning, and was a collection of ingenious fallacies, but later changed his opinion.\nIn 1690, Rolle published \"Traité d'Algebre.\" It contains the first \"published\" description in Europe of the Gaussian elimination algorithm, which Rolle called the method of substitution. Some examples of the method had previously appeared in algebra books, and Isaac Newton had previously described the method in his lecture notes, but Newton's lesson was not published until 1707. Rolle's statement of the method seems not to have been noticed in so far as the lesson for Gaussian elimination that was taught in 18 and 19th century algebra textbooks owes more to Newton than to Rolle.\n\nRolle is best known for Rolle's theorem in differential calculus. Rolle had used the result in 1690, and he proved it (by the standards of the time) in 1691. Given his animosity to infinitesimals it is fitting that the result was couched in terms of algebra rather than analysis. Only in the 18th century was the theorem interpreted as a fundamental result in differential calculus. Indeed, it is needed to prove both the mean value theorem and the existence of Taylor series. As the importance of the theorem grew, so did the interest in identifying the origin, and it was finally named \"Rolle's theorem\" in the 19th century. Barrow-Green remarks that the theorem might well have been named for someone else had not a few copies of Rolle's 1691 publication survived.\n\nIn a criticism of infinitesimal calculus that predated George Berkeley's, Rolle presented a series of papers at the French academy, alleging that the use of the methods of infinitesimal calculus leads to errors. Specifically, he presented an explicit algebraic curve, and alleged that some of its local minima are missed when one applies the methods of infinitesimal calculus. Pierre Varignon responded by pointing out that Rolle had misrepresented the curve, and that the alleged local minima are in fact singular points with a vertical tangent.\n\n\n"}
{"id": "55033847", "url": "https://en.wikipedia.org/wiki?curid=55033847", "title": "Modal fallacy", "text": "Modal fallacy\n\nThe formal fallacy of the modal fallacy is a special type of fallacy that occurs in modal logic. It is the fallacy of placing a proposition in the wrong modal scope, specifically inferring that because something \"is\" true, it is \"necessarily\" true. A statement is considered necessarily true if and only if it is impossible for the statement to be untrue and that there is no situation that would cause the statement to be false. Some philosophers further argue that a necessarily true statement must be true in all possible worlds.\n\nIn modal logic, a proposition formula_1 can be necessarily true or false (denoted formula_2 and formula_3, respectively), meaning that it is logically necessary that it is true or false; or it could be possibly true or false (denoted formula_4 and formula_5), meaning that it is true or false, but it is not logically necessary that it is so: its truth or falseness is \"contingent\". The modal fallacy occurs when there is a confusion of the distinction between the two.\n\nIn modal logic, there is an important distinction between what is logically necessary to be true and what is true but not logically necessary to be so. One common form is replacing formula_6 with formula_7. In the first statement, formula_8 is true given formula_9 but is not logically necessary to be so.\n\nA common example in everyday life might be the following:\nThe conclusion is false, since, even though Donald Trump is over 35 years old, there is no logical necessity for him to be. Even though it is certainly true in this world, a possible world can exist in which Donald Trump is not yet 35 years old. If instead of adding a stipulation of necessity, the argument just concluded that Donald Trump is 35 or older, it would be valid.\n\nNorman Swartz gave the following example of how the modal fallacy can lead one to conclude that the future is already set, regardless of one's decisions; this is based on the \"sea battle\" example used by Aristotle to discuss the problem of future contingents in his \"On Interpretation:\"Two admirals, A and B, are preparing their navies for a sea battle tomorrow. The battle will be fought until one side is victorious. But the 'laws' of the excluded middle (no third truth-value) and of non-contradiction (not both truth-values), mandate that one of the propositions, 'A wins' and 'B wins', is true (always has been and ever will be) and the other is false (always has been and ever will be). Suppose 'A wins' is today true. Then whatever A does (or fails to do) today will make no difference; similarly, whatever B does (or fails to do) today will make no difference: the outcome is already settled. Or again, suppose 'A wins' is today false. Then no matter what A does today (or fails to do), it will make no difference; similarly, no matter what B does (or fails to do), it will make no difference: the outcome is already settled. Thus, if propositions bear their truth-values timelessly (or unchangingly and eternally), then planning, or as Aristotle put it 'taking care', is illusory in its efficacy. The future will be what it will be, irrespective of our planning, intentions, etc.Suppose that the statement \"A wins\" is given by formula_10 and \"B wins\" is given by formula_11. It is true here that only one of the statements \"A wins\" or \"B wins\" must be true. In other words, only one of formula_12 or formula_13 is true. In logic syntax, this is equivalent to\n\nformula_14 (either formula_10 or formula_11 is true)\n\nformula_17 (it is not possible that formula_10 and formula_11 are both true at the same time)\n\nThe fallacy here occurs because one assumes that formula_12 and formula_13 implies formula_22 and formula_23. Thus, one believes that, since one of both events is logically necessarily true, no action by either can change the outcome.\n\nSwartz also argued that the argument from free will suffers from the modal fallacy.\n"}
{"id": "45601973", "url": "https://en.wikipedia.org/wiki?curid=45601973", "title": "Niccolò Guicciardini", "text": "Niccolò Guicciardini\n\nNiccolò Guicciardini Corsi Salviati (born 28 May 1957 in Firenze) is an Italian historian of mathematics. He is a professor at the University of Bergamo, and is known for his studies on the works of Isaac Newton.\n\nGuicciardini obtained his Ph.D. in 1987 under the supervision of Ivor Grattan-Guinness.\n\nIn 2011 he was awarded the Fernando Gil International Prize for the Philosophy of Science.\n\n\n"}
{"id": "48845497", "url": "https://en.wikipedia.org/wiki?curid=48845497", "title": "Number sense in animals", "text": "Number sense in animals\n\nNumber sense in animals is the ability of creatures to represent and discriminate quantities of relative sizes by number sense. It has been observed in various species, from fish to primates. Animals are believed to have an approximate number system, the same system for number representation demonstrated by humans, which is more precise for smaller quantities and less so for larger values. An exact representation of numbers higher than 3 has not been attested in wild animals, but can be demonstrated after a period of training in captive animals.\n\nIn order to distinguish number sense in animals from the symbolic and verbal number system in humans researchers use the term \"numerosity\", rather than \"number\", to refer to the concept that supports approximate estimation but does not support an exact representation of number quality.\n\nNumber sense in animals includes the recognition and comparison of number quantities. Some numerical operations, such as addition, have been demonstrated in many species, including rats and great apes. Representing fractions and fraction addition has been observed in chimpanzees. A wide range of species with an approximate number system suggests an early evolutionary origin of this mechanism or multiple convergent evolution events.\n\nAt the beginning of the 20th century, Wilhelm von Osten famously, but prematurely, claimed human-like counting abilities in animals on the example of his horse named Hans. His claim is widely rejected today, as it is attributed to a methodological fallacy, which received the name Clever Hans phenomenon after this case. Von Osten claimed that his horse could perform arithmetic operations presented to the horse in writing or verbally, upon which the horse would knock on the ground with its hoof the number of times that corresponded to the answer. This apparent ability was demonstrated numerous times in the presence of the horse's owner and a wider audience, and was also observed when the owner was absent. However, upon a rigorous investigation by Oskar Pfungst in the first decade of 20th century, Hans' ability was shown to be not arithmetic in nature, but to be the ability to interpret minimal unconscious changes in body language of people when the correct answer was approaching. Today, the arithmetic abilities of Clever Hans are commonly rejected and the case serves as a reminder to the scientific community about the necessity of rigorous control for experimenter expectation in experiments.\n\nThere were, however, other early and more reliable studies on number sense in animals. A prominent example is the work of Otto Koehler, who conducted a number of studies on number sense in animals between 1920s and 1970s. In one of his studies he showed that a raven named Jacob could reliably distinguish the number 5 across different tasks. This study was remarkable in that Koehler provided a systematic control condition in his experiment, which allowed him to test the number ability of the raven separately from the ability of the raven to encode other features, such as size and location of the objects. However, Koehler's work was largely overlooked in the English-speaking world, due to the limited availability of his publications, which were in German and partially published during World War II.\n\nThe experimental setup for the study of numerical cognition in animals was further enriched by the work of Francis and Platt and Johnson. In their experiments, the researchers deprived rats of food and then taught them to press a lever a specific number of times to obtain food. The rats learned to press the lever approximately the number of times specified by the researchers. Additionally, the researchers showed that rats' behavior was dependent on the number of required presses, and not for example on the time of pressing, as they varied the experiment to include faster and slower behavior on the rat's part by controlling how hungry the animal was.\n\nExamining the representation of numerosity in animals is a challenging task, since it is not possible to use language as a medium. Because of this, carefully designed experimental setups are required to differentiate between numerical abilities and other phenomena, such as the Clever Hans phenomenon, memorization of the single objects or perception of object size and time.Also these abilities are seen only from the past few decades and not form the time of evolution.\n\nOne of the ways that numerical ability is thought to be demonstrated is the transfer of the concept of numerosity across modalities. This was for example the case in the experiment of Church and Meck, in which rats learned to \"add\" the number of light flashes to the number of tones to find out the number of expected lever presses, showing a concept of numerosity independent of visual and auditory modalities.\n\nModern studies in number sense in animals try to control for other possible explanations of animal behavior by establishing control conditions in which the other explanations are tested. For example, when the number sense is investigated on the example of apple pieces, an alternative explanation is tested that assumes that the animal represents the volume of apple rather than a number of apple pieces. To test this alternative, an additional condition is introduced in which the volume of the apple varies and is occasionally smaller in the condition with a greater number of pieces. If the animal prefers a bigger number of pieces also in this condition, the alternative explanation is rejected, and the claim of numerical ability supported.\n\nNumerosity is believed to be represented by two separate systems in animals, similarly to humans. The first system is the approximate number system, an imprecise system used for estimations of quantities. This system is distinguished by distance and magnitude effects, which means that a comparison between numbers is easier and more precise when the distance between them is smaller and when the values of the numbers are smaller. The second system for representation of numerosity is the parallel individuation system which supports the exact representation of numbers from 1 to 4. In addition, humans can represent numbers through symbolic systems, such as language.\n\nThe distinction between the approximate number system and the parallel individuation system is, however, still disputed, and some experiments record behavior that can be fully explained with the approximate number system, without the need to assume another separate system for smaller numbers. For example, New Zealand robins repeatedly selected larger quantities of cached food with a precision that correlated with the total number of cache pieces. However, there was no significant discontinuity in their performance between small (1 to 4) and larger (above 4) sets, which would be predicted by the parallel individuation system. On the other hand, other experiments only report knowledge of numbers up to 4, supporting the existence of the parallel individuation system and not the approximate number system.\n\nAn approximate number system has been found in a number of fish species, such as guppies, green swordtails and mosquitofish. For example, preference for a bigger social group in mosquitofish was exploited to test the ability of the fish to discriminate numerosity. The fish successfully discriminated between different amounts up to three, after which they could discriminate groups if the difference between them also increased so that the ratio of the two groups was 1:2. Similarly, guppies discriminated between values up to 4, after which they only detected differences when the ratio between the two quantities was 1:2.\n\nRats have demonstrated behavior consistent with an approximate number system in experiments where they had to learn to press a lever a specified number of times to obtain food. While they did learn to press the lever the amount specified by the researchers, between 4 and 16, their behavior was approximate, proportional to the number of lever presses expected from them. This means that for the target number of 4, the rats' responses varied from 3 to 7, and for the target number of 16 the responses varied from 12 to 24, showing a much greater interval. This is compatible with the approximate number system and magnitude and distance effects.\n\nBirds were one of the first animal species tested on their number sense. A raven named Jacob was able to distinguish the number 5 across different tasks in the experiments by Otto Koehler. Later experiments supported the claim of existence of a number sense in birds, with Alex, a grey parrot, able to label and comprehend labels for sets with up to six elements. Other studies suggest that pigeons can also represent numbers up to 6 after an extensive training.\n\nA sense of number has also been found in dogs. For example, dogs were able to perform simple additions of two objects, as revealed by their surprise when the result was incorrect. It is however argued that wolves perform better on quantity discrimination tasks than dogs and that this could be a result of a less demanding natural selection for number sense in dogs.\n\nRhesus monkeys seem to have an innate understanding of numbers up to 4. This is shown by the study on semi-free-ranging rhesus monkeys in their natural environment in which the monkeys spontaneously discriminated numbers from 1 to 3 but did not demonstrate a numerical ability beyond this number. In another study that included training, rhesus monkeys showed the ability to generalize this knowledge to numerosities up to nine.\n\nChimpanzees are believed to have a sense of number, albeit an imprecise one. While they can correctly compare numbers such as 6 and 7 by choosing a tray with more pieces of reward, their performance is more error-free when the difference between the numbers is bigger and when the numbers are smaller; that is when the distance and magnitude effects take place. Chimpanzees also seem to have knowledge of proportions. In an experiment by Woodroof and Premack an adult chimpanzee correctly identified and performed operations on exemplars of numbers and proportions presented on a variety of objects, such as apples, wood disks and water containers. For example, when presented with of an apple and full water container, the chimpanzee chose of a wooden disk, suggesting an addition-like ability. The ability to add fractions has only been demonstrated in primates so far.\n\n"}
{"id": "30670886", "url": "https://en.wikipedia.org/wiki?curid=30670886", "title": "Ones' complement", "text": "Ones' complement\n\nThe ones' complement of a binary number is defined as the value obtained by inverting all the bits in the binary representation of the number (swapping 0s for 1s and vice versa). The ones' complement of the number then behaves like the negative of the original number in some arithmetic operations. To within a constant (of −1), the ones' complement behaves like the negative of the original number with binary addition. However, unlike two's complement, these numbers have not seen widespread use because of issues such as the offset of −1, that negating zero results in a distinct negative zero bit pattern, less simplicity with arithmetic borrowing, etc.\n\nA ones' complement system or ones' complement arithmetic is a system in which negative numbers are represented by the inverse of the binary representations of their corresponding positive numbers. In such a system, a number is negated (converted from positive to negative or vice versa) by computing its ones' complement. An N-bit ones' complement numeral system can only represent integers in the range −(2−1) to 2−1 while two's complement can express −2 to 2−1.\n\nThe ones' complement binary numeral system is characterized by the bit complement of any integer value being the arithmetic negative of the value. That is, inverting all of the bits of a number (the logical complement) produces the same result as subtracting the value from 0.\n\nMany early computers, including the CDC 6600, the LINC, the PDP-1, and the UNIVAC 1107, used ones' complement notation. Successors of the CDC 6600 continued to use ones' complement until the late 1980s, and the descendants of the UNIVAC 1107 (the UNIVAC 1100/2200 series) still do, but the majority of modern computers use two's complement.\n\nPositive numbers are the same simple, binary system used by two's complement and sign-magnitude. Negative values are the bit complement of the corresponding positive value. The largest positive value is characterized by the sign (high-order) bit being off (0) and all other bits being on (1). The smallest negative value is characterized by the sign bit being 1, and all other bits being 0. The table below shows all possible values in a 4-bit system, from −7 to +7.\n\nAdding two values is straightforward. Simply align the values on the least significant bit and add, propagating any carry to the bit one position left. If the carry extends past the end of the word it is said to have \"wrapped around\", a condition called an \"end-around carry\". When this occurs, the bit must be added back in at the right-most bit. This phenomenon does not occur in two's complement arithmetic.\n\nSubtraction is similar, except that borrows, rather than carries, are propagated to the left. If the borrow extends past the end of the word it is said to have \"wrapped around\", a condition called an \"end-around borrow\". When this occurs, the bit must be subtracted from the right-most bit. This phenomenon does not occur in two's complement arithmetic.\n\nIt is easy to demonstrate that the bit complement of a positive value is the negative magnitude of the positive value. The computation of 19 + 3 produces the same result as 19 − (−3).\n\nAdd 3 to 19.\n\nSubtract −3 from 19.\n\nNegative zero is the condition where all bits in a signed word are 1. This follows the ones' complement rules that a value is negative when the left-most bit is 1, and that a negative number is the bit complement of the number's magnitude. The value also behaves as zero when computing. Adding or subtracting negative zero to/from another value produces the original value.\n\nAdding negative zero:\n\nSubtracting negative zero:\n\nNegative zero is easily produced in a 1's complement adder. Simply add the positive and negative of the same magnitude.\n\nAlthough the math always produces the correct results, a side effect of negative zero is that software must test for negative zero.\n\nThe generation of negative zero becomes a non-issue if addition is achieved with a complementing subtractor. The first operand is passed to the subtract unmodified, the second operand is complemented, and the subtraction generates the correct result, avoiding negative zero. The previous example added 22 and −22 and produced −0.\n\n\"Corner cases\" arise when one or both operands are zero and/or negative zero.\n\nSubtracting +0 is trivial (as shown above). If the second operand is negative zero it is inverted and the original value of the first operand is the result. Subtracting −0 is also trivial. The result can be only 1 of two cases. In case 1, operand 1 is −0 so the result is produced simply by subtracting 1 from 1 at every bit position. In case 2, the subtraction will generate a value that is 1 larger than operand 1 and an end-around borrow. Completing the borrow generates the same value as operand 1.\n\nThe next example shows what happens when both operands are plus or minus zero:\n\nThis example shows that of the 4 possible conditions when adding only ±0, an adder will produce −0 in three of them. A complementing subtractor will produce −0 only when both operands are −0.\n\n\n"}
{"id": "16706608", "url": "https://en.wikipedia.org/wiki?curid=16706608", "title": "Phase field models", "text": "Phase field models\n\nA phase field model is a mathematical model for solving interfacial problems. It has mainly been applied to solidification dynamics, but it has also been applied to other situations such as viscous fingering, fracture dynamics, vesicle dynamics, etc.\n\nThe method substitutes boundary conditions at the interface by a partial differential equation for the evolution of an auxiliary field (the phase field) that takes the role of an order parameter. This phase field takes two distinct values (for instance +1 and −1) in each of the phases, with a smooth change between both values in the zone around the interface, which is then diffuse with a finite width. A discrete location of the interface may be defined as the collection of all points where the phase field takes a certain value (e.g., 0). \n\nA phase field model is usually constructed in such a way that in the limit of an infinitesimal interface width (the so-called sharp interface limit) the correct interfacial dynamics are recovered. This approach permits to solve the problem by integrating a set of partial differential equations for the whole system, thus avoiding the explicit treatment of the boundary conditions at the interface.\n\nPhase field models were first introduced by Fix and Langer, and have experienced a growing interest in solidification and other areas.\n\nPhase field models are usually constructed in order to reproduce a given interfacial dynamics. For instance, in solidification problems the front dynamics is given by a diffusion equation for either concentration or temperature in the bulk and some boundary conditions at the interface (a local equilibrium condition and a conservation law), which constitutes the sharp interface model.\nA number of formulations of the phase field model are based on a free energy function depending on an order parameter (the phase field) and a diffusive field (variational formulations). Equations of the model are then obtained by using general relations of statistical physics. Such a function is constructed from physical considerations, but contains a parameter or combination of parameters related to the interface width. Parameters of the model are then chosen by studying the limit of the model with this width going to zero, in such a way that one can identify this limit with the intended sharp interface model.\n\nOther formulations start by writing directly the phase field equations, without referring to any thermodynamical functional (non-variational formulations). In this case the only reference is the sharp interface model, in the sense that it should be recovered when performing the small interface width limit of the phase field model.\n\nPhase field equations in principle reproduce the interfacial dynamics when the interface width is small compared with the smallest length scale in the problem. In solidification this scale is the capillary length formula_1, which is a microscopic scale. From a computational point of view integration of partial differential equations resolving such a small scale is prohibitive. However, Karma and Rappel introduced the thin interface limit, which permitted to relax this condition and has opened the way to practical quantitative simulations with phase field models.\nWith the increasing power of computers and the theoretical progress in phase field modelling, phase field models have become a useful tool for the numerical simulation of interfacial problems.\n\nA model for a phase field can be constructed by physical arguments if one has an explicit expression for the free energy of the system. A simple example for solidification problems is the following:\n\nformula_2\n\nwhere formula_3 is the phase field, formula_4, formula_5 is the local enthalpy per unit volume, formula_6 is a certain polynomial function of formula_7, and formula_8 (where formula_9 is the latent heat, formula_10 is the melting temperature, and formula_11 is the specific heat). The term with formula_12 corresponds to the interfacial energy. The function formula_13 is usually taken as a double-well potential describing the free energy density of the bulk of each phase, which themselves correspond to the two minima of the function formula_13. The constants formula_15 and formula_16 have respectively dimensions of energy per unit length and energy per unit volume. The interface width is then given by formula_17. \nThe phase field model can then be obtained from the following variational relations:\n\nformula_18\n\nformula_19\n\nwhere D is a diffusion coefficient for the variable formula_5, and formula_21 and formula_22 are stochastic terms accounting for thermal fluctuations (and whose statistical properties can be obtained from the fluctuation dissipation theorem). The first equation gives an equation for the evolution of the phase field, whereas the second one is a diffusion equation, which usually is rewritten for the temperature or for the concentration (in the case of an alloy). These equations are, scaling space with formula_23 and times with formula_24:\n\nformula_25\n\nformula_26\n\nwhere formula_27 is the nondimensional interface width, formula_28, and formula_29, formula_30 are nondimensionalized noises.\n\nThe choice of free energy function, formula_13, can have a significant effect on the physical behaviour of the interface, and should be selected with care. The double-well function represents an approximation of the Van der Waals EOS near the critical point, and has historically been used for its simplicity of implementation when the phase field model is employed solely for interface tracking purposes. However, this has led to the frequently observed spontaneous drop shrinkage phenomenon, whereby the high phase miscibility predicted by an Equation of State near the critical point allows significant interpenetration of the phases and can eventually lead to the complete disappearance of a droplet whose radius is below some critical value. Minimizing perceived continuity losses over the duration of a simulation requires limits on the Mobility parameter, resulting in a delicate balance between interfacial smearing due to convection, interfacial reconstruction due to free energy minimization (i.e. mobility-based diffusion), and phase interpenetration, also dependent on the mobility. A recent review of alternative energy density functions for interface tracking applications has proposed a modified form of the double-obstacle function which avoids the spontaneous drop shrinkage phenomena and limits on mobility, with comparative results provide for a number of benchmark simulations using the double-well function and the VOF sharp interface technique. The proposed implementation has a computational complexity only slightly greater than that of the double-well function, and may prove useful for interface tracking applications of the phase field model where the duration/nature of the simulated phenomena introduces phase continuity concerns (i.e. small droplets, extended simulations, multiple interfaces, etc.).\n\nA phase field model can be constructed to purposely reproduce a given interfacial dynamics as represented by a sharp interface model. In such a case the sharp interface limit (i.e. the limit when the interface width goes to zero) of the proposed set of phase field equations should be performed. This limit is usually taken by asymptotic expansions of the fields of the model in powers of the interface width formula_32. These expansions are performed both in the interfacial region (inner expansion) and in the bulk (outer expansion), and then are asymptotically matched order by order. The result gives a partial differential equation for the diffusive field and a series of boundary conditions at the interface, which should correspond to the sharp interface model and whose comparison with it provides the values of the parameters of the phase field model.\n\nWhereas such expansions were in early phase field models performed up to the lower order in formula_32 only, more recent models use higher order asymptotics (thin interface limits) in order to cancel undesired spureous effects or to include new physics in the model. For example, this technique has permitted to cancel kinetic effects, to treat cases with unequal diffusivities in the phases, to model viscous fingering and two-phase Navier–Stokes flows, to include fluctuations in the model, etc.\n\nIn multi-phase field models, microstructure is described by set of order parameters, each of which is related to a specific phase or crystallographic orientation. This model is mostly used for solid state phase transformations where multiple grains evolve (e.g. grain growth, recrystallization or first order transformation like austenite to ferrite in ferrous alloys). Besides allowing the description of multiple grains in a microstructure, multi-phase field models especially allow for consideration of multiple thermodynamic phases occurring e.g. in technical alloy grades.\n\n\n"}
{"id": "2006896", "url": "https://en.wikipedia.org/wiki?curid=2006896", "title": "Pseudorandom binary sequence", "text": "Pseudorandom binary sequence\n\nA pseudorandom binary sequence (PRBS) is a binary sequence that, while generated with a deterministic algorithm, is difficult to predict and exhibits statistical behavior similar to a truly random sequence. PRBS are used in telecommunication, encryption, simulation, correlation technique and time-of-flight spectroscopy.\n\nA binary sequence (BS) is a sequence formula_1 of formula_2 bits, i.e.\n\nA BS consists of formula_5 ones and formula_6 zeros.\n\nA BS is a pseudorandom binary sequence (PRBS) if its autocorrelation function, given by\n\nhas only two values:\n\nwhere\n\nis called the \"duty cycle\" of the PRBS, similar to the duty cycle of a continuous time signal. For a maximum length sequence, where formula_10, the duty cycle is 1/2.\n\nA PRBS is 'pseudorandom', because, although it is in fact deterministic, it seems to be random in a sense that the value of an formula_11 element is independent of the values of any of the other elements, similar to real random sequences.\n\nA PRBS can be stretched to infinity by repeating it after formula_2 elements, but it will then be cyclical and thus non-random. In contrast, truly random sequence sources, such as sequences generated by radioactive decay or by white noise, are infinite (no pre-determined end or cycle-period). However, as a result of this predictability, PRBS signals can be used as reproducible patterns (for example, signals used in testing telecommunications signal paths).\n\nPseudorandom binary sequences can be generated using linear feedback shift registers.\n\nSome common sequence generating monic polynomials are\n\nAn example of generating a \"PRBS-7\" sequence can be expressed in C as\n\nIn this particular case, \"PRBS-7\" has a repetition period of 127 bits.\n\nThe PRBS\"k\" or PRBS-\"k\" notation (such as \"PRBS7\" or \"PRBS-7\") gives an indication of the size of the sequence. formula_10 is the maximum number of bits that be in the sequence. The \"k\" indicates the size of a unique word of data in the sequence. If you segment the \"N\" bits of data into every possible word of length \"k\", you will be able to list every possible combination of 0s and 1s for a k-bit binary word, with the exception of the all-0s word. For example, PRBS3 = \"1011100\" could be generated from formula_21. If you take every sequential group of three bit words in the PRBS3 sequence (wrapping around to the beginning for the last few three-bit words), you will find the following 7 words:\nThose 7 words are all of the formula_22 possible non-zero 3-bit binary words, not in numeric order. The same holds true for any PRBS\"k\", not just PRBS3.\n\n\n"}
{"id": "2508505", "url": "https://en.wikipedia.org/wiki?curid=2508505", "title": "Pythagoras tree (fractal)", "text": "Pythagoras tree (fractal)\n\nThe Pythagoras tree is a plane fractal constructed from squares. Invented by the Dutch mathematics teacher Albert E. Bosman in 1942, it is named after the ancient Greek mathematician Pythagoras because each triple of touching squares encloses a right triangle, in a configuration traditionally used to depict the Pythagorean theorem.\nIf the largest square has a size of \"L\" × \"L\", the entire Pythagoras tree fits snugly inside a box of size 6\"L\" × 4\"L\". The finer details of the tree resemble the Lévy C curve.\n\nThe construction of the Pythagoras tree begins with a square. Upon this square are constructed two squares, each scaled down by a linear factor of /2, such that the corners of the squares coincide pairwise. The same procedure is then applied recursively to the two smaller squares, \"ad infinitum\". The illustration below shows the first few iterations in the construction process.\n\nIteration \"n\" in the construction adds 2 squares of area formula_1, for a total area of 1. Thus the area of the tree might seem to grow without bound in the limit as \"n\" → ∞. However, some of the squares overlap starting at the order 5 iteration, and the tree actually has a finite area because it fits inside a 6×4 box.\n\nIt can be shown easily that the area \"A\" of the Pythagoras tree must be in the range 5 < \"A\" < 18, which can be narrowed down further with extra effort. Little seems to be known about the actual value of \"A\".\n\nAn interesting set of variations can be constructed by maintaining an isosceles triangle but changing the base angle (90 degrees for the standard Pythagoras tree). In particular, when the base half-angle is set to (30°) = arcsin(0.5), it is easily seen that the size of the squares remains constant. The first overlap occurs at the fourth iteration. The general pattern produced is the rhombitrihexagonal tiling, an array of hexagons bordered by the constructing squares.\nIn the limit where the half-angle is 90 degrees, there is obviously no overlap, and the total area is twice the area of the base square. It would be interesting to know if there's an algorithmic relationship between the value of the base half-angle and the iteration at which the squares first overlap each other.\n\nPythagoras tree was first constructed by Albert E. Bosman (1891–1961), Dutch mathematics teacher, in 1942.\n\n\n"}
{"id": "35887507", "url": "https://en.wikipedia.org/wiki?curid=35887507", "title": "Representer theorem", "text": "Representer theorem\n\nIn statistical learning theory, a representer theorem is any of several related results stating that a minimizer formula_1 of a regularized empirical risk function defined over a reproducing kernel Hilbert space can be represented as a finite linear combination of kernel products evaluated on the input points in the training set data.\n\nThe following Representer Theorem and its proof are due to Schölkopf, Herbrich, and Smola:\n\nTheorem: Let formula_2 be a nonempty set and formula_3 a positive-definite real-valued kernel on formula_4 with corresponding reproducing kernel Hilbert space formula_5. Given a training sample formula_6, a strictly monotonically increasing real-valued function formula_7, and an arbitrary empirical risk function formula_8, then for any formula_9 satisfying\n\nformula_1 admits a representation of the form:\n\nwhere formula_13 for all formula_14.\n\nProof:\nDefine a mapping\n\n(so that formula_16 is itself a map formula_17). Since formula_3 is a reproducing kernel, then\n\nwhere formula_20 is the inner product on formula_5.\n\nGiven any formula_22, one can use orthogonal projection to decompose any formula_23 into a sum of two functions, one lying in formula_24, and the other lying in the orthogonal complement:\n\nwhere formula_26 for all formula_27.\n\nThe above orthogonal decomposition and the reproducing property together show that applying formula_28 to any training point formula_29 produces\n\nwhich we observe is independent of formula_31. Consequently, the value of the empirical risk formula_32 in (*) is likewise independent of formula_31. For the second term (the regularization term), since formula_31 is orthogonal to formula_35 and formula_36 is strictly monotonic, we have\n\nTherefore setting formula_38 does not affect the first term of (*), while it strictly decreasing the second term. Consequently, any minimizer formula_1 in (*) must have formula_38, i.e., it must be of the form\n\nwhich is the desired result.\n\nThe Theorem stated above is a particular example of a family of results that are collectively referred to as \"representer theorems\"; here we describe several such.\n\nThe first statement of a representer theorem was due to Kimeldorf and Wahba for the special case in which\n\nfor formula_43. Schölkopf, Herbrich, and Smola generalized this result by relaxing the assumption of the squared-loss cost and allowing the regularizer to be any strictly monotonically increasing function formula_44 of the Hilbert space norm.\n\nIt is possible to generalize further by augmenting the regularized empirical risk function through the addition of unpenalized offset terms. For example, Schölkopf, Herbrich, and Smola also consider the minimization\n\ni.e., we consider functions of the form formula_46, where formula_23 and formula_48 is an unpenalized function lying in the span of a finite set of real-valued functions formula_49. Under the assumption that the formula_50 matrix formula_51 has rank formula_52, they show that the minimizer formula_53 in formula_54\nadmits a representation of the form\n\nwhere formula_56 and the formula_57 are all uniquely determined.\n\nThe conditions under which a representer theorem exists were investigated by Argyriou, Miccheli, and Pontil, who proved the following:\n\nTheorem: Let formula_2 be a nonempty set, formula_3 a positive-definite real-valued kernel on formula_4 with corresponding reproducing kernel Hilbert space formula_5, and let formula_62 be a differentiable regularization function. Then given a training sample formula_63 and an arbitrary empirical risk function formula_64, a minimizer\n\nof the regularized empirical risk minimization problem admits a representation of the form\n\nwhere formula_13 for all formula_14, if and only if there exists a nondecreasing function formula_69 for which\n\nEffectively, this result provides a necessary and sufficient condition on a differentiable regularizer formula_71 under which the corresponding regularized empirical risk minimization formula_72 will have a representer theorem. In particular, this shows that a broad class of regularized risk minimizations (much broader than those originally considered by Kimeldorf and Wahba) have representer theorems.\n\nRepresenter theorems are useful from a practical standpoint because they dramatically simplify the regularized empirical risk minimization problem formula_72. In most interesting applications, the search domain formula_5 for the minimization will be an infinite-dimensional subspace of formula_75, and therefore the search (as written) does not admit implementation on finite-memory and finite-precision computers. In contrast, the representation of formula_76 afforded by a representer theorem reduces the original (infinite-dimensional) minimization problem to a search for the optimal formula_77-dimensional vector of coefficients formula_78; formula_79 can then be obtained by applying any standard function minimization algorithm. Consequently, representer theorems provide the theoretical basis for the reduction of the general machine learning problem to algorithms that can actually be implemented on computers in practice.\n\n\n"}
{"id": "650022", "url": "https://en.wikipedia.org/wiki?curid=650022", "title": "Richardson extrapolation", "text": "Richardson extrapolation\n\nIn numerical analysis, Richardson extrapolation is a sequence acceleration method, used to improve the rate of convergence of a sequence. It is named after Lewis Fry Richardson, who introduced the technique in the early 20th century. In the words of Birkhoff and Rota, \"its usefulness for practical computations can hardly be overestimated.\"\n\nPractical applications of Richardson extrapolation include Romberg integration, which applies Richardson extrapolation to the trapezoid rule, and the Bulirsch–Stoer algorithm for solving ordinary differential equations.\n\nSuppose that we wish to approximate formula_1, and we have a method formula_2 that depends on a small parameter formula_3 in such a way that\n\nformula_4\n\nLet's define a new function \n\nformula_5\nwhere formula_3 and formula_7 are two distinct step sizes.\n\nThen\n\nformula_8\n\nformula_9 is called the Richardson extrapolation of \"A\"(\"h\"), and has a higher-order\nerror estimate formula_10 compared to formula_11.\n\nVery often, it is much easier to obtain a given precision by using \"R(h)\" rather\nthan \"A(h')\" with a much smaller \" h' \", which can cause problems due to limited precision (rounding errors) and/or due to the increasing number of calculations needed (see examples below).\n\nLet \"formula_2\" be an approximation of \"formula_1\"(exact value) that depends on a positive step size \"h\" with an error formula of the form \nwhere the \"a\" are unknown constants and the \"k\" are known constants such that \"h\" > \"h\".\n\n\"k\" is the leading order step size behavior of Truncation error as formula_15\n\nThe exact value sought can be given by\nwhich can be simplified with Big O notation to be\n\nUsing the step sizes \"h\" and \"h / t\" for some \"t\", the two formulas for \"A\" are:\n\nMultiplying the second equation by \"t\" and subtracting the first equation gives\nwhich can be solved for formula_1 to give\n\nTherefore using formula_23 the truncation error has been reduced to \nformula_24. This is in contrast to formula_2 where the truncation error is formula_26 for the same step size formula_3\n\nBy this process, we have achieved a better approximation of \"A\" by subtracting the largest term in the error which was \"O\"(\"h\"). This process can be repeated to remove more error terms to get even better approximations.\n\nA general recurrence relation beginning with formula_28 can be defined for the approximations by\nwhere formula_30 satisfies\nThe Richardson extrapolation can be considered as a linear sequence transformation.\n\nAdditionally, the general formula can be used to estimate \"k\" (leading order step size behavior of Truncation error) when neither its value nor \"A\" (exact value) is known \"a priori\". Such a technique can be useful for quantifying an unknown rate of convergence. Given approximations of \"A\" from three distinct step sizes \"h\", \"h / t\", and \"h / s\", the exact relationship\nyields an approximate relationship (please note that the notation here may cause a bit of confusion, the two O appearing in the equation above only indicates the leading order step size behavior but their explicit forms are different and hence cancelling out of the two O terms is approximately valid)\nwhich can be solved numerically to estimate \"k\".\n\nThe following pseudocode in MATLAB style demonstrates Richardson extrapolation to help solve the ODE formula_34, formula_35 with the Trapezoidal method. In this example we halve the step size formula_3 each iteration and so in the discussion above we'd have that formula_37. The error of the Trapezoidal method can be expressed in terms of odd powers so that the error over multiple steps can be expressed in even powers; this leads us to raise formula_38 to the second power and to take powers of formula_39 in the pseudocode. We want to find the value of formula_40, which has the exact solution of formula_41 since the exact solution of the ODE is formula_42. This pseudocode assumes that a function called codice_1 exists which attempts to computes codice_2 by performing the trapezoidal method on the function codice_3, with starting point codice_4 and codice_5 and step size codice_6. \n\nNote that starting with too small an initial step size can potentially introduce error into the final solution. Although there are methods designed to help pick the best initial step size, one option is to start with a large step size and then to allow the Richardson extrapolation to reduce the step size each iteration until the error reaches the desired tolerance. \n\n\n\n"}
{"id": "236798", "url": "https://en.wikipedia.org/wiki?curid=236798", "title": "Roger Cotes", "text": "Roger Cotes\n\nRoger Cotes FRS (10 July 1682 – 5 June 1716) was an English mathematician, known for working closely with Isaac Newton by proofreading the second edition of his famous book, the \"Principia\", before publication. He also invented the quadrature formulas known as Newton–Cotes formulas and first introduced what is known today as Euler's formula. He was the first Plumian Professor at Cambridge University from 1707 until his death.\n\nCotes was born in Burbage, Leicestershire. His parents were Robert, the rector of Burbage, and his wife, Grace, \"née\" Farmer. Roger had an elder brother, Anthony (born 1681), and a younger sister, Susanna (born 1683), both of whom died young. At first Roger attended Leicester School, where his mathematical talent was recognised. His aunt Hannah had married Rev. John Smith, and Smith took on the role of tutor to encourage Roger's talent. The Smiths' son, Robert Smith, would become a close associate of Roger Cotes throughout his life. Cotes later studied at St Paul's School in London and entered Trinity College, Cambridge, in 1699. He graduated BA in 1702 and MA in 1706.\n\nRoger Cotes's contributions to modern computational methods lie heavily in the fields of astronomy and mathematics. Cotes began his educational career with a focus on astronomy. He became a fellow of Trinity College in 1707, and at age 26 he became the first Plumian Professor of Astronomy and Experimental Philosophy. On his appointment to professor, he opened a subscription list in an effort to provide an observatory for Trinity. Unfortunately, the observatory still was unfinished when Cotes died, and was demolished in 1797.\n\nIn correspondence with Isaac Newton, Cotes designed a heliostat telescope with a mirror revolving by clockwork. He recomputed the solar and planetary tables of Giovanni Domenico Cassini and John Flamsteed, and he intended to create tables of the moon's motion, based on Newtonian principles. Finally, in 1707 he formed a school of physical sciences at Trinity in partnership with William Whiston.\n\nFrom 1709 to 1713, Cotes became heavily involved with the second edition of Newton's \"Principia\", a book that explained Newton's theory of universal gravitation. The first edition of \"Principia\" had only a few copies printed and was in need of revision to include Newton's works and principles of lunar and planetary theory. Newton at first had a casual approach to the revision, since he had all but given up scientific work. However, through the vigorous passion displayed by Cotes, Newton's scientific hunger was once again reignited. The two spent nearly three and half years collaborating on the work, in which they fully deduce, from Newton's laws of motion, the theory of the moon, the equinoxes, and the orbits of comets. Only 750 copies of the second edition were printed. However, a pirate copy from Amsterdam met all other demand. As reward to Cotes, he was given a share of the profits and 12 copies of his own. Cotes's original contribution to the work was a preface which supported the scientific superiority of Newton's principles over the then popular vortex theory of gravity advocated by René Descartes. Cotes concluded that the Newton's law of gravitation was confirmed by observation of celestial phenomena that were inconsistent with the vortex phenomena that Cartesian critics alleged.\n\nCotes's major original work was in mathematics, especially in the fields of integral calculus, logarithms, and numerical analysis. He published only one scientific paper in his lifetime, titled \"Logometria\", in which he successfully constructs the logarithmic spiral. After his death, many of Cotes's mathematical papers were hastily edited by his cousin Robert Smith and published in a book, \"Harmonia mensurarum\". Cotes's additional works were later published in Thomas Simpson's \"The Doctrine and Application of Fluxions\". Although Cotes's style was somewhat obscure, his systematic approach to integration and mathematical theory was highly regarded by his peers. Cotes discovered an important theorem on the \"n\"-th roots of unity, foresaw the method of least squares, and he discovered a method for integrating rational fractions with binomial denominators. He was also praised for his efforts in numerical methods, especially in interpolation methods and his table construction techniques. He was regarded as one of the few British mathematicians capable of following the powerful work of Sir Isaac Newton.\n\nCotes died from a violent fever in Cambridge in 1716 at the early age of 33. Isaac Newton remarked, \"If he had lived we would have known something.\"\n\n\n\n"}
{"id": "18132644", "url": "https://en.wikipedia.org/wiki?curid=18132644", "title": "Signomial", "text": "Signomial\n\nA signomial is an algebraic function of one or more independent variables. It is perhaps most easily thought of as an algebraic extension of multi-dimensional polynomials—an extension that permits exponents to be arbitrary real numbers (rather than just non-negative integers) while requiring the independent variables to be strictly positive (so that division by zero and other inappropriate algebraic operations are not encountered).\n\nFormally, let formula_1 be a vector of real, positive numbers.\n\nThen a signomial function has the form\n\nwhere the coefficients formula_4 and the exponents formula_5 are real numbers. Signomials are closed under addition, subtraction, multiplication, and scaling.\n\nIf we restrict all formula_6 to be positive, then the function f is a posynomial. Consequently, each signomial is either a posynomial, the negative of a posynomial, or the difference of two posynomials. If, in addition, all exponents formula_5 are non-negative integers, then the signomial becomes a polynomial whose domain is the positive orthant.\n\nFor example, \n\nis a signomial. \n\nThe term \"signomial\" was introduced by Richard J. Duffin and Elmor L. Peterson in their seminal joint work on general algebraic optimization—published in the late 1960s and early 1970s. A recent introductory exposition involves optimization problems. Although nonlinear optimization problems with constraints and/or objectives defined by signomials are normally harder to solve than those defined by only posynomials (because, unlike posynomials, signomials are not guaranteed to be globally convex), signomial optimization problems often provide a much more accurate mathematical representation of real-world nonlinear optimization problems.\n\n"}
{"id": "32126054", "url": "https://en.wikipedia.org/wiki?curid=32126054", "title": "Tomasz Łuczak", "text": "Tomasz Łuczak\n\nTomasz Łuczak (born 13 March 1963 in Poznań) is a Polish mathematician and professor at Adam Mickiewicz University in Poznań and Emory University. His main field of research is combinatorics, specifically discrete structures, such as random graphs, and their chromatic number.\n\nUnder supervision of Michał Karoński, Łuczak earned his doctorate at Adam Mickiewicz University in Poznań in 1987. In 1992, he was awarded the EMS Prize and in 1997 he received the prestigious Prize of the Foundation for Polish Science for his work on the theory of random discrete structures.\n\n"}
{"id": "13853397", "url": "https://en.wikipedia.org/wiki?curid=13853397", "title": "Twisted: The Distorted Mathematics of Greenhouse Denial", "text": "Twisted: The Distorted Mathematics of Greenhouse Denial\n\nTwisted: The Distorted Mathematics of Greenhouse Denial is a 2007 book by Ian G. Enting, who is the Professorial Research Fellow in the ARC Centre of Excellence for Mathematics and Statistics of Complex Systems (MASCOS) based at the University of Melbourne.\n\nThe book analyses the arguments of greenhouse sceptics and the use and presentation of statistics. Enting contends there are contradictions in the various arguments of the climate change sceptics.\n\nThe author also presents calculations of the actual emission levels that would be required to stabilise CO concentrations. This is an update of calculations that he contributed to the pre-Kyoto IPCC report on Radiative Forcing of Climate.\n\n\n"}
{"id": "3647181", "url": "https://en.wikipedia.org/wiki?curid=3647181", "title": "Type class", "text": "Type class\n\nIn computer science, a type class is a type system construct that supports ad hoc polymorphism. This is achieved by adding constraints to type variables in parametrically polymorphic types. Such a constraint typically involves a type class codice_1 and a type variable codice_2, and means that codice_2 can only be instantiated to a type whose members support the overloaded operations associated with codice_1.\n\nType classes first appeared in the Haskell programming language, and were originally conceived as a way of implementing overloaded arithmetic and equality operators in a principled fashion.\nIn contrast with the \"eqtypes\" of Standard ML, overloading the equality operator through the use of type classes in Haskell does not require extensive modification of the compiler frontend or the underlying type system.\n\nSince their creation, many other applications of type classes have been discovered.\n\nThe programmer defines a type class by specifying a set of function or constant names, together with their respective types, that must exist for every type that belongs to the class. In Haskell, types can be parameterized; a type class codice_5 intended to contain types that admit equality would be declared in the following way:\nclass Eq a where\nThe type variable codice_2 has kind formula_1 (also known as codice_7 in the latest GHC release), meaning that the kind of codice_5 is\nEq :: Type -> Constraint\nThe declaration may be read as stating a \"type codice_2 belongs to type class codice_5 if there are functions named codice_11, and codice_12, of the appropriate types, defined on it.\" A programmer could then define a function codice_13 (which determines if an element is in a list) in the following way:\nelem :: Eq a => a -> [a] -> Bool\nelem y [] = False\nelem y (x:xs) = (x == y) || elem y xs\nThe function codice_13 has the type codice_15 with the context codice_16, which constrains the types which codice_2 can range over to those codice_2 which belong to the codice_5 type class. (\"Note\": Haskell codice_20 can be called a 'class constraint'.)\n\nA programmer can make any type codice_21 a member of a given type class codice_22 by using an \"instance declaration\" that defines implementations of all of codice_22's methods for the particular type codice_21. For instance, if a programmer defines a new data type codice_21, they may then make this new type an instance of codice_5 by providing an equality function over values of type codice_21 in whatever way they see fit. Once they have done this, they may use the function codice_13 on codice_29, that is, lists of elements of type codice_21.\n\nNote that type classes are different from classes in object-oriented programming languages. In particular, codice_5 is not a type: there is no such thing as a \"value\" of type codice_5.\n\nType classes are closely related to parametric polymorphism. For example, note that the type of codice_13 as specified above would be the parametrically polymorphic type codice_15 were it not for the type class constraint \"codice_35\".\n\nA type class need not take a type variable of kind codice_7 but can take one of any kind. These type classes with higher kinds are sometimes called constructor classes (the constructors referred to are type constructors such as codice_37, rather than data constructors such as codice_38). An example is the codice_39 class:\nclass Monad m where\nThe fact that m is applied to a type variable indicates that it has kind codice_40, i.e. it takes a type and returns a type, the kind of codice_39 is thus:\nMonad :: (Type -> Type) -> Constraint\nType classes permit multiple type parameters, and so type classes can be seen as relations on types. For example, in the GHC standard library, the class codice_42 expresses a general immutable array interface. In this class, the type class constraint codice_43 means that codice_2 is an array type that contains elements of type codice_45. (This restriction on polymorphism is used to implement unboxed array types, for example.)\n\nLike multimethods, multi-parameter type classes support calling different implementations of a method depending on the types of multiple arguments, and indeed return types. Multi-parameter type classes do not require searching for the method to call on every call at runtime; rather the method to call is first compiled and stored in the dictionary of the type class instance, just as with single-parameter type classes.\n\nHaskell code that uses multi-parameter type classes is not portable, as this feature is not part of the Haskell 98 standard. The popular Haskell implementations, GHC and Hugs, support multi-parameter type classes.\n\nIn Haskell, type classes have been refined to allow the programmer to declare functional dependencies between type parameters—a concept inspired from relational database theory. That is, the programmer can assert that a given assignment of some subset of the type parameters uniquely determines the remaining type parameters. For example, general monads codice_46 which carry a state parameter of type codice_47 satisfy the type class constraint codice_48. In this constraint, there is a functional dependency codice_49. This means that for a given monad, the state type accessible from this interface is uniquely determined. This aids the compiler in type inference, as well as aiding the programmer in type-directed programming.\n\nSimon Peyton-Jones has objected to the introduction of functional dependencies in Haskell on grounds of complexity.\n\nType classes and implicit parameters are very similar in nature, although not quite the same. A polymorphic function with a type class constraint such as:\n\ncan be intuitively treated as a function that implicitly accepts an instance of codice_50:\n\nThe instance codice_51 is essentially a record that contains the instance definition of codice_52. (This is in fact how type classes are implemented under the hood by the Glasgow Haskell Compiler.)\n\nHowever, there is a crucial difference: implicit parameters are more \"flexible\" – you can pass different instances of codice_53. In contrast, type classes enforce the so-called \"coherence\" property, which requires that there should only be one unique choice of instance for any given type. The coherence property makes type classes somewhat antimodular, which is why orphan instances (instances that are defined in a module that neither contains the class nor the type of interest) are strongly discouraged. On the other hand, coherence adds an additional level of safety to the language, providing the programmer a guarantee that two disjoint parts of the same code will share the same instance.\n\nAs an example, an ordered set (of type codice_54) requires a total ordering on the elements (of type codice_2) in order to function. This can be evidenced by a constraint codice_56, which defines a comparison operator on the elements. However, there can be numerous ways to impose a total order. Since set algorithms are generally intolerant of changes in the ordering once a set has been constructed, passing an incompatible instance of codice_56 to functions that operate on the set may lead to incorrect results (or crashes). Thus, enforcing coherence of codice_56 in this particular scenario is crucial.\n\nInstances (or \"dictionaries\") in Scala type classes are just ordinary values in the language, rather than a completely separate kind of entity. While these instances are by default supplied by finding appropriate instances in scope to be used as the implicit actual parameters for explicitly-declared implicit formal parameters, the fact that they are ordinary values means that they can be supplied explicitly, to resolve ambiguity. As a result, Scala type classes do not satisfy the coherence property and are effectively a syntactic sugar for implicit parameters.\n\nThis is an example taken from the Cats documentation:\n// A type class to provide textual representation\ntrait Show[A] {\n\n// A polymorphic function that works only when there is an implicit \n// instance of Show[A] available\ndef log[A](a: A)(implicit s: Show[A]) = println(s.show(a))\n\n// An instance for String\nimplicit val stringShow = new Show[String] {\n\n// The parameter stringShow was inserted by the compiler.\nscala> log(\"a string\")\na string\n\nCoq (version 8.2 onwards) also supports type classes by inferring the appropriate instances. Recent versions of Agda 2 also provide a similar feature, called \"instance arguments\".\n\nIn Standard ML, the mechanism of \"equality types\" corresponds roughly to Haskell's built-in type class codice_5, but all equality operators are derived automatically by the compiler. The programmer's control of the process is limited to designating which type components in a structure are equality types and which type variables in a polymorphic type range over equality types.\n\nSML's and OCaml's modules and functors can play a role similar to that of Haskell's type classes, the principal difference being the role of type inference, which makes type classes suitable for \"ad hoc\" polymorphism.\nThe object oriented subset of OCaml is yet another approach which is somewhat comparable to the one of type classes.\n\nAn analogous notion for overloaded data (implemented in GHC) is that of type family.\n\nRust supports traits, which are a limited form of type classes with coherence.\n\nMercury has typeclasses, although they are not exactly the same as in Haskell. \n\nIn Scala, type classes are a programming idiom which can be implemented with existing language features such as implicit parameters, not a separate language feature per se. Because of the way they are implemented in Scala, it is possible to explicitly specify which type class instance to use for a type at a particular place in the code, in case of ambiguity. However, this is not necessarily a benefit as ambiguous type class instances can be error-prone.\n\nThe proof assistant Coq has also supported type classes in recent versions. Unlike in ordinary programming languages, in Coq, any laws of a type class (such as the monad laws) that are stated within the type class definition, must be mathematically proved of each type class instance before using them.\n\n\n"}
{"id": "145018", "url": "https://en.wikipedia.org/wiki?curid=145018", "title": "Ultrafinitism", "text": "Ultrafinitism\n\nIn the philosophy of mathematics, ultrafinitism (also known as ultraintuitionism, strict formalism, strict finitism, actualism, predicativism, and strong finitism) is a form of finitism. There are various philosophies of mathematics that are called ultrafinitism. A major identifying property common among most of these philosophies is their objections to totality of number theoretic functions like exponentiation over natural numbers.\n\nLike other finitists, ultrafinitists deny the existence of the infinite set N of natural numbers.\n\nIn addition, some ultrafinitists are concerned with acceptance of objects in mathematics that no one can construct in practice because of physical restrictions in constructing large finite mathematical objects.\nThus some ultrafinitists will deny or refrain from accepting the existence of large numbers, for example, the floor of the first Skewes's number, which is a huge number defined using the exponential function as exp(exp(exp(79))), or\nThe reason is that nobody has yet calculated what natural number is the floor of this real number, and it may not even be physically possible to do so. Similarly, formula_2 (in Knuth's up-arrow notation) would be considered only a formal expression which does not correspond to a natural number. The brand of ultrafinitism concerned with physical realizability of mathematics is often called actualism.\n\nEdward Nelson criticized the classical conception of natural numbers because of the circularity of its definition. In classical mathematics the natural numbers are defined as 0 and numbers obtained by the iterative applications of the successor function to 0. But the concept of natural number is already assumed for the iteration. In other words, to obtain a number like formula_2 one needs to perform the successor function iteratively, in fact exactly formula_2 times to 0.\n\nSome versions of ultrafinitism are forms of constructivism, but most constructivists view the philosophy as unworkably extreme. The logical foundation of ultrafinitism is unclear; in his comprehensive survey \"Constructivism in Mathematics\" (1988), the constructive logician A. S. Troelstra dismissed it by saying \"no satisfactory development exists at present.\" This was not so much a philosophical objection as it was an admission that, in a rigorous work of mathematical logic, there was simply nothing precise enough to include.\n\nSerious work on ultrafinitism has been led, since 1959, by Alexander Esenin-Volpin, who in 1961 sketched a program for proving the consistency of Zermelo–Fraenkel set theory in ultrafinite mathematics. Other mathematicians who have worked in the topic include Doron Zeilberger, Edward Nelson, Rohit Jivanlal Parikh, and Jean Paul Van Bendegem. The philosophy is also sometimes associated with the beliefs of Ludwig Wittgenstein, Robin Gandy, Petr Vopenka, and J. Hjelmslev.\n\nShaughan Lavine has developed a form of set-theoretical ultra-finitism that is consistent with classical mathematics.\nLavine has shown that the basic principles of arithmetic such as \"there is no largest natural number\" can be upheld, as Lavine allows for the inclusion of \"indefinitely large\" numbers.\n\nOther considerations of the possibility of avoiding unwieldy large numbers can be based on computational complexity theory, as in Andras Kornai's work on explicit finitism (which does not deny the existence of large numbers) and Vladimir Sazonov's notion of feasible number.\n\nThere has also been considerable formal development on versions of ultrafinitism that are based on complexity theory, like Samuel Buss's Bounded Arithmetic theories, which capture mathematics associated with various complexity classes like P and PSPACE. Buss's work can be considered the continuation of Edward Nelson's work on predicative arithmetic as bounded arithmetic theories like S12 are interpretable in Raphael Robinson's theory Q and therefore are predicative in Nelson's sense. The power of these theories for developing mathematics is studied in Bounded reverse mathematics as can be found in the works of Stephen A. Cook and Phuong The Nguyen. However these researches are not philosophies of mathematics but rather the study of restricted forms of reasoning similar to Reverse Mathematics.\n\n\n\n"}
{"id": "32245", "url": "https://en.wikipedia.org/wiki?curid=32245", "title": "Universal property", "text": "Universal property\n\nIn various branches of mathematics, a useful construction is often viewed as the “most efficient solution” to a certain problem. The definition of a universal property uses the language of category theory to make this notion precise and to study it abstractly.\n\nThis article gives a general treatment of universal properties. To understand the concept, it is useful to study several examples first, of which there are many: all free objects, direct product and direct sum, free group, free lattice, Grothendieck group, Dedekind–MacNeille completion, product topology, Stone–Čech compactification, tensor product, inverse limit and direct limit, kernel and cokernel, pullback, pushout and equalizer.\n\nBefore giving a formal definition of universal properties, we offer some motivation for studying such constructions.\n\n\nSuppose that \"U\": \"D\" → \"C\" is a functor from a category \"D\" to a category \"C\", and let \"X\" be an object of \"C\". Consider the following dual (opposite) notions:\nAn initial morphism from \"X\" to \"U\" is an initial object in the category formula_1 of morphisms from \"X\" to \"U\". In other words, it consists of a pair (\"A\", \"Φ\") where \"A\" is an object of \"D\" and \"Φ\": \"X\" → \"U\"(\"A\") is a morphism in \"C\", such that the following initial property is satisfied:\n\nA terminal morphism from \"U\" to \"X\" is a terminal object in the comma category formula_2 of morphisms from \"U\" to \"X\". In other words, it consists of a pair (\"A\", \"Φ\") where \"A\" is an object of \"D\" and \"Φ\": \"U\"(\"A\") → \"X\" is a morphism in \"C\", such that the following terminal property is satisfied:\nThe term universal morphism refers either to an initial morphism or a terminal morphism, and the term universal property refers either to an initial property or a terminal property. In each definition, the existence of the morphism \"g\" intuitively expresses the fact that (\"A\", \"Φ\") is \"general enough\", while the uniqueness of the morphism ensures that (\"A\", \"Φ\") is \"not too general\".\n\nSince the notions of \"initial\" and \"terminal\" are dual, it is often enough to discuss only one of them, and simply reverse arrows in \"C\" for the dual discussion. Alternatively, the word \"universal\" is often used in place of both words.\n\nNote: some authors may call only one of these constructions a \"universal morphism\" and the other one a \"co-universal morphism\". Which is which depends on the author, although in order to be consistent with the naming of limits and colimits the latter construction should be named universal and the former couniversal. This article uses the unambiguous terminology of initial and terminal objects.\n\nBelow are a few examples, to highlight the general idea. The reader can construct numerous other examples by consulting the articles mentioned in the introduction.\n\nLet \"C\" be the category of vector spaces K\"-Vect over a field \"K\" and let \"D\" be the category of algebras K\"-Alg over \"K\" (assumed to be unital and associative). Let\nbe the forgetful functor which assigns to each algebra its underlying vector space.\n\nGiven any vector space \"V\" over \"K\" we can construct the tensor algebra \"T\"(\"V\") of \"V\". The tensor algebra is characterized by the fact:\nThis statement is an initial property of the tensor algebra since it expresses the fact that the pair (\"T\"(\"V\"), \"i\"), where \"i\" : \"V\" → \"U\"(\"T\"(\"V\")) is the inclusion map, is an initial morphism from the vector space \"V\" to the functor \"U\".\n\nSince this construction works for any vector space \"V\", we conclude that \"T\" is a functor from K\"-Vect to K\"-Alg. This means that \"T\" is \"left adjoint\" to the forgetful functor \"U\" (see the section below on relation to adjoint functors).\n\nA categorical product can be characterized by a terminal property. For concreteness, one may consider the Cartesian product in Set, the direct product in Grp, or the product topology in Top, where products exist.\n\nLet \"X\" and \"Y\" be objects of a category \"D\". The product of \"X\" and \"Y\" is an object \"X\" × \"Y\" together with two morphisms\nsuch that for any other object \"Z\" of \"D\" and morphisms \"f\" : \"Z\" → \"X\" and \"g\" : \"Z\" → \"Y\" there exists a unique morphism \"h\" : \"Z\" → \"X\" × \"Y\" such that \"f\" = π∘\"h\" and \"g\" = π∘\"h\".\n\nTo understand this characterization as a terminal property we take the category \"C\" to be the product category \"D\" × \"D\" and define the diagonal functor\nby Δ(\"X\") = (\"X\", \"X\") and Δ(\"f\" : \"X\" → \"Y\") = (\"f\", \"f\"). Then (\"X\" × \"Y\", (π, π)) is a terminal morphism from Δ to the object (\"X\", \"Y\") of \"D\" × \"D\": If (\"f\", \"g\") is any morphism from (\"Z\", \"Z\") to (\"X\", \"Y\"), then it must equal a morphism Δ(\"h\" : \"Z\" → \"X\" × \"Y\") = (\"h\", \"h\") from Δ(\"Z\") = (\"Z\", \"Z\") to Δ(\"X\" × \"Y\") = (\"X\" × \"Y\",\"X\" × \"Y\"), followed by (π, π).\n\nCategorical products are a particular kind of limit in category theory. One can generalize the above example to arbitrary limits and colimits.\n\nLet \"J\" and \"C\" be categories with \"J\" a small index category and let \"C\" be the corresponding functor category. The \"diagonal functor\"\nis the functor that maps each object \"N\" in \"C\" to the constant functor Δ(\"N\"): \"J\" → \"C\" to \"N\" (i.e. Δ(\"N\")(\"X\") = \"N\" for each \"X\" in \"J\").\n\nGiven a functor \"F\" : \"J\" → \"C\" (thought of as an object in \"C\"), the \"limit\" of \"F\", if it exists, is nothing but a terminal morphism from Δ to \"F\". Dually, the \"colimit\" of \"F\" is an initial morphism from \"F\" to Δ.\n\nDefining a quantity does not guarantee its existence. Given a functor \"U\" and an object \"X\" as above, there may or may not exist an initial morphism from \"X\" to \"U\". If, however, an initial morphism (\"A\", φ) does exist then it is essentially unique. Specifically, it is unique up to a \"unique\" isomorphism: if (\"A\"′, φ′) is another such pair, then there exists a unique isomorphism \"k\": \"A\" → \"A\"′ such that φ′ = \"U\"(\"k\")φ. This is easily seen by substituting (\"A\"′, φ′) for (\"Y\", \"f\") in the definition of the initial property.\n\nIt is the pair (\"A\", φ) which is essentially unique in this fashion. The object \"A\" itself is only unique up to isomorphism. Indeed, if (\"A\", φ) is an initial morphism and \"k\": \"A\" → \"A\"′ is any isomorphism then the pair (\"A\"′, φ′), where φ′ = \"U\"(\"k\")φ, is also an initial morphism.\n\nThe definition of a universal morphism can be rephrased in a variety of ways. Let \"U\" be a functor from \"D\" to \"C\", and let \"X\" be an object of \"C\". Then the following statements are equivalent:\n\nThe dual statements are also equivalent:\n\nSuppose (\"A\", φ) is an initial morphism from \"X\" to \"U\" and (\"A\", φ) is an initial morphism from \"X\" to \"U\". By the initial property, given any morphism \"h\": \"X\" → \"X\" there exists a unique morphism \"g\": \"A\" → \"A\" such that the following diagram commutes:\nIf \"every\" object \"X\" of \"C\" admits an initial morphism to \"U\", then the assignment formula_3 and formula_4 defines a functor \"V\" from \"C\" to \"D\". The maps φ then define a natural transformation from 1 (the identity functor on \"C\") to \"UV\". The functors (\"V\", \"U\") are then a pair of adjoint functors, with \"V\" left-adjoint to \"U\" and \"U\" right-adjoint to \"V\".\n\nSimilar statements apply to the dual situation of terminal morphisms from \"U\". If such morphisms exist for every \"X\" in \"C\" one obtains a functor \"V\": \"C\" → \"D\" which is right-adjoint to \"U\" (so \"U\" is left-adjoint to \"V\").\n\nIndeed, all pairs of adjoint functors arise from universal constructions in this manner. Let \"F\" and \"G\" be a pair of adjoint functors with unit η and co-unit ε (see the article on adjoint functors for the definitions). Then we have a universal morphism for each object in \"C\" and \"D\":\n\nUniversal constructions are more general than adjoint functor pairs: a universal construction is like an optimization problem; it gives rise to an adjoint pair if and only if this problem has a solution for every object of \"C\" (equivalently, every object of \"D\").\n\nUniversal properties of various topological constructions were presented by Pierre Samuel in 1948. They were later used extensively by Bourbaki. The closely related concept of adjoint functors was introduced independently by Daniel Kan in 1958.\n\n\n\n"}
{"id": "252461", "url": "https://en.wikipedia.org/wiki?curid=252461", "title": "Winning Ways for your Mathematical Plays", "text": "Winning Ways for your Mathematical Plays\n\nWinning Ways for your Mathematical Plays (Academic Press, 1982) by Elwyn R. Berlekamp, John H. Conway, and Richard K. Guy is a compendium of information on mathematical games. It was first published in 1982 in two volumes.\n\nThe first volume introduces combinatorial game theory and its foundation in the surreal numbers; partizan and impartial games; Sprague–Grundy theory and misère games. The second volume applies the theorems of the first volume to many games, including nim, sprouts, dots and boxes, Sylver coinage, philosopher's football, fox and geese. A final section on puzzles analyzes the Soma cube, Rubik's Cube, peg solitaire, and Conway's game of life.\n\nA republication of the work by A K Peters splits the content into four volumes.\n\n\nThis is a partial list of the games mentioned in the book.\n\nNote: Misere games not included\n\n\n"}
{"id": "2007187", "url": "https://en.wikipedia.org/wiki?curid=2007187", "title": "Zacharias Dase", "text": "Zacharias Dase\n\nJohann Martin Zacharias Dase (June 23, 1824, Hamburg – September 11, 1861, Hamburg) was a German mental calculator.\n\nHe attended schools in Hamburg from a very early age, but later admitted that his instruction had little influence on him. He used to spend a lot of time playing dominoes, and suggested that this played a significant role in developing his calculating skills. Dase suffered from epilepsy from early childhood throughout his life.\n\nAt age 15 he began to travel extensively, giving exhibitions in Germany, Austria and England. Among his most impressive feats, he multiplied 79532853 × 93758479 in 54 seconds. He multiplied two 20-digit numbers in 6 minutes; two 40-digit numbers in 40 minutes; and two 100-digit numbers in 8 hours 45 minutes. The famous mathematician Carl Friedrich Gauss commented that someone skilled in calculation could have done the 100-digit calculation in about half that time with pencil and paper.\n\nThese exhibitions however did not earn him enough money, so he tried to find other employments. In 1844 he obtained a position in the Railway Department of Vienna, but this didn't last long since in 1845 he was reported in Mannheim and in 1846 in Berlin.\n\nIn 1844, Dase calculated π to 200 decimal places over the course of approximately two months, a record for the time, from the Machin-like formula:\n\nHe also calculated a 7-digit logarithm table and extended a table of integer factorizations from 6,000,000 to 9,000,000.\n\nDase had very little knowledge of mathematical theory. The mathematician Julius Petersen tried to teach him some of Euclid's theorems, but gave up the task once he realized that their comprehension was beyond Dase's capabilities.\nGauss however was very impressed with his calculating skill, and he recommended that the \"Hamburg Academy of Sciences\" should allow Dase to do mathematical work on a full-time basis, but Dase died shortly thereafter.\n\nThe book \"Gödel, Escher, Bach\" by Douglas Hofstadter mentions his calculating abilities. \"... he also had an uncanny sense of quantity. That is, he could just 'tell', without counting, how many sheep were in a field, or words in a sentence, and so forth, up to about 30.\"\n\n"}
