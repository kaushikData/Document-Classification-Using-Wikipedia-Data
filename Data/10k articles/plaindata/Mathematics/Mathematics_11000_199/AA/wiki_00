{"id": "366436", "url": "https://en.wikipedia.org/wiki?curid=366436", "title": "Active and passive transformation", "text": "Active and passive transformation\n\nIn physics and engineering, spatial transformations in the 3-dimensional Euclidian space formula_1 are distinguished into active or alibi transformations, and passive or alias transformations. An active transformation is a transformation which actually changes the physical position (alibi, elsewhere) of a point, or rigid body, which can be defined in the absence of a coordinate system; whereas a passive transformation is merely a change in the coordinate system in which the object is described (alias, other name) (change of coordinate map, or change of basis). By \"transformation\", mathematicians usually refer to active transformations, while physicists and engineers could mean either. Both types of transformation can be represented by a combination of a translation and a linear transformation.\n\nPut differently, a \"passive\" transformation refers to description of the \"same\" object in two different coordinate systems.\nOn the other hand, an \"active transformation\" is a transformation of one or more objects with respect to the same coordinate system. For instance, active transformations are useful to describe successive positions of a rigid body. On the other hand, passive transformations may be useful in human motion analysis to observe the motion of the tibia relative to the femur, that is, its motion relative to a (\"local\") coordinate system which moves together with the femur, rather than a (\"global\") coordinate system which is fixed to the floor.\n\nAs an example, let the vector formula_2, be a vector in the plane. A rotation of the vector through an angle θ in counterclockwise direction is given by the rotation matrix:\nwhich can be viewed either as an \"active transformation\" or a \"passive transformation\" (where the above matrix will be inverted), as described below.\n\nIn general a spatial transformation formula_5 may consist of a translation and a linear transformation. In the following, the translation will be omitted, and the linear transformation will be represented by a 3×3-matrix formula_6.\n\nAs an active transformation, formula_6 transforms the initial vector formula_8 into a new vector formula_9. \n\nIf one views formula_10 as a new basis, then the coordinates of the new vector formula_11 in the new basis are the same as those of formula_12 in the original basis. Note that active transformations make sense even as a linear transformation into a different vector space. It makes sense to write the new vector in the unprimed basis (as above) only when the transformation is from the space into itself.\n\nOn the other hand, when one views formula_6 as a passive transformation, the initial vector formula_8 is left unchanged, while the coordinate system and its basis vectors are transformed in the opposite direction, i.e. with the inverse transformation formula_15.\n\nThe new coordinates formula_17 of formula_18 with respect to the new coordinate system XYZ are given by: \n\nFrom this equation one sees that the new coordinates are given by\n\nAs a passive transformation formula_6 transforms the old coordinates into the new ones.\n\nNote the difference between formula_17 and formula_23\n\n\n"}
{"id": "46955123", "url": "https://en.wikipedia.org/wiki?curid=46955123", "title": "Angles between flats", "text": "Angles between flats\n\nThe concept of angles between lines in the plane and between pairs of two lines, two planes or a line and a plane in space can be generalized to arbitrary dimension. This generalization was first discussed by Jordan. For any pair of flats in a Euclidean space of arbitrary dimension one can define a set of mutual angles which are invariant under isometric transformation of the Euclidean space. If the flats do not intersect, their shortest distance is one more invariant. These angles are called canonical or principal. The concept of angles can be generalized to pairs of flats in a finite-dimensional inner product space over the complex numbers.\n\nLet formula_1 and formula_2 be flats of dimensions formula_3 and formula_4 in the formula_5-dimensional Euclidean space formula_6. By definition, a translation of formula_1 or formula_8 does not alter their mutual angles. If formula_1 and formula_2 do not intersect, they will do so upon any translation of formula_2 which maps some point in formula_2 to some point in formula_1. It can therefore be assumed without loss of generality that formula_1 and formula_2 intersect.\n\nJordan shows that Cartesian coordinates formula_16 formula_17 formula_18 formula_19 formula_20 formula_21 in formula_6 can then be defined such that formula_1 and formula_2 are described, respectively, by the sets of equations\n\nand\n\nwith formula_31. Jordan calls these coordinates canonical. By definition, the angles formula_32 are the angles between formula_1 and formula_2.\n\nThe non-negative integers formula_35 are constrained by\n\nFor these equations to determine the five non-negative integers completely, besides the dimensions formula_39 and formula_40 and the number formula_41 of angles formula_32, the non-negative integer formula_43 must be given. This is the number of coordinates formula_44, whose corresponding axes are those lying entirely within both formula_1 and formula_2. The integer formula_43 is thus the dimension of formula_48. The set of angles formula_32 may be supplemented with formula_43 angles formula_51 to indicate that formula_48 has that dimension.\n\nJordan's proof applies essentially unaltered when formula_6 is replaced with the formula_5-dimensional inner product space formula_55 over the complex numbers. (For angles between subspaces, the generalization to formula_55 is discussed by Galántai and Hegedũs in terms of the below variational characterization.)\n\nNow let formula_1 and formula_2 be subspaces of the formula_5-dimensional inner product space over the real or complex numbers. Geometrically, formula_1 and formula_2 are flats, so Jordan's definition of mutual angles applies. When for any canonical coordinate formula_62 the symbol formula_63 denotes the unit vector of the formula_62 axis, the vectors formula_65 formula_66 formula_67 form an orthonormal basis for formula_1 and the vectors formula_65 formula_70 formula_71 form an orthonormal basis for formula_2, where\n\nBeing related to canonical coordinates, these basic vectors may be called canonical.\n\nWhen formula_74 denote the canonical basic vectors for formula_1 and formula_76 the canonical basic vectors for formula_2 then the inner product formula_78 vanishes for any pair of formula_79 and formula_80 except the following ones.\n\nWith the above ordering of the basic vectors, the matrix of the inner products formula_78 is thus diagonal. In other words, if formula_83 and formula_84 are arbitrary orthonormal bases in formula_1 and formula_2 then the real, orthogonal or unitary transformations from the basis formula_87 to the basis formula_88 and from the basis formula_89 to the basis formula_90 realize a singular value decomposition of the matrix of inner products formula_91. The diagonal matrix elements formula_92 are the singular values of the latter matrix. By the uniqueness of the singular value decomposition, the vectors formula_93 are then unique up to a real, orthogonal or unitary transformation among them, and the vectors formula_94 and formula_95 (and hence formula_96) are unique up to equal real, orthogonal or unitary transformations applied simultaneously to the sets of the vectors formula_94 associated with a common value of formula_32 and to the corresponding sets of vectors formula_95 (and hence to the corresponding sets of formula_96).\n\nA singular value formula_101 can be interpreted as formula_102 corresponding to the angles formula_51 introduced above and associated with formula_48 and a singular value formula_51 can be interpreted as formula_106 corresponding to right angles between the orthogonal spaces formula_107 and formula_108, where superscript formula_109 denotes the orthogonal complement.\n\nThe variational characterization of singular values and vectors implies as a special case a variational characterization of the angles between subspaces and their associated canonical vectors. This characterization includes the angles formula_51 and formula_111 introduced above and orders the angles by increasing value. It can be given the form of the below alternative definition. In this context, it is customary to talk of principal angles and vectors.\n\nLet formula_112 be an inner product space. Given two subspaces formula_113 with formula_114, there exists then a sequence of formula_3 angles formula_116 called the principal angles, the first one defined as\n\nwhere formula_118 is the inner product and formula_119 the induced norm. The vectors formula_120 and formula_121 are the corresponding \"principal vectors.\"\n\nThe other principal angles and vectors are then defined recursively via\n\nThis means that the principal angles formula_123 form a set of minimized angles between the two subspaces, and the principal vectors in each subspace are orthogonal to each other.\n\nGeometrically, subspaces are flats (points, lines, planes etc.) that include the origin, thus any two subspaces intersect at least in the origin. Two two-dimensional subspaces formula_124 and formula_125 generate a set of two angles. In a three-dimensional Euclidean space, the subspaces formula_124 and formula_125 are either identical, or their intersection forms a line. In the former case, both formula_128. In the latter case, only formula_129, where vectors formula_120 and formula_121 are on the line of the intersection formula_132 and have the same direction. The angle formula_133 will be the angle between the subspaces formula_124 and formula_125 in the orthogonal complement to formula_132. Imagining the angle between two planes in 3D, one intuitively thinks of the largest angle, formula_133.\n\nIn 4-dimensional real coordinate space R, let the two-dimensional subspace formula_124 be \nspanned by formula_139 and formula_140, and let the two-dimensional subspace formula_125 be \nspanned by formula_142 and formula_143 with some real formula_144 and formula_145 such that formula_146. Then formula_120 and formula_121 are, in fact, the pair of principal vectors corresponding to the angle formula_149 with formula_150, and formula_151 and formula_152 are the principal vectors corresponding to the angle formula_153 with formula_154\n\nTo construct a pair of subspaces with any given set of formula_3 angles formula_156 in a formula_157 (or larger) dimensional Euclidean space, take a subspace formula_124 with an orthonormal basis formula_159 and complete it to an orthonormal basis formula_160 of the Euclidean space, where formula_161. Then, an orthonormal basis of the other subspace formula_125 is, e.g.,\n\n\n\nThe notion of the angles and some of the variational properties can be naturally extended to arbitrary inner products and subspaces with infinite dimensions.\n\n"}
{"id": "26974141", "url": "https://en.wikipedia.org/wiki?curid=26974141", "title": "Auerbach's lemma", "text": "Auerbach's lemma\n\nIn mathematics, Auerbach's lemma, named after Herman Auerbach, is a theorem in functional analysis which asserts that a certain property of Euclidean spaces holds for general finite-dimensional normed vector spaces.\n\nLet (\"V\", ||·||) be an \"n\"-dimensional normed vector space. Then there exists a basis {\"e\", ..., \"e\"} of V such that\nwhere {\"e\", ..., \"e\"} is a basis of \"V\"* dual to {\"e\", ..., \"e\"}, i. e. \"e\"(\"e\") = δ. \n\nA basis with this property is called an \"Auerbach basis\".\n\nIf \"V\" is an inner product space (or even infinite-dimensional Hilbert space) then this result is obvious as one may take for {\"e\"} any orthonormal basis of \"V\" (the dual basis is then {(\"e\"|·)}).\n\nAn equivalent statement is the following: any centrally symmetric convex body in formula_1 has a linear image which contains the unit cross-polytope (the unit ball for the formula_2 norm) and is contained in the unit cube (the unit ball for the formula_3 norm).\n\nThe lemma has a corollary with implications to approximation theory. \n\nLet \"V\" be an \"n\"-dimensional subspace of a normed vector space (\"X\", ||·||). Then there exists a projection \"P\" of \"X\" onto \"V\" such that ||\"P\"|| ≤ \"n\".\n\nLet {\"e\", ..., \"e\"} be an Auerbach basis of \"V\" and {\"e\", ..., \"e\"} corresponding dual basis. By Hahn–Banach theorem each \"e\" extends to \"f\" ∈ \"X\"* such that \nNow set \nIt's easy to check that \"P\" is indeed a projection onto \"V\" and that ||\"P\"|| ≤ \"n\" (this follows from triangle inequality).\n\n"}
{"id": "5089733", "url": "https://en.wikipedia.org/wiki?curid=5089733", "title": "Bitopological space", "text": "Bitopological space\n\nIn mathematics, a bitopological space is a set endowed with \"two\" topologies. Typically, if the set is formula_1 and the topologies are formula_2 and formula_3 then the bitopological space is referred to as formula_4.\n\nA map formula_5 from a bitopological space formula_6 to another bitopological space formula_7 is called continuous or sometimes pairwise continuous if formula_8 is continuous both as a map from formula_9 to formula_10 and as map from formula_11 to formula_12.\n\nCorresponding to well-known properties of topological spaces, there are versions for bitopological spaces.\n\n"}
{"id": "32226166", "url": "https://en.wikipedia.org/wiki?curid=32226166", "title": "Boris Levitan", "text": "Boris Levitan\n\nBoris Levitan (7 June 1914 – 4 April 2004) was a mathematician known in particular for his work on almost periodic functions, and Sturm–Liouville operators, especially, on inverse scattering.\n\nBoris Levitan was born in Berdyansk (now south-eastern Ukraine), and grew up in Kharkov. He graduated from Kharkov University in 1936; in 1938, he submitted his PhD thesis \"\"Some Generalization of Almost Periodic Function\" under the supervision of Naum Akhiezer. Then he defended the habilitation thesis \"Theory of Generalized Translation Operators\"\".\n\nHe was drafted into the army at the beginning of World War II in 1941, and served until 1944. From 1944 to 1961 he worked at the Dzerzhinsky Military Academy, and from 1961 in Moscow University. During the last years of his life, he worked in the University of Minnesota.\n"}
{"id": "1305071", "url": "https://en.wikipedia.org/wiki?curid=1305071", "title": "Bridge (graph theory)", "text": "Bridge (graph theory)\n\nIn graph theory, a bridge, isthmus, cut-edge, or cut arc is an edge of a graph whose deletion increases its number of connected components. Equivalently, an edge is a bridge if and only if it is not contained in any cycle. A graph is said to be bridgeless or isthmus-free if it contains no bridges.\n\nAnother meaning of \"bridge\" appears in the term bridge of a subgraph. If \"H\" is a subgraph of \"G\", a bridge of \"H\" in \"G\" is a maximal subgraph of \"G\" that is not contained in \"H\" and is not separated by \"H\".\n\nA graph with formula_1 nodes can contain at most formula_2 bridges, since adding additional edges must create a cycle. The graphs with exactly formula_2 bridges are exactly the trees, and the graphs in which every edge is a bridge are exactly the forests.\n\nIn every undirected graph, there is an equivalence relation on the vertices according to which two vertices are related to each other whenever there are two edge-disjoint paths connecting them. (Every vertex is related to itself via two length-zero paths, which are identical but nevertheless edge-disjoint.) The equivalence classes of this relation are called 2-edge-connected components, and the bridges of the graph are exactly the edges whose endpoints belong to different components. The bridge-block tree of the graph has a vertex for every nontrivial component and an edge for every bridge.\n\nBridges are closely related to the concept of articulation vertices, vertices that belong to every path between some pair of other vertices. The two endpoints of a bridge are articulation vertices unless they have a degree of 1, although it may also be possible for a non-bridge edge to have two articulation vertices as endpoints. Analogously to bridgeless graphs being 2-edge-connected, graphs without articulation vertices are 2-vertex-connected.\n\nIn a cubic graph, every cut vertex is an endpoint of at least one bridge.\n\nA bridgeless graph is a graph that does not have any bridges. Equivalent conditions are that each connected component of the graph has an open ear decomposition, that each connected component is 2-edge-connected, or (by Robbins' theorem) that every connected component has a strong orientation.\n\nAn important open problem involving bridges is the cycle double cover conjecture, due to Seymour and Szekeres (1978 and 1979, independently), which states that every bridgeless graph admits a set of simple cycles which contains each edge exactly twice.\n\nThe first linear time algorithm for finding the bridges in a graph was described by Robert Tarjan in 1974. It performs the following steps:\n\nA very simple bridge-finding algorithm uses chain decompositions.\nChain decompositions do not only allow to compute all bridges of a graph, they also allow to \"read off\" every cut vertex of \"G\" (and the block-cut tree of \"G\"), giving a general framework for testing 2-edge- and 2-vertex-connectivity (which extends to linear-time 3-edge- and 3-vertex-connectivity tests).\n\nChain decompositions are special ear decompositions depending on a DFS-tree \"T\" of \"G\" and can be computed very simply: Let every vertex be marked as unvisited. For each vertex \"v\" in ascending DFS-numbers 1...\"n\", traverse every backedge (i.e. every edge not in the DFS tree) that is incident to \"v\" and follow the path of tree-edges back to the root of \"T\", stopping at the first vertex that is marked as visited. During such a traversal, every traversed vertex is marked as visited. Thus, a traversal stops at the latest at \"v\" and forms either a directed path or cycle, beginning with v; we call this path\nor cycle a \"chain\". The \"i\"th chain found by this procedure is referred to as \"C\". \"C=C,C...\" is then a \"chain decomposition\" of \"G\".\n\nThe following characterizations then allow to \"read off\" several properties of \"G\" from \"C\" efficiently, including all bridges of \"G\". Let \"C\" be a chain decomposition of a simple connected graph \"G=(V,E)\".\n\n"}
{"id": "57172319", "url": "https://en.wikipedia.org/wiki?curid=57172319", "title": "Busemann G-space", "text": "Busemann G-space\n\nIn mathematics, a Busemann \"G\"-space is a type of metric space first described by Herbert Busemann in 1942.\n\nIf formula_1 is a metric space such that\n\n\nthen \"X\" is said to be a \"Busemann\" \"G\"-\"space\". Every Busemann \"G\"-space is a homogenous space.\n\nThe Busemann conjecture states that every Busemann \"G\"-space is a topological manifold. It is a special case of the Bing–Borsuk conjecture. The Busemann conjecture is known to be true for dimensions 1 to 4. \n"}
{"id": "16372307", "url": "https://en.wikipedia.org/wiki?curid=16372307", "title": "Combinatory categorial grammar", "text": "Combinatory categorial grammar\n\nCombinatory categorial grammar (CCG) is an efficiently parsable, yet linguistically expressive grammar formalism. It has a transparent interface between surface syntax and underlying semantic representation, including predicate-argument structure, quantification and information structure. The formalism generates constituency-based structures (as opposed to dependency-based ones) and is therefore a type of phrase structure grammar (as opposed to a dependency grammar).\n\nCCG relies on combinatory logic, which has the same expressive power as the lambda calculus, but builds its expressions differently. The first linguistic and psycholinguistic arguments for basing the grammar on combinators were put forth by Steedman and Szabolcsi. More recent prominent proponents of the approach are Jacobson and Baldridge.\n\nFor example, the combinator B (the compositor) is useful in creating long-distance dependencies, as in \"Who do you think Mary is talking about?\" and the combinator W (the duplicator) is useful as the lexical interpretation of reflexive pronouns, as in \"Mary talks about herself\". Together with I (the identity mapping) and C (the permutator) these form a set of primitive, non-interdefinable combinators. Jacobson interprets personal pronouns as the combinator I, and their binding is aided by a complex combinator Z, as in \"Mary lost her way\". Z is definable using W and B.\n\nThe CCG formalism defines a number of combinators (application, composition, and type-raising being the most common). These operate on syntactically-typed lexical items, by means of Natural deduction style proofs. The goal of the proof is to find some way of applying the combinators to a sequence of lexical items until no lexical item is unused in the proof. The resulting type after the proof is complete is the type of the whole expression. Thus, proving that some sequence of words is a sentence of some language amounts to proving that the words reduce to the type \"S\".\n\nThe syntactic type of a lexical item can be either a primitive type, such as \"S\", \"N\", or \"NP\", or complex, such as \"S\\NP\", or \"NP/N\".\n\nThe complex types, schematizable as \"X/Y\" and \"X\\Y\", denote functor types that take an argument of type \"Y\" and return an object of type \"X\". A forward slash denotes that the argument should appear to the right, while a backslash denotes that the argument should appear on the left. Any type can stand in for the \"X\" and \"Y\" here, making syntactic types in CCG a recursive type system.\n\nThe application combinators, often denoted by \">\" for forward application and \"<\" for backward application, apply a lexical item with a functor type to an argument with an appropriate type. The definition of application is given as:\n\nformula_1\n\nformula_2\n\nThe composition combinators, often denoted by formula_3 for forward composition and formula_4 for backward composition, are similar to function composition from mathematics, and can be defined as follows:\n\nformula_5\n\nformula_6\n\nThe type-raising combinators, often denoted as formula_7 for forward type-raising and formula_8 for backward type-raising, take argument types (usually primitive types) to functor types, which take as their argument the functors that, before type-raising, would have taken them as arguments.\n\nformula_9\n\nformula_10\n\nThe sentence \"the dog bit John\" has a number of different possible proofs. Below are a few of them. The variety of proofs demonstrates the fact that in CCG, sentences don't have a single structure, as in other models of grammar.\n\nLet the types of these lexical items be\n\nformula_11\n\nWe can perform the simplest proof (changing notation slightly for brevity) as:\n\nformula_12\n\nOpting to type-raise and compose some, we could get a fully incremental, left-to-right proof. The ability to construct such a proof is an argument for the psycholinguistic plausibility of CCG, because listeners do in fact construct partial interpretations (syntactic and semantic) of utterances before they have been completed.\n\nformula_13\n\nCCGs are known to be able to generate the language formula_14 (which is a non-context-free indexed language). A grammar for this language can be found in Vijay-Shanker and Weir (1994).\n\nVijay-Shanker and Weir (1994) demonstrates that Linear Indexed Grammars, Combinatory Categorial Grammars, Tree-adjoining Grammars, and Head Grammars are weakly equivalent formalisms, in that they all define the same string languages. Kuhlmann et al. (2015) show that this equivalence, and the ability of CCG to describe formula_15, rely crucially on the ability to restrict the use of the combinatory rules to certain categories, in ways not explained above.\n\n\n\n\n"}
{"id": "45092796", "url": "https://en.wikipedia.org/wiki?curid=45092796", "title": "Contact copier", "text": "Contact copier\n\nA contact copier (also known as contact printer), is a device used to copy an image by illuminating a film negative with the image in direct contact with a photosensitive surface (film, paper, plate, etc.). The more common processes are negative, where clear areas in the original produce an opaque or hardened photosensitive surface, but positive processes are available. The light source is usually an actínic bulb internal or external to the device\n\nCommercial contact printers or process cameras usually use a pump-operated vacuum frame where the original and the photosensitive surface are pressed together against a flat glass by a grooved rubber mat connected to the vacuum source. A timer-controlled mercury vapor (arc or fluorescent) light source is on the other side of the glass.\n\nThe contact copying process was used in the early days of photography and sunlight-exposed blueprints; it is still used in amateur photography, silkscreen printing, offset printing, and photochemical machining, such as the manufacture of Printed circuit boards. By the early 20th century, blueprinting (producing white lines) or diazo blue line printing used contact-rollers rather than flat-glass exposure.\n\nSilkscreen printing and photochemical machining originally were based on gum bichromate photosensitive materials, where exposure to intense ultraviolet light made previously-soluble gum or gelatin colloids insoluble; after exposure, the exposed surface was washed in water and the unexposed coating dissolved, leaving the hardened gum or gelatin to resist the passage of the silk-screen ink or the metal-etching solution. Offset printing can use either a negative plate, where the hardened, exposed photosensitive coating attracts ink and repels water, or a positive plate, where the exposed photosensitive coating decomposes or exposes the metal, water-attracting surface.\n\nThe contact copier is used for duplication of\" negative or positive prints\" obtaining what are called \"contact prints\", that is, to reproduce on paper or film, a photographic negative \"or positive \" of exactly the same size of the original. (With normal photographic non inverting processes, \"black\" generates \"white\" on the target while \"white\" generates black). It was the common mode to make prints until it began the use of the alternative photographic enlarger. There are some models with internal light source constructed as a closed box, in which one or more lamps illuminate the negative through an opal or frosted glass.\n\nThe reproduction is done by placing the negative on top of the glass and then the photo paper with the emulsion in contact with the negative. The negative role fits the body, the lid is closed applying some pressure against the glass to prevent blur; then proceed to impress photographic paper by turning on the interior light (contact copier) or an external focus, in the last case, usually from a photographic enlarger. Exposure time can be controlled \"manually\" or using a timer that controls the light source for a preset time and with greater precision.\n\nIn the process of \"whiteprint copying \", (essentially a diazotype process), a dynamic contact copier is used (similar to the manual action of exposing both sheets strongly bonded directly to the sunlight.). The exposition is made progressively as the twin rollers pull the two papers together (original and copy) against a source of ultraviolet light, typically a powerful black light lamp . The original plan (on a transparent support) and the diazo paper are introduced, in perfect contact, within the pinch rollers of the contact copier. The sensitized paper, has a photosensitive coating -an impregnation of diazo - covering the surface of the paper. Once exposed, the copied paper is immersed in a developer solution made from ammonia (or ammonia vapor) converting the parts of the paper not exposed to the light source to a characteristic dark violet colour (blue-line).\n\nIn screenprinting it is normally use a contact copier box with several fluorescent tubes close to the silk frame. The emulsion reacts depending on the amount of light received, for that reason it is important to make some tests for determining the exposure time. Usually for a \"box type\" contact copier, exposure is usually not more than one minute. In the industrial type contact copiers with actinic lamp placed at a distance, exposure time can be about 20 minutes.\n\nThe photolith (also called \"Art\") is placed under the frame against the emulsion. The black portions of photolith film does not let light into the relevant parts of the coated silk, therefore, in these parts Emulsion will not heal and may be subsequently washed, keep in mind that there must be no gap between the photolith and the silk, to achieve that purpose any element with enough weight may be used to press the silk against the photolith (or a vacuum pump in professional machines).\n\nIn CTF offset technology, a photolith is placed in direct contact with the printing plate and pressed with an opaque lid that sometimes has a vacuum pump that helps making a good contact. The plate comes coated with a layer of photoresist in which a (negative) inverted image is formed with respect to the original photolith and after the transfer of information (burning) it's accomplished, then, after undergoing a developing process, the plate will be ready to be used in an offset printer.\n\nThe process shares some fundamental principles with the photographic processes, since the pattern engraved on the plate is generated by exposure to light with an image created in a contact copier using an optical mask. This procedure is comparable to the high-precision method of the version used to make printed circuit boards.\n\nThe contact copier is used today, particularly in the areas hobbyist, for the photoengraving of prototype printed circuit boards (PCBs) before being sent to production (artisanal creation ). Substantially similar to the contact printer used in photography, this variant usually uses ultraviolet lamps to impress a copper base specifically pre-sensitized.\n\nBurned by exposure to light parts reproduce patterns drawn on a transparent photolith film on a pre-sensitized plate (epoxy or Bakelite). This pre-sensitized plate comprises an insulating plate (epoxy resin or bakelite), adhered with a layer of copper, and coated with a varnish layer sensitized. The varnish is sensitive to UV rays, which weaken or strengthen its structure depending on whether a \"positive\" or \"negative\" process. Photolith film printed parts (usually black), inserted between the light source and the pre-sensitized plate, protect the varnish from the UV. The base copper impressed must then be engraved in a specific bath (usually iron chloride -FeCl3-), that removes the excess copper.\n\n\n"}
{"id": "7809399", "url": "https://en.wikipedia.org/wiki?curid=7809399", "title": "Countably generated space", "text": "Countably generated space\n\nIn mathematics, a topological space \"X\" is called countably generated if the topology of \"X\" is determined by the countable sets in a similar way as the topology of a sequential space (or a Fréchet space) by the convergent sequences.\n\nThe countable generated spaces are precisely the spaces having countable tightness - therefore the name \"countably tight\" is used as well.\n\nA topological space \"X\" is called countably generated if \"V\" is closed in \"X\" whenever for each countable subspace \"U\" of \"X\" the set formula_1 is closed in \"U\". Equivalently, \"X\" is countably generated if and only if the closure of any subset \"A\" of \"X\" equals the union of closures of all countable subsets of \"A\".\n\nA topological space formula_2 has countable fan tightness if for every point formula_3 and every sequence formula_4 of subsets of the space formula_2 such that formula_6 , there are finite set formula_7 such that formula_8.\n\nA topological space formula_2 has countable strong fan tightness if for every point formula_3 and every sequence formula_4 of subsets of the space formula_2 such that formula_6 , there are points formula_14 such that formula_15. Every strong Fréchet–Urysohn space has strong countable fan tightness.\n\nA quotient of countably generated space is again countably generated. Similarly, a topological sum of countably generated spaces is countably generated. Therefore the countably generated spaces form a coreflective subcategory of the category of topological spaces. They are the coreflective hull of all countable spaces.\n\nAny subspace of a countably generated space is again countably generated.\n\nEvery sequential space (in particular, every metrizable space) is countably generated.\n\nAn example of a space which is countably generated but not sequential can be obtained, for instance, as a subspace of Arens–Fort space.\n\n\n"}
{"id": "4130888", "url": "https://en.wikipedia.org/wiki?curid=4130888", "title": "Darboux's theorem (analysis)", "text": "Darboux's theorem (analysis)\n\nIn mathematics, Darboux's theorem is a theorem in real analysis, named after Jean Gaston Darboux. It states that every function that results from the differentiation of other functions has the intermediate value property: the image of an interval is also an interval.\n\nWhen \"ƒ\" is continuously differentiable (\"ƒ\" in \"C\"([\"a\",\"b\"])), this is a consequence of the intermediate value theorem. But even when \"ƒ′\" is \"not\" continuous, Darboux's theorem places a severe restriction on what it can be.\n\nLet formula_1 be a closed interval, formula_2 a real-valued differentiable function. Then formula_3 has the intermediate value property: If formula_4 and formula_5 are points in formula_1 with formula_7 between formula_8 and formula_9, there exists an formula_10 in formula_11 such that formula_12.\n\nProof 1. The first proof is based on the extreme value theorem.\n\nIf formula_13 equals formula_8 or formula_9, then setting formula_10 equal to formula_4 or formula_5, respectively, gives the desired result. Now assume that formula_13 is strictly between formula_8 and formula_9, and in particular that formula_22. Let formula_23 such that formula_24. If it is the case that formula_25 we adjust our below proof, instead asserting that formula_26 has its minimum on formula_27.\n\nSince formula_26 is continuous on the closed interval formula_27, the maximum value of formula_26 on formula_27 is attained at some point in formula_27, according to the extreme value theorem. \n\nBecause formula_33, we know formula_26 cannot attain its maximum value at formula_4. (If it did, then formula_36 for all formula_37, which implies formula_38.)\nLikewise, because formula_39, we know formula_26 cannot attain its maximum value at formula_5. \n\nTherefore formula_26 must attain its maximum value at some point formula_43. Hence, by Fermat's theorem, formula_44, i.e. formula_12. \n\nProof 2. The second proof is based on combining the mean value theorem and the intermediate value theorem.\n\nDefine formula_46.\nFor formula_47 define formula_48 and formula_49.\nAnd for formula_50 define formula_51 and formula_52.\n\nThus, for formula_53 we have formula_54.\nNow, define formula_55 with formula_56.\nformula_57 is continuous in formula_58.\n\nFurthermore, formula_59 when formula_60 and formula_61 when formula_62; therefore, from the Intermediate Value Theorem, if formula_63 then, there exists formula_64 such that formula_65.\nLet's fix formula_66.\n\nFrom the Mean Value Theorem, there exists a point formula_67 such that formula_68.\nHence, formula_69.\n\nA Darboux function is a real-valued function \"ƒ\" which has the \"intermediate value property\": for any two values \"a\" and \"b\" in the domain of \"ƒ\", and any \"y\" between \"ƒ\"(\"a\") and \"ƒ\"(\"b\"), there is some \"c\" between \"a\" and \"b\" with \"ƒ\"(\"c\") = \"y\". By the intermediate value theorem, every continuous function on a real interval is a Darboux function. Darboux's contribution was to show that there are discontinuous Darboux functions.\n\nEvery discontinuity of a Darboux function is essential, that is, at any point of discontinuity, at least one of the left hand and right hand limits does not exist.\n\nAn example of a Darboux function that is discontinuous at one point is the function\n\nBy Darboux's theorem, the derivative of any differentiable function is a Darboux function. In particular, the derivative of the function formula_71 is a Darboux function even though it is not continuous at one point.\n\nAn example of a Darboux function that is nowhere continuous is the Conway base 13 function. \n\nDarboux functions are a quite general class of functions. It turns out that any real-valued function \"ƒ\" on the real line can be written as the sum of two Darboux functions. This implies in particular that the class of Darboux functions is not closed under addition.\n\nA strongly Darboux function is one for which the image of every (non-empty) open interval is the whole real line. Such functions exist and are Darboux but nowhere continuous.\n\n"}
{"id": "91110", "url": "https://en.wikipedia.org/wiki?curid=91110", "title": "Doubling the cube", "text": "Doubling the cube\n\nDoubling the cube, also known as the Delian problem, is an ancient geometric problem. Given the edge of a cube, the problem requires the construction of the edge of a second cube whose volume is double that of the first. As with the related problems of squaring the circle and trisecting the angle, doubling the cube is now known to be impossible using only a compass and straightedge, but even in ancient times solutions were known that employed other tools.\n\nThe Egyptians, Indians, and particularly the Greeks were aware of the problem and made many futile attempts at solving what they saw as an obstinate but soluble problem. However, the nonexistence of a solution was finally proven by Pierre Wantzel in 1837.\n\nIn algebraic terms, doubling a unit cube requires the construction of a line segment of length , where ; in other words, . This is because a cube of side length 1 has a volume of , and a cube of twice that volume (a volume of 2) has a side length of the cube root of 2. The impossibility of doubling the cube is therefore equivalent to the statement that is not a constructible number. This is a consequence of the fact that the coordinates of a new point constructed by a compass and straightedge are roots of polynomials over the field generated by the coordinates of previous points, of no greater degree than a quadratic. This implies that the degree of the field extension generated by a constructible point must be a power of 2. The field extension generated by , however, is of degree 3.\n\nWe begin with the unit line segment defined by points (0,0) and (1,0) in the plane. We are required to construct a line segment defined by two points separated by a distance of . It is easily shown that compass and straightedge constructions would allow such a line segment to be freely moved to touch the origin, parallel with the unit line segment - so equivalently we may consider the task of constructing a line segment from (0,0) to (, 0), which entails constructing the point (, 0).\n\nRespectively, the tools of a compass and straightedge allow us to create circles centred on one previously defined point and passing through another, and to create lines passing through two previously defined points. Any newly defined point either arises as the result of the intersection of two such circles, as the intersection of a circle and a line, or as the intersection of two lines. An exercise of elementary analytic geometry shows that in all three cases, both the - and -coordinates of the newly defined point satisfy a polynomial of degree no higher than a quadratic, with coefficients that are additions, subtractions, multiplications, and divisions involving the coordinates of the previously defined points (and rational numbers). Restated in more abstract terminology, the new - and -coordinates have minimal polynomials of degree at most 2 over the subfield of generated by the previous coordinates. Therefore, the degree of the field extension corresponding to each new coordinate is 2 or 1.\n\nSo, given a coordinate of any constructed point, we may proceed inductively backwards through the - and -coordinates of the points in the order that they were defined until we reach the original pair of points (0,0) and (1,0). As every field extension has degree 2 or 1, and as the field extension over of the coordinates of the original pair of points is clearly of degree 1, it follows from the tower rule that the degree of the field extension over of any coordinate of a constructed point is a power of 2.\n\nNow, is easily seen to be irreducible over – any factorisation would involve a linear factor for some , and so must be a root of ; but also must divide 2, that is, or , and none of these are roots of . By Gauss's Lemma, is also irreducible over , and is thus a minimal polynomial over for . The field extension is therefore of degree 3. But this is not a power of 2, so by the above, is not the coordinate of a constructible point, and thus a line segment of cannot be constructed, and the cube cannot be doubled. By Eisenstein's criterion, is also irreducible over , and indeed it does since 2 does not divide the leading coefficient , it divides all other coefficients and its square does not divide the constant coefficient −2.\n\nThe problem owes its name to a story concerning the citizens of Delos, who consulted the oracle at Delphi in order to learn how to defeat a plague sent by Apollo. According to Plutarch it was the citizens of Delos who consulted the oracle at Delphi, seeking a solution for their internal political problems at the time, which had intensified relationships among the citizens. The oracle responded that they must double the size of the altar to Apollo, which was a regular cube. The answer seemed strange to the Delians and they consulted Plato, who was able to interpret the oracle as the mathematical problem of doubling the volume of a given cube, thus explaining the oracle as the advice of Apollo for the citizens of Delos to occupy themselves with the study of geometry and mathematics in order to calm down their passions.\n\nAccording to Plutarch, Plato gave the problem to Eudoxus and Archytas and Menaechmus, who solved the problem using mechanical means, earning a rebuke from Plato for not solving the problem using pure geometry (Plut., \"Quaestiones convivales\" VIII.ii, 718ef). This may be why the problem is referred to in the 350s BC by the author of the pseudo-Platonic \"Sisyphus\" (388e) as still unsolved. However another version of the story (attributed to Eratosthenes by Eutocius of Ascalon) says that all three found solutions but they were too abstract to be of practical value.\n\nA significant development in finding a solution to the problem was the discovery by Hippocrates of Chios that it is equivalent to finding two mean proportionals between a line segment and another with twice the length. In modern notation, this means that given segments of lengths and , the duplication of the cube is equivalent to finding segments of lengths and so that\n\nIn turn, this means that\n\nBut Pierre Wantzel proved in 1837 that the cube root of 2 is not constructible; that is, it cannot be constructed with straightedge and compass.\n\nMenaechmus' original solution involves the intersection of two conic curves. Other more complicated methods of doubling the cube involve neusis, the cissoid of Diocles, the conchoid of Nicomedes, or the Philo line. Archytas solved the problem in the 4th century BC using geometric construction in three dimensions, determining a certain point as the intersection of three surfaces of revolution.\n\nFalse claims of doubling the cube with compass and straightedge abound in mathematical crank literature (pseudomathematics).\n\nOrigami may also be used to construct the cube root of two by folding paper.\n\nThere is a simple neusis construction using a marked ruler for a length which is the cube root of 2 times another length.\n\n\nThen AG is the given length times .\n\nIn music theory, a natural analogue of doubling is the octave (a musical interval caused by doubling the frequency of a tone), and a natural analogue of a cube is three repetitions of the same interval. In this sense, the problem of doubling the cube is solved by the major third in equal temperament. This is a musical interval that is exactly one third of an octave. It multiplies the frequency of a tone by , the side length of the Delian cube.\n\n\n"}
{"id": "61028", "url": "https://en.wikipedia.org/wiki?curid=61028", "title": "East", "text": "East\n\nEast is one of the four cardinal directions or points of the compass. It is the opposite direction from west.\n\nThe word \"east\" comes from Middle English \"est\", from Old English \"ēast\", which itself comes from the Proto-Germanic *\"aus-to-\" or *\"austra-\" \"east, toward the sunrise\", from Proto-Indo-European *aus- \"to shine,\" or \"dawn\". This is similar to Old High German \"*ōstar\" \"to the east\", Latin \"aurora\" \"dawn\", and Greek \"ēōs\" . \"Ēostre\", a Germanic goddess of dawn, might have been a personification of both dawn and the cardinal points.\n\nBy convention, the right hand side of a map is east. This convention has developed from the use of a compass, which places north at the top. However, on maps of planets such as Venus which rotate retrograde, the left hand side is east.\n\nTo go east using a compass for navigation, one sets a bearing or azimuth of 90°.\n\nEast is the direction toward which the Earth rotates about its axis, and therefore the general direction from which the Sun appears to rise. The practice of praying towards the East is older than Christianity, but has been adopted by this religion as the Orient was thought of as containing mankind's original home. Hence, some Christian churches have been traditionally oriented towards the east.\n\nThe \"Orient\" is the \"East\", traditionally comprising anything that belongs to the Eastern world, in relation to Europe. In English, it is largely a metonym for, and coterminous with, the continent of Asia, divided into the Far East, Middle East, and Near East. Despite this Eurocentric origin, these regions are still located to the east of the Geographical centre of Earth.\n\n"}
{"id": "9970499", "url": "https://en.wikipedia.org/wiki?curid=9970499", "title": "Este", "text": "Este\n\nEste may refer to:\n\n\n\n"}
{"id": "507354", "url": "https://en.wikipedia.org/wiki?curid=507354", "title": "Glossary of differential geometry and topology", "text": "Glossary of differential geometry and topology\n\nThis is a glossary of terms specific to differential geometry and differential topology. \nThe following three glossaries are closely related:\n\nSee also:\n\nWords in \"italics\" denote a self-reference to this glossary.\n\nAtlas\n\nBundle, see \"fiber bundle\".\n\nChart\n\nCobordism\n\nCodimension. The codimension of a submanifold is the dimension of the ambient space minus the dimension of the submanifold.\n\nConnected sum\n\nConnection\n\nCotangent bundle, the vector bundle of cotangent spaces on a manifold.\n\nCotangent space\n\nDiffeomorphism. Given two differentiable manifolds \n\"M\" and \"N\", a bijective map formula_1 from \"M\" to \"N\" is called a diffeomorphism if both formula_2 and its inverse formula_3 are smooth functions.\n\nDoubling, given a manifold \"M\" with boundary, doubling is taking two copies of \"M\" and identifying their boundaries. \nAs the result we get a manifold without boundary.\n\nEmbedding\n\nFiber. In a fiber bundle, π: \"E\" → \"B\" the preimage π(\"x\") of a point \"x\" in the base \"B\" is called the fiber over \"x\", often denoted \"E\".\n\nFiber bundle\n\nFrame. A frame at a point of a differentiable manifold \"M\" is a basis of the tangent space at the point.\n\nFrame bundle, the principal bundle of frames on a smooth manifold.\n\nFlow\n\nGenus\n\nHypersurface. A hypersurface is a submanifold of \"codimension\" one.\n\nImmersion\n\nLens space. A lens space is a quotient of the 3-sphere (or (2\"n\" + 1)-sphere) by a free isometric action of Z.\n\nManifold. A topological manifold is a locally Euclidean Hausdorff space. (In Wikipedia, a manifold need not be paracompact or second-countable.) A \"C\" manifold is a differentiable manifold whose chart overlap functions are \"k\" times continuously differentiable. A \"C\" or smooth manifold is a differentiable manifold whose chart overlap functions are infinitely continuously differentiable.\n\nNeat submanifold. A submanifold whose boundary equals its intersection with the boundary of the manifold into which it is embedded.\n\nParallelizable. A smooth manifold is parallelizable if it admits a smooth global frame. This is equivalent to the tangent bundle being trivial.\n\nPrincipal bundle. A principal bundle is a fiber bundle \"P\" → \"B\" together with an action on \"P\" by a Lie group \"G\" that preserves the fibers of \"P\" and acts simply transitively on those fibers.\n\nPullback\n\nSection\n\nSubmanifold, the image of a smooth embedding of a manifold.\n\nSubmersion\n\nSurface, a two-dimensional manifold or submanifold.\n\nSystole, least length of a noncontractible loop.\n\nTangent bundle, the vector bundle of tangent spaces on a differentiable manifold.\n\nTangent field, a \"section\" of the tangent bundle. Also called a \"vector field\".\n\nTangent space\n\nTorus\n\nTransversality. Two submanifolds \"M\" and \"N\" intersect transversally if at each point of intersection \"p\" their tangent spaces formula_4 and formula_5 generate the whole tangent space at \"p\" of the total manifold.\n\nTrivialization\n\nVector bundle, a fiber bundle whose fibers are vector spaces and whose transition functions are linear maps.\n\nVector field, a section of a vector bundle. More specifically, a vector field can mean a section of the tangent bundle.\n\nWhitney sum. A Whitney sum is an analog of the direct product for vector bundles. Given two vector bundles α and β over the same base \"B\" their cartesian product is a vector bundle over \"B\" ×\"B\". The diagonal map formula_6 induces a vector bundle over \"B\" called the Whitney sum of these vector bundles and denoted by α⊕β.\n"}
{"id": "20199287", "url": "https://en.wikipedia.org/wiki?curid=20199287", "title": "Graph canonization", "text": "Graph canonization\n\nIn graph theory, a branch of mathematics, graph canonization is the problem finding a canonical form of a given graph \"G\". A canonical form is a labeled graph Canon(\"G\") that is isomorphic to \"G\", such that every graph that is isomorphic to \"G\" has the same canonical form as \"G\". Thus, from a solution to the graph canonization problem, one could also solve the problem of graph isomorphism: to test whether two graphs \"G\" and \"H\" are isomorphic, compute their canonical forms Canon(\"G\") and Canon(\"H\"), and test whether these two canonical forms are identical.\n\nThe canonical form of a graph is an example of a complete graph invariant: every two isomorphic graphs have the same canonical form, and every two non-isomorphic graphs have different canonical forms. Conversely, every complete invariant of graphs may be used to construct a canonical form. The vertex set of an \"n\"-vertex graph may be identified with the integers from 1 to \"n\", and using such an identification a canonical form of a graph may also be described as a permutation of its vertices. Canonical forms of a graph are also called canonical labelings, and graph canonization is also sometimes known as graph canonicalization.\n\nClearly, the graph canonization problem is at least as computationally hard as the graph isomorphism problem. In fact, graph isomorphism is even AC-reducible to graph canonization. However it is still an open question whether the two problems are polynomial time equivalent.\n\nWhile existence of (deterministic) polynomial algorithms for graph isomorphism is still an open problem in computational complexity theory, in 1977 László Babai reported that with probability at least 1 − exp(−O(\"n\")), a simple vertex classification algorithm after only two refinement steps produces a canonical labeling of a graph chosen uniformly at random from the set of all \"n\"-vertex graphs. Small modifications and an added depth-first search step produce canonical labeling of such uniformly-chosen random graphs in linear expected time. This result sheds some light on the fact why many reported graph isomorphism algorithms behave well in practice. This was an important breakthrough in probabilistic complexity theory which became widely known in its manuscript form and which was still cited as an \"unpublished manuscript\" long after it was reported at a symposium.\n\nA commonly known canonical form is the lexicographically smallest graph within the isomorphism class, which is the graph of the class with lexicographically smallest adjacency matrix considered as a linear string.\nHowever, the computation of the lexicographically smallest graph is NP-hard.\n\nFor trees, a concise polynomial canonization algorithm requiring O(n) space is presented by . Begin by labeling each vertex with the string 01. Iteratively for each non-leaf x remove the leading 0 and trailing 1 from x's label; then sort x's label along with the labels of all adjacent leaves in lexicographic order. Concatenate these sorted labels, add back a leading 0 and trailing 1, make this the new label of x, and delete the adjacent leaves. If there are two vertices remaining, concatenate their labels in lexicographic order.\n\nGraph canonization is the essence of many graph isomorphism algorithms. One of the leading tools is Nauty.\n\nA common application of graph canonization is in graphical data mining, in particular in chemical database applications.\n\nA number of identifiers for chemical substances, such as SMILES and InChI use canonicalization steps in their computation, which is essentially the canonicalization of the graph which represents the molecule.\n\nThese identifiers are designed to provide a standard (and sometimes human-readable) way to encode molecular information and to facilitate the search for such information in databases and on the web.\n"}
{"id": "2349088", "url": "https://en.wikipedia.org/wiki?curid=2349088", "title": "Guillotine problem", "text": "Guillotine problem\n\nThe guillotine problem is a problem in combinatorial geometry and in printing. \n\nClosely related to packing problems and specifically to cutting stock and bin packing problems, it is the question of how to get the maximum number of sheets of one rectangular size out of a larger sheet, only orthogonal cuts that bisect one component of the sheet are allowed, as on a paper cutting guillotine.\n\nThe Guillotine problem is important in glass machining. Glass sheets are scored along horizontal and vertical lines and then broken along these lines to obtain smaller panels.\n\nLike the cutting stock problem, it is NP hard, but various approximate and exact solutions have been devised.\n"}
{"id": "59945", "url": "https://en.wikipedia.org/wiki?curid=59945", "title": "History of logic", "text": "History of logic\n\nThe history of logic deals with the study of the development of the science of valid inference (logic). Formal logics developed in ancient times in India, China, and Greece. Greek methods, particularly Aristotelian logic (or term logic) as found in the \"Organon\", found wide application and acceptance in Western science and mathematics for millennia. The Stoics, especially Chrysippus, began the development of predicate logic.\n\nChristian and Islamic philosophers such as Boethius (died 524), Ibn Sina (Avicenna, died 1037) and William of Ockham (died 1347) further developed Aristotle's logic in the Middle Ages, reaching a high point in the mid-fourteenth century, with Jean Buridan. The period between the fourteenth century and the beginning of the nineteenth century saw largely decline and neglect, and at least one historian of logic regards this time as barren. Empirical methods ruled the day, as evidenced by Sir Francis Bacon's \"Novum Organon\" of 1620.\n\nLogic revived in the mid-nineteenth century, at the beginning of a revolutionary period when the subject developed into a rigorous and formal discipline which took as its exemplar the exact method of proof used in mathematics, a hearkening back to the Greek tradition. The development of the modern \"symbolic\" or \"mathematical\" logic during this period by the likes of Boole, Frege, Russell, and Peano is the most significant in the two-thousand-year history of logic, and is arguably one of the most important and remarkable events in human intellectual history.\n\nProgress in mathematical logic in the first few decades of the twentieth century, particularly arising from the work of Gödel and Tarski, had a significant impact on analytic philosophy and philosophical logic, particularly from the 1950s onwards, in subjects such as modal logic, temporal logic, deontic logic, and relevance logic.\n\nLogic began independently in ancient India and continued to develop to early modern times without any known influence from Greek logic. Medhatithi Gautama (c. 6th century BC) founded the \"anviksiki\" school of logic. The \"Mahabharata\" (12.173.45), around the 5th century BC, refers to the \"anviksiki\" and \"tarka\" schools of logic. (c. 5th century BC) developed a form of logic (to which Boolean logic has some similarities) for his formulation of Sanskrit grammar. Logic is described by Chanakya (c. 350-283 BC) in his \"Arthashastra\" as an independent field of inquiry.\n\nTwo of the six Indian schools of thought deal with logic: Nyaya and Vaisheshika. The Nyaya Sutras of Aksapada Gautama (c. 2nd century AD) constitute the core texts of the Nyaya school, one of the six orthodox schools of Hindu philosophy. This realist school developed a rigid five-member schema of inference involving an initial premise, a reason, an example, an application, and a conclusion. The idealist Buddhist philosophy became the chief opponent to the Naiyayikas. Nagarjuna (c. 150-250 AD), the founder of the Madhyamika (\"Middle Way\") developed an analysis known as the catuṣkoṭi (Sanskrit), a \"four-cornered\" system of argumentation that involves the systematic examination and rejection of each of the 4 possibilities of a proposition, \"P\":\n\nHowever, Dignaga (c 480-540 AD) is sometimes said to have developed a formal syllogism, and it was through him and his successor, Dharmakirti, that Buddhist logic reached its height; it is contested whether their analysis actually constitutes a formal syllogistic system. In particular, their analysis centered on the definition of an inference-warranting relation, \"vyapti\", also known as invariable concomitance or pervasion. To this end, a doctrine known as \"apoha\" or differentiation was developed. This involved what might be called inclusion and exclusion of defining properties.\n\nDignāga's famous \"wheel of reason\" (\"Hetucakra\") is a method of indicating when one thing (such as smoke) can be taken as an invariable sign of another thing (like fire), but the inference is often inductive and based on past observation. Matilal remarks that Dignāga's analysis is much like John Stuart Mill's Joint Method of Agreement and Difference, which is inductive.\n\nIn addition, the traditional five-member Indian syllogism, though deductively valid, has repetitions that are unnecessary to its logical validity. As a result, some commentators see the traditional Indian syllogism as a rhetorical form that is entirely natural in many cultures of the world, and yet not as a logical form—not in the sense that all logically unnecessary elements have been omitted for the sake of analysis.\n\nIn China, a contemporary of Confucius, Mozi, \"Master Mo\", is credited with founding the Mohist school, whose canons dealt with issues relating to valid inference and the conditions of correct conclusions. In particular, one of the schools that grew out of Mohism, the Logicians, are credited by some scholars for their early investigation of formal logic. Due to the harsh rule of Legalism in the subsequent Qin Dynasty, this line of investigation disappeared in China until the introduction of Indian philosophy by Buddhists.\n\nValid reasoning has been employed in all periods of human history. However, logic studies the \"principles\" of valid reasoning, inference and demonstration. It is probable that the idea of demonstrating a conclusion first arose in connection with geometry, which originally meant the same as \"land measurement\". The ancient Egyptians discovered geometry, including the formula for the volume of a truncated pyramid. Ancient Babylon was also skilled in mathematics. Esagil-kin-apli's medical \"Diagnostic Handbook\" in the 11th century BC was based on a logical set of axioms and assumptions, while Babylonian astronomers in the 8th and 7th centuries BC employed an internal logic within their predictive planetary systems, an important contribution to the philosophy of science.\n\nWhile the ancient Egyptians empirically discovered some truths of geometry, the great achievement of the ancient Greeks was to replace empirical methods by demonstrative proof. Both Thales and Pythagoras of the Pre-Socratic philosophers seem aware of geometry's methods.\n\nFragments of early proofs are preserved in the works of Plato and Aristotle, and the idea of a deductive system was probably known in the Pythagorean school and the Platonic Academy. The proofs of Euclid of Alexandria are a paradigm of Greek geometry. The three basic principles of geometry are as follows:\n\nFurther evidence that early Greek thinkers were concerned with the principles of reasoning is found in the fragment called \"dissoi logoi\", probably written at the beginning of the fourth century BC. This is part of a protracted debate about truth and falsity. In the case of the classical Greek city-states, interest in argumentation was also stimulated by the activities of the Rhetoricians or Orators and the Sophists, who used arguments to defend or attack a thesis, both in legal and political contexts.\nIt is said Thales, most widely regarded as the first philosopher in the Greek tradition, measured the height of the pyramids by their shadows at the moment when his own shadow was equal to his height. Thales was said to have had a sacrifice in celebration of discovering Thales' theorem just as Pythagoras had the Pythagorean theorem.\n\nThales is the first known individual to use deductive reasoning applied to geometry, by deriving four corollaries to his theorem, and the first known individual to whom a mathematical discovery has been attributed. Indian and Babylonian mathematicians knew his theorem for special cases before he proved it. It is believed that Thales learned that an angle inscribed in a semicircle is a right angle during his travels to Babylon.\n\nBefore 520 BC, on one of his visits to Egypt or Greece, Pythagoras might have met the c. 54 years older Thales. The systematic study of proof seems to have begun with the school of Pythagoras (i. e. the Pythagoreans) in the late sixth century BC. Indeed, the Pythagoreans, believing all was number, are the first philosophers to emphasize \"form\" rather than \"matter\".\n\nThe writing of Heraclitus (c. 535 – c. 475 BC) was the first place where the word \"logos\" was given special attention in ancient Greek philosophy, Heraclitus held that everything changes and all was fire and conflicting opposites, seemingly unified only by this \"Logos\". He is known for his obscure sayings.\nIn contrast to Heraclitus, Parmenides held that all is one and nothing changes. He may have been a dissident Pythagorean, disagreeing that One (a number) produced the many. \"X is not\" must always be false or meaningless. What exists can in no way not exist. Our sense perceptions with its noticing of generation and destruction are in grievous error. Instead of sense perception, Parmenides advocated \"logos\" as the means to Truth. He has been called the discoverer of logic,\n\nZeno of Elea, a pupil of Parmenides, had the idea of a standard argument pattern found in the method of proof known as \"reductio ad absurdum\". This is the technique of drawing an obviously false (that is, \"absurd\") conclusion from an assumption, thus demonstrating that the assumption is false. Therefore, Zeno and his teacher are seen as the first to apply the art of logic. Plato's dialogue Parmenides portrays Zeno as claiming to have written a book defending the monism of Parmenides by demonstrating the absurd consequence of assuming that there is plurality. Zeno famously used this method to develop his paradoxes in his arguments against motion. Such \"dialectic\" reasoning later became popular. The members of this school were called \"dialecticians\" (from a Greek word meaning \"to discuss\").\n\nNone of the surviving works of the great fourth-century philosopher Plato (428–347 BC) include any formal logic, but they include important contributions to the field of philosophical logic. Plato raises three questions:\n\n\nThe first question arises in the dialogue \"Theaetetus\", where Plato identifies thought or opinion with talk or discourse (\"logos\"). The second question is a result of Plato's theory of Forms. Forms are not things in the ordinary sense, nor strictly ideas in the mind, but they correspond to what philosophers later called universals, namely an abstract entity common to each set of things that have the same name. In both the \"Republic\" and the \"Sophist\", Plato suggests that the necessary connection between the assumptions of a valid argument and its conclusion corresponds to a necessary connection between \"forms\". The third question is about definition. Many of Plato's dialogues concern the search for a definition of some important concept (justice, truth, the Good), and it is likely that Plato was impressed by the importance of definition in mathematics. What underlies every definition is a Platonic Form, the common nature present in different particular things. Thus, a definition reflects the ultimate object of understanding, and is the foundation of all valid inference. This had a great influence on Plato's student Aristotle, in particular Aristotle's notion of the essence of a thing.\n\nThe logic of Aristotle, and particularly his theory of the syllogism, has had an enormous influence in Western thought. Aristotle was the first logician to attempt a systematic analysis of logical syntax, of noun (or \"term\"), and of verb. He was the first \"formal logician\", in that he demonstrated the principles of reasoning by employing variables to show the underlying logical form of an argument. He sought relations of dependence which characterize necessary inference, and distinguished the validity of these relations, from the truth of the premises (the soundness of the argument). He was the first to deal with the principles of contradiction and excluded middle in a systematic way. \n\nHis logical works, called the \"Organon\", are the earliest formal study of logic that have come down to modern times. Though it is difficult to determine the dates, the probable order of writing of Aristotle's logical works is:\n\nThese works are of outstanding importance in the history of logic. In the \"Categories\", he attempts to discern all the possible things to which a term can refer; this idea underpins his philosophical work \"Metaphysics\", which itself had a profound influence on Western thought. \n\nHe also developed a theory of non-formal logic (\"i.e.,\" the theory of fallacies), which is presented in \"Topics\" and \"Sophistical Refutations\".\n\n\"On Interpretation\" contains a comprehensive treatment of the notions of opposition and conversion; chapter 7 is at the origin of the square of opposition (or logical square); chapter 9 contains the beginning of modal logic.\n\nThe \"Prior Analytics\" contains his exposition of the \"syllogism\", where three important principles are applied for the first time in history: the use of variables, a purely formal treatment, and the use of an axiomatic system.\n\nThe other great school of Greek logic is that of the Stoics. Stoic logic traces its roots back to the late 5th century BC philosopher Euclid of Megara, a pupil of Socrates and slightly older contemporary of Plato, probably following in the tradition of Parmenides and Zeno. His pupils and successors were called \"Megarians\", or \"Eristics\", and later the \"Dialecticians\". The two most important dialecticians of the Megarian school were Diodorus Cronus and Philo, who were active in the late 4th century BC. \n\nThe Stoics adopted the Megarian logic and systemized it. The most important member of the school was Chrysippus (c. 278–c. 206 BC), who was its third head, and who formalized much of Stoic doctrine. He is supposed to have written over 700 works, including at least 300 on logic, almost none of which survive. Unlike with Aristotle, we have no complete works by the Megarians or the early Stoics, and have to rely mostly on accounts (sometimes hostile) by later sources, including prominently Diogenes Laertius, Sextus Empiricus, Galen, Aulus Gellius, Alexander of Aphrodisias, and Cicero.\n\nThree significant contributions of the Stoic school were (i) their account of modality, (ii) their theory of the Material conditional, and (iii) their account of meaning and truth.\n\n\n\n\nThe works of Al-Kindi, Al-Farabi, Avicenna, Al-Ghazali, Averroes and other Muslim logicians were based on Aristotelian logic and were important in communicating the ideas of the ancient world to the medieval West. Al-Farabi (Alfarabi) (873–950) was an Aristotelian logician who discussed the topics of future contingents, the number and relation of the categories, the relation between logic and grammar, and non-Aristotelian forms of inference. Al-Farabi also considered the theories of conditional syllogisms and analogical inference, which were part of the Stoic tradition of logic rather than the Aristotelian.\n\nIbn Sina (Avicenna) (980–1037) was the founder of Avicennian logic, which replaced Aristotelian logic as the dominant system of logic in the Islamic world, and also had an important influence on Western medieval writers such as Albertus Magnus. Avicenna wrote on the hypothetical syllogism and on the propositional calculus, which were both part of the Stoic logical tradition. He developed an original \"temporally modalized\" syllogistic theory, involving temporal logic and modal logic. He also made use of inductive logic, such as the methods of agreement, difference, and concomitant variation which are critical to the scientific method. One of Avicenna's ideas had a particularly important influence on Western logicians such as William of Ockham: Avicenna's word for a meaning or notion (\"ma'na\"), was translated by the scholastic logicians as the Latin \"intentio\"; in medieval logic and epistemology, this is a sign in the mind that naturally represents a thing. This was crucial to the development of Ockham's conceptualism: A universal term (\"e.g.,\" \"man\") does not signify a thing existing in reality, but rather a sign in the mind (\"intentio in intellectu\") which represents many things in reality; Ockham cites Avicenna's commentary on \"Metaphysics\" V in support of this view.\n\nFakhr al-Din al-Razi (b. 1149) criticised Aristotle's \"first figure\" and formulated an early system of inductive logic, foreshadowing the system of inductive logic developed by John Stuart Mill (1806–1873). Al-Razi's work was seen by later Islamic scholars as marking a new direction for Islamic logic, towards a Post-Avicennian logic. This was further elaborated by his student Afdaladdîn al-Khûnajî (d. 1249), who developed a form of logic revolving around the subject matter of conceptions and assents. In response to this tradition, Nasir al-Din al-Tusi (1201–1274) began a tradition of Neo-Avicennian logic which remained faithful to Avicenna's work and existed as an alternative to the more dominant Post-Avicennian school over the following centuries.\n\nThe Illuminationist school was founded by Shahab al-Din Suhrawardi (1155–1191), who developed the idea of \"decisive necessity\", which refers to the reduction of all modalities (necessity, possibility, contingency and impossibility) to the single mode of necessity. Ibn al-Nafis (1213–1288) wrote a book on Avicennian logic, which was a commentary of Avicenna's \"Al-Isharat\" (\"The Signs\") and \"Al-Hidayah\" (\"The Guidance\"). Ibn Taymiyyah (1263–1328), wrote the \"Ar-Radd 'ala al-Mantiqiyyin\", where he argued against the usefulness, though not the validity, of the syllogism and in favour of inductive reasoning. Ibn Taymiyyah also argued against the certainty of syllogistic arguments and in favour of analogy; his argument is that concepts founded on induction are themselves not certain but only probable, and thus a syllogism based on such concepts is no more certain than an argument based on analogy. He further claimed that induction itself is founded on a process of analogy. His model of analogical reasoning was based on that of juridical arguments. This model of analogy has been used in the recent work of John F. Sowa.\n\nThe \"Sharh al-takmil fi'l-mantiq\" written by Muhammad ibn Fayd Allah ibn Muhammad Amin al-Sharwani in the 15th century is the last major Arabic work on logic that has been studied. However, \"thousands upon thousands of pages\" on logic were written between the 14th and 19th centuries, though only a fraction of the texts written during this period have been studied by historians, hence little is known about the original work on Islamic logic produced during this later period.\n\n\"Medieval logic\" (also known as \"Scholastic logic\") generally means the form of Aristotelian logic developed in medieval Europe throughout roughly the period 1200–1600. For centuries after Stoic logic had been formulated, it was the dominant system of logic in the classical world. When the study of logic resumed after the Dark Ages, the main source was the work of the Christian philosopher Boethius, who was familiar with some of Aristotle's logic, but almost none of the work of the Stoics. Until the twelfth century, the only works of Aristotle available in the West were the \"Categories\", \"On Interpretation\", and Boethius's translation of the Isagoge of Porphyry (a commentary on the Categories). These works were known as the \"Old Logic\" (\"Logica Vetus\" or \"Ars Vetus\"). An important work in this tradition was the \"Logica Ingredientibus\" of Peter Abelard (1079–1142). His direct influence was small, but his influence through pupils such as John of Salisbury was great, and his method of applying rigorous logical analysis to theology shaped the way that theological criticism developed in the period that followed.\n\nBy the early thirteenth century, the remaining works of Aristotle's \"Organon\" (including the \"Prior Analytics\", \"Posterior Analytics\", and the \"Sophistical Refutations\") had been recovered in the West. Logical work until then was mostly paraphrasis or commentary on the work of Aristotle. The period from the middle of the thirteenth to the middle of the fourteenth century was one of significant developments in logic, particularly in three areas which were original, with little foundation in the Aristotelian tradition that came before. These were:\n\n\nThe last great works in this tradition are the \"Logic\" of John Poinsot (1589–1644, known as John of St Thomas), the \"Metaphysical Disputations\" of Francisco Suarez (1548–1617), and the \"Logica Demonstrativa\" of Giovanni Girolamo Saccheri (1667–1733).\n\n\"Traditional logic\" generally means the textbook tradition that begins with Antoine Arnauld's and Pierre Nicole's \"Logic, or the Art of Thinking\", better known as the \"Port-Royal Logic\". Published in 1662, it was the most influential work on logic after Aristotle until the nineteenth century. The book presents a loosely Cartesian doctrine (that the proposition is a combining of ideas rather than terms, for example) within a framework that is broadly derived from Aristotelian and medieval term logic. Between 1664 and 1700, there were eight editions, and the book had considerable influence after that. The Port-Royal introduces the concepts of extension and intension. The account of propositions that Locke gives in the \"Essay\" is essentially that of the Port-Royal: \"Verbal propositions, which are words, [are] the signs of our ideas, put together or separated in affirmative or negative sentences. So that proposition consists in the putting together or separating these signs, according as the things which they stand for agree or disagree.\"\n\nDudley Fenner helped popularize Ramist logic, a reaction against Aristotle. Another influential work was the \"Novum Organum\" by Francis Bacon, published in 1620. The title translates as \"new instrument\". This is a reference to Aristotle's work known as the \"Organon\". In this work, Bacon rejects the syllogistic method of Aristotle in favor of an alternative procedure \"which by slow and faithful toil gathers information from things and brings it into understanding\". This method is known as inductive reasoning, a method which starts from empirical observation and proceeds to lower axioms or propositions; from these lower axioms, more general ones can be induced. For example, in finding the cause of a \"phenomenal nature\" such as heat, 3 lists should be constructed:\nThen, the \"form nature\" (or cause) of heat may be defined as that which is common to every situation of the presence list, and which is lacking from every situation of the absence list, and which varies by degree in every situation of the variability list.\n\nOther works in the textbook tradition include Isaac Watts's \"Logick: Or, the Right Use of Reason\" (1725), Richard Whately's \"Logic\" (1826), and John Stuart Mill's \"A System of Logic\" (1843). Although the latter was one of the last great works in the tradition, Mill's view that the foundations of logic lie in introspection influenced the view that logic is best understood as a branch of psychology, a view which dominated the next fifty years of its development, especially in Germany.\n\nG.W.F. Hegel indicated the importance of logic to his philosophical system when he condensed his extensive \"Science of Logic\" into a shorter work published in 1817 as the first volume of his \"Encyclopaedia of the Philosophical Sciences.\" The \"Shorter\" or \"Encyclopaedia\" \"Logic\", as it is often known, lays out a series of transitions which leads from the most empty and abstract of categories—Hegel begins with \"Pure Being\" and \"Pure Nothing\"—to the \"Absolute, the category which contains and resolves all the categories which preceded it. Despite the title, Hegel's \"Logic\" is not really a contribution to the science of valid inference. Rather than deriving conclusions about concepts through valid inference from premises, Hegel seeks to show that thinking about one concept compels thinking about another concept (one cannot, he argues, possess the concept of \"Quality\" without the concept of \"Quantity\"); this compulsion is, supposedly, not a matter of individual psychology, because it arises almost organically from the content of the concepts themselves. His purpose is to show the rational structure of the \"Absolute\"—indeed of rationality itself. The method by which thought is driven from one concept to its contrary, and then to further concepts, is known as the Hegelian dialectic.\n\nAlthough Hegel's \"Logic\" has had little impact on mainstream logical studies, its influence can be seen elsewhere:\n\nBetween the work of Mill and Frege stretched half a century during which logic was widely treated as a descriptive science, an empirical study of the structure of reasoning, and thus essentially as a branch of psychology. The German psychologist Wilhelm Wundt, for example, discussed deriving \"the logical from the psychological laws of thought\", emphasizing that \"psychological thinking is always the more comprehensive form of thinking.\" This view was widespread among German philosophers of the period:\nSuch was the dominant view of logic in the years following Mill's work. This psychological approach to logic was rejected by Gottlob Frege. It was also subjected to an extended and destructive critique by Edmund Husserl in the first volume of his \"Logical Investigations\" (1900), an assault which has been described as \"overwhelming\". Husserl argued forcefully that grounding logic in psychological observations implied that all logical truths remained unproven, and that skepticism and relativism were unavoidable consequences.\n\nSuch criticisms did not immediately extirpate what is called \"psychologism\". For example, the American philosopher Josiah Royce, while acknowledging the force of Husserl's critique, remained \"unable to doubt\" that progress in psychology would be accompanied by progress in logic, and vice versa.\n\nThe period between the fourteenth century and the beginning of the nineteenth century had been largely one of decline and neglect, and is generally regarded as barren by historians of logic. The revival of logic occurred in the mid-nineteenth century, at the beginning of a revolutionary period where the subject developed into a rigorous and formalistic discipline whose exemplar was the exact method of proof used in mathematics. The development of the modern \"symbolic\" or \"mathematical\" logic during this period is the most significant in the 2000-year history of logic, and is arguably one of the most important and remarkable events in human intellectual history.\n\nA number of features distinguish modern logic from the old Aristotelian or traditional logic, the most important of which are as follows: Modern logic is fundamentally a \"calculus\" whose rules of operation are determined only by the \"shape\" and not by the \"meaning\" of the symbols it employs, as in mathematics. Many logicians were impressed by the \"success\" of mathematics, in that there had been no prolonged dispute about any truly mathematical result. C.S. Peirce noted that even though a mistake in the evaluation of a definite integral by Laplace led to an error concerning the moon's orbit that persisted for nearly 50 years, the mistake, once spotted, was corrected without any serious dispute. Peirce contrasted this with the disputation and uncertainty surrounding traditional logic, and especially reasoning in metaphysics. He argued that a truly \"exact\" logic would depend upon mathematical, i.e., \"diagrammatic\" or \"iconic\" thought. \"Those who follow such methods will ... escape all error except such as will be speedily corrected after it is once suspected\". Modern logic is also \"constructive\" rather than \"abstractive\"; i.e., rather than abstracting and formalising theorems derived from ordinary language (or from psychological intuitions about validity), it constructs theorems by formal methods, then looks for an interpretation in ordinary language. It is entirely symbolic, meaning that even the logical constants (which the medieval logicians called \"syncategoremata\") and the categoric terms are expressed in symbols.\n\nThe development of modern logic falls into roughly five periods:\n\nThe idea that inference could be represented by a purely mechanical process is found as early as Raymond Llull, who proposed a (somewhat eccentric) method of drawing conclusions by a system of concentric rings. The work of logicians such as the Oxford Calculators led to a method of using letters instead of writing out logical calculations (\"calculationes\") in words, a method used, for instance, in the \"Logica magna\" by Paul of Venice. Three hundred years after Llull, the English philosopher and logician Thomas Hobbes suggested that all logic and reasoning could be reduced to the mathematical operations of addition and subtraction. The same idea is found in the work of Leibniz, who had read both Llull and Hobbes, and who argued that logic can be represented through a combinatorial process or calculus. But, like Llull and Hobbes, he failed to develop a detailed or comprehensive system, and his work on this topic was not published until long after his death. Leibniz says that ordinary languages are subject to \"countless ambiguities\" and are unsuited for a calculus, whose task is to expose mistakes in inference arising from the forms and structures of words; hence, he proposed to identify an alphabet of human thought comprising fundamental concepts which could be composed to express complex ideas, and create a \"calculus ratiocinator\" that would make all arguments \"as tangible as those of the Mathematicians, so that we can find our error at a glance, and when there are disputes among persons, we can simply say: Let us calculate.\"\n\nGergonne (1816) said that reasoning does not have to be about objects about which one has perfectly clear ideas, because algebraic operations can be carried out without having any idea of the meaning of the symbols involved. Bolzano anticipated a fundamental idea of modern proof theory when he defined logical consequence or \"deducibility\" in terms of variables:Hence I say that propositions formula_1, formula_2, formula_3,… are \"deducible\" from propositions formula_4, formula_5, formula_6, formula_7,… with respect to variable parts formula_8, formula_9,…, if every class of ideas whose substitution for formula_8, formula_9,… makes all of formula_4, formula_5, formula_6, formula_7,… true, also makes all of formula_1, formula_2, formula_3,… true. Occasionally, since it is customary, I shall say that propositions formula_1, formula_2, formula_3,… \"follow\", or can be \"inferred\" or \"derived\", from formula_4, formula_5, formula_6, formula_7,…. Propositions formula_4, formula_5, formula_6, formula_7,… I shall call the \"premises\", formula_1, formula_2, formula_3,… the \"conclusions.\"This is now known as semantic validity.\n\nModern logic begins with what is known as the \"algebraic school\", originating with Boole and including Peirce, Jevons, Schröder, and Venn. Their objective was to develop a calculus to formalise reasoning in the area of classes, propositions, and probabilities. The school begins with Boole's seminal work \"Mathematical Analysis of Logic\" which appeared in 1847, although De Morgan (1847) is its immediate precursor. The fundamental idea of Boole's system is that algebraic formulae can be used to express logical relations. This idea occurred to Boole in his teenage years, working as an usher in a private school in Lincoln, Lincolnshire. For example, let x and y stand for classes let the symbol \"=\" signify that the classes have the same members, xy stand for the class containing all and only the members of x and y and so on. Boole calls these \"elective symbols\", i.e. symbols which select certain objects for consideration. An expression in which elective symbols are used is called an \"elective function\", and an equation of which the members are elective functions, is an \"elective equation\". The theory of elective functions and their \"development\" is essentially the modern idea of truth-functions and their expression in disjunctive normal form.\n\nBoole's system admits of two interpretations, in class logic, and propositional logic. Boole distinguished between \"primary propositions\" which are the subject of syllogistic theory, and \"secondary propositions\", which are the subject of propositional logic, and showed how under different \"interpretations\" the same algebraic system could represent both. An example of a primary proposition is \"All inhabitants are either Europeans or Asiatics.\" An example of a secondary proposition is \"Either all inhabitants are Europeans or they are all Asiatics.\" These are easily distinguished in modern propositional calculus, where it is also possible to show that the first follows from the second, but it is a significant disadvantage that there is no way of representing this in the Boolean system.\n\nIn his \"Symbolic Logic\" (1881), John Venn used diagrams of overlapping areas to express Boolean relations between classes or truth-conditions of propositions. In 1869 Jevons realised that Boole's methods could be mechanised, and constructed a \"logical machine\" which he showed to the Royal Society the following year. In 1885 Allan Marquand proposed an electrical version of the machine that is still extant (picture at the Firestone Library).\n\nThe defects in Boole's system (such as the use of the letter \"v\" for existential propositions) were all remedied by his followers. Jevons published \"Pure Logic, or the Logic of Quality apart from Quantity\" in 1864, where he suggested a symbol to signify exclusive or, which allowed Boole's system to be greatly simplified. This was usefully exploited by Schröder when he set out theorems in parallel columns in his \"Vorlesungen\" (1890–1905). Peirce (1880) showed how all the Boolean elective functions could be expressed by the use of a single primitive binary operation, \"neither ... nor ...\" and equally well \"not both ... and ...\", however, like many of Peirce's innovations, this remained unknown or unnoticed until Sheffer rediscovered it in 1913. Boole's early work also lacks the idea of the logical sum which originates in Peirce (1867), Schröder (1877) and Jevons (1890), and the concept of inclusion, first suggested by Gergonne (1816) and clearly articulated by Peirce (1870).\n\nThe success of Boole's algebraic system suggested that all logic must be capable of algebraic representation, and there were attempts to express a logic of relations in such form, of which the most ambitious was Schröder's monumental \"Vorlesungen über die Algebra der Logik\" (\"Lectures on the Algebra of Logic\", vol iii 1895), although the original idea was again anticipated by Peirce.\n\nBoole's unwavering acceptance of Aristotle's logic is emphasized by the historian of logic John Corcoran in an accessible introduction to \"Laws of Thought\" Corcoran also wrote a point-by-point comparison of \"Prior Analytics\" and \"Laws of Thought\". According to Corcoran, Boole fully accepted and endorsed Aristotle's logic. Boole's goals were \"to go under, over, and beyond\" Aristotle's logic by 1) providing it with mathematical foundations involving equations, 2) extending the class of problems it could treat — from assessing validity to solving equations — and 3) expanding the range of applications it could handle — e.g. from propositions having only two terms to those having arbitrarily many.\n\nMore specifically, Boole agreed with what Aristotle said; Boole's 'disagreements', if they might be called that, concern what Aristotle did not say. \nFirst, in the realm of foundations, Boole reduced the four propositional forms of Aristotelian logic to formulas in the form of equations — by itself a revolutionary idea. \nSecond, in the realm of logic's problems, Boole's addition of equation solving to logic — another revolutionary idea — involved Boole's doctrine that Aristotle's rules of inference (the \"perfect syllogisms\") must be supplemented by rules for equation solving. \nThird, in the realm of applications, Boole's system could handle multi-term propositions and arguments whereas Aristotle could handle only two-termed subject-predicate propositions and arguments. For example, Aristotle's system could not deduce \"No quadrangle that is a square is a rectangle that is a rhombus\" from \"No square that is a quadrangle is a rhombus that is a rectangle\" or from \"No rhombus that is a rectangle is a square that is a quadrangle\".\n\nAfter Boole, the next great advances were made by the German mathematician Gottlob Frege. Frege's objective was the program of Logicism, i.e. demonstrating that arithmetic is identical with logic. Frege went much further than any of his predecessors in his rigorous and formal approach to logic, and his calculus or Begriffsschrift is important. Frege also tried to show that the concept of number can be defined by purely logical means, so that (if he was right) logic includes arithmetic and all branches of mathematics that are reducible to arithmetic. He was not the first writer to suggest this. In his pioneering work \"Die Grundlagen der Arithmetik\" (The Foundations of Arithmetic), sections 15–17, he acknowledges the efforts of Leibniz, J.S. Mill as well as Jevons, citing the latter's claim that \"algebra is a highly developed logic, and number but logical discrimination.\"\n\nFrege's first work, the \"Begriffsschrift\" (\"concept script\") is a rigorously axiomatised system of propositional logic, relying on just two connectives (negational and conditional), two rules of inference (\"modus ponens\" and substitution), and six axioms. Frege referred to the \"completeness\" of this system, but was unable to prove this. The most significant innovation, however, was his explanation of the quantifier in terms of mathematical functions. Traditional logic regards the sentence \"Caesar is a man\" as of fundamentally the same form as \"all men are mortal.\" Sentences with a proper name subject were regarded as universal in character, interpretable as \"every Caesar is a man\". At the outset Frege abandons the traditional \"concepts \"subject\" and \"predicate\"\", replacing them with \"argument\" and \"function\" respectively, which he believes \"will stand the test of time. It is easy to see how regarding a content as a function of an argument leads to the formation of concepts. Furthermore, the demonstration of the connection between the meanings of the words \"if, and, not, or, there is, some, all,\" and so forth, deserves attention\". Frege argued that the quantifier expression \"all men\" does not have the same logical or semantic form as \"all men\", and that the universal proposition \"every A is B\" is a complex proposition involving two \"functions\", namely ' – is A' and ' – is B' such that whatever satisfies the first, also satisfies the second. In modern notation, this would be expressed as\n\nIn English, \"for all x, if Ax then Bx\". Thus only singular propositions are of subject-predicate form, and they are irreducibly singular, i.e. not reducible to a general proposition. Universal and particular propositions, by contrast, are not of simple subject-predicate form at all. If \"all mammals\" were the logical subject of the sentence \"all mammals are land-dwellers\", then to negate the whole sentence we would have to negate the predicate to give \"all mammals are \"not\" land-dwellers\". But this is not the case. This functional analysis of ordinary-language sentences later had a great impact on philosophy and linguistics.\n\nThis means that in Frege's calculus, Boole's \"primary\" propositions can be represented in a different way from \"secondary\" propositions. \"All inhabitants are either men or women\" is\n\nwhereas \"All the inhabitants are men or all the inhabitants are women\" is\n\nAs Frege remarked in a critique of Boole's calculus:\n\nAs well as providing a unified and comprehensive system of logic, Frege's calculus also resolved the ancient problem of multiple generality. The ambiguity of \"every girl kissed a boy\" is difficult to express in traditional logic, but Frege's logic resolves this through the different scope of the quantifiers. Thus\n\nmeans that to every girl there corresponds some boy (any one will do) who the girl kissed. But\n\nmeans that there is some particular boy whom every girl kissed. Without this device, the project of logicism would have been doubtful or impossible. Using it, Frege provided a definition of the ancestral relation, of the many-to-one relation, and of mathematical induction.\n\nThis period overlaps with the work of what is known as the \"mathematical school\", which included Dedekind, Pasch, Peano, Hilbert, Zermelo, Huntington, Veblen and Heyting. Their objective was the axiomatisation of branches of mathematics like geometry, arithmetic, analysis and set theory. Most notable was Hilbert's Program, which sought to ground all of mathematics to a finite set of axioms, proving its consistency by \"finitistic\" means and providing a procedure which would decide the truth or falsity of any mathematical statement. The standard axiomatization of the natural numbers is named the Peano axioms in his honor. Peano maintained a clear distinction between mathematical and logical symbols. While unaware of Frege's work, he independently recreated his logical apparatus based on the work of Boole and Schröder.\n\nThe logicist project received a near-fatal setback with the discovery of a paradox in 1901 by Bertrand Russell. This proved Frege's naive set theory led to a contradiction. Frege's theory contained the axiom that for any formal criterion, there is a set of all objects that meet the criterion. Russell showed that a set containing exactly the sets that are not members of themselves would contradict its own definition (if it is not a member of itself, it is a member of itself, and if it is a member of itself, it is not). This contradiction is now known as Russell's paradox. One important method of resolving this paradox was proposed by Ernst Zermelo. Zermelo set theory was the first axiomatic set theory. It was developed into the now-canonical Zermelo–Fraenkel set theory (ZF). Russell's paradox symbolically is as follows:\n\nThe monumental Principia Mathematica, a three-volume work on the foundations of mathematics, written by Russell and Alfred North Whitehead and published 1910–13 also included an attempt to resolve the paradox, by means of an elaborate system of types: a set of elements is of a different type than is each of its elements (set is not the element; one element is not the set) and one cannot speak of the \"set of all sets\". The \"Principia\" was an attempt to derive all mathematical truths from a well-defined set of axioms and inference rules in symbolic logic.\n\nThe names of Gödel and Tarski dominate the 1930s, a crucial period in the development of metamathematics – the study of mathematics using mathematical methods to produce metatheories, or mathematical theories about other mathematical theories. Early investigations into metamathematics had been driven by Hilbert's program. Work on metamathematics culminated in the work of Gödel, who in 1929 showed that a given first-order sentence is deducible if and only if it is logically valid – i.e. it is true in every structure for its language. This is known as Gödel's completeness theorem. A year later, he proved two important theorems, which showed Hibert's program to be unattainable in its original form. The first is that no consistent system of axioms whose theorems can be listed by an effective procedure such as an algorithm or computer program is capable of proving all facts about the natural numbers. For any such system, there will always be statements about the natural numbers that are true, but that are unprovable within the system. The second is that if such a system is also capable of proving certain basic facts about the natural numbers, then the system cannot prove the consistency of the system itself. These two results are known as Gödel's incompleteness theorems, or simply \"Gödel's Theorem\". Later in the decade, Gödel developed the concept of set-theoretic constructibility, as part of his proof that the axiom of choice and the continuum hypothesis are consistent with Zermelo–Fraenkel set theory.\n\nIn proof theory, Gerhard Gentzen developed natural deduction and the sequent calculus. The former attempts to model logical reasoning as it 'naturally' occurs in practice and is most easily applied to intuitionistic logic, while the latter was devised to clarify the derivation of logical proofs in any formal system. Since Gentzen's work, natural deduction and sequent calculi have been widely applied in the fields of proof theory, mathematical logic and computer science. Gentzen also proved normalization and cut-elimination theorems for intuitionistic and classical logic which could be used to reduce logical proofs to a normal form.\n\nAlfred Tarski, a pupil of Łukasiewicz, is best known for his definition of truth and logical consequence, and the semantic concept of logical satisfaction. In 1933, he published (in Polish) \"The concept of truth in formalized languages\", in which he proposed his semantic theory of truth: a sentence such as \"snow is white\" is true if and only if snow is white. Tarski's theory separated the metalanguage, which makes the statement about truth, from the object language, which contains the sentence whose truth is being asserted, and gave a correspondence (the T-schema) between phrases in the object language and elements of an interpretation. Tarski's approach to the difficult idea of explaining truth has been enduringly influential in logic and philosophy, especially in the development of model theory. Tarski also produced important work on the methodology of deductive systems, and on fundamental principles such as completeness, decidability, consistency and definability. According to Anita Feferman, Tarski \"changed the face of logic in the twentieth century\".\n\nAlonzo Church and Alan Turing proposed formal models of computability, giving independent negative solutions to Hilbert's \"Entscheidungsproblem\" in 1936 and 1937, respectively. The \"Entscheidungsproblem\" asked for a procedure that, given any formal mathematical statement, would algorithmically determine whether the statement is true. Church and Turing proved there is no such procedure; Turing's paper introduced the halting problem as a key example of a mathematical problem without an algorithmic solution.\n\nChurch's system for computation developed into the modern λ-calculus, while the Turing machine became a standard model for a general-purpose computing device. It was soon shown that many other proposed models of computation were equivalent in power to those proposed by Church and Turing. These results led to the Church–Turing thesis that any deterministic algorithm that can be carried out by a human can be carried out by a Turing machine. Church proved additional undecidability results, showing that both Peano arithmetic and first-order logic are undecidable. Later work by Emil Post and Stephen Cole Kleene in the 1940s extended the scope of computability theory and introduced the concept of degrees of unsolvability.\n\nThe results of the first few decades of the twentieth century also had an impact upon analytic philosophy and philosophical logic, particularly from the 1950s onwards, in subjects such as modal logic, temporal logic, deontic logic, and relevance logic.\n\nAfter World War II, mathematical logic branched into four inter-related but separate areas of research: model theory, proof theory, computability theory, and set theory.\n\nIn set theory, the method of forcing revolutionized the field by providing a robust method for constructing models and obtaining independence results. Paul Cohen introduced this method in 1963 to prove the independence of the continuum hypothesis and the axiom of choice from Zermelo–Fraenkel set theory. His technique, which was simplified and extended soon after its introduction, has since been applied to many other problems in all areas of mathematical logic.\n\nComputability theory had its roots in the work of Turing, Church, Kleene, and Post in the 1930s and 40s. It developed into a study of abstract computability, which became known as recursion theory. The priority method, discovered independently by Albert Muchnik and Richard Friedberg in the 1950s, led to major advances in the understanding of the degrees of unsolvability and related structures. Research into higher-order computability theory demonstrated its connections to set theory. The fields of constructive analysis and computable analysis were developed to study the effective content of classical mathematical theorems; these in turn inspired the program of reverse mathematics. A separate branch of computability theory, computational complexity theory, was also characterized in logical terms as a result of investigations into descriptive complexity.\n\nModel theory applies the methods of mathematical logic to study models of particular mathematical theories. Alfred Tarski published much pioneering work in the field, which is named after a series of papers he published under the title \"Contributions to the theory of models\". In the 1960s, Abraham Robinson used model-theoretic techniques to develop calculus and analysis based on infinitesimals, a problem that first had been proposed by Leibniz. \n\nIn proof theory, the relationship between classical mathematics and intuitionistic mathematics was clarified via tools such as the realizability method invented by Georg Kreisel and Gödel's \"Dialectica\" interpretation. This work inspired the contemporary area of proof mining. The Curry-Howard correspondence emerged as a deep analogy between logic and computation, including a correspondence between systems of natural deduction and typed lambda calculi used in computer science. As a result, research into this class of formal systems began to address both logical and computational aspects; this area of research came to be known as modern type theory. Advances were also made in ordinal analysis and the study of independence results in arithmetic such as the Paris–Harrington theorem.\n\nThis was also a period, particularly in the 1950s and afterwards, when the ideas of mathematical logic begin to influence philosophical thinking. For example, tense logic is a formalised system for representing, and reasoning about, propositions qualified in terms of time. The philosopher Arthur Prior played a significant role in its development in the 1960s. Modal logics extend the scope of formal logic to include the elements of modality (for example, possibility and necessity). The ideas of Saul Kripke, particularly about possible worlds, and the formal system now called Kripke semantics have had a profound impact on analytic philosophy. His best known and most influential work is \"Naming and Necessity\" (1980). Deontic logics are closely related to modal logics: they attempt to capture the logical features of obligation, permission and related concepts. Although some basic novelties syncretizing mathematical and philosophical logic were shown by Bolzano in the early 1800s, it was Ernst Mally, a pupil of Alexius Meinong, who was to propose the first formal deontic system in his \"Grundgesetze des Sollens\", based on the syntax of Whitehead's and Russell's propositional calculus.\n\nAnother logical system founded after World War II was fuzzy logic by Azerbaijani mathematician Lotfi Asker Zadeh in 1965.\n\n\n\n\n"}
{"id": "314743", "url": "https://en.wikipedia.org/wiki?curid=314743", "title": "Hume's principle", "text": "Hume's principle\n\nHume's principle or HP—the terms were coined by George Boolos—says that the number of \"F\"s is equal to the number of \"G\"s if and only if there is a one-to-one correspondence (a bijection) between the \"F\"s and the \"G\"s. HP can be stated formally in systems of second-order logic. Hume's principle is named for the Scottish philosopher David Hume.\n\nHP plays a central role in Gottlob Frege's philosophy of mathematics. Frege shows that HP and suitable definitions of arithmetical notions entail all axioms of what we now call second-order arithmetic. This result is known as Frege's theorem, which is the foundation for a philosophy of mathematics known as neo-logicism.\n\nHume's principle appears in Frege's \"Foundations of Arithmetic\" (§73), which quotes from Part III of Book I of David Hume's \"A Treatise of Human Nature\" (1740). Hume there sets out seven fundamental relations between ideas. Concerning one of these, proportion in quantity or number, Hume argues that our reasoning about proportion in quantity, as represented by geometry, can never achieve \"perfect precision and exactness\", since its principles are derived from sense-appearance. He contrasts this with reasoning about number or arithmetic, in which such a precision \"can\" be attained:\n\nAlgebra and arithmetic [are] the only sciences in which we can carry on a chain of reasoning to any degree of intricacy, and yet preserve a perfect exactness and certainty. We are possessed of a precise standard, by which we can judge of the equality and proportion of numbers; and according as they correspond or not to that standard, we determine their relations, without any possibility of error. \"When two numbers are so combined, as that the one has always a unit answering to every unit of the other, we pronounce them equal\"; and it is for want of such a standard of equality in [spatial] extension, that geometry can scarce be esteemed a perfect and infallible science. (I. III. I.)\n\nNote Hume's use of the word \"number\" in the ancient sense, to mean a set or collection of things rather than the common modern notion of \"positive integer\". The ancient Greek notion of number (\"arithmos\") is of a finite plurality composed of units. See Aristotle, \"Metaphysics\", 1020a14 and Euclid, \"Elements\", Book VII, Definition 1 and 2. The contrast between the old and modern conception of number is discussed in detail in Mayberry (2000).\n\nThe principle that cardinal number was to be characterized in terms of one-to-one correspondence had previously been used to great effect by Georg Cantor, whose writings Frege knew. The suggestion has therefore been made that Hume's principle ought better be called \"Cantor's Principle\". But Frege criticized Cantor on the ground that Cantor defines cardinal numbers in terms of ordinal numbers, whereas Frege wanted to give a characterization of cardinals that was independent of the ordinals. Cantor's point of view, however, is the one embedded in contemporary theories of transfinite numbers, as developed in axiomatic set theory.\n\n\n"}
{"id": "18502646", "url": "https://en.wikipedia.org/wiki?curid=18502646", "title": "Hutchinson operator", "text": "Hutchinson operator\n\nIn mathematics, in the study of fractals, a Hutchinson operator is the collective action of a set of contractions, called an iterated function system. The iteration of the operator converges to a unique attractor, which is the often self-similar fixed set of the operator.\n\nLet formula_1 be an iterated function system, or a set of contractions from a compact set formula_2 to itself. The operator formula_3 is defined over subsets formula_4 as\n\nA key question is to describe the attractors formula_6 of this operator, which are compact sets. One way of generating such a set is to start with an initial compact set formula_7 (which can be a single point, called a seed) and iterate formula_3 as follows\n\nand taking the limit, the iteration converges to the attractor\n\nHutchinson showed in 1981 the existence and uniqueness of the attractor formula_11. The proof follows by showing that the Hutchinson operator is contractive on the set of compact subsets of formula_2 in the Hausdorff distance.\n\nThe collection of functions formula_13 together with composition form a monoid. With \"N\" functions, then one may visualize the monoid as a full N-ary tree or a Cayley tree.\n"}
{"id": "30723461", "url": "https://en.wikipedia.org/wiki?curid=30723461", "title": "Isostructural", "text": "Isostructural\n\nIsostructural chemical compounds have similar chemical structures. Isomorphous when used in the relation to crystal structures is not synonymous: in addition to the same atomic connectivity that characterises isostructural compounds, isomorphous substances crystallise in the same space group and have the same unit cell dimensions. The IUCR definition used by crystallographers is:\nExamples include:\n\nMany minerals are isostructural when they differ only in the nature of a cation.\n\nCompounds which are isoelectronic usually have similar chemical structures. For example, methane, CH, and the ammonium ion, NH, are isoelectric and are isostructural \nas both have a tetrahedral structure. The C-H and N-H bond lengths are different and crystal structures are completely different because the ammonium ion only occurs in salts.\n"}
{"id": "1448702", "url": "https://en.wikipedia.org/wiki?curid=1448702", "title": "Iterated function", "text": "Iterated function\n\nIn mathematics, an iterated function is a function (that is, a function from some set to itself) which is obtained by composing another function with itself a certain number of times. The process of repeatedly applying the same function is called iteration. In this process, starting from some initial number, the result of applying a given function is fed again in the function as input, and this process is repeated.\n\nIterated functions are objects of study in computer science, fractals, dynamical systems, mathematics and renormalization group physics.\n\nThe formal definition of an iterated function on a set \"X\" follows.\n\nLet be a set and be a function.\n\nDefine as the \"n\"-th iterate of , where \"n\" is a non-negative integer, by:\n\nand\n\nwhere is the identity function on and denotes function composition. That is, \nalways associative.\n\nBecause the notation may refer to both iteration (composition) of the function or exponentiation of the function (the latter is commonly used in trigonometry), some mathematicians choose to write for the \"n\"-th iterate of the function .\n\nIn general, the following identity holds for all non-negative integers and ,\n\nThis is structurally identical to the property of exponentiation that , i.e. the special case .\n\nIn general, for arbitrary general (negative, non-integer, etc.) indices and , this relation is called the translation functional equation, cf. Schröder's equation and Abel equation. On a logarithmic scale, this reduces to the nesting property of Chebyshev polynomials, , since .\n\nThe relation also holds, analogous to the property of exponentiation that .\n\nThe sequence of functions is called a Picard sequence, named after Charles Émile Picard.\n\nFor a given in , the sequence of values is called the orbit of .\n\nIf for some integer , the orbit is called a periodic orbit. The smallest such value of for a given is called the period of the orbit. The point itself is called a periodic point. The cycle detection problem in computer science is the algorithmic problem of finding the first periodic point in an orbit, and the period of the orbit.\n\nIf \"f\"(\"x\") = \"x\" for some \"x\" in \"X\" (that is, the period of the orbit of \"x\" is 1), then \"x\" is called a fixed point of the iterated sequence. The set of fixed points is often denoted as Fix(\"f\" ). There exist a number of fixed-point theorems that guarantee the existence of fixed points in various situations, including the Banach fixed point theorem and the Brouwer fixed point theorem.\n\nThere are several techniques for convergence acceleration of the sequences produced by fixed point iteration. For example, the Aitken method applied to an iterated fixed point is known as Steffensen's method, and produces quadratic convergence.\n\nUpon iteration, one may find that there are sets that shrink and converge towards a single point. In such a case, the point that is converged to is known as an attractive fixed point. Conversely, iteration may give the appearance of points diverging away from a single point; this would be the case for an unstable fixed point. \nWhen the points of the orbit converge to one or more limits, the set of accumulation points of the orbit is known as the limit set or the ω-limit set.\n\nThe ideas of attraction and repulsion generalize similarly; one may categorize iterates into stable sets and unstable sets, according to the behaviour of small neighborhoods under iteration. (Also see Infinite compositions of analytic functions.)\n\nOther limiting behaviours are possible; for example, wandering points are points that move away, and never come back even close to where they started.\n\nIf one considers the evolution of a density distribution, rather than that of individual point dynamics, then the limiting behavior is given by the invariant measure. It can be visualized as the behavior of a point-cloud or dust-cloud under repeated iteration. The invariant measure is a eigenstate of the Ruelle-Frobenius-Perron operator or transfer operator, corresponding to an eigenvalue of 1. Smaller eigenvalues correspond to unstable, decaying states.\n\nIn general, because repeated iteration corresponds to a shift, the transfer operator, and its adjoint, the Koopman operator can both be interpreted as shift operators action on a shift space. The theory of subshifts of finite type provides general insight into many iterated functions, especially those leading to chaos.\n\nIn some instances, fractional iteration of a function can be defined: for instance, a half iterate of a function is a function such that . This function can be written using the index notation as . Similarly, is the function defined such that , while may be defined equal to , and so forth, all based on the principle, mentioned earlier, that . This idea can be generalized so that the iteration count becomes a continuous parameter, a sort of continuous \"time\" of a continuous orbit.\n\nIn such cases, one refers to the system as a flow, specified by Schröder's equation. (cf. Section on conjugacy below.)\n\nNegative iterates correspond to function inverses and their compositions. For example, is the normal inverse of , while is the inverse composed with itself, i.e. . Fractional negative iterates are defined analogously to fractional positive ones; for example, is defined such that , or, equivalently, such that .\n\nOne of several methods of finding a series formula for fractional iteration, making use of a fixed point, is as follows.\n\nThis can be carried on indefinitely, although inefficiently, as the latter terms become increasingly complicated.\nA more systematic procedure is outlined in the following section on Conjugacy.\n\nFor example, setting gives the fixed point , so the above formula terminates to just\nwhich is trivial to check.\n\nFind the value of formula_10 where this is done \"n\" times (and possibly the interpolated values when \"n\" is not an integer). We have . A fixed point is .\n\nSo set \"x\"=1 and expanded around the fixed point value of 2 is then an infinite series,\nwhich, taking just the first three terms, is correct to the first decimal place when \"n\" is positive—cf. Tetration: . (Using the other fixed point causes the series to diverge.)\n\nFor , the series computes the inverse function, .\n\nWith the function , expand around the fixed point 1 to get the series\nwhich is simply the Taylor series of \"x\" expanded around 1.\n\nIf and are two iterated functions, and there exists a homeomorphism such that , then and are said to be topologically conjugate.\n\nClearly, topological conjugacy is preserved under iteration, as . Thus, if one can solve for one iterated function system, one also has solutions for all topologically conjugate systems. For example, the tent map is topologically conjugate to the logistic map. As a special case, taking , one has the iteration of as \n\nMaking the substitution yields \n\nEven in the absence of a strict homeomorphism, near a fixed point, here taken to be at = 0, (0) = 0, one may often solve Schröder's equation for a function , which makes locally conjugate to a mere dilation, , that is \n\nThus, its iteration orbit, or flow, under suitable provisions (e.g., ), amounts to the conjugate of the orbit of the monomial, \nwhere in this expression serves as a plain exponent: \"functional iteration has been reduced to multiplication!\" Here, however, the exponent no longer needs be integer or positive, and is a continuous \"time\" of evolution for the full orbit: the monoid of the Picard sequence (cf. transformation semigroup) has generalized to a full continuous group.\n\nThis method (perturbative determination of the principal eigenfunction , cf. Carleman matrix) is equivalent to the algorithm of the preceding section, albeit, in practice, more powerful and systematic.\n\nIf the function is linear and can be described by a stochastic matrix, that is, a matrix whose rows or columns sum to one, then the iterated system is known as a Markov chain.\n\nThere are many chaotic maps. \nWell-known iterated functions include the Mandelbrot set and iterated function systems.\n\nErnst Schröder, in 1870, worked out special cases of the logistic map, such as the chaotic case , so that , hence .\n\nA nonchaotic case Schröder also illustrated with his method, , yielded , and hence .\n\nIf is the action of a group element on a set, then the iterated function corresponds to a free group.\n\nMost functions do not have explicit general closed-form expressions for the \"n\"-th iterate. The table below lists some that do. Note that all these expressions are valid even for non-integer and negative \"n\", as well as positive integer \"n\".\nNote: these two special cases of are the only cases that have a closed-form solution. Choosing \"b\" = 2 = –\"a\" and \"b\" = 4 = –\"a\", respectively, further reduces them to the nonchaotic and chaotic logistic cases discussed prior to the table.\n\nSome of these examples are related among themselves by simple conjugacies. A few further examples, essentially amounting to simple conjugacies of Schröder's examples can be found in ref.\n\nIterated functions can be studied with the Artin–Mazur zeta function and with transfer operators.\n\nIn computer science, iterated functions occur as a special case of recursive functions, which in turn anchor the study of such broad topics as lambda calculus, or narrower ones, such as the denotational semantics of computer programs.\n\nTwo important functionals can be defined in terms of iterated functions. These are summation:\n\nand the equivalent product:\n\nThe functional derivative of an iterated function is given by the recursive formula:\n\nIterated functions crop up in the series expansion of combined functions, such as .\n\nGiven the iteration velocity, or beta function (physics), \nfor the iterate of the function , we have\nFor example, for rigid advection, if , then . Consequently, , action by a plain shift operator.\n\nConversely, one may specify given an arbitrary , through the generic Abel equation discussed above,\nwhere\nThis is evident by noting that \n\nFor continuous iteration index , then, now written as a subscript, this amounts to Lie's celebrated exponential realization of a continuous group,\nThe initial flow velocity suffices to determine the entire flow, given this exponential realization which automatically provides the general solution to the \"translation functional equation\", \n\n"}
{"id": "42777732", "url": "https://en.wikipedia.org/wiki?curid=42777732", "title": "J-multiplicity", "text": "J-multiplicity\n\nIn algebra, a j-multiplicity is a generalization of a Hilbert–Samuel multiplicity. For \"m\"-primary ideals, the two notions coincide.\n\nLet formula_1 be a local Noetherian ring of Krull dimension formula_2. Then the j-multiplicity of an ideal \"I\" is\nwhere formula_4 is the normalized coefficient of the degree \"d\" − 1 term in the Hilbert polynomial formula_5; formula_6 means the space of sections supported at formula_7.\n\n"}
{"id": "53660140", "url": "https://en.wikipedia.org/wiki?curid=53660140", "title": "Jean-Éric Pin", "text": "Jean-Éric Pin\n\nJean-Éric Pin is a French mathematician and theoretical computer scientist known for his contributions to the algebraic automata theory and semigroup theory. He is a CNRS research director.\n\nPin earned his undergraduate degree from ENS Cachan in 1976, and his doctorate (Doctorat d'état) from the Pierre and Marie Curie University in 1981. Since 1988 he is a CNRS research director at Paris Diderot University. In 1992–2006 he was a professor at École Polytechnique.\n\nPin is a member of Academia Europaea (2011) and an EATCS fellow (2014).\n\n"}
{"id": "1024131", "url": "https://en.wikipedia.org/wiki?curid=1024131", "title": "Jean Bourgain", "text": "Jean Bourgain\n\nJean, Baron Bourgain (; born 28 February 1954) is a Belgian mathematician. He has been a faculty member at the University of Illinois, Urbana-Champaign and, from 1985 until 1995, professor at Institut des Hautes Études Scientifiques at Bures-sur-Yvette in France, and since 1994 at the Institute for Advanced Study in Princeton, New Jersey. He is currently an editor for the Annals of Mathematics. From 2012–2014, he was appointed a visiting scholar at UC Berkeley.\n\nBourgain received his Ph.D. from the Vrije Universiteit Brussel in 1977.\n\nHis work is in various areas of mathematical analysis such as the geometry of Banach spaces, harmonic analysis, analytic number theory, combinatorics, ergodic theory, partial differential equations, spectral theory and recently also in group theory. He has been recognised by a number of awards, most notably the Fields Medal in 1994.\n\nIn 2000 Bourgain connected the Kakeya problem to arithmetic combinatorics.\n\nIn 2009 Bourgain was elected a foreign member of the Royal Swedish Academy of Sciences.\n\nIn 2010, he received the Shaw Prize in Mathematics.\n\nIn 2012, he and Terence Tao received the Crafoord Prize in Mathematics from the Royal Swedish Academy of Sciences.\n\nIn 2016, he received the 2017 Breakthrough Prize in Mathematics.\n\n"}
{"id": "1498176", "url": "https://en.wikipedia.org/wiki?curid=1498176", "title": "Johan van Benthem (logician)", "text": "Johan van Benthem (logician)\n\nJohannes Franciscus Abraham Karel (Johan) van Benthem (born 12 June 1949 in Rijswijk) is a University Professor \n(\"universiteitshoogleraar\") of logic at the University of Amsterdam at the Institute for Logic, Language and Computation and professor of philosophy at Stanford University (at CSLI). He was awarded the Spinozapremie in 1996 and elected a Foreign Fellow of the American Academy of Arts & Sciences in 2015.\n\nHe studied physics (B.Sc. 1969), philosophy (M.A. 1972) and mathematics (M.Sc. 1973) at the University of Amsterdam and received a PhD from the same university under supervision of Martin Löb in 1977. Before becoming University Professor in 2003, he held appointments at the University of Amsterdam (1973–1977), at the University of Groningen (1977–1986), and as a professor at the University of Amsterdam (1986–2003).\n\nIn 1992 he was elected member of the Royal Netherlands Academy of Arts and Sciences.\n\nVan Benthem is known for his research in the area of modal logic. This research has resulted in Van Benthem's Theorem, which states that propositional modal logic is the fragment of first-order logic that is closed under bisimulation.\n\nHe has also been active in the fields of philosophy of science, logical structures in natural language (generalized quantifiers, categorial grammar, substructural proof theory), dynamic logic and update logic and applications of logic to game theory as well as applications of game theory to logic (game semantics). Van Benthem is a member of the group collectively publishing under the pseudonym L. T. F. Gamut. He has also taught in China. He made an effort to encourage and organize international collaboration between Chinese and Western logicians.\n\nProfessor van Benthem retired from the Institute for Logic, Language and Computation in September 2014.\n\n\n"}
{"id": "2180750", "url": "https://en.wikipedia.org/wiki?curid=2180750", "title": "Jump process", "text": "Jump process\n\nA jump process is a type of stochastic process that has discrete movements, called jumps, with random arrival times, rather than continuous movement, typically modelled as a simple or compound Poisson process.\n\nIn finance, various stochastic models are used to model the price movements of financial instruments; for example the Black–Scholes model for pricing options assumes that the underlying instrument follows a traditional diffusion process, with continuous, random movements at all scales, no matter how small. John Carrington Cox and Stephen Ross proposed that prices actually follow a 'jump process'.\n\nRobert C. Merton extended this approach to a hybrid model known as jump diffusion, which states that the prices have large jumps interspersed with small continuous movements.\n\n"}
{"id": "37647580", "url": "https://en.wikipedia.org/wiki?curid=37647580", "title": "Lie–Palais theorem", "text": "Lie–Palais theorem\n\nIn differential geometry, the Lie–Palais theorem states that an action of a finite-dimensional Lie algebra on a smooth compact manifold can be lifted to an action of a finite-dimensional Lie group. For manifolds with boundary the action must preserve the boundary, in other words the vector fields on the boundary must be tangent to the boundary. proved it as a global form of an earlier local theorem due to Sophus Lie.\n\nThe example of the vector field \"d\"/\"dx\" on the open unit interval shows that the result is false for non-compact manifolds.\n\nWithout the assumption that the Lie algebra is finite dimensional the result can be false. gives the following example due to Omori: the Lie algebra is all vector fields \"f\"(\"x\",\"y\")∂/∂\"x\" + \"g\"(\"x\",\"y\")∂/∂y acting on the torus R/Z such that \"g\"(\"x\", \"y\") = 0 for 0 ≤ \"x\" ≤ 1/2. This Lie algebra is not the Lie algebra of any group. gives an infinite dimensional generalization of the Lie–Palais theorem for Banach–Lie algebras with finite-dimensional center.\n\n"}
{"id": "25956787", "url": "https://en.wikipedia.org/wiki?curid=25956787", "title": "Lifting theory", "text": "Lifting theory\n\nIn mathematics, lifting theory was first introduced by John von Neumann in his (1931) pioneering paper (answering a question raised by Alfréd Haar), followed later by Dorothy Maharam’s (1958) paper, and by A. Ionescu Tulcea and ’s (1961) paper. Lifting theory was motivated to a large extent by its striking applications; for its development up to 1969, see the Ionescu Tulceas' work and the monograph, now a standard reference in the field. Lifting theory continued to develop after 1969, yielding significant new results and applications.\n\nA lifting on a measure space (\"X\", Σ, \"μ\") is a linear and multiplicative inverse\n\nof the quotient map\n\nwhere formula_3 is the seminormed L space of measurable functions and formula_4 is its usual normed quotient. In other words, a lifting picks from every equivalence class [\"f\"] of bounded measurable functions modulo negligible functions a representative— which is henceforth written \"T\"([\"f\"]) or \"T\"[\"f\"] or simply \"Tf\" — in such a way that\n\nLiftings are used to produce disintegrations of measures, for instance conditional probability distributions given continuous random variables, and fibrations of Lebesgue measure on the level sets of a function. \nTheorem. Suppose (\"X\", Σ, \"μ\") is complete. Then (\"X\", Σ, \"μ\") admits a lifting if and only if there exists a collection of mutually disjoint integrable sets in Σ whose union is \"X\".\n\nIn particular, if (\"X\", Σ, \"μ\") is the completion of a \"σ\"-finite measure or of an inner regular Borel measure on a locally compact space, then (\"X\", Σ, \"μ\") admits a lifting.\n\nThe proof consists in extending a lifting to ever larger sub-\"σ\"-algebras, applying Doob's martingale convergence theorem if one encounters a countable chain in the process.\n\nSuppose (\"X\", Σ, \"μ\") is complete and \"X\" is equipped with a completely regular Hausdorff topology τ ⊂ Σ such that the union of any collection of negligible open sets is again negligible – this is the case if (\"X\", Σ, \"μ\") is \"σ\"-finite or comes from a Radon measure. Then the \"support\" of \"μ\", Supp(\"μ\"), can be defined as the complement of the largest negligible open subset, and the collection \"C\"(\"X\", \"τ\") of bounded continuous functions belongs to formula_3.\n\nA strong lifting for (\"X\", Σ, \"μ\") is a lifting \nsuch that \"Tφ\" = \"φ\" on Supp(\"μ\") for all φ in \"C\"(\"X\", τ). This is the same as requiring that \"TU\" ≥ (\"U\" ∩ Supp(\"μ\")) for all open sets \"U\" in \"τ\".\n\nTheorem. If (Σ, \"μ\") is \"σ\"-finite and complete and \"τ\" has a countable basis then (\"X\", Σ, \"μ\") admits a strong lifting.\n\nProof. Let \"T\" be a lifting for (\"X\", Σ, \"μ\") and {\"U\", \"U\", ...} a countable basis for \"τ\". For any point \"p\" in the negligible set\n\nlet \"T\" be any character on \"L\"(\"X\", Σ, \"μ\") that extends the character φ ↦ φ(\"p\") of \"C\"(\"X\", τ). Then for \"p\" in \"X\" and [\"f\"] in \"L\"(\"X\", Σ, \"μ\") define:\n\n\"T\" is the desired strong lifting.\n\nSuppose (\"X\", Σ, \"μ\"), (\"Y\", Φ, ν) are \"σ\"-finite measure spaces (\"μ\", \"ν\" positive) and \"π\" : \"X\" → \"Y\" is a measurable map. A disintegration of \"μ\" along \"π\" with respect to \"ν\" is a slew formula_12 of positive \"σ\"-additive measures on (\"X\", Σ) such that\n\n\nDisintegrations exist in various circumstances, the proofs varying but almost all using strong liftings. Here is a rather general result. Its short proof gives the general flavor.\n\nTheorem. Suppose \"X\" is a Polish space and \"Y\" a separable Hausdorff space, both equipped with their Borel \"σ\"-algebras. Let \"μ\" be a \"σ\"-finite Borel measure on \"X\" and π : \"X\" → \"Y\" a Σ, Φ–measurable map. Then there exists a σ-finite Borel measure ν on \"Y\" and a disintegration (*).\n\nIf \"μ\" is finite, \"ν\" can be taken to be the pushforward \"π\"\"μ\", and then the \"λ\" are probabilities.\n\nProof. Because of the polish nature of \"X\" there is a sequence of compact subsets of \"X\" that are mutually disjoint, whose union has negligible complement, and on which π is continuous. This observation reduces the problem to the case that both \"X\" and \"Y\" are compact and π is continuous, and \"ν\" = \"π\"\"μ\". Complete Φ under \"ν\" and fix a strong lifting \"T\" for (\"Y\", Φ, \"ν\"). Given a bounded \"μ\"-measurable function \"f\", let denote its conditional expectation under π, i.e., the Radon-Nikodym derivative of \"π\"(\"fμ\") with respect to \"π\"\"μ\". Then set, for every \"y\" in \"Y\", formula_17 To show that this defines a disintegration is a matter of bookkeeping and a suitable Fubini theorem. To see how the strongness of the lifting enters, note that\n\nand take the infimum over all positive \"φ\" in \"C\"(\"Y\") with \"φ\"(\"y\") = 1; it becomes apparent that the support of \"λ\" lies in the fiber over \"y\".\n"}
{"id": "39500076", "url": "https://en.wikipedia.org/wiki?curid=39500076", "title": "List of things named after Jacques Hadamard", "text": "List of things named after Jacques Hadamard\n\nThese are things named after Jacques Hadamard (1865–1963), a French mathematician. (For references, see the respective articles.)\n"}
{"id": "24738627", "url": "https://en.wikipedia.org/wiki?curid=24738627", "title": "Lyapunov–Malkin theorem", "text": "Lyapunov–Malkin theorem\n\nThe Lyapunov–Malkin theorem (named for Aleksandr Lyapunov and ) is a mathematical theorem detailing nonlinear stability of systems.\n\nIn the system of differential equations,\n\nwhere, formula_2, formula_3, formula_4 in an \"m\" × \"m\" matrix, and \"X\"(\"x\", \"y\"), \"Y\"(\"x\", \"y\") represent higher order nonlinear terms. If all eigenvalues of the matrix formula_4 have negative real parts, and \"X\"(\"x\", \"y\"), \"Y\"(\"x\", \"y\") vanish when \"x\" = 0, then the solution \"x\" = 0, \"y\" = 0 of this system is stable with respect to (\"x\", \"y\") and asymptotically stable with respect to  \"x\". If a solution (\"x\"(\"t\"), \"y\"(\"t\")) is close enough to the solution \"x\" = 0, \"y\" = 0, then\n"}
{"id": "18831", "url": "https://en.wikipedia.org/wiki?curid=18831", "title": "Mathematics", "text": "Mathematics\n\nMathematics (from Greek μάθημα \"máthēma\", \"knowledge, study, learning\") includes the study of such topics as quantity, structure, space, and change.\n\nMathematicians seek and use patterns to formulate new conjectures; they resolve the truth or falsity of conjectures by mathematical proof. When mathematical structures are good models of real phenomena, then mathematical reasoning can provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity from as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry.\n\nRigorous arguments first appeared in Greek mathematics, most notably in Euclid's \"Elements\". Since the pioneering work of Giuseppe Peano (1858–1932), David Hilbert (1862–1943), and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day.\n\nMathematics is essential in many fields, including natural science, engineering, medicine, finance and the social sciences. Applied mathematics has led to entirely new mathematical disciplines, such as statistics and game theory. Mathematicians engage in pure mathematics, or mathematics for its own sake, without having any application in mind. Practical applications for what began as pure mathematics are often discovered.\n\nThe history of mathematics can be seen as an ever-increasing series of abstractions. The first abstraction, which is shared by many animals, was probably that of numbers: the realization that a collection of two apples and a collection of two oranges (for example) have something in common, namely quantity of their members.\n\nAs evidenced by tallies found on bone, in addition to recognizing how to count physical objects, prehistoric peoples may have also recognized how to count abstract quantities, like time – days, seasons, years.\n\nEvidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra and geometry for taxation and other financial calculations, for building and construction, and for astronomy. The most ancient mathematical texts from Mesopotamia and Egypt are from 2000–1800 BC. Many early texts mention Pythagorean triples and so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry. It is in Babylonian mathematics that elementary arithmetic (addition, subtraction, multiplication and division) first appear in the archaeological record. The Babylonians also possessed a place-value system, and used a sexagesimal numeral system, still in use today for measuring angles and time.\n\nBeginning in the 6th century BC with the Pythagoreans, the Ancient Greeks began a systematic study of mathematics as a subject in its own right with Greek mathematics. Around 300 BC, Euclid introduced the axiomatic method still used in mathematics today, consisting of definition, axiom, theorem, and proof. His textbook \"Elements\" is widely considered the most successful and influential textbook of all time. The greatest mathematician of antiquity is often held to be Archimedes (c. 287–212 BC) of Syracuse. He developed formulas for calculating the surface area and volume of solids of revolution and used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus. Other notable achievements of Greek mathematics are conic sections (Apollonius of Perga, 3rd century BC), trigonometry (Hipparchus of Nicaea (2nd century BC), and the beginnings of algebra (Diophantus, 3rd century AD).\n\nThe Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics. Other notable developments of Indian mathematics include the modern definition of sine and cosine, and an early form of infinite series.\nDuring the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics. The most notable achievement of Islamic mathematics was the development of algebra. Other notable achievements of the Islamic period are advances in spherical trigonometry and the addition of the decimal point to the Arabic numeral system. Many notable mathematicians from this period were Persian, such as Al-Khwarismi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī. \n\nDuring the early modern period, mathematics began to develop at an accelerating pace in Western Europe. The development of calculus by Newton and Leibniz in the 17th century revolutionized mathematics. Leonhard Euler was the most notable mathematician of the 18th century, contributing numerous theorems and discoveries. Perhaps the foremost mathematician of the 19th century was the German mathematician Carl Friedrich Gauss, who made numerous contributions to fields such as algebra, analysis, differential geometry, matrix theory,number theory, and statistics. In the early 20th century, Kurt Gödel transformed mathematics by publishing his incompleteness theorems, which show that any axiomatic system that is consistent will contain unprovable propositions.\n\nMathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made today. According to Mikhail B. Sevryuk, in the January 2006 issue of the \"Bulletin of the American Mathematical Society\", \"The number of papers and books included in the \"Mathematical Reviews\" database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs.\"\n\nThe word \"mathematics\" comes from Ancient Greek μάθημα (\"máthēma\"), meaning \"that which is learnt\", \"what one gets to know\", hence also \"study\" and \"science\". The word for \"mathematics\" came to have the narrower and more technical meaning \"mathematical study\" even in Classical times. Its adjective is (\"mathēmatikós\"), meaning \"related to learning\" or \"studious\", which likewise further came to mean \"mathematical\". In particular, (\"mathēmatikḗ tékhnē\"), , meant \"the mathematical art\".\n\nSimilarly, one of the two main schools of thought in Pythagoreanism was known as the \"mathēmatikoi\" (μαθηματικοί)—which at the time meant \"teachers\" rather than \"mathematicians\" in the modern sense.\n\nIn Latin, and in English until around 1700, the term \"mathematics\" more commonly meant \"astrology\" (or sometimes \"astronomy\") rather than \"mathematics\"; the meaning gradually changed to its present one from about 1500 to 1800. This has resulted in several mistranslations. For example, Saint Augustine's warning that Christians should beware of \"mathematici\", meaning astrologers, is sometimes mistranslated as a condemnation of mathematicians.\n\nThe apparent plural form in English, like the French plural form (and the less commonly used singular derivative ), goes back to the Latin neuter plural (Cicero), based on the Greek plural (\"ta mathēmatiká\"), used by Aristotle (384–322 BC), and meaning roughly \"all things mathematical\"; although it is plausible that English borrowed only the adjective \"mathematic(al)\" and formed the noun \"mathematics\" anew, after the pattern of \"physics\" and \"metaphysics\", which were inherited from Greek. In English, the noun \"mathematics\" takes a singular verb. It is often shortened to \"maths\" or, in North America, \"math\".\n\nMathematics has no generally accepted definition. Aristotle defined mathematics as \"the science of quantity\", and this definition prevailed until the 18th century. Galileo Galilei (1564–1642) said, \"The universe cannot be read until we have learned the language and become familiar with the characters in which it is written. It is written in mathematical language, and the letters are triangles, circles and other geometrical figures, without which means it is humanly impossible to comprehend a single word. Without these, one is wandering about in a dark labyrinth.\" Carl Friedrich Gauss (1777–1855) referred to mathematics as \"the Queen of the Sciences\". Benjamin Peirce (1809–1880) called mathematics \"the science that draws necessary conclusions\". David Hilbert said of mathematics: \"We are not speaking here of arbitrariness in any sense. Mathematics is not like a game whose tasks are determined by arbitrarily stipulated rules. Rather, it is a conceptual system possessing internal necessity that can only be so and by no means otherwise.\" Albert Einstein (1879–1955) stated that \"as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.\"\n\nStarting in the 19th century, when the study of mathematics increased in rigor and began to address abstract topics such as group theory and projective geometry, which have no clear-cut relation to quantity and measurement, mathematicians and philosophers began to propose a variety of new definitions. Some of these definitions emphasize the deductive character of much of mathematics, some emphasize its abstractness, some emphasize certain topics within mathematics. Today, no consensus on the definition of mathematics prevails, even among professionals. There is not even consensus on whether mathematics is an art or a science. A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable. Some just say, \"Mathematics is what mathematicians do.\"\n\nThree leading types of definition of mathematics are called logicist, intuitionist, and formalist, each reflecting a different philosophical school of thought. All have severe problems, none has widespread acceptance, and no reconciliation seems possible.\n\nAn early definition of mathematics in terms of logic was Benjamin Peirce's \"the science that draws necessary conclusions\" (1870). In the \"Principia Mathematica\", Bertrand Russell and Alfred North Whitehead advanced the philosophical program known as logicism, and attempted to prove that all mathematical concepts, statements, and principles can be defined and proved entirely in terms of symbolic logic. A logicist definition of mathematics is Russell's \"All Mathematics is Symbolic Logic\" (1903).\n\nIntuitionist definitions, developing from the philosophy of mathematician L. E. J. Brouwer, identify mathematics with certain mental phenomena. An example of an intuitionist definition is \"Mathematics is the mental activity which consists in carrying out constructs one after the other.\" A peculiarity of intuitionism is that it rejects some mathematical ideas considered valid according to other definitions. In particular, while other philosophies of mathematics allow objects that can be proved to exist even though they cannot be constructed, intuitionism allows only mathematical objects that one can actually construct.\n\nFormalist definitions identify mathematics with its symbols and the rules for operating on them. Haskell Curry defined mathematics simply as \"the science of formal systems\". A formal system is a set of symbols, or \"tokens\", and some \"rules\" telling how the tokens may be combined into \"formulas\". In formal systems, the word \"axiom\" has a special meaning, different from the ordinary meaning of \"a self-evident truth\". In formal systems, an axiom is a combination of tokens that is included in a given formal system without needing to be derived using the rules of the system.\n\nThe German mathematician Carl Friedrich Gauss referred to mathematics as \"the Queen of the Sciences\". More recently, Marcus du Sautoy has called mathematics \"the Queen of Science ... the main driving force behind scientific discovery\". In the original Latin \"Regina Scientiarum\", as well as in German \"Königin der Wissenschaften\", the word corresponding to \"science\" means a \"field of knowledge\", and this was the original meaning of \"science\" in English, also; mathematics is in this sense a field of knowledge. The specialization restricting the meaning of \"science\" to \"natural science\" follows the rise of Baconian science, which contrasted \"natural science\" to scholasticism, the Aristotelean method of inquiring from first principles. The role of empirical experimentation and observation is negligible in mathematics, compared to natural sciences such as biology, chemistry, or physics. Albert Einstein stated that \"as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.\"\n\nMany philosophers believe that mathematics is not experimentally falsifiable, and thus not a science according to the definition of Karl Popper. However, in the 1930s Gödel's incompleteness theorems convinced many mathematicians that mathematics cannot be reduced to logic alone, and Karl Popper concluded that \"most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently.\" Other thinkers, notably Imre Lakatos, have applied a version of falsificationism to mathematics itself.\n\nAn alternative view is that certain scientific fields (such as theoretical physics) are mathematics with axioms that are intended to correspond to reality. Mathematics shares much in common with many fields in the physical sciences, notably the exploration of the logical consequences of assumptions. Intuition and experimentation also play a role in the formulation of conjectures in both mathematics and the (other) sciences. Experimental mathematics continues to grow in importance within mathematics, and computation and simulation are playing an increasing role in both the sciences and mathematics.\n\nThe opinions of mathematicians on this matter are varied. Many mathematicians feel that to call their area a science is to downplay the importance of its aesthetic side, and its history in the traditional seven liberal arts; others feel that to ignore its connection to the sciences is to turn a blind eye to the fact that the interface between mathematics and its applications in science and engineering has driven much development in mathematics. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematics is \"created\" (as in art) or \"discovered\" (as in science). It is common to see universities divided into sections that include a division of \"Science and Mathematics\", indicating that the fields are seen as being allied but that they do not coincide. In practice, mathematicians are typically grouped with scientists at the gross level but separated at finer levels. This is one of many issues considered in the philosophy of mathematics.\n\nMathematics arises from many different kinds of problems. At first these were found in commerce, land measurement, architecture and later astronomy; today, all sciences suggest problems studied by mathematicians, and many problems arise within mathematics itself. For example, the physicist Richard Feynman invented the path integral formulation of quantum mechanics using a combination of mathematical reasoning and physical insight, and today's string theory, a still-developing scientific theory which attempts to unify the four fundamental forces of nature, continues to inspire new mathematics.\n\nSome mathematics is relevant only in the area that inspired it, and is applied to solve further problems in that area. But often mathematics inspired by one area proves useful in many areas, and joins the general stock of mathematical concepts. A distinction is often made between pure mathematics and applied mathematics. However pure mathematics topics often turn out to have applications, e.g. number theory in cryptography. This remarkable fact, that even the \"purest\" mathematics often turns out to have practical applications, is what Eugene Wigner has called \"the unreasonable effectiveness of mathematics\". As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: there are now hundreds of specialized areas in mathematics and the latest Mathematics Subject Classification runs to 46 pages. Several areas of applied mathematics have merged with related traditions outside of mathematics and become disciplines in their own right, including statistics, operations research, and computer science.\n\nFor those who are mathematically inclined, there is often a definite aesthetic aspect to much of mathematics. Many mathematicians talk about the \"elegance\" of mathematics, its intrinsic aesthetics and inner beauty. Simplicity and generality are valued. There is beauty in a simple and elegant proof, such as Euclid's proof that there are infinitely many prime numbers, and in an elegant numerical method that speeds calculation, such as the fast Fourier transform. G. H. Hardy in \"A Mathematician's Apology\" expressed the belief that these aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He identified criteria such as significance, unexpectedness, inevitability, and economy as factors that contribute to a mathematical aesthetic. Mathematicians often strive to find proofs that are particularly elegant, proofs from \"The Book\" of God according to Paul Erdős. The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions.\n\nMost of the mathematical notation in use today was not invented until the 16th century. Before that, mathematics was written out in words, limiting mathematical discovery. Euler (1707–1783) was responsible for many of the notations in use today. Modern notation makes mathematics much easier for the professional, but beginners often find it daunting. According to Barbara Oakley, this can be attributed to the fact that mathematical ideas are both more \"abstract\" and more \"encrypted\" than those of natural language. Unlike natural language, where people can often equate a word (such as \"cow\") with the physical object it corresponds to, mathematical symbols are abstract, lacking any physical analog. Mathematical symbols are also more highly encrypted than regular words, meaning a single symbol can encode a number of different operations or ideas.\n\nMathematical language can be difficult to understand for beginners because even common terms, such as \"or\" and \"only\", have a more precise meaning than they have in everyday speech, and other terms such as \"open\" and \"field\" refer to specific mathematical ideas, not covered by their laymen's meanings. Mathematical language also includes many technical terms such as \"homeomorphism\" and \"integrable\" that have no meaning outside of mathematics. Additionally, shorthand phrases such as \"iff\" for \"if and only if\" belong to mathematical jargon. There is a reason for special notation and technical vocabulary: mathematics requires more precision than everyday speech. Mathematicians refer to this precision of language and logic as \"rigor\".\n\nMathematical proof is fundamentally a matter of rigor. Mathematicians want their theorems to follow from axioms by means of systematic reasoning. This is to avoid mistaken \"theorems\", based on fallible intuitions, of which many instances have occurred in the history of the subject. The level of rigor expected in mathematics has varied over time: the Greeks expected detailed arguments, but at the time of Isaac Newton the methods employed were less rigorous. Problems inherent in the definitions used by Newton would lead to a resurgence of careful analysis and formal proof in the 19th century. Misunderstanding the rigor is a cause for some of the common misconceptions of mathematics. Today, mathematicians continue to argue among themselves about computer-assisted proofs. Since large computations are hard to verify, such proofs may not be sufficiently rigorous.\n\nAxioms in traditional thought were \"self-evident truths\", but that conception is problematic. At a formal level, an axiom is just a string of symbols, which has an intrinsic meaning only in the context of all derivable formulas of an axiomatic system. It was the goal of Hilbert's program to put all of mathematics on a firm axiomatic basis, but according to Gödel's incompleteness theorem every (sufficiently powerful) axiomatic system has undecidable formulas; and so a final axiomatization of mathematics is impossible. Nonetheless mathematics is often imagined to be (as far as its formal content) nothing but set theory in some axiomatization, in the sense that every mathematical statement or proof could be cast into formulas within set theory.\n\nMathematics can, broadly speaking, be subdivided into the study of quantity, structure, space, and change (i.e. arithmetic, algebra, geometry, and analysis). In addition to these main concerns, there are also subdivisions dedicated to exploring links from the heart of mathematics to other fields: to logic, to set theory (foundations), to the empirical mathematics of the various sciences (applied mathematics), and more recently to the rigorous study of uncertainty. While some areas might seem unrelated, the Langlands program has found connections between areas previously thought unconnected, such as Galois groups, Riemann surfaces and number theory.\n\nIn order to clarify the foundations of mathematics, the fields of mathematical logic and set theory were developed. Mathematical logic includes the mathematical study of logic and the applications of formal logic to other areas of mathematics; set theory is the branch of mathematics that studies sets or collections of objects. Category theory, which deals in an abstract way with mathematical structures and relationships between them, is still in development. The phrase \"crisis of foundations\" describes the search for a rigorous foundation for mathematics that took place from approximately 1900 to 1930. Some disagreement about the foundations of mathematics continues to the present day. The crisis of foundations was stimulated by a number of controversies at the time, including the controversy over Cantor's set theory and the Brouwer–Hilbert controversy.\n\nMathematical logic is concerned with setting mathematics within a rigorous axiomatic framework, and studying the implications of such a framework. As such, it is home to Gödel's incompleteness theorems which (informally) imply that any effective formal system that contains basic arithmetic, if \"sound\" (meaning that all theorems that can be proved are true), is necessarily \"incomplete\" (meaning that there are true theorems which cannot be proved \"in that system\"). Whatever finite collection of number-theoretical axioms is taken as a foundation, Gödel showed how to construct a formal statement that is a true number-theoretical fact, but which does not follow from those axioms. Therefore, no formal system is a complete axiomatization of full number theory. Modern logic is divided into recursion theory, model theory, and proof theory, and is closely linked to theoretical computer science, as well as to category theory. In the context of recursion theory, the impossibility of a full axiomatization of number theory can also be formally demonstrated as a consequence of the MRDP theorem.\n\nTheoretical computer science includes computability theory, computational complexity theory, and information theory. Computability theory examines the limitations of various theoretical models of the computer, including the most well-known model – the Turing machine. Complexity theory is the study of tractability by computer; some problems, although theoretically solvable by computer, are so expensive in terms of time or space that solving them is likely to remain practically unfeasible, even with the rapid advancement of computer hardware. A famous problem is the \"\" problem, one of the Millennium Prize Problems. Finally, information theory is concerned with the amount of data that can be stored on a given medium, and hence deals with concepts such as compression and entropy.\n\nThe study of quantity starts with numbers, first the familiar natural numbers and integers (\"whole numbers\") and arithmetical operations on them, which are characterized in arithmetic. The deeper properties of integers are studied in number theory, from which come such popular results as Fermat's Last Theorem. The twin prime conjecture and Goldbach's conjecture are two unsolved problems in number theory.\n\nAs the number system is further developed, the integers are recognized as a subset of the rational numbers (\"fractions\"). These, in turn, are contained within the real numbers, which are used to represent continuous quantities. Real numbers are generalized to complex numbers. These are the first steps of a hierarchy of numbers that goes on to include quaternions and octonions. Consideration of the natural numbers also leads to the transfinite numbers, which formalize the concept of \"infinity\". According to the fundamental theorem of algebra all solutions of equations in one unknown with complex coefficients are complex numbers, regardless of degree. Another area of study is the size of sets, which is described with the cardinal numbers. These include the aleph numbers, which allow meaningful comparison of the size of infinitely large sets.\n\nMany mathematical objects, such as sets of numbers and functions, exhibit internal structure as a consequence of operations or relations that are defined on the set. Mathematics then studies properties of those sets that can be expressed in terms of that structure; for instance number theory studies properties of the set of integers that can be expressed in terms of arithmetic operations. Moreover, it frequently happens that different such structured sets (or structures) exhibit similar properties, which makes it possible, by a further step of abstraction, to state axioms for a class of structures, and then study at once the whole class of structures satisfying these axioms. Thus one can study groups, rings, fields and other abstract systems; together such studies (for structures defined by algebraic operations) constitute the domain of abstract algebra.\n\nBy its great generality, abstract algebra can often be applied to seemingly unrelated problems; for instance a number of ancient problems concerning compass and straightedge constructions were finally solved using Galois theory, which involves field theory and group theory. Another example of an algebraic theory is linear algebra, which is the general study of vector spaces, whose elements called vectors have both quantity and direction, and can be used to model (relations between) points in space. This is one example of the phenomenon that the originally unrelated areas of geometry and algebra have very strong interactions in modern mathematics. Combinatorics studies ways of enumerating the number of objects that fit a given structure.\n\nThe study of space originates with geometry – in particular, Euclidean geometry, which combines space and numbers, and encompasses the well-known Pythagorean theorem. Trigonometry is the branch of mathematics that deals with relationships between the sides and the angles of triangles and with the trigonometric functions. The modern study of space generalizes these ideas to include higher-dimensional geometry, non-Euclidean geometries (which play a central role in general relativity) and topology. Quantity and space both play a role in analytic geometry, differential geometry, and algebraic geometry. Convex and discrete geometry were developed to solve problems in number theory and functional analysis but now are pursued with an eye on applications in optimization and computer science. Within differential geometry are the concepts of fiber bundles and calculus on manifolds, in particular, vector and tensor calculus. Within algebraic geometry is the description of geometric objects as solution sets of polynomial equations, combining the concepts of quantity and space, and also the study of topological groups, which combine structure and space. Lie groups are used to study space, structure, and change. Topology in all its many ramifications may have been the greatest growth area in 20th-century mathematics; it includes point-set topology, set-theoretic topology, algebraic topology and differential topology. In particular, instances of modern-day topology are metrizability theory, axiomatic set theory, homotopy theory, and Morse theory. Topology also includes the now solved Poincaré conjecture, and the still unsolved areas of the Hodge conjecture. Other results in geometry and topology, including the four color theorem and Kepler conjecture, have been proved only with the help of computers.\n\nUnderstanding and describing change is a common theme in the natural sciences, and calculus was developed as a powerful tool to investigate it. Functions arise here, as a central concept describing a changing quantity. The rigorous study of real numbers and functions of a real variable is known as real analysis, with complex analysis the equivalent field for the complex numbers. Functional analysis focuses attention on (typically infinite-dimensional) spaces of functions. One of many applications of functional analysis is quantum mechanics. Many problems lead naturally to relationships between a quantity and its rate of change, and these are studied as differential equations. Many phenomena in nature can be described by dynamical systems; chaos theory makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.\n\nApplied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry. Thus, \"applied mathematics\" is a mathematical science with specialized knowledge. The term \"applied mathematics\" also describes the professional specialty in which mathematicians work on practical problems; as a profession focused on practical problems, \"applied mathematics\" focuses on the \"formulation, study, and use of mathematical models\" in science, engineering, and other areas of mathematical practice.\n\nIn the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics, where mathematics is developed primarily for its own sake. Thus, the activity of applied mathematics is vitally connected with research in pure mathematics.\n\nApplied mathematics has significant overlap with the discipline of statistics, whose theory is formulated mathematically, especially with probability theory. Statisticians (working as part of a research project) \"create data that makes sense\" with random sampling and with randomized experiments; the design of a statistical sample or experiment specifies the analysis of the data (before the data be available). When reconsidering data from experiments and samples or when analyzing data from observational studies, statisticians \"make sense of the data\" using the art of modelling and the theory of inference – with model selection and estimation; the estimated models and consequential predictions should be tested on new data.\n\nStatistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints: For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence. Because of its use of optimization, the mathematical theory of statistics shares concerns with other decision sciences, such as operations research, control theory, and mathematical economics.\n\nComputational mathematics proposes and studies methods for solving mathematical problems that are typically too large for human numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis includes the study of approximation and discretization broadly with special concern for rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic matrix and graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.\n\nArguably the most prestigious award in mathematics is the Fields Medal, established in 1936 and awarded every four years (except around World War II) to as many as four individuals. The Fields Medal is often considered a mathematical equivalent to the Nobel Prize.\n\nThe Wolf Prize in Mathematics, instituted in 1978, recognizes lifetime achievement, and another major international award, the Abel Prize, was instituted in 2003. The Chern Medal was introduced in 2010 to recognize lifetime achievement. These accolades are awarded in recognition of a particular body of work, which may be innovational, or provide a solution to an outstanding problem in an established field.\n\nA famous list of 23 open problems, called \"Hilbert's problems\", was compiled in 1900 by German mathematician David Hilbert. This list achieved great celebrity among mathematicians, and at least nine of the problems have now been solved. A new list of seven important problems, titled the \"Millennium Prize Problems\", was published in 2000. Only one of them, the Riemann hypothesis, duplicates one of Hilbert's problems. A solution to any of these problems carries a $1 million reward.\n\n\n"}
{"id": "21708", "url": "https://en.wikipedia.org/wiki?curid=21708", "title": "Non-standard analysis", "text": "Non-standard analysis\n\nThe history of calculus is fraught with philosophical debates about the meaning and logical validity of fluxions or infinitesimal numbers. The standard way to resolve these debates is to define the operations of calculus using epsilon–delta procedures rather than infinitesimals. Non-standard analysis instead reformulates the calculus using a logically rigorous notion of infinitesimal numbers.\n\nNon-standard analysis was originated in the early 1960s by the mathematician Abraham Robinson. He wrote:\n\n[...] the idea of infinitely small or \"infinitesimal\" quantities seems to appeal naturally to our intuition. At any rate, the use of infinitesimals was widespread during the formative stages of the Differential and Integral Calculus. As for the objection [...] that the distance between two distinct real numbers cannot be infinitely small, Gottfried Wilhelm Leibniz argued that the theory of infinitesimals implies the introduction of ideal numbers which might be infinitely small or infinitely large compared with the real numbers but which were \"to possess the same properties as the latter\"\n\nRobinson argued that this law of continuity of Leibniz's is a precursor of the transfer principle. Robinson continued:\n\nHowever, neither he nor his disciples and successors were able to give a rational development leading up to a system of this sort. As a result, the theory of infinitesimals gradually fell into disrepute and was replaced eventually by the classical theory of limits.\n\nRobinson continues:\n\nIt is shown in this book that Leibniz's ideas can be fully vindicated and that they lead to a novel and fruitful approach to classical Analysis and to many other branches of mathematics. The key to our method is provided by the detailed analysis of the relation between mathematical languages and mathematical structures which lies at the bottom of contemporary model theory.\n\nIn 1973, intuitionist Arend Heyting praised non-standard analysis as \"a standard model of important mathematical research\".\n\nA non-zero element of an ordered field formula_1 is infinitesimal if and only if its absolute value is smaller than any element of formula_1 of the form formula_3, for formula_4 a standard natural number. Ordered fields that have infinitesimal elements are also called non-Archimedean. More generally, non-standard analysis is any form of mathematics that relies on non-standard models and the transfer principle. A field which satisfies the transfer principle for real numbers is a hyperreal field, and non-standard real analysis uses these fields as \"non-standard models\" of the real numbers.\n\nRobinson's original approach was based on these non-standard models of the field of real numbers. His classic foundational book on the subject \"Non-standard Analysis\" was published in 1966 and is still in print. On page 88, Robinson writes:\n\nThe existence of non-standard models of arithmetic was discovered by Thoralf Skolem (1934). Skolem's method foreshadows the ultrapower construction [...]\n\nSeveral technical issues must be addressed to develop a calculus of infinitesimals. For example, it is not enough to construct an ordered field with infinitesimals. See the article on hyperreal numbers for a discussion of some of the relevant ideas.\n\nIn this section we outline one of the simplest approaches to defining a hyperreal field formula_5. Let formula_6 be the field of real numbers, and let formula_7 be the semiring of natural numbers. Denote by formula_8 the set of sequences of real numbers. A field formula_5 is defined as a suitable quotient of formula_10, as follows. Take a nonprincipal ultrafilter formula_11. In particular, formula_12 contains the Fréchet filter. Consider a pair of sequences\n\nWe say that formula_14 and formula_15 are equivalent if they coincide on a set of indices which is a member of the ultrafilter, or in formulas:\n\nThe quotient of formula_10 by the resulting equivalence relation is a hyperreal field formula_5, a situation summarized by the formula formula_19.\n\nThere are at least three reasons to consider non-standard analysis: historical, pedagogical, and technical.\n\nMuch of the earliest development of the infinitesimal calculus by Newton and Leibniz was formulated using expressions such as \"infinitesimal number\" and \"vanishing quantity\". As noted in the article on hyperreal numbers, these formulations were widely criticized by George Berkeley and others. It was a challenge to develop a consistent theory of analysis using infinitesimals and the first person to do this in a satisfactory way was Abraham Robinson.\n\nIn 1958 Curt Schmieden and Detlef Laugwitz published an Article \"Eine Erweiterung der Infinitesimalrechnung\" - \"An Extension of Infinitesimal Calculus\", which proposed a construction of a ring containing infinitesimals. The ring was constructed from sequences of real numbers. Two sequences were considered equivalent if they differed only in a finite number of elements. Arithmetic operations were defined elementwise. However, the ring constructed in this way contains zero divisors and thus cannot be a field.\n\nH. Jerome Keisler, David Tall, and other educators maintain that the use of infinitesimals is more intuitive and more easily grasped by students than the \"epsilon-delta\" approach to analytic concepts. This approach can sometimes provide easier proofs of results than the corresponding epsilon-delta formulation of the proof. Much of the simplification comes from applying very easy rules of nonstandard arithmetic, as follows:\n\ntogether with the transfer principle mentioned below.\n\nAnother pedagogical application of non-standard analysis is Edward Nelson's treatment of the theory of stochastic processes.\n\nSome recent work has been done in analysis using concepts from non-standard analysis, particularly in investigating limiting processes of statistics and mathematical physics. Sergio Albeverio et al. discuss some of these applications.\n\nThere are two very different approaches to non-standard analysis: the semantic or model-theoretic approach and the syntactic approach. Both these approaches apply to other areas of mathematics beyond analysis, including number theory, algebra and topology.\n\nRobinson's original formulation of non-standard analysis falls into the category of the \"semantic approach\". As developed by him in his papers, it is based on studying models (in particular saturated models) of a theory. Since Robinson's work first appeared, a simpler semantic approach (due to Elias Zakon) has been developed using purely set-theoretic objects called superstructures. In this approach \"a model of a theory\" is replaced by an object called a \"superstructure\" over a set . Starting from a superstructure one constructs another object using the ultrapower construction together with a mapping that satisfies the transfer principle. The map * relates formal properties of and . Moreover, it is possible to consider a simpler form of saturation called countable saturation. This simplified approach is also more suitable for use by mathematicians who are not specialists in model theory or logic.\n\nThe \"syntactic approach\" requires much less logic and model theory to understand and use. This approach was developed in the mid-1970s by the mathematician Edward Nelson. Nelson introduced an entirely axiomatic formulation of non-standard analysis that he called Internal Set Theory (IST). IST is an extension of Zermelo-Fraenkel set theory (ZF) in that alongside the basic binary membership relation ∈, it introduces a new unary predicate \"standard\", which can be applied to elements of the mathematical universe together with some axioms for reasoning with this new predicate.\n\nSyntactic non-standard analysis requires a great deal of care in applying the principle of set formation (formally known as the axiom of comprehension), which mathematicians usually take for granted. As Nelson points out, a fallacy in reasoning in IST is that of \"illegal set formation\". For instance, there is no set in IST whose elements are precisely the standard integers (here \"standard\" is understood in the sense of the new predicate). To avoid illegal set formation, one must only use predicates of ZFC to define subsets.\n\nAnother example of the syntactic approach is the Alternative Set Theory introduced by Vopěnka, trying to find set-theory axioms more compatible with the non-standard analysis than the axioms of ZF.\n\nAbraham Robinson's book \"Non-standard analysis\" was published in 1966. Some of the topics developed in the book were already present in his 1961 article by the same title (Robinson 1961). In addition to containing the first full treatment of non-standard analysis, the book contains a detailed historical section where Robinson challenges some of the received opinions on the history of mathematics based on the pre–non-standard analysis perception of infinitesimals as inconsistent entities. Thus, Robinson challenges the idea that Augustin-Louis Cauchy's \"sum theorem\" in Cours d'Analyse concerning the convergence of a series of continuous functions was incorrect, and proposes an infinitesimal-based interpretation of its hypothesis that results in a correct theorem.\n\nAbraham Robinson and Allen Bernstein used non-standard analysis to prove that every polynomially compact linear operator on a Hilbert space has an invariant subspace.\n\nGiven an operator on Hilbert space , consider the orbit of a point in under the iterates of . Applying Gram-Schmidt one obtains an orthonormal basis for . Let be the corresponding nested sequence of \"coordinate\" subspaces of . The matrix expressing with respect to is almost upper triangular, in the sense that the coefficients are the only nonzero sub-diagonal coefficients. Bernstein and Robinson show that if is polynomially compact, then there is a hyperfinite index such that the matrix coefficient is infinitesimal. Next, consider the subspace of . If in has finite norm, then is infinitely close to .\n\nNow let be the operator formula_20 acting on , where is the orthogonal projection to . Denote by the polynomial such that is compact. The subspace is internal of hyperfinite dimension. By transferring upper triangularisation of operators of finite-dimensional complex vector space, there is an internal orthonormal Hilbert space basis for where runs from to , such that each of the corresponding -dimensional subspaces is -invariant. Denote by the projection to the subspace . For a nonzero vector of finite norm in , one can assume that is nonzero, or to fix ideas. Since is a compact operator, is infinitely close to and therefore one has also . Now let be the greatest index such that formula_21. Then the space of all standard elements infinitely close to is the desired invariant subspace.\n\nUpon reading a preprint of the Bernstein-Robinson paper, Paul Halmos reinterpreted their proof using standard techniques. Both papers appeared back-to-back in the same issue of the \"Pacific Journal of Mathematics\". Some of the ideas used in Halmos' proof reappeared many years later in Halmos' own work on quasi-triangular operators.\n\nOther results were received along the line of reinterpreting or reproving previously known results. Of particular interest is Kamae's proof of the individual ergodic theorem or van den Dries and Wilkie's treatment of Gromov's theorem on groups of polynomial growth. Nonstandard analysis was used by Larry Manevitz and Shmuel Weinberger to prove a result in algebraic topology.\n\nThe real contributions of non-standard analysis lie however in the concepts and theorems that utilizes the new extended language of non-standard set theory. Among the list of new applications in mathematics there are new approaches to probability \nhydrodynamics, measure theory, nonsmooth and harmonic analysis, etc.\n\nThere are also applications of non-standard analysis to the theory of stochastic processes, particularly constructions of Brownian motion as random walks. Albeverio et-al have an excellent introduction to this area of research.\n\nAs an application to mathematical education, H. Jerome Keisler wrote \"\". Covering non-standard calculus, it develops differential and integral calculus using the hyperreal numbers, which include infinitesimal elements. These applications of non-standard analysis depend on the existence of the \"standard part\" of a finite hyperreal . The standard part of , denoted , is a standard real number infinitely close to . One of the visualization devices Keisler uses is that of an imaginary infinite-magnification microscope to distinguish points infinitely close together. Keisler's book is now out of print, but is freely available from his website; see references below.\n\nDespite the elegance and appeal of some aspects of non-standard analysis, criticisms have been voiced, as well, such as those by E. Bishop, A. Connes, and P. Halmos, as documented at criticism of non-standard analysis.\n\nGiven any set , the \"superstructure\" over a set is the set defined by the conditions\n\nThus the superstructure over is obtained by starting from and iterating the operation of adjoining the power set of and taking the union of the resulting sequence. The superstructure over the real numbers includes a wealth of mathematical structures: For instance, it contains isomorphic copies of all separable metric spaces and metrizable topological vector spaces. Virtually all of mathematics that interests an analyst goes on within .\n\nThe working view of nonstandard analysis is a set and a mapping which satisfies some additional properties. To formulate these principles we first state some definitions.\n\nA formula has \"bounded quantification\" if and only if the only quantifiers which occur in the formula have range restricted over sets, that is are all of the form:\n\nFor example, the formula\n\nhas bounded quantification, the universally quantified variable ranges over , the existentially quantified variable ranges over the powerset of . On the other hand,\n\ndoes not have bounded quantification because the quantification of \"y\" is unrestricted.\n\nA set \"x\" is \"internal\" if and only if \"x\" is an element of *\"A\" for some element \"A\" of . *\"A\" itself is internal if \"A\" belongs to .\n\nWe now formulate the basic logical framework of nonstandard analysis:\n\n\nOne can show using ultraproducts that such a map * exists. Elements of are called \"standard\". Elements of are called hyperreal numbers.\n\nThe symbol denotes the nonstandard natural numbers. By the extension principle, this is a superset of . The set is nonempty. To see this, apply countable saturation to the sequence of internal sets\n\nThe sequence has a nonempty intersection, proving the result.\n\nWe begin with some definitions: Hyperreals \"r\", \"s\" are \"infinitely close\" if and only if\n\nA hyperreal is \"infinitesimal\" if and only if it is infinitely close to 0. For example, if is a hyperinteger, i.e. an element of , then is an infinitesimal. A hyperreal is \"limited\" (or \"finite\") if and only if its absolute value is dominated by (less than) a standard integer. The limited hyperreals form a subring of containing the reals. In this ring, the infinitesimal hyperreals are an ideal.\n\nThe set of limited hyperreals or the set of infinitesimal hyperreals are \"external\" subsets of ; what this means in practice is that bounded quantification, where the bound is an internal set, never ranges over these sets.\n\nExample: The plane with and ranging over is internal, and is a model of plane Euclidean geometry. The plane with and restricted to limited values (analogous to the Dehn plane) is external, and in this limited plane the parallel postulate is violated. For example, any line passing through the point on the -axis and having infinitesimal slope is parallel to the -axis.\n\nTheorem. For any limited hyperreal there is a unique standard real denoted infinitely close to . The mapping is a ring homomorphism from the ring of limited hyperreals to .\n\nThe mapping st is also external.\n\nOne way of thinking of the standard part of a hyperreal, is in terms of Dedekind cuts; any limited hyperreal defines a cut by considering the pair of sets where is the set of standard rationals less than and is the set of standard rationals greater than . The real number corresponding to can be seen to satisfy the condition of being the standard part of .\n\nOne intuitive characterization of continuity is as follows:\n\nTheorem. A real-valued function on the interval is continuous if and only if for every hyperreal in the interval , we have: .\n\n(see microcontinuity for more details). Similarly,\n\nTheorem. A real-valued function is differentiable at the real value if and only if for every infinitesimal hyperreal number , the value\n"}
{"id": "280582", "url": "https://en.wikipedia.org/wiki?curid=280582", "title": "Numerical digit", "text": "Numerical digit\n\nA numerical digit is a single symbol (such as \"2\" or \"5\") used alone, or in combinations (such as \"25\"), to represent numbers (such as the number 25) according to some positional numeral systems. The single digits (as one-digit-numerals) and their combinations (such as \"25\") are the numerals of the numeral system they belong to. The name \"digit\" comes from the fact that the ten digits (Latin \"digiti\" meaning fingers) of the hands correspond to the ten symbols of the common base 10 numeral system, i.e. the decimal (ancient Latin adjective \"decem\" meaning ten) digits. \n\nFor a given numeral system with an integer base, the number of digits required to express arbitrary numbers is given by the absolute value of the base. For example, the decimal system (base 10) requires ten digits (0 through to 9), whereas the binary system (base 2) has two digits (e.g.: 0 and 1).\n\nIn a basic digital system, a numeral is a sequence of digits, which may be of arbitrary length. Each position in the sequence has a place value, and each digit has a value. The value of the numeral is computed by multiplying each digit in the sequence by its place value, and summing the results.\n\nEach digit in a number system represents an integer. For example, in decimal the digit \"1\" represents the integer one, and in the hexadecimal system, the letter \"A\" represents the number ten. A positional number system has one unique digit for each integer from zero up to, but not including, the radix of the number system.\n\nThus in the positional decimal system, the numbers 0 to 9 can be expressed using their respective numerals \"0\" to \"9\" in the rightmost \"units\" position. The number 12 can be expressed with the numeral \"2\" in the units position, and with the numeral \"1\" in the \"tens\" position, to the left of the \"2\" while the number 312 can be expressed by three numerals: \"3\" in the \"hundreds\" position, \"1\" in the \"tens\" position, and \"2\" in the \"units\" position.\n\nThe Hindu–Arabic numeral system (or the Hindu numeral system) uses a decimal separator, commonly a period in English, or a comma in other European languages, to denote the \"ones place\" or \"units place\", which has a place value one. Each successive place to the left of this has a place value equal to the place value of the previous digit times the base. Similarly, each successive place to the right of the separator has a place value equal to the place value of the previous digit divided by the base. For example, in the numeral 10.34 (written in base 10),\n\nThe total value of the number is 1 ten, 0 ones, 3 tenths, and 4 hundredths. Note that the zero, which contributes no value to the number, indicates that the 1 is in the tens place rather than the ones place.\n\nThe place value of any given digit in a numeral can be given by a simple calculation, which in itself is a compliment to the logic behind numeral systems. The calculation involves the multiplication of the given digit by the base raised by the exponent , where \"n\" represents the position of the digit from the separator; the value of \"n\" is positive (+), but this is only if the digit is to the left of the separator. And to the right, the digit is multiplied by the base raised by a negative (−) \"n\". For example, in the number 10.34 (written in base 10),\n\nThe first true written positional numeral system is considered to be the Hindu–Arabic numeral system. This system was established by the 7th century in India, but was not yet in its modern form because the use of the digit zero had not yet been widely accepted. Instead of a zero sometimes the digits were marked with dots to indicate their significance, or a space was used as a placeholder. The first widely acknowledged use of zero was in 876. The original numerals were very similar to the modern ones, even down to the glyphs used to represent digits.\nBy the 13th century, Western Arabic numerals were accepted in European mathematical circles (Fibonacci used them in his \"Liber Abaci\"). They began to enter common use in the 15th century. By the end of the 20th century virtually all non-computerized calculations in the world were done with Arabic numerals, which have replaced native numeral systems in most cultures.\n\nThe exact age of the Maya numerals is unclear, but it is possible that it is older than the Hindu–Arabic system. The system was vigesimal (base 20), so it has twenty digits. The Mayas used a shell symbol to represent zero. Numerals were written vertically, with the ones place at the bottom. The Mayas had no equivalent of the modern decimal separator, so their system could not represent fractions.\n\nThe Thai numeral system is identical to the Hindu–Arabic numeral system except for the symbols used to represent digits. The use of these digits is less common in Thailand than it once was, but they are still used alongside Arabic numerals.\n\nThe rod numerals, the written forms of counting rods once used by Chinese and Japanese mathematicians, are a decimal positional system able to represent not only zero but also negative numbers. Counting rods themselves predate the Hindu–Arabic numeral system. The Suzhou numerals are variants of rod numerals.\n\nThe binary (base 2), octal (base 8), and hexadecimal (base 16) systems, extensively used in computer science, all follow the conventions of the Hindu–Arabic numeral system. The binary system uses only the digits \"0\" and \"1\", while the octal system uses the digits from \"0\" through \"7\". The hexadecimal system uses all the digits from the decimal system, plus the letters \"A\" through \"F\", which represent the numbers 10 to 15 respectively.\n\nThe ternary and balanced ternary systems have sometimes been used. They are both base 3 systems.\n\nBalanced ternary is unusual in having the digit values 1, 0 and –1. Balanced ternary turns out to have some useful properties and the system has been used in the experimental Russian Setun computers.\n\nSeveral authors in the last 300 years have noted a facility of positional notation that amounts to a \"modified\" decimal representation. Some advantages are cited for use of numerical digits that represent negative values. In 1840 Augustin-Louis Cauchy advocated use of signed-digit representation of numbers, and in 1928 Florian Cajori presented his collection of references for negative numerals. The concept of signed-digit representation has also been taken up in computer design.\n\nDespite the essential role of digits in describing numbers, they are relatively unimportant to modern mathematics. Nevertheless, there are a few important mathematical concepts that make use of the representation of a number as a sequence of digits.\n\nThe digital root is the single-digit number obtained by summing the digits of a given number, then summing the digits of the result, and so on until a single-digit number is obtained.\n\nCasting out nines is a procedure for checking arithmetic done by hand. To describe it, let formula_5 represent the digital root of formula_6, as described above. Casting out nines makes use of the fact that if formula_7, then formula_8. In the process of casting out nines, both sides of the latter equation are computed, and if they are not equal the original addition must have been faulty.\n\nRepunits are integers that are represented with only the digit 1. For example, 1111 (one thousand, one hundred and eleven) is a repunit. Repdigits are a generalization of repunits; they are integers represented by repeated instances of the same digit. For example, 333 is a repdigit. The primality of repunits is of interest to mathematicians.\n\nPalindromic numbers are numbers that read the same when their digits are reversed. A Lychrel number is a positive integer that never yields a palindromic number when subjected to the iterative process of being added to itself with digits reversed. The question of whether there are any Lychrel numbers in base 10 is an open problem in recreational mathematics; the smallest candidate is 196.\n\nCounting aids, especially the use of body parts (counting on fingers), were certainly used in prehistoric times as today. There are many variations. Besides counting ten fingers, some cultures have counted knuckles, the space between fingers, and toes as well as fingers. The Oksapmin culture of New Guinea uses a system of 27 upper body locations to represent numbers.\n\nTo preserve numerical information, tallies carved in wood, bone, and stone have been used since prehistoric times. Stone age cultures, including ancient indigenous American groups, used tallies for gambling, personal services, and trade-goods.\n\nA method of preserving numeric information in clay was invented by the Sumerians between 8000 and 3500 BC. This was done with small clay tokens of various shapes that were strung like beads on a string. Beginning about 3500 BC, clay tokens were gradually replaced by number signs impressed with a round stylus at different angles in clay tablets (originally containers for tokens) which were then baked. About 3100  BC, written numbers were dissociated from the things being counted and became abstract numerals.\n\nBetween 2700 and 2000 BC, in Sumer, the round stylus was gradually replaced by a reed stylus that was used to press wedge-shaped cuneiform signs in clay. These cuneiform number signs resembled the round number signs they replaced and retained the additive sign-value notation of the round number signs. These systems gradually converged on a common sexagesimal number system; this was a place-value system consisting of only two impressed marks, the vertical wedge and the chevron, which could also represent fractions. This sexagesimal number system was fully developed at the beginning of the Old Babylonia period (about 1950 BC) and became standard in Babylonia.\n\nSexagesimal numerals were a mixed radix system that retained the alternating base 10 and base 6 in a sequence of cuneiform vertical wedges and chevrons. By 1950 BC, this was a positional notation system. Sexagesimal numerals came to be widely used in commerce, but were also used in astronomical and other calculations. This system was exported from Babylonia and used throughout Mesopotamia, and by every Mediterranean nation that used standard Babylonian units of measure and counting, including the Greeks, Romans and Egyptians. Babylonian-style sexagesimal numeration is still used in modern societies to measure time (minutes per hour) and angles (degrees).\n\nIn China, armies and provisions were counted using modular tallies of prime numbers. Unique numbers of troops and measures of rice appear as unique combinations of these tallies. A great convenience of modular arithmetic is that it is easy to multiply, though quite difficult to add. This makes use of modular arithmetic for provisions especially attractive. Conventional tallies are quite difficult to multiply and divide. In modern times modular arithmetic is sometimes used in digital signal processing.\n\nThe oldest Greek system was that of the Attic numerals, but in the 4th century BC they began to use a quasidecimal alphabetic system (see Greek numerals). Jews began using a similar system (Hebrew numerals), with the oldest examples known being coins from around 100 BC.\n\nThe Roman empire used tallies written on wax, papyrus and stone, and roughly followed the Greek custom of assigning letters to various numbers. The Roman numerals system remained in common use in Europe until positional notation came into common use in the 16th century.\n\nThe Maya of Central America used a mixed base 18 and base 20 system, possibly inherited from the Olmec, including advanced features such as positional notation and a zero. They used this system to make advanced astronomical calculations, including highly accurate calculations of the length of the solar year and the orbit of Venus.\n\nThe Incan Empire ran a large command economy using quipu, tallies made by knotting colored fibers. Knowledge of the encodings of the knots and colors was suppressed by the Spanish conquistadors in the 16th century, and has not survived although simple quipu-like recording devices are still used in the Andean region.\n\nSome authorities believe that positional arithmetic began with the wide use of counting rods in China. The earliest written positional records seem to be rod calculus results in China around 400. In particular, zero was correctly described by Chinese mathematicians around 932.\n\nThe modern positional Arabic numeral system was developed by mathematicians in India, and passed on to Muslim mathematicians, along with astronomical tables brought to Baghdad by an Indian ambassador around 773.\n\nFrom India, the thriving trade between Islamic sultans and Africa carried the concept to Cairo. Arabic mathematicians extended the system to include decimal fractions, and Muḥammad ibn Mūsā al-Ḵwārizmī wrote an important work about it in the 9th  century. The modern Arabic numerals were introduced to Europe with the translation of this work in the 12th century in Spain and Leonardo of Pisa's \"Liber Abaci\" of 1201. In Europe, the complete Indian system with the zero was derived from the Arabs in the 12th century.\n\nThe binary system (base 2), was propagated in the 17th century by Gottfried Leibniz. Leibniz had developed the concept early in his career, and had revisited it when he reviewed a copy of the I ching from China. Binary numbers came into common use in the 20th century because of computer applications.\n\n\n"}
{"id": "49083204", "url": "https://en.wikipedia.org/wiki?curid=49083204", "title": "Panagiota Daskalopoulos", "text": "Panagiota Daskalopoulos\n\nPanagiota Daskalopoulos is a professor of mathematics at Columbia University whose research involves partial differential equations and differential geometry. At Columbia, she also serves as Director of Undergraduate Studies for mathematics.\n\nDaskalopoulos earned a degree from the University of Athens in 1986,\nand completed her Ph.D. from the University of Chicago in 1992, under the supervision of Carlos Kenig. After a visiting position at the Institute for Advanced Study, she joined the faculty of the University of Minnesota in 1993, moved to the University of California, Irvine in 1995, and moved again to Columbia in 2001. She was a Guggenheim Fellow in 2004, and an invited speaker at the International Congress of Mathematicians in 2014. She is a member of the Scientific Advisory Committee of the Mathematical Sciences Research Institute in Berkeley, California for the term 2013–2017.\n"}
{"id": "43919112", "url": "https://en.wikipedia.org/wiki?curid=43919112", "title": "Pantachy", "text": "Pantachy\n\nIn mathematics, a pantachy or pantachie (from the Greek word πανταχη meaning everywhere) is a maximal totally ordered subset of a partially ordered set, especially a set of equivalence classes of sequences of real numbers. The term was introduced by to mean a dense subset of an ordered set, and he also introduced \"infinitary pantachies\" to mean the ordered set of equivalence classes of real functions ordered by domination, but as Felix Hausdorff pointed out this is not a totally ordered set. redefined a pantachy to be a maximal totally ordered subset of this set.\n\n"}
{"id": "47261948", "url": "https://en.wikipedia.org/wiki?curid=47261948", "title": "Peirce's law", "text": "Peirce's law\n\nIn logic, Peirce's law is named after the philosopher and logician Charles Sanders Peirce. It was taken as an axiom in his first axiomatisation of propositional logic. It can be thought of as the law of excluded middle written in a form that involves only one sort of connective, namely implication.\n\nIn propositional calculus, Peirce's law says that ((\"P\"→\"Q\")→\"P\")→\"P\". Written out, this means that \"P\" must be true if there is a proposition \"Q\" such that the truth of \"P\" follows from the truth of \"if \"P\" then \"Q\"\". In particular, when \"Q\" is taken to be a false formula, the law says that if \"P\" must be true whenever it implies falsity, then \"P\" is true. In this way Peirce's law implies the law of excluded middle.\n\nPeirce's law does not hold in intuitionistic logic or intermediate logics and cannot be deduced from the deduction theorem alone.\n\nUnder the Curry–Howard isomorphism, Peirce's law is the type of continuation operators, e.g. call/cc in Scheme.\n\nHere is Peirce's own statement of the law:\n\nPeirce goes on to point out an immediate application of the law:\n\nWarning: ((\"x\"→\"y\")→\"a\")→\"x\" is \"not\" a tautology. However, [\"a\"→\"x\"]→[((\"x\"→\"y\")→\"a\")→\"x\"] is a tautology.\n\nHere is a simple proof of Peirce's law assuming double negation formula_1\n\nformula_2\n\nPeirce's law allows one to enhance the technique of using the deduction theorem to prove theorems. Suppose one is given a set of premises Γ and one wants to deduce a proposition \"Z\" from them. With Peirce's law, one can add (at no cost) additional premises of the form \"Z\"→\"P\" to Γ. For example, suppose we are given \"P\"→\"Z\" and (\"P\"→\"Q\")→\"Z\" and we wish to deduce \"Z\" so that we can use the deduction theorem to conclude that (\"P\"→\"Z\")→(((\"P\"→\"Q\")→\"Z\")→\"Z\") is a theorem. Then we can add another premise \"Z\"→\"Q\". From that and \"P\"→\"Z\", we get \"P\"→\"Q\". Then we apply modus ponens with (\"P\"→\"Q\")→\"Z\" as the major premise to get \"Z\". Applying the deduction theorem, we get that (\"Z\"→\"Q\")→\"Z\" follows from the original premises. Then we use Peirce's law in the form ((\"Z\"→\"Q\")→\"Z\")→\"Z\" and modus ponens to derive \"Z\" from the original premises. Then we can finish off proving the theorem as we originally intended.\n\nOne reason that Peirce's law is important is that it can substitute for the law of excluded middle in the logic which only uses implication. The sentences which can be deduced from the axiom schemas:\n(where \"P\",\"Q\",\"R\" contain only \"→\" as a connective) are all the tautologies which use only \"→\" as a connective.\n\n\n"}
{"id": "32999940", "url": "https://en.wikipedia.org/wiki?curid=32999940", "title": "Peters polynomials", "text": "Peters polynomials\n\nIn mathematics, the Peters polynomials \"s\"(\"x\") are polynomials studied by given by the generating function\n\n, . They are a generalization of the Boole polynomials.\n\n\n"}
{"id": "14644287", "url": "https://en.wikipedia.org/wiki?curid=14644287", "title": "Pfaffian function", "text": "Pfaffian function\n\nIn mathematics, Pfaffian functions are a certain class of functions whose derivative can be written in terms of the original function. They were originally introduced by Askold Georgevich Khovanskiǐ in the 1970s, but are named after German mathematician Johann Pfaff.\n\nSome functions, when differentiated, give a result which can be written in terms of the original function. Perhaps the simplest example is the exponential function, \"f\"(\"x\") = \"e\". If we differentiate this function we get \"e\" again, that is\n\nAnother example of a function like this is the reciprocal function, \"g\"(\"x\") = 1/\"x\". If we differentiate this function we will see that\n\nOther functions may not have the above property, but their derivative may be written in terms of functions like those above. For example, if we take the function \"h\"(\"x\") = \"e\"log(\"x\") then we see\n\nFunctions like these form the links in a so-called Pfaffian chain. Such a chain is a sequence of functions, say \"f\", \"f\", \"f\", etc., with the property that if we differentiate any of the functions in this chain then the result can be written in terms of the function itself and all the functions preceding it in the chain (specifically as a polynomial in those functions and the variables involved). So with the functions above we have that \"f\", \"g\", \"h\" is a Pfaffian chain.\n\nA Pfaffian function is then just a polynomial in the functions appearing in a Pfaffian chain and the function argument. So with the Pfaffian chain just mentioned, functions such as \"F\"(\"x\") = \"x\"\"f\"(\"x\") − 2\"g\"(\"x\")\"h\"(\"x\") are Pfaffian.\n\nLet \"U\" be an open domain in R. A Pfaffian chain of order \"r\" ≥ 0 and degree \"α\" ≥ 1 in \"U\" is a sequence of real analytic functions \"f\",…, \"f\" in \"U\" satisfying differential equations\n\nfor \"i\" = 1,…,\"r\" where \"P\" ∈ R[\"x\"...,\"x\",\"y\"...,\"y\"] are polynomials of degree ≤ \"α\". A function \"f\" on \"U\" is called a Pfaffian function of order \"r\" and degree (\"α\",\"β\") if\n\nwhere \"P\" ∈ R[\"x\"...,\"x\",\"y\"...,\"y\"] is a polynomial of degree at most \"β\" ≥ 1. The numbers \"r\", \"α\", and \"β\" are collectively known as the format of the Pfaffian function, and give a useful measure of its complexity.\n\n\nConsider the structure R = (R,+,−,·,<,0,1), the ordered field of real numbers. In the 1960s Andrei Gabrielov proved that the structure obtained by starting with R and adding a function symbol for every analytic function restricted to the unit box [0,1] is model complete. That is, any set definable in this structure R was just the projection of some higher-dimensional set defined by identities and inequalities involving these restricted analytic functions.\n\nIn the 1990s, Alex Wilkie showed that one has the same result if instead of adding \"every\" analytic function, one just adds the exponential function to R to get the ordered real field with exponentiation, R, a result known as Wilkie's theorem. Wilkie then tackled the question of which finite sets of functions could be added to R to get this result. It turned out that adding any Pfaffian chain restricted to the box [0,1] would give the same result. In particular one may add \"all\" Pfaffian functions to R to get the structure R as an intermediate result between Gabrielov's result and Wilkie's theorem. Since the exponential function is a Pfaffian chain by itself, the result on exponentiation can be viewed as a special case of this latter result.\n\nThis result of Wilkie's proved that the structure R is an o-minimal structure.\n\nThe equations above that define a Pfaffian chain are said to satisfy a triangular condition, since the derivative of each successive function in the chain is a polynomial in one extra variable. Thus if they are written out in turn a triangular shape appears:\nand so on. If this triangularity condition is relaxed so that the derivative of each function in the chain is a polynomial in all the other functions in the chain, then the chain of functions is known as a Noetherian chain, and a function constructed as a polynomial in this chain is called a Noetherian function. So, for example, a Noetherian chain of order three is composed of three functions \"f\", \"f\", \"f\", satisfying the equations\nThe name stems from the fact that the ring generated by the functions in such a chain is Noetherian.\n\nAny Pfaffian chain is also a Noetherian chain; the extra variables in each polynomial are simply redundant in this case. But not every Noetherian chain is Pfaffian. If we take \"f\"(\"x\") = sin(\"x\") and \"f\"(\"x\") = cos(\"x\") then we have the equations\nand these hold for all real numbers \"x\", so \"f\",\"f\" is a Noetherian chain on all of R. But there is no polynomial \"P\"(\"x\",\"y\") such that the derivative of sin(\"x\") can be written as \"P\"(\"x\",sin(\"x\")), and so this chain is not Pfaffian.\n"}
{"id": "4500115", "url": "https://en.wikipedia.org/wiki?curid=4500115", "title": "Proofs from THE BOOK", "text": "Proofs from THE BOOK\n\nProofs from THE BOOK is a book of mathematical proofs by Martin Aigner and Günter M. Ziegler. The book is dedicated to the mathematician Paul Erdős, who often referred to \"The Book\" in which God keeps the most elegant proof of each mathematical theorem. During a lecture in 1985, Erdős said, \"You don't have to believe in God, but you should believe in The Book.\"\n\n\"Proofs from THE BOOK\" contains 32 sections (44 in the fifth edition), each devoted to one theorem but often containing multiple proofs and related results. It spans a broad range of mathematical fields: number theory, geometry, analysis, combinatorics and graph theory. Erdős himself made many suggestions for the book, but died before its publication. The book is illustrated by Karl Heinrich Hofmann. It has gone through five editions in English, and has been translated into Persian, French, German, Hungarian, Italian, Japanese, Chinese, Polish, Portuguese, Korean, Turkish, Russian and Spanish.\n\nIn November 2017 the American Mathematical Society announced the 2018 Leroy P. Steele Prize for Mathematical Exposition to be awarded to Aigner and Ziegler for this book.\n\nThe proofs include:\n\n"}
{"id": "7025898", "url": "https://en.wikipedia.org/wiki?curid=7025898", "title": "Quantum cloning", "text": "Quantum cloning\n\nQuantum cloning is a process that takes an arbitrary, unknown quantum state and makes an exact copy without altering the original state in any way. In Dirac notation, the process of quantum cloning is described by:\nwhere formula_2 is the actual cloning operation, formula_3 is the state to be cloned, and formula_4 is the initial state of the copy.\n\nQuantum cloning is forbidden by the laws of quantum mechanics as shown by the no cloning theorem, which states that there is no operation for cloning any arbitrary state formula_3 perfectly. Though perfect quantum cloning is not possible, it is possible to perform imperfect cloning, where the copies have a non-unit (i.e. non-perfect) fidelity. As a result of cloning, the copies resulted from a pure state will be entangled. A cloner often increases the entanglement between the states. However, outputs of an optimal phase-covariant cloner can be unentangled (see References).\n\nThe accuracy of copies is measured by fidelity and other metrics. The universal property of cloning machines means the input state is equally likely to be any pure state. In universal cloning, the qualities of the two outputs have to be independent of the input states. Other alternative prior distributions include phase-covariant cloners and real cloners. A universal formula_6 cloning machine can have a fidelity as high as 5/6.\n\nThe quantum cloning operation is the best way to make copies of quantum information; therefore, cloning is an important task in quantum information processing, especially in the context of quantum cryptography. Researchers are seeking ways to build quantum cloning machines which work at the so-called quantum limit. The first cloning machine relied on stimulated emission to copy quantum information encoded into single photons. Teleportation, nuclear magnetic resonance, quantum amplification, and superior phase conjugation have been some other methods utilized to realize a quantum cloning machine. Ion trapping techniques have been applied to cloning quantum states of ions.\n\nA paper from 2014 entitled \"Quantum cloning machines and the applications\" contains a complete and updated review about various quantum cloning machines, their applications, and implementations.\n\nIt may be possible to clone a quantum state to arbitrary accuracy in the presence of closed timelike curves.\n\nIt is also possible to consider quantum cloning in more complicated cases; for instance, if the input states are restricted to a special form such that they are equally distributed in the equator of the Bloch sphere which can represent arbitrary states of qubits; alternatively, approximately but optimally quantum-copying N identical states to M states (where M is larger than N) can be considered; . Based on different aims and applications, various quantum cloning machines can be constructed. Universal and phase-covariant quantum cloning machines can be directly related with BB84 and the Six-State Protocol of quantum cryptography. A probabilistic quantum cloning machine can be related with the simplified B92 quantum key distribution protocol. Such quantum cloning machines can be implemented in various physical systems for quantum information processing.\n\nThe Uncertainty principle puts a limit on the fidelity of cloning. However, a higher fidelity can be achieved in one of the copies if the other copy or copies require less fidelity. If the fidelities of the clones are designed to be unequal, the optimal quantum cloning machine is asymmetric. This can be used to customize the accuracy by choosing any arbitrary point in the trade-off curve between the qualities of the copies. The trade-off of optimal accuracy between the resulting copies has been studied in quantum circuits, and with regards to theoretical bounds.\n\nOptimal asymmetric cloning machines are extended to formula_7 in formula_8 dimensions.\n\n\n"}
{"id": "53058", "url": "https://en.wikipedia.org/wiki?curid=53058", "title": "Regular space", "text": "Regular space\n\nIn topology and related fields of mathematics, a topological space \"X\" is called a regular space if every closed subset \"C\" of \"X\" and a point \"p\" not contained in \"C\" admit non-overlapping open neighborhoods. Thus \"p\" and \"C\" can be separated by neighborhoods. This condition is known as Axiom T. The term \"T space\" usually means \"a regular Hausdorff space\". These conditions are examples of separation axioms.\n\nA topological space \"X\" is a regular space if, given any closed set \"F\" and any point \"x\" that does not belong to \"F\", there exists a neighbourhood \"U\" of \"x\" and a neighbourhood \"V\" of \"F\" that are disjoint. Concisely put, it must be possible to separate \"x\" and \"F\" with disjoint neighborhoods.\n\nA T space or regular Hausdorff space is a topological space that is both regular and a Hausdorff space. (A Hausdorff space or T space is a topological space in which any two distinct points are separated by neighbourhoods.) It turns out that a space is T if and only if it is both regular and T. (A T or Kolmogorov space is a topological space in which any two distinct points are topologically distinguishable, i.e., for every pair of distinct points, at least one of them has an open neighborhood not containing the other.) Indeed, if a space is Hausdorff then it is T, and each T regular space is Hausdorff: given two distinct points, at least one of them misses the closure of the other one, so (by regularity) there exist disjoint neighborhoods separating one point from (the closure of) the other.\n\nAlthough the definitions presented here for \"regular\" and \"T\" are not uncommon, there is significant variation in the literature: some authors switch the definitions of \"regular\" and \"T\" as they are used here, or use both terms interchangeably. In this article, we will use the term \"regular\" freely, but we will usually say \"regular Hausdorff\", which is unambiguous, instead of the less precise \"T\". For more on this issue, see History of the separation axioms.\n\nA locally regular space is a topological space where every point has an open neighbourhood that is regular. Every regular space is locally regular, but the converse is not true. A classical example of a locally regular space that is not regular is the bug-eyed line.\n\nA regular space is necessarily also preregular, i.e., any two topologically distinguishable points can be separated by neighbourhoods.\nSince a Hausdorff space is the same as a preregular T space, a regular space which is also T must be Hausdorff (and thus T).\nIn fact, a regular Hausdorff space satisfies the slightly stronger condition T.\nThus, the definition of T may cite T, T, or T instead of T (Hausdorffness); all are equivalent in the context of regular spaces.\n\nSpeaking more theoretically, the conditions of regularity and T-ness are related by Kolmogorov quotients.\nA space is regular if and only if its Kolmogorov quotient is T; and, as mentioned, a space is T if and only if it's both regular and T.\nThus a regular space encountered in practice can usually be assumed to be T, by replacing the space with its Kolmogorov quotient.\n\nThere are many results for topological spaces that hold for both regular and Hausdorff spaces.\nMost of the time, these results hold for all preregular spaces; they were listed for regular and Hausdorff spaces separately because the idea of preregular spaces came later.\nOn the other hand, those results that are truly about regularity generally don't also apply to nonregular Hausdorff spaces.\n\nThere are many situations where another condition of topological spaces (such as normality, pseudonormality, paracompactness, or local compactness) will imply regularity if some weaker separation axiom, such as preregularity, is satisfied.\nSuch conditions often come in two versions: a regular version and a Hausdorff version.\nAlthough Hausdorff spaces aren't generally regular, a Hausdorff space that is also (say) locally compact will be regular, because any Hausdorff space is preregular.\nThus from a certain point of view, regularity is not really the issue here, and we could impose a weaker condition instead to get the same result.\nHowever, definitions are usually still phrased in terms of regularity, since this condition is more well known than any weaker one.\n\nMost topological spaces studied in mathematical analysis are regular; in fact, they are usually completely regular, which is a stronger condition.\nRegular spaces should also be contrasted with normal spaces.\n\nA zero-dimensional space with respect to the small inductive dimension has a base consisting of clopen sets.\nEvery such space is regular.\n\nAs described above, any completely regular space is regular, and any T space that is not Hausdorff (and hence not preregular) cannot be regular.\nMost examples of regular and nonregular spaces studied in mathematics may be found in those two articles.\nOn the other hand, spaces that are regular but not completely regular, or preregular but not regular, are usually constructed only to provide counterexamples to conjectures, showing the boundaries of possible theorems.\nOf course, one can easily find regular spaces that are not T, and thus not Hausdorff, such as an indiscrete space, but these examples provide more insight on the T axiom than on regularity. An example of a regular space that is not completely regular is the Tychonoff corkscrew.\n\nMost interesting spaces in mathematics that are regular also satisfy some stronger condition.\nThus, regular spaces are usually studied to find properties and theorems, such as the ones below, that are actually applied to completely regular spaces, typically in analysis.\n\nThere exist Hausdorff spaces that are not regular. An example is the set R with the topology generated by sets of the form \"U — C\", where \"U\" is an open set in the usual sense, and \"C\" is any countable subset of \"U\".\n\nSuppose that \"X\" is a regular space.\nThen, given any point \"x\" and neighbourhood \"G\" of \"x\", there is a closed neighbourhood \"E\" of \"x\" that is a subset of \"G\".\nIn fancier terms, the closed neighbourhoods of \"x\" form a local base at \"x\".\nIn fact, this property characterises regular spaces; if the closed neighbourhoods of each point in a topological space form a local base at that point, then the space must be regular.\n\nTaking the interiors of these closed neighbourhoods, we see that the regular open sets form a base for the open sets of the regular space \"X\".\nThis property is actually weaker than regularity; a topological space whose regular open sets form a base is \"semiregular\".\n"}
{"id": "76996", "url": "https://en.wikipedia.org/wiki?curid=76996", "title": "Self-organizing map", "text": "Self-organizing map\n\nA self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.\nThis makes SOMs useful for visualization by creating low-dimensional views of high-dimensional data, akin to multidimensional scaling. The artificial neural network introduced by the Finnish professor Teuvo Kohonen in the 1980s is sometimes called a Kohonen map or network. The Kohonen net is a computationally convenient abstraction building on biological models of neural systems from the 1970s and morphogenesis models dating back to Alan Turing in the 1950s.\n\nWhile it is typical to consider this type of network structure as related to feedforward networks where the nodes are visualized as being attached, this type of architecture is fundamentally different in arrangement and motivation.\n\nUseful extensions include using toroidal grids where opposite edges are connected and using large numbers of nodes.\n\nIt has been shown that while self-organizing maps with a small number of nodes behave in a way that is similar to K-means, larger self-organizing maps rearrange data in a way that is fundamentally topological in character.\n\nIt is also common to use the U-Matrix. The U-Matrix value of a particular node is the average distance between the node's weight vector and that of its closest neighbors. In a square grid, for instance, we might consider the closest 4 or 8 nodes (the Von Neumann and Moore neighborhoods, respectively), or six nodes in a hexagonal grid.\n\nLarge SOMs display emergent properties. In maps consisting of thousands of nodes, it is possible to perform cluster operations on the map itself.\n\nLike most artificial neural networks, SOMs operate in two modes: training and mapping. \"Training\" builds the map using input examples (a competitive process, also called vector quantization), while \"mapping\" automatically classifies a new input vector.\n\nThe visible part of a self-organizing map is the map space, it consists of components called nodes or neurons. The map space is defined beforehand, usually as a finite two-dimensional region where nodes are arranged in a regular hexagonal or rectangular grid. Each node is associated with a \"weight\" vector, which is a position in the input space; that is, it has the same dimension as each input vector. While nodes in the map space stay fixed, training consists in moving weight vectors toward the input data (reducing a distance metric) without spoiling the topology induced from the map space. Thus, the self-organizing map describes a mapping from a higher-dimensional input space to a lower-dimensional map space. Once trained, the map can classify a vector from the input space by finding the node with the closest (smallest distance metric) weight vector to the input space vector.\n\nThe goal of learning in the self-organizing map is to cause different parts of the network to respond similarly to certain input patterns. This is partly motivated by how visual, auditory or other sensory information is handled in separate parts of the cerebral cortex in the human brain.\nThe weights of the neurons are initialized either to small random values or sampled evenly from the subspace spanned by the two largest principal component eigenvectors. With the latter alternative, learning is much faster because the initial weights already give a good approximation of SOM weights.\n\nThe network must be fed a large number of example vectors that represent, as close as possible, the kinds of vectors expected during mapping. The examples are usually administered several times as iterations.\n\nThe training utilizes competitive learning. When a training example is fed to the network, its Euclidean distance to all weight vectors is computed. The neuron whose weight vector is most similar to the input is called the best matching unit (BMU). The weights of the BMU and neurons close to it in the SOM grid are adjusted towards the input vector. The magnitude of the change decreases with time and with the grid-distance from the BMU. The update formula for a neuron v with weight vector W(s) is\nwhere s is the step index, t an index into the training sample, u is the index of the BMU for D(t), α(s) is a monotonically decreasing learning coefficient and D(t) is the input vector; Θ(u, v, s) is the neighborhood function which gives the distance between the neuron u and the neuron v in step s. Depending on the implementations, t can scan the training data set systematically (t is 0, 1, 2...T-1, then repeat, T being the training sample's size), be randomly drawn from the data set (bootstrap sampling), or implement some other sampling method (such as jackknifing).\n\nThe neighborhood function Θ(u, v, s) depends on the grid-distance between the BMU (neuron \"u)\" and neuron \"v\". In the simplest form it is 1 for all neurons close enough to BMU and 0 for others, but a Gaussian function is a common choice, too. Regardless of the functional form, the neighborhood function shrinks with time. At the beginning when the neighborhood is broad, the self-organizing takes place on the global scale. When the neighborhood has shrunk to just a couple of neurons, the weights are converging to local estimates. In some implementations the learning coefficient α and the neighborhood function Θ decrease steadily with increasing s, in others (in particular those where t scans the training data set) they decrease in step-wise fashion, once every T steps.\n\nThis process is repeated for each input vector for a (usually large) number of cycles λ. The network winds up associating output nodes with groups or patterns in the input data set. If these patterns can be named, the names can be attached to the associated nodes in the trained net.\n\nDuring mapping, there will be one single \"winning\" neuron: the neuron whose weight vector lies closest to the input vector. This can be simply determined by calculating the Euclidean distance between input vector and weight vector.\n\nWhile representing input data as vectors has been emphasized in this article, it should be noted that any kind of object which can be represented digitally, which has an appropriate distance measure associated with it, and in which the necessary operations for training are possible can be used to construct a self-organizing map. This includes matrices, continuous functions or even other self-organizing maps.\n\nThese are the variables needed, with vectors in bold,\n\n\nA variant algorithm:\n\nSelection of a good initial approximation is a well-known problem for all iterative methods of learning neural networks. Kohonen used random initiation of SOM weights. Recently, principal component initialization, in which initial map weights are chosen from the space of the first principal components, has become popular due to the exact reproducibility of the results.\n\nCareful comparison of the random initiation approach to principal component initialization for one-dimensional SOM (models of principal curves) demonstrated that the advantages of principal component SOM initialization are not universal. The best initialization method depends on the geometry of the specific dataset. Principal component initialization is preferable (in dimension one) if the principal curve approximating the dataset can be univalently and linearly projected on the first principal component (quasilinear sets). For nonlinear datasets, however, random initiation performs better.\n\nConsider an array of nodes, each of which contains a weight vector and is aware of its location in the array. Each weight vector is of the same dimension as the node's input vector. The weights may initially be set to random values.\n\nNow we need input to feed the map. Colors can be represented by their red, green, and blue components. Consequently, we will represent colors as vectors in the unit cube of the free vector space over generated by the basis:\n\nThe diagram shown compares the results of training on the data sets\n\nand the original images. Note the striking resemblance between the two.\n\nSimilarly, after training a grid of neurons for 250 iterations with a learning rate of 0.1 on Fisher's Iris, the map can already detect the main differences between species. \n\nThere are two ways to interpret a SOM. Because in the training phase weights of the whole neighborhood are moved in the same direction, similar items tend to excite adjacent neurons. Therefore, SOM forms a semantic map where similar samples are mapped close together and dissimilar ones apart. This may be visualized by a U-Matrix (Euclidean distance between weight vectors of neighboring cells) of the SOM.\n\nThe other way is to think of neuronal weights as pointers to the input space. They form a discrete approximation of the distribution of training samples. More neurons point to regions with high training sample concentration and fewer where the samples are scarce.\n\nSOM may be considered a nonlinear generalization of Principal components analysis (PCA). It has been shown, using both artificial and real geophysical data, that SOM has many advantages over the conventional feature extraction methods such as Empirical Orthogonal Functions (EOF) or PCA.\n\nOriginally, SOM was not formulated as a solution to an optimisation problem. Nevertheless, there have been several attempts to modify the definition of SOM and to formulate an optimisation problem which gives similar results. For example, Elastic maps use the mechanical metaphor of elasticity to approximate principal manifolds: the analogy is an elastic membrane and plate.\n\n\n\n"}
{"id": "3004023", "url": "https://en.wikipedia.org/wiki?curid=3004023", "title": "Shlomi Dolev", "text": "Shlomi Dolev\n\nShlomi Dolev (, born December 5, 1958) is a Rita Altura Trust Chair Professor in Computer Science at Ben-Gurion University of the Negev (BGU) and the head of the Frankel Center for Computer Science.\n\nDolev received B.Sc. in Civil Engineering and B.A. in Computer Science in 1984 and 1985, and his M.Sc. and D.Sc. in computer science in 1990 and 1992 from the Technion Israel Institute of Technology. From 1992 to 1995 he was at Texas A&M University as a visiting research specialist. In 1995 he joined the Department of Mathematics and Computer Science at BGU. Shlomi is the founder and the first department head of the Computer Science Department at BGU, established in 2000. After just 15 years, the department has been ranked among the first 150 best departments in the world. He is the author of a book entitled Self-Stabilization published by MIT Press in 2000. From 2011 to 2014, Prof. Dolev served as the Dean of the Natural Sciences Faculty at Ben-Gurion University of the Negev. From 2010 he has served for six years, as the Head of the Inter University Computation Center of Israel. He is a co-founder, board member and CSO of Secret Double Octopus Ltd. He is also a co-founder of Secret Sky (SecretSkyDB) Ltd. In 2015 Shlomi was appointed as the steering committee head of the computer science discipline of the Ministry of Education of Israel.\n\nDolev together with Yuval Elovici and Ehud Gudes established the Telekom Innovation Laboratories at Ben-Gurion University. Dolev was also instrumental in establishing the IBM Cyber Security Center of Excellence (CCoE) in Collaboration with Ben-Gurion University of the Negev, and JVP Cyber Labs. Several agencies and companies support his research including ISF, NSF, IBM (faculty awards), Verisign, EMC, Intel, Orange France, Deutsche Telekom, US Airforce and the European Union in the sum of several millions of dollars.\n\nDuring his stay at Ben-Gurion University Shlomi had visiting positions in several institutions including MIT, Paris 11, Paris 6 and DIMACS. He served in more than a hundred program committees, chairing several including the two leading conferences in distributed computing, DISC 2006, and PODC 2014. Recently Prof. Dolev established and chaired the International Symposium on Cyber Security Cryptography and Machine Learning.\n\nShlomi served and serves as an Associate Editor in several international journals including the IEEE Transactions on Computers and PeerJ. His research students more than ten PostDocs, over fifteen PhD students and twenty MSc students, are positioned in Hi-Tech companies, including IBM, Microsoft, Google and Academia.\n\nDolev initiated a transfer of single event workshop on self-stabilization (Austin,1989) to a series of events on the subject.\n\nDolev's Master thesis, under the supervision of Shlomo Moran and Amos Israeli, resulted in the most cited paper on self-stabilization, following the pioneering paper of Dijkstra on the subject, is introducing the concept of fair composition of self-stabilizing systems. Dolev's contribution to the investigation of self-stabilization spans several decades of research and publications, including research on randomized self-stabilizing algorithms, Super Stabilizing algorithms that react gracefully to dynamic changes while preserving the automatic recovery property offered by self-stabilizing systems. Convergence in spite of Byzantine malicious activity of a portion of the participants. Dolev also introduced with co-authors, the concepts of, Silent Stabilization, Local stabilization, Practically Stabilizing, Self-stabilizing and Self-organizing, Transient Failure Detectors and yielding Labeling Schemes. He also presented the first silent self-stabilizing depth first search distributed algorithm.\n\nAnother prominent body of research in Dolev's research is related to mobile ad-hoc networks, including the use of messages random walks, GeoQuarum and virtual infrastructure, where mobile devices currently populating a geographic region implement virtual automata for the region, yielding a fixed infrastructure.\n\nDolev's research in cryptography and cyber security research contributions include the introduction of the xor-trees and buses, secret sharing communication and the accumulating automata and secret shared random-access machine, which evolved to patents and establishment of start-ups.\n\nThe research on optical computing and complexity complements Dolev's cryptographic research, searching for the use of computation gaps, and provable hard on average instances. Dolev initiated a series of four optical supercomputing workshops and several journal special issues (e.g., Optical High-Performance Computing—JOSA A and \"Applied Optics\" \"and\" Optical SuperComputing). Published several papers including a commentary in the Nature photonics journal, a nature communication contribution on reversible computing and a patent.\n\nDolev also contributed to other research field samples include: Complex Networks, Hash Function Data Structures, Brain Science, Real-time Computation, Compression, Game Theory, Erasure Correcting, Transactional Memory, Error Correcting Computations, Verification, Machine Learning and Nanotechnology.\n\nShlomi frequently collaborates with many other researchers in computer science including Jeffrey Ullman, Nancy Lynch, Adi Shamir, Moti Yung and  Noga Alon.\n\n"}
{"id": "13674471", "url": "https://en.wikipedia.org/wiki?curid=13674471", "title": "Swarm Development Group", "text": "Swarm Development Group\n\nThe Swarm Development Group (SDG) is an American non-profit organization to advance the development of complex adaptive system-oriented agent-based modeling (ABM) tools initiated at the Santa Fe Institute (SFI) in Santa Fe, New Mexico, US. It was formed in 1999 by a group of multidisciplinary scientists, researchers, and software developers, led by Chris Langton. Langton was also the founder of the emerging field of research called artificial life. The initial, primary, role for the SDG was to house continued development of the Swarm simulation software after the software become independent of the SFI in 1999.\n\nThe role of the Swarm Development Group, has since expanded to include the co-ordination of a long-running conference SwarmFest during May, June, or July each summer – typically hosted by a different research university each year. Developers, users, and researchers gather to present research papers and discuss the state of Swarm and other agent-based modeling platforms like RePast (University of Chicago) and Ascape (Brookings). Typically, a wide range of academic, corporate, and government organizations are represented at SwarmFest. The first SwarmFest was in 1998 while Swarm was still sponsored by the Santa Fe Institute. From SwarmFest 2000 onwards, after the SDG was formed in late 1999, SwarmFests were organized directly by the SDG.\n\nRecent SwarmFests have been held at a variety of institutions. SwarmFest 2007 was held at DePaul University's School of Computer Science, Telecommunications, and Information Systems, in downtown Chicago, Illinois. SwarmFest 2008 was held at Northwestern Memorial Hospital/Northwestern University Feinberg School of Medicine, in downtown Chicago, Illinois. Swarmfest 2008 had special focus areas on agent based modeling in Systems Biology, and the implementation of agent based models in high-performance computing environments. Between 2009 and 2012, SwarmFest was held at the Santa Fe Complex in Santa Fe, New Mexico. Swarmfest 2013 was held at the University of Central Florida in Orlando, Florida July 8–9.; SwarmFest 2014 was held at the University of Notre Dame in Notre Dame, IN from June 29 – July 1, 2014; the University of South Carolina in Columbia, SC (in 2015), the University of Vermont in Burlington, VT (in 2016), and Old Dominion University in Suffolk, VA (in 2017), were the selected venues.\n\n"}
{"id": "3140914", "url": "https://en.wikipedia.org/wiki?curid=3140914", "title": "Symmetric product of an algebraic curve", "text": "Symmetric product of an algebraic curve\n\nIn mathematics, the \"n\"-fold symmetric product of an algebraic curve \"C\" is the quotient space of the \"n\"-fold cartesian product\n\nor \"C\" by the group action of the symmetric group on \"n\" letters permuting the factors. It exists as a smooth algebraic variety Σ\"C\"; if \"C\" is a compact Riemann surface it is therefore a complex manifold. Its interest in relation to the classical geometry of curves is that its points correspond to effective divisors on \"C\" of degree \"n\", that is, formal sums of points with non-negative integer coefficients.\n\nFor \"C\" the projective line (say the Riemann sphere) Σ\"C\" can be identified with projective space of dimension \"n\".\n\nIf \"G\" has genus \"g\" ≥ 1 then the Σ\"C\" are closely related to the Jacobian variety \"J\" of \"C\". More accurately for \"n\" taking values up to \"g\" they form a sequence of approximations to \"J\" from below: their images in \"J\" under addition on \"J\" (see theta-divisor) have dimension \"n\" and fill up \"J\", with some identifications caused by special divisors.\n\nFor \"g\" = \"n\" we have Σ\"C\" actually birationally equivalent to \"J\"; the Jacobian is a blowing down of the symmetric product. That means that at the level of function fields it is possible to construct \"J\" by taking linearly disjoint copies of the function field of \"C\", and within their compositum taking the fixed subfield of the symmetric group. This is the source of André Weil's technique of constructing \"J\" as an abstract variety from 'birational data'. Other ways of constructing \"J\", for example as a Picard variety, are preferred now (Greg W. Anderson (\"Advances in Mathematics\" 172 (2002) 169–205) provided an elementary construction as lines of matrices). But this does mean that for any rational function \"F\" on \"C\"\n\nmakes sense as a rational function on \"J\", for the \"x\" staying away from the poles of \"F\".\n\nFor \"N\" > \"g\" the mapping from Σ\"C\" to \"J\" by addition fibers it over \"J\"; when \"n\" is large enough (around twice \"g\") this becomes a projective space bundle (the Picard bundle). It has been studied in detail, for example by Kempf and Mukai.\n\nLet \"C\" be smooth and projective of genus \"g\" over the complex numbers C. The Betti numbers \"b\"(ΣC) of the symmetric product are given by\nand the topological Euler characteristic \"e\"(ΣC) is given by\nHere we have set \"u\"=-1 and \"y\" = - \"p\" in the formula before.\n\n"}
{"id": "2163279", "url": "https://en.wikipedia.org/wiki?curid=2163279", "title": "Undulating number", "text": "Undulating number\n\nAn undulating number is a number that has the digit form ABABAB... when in the base 10 number system. It is sometimes restricted to non-trivial undulating numbers which are required to have at least 3 digits and A ≠ B. The first few such numbers are: \n\nFor the full sequence of undulating numbers, see .\n\nSome higher undulating numbers are: 6363, 80808, 1717171.\n\nFor any \"n\" ≥ 3, there are 9 × 9 = 81 non-trivial \"n\"-digit undulating numbers, since the first digit can have 9 values (it cannot be 0), and the second digit can have 9 values when it must be different from the first.\n\nAn \"undulating prime\" is an undulating number that is also prime. In every base, all undulating primes having at least 3 digits have an odd number of digits. The undulating primes in base 10 are:\n"}
{"id": "51672", "url": "https://en.wikipedia.org/wiki?curid=51672", "title": "Vacuous truth", "text": "Vacuous truth\n\nIn mathematics and logic, a vacuous truth is a statement that asserts that all members of the empty set have a certain property. For example, the statement \"all cell phones in the room are turned off\" will be true whenever there are no cell phones in the room. In this case, the statement \"all cell phones in the room are turned \"on\"\" would also be vacuously true, as would the conjunction of the two: \"all cell phones in the room are turned on \"and\" turned off\".\n\nMore formally, a relatively well-defined usage refers to a conditional statement with a false antecedent. One example of such a statement is \"if Uluru is in France, then the Eiffel Tower is in Bolivia\". Such statements are considered vacuous because the fact that the antecedent is false prevents using the statement to infer anything about the truth value of the consequent. They are true because a material conditional is defined to be true when the antecedent is false (regardless of whether the conclusion is true).\n\nIn pure mathematics, vacuously true statements are not generally of interest by themselves, but they frequently arise as the base case of proofs by mathematical induction. This notion has relevance in pure mathematics, as well as in any other field which uses classical logic.\n\nOutside of mathematics, statements which can be characterized informally as vacuously true can be misleading. Such statements make reasonable assertions about qualified objects which do not actually exist. For example, a child might tell their parent \"I ate every vegetable on my plate\", when there were no vegetables on the child's plate to begin with.\n\nA statement formula_1 is \"vacuously true\" if it resembles the statement formula_2, where formula_3 is known to be false.\n\nStatements that can be reduced (with suitable transformations) to this basic form include the following universally quantified statements:\n\nVacuous truth most commonly appears in classical logic, which in particular is two-valued. However, vacuous truth also appears in, for example, intuitionistic logic in the same situations given above. Indeed, if formula_3 is false, formula_2 will yield vacuous truth in any logic that uses the material conditional; if formula_3 is a necessary falsehood, then it will also yield vacuous truth under the strict conditional.\n\nOther non-classical logics (for example, relevance logic) may attempt to avoid vacuous truths by using alternative conditionals (for example, the counterfactual conditional).\n\nThese examples, one from mathematics and one from natural language, illustrate the concept:\n\n\"For any integer x, if x > 5 then x > 3.\" – This statement is true non-vacuously (since some integers are greater than 5), but some of its implications are only vacuously true: for example, when x is the integer 2, the statement implies the vacuous truth that \"if 2 > 5 then 2 > 3\".\n\n\"All my children are cats\" is a vacuous truth when spoken by someone without children.\n\n\n\n"}
{"id": "20369415", "url": "https://en.wikipedia.org/wiki?curid=20369415", "title": "Victor Pan", "text": "Victor Pan\n\nVictor Yakovlevich Pan () is a Soviet and American mathematician and computer scientist, known for his research on algorithms for polynomials and matrix multiplication.\n\nPan earned his Ph.D. at Moscow University in 1964, under the supervision of Anatoli Georgievich Vitushkin, and continued his work at the Soviet Academy of Sciences. During that time, he published a number of significant papers and became known informally as \"polynomial Pan\" for his pioneering work in the area of polynomial computations. In late 1970s, he immigrated to the United States and held positions at several institutions including IBM Research. Since 1988, he has taught at Lehman College of the City University of New York.\n\nVictor Pan is an expert in computational complexity and has developed a number of new algorithms. One of his notable early results is a proof that the number of multiplications in Horner's method is optimal.\n\nIn the theory of matrix multiplication algorithms, Pan in 1978 published an algorithm with running time formula_1. This was the first improvement over the Strassen algorithm, and kicked off a long line of improvements in fast matrix multiplication that later included the Coppersmith–Winograd algorithm and subsequent developments. He wrote the text \"How to Multiply Matrices Faster\" (Springer, 1984) surveying early developments in this area. In 1998, with his student Xiaohan Huang, Pan showed that matrix multiplication algorithms can take advantage of rectangular matrices with unbalanced aspect ratios, multiplying them more quickly than the time bounds one would obtain using square matrix multiplication algorithms.\n\nSince that work, Pan has returned to symbolic and numeric computation and to an earlier theme of his research, computations with polynomials. He developed fast algorithms for the numerical computation of polynomial roots,\nand, with Bernard Mourrain, algorithms for multivariate polynomials based on their relations to structured matrices.\nHe also authored or co-authored several more books, on matrix and polynomial computation,\nstructured matrices, and on\nnumerical root-finding proceedures.\n\nPan was appointed Distinguished Professor at Lehman College in 2000.\n\nIn 2013 he became a fellow of the American Mathematical Society, for \"contributions to the mathematical theory of computation\".\n\n \n"}
{"id": "877209", "url": "https://en.wikipedia.org/wiki?curid=877209", "title": "Zassenhaus lemma", "text": "Zassenhaus lemma\n\nIn mathematics, the butterfly lemma or Zassenhaus lemma, named after Hans Zassenhaus, is a technical result on the lattice of subgroups of a group or the lattice of submodules of a module, or more generally for any modular lattice.\n\nLemma: Suppose formula_1 is a group with subgroups formula_2 and formula_3. Suppose\n\nare normal subgroups. Then there is an isomorphism of quotient groups:\n\nThis can be generalized to the case of a group with operators formula_7 with stable subgroups formula_2 and formula_3, the above statement being the case of formula_10 acting on itself by conjugation.\n\nZassenhaus proved this lemma specifically to give the most direct proof of the Schreier refinement theorem. The 'butterfly' becomes apparent when trying to draw the Hasse diagram of the various groups involved.\n\nZassenhaus' lemma for groups can be derived from a more general result known as Goursat's theorem stated in a Goursat variety (of which groups are an instance); however the group-specific modular law also needs to be used in the derivation.\n\n\n"}
