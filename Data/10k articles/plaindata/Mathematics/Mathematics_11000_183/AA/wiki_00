{"id": "378154", "url": "https://en.wikipedia.org/wiki?curid=378154", "title": "76 (number)", "text": "76 (number)\n\n76 (seventy-six) is the natural number following 75 and preceding 77.\n\n76 is:\n\n\nSeventy-six is also:\n\n"}
{"id": "723105", "url": "https://en.wikipedia.org/wiki?curid=723105", "title": "Abstract simplicial complex", "text": "Abstract simplicial complex\n\nIn mathematics, an abstract simplicial complex is a purely combinatorial description of the geometric notion of a simplicial complex, consisting of a family of non-empty finite sets closed under the operation of taking non-empty subsets. In the context of matroids and greedoids, abstract simplicial complexes are also called independence systems. \n\nAn abstract simplex can be studied algebraically by forming its Stanley–Reisner ring; this sets up a powerful relation between combinatorics and commutative algebra.\n\nA family of non-empty finite subsets of a set \"S\" is an abstract simplicial complex if, for every set in , and every non-empty subset , also belongs to .\n\nThe finite sets that belong to are called faces of the complex, and a face is said to belong to another face if , so the definition of an abstract simplicial complex can be restated as saying that every face of a face of a complex is itself a face of . The vertex set of is defined as , the union of all faces of . The elements of the vertex set are called the vertices of the complex. For every vertex \"v\" of , the set {\"v\"} is a face of the complex, and every face of the complex is a finite subset of the vertex set. \n\nThe maximal faces of (i.e., faces that are not subsets of any other faces) are called facets of the complex. The dimension of a face in is defined as : faces consisting of a single element are zero-dimensional, faces consisting of two elements are one-dimensional, etc. The dimension of the complex is defined as the largest dimension of any of its faces, or infinity if there is no finite bound on the dimension of the faces.\n\nThe complex is said to be finite if it has finitely many faces, or equivalently if its vertex set is finite. Also, is said to be pure if it is finite-dimensional (but not necessarily finite) and every facet has the same dimension. In other words, is pure if is finite and every face is contained in a facet of dimension .\n\nOne-dimensional abstract simplicial complexes are mathematically equivalent to simple undirected graphs: the vertex set of the complex can be viewed as the vertex set of a graph, and the two-element facets of the complex correspond to undirected edges of a graph. In this view, one-element facets of a complex correspond to isolated vertices that do not have any incident edges. \n\nA subcomplex of is a simplicial complex \"L\" such that every face of \"L\" belongs to ; that is, and \"L\" is a simplicial complex. A subcomplex that consists of all of the subsets of a single face of is often called a simplex of . (However, some authors use the term \"simplex\" for a face or, rather ambiguously, for both a face and the subcomplex associated with a face, by analogy with the non-abstract (geometric) simplicial complex terminology. To avoid ambiguity, we do not use in this article the term \"simplex\" for a face in the context of abstract complexes.)\n\nThe d-skeleton of is the subcomplex of consisting of all of the faces of that have dimension at most \"d\". In particular, the 1-skeleton is called the underlying graph of . The 0-skeleton of can be identified with its vertex set, although formally it is not quite the same thing (the vertex set is a single set of all of the vertices, while the 0-skeleton is a family of single-element sets).\n\nThe link of a face in , often denoted or , is the subcomplex of defined by\n\nNote that the link of the empty set is itself.\n\nGiven two abstract simplicial complexes, and , a simplicial map is a function that maps the vertices of to the vertices of and that has the property that for any face of , the image set is a face of . There is a category SCpx with abstract simplicial complexes as objects and simplicial maps as morphisms. This is equivalent to a suitable category defined using non-abstract simplicial complexes. \n\nMoreover, the categorical point of view allows us to tighten the relation between the underlying set \"S\" of an abstract simplicial complex and the vertex set of : for the purposes of defining a category of abstract simplicial complexes, the elements of \"S\" not lying in are irrelevant. More precisely, SCpx is equivalent to the category where:\n\nWe can associate to an abstract simplicial complex \"K\" a topological space |\"K\"|, called its geometric realization, which is the carrier of a simplicial complex. The construction goes as follows.\n\nFirst, define |\"K\"| as a subset of consisting of functions satisfying the two conditions:\nNow think of as the direct limit of where \"A\" ranges over finite subsets of \"S\", and give the induced topology. Now give |\"K\"| the subspace topology.\n\nAlternatively, let formula_4 denote the category whose objects are the faces of and whose morphisms are inclusions. Next choose a total order on the vertex set of and define a functor \"F\" from formula_4 to the category of topological spaces as follows. For any face of dimension \"n\", let be the standard \"n\"-simplex. The order on the vertex set then specifies a unique bijection between the elements of and vertices of , ordered in the usual way . If is a face of dimension , then this bijection specifies a unique \"m\"-dimensional face of . Define to be the unique affine linear embedding of as that distinguished face of , such that the map on vertices is order preserving.\n\nWe can then define the geometric realization |\"K\"| as the colimit of the functor \"F\". More specifically |\"K\"| is the quotient space of the disjoint union\n\nby the equivalence relation which identifies a point with its image under the map , for every inclusion .\n\nIf \"K\" is finite, then we can describe |\"K\"| more simply. Choose an embedding of the vertex set of \"K\" as an affinely independent subset of some Euclidean space of sufficiently high dimension \"N\". Then any face can be identified with the geometric simplex in spanned by the corresponding embedded vertices. Take |\"K\"| to be the union of all such simplices.\n\nIf \"K\" is the standard combinatorial \"n\"-simplex, then |\"K\"| can be naturally identified with .\n\n\nThe number of abstract simplicial complexes on up to \"n\" elements is one less than the \"n\"th Dedekind number. These numbers grow very rapidly, and are known only for ; they are (starting with \"n\" = 0):\n\nThe number of abstract simplicial complexes on exactly \"n\" labeled elements is given by the sequence \"1, 2, 9, 114, 6894, 7785062, 2414627396434, 56130437209370320359966\" , starting at \"n\" = 1. This corresponds to the number of antichain covers of a labeled \"n\"-set; there is a clear bijection between antichain covers of an \"n\"-set and simplicial complexes on \"n\" elements described in terms of their maximal faces.\n\nThe number of abstract simplicial complexes on exactly \"n\" unlabeled elements is given by the sequence \"1, 2, 5, 20, 180, 16143\" , starting at \"n\" = 1.\n\n"}
{"id": "5438924", "url": "https://en.wikipedia.org/wiki?curid=5438924", "title": "Algebraic operation", "text": "Algebraic operation\n\nIn mathematics, a basic algebraic operation is any one of the traditional operations of arithmetic, which are addition, subtraction, multiplication, division, raising to an integer power, and taking roots (fractional power). These operations may be performed on numbers, in which case they are often called arithmetic operations. They may also be performed, in a similar way, on variables, algebraic expressions, and, more generally on elements of algebraic structures, such as groups and fields.\n\nThe term \"algebraic operation\" may also be used for operations that may be defined by compounding basic algebraic operations, such as the dot product. In calculus and mathematical analysis, \"algebraic operation\" is also used for the operations that may be defined by purely algebraic methods. For example, exponentiation with an integer or rational exponent is an algebraic operation, but not the general exponentiation with a real or complex exponent. Also, the derivative is an operation that is not algebraic.\n\nMultiplication symbols are usually omitted, and implied, when there is no operator between two variables or terms, or when a coefficient is used. For example, 3 × \"x\" is written as 3\"x\", and 2 × \"x\" × \"y\" is written as 2\"xy\". Sometimes multiplication symbols are replaced with either a dot, or center-dot, so that \"x\" × \"y\" is written as either \"x\" . \"y\" or \"x\" · \"y\". Plain text, programming languages, and calculators also use a single asterisk to represent the multiplication symbol, and it must be explicitly used; for example, 3\"x\" is written as 3 * \"x\".\n\nRather than using the obelus symbol, ÷, division is usual represented with a vinculum, a horizontal line, e.g. . In plain text and programming languages a slash (also called a solidus) is used, e.g. 3 / (\"x\" + 1).\n\nExponents are usually formatted using superscripts, e.g. \"x\". In plain text, and in the TeX mark-up language, the caret symbol, ^, represents exponents, so \"x\" is written as \"x\" ^ 2. In programming languages such as Ada, Fortran, Perl, Python and Ruby, a double asterisk is used, so \"x\" is written as \"x\" ** 2.\n\nThe plus-minus sign, ±, is used as a shorthand notation for two expressions written as one, representing one expression with a plus sign, the other with a minus sign. For example, \"y\" = \"x\" ± 1 represents the two equations \"y\" = \"x\" + 1 and \"y\" = \"x\" − 1. Sometimes it is used for denoting a positive-or-negative term such as ±\"x\".\n\nAlgebraic operations work in the same way as arithmetic operations, as can be seen in the table below.\nNote: the use of the letters formula_1 and formula_2 is arbitrary, and the examples would be equally valid if we had used formula_3 and formula_4.\n\n"}
{"id": "17960231", "url": "https://en.wikipedia.org/wiki?curid=17960231", "title": "Algebraic signal processing", "text": "Algebraic signal processing\n\nIn the algebraic theory of linear signal processing, a set of filters is treated as an algebra and a set of signals is treated as a module and the z-transform is generalized to linear maps.\n\n"}
{"id": "15212413", "url": "https://en.wikipedia.org/wiki?curid=15212413", "title": "Analyse des Infiniment Petits pour l'Intelligence des Lignes Courbes", "text": "Analyse des Infiniment Petits pour l'Intelligence des Lignes Courbes\n\nAnalyse des Infiniment Petits pour l'Intelligence des Lignes Courbes (literal translation: \"Analysis of the infinitely small to understand curves\"), 1696, is the first textbook published on the infinitesimal calculus of Leibniz. It was written by the French mathematician Guillaume de l'Hôpital, and treated only the subject of differential calculus. Two volumes treating the differential and integral calculus, respectively, had been authored by Johann Bernoulli in 1691–1692, and the latter was published in 1724 to become the first published textbook on the integral calculus.\n\nIn this book is the first appearance of L'Hôpital's rule. The rule is believed to be the work of Johann Bernoulli since l'Hôpital, a nobleman, paid Bernoulli a retainer of 300₣ per year to keep him updated on developments in calculus and to solve problems he had. Moreover, the two signed a contract allowing l'Hôpital to use Bernoulli's discoveries in any way he wished. Among these problems was that of limits of indeterminate forms. When l'Hôpital published his book, he gave due credit to Bernoulli and, not wishing to take credit for any of the mathematics in the book, he published the work anonymously. Bernoulli, who was known for being extremely jealous, claimed to be the author of the entire work. Nevertheless, the rule was named for l'Hôpital, who never claimed to have invented it in the first place.\n\n\n"}
{"id": "2738697", "url": "https://en.wikipedia.org/wiki?curid=2738697", "title": "Autoepistemic logic", "text": "Autoepistemic logic\n\nThe autoepistemic logic is a formal logic for the representation and reasoning of knowledge about knowledge. While propositional logic can only express facts, autoepistemic logic can express knowledge and lack of knowledge about facts.\n\nThe stable model semantics, which is used to give a semantics to logic programming with negation as failure, can be seen as a simplified form of autoepistemic logic.\n\nThe syntax of autoepistemic logic extends that of propositional logic by a modal operator formula_1 indicating knowledge: if formula_2 is a formula, formula_3 indicates that formula_2 is known. As a result, formula_5 indicates that formula_6 is known and formula_7 indicates that formula_2 is not known.\n\nThis syntax is used for allowing reasoning based on knowledge of facts. For example, formula_9 means that formula_2 is assumed false if it is not known to be true. This is a form of negation as failure.\n\nThe semantics of autoepistemic logic is based on the \"expansions\" of a theory, which have a role similar to models in propositional logic. While a propositional model specifies which axioms are true or false, an expansion specifies which formulae formula_3 are true and which ones are false. In particular, the expansions of an autoepistemic formula formula_12 makes this distinction for every subformula formula_3 contained in formula_12. This distinction allows formula_12 to be treated as a propositional formula, as all its subformulae containing formula_1 are either true or false. In particular, checking whether formula_12 entails formula_2 in this condition can be done using the rules of the propositional calculus. In order for an initial assumption to be an expansion, it must be that a subformula formula_2 is entailed if and only if formula_3 has been initially assumed true.\n\nIn terms of possible world semantics, an expansion of formula_12 consists of an S5 model of formula_12 in which the possible worlds consist only of worlds where formula_12 is true. [The possible worlds need not contain all such consistent worlds; this corresponds to the fact that modal propositions are assigned truth values before checking derivability of the ordinary propositions.] Thus, autoepistemic logic extends S5; the extension is proper, since formula_24 and formula_25 are tautologies of autoepistemic logic, but not of S5.\n\nFor example, in the formula formula_26, there is only a single “boxed subformula”, which is formula_27. Therefore, there are only two candidate expansions, assuming it true or false, respectively. The check for them being actual expansions is as follows.\n\nformula_27 is false : with this assumption, formula_12 becomes tautological, as formula_30 is equivalent to formula_31, and formula_32 is assumed true; therefore, formula_33 is not entailed. This result confirms the assumption implicit in formula_27 being false, that is, that formula_33 is not currently known. Therefore, the assumption that formula_27 is false is an expansion.\n\nformula_27 is true : together with this assumption, formula_12 entails formula_33; therefore, the initial assumption that is implicit in formula_27 being true, i.e., that formula_33 is known to be true, is satisfied. As a result, this is another expansion.\n\nThe formula formula_12 has therefore two expansions, one in which formula_33 is not known and one in which formula_33 is known. The second one has been regarded as unintuitive, as the initial assumption that formula_27 is true is the only reason why formula_33 is true, which confirms the assumption. In other words, this is a self-supporting assumption. A logic allowing such a self-support of beliefs is called \"not strongly grounded\" to differentiate them from \"strongly grounded\" logics, in which self-support is not possible. Strongly grounded variants of autoepistemic logic exist.\n\nIn uncertain inference, the known/unknown duality of truth values is replaced by a degree of certainty of a fact or deduction; certainty may vary from 0 (completely uncertain/unknown) to 1 (certain/known). In probabilistic logic networks, truth values are also given a probabilistic interpretation (\"i.e.\" truth values may be uncertain, and, even if almost certain, they may still be \"probably\" true (or false).)\n\n\n"}
{"id": "717519", "url": "https://en.wikipedia.org/wiki?curid=717519", "title": "Centered pentagonal number", "text": "Centered pentagonal number\n\nA centered pentagonal number is a centered figurate number that represents a pentagon with a dot in the center and all other dots surrounding the center in successive pentagonal layers. The centered pentagonal number for \"n\" is given by the formula \n"}
{"id": "4094671", "url": "https://en.wikipedia.org/wiki?curid=4094671", "title": "Chauvenet Prize", "text": "Chauvenet Prize\n\nThe Chauvenet Prize is the highest award for mathematical expository writing. It consists of a prize of $1,000 and a certificate, and is awarded yearly by the Mathematical Association of America in recognition of an outstanding expository article on a mathematical topic. The prize is named in honor of William Chauvenet and was established through a gift from J. L. Coolidge in 1925. The Chauvenet Prize was the first award established by the Mathematical Association of America.\n\nSource:Mathematical Association of America\n"}
{"id": "2934314", "url": "https://en.wikipedia.org/wiki?curid=2934314", "title": "Chevalley–Warning theorem", "text": "Chevalley–Warning theorem\n\nIn number theory, the Chevalley–Warning theorem implies that certain polynomial equations in sufficiently many variables over a finite field have solutions. It was proved by and a slightly weaker form of the theorem, known as Chevalley's theorem, was proved by . Chevalley's theorem implied Artin's and Dickson's conjecture that finite fields are quasi-algebraically closed fields .\n\nLet formula_1 be a finite field and formula_2 be a set of polynomials such that the number of variables satisfies \n\nwhere formula_4 is the total degree of formula_5. The theorems are statements about the solutions of the following system of polynomial equations\n\n\nChevalley's theorem is an immediate consequence of the Chevalley–Warning theorem since formula_8 is at least 2. \n\nBoth theorems are best possible in the sense that, given any formula_16, the list formula_17 has total degree formula_16 and only the trivial solution. Alternatively, using just one polynomial, we can take \"f\" to be the degree \"n\" polynomial given by the norm of \"x\"\"a\" + ... + \"x\"\"a\" where the elements \"a\" form a basis of the finite field of order \"p\".\n\nWarning proved another theorem, known as Warning's second theorem, which states that if the system of polynomial equations has the trivial solution, then it has at least formula_19 solutions where formula_20 is the size of the finite field and formula_21. Chevalley's theorem also follows directly from this.\n\n\"Remark:\" If formula_22 then \nso the sum over formula_24 of any polynomial in formula_25 of degree less than formula_26 also vanishes.\n\nThe total number of common solutions modulo formula_8 of formula_28 is equal to\nbecause each term is 1 for a solution and 0 otherwise.\nIf the sum of the degrees of the polynomials formula_30 is less than \"n\" then this vanishes by the remark above.\n\nIt is a consequence of Chevalley's theorem that finite fields are quasi-algebraically closed. This had been conjectured by Emil Artin in 1935. The motivation behind Artin's conjecture was his observation that quasi-algebraically closed fields have trivial Brauer group, together with the fact that finite fields have trivial Brauer group by Wedderburn's theorem.\n\nThe Ax–Katz theorem, named after James Ax and Nicholas Katz, determines more accurately a power formula_31 of the cardinality formula_20 of formula_1 dividing the number of solutions; here, if formula_34 is the largest of the formula_4, then the exponent formula_36 can be taken as the ceiling function of \n\nThe Ax–Katz result has an interpretation in étale cohomology as a divisibility result for the (reciprocals of) the zeroes and poles of the local zeta-function. Namely, the same power of formula_20 divides each of these algebraic integers.\n\n\n"}
{"id": "39675445", "url": "https://en.wikipedia.org/wiki?curid=39675445", "title": "Data literacy", "text": "Data literacy\n\nData literacy is the ability to read, understand, create and communicate data as information. Much like literacy as a general concept, data literacy focuses on the competencies involved in working with data. \nAs data collection and data sharing become routine and data analysis and big data become common ideas in the news, business, government and society, it becomes more and more important for students, citizens, and readers to have some data literacy. \n\nData literacy focuses on the ability to understand and build knowledge from data, and to communicate that meaning to others. It is related to other fields, including:\n\n\n\nAs guides for finding and using information, librarians lead workshops on data literacy for students and researchers, and also work on developing their own data literacy skills. \n\nResources created by librarians include MIT's Data Management and Publishing tutorial, the EDINA Research Data Management Training (MANTRA), the University of Edinburgh’s Data Library and the University of Minnesota libraries’ Data Management Course for Structural Engineers.\n"}
{"id": "58279466", "url": "https://en.wikipedia.org/wiki?curid=58279466", "title": "David Clark Dobson", "text": "David Clark Dobson\n\nDavid Clark Dobson (born 17 November 1962 in Salt Lake City) is an American mathematician.\n\nDobson received in 1986 his bachelor's degree from Utah State University and his PhD in 1990 from Rice University. He was at the University of Minnesota from 1990 to 1993. He was from 1993 to 1996 an assistant professor, from 1996 to 2000 an assistant Professor and from 1996 to 2000 an associate professor at Texas A&M University. Since 2002 he is a full professor at the University of Utah.\n\nDobson does research on applied mathematics, specifically, \"computational methods and optimal design for plasmonic structures and metamaterials, optimal design of photonic structures and devices, data analysis with physical constraints, and inverse problems in acoustics.\"\n\nIn 2000 he received the first Felix Klein Prize. From 1997 to 1999 he was a Sloan Fellow.\n\n"}
{"id": "32848697", "url": "https://en.wikipedia.org/wiki?curid=32848697", "title": "Discrete q-Hermite polynomials", "text": "Discrete q-Hermite polynomials\n\nIn mathematics, the discrete \"q\"-Hermite polynomials are two closely related families \"h\"(\"x\";\"q\") and \"ĥ\"(\"x\";\"q\") of basic hypergeometric orthogonal polynomials in the basic Askey scheme, introduced by . give a detailed list of their properties.\n\nThe discrete \"q\"-Hermite polynomials are given in terms of basic hypergeometric functions and the Al-Salam–Carlitz polynomials by \nand are related by\n"}
{"id": "35228357", "url": "https://en.wikipedia.org/wiki?curid=35228357", "title": "Eberhard Knobloch", "text": "Eberhard Knobloch\n\nEberhard Knobloch (born 6 November 1943 in Görlitz) is a German historian of science and mathematics.\n\nFrom 1962 to 1967 Knobloch studied classics and mathematics at the University of Berlin and the Technical University of Berlin, after which he passed his state examination as a high school teacher and even as a high school teacher in ancient languages at Goethe began high school in Berlin before 1970 as a research assistant in the history of science back to the TU Berlin was, where he in 1972 with a thesis on Leibniz's combinatorial in Scriba, Christoph received his doctorate.\n\nFrom 1973 he was professor of mathematics at the College of Education in Berlin . In 1976 he qualified as a professor in Berlin and was a visiting scholar at Oxford, London and Edinburgh. Since 1976 he is head of the math sections of the Academy edition of the works of Gottfried Wilhelm Leibniz (and later the technical-scientific parts). In 1981 he became professor of history of science at the Technical University of Berlin (since 2002 academy professor); retiring in 2009. In 1984 he was a visiting professor at the Russian Academy of Sciences in Leningrad. Since 1999 he has been a regular guest professor at Northwestern Polytechnical University in Xian, China. He also was a visiting professor at the Ecole Normale Supérieure in Paris.\n\nBesides the Leibniz Edition, he also oversaw the Tschirnhaus edition of the Saxon Academy of Sciences and worked at Kepler with edition. He is also director of the Alexander von Humboldt Research Centre of the Berlin-Brandenburg Academy of Sciences. He also dealt with Renaissance technology (such as military engineer Mariano Taccola), the notebooks of Leonhard Euler and Jesuit scholars like Christopher Clavius.\nKnobloch assisted the Dieter Lelgemann surveyors to decode and interpret the Ptolemy chart with Susudata. \n\nHe is a member of the International Academy of the History of Science in Paris (corresponding member since 1984, member since 1988, 2001 to 2005 as Vice President and later its president). Since 1996, a member of the Leopoldina, corresponding member of the Saxon Academy of Sciences, Member of Academia Scientiarum et Artium Europaea since 1997 and the Berlin-Brandenburg Academy of Sciences . From 2001 to 2005 he was president of the German National Committee for the History of Science. In 2006 he became president of the European Society for the History of Science.\n\n\n"}
{"id": "33466771", "url": "https://en.wikipedia.org/wiki?curid=33466771", "title": "Effective domain", "text": "Effective domain\n\nIn convex analysis, a branch of mathematics, the effective domain is an extension of the domain of a function.\n\nGiven a vector space \"X\" then a convex function mapping to the extended reals, formula_1, has an \"effective domain\" defined by\nIf the function is concave, then the \"effective domain\" is\n\nThe effective domain is equivalent to the projection of the epigraph of a function formula_1 onto \"X\". That is\n\nNote that if a convex function is mapping to the normal real number line given by formula_6 then the effective domain is the same as the normal definition of the domain.\n\nA function formula_1 is a proper convex function if and only if \"f\" is convex, the effective domain of \"f\" is nonempty and formula_8 for every formula_9.\n"}
{"id": "38231808", "url": "https://en.wikipedia.org/wiki?curid=38231808", "title": "Finite lattice representation problem", "text": "Finite lattice representation problem\n\nIn mathematics, the finite lattice representation problem, or finite congruence lattice problem, asks whether every finite lattice is isomorphic to the congruence lattice of some finite algebra.\n\nA lattice is called algebraic if it is complete and compactly generated. In 1963, Grätzer and Schmidt proved that every algebraic lattice is isomorphic to the congruence lattice of some algebra. Thus there is essentially no restriction on the shape of a congruence lattice of an algebra. The finite lattice representation problem asks whether the same is true for finite lattices and finite algebras. That is, does every finite lattice occur as the congruence lattice of a \"finite\" algebra?\n\nIn 1980, Pálfy and Pudlák proved that this problem is equivalent to the problem of deciding whether every finite lattice occurs as an interval in the subgroup lattice of a finite group. For an overview of the group theoretic approach to the problem, see Pálfy (1993) and Pálfy (2001).\n\nThis problem should not be confused with the congruence lattice problem.\n\nThis is among the oldest unsolved problems in universal algebra. Until it is answered, the theory of finite algebras is incomplete since, given a finite algebra, it is unknown whether there are, \"a priori\", any restrictions on the shape of its congruence lattice.\n\n"}
{"id": "15992030", "url": "https://en.wikipedia.org/wiki?curid=15992030", "title": "Gregory number", "text": "Gregory number\n\nIn mathematics, a Gregory number, named after James Gregory, is a real number of the form:\n\nwhere \"x\" is any rational number greater or equal to 1. Considering the power series expansion for arctangent, we have\n\nSetting \"x\" = 1 gives the well-known Leibniz formula for pi. Thus, in particular,\nis a Gregory number.\n\n"}
{"id": "161888", "url": "https://en.wikipedia.org/wiki?curid=161888", "title": "HOL (proof assistant)", "text": "HOL (proof assistant)\n\nHOL (Higher Order Logic) denotes a family of interactive theorem proving systems using\nsimilar (higher-order) logics and implementation strategies. \nSystems in this family follow the LCF approach as they are implemented as a library in some programming language.\nThis library implements an abstract data type of proven theorems so that new objects of this type can only be created using the functions in the library which correspond to inference rules in higher-order logic. As long as these functions are correctly implemented, all theorems proven in the system must be valid. In this way, a large system can be built on top of a small trusted kernel.\n\nSystems in the HOL family use the ML programming language or its successors. ML was originally developed along with LCF to serve the purpose of a meta-language for theorem proving systems; in fact, the name stands for \"Meta-Language\".\n\nHOL systems use variants of classical Higher-order logic, which has simple axiomatic foundations with few axioms and well-understood semantics.\nThe logic used in HOL provers is closely related to Isabelle/HOL, the most widely used logic of Isabelle (proof assistant).\n\nThere are four HOL systems (sharing essentially the same logic) that are still maintained and developed.\n\n\nAlthough HOL is a predecessor of Isabelle, various HOL derivatives such as HOL4 and HOL Light remain active and in use.\n\nCakeML project developed a formally proven compiler for ML (programming language).\nPreviously, HOL was used to developed a formally proven LISP implementation running on ARM, x86 and PowerPC.\n\nHOL was also used to develop formal semantics for x86 multiprocessors as well as semantics of machine code for POWER and ARM architectures\n\n"}
{"id": "47392500", "url": "https://en.wikipedia.org/wiki?curid=47392500", "title": "Hamiltonian field theory", "text": "Hamiltonian field theory\n\nIn theoretical physics, Hamiltonian field theory is the field-theoretic analogue to classical Hamiltonian mechanics. It is a formalism in classical field theory alongside Lagrangian field theory, and has applications in quantum field theory also.\n\nThe Hamiltonian for a system of discrete particles is a function of their generalized coordinates and conjugate momenta, and possibly, time. For continua and fields, Hamiltonian mechanics is unsuitable but can be extended by considering a large number of point masses, and taking the continuous limit, that is, infinitely many particles forming a continuum or field. Since each point mass has one or more degrees of freedom, the field formulation has infinitely many degrees of freedom.\n\nThe Hamiltonian density is the continuous analogue for fields; it is a function of the fields, the conjugate \"momentum\" fields, and possibly the space and time coordinates themselves. For one scalar field , the Hamiltonian density is defined from the Lagrangian density by\n\nwith the \"del\" or \"nabla\" operator, is the position vector of some point in space, and is time. The Lagrangian density is a function of the fields in the system, their space and time derivatives, and possibly the space and time coordinates themselves. It is the field analogue to the Lagrangian function for a system of discrete particles described by generalized coordinates.\n\nAs in Hamiltonian mechanics where every generalized coordinate has a corresponding generalized momentum, the field has a conjugate momentum field , defined as the partial derivative of the Lagrangian density with respect to the time derivative of the field,\n\nin which the overdot denotes a \"partial\" time derivative , not a total time derivative .\n\nFor many fields and their conjugates the Hamiltonian density is a function of them all:\n\nwhere each conjugate field is defined with respect to its field,\n\nIn general, for any number of fields, the volume integral of the Hamiltonian density gives the Hamiltonian, in three spatial dimensions:\n\nThe Hamiltonian density is the Hamiltonian per unit spatial volume. The corresponding dimension is [energy][length], in SI units Joules per metre cubed, J m.\n\nThe above equations and definitions can be extended to vector fields and more generally tensor fields and spinor fields. In physics, tensor fields describe bosons and spinor fields describe fermions.\n\nThe equations of motion for the fields are similar to the Hamiltonian equations for discrete particles. For any number of fields:\n\nwhere again the overdots are partial time derivatives, the variational derivative with respect to the fields\n\nwith · the dot product, must be used instead of simply partial derivatives. In tensor index notation (including the summation convention) this is\n\nwhere is the four gradient.\n\nThe fields and conjugates form an infinite dimensional phase space, because fields have an infinite number of degrees of freedom.\n\nFor two functions which depend on the fields and , their spatial derivatives, and the space and time coordinates,\n\nand the fields are zero on the boundary of the volume the integrals are taken over, the field theoretic Poisson bracket is defined as (do not be confused with the commutator from quantum mechanics).\n\nwhere formula_11 is the variational derivative\n\nUnder the same conditions of vanishing fields on the surface, the following result holds for the time evolution of (similarly for ):\n\nwhich can be found from the total time derivative of , integration by parts, and using the above Poisson bracket.\n\nThe following results are true if the Lagrangian and Hamiltonian densities are explicitly time-independent (they can still have implicit time-dependence via the fields and their derivatives),\n\nThe Hamiltonian density is the total energy density, the sum of the kinetic energy density (calligraphic T) and the potential energy density (calligraphic V),\n\nTaking the partial time derivative of the definition of the Hamiltonian density above, and using the chain rule for implicit differentiation and the definition of the conjugate momentum field, gives the continuity equation:\n\nin which the Hamiltonian density can be interpreted as the energy density, and\n\nthe energy flux, or flow of energy per unit time per unit surface area.\n\nCovariant Hamiltonian field theory is the relativistic formulation of Hamiltonian field theory.\n\nHamiltonian field theory usually means the symplectic Hamiltonian formalism when applied to classical field theory, that takes the form of the instantaneous Hamiltonian formalism on an infinite-dimensional phase space, and where canonical coordinates are field functions at some instant of time. This Hamiltonian formalism is applied to quantization of fields, e.g., in quantum gauge theory. In Covariant Hamiltonian field theory, canonical momenta \"p\" corresponds to derivatives of fields with respect to all world coordinates \"x\". Covariant Hamilton equations are equivalent to the Euler-Lagrange equations in the case of hyperregular Lagrangians. Covariant Hamiltonian field theory is developed in the Hamilton–De Donder, polysymplectic, multisymplectic and \"k\"-symplectic variants. A phase space of covariant Hamiltonian field theory is a finite-dimensional polysymplectic or multisymplectic manifold.\n\nHamiltonian non-autonomous mechanics is formulated as covariant Hamiltonian field theory on fiber bundles over the time axis, i.e. the real line ℝ.\n\n\n"}
{"id": "7974982", "url": "https://en.wikipedia.org/wiki?curid=7974982", "title": "Inversion transformation", "text": "Inversion transformation\n\nIn mathematical physics, inversion transformations are a natural extension of Poincaré transformations to include all conformal one-to-one transformations on coordinate space-time. They are less studied in physics because unlike the rotations and translations of Poincaré symmetry an object cannot be physically transformed by the inversion symmetry. Some physical theories are invariant under this symmetry, in these cases it is what is known as a 'hidden symmetry'. Other hidden symmetries of physics include gauge symmetry and general covariance.\n\nIn 1831 the mathematician Ludwig Immanuel Magnus began to publish on transformations of the plane generated by inversion in a circle of radius \"R\". His work initiated a large body of publications, now called inversive geometry. The most prominently named mathematician became August Ferdinand Möbius once he reduced the planar transformations to complex number arithmetic. In the company of physicists employing the inversion transformation early on was Lord Kelvin, and the association with him leads it to be called the Kelvin transform.\n\nIn the following we shall use imaginary time (formula_1) so that space-time is Euclidean and the equations are simpler. The Poincaré transformations are given by the coordinate transformation on space-time parametrized by the 4-vectors \"V\"\n\nwhere formula_3 is an orthogonal matrix and formula_4 is a 4-vector. Applying this transformation twice on a 4-vector gives a third transformation of the same form. The basic invariant under this transformation is the space-time length given by the distance between two space-time points given by 4-vectors \"x\" and \"y\":\n\nThese transformations are subgroups of general 1-1 conformal transformations on space-time. It is possible to extend these transformations to include all 1-1 conformal transformations on space-time\n\nWe must also have an equivalent condition to the orthogonality condition of the Poincaré transformations:\n\nBecause one can divide the top and bottom of the transformation by formula_8 we lose no generality by setting formula_9 to the unit matrix. We end up with\n\nApplying this transformation twice on a 4-vector gives a transformation of the same form. The new symmetry of 'inversion' is given by the 3-tensor formula_11 This symmetry becomes Poincaré symmetry if we set formula_12 When formula_13 the second condition requires that formula_3 is an orthogonal matrix. This transformation is 1-1 meaning that each point is mapped to a unique point only if we theoretically include the points at infinity.\n\nThe invariants for this symmetry in 4 dimensions is unknown however it is known that the invariant requires a minimum of 4 space-time points. In one dimension, the invariant is the well known cross-ratio from Möbius transformations:\n\nBecause the only invariants under this symmetry involve a minimum of 4 points, this symmetry cannot be a symmetry of point particle theory. Point particle theory relies on knowing the lengths of paths of particles through space-time (e.g., from formula_16 to formula_17). The symmetry can be a symmetry of a string theory in which the strings are uniquely determined by their endpoints. The propagator for this theory for a string starting at the endpoints formula_18 and ending at the endpoints formula_19 is a conformal function of the 4-dimensional invariant. A string field in endpoint-string theory is a function over the endpoints.\n\nAlthough it is natural to generalize the Poincaré transformations in order to find hidden symmetries in physics and thus narrow down the number of possible theories of high-energy physics, it is difficult to experimentally examine this symmetry as it is not possible to transform an object under this symmetry. The indirect evidence of this symmetry is given by how accurately fundamental theories of physics that are invariant under this symmetry make predictions. Other indirect evidence is whether theories that are invariant under this symmetry lead to contradictions such as giving probabilities greater than 1. So far there has been no direct evidence that the fundamental constituents of the Universe are strings. The symmetry could also be a broken symmetry meaning that although it is a symmetry of physics, the Universe has 'frozen out' in one particular direction so this symmetry is no longer evident.\n\n"}
{"id": "14855952", "url": "https://en.wikipedia.org/wiki?curid=14855952", "title": "Johnson–Lindenstrauss lemma", "text": "Johnson–Lindenstrauss lemma\n\nIn mathematics, the Johnson–Lindenstrauss lemma is a result named after William B. Johnson and Joram Lindenstrauss concerning low-distortion embeddings of points from high-dimensional into low-dimensional Euclidean space. The lemma states that a set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved. The map used for the embedding is at least Lipschitz, and can even be taken to be an orthogonal projection.\n\nThe lemma has uses in compressed sensing, manifold learning, dimensionality reduction, and graph embedding. Much of the data stored and manipulated on computers, including text and images, can be represented as points in a high-dimensional space (see vector space model for the case of text). However, the essential algorithms for working with such data tend to become bogged down very quickly as dimension increases. It is therefore desirable to reduce the dimensionality of the data in a way that preserves its relevant structure. The Johnson–Lindenstrauss lemma is a classic result in this vein.\n\nAlso, the lemma is tight up to a constant factor, i.e. there exists a set of points of size \"m\" that needs dimension\n\nin order to preserve the distances between all pair of points.\n\nGiven formula_2, a set formula_3 of formula_4 points in formula_5, and a number formula_6, there is a linear map formula_7 such that\n\nfor all formula_9. \n\nThe formula can be rearranged:formula_10\n\nOne proof of the lemma takes \"ƒ\" to be a suitable multiple of the orthogonal projection onto a random subspace of dimension formula_11 in formula_5, and exploits the phenomenon of concentration of measure.\n\nObviously an orthogonal projection will, in general, reduce the average distance between points, but the lemma can be viewed as dealing with \"relative distances\", which do not change under scaling. In a nutshell, you roll the dice and obtain a random projection, which will reduce the average distance, and then you scale up the distances so that the average distance returns to its previous value. If you keep rolling the dice, you will, in polynomial random time, find a projection for which the (scaled) distances satisfy the lemma.\n\nA related lemma is the distributional JL lemma. This lemma states that for any 0 < \"ε\", \"δ\" < 1/2 and positive integer \"d\", there exists a distribution over R from which the matrix \"A\" is drawn such that for \"k\" = \"O\"(\"ε\"log(1/\"δ\")) and for any unit-length vector \"x\" ∈ R, the claim below holds.\n\nOne can obtain the JL lemma from the distributional version by setting formula_14 and formula_15 for some pair \"u\",\"v\" both in \"X\". Then the JL lemma follows by a union bound over all such pairs.\n\nGiven \"A\", computing the matrix vector product takes \"O\"(\"kd\") time. There has been some work in deriving distributions for which the matrix vector product can be computed in less than \"O\"(\"kd\") time. There are two major lines of work. The first, \"Fast Johnson Lindenstrauss Transform\" (FJLT), was introduced by Ailon and Chazelle in 2006. Another approach is to build a distribution supported over matrices that are sparse.\n\n"}
{"id": "35227606", "url": "https://en.wikipedia.org/wiki?curid=35227606", "title": "Josef Anton Gmeiner", "text": "Josef Anton Gmeiner\n\nJosef Anton Gmeiner (1862-1926) was an Austrian mathematician working in number theory and analysis.\n"}
{"id": "19434991", "url": "https://en.wikipedia.org/wiki?curid=19434991", "title": "KANT (software)", "text": "KANT (software)\n\nKANT is a computer algebra system for mathematicians interested in algebraic number theory, performing sophisticated computations in algebraic number fields, in global function fields, and in local fields. KASH is the associated command line interface. They have been developed by the Algebra and Number Theory research group of the Institute of Mathematics at Technische Universität Berlin under the project leadership of Prof. Dr Michael Pohst. Kant is free for non-commercial use.\n\n\n\n"}
{"id": "11491940", "url": "https://en.wikipedia.org/wiki?curid=11491940", "title": "Karger's algorithm", "text": "Karger's algorithm\n\nIn computer science and graph theory, Karger's algorithm is a randomized algorithm to compute a minimum cut of a connected graph. It was invented by David Karger and first published in 1993.\n\nThe idea of the algorithm is based on the concept of contraction of an edge formula_1 in an undirected graph formula_2. Informally speaking, the contraction of an edge merges the nodes formula_3 and formula_4 into one, reducing the total number of nodes of the graph by one. All other edges connecting either formula_3 or formula_4 are \"reattached\" to the merged node, effectively producing a multigraph. Karger's basic algorithm iteratively contracts randomly chosen edges until only two nodes remain; those nodes represent a cut in the original graph. By iterating this basic algorithm a sufficient number of times, a minimum cut can be found with high probability.\n\nA \"cut\" formula_7 in an undirected graph formula_2 is a partition of the vertices formula_9 into two non-empty, disjoint sets formula_10. \nThe \"cutset\" of a cut consists of the edges formula_11 between the two parts.\nThe \"size\" (or \"weight\") of a cut in an unweighted graph is the cardinality of the cutset, i.e., the number of edges between the two parts, \nThere are formula_13 ways of choosing for each vertex whether it belongs to formula_14 or to formula_15, but two of these choices make formula_14 or formula_15 empty and do not give rise to cuts. Among the remaining choices, swapping the roles of formula_14 and formula_15 does not change the cut, so each cut is counted twice; therefore, there are formula_20\nThe probability formula_21 that the contraction algorithm on an formula_22-vertex graph avoids formula_23 satisfies the recurrence formula_24, with formula_25, which can be expanded as\n\nBy repeating the contraction algorithm formula_27 times with independent random choices and returning the smallest cut, the probability of not finding a minimum cut is\n\nThe total running time for formula_15 repetitions for a graph with formula_22 vertices and formula_31 edges is formula_32.\n\nAn extension of Karger’s algorithm due to David Karger and Clifford Stein achieves an order of magnitude improvement.\n\nThe basic idea is to perform the contraction procedure until the graph reaches formula_33 vertices.\n\nThe probability formula_40 that this contraction procedure avoids a specific cut formula_23 in an formula_22-vertex graph is\nformula_43\n\nThis expression is approximately formula_44 and becomes less than formula_45 around formula_46. \nIn particular, the probability that an edge from formula_23 is contracted grows towards the end. This motivates the idea of switching to a slower algorithm after a certain number of contraction steps.\n\nThe probability formula_60 the algorithm finds a specific cutset formula_23 is given by the recurrence relation\nwith solution formula_63. The running time of fastmincut satisfies\nwith solution formula_65. \nTo achieve error probability formula_66, the algorithm can be repeated formula_67 times, for an overall running time of formula_68. This is an order of magnitude improvement over Karger’s original algorithm.\n\nTheorem: With high probability we can find all min cuts in the running time of formula_69.\n\nProof: We know that formula_70, therefore after running this algorithm formula_71 times the probability of missing a specific min-cut is \nAnd there are at most formula_73 min-cuts, hence the probability of missing any min-cut is\nThe probability of failures is considerably small when \"n\" is large enough.∎\n\nTo determine a min-cut, one has to touch every edge in the graph at least once, which is formula_75 time in a dense graph. The Karger–Stein's min-cut algorithm takes the running time of formula_76, which is very close to that.\n"}
{"id": "3423431", "url": "https://en.wikipedia.org/wiki?curid=3423431", "title": "LOCC", "text": "LOCC\n\nLOCC, or local operations and classical communication, is a method in quantum information theory where a local (product) operation is performed on part of the system, and where the result of that operation is \"communicated\" classically to another part where usually another local operation is performed conditioned on the information received. \n\nThe formal definition of the set of LOCC operations is complicated due to the fact that later local operations depend in general on all the previous classical communication and due to the unbounded number of communication rounds. For any finite number formula_1 one can define formula_2, the set of LOCC operations that can be achieved with formula_3 rounds of classical communication. The set becomes strictly larger whenever formula_3 is increased and care has to be taken to define the limit of infinitely many rounds. In particular, the set LOCC is not topologically closed, that is there are quantum operations that can be approximated arbitrarily closely by LOCC but that are not themselves LOCC.\n\nA \"one-round LOCC\" formula_5 is a quantum instrument formula_6, for which the trace-non-increasing completely positive maps (CPMs) formula_7 are local for all measurement results formula_8, i.e., formula_9 and there is one site formula_10 such that only at formula_11 the map formula_12 formula_13 is not trace-preserving. This means that the instrument can be realized by the party at site formula_11 applying the (local) instrument formula_15 and communicating the classical result formula_8 to all other parties, which then each perform (conditioned on formula_8) trace-preserving (deterministic) local quantum operations formula_18.\n\nThen formula_2 are defined recursively as those operations that can be realized by following up a operation formula_20 with a formula_5-operation. here it is allowed that the party, which performs the follow-up operations depends on the result of the previous rounds. Moreover, we allow also \"coarse-graining\", i.e., discarding some of the classical information encoded in the measurement results (of all rounds).\n\nThe union of all formula_2 operations is denoted by formula_23 and contains instruments that can be approximated better and better with more LOCC rounds. It topological closure formula_24 contains \"all\" such operations.\n\nIt can be shown that all these sets are different:\n\nThe set of all LOCC operations is contained in the set formula_26 of all \"separable operations\". formula_26 contains all operations that can be written using Kraus operators that have all product form, i.e., \nwith formula_29. Not all operations in formula_26 are LOCC, \ni.e., there are examples that cannot be implemented locally even with infinite rounds of communication. \n\nLOCC are the \"free operations\" in the resource theories of entanglement: Entanglement cannot be produced from separable states with LOCC and if the local parties in addition to being able to perform all LOCC operations are also furnished with some entangled states, they can realize more operations than with LOCC alone.\n\nLOCC operations are useful for \"state preparation\", \"state discrimination\", and \"entanglement transformations\".\n\nAlice and Bob are given a two quantum systems in the product state formula_32. There task is to produce the separable state formula_33. With local operations alone this cannot be achieved, since they do not allow to produce the (classical) correlations present in formula_34. But with LOCC (with one round of communication) formula_34 can be prepared: Alice throws an unbiased coin (that shows heads or tails each with 50% probability) and flips her qubit (to formula_36) if the coin shows \"tails\", otherwise it is left unchanged. She then sends the result of the coin-flip (classical information) to Bob who also flips his qubit if he receives the message \"tails\". The resulting state is formula_34. In general, \"all\" separable states (and only these) can be prepared from a product states with LOCC operations alone.\n\nGiven two quantum states formula_38 on a bi- or multipartite Hilbert space formula_39, the task is to determine which one of two (or more) possible states formula_40 it is. As a simple example, consider the two Bell states \n\nLet's say the two-qubit system is separated, where the first qubit is given to Alice and the second is given to Bob. Without communication, Alice and Bob cannot distinguish the two states, since for all local measurements all measurement statistics are exactly the same (both states have the same reduced density matrix). E.g., assume that Alice measures the first qubit, and obtains the result 0. Since this result is equally likely to occur (with probability 50%) in each of the two cases, she does not gain any information on which Bell pair she was given and the same holds for Bob if he performs any measurement. But now let Alice send her result to Bob over a classical channel. Now Bob can compare his result to hers and if they are the same he can conclude that the pair given was formula_43, since only this allows for a joint measurement outcome formula_44. Thus with LOCC and two measurements these two states can be distinguished perfectly. Note that with global (nonlocal or entangled) measurements, a single measurement (on the joint Hilbert space) is sufficient to distinguish these two (mutually orthogonal) states. \n\nThere are quantum states that cannot be distinguished with LOCC operations.\n\nWhile LOCC cannot \"generate\" entangled states out of product states, they can be used to transform entangled states into other entangled states. The restriction to LOCC severely limits which transformations are possible.\n\nNielsen has derived a general condition to determine whether one pure state of a bipartite quantum system may be transformed into another using only LOCC. Full details may be found in the paper referenced earlier, the results are sketched out here.\n\nConsider two particles in a Hilbert space of dimension formula_45 with particle states formula_46 and formula_47 with Schmidt decompositions\n\nThe formula_50's are known as Schmidt coefficients. If they are ordered largest to smallest (i.e., with formula_51) then formula_46 can only be transformed into formula_47 using only local operations if and only if for all formula_54 in the range formula_55\n\nIn more concise notation:\n\nThis is a more restrictive condition than that local operations cannot increase entanglement measures. It is quite possible that formula_46 and formula_47 have the same amount of entanglement but converting one into the other is not possible and even that conversion in either direction is impossible because neither set of Schmidt coefficients majorises the other. For large formula_45 if all Schmidt coefficients are non-zero then the probability of one set of coefficients majorising the other becomes negligible. Therefore, for large formula_45 the probability of any arbitrary state being convertible into another via LOCC becomes negligible.\n\nThe operations described so far are deterministic, i.e., they succeed with probability 100%. If one is satisfied by \"probabilistic\" transformations, many more transformations are possible using LOCC. These operations are called \"stochastic LOCC\" (SLOCC). In particular for multi-partite states the convertibility under SLOCC is studied to gain a qualitative insight into the entanglement properties of the involved states.\n\nIf entangled states are available as a resource, these together with LOCC allow a much larger class of transformations. This is the case even if these resource states are not consumed in the process (as they are, for example, in quantum teleportation). Thus transformations are called \"entanglement catalysis\". In this procedure, the conversion of an initial state to a final state that is impossible with LOCC is made possible by taking a tensor product of the initial state with a \"catalyst state\" formula_62 and requiring that this state is still available at the end of the conversion process. I.e., the catalyst state is left unchanged by the conversion and can then be removed, leaving only the desired final state. Consider the states,\nThese states are written in the form of Schmidt decomposition and in a descending order. We compare the sum of the coefficients of formula_46 and formula_47\nIn the table, red color is put if formula_68, green color is put if formula_69, and white color is remained if formula_70. After building up the table, one can easily to find out whether formula_46 and formula_47 are convertible by looking at the color in the formula_54 direction. formula_46 can be converted into formula_47 by LOCC if the color are all green or white, and formula_47 can be converted into formula_46 by LOCC if the color are all red or white. When the table presents both red and green color, the states are not convertible.\n\nNow we consider the product states formula_78 and formula_79：\nSimilarly, we make up the table:\nThe color in the formula_54 direction are all green or white, therefore, according to the Nielsen's theorem, formula_78 is possible to be converted into formula_79 by the LOCC. The \"catalyst\" state formula_62 is taken away after the conversion. Finally we find formula_86 by the LOCC.\n\n"}
{"id": "31627584", "url": "https://en.wikipedia.org/wiki?curid=31627584", "title": "Ladislav Rieger", "text": "Ladislav Rieger\n\nLadislav Svante Rieger (1916–1963) was a Czech mathematician who worked in the areas of algebra, mathematical logic, and axiomatic set theory. He is considered to be the founder of mathematical logic in the Czech Republic and Slovakia, having begun his work around 1957.\n\n"}
{"id": "2403085", "url": "https://en.wikipedia.org/wiki?curid=2403085", "title": "Lanczos tensor", "text": "Lanczos tensor\n\nThe Lanczos tensor or Lanczos potential is a rank 3 tensor in general relativity that generates the Weyl tensor. It was first introduced by Cornelius Lanczos in 1949. The theoretical importance of the Lanczos tensor is that it serves as the gauge field for the gravitational field in the same way that, by analogy, the electromagnetic four-potential generates the electromagnetic field.\n\nThe Lanczos tensor can be defined in a few different ways. The most common modern definition is through the Weyl–Lanczos equations, which demonstrate the generation of the Weyl tensor from the Lanczos tensor. These equations, presented below, were given by Takeno in 1964. The way that Lanczos introduced the tensor originally was as a Lagrange multiplier on constraint terms studied in the variational approach to general relativity. Under any definition, the Lanczos tensor \"H\" exhibits the following symmetries:\n\nThe Lanczos tensor always exists in four dimensions but does not generalize to higher dimensions. This highlights the specialness of four dimensions. Note further that the full Riemann tensor cannot in general be derived from derivatives of the Lanczos potential alone. The Einstein field equations must provide the Ricci tensor to complete the components of the Ricci decomposition.\n\nThe Weyl–Lanczos equations express the Weyl tensor entirely as derivatives of the Lanczos tensor:\n\nwhere formula_4 is the Weyl tensor, the semicolon denotes the covariant derivative, and the subscripted parentheses indicate symmetrization. Although the above equations can be used to define the Lanczos tensor, they also show that it is not unique but rather has gauge freedom under an affine group. If formula_5 is an arbitrary vector field, then the Weyl–Lanczos equations are invariant under the gauge transformation\n\nwhere the subscripted brackets indicate antisymmetrization. An often convenient choice is the Lanczos algebraic gauge, formula_7 which sets formula_8 The gauge can be further restricted through the Lanczos differential gauge formula_9. These gauge choices reduce the Weyl–Lanczos equations to the simpler form\n\nThe Lanczos potential tensor satisfies a wave equation\n\nwhere formula_12 is the d'Alembert operator and\nis known as the Cotton tensor. Since the Cotton tensor depends only on covariant derivatives of the Ricci tensor, it can perhaps be interpreted as a kind of matter current. The additional self-coupling terms have no direct electromagnetic equivalent. These self-coupling terms, however, do not affect the vacuum solutions, where the Ricci tensor vanishes and the curvature is described entirely by the Weyl tensor. Thus in vacuum, the Einstein field equations are equivalent to the homogeneous wave equation formula_14 in perfect analogy to the vacuum wave equation formula_15 of the electromagnetic four-potential. This shows a formal similarity between gravitational waves and electromagnetic waves, with the Lanczos tensor well-suited for studying gravitational waves.\n\nIn the weak field approximation where formula_16, a convenient form for the Lanczos tensor in the Lanczos gauge is\n\nThe most basic nontrivial case for expressing the Lanczos tensor is, of course, for the Schwarzschild metric. The simplest, explicit component representation in natural units for the Lanczos tensor in this case is\nwith all other components vanishing up to symmetries. This form, however, is not in the Lanczos gauge. The nonvanishing terms of the Lanczos tensor in the Lanczos gauge are\n\nIt is further possible to show, even in this simple case, that the Lanczos tensor cannot in general be reduced to a linear combination of the spin coefficients of the Newman–Penrose formalism, which attests to the Lanczos tensor's fundamental nature. Similar calculations have been used to construct arbitrary Petrov type D solutions.\n\n\n"}
{"id": "719460", "url": "https://en.wikipedia.org/wiki?curid=719460", "title": "Laplace–Runge–Lenz vector", "text": "Laplace–Runge–Lenz vector\n\nIn classical mechanics, the Laplace–Runge–Lenz (LRL) vector is a vector used chiefly to describe the shape and orientation of the orbit of one astronomical body around another, such as a planet revolving around a star. For two bodies interacting by Newtonian gravity, the LRL vector is a constant of motion, meaning that it is the same no matter where it is calculated on the orbit; equivalently, the LRL vector is said to be \"conserved\". More generally, the LRL vector is conserved in all problems in which two bodies interact by a central force that varies as the inverse square of the distance between them; such problems are called Kepler problems.\n\nThe hydrogen atom is a Kepler problem, since it comprises two charged particles interacting by Coulomb's law of electrostatics, another inverse square central force. The LRL vector was essential in the first quantum mechanical derivation of the spectrum of the hydrogen atom, before the development of the Schrödinger equation. However, this approach is rarely used today.\n\nIn classical and quantum mechanics, conserved quantities generally correspond to a symmetry of the system. The conservation of the LRL vector corresponds to an unusual symmetry; the Kepler problem is mathematically equivalent to a particle moving freely on the surface of a four-dimensional (hyper-)sphere, so that the whole problem is symmetric under certain rotations of the four-dimensional space. This higher symmetry results from two properties of the Kepler problem: the velocity vector always moves in a perfect circle and, for a given total energy, all such velocity circles intersect each other in the same two points.\n\nThe Laplace–Runge–Lenz vector is named after Pierre-Simon de Laplace, Carl Runge and Wilhelm Lenz. It is also known as the Laplace vector, the Runge–Lenz vector and the Lenz vector. Ironically, none of those scientists discovered it. The LRL vector has been re-discovered several times and is also equivalent to the dimensionless eccentricity vector of celestial mechanics. Various generalizations of the LRL vector have been defined, which incorporate the effects of special relativity, electromagnetic fields and even different types of central forces.\n\nA single particle moving under any conservative central force has at least four constants of motion, the total energy \"E\" and the three Cartesian components of the angular momentum vector L with respect to the origin. The particle's orbit is confined to a plane defined by the particle's initial momentum p (or, equivalently, its velocity v) and the vector r between the particle and the center of force (see Figure 1, below).\n\nAs defined below (see Mathematical definition), the Laplace–Runge–Lenz vector (LRL vector) A always lies in the plane of motion for any central force. However, A is constant only for an inverse-square central force. For most central forces, however, this vector A is not constant, but changes in both length and direction; if the central force is \"approximately\" an inverse-square law, the vector A is approximately constant in length, but slowly rotates its direction. A \"generalized\" conserved LRL vector formula_1 can be defined for all central forces, but this generalized vector is a complicated function of position, and usually not expressible in closed form.\n\nThe plane of motion is perpendicular to the angular momentum vector L, which is constant; this may be expressed mathematically by the vector dot product equation ; likewise, since A lies in that plane, .\n\nThe LRL vector differs from other conserved quantities in the following property. Whereas for typical conserved quantities, there is a corresponding cyclic coordinate in the three-dimensional Lagrangian of the system, there does \"not\" exist such a coordinate for the LRL vector. Thus, the conservation of the LRL vector must be derived directly, e.g., by the method of Poisson brackets, as described below. Conserved quantities of this kind are called \"dynamic\", in contrast to the usual \"geometric\" conservation laws, e.g., that of the angular momentum.\n\nThe LRL vector A is a constant of motion of the important Kepler problem, and is useful in describing astronomical orbits, such as the motion of the planets. Nevertheless, it has never been well-known among physicists, possibly because it is less intuitive than momentum and angular momentum. Consequently, it has been rediscovered independently several times over the last three centuries.\n\nJakob Hermann was the first to show that A is conserved for a special case of the inverse-square central force, and worked out its connection to the eccentricity of the orbital ellipse. Hermann's work was generalized to its modern form by Johann Bernoulli in 1710. At the end of the century, Pierre-Simon de Laplace rediscovered the conservation of A, deriving it analytically, rather than geometrically. In the middle of the nineteenth century, William Rowan Hamilton derived the equivalent eccentricity vector defined below, using it to show that the momentum vector p moves on a circle for motion under an inverse-square central force (Figure 3).\n\nAt the beginning of the twentieth century, Josiah Willard Gibbs derived the same vector by vector analysis. Gibbs' derivation was used as an example by Carle Runge in a popular German textbook on vectors, which was referenced by Wilhelm Lenz in his paper on the (old) quantum mechanical treatment of the hydrogen atom. In 1926, the vector was used by Wolfgang Pauli to derive the spectrum of hydrogen using modern quantum mechanics, but not the Schrödinger equation; after Pauli's publication, it became known mainly as the \"Runge–Lenz vector\".\n\nFor a single particle acted on by an inverse-square central force described by the equation \nthe LRL vector A is defined mathematically by the formula\n</math>\nwhere\n\n\nSince the assumed force is conservative, the total energy is a constant of motion,\n\nFurthermore, the assumed force is a central force, and thus the angular momentum vector L is also conserved and defines the plane in which the particle travels. The LRL vector A is perpendicular to the angular momentum vector L because both and r are perpendicular to L. It follows that A lies in the plane of the orbit.\n\nThis definition of the LRL vector A pertains to a single point particle of mass moving under the action of a fixed force. However, the same definition may be extended to two-body problems such as Kepler's problem, by taking as the reduced mass of the two bodies and r as the vector between the two bodies.\n\nA variety of alternative formulations for the same constant of motion may also be used. The most common is to scale by to define the eccentricity vector\n\nThe \"shape\" and \"orientation\" of the Kepler problem orbits can be determined from the LRL vector as follows. Taking the dot product of A with the position vector r gives the equation\n\nwhere \"θ\" is the angle between r and A (Figure 2). Permuting the scalar triple product\n\nand rearranging yields the defining formula for a conic section, provided that \"A\" is a constant, which is the case for the inverse square force law,\n\nof eccentricity \"e\", \nand latus rectum\n\nThe major semiaxis of the conic section may be defined using the latus rectum and the eccentricity\nwhere the minus sign pertains to ellipses and the plus sign to hyperbolae.\n\nTaking the dot product of A with itself yields an equation involving the energy ,\nwhich may be rewritten in terms of the eccentricity,\n\nThus, if the energy \"E\" is negative (bound orbits), the eccentricity is less than one and the orbit is an ellipse. Conversely, if the energy is positive (unbound orbits, also called \"scattered orbits\"), the eccentricity is greater than one and the orbit is a hyperbola. Finally, if the energy is exactly zero, the eccentricity is one and the orbit is a parabola. In all cases, the direction of A lies along the symmetry axis of the conic section and points from the center of force toward the periapsis, the point of closest approach.\n\nThe conservation of the LRL vector A and angular momentum vector L is useful in showing that the momentum vector p moves on a circle under an inverse-square central force.\n\nTaking the dot product of\nwith itself yields\n\nFurther choosing L along the -axis, and the major semiaxis as the -axis, yields the locus equation for p,\n\nIn other words, the momentum vector p is confined to a circle of radius centered on . The eccentricity corresponds to the cosine of the angle shown in Figure 3.\n\nIn the degenerate limit of circular orbits, and thus vanishing A, the circle centers at the origin (0,0).\nFor brevity, it is also useful to introduce the variable formula_16.\n\nThis circular hodograph is useful in illustrating the symmetry of the Kepler problem.\n\nThe seven scalar quantities \"E\", A and L (being vectors, the latter two contribute three conserved quantities each) are related by two equations, and , giving five independent constants of motion. (Since the magnitude of A, hence the eccentricity \"e\" of the orbit, can be determined from the total angular momentum \"L\" and the energy \"E\", only the \"direction\" of A is conserved independently; moreover, since A must be perpendicular to L, it contributes \"only one\" additional conserved quantity.)\n\nThis is consistent with the six initial conditions (the particle's initial position and velocity vectors, each with three components) that specify the orbit of the particle, since the initial time is not determined by a constant of motion. The resulting 1-dimensional orbit in 6-dimensional phase space is thus completely specified.\n\nA mechanical system with \"d\" degrees of freedom can have at most constants of motion, since there are 2\"d\" initial conditions and the initial time cannot be determined by a constant of motion. A system with more than \"d\" constants of motion is called \"superintegrable\" and a system with constants is called maximally superintegrable. Since the solution of the Hamilton–Jacobi equation in one coordinate system can yield only \"d\" constants of motion, superintegrable systems must be separable in more than one coordinate system. The Kepler problem is maximally superintegrable, since it has three degrees of freedom () and five independent constant of motion; its Hamilton–Jacobi equation is separable in both spherical coordinates and parabolic coordinates, as described below.\n\nMaximally superintegrable systems follow closed, one-dimensional orbits in phase space, since the orbit is the intersection of the phase-space isosurfaces of their constants of motion. Consequently, the orbits are perpendicular to all gradients of all these\nindependent isosurfaces, five in this specific problem, and hence are determined by the generalized cross products of all of these gradients. As a result, all superintegrable systems are automatically describable by Nambu mechanics, alternatively, and equivalently, to Hamiltonian mechanics.\n\nMaximally superintegrable systems can be quantized using commutation relations, as illustrated below. Nevertheless, equivalently, they are also quantized in the Nambu framework,\nsuch as this classical Kepler problem into the quantum hydrogen atom.\n\nThe Laplace–Runge–Lenz vector A is conserved only for a perfect inverse-square central force. In most practical problems such as planetary motion, however, the interaction potential energy between two bodies is not exactly an inverse square law, but may include an additional central force, a so-called \"perturbation\" described by a potential energy . In such cases, the LRL vector rotates slowly in the plane of the orbit, corresponding to a slow apsidal precession of the orbit.\n\nBy assumption, the perturbing potential is a conservative central force, which implies that the total energy and angular momentum vector L are conserved. Thus, the motion still lies in a plane perpendicular to L and the magnitude is conserved, from the equation . The perturbation potential may be any sort of function, but should be significantly weaker than the main inverse-square force between the two bodies.\n\nThe \"rate\" at which the LRL vector rotates provides information about the perturbing potential . Using canonical perturbation theory and action-angle coordinates, it is straightforward to show that A rotates at a rate of,\nwhere is the orbital period, and the identity was used to convert the time integral into an angular integral (Figure 5). The expression in angular brackets, , represents the perturbing potential, but \"averaged\" over one full period; that is, averaged over one full passage of the body around its orbit. Mathematically, this time average corresponds to the following quantity in curly braces. This averaging helps to suppress fluctuations in the rate of rotation.\n\nThis approach was used to help verify Einstein's theory of general relativity, which adds a small effective inverse-cubic perturbation to the normal Newtonian gravitational potential,\n\nInserting this function into the integral and using the equation\nto express in terms of , the precession rate of the periapsis caused by this non-Newtonian perturbation is calculated to be\nwhich closely matches the observed anomalous precession of Mercury and binary pulsars. This agreement with experiment is strong evidence for general relativity.\n\nThe algebraic structure of the problem is, as explained in later sections, SO(4)/ℤ ~ SO(3) × SO(3).\nThe three components \"L\" of the angular momentum vector L have the Poisson brackets\n\nwhere =1,2,3 and is the fully antisymmetric tensor, i.e., the Levi-Civita symbol; the summation index is used here to avoid confusion with the force parameter defined above. Then since the LRL vector A transforms like a vector, we have the following Poisson bracket relations between A and L:\nFinally, the Poisson bracket relations between the different components of A are as follows:\nwhere formula_24 is the Hamiltonian. Note that the span of the components of A and the components of L is not closed under Poisson brackets, because of the factor of formula_24 on the right-hand side of this last relation.\n\nFinally, since both L and A are constants of motion, we have\n\nThe Poisson brackets will be extended to quantum mechanical commutation relations in the next section and Lie brackets in a following section.\n\nAs noted below, a scaled Laplace–Runge–Lenz vector D may be defined with the same units as angular momentum by dividing A by formula_27. Since D still transforms like a vector, the Poisson brackets of D with the angular momentum vector L can then be written in a similar form\n\nThe Poisson brackets of D with \"itself\" depend on the sign of \"H\", i.e., on whether the energy is negative (producing closed, elliptical orbits under an inverse-square central force) or positive (producing open, hyperbolic orbits under an inverse-square central force). For \"negative\" energies—i.e., for bound systems—the Poisson brackets are\nWe may now appreciate the motivation for the chosen scaling of D: With this scaling, the Hamiltonian no longer appears on the right-hand side of the preceding relation. Thus, the span of the three components of L and the three components of D forms a six-dimensional Lie algebra under the Poisson bracket. This Lie algebra is isomorphic to so(4), the Lie algebra of the 4-dimensional rotation group SO(4).\n\nBy contrast, for \"positive\" energy, the Poisson brackets have the opposite sign,\nIn this case, the Lie algebra is isomorphic to so(3,1).\n\nThe distinction between positive and negative energies arises because the desired scaling—the one that eliminates the Hamiltonian from the right-hand side of the Poisson bracket relations between the components of the scaled LRL vector—involves the \"square root\" of the Hamiltonian. To obtain real-valued functions, we must then take the absolute value of the Hamiltonian, which distinguishes between positive values (where formula_31) and negative values (where formula_32).\n\nThe Casimir invariants for negative energies are\n\nand have vanishing Poisson brackets with all components of D and L,\n\"C\" is trivially zero, since the two vectors are always perpendicular.\n\nHowever, the other invariant, \"C\", is non-trivial and depends only on \"m\", \"k\" and \"E\". Upon canonical quantization, this invariant allows the energy levels of hydrogen-like atoms to be derived using only quantum mechanical canonical commutation relations, instead of the conventional solution of the Schrödinger equation. This derivation is discussed in detail in the next section.\n\nPoisson brackets provide a simple guide for quantizing most classical systems: the commutation relation of two quantum mechanical operators is specified by the Poisson bracket of the corresponding classical variables, multiplied by .\n\nBy carrying out this quantization and calculating the eigenvalues of the Casimir operator for the Kepler problem, Wolfgang Pauli was able to derive the energy levels of hydrogen-like atoms (Figure 6) and, thus, their atomic emission spectrum. This elegant 1926 derivation was obtained \"before the development of the Schrödinger equation\".\n\nA subtlety of the quantum mechanical operator for the LRL vector A is that the momentum and angular momentum operators do not commute; hence, the quantum operator cross product of p and L must be defined carefully. Typically, the operators for the Cartesian components are defined using a symmetrized (Hermitian) product,\nOnce this is done, one can show that the quantum LRL operators satisfy commutations relations exactly analogous to the Poisson bracket relations in the previous section—just replacing the Poisson bracket with formula_37 times the commutator.\n\nFrom these operators, additional ladder operators for L can be defined,\nThese further connect \"different\" eigenstates of L, so different spin multiplets, among themselves.\n\nA normalized first Casimir invariant operator, quantum analog of the above, can likewise be defined,\nwhere is the inverse of the Hamiltonian energy operator, and is the identity operator.\n\nApplying these ladder operators to the eigenstates |\"ℓ\"〉 of the total angular momentum, azimuthal angular momentum and energy operators, the eigenvalues of the first Casimir operator, , are seen to be quantized, . Importantly, by dint of the vanishing of \"C\", they are independent of the ℓ and quantum numbers, making the energy levels degenerate.\n\nHence, the energy levels are given by\nwhich coincides with the Rydberg formula for hydrogen-like atoms (Figure 6). The additional symmetry operators A have connected the different ℓ multiplets among themselves, for a given energy (and \"C\"), dictating states at each level. In effect, they have enlarged the angular momentum group SO(3) to SO(4)/ℤ ~ SO(3) × SO(3).\n\nThe conservation of the LRL vector corresponds to a subtle symmetry of the system. In classical mechanics, symmetries are continuous operations that map one orbit onto another without changing the energy of the system; in quantum mechanics, symmetries are continuous operations that \"mix\" electronic orbitals of the same energy, i.e., degenerate energy levels. A conserved quantity is usually associated with such symmetries. For example, every central force is symmetric under the rotation group SO(3), leading to the conservation of angular momentum L. Classically, an overall rotation of the system does not affect the energy of an orbit; quantum mechanically, rotations mix the spherical harmonics of the same quantum number \"l\" without changing the energy.\n\nThe symmetry for the inverse-square central force is higher and more subtle. The peculiar symmetry of the Kepler problem results in the conservation of both the angular momentum vector L and the LRL vector A (as defined above) and, quantum mechanically, ensures that the energy levels of hydrogen do not depend on the angular momentum quantum numbers \"l\" and \"m\". The symmetry is more subtle, however, because the symmetry operation must take place in a higher-dimensional space; such symmetries are often called \"hidden symmetries\".\n\nClassically, the higher symmetry of the Kepler problem allows for continuous alterations of the orbits that preserve energy but not angular momentum; expressed another way, orbits of the same energy but different angular momentum (eccentricity) can be transformed continuously into one another. Quantum mechanically, this corresponds to mixing orbitals that differ in the \"l\" and \"m\" quantum numbers, such as the \"s\" () and \"p\" () atomic orbitals. Such mixing cannot be done with ordinary three-dimensional translations or rotations, but is equivalent to a rotation in a higher dimension.\n\nFor \"negative\" energies – i.e., for bound systems – the higher symmetry group is SO(4), which preserves the length of four-dimensional vectors\n\nIn 1935, Vladimir Fock showed that the quantum mechanical bound Kepler problem is equivalent to the problem of a free particle confined to a three-dimensional unit sphere in four-dimensional space. Specifically, Fock showed that the Schrödinger wavefunction in the momentum space for the Kepler problem was the stereographic projection of the spherical harmonics on the sphere. Rotation of the sphere and reprojection results in a continuous mapping of the elliptical orbits without changing the energy; quantum mechanically, this corresponds to a mixing of all orbitals of the same energy quantum number \"n\". Valentine Bargmann noted subsequently that the Poisson brackets for the angular momentum vector L and the scaled LRL vector D formed the Lie algebra for SO(4). Simply put, the six quantities D and L correspond to the six conserved angular momenta in four dimensions, associated with the six possible simple rotations in that space (there are six ways of choosing two axes from four). This conclusion does not imply that our universe is a three-dimensional sphere; it merely means that this particular physics problem (the two-body problem for inverse-square central forces) is \"mathematically equivalent\" to a free particle on a three-dimensional sphere.\n\nFor \"positive\" energies – i.e., for unbound, \"scattered\" systems – the higher symmetry group is SO(3,1), which preserves the Minkowski length of 4-vectors\n\nBoth the negative- and positive-energy cases were considered by Fock and Bargmann and have been reviewed encyclopedically by Bander and Itzykson.\n\nThe orbits of central-force systems – and those of the Kepler problem in particular – are also symmetric under reflection. Therefore, the SO(3), SO(4) and SO(3,1) groups cited above are not the full symmetry groups of their orbits; the full groups are O(3), O(4) and O(3,1), respectively. Nevertheless, only the connected subgroups, SO(3), SO(4) and SO(3,1), are needed to demonstrate the conservation of the angular momentum and LRL vectors; the reflection symmetry is irrelevant for conservation, which may be derived from the Lie algebra of the group.\n\nThe connection between the Kepler problem and four-dimensional rotational symmetry SO(4) can be readily visualized. Let the four-dimensional Cartesian coordinates be denoted (\"w\", \"x\", \"y\", \"z\") where (\"x\", \"y\", \"z\") represent the Cartesian coordinates of the normal position vector r. The three-dimensional momentum vector p is associated with a four-dimensional vector formula_44 on a three-dimensional unit sphere\n\nwhere formula_46 is the unit vector along the new \"w\"-axis. The transformation mapping p to η can be uniquely inverted; for example, the \"x\"-component of the momentum equals\n\nand similarly for \"p\" and \"p\". In other words, the three-dimensional vector p is a stereographic projection of the four-dimensional formula_44 vector, scaled by \"p\" (Figure 8).\n\nWithout loss of generality, we may eliminate the normal rotational symmetry by choosing the Cartesian coordinates such that the \"z\"-axis is aligned with the angular momentum vector L and the momentum hodographs are aligned as they are in Figure 7, with the centers of the circles on the \"y\"-axis. Since the motion is planar, and p and L are perpendicular, \"p\" = \"η\" = 0 and attention may be restricted to the three-dimensional vector . The family of Apollonian circles of momentum hodographs (Figure 7) correspond to a family of great circles on the three-dimensional formula_44 sphere, all of which intersect the \"η\"-axis at the two foci , corresponding to the momentum hodograph foci at \"p\" = ±\"p\". These great circles are related by a simple rotation about the \"η\"-axis (Figure 8). This rotational symmetry transforms all the orbits of the same energy into one another; however, such a rotation is orthogonal to the usual three-dimensional rotations, since it transforms the fourth dimension \"η\". This higher symmetry is characteristic of the Kepler problem and corresponds to the conservation of the LRL vector.\n\nAn elegant action-angle variables solution for the Kepler problem can be obtained by eliminating the redundant four-dimensional coordinates formula_44 in favor of elliptic cylindrical coordinates \n\nwhere sn, cn and dn are Jacobi's elliptic functions.\n\nThe Laplace–Runge–Lenz vector can also be generalized to identify conserved quantities that apply to other situations.\n\nIn the presence of a uniform electric field E, the generalized Laplace–Runge–Lenz vector formula_1 is\n\nwhere \"q\" is the charge of the orbiting particle. Although formula_1 is not conserved, it gives rise to a conserved quantity, namely formula_58.\n\nFurther generalizing the Laplace–Runge–Lenz vector to other potentials and special relativity, the most general form can be written as\n\nwhere (cf. Bertrand's theorem) and , with the angle \"θ\" defined by\n\nand \"γ\" is the Lorentz factor. As before, we may obtain a conserved binormal vector B by taking the cross product with the conserved angular momentum vector\n\nThese two vectors may likewise be combined into a conserved dyadic tensor W,\n\nIn illustration, the LRL vector for a non-relativistic, isotropic harmonic oscillator can be calculated. Since the force is central,\nthe angular momentum vector is conserved and the motion lies in a plane.\n\nThe conserved dyadic tensor can be written in a simple form\nalthough it should be noted that p and r are not necessarily perpendicular.\n\nThe corresponding Runge–Lenz vector is more complicated,\nwhere \nis the natural oscillation frequency and \n\nThe following are arguments showing that the LRL vector is conserved under central forces that obey an inverse-square law.\n\nA central force formula_68 acting on the particle is\n\nfor some function formula_70 of the radius formula_71. Since the angular momentum formula_72 is conserved under central forces, formula_73 and\n\nwhere the momentum formula_75 and where the triple cross product has been simplified using Lagrange's formula\n\nThe identity\n\nyields the equation\n\nFor the special case of an inverse-square central force formula_79, this equals\n\nTherefore, A is conserved for inverse-square central forces\n\nA shorter proof is obtained by using the relation of angular momentum to angular velocity, formula_82, which holds for a particle traveling in a plane perpendicular to formula_83. Specifying to inverse-square central forces, the time derivative of formula_84 is\nwhere the last equality holds because a unit vector can only change by rotation, and formula_86 is the orbital velocity of the rotating vector. Thus, A is seen to be a difference of two vectors with equal time derivatives.\n\nAs described below, this LRL vector A is a special case of a general conserved vector formula_1 that can be defined for all central forces. However, since most central forces do not produce closed orbits (see Bertrand's theorem), the analogous vector formula_1 rarely has a simple definition and is generally a multivalued function of the angle \"θ\" between r and formula_1.\n\nThe constancy of the LRL vector can also be derived from the Hamilton–Jacobi equation in parabolic coordinates , which are defined by the equations\n\nwhere \"r\" represents the radius in the plane of the orbit\n\nThe inversion of these coordinates is\n\nSeparation of the Hamilton–Jacobi equation in these coordinates yields the two equivalent equations\n\nwhere Γ is a constant of motion. Subtraction and re-expression in terms of the Cartesian momenta \"p\" and \"p\" shows that Γ is equivalent to the LRL vector\n\nThe connection between the rotational symmetry described above and the conservation of the LRL vector can be made quantitative by way of Noether's theorem. This theorem, which is used for finding constants of motion, states that any infinitesimal variation of the generalized coordinates of a physical system\n\nthat causes the Lagrangian to vary to first order by a total time derivative\n\ncorresponds to a conserved quantity Γ\n\nIn particular, the conserved LRL vector component \"A\" corresponds to the variation in the coordinates\n\nwhere \"i\" equals 1, 2 and 3, with \"x\" and \"p\" being the \"i\"th components of the position and momentum vectors r and p, respectively; as usual, \"δ\" represents the Kronecker delta. The resulting first-order change in the Lagrangian is\n\nSubstitution into the general formula for the conserved quantity Γ yields the conserved component \"A\" of the LRL vector,\n\nThe Noether theorem derivation of the conservation of the LRL vector A is elegant, but has one drawback: the coordinate variation \"δx\" involves not only the \"position\" r, but also the \"momentum\" p or, equivalently, the \"velocity\" v. This drawback may be eliminated by instead deriving the conservation of A using an approach pioneered by Sophus Lie. Specifically, one may define a Lie transformation in which the coordinates r and the time \"t\" are scaled by different powers of a parameter λ (Figure 9),\n\nThis transformation changes the total angular momentum \"L\" and energy \"E\",\nbut preserves their product \"EL\". Therefore, the eccentricity \"e\" and the magnitude \"A\" are preserved, as may be seen from the equation for \"A\"\n\nThe direction of A is preserved as well, since the semiaxes are not altered by a global scaling. This transformation also preserves Kepler's third law, namely, that the semiaxis \"a\" and the period \"T\" form a constant \"T\"/\"a\".\n\nUnlike the momentum and angular momentum vectors p and L, there is no universally accepted definition of the Laplace–Runge–Lenz vector; several different scaling factors and symbols are used in the scientific literature. The most common definition is given above, but another common alternative is to divide by the constant \"mk\" to obtain a dimensionless conserved eccentricity vector\n\nwhere v is the velocity vector. This scaled vector e has the same direction as A and its magnitude equals the eccentricity of the orbit. Other scaled versions are also possible, e.g., by dividing A by \"m\" alone\n\nor by \"p\"\n\nwhich has the same units as the angular momentum vector L. In rare cases, the sign of the LRL vector may be reversed, i.e., scaled by −1. Other common symbols for the LRL vector include a, R, F, J and V. However, the choice of scaling and symbol for the LRL vector do not affect its conservation.\n\nAn alternative conserved vector is the binormal vector B studied by William Rowan Hamilton\n\nwhich is conserved and points along the \"minor\" semiaxis of the ellipse; the LRL vector is the cross product of B and L (Figure 4).\n\nThe vector B is denoted as \"binormal\" since it is perpendicular to both A and L. Similar to the LRL vector itself, the binormal vector can be defined with different scalings and symbols.\n\nThe two conserved vectors, A and B can be combined to form a conserved dyadic tensor W,\n\nwhere \"α\" and \"β\" are arbitrary scaling constants and formula_112 represents the tensor product (which is not related to the vector cross product, despite their similar symbol). Written in explicit components, this equation reads\n\nBeing perpendicular to each another, the vectors A and B can be viewed as the principal axes of the conserved tensor W, i.e., its scaled eigenvectors. W is perpendicular to L\n\nsince A and B are both perpendicular to L as well, . For clarification, this equation reads, in explicit components,\n\n\n"}
{"id": "48539131", "url": "https://en.wikipedia.org/wiki?curid=48539131", "title": "List of Fields medalists affiliated with the Institute for Advanced Study", "text": "List of Fields medalists affiliated with the Institute for Advanced Study\n\nThis is a comprehensive list of Fields Medal winners affiliated with the Institute for Advanced Study in Princeton, New Jersey as current and former faculty members, visiting scholars, and other affiliates. Of the 56 individuals who have received the Fields Medal as of 2015, 41 are mathematicians who have been affiliated with the IAS as some point in their career. \n\nThe Fields Medal is the world’s most prestigious award in mathematics. It is presented every four years by the International Mathematical Union and is often referred to as the \"Nobel prize of mathematics.\" It is generally shared by four different researchers. Members of the IAS have dominated the award since its inception in 1936 and in 2010 they took all four of them.\n\n"}
{"id": "1152126", "url": "https://en.wikipedia.org/wiki?curid=1152126", "title": "Mathematics Genealogy Project", "text": "Mathematics Genealogy Project\n\nThe Mathematics Genealogy Project is a web-based database for the academic genealogy of mathematicians. By 4 August 2018, it contained information on 231,480 mathematical scientists who contributed to research-level mathematics. For a typical mathematician, the project entry includes graduation year, thesis title, \"alma mater\", doctoral advisor, and doctoral students.\n\nThe project grew out of founder Harry Coonce's desire to know the name of his advisor's advisor. Coonce was Professor of Mathematics at Minnesota State University, Mankato, at the time of the project's founding, and the project went online there in fall 1997. Coonce retired from Mankato in 1999, and in fall 2002 the university decided that it would no longer support the project. The project relocated at that time to North Dakota State University. Since 2003, the project has also operated under the auspices of the American Mathematical Society and in 2005 it received a grant from the Clay Mathematics Institute. Harry Coonce has been assisted by Mitchel T. Keller, Assistant Professor at Washington and Lee University. Dr Keller is currently the Managing Director of the project.\n\nThe Mathematics Genealogy Mission statement states, \"Throughout this project when we use the word \"mathematics\" or \"mathematician\" we mean that word in a very inclusive sense. Thus, all relevant data from statistics, computer science, philosophy or operations research is welcome.\"\n\nThe genealogy information is obtained from sources such as \"Dissertation Abstracts International\" and \"Notices of the American Mathematical Society\", but may be supplied by anyone via the project's website. The searchable database contains the name of the mathematician, university which awarded the degree, year when the degree was awarded, title of the dissertation, names of the advisor and second advisor, a flag of the country where the degree was awarded, a listing of doctoral students, and a count of academic descendants. Some historically significant figures who lacked a doctoral degree are listed, notably Joseph Louis Lagrange.\n\nIt has been noted that \"The data collected by the mathematics genealogy project are self-reported, so there is no guarantee that the observed genealogy network is a complete description of the mentorship network. In fact, 16,147 mathematicians do not have a recorded mentor, and of these, 8,336 do not have any recorded proteges.\" Maimgren, Ottino and Amaral (2010) stated that \"for [mathematicians who graduated between 1900 and 1960] we believe that the graduation and mentorship record is the most reliable.\"\n\n"}
{"id": "965376", "url": "https://en.wikipedia.org/wiki?curid=965376", "title": "Minimal counterexample", "text": "Minimal counterexample\n\nIn mathematics, the method of considering a minimal counterexample combines the ideas of inductive proof and proof by contradiction. Abstractly, in trying to prove a proposition \"P\", one assumes that it is false, and that therefore there is at least one counterexample. With respect to some idea of size, which may need to be chosen skillfully, one assumes that there is such a counterexample \"C\" that is \"minimal\". We expect that \"C\" is something quite hypothetical (since we are trying to prove \"P\"), but it may be possible to argue that if \"C\" existed, it would have some definite properties. From those we then try to get a contradiction. \n\nIf the form of the contradiction is that we can derive a further counterexample \"D\", and that \"D\" is smaller than \"C\" in the sense of the working hypothesis of minimality, then this technique is traditionally called infinite descent. There may however be more complicated ways to argue. For example, the minimal counterexample method has been much used in the classification of finite simple groups. The Feit–Thompson theorem, that finite simple groups that are not cyclic groups have even order, was based on the hypothesis of some, and therefore some minimal, simple group \"G\" of odd order. Every proper subgroup of \"G\" can be assumed a solvable group, meaning that much theory of such subgroups could be applied.\n\nThe assumption that if there is a counterexample, there is a minimal counterexample, is based on a well-ordering of some kind. The usual ordering on the natural numbers is clearly possible, by the most usual formulation of mathematical induction; but the scope of the method is well-ordered induction of any kind.\n\nEuclid's proof of the fundamental theorem of arithmetic is a simple proof using a minimal counterexample.\n"}
{"id": "8499571", "url": "https://en.wikipedia.org/wiki?curid=8499571", "title": "Negative probability", "text": "Negative probability\n\nThe probability of the outcome of an experiment is never negative, although a quasiprobability distribution allows a negative probability, or quasiprobability for some events. These distributions may apply to unobservable events or conditional probabilities.\n\nIn 1942, Paul Dirac wrote a paper \"The Physical Interpretation of Quantum Mechanics\" where he introduced the concept of negative energies and negative probabilities:\n\nThe idea of negative probabilities later received increased attention in physics and particularly in quantum mechanics. Richard Feynman argued that no one objects to using negative numbers in calculations: although \"minus three apples\" is not a valid concept in real life, negative money is valid. Similarly he argued how negative probabilities as well as probabilities above unity possibly could be useful in probability calculations.\n\nMark Burgin gives another example:\nNegative probabilities have later been suggested to solve several problems and paradoxes. \"Half-coins\" provide simple examples for negative probabilities. These strange coins were introduced in 2005 by Gábor J. Székely. Half-coins have infinitely many sides numbered with 0,1,2... and the positive even numbers are taken with negative probabilities. Two half-coins make a complete coin in the sense that if we flip two half-coins then the sum of the outcomes is 0 or 1 with probability 1/2 as if we simply flipped a fair coin.\n\nIn \"Convolution quotients of nonnegative definite functions\" and \"Algebraic Probability Theory\" Imre Z. Ruzsa and Gábor J. Székely proved that if a random variable X has a signed or quasi distribution where some of the probabilities are negative then one can always find two random variables, Y and Z, with ordinary (not signed / not quasi) distributions such that X, Y are independent and X + Y = Z in distribution. Thus X can always be interpreted as the \"difference\" of two ordinary random variables, Z and Y. If Y is interpreted as a measurement error of X and the observed value is Z then the negative regions of the distribution of X are masked / shielded by the error Y.\n\nAnother example known as the Wigner distribution in phase space, introduced by Eugene Wigner in 1932 to study quantum corrections, often leads to negative probabilities. For this reason, it has later been better known as the Wigner quasiprobability distribution. In 1945, M. S. Bartlett worked out the mathematical and logical consistency of such negative valuedness. The Wigner distribution function is routinely used in physics nowadays, and provides the cornerstone of phase-space quantization. Its negative features are an asset to the formalism, and often indicate quantum interference. The negative regions of the distribution are shielded from direct observation by the quantum uncertainty principle: typically, the moments of such a non-positive-semidefinite quasiprobability distribution are highly constrained, and prevent \"direct measurability\" of the negative regions of the distribution. But these regions contribute negatively and crucially to the expected values of observable quantities computed through such distributions, nevertheless.\n\nConsider a double slit experiment with photons. The two waves exiting each slit can be written as:\n\nformula_1\n\nand\n\nformula_2\n\nwhere \"d\" is the distance to the detection screen, \"a\" is the separation between the two slits, \"x\" the distance to the center of the screen, \"λ\" the wavelength and \"dN/dt\" is the number of photons emitted per unit time at the source. The amplitude of measuring a photon at distance \"x\" from the center of the screen is the sum of these two amplitudes coming out of each hole, and therefore the probability that a photon is detected at position \"x\" will be given by the square of this sum:\n\nformula_3,\n\nThis should strike you as the well-known probability rule:\n\nformula_4\n\nwhatever the last term means. Indeed, if one closes either one of the holes forcing the photon to go through the other slit, the two corresponding intensities are\n\nformula_5 and formula_6.\n\nBut now, if one does interpret each of these terms in this way, the joint probability takes negative values roughly every formula_7 !formula_8\n\nHowever, these negative probabilities are never observed as one can't isolate the cases in which the photon \"goes through both slits\", but can hint at the existence of anti-particles.\n\nNegative probabilities have more recently been applied to mathematical finance. In quantitative finance most probabilities are not real probabilities but pseudo probabilities, often what is known as risk neutral probabilities. These are not real probabilities, but theoretical \"probabilities\" under a series of assumptions that helps simplify calculations by allowing such pseudo probabilities to be negative in certain cases as first pointed out by Espen Gaarder Haug in 2004.\n\nA rigorous mathematical definition of negative probabilities and their properties was recently derived by Mark Burgin and Gunter Meissner (2011). The authors also show how negative probabilities can be applied to financial option pricing.\n\nThe concept of negative probabilities have also been proposed for reliable facility location models where facilities are subject to negatively correlated disruption risks when facility locations, customer allocation, and backup service plans are determined simultaneously. Li et al. proposed a virtual station structure that transforms a facility network with positively correlated disruptions into an equivalent one with added virtual supporting stations, and these virtual stations were subject to independent disruptions. This approach reduces a problem from one with correlated disruptions to one without. Xie et al. later showed how negatively correlated disruptions can also be addressed by the same modeling framework, except that a supporting station now may be disrupted with a “failure propensity” which\n\n“... inherits all mathematical characteristics and properties of a failure probability except that we allow it to be larger than 1...”\n\nThis finding paves ways for using compact mixed-integer mathematical programs to optimally design reliable location of service facilities under site-dependent and positive/negative/mixed disruption correlations.\n\nThe proposed “propensity” concept in Xie et al. turns out to be what Feynman and others referred to as “quasi-probability.” Note that when a quasi-probability is larger than 1, then 1 minus this value gives a negative probability. The truly physically verifiable observation is the facility disruption states, and there is no direct information on the station states or their corresponding probabilities. Hence the failure probability of the stations, interpreted as “probabilities of imagined intermediary states,” could exceed unity.\n\n"}
{"id": "6133075", "url": "https://en.wikipedia.org/wiki?curid=6133075", "title": "Phi-hiding assumption", "text": "Phi-hiding assumption\n\nThe phi-hiding assumption or Φ-hiding assumption is an assumption about the difficulty of finding small factors of φ(\"m\") where \"m\" is a number whose factorization is unknown, and φ is Euler's totient function. The security of many modern cryptosystems comes from the perceived difficulty of certain problems. Since P vs. NP problem is still unresolved, cryptographers cannot be sure computationally intractable problems exist. Cryptographers thus make assumptions as to which problems are \"hard\". It is commonly believed that if \"m\" is the product of two large primes, then calculating φ(\"m\") is currently computationally infeasible; this assumption is required for the security of the RSA Cryptosystem. The Φ-Hiding assumption is a stronger assumption, namely that if \"p\" and \"p\" are small primes exactly one of which divides φ(\"m\"), there is no polynomial-time algorithm which can distinguish which of the primes \"p\" and \"p\" divides φ(\"m\") with probability significantly greater than one-half.\n\nThis assumption was first stated in the 1999 paper Computationally Private Information Retrieval with Polylogarithmic Communication.\nThe Phi-hiding assumption has found applications in the construction of a few cryptographic primitives. Some of the constructions include:\n"}
{"id": "15366532", "url": "https://en.wikipedia.org/wiki?curid=15366532", "title": "Preordered class", "text": "Preordered class\n\nIn mathematics, a preordered class is a class equipped with a preorder.\n\nWhen dealing with a class \"C\", it is possible to define a class relation on \"C\" as a subclass of the power class \"C formula_1 C\" . Then, it is convenient to use the language of relations on a set.\n\nA preordered class is a class with a preorder on it. \"Partially ordered class\" and \"totally ordered class\" are defined in a similar way. These concepts generalize respectively those of preordered set, partially ordered set and totally ordered set. However, it is difficult to work with them as in the \"small\" case because many constructions common in a set theory are no longer possible in this framework.\n\nEquivalently, a preordered class is a thin category, that is, a category with at most one morphism from an object to another.\n\n\n"}
{"id": "24797", "url": "https://en.wikipedia.org/wiki?curid=24797", "title": "Proclus", "text": "Proclus\n\nProclus Lycaeus (; 8 February 412 – 17 April 485 AD), called the Successor (Greek , \"Próklos ho Diádokhos\"), was a Greek Neoplatonist philosopher, one of the last major classical philosophers (see Damascius). He set forth one of the most elaborate and fully developed systems of Neoplatonism. He stands near the end of the classical development of philosophy, and was very influential on Western medieval philosophy (Greek and Latin).\n\nProclus was born on February 8, 412 AD (his birth date is deduced from a horoscope cast by a disciple, Marinus) in Constantinople to a family of high social status in Lycia (his father Patricius was a high legal official, very important in the Eastern Roman Empire's court system) and raised in Xanthus. He studied rhetoric, philosophy and mathematics in Alexandria, with the intent of pursuing a judicial position like his father. Before completing his studies, he returned to Constantinople when his rector, his principal instructor (one Leonas), had business there.\n\nProclus became a successful practicing lawyer. However, the experience of the practice of law made Proclus realize that he truly preferred philosophy. He returned to Alexandria, and began determinedly studying the works of Aristotle under Olympiodorus the Elder. He also began studying mathematics during this period as well with a teacher named Heron (no relation to Hero of Alexandria, who was also known as Heron). As a gifted student, he eventually became dissatisfied with the level of philosophical instruction available in Alexandria, and went to Athens, the pre-eminent philosophical center of the day, in 431 to study at the Neoplatonic successor of the famous Academy founded 800 years earlier (in 387 BC) by Plato; there he was taught by Plutarch of Athens (not to be confused with Plutarch of Chaeronea), Syrianus, and Asclepigenia; he succeeded Syrianus as head of the Academy, and would in turn be succeeded on his death by Marinus of Neapolis.\n\nHe lived in Athens as a vegetarian bachelor, prosperous and generous to his friends, until the end of his life, except for a voluntary one-year exile, which was designed to lessen the pressure put on him by his political-philosophical activity, little appreciated by the Christian rulers; he spent the exile traveling and being initiated into various mystery cults. He was also instructed in the \"theurgic\" Neoplatonism, as derived from the Orphic and Chaldean Oracles. His house has been discovered recently in Athens, under the pavement of Dionysiou Areopagitou Street, south of Acropolis, opposite the theater of Dionysus. He had a great devotion to the goddess Athena, who he believed guided him at key moments in his life. Marinus reports that when Christians removed the statue of the goddess from the Parthenon, a beautiful woman appeared to Proclus in a dream and announced that the \"Athenian Lady\" wished to stay at his home. Proclus died aged 73, and was buried near Mount Lycabettus in a tomb. It is reported that he was writing 700 lines each day.\n\nThe majority of Proclus's works are commentaries on dialogues of Plato (\"Alcibiades\", \"Cratylus\", \"Parmenides\", \"Republic\", \"Timaeus\"). In these commentaries he presents his own philosophical system as a faithful interpretation of Plato, and in this he did not differ from other Neoplatonists, as he considered the Platonic texts to be divinely inspired (ὁ θεῖος Πλάτων \"ho theios Platon\"—the divine Plato, inspired by the gods) and therefore that they spoke often of things under a veil, hiding the truth from the philosophically uninitiate. Proclus was however a close reader of Plato, and quite often makes very astute points about his Platonic sources. A number of his Platonic commentaries are lost.\n\nProclus, the scholiast to Euclid, knew Eudemus of Rhodes' \"History of Geometry\" well, and gave a short sketch of the early history of geometry, which appeared to be founded on the older, lost book of Eudemus. The passage has been referred to as \"the Eudemian summary,\" and determines some approximate dates, which otherwise might have remained unknown. The influential commentary on the first book of Euclid's \"Elements of Geometry\" is one of the most valuable sources we have for the history of ancient mathematics, and its Platonic account of the status of mathematical objects was influential. In this work, Proclus also listed the first mathematicians associated with Plato: a mature set of mathematicians (Leodamas of Thasos, Archytas of Taras, and Theaetetus), a second set of younger mathematicians (Neoclides, Eudoxus of Cnidus), and a third yet younger set (Amyntas, Menaechmus and his brother Dinostratus, Theudius of Magnesia, Hermotimus of Colophon and Philip of Opus). Some of these mathematicians were influential in arranging the Elements that Euclid later published.\n\nIn addition to his commentaries, Proclus wrote two major systematic works. The \"Elements of Theology\" (Στοιχείωσις θεολογική) consists of 211 propositions, each followed by a proof, beginning from the existence of the One (divine Unity) and ending with the descent of individual souls into the material world. The \"Platonic Theology\" (Περὶ τῆς κατὰ Πλάτωνα θεολογίας) is a systematisation of material from Platonic dialogues, showing from them the characteristics of the divine orders, the part of the universe which is closest to the One.\n\nWe also have three essays, extant only in Latin translation: \"Ten doubts concerning providence\" (\"De decem dubitationibus circa providentiam\"); \"On providence and fate\" (\"De providentia et fato\"); \"On the existence of evils\" (\"De malorum subsistentia\").\n\nHe also wrote a number of minor works, which are listed in the bibliography below.\n\nProclus's system, like that of the other Neoplatonists, is a combination of Platonic, Aristotelian, and Stoic elements. In its broad outlines, Proclus's system agrees with that of Plotinus. However, following Iamblichus, Plutarch of Athens, and his master Syrianus, Proclus presents a much more elaborate universe than Plotinus, subdividing the elements of Plotinus's system into their logically distinct parts, and positing these parts as individual things. This multiplication of entities is balanced by the monism which is common to all Neoplatonists. What this means is that, on the one hand the universe is composed of hierarchically distinct things, but on the other all things are part of a single continuous emanation of power from the One. From this latter perspective, the many distinctions to be found in the universe are a result of the divided perspective of the human soul, which needs to make distinctions in its own thought in order to understand unified realities. The idealist tendency is taken further in John Scotus Eriugena.\n\nThere is a double motivation found in Neoplatonic systems. The first is a need to account for the origin and character of all things in the universe. The second is a need to account for how we can know this origin and character of things. These two aims are related: they begin from the assumption that we can know reality, and then ask the question of what reality must be like, in its origin and unfolding, so that we can know it. An important element in the Neoplatonic answer to these questions is its reaction to Scepticism. In response to the sceptical position that we only know the appearances presented by our senses, and not the world as it is, Plotinus placed the object of knowledge inside the soul itself, and accounted for this interior truth through the soul's kinship with its own productive principles.\n\nThe first principle in Neoplatonism is the One (Greek: \"to Hen\"). Being proceeds from the One. The One cannot itself be a being. If it were a being, it would have a particular nature, and so could not be universally productive. Because it is \"beyond being\" (\"epekeina tes ousias\", a phrase from Plato's \"Republic\" 509b), it is also beyond thought, because thinking requires the determinations which belong to being: the division between subject and object, and the distinction of one thing from another. For this reason, even the name \"The One\" is not a positive name, but rather the most non-multiple name possible, a name derived from our own inadequate conception of the simplicity of the first principle. The One causes all things by conferring unity, in the form of individuality, on them, and in Neoplatonism existence, unity, and form tend to become equivalent. The One causes things to exist by donating unity, and the particular manner in which a thing is one is its form (a dog and a house are individual in different manners, for example). Because the One makes things exist by giving them the individuality which makes them what they are as distinct and separate beings, the Neoplatonists thought of it also as the source of the good of everything. So the other name for the One is the Good. Despite appearances, the first principle is not double; all things have a double relation to it, as coming from them (One) and then being oriented back towards them to receive their perfection or completion (Good).\n\nThe particular characteristic of Proclus's system is his elaboration of a level of individual ones, called \"henads,\" between the One which is before being and intelligible divinity. The henads exist \"superabundantly\", also beyond being, but they stand at the head of chains of causation (\"seirai\") and in some manner give to these chains their particular character. He identifies them with the Greek gods, so one henad might be Apollo and be the cause of all things apollonian, while another might be Helios and be the cause of all \"sunny\" things. Each henad participates in every other henad, according to its character. What appears to be multiplicity is not multiplicity at all, because any henad may rightly be considered the center of the polycentric system.\n\nThe principle which is produced below the level of the One and the Henads is the divine Intellect (\"Nous\"). The One cannot have a determinate nature if it is to be the source of all determinate natures, so what it produces is the totality of all determinate natures, or Being. By determination is meant existence within boundaries, a being \"this\" and not \"that\". The most important determinate natures are the \"Greatest Kinds\" from Plato's \"Sophist\" (Being, Same, Other, Rest, Motion) and Aristotle's ten categories (Quantity, Quality, etc.). In other words, the One produces what Plato called the Forms, and the Forms are understood to be the first determinations into which all things fall. The One produces the Forms through the activity of thinking. The One itself does not think, but instead produces a divine mind, Intellect, whose thoughts are themselves the Forms. Intellect is both Thinking and Being. It is a mind which has its own contents as its object. All things relate to the first principle as both One and Good. As Being, Intellect is the product of the One. But it also seeks to return to its cause, and so in Thinking it attempts to grasp the One as its Good. But because the simplicity of the One/Good does not allow Intellect to grasp it, what Intellect does is generate a succession of perspectives around its simple source. Each of these perspectives is itself a Form, and is how Intellect generates for itself its own content.\n\nPlotinus speaks about the generation of Intellect from the One, and Intellect's attempt to return to the One in a thinking which is also a desiring. Proclus systematises this production through a threefold movement of remaining, procession, and return (\"mone, proodos, epistrophe\"). Intellect remains in the One, which means that it has the One as its origin. It proceeds from the One, which means that it comes to be as a separate entity. But it returns to the One, which means that it does not cut itself off from its source, but receives the good which is its identity from the One. This threefold motion is used by Proclus to structure all levels of his system below the One and above material reality, so that all things except those mentioned remain, proceed, and return.\n\nProclus also gives a much more elaborate account of Intellect than does Plotinus. In Plotinus we find the distinction between Being and Thinking in Intellect. Proclus, in keeping with his triadic structure of remaining, procession, and return, distinguishes three moments in Intellect: Intelligible, Intelligible-Intellectual, and Intellectual. They correspond to the object of thought, the power of the object to be grasped by the subject, and the thinking subject. These three divisions are elaborated further, so that the intelligible moment consists of three triads (Being, Eternity, and the Living Being or Paradigm from Plato's \"Timaeus\"). The intelligible-intellectual moment also consists of three triads, and the intellectual moment is a hebdomad (seven elements), among which is numbered the Demiurge from Plato's \"Timaeus\" and also the monad of Time (which is before temporal things). In this elaboration of Intellect as a whole, Proclus is attempting to give a hierarchical ordering to the various metaphysical elements and principles that other philosophers have discussed, by containing them within a single triadic logic of unfolding.\n\nProclus's universe unfolds according to the smallest steps possible, from unity to multiplicity. With Intellect emerges the multiplicity which allows one being to be different from another being. But as a divine mind, Intellect has a complete grasp of all its moments in one act of thought. For this reason, Intellect is outside of Time.\n\nIntellect as the second principle also gives rise to individual intellects, which hold various places within Proclus's cosmos.\n\nIn terms of his sources, Intellect is like taking the Platonic Forms and placing them in the self-thinking thought which is Aristotle's Unmoved Mover.\n\nSoul (\"Psyche\") is produced by Intellect, and so is the third principle in the Neoplatonic system. It is a mind, like Intellect, but it does not grasp all of its own content as one. Therefore with Soul, Time comes to be, as a measure of Soul's movement from one object of thought to another. Intellect tries to grasp the One, and ends up producing its own ideas as its content. Soul attempts to grasp Intellect in its return, and ends up producing its own secondary unfoldings of the Forms in Intellect. Soul, in turn, produces Body, the material world.\n\nIn his commentary on Plato's \"Timaeus\" Proclus explains the role the Soul as a principle has in mediating the Forms in Intellect to the body of the material world as a whole. The Soul is constructed through certain proportions, described mathematically in the \"Timaeus\", which allow it to make Body as a divided image of its own arithmetical and geometrical ideas.\n\nIndividual souls have the same overall structure as the principle of Soul, but they are weaker. They have a tendency to be fascinated with the material world, and be overpowered by it. It is at this point that individual souls are united with a material body (i.e. when they are born). Once in the body, our passions have a tendency to overwhelm our reason. According to Proclus, philosophy is the activity which can liberate the soul from a subjection to bodily passions, remind it of its origin in Soul, Intellect, and the One, and prepare it not only to ascend to the higher levels while still in this life, but to avoid falling immediately back into a new body after death.\n\nBecause the soul's attention, while inhabiting a body, is turned so far away from its origin in the intelligible world, Proclus thinks that we need to make use of bodily reminders of our spiritual origin. In this he agrees with the doctrines of theurgy put forward by Iamblichus. Theurgy is possible because the powers of the gods (the \"henads\") extend through their series of causation even down to the material world. And by certain power-laden words, acts, and objects, the soul can be drawn back up the series, so to speak. Proclus himself was a devotee of many of the religions in Athens, considering that the power of the gods could be present in these various approaches.\n\nFor Proclus, philosophy is important because it is one of the primary ways to rescue the soul from a fascination with the body and restore it to its station. However, beyond its own station, the soul has Intellect as its goal, and ultimately has unification with the One as its goal. So higher than philosophy is the non-discursive reason of Intellect, and the pre-intellectual unity of the One. Philosophy is therefore a means of its own overcoming, in that it points the soul beyond itself.\n\nProclus can be considered as the spokesman of mature Neoplatonism. His works had a great influence on the history of western philosophy. The extent of this influence, however, is obscured by the channels through which it was exercised. An important source of Procline ideas was through the Pseudo-Dionysius. This late-5th- or early-6th-century Christian Greek author wrote under the pseudonym Dionysius the Areopagite, the figure converted by St. Paul in Athens. Because of this fiction, his writings were taken to have almost apostolic authority. He is an original Christian writer, and in his works can be found a great number of Proclus's metaphysical principles.\n\nAnother important source for the influence of Proclus on the Middle Ages is Boethius's \"Consolation of Philosophy\", which has a number of Proclus principles and motifs. The central poem of Book III is a summary of Proclus's \"Commentary on the Timaeus\", and Book V contains the important principle of Proclus that things are known not according to their own nature, but according to the character of the knowing subject.\n\nA summary of Proclus's \"Elements of Theology\" circulated under the name \"Liber de Causis\" (the \"Book of Causes\"). This book is of uncertain origin, but circulated in the Arabic world as a work of Aristotle, and was translated into Latin as such. It had great authority because of its supposed Aristotelian origin, and it was only when Proclus's \"Elements\" were translated into Latin that Thomas Aquinas realised its true origin.\n\nProclus's works also exercised an influence during the Renaissance through figures such as Georgius Gemistus Pletho and Marsilio Ficino. Before the contemporary period, the most significant scholar of Proclus in the English-speaking world was Thomas Taylor, who produced English translations of most of his works, with commentaries.\n\nHis work inspired the New England Transcendentalists, including Ralph Waldo Emerson, who declared in 1843 that, in reading Proclus, \"I am filled with hilarity & spring, my heart dances, my sight is quickened, I behold shining relations between all beings, and am impelled to write and almost to sing.\"\n\nModern scholarship on Proclus essentially begins with E.R. Dodd's edition of the \"Elements of Theology\" in 1933. Since then he has attracted considerable attention, especially in the French-speaking world. Procline scholarship, however, still (2006) falls far short of the attention paid to Plotinus.\n\nThe following epigram is engraved on the tomb which houses Proclus and his master Syrianus:\n\nThe crater Proclus on the Moon is named after him.\n\n\nA number of other minor works or fragments of works survive. A number of major commentaries have been lost.\n\nThe \"Liber de Causis\" (Book of Causes) is not a work by Proclus, but a summary of his work the \"Elements of Theology\", likely written by an Arabic interpreter. It was mistakenly thought in the Middle Ages to be a work of Aristotle, but was recognised by Aquinas not to be so.\n\nA list of modern editions and translations of his surviving works is available at:\n\nMonographs\n\nCollections of essays\n\nBibliographic resources\n\n\n"}
{"id": "1046155", "url": "https://en.wikipedia.org/wiki?curid=1046155", "title": "Projection-valued measure", "text": "Projection-valued measure\n\nIn mathematics, particularly in functional analysis, a projection-valued measure (PVM) is a function defined on certain subsets of a fixed set and whose values are self-adjoint projections on a fixed Hilbert space. Projection-valued measures are formally similar to real-valued measures, except that their values are self-adjoint projections rather than real numbers. As in the case of ordinary measures, it is possible to integrate complex-valued functions with respect to a PVM; the result of such an integration is a linear operator on the given Hilbert space.\n\nProjection-valued measures are used to express results in spectral theory, such as the important spectral theorem for self-adjoint operators. The Borel functional calculus for self-adjoint operators is constructed using integrals with respect to PVMs. In quantum mechanics, PVMs are the mathematical description of projective measurements. They are generalized by positive operator valued measures (POVMs) in the same sense that a mixed state or density matrix generalizes the notion of a pure state.\n\nA projection-valued measure on a measurable space \nformula_1, where formula_2 is a σ-algebra of subsets of formula_3, is a mapping formula_4 from formula_2 to the set of self-adjoint projections on a Hilbert space formula_6 (i.e. the orthogonal projections) such that\n\n(where formula_8 is the identity operator of \"H\") and for every ξ, η ∈ \"H\", the set-function\n\nis a complex measure on \"M\" (that is, a complex-valued countably additive function).\n\nWe denote this measure by \nformula_10.\n\nNote that formula_11 is a real-valued measure, and a probability measure when formula_12 has length one.\n\nIf π is a projection-valued measure and\n\nthen the images π(\"E\"), π(\"F\") are orthogonal to each other. From this follows that in general,\n\nand they commute.\n\nExample. Suppose formula_15 is a measure space. Let, for every measurable subset E in M, \nbe the operator of multiplication by the indicator function 1 on \"L\"(\"X\"). Then π is a projection-valued measure.\n\nIf π is a projection-valued measure on a measurable space (\"X\", \"M\"), then the map\n\nextends to a linear map on the vector space of step functions on \"X\". In fact, it is easy to check that this map is a ring homomorphism. This map extends in a canonical way to all bounded complex-valued measurable functions on \"X\", and we have the following.\n\nTheorem. \"For any bounded\" \"M\"-\"measurable function f on X, there exists\" \"a unique bounded linear operator\" \nsuch that\"\n\n\"for all\" formula_20. \"Where,\" formula_21 \"denotes the complex measure\" \n\n\"from the definition of\" formula_4.\n\nThe map\n\nformula_24\n\nis a homomorphism of rings. \n\nAn integral notation is often used for formula_25, as in\n\nThe theorem is also correct for unbounded measurable functions \"f\", but then formula_25 will be an unbounded linear operator on the Hilbert space \"H\".\n\nThe spectral theorem says that every self-adjoint operator formula_28 has an associated projection-valued measure formula_29 defined on the real axis, such that\nThis allows to define the Borel functional calculus for such operators: if formula_31 is a measurable function, we set\n\nFirst we provide a general example of projection-valued measure based on direct integrals. Suppose (\"X\", \"M\", μ) is a measure space and let {\"H\"} be a μ-measurable family of separable Hilbert spaces. For every \"E\" ∈ \"M\", let π(\"E\") be the operator of multiplication by 1 on the Hilbert space\n\nThen π is a projection-valued measure on (\"X\", \"M\").\n\nSuppose π, ρ are projection-valued measures on (\"X\", \"M\") with values in the projections of \"H\", \"K\". π, ρ are unitarily equivalent if and only if there is a unitary operator \"U\":\"H\" → \"K\" such that\n\nfor every \"E\" ∈ \"M\".\n\nTheorem. If (\"X\", \"M\") is a standard Borel space, then for every projection-valued measure π on (\"X\", \"M\") taking values in the projections of a \"separable\" Hilbert space, there is a Borel measure μ and a μ-measurable family of Hilbert spaces {\"H\"}, such that π is unitarily equivalent to multiplication by 1 on the Hilbert space\n\nThe measure class of μ and the measure equivalence class of the multiplicity function \"x\" → dim \"H\" completely characterize the projection-valued measure up to unitary equivalence.\n\nA projection-valued measure π is \"homogeneous of multiplicity\" \"n\" if and only if the multiplicity function has constant value \"n\". Clearly,\n\nTheorem. Any projection-valued measure π taking values in the projections of a separable Hilbert space is an orthogonal direct sum of homogeneous projection-valued measures:\n\nwhere\n\nand\n\nIn quantum mechanics, given a projection valued measure of a measurable space X to the space of continuous endomorphisms upon a Hilbert space H, \n\n- the unit sphere of the Hilbert space \"H\" is interpreted as the set of possible states Φ of a quantum system,\n\n- the measurable space \"X\" is the value space for some quantum property of the system (an \"observable\"), \n\n- the projection-valued measure π expresses the probability that the observable takes on various values.\n\nA common choice for \"X\" is the real line, but it may also be \n\n- R (for position or momentum in three dimensions ), \n\n- a discrete set (for angular momentum, energy of a bound state, etc.), \n\n- the 2-point set \"true\" and \"false\" for the truth-value of an arbitrary proposition about Φ.\n\nLet \"E\" be a measurable subset of the measurable space \"X\" and Φ a normalized vector-state in \"H\", so that its Hilbert norm is unitary, ||Φ|| = 1. The probability that the observable takes its value in the subset \"E,\" given the system in state Φ, is\n\nwhere the latter notation is preferred in physics.\n\nWe can parse this in two ways. \n\nFirst, for each fixed \"E\", the projection π(\"E\") is a self-adjoint operator on \"H\" whose 1-eigenspace is the states Φ for which the value of the observable always lies in \"E\", and whose 0-eigenspace is the states Φ for which the value of the observable never lies in \"E\". \n\nSecond, for each fixed normalized vector state formula_40, the association \n\nis a probability measure on \"X\" making the values of the observable into a random variable.\n\nA measurement that can be performed by a projection-valued measure π is called a projective measurement. \n\nIf \"X\" is the real number line, there exists, associated to π, a Hermitian operator \"A\" defined on \"H\" by\n\nwhich takes the more readable form\n\nif the support of π is a discrete subset of R. \n\nThe above operator A is called the observable associated with the spectral measure.\n\nAny operator so obtained is called an observable , in quantum mechanics.\n\nThe idea of a projection-valued measure is generalized by the positive operator-valued measure (POVM), where the need for the orthogonality implied by projection operators is replaced by the idea of a set of operators that are a non-orthogonal partition of unity. This generalization is motivated by applications to quantum information theory.\n\n"}
{"id": "270041", "url": "https://en.wikipedia.org/wiki?curid=270041", "title": "Prony equation", "text": "Prony equation\n\nThe Prony equation (named after Gaspard de Prony) is a historically important equation in hydraulics, used to calculate the head loss due to friction within a given run of pipe. It is an empirical equation developed by Frenchman Gaspard de Prony in the 19th century:\n\nwhere \"h\" is the head loss due to friction, calculated from: the ratio of the length to diameter of the pipe \"L/D\", the velocity of the flow \"V\", and two empirical factors \"a\" and \"b\" to account for friction.\n\nThis equation has been supplanted in modern hydraulics by the Darcy–Weisbach equation, which used it as a starting point.\n\n"}
{"id": "7527818", "url": "https://en.wikipedia.org/wiki?curid=7527818", "title": "Proofs of trigonometric identities", "text": "Proofs of trigonometric identities\n\nThe main trigonometric identities between trigonometric functions are proved, using mainly the geometry of the right triangle. For greater and negative angles, see Trigonometric functions\n\nThe six trigonometric functions are defined for every real number, except, for some of them, for angles that differ from 0 by a multiple of the right angle (90°). Referring to the diagram at the right, the six trigonometric functions of θ are, for angles smaller than the right angle:\n\nIn the case of angles smaller than a right angle, the following identities are direct consequences of above definitions through the division identity\nThey remain valid for angles greater than 90° and for negative angles.\n\nOr\n\nTwo angles whose sum is π/2 radians (90 degrees) are \"complementary\". In the diagram, the angles at vertices A and B are complementary, so we can exchange a and b, and change θ to π/2 − θ, obtaining:\n\nIdentity 1:\n\nThe following two results follow from this and the ratio identities. To obtain the first, divide both sides of formula_21 by formula_23; for the second, divide by formula_24.\n\nSimilarly\n\nIdentity 2:\n\nThe following accounts for all three reciprocal functions.\n\nProof 2:\n\nRefer to the triangle diagram above. Note that formula_30 by Pythagorean theorem.\n\nSubstituting with appropriate functions -\n\nRearranging gives:\n\nDraw a horizontal line (the \"x\"-axis); mark an origin O. Draw a line from O at an angle formula_34 above the horizontal line and a second line at an angle formula_35 above that; the angle between the second line and the \"x\"-axis is formula_36.\n\nPlace P on the line defined by formula_36 at a unit distance from the origin.\n\nLet PQ be a line perpendicular to line defined by angle formula_34, drawn from point Q on this line to point P. formula_39 OQP is a right angle.\n\nLet QA be a perpendicular from point A on the \"x\"-axis to Q and PB be a perpendicular from point B on the \"x\"-axis to P. formula_39 OAQ and OBP are right angles.\n\nDraw R on PB so that QR is parallel to the \"x\"-axis.\n\nNow angle formula_41 (because formula_42,\nmaking formula_43, and finally formula_41)\n\nBy substituting formula_54 for formula_35 and using Symmetry, we also get:\n\nAnother rigorous proof, and much easier, can be given by using Euler's formula, known from complex analysis. \nEuler's formula is:\n\nIt follows that for angles formula_34 and formula_35 we have:\n\nAlso using the following properties of exponential functions:\n\nEvaluating the product:\n\nEquating real and imaginary parts:\n\nUsing the figure above,\n\nBy substituting formula_54 for formula_35 and using Symmetry, we also get:\n\nAlso, using the complementary angle formulae,\n\nFrom the sine and cosine formulae, we get\n\nDividing both numerator and denominator by formula_80, we get\n\nSubtracting formula_82 from formula_83, using formula_84,\n\nSimilarly from the sine and cosine formulae, we get\n\nThen by dividing both numerator and denominator by formula_87, we get\n\nOr, using formula_89,\n\nUsing formula_91,\n\nFrom the angle sum identities, we get\n\nand\n\nThe Pythagorean identities give the two alternative forms for the latter of these:\n\nThe angle sum identities also give\n\nIt can also be proved using Euler's formula\n\nSquaring both sides yields\n\nBut replacing the angle with its doubled version, which achieves the same result in the left side of the equation, yields\n\nIt follows that\n\nExpanding the square and simplifying on the left hand side of the equation gives\n\nBecause the imaginary and real parts have to be the same, we are left with the original identities\n\nand also\n\nThe two identities giving the alternative forms for cos 2θ lead to the following equations:\n\nThe sign of the square root needs to be chosen properly—note that if 2 is added to θ, the quantities inside the square roots are unchanged, but the left-hand-sides of the equations change sign. Therefore, the correct sign to use depends on the value of θ.\n\nFor the tan function, the equation is:\n\nThen multiplying the numerator and denominator inside the square root by (1 + cos θ) and using Pythagorean identities leads to:\n\nAlso, if the numerator and denominator are both multiplied by (1 - cos θ), the result is:\n\nThis also gives:\n\nSimilar manipulations for the cot function give:\n\nIf formula_113 half circle (for example, formula_114, formula_115 and formula_116 are the angles of a triangle),\n\nProof:\n\nIf formula_119 quarter circle,\n\nProof:\n\nReplace each of formula_121, formula_122, and formula_123 with their complementary angles, so cotangents turn into tangents and vice versa.\n\nGiven\n\nso the result follows from the triple tangent identity.\n\n\nFirst, start with the sum-angle identities:\n\nBy adding these together,\n\nSimilarly, by subtracting the two sum-angle identities,\n\nLet formula_133 and formula_134,\n\nSubstitute formula_115 and formula_116\n\nTherefore,\n\nSimilarly for cosine, start with the sum-angle identities:\n\nAgain, by adding and subtracting\n\nSubstitute formula_115 and formula_116 as before,\n\nThe figure at the right shows a sector of a circle with radius 1. The sector is of the whole circle, so its area is . We assume here that .\n\nThe area of triangle is , or . The area of triangle is , or .\n\nSince triangle lies completely inside the sector, which in turn lies completely inside triangle , we have\n\nThis geometric argument relies on definitions of arc length and\narea, which act as assumptions, so it is rather a condition imposed in construction of trigonometric functions than\na provable property. For the sine function, we can handle other values. If , then . But (because of the Pythagorean identity), so . So we have\n\nFor negative values of we have, by symmetry of the sine function\n\nHence\n\nand\n\nIn other words, the function sine is differentiable at 0, and its derivative is 1.\n\nProof: From the previous inequalities, we have, for small angles\n\nTherefore,\nConsider the right-hand inequality. Since\nMultiply through by formula_165\nCombining with the left-hand inequality:\nTaking formula_165 to the limit as formula_169\nTherefore,\n\nProof:\n\nThe limits of those three quantities are 1, 0, and 1/2, so the resultant limit is zero.\n\nProof:\n\nAs in the preceding proof,\n\nThe limits of those three quantities are 1, 1, and 1/2, so the resultant limit is 1/2.\n\nAll these functions follow from the Pythagorean trigonometric identity. We can prove for instance the function\n\nProof:\n\nWe start from\nThen we divide this equation by formula_178\n\nThen use the substitution formula_180, also use the Pythagorean trigonometric identity:\n\nThen we use the identity formula_182\n\n"}
{"id": "3547060", "url": "https://en.wikipedia.org/wiki?curid=3547060", "title": "Puncturing", "text": "Puncturing\n\nIn coding theory, puncturing is the process of removing some of the parity bits after encoding with an error-correction code. This has the same effect as encoding with an error-correction code with a higher rate, or less redundancy. However, with puncturing the same decoder can be used regardless of how many bits have been punctured, thus puncturing considerably increases the flexibility of the system without significantly increasing its complexity.\n\nIn some cases, a pre-defined pattern of puncturing is used in an encoder. Then, the inverse operation, known as depuncturing, is implemented by the decoder.\n\nPuncturing is used in UMTS during the rate matching process. It is also used in Wi-Fi, GPRS and EDGE, as well as in the DVB-T and DRM Standards.\n\nPuncturing is often used with the Viterbi algorithm in coding systems.\n\nDuring Radio Resource Control (RRC) Connection set procedure, during sending NBAP radio link setup message the uplink puncturing limit will send to NODE B, along with U/L spreading factor & U/L scrambling code.\n\n"}
{"id": "22932101", "url": "https://en.wikipedia.org/wiki?curid=22932101", "title": "R. P. Gupta", "text": "R. P. Gupta\n\nRam Prakash Gupta was a professor of graph theory at Waterloo, Canada and at Ohio State University. He received his Ph.D. in graph theory from the Indian Statistical Institute, Calcutta, India in 1968. His advisor was C. R. Rao. Gupta is known for his work in chromatic numbers.\n\n"}
{"id": "35857112", "url": "https://en.wikipedia.org/wiki?curid=35857112", "title": "Regularization perspectives on support vector machines", "text": "Regularization perspectives on support vector machines\n\nRegularization perspectives on support vector machines provide a way of interpreting support vector machines (SVMs) in the context of other machine learning algorithms. SVM algorithms categorize multidimensional data, with the goal of fitting the training set data well, but also avoiding overfitting, so that the solution generalizes to new data points. Regularization algorithms also aim to fit training set data and avoid overfitting. They do this by choosing a fitting function that has low error on the training set, but also is not too complicated, where complicated functions are functions with high norms in some function space. Specifically, Tikhonov regularization algorithms choose a function that minimize the sum of training set error plus the function's norm. The training set error can be calculated with different loss functions. For example, regularized least squares is a special case of Tikhonov regularization using the squared error loss as the loss function.\n\nRegularization perspectives on support vector machines interpret SVM as a special case Tikhonov regularization, specifically Tikhonov regularization with the hinge loss for a loss function. This provides a theoretical framework with which to analyze SVM algorithms and compare them to other algorithms with the same goals: to generalize without overfitting. SVM was first proposed in 1995 by Corinna Cortes and Vladimir Vapnik, and framed geometrically as a method for finding hyperplanes that can separate multidimensional data into two categories. This traditional geometric interpretation of SVMs provides useful intuition about how SVMs work, but is difficult to relate to other machine learning techniques for avoiding overfitting like regularization, early stopping, sparsity and Bayesian inference. However, once it was discovered that SVM is also a special case of Tikhonov regularization, regularization perspectives on SVM provided the theory necessary to fit SVM within a broader class of algorithms. This has enabled detailed comparisons between SVM and other forms of Tikhonov regularization, and theoretical grounding for why it is beneficial to use SVM's loss function, the hinge loss.\n\nIn the statistical learning theory framework, an algorithm is a strategy for choosing a function formula_1 given a training set formula_2 of inputs, formula_3, and their labels, formula_4 (the labels are usually formula_5). Regularization strategies avoid overfitting by choosing a function that fits the data, but is not too complex. Specifically:\n\nformula_6,\n\nwhere formula_7 is a hypothesis space of functions, formula_8 is the loss function, formula_9 is a norm on the hypothesis space of functions, and formula_10 is the regularization parameter.\n\nWhen formula_7 is a reproducing kernel Hilbert space, there exists a kernel function formula_12 that can be written as an formula_13 symmetric positive definite matrix formula_14. By the representer theorem, formula_15, and formula_16\n\nThe simplest and most intuitive loss function for categorization is the misclassification loss, or 0-1 loss, which is 0 if formula_17 and 1 if formula_18, i.e. the Heaviside step function on formula_19. However, this loss function is not convex, which makes the regularization problem very difficult to minimize computationally. Therefore, we look for convex substitutes for the 0-1 loss. The hinge loss, formula_20 where formula_21, provides such a convex relaxation. In fact, the hinge loss is the tightest convex upper bound to the 0-1 misclassification loss function, and with infinite data returns the Bayes optimal solution:\n\n<math>f_b(x) = \\left\\{\\begin{matrix}1&p(1|x)>p(-1|x)\\\\-1&p(1|x)\n\nThe Tikhonov regularization problem can be shown to be equivalent to traditional formulations of SVM by expressing it in terms of the hinge loss. With the hinge loss,\n\nformula_20\n\nwhere formula_21, the regularization problem becomes\n\nformula_24.\n\nMultiplying by formula_25 yields\n\nformula_26,\n\nwith formula_27, which is equivalent to the standard SVM minimization problem.\n\n"}
{"id": "24985094", "url": "https://en.wikipedia.org/wiki?curid=24985094", "title": "Statistics education", "text": "Statistics education\n\nStatistics education is the practice of teaching and learning of statistics, along with the associated scholarly research.\n\nStatistics is both a formal science and a practical theory of scientific inquiry, and both aspects are considered in statistics education. Education in statistics has similar concerns as does education in other mathematical sciences, like logic, mathematics, and computer science. At the same time, statistics is concerned with evidence-based reasoning, particularly with the analysis of data. Therefore, education in statistics has strong similarities to education in empirical disciplines like psychology and chemistry, in which education is closely tied to \"hands-on\" experimentation.\n\nMathematicians and statisticians often work in a department of mathematical sciences (particularly at colleges and small universities). Statistics courses have been sometimes taught by non-statisticians, against the recommendations of some professional organizations of statisticians and of mathematicians.\n\nStatistics education research is an emerging field that grew out of different disciplines and is currently establishing itself as a unique field that is devoted to the improvement of teaching and learning statistics at all educational levels.\n\nStatistics educators have cognitive and noncognitive goals for students. For example, former American Statistical Association (ASA) President Katherine Wallman defined statistical literacy as including the cognitive abilities of understanding and critically evaluating statistical results as well as appreciating the contributions statistical thinking can make.\n\nIn the text rising from the 2008 joint conference of the International Commission on Mathematical Instruction and the International Association of Statistics Educators, editors Carmen Batanero, Gail Burrill, and Chris Reading (Universidad de Granada, Spain, Michigan State University, USA, and University of New England, Australia, respectively) note worldwide trends in curricula which reflect data-oriented goals. In particular, educators currently seek to have students: \"design investigations; formulate research questions; collect data using observations, surveys, and experiments; describe and compare data sets; and propose and justify conclusions and predictions based on data.\" The authors note the importance of developing statistical thinking and reasoning in addition to statistical knowledge.\n\nDespite the fact that cognitive goals for statistics education increasingly focus on statistical literacy, statistical reasoning, and statistical thinking rather than on skills, computations and procedures alone, there is no agreement about what these terms mean or how to assess these outcomes. A first attempt to define and distinguish between these three terms appears in the ARTIST website which was created by Garfield, delMas and Chance and has since been included in several publications.\nBrief definitions of these terms are as follows:\n\nFurther cognitive goals of statistics education vary across students' educational level and the contexts in which they expect to encounter statistics.\n\nStatisticians have proposed what they consider the most important statistical concepts for educated citizens. For example, Utts (2003) published seven areas of what every educated citizen should know, including understanding that \"variability is normal\" and how \"coincidences… are not uncommon because there are so many possibilities.\" Gal (2002) suggests adults in industrialized societies are expected to exercise statistical literacy, \"the ability to interpret and critically evaluate statistical information… in diverse contexts, and the ability to… communicate understandings and concerns regarding the… conclusions.\"\n\nNon-cognitive outcomes include affective constructs such as attitudes, beliefs, emotions, dispositions, and motivation. According to prominent researchers Gal & Ginsburg, statistics educators should make it a priority to be aware of students’ ideas, reactions, and feelings towards statistics and how these affect their learning.\n\nBeliefs are defined as one’s individually held ideas about statistics, about oneself as a learner of statistics, and about the social context of learning statistics. Beliefs are distinct from attitudes in the sense that attitudes are relatively stable and intense feelings that develop over time in the context of experiences learning statistics. Students’ web of beliefs provides a context for their approach towards their classroom experiences in statistics. Many students enter a statistics course with apprehension towards learning the subject, which works against the learning environment that the instructor is trying to accomplish. Therefore, it is important for instructors to have access to assessment instruments that can give an initial diagnosis of student beliefs and monitor beliefs during a course. Frequently, assessment instruments have monitored beliefs and attitudes together. For examples of such instruments, see the attitudes section below.\n\nDisposition has to do with the ways students question the data and approach a statistical problem. Dispositions is one of the four dimensions in Wild and Pfannkuch’s framework for statistical thinking, and contains the following elements:\n\n\nScheaffer states that a goal of statistics education is to have students see statistics broadly. He developed a list of views of statistics that can lead to this broad view, and describes them as follows:\n\nSince students often experience math anxiety and negative opinions about statistics courses, various researchers have addressed attitudes and anxiety towards statistics. Some instruments have been developed to measure college students’ attitudes towards statistics, and have been shown to have appropriate psychometric properties. Examples of such instruments include:\n\n\nCareful use of instruments such as these can help statistics instructors to learn about students’ perception of statistics, including their anxiety towards learning statistics, the perceived difficulty of learning statistics, and their perceived usefulness of the subject. Some studies have shown modest success at improving student attitudes in individual courses, but no generalizable studies showing improvement in student attitudes have been seen.\n\nNevertheless, one of the goals of statistics education is to make the study of statistics a positive experience for students and to bring in interesting and engaging examples and data that will motivate students. According to a fairly recent literature review, improved student attitudes towards statistics can lead to better motivation and engagement, which also improves cognitive learning outcomes.\n\nIn New Zealand, a new curriculum for statistics has been developed by Chris Wild and colleagues at Auckland University. Rejecting the contrived, and now unnecessary due to computer power, approach of reasoning under the null and the restrictions of normal theory, they use comparative box plots and bootstrap to introduce concepts of sampling variability and inference. The developing curriculum also contains aspects of statistical literacy.\n\nIn the United Kingdom, at least some statistics has been taught in schools since the 1930s. At present, A-level qualifications (typically taken by 17-18 year olds) are being developed in \"Statistics\" and \"Further Statistics\". The coverage of the former includes: Probability; Data Collection; Descriptive Statistics; Discrete Probability Distributions; Binomial Distribution; Poisson Distributions; Continuous Probability Distributions; The Normal Distribution; Estimation; Hypothesis Testing; Chi-Squared; Correlation and Regression. The coverage of \"Further Statistics\" includes: Continuous Probability Distributions; Estimation; Hypothesis Testing; One Sample Tests; Hypothesis Testing; Two Sample Tests; Goodness of Fit Tests; Experimental Design; Analysis of Variance (Anova); Statistical Process Control; Acceptance Sampling. The Centre for Innovation in Mathematics Teaching (CIMT) has online course notes for these sets of topics. Revision notes for an existing qualification indicate a similar coverage. At an earlier age (typically 15–16 years) GCSE qualifications in mathematics contain \"Statistics and Probability\" topics on: Probability; Averages; Standard Deviation; Sampling; Cumumulative Frequency Graphs (including median and quantiles); Representing Data; Histograms. The UK's Office for National Statistics has a webpage leading to material suitable for both teachers and students at school level. In 2004 the Smith inquiry made the following statement:\n\"There is much concern and debate about the positioning of Statistics and Data Handling within the current mathematics GCSE, where it occupies some 25 per cent of the timetable allocation. On the one hand, there is widespread agreement that the Key Stage 4 curriculum is over-crowded and that the introduction of Statistics and Data Handling may have been at the expense of time needed for practising and acquiring fluency in core mathematical manipulations. Many in higher education mathematics and engineering departments take this view. On the other hand, there is overwhelming recognition, shared by the Inquiry, of the vital importance of Statistics and Data Handling skills both for a number of other academic disciplines and in the workplace. The Inquiry recommends that there be a radical re-look at this issue and that much of the teaching and learning of Statistics and Data Handling would be better removed from the mathematics timetable and integrated with the teaching and learning of other disciplines (e.g. biology or geography). The time restored to the mathematics timetable should be used for acquiring greater mastery of core mathematical concepts and operations.\" \n\nIn the United States, schooling has increased the use of probability and statistics, especially since the 1990s. Summary statistics and graphs are taught in elementary school in many states. Topics in probability and statistical reasoning are taught in high school algebra (or mathematical science) courses; statistical reasoning has been examined in the SAT test since 1994. The College Board has developed an Advanced Placement course in statistics, which has provided a college-level course in statistics to hundreds of thousands of high school students, with the first examination happening in May 1997. In 2007, the ASA endorsed the Guidelines for Assessment and Instruction in Statistics Education (GAISE), a two-dimensional framework for the conceptual understanding of statistics in Pre-K-12 students. The framework contains learning objectives for students at each conceptual level and provides pedagogical examples that are consistent with the conceptual levels.\n\nEstonia is piloting a new statistics curriculum developed by the Computer-Based Math foundation based around its principles of using computers as the primary tool of education. in cooperation with the University of Tartu.\n\nStatistics is often taught in departments of mathematics or in departments of mathematical sciences. At the undergraduate level, statistics is often taught as a service course.\n\nBy tradition in the U.K., most professional statisticians are trained at the Master level. A difficulty of recruiting strong undergraduates has been noted: \"Very few undergraduates positively choose to study statistics degrees; most choose some statistics options within a mathematics programme, often to avoid the advanced pure and applied mathematics courses. My view is that statistics as a theoretical discipline is better taught late rather than early, whereas statistics as part of scientific methodology should be taught as part of science.\"\n\nIn the United Kingdom, the teaching of statistics at university level was originally done within science departments that needed the topic to accompany the teaching of their own subjects, and departments of mathematics had limited coverage before the 1930s. For the twenty years subsequent to this, while departments of mathematics had started to teach statistics, there was little realisation that essentially the same basic statistical methodology was being applied across a variety of sciences. Statistical departments have had difficulty when they have been separated from mathematics departments.\n\nPsychologist Andy Field (British Psychological Society Teaching and Book Award) created a new concept of statistical teaching and textbooks that goes beyond the printed page.\n\nEnrollments in statistics have increased in community colleges, in four year colleges and universities in the United States. At community colleges in the United States, mathematics has experienced increased enrollment since 1990. At community colleges, the ratio of the students enrolled in statistics to those enrolled in calculus rose from 56% in 1990 to 82% in 1995. One of the ASA-endorsed GAISE reports focused on statistics education at the introductory college level. The report includes a brief history of the introductory statistics course and recommendations for how it should be taught.\n\nIn many colleges, a basic course in \"statistics for non-statisticians\" has required only algebra (and not calculus); for future statisticians, in contrast, the undergraduate exposure to statistics is highly mathematical. As undergraduates, future statisticians should have completed courses in multivariate calculus, linear algebra, computer programming, and a year of calculus-based probability and statistics. Students wanting to obtain a doctorate in statistics from \"any of the better graduate programs in statistics\" should also take \"real analysis\". Laboratory courses in physics, chemistry and psychology also provide useful experiences with planning and conducting experiments and with analyzing data. The ASA recommends that undergraduate students consider obtaining a bachelor's degree in applied mathematics as preparation for entering a master program in statistics.\n\nHistorically, professional degrees in statistics have been at the Master level, although some students may qualify to work with a bachelor's degree and job-related experience or further self-study. Professional competence requires a background in mathematics---including at least multivariate calculus, linear algebra, and a year of calculus-based probability and statistics. In the United States, a master program in statistics requires courses in probability, mathematical statistics, and applied statistics (e.g., design of experiments, survey sampling, etc.).\n\nFor a doctoral degree in statistics, it has been traditional that students complete a course in measure-theoretic probability as well as courses in mathematical statistics. Such courses require a good course in real analysis, covering the proofs of the theory of calculus and topics like the uniform convergence of functions. In recent decades, some departments have discussed allowing doctoral students to waive the course in measure-theoretic probability by demonstrating advanced skills in computer programming or scientific computing.\n\nThe question of what qualities are needed to teach statistics has been much discussed, and sometimes this discussion is concentrated on the qualifications necessary for those undertaking such teaching. The question arises separately for teaching at both school and university levels, partly because of the need for numerically more such teachers at school level and partly because of need for such teachers to cover a broad range of other topics within their overall duties. Given that \"statistics\" is often taught to non-scientists, opinions can range all the way from \"statistics should be taught by statisticians\", through \"teaching of statistics is too mathematical\" to the extreme that \"statistics should not be taught by statisticians\".\n\nIn the United States especially, statisticians have long complained that many mathematics departments have assigned mathematicians (without statistical competence) to teach statistics courses, effectively giving \"double blind\" courses. The principle that college-instructors should have qualifications and engagement with their academic discipline has long been violated in United States colleges and universities, according to generations of statisticians. For example, the journal Statistical Science reprinted \"classic\" articles on the teaching of statistics by non-statisticians by Harold Hotelling; Hotelling's articles\nare followed by the comments of Kenneth J. Arrow, W. Edwards Deming, Ingram Olkin, David S. Moore, James V. Sidek, Shanti S. Gupta, Robert V. Hogg, Ralph A. Bradley, and by Harold Hotelling, Jr. (an economist and son of Harold Hotelling).\n\nData on the teaching of statistics in the United States has been collected on behalf of the Conference Board of the Mathematical Sciences (CBMS). Examining data from 2000, Schaeffer and Stasny reported\n\nBy far the majority of instructors within statistics departments\nhave at least a master’s degree in statistics or biostatistics (about\n89% for doctoral departments and about 79% for master’s departments).\nIn doctoral mathematics departments, however, only\nabout 58% of statistics course instructors had at least a master’s\ndegree in statistics or biostatistics as their highest degree earned.\nIn master’s-level mathematics departments, the corresponding\npercentage was near 44%, and in bachelor’s-level departments\nonly 19% of statistics course instructors had at least a master’s\ndegree in statistics or biostatistics as their highest degree\nearned. As we expected, a large majority of instructors in statistics\ndepartments (83% for doctoral departments and 62% for\nmaster’s departments) held doctoral degrees in either statistics\nor biostatistics. The comparable percentages for instructors of\nstatistics in mathematics departments were about 52% and 38%.\n\nThe principle that statistics-instructors should have statistical competence has been affirmed by the guidelines of the Mathematical Association of America, which has been endorsed by the ASA. The unprofessional teaching of statistics by mathematicians (without qualifications in statistics) has been addressed in many articles.\n\nThe literature on methods of teaching statistics is closely related to the literature on the teaching of mathematics for two reasons. First, statistics is often taught as part of the mathematics curriculum, by instructors trained in mathematics and working in a mathematics department. Second, statistical theory has often been taught as a mathematical theory rather than as the practical logic of science --- as the science that \"puts chance to work\" in Rao's phrase--- and this has entailed an emphasis on formal and manipulative training, such as solving combinatorial problems involving red and green jelly beans. Statisticians have complained that mathematicians are prone to over-emphasize mathematical manipulations and probability theory and under-emphasize questions of experimentation, survey methodology, exploratory data analysis, and statistical inference.\n\nIn recent decades, there has been an increased emphasis on data analysis and scientific inquiry in statistics education. In the United Kingdom, the Smith inquiry \"Making Mathematics Count\" suggests teaching basic statistical concepts as part of the science curriculum, rather than as part of mathematics. In the United States, the ASA's guidelines for undergraduate statistics specify that introductory statistics should emphasize the scientific methods of data collection, particularly randomized experiments and random samples: further, the first course should review these topics when the theory of \"statistical inference\" is studied. Similar recommendations occur hold for the Advanced Placement (AP) course in Statistics. The ASA and AP guidelines are followed by contemporary textbooks in the USA, such as those by Freedman, Purvis & Pisani (\"Statistics\") and by David S. Moore (\"Introduction to the Practice of Statistics\" with McCabe and \"Statistics: Concepts and Controversies\" with Notz) and by Watkins, Schaeffer & Cobb (\"Statistics: From Data to Decisions\" and \"Statistics in Action\").\n\nBesides an emphasis on the scientific inquiry in the content of beginning of statistics, there has also been an increase on active learning in the conduct of the statistics classroom.\n\nThe International Statistical Institute (ISI) now has one section devoted to education, the International Association for Statistical Education(IASE), which runs the International Conference on Teaching Statistics every four years as well as IASE satellite conferences around ISI and ICMI meetings. The UK established the Royal Statistical Society Centre for Statistics Education and the ASA now also has a Section on Statistical Education, focused mostly on statistics teaching at the elementary and secondary levels.\n\nIn addition to the international gatherings of statistics educators at ICOTS every four years, the US hosts a US Conference on Teaching Statistics (USCOTS) every two years and has recently started an Electronic Conference on Teaching Statistics (eCOTS) to alternate with USCOTS. Sessions on statistics education area also offered at many conferences in mathematics educations such as the International Congress on Mathematical Education, the National Council of Teachers of Mathematics, the Conference of the International Group for the Psychology of Mathematics Education, and the Mathematics Education Research Group of Australasia. The annual Joint Statistical Meetings (offered by the ASA and Statistics Canada) offer many sessions and roundtables on statistics education. The International Research Forums on Statistical Reasoning, Thinking, and Literacy offer scientific gatherings every two years and related publications in journals, CD-ROMs and books on research in statistics education.\n\nOnly three universities currently offer graduate programs in statistics education: the University of Granada, the University of Minnesota, and the University of Florida. However, graduate students in a variety of disciplines (e.g., mathematics education, psychology, educational psychology) have been finding ways to complete dissertations on topics related to teaching and learning statistics. These dissertations are archived on the IASE web site.\n\nTwo main courses in statistics education that have been taught in a variety of settings and departments are a course on teaching statistics and a course on statistics education research. An ASA-sponsored workshop has established recommendations for additional graduate programs and courses.\n\n\nTeachers of statistics have been encouraged to explore new directions in curriculum content, pedagogy and assessment. In an influential talk at USCOTS, researcher George Cobb presented an innovative approach to teaching statistics that put simulation, randomization, and bootstrapping techniques at the core of the college-level introductory course, in place of traditional content such as probability theory and the \"t\"-test. Several teachers and curriculum developers have been exploring ways to introduce simulation, randomization, and bootstrapping as teaching tools for the secondary and postsecondary levels. Courses such as the University of Minnesota's CATALST, Nathan Tintle and collaborators' \"Introduction to Statistical Investigations\", and the Lock team's \"Unlocking the Power of Data\", are curriculum projects based on Cobb's ideas. Other researchers have been exploring the development of informal inferential reasoning as a way to use these methods to build a better understanding of statistical inference.\n\nAnother recent direction is addressing the big data sets that are increasingly affecting or being contributed to in our daily lives. Statistician Rob Gould, creator of \"Data Cycle, The Musical\" dinner and theatre spectacular, outlines many of these types of data and encourages teachers to find ways to use the data and address issues around big data. According to Gould, curricula focused on big data will address issues of sampling, prediction, visualization, data cleaning, and the underlying processes that generate data, rather than traditionally emphasized methods of making statistical inferences such as hypothesis testing.\n\nDriving both of these changes is the increased role of computing in teaching and learning statistics. Some researchers argue that as the use of modeling and simulation increase, and as data sets become larger and more complex, students will need better and more technical computing skills. Projects such as MOSAIC have been creating courses that blend computer science, modeling, and statistics.\n\n\n\n\n\n"}
{"id": "27867508", "url": "https://en.wikipedia.org/wiki?curid=27867508", "title": "Svante Janson", "text": "Svante Janson\n\nSvante Janson (born 21 May 1955) is a Swedish mathematician. A member of the Royal Swedish Academy of Sciences since 1994, Janson has been the chaired professor of mathematics at Uppsala University since 1987.\n\nIn mathematical analysis, Janson has publications in functional analysis (especially harmonic analysis) and probability theory. In mathematical statistics, Janson has made contributions to the theory of U-statistics. In combinatorics, Janson has publications in probabilistic combinatorics, particularly random graphs and in the analysis of algorithms: In the study of random graphs, Janson introduced U-statistics and the Hoeffding decomposition.\n\nJanson has published four books and over 300 academic papers (). He has an Erdős number of 1.\n\nSvante Janson has already had a long career in mathematics, because he started research at a very young age.\nA child prodigy in mathematics, Janson took high-school and even university classes while in primary school. He was admitted in 1968 to Gothenburg University at age 12. After his 1968 matriculation at Uppsala University at age 13, Janson obtained the following degrees in mathematics: a \"candidate of philosophy\" (roughly an \"honours\" B.S. with a thesis) at age 14 (in 1970) and a doctor of philosophy at age 21–22 (in 1977). Janson's Ph.D. was awarded on his 22nd birthday. Janson's doctoral dissertation was supervised by Lennart Carleson, who had himself received his doctoral degree when he was 22 years old.\n\nAfter having earned his doctorate, Janson was a postdoc with the Mittag-Leffler Institute from 1978 to 1980. Thereafter he worked at Uppsala University. Janson's ongoing research earned him another PhD from Uppsala University in 1984 – this second doctoral degree being in mathematical statistics; the supervisor was Carl-Gustav Esseen.\n\nIn 1984, Janson was hired by Stockholm University as docent (roughly associate professor in the USA).\n\nIn 1985 Janson returned to Uppsala University, where he was named the chaired professor in mathematical statistics. In 1987 Janson became the chaired professor of mathematics at Uppsala university. Traditionally in Sweden, the \"chaired professor\" has had the role of a \"professor ordinarius\" in a German university (roughly combining the roles of research professor and director of graduate studies at a research university in the USA).\n\nSvante Janson's full name is \"Carl Svante Janson\".\n\nBesides being a member of the Royal Swedish Academy of Sciences (KVA), Svante Janson is a member of the Royal Society of Sciences in Uppsala. His thesis received the 1978 Sparre Award from the KVA. He received the 1994 Swedish medal for the best young mathematical scientist, the Göran Gustafsson Prize. Janson's former doctoral student, Ola Hössjer, received the Göran Gustafsson prize in 2009, becoming the first statistician so honored.\n\nIn December 2009, Janson received the Eva & Lars Gårding prize from the Royal Physiographic Society in Lund.\n\n\n\n\n"}
{"id": "16173250", "url": "https://en.wikipedia.org/wiki?curid=16173250", "title": "Synergetics coordinates", "text": "Synergetics coordinates\n\nSynergetics coordinates is Clifford Nelson's attempt to describe, from another mathematical point of view, Buckminster Fuller's '60 degree coordinate system' for understanding nature. Synergetics is the word Fuller used to label his approach to mathematics.\n\nA system of synergetics coordinates uses only one type of simplex (triangle, tetrahedron, pentachoron, ..., n-simplex) as space units, and in fact uses a regular simplex, rather like Cartesian coordinates use hypercubes (square, cube, tesseract, ..., n-cube.)\n\nThe n Synergetics coordinates axes are perpendicular to the n defining geometric objects that define a regular simplex; 2 end points for line segments, 3 lines for triangles, 4 planes for tetrahedrons etc.. The angles between the directions of the coordinate axes are Arc Cosine (-1/(n-1)). The coordinates can be positive or negative or zero and so can their sum. The sum of the n coordinates is the edge length of the regular simplex defined by moving the n geometric objects in increments of the height of the n-1 dimensional regular simplex that has an edge length of one. If the sum of the n coordinates is negative the triangle (n = 3) or tetrahedron (n = 4) is upside down and inside out.\n\nRegular triangular coordinates are in a grid of equilateral triangles and are of the form formula_1 such that formula_2 are equal to or greater than 0.\n\nRegular tetrahedral coordinates are in a Euclidean 3-space 'grid' of equilateral tetrahedra and are of the form formula_3 such that formula_4 are equal to or greater than 0.\n\n\nSec. 966.20;\nSec. 987.011; Vol. 1, Sec. 400.011 and Fig. 401.01.\n\n"}
{"id": "31482", "url": "https://en.wikipedia.org/wiki?curid=31482", "title": "Tangent", "text": "Tangent\n\nIn geometry, the tangent line (or simply tangent) to a plane curve at a given point is the straight line that \"just touches\" the curve at that point. Leibniz defined it as the line through a pair of infinitely close points on the curve. More precisely, a straight line is said to be a tangent of a curve at a point on the curve if the line passes through the point on the curve and has slope where \"f\" is the derivative of \"f\". A similar definition applies to space curves and curves in \"n\"-dimensional Euclidean space.\n\nAs it passes through the point where the tangent line and the curve meet, called the point of tangency, the tangent line is \"going in the same direction\" as the curve, and is thus the best straight-line approximation to the curve at that point.\n\nSimilarly, the tangent plane to a surface at a given point is the plane that \"just touches\" the surface at that point. The concept of a tangent is one of the most fundamental notions in differential geometry and has been extensively generalized; see Tangent space.\n\nThe word \"tangent\" comes from the Latin \"tangere\", 'to touch'.\n\nEuclid makes several references to the tangent ( \"ephaptoménē\") to a circle in book III of the \"Elements\" (c. 300 BC). In Apollonius work \"Conics\" (c. 225 BC) he defines a tangent as being \"a line such that no other straight line could\nfall between it and the curve\".\n\nArchimedes (c.  287 – c.  212 BC) found the tangent to an Archimedean spiral by considering the path of a point moving along the curve.\n\nIn the 1630s Fermat developed the technique of adequality to calculate tangents and other problems in analysis and used this to calculate tangents to the parabola. The technique of adeqality is similar to taking the difference between formula_1 and formula_2 and dividing by a power of formula_3. Independently Descartes used his method of normals based on the observation that the radius of a circle is always normal to the circle itself.\n\nThese methods led to the development of differential calculus in the 17th century. Many people contributed. Roberval discovered a general method of drawing tangents, by considering a curve as described by a moving point whose motion is the resultant of several simpler motions.\nRené-François de Sluse and Johannes Hudde found algebraic algorithms for finding tangents. Further developments included those of John Wallis and Isaac Barrow, leading to the theory of Isaac Newton and Gottfried Leibniz.\nAn 1828 definition of a tangent was \"a right line which touches a curve, but which when produced, does not cut it\". This old definition prevents inflection points from having any tangent. It has been dismissed and the modern definitions are equivalent to those of Leibniz who defined the tangent line as the line through a pair of infinitely close points on the curve.\n\nThe intuitive notion that a tangent line \"touches\" a curve can be made more explicit by considering the sequence of straight lines (secant lines) passing through two points, \"A\" and \"B\", those that lie on the function curve. The tangent at \"A\" is the limit when point \"B\" approximates or tends to \"A\". The existence and uniqueness of the tangent line depends on a certain type of mathematical smoothness, known as \"differentiability.\" For example, if two circular arcs meet at a sharp point (a vertex) then there is no uniquely defined tangent at the vertex because the limit of the progression of secant lines depends on the direction in which \"point \"B\"\" approaches the vertex.\n\nAt most points, the tangent touches the curve without crossing it (though it may, when continued, cross the curve at other places away from the point of tangent). A point where the tangent (at this point) crosses the curve is called an \"inflection point\". Circles, parabolas, hyperbolas and ellipses do not have any inflection point, but more complicated curves do have, like the graph of a cubic function, which has exactly one inflection point, or a sinusoid, which has two inflection points per each period of the sine.\n\nConversely, it may happen that the curve lies entirely on one side of a straight line passing through a point on it, and yet this straight line is not a tangent line. This is the case, for example, for a line passing through the vertex of a triangle and not intersecting it otherwise—where the tangent line does not exist for the reasons explained above. In convex geometry, such lines are called supporting lines.\n\nThe geometrical idea of the tangent line as the limit of secant lines serves as the motivation for analytical methods that are used to find tangent lines explicitly. The question of finding the tangent line to a graph, or the tangent line problem, was one of the central questions leading to the development of calculus in the 17th century. In the second book of his \"Geometry\", René Descartes of the problem of constructing the tangent to a curve, \"And I dare say that this is not only the most useful and most general problem in geometry that I know, but even that I have ever desired to know\".\n\nSuppose that a curve is given as the graph of a function, \"y\" = \"f\"(\"x\"). To find the tangent line at the point \"p\" = (\"a\", \"f\"(\"a\")), consider another nearby point \"q\" = (\"a\" + \"h\", \"f\"(\"a\" + \"h\")) on the curve. The slope of the secant line passing through \"p\" and \"q\" is equal to the difference quotient\n\nAs the point \"q\" approaches \"p\", which corresponds to making \"h\" smaller and smaller, the difference quotient should approach a certain limiting value \"k\", which is the slope of the tangent line at the point \"p\". If \"k\" is known, the equation of the tangent line can be found in the point-slope form:\n\nTo make the preceding reasoning rigorous, one has to explain what is meant by the difference quotient approaching a certain limiting value \"k\". The precise mathematical formulation was given by Cauchy in the 19th century and is based on the notion of limit. Suppose that the graph does not have a break or a sharp edge at \"p\" and it is neither plumb nor too wiggly near \"p\". Then there is a unique value of \"k\" such that, as \"h\" approaches 0, the difference quotient gets closer and closer to \"k\", and the distance between them becomes negligible compared with the size of \"h\", if \"h\" is small enough. This leads to the definition of the slope of the tangent line to the graph as the limit of the difference quotients for the function \"f\". This limit is the derivative of the function \"f\" at \"x\" = \"a\", denoted \"f\" ′(\"a\"). Using derivatives, the equation of the tangent line can be stated as follows:\n\nCalculus provides rules for computing the derivatives of functions that are given by formulas, such as the power function, trigonometric functions, exponential function, logarithm, and their various combinations. Thus, equations of the tangents to graphs of all these functions, as well as many others, can be found by the methods of calculus.\n\nCalculus also demonstrates that there are functions and points on their graphs for which the limit determining the slope of the tangent line does not exist. For these points the function \"f\" is \"non-differentiable\". There are two possible reasons for the method of finding the tangents based on the limits and derivatives to fail: either the geometric tangent exists, but it is a vertical line, which cannot be given in the point-slope form since it does not have a slope, or the graph exhibits one of three behaviors that precludes a geometric tangent.\n\nThe graph \"y\" = \"x\" illustrates the first possibility: here the difference quotient at \"a\" = 0 is equal to \"h\"/\"h\" = \"h\", which becomes very large as \"h\" approaches 0. This curve has a tangent line at the origin that is vertical.\n\nThe graph \"y\" = \"x\" illustrates another possibility: this graph has a \"cusp\" at the origin. This means that, when \"h\" approaches 0, the difference quotient at \"a\" = 0 approaches plus or minus infinity depending on the sign of \"x\". Thus both branches of the curve are near to the half vertical line for which \"y\"=0, but none is near to the negative part of this line. Basically, there is no tangent at the origin in this case, but in some context one may consider this line as a tangent, and even, in algebraic geometry, as a \"double tangent\".\n\nThe graph \"y\" = |\"x\"| of the absolute value function consists of two straight lines with different slopes joined at the origin. As a point \"q\" approaches the origin from the right, the secant line always has slope 1. As a point \"q\" approaches the origin from the left, the secant line always has slope −1. Therefore, there is no unique tangent to the graph at the origin. Having two different (but finite) slopes is called a \"corner\".\n\nFinally, since differentiability implies continuity, the contrapositive states \"discontinuity\" implies non-differentiability. Any such jump or point discontinuity will have no tangent line. This includes cases where one slope approaches positive infinity while the other approaches negative infinity, leading to an infinite jump discontinuity\n\nWhen the curve is given by \"y\" = \"f\"(\"x\") then the slope of the tangent is formula_7\nso by the point–slope formula the equation of the tangent line at (\"X\", \"Y\") is\nwhere (\"x\", \"y\") are the coordinates of any point on the tangent line, and where the derivative is evaluated at formula_9.\n\nWhen the curve is given by \"y\" = \"f\"(\"x\"), the tangent line's equation can also be found by using polynomial division to divide formula_10 by formula_11; if the remainder is denoted by formula_12, then the equation of the tangent line is given by\n\nWhen the equation of the curve is given in the form \"f\"(\"x\", \"y\") = 0 then the value of the slope can be found by implicit differentiation, giving\nThe equation of the tangent line at a point (\"X\",\"Y\") such that \"f\"(\"X\",\"Y\") = 0 is then\nThis equation remains true if formula_16 but formula_17 (in this case the slope of the tangent is infinite). If formula_18 the tangent line is not defined and the point (\"X\",\"Y\") is said singular.\nFor algebraic curves, computations may be simplified somewhat by converting to homogeneous coordinates. Specifically, let the homogeneous equation of the curve be \"g\"(\"x\", \"y\", \"z\") = 0 where \"g\" is a homogeneous function of degree \"n\". Then, if (\"X\", \"Y\", \"Z\") lies on the curve, Euler's theorem implies\nformula_19\nIt follows that the homogeneous equation of the tangent line is\nThe equation of the tangent line in Cartesian coordinates can be found by setting \"z\"=1 in this equation.\n\nTo apply this to algebraic curves, write \"f\"(\"x\", \"y\") as\nwhere each \"u\" is the sum of all terms of degree \"r\". The homogeneous equation of the curve is then\nApplying the equation above and setting \"z\"=1 produces\nas the equation of the tangent line. The equation in this form is often simpler to use in practice since no further simplification is needed after it is applied.\n\nIf the curve is given parametrically by\nthen the slope of the tangent is\ngiving the equation for the tangent line at formula_26 as\nIf formula_28 the tangent line is not defined. However, it may occur that the tangent line exists and may be computed from an implicit equation of the curve.\n\nThe line perpendicular to the tangent line to a curve at the point of tangency is called the \"normal line\" to the curve at that point. The slopes of perpendicular lines have product −1, so if the equation of the curve is \"y\" = \"f\"(\"x\") then slope of the normal line is\nand it follows that the equation of the normal line at (X, Y) is\nSimilarly, if the equation of the curve has the form \"f\"(\"x\", \"y\") = 0 then the equation of the normal line is given by\n\nIf the curve is given parametrically by\nthen the equation of the normal line is\n\nThe angle between two curves at a point where they intersect is defined as the angle between their tangent lines at that point. More specifically, two curves are said to be tangent at a point if they have the same tangent at a point, and orthogonal if their tangent lines are orthogonal.\n\nThe formulas above fail when the point is a singular point. In this case there may be two or more branches of the curve that pass through the point, each branch having its own tangent line. When the point is the origin, the equations of these lines can be found for algebraic curves by factoring the equation formed by eliminating all but the lowest degree terms from the original equation. Since any point can be made the origin by a change of variables, this gives a method for finding the tangent lines at any singular point.\n\nFor example, the equation of the limaçon trisectrix shown to the right is\nExpanding this and eliminating all but terms of degree 2 gives\nwhich, when factored, becomes\nSo these are the equations of the two tangent lines through the origin.\n\nWhen the curve is not self-crossing, the tangent at a reference point may still not be uniquely defined because the curve is not differentiable at that point although it is differentiable elsewhere. In this case the left and right derivatives are defined as the limits of the derivative as the point at which it is evaluated approaches the reference point from respectively the left (lower values) or the right (higher values). For example, the curve \"y\" = |\"x\" | is not differentiable at \"x\" = 0: its left and right derivatives have respective slopes –1 and 1; the tangents at that point with those slopes are called the left and right tangents.\n\nSometimes the slopes of the left and right tangent lines are equal, so the tangent lines coincide. This is true, for example, for the curve \"y\" = \"x\" , for which both the left and right derivatives at \"x\" = 0 are infinite; both the left and right tangent lines have equation \"x\" = 0.\n\nTwo circles of non-equal radius, both in the same plane, are said to be tangent to each other if they meet at only one point. Equivalently, two circles, with radii of \"r\" and centers at (\"x\", \"y\"), for \"i\" = 1, 2 are said to be tangent to each other if\n\n\nThe \"tangent plane\" to a surface at a given point \"p\" is defined in an analogous way to the tangent line in the case of curves. It is the best approximation of the surface by a plane at \"p\", and can be obtained as the limiting position of the planes passing through 3 distinct points on the surface close to \"p\" as these points converge to \"p\". More generally, there is a \"k\"-dimensional tangent space at each point of a \"k\"-dimensional manifold in the \"n\"-dimensional Euclidean space.\n\n\n"}
{"id": "25768005", "url": "https://en.wikipedia.org/wiki?curid=25768005", "title": "Tree-depth", "text": "Tree-depth\n\nIn graph theory, the tree-depth of a connected undirected graph \"G\" is a numerical invariant of \"G\", the minimum height of a Trémaux tree for a supergraph of \"G\". This invariant and its close relatives have gone under many different names in the literature, including vertex ranking number, ordered chromatic number, and minimum elimination tree height; it is also closely related to the cycle rank of directed graphs and the star height of regular languages. Intuitively, where the treewidth graph width parameter measures how far a graph is from being a tree, this parameter measures how far a graph is from being a star.\n\nThe tree-depth of a graph \"G\" may be defined as the minimum height of a forest \"F\" with the property that every edge of \"G\" connects a pair of nodes that have an ancestor-descendant relationship to each other in \"F\". If \"G\" is connected, this forest must be a single tree; it need not be a subgraph of \"G\", but if it is, it is a Trémaux tree for \"G\".\n\nThe set of ancestor-descendant pairs in \"F\" forms a trivially perfect graph, and the height of \"F\" is the size of the largest clique in this graph. Thus, the tree-depth may alternatively be defined as the size of the largest clique in a trivially perfect supergraph of \"G\", mirroring the definition of treewidth as one less than the size of the largest clique in a chordal supergraph of \"G\".\n\nAnother definition is the following:\n\nformula_1\n\nwhere \"V\" is the set of vertices of \"G\" and the formula_2 are the connected components of \"G\". This definition mirrors the definition of cycle rank of directed graphs, which uses strong connectivity and strongly connected components in place of undirected connectivity and connected components.\n\nTree-depth may also be defined using a form of graph coloring. A centered coloring of a graph is a coloring of its vertices with the property that every connected induced subgraph has a color that appears exactly once. Then, the tree-depth is the minimum number of colors in a centered coloring of the given graph. If \"F\" is a forest of height \"d\" with the property that every edge of \"G\" connects an ancestor and a descendant in the tree, then a centered coloring of \"G\" using \"d\" colors may be obtained by coloring each vertex by its distance from the root of its tree in \"F\".\n\nFinally, one can define this in terms of a pebble game, or more precisely as a cops and robber game. Consider the following game, played on an undirected graph. There are two players, a robber and a cop. The robber has one pebble he can move along the edges of the given graph. The cop has an unlimited number of pebbles, but she wants to minimize the amount of pebbles she uses. The cop cannot move a pebble after it has been placed on the graph. The game proceeds as follows. The robber places his pebble. The cop then announces where she wants to place a new cop pebble. The robber can then move his pebble along edges, but not through occupied vertices. The game is over when the cop player places a pebble on top of the robber pebble. The tree-depth of the given graph is the minimum number of pebbles needed by the cop to guarantee a win. For a star graph, two pebbles suffice: the strategy is to place a pebble at the center vertex, forcing the robber to one arm, and then to place the remaining pebble on the robber. For a path with formula_3 vertices, the cop uses a binary search strategy, which guarantees that at most formula_4 pebbles are needed.\n\nThe tree-depth of a complete graph equals its number of vertices. For, in this case, the only possible forest \"F\" for which every pair of vertices are in an ancestor-descendant relationship is a single path. Similarly, the tree-depth of a complete bipartite graph \"K\" is min(\"x\",\"y\") + 1. For, the nodes that are placed at the leaves of the forest \"F\" must have at least min(\"x\",\"y\") ancestors in \"F\". A forest achieving this min(\"x\",\"y\") + 1 bound may be constructed by forming a path for the smaller side of the bipartition, with each vertex on the larger side of the bipartition forming a leaf in \"F\" connected to the bottom vertex of this path.\n\nThe tree-depth of a path with \"n\" vertices is exactly formula_4. A forest \"F\" representing this path with this depth may be formed by placing the midpoint of the path as the root of \"F\" and recursing within the two smaller paths on either side of it.\n\nAny \"n\"-vertex forest has tree-depth O(log \"n\"). For, in a forest, one can always find a constant number of vertices the removal of which leaves a forest that can be partitioned into two smaller subforests with at most 2\"n\"/3 vertices each. By recursively partitioning each of these two subforests, we can easily derive a logarithmic upper bound on the tree-depth. The same technique, applied to a tree decomposition of a graph, shows that, if the treewidth of an \"n\"-vertex graph \"G\" is \"t\", then the tree-depth of \"G\" is O(\"t\" log \"n\"). Since outerplanar graphs, series-parallel graphs, and Halin graphs all have bounded treewidth, they all also have at most logarithmic tree-depth.\n\nIn the other direction, the treewidth of a graph is at most equal to its tree-depth. More precisely, the treewidth is at most the pathwidth, which is at most one less than the tree-depth.\n\nA minor of a graph \"G\" is another graph formed from a subgraph of \"G\" by contracting some of its edges. Tree-depth is monotonic under minors: every minor of a graph \"G\" has tree-depth at most equal to the tree-depth of \"G\" itself. Thus, by the Robertson–Seymour theorem, for every fixed \"d\" the set of graphs with tree-depth at most \"d\" has a finite set of forbidden minors.\n\nIf \"C\" is a class of graphs closed under taking graph minors, then the graphs in \"C\" have tree-depth formula_6 if and only if \"C\" does not include all the path graphs.\n\nAs well as behaving well under graph minors, tree-depth has close connections to the theory of induced subgraphs of a graph. Within the class of graphs that have tree-depth at most \"d\" (for any fixed integer \"d\"), the relation of being an induced subgraph forms a well-quasi-ordering. The basic idea of the proof that this relation is a well-quasi-ordering is to use induction on \"d\"; the forests of height \"d\" may be interpreted as sequences of forests of height \"d\" − 1 (formed by deleting the roots of the trees in the height-\"d\" forest) and Higman's lemma can be used together with the induction hypothesis to show that these sequences are well-quasi-ordered.\n\nWell-quasi-ordering implies that any property of graphs that is monotonic with respect to induced subgraphs has finitely many forbidden induced subgraphs, and therefore may be tested in polynomial time on graphs of bounded tree-depth. The graphs with tree-depth at most \"d\" themselves also have a finite set of forbidden induced subgraphs.\n\nIf \"C\" is a class of graphs with bounded degeneracy, the graphs in \"C\" have bounded tree-depth if and only if there is a path graph that cannot occur as an induced subgraph of a graph in \"C\".\n\nComputing tree-depth is computationally hard: the corresponding decision problem is NP-complete. The problem remains NP-complete for bipartite graphs , as well as for chordal graphs.\n\nOn the positive side, tree-depth can be computed in polynomial time on interval graphs, as well as on permutation, trapezoid, circular-arc, circular permutation graphs, and cocomparability graphs of bounded dimension. For undirected trees, tree-depth can be computed in linear time.\n\nBecause tree-depth is monotonic under graph minors, it is fixed-parameter tractable: there is an algorithm for computing tree-depth running in time is formula_8, where \"d\" is the depth of the given graph and \"n\" is its number of vertices. Thus, for every fixed value of \"d\", the problem of testing whether the tree-depth is at most \"d\" can be solved in polynomial time. More specifically, the dependence on \"n\" in this algorithm can be made linear, by the following method: compute a depth first search tree, and test whether this tree's depth is greater than 2. If so, the tree-depth of the graph is greater than \"d\" and the problem is solved. If not, the shallow depth first search tree can be used to construct a tree decomposition with bounded width, and standard dynamic programming techniques for graphs of bounded treewidth can be used to compute the depth in linear time.\n\nIt is also possible to compute the tree-depth exactly, for graphs whose tree-depth may be large, in time \"O\"(\"c\") for a constant \"c\" slightly smaller than 2.\n\n"}
{"id": "45107264", "url": "https://en.wikipedia.org/wiki?curid=45107264", "title": "Vector addition system", "text": "Vector addition system\n\nA vector addition system (VAS) is one of several mathematical modeling languages for the description of distributed systems. Vector addition systems were introduced by Richard M. Karp and Raymond E. Miller in 1969, and generalized to vector addition systems with states (VASS) by John E. Hopcroft and Jean-Jacques Pansiot in 1979. Both VAS and VASS are equivalent in many ways to Petri nets introduced earlier by Carl Adam Petri. \n\nA \"vector addition system\" consists of a finite set of integer vectors. An initial vector is seen as the initial values of multiple counters, and the vectors of the VAS are seen as updates. These counters may never drop below zero. More precisely, given an initial vector with non negative values, the vectors of the VAS can be added componentwise, given that every intermediate vector has non negative values. A \"vector addition system with states\" is a VAS equipped with control states. More precisely, it is a finite directed graph with arcs labelled by integer vectors. VASS have the same restriction that the counter values should never drop below zero.\n\n\n\n"}
{"id": "43414656", "url": "https://en.wikipedia.org/wiki?curid=43414656", "title": "Vincent average", "text": "Vincent average\n\nIn applied statistics, Vincentization was described by Ratcliff (1979), and is named after biologist S. B. Vincent (1912), who used something very similar to it for constructing learning curves at the beginning of the 1900s. It basically consists of averaging \"n\" > 2 subjects' estimated or elicited quantile functions in order to define group quantiles from which \"F\" can be constructed.\n\nTo cast it in its greatest generality, let \"F\"..., \"F\" represent arbitrary (empirical or theoretical) distribution functions and define their corresponding quantile functions by\n\nThe Vincent average of the \"F\"s is then computed as\n\nwhere the non-negative numbers \"w\"..., \"w\" have a sum of 1.\n"}
{"id": "48754551", "url": "https://en.wikipedia.org/wiki?curid=48754551", "title": "William Bigelow Easton", "text": "William Bigelow Easton\n\nWilliam Bigelow Easton is a mathematician who proved Easton's theorem about the possible values of the continuum function.\n"}
{"id": "58552301", "url": "https://en.wikipedia.org/wiki?curid=58552301", "title": "Σ-Algebra of τ-past", "text": "Σ-Algebra of τ-past\n\nThe σ-algebra of τ-past, (also named stopped σ-algebra, stopped σ-field, or σ-field of τ-past) is a σ-algebra associated with a stopping time in the theory of stochastic processes, a branch of probability theory.\nLet formula_1 be a stopping time on the filtered probability space formula_2. Then the σ-algebra\n\nis called the σ-algebra of τ-past.\n\nIs formula_4 are two stopping times and\nalmost surely, then\n\nA stopping time formula_1 is always formula_8-measurable\n"}
