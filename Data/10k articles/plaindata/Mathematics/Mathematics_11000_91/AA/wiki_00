{"id": "15067130", "url": "https://en.wikipedia.org/wiki?curid=15067130", "title": "Antoine's necklace", "text": "Antoine's necklace\n\nIn mathematics Antoine's necklace is a topological embedding of the Cantor set in 3-dimensional Euclidean space, whose complement is not simply connected. It also serves as a counterexample to the claim that all Cantor spaces are ambiently homeomorphic to each other. It was discovered by .\n\nAntoine's necklace is constructed iteratively like so: Begin with a solid torus \"A\" (iteration 0). Next, construct a \"necklace\" of smaller, linked tori that lie inside \"A\". This necklace is \"A\" (iteration 1). Each torus composing \"A\" can be replaced with another smaller necklace as was done for \"A\". Doing this yields \"A\" (iteration 2).\n\nThis process can be repeated a countably infinite number of times to create an \"A\" for all \"n\". Antoine's necklace \"A\" is defined as the intersection of all the iterations.\n\nSince the solid tori are chosen to become arbitrarily small as the iteration number increases, the connected components of \"A\" must be single points. It is then easy to verify that \"A\" is closed, dense-in-itself, and totally disconnected, having the cardinality of the continuum. This is sufficient to conclude that as an abstract metric space \"A\" is homeomorphic to the Cantor set.\n\nHowever, as a subset of Euclidean space \"A\" is not ambiently homeomorphic to the standard Cantor set \"C\", embedded in R on a line segment. That is, there is no bi-continuous map from R → R that carries \"C\" onto \"A\". To show this, suppose there was such a map \"h\" : R → R, and consider a loop \"k\" that is interlocked with the necklace. \"k\" cannot be continuously shrunk to a point without touching \"A\" because two loops cannot be continuously unlinked. Now consider any loop \"j\" disjoint from \"C\". \"j\" can be shrunk to a point without touching \"C\" because we can simply move it through the gap intervals. However, the loop \"g\" = \"h\"(\"k\") is a loop that \"cannot\" be shrunk to a point without touching \"C\", which contradicts the previous statement. Therefore, \"h\" cannot exist.\n\nAntoine's necklace was used by to construct Antoine's horned sphere (similar to but not the same as Alexander's horned sphere).\n\n\n"}
{"id": "462705", "url": "https://en.wikipedia.org/wiki?curid=462705", "title": "Central angle", "text": "Central angle\n\nCentral angles are subtended by an arc between those two points, and the arc length is the central angle of a circle of radius one (measured in radians). The central angle is also known as the arc's angular distance.\n\nThe size of a central angle is or (radians). When defining or drawing a central angle, in addition to specifying the points and , one must specify whether the angle being defined is the convex angle (<180°) or the reflex angle (>180°). Equivalently, one must specify whether the movement from point to point is clockwise or counterclockwise.\n\nIf the intersection points and of the legs of the angle with the circle form a diameter, then is a straight angle. (In radians, .)\n\nLet be the minor arc of the circle between points and , and let be the radius of the circle.\n\nIf the central angle is subtended by , then \nIf the central angle is not subtended by the minor arc , then is a reflex angle and\n\nIf a tangent at and a tangent at intersect at the exterior point , then denoting the center as , the angles (convex) and are supplementary (sum to 180°).\n\nA regular polygon with sides has a circumscribed circle upon which all its vertices lie, and the center of the circle is also the center of the polygon. The central angle of the regular polygon is formed at the center by the radii to two adjacent vertices. The measure of this angle is formula_3\n\n\n"}
{"id": "1010712", "url": "https://en.wikipedia.org/wiki?curid=1010712", "title": "Closed convex function", "text": "Closed convex function\n\nIn mathematics, a function formula_1 is said to be closed if for each formula_2, the sublevel set\nformula_3\nis a closed set.\n\nEquivalently, if the epigraph defined by\nformula_4\nis closed, then the function formula_5 is closed.\n\nThis definition is valid for any function, but most used for convex functions. A proper convex function is closed if and only if it is lower semi-continuous. For a convex function which is not proper there is disagreement as to the definition of the \"closure\" of the function.\n\n"}
{"id": "1815224", "url": "https://en.wikipedia.org/wiki?curid=1815224", "title": "Coherent duality", "text": "Coherent duality\n\nIn mathematics, coherent duality is any of a number of generalisations of Serre duality, applying to coherent sheaves, in algebraic geometry and complex manifold theory, as well as some aspects of commutative algebra that are part of the 'local' theory.\n\nThe historical roots of the theory lie in the idea of the adjoint linear system of a linear system of divisors in classical algebraic geometry. This was re-expressed, with the advent of sheaf theory, in a way that made an analogy with Poincaré duality more apparent. Then according to a general principle, Grothendieck's relative point of view, the theory of Jean-Pierre Serre was extended to a proper morphism; Serre duality was recovered as the case of the morphism of a non-singular projective variety (or complete variety) to a point. The resulting theory is now sometimes called Serre–Grothendieck–Verdier duality, and is a basic tool in algebraic geometry. A treatment of this theory, \"Residues and Duality\" (1966) by Robin Hartshorne, became an accessible reference. One concrete spin-off was the Grothendieck residue.\n\nTo go beyond proper morphisms, as for the versions of Poincaré duality that are not for closed manifolds, requires some version of the \"compact support\" concept. This was addressed in SGA2 in terms of local cohomology, and Grothendieck local duality; and subsequently. The Greenlees–May duality, first formulated in 1976 by Ralf Strebel and in 1978 by Eben Matlis, is part of the continuing consideration of this area.\n\nWhile Serre duality uses a line bundle or invertible sheaf as a dualizing sheaf, the general theory (it turns out) cannot be quite so simple. (More precisely, it can, but at the cost of the Gorenstein ring condition.) In a characteristic turn, Grothendieck reformulated general coherent duality as the existence of a right adjoint functor \"f\", called \"twisted\" or \"exceptional inverse image functor\", to a higher direct image with compact support functor \"Rf\".\n\n\"Higher direct images\" are a sheafified form of sheaf cohomology in this case with proper (compact) support; they are bundled up into a single functor by means of the derived category formulation of homological algebra (introduced with this case in mind). In case f is proper \"Rf\" = \"Rf\" is itself a right adjoint, to the \"inverse image\" functor \"f\". The \"existence theorem\" for the twisted inverse image is the name given to the proof of the existence for what would be the counit for the comonad of the sought-for adjunction, namely a natural transformation\n\nwhich is denoted by \"Tr\" (Hartshorne) or \"∫\" (Verdier). It is the aspect of the theory closest to the classical meaning, as the notation suggests, that duality is defined by integration.\n\nTo be more precise, \"f\" exists as an exact functor from a derived category of quasi-coherent sheaves on \"Y\", to the analogous category on \"X\", whenever\n\nis a proper or quasi projective morphism of noetherian schemes, of finite Krull dimension. From this the rest of the theory can be derived: dualizing complexes pull back via \"f\", the Grothendieck residue symbol, the dualizing sheaf in the Cohen–Macaulay case.\n\nIn order to get a statement in more classical language, but still wider than Serre duality, Hartshorne (\"Algebraic Geometry\") uses the Ext functor of sheaves; this is a kind of stepping stone to the derived category.\n\nThe classical statement of Grothendieck duality for a projective or proper morphism formula_1 of noetherian schemes of finite dimension, found in Hartshorne (\"Residues and duality\") is the following quasi-isomorphism\n\nfor \"F\" a bounded above complex of \"O\"-modules with quasi-coherent cohomology and \"G\" a bounded below complex of \"O\"-modules with coherent cohomology. Here the \"Hom\"'s are the sheaf of homomorphisms.\n\nOver the years, several approaches for constructing the formula_3 pseudofunctor emerged. One quite recent successful approach is based on the notion of a rigid dualizing complex. This notion was first defined by Van den Bergh in a noncommutative context. The construction is based on a variant of derived Hochschild cohomology (Shukla cohomology): Let \"k\" be a commutative ring, and let \"A\" be a commutative \"k-\"algebra. There is a functor formula_5 which takes a cochain complex \"M\" to an object formula_5 in the derived category over \"A\".\n\nAsumming \"A\" is noetherian, a rigid dualizing complex over \"A\" relative to \"k\" is by definition a pair formula_7 where \"R\" is a dualizing complex over \"A\" which has finite flat dimension over \"k\", and where\nformula_8 is an isomorphism in the derived category \"D(A)\". If such a rigid dualizing complex exists, then it is unique in a strong sense.\n\nAssuming \"A\" is a localization of a finite type \"k\"-algebra, existence of a rigid dualizing complex over \"A\" relative to \"k\" was first proved by Yekutieli and Zhang assuming \"k\" is a regular noetherian ring of finite Krull dimension, and by Avramov, Iyengar and Lipman assuming \"k\" is a Gorenstein ring of finite Krull dimension and \"A\" is of finite flat dimension over \"A\".\n\nIf \"X\" is a scheme of finite type over \"k\", one can glue the rigid dualizing complexes that its affine pieces have, and obtain a rigid dualizing complex formula_9. Once one establishes a global existence of a rigid dualizing complex, given a map formula_10 of schemes over \"k\", one can define formula_11, where for a scheme \"X\", we set formula_12.\n\nThe dualizing complex for a projective variety formula_13 is given by the complex\nConsider the projective variety\nWe can compute formula_16 using a resolution formula_17 by locally free sheaves. This is given by the complex\nSince formula_19 we have that\nThis is the complex\n\n\n"}
{"id": "2156387", "url": "https://en.wikipedia.org/wiki?curid=2156387", "title": "Common Algebraic Specification Language", "text": "Common Algebraic Specification Language\n\nThe Common Algebraic Specification Language (CASL) is a general-purpose specification language based on first-order logic with induction. Partial functions and subsorting are also supported.\n\nCASL has been designed by CoFI, the Common Framework Initiative, with the aim to subsume many existing specification languages.\n\nCASL comprises four levels:\n\nThe four levels are orthogonal to each other. In particular, it is possible to use CASL structured and architectural specifications and libraries with logics other than CASL. For this purpose, the logic has to be formalized as an institution. This feature is also used by the CASL extensions.\n\nSeveral extensions of CASL have been designed: \n\n"}
{"id": "10063692", "url": "https://en.wikipedia.org/wiki?curid=10063692", "title": "Constant-Q transform", "text": "Constant-Q transform\n\nIn mathematics and signal processing, the constant-Q transform transforms a data series to the frequency domain. It is related to the Fourier transform and very closely related to the complex Morlet wavelet transform.\n\nThe transform can be thought of as a series of logarithmically spaced filters \"f\", with the \"k\"-th filter having a spectral width \"δf\" equal to a multiple of the previous filter's width:\n\nwhere \"δf\" is the bandwidth of the \"k\"-th filter, \"f\" is the central frequency of the lowest filter, and \"n\" is the number of filters per octave.\n\nThe short-time Fourier transform of \"x\"[\"n\"] for a frame shifted to sample \"m\" is calculated as follows:\n\nGiven a data series sampled at \"f\" = 1/\"T\", \"T\" being the sampling period of our data, for each frequency bin we can define the following:\n\nThe equivalent transform kernel can be found by using the following substitutions:\n\nAfter these modifications, we are left with\n\nThe direct calculation of the constant-Q transform is slow when compared against the fast Fourier transform (FFT). However, the FFT can itself be employed, in conjunction with the use of a kernel, to perform the equivalent calculation but much faster.\n\nIn general, the transform is well suited to musical data, and this can be seen in some of its advantages compared to the fast Fourier transform. As the output of the transform is effectively amplitude/phase against log frequency, fewer frequency bins are required to cover a given range effectively, and this proves useful where frequencies span several octaves. As the range of human hearing covers approximately ten octaves from 20 Hz to around 20 kHz, this reduction in output data is significant.\n\nThe transform exhibits a reduction in frequency resolution with higher frequency bins, which is desirable for auditory applications. The transform mirrors the human auditory system, whereby at lower-frequencies spectral resolution is better, whereas temporal resolution improves at higher frequencies. At the bottom of the piano scale (about 30 Hz), a difference of 1 semitone is a difference of approximately 1.5 Hz, whereas at the top of the musical scale (about 5 kHz), a difference of 1 semitone is a difference of approximately 200 Hz. So for musical data the exponential frequency resolution of constant-Q transform is ideal.\n\nIn addition, the harmonics of musical notes form a pattern characteristic of the timbre of the instrument in this transform. Assuming the same relative strengths of each harmonic, as the fundamental frequency changes, the relative position of these harmonics remains constant. This can make identification of instruments much easier. The constant Q transform can also be used for automatic recognition of musical keys based on accumulated chroma content.\n\nRelative to the Fourier transform, implementation of this transform is more tricky. This is due to the varying number of samples used in the calculation of each frequency bin, which also affects the length of any windowing function implemented.\n\nAlso note that because the frequency scale is logarithmic, there is no true zero-frequency / DC term present, perhaps limiting possible utility of the transform.\n"}
{"id": "52851946", "url": "https://en.wikipedia.org/wiki?curid=52851946", "title": "Cool S", "text": "Cool S\n\nThe \"Cool S\", also known as Superman S, Stüssy S, Super S, Pointy S and Graffiti S, as well as many other names, is a graffiti signature of popular culture that is typically doodled on children's notebooks or graffitied on walls.\n\nThe Cool S consists of 14 line segments, forming a stylized, pointed S-shape. The tails of the S appear to link underneath so that it loops around on itself in the same way as the infinity symbol does. The Cool S has no reflection symmetry, but has 2-fold rotational symmetry and tessellates with squares. As illustrated, a common way to draw the shape begins with two sets of three parallel, vertical lines, one above the other.\n\nThe origin of the Cool S is unclear. The name 'Superman S' comes from a belief that it was a symbol for Superman, whose costume features a stylised S in a diamond shape, but that shape is quite different. Similarly, the name 'Stussy S' is from a theory that it might be a symbol of the American surfwear company Stüssy.\n\n"}
{"id": "6876683", "url": "https://en.wikipedia.org/wiki?curid=6876683", "title": "Core-Plus Mathematics Project", "text": "Core-Plus Mathematics Project\n\nCore-Plus Mathematics is a high school mathematics program consisting of a four-year series of print and digital student textbooks and supporting materials for teachers, developed by the Core-Plus Mathematics Project (CPMP) at Western Michigan University, with funding from the National Science Foundation. Development of the program started in 1992. The first edition, entitled \"Contemporary Mathematics in Context: A Unified Approach\", was completed in 1995. The third edition, entitled \"Core-Plus Mathematics: Contemporary Mathematics in Context\", was published by McGraw-Hill Education in 2015.\n\nThe first edition of \"Core-Plus Mathematics\" was designed to meet the curriculum, teaching, and assessment standards from the National Council of Teachers of Mathematics and the broad goals outlined in the National Research Council report, \"Everybody Counts: A Report to the Nation on the Future of Mathematics Education\". Later editions were designed to also meet the American Statistical Association \"Guidelines for Assessment and Instruction in Statistics Education\" (GAISE) and most recently the standards for mathematical content and practice in the \"Common Core State Standards for Mathematics\" (CCSSM).\n\nThe program puts an emphasis on teaching and learning mathematics through mathematical modeling and mathematical inquiry. Each year, students learn mathematics in four interconnected strands: algebra and functions, geometry and trigonometry, statistics and probability, and discrete mathematical modeling.\n\nThe program originally comprised three courses, intended to be taught in grades 9 through 11. Later, authors added a fourth course intended for college-bound students.\n\nThe course was re-organized around interwoven strands of algebra and functions, geometry and trigonometry, statistics and probability, and discrete mathematics. Lesson structure was updated, and technology tools, including \"CPMP-Tools\" software was introduced.\n\nThe course was aligned with the Common Core State Standards (CCSS) mathematical practices and content expectations. Expanded and enhanced Teacher's Guides include a CCSS pathway and a CPMP pathway through each unit. Course 4 was split into two versions: one called \"Preparation for Calculus\", for STEM-oriented students, and an alternative course, \"Transition to College Mathematics and Statistics\" (TCMS), for college-bound students whose intended program of study does not require calculus.\n\nProject and independent evaluations and many research studies have been conducted on \"Core-Plus Mathematics\", including content analyses, case studies, surveys, small- and large-scale comparison studies, research reviews, and a longitudinal study.\n\nThere are multiple research studies and evaluations in which students using \"Core-Plus Mathematics\" performed significantly better than comparison students on assessments of conceptual understanding, problem solving, and applications, and results were mixed for performance on assessments of by-hand calculation skills. Some of these studies were funded by the National Science Foundation, the same organization that funded the development of \"Core-Plus Mathematics\" program.\n\nA three-part study of \"Core-Plus Mathematics\" and more conventional curricula were reported by researchers at the University of Missouri–Columbia. The research was conducted as part of the \"Comparing Options in Secondary Mathematics: Investigating Curricula\" project, supported by the National Science Foundation under REC-0532214. The research was reported in the March and July 2013 issues of the \"Journal for Research in Mathematics Education\" and in the December 2013 issue of the \"International Journal of Science and Mathematics Education\". The three studies examined student achievement in schools in 5 geographically dispersed states. The first study involved 2,161 students in 10 schools in first-year high school mathematics courses, the second study involved 3,258 students in 11 schools in second-year mathematics courses, and the third study involved 2,242 students in 10 schools in third-year mathematics courses. Results in the first study showed that \"Core-Plus Mathematics\" students scored significantly higher on all three end-of-year outcome measures: a test of common objectives, a problem solving and reasoning test, and a standardized achievement test. Results in the second study showed that \"Core-Plus Mathematics\" students scored significantly higher on a standardized achievement test, with no differences on the other measures. Results in the third study showed that \"Core-Plus Mathematics\" students scored significantly higher on a test of common objectives, with no differences on the other measure.\n\nA study conducted by Schoen and Hirsch, two authors of \"Core-Plus Mathematics\", reported that students using early versions of \"Core-Plus Mathematics\" did as well as or better than those in traditional single-subject curricula on all measures except paper-and-pencil algebra skills.\n\nA study on field-test versions of \"Core-Plus Mathematics\", supported by a grant from the National Science Foundation (Award MDR 9255257) and published in 2000 in the \"Journal for Research in Mathematics Education\", reported that students using the first field-test versions of \"Core-Plus Mathematics\" scored significantly better on tests of conceptual understanding and problem solving, while Algebra II students in conventional programs scored significantly better on a test of paper-and-pencil procedures.\n\nOther studies reported that \"Core-Plus Mathematics\" students displayed qualities such as engagement, eagerness, communication, flexibility, and curiosity to a much higher degree than did students who studied from more conventional programs. A review of research in 2008 concluded that there were modest effects for \"Core-Plus Mathematics\" on mostly standardized tests of mathematics.\n\nWith regard to achievement of students in minority groups, an early peer-reviewed paper documenting the performance of students from under-represented groups using \"Core-Plus Mathematics\" reported that at the end of each of Course 1, Course 2, and Course 3, the posttest means on standardized mathematics achievement tests of \"Core-Plus Mathematics\" students in all minority groups (African Americans, Asian Americans, Hispanics, and Native/Alaskan Americans) were greater than those of the national norm group at the same pretest levels. Hispanics made the greatest pretest to posttest gains at the end of each course. A later comparative study reported that Hispanic high school students using \"Core-Plus Mathematics\" made modest gains compared to the performance of students with other demographic backgrounds.\n\nRegarding preparation for college, studies of SAT and ACT test results reported that \"Core-Plus Mathematics\" students performed significantly better than comparison students on the SAT and performed as well on the ACT. Several studies examined the subsequent college mathematics performance of students who used different high school textbook series. These studies did not detect any differential effect of high school curriculum on placement in college mathematics courses, in subsequent performance, or in course-taking patterns.\n\nEdReports, an independent nonprofit, recently completed evidenced-based reviews of K-12 instructional materials. In their analysis of Core-Plus Mathematics Courses 1-3, the three-year core program was found to meet expectations for alignment to the high school Common Core State Standards for Mathematics in terms of content, focus, and coherence, and in terms of rigor and mathematical practices. The Core-Plus Mathematics instructional materials also met EdReports criteria that the materials are well designed and reflect effective lesson structure and pacing.\n\nIn an in-depth analysis by The Center for Research on Reform in Education at Johns Hopkins University, Core-Plus Mathematics was given a \"moderate\" evidence rating, and is the only comprehensive three-year high school mathematics program to be rated at any level (strong, moderate, or promising) for meeting federal ESSA Standards for Evidence in terms of promoting student achievement.\n\nIn terms of core content development, a study comparing the development of quadratic equations in the Korean national curriculum and \"Core-Plus Mathematics\" found that some quadratic equation topics are developed earlier in Korean textbooks, while \"Core-Plus Mathematics\" includes more problems requiring explanations, various representations, and higher cognitive demand.\n\nSeveral studies have analyzed the teacher’s role in \"Core-Plus Mathematics\".\n\nIn November 1999, David Klein, professor of mathematics at California State University, Northridge, sent an open letter to the U.S. Department of Education, in response to the U.S. Department of Education Expert Panel in Mathematics and Science designation of \"Core-Plus Mathematics\" as \"exemplary.\" Klein's open letter urged the Department of Education to withdraw its recommendations of the several reform mathematics programs including \"Core-Plus Mathematics\". The letter was co-signed by more than 200 American scientists and mathematicians. Other mathematicians were critical of the letter and were concerned that the letter gives the false impression that the mathematical community agrees with the letter’s conclusions. “In fact, there is no such clear-cut consensus on the issues the letter raises.”\n\nProf. Klein asserts that the mathematics programs criticized by the open letter had common features: they overemphasized data analysis and statistics, while de-emphasizing far more important areas of arithmetic and algebra. Many of the \"higher-order thinking projects\" turned out to be just aimless activities. The programs were obsessed with electronic calculators, and basic skills were disparaged.\n\nSpecifically, \"Core-Plus Mathematics\" was criticized for exhibiting \"too shallow a coverage of traditional algebra, and a focus on highly contextualized work\".\n\nR. James Milgram, Professor of Mathematics at Stanford University, analyzed the program's effect on students in a top-performing high school. According to Milgram, \"...there was no measure represented in the survey, such as ACT scores, SAT Math scores, grades in college math courses, level of college math courses attempted, where the students even met, let alone surpassed the comparison group [which used a more traditional program].\"\n\nOne of the first schools to pilot Core-Plus was Andover High School in Bloomfield Hills, Michigan, which was ranked one of America's \"100 best\" high schools. Andover stopped traditional mathematics in 1994 and began using Core-Plus Mathematics.\n\nA survey conducted in 1997 of Andover graduates found that 96 percent of students who returned the survey said they were placed into “remedial math” in college. In a neighboring school, 62 percent of the students who returned the survey took remedial math in college. Activism by a group of parents caused Andover to return to offering a traditional math option. By 2000, half of students at Andover were taking Core-Plus and the other half were taking traditional math.\n\nStudents commented on the survey that Core Plus was one of the worst math programs and a waste of their time. They lamented never being taught \"any of the basics and most are suffering in college math courses\". They found themselves \"completely unprepared\" for understanding college math.\n\nThe survey study has been criticized for involving a self-selected sample, self-reported data, and biased survey methods. Data provided by the University of Michigan registrar at this same time indicated that in collegiate mathematics courses at the University of Michigan graduates of Core-Plus did as well as or better than graduates of a traditional mathematics curriculum. A later study (see below) found that graduates of the Core-Plus curriculum entering Michigan State University have placed into increasingly lower level mathematics courses as the implementation of the curriculum has progressed. This study and the published report have been criticized for design flaws and for drawing conclusions that are not supported by the data.\n\nIn 2006, Richard O. Hill and Thomas H. Parker from Michigan State University (MSU) evaluated the effectiveness of the Core-Plus Mathematics Project in preparing the students for subsequent university mathematics. R. Hill and T. Parker analyzed the college mathematics records of students arriving at MSU from four high schools that implemented the Core-Plus Mathematics program between 1996 and 1999. They found a \"disconnect\" between the mathematics expectations that students encounter in K-12 education and those that they encounter in college. The effectiveness of Core-Plus and the other NSF-funded high school curricula programs became a significant issue for college mathematics faculty.\n\nCore-Plus students placed into, and enrolled in, increasingly lower level courses. The percentages of students who eventually passed a technical calculus course showed a statistically significant decline averaging 27 percent a year; this trend was accompanied by an obvious and statistically significant increase in percentages of students who placed into low-level and remedial algebra courses. Except for some top students, graduates of Core-Plus mathematics were struggling in college mathematics, earning below average grades. They were less well prepared than either graduates in the Control group (who came from a broad mix of curricula) or graduates of their own high schools before the implementation of Core-Plus mathematics.\n\nIn 2009 professor of mathematics at the University of California in San Diego, Guershon Harel reviewed four high-school mathematics programs. The examined programs included Core-Plus Courses 1, 2, and 3. The examination focused on two topics in algebra and one topic in geometry, deemed by Prof. Harel central to the high school curriculum. The examination was intended \"to ensure these topics are coherently developed, completely covered, mathematically correct, and provide students a solid foundation for further study in mathematics\".\n\nFrom the outset, Prof. Harel noted that the content presentation in Core-Plus program is unusual in that its instructional units, from the start to the end, are made of word problems involving \"real-life\" situations. This structure is reflected in the subtitle of the Core-Plus series: \"Contemporary Mathematics in Context\". To review the program, it was necessary to go through all the problems in the core units and their corresponding materials in the Teacher’s Edition. Despite the unconventional textbook structure, the language used by the Core-Plus program was found mathematically sound.\n\nIn the algebra section, fundamental theorems on linear functions and quadratic functions were found not justified, except for the quadratic formula. Theorems are often presented without proof.\n\nLike in the algebra texts, the geometry text does not lead to a clear logical structure of the material taught. Because theoretical material is concealed within the text of the problems, \"a teacher must identify all the critical problems and know in advance the intended structure to establish the essential mathematical progression. This task is further complicated by the fact that many critical problems appear in the homework sections. Important theorems in geometry are not justified. Moreover, with the way the material is sequenced, some of these theorems cannot be justified\".\n\nAccording to Prof. Harel, the Core-Plus program \"excels in providing ample experience in solving application problems and in ensuring that students understand the meanings of the different parts of the modeling functions. The program also excels in its mission to contextualize the mathematics taught\". However, it fails \"to convey critical mathematical concepts and ideas that should and can be within reach for high school students\".\n\nProfessor W. Stephen Wilson from Johns Hopkins University evaluated the mathematical development and coherence of the Core-Plus program in 2009. In particular, he examined \"the algebraic concepts and skills associated with linear functions because they are a critical foundation for the further study of algebra\", and evaluated how the program presents the theorem that the sum of the angles of a triangle is 180 degrees, \"which is a fundamental theorem of Euclidean geometry and it connects many of the basics in geometry to each other\".\n\nProf. Wilson noted that the major theme of the algebra portion of the program seems to involve creating a table from data, graphing the points from the table; given the table students are asked to find a corresponding function. In case of linear function, \"at no point is there an attempt to show that the equation's graph really is a line. Likewise, there is never an attempt to show that a line graph comes from the usual form of a linear equation\". Prof. Wilson considered this approach to be \"a significant flaw in the mathematical foundation\".\n\nQuoting the textbook, \"Linear functions relating two variables x and y can be represented using tables, graphs, symbolic rules, or verbal descriptions\", Prof. Wilson laments that although this statement is true, \"the essence of algebra involves abstraction using symbols\".\n\nProf. Wilson says that the Core-Plus program \"has a multitude of good problems, but never develops the core of the mathematics of linear functions. The problems are set in contexts and mathematics itself is rarely considered as a legitimate enterprise to investigate\". The program lacks attention to algebraic manipulation\" to the point that \"symbolic algebra is minimized\".\n\nIn regards to geometry portion, Prof. Wilson concludes that the program fails to build geometry up from foundations in a mathematically sound and coherent way\". He stresses out that \"one significant goal of a geometry course is to teach logic, and this program fails on that account\".\n\nOverall, the \"unacceptable nature of geometry\" and the fashion in which the program downplays \"algebraic structure and skills\" make the Core-Plus program unacceptable.\n\nMathematics programs initially developed in the 1990s that were based on the NCTM’s Curriculum and Evaluation Standards for School Mathematics, like \"Core-Plus Mathematics\", have been the subject of controversy due to their differences from more conventional mathematics programs. In the case of \"Core-Plus Mathematics\", there has been debate about (a) the international-like integrated nature of the curriculum, whereby each year students learn algebra, geometry, statistics, probability, and discrete mathematical modeling, as opposed to conventional U.S. curricula in which just a single subject is studied each year, (b) a concern that students may not adequately develop conventional algebraic skills, (c) a concern that students may not be adequately prepared for college, and (d) a mode of instruction that relies less on teacher lecture and demonstration and more on inquiry, problem solving in contextualized settings, and collaborative work by students.\n\nFor example, this debate led to some schools in Minnesota abandoning \"Core-Plus Mathematics\" in the early 2000s and returning to traditional mathematics curricula. In a master's degree research paper at the time, interviews with teachers at four schools that had dropped \"Core-Plus Mathematics\" suggested that many teachers \"did not feel that Core-Plus emphasized mastering skills enough\", while parents \"felt that it did not prepare students for college\" and some parents commented that the text was difficult to read. The author of the paper made suggestions for successful adoption of any new materials, including \"don't rush the adoption process,\" have \"continued professional development for all,\" and \"school districts need to be proactive regarding parent questions.\" \n\nWhile discussion of these issues continues, a growing body of research (referenced above) has provided reassuring results about learning algebraic skills and college preparation and also suggests that \"Core-Plus Mathematics\" students are advantaged in terms of conceptual understanding, problem solving, and reasoning; an integrated high school mathematics curriculum is now increasingly recognized as the most common curriculum organization outside the U.S. and also acknowledged as a valid curriculum organizational structure in the \"Common Core State Standards for Mathematics\" in the U.S. (e.g., CCSSI-Courses and Transitions); and classroom instruction that is more inquiry- and problem-solving-oriented is recognized as a viable instructional methodology.\n\n"}
{"id": "47734869", "url": "https://en.wikipedia.org/wiki?curid=47734869", "title": "Counterfactual quantum computation", "text": "Counterfactual quantum computation\n\nCounterfactual Quantum Computation is a method of inferring the result of a computation without actually running a quantum computer otherwise capable of actively performing that computation.\n\nPhysicists Graeme Mitchison and Richard Jozsa introduced the notion of counterfactual computing as an application of quantum computing, founded on the concepts of counterfactual definiteness, on a re-interpretation of the Elitzur–Vaidman bomb tester thought experiment, and making theoretical use of the phenomenon of interaction-free measurement.\n\nAs an example of this idea, in 1997, after seeing a talk on Counterfactual Computation by Richard Jozsa at the Isaac Newton Institute, Keith Bowden (based in the Theoretical Physics Research Unit at Birkbeck College, University of London) published a paper describing a digital computer that could be counterfactually interrogated to calculate whether a light beam would fail to pass through a maze.\n\nMore recently the idea of counterfactual quantum communication has been proposed and demonstrated. \n\nThe quantum computer may be physically implemented in arbitrary ways but the common apparatus considered to date features a Mach–Zehnder interferometer. The quantum computer is set in a superposition of \"not running\" and \"running\" states by means such as the Quantum Zeno Effect. Those state histories are quantum interfered. After many repetitions of very rapid projective measurements, the \"not running\" state evolves to a final value imprinted into the properties of the quantum computer. Measuring that value allows for learning the result of some types of computations such as Grover's algorithm even though the result was derived from the non-running state of the quantum computer.\n\nThe original formulation of Counterfactual Quantum Computation stated that a set \"m\" of measurement outcomes is a counterfactual outcome if (1) there is only one history associated to \"m\" and that history contains only \"off\" (non-running) states, and (2) there is only a single possible computational output associated to \"m\".\n\nA refined definition of counterfactual computation expressed in procedures and conditions is: \n(i) Identify and label all histories (quantum paths), with as many labels as needed, which lead to the same set \"m\" of measurement outcomes, and (ii) coherently superpose all possible histories. (iii) After cancelling the terms (if any) whose complex amplitudes together add to zero, the set \"m\" of measurement outcomes is a counterfactual outcome if (iv) there are no terms left with the computer-running label in their history labels, and (v) there is only a single possible computer output associated to \"m\".\n\nIn 1997, after discussions with Abner Shimony and Richard Jozsa, and inspired by the idea of the (1993) Elitzur-Vaidman Bomb Tester, Keith Bowden published a paper describing a digital computer that could be counterfactually interrogated to calculate whether a photon would fail to pass through a maze of mirrors. This so called Mirror-Array replaces the tentative Bomb in Elitzur and Vaidman’s device (actually a Mach-Zender interferometer). One time in four a photon will exit the device in such a way as to indicate that the maze is not navigable, even though the photon never passed through the Mirror Array. The Mirror Array itself is set up in such a way that it is defined by an n by n matrix of bits. The output (fail or otherwise) is itself defined by a single bit. Thus the Mirror Array itself is an n-squared bit in, 1 bit out digital computer which calculates mazes and can be run counterfactually. Although the overall device is clearly a quantum computer, the part which is counterfactually tested is semi classical.\n\nIn 2015, Counterfactual Quantum Computation was demonstrated in the experimental context of \"spins of a negatively charged Nitrogen-vacancy color center in a diamond\". Previously suspected limits of efficiency were exceeded, achieving counterfactual computational efficiency of 85% with the higher efficiency foreseen in principle.\n"}
{"id": "6123", "url": "https://en.wikipedia.org/wiki?curid=6123", "title": "Curl (mathematics)", "text": "Curl (mathematics)\n\nIn vector calculus, the curl is a vector operator that describes the infinitesimal rotation of a vector field in three-dimensional Euclidean space. At every point in the field, the curl of that point is represented by a vector. The attributes of this vector (length and direction) characterize the rotation at that point.\n\nThe direction of the curl is the axis of rotation, as determined by the right-hand rule, and the magnitude of the curl is the magnitude of rotation. If the vector field represents the flow velocity of a moving fluid, then the curl is the circulation density of the fluid. A vector field whose curl is zero is called irrotational. The curl is a form of differentiation for vector fields. The corresponding form of the fundamental theorem of calculus is Stokes' theorem, which relates the surface integral of the curl of a vector field to the line integral of the vector field around the boundary curve.\n\nThe alternative terminology \"rotor\", \"rotation\" or \"rotational\" and alternative notations and are often used (the former especially in many European countries, the latter, using the del (or nabla) operator and the cross product, is more used in other countries) for . In a context where the cross product is denoted with the wedge symbol, would be used.\n\nUnlike the gradient and divergence, curl does not generalize as simply to other dimensions; some generalizations are possible, but only in three dimensions is the geometrically defined curl of a vector field again a vector field. This is a similar phenomenon as in the 3 dimensional cross product, and the connection is reflected in the notation for the curl.\n\nThe name \"curl\" was first suggested by James Clerk Maxwell in 1871 but the concept was apparently first used in the construction of an optical field theory by James MacCullagh in 1839.\n\nThe curl of a vector field , denoted by , or , or , at a point is defined in terms of its projection onto various lines through the point. If is any unit vector, the projection of the curl of onto is defined to be the limiting value of a closed line integral in a plane orthogonal to divided by the area enclosed, as the path of integration is contracted around the point.\n\nThe curl operator maps continuously differentiable functions to continuous functions , and more generally, it maps functions in to functions in . \n\nImplicitly, curl is defined by:\n\nwhere is a line integral along the boundary of the area in question, and is the magnitude of the area. If is the unit vector perpendicular to the plane, whereas is an outward-pointing in-plane normal (see caption at right), then the orientation of is chosen so that a tangent vector to is positively oriented if and only if forms a positively oriented basis for (right-hand rule).\n\nThe above formula means that the curl of a vector field is defined as the infinitesimal area density of the \"circulation\" of that field. To this definition fit naturally \n\nThe equation for each component can be obtained by exchanging each occurrence of a subscript 1, 2, 3 in cyclic permutation: 1 → 2, 2 → 3, and 3 → 1 (where the subscripts represent the relevant indices).\n\nIf are the Cartesian coordinates and are the orthogonal coordinates, then \nis the length of the coordinate vector corresponding to . The remaining two components of curl result from cyclic permutation of indices: 3,1,2 → 1,2,3 → 2,3,1.\n\nSuppose the vector field describes the velocity field of a fluid flow (such as a large tank of liquid or gas) and a small ball is located within the fluid or gas (the centre of the ball being fixed at a certain point). If the ball has a rough surface, the fluid flowing past it will make it rotate. The rotation axis (oriented according to the right hand rule) points in the direction of the curl of the field at the centre of the ball, and the angular speed of the rotation is half the magnitude of the curl at this point.\n\nThe curl of the vector at any point is given by the rotation of a infinitesimal area in the \"xy\"-plane (for \"z\"-axis component of the curl), \"zx\"-plane (for \"y\"-axis component of the curl) and \"yz\"-plane (for \"x\"-axis component of the curl vector). This can be clearly seen in the examples below.\n\nIn practice, the above definition is rarely used because in virtually all cases, the curl operator can be applied using some set of curvilinear coordinates, for which simpler representations have been derived.\n\nThe notation has its origins in the similarities to the 3-dimensional cross product, and it is useful as a mnemonic in Cartesian coordinates if is taken as a vector differential operator del. Such notation involving operators is common in physics and algebra.\n\nExpanded in 3-dimensional Cartesian coordinates (see \"Del in cylindrical and spherical coordinates\" for spherical and cylindrical coordinate representations), is, for composed of :\n\nwhere , , and are the unit vectors for the -, -, and -axes, respectively. This expands as follows:\n\nAlthough expressed in terms of coordinates, the result is invariant under proper rotations of the coordinate axes but the result inverts under reflection.\n\nIn a general coordinate system, the curl is given by\n\nwhere denotes the Levi-Civita tensor and the covariant derivative, the metric tensor is used to lower the index on , and the Einstein summation convention implies that repeated indices are summed over. Equivalently,\n\nwhere are the coordinate vector fields. Equivalently, using the exterior derivative, the curl can be expressed as:\n\nHere and are the musical isomorphisms, and is the Hodge star operator. This formula shows how to calculate the curl of in any coordinate system, and how to extend the curl to any oriented three-dimensional Riemannian manifold. Since this depends on a choice of orientation, curl is a chiral operation. In other words, if the orientation is reversed, then the direction of the curl is also reversed.\n\nTake the vector field:\n\nIts corresponding plot:\n\nUpon visual inspection, the field can be described as \"rotating\". If a stationary object were to be placed in the field with the vectors representing a linear force, the object would rotate clockwise.\n\nCalculating the curl:\n\nThe resulting vector field describing the curl would be uniformly going in the negative direction. It should be noted that the results of this equation align with what could have been predicted using the right-hand rule using a right-handed coordinate system. Being a uniform vector field, the object described before would have the same rotational intensity regardless of where it was placed.\n\nThe plot describing the curl of :\n\nTake the vector field:\n\nIts corresponding plot:\n\nUpon initial inspection, curl existing in this graph would not be obvious. However, taking the object in the previous example, and placing it anywhere on the line , the force exerted on the right side would be slightly greater than the force exerted on the left, causing it to rotate clockwise. Using the right-hand rule, it can be predicted that the resulting curl would be straight in the negative direction. Inversely, if placed on , the object would rotate counterclockwise and the right-hand rule would result in a positive direction.\n\nCalculating the curl:\n\nAs predicted, the curl points in the negative direction when is positive and vice versa. In this field, the intensity of rotation would be greater as the object moves away from the plane .\n\nThe plot describing the curl of :\n\nIn general curvilinear coordinates (not only in Cartesian coordinates), the curl of a cross product of vector fields and can be shown to be\n\nInterchanging the vector field and operator, we arrive at the cross product of a vector field with curl of a vector field:\n\nwhere is the Feynman subscript notation, which considers only the variation due to the vector field (i.e., in this case, is treated as being constant in space).\n\nAnother example is the curl of a curl of a vector field. It can be shown that in general coordinates\n\nand this identity defines the vector Laplacian of , symbolized as .\n\nThe curl of the gradient of \"any\" scalar field is always the zero vector field\n\nwhich follows from the antisymmetry in the definition of the curl, and the symmetry of second derivatives.\n\nIf is a scalar valued function and is a vector field, then\n\n\nThe vector calculus operations of grad, curl, and div are most easily generalized and understood in the context of differential forms, which involves a number of steps. In a nutshell, they correspond to the derivatives of 0-forms, 1-forms, and 2-forms, respectively. The geometric interpretation of curl as rotation corresponds to identifying bivectors (2-vectors) in 3 dimensions with the special orthogonal Lie algebra of infinitesimal rotations (in coordinates, skew-symmetric 3 × 3 matrices), while representing rotations by vectors corresponds to identifying 1-vectors (equivalently, 2-vectors) and , these all being 3-dimensional spaces.\n\nIn 3 dimensions, a differential 0-form is simply a function ; a differential 1-form is the following expression:\na differential 2-form is the formal sum:\nand a differential 3-form is defined by a single term:\n\nThe exterior derivative of a -form in is defined as the -form from above—and in if, e.g.,\n\nthen the exterior derivative leads to\n\nThe exterior derivative of a 1-form is therefore a 2-form, and that of a 2-form is a 3-form. On the other hand, because of the interchangeability of mixed derivatives, e.g. because of\n\nthe twofold application of the exterior derivative leads to 0.\n\nThus, denoting the space of -forms by and the exterior derivative by one gets a sequence:\n\nHere is the space of sections of the exterior algebra vector bundle over ℝ, whose dimension is the binomial coefficient ; note that for or . Writing only dimensions, one obtains a row of Pascal's triangle:\n\nthe 1-dimensional fibers correspond to functions, and the 3-dimensional fibers to vector fields, as described below. Modulo suitable identifications, the three nontrivial occurrences of the exterior derivative correspond to grad, curl, and div.\n\nDifferential forms and the differential can be defined on any Euclidean space, or indeed any manifold, without any notion of a Riemannian metric. On a Riemannian manifold, or more generally pseudo-Riemannian manifold, -forms can be identified with -vector fields (-forms are -covector fields, and a pseudo-Riemannian metric gives an isomorphism between vectors and covectors), and on an \"oriented\" vector space with a nondegenerate form (an isomorphism between vectors and covectors), there is an isomorphism between -vectors and -vectors; in particular on (the tangent space of) an oriented pseudo-Riemannian manifold. Thus on an oriented pseudo-Riemannian manifold, one can interchange -forms, -vector fields, -forms, and -vector fields; this is known as Hodge duality. Concretely, on this is given by:\n\nThus, identifying 0-forms and 3-forms with functions, and 1-forms and 2-forms with vector fields:\n\nOn the other hand, the fact that corresponds to the identities\nfor any function , and\nfor any vector field .\n\nGrad and div generalize to all oriented pseudo-Riemannian manifolds, with the same geometric interpretation, because the spaces of 0-forms and -forms is always (fiberwise) 1-dimensional and can be identified with scalar functions, while the spaces of 1-forms and -forms are always fiberwise -dimensional and can be identified with vector fields.\n\nCurl does not generalize in this way to 4 or more dimensions (or down to 2 or fewer dimensions); in 4 dimensions the dimensions are\n\nso the curl of a 1-vector field (fiberwise 4-dimensional) is a \"2-vector field\", which is fiberwise 6-dimensional, one has\n\nwhich yields a sum of six independent terms, and cannot be identified with a 1-vector field. Nor can one meaningfully go from a 1-vector field to a 2-vector field to a 3-vector field (4 → 6 → 4), as taking the differential twice yields zero (). Thus there is no curl function from vector fields to vector fields in other dimensions arising in this way.\n\nHowever, one can define a curl of a vector field as a \"2-vector field\" in general, as described below.\n\n2-vectors correspond to the exterior power ; in the presence of an inner product, in coordinates these are the skew-symmetric matrices, which are geometrically considered as the special orthogonal Lie algebra of infinitesimal rotations. This has dimensions, and allows one to interpret the differential of a 1-vector field as its infinitesimal rotations. Only in 3 dimensions (or trivially in 0 dimensions) does , which is the most elegant and common case. In 2 dimensions the curl of a vector field is not a vector field but a function, as 2-dimensional rotations are given by an angle (a scalar – an orientation is required to choose whether one counts clockwise or counterclockwise rotations as positive); this is not the div, but is rather perpendicular to it. In 3 dimensions the curl of a vector field is a vector field as is familiar (in 1 and 0 dimensions the curl of a vector field is 0, because there are no non-trivial 2-vectors), while in 4 dimensions the curl of a vector field is, geometrically, at each point an element of the 6-dimensional Lie algebra .\n\nNote also that the curl of a 3-dimensional vector field which only depends on 2 coordinates (say and ) is simply a vertical vector field (in the direction) whose magnitude is the curl of the 2-dimensional vector field, as in the examples on this page.\n\nConsidering curl as a 2-vector field (an antisymmetric 2-tensor) has been used to generalize vector calculus and associated physics to higher dimensions.\n\n\n\n"}
{"id": "52559029", "url": "https://en.wikipedia.org/wiki?curid=52559029", "title": "David Ben-Zvi", "text": "David Ben-Zvi\n\nDavid Dror Ben-Zvi is an American mathematician, currently the Joe B. and Louise Cook Professor of Mathematics at University of Texas at Austin.\n\nBen-Zvi earned his Ph.D. from Harvard University in 1999, with a dissertation entitled \"Spectral Curves, Opers And Integrable Systems\" supervised by Edward Frenkel.\nIn 2012, he became one of the inaugural Fellows of the American Mathematical Society.\n"}
{"id": "53725340", "url": "https://en.wikipedia.org/wiki?curid=53725340", "title": "Deanna Haunsperger", "text": "Deanna Haunsperger\n\nDeanna Haunsperger is an American mathematician and Professor of Mathematics at Carleton College.\nShe is the president of the Mathematical Association of America for the 2017–2018 term.\nShe co-created and co-organized the Carleton College Summer Mathematics Program for Women, which ran every summer from 1995 to 2014.\n\nHaunsperger received her Bachelor of Arts in mathematics and computer science from Simpson College in 1986. She received her Masters in mathematics in 1989 and her Ph.D. in mathematics in 1991 from Northwestern University.\nHer dissertation was entitled \"Projection and Aggregation Paradoxes in Nonparametrical Statistical Tests\" and her advisor was Donald Gene Saari.\n\nHaunsperger was an assistant professor of mathematics at St. Olaf College from 1991 to 1994.\nSince 1994, she has been a faculty member in the mathematics department at Carleton College.\n\nFrom 1995 to 2014, Haunsperger directed the Carleton College Summer Mathematics Program for Women. This program worked to prepare undergraduate women to pursue a Ph.D. in mathematics.\n\nFrom 1999 to 2003, Haunsperger was a co-editor of \"Math Horizons\", a magazine aimed at undergraduate students who are interested in mathematics.\n\nHaunsperger was the second vice-president of the Mathematical Association of America (MAA) from 2006 to 2008. She was elected president of the MAA for the 2017–2018 term.\n\nHaunsperger has won several awards from the Association for Women in Mathematics (AWM). In 2012, she was selected for the M. Gweneth Humphreys Award, which recognizes mathematics educators who have exhibited outstanding mentorship.\nShe was presented with the second annual AWM Presidential Award in 2017.\nIn 2017, she was selected as a fellow of the AWM in the inaugural class.\n\n"}
{"id": "48917029", "url": "https://en.wikipedia.org/wiki?curid=48917029", "title": "Degree of a differential equation", "text": "Degree of a differential equation\n\nIn mathematics, the degree of a differential equation is the power of its highest derivative, after the equation has been made rational and integral in all of its derivatives.\n"}
{"id": "40639179", "url": "https://en.wikipedia.org/wiki?curid=40639179", "title": "Deligne–Mumford stack", "text": "Deligne–Mumford stack\n\nIn algebraic geometry, a Deligne–Mumford stack is a stack \"F\" such that\n\nPierre Deligne and David Mumford introduced this notion in 1969 when they proved that moduli spaces of stable curves of fixed arithmetic genus are proper smooth Deligne–Mumford stacks.\n\nIf the \"étale\" is weakened to \"smooth\", then such a stack is called an algebraic stack (also called an Artin stack, after Michael Artin). An algebraic space is Deligne–Mumford.\n\nA key fact about a Deligne–Mumford stack \"F\" is that any \"X\" in formula_3, where \"B\" is quasi-compact, has only finitely many automorphisms.\nA Deligne–Mumford stack admits a presentation by a groupoid; see groupoid scheme.\n\nDeligne–Mumford stacks are typically constructed by taking the stack quotient of some variety where the stabilizers are finite groups. For example, consider the action of the cyclic group formula_4 on formula_5 given by\nThen the stack quotient formula_7 is an affine smooth Deligne–Mumford stack with a non-trivial stabilizer at the origin. If we wish to think about this as a category fibered in groupoids over formula_8 then given a scheme formula_9 the over category is given by\nNote that we could be slightly more general if we consider the group action on formula_11.\n\nNon-affine examples come up when taking the stack quotient for weighted projective space/varieties. For example, the space formula_12 is constructed by the stack quotient formula_13 where the formula_14-action is given by\nNotice that since this quotient is not from a finite group we have to look for points with stabilizers and their respective stabilizer groups. Then formula_16 if and only if formula_17 or formula_18 and formula_19 or formula_20, respectively, showing that the only stabilizers are finite, hence the stack is Deligne–Mumford.\n\nOne simple non-example of a Deligne–Mumford stack is formula_21 since this has an infinite stabilizer. Stacks of this form are examples of Artin stacks.\n"}
{"id": "196095", "url": "https://en.wikipedia.org/wiki?curid=196095", "title": "Discrete sine transform", "text": "Discrete sine transform\n\nIn mathematics, the discrete sine transform (DST) is a Fourier-related transform similar to the discrete Fourier transform (DFT), but using a purely real matrix. It is equivalent to the imaginary parts of a DFT of roughly twice the length, operating on real data with odd symmetry (since the Fourier transform of a real and odd function is imaginary and odd), where in some variants the input and/or output data are shifted by half a sample.\n\nA related transform is the discrete cosine transform (DCT), which is equivalent to a DFT of real and \"even\" functions. See the DCT article for a general discussion of how the boundary conditions relate the various DCT and DST types.\n\nDSTs are widely employed in solving partial differential equations by spectral methods, where the different variants of the DST correspond to slightly different odd/even boundary conditions at the two ends of the array.\n\nLike any Fourier-related transform, discrete sine transforms (DSTs) express a function or a signal in terms of a sum of sinusoids with different frequencies and amplitudes. Like the discrete Fourier transform (DFT), a DST operates on a function at a finite number of discrete data points. The obvious distinction between a DST and a DFT is that the former uses only sine functions, while the latter uses both cosines and sines (in the form of complex exponentials). However, this visible difference is merely a consequence of a deeper distinction: a DST implies different boundary conditions than the DFT or other related transforms.\n\nThe Fourier-related transforms that operate on a function over a finite domain, such as the DFT or DST or a Fourier series, can be thought of as implicitly defining an \"extension\" of that function outside the domain. That is, once you write a function formula_1 as a sum of sinusoids, you can evaluate that sum at any formula_2, even for formula_2 where the original formula_1 was not specified. The DFT, like the Fourier series, implies a periodic extension of the original function. A DST, like a sine transform, implies an odd extension of the original function.\n\nHowever, because DSTs operate on \"finite\", \"discrete\" sequences, two issues arise that do not apply for the continuous sine transform. First, one has to specify whether the function is even or odd at \"both\" the left and right boundaries of the domain (i.e. the min-\"n\" and max-\"n\" boundaries in the definitions below, respectively). Second, one has to specify around \"what point\" the function is even or odd. In particular, consider a sequence (\"a\",\"b\",\"c\") of three equally spaced data points, and say that we specify an odd \"left\" boundary. There are two sensible possibilities: either the data is odd about the point \"prior\" to \"a\", in which case the odd extension is (−\"c\",−\"b\",−\"a\",0,\"a\",\"b\",\"c\"), or the data is odd about the point \"halfway\" between \"a\" and the previous point, in which case the odd extension is (−\"c\",−\"b\",−\"a\",\"a\",\"b\",\"c\")\n\nThese choices lead to all the standard variations of DSTs and also discrete cosine transforms (DCTs). \nEach boundary can be either even or odd (2 choices per boundary) and can be symmetric about a data point or the point halfway between two data points (2 choices per boundary), for a total of formula_5 possibilities. Half of these possibilities, those where the \"left\" boundary is odd, correspond to the 8 types of DST; the other half are the 8 types of DCT.\n\nThese different boundary conditions strongly affect the applications of the transform, and lead to uniquely useful properties for the various DCT types. Most directly, when using Fourier-related transforms to solve partial differential equations by spectral methods, the boundary conditions are directly specified as a part of the problem being solved.\n\nFormally, the discrete sine transform is a linear, invertible function \"F\" : R -> R (where R denotes the set of real numbers), or equivalently an \"N\" × \"N\" square matrix. There are several variants of the DST with slightly modified definitions. The \"N\" real numbers \"x\", ..., \"x\" are transformed into the \"N\" real numbers \"X\", ..., \"X\" according to one of the formulas:\n\nThe DST-I matrix is orthogonal (up to a scale factor).\n\nA DST-I is exactly equivalent to a DFT of a real sequence that is odd around the zero-th and middle points, scaled by 1/2. For example, a DST-I of \"N\"=3 real numbers (\"a\",\"b\",\"c\") is exactly equivalent to a DFT of eight real numbers (0,\"a\",\"b\",\"c\",0,−\"c\",−\"b\",−\"a\") (odd symmetry), scaled by 1/2. (In contrast, DST types II–IV involve a half-sample shift in the equivalent DFT.) This is the reason for the \"N\" + 1 in the denominator of the sine function: the equivalent DFT has 2(\"N\"+1) points and has 2π/2(\"N\"+1) in its sinusoid frequency, so the DST-I has π/(\"N\"+1) in its frequency.\n\nThus, the DST-I corresponds to the boundary conditions: \"x\" is odd around \"n\" = −1 and odd around \"n\"=\"N\"; similarly for \"X\".\n\nSome authors further multiply the \"X\" term by 1/ (see below for the corresponding change in DST-III). This makes the DST-II matrix orthogonal (up to a scale factor), but breaks the direct correspondence with a real-odd DFT of half-shifted input.\n\nThe DST-II implies the boundary conditions: \"x\" is odd around \"n\" = −1/2 and odd around \"n\" = \"N\" − 1/2; \"X\" is odd around \"k\" = −1 and even around \"k\" = \"N\" − 1.\n\nSome authors further multiply the \"x\" term by (see above for the corresponding change in DST-II). This makes the DST-III matrix orthogonal (up to a scale factor), but breaks the direct correspondence with a real-odd DFT of half-shifted output.\n\nThe DST-III implies the boundary conditions: \"x\" is odd around \"n\" = −1 and even around \"n\" = \"N\" − 1; \"X\" is odd around \"k\" = −1/2 and odd around \"k\" = \"N\" − 1/2.\n\nThe DST-IV matrix is orthogonal (up to a scale factor).\n\nThe DST-IV implies the boundary conditions: \"x\" is odd around \"n\" = −1/2 and even around \"n\" = \"N\" − 1/2; similarly for \"X\".\n\nDST types I–IV are equivalent to real-odd DFTs of even order. In principle, there are actually four additional types of discrete sine transform (Martucci, 1994), corresponding to real-odd DFTs of logically odd order, which have factors of \"N\"+1/2 in the denominators of the sine arguments. However, these variants seem to be rarely used in practice.\n\nThe inverse of DST-I is DST-I multiplied by 2/(\"N\" + 1). The inverse of DST-IV is DST-IV multiplied by 2/\"N\". The inverse of DST-II is DST-III multiplied by 2/\"N\" (and vice versa).\n\nAs for the DFT, the normalization factor in front of these transform definitions is merely a convention and differs between treatments. For example, some authors multiply the transforms by formula_10 so that the inverse does not require any additional multiplicative factor.\n\nAlthough the direct application of these formulas would require O(\"N\") operations, it is possible to compute the same thing with only O(\"N\" log \"N\") complexity by factorizing the computation similar to the fast Fourier transform (FFT). (One can also compute DSTs via FFTs combined with O(\"N\") pre- and post-processing steps.)\n\nA DST-III or DST-IV can be computed from a DCT-III or DCT-IV (see discrete cosine transform), respectively, by reversing the order of the inputs and flipping the sign of every other output, and vice versa for DST-II from DCT-II. In this way it follows that types II–IV of the DST require exactly the same number of arithmetic operations (additions and multiplications) as the corresponding DCT types.\n\n"}
{"id": "151864", "url": "https://en.wikipedia.org/wiki?curid=151864", "title": "Divergence theorem", "text": "Divergence theorem\n\nIn tensor calculus, the divergence theorem, also known as Gauss's theorem or Ostrogradsky's theorem, is a result that relates the flow (that is, flux) of a tensor field through a surface to the behavior of the tensor field inside the surface.\n\nMore precisely, the divergence theorem states that the outward flux of a tensor field through a closed surface is equal to the volume integral of the divergence over the region inside the surface. Intuitively, it states that \"the sum of all sources (with sinks regarded as negative sources) gives the net flux out of a region\".\n\nThe divergence theorem is an important result for the mathematics of physics and engineering, in particular in electrostatics and fluid dynamics.\n\nIn physics and engineering, the divergence theorem is usually applied in three dimensions. However, it generalizes to any number of dimensions. In one dimension, it is equivalent to the fundamental theorem of calculus. In two dimensions, it is equivalent to Green's theorem.\n\nThe theorem is a special case of the more general Stokes' theorem.\n\nIf a fluid is flowing in some area, then the rate at which fluid flows out of a certain region within that area can be calculated by adding up the sources inside the region and subtracting the sinks. The fluid flow is represented by a first order (or a vector) field, and the vector field's divergence at a given point describes the strength of the source or sink there. So, integrating the field's divergence over the interior of the region should equal the integral of the vector field over the region's boundary. The divergence theorem says that this is true.\n\nThe divergence theorem is employed in any conservation law which states that the volume total of all sinks and sources, that is the volume integral of the divergence, is equal to the net flow across the volume's boundary.\n\nSuppose is a subset of formula_1 (in the case of represents a volume in three-dimensional space) which is compact and has a piecewise smooth boundary (also indicated with ). If is a continuously differentiable vector field defined on a neighborhood of , then we have:\n\nThe left side is a volume integral over the volume , the right side is the surface integral over the boundary of the volume . The closed manifold is quite generally the boundary of oriented by outward-pointing normals, and is the outward pointing unit normal field of the boundary . ( may be used as a shorthand for .) The symbol within the two integrals stresses once more that is a \"closed\" surface. In terms of the intuitive description above, the left-hand side of the equation represents the total of the sources in the volume , and the right-hand side represents the total flow across the boundary .\n\nBy replacing formula_2 in the divergence theorem with specific forms, other useful identities can be derived (cf. vector identities).\n\n\n\n\n\nSuppose we wish to evaluate\n\nwhere is the unit sphere defined by\n\nand is the vector field\n\nThe direct computation of this integral is quite difficult, but we can simplify the derivation of the result using the divergence theorem, because the divergence theorem says that the integral is equal to:\n\nwhere is the unit ball:\n\nSince the function is positive in one hemisphere of and negative in the other, in an equal and opposite way, its total integral over is zero. The same is true for :\n\nTherefore,\n\nbecause the unit ball has volume .\n\nAs a result of the divergence theorem, a host of physical laws can be written in both a differential form (where one quantity is the divergence of another) and an integral form (where the flux of one quantity through a closed surface is equal to another quantity). Three examples are Gauss's law (in electrostatics), Gauss's law for magnetism, and Gauss's law for gravity.\n\nContinuity equations offer more examples of laws with both differential and integral forms, related to each other by the divergence theorem. In fluid dynamics, electromagnetism, quantum mechanics, relativity theory, and a number of other fields, there are continuity equations that describe the conservation of mass, momentum, energy, probability, or other quantities. Generically, these equations state that the divergence of the flow of the conserved quantity is equal to the distribution of \"sources\" or \"sinks\" of that quantity. The divergence theorem states that any such continuity equation can be written in a differential form (in terms of a divergence) and an integral form (in terms of a flux).\n\nAny \"inverse-square law\" can instead be written in a \"Gauss's law\"-type form (with a differential and integral form, as described above). Two examples are Gauss's law (in electrostatics), which follows from the inverse-square Coulomb's law, and Gauss's law for gravity, which follows from the inverse-square Newton's law of universal gravitation. The derivation of the Gauss's law-type equation from the inverse-square formulation or vice versa is exactly the same in both cases; see either of those articles for details.\n\nThe theorem was first discovered by Lagrange in 1762, then later independently rediscovered by Gauss in 1813, by Ostrogradsky, who also gave the first proof of the general theorem, in 1826, by Green in 1828, etc. Subsequently, variations on the divergence theorem are correctly called Ostrogradsky's theorem, but also commonly Gauss's theorem, or Green's theorem.\n\nTo verify the planar variant of the divergence theorem for a region :\n\nand the vector field:\n\nThe boundary of is the unit circle, , that can be represented parametrically by:\n\nsuch that where units is the length arc from the point to the point on . Then a vector equation of is\n\nAt a point on :\n\nTherefore,\n\nBecause , and because . Thus\n\nOne can use the general Stokes' Theorem to equate the -dimensional volume integral of the divergence of a vector field over a region to the -dimensional surface integral of over the boundary of :\n\nThis equation is also known as the Divergence theorem.\n\nWhen , this is equivalent to Green's theorem.\n\nWhen , it reduces to the Fundamental theorem of calculus.\n\nWriting the theorem in Einstein notation:\n\nsuggestively, replacing the vector field with a rank- tensor field , this can be generalized to:\n\nwhere on each side, tensor contraction occurs for at least one index. This form of the theorem is still in 3d, each index takes values 1, 2, and 3. It can be generalized further still to higher (or lower) dimensions (for example to 4d spacetime in general relativity).\n\n\n"}
{"id": "46612953", "url": "https://en.wikipedia.org/wiki?curid=46612953", "title": "Djairo Guedes de Figueiredo", "text": "Djairo Guedes de Figueiredo\n\nDjairo Guedes de Figueiredo (academic signature: D. G. De Figueiredo, born on 2 April 1934, in Limoeiro do Norte) is a Brazilian mathematician noted for his researches on differential equations, elliptic operators, and calculus of variations. He is considered the greatest analyst from Brazil. He was the president of the Brazilian Mathematical Society from 1977 to 1979.\n\nFigueiredo is a well-known figure among mathematicians in analysis and differential equations and among Brazilian students in physics, engineering and mathematics. He has received many Brazilian national and international prizes, both for his research in pure mathematics and also for his popular mathematics textbooks (about analysis and differential equations) and expository writing papers. In 1995 he received the National Order of Scientific Merit and in 2004 the title of \"Doctor Honoris Causa\" from the Federal University of Paraíba. In 2009 he became a member of the National Academy of Science of Buenos Aires. In 2011 he became the first Brazilian to receive a gold medal from the Telesio-Galilei Academy of Sciences from Great Britain \"\nfor his great contribution to Mathematics, especially to the theory of elliptical partial differential equations\".\n\nHe was a Ph.D. student of Louis Nirenberg at New York University, and is currently a titular professor at UNICAMP, a position he earned in 1989.\n\nHe is a recipient of Brazil's National Order of Scientific Merit in mathematics (1995). Since 1969 he has been a member of the Brazilian Academy of Sciences.\n\n\nThe book \"Selected Papers of Djairo Guedes Figueiredo\" has been published by Springer, as part of the collection “Selected Works of Outstanding Brazilian Mathematicians”. (Google Preview)\n\n"}
{"id": "5560194", "url": "https://en.wikipedia.org/wiki?curid=5560194", "title": "Dual object", "text": "Dual object\n\nIn category theory, a branch of mathematics, a dual object is an analogue of a dual vector space from linear algebra for objects in arbitrary monoidal categories. It's only a partial generalization, based upon the categorical properties of duality for finite-dimensional vector spaces. An object admitting a dual is called a dualizable object. In this formalism, infinite-dimensional vector spaces are not dualizable, since the dual vector space \"V\" doesn't satisfy the axioms. Often, an object is dualizable only when it satisfies some finiteness or compactness property.\n\nA category in which each object has a dual is called autonomous or rigid. A category of finite-dimensional vector spaces with a standard tensor product is rigid, while the category of all vector spaces is not. \n\nLet \"V\" be a finite-dimensional vector space over some field \"k\". A standard notion of a dual vector space \"V\" has the following property. For any vector spaces \"U\" and \"W\" there is an adjunction Hom(\"U\" ⊗ \"V\",\"W\") = Hom(\"U\", \"V\" ⊗ \"W\"), and this characterizes \"V\" up to a unique isomorphism. This expression makes sense in any category with an appropriate replacement for the tensor product of vector spaces. For any monoidal category (\"C\", ⊗) one may attempt to define a dual of an object \"V\" to be an object \"V\" ∈ \"C\" with a natural isomorphism of bifunctors\nFor a well-behaved notion of duality, this map should be not only natural in the sense of category theory, but also respect the monoidal structure in some way. An actual definition of a dual object is thus more complicated.\n\nIn a closed monoidal category \"C\", i.e. a monoidal category with an internal Hom functor, an alternative approach is to simulate the standard definition of a dual vector space as a space of functionals. For an object \"V\" ∈ \"C\" define \"V\" to be formula_1, where 1 is the monoidal identity. In some cases, this object will be a dual object to \"V\" in a sense above, but in general it leads to a different theory.\n\nConsider an object formula_2 in a monoidal category formula_3. The object formula_4 is called a left dual of formula_2 if there exist two morphsims\nsuch that the following two diagrams commute\nThe object formula_2 is called the right dual of formula_4. Left duals are canonically isomorphic when they exist, as are right duals. When \"C\" is braided (or symmetric), every left dual is also a right dual, and vice versa.\n\nIf we consider a monoidal category as a bicategory with one object, a dual pair is exactly an adjoint pair.\n\n\nA monoidal category where every object has a left (respectively right) dual is sometimes called a left (respectively right) autonomous category. Algebraic geometers call it a left (respectively right) rigid category. A monoidal category where every object has both a left and a right dual is called an autonomous category. An autonomous category that is also symmetric is called a compact closed category.\n\n\n"}
{"id": "191933", "url": "https://en.wikipedia.org/wiki?curid=191933", "title": "Exponential growth", "text": "Exponential growth\n\nExponential growth is exhibited when the rate of change—the change per instant or unit of time—of the value of a mathematical function is proportional to the function's current value, resulting in its value at any time being an exponential function of time, i.e., a function in which the time value is the exponent.\nExponential decay occurs in the same way when the growth rate is negative. In the case of a discrete domain of definition with equal intervals, it is also called geometric growth or geometric decay, the function values forming a geometric progression. In either exponential growth or exponential decay, the ratio of the rate of change of the quantity to its current size remains constant over time.\n\nThe formula for exponential growth of a variable \"x\" at the growth rate \"r\", as time \"t\" goes on in discrete intervals (that is, at integer times 0, 1, 2, 3, ...), is\n\nwhere \"x\" is the value of \"x\" at time 0. This formula is transparent when the exponents are converted to multiplication. For instance, with a starting value of 50 and a growth rate of per interval, the passage of one interval would give ; two intervals would give ; and three intervals would give . In this way, each increase in the exponent by a full interval can be seen to increase the previous total by another five percent. (The order of multiplication does not change the result based on the associative property of multiplication.)\n\nSince the time variable, which is the input to this function, occurs as the exponent, this is an exponential function. This contrasts with growth based on a power function, where the time variable is the base value raised to a fixed exponent, such as cubic growth (or in general terms denoted as polynomial growth).\n\n\nA quantity \"x\" depends exponentially on time \"t\" if\n\nwhere the constant \"a\" is the initial value of \"x\",\n\nthe constant \"b\" is a positive growth factor, and \"τ\" is the time constant—the time required for \"x\" to increase by one factor of \"b\":\n\nIf and , then \"x\" has exponential growth. If and , or and 0 < , then \"x\" has exponential decay.\n\nExample: \"If a species of bacteria doubles every ten minutes, starting out with only one bacterium, how many bacteria would be present after one hour?\" The question implies \"a\" = 1, \"b\" = 2 and \"τ\" = 10 min.\n\nAfter one hour, or six ten-minute intervals, there would be sixty-four bacteria.\n\nMany pairs (\"b\", \"τ\") of a dimensionless non-negative number \"b\" and an amount of time \"τ\" (a physical quantity which can be expressed as the product of a number of units and a unit of time) represent the same growth rate, with \"τ\" proportional to log \"b\". For any fixed \"b\" not equal to 1 (e.g. \"e\" or 2), the growth rate is given by the non-zero time \"τ\". For any non-zero time \"τ\" the growth rate is given by the dimensionless positive number \"b\".\n\nThus the law of exponential growth can be written in different but mathematically equivalent forms, by using a different base. The most common forms are the following:\n\nwhere \"x\" expresses the initial quantity \"x\"(0).\n\nParameters (negative in the case of exponential decay):\nThe quantities \"k\", \"τ\", and \"T\", and for a given \"p\" also \"r\", have a one-to-one connection given by the following equation (which can be derived by taking the natural logarithm of the above):\n\nwhere \"k\" = 0 corresponds to \"r\" = 0 and to \"τ\" and \"T\" being infinite.\n\nIf \"p\" is the unit of time the quotient \"t\"/\"p\" is simply the number of units of time. Using the notation \"t\" for the (dimensionless) number of units of time rather than the time itself, \"t\"/\"p\" can be replaced by \"t\", but for uniformity this has been avoided here. In this case the division by \"p\" in the last formula is not a numerical division either, but converts a dimensionless number to the correct quantity including unit.\n\nA popular approximated method for calculating the doubling time from the growth rate is the rule of 70,\ni.e. formula_9.\n\nIf a variable \"x\" exhibits exponential growth according to formula_10, then the log (to any base) of \"x\" grows linearly over time, as can be seen by taking logarithms of both sides of the exponential growth equation:\n\nThis allows an exponentially growing variable to be modeled with a log-linear model. For example, if one wishes to empirically estimate the growth rate from intertemporal data on \"x\", one can linearly regress log \"x\" on \"t\".\n\nThe exponential function formula_12 satisfies the linear differential equation:\n\nsaying that the change per instant of time of \"x\" at time \"t\" is proportional to the value of \"x\"(\"t\"), and \"x\"(\"t\") has the initial value\n\nThe differential equation is solved by direct integration:\n\nso that\n\nIn the above differential equation, if , then the quantity experiences exponential decay.\n\nFor a nonlinear variation of this growth model see logistic function.\n\nThe difference equation\n\nhas solution\n\nshowing that \"x\" experiences exponential growth.\n\nIn the long run, exponential growth of any kind will overtake linear growth of any kind (the basis of the Malthusian catastrophe) as well as any polynomial growth, i.e., for all α:\n\nThere is a whole hierarchy of conceivable growth rates that are slower than exponential and faster than linear (in the long run). See Degree of a polynomial#The degree computed from the function values.\n\nGrowth rates may also be faster than exponential. In the most extreme case, when growth increases without bound in finite time, it is called hyperbolic growth. In between exponential and hyperbolic growth lie more classes of growth behavior, like the hyperoperations beginning at tetration, and formula_20, the diagonal of the Ackermann function.\n\nExponential growth models of physical phenomena only apply within limited regions, as unbounded growth is not physically realistic. Although growth may initially be exponential, the modelled phenomena will eventually enter a region in which previously ignored negative feedback factors become significant (leading to a logistic growth model) or other underlying assumptions of the exponential growth model, such as continuity or instantaneous feedback, break down.\n\nAccording to an old legend, vizier Sissa Ben Dahir presented an Indian King Sharim with a beautiful, hand-made chessboard. The king asked what he would like in return for his gift and the courtier surprised the king by asking for one grain of rice on the first square, two grains on the second, four grains on the third etc. The king readily agreed and asked for the rice to be brought. All went well at first, but the requirement for 2 grains on the \"n\"th square demanded over a million grains on the 21st square, more than a million million ( trillion) on the 41st and there simply was not enough rice in the whole world for the final squares. (From Swirski, 2006)\n\nThe second half of the chessboard is the time when an exponentially growing influence is having a significant economic impact on an organization's overall business strategy.\n\nFrench children are told a story in which they imagine having a pond with water lily leaves floating on the surface. The lily population doubles in size every day and, if left unchecked, it will smother the pond in thirty days killing all the other living things in the water. Day after day, the plant's growth is small and so it is decided that it shall be cut down when the water lilies cover half of the pond. The children are then asked on what day will half of the pond be covered in water lilies. The solution is simple when one considers that the water lilies must double to completely cover the pond on the thirtieth day. Therefore, the water lilies will cover half of the pond on the twenty-ninth day. There is only one day to save the pond. (From Meadows \"et al\". 1972)\n\n\n"}
{"id": "39605149", "url": "https://en.wikipedia.org/wiki?curid=39605149", "title": "Fractional-order system", "text": "Fractional-order system\n\nIn the fields of dynamical systems and control theory, a fractional-order system is a dynamical system that can be modeled by a fractional differential equation containing derivatives of non-integer order. Such systems are said to have \"fractional dynamics\". Derivatives and integrals of fractional orders are used to describe objects that can be characterized by power-law nonlocality, power-law long-range dependence or fractal properties. Fractional-order systems are useful in studying the anomalous behavior of dynamical systems in physics, electrochemistry, biology, viscoelasticity and chaotic systems.\n\nA general dynamical system of fractional order can be written in the form\n\nwhere formula_2 and formula_3 are functions of the fractional derivative operator formula_4 of orders formula_5 and formula_6 and formula_7 and formula_8 are functions of time. A common special case of this is the linear time-invariant (LTI) system in one variable:\n\nThe orders formula_10 and formula_11 are in general complex quantities, but two interesting cases are when the orders are \"commensurate\"\n\nand when they are also \"rational\":\n\nWhen formula_14, the derivatives are of integer order and the system becomes an ordinary differential equation. Thus by increasing specialization, LTI systems can be of general order, commensurate order, rational order or integer order.\n\nBy applying a Laplace transform to the LTI system above, the transfer function becomes\n\nFor general orders formula_10 and formula_11 this is a non-rational transfer function. Non-rational transfer functions cannot be written as an expansion in a finite number of terms (e.g., a binomial expansion would have an infinite number of terms) and in this sense fractional orders systems can be said to have the potential for unlimited memory.\n\nExponential laws are classical approach to study dynamics of population densities, but there are many systems where dynamics undergo faster or slower-than-exponential laws. In such case the anomalous changes in dynamics may be best described by Mittag-Leffler functions.\n\nAnomalous diffusion is one more dynamic system where fractional-order systems play significant role to describe the anomalous flow in the diffusion process.\n\nViscoelasticity is the property of material in which the material exhibits its nature between purely elastic and pure fluid. In case of real materials the relationship between stress and strain given by Hooke's law and Newton's law both have obvious disadvances. So G. W. Scott Blair introduced a new relationship between stress and strain given by\n\nIn chaos theory, it has been observed that chaos occurs in dynamical systems of order 3 or more. With the introduction of fractional-order systems, some researchers study chaos in the system of total order less than 3.\n\nConsider a fractional-order initial value problem:\n\nHere, under the continuity condition on function f, one can convert the above equation into corresponding integral equation.\n\nOne can construct a solution space and define, by that equation, a continuous self-map on the solution space, then apply a fixed-point theorem, to get a fixed-point, which is the solution of above equation.\n\nFor numerical simulation of solution of the above equations, Kai Diethelm has suggested fractional linear multistep Adams–Bashforth method or quadrature methods.\n\n\n\n"}
{"id": "10396863", "url": "https://en.wikipedia.org/wiki?curid=10396863", "title": "Frugal number", "text": "Frugal number\n\nA frugal number is a natural number that has more digits than the number of digits in its prime factorization (including exponents). For example, using base-10 arithmetic, the first few frugal numbers are 125 (5), 128 (2), 243 (3), and 256 (2). Frugal numbers also exist in other bases; for instance, in binary arithmetic thirty-two is a frugal number, since 10 = 100000.\n\nThe base-10 frugal numbers up to 2000 are:\n\nThe term economical number has been used about a frugal number, but also about a number which is either frugal or equidigital.\n\n\n"}
{"id": "1042902", "url": "https://en.wikipedia.org/wiki?curid=1042902", "title": "GNU MPFR", "text": "GNU MPFR\n\nGNU MPFR (GNU Multiple Precision Floating-Point Reliably) is a GNU portable C library for arbitrary-precision binary floating-point computation with correct rounding, based on GNU Multi-Precision Library. The computation is both efficient and has a well-defined semantics: the functions are completely specified on all the possible operands and the results do not depend on the platform. This is done by copying the ideas from the ANSI/IEEE-754 standard for fixed-precision floating-point arithmetic (correct rounding and exceptions, in particular). More precisely, its main features are:\n\nMPFR is not able to track the accuracy of numbers in a whole program or expression; this is not its goal. Interval arithmetic packages like Arb, MPFI, or Real RAM implementations like iRRAM, which may be based on MPFR, can do that for the user.\n\nMPFR is needed to build the GNU Compiler Collection (GCC).\n\n"}
{"id": "8839340", "url": "https://en.wikipedia.org/wiki?curid=8839340", "title": "Graph toughness", "text": "Graph toughness\n\nIn graph theory, toughness is a measure of the connectivity of a graph. A graph is said to be -tough for a given real number if, for every integer , cannot be split into different connected components by the removal of fewer than vertices. For instance, a graph is -tough if the number of components formed by removing a set of vertices is always at most as large as the number of removed vertices. The toughness of a graph is the maximum for which it is -tough; this is a finite number for all finite graphs except the complete graphs, which by convention have infinite toughness.\n\nGraph toughness was first introduced by . Since then there has been extensive work by other mathematicians on toughness; the recent survey by lists 99 theorems and 162 papers on the subject.\n\nRemoving vertices from a path graph can split the remaining graph into as many as connected components. The maximum ratio of components to removed vertices is achieved by removing one vertex (from the interior of the path) and splitting it into two components. Therefore, paths are -tough. In contrast, removing vertices from a cycle graph leaves at most remaining connected components, and sometimes leaves exactly connected components, so a cycle is -tough.\n\nIf a graph is -tough, then one consequence (obtained by setting ) is that any set of nodes can be removed without splitting the graph in two. That is, every -tough graph is also -vertex-connected.\n\n observed that every cycle, and therefore every Hamiltonian graph, is -tough; that is, being -tough is a necessary condition for a graph to be Hamiltonian. He conjectured that the connection between toughness and Hamiltonicity goes in both directions: that there exists a threshold such that every -tough graph is Hamiltonian. Chvátal's original conjecture that would have proven Fleischner's theorem but was disproved by . The existence of a larger toughness threshold for Hamiltonicity remains open, and is sometimes called Chvátal's toughness conjecture.\n\nTesting whether a graph is -tough is co-NP-complete. That is, the decision problem whose answer is \"yes\" for a graph that is not 1-tough, and \"no\" for a graph that is 1-tough, is NP-complete. The same is true for any fixed positive rational number : testing whether a graph is -tough is co-NP-complete .\n\n\n"}
{"id": "18832302", "url": "https://en.wikipedia.org/wiki?curid=18832302", "title": "Hall–Littlewood polynomials", "text": "Hall–Littlewood polynomials\n\nIn mathematics, the Hall–Littlewood polynomials are symmetric functions depending on a parameter \"t\" and a partition λ. They are Schur functions when \"t\" is 0 and monomial symmetric functions when \"t\" is 1 and are special cases of Macdonald polynomials.\nThey were first defined indirectly by Philip Hall using the Hall algebra, and later defined directly by .\n\nThe Hall–Littlewood polynomial \"P\" is defined by\nwhere λ is a partition of at most \"n\" with elements λ, and \"m\"(\"i\") elements equal to \"i\", and \"S\" is the symmetric group of order \"n\"!.\nAs an example,\n\nWe have that formula_3, formula_4 and \nformula_5 where the latter is the Schur \"P\" polynomials.\n\nExpanding the Schur polynomials in terms of the Hall–Littlewood polynomials, one has\nwhere formula_7 are the Kostka–Foulkes polynomials.\nNote that as formula_8, these reduce to the ordinary Kostka coefficients.\n\nA combinatorial description for the Kostka–Foulkes polynomials was given by Lascoux and Schützenberger,\nwhere \"charge\" is a certain combinatorial statistic on semistandard Young tableaux,\nand the sum is taken over all semi-standard Young tableaux with shape \"λ\" and type \"μ\".\n\n\n"}
{"id": "99242", "url": "https://en.wikipedia.org/wiki?curid=99242", "title": "Hermann Grassmann", "text": "Hermann Grassmann\n\nHermann Günther Grassmann (; April 15, 1809 – September 26, 1877) was a German polymath, known in his day as a linguist and now also as a mathematician. He was also a physicist, neohumanist, general scholar, and publisher. His mathematical work was little noted until he was in his sixties.\n\nGrassmann was the third of 12 children of Justus Günter Grassmann, an ordained minister who taught mathematics and physics at the Stettin Gymnasium, where Hermann was educated.\n\nGrassmann was an undistinguished student until he obtained a high mark on the examinations for admission to Prussian universities. Beginning in 1827, he studied theology at the University of Berlin, also taking classes in classical languages, philosophy, and literature. He does not appear to have taken courses in mathematics or physics.\n\nAlthough lacking university training in mathematics, it was the field that most interested him when he returned to Stettin in 1830 after completing his studies in Berlin. After a year of preparation, he sat the examinations needed to teach mathematics in a gymnasium, but achieved a result good enough to allow him to teach only at the lower levels. Around this time, he made his first significant mathematical discoveries, ones that led him to the important ideas he set out in his 1844 paper referred to as A1 (see references).\n\nIn 1834 Grassmann began teaching mathematics at the Gewerbeschule in Berlin. A year later, he returned to Stettin to teach mathematics, physics, German, Latin, and religious studies at a new school, the Otto Schule. Over the next four years, Grassmann passed examinations enabling him to teach mathematics, physics, chemistry, and mineralogy at all secondary school levels.\n\nIn 1847, he was made an \"Oberlehrer\" or head teacher. In 1852, he was appointed to his late father's position at the Stettin Gymnasium, thereby acquiring the title of Professor. In 1847, he asked the Prussian Ministry of Education to be considered for a university position, whereupon that Ministry asked Kummer for his opinion of Grassmann. Kummer wrote back saying that Grassmann's 1846 prize essay (see below) contained \"... commendably good material expressed in a deficient form.\" Kummer's report ended any chance that Grassmann might obtain a university post. This episode proved the norm; time and again, leading figures of Grassmann's day failed to recognize the value of his mathematics.\n\nStarting during the political turmoil in Germany, 1848–49, Hermann and his brother Robert published a Stettin newspaper, \"Deutsche Wochenschrift für Staat, Kirche und Volksleben\", calling for German unification under a constitutional monarchy. (This eventuated in 1871.) After writing a series of articles on constitutional law, Hermann parted company with the newspaper, finding himself increasingly at odds with its political direction.\n\nGrassmann had eleven children, seven of whom reached adulthood. A son, Hermann Ernst Grassmann, became a professor of mathematics at the University of Giessen.\n\nOne of the many examinations for which Grassmann sat required that he submit an essay on the theory of the tides. In 1840, he did so, taking the basic theory from Laplace's \"Mécanique céleste\" and from Lagrange's \"Mécanique analytique\", but expositing this theory making use of the vector methods he had been mulling over since 1832. This essay, first published in the \"Collected Works\" of 1894–1911, contains the first known appearance of what is now called linear algebra and the notion of a vector space. He went on to develop those methods in his A1 and A2 (see bibliography).\n\nIn 1844, Grassmann published his masterpiece, his \"Die Lineale Ausdehnungslehre, ein neuer Zweig der Mathematik\" [The Theory of Linear Extension, a New Branch of Mathematics], hereinafter denoted A1 and commonly referred to as the \"Ausdehnungslehre,\" which translates as \"theory of extension\" or \"theory of extensive magnitudes.\" Since A1 proposed a new foundation for all of mathematics, the work began with quite general definitions of a philosophical nature. Grassmann then showed that once geometry is put into the algebraic form he advocated, the number three has no privileged role as the number of spatial dimensions; the number of possible dimensions is in fact unbounded.\n\nFearnley-Sander (1979) describes Grassmann's foundation of linear algebra as follows:\n\nFollowing an idea of Grassmann's father, A1 also defined the exterior product, also called \"combinatorial product\" (in German: \"äußeres Produkt\" or \"kombinatorisches Produkt\"), the key operation of an algebra now called exterior algebra. (One should keep in mind that in Grassmann's day, the only axiomatic theory was Euclidean geometry, and the general notion of an abstract algebra had yet to be defined.) In 1878, William Kingdon Clifford joined this exterior algebra to William Rowan Hamilton's quaternions by replacing Grassmann's rule \"ee\" = 0 by the rule \"ee\" = 1. (For quaternions, we have the rule \"i\" = \"j\" = \"k\" = −1.) For more details, see exterior algebra.\n\nA1 was a revolutionary text, too far ahead of its time to be appreciated. When Grassmann submitted it to apply for a professorship in 1847, the ministry asked Ernst Kummer for a report. Kummer assured that there were good ideas in it, but found the exposition deficient and advised against giving Grassmann a university position. Over the next 10-odd years, Grassmann wrote a variety of work applying his theory of extension, including his 1845 \"Neue Theorie der Elektrodynamik\" and several papers on algebraic curves and surfaces, in the hope that these applications would lead others to take his theory seriously.\n\nIn 1846, Möbius invited Grassmann to enter a competition to solve a problem first proposed by Leibniz: to devise a geometric calculus devoid of coordinates and metric properties (what Leibniz termed \"analysis situs\"). Grassmann's \"Geometrische Analyse geknüpft an die von Leibniz erfundene geometrische Charakteristik\", was the winning entry (also the only entry). Möbius, as one of the judges, criticized the way Grassmann introduced abstract notions without giving the reader any intuition as to why those notions were of value.\n\nIn 1853, Grassmann published a theory of how colors mix; it and its three color laws are still taught, as Grassmann's law. Grassmann's work on this subject was inconsistent with that of Helmholtz. Grassmann also wrote on crystallography, electromagnetism, and mechanics.\n\nGrassmann (1861) set out the first axiomatic presentation of arithmetic, making free use of the principle of induction. Peano and his followers cited this work freely starting around 1890. Lloyd C. Kannenberg published an English translation of The Ausdehnungslehre and Other works in 1995 (. -- ).\n\nIn 1862, Grassmann published a thoroughly rewritten second edition of A1, hoping to earn belated recognition for his theory of extension, and containing the definitive exposition of his linear algebra. The result, \"Die Ausdehnungslehre: Vollständig und in strenger Form bearbeitet\" [The Theory of Extension, Thoroughly and Rigorously Treated], hereinafter denoted A2, fared no better than A1, even though A2's manner of exposition anticipates the textbooks of the 20th century.\n\nIn 1840s, mathematicians were generally unprepared to understand Grassmann's ideas. In the 1860s and 1870s various mathematicians came to ideas similar to that of Grassmann's, but Grassmann himself was not interested in mathematics anymore.\n\nOne of the first mathematicians to appreciate Grassmann's ideas during his lifetime was Hermann Hankel, whose 1867 \"Theorie der complexen Zahlensysteme\"\n\nIn 1872 Victor Schlegel published the first part of his \"System der Raumlehre\" which used Grassmann's approach to derive ancient and modern results in plane geometry. Felix Klein wrote a negative review of Schlegel's book citing its incompleteness and lack of perspective on Grassmann. Schlegel followed in 1875 with a second part of his \"System\" according to Grassmann, this time developing higher geometry. Meanwhile, Klein was advancing his Erlangen Program which also expanded the scope of geometry.\n\nComprehension of Grassmann awaited the concept of vector spaces which then could express the multilinear algebra of his extension theory. To establish the priority of Grassmann over Hamilton, Josiah Willard Gibbs urged Grassmann's heirs to have the 1840 essay on tides published. A. N. Whitehead's first monograph, the \"Universal Algebra\" (1898), included the first systematic exposition in English of the theory of extension and the exterior algebra. With the rise of differential geometry the exterior algebra was applied to differential forms.\n\nFor an introduction to the role of Grassmann's work in contemporary mathematical physics see \"The Road to Reality\" by Roger Penrose.\n\nAdhémar Jean Claude Barré de Saint-Venant developed a vector calculus similar to that of Grassmann which he published in 1845. He then entered into a dispute with Grassmann about which of the two had thought of the ideas first. Grassmann had published his results in 1844, but Saint-Venant claimed that he had first developed these ideas in 1832.\n\nGrassmann's mathematical ideas began to spread only towards the end of his life. 30 years after the publication of A1 the publisher wrote to Grassmann: “Your book \"Die Ausdehnungslehre\" has been out of print for some time. Since your work hardly sold at all, roughly 600 copies were used in 1864 as waste paper and the remaining few odd copies have now been sold out, with the exception of the one copy in our library”. Disappointed by the reception of his work in mathematical circles, Grassmann lost his contacts with mathematicians as well as his interest in geometry. The last years of his life he turned to historical linguistics and the study of Sanskrit. He wrote books on German grammar, collected folk songs, and learned Sanskrit. He wrote a 2,000-page dictionary and a translation of the Rigveda (more than 1,000 pages) which earned him a membership of the American Orientalists' Society. In modern\nRigvedic studies Grassmann's work is often cited. In 1955 the third edition of his dictionary to Rigveda was issued.\n\nGrassmann also discovered a sound law of Indo-European languages, which was named \"Grassmann's Law\" in his honor.\n\nThese philological accomplishments were honored during his lifetime; he was elected to the American Oriental Society and in 1876, he received an honorary doctorate from the University of Tübingen.\n\n\n\n\nExtensive online bibliography, revealing substantial contemporary interest in Grassmann's life and work. References each chapter in Schubring.\n\n"}
{"id": "15532", "url": "https://en.wikipedia.org/wiki?curid=15532", "title": "Integral", "text": "Integral\n\nIn mathematics, an integral assigns numbers to functions in a way that can describe displacement, area, volume, and other concepts that arise by combining infinitesimal data. Integration is one of the two main operations of calculus, with its inverse operation, differentiation, being the other. Given a function of a real variable and an interval of the real line, the definite integral\n\nis defined informally as the signed area of the region in the -plane that is bounded by the graph of , the -axis and the vertical lines and . The area above the -axis adds to the total and that below the -axis subtracts from the total.\n\nThe operation of integration, up to an additive constant, is the inverse of the operation of differentiation. For this reason, the term \"integral\" may also refer to the related notion of the antiderivative, a function whose derivative is the given function . In this case, it is called an indefinite integral and is written:\n\nThe integrals discussed in this article are those termed \"definite integrals\". It is the fundamental theorem of calculus that connects differentiation with the definite integral: if is a continuous real-valued function defined on a closed interval , then, once an antiderivative of is known, the definite integral of over that interval is given by\n\nThe principles of integration were formulated independently by Isaac Newton and Gottfried Wilhelm Leibniz in the late 17th century, who thought of the integral as an infinite sum of rectangles of infinitesimal width. Bernhard Riemann gave a rigorous mathematical definition of integrals. It is based on a limiting procedure that approximates the area of a curvilinear region by breaking the region into thin vertical slabs. Beginning in the nineteenth century, more sophisticated notions of integrals began to appear, where the type of the function as well as the domain over which the integration is performed has been generalised. A line integral is defined for functions of two or more variables, and the interval of integration is replaced by a curve connecting the two endpoints. In a surface integral, the curve is replaced by a piece of a surface in three-dimensional space.\n\nThe first documented systematic technique capable of determining integrals is the method of exhaustion of the ancient Greek astronomer Eudoxus (\"ca.\" 370 BC), which sought to find areas and volumes by breaking them up into an infinite number of divisions for which the area or volume was known. This method was further developed and employed by Archimedes in the 3rd century BC and used to calculate areas for parabolas and an approximation to the area of a circle.\n\nA similar method was independently developed in China around the 3rd century AD by Liu Hui, who used it to find the area of the circle. This method was later used in the 5th century by Chinese father-and-son mathematicians Zu Chongzhi and Zu Geng to find the volume of a sphere (; ).\n\nThe next significant advances in integral calculus did not begin to appear until the 17th century. At this time, the work of Cavalieri with his method of Indivisibles, and work by Fermat, began to lay the foundations of modern calculus, with Cavalieri computing the integrals of up to degree in Cavalieri's quadrature formula. Further steps were made in the early 17th century by Barrow and Torricelli, who provided the first hints of a connection between integration and differentiation. Barrow provided the first proof of the fundamental theorem of calculus. Wallis generalized Cavalieri's method, computing integrals of to a general power, including negative powers and fractional powers.\n\nThe major advance in integration came in the 17th century with the independent discovery of the fundamental theorem of calculus by Newton and Leibniz. The theorem demonstrates a connection between integration and differentiation. This connection, combined with the comparative ease of differentiation, can be exploited to calculate integrals. In particular, the fundamental theorem of calculus allows one to solve a much broader class of problems. Equal in importance is the comprehensive mathematical framework that both Newton and Leibniz developed. Given the name infinitesimal calculus, it allowed for precise analysis of functions within continuous domains. This framework eventually became modern calculus, whose notation for integrals is drawn directly from the work of Leibniz.\nWhile Newton and Leibniz provided a systematic approach to integration, their work lacked a degree of rigour. Bishop Berkeley memorably attacked the vanishing increments used by Newton, calling them \"ghosts of departed quantities\". Calculus acquired a firmer footing with the development of limits. Integration was first rigorously formalized, using limits, by Riemann. Although all bounded piecewise continuous functions are Riemann-integrable on a bounded interval, subsequently more general functions were considered—particularly in the context of Fourier analysis—to which Riemann's definition does not apply, and Lebesgue formulated a different definition of integral, founded in measure theory (a subfield of real analysis). Other definitions of integral, extending Riemann's and Lebesgue's approaches, were proposed. These approaches based on the real number system are the ones most common today, but alternative approaches exist, such as a definition of integral as the standard part of an infinite Riemann sum, based on the hyperreal number system.\n\nIsaac Newton used a small vertical bar above a variable to indicate integration, or placed the variable inside a box. The vertical bar was easily confused with or , which are used to indicate differentiation, and the box notation was difficult for printers to reproduce, so these notations were not widely adopted.\n\nThe modern notation for the indefinite integral was introduced by Gottfried Wilhelm Leibniz in 1675 (; ). He adapted the integral symbol, ∫, from the letter \"ſ\" (long s), standing for \"summa\" (written as \"ſumma\"; Latin for \"sum\" or \"total\"). The modern notation for the definite integral, with limits above and below the integral sign, was first used by Joseph Fourier in \"Mémoires\" of the French Academy around 1819–20, reprinted in his book of 1822 (; ).\n\nIntegrals are used extensively in many areas of mathematics as well as in many other areas that rely on mathematics.\n\nFor example, in probability theory, integrals are used to determine the probability of some random variable falling within a certain range. Moreover, the integral under an entire probability density function must equal 1, which provides a test of whether a function with no negative values could be a density function or not.\n\nIntegrals can be used for computing the area of a two-dimensional region that has a curved boundary, as well as computing the volume of a three-dimensional object that has a curved boundary.\n\nIntegrals are also used in physics, in areas like kinematics to find quantities like displacement, time, and velocity. For example, in rectilinear motion, the displacement of an object over the time interval formula_4 is given by: \n\nwhere formula_6 is the velocity expressed as a function of time. The work done by a force formula_7 (given as a function of position) from an initial position formula_8 to a final position formula_9 is:\n\nIntegrals are also used in thermodynamics, where thermodynamic integration is used to calculate the difference in free energy between two given states.\n\nThe integral with respect to of a real-valued function of a real variable on the interval is written as\n\nThe integral sign represents integration. The symbol , called the differential of the variable , indicates that the variable of integration is . The function to be integrated is called the integrand. The symbol is separated from the integrand by a space (as shown). If a function has an integral, it is said to be integrable. The points and are called the limits of the integral. An integral where the limits are specified is called a definite integral. The integral is said to be over the interval .\n\nIf the integral goes from a finite value \"a\" to the upper limit infinity, the integral expresses the limit of the integral from \"a\" to a value \"b\" as \"b\" goes to infinity. If the value of the integral gets closer and closer to a finite value, the integral is said to converge to that value. If not, the integral is said to diverge.\n\nWhen the limits are omitted, as in\nthe integral is called an indefinite integral, which represents a class of functions (the antiderivative) whose derivative is the integrand. The fundamental theorem of calculus relates the evaluation of definite integrals to indefinite integrals. Occasionally, limits of integration are omitted for definite integrals when the same limits occur repeatedly in a particular context. Usually, the author will make this convention clear at the beginning of the relevant text.\n\nThere are several extensions of the notation for integrals to encompass integration on unbounded domains and/or in multiple dimensions (see later sections of this article).\n\nHistorically, the symbol \"dx\" was taken to represent an infinitesimally \"small piece\" of the independent variable \"x\" to be multiplied by the integrand and summed up in an infinite sense. While this notion is still heuristically useful, later mathematicians have deemed infinitesimal quantities to be untenable from the standpoint of the real number system. In introductory calculus, the expression \"dx\" is therefore not assigned an independent meaning; instead, it is viewed as part of the symbol for integration and serves as its delimiter on the right side of the expression being integrated. \n\nIn more sophisticated contexts, \"dx\" can have its own significance, the meaning of which depending on the particular area of mathematics being discussed. When used in one of these ways, the original Leibnitz notation is co-opted to apply to a generalization of the original definition of the integral. Some common interpretations of \"dx\" include: an integrator function in Riemann-Stieltjes integration (indicated by \"dα\"(\"x\") in general), a measure in Lebesgue theory (indicated by \"dμ\" in general), or a differential form in exterior calculus (indicated by formula_13 in general). In the last case, even the letter \"d\" has an independent meaning — as the exterior derivative operator on differential forms.\n\nConversely, in advanced settings, it is not uncommon to leave out \"dx\" when only the simple Riemann integral is being used, or the exact type of integral is immaterial. For instance, one might write formula_14 to express the linearity of the integral, a property shared by the Riemann integral and all generalizations thereof.\n\nIn modern Arabic mathematical notation, a reflected integral symbol is used instead of the symbol , since the Arabic script and mathematical expressions go right to left.\n\nSome authors, particularly of European origin, use an upright \"d\" to indicate the variable of integration (i.e., instead of ), since properly speaking, \"d\" is not a variable. \n\nThe symbol is not always placed after , as for instance in\nIn the first expression, the differential is treated as an infinitesimal \"multiplicative\" factor, formally following a \"commutative property\" when \"multiplied\" by the expression 3/(\"x\"+1). In the second expression, showing the differentials first highlights and clarifies the variables that are being integrated with respect to, a practice particularly popular with physicists.\n\nIntegrals appear in many practical situations. If a swimming pool is rectangular with a flat bottom, then from its length, width, and depth we can easily determine the volume of water it can contain (to fill it), the area of its surface (to cover it), and the length of its edge (to rope it). But if it is oval with a rounded bottom, all of these quantities call for integrals. Practical approximations may suffice for such trivial examples, but precision engineering (of any discipline) requires exact and rigorous values for these elements.\nTo start off, consider the curve between and with (see figure). We ask:\nand call this (yet unknown) area the (definite) integral of . The notation for this integral will be\n\nAs a first approximation, look at the unit square given by the sides to and and . Its area is exactly 1. Actually, the true value of the integral must be somewhat less than 1. Decreasing the width of the approximation rectangles and increasing the number of rectangles gives a better result; so cross the interval in five steps, using the approximation points 0, 1/5, 2/5, and so on to 1. Fit a box for each step using the right end height of each curve piece, thus , , and so on to . Summing the areas of these rectangles, we get a better approximation for the sought integral, namely\n\nWe are taking a sum of finitely many function values of , multiplied with the differences of two subsequent approximation points. We can easily see that the approximation is still too large. Using more steps produces a closer approximation, but will always be too high and will never be exact. Alternatively, replacing these subintervals by ones with the left end height of each piece, we will get an approximation that is too low: for example, with twelve such subintervals we will get an approximate value for the area of 0.6203. \n\nThe key idea is the transition from adding \"finitely many\" differences of approximation points multiplied by their respective function values to using infinitely many fine, or \"infinitesimal\" steps. When this transition is completed in the above example, it turns out that the area under the curve within the stated bounds is 2/3.\n\nThe notation\nconceives the integral as a weighted sum, denoted by the elongated , of function values, , multiplied by infinitesimal step widths, the so-called \"differentials\", denoted by .\n\nHistorically, after the failure of early efforts to rigorously interpret infinitesimals, Riemann formally defined integrals as a limit of weighted sums, so that the suggested the limit of a difference (namely, the interval width). Shortcomings of Riemann's dependence on intervals and continuity motivated newer definitions, especially the Lebesgue integral, which is founded on an ability to extend the idea of \"measure\" in much more flexible ways. Thus the notation\nrefers to a weighted sum in which the function values are partitioned, with measuring the weight to be assigned to each value. Here denotes the region of integration.\n\nThere are many ways of formally defining an integral, not all of which are equivalent. The differences exist mostly to deal with differing special cases which may not be integrable under other definitions, but also occasionally for pedagogical reasons. The most commonly used definitions of integral are Riemann integrals and Lebesgue integrals.\n\nThe Riemann integral is defined in terms of Riemann sums of functions with respect to \"tagged partitions\" of an interval. Let be a closed interval of the real line; then a \"tagged partition\" of is a finite sequence\n\nThis partitions the interval into sub-intervals indexed by , each of which is \"tagged\" with a distinguished point . A \"Riemann sum\" of a function with respect to such a tagged partition is defined as\nthus each term of the sum is the area of a rectangle with height equal to the function value at the distinguished point of the given sub-interval, and width the same as the sub-interval width. Let be the width of sub-interval ; then the \"mesh\" of such a tagged partition is the width of the largest sub-interval formed by the partition, . The \"Riemann integral\" of a function over the interval is equal to if:\nWhen the chosen tags give the maximum (respectively, minimum) value of each interval, the Riemann sum becomes an upper (respectively, lower) Darboux sum, suggesting the close connection between the Riemann integral and the Darboux integral.\n\nIt is often of interest, both in theory and applications, to be able to pass to the limit under the integral. For instance, a sequence of functions can frequently be constructed that approximate, in a suitable sense, the solution to a problem. Then the integral of the solution function should be the limit of the integrals of the approximations. However, many functions that can be obtained as limits are not Riemann-integrable, and so such limit theorems do not hold with the Riemann integral. Therefore, it is of great importance to have a definition of the integral that allows a wider class of functions to be integrated .\n\nSuch an integral is the Lebesgue integral, that exploits the following fact to enlarge the class of integrable functions: if the values of a function are rearranged over the domain, the integral of a function should remain the same. Thus Henri Lebesgue introduced the integral bearing his name, explaining this integral thus in a letter to Paul Montel:\nAs puts it, \"To compute the Riemann integral of , one partitions the domain into subintervals\", while in the Lebesgue integral, \"one is in effect partitioning the range of \". The definition of the Lebesgue integral thus begins with a measure, μ. In the simplest case, the Lebesgue measure of an interval is its width, , so that the Lebesgue integral agrees with the (proper) Riemann integral when both exist. In more complicated cases, the sets being measured can be highly fragmented, with no continuity and no resemblance to intervals.\n\nUsing the \"partitioning the range of \" philosophy, the integral of a non-negative function should be the sum over of the areas between a thin horizontal strip between and . This area is just . Let }. The Lebesgue integral of is then defined by \nwhere the integral on the right is an ordinary improper Riemann integral ( is a strictly decreasing positive function, and therefore has a well-defined improper Riemann integral). For a suitable class of functions (the measurable functions) this defines the Lebesgue integral.\n\nA general measurable function is Lebesgue-integrable if the sum of the absolute values of the areas of the regions between the graph of and the -axis is finite:\nIn that case, the integral is, as in the Riemannian case, the difference between the area above the -axis and the area below the -axis:\nwhere\n\nAlthough the Riemann and Lebesgue integrals are the most widely used definitions of the integral, a number of others exist, including:\n\nThe collection of Riemann-integrable functions on a closed interval forms a vector space under the operations of pointwise addition and multiplication by a scalar, and the operation of integration\n\nis a linear functional on this vector space. Thus, firstly, the collection of integrable functions is closed under taking linear combinations; and, secondly, the integral of a linear combination is the linear combination of the integrals,\n\nSimilarly, the set of real-valued Lebesgue-integrable functions on a given measure space with measure is closed under taking linear combinations and hence form a vector space, and the Lebesgue integral\n\nis a linear functional on this vector space, so that\n\nMore generally, consider the vector space of all measurable functions on a measure space , taking values in a locally compact complete topological vector space over a locally compact topological field . Then one may define an abstract integration map assigning to each function an element of or the symbol ,\nthat is compatible with linear combinations. In this situation, the linearity holds for the subspace of functions whose integral is an element of (i.e. \"finite\"). The most important special cases arise when is , , or a finite extension of the field of p-adic numbers, and is a finite-dimensional vector space over , and when and is a complex Hilbert space.\n\nLinearity, together with some natural continuity properties and normalisation for a certain class of \"simple\" functions, may be used to give an alternative definition of the integral. This is the approach of Daniell for the case of real-valued functions on a set , generalized by Nicolas Bourbaki to functions with values in a locally compact topological vector space. See for an axiomatic characterisation of the integral.\n\nA number of general inequalities hold for Riemann-integrable functions defined on a closed and bounded interval and can be generalized to other notions of integral (Lebesgue and Daniell).\n\n\n\n\n\n\n\nIn this section, is a real-valued Riemann-integrable function. The integral\nover an interval is defined if . This means that the upper and lower sums of the function are evaluated on a partition whose values are increasing. Geometrically, this signifies that integration takes place \"left to right\", evaluating within intervals where an interval with a higher index lies to the right of one with a lower index. The values and , the end-points of the interval, are called the limits of integration of . Integrals can also be defined if :\n\nThis, with , implies:\n\nThe first convention is necessary in consideration of taking integrals over subintervals of ; the second says that an integral taken over a degenerate interval, or a point, should be zero. One reason for the first convention is that the integrability of on an interval implies that is integrable on any subinterval , but in particular integrals have the property that:\n\nWith the first convention, the resulting relation\nis then well-defined for any cyclic permutation of , , and .\n\nThe \"fundamental theorem of calculus\" is the statement that differentiation and integration are inverse operations: if a continuous function is first integrated and then differentiated, the original function is retrieved. An important consequence, sometimes called the \"second fundamental theorem of calculus\", allows one to compute integrals by using an antiderivative of the function to be integrated.\n\nLet be a continuous real-valued function defined on a closed interval . Let be the function defined, for all in , by\nThen, is continuous on , differentiable on the open interval , and\n\nfor all in .\n\nLet be a real-valued function defined on a closed interval [] that admits an antiderivative on . That is, and are functions such that for all in ,\n\nIf is integrable on then\n\nThe second fundamental theorem allows many integrals to be calculated explicitly. For example, to calculate the integral\nof the square root function between 0 and 1, it is sufficient to find an antiderivative, that is, a function whose derivative equals :\nOne such function is . Then the value of the integral in question is\n\nThis is a case of a general rule, that for , with , the related function, the so-called antiderivative is Tables of this and similar antiderivatives can be used to calculate integrals explicitly, in much the same way that tables of derivatives can be used.\n\nA \"proper\" Riemann integral assumes the integrand is defined and finite on a closed and bounded interval, bracketed by the limits of integration. An improper integral occurs when one or more of these conditions is not satisfied. In some cases such integrals may be defined by considering the limit of a sequence of proper Riemann integrals on progressively larger intervals.\n\nIf the interval is unbounded, for instance at its upper end, then the improper integral is the limit as that endpoint goes to infinity.\nIf the integrand is only defined or finite on a half-open interval, for instance , then again a limit may provide a finite result.\n\nThat is, the improper integral is the limit of proper integrals as one endpoint of the interval of integration approaches either a specified real number, or , or . In more complicated cases, limits are required at both endpoints, or at interior points.\n\nJust as the definite integral of a positive function of one variable represents the area of the region between the graph of the function and the \"x\"-axis, the \"double integral\" of a positive function of two variables represents the volume of the region between the surface defined by the function and the plane that contains its domain. For example, a function in two dimensions depends on two real variables, \"x\" and \"y\", and the integral of a function \"f\" over the rectangle \"R\" given as the Cartesian product of two intervals formula_56 can be written\n\nwhere the differential indicates that integration is taken with respect to area. This double integral can be defined using Riemann sums, and represents the (signed) volume under the graph of over the domain \"R\". Under suitable conditions (e.g., if \"f\" is continuous), then Fubini's theorem guarantees that this integral can be expressed as an equivalent iterated integral\n\nThis reduces the problem of computing a double integral to computing one-dimensional integrals. Because of this, another notation for the integral over \"R\" uses a double integral sign:\n\nIntegration over more general domains is possible. The integral of a function \"f\", with respect to volume, over a subset \"D\" of ℝ is denoted by notation such as\n\nor similar. See volume integral.\n\nThe concept of an integral can be extended to more general domains of integration, such as curved lines and surfaces. Such integrals are known as line integrals and surface integrals respectively. These have important applications in physics, as when dealing with vector fields.\n\nA \"line integral\" (sometimes called a \"path integral\") is an integral where the function to be integrated is evaluated along a curve. Various different line integrals are in use. In the case of a closed curve it is also called a \"contour integral\".\n\nThe function to be integrated may be a scalar field or a vector field. The value of the line integral is the sum of values of the field at all points on the curve, weighted by some scalar function on the curve (commonly arc length or, for a vector field, the scalar product of the vector field with a differential vector in the curve). This weighting distinguishes the line integral from simpler integrals defined on intervals. Many simple formulas in physics have natural continuous analogs in terms of line integrals; for example, the fact that work is equal to force, , multiplied by displacement, , may be expressed (in terms of vector quantities) as:\nFor an object moving along a path in a vector field such as an electric field or gravitational field, the total work done by the field on the object is obtained by summing up the differential work done in moving from to . This gives the line integral\n\nA \"surface integral\" is a definite integral taken over a surface (which may be a curved set in space); it can be thought of as the double integral analog of the line integral. The function to be integrated may be a scalar field or a vector field. The value of the surface integral is the sum of the field at all points on the surface. This can be achieved by splitting the surface into surface elements, which provide the partitioning for Riemann sums.\n\nFor an example of applications of surface integrals, consider a vector field on a surface ; that is, for each point in , is a vector. Imagine that we have a fluid flowing through , such that determines the velocity of the fluid at . The flux is defined as the quantity of fluid flowing through in unit amount of time. To find the flux, we need to take the dot product of with the unit surface normal to at each point, which will give us a scalar field, which we integrate over the surface:\nThe fluid flux in this example may be from a physical fluid such as water or air, or from electrical or magnetic flux. Thus surface integrals have applications in physics, particularly with the classical theory of electromagnetism.\n\nIn complex analysis, the integrand is a complex-valued function of a complex variable instead of a real function of a real variable . When a complex function is integrated along a curve formula_64 in the complex plane, the integral is denoted as follows\n\nThis is known as a contour integral.\n\nA differential form is a mathematical concept in the fields of multivariable calculus, differential topology, and tensors. Differential forms are organized by degree. For example, a one-form is a weighted sum of the differentials of the coordinates, such as:\nwhere \"E\", \"F\", \"G\" are functions in three dimensions. A differential one-form can be integrated over an oriented path, and the resulting integral is just another way of writing a line integral. Here the basic differentials \"dx\", \"dy\", \"dz\" measure infinitesimal oriented lengths parallel to the three coordinate axes.\n\nA differential two-form is a sum of the form\nHere the basic two-forms formula_68 measure oriented areas parallel to the coordinate two-planes. The symbol formula_69 denotes the wedge product, which is similar to the cross product in the sense that the wedge product of two forms representing oriented lengths represents an oriented area. A two-form can be integrated over an oriented surface, and the resulting integral is equivalent to the surface integral giving the flux of formula_70.\n\nUnlike the cross product, and the three-dimensional vector calculus, the wedge product and the calculus of differential forms makes sense in arbitrary dimension and on more general manifolds (curves, surfaces, and their higher-dimensional analogs). The exterior derivative plays the role of the gradient and curl of vector calculus, and Stokes' theorem simultaneously generalizes the three theorems of vector calculus: the divergence theorem, Green's theorem, and the Kelvin-Stokes theorem.\n\nThe discrete equivalent of integration is summation. Summations and integrals can be put on the same foundations using the theory of Lebesgue integrals or time scale calculus.\n\nThe most basic technique for computing definite integrals of one real variable is based on the fundamental theorem of calculus. Let be the function of to be integrated over a given interval . Then, find an antiderivative of ; that is, a function such that on the interval. Provided the integrand and integral have no singularities on the path of integration, by the fundamental theorem of calculus,\n\nThe integral is not actually the antiderivative, but the fundamental theorem provides a way to use antiderivatives to evaluate definite integrals.\n\nThe most difficult step is usually to find the antiderivative of . It is rarely possible to glance at a function and write down its antiderivative. More often, it is necessary to use one of the many techniques that have been developed to evaluate integrals. Most of these techniques rewrite one integral as a different one which is hopefully more tractable. Techniques include:\nAlternative methods exist to compute more complex integrals. Many nonelementary integrals can be expanded in a Taylor series and integrated term by term. Occasionally, the resulting infinite series can be summed analytically. The method of convolution using Meijer G-functions can also be used, assuming that the integrand can be written as a product of Meijer G-functions. There are also many less common ways of calculating definite integrals; for instance, Parseval's identity can be used to transform an integral over a rectangular region into an infinite sum. Occasionally, an integral can be evaluated by a trick; for an example of this, see Gaussian integral.\n\nComputations of volumes of solids of revolution can usually be done with disk integration or shell integration.\n\nSpecific results which have been worked out by various techniques are collected in the list of integrals.\n\nMany problems in mathematics, physics, and engineering involve integration where an explicit formula for the integral is desired. Extensive tables of integrals have been compiled and published over the years for this purpose. With the spread of computers, many professionals, educators, and students have turned to computer algebra systems that are specifically designed to perform difficult or tedious tasks, including integration. Symbolic integration has been one of the motivations for the development of the first such systems, like Macsyma.\n\nA major mathematical difficulty in symbolic integration is that in many cases, a closed formula for the antiderivative of a rather simple-looking function does not exist. For instance, it is known that the antiderivatives of the functions and cannot be expressed in the closed form involving only rational and exponential functions, logarithm, trigonometric functions and inverse trigonometric functions, and the operations of multiplication and composition; in other words, none of the three given functions is integrable in elementary functions, which are the functions which may be built from rational functions, roots of a polynomial, logarithm, and exponential functions. The Risch algorithm provides a general criterion to determine whether the antiderivative of an elementary function is elementary, and, if it is, to compute it. Unfortunately, it turns out that functions with closed expressions of antiderivatives are the exception rather than the rule. Consequently, computerized algebra systems have no hope of being able to find an antiderivative for a randomly constructed elementary function. On the positive side, if the 'building blocks' for antiderivatives are fixed in advance, it may be still be possible to decide whether the antiderivative of a given function can be expressed using these blocks and operations of multiplication and composition, and to find the symbolic answer whenever it exists. The Risch algorithm, implemented in Mathematica and other computer algebra systems, does just that for functions and antiderivatives built from rational functions, radicals, logarithm, and exponential functions.\n\nSome special integrands occur often enough to warrant special study. In particular, it may be useful to have, in the set of antiderivatives, the special functions (like the Legendre functions, the hypergeometric function, the gamma function, the incomplete gamma function and so on — see Symbolic integration for more details). Extending the Risch's algorithm to include such functions is possible but challenging and has been an active research subject.\n\nMore recently a new approach has emerged, using \"D\"-finite functions, which are the solutions of linear differential equations with polynomial coefficients. Most of the elementary and special functions are \"D\"-finite, and the integral of a \"D\"-finite function is also a \"D\"-finite function. This provides an algorithm to express the antiderivative of a \"D\"-finite function as the solution of a differential equation.\n\nThis theory also allows one to compute the definite integral of a \"D\"-function as the sum of a series given by the first coefficients, and provides an algorithm to compute any coefficient.\n\nSome integrals found in real applications can be computed by closed-form antiderivatives. Others are not so accommodating. Some antiderivatives do not have closed forms, some closed forms require special functions that themselves are a challenge to compute, and others are so complex that finding the exact answer is too slow. This motivates the study and application of numerical approximations of integrals. This subject, called \"numerical integration\" or \"numerical quadrature\", arose early in the study of integration for the purpose of making hand calculations. The development of general-purpose computers made numerical integration more practical and drove a desire for improvements. The goals of numerical integration are accuracy, reliability, efficiency, and generality, and sophisticated modern methods can vastly outperform a naive method by all four measures (; ; ).\n\nConsider, for example, the integral\nwhich has the exact answer . (In ordinary practice, the answer is not known in advance, so an important task — not explored here — is to decide when an approximation is good enough.) A “calculus book” approach divides the integration range into, say, 16 equal pieces, and computes function values.\n\nUsing the left end of each piece, the rectangle method sums 16 function values and multiplies by the step width, , here 0.25, to get an approximate value of 3.94325 for the integral. The accuracy is not impressive, but calculus formally uses pieces of infinitesimal width, so initially this may seem little cause for concern. Indeed, repeatedly doubling the number of steps eventually produces an approximation of 3.76001. However, 2 pieces are required, a great computational expense for such little accuracy; and a reach for greater accuracy can force steps so small that arithmetic precision becomes an obstacle.\n\nA better approach replaces the rectangles used in a Riemann sum with trapezoids. The trapezoid rule is almost as easy to calculate; it sums all 17 function values, but weights the first and last by one half, and again multiplies by the step width. This immediately improves the approximation to 3.76925, which is noticeably more accurate. Furthermore, only 2 pieces are needed to achieve 3.76000, substantially less computation than the rectangle method for comparable accuracy. The idea behind the trapezoid rule, that more accurate approximations to the function yield better approximations to the integral, can be carried further. Simpson's rule approximates the integrand by a piecewise quadratic function. Riemann sums, the trapezoid rule, and Simpson's rule are examples of a family of quadrature rules called Newton–Cotes formulas. The degree Newton–Cotes quadrature rule approximates the polynomial on each subinterval by a degree polynomial. This polynomial is chosen to interpolate the values of the function on the interval. Higher degree Newton-Cotes approximations can be more accurate, but they require more function evaluations (already Simpson's rule requires twice the function evaluations of the trapezoid rule), and they can suffer from numerical inaccuracy due to Runge's phenomenon. One solution to this problem is Clenshaw–Curtis quadrature, in which the integrand is approximated by expanding it in terms of Chebyshev polynomials. This produces an approximation whose values never deviate far from those of the original function.\n\nRomberg's method builds on the trapezoid method to great effect. First, the step lengths are halved incrementally, giving trapezoid approximations denoted by , and so on, where is half of . For each new step size, only half the new function values need to be computed; the others carry over from the previous size (as shown in the table above). But the really powerful idea is to interpolate a polynomial through the approximations, and extrapolate to . With this method a numerically \"exact\" answer here requires only four pieces (five function values). The Lagrange polynomial interpolating {(4.00,6.128), (2.00,4.352), (1.00,3.908)} is 3.76 + 0.148, producing the extrapolated value 3.76 at .\n\nGaussian quadrature often requires noticeably less work for superior accuracy. In this example, it can compute the function values at just two positions, , then double each value and sum to get the numerically exact answer. The explanation for this dramatic success lies in the choice of points. Unlike Newton–Cotes rules, which interpolate the integrand at evenly spaced points, Gaussian quadrature evaluates the function at the roots of a set of orthogonal polynomials. An -point Gaussian method is exact for polynomials of degree up to . The function in this example is a degree 3 polynomial, plus a term that cancels because the chosen endpoints are symmetric around zero. (Cancellation also benefits the Romberg method.)\n\nIn practice, each method must use extra evaluations to ensure an error bound on an unknown function; this tends to offset some of the advantage of the pure Gaussian method, and motivates the popular Gauss–Kronrod quadrature formulae. More broadly, adaptive quadrature partitions a range into pieces based on function properties, so that data points are concentrated where they are needed most.\n\nThe computation of higher-dimensional integrals (for example, volume calculations) makes important use of such alternatives as Monte Carlo integration.\n\nA calculus text is no substitute for numerical analysis, but the reverse is also true. Even the best adaptive numerical code sometimes requires a user to help with the more demanding integrals. For example, improper integrals may require a change of variable or methods that can avoid infinite function values, and known properties like symmetry and periodicity may provide critical leverage. For example, the integral formula_73 is difficult to evaluate numerically because it is infinite at . However, the substitution transforms the integral into formula_74, which has no singularities at all.\n\nThe area of an arbitrary two-dimensional shape can be determined using a measuring instrument called planimeter. The volume of irregular objects can be measured with precision by the fluid displaced as the object is submerged.\n\nArea can sometimes be found via geometrical compass-and-straightedge constructions of an equivalent square.\n\n\n\n\n\n"}
{"id": "20941606", "url": "https://en.wikipedia.org/wiki?curid=20941606", "title": "Kaplansky's theorem on quadratic forms", "text": "Kaplansky's theorem on quadratic forms\n\nIn mathematics, Kaplansky's theorem on quadratic forms is a result on simultaneous representation of primes by quadratic forms. It was proved in 2003 by Irving Kaplansky.\n\nKaplansky's theorem states that a prime \"p\" congruent to 1 modulo 16 is representable by both or none of \"x\" + 32\"y\" and \"x\" + 64\"y\", whereas a prime \"p\" congruent to 9 modulo 16 is representable by exactly one of these quadratic forms.\n\nThis is remarkable since the primes represented by each of these forms individually are \"not\" describable by congruence conditions.\n\nKaplansky's proof uses the facts that 2 is a 4th power modulo \"p\" if and only if \"p\" is representable by \"x\" + 64\"y\", and that −4 is an 8th power modulo \"p\" if and only if \"p\" is representable by \"x\" + 32\"y\".\n\n\nFive results similar to Kaplansky's theorem are known:\n\n\nIt is conjectured that there are no other similar results involving definite forms.\n"}
{"id": "55798156", "url": "https://en.wikipedia.org/wiki?curid=55798156", "title": "Kosaburo Hashiguchi", "text": "Kosaburo Hashiguchi\n\nIn 1988, he found the first algorithm to determine the star height of a regular language, a problem that had been open since 1963 when Lawrence Eggan solved the related star height problem of showing that there is no finite bound on the star height.\nHashiguchi's algorithm for star height is extremely complex, and impractical on all but the smallest examples. A simpler method, showing also that the problem is PSPACE-complete, was provided in 2005 by Kirsten.\n\nEarlier, in 1979, Hashiguchi had also solved another open problem on regular languages, of deciding whether, for a given language formula_1, there exists a finite number formula_2 such that formula_3.\n\nHashiguchi is the uncle of Japanese-born American pianist Grace Nikae.\n\n"}
{"id": "1240641", "url": "https://en.wikipedia.org/wiki?curid=1240641", "title": "Lickorish–Wallace theorem", "text": "Lickorish–Wallace theorem\n\nIn mathematics, the Lickorish–Wallace theorem in the theory of 3-manifolds states that any closed, orientable, connected 3-manifold may be obtained by performing Dehn surgery on a framed link in the 3-sphere with ±1 surgery coefficients. Furthermore, each component of the link can be assumed to be unknotted.\n\nThe theorem was proved in the early 1960s by W. B. R. Lickorish and Andrew H. Wallace, independently and by different methods. Lickorish's proof rested on the Lickorish twist theorem, which states that any orientable automorphism of a closed orientable surface is generated by Dehn twists along 3\"g\" − 1 specific simple closed curves in the surface, where \"g\" denotes the genus of the surface. Wallace's proof was more general and involved adding handles to the boundary of a higher-dimensional ball.\n\nA corollary of the theorem is that every closed, orientable 3-manifold bounds a simply-connected compact 4-manifold.\n\nBy using his work on automorphisms of non-orientable surfaces, Lickorish also showed that every closed, non-orientable, connected 3-manifold is obtained by Dehn surgery on a link in the non-orientable 2-sphere bundle over the circle. Similar to the orientable case, the surgery can be done in a special way which allows the conclusion that every closed, non-orientable 3-manifold bounds a compact 4-manifold.\n\n"}
{"id": "4786318", "url": "https://en.wikipedia.org/wiki?curid=4786318", "title": "Linear dynamical system", "text": "Linear dynamical system\n\nLinear dynamical systems are dynamical systems whose evaluation functions are linear. While dynamical systems in general do not have closed-form solutions, linear dynamical systems can be solved exactly, and they have a rich set of mathematical properties. Linear systems can also be used to understand the qualitative behavior of general dynamical systems, by calculating the equilibrium points of the system and approximating it as a linear system around each such point.\n\nIn a linear dynamical system, the variation of a state vector \n(an formula_1-dimensional vector denoted formula_2) equals a constant matrix\n(denoted formula_3) multiplied by \nformula_2. This variation can take two forms: either \nas a flow, in which formula_2 varies \ncontinuously with time\n\nor as a mapping, in which \nformula_2 varies in discrete steps\n\nThese equations are linear in the following sense: if \nformula_9 and formula_10 \nare two valid solutions, then so is any linear combination \nof the two solutions, e.g., \nformula_11 \nwhere formula_12 and formula_13\nare any two scalars. The matrix formula_3 \nneed not be symmetric.\n\nLinear dynamical systems can be solved exactly, in contrast to most nonlinear ones. Occasionally, a nonlinear system can be solved exactly by a change of variables to a linear system. Moreover, the solutions of (almost) any nonlinear system can be well-approximated by an equivalent linear system near its fixed points. Hence, understanding linear systems and their solutions is a crucial first step to understanding the more complex nonlinear systems.\n\nIf the initial vector formula_15\nis aligned with a right eigenvector formula_16 of \nthe matrix formula_3, the dynamics are simple\n\nwhere formula_19 is the corresponding eigenvalue;\nthe solution of this equation is \nas may be confirmed by substitution.\n\nIf formula_3 is diagonalizable, then any vector in an formula_1-dimensional space can be represented by a linear combination of the right and left eigenvectors (denoted formula_23) of the matrix formula_3.\n\nTherefore, the general solution for formula_9 is \na linear combination of the individual solutions for the right\neigenvectors\n\nSimilar considerations apply to the discrete mappings.\n\nThe roots of the characteristic polynomial det(A - λI) are the eigenvalues of A. The sign and relation of these roots, formula_28, to each other may be used to determine the stability of the dynamical system \nFor a 2-dimensional system, the characteristic polynomial is of the form formula_30 where formula_31 is the trace and formula_32 is the determinant of A. Thus the two roots are in the form:\nNote also that formula_35 and formula_36. Thus if formula_37 then the eigenvalues are of opposite sign, and the fixed point is a saddle. If formula_38 then the eigenvalues are of the same sign. Therefore, if formula_39 both are positive and the point is unstable, and if formula_40 then both are negative and the point is stable. The discriminant will tell you if the point is nodal or spiral (i.e. if the eigenvalues are real or complex).\n\n"}
{"id": "346955", "url": "https://en.wikipedia.org/wiki?curid=346955", "title": "List of number theory topics", "text": "List of number theory topics\n\nThis is a list of number theory topics, by Wikipedia page. See also\n\n\n\n\n\n\n\n\"See list of algebraic number theory topics\"\n\n\n\n\n\n\n\n\nNote: Computational number theory is also known as algorithmic number theory.\n\n\n\n\n\n"}
{"id": "47548801", "url": "https://en.wikipedia.org/wiki?curid=47548801", "title": "Marcy Barge", "text": "Marcy Barge\n\nMarcy Barge is a professor of mathematics at Montana State University.\n\nBarge received his Ph.D. from the University of Colorado at Boulder in 1980.\n\nIn 2012, Barge became a fellow of the American Mathematical Society.\n"}
{"id": "387297", "url": "https://en.wikipedia.org/wiki?curid=387297", "title": "Mellin transform", "text": "Mellin transform\n\nIn mathematics, the Mellin transform is an integral transform that may be regarded as the multiplicative version of the two-sided Laplace transform. This integral transform is closely connected to the theory of Dirichlet series, and is\noften used in number theory, mathematical statistics, and the theory of asymptotic expansions; it is closely related to the Laplace transform and the Fourier transform, and the theory of the gamma function and allied special functions.\n\nThe Mellin transform of a function \"f\" is\n\nThe inverse transform is\n\nThe notation implies this is a line integral taken over a vertical line in the complex plane. Conditions under which this inversion is valid are given in the Mellin inversion theorem.\n\nThe transform is named after the Finnish mathematician Hjalmar Mellin.\n\nThe two-sided Laplace transform may be defined in terms of the Mellin\ntransform by\n\nand conversely we can get the Mellin transform from the two-sided Laplace transform by\n\nThe Mellin transform may be thought of as integrating using a kernel \"x\" with respect to the multiplicative Haar measure, \nformula_5, which is invariant\nunder dilation formula_6, so that\nformula_7 the two-sided Laplace transform integrates with respect to the additive Haar measure formula_8, which is translation invariant, so that formula_9.\n\nWe also may define the Fourier transform in terms of the Mellin transform and vice versa; in terms of the Mellin transform and of the two-sided Laplace transform defined above\n\nWe may also reverse the process and obtain\n\nThe Mellin transform also connects the Newton series or binomial transform together with the Poisson generating function, by means of the Poisson–Mellin–Newton cycle.\n\nThe Mellin transform may also be viewed as the Gelfand transform for the convolution algebra of the locally compact abelian group of positive real numbers with multiplication.\n\nFor formula_12, formula_13 and formula_14 on the principal branch, one has\n\nwhere formula_16 is the gamma function. This integral is known as the Cahen–Mellin integral.\n\nAn important application in number theory includes the simple function \n\nfor which \n\nassuming \n\nIn the study of Hilbert spaces, the Mellin transform is often posed in a slightly different way. For functions in formula_20 (see Lp space) the fundamental strip always includes formula_21, so we may define a linear operator formula_22 as\n\nIn other words, we have set\n\nThis operator is usually denoted by just plain formula_26 and called the \"Mellin transform\", but formula_22 is used here to distinguish from the definition used elsewhere in this article. The Mellin inversion theorem then shows that formula_22 is invertible with inverse\n\nFurthermore, this operator is an isometry, that is to say formula_31 for all formula_32 (this explains why the factor of formula_33 was used).\n\nIn probability theory, the Mellin transform is an essential tool in studying the distributions of products of random variables. If \"X\" is a random variable, and } denotes its positive part, while } is its negative part, then the \"Mellin transform\" of \"X\" is defined as \n\nwhere \"γ\" is a formal indeterminate with . This transform exists for all \"s\" in some complex strip , where .\n\nThe Mellin transform formula_35 of a random variable \"X\" uniquely determines its distribution function \"F\". The importance of the Mellin transform in probability theory lies in the fact that if \"X\" and \"Y\" are two independent random variables, then the Mellin transform of their products is equal to the product of the Mellin transforms of \"X\" and \"Y\":\n\nIn the Laplacian in cylindrical coordinates in a generic dimension (orthogonal coordinates with one angle and one radius, and the remaining lengths) there is always a term:\n\nFor example, in 2-D polar coordinates the laplacian is:\n\nand in 3-D cylindrical coordinates the laplacian is,\n\nThis term can be easily treated with the Mellin transform, since:\n\nFor example, the 2-D Laplace equation in polar coordinates is the PDE in two variables:\n\nand by multiplication:\n\nwith a Mellin transform on radius becomes the simple harmonic oscillator:\n\nwith general solution:\n\nNow let's impose for example some simple wedge boundary conditions to the original Laplace equation:\n\nthese are particularly simple for Mellin transform, becoming:\n\nThese conditions imposed to the solution particularise it to:\n\nNow by the convolution theorem for Mellin transform, the solution in the Mellin domain can be inverted:\n\nwhere the following inverse transform relation was employed:\n\nwhere formula_50.\n\nThe Mellin Transform is widely used in computer science for the analysis of algorithms because of its scale invariance property. The magnitude of the Mellin Transform of a scaled function is identical to the magnitude of the original function for purely imaginary inputs. This scale invariance property is analogous to the Fourier Transform's shift invariance property. The magnitude of a Fourier transform of a time-shifted function is identical to the magnitude of the Fourier transform of the original function.\n\nThis property is useful in image recognition. An image of an object is easily scaled when the object is moved towards or away from the camera.\n\nIn quantum mechanics and especially quantum field theory, Fourier space is enormously useful and used extensively because momentum and position are Fourier transforms of each other (for instance, Feynman diagrams are much more easily computed in momentum space). In 2011, A. Liam Fitzpatrick, Jared Kaplan, João Penedones, Suvrat Raju, and Balt C. van Rees showed that Mellin space serves an analogous role in the context of the AdS/CFT correspondence.\n\n\n\n"}
{"id": "40933052", "url": "https://en.wikipedia.org/wiki?curid=40933052", "title": "Monika Henzinger", "text": "Monika Henzinger\n\nMonika Henzinger (born as Monika Rauch, 17 April 1966 in Weiden in der Oberpfalz) is a German computer scientist, and is a former director of research at Google. She is currently a professor at the University of Vienna. Her expertise is mainly on algorithms with a focus on data structures, algorithmic game theory, information retrieval, search algorithms and Web data mining. She is married to Thomas Henzinger and has three children.\n\nShe completed her PhD in 1993 from Princeton University under the supervision of Robert Tarjan. She then became an assistant professor of computer science at Cornell University, a research staff at Digital Equipment Corporation, an associate professor at the University of the Saarland, a director of research at Google, and a full professor of computer science at École Polytechnique Fédérale de Lausanne. She is currently a full professor of computer science at the University of Vienna, Austria.\n\n\n\n"}
{"id": "27345568", "url": "https://en.wikipedia.org/wiki?curid=27345568", "title": "Nominal terms (computer science)", "text": "Nominal terms (computer science)\n\nNominal terms are a metalanguage for embedding object languages with binding constructs into. Intuitively, they may be seen as an extension of first-order terms with support for name binding. Consequently, the native notion of equality between two nominal terms is alpha-equivalence (equivalence up to a permutative renaming of bound names). Nominal terms came out of a programme of research into nominal sets, and have a concrete semantics in those sets.\n\nNominal unification is efficiently decidable. This fact led to the development of alphaProlog, a Prolog-like logic programming language with facilities for binding names in terms, where Prolog's standard first-order unification algorithm is replaced with nominal unification.\n\nNominal term embeddings may be seen as alternatives to de Bruijn encodings and higher-order abstract syntax, where the latter uses the simply typed lambda calculus as a metalanguage.\n\nMany interesting calculi, logics and programming languages that are commonly seen in computer science feature name binding constructs. For instance, the universal quantifier from first-order logic, the lambda-binder from the lambda-calculus, and the pi-binder from the pi-calculus are all examples of name-binding constructs.\n\nComputer scientists often need to manipulate abstract syntax trees. For instance, compiler writers perform many manipulations of abstract syntax trees during the various optimisation and elaboration phases of compiler execution. In particular, when working with abstract syntax trees with name binding constructs, we often want to work on alpha-equivalence classes, implement capture-avoiding substitutions, and make it easy to generate fresh names. How best to do this, in a bug free and reliable manner, motivates a large amount of research.\n\nPrior attempts at solving this problem include 'nameless approaches' such as de Bruijn indices and levels, and higher-order approaches such as higher-order abstract syntax. Nominal terms are another, relatively new, approach that retain explicit names for bound variables like higher-order abstract syntax, whilst retaining the first-order flavour (and first-order computational properties) of de Bruijn encodings.\n\nHigher-order unification is known to be undecidable. This motivates the search for subsets of lambda-terms that enjoy a computationally well-behaved unification procedure. Higher-order patterns, proposed by Miller, are one such set.\n\nHigher-order patterns are lambda-terms where the arguments of a free variable are all distinct bound variables. They possess an efficiently decidable unification procedure, and as a result, have been widely implemented, notably in the logic programming language lambdaProlog.\n\nA recent body of work has investigated the connections between nominal terms and higher-order patterns, and consequently between nominal unification and higher-order pattern unification. Cheney proposed an extension of nominal terms called nominal patterns. He then provided a translation between nominal patterns and higher-order patterns which preserved unifiers. Later, Levy and Villaret demonstrated a translation between nominal terms and higher-order patterns that preserves the notion of unifiability. That is, if two nominal terms are unifiable, then their translated pattern counterparts are also unifiable.\n\nDowek and Gabbay later sharpened Levy and Villaret's translation, proving that in some sense their translation is the best that there can be, and proved that the improved translation preserves unifiers. That is, if two nominal terms are unifiable by some substitution, then the corresponding higher-order pattern unification problem under the translation is solved by the translated substitution. For their proof, Dowek and Gabbay used a variation of nominal terms called permissive nominal terms. However, a translation from permissive nominal terms and back again also exists, completing the translation between nominal terms and higher-order patterns.\n\n"}
{"id": "40360489", "url": "https://en.wikipedia.org/wiki?curid=40360489", "title": "Oliver Sin", "text": "Oliver Sin\n\nOliver Sin (born 18 May 1985) is a Hungarian artist.\n\nBorn in Budapest, Hungary, Oliver Sin was raised with his sister Judit by his father, Zoltan Sin, a ropemaker. He became interested in art and science from an early age. He studied visual art from 2003 to 2009 at Dunakeszi's Miklós Radnóti Gymnasium. After graduation Sin enrolled in Szombathely's University of West Hungary where he majored in Visual Arts and Croatian.\n\nSin' work first came to international attention after his collaboration with Fibenare Guitars Co., when they made the Fibenare - Oliver Sin Collaboration Guitar in 2012. He made the cover of \"Guitar Connoisseur Magazine\" (in New York, USA) in 2013.\n\n\n\n\n\n\nSin's work is in private and public collections such as MODESSQE (Poland).\n\n\n"}
{"id": "1493395", "url": "https://en.wikipedia.org/wiki?curid=1493395", "title": "Pappus's hexagon theorem", "text": "Pappus's hexagon theorem\n\nIn mathematics, Pappus's hexagon theorem (attributed to Pappus of Alexandria) states that given one set of collinear points \"A\", \"B\", \"C\", and another set of collinear points \"a\", \"b\", \"c\", then the intersection points \"X\", \"Y\", \"Z\" of line pairs \"Ab\" and \"aB\", \"Ac\" and \"aC\", \"Bc\" and \"bC\" are collinear, lying on the \"Pappus line\". These three points are the points of intersection of the \"opposite\" sides of the hexagon \"AbCaBc\". It holds in a projective plane over any field, but fails for projective planes over any noncommutative division ring. Projective planes in which the \"theorem\" is valid are called pappian planes.\n\nThe dual of this incidence theorem states that given one set of concurrent lines \"A\", \"B\", \"C\", and another set of concurrent lines \"a\", \"b\", \"c\", then the lines \"x\", \"y\", \"z\" defined by pairs of points resulting from pairs of intersections \"A\"∩\"b\" and \"a\" ∩ \"B\", \"A\" ∩ \"c\" and \"a\" ∩ \"C\", \"B\" ∩ \"c\" and \"b\" ∩ \"C\" are concurrent. (\"Concurrent\" means that the lines pass through one point.)\n\nPappus's theorem is a special case of Pascal's theorem for a conic—the limiting case when the conic degenerates into 2 straight lines. Pascal's theorem is in turn a special case of the Cayley–Bacharach theorem.\n\nThe Pappus configuration is the configuration of 9 lines and 9 points that occurs in Pappus's theorem, with each line meeting 3 of the points and each point meeting 3 lines. In general, the Pappus line does not pass through the point of intersection of \"ABC\" and \"abc\". This configuration is self dual. Since, in particular, the lines \"Bc\", \"bC\", \"XY\" have the properties of the lines \"x\", \"y\", \"z\" of the dual theorem, and collinearity of \"X\", \"Y\", \"Z\" is equivalent to concurrence of \"Bc\", \"bC\", \"XY\", the dual theorem is therefore just the same as the theorem itself. The Levi graph of the Pappus configuration is the Pappus graph, a bipartite distance-regular graph with 18 vertices and 27 edges.\n\nChoose projective coordinates with \nOn the lines \"AC\", \"Ac\", \"AX\", given by , , , take the points \"B\", \"Y\", \"b\" to be \nfor some \"p\", \"q\", \"r\". The three lines \"XB\", \"CY\", \"cb\" are , , , so they pass through the same point \"a\" if and only if . The condition for the three lines \"Cb\", \"cB\" and \"XY\" , , to pass through the same point \"Z\" is . So this last set of three lines is concurrent if all the other eight sets are because multiplication is commutative, so . Equivalently, \"X\", \"Y\", \"Z\" are collinear.\n\nThe proof above also shows that for Pappus's theorem to hold for a projective space over a division ring it is both sufficient and necessary that the division ring is a (commutative) field. German mathematician Gerhard Hessenberg proved that Pappus's theorem implies Desargues's theorem. In general, Pappus's theorem holds for some projective plane if and only if it is a projective plane over a commutative field. The projective planes in which Pappus's theorem does not hold are Desarguesian projective planes over noncommutative division rings, and non-Desarguesian planes.\n\nThe proof is invalid if \"C\", \"c\", \"X\" happen to be collinear. In that case an alternative proof can be provided, for example, using a different projective reference.\n\nIn its earliest known form, Pappus's Theorem is Propositions 138, 139, 141, and 143 of Book VII of Pappus's \"Collection\". These are Lemmas XII, XIII, XV, and XVII in the part of Book VII consisting of lemmas to the first of the three books of Euclid's \"Porisms.\"\n\nThe lemmas are proved in terms of what today is known as the cross ratio of four collinear points. Three earlier lemmas are used. The first of these, Lemma III, has the diagram below (which uses Pappus's lettering, with G for Γ, D for Δ, J for Θ, and L for Λ). \nHere three concurrent straight lines, AB, AG, and AD, are crossed by two lines, JB and JE, which concur at J. \nAlso KL is drawn parallel to AZ.\nThen\nThese proportions might be written today as equations:\nThe last compound ratio (namely JD : GD & BG : JB) is what is known today as the cross ratio of the collinear points J, G, D, and B in that order; it is denoted today by (J, G; D, B). So we have shown that this is independent of the choice of the particular straight line JD that crosses the three straight lines that concur at A. In particular\nIt does not matter on which side of A the straight line JE falls. In particular, the situation may be as in the next diagram, which is the diagram for Lemma X. \nJust as before, we have (J, G; D, B) = (J, Z; H, E). Pappus does not explicitly prove this; but Lemma X is a converse, namely that if these two cross ratios are the same, and the straight lines BE and DH cross at A, then the points G, A, and Z must be collinear.\n\nWhat we showed originally can be written as (J, ∞; K, L) = (J, G; D, B), with ∞ taking the place of the (nonexistent) intersection of JK and AG. Pappus shows this, in effect, in Lemma XI, whose diagram, however, has different lettering: \nWhat Pappus shows is DE.ZH : EZ.HD :: GB : BE, which we may write as\nThe diagram for Lemma XII is:\nThe diagram for Lemma XIII is the same, but BA and DG, extended, meet at N. In any case, considering straight lines through G as cut by the three straight lines through A, (and accepting that equations of cross ratios remain valid after permutation of the entries,) we have by Lemma III or XI\nConsidering straight lines through D as cut by the three straight lines through B, we have\nThus (E, H; J, G) = (E, K; D, L), so by Lemma X, the points H, M, and K are collinear. That is, the points of intersection of the pairs of opposite sides of the hexagon ADEGBZ are collinear.\n\nLemmas XV and XVII are that, if the point M is determined as the intersection of HK and BG, then the points A, M, and D are collinear. That is, the points of intersection of the pairs of opposite sides of the hexagon BEKHZG are collinear.\n\nIn addition to the above characterizations of Pappus's theorem and its dual, the following are equivalent statements:\n\n\n"}
{"id": "33731493", "url": "https://en.wikipedia.org/wiki?curid=33731493", "title": "Parallel postulate", "text": "Parallel postulate\n\nIn geometry, the parallel postulate, also called Euclid's fifth postulate because it is the fifth postulate in Euclid's \"Elements\", is a distinctive axiom in Euclidean geometry. It states that, in two-dimensional geometry:\n\"If a line segment intersects two straight lines forming two interior angles on the same side that sum to less than two right angles, then the two lines, if extended indefinitely, meet on that side on which the angles sum to less than two right angles.\"\nThis postulate does not specifically talk about parallel lines; Euclid gave the definition of parallel lines in Book I, Definition 23 just before the five postulates.\n\n\"Euclidean geometry\" is the study of geometry that satisfies all of Euclid's axioms, \"including\" the parallel postulate. A geometry where the parallel postulate does not hold is known as a non-Euclidean geometry. Geometry that is independent of Euclid's fifth postulate (i.e., only assumes the modern equivalent of the first four postulates) is known as absolute geometry (or, in other places known as neutral geometry).\n\nProbably the best known equivalent of Euclid's parallel postulate, contingent on his other postulates, is Playfair's axiom, named after the Scottish mathematician John Playfair, which states:\n\"In a plane, given a line and a point not on it, at most one line parallel to the given line can be drawn through the point.\"\nThis axiom by itself is not logically equivalent to the Euclidean parallel postulate since there are geometries in which one is true and the other is not. However, in the presence of the remaining axioms which give Euclidean geometry, each of these can be used to prove the other, so they are equivalent in the context of absolute geometry.\n\nMany other statements equivalent to the parallel postulate have been suggested, some of them appearing at first to be unrelated to parallelism, and some seeming so self-evident that they were unconsciously assumed by people who claimed to have proven the parallel postulate from Euclid's other postulates. These equivalent statements include:\n\n\nHowever, the alternatives which employ the word \"parallel\" cease appearing so simple when one is obliged to explain which of the four common definitions of \"parallel\" is meant – constant separation, never meeting, same angles where crossed by \"some\" third line, or same angles where crossed by \"any\" third line – since the equivalence of these four is itself one of the unconsciously obvious assumptions equivalent to Euclid's fifth postulate. In the list above, it is always taken to refer to non-intersecting lines. For example, if the word \"parallel\" in Playfair's axiom is taken to mean 'constant separation' or 'same angles where crossed by any third line', then it is no longer equivalent to Euclid's fifth postulate, and is provable from the first four (the axiom says 'There is at most one line...', which is consistent with there being no such lines). However, if the definition is taken so that parallel lines are lines that do not intersect, or that have some line intersecting them in the same angles, Playfair's axiom is contextually equivalent to Euclid's fifth postulate and is thus logically independent of the first four postulates. Note that the latter two definitions are not equivalent, because in hyperbolic geometry the second definition holds only for ultraparallel lines.\n\nFor two thousand years, many attempts were made to prove the parallel postulate using Euclid's first four postulates. The main reason that such a proof was so highly sought after was that, unlike the first four postulates, the parallel postulate is not self-evident. If the order the postulates were listed in the Elements is significant, it indicates that Euclid included this postulate only when he realised he could not prove it or proceed without it.\nMany attempts were made to prove the fifth postulate from the other four, many of them being accepted as proofs for long periods until the mistake was found. Invariably the mistake was assuming some 'obvious' property which turned out to be equivalent to the fifth postulate (Playfair's axiom). Although known from the time of Proclus, this became known as Playfair's Axiom after John Playfair wrote a famous commentary on Euclid in 1795 in which he proposed replacing Euclid's fifth postulate by his own axiom.\n\nProclus (410-485) wrote a commentary on \"The Elements\" where he comments on attempted proofs to deduce the fifth postulate from the other four, in particular he notes that Ptolemy had produced a false 'proof'. Proclus then goes on to give a false proof of his own. However he did give a postulate which is equivalent to the fifth postulate.\n\nIbn al-Haytham (Alhazen) (965-1039), an Arab mathematician, made an attempt at proving the parallel postulate using a proof by contradiction, in the course of which he introduced the concept of motion and transformation into geometry. He formulated the Lambert quadrilateral, which Boris Abramovich Rozenfeld names the \"Ibn al-Haytham–Lambert quadrilateral\", and his attempted proof contains elements similar to those found in Lambert quadrilaterals and Playfair's axiom.\n\nThe Persian mathematician, astronomer, philosopher, and poet Omar Khayyám (1050–1123), attempted to prove the fifth postulate from another explicitly given postulate (based on the fourth of the five \"principles due to the Philosopher\" (Aristotle), namely, \"Two convergent straight lines intersect and it is impossible for two convergent straight lines to diverge in the direction in which they converge.\" He derived some of the earlier results belonging to elliptical geometry and hyperbolic geometry, though his postulate excluded the latter possibility. The Saccheri quadrilateral was also first considered by Omar Khayyám in the late 11th century in Book I of \"Explanations of the Difficulties in the Postulates of Euclid\". Unlike many commentators on Euclid before and after him (including Giovanni Girolamo Saccheri), Khayyám was not trying to prove the parallel postulate as such but to derive it from his equivalent postulate. He recognized that three possibilities arose from omitting Euclid's fifth postulate; if two perpendiculars to one line cross another line, judicious choice of the last can make the internal angles where it meets the two perpendiculars equal (it is then parallel to the first line). If those equal internal angles are right angles, we get Euclid's fifth postulate, otherwise, they must be either acute or obtuse. He showed that the acute and obtuse cases led to contradictions using his postulate, but his postulate is now known to be equivalent to the fifth postulate.\n\nNasir al-Din al-Tusi (1201–1274), in his \"Al-risala al-shafiya'an al-shakk fi'l-khutut al-mutawaziya\" (\"Discussion Which Removes Doubt about Parallel Lines\") (1250), wrote detailed critiques of the parallel postulate and on Khayyám's attempted proof a century earlier. Nasir al-Din attempted to derive a proof by contradiction of the parallel postulate. He also considered the cases of what are now known as elliptical and hyperbolic geometry, though he ruled out both of them.\n\nNasir al-Din's son, Sadr al-Din (sometimes known as \"Pseudo-Tusi\"), wrote a book on the subject in 1298, based on his father's later thoughts, which presented one of the earliest arguments for a non-Euclidean hypothesis equivalent to the parallel postulate. \"He essentially revised both the Euclidean system of axioms and postulates and the proofs of many propositions from the \"Elements\".\" His work was published in Rome in 1594 and was studied by European geometers. This work marked the starting point for Saccheri's work on the subject which opened with a criticism of Sadr al-Din's work and the work of Wallis.\n\nGiordano Vitale (1633-1711), in his book \"Euclide restituo\" (1680, 1686), used the Khayyam-Saccheri quadrilateral to prove that if three points are equidistant on the base AB and the summit CD, then AB and CD are everywhere equidistant. Girolamo Saccheri (1667-1733) pursued the same line of reasoning more thoroughly, correctly obtaining absurdity from the obtuse case (proceeding, like Euclid, from the implicit assumption that lines can be extended indefinitely and have infinite length), but failing to refute the acute case (although he managed to wrongly persuade himself that he had).\n\nIn 1766 Johann Lambert wrote, but did not publish, \"Theorie der Parallellinien\" in which he attempted, as Saccheri did, to prove the fifth postulate. He worked with a figure that today we call a \"Lambert quadrilateral\", a quadrilateral with three right angles (can be considered half of a Saccheri quadrilateral). He quickly eliminated the possibility that the fourth angle is obtuse, as had Saccheri and Khayyám, and then proceeded to prove many theorems under the assumption of an acute angle. Unlike Saccheri, he never felt that he had reached a contradiction with this assumption. He had proved the non-Euclidean result that the sum of the angles in a triangle increases as the area of the triangle decreases, and this led him to speculate on the possibility of a model of the acute case on a sphere of imaginary radius. He did not carry this idea any further.\n\nWhere Khayyám and Saccheri had attempted to prove Euclid's fifth by disproving the only possible alternatives, the nineteenth century finally saw mathematicians exploring those alternatives and discovering the logically consistent geometries which result. In 1829, Nikolai Ivanovich Lobachevsky published an account of acute geometry in an obscure Russian journal (later re-published in 1840 in German). In 1831, János Bolyai included, in a book by his father, an appendix describing acute geometry, which, doubtlessly, he had developed independently of Lobachevsky. Carl Friedrich Gauss had also studied the problem, but he did not publish any of his results. Upon hearing of Bolyai's results in a letter from Bolyai's father, Farkas Bolyai, Gauss stated:\n\n\"If I commenced by saying that I am unable to praise this work, you would certainly be surprised for a moment. But I cannot say otherwise. To praise it would be to praise myself. Indeed the whole contents of the work, the path taken by your son, the results to which he is led, coincide almost entirely with my meditations, which have occupied my mind partly for the last thirty or thirty-five years.\" \n\nThe resulting geometries were later developed by Lobachevsky, Riemann and Poincaré into hyperbolic geometry (the acute case) and elliptic geometry (the obtuse case). The independence of the parallel postulate from Euclid's other axioms was finally demonstrated by Eugenio Beltrami in 1868.\n\nEuclid did not postulate the converse of his fifth postulate, which is one way to distinguish Euclidean geometry from elliptic geometry. The Elements contains the proof of an equivalent statement (Book I, Proposition 27): \"If a straight line falling on two straight lines make the alternate angles equal to one another, the straight lines will be parallel to one another.\" As De Morgan pointed out, this is logically equivalent to (Book I, Proposition 16). These results do not depend upon the fifth postulate, but they do require the second postulate which is violated in elliptic geometry.\n\nAttempts to logically prove the parallel postulate, rather than the eighth axiom, were criticized by Arthur Schopenhauer. However, the argument used by Schopenhauer was that the postulate is evident by perception, not that it was not a logical consequence of the other axioms.\n\n\n"}
{"id": "3120758", "url": "https://en.wikipedia.org/wiki?curid=3120758", "title": "Peetre theorem", "text": "Peetre theorem\n\nIn mathematics, the (linear) Peetre theorem, named after Jaak Peetre, is a result of functional analysis that gives a characterisation of differential operators in terms of their effect on generalized function spaces, and without mentioning differentiation in explicit terms. The Peetre theorem is an example of a finite order theorem in which a function or a functor, defined in a very general way, can in fact be shown to be a polynomial because of some extraneous condition or symmetry imposed upon it.\n\nThis article treats two forms of the Peetre theorem. The first is the original version which, although quite useful in its own right, is actually too general for most applications.\n\nLet \"M\" be a smooth manifold and let \"E\" and \"F\" be two vector bundles on \"M\". Let\nbe the spaces of smooth sections of \"E\" and \"F\". An \"operator\"\nis a morphism of sheaves which is linear on sections such that the support of \"D\" is non-increasing: \"supp Ds\" ⊆ \"supp s\" for every smooth section \"s\" of \"E\". The original Peetre theorem asserts that, for every point \"p\" in \"M\", there is a neighborhood \"U\" of \"p\" and an integer \"k\" (depending on \"U\") such that \"D\" is a differential operator of order \"k\" over \"U\". This means that \"D\" factors through a linear mapping \"i\" from the \"k\"-jet of sections of \"E\" into the space of smooth sections of \"F\":\nwhere\nis the \"k\"-jet operator and\nis a linear mapping of vector bundles.\n\nThe problem is invariant under local diffeomorphism, so it is sufficient to prove it when \"M\" is an open set in R and \"E\" and \"F\" are trivial bundles. At this point, it relies primarily on two lemmas:\n\nWe begin with the proof of Lemma 1.\n\nWe now prove Lemma 2.\n\nLet \"M\" be a compact smooth manifold (possibly with boundary), and \"E\" and \"F\" be finite dimensional vector bundles on \"M\". Let\n\nis a smooth function (of Fréchet manifolds) which is linear on the fibres and respects the base point on \"M\":\n\nThe Peetre theorem asserts that for each operator \"D\", there exists an integer \"k\" such that \"D\" is a differential operator of order \"k\". Specifically, we can decompose\n\nwhere formula_18 is a mapping from the jets of sections of \"E\" to the bundle \"F\". See also intrinsic differential operators.\n\n"}
{"id": "6588124", "url": "https://en.wikipedia.org/wiki?curid=6588124", "title": "Pompeiu problem", "text": "Pompeiu problem\n\nIn mathematics, the Pompeiu problem is a conjecture in integral geometry, named for Dimitrie Pompeiu, who posed the problem in 1929, \nas follows. Suppose \"f\" is a nonzero continuous function defined on a Euclidean space, and \"K\" is a simply connected Lipschitz domain, so that the integral of \"f\" vanishes on every congruent copy of \"K\". Then the domain is a ball.\n\nA special case is Schiffer's conjecture.\n\n\n"}
{"id": "388755", "url": "https://en.wikipedia.org/wiki?curid=388755", "title": "Prime95", "text": "Prime95\n\nPrime95 is the freeware application written by George Woltman that is used by GIMPS, a distributed computing project dedicated to finding new Mersenne prime numbers. More specifically, Prime95 refers to the Windows and macOS versions of the software.\n\nMPrime is the Linux command-line interface version of Prime95, to be run in a text terminal or in a terminal emulator window as a remote shell client. It is identical to Prime95 in functionality, except it lacks a graphical user interface.\n\nAlthough most of the GIMPS software's source code is publicly available, it is technically not free software as users must abide by the project's distribution terms if the software is used to discover a prime number with at least 100,000,000 decimal digits and wins the $150,000 bounty offered by the EFF. As such, a user who uses Prime95 to discover a qualifying prime number would not be able to claim the prize directly ($50,000 will go to the person who finds the prime number, $50,000 will go to a mathematics-related charity, $50,000 will be kept as reserve by GIMPS). A free software package would not have this restriction.\n\nThe code that is used to generate checksums is not publicly available for security reasons. The rewritten FFT assembly code in the current stable version 28 (since June 1, 2014) uses FMA instruction set (FMA3) instructions of Haswell (microarchitecture) CPUs (Core i3/i5/i7-4xxx models), resulting in a huge performance increase.\n\nPrime95 currently does not have GPU (Graphical Processing Unit) support, although Woltman has indicated that it is under development. However, there are third-party programs, such as CUDALucas, that make use of the processing power of GPUs.\n\n, 16 new Mersenne prime numbers have been found by the network of participants, and a new Mersenne prime was discovered approximately every year until 2009; the one after that was 4 years after. Scott Kurowski wrote the Internet PrimeNet Server that supports the Prime95/MPrime software on GIMPS, one of the earliest grid computing projects, researching Mersenne prime numbers, to demonstrate Entropia-distributed computing software, a company he founded in 1997.\n\nTo perform the Mersenne prime search, the program implements two algorithms:\n\nIt also implements a few algorithms which try to factor the numbers:\n\nA table of selected benchmarks is provided below. The complete list can be found at the official GIMPS website.\n\nOver the years, Prime95 has become extremely popular among PC enthusiasts and overclockers as a stability testing utility. It includes a \"Torture Test\" mode designed specifically for testing PC subsystems for errors in order to help ensure the correct operation of Prime95 on that system. This is important because each iteration of the Lucas-Lehmer depends on the previous one; if one iteration is incorrect, so will be the entire primality test.\n\nThe stress-test feature in Prime95 can be configured to better test various components of the computer by changing the fast fourier transform (FFT) size. Three pre-set configurations are available: Small FFTs and In-place FFTs, and Blend. Small and In-place modes primarily test the FPU and the caches of the CPU, whereas the Blend mode tests everything, including the memory.\n\nBy selecting Custom, the user can gain further control of the configuration. For example, by selecting 8-8 kB as the FFT size, the program stresses primarily the CPU. By selecting 2048-4096 kB and unchecking the \"Run FFTs in-place\" checkbox, providing the maximum amount of RAM free in the system, the program tests the memory and the chipset. If the amount of memory to use option is set too high, then the system will start using the paging file and the test will not stress the memory.\n\nOn an absolutely stable system, Prime95 would run indefinitely. If an error occurs, at which point the stress test would terminate, this would indicate that the system may be unstable. There is an ongoing debate about terms \"stable\" and \"Prime-stable\", as Prime95 often fails before the system becomes unstable or crashes in any other application. This is because Prime95 is \"designed\" to subject the CPU to an incredibly intense workload, and to halt when it encounters even one minor error, whereas most normal applications do not stress the CPU anywhere near as much, and will continue to operate unless they encounter a fatal error.\n\nIn the overclocking community, a rule of thumb is often used to determine how long to run Prime95: test the CPU (8 kB FFT) for 10 hours and the memory (4096 kB FFT) for 10 hours, and if the system passes, there is a high chance that it is stable. Twenty-four hours of testing is recommended to be sure, as errors may show up after 16 or more hours of testing (compared to, say, just four hours of testing). Moreover, a large proportion of system overclockers and enthusiasts favor Prime95 over other benchmarking suites because Prime95 pushes the CPU's floating point units extremely hard, causing the CPU to become extremely hot. In addition, Prime95 stresses a computer far more than the majority of software-based torture suites. The nature of this is because the operating system usually shuts down the floating-point unit when unused by other programs, whereas Prime95 is well-optimized to continuously and effectively thread the FPU, causing it to be deeply pipelined, thereby generating significantly more heat because of elevated power consumption under the massive workload conditions. In CPUs which are not adequately cooled, errors are likely to occur. Prime95 also constantly accesses main memory at up to 60 MB per second. This constant activity will detect memory problems that other programs will not.\n\nLastly, power supply units of any machine running Prime95 are subject to the consistent ramifications of such harsh conditions. Power must be maintained clean, while providing adequate voltage, particularly to the CPU, RAM, and chipsets (mainboard chipsets such as the Northbridge where the memory controller may or may not reside; see Athlon 64 or Intel Core i7 for on-die memory controllers) to provide peak performance while maintaining stability. Cray Research used programs similar to Prime95 for over a decade for the purpose of stability testing.\n\nVersion 24 and older of Prime95 cannot test Mersenne numbers beyond formula_1, which means that it cannot test exponents above 79,300,000. This is slightly shorter than a 24 million digit number. As of August 14, 2018, all of the numbers up to this threshold have been tested. There is a countdown at https://www.mersenne.org/report_milestones/. Newer versions of Prime95 (version 25, 26, 27 and 28) can handle Mersenne numbers up to the limit formula_2, meaning we can handle exponents up to 596,000,000. However, they can perform Trial Factoring on Mersenne numbers up to formula_3.\n\nPrime95 does not fully stress all processor threads when the threads number is more than 64 in Windows, or 32 for the 32-bit version. Windows will manage the processors in groups when the number is beyond 64. Each group will only have a maximum of 64. Prime95 will only load into one processor group.\n\nMore details are located in the whatsnew.txt file.\n\n\n"}
{"id": "30770384", "url": "https://en.wikipedia.org/wiki?curid=30770384", "title": "Prize of the Austrian Mathematical Society", "text": "Prize of the Austrian Mathematical Society\n\nThe Prize of the Austrian Mathematical Society () is the highest mathematics award in Austria. It is awarded every year by the Austrian Mathematical Society to a promising young mathematician for outstanding achievements. A substantial part of the work must have been performed in Austria. The recipient receives, in addition to a monetary reward, a medal showing Inzinger. The prize was established in 1955 and is awarded since 1956.\n\n"}
{"id": "2642185", "url": "https://en.wikipedia.org/wiki?curid=2642185", "title": "Problem of Apollonius", "text": "Problem of Apollonius\n\nIn Euclidean plane geometry, Apollonius's problem is to construct circles that are tangent to three given circles in a plane (Figure 1). Apollonius of Perga (c. 262 190 BC) posed and solved this famous problem in his work (\"\", \"Tangencies\"); this work has been lost, but a 4th-century AD report of his results by Pappus of Alexandria has survived. Three given circles generically have eight different circles that are tangent to them (Figure 2), a pair of solutions for each way to divide the three given circles in two subsets (there are 4 ways to divide a set of cardinality 3 in 2 parts).\n\nIn the 16th century, Adriaan van Roomen solved the problem using intersecting hyperbolas, but this solution does not use only straightedge and compass constructions. François Viète found such a solution by exploiting limiting cases: any of the three given circles can be shrunk to zero radius (a point) or expanded to infinite radius (a line). Viète's approach, which uses simpler limiting cases to solve more complicated ones, is considered a plausible reconstruction of Apollonius' method. The method of van Roomen was simplified by Isaac Newton, who showed that Apollonius' problem is equivalent to finding a position from the differences of its distances to three known points. This has applications in navigation and positioning systems such as LORAN.\n\nLater mathematicians introduced algebraic methods, which transform a geometric problem into algebraic equations. These methods were simplified by exploiting symmetries inherent in the problem of Apollonius: for instance solution circles generically occur in pairs, with one solution enclosing the given circles that the other excludes (Figure 2). Joseph Diaz Gergonne used this symmetry to provide an elegant straightedge and compass solution, while other mathematicians used geometrical transformations such as reflection in a circle to simplify the configuration of the given circles. These developments provide a geometrical setting for algebraic methods (using Lie sphere geometry) and a classification of solutions according to 33 essentially different configurations of the given circles.\n\nApollonius' problem has stimulated much further work. Generalizations to three dimensions—constructing a sphere tangent to four given spheres—and beyond have been studied. The configuration of three mutually tangent circles has received particular attention. René Descartes gave a formula relating the radii of the solution circles and the given circles, now known as Descartes' theorem. Solving Apollonius' problem iteratively in this case leads to the Apollonian gasket, which is one of the earliest fractals to be described in print, and is important in number theory via Ford circles and the Hardy–Littlewood circle method.\n\nThe general statement of Apollonius' problem is to construct one or more circles that are tangent to three given objects in a plane, where an object may be a line, a point or a circle of any size. These objects may be arranged in any way and may cross one another; however, they are usually taken to be distinct, meaning that they do not coincide. Solutions to Apollonius' problem are sometimes called \"Apollonius circles\", although the term is also used for other types of circles associated with Apollonius.\n\nThe property of tangency is defined as follows. First, a point, line or circle is assumed to be tangent to itself; hence, if a given circle is already tangent to the other two given objects, it is counted as a solution to Apollonius' problem. Two distinct geometrical objects are said to \"intersect\" if they have a point in common. By definition, a point is tangent to a circle or a line if it intersects them, that is, if it lies on them; thus, two distinct points cannot be tangent. If the angle between lines or circles at an intersection point is zero, they are said to be \"tangent\"; the intersection point is called a \"tangent point\" or a \"point of tangency\". (The word \"tangent\" derives from the Latin present participle, \"tangens\", meaning \"touching\".) In practice, two distinct circles are tangent if they intersect at only one point; if they intersect at zero or two points, they are not tangent. The same holds true for a line and a circle. Two distinct lines cannot be tangent in the plane, although two parallel lines can be considered as tangent at a point at infinity in inversive geometry (see below).\n\nThe solution circle may be either internally or externally tangent to each of the given circles. An \"external\" tangency is one where the two circles bend away from each other at their point of contact; they lie on opposite sides of the tangent line at that point, and they exclude one another. The distance between their centers equals the sum of their radii. By contrast, an \"internal\" tangency is one in which the two circles curve in the same way at their point of contact; the two circles lie on the same side of the tangent line, and one circle encloses the other. In this case, the distance between their centers equals the difference of their radii. As an illustration, in Figure 1, the pink solution circle is internally tangent to the medium-sized given black circle on the right, whereas it is externally tangent to the smallest and largest given circles on the left.\n\nApollonius' problem can also be formulated as the problem of locating one or more points such that the \"differences\" of its distances to three given points equal three known values. Consider a solution circle of radius \"r\" and three given circles of radii \"r\", \"r\" and \"r\". If the solution circle is externally tangent to all three given circles, the distances between the center of the solution circle and the centers of the given circles equal , and , respectively. Therefore, differences in these distances are constants, such as ; they depend only on the known radii of the given circles and not on the radius \"r\" of the solution circle, which cancels out. This second formulation of Apollonius' problem can be generalized to internally tangent solution circles (for which the center-center distance equals the difference of radii), by changing the corresponding differences of distances to sums of distances, so that the solution-circle radius \"r\" again cancels out. The re-formulation in terms of center-center distances is useful in the solutions below of Adriaan van Roomen and Isaac Newton, and also in hyperbolic positioning or trilateration, which is the task of locating a position from differences in distances to three known points. For example, navigation systems such as LORAN identify a receiver's position from the differences in arrival times of signals from three fixed positions, which correspond to the differences in distances to those transmitters.\n\nA rich repertoire of geometrical and algebraic methods have been developed to solve Apollonius' problem, which has been called \"the most famous of all\" geometry problems. The original approach of Apollonius of Perga has been lost, but reconstructions have been offered by François Viète and others, based on the clues in the description by Pappus of Alexandria. The first new solution method was published in 1596 by Adriaan van Roomen, who identified the centers of the solution circles as the intersection points of two hyperbolas. Van Roomen's method was refined in 1687 by Isaac Newton in his \"Principia\", and by John Casey in 1881.\n\nAlthough successful in solving Apollonius' problem, van Roomen's method has a drawback. A prized property in classical Euclidean geometry is the ability to solve problems using only a compass and a straightedge. Many constructions are impossible using only these tools, such as dividing an angle in three equal parts. However, many such \"impossible\" problems can be solved by intersecting curves such as hyperbolas, ellipses and parabolas (conic sections). For example, doubling the cube (the problem of constructing a cube of twice the volume of a given cube) cannot be done using only a straightedge and compass, but Menaechmus showed that the problem can be solved by using the intersections of two parabolas. Therefore, van Roomen's solution—which uses the intersection of two hyperbolas—did not determine if the problem satisfied the straightedge-and-compass property.\n\nVan Roomen's friend François Viète, who had urged van Roomen to work on Apollonius' problem in the first place, developed a method that used only compass and straightedge. Prior to Viète's solution, Regiomontanus doubted whether Apollonius' problem could be solved by straightedge and compass. Viète first solved some simple special cases of Apollonius' problem, such as finding a circle that passes through three given points which has only one solution if the points are distinct; he then built up to solving more complicated special cases, in some cases by shrinking or swelling the given circles. According to the 4th-century report of Pappus, Apollonius' own book on this problem—entitled (\"\", \"Tangencies\"; Latin: \"De tactionibus\", \"De contactibus\")—followed a similar progressive approach. Hence, Viète's solution is considered to be a plausible reconstruction of Apollonius' solution, although other reconstructions have been published independently by three different authors.\n\nSeveral other geometrical solutions to Apollonius' problem were developed in the 19th century. The most notable solutions are those of Jean-Victor Poncelet (1811) and of Joseph Diaz Gergonne (1814). Whereas Poncelet's proof relies on homothetic centers of circles and the power of a point theorem, Gergonne's method exploits the conjugate relation between lines and their poles in a circle. Methods using circle inversion were pioneered by Julius Petersen in 1879; one example is the annular solution method of HSM Coxeter. Another approach uses Lie sphere geometry, which was developed by Sophus Lie.\n\nAlgebraic solutions to Apollonius' problem were pioneered in the 17th century by René Descartes and Princess Elisabeth of Bohemia, although their solutions were rather complex. Practical algebraic methods were developed in the late 18th and 19th centuries by several mathematicians, including Leonhard Euler, Nicolas Fuss, Carl Friedrich Gauss, Lazare Carnot, and Augustin Louis Cauchy.\n\nThe solution of Adriaan van Roomen (1596) is based on the intersection of two hyperbolas. Let the given circles be denoted as \"C\", \"C\" and \"C\". Van Roomen solved the general problem by solving a simpler problem, that of finding the circles that are tangent to \"two\" given circles, such as \"C\" and \"C\". He noted that the center of a circle tangent to both given circles must lie on a hyperbola whose foci are the centers of the given circles. To understand this, let the radii of the solution circle and the two given circles be denoted as \"r\", \"r\" and \"r\", respectively (Figure 3). The distance \"d\" between the centers of the solution circle and \"C\" is either or , depending on whether these circles are chosen to be externally or internally tangent, respectively. Similarly, the distance \"d\" between the centers of the solution circle and \"C\" is either or , again depending on their chosen tangency. Thus, the difference between these distances is always a constant that is independent of \"r\". This property, of having a fixed difference between the distances to the foci, characterizes hyperbolas, so the possible centers of the solution circle lie on a hyperbola. A second hyperbola can be drawn for the pair of given circles \"C\" and \"C\", where the internal or external tangency of the solution and \"C\" should be chosen consistently with that of the first hyperbola. An intersection of these two hyperbolas (if any) gives the center of a solution circle that has the chosen internal and external tangencies to the three given circles. The full set of solutions to Apollonius' problem can be found by considering all possible combinations of internal and external tangency of the solution circle to the three given circles.\n\nIsaac Newton (1687) refined van Roomen's solution, so that the solution-circle centers were located at the intersections of a line with a circle. Newton formulates Apollonius' problem as a problem in trilateration: to locate a point Z from three given points A, B and C, such that the differences in distances from Z to the three given points have known values. These four points correspond to the center of the solution circle (Z) and the centers of the three given circles (A, B and C).\n\nInstead of solving for the two hyperbolas, Newton constructs their directrix lines instead. For any hyperbola, the ratio of distances from a point Z to a focus A and to the directrix is a fixed constant called the eccentricity. The two directrices intersect at a point T, and from their two known distance ratios, Newton constructs a line passing through T on which Z must lie. However, the ratio of distances TZ/TA is also known; hence, Z also lies on a known circle, since Apollonius had shown that a circle can be defined as the set of points that have a given ratio of distances to two fixed points. (As an aside, this definition is the basis of bipolar coordinates.) Thus, the solutions to Apollonius' problem are the intersections of a line with a circle.\n\nAs described below, Apollonius' problem has ten special cases, depending on the nature of the three given objects, which may be a circle (C), line (L) or point (P). By custom, these ten cases are distinguished by three letter codes such as CCP. Viète solved all ten of these cases using only compass and straightedge constructions, and used the solutions of simpler cases to solve the more complex cases.\n\nViète began by solving the PPP case (three points) following the method of Euclid in his \"Elements\". From this, he derived a lemma corresponding to the power of a point theorem, which he used to solve the LPP case (a line and two points). Following Euclid a second time, Viète solved the LLL case (three lines) using the angle bisectors. He then derived a lemma for constructing the line perpendicular to an angle bisector that passes through a point, which he used to solve the LLP problem (two lines and a point). This accounts for the first four cases of Apollonius' problem, those that do not involve circles.\n\nTo solve the remaining problems, Viète exploited the fact that the given circles and the solution circle may be re-sized in tandem while preserving their tangencies (Figure 4). If the solution-circle radius is changed by an amount Δ\"r\", the radius of its internally tangent given circles must be likewise changed by Δ\"r\", whereas the radius of its externally tangent given circles must be changed by −Δ\"r\". Thus, as the solution circle swells, the internally tangent given circles must swell in tandem, whereas the externally tangent given circles must shrink, to maintain their tangencies.\n\nViète used this approach to shrink one of the given circles to a point, thus reducing the problem to a simpler, already solved case. He first solved the CLL case (a circle and two lines) by shrinking the circle into a point, rendering it a LLP case. He then solved the CLP case (a circle, a line and a point) using three lemmas. Again shrinking one circle to a point, Viète transformed the CCL case into a CLP case. He then solved the CPP case (a circle and two points) and the CCP case (two circles and a point), the latter case by two lemmas. Finally, Viète solved the general CCC case (three circles) by shrinking one circle to a point, rendering it a CCP case.\n\nApollonius' problem can be framed as a system of three equations for the center and radius of the solution circle. Since the three given circles and any solution circle must lie in the same plane, their positions can be specified in terms of the (\"x\", \"y\") coordinates of their centers. For example, the center positions of the three given circles may be written as (\"x\", \"y\"), (\"x\", \"y\") and (\"x\", \"y\"), whereas that of a solution circle can be written as (\"x\", \"y\"). Similarly, the radii of the given circles and a solution circle can be written as \"r\", \"r\", \"r\" and \"r\", respectively. The requirement that a solution circle must exactly touch each of the three given circles can be expressed as three coupled quadratic equations for \"x\", \"y\" and \"r\":\n\nThe three numbers \"s\", \"s\" and \"s\" on the right-hand side, called signs, may equal ±1, and specify whether the desired solution circle should touch the corresponding given circle internally (\"s\" = 1) or externally (\"s\" = −1). For example, in Figures 1 and 4, the pink solution is internally tangent to the medium-sized given circle on the right and externally tangent to the smallest and largest given circles on the left; if the given circles are ordered by radius, the signs for this solution are . Since the three signs may be chosen independently, there are eight possible sets of equations , each set corresponding to one of the eight types of solution circles.\n\nThe general system of three equations may be solved by the method of resultants. When multiplied out, all three equations have on the left-hand side, and \"r\" on the right-hand side. Subtracting one equation from another eliminates these quadratic terms; the remaining linear terms may be re-arranged to yield formulae for the coordinates \"x\" and \"y\"\n\nwhere \"M\", \"N\", \"P\" and \"Q\" are known functions of the given circles and the choice of signs. Substitution of these formulae into one of the initial three equations gives a quadratic equation for \"r\", which can be solved by the quadratic formula. Substitution of the numerical value of \"r\" into the linear formulae yields the corresponding values of \"x\" and \"y\".\n\nThe signs \"s\", \"s\" and \"s\" on the right-hand sides of the equations may be chosen in eight possible ways, and each choice of signs gives up to two solutions, since the equation for \"r\" is quadratic. This might suggest (incorrectly) that there are up to sixteen solutions of Apollonius' problem. However, due to a symmetry of the equations, if (\"r\", \"x\", \"y\") is a solution, with signs \"s\", then so is (−\"r\", \"x\", \"y\"), with opposite signs −\"s\", which represents the same solution circle. Therefore, Apollonius' problem has at most eight independent solutions (Figure 2). One way to avoid this double-counting is to consider only solution circles with non-negative radius.\n\nThe two roots of any quadratic equation may be of three possible types: two different real numbers, two identical real numbers (i.e., a degenerate double root), or a pair of complex conjugate roots. The first case corresponds to the usual situation; each pair of roots corresponds to a pair of solutions that are related by circle inversion, as described below (Figure 6). In the second case, both roots are identical, corresponding to a solution circle that transforms into itself under inversion. In this case, one of the given circles is itself a solution to the Apollonius problem, and the number of distinct solutions is reduced by one. The third case of complex conjugate radii does not correspond to a geometrically possible solution for Apollonius' problem, since a solution circle cannot have an imaginary radius; therefore, the number of solutions is reduced by two. Apollonius' problem cannot have seven solutions, although it may have any other number of solutions from zero to eight.\n\nThe same algebraic equations can be derived in the context of Lie sphere geometry. That geometry represents circles, lines and points in a unified way, as a five-dimensional vector \"X\" = (\"v\", \"c\", \"c\", \"w\", \"sr\"), where c = (\"c\", \"c\") is the center of the circle, and \"r\" is its (non-negative) radius. If \"r\" is not zero, the sign \"s\" may be positive or negative; for visualization, \"s\" represents the orientation of the circle, with counterclockwise circles having a positive \"s\" and clockwise circles having a negative \"s\". The parameter \"w\" is zero for a straight line, and one otherwise.\n\nIn this five-dimensional world, there is a bilinear product similar to the dot product:\n\nThe Lie quadric is defined as those vectors whose product with themselves (their square norm) is zero, (\"X\"|\"X\") = 0. Let \"X\" and \"X\" be two vectors belonging to this quadric; the norm of their difference equals\n\nThe product distributes over addition and subtraction (more precisely, it is bilinear):\n\nSince (\"X\"|\"X\") = (\"X\"|\"X\") = 0 (both belong to the Lie quadric) and since \"w\" = \"w\" = 1 for circles, the product of any two such vectors on the quadric equals\n\nwhere the vertical bars sandwiching represent the length of that difference vector, i.e., the Euclidean norm. This formula shows that if two quadric vectors \"X\" and \"X\" are orthogonal (perpendicular) to one another—that is, if (\"X\"|\"X\")=0—then their corresponding circles are tangent. For if the two signs \"s\" and \"s\" are the same (i.e. the circles have the same \"orientation\"), the circles are internally tangent; the distance between their centers equals the \"difference\" in the radii\n\nConversely, if the two signs \"s\" and \"s\" are different (i.e. the circles have opposite \"orientations\"), the circles are externally tangent; the distance between their centers equals the \"sum\" of the radii\n\nTherefore, Apollonius' problem can be re-stated in Lie geometry as a problem of finding perpendicular vectors on the Lie quadric; specifically, the goal is to identify solution vectors \"X\" that belong to the Lie quadric and are also orthogonal (perpendicular) to the vectors \"X\", \"X\" and \"X\" corresponding to the given circles.\n\nThe advantage of this re-statement is that one can exploit theorems from linear algebra on the maximum number of linearly independent, simultaneously perpendicular vectors. This gives another way to calculate the maximum number of solutions and extend the theorem to higher-dimensional spaces.\n\nA natural setting for problem of Apollonius is inversive geometry. The basic strategy of inversive methods is to transform a given Apollonius problem into another Apollonius problem that is simpler to solve; the solutions to the original problem are found from the solutions of the transformed problem by undoing the transformation. Candidate transformations must change one Apollonius problem into another; therefore, they must transform the given points, circles and lines to other points, circles and lines, and no other shapes. Circle inversion has this property and allows the center and radius of the inversion circle to be chosen judiciously. Other candidates include the Euclidean plane isometries; however, they do not simplify the problem, since they merely shift, rotate, and mirror the original problem.\n\nInversion in a circle with center O and radius \"R\" consists of the following operation (Figure 5): every point P is mapped into a new point P' such that O, P, and P' are collinear, and the product of the distances of P and P' to the center O equal the radius \"R\" squared\n\nThus, if P lies outside the circle, then P' lies within, and vice versa. When P is the same as O, the inversion is said to send P to infinity. (In complex analysis, \"infinity\" is defined in terms of the Riemann sphere.) Inversion has the useful property that lines and circles are always transformed into lines and circles, and points are always transformed into points. Circles are generally transformed into other circles under inversion; however, if a circle passes through the center of the inversion circle, it is transformed into a straight line, and vice versa. Importantly, if a circle crosses the circle of inversion at right angles (intersects perpendicularly), it is left unchanged by the inversion; it is transformed into itself.\n\nCircle inversions correspond to a subset of Möbius transformations on the Riemann sphere. The planar Apollonius problem can be transferred to the sphere by an inverse stereographic projection; hence, solutions of the planar Apollonius problem also pertain to its counterpart on the sphere. Other inversive solutions to the planar problem are possible besides the common ones described below.\n\nSolutions to Apollonius' problem generally occur in pairs; for each solution circle, there is a conjugate solution circle (Figure 6). One solution circle excludes the given circles that are enclosed by its conjugate solution, and vice versa. For example, in Figure 6, one solution circle (pink, upper left) encloses two given circles (black), but excludes a third; conversely, its conjugate solution (also pink, lower right) encloses that third given circle, but excludes the other two. The two conjugate solution circles are related by inversion, by the following argument.\n\nIn general, any three distinct circles have a unique circle—the radical circle—that intersects all of them perpendicularly; the center of that circle is the radical center of the three circles. For illustration, the orange circle in Figure 6 crosses the black given circles at right angles. Inversion in the radical circle leaves the given circles unchanged, but transforms the two conjugate pink solution circles into one another. Under the same inversion, the corresponding points of tangency of the two solution circles are transformed into one another; for illustration, in Figure 6, the two blue points lying on each green line are transformed into one another. Hence, the lines connecting these conjugate tangent points are invariant under the inversion; therefore, they must pass through the center of inversion, which is the radical center (green lines intersecting at the orange dot in Figure 6).\n\nIf two of the three given circles do not intersect, a center of inversion can be chosen so that those two given circles become concentric. Under this inversion, the solution circles must fall within the annulus between the two concentric circles. Therefore, they belong to two one-parameter families. In the first family (Figure 7), the solutions do \"not\" enclose the inner concentric circle, but rather revolve like ball bearings in the annulus. In the second family (Figure 8), the solution circles enclose the inner concentric circle. There are generally four solutions for each family, yielding eight possible solutions, consistent with the algebraic solution.\n\nWhen two of the given circles are concentric, Apollonius' problem can be solved easily using a method of Gauss. The radii of the three given circles are known, as is the distance \"d\" from the common concentric center to the non-concentric circle (Figure 7). The solution circle can be determined from its radius \"r\", the angle θ, and the distances \"d\" and \"d\" from its center to the common concentric center and the center of the non-concentric circle, respectively. The radius and distance \"d\" are known (Figure 7), and the distance \"d\" = \"r\" ± \"r\", depending on whether the solution circle is internally or externally tangent to the non-concentric circle. Therefore, by the law of cosines,\n\nHere, a new constant \"C\" has been defined for brevity, with the subscript indicating whether the solution is externally or internally tangent. A simple trigonometric rearrangement yields the four solutions\n\nThis formula represents four solutions, corresponding to the two choices of the sign of θ, and the two choices for \"C\". The remaining four solutions can be obtained by the same method, using the substitutions for \"r\" and \"d\" indicated in Figure 8. Thus, all eight solutions of the general Apollonius problem can be found by this method.\n\nAny initial two disjoint given circles can be rendered concentric as follows. The radical axis of the two given circles is constructed; choosing two arbitrary points P and Q on this radical axis, two circles can be constructed that are centered on P and Q and that intersect the two given circles orthogonally. These two constructed circles intersect each other in two points. Inversion in one such intersection point F renders the constructed circles into straight lines emanating from F and the two given circles into concentric circles, with the third given circle becoming another circle (in general). This follows because the system of circles is equivalent to a set of Apollonian circles, forming a bipolar coordinate system.\n\nThe usefulness of inversion can be increased significantly by resizing. As noted in Viète's reconstruction, the three given circles and the solution circle can be resized in tandem while preserving their tangencies. Thus, the initial Apollonius problem is transformed into another problem that may be easier to solve. For example, the four circles can be resized so that one given circle is shrunk to a point; alternatively, two given circles can often be resized so that they are tangent to one another. Thirdly, given circles that intersect can be resized so that they become non-intersecting, after which the method for inverting to an annulus can be applied. In all such cases, the solution of the original Apollonius problem is obtained from the solution of the transformed problem by undoing the resizing and inversion.\n\nIn the first approach, the given circles are shrunk or swelled (appropriately to their tangency) until one given circle is shrunk to a point P. In that case, Apollonius' problem degenerates to the CCP , which is the problem of finding a solution circle tangent to the two remaining given circles that passes through the point P. Inversion in a circle centered on P transforms the two given circles into new circles, and the solution circle into a line. Therefore, the transformed solution is a line that is tangent to the two transformed given circles. There are four such solution lines, which may be constructed from the external and internal homothetic centers of the two circles. Re-inversion in P and undoing the resizing transforms such a solution line into the desired solution circle of the original Apollonius problem. All eight general solutions can be obtained by shrinking and swelling the circles according to the differing internal and external tangencies of each solution; however, different given circles may be shrunk to a point for different solutions.\n\nIn the second approach, the radii of the given circles are modified appropriately by an amount Δ\"r\" so that two of them are tangential (touching). Their point of tangency is chosen as the center of inversion in a circle that intersects each of the two touching circles in two places. Upon inversion, the touching circles become two parallel lines: Their only point of intersection is sent to infinity under inversion, so they cannot meet. The same inversion transforms the third circle into another circle. The solution of the inverted problem must either be (1) a straight line parallel to the two given parallel lines and tangent to the transformed third given circle; or (2) a circle of constant radius that is tangent to the two given parallel lines and the transformed given circle. Re-inversion and adjusting the radii of all circles by Δ\"r\" produces a solution circle tangent to the original three circles.\n\nGergonne's approach is to consider the solution circles in pairs. Let a pair of solution circles be denoted as \"C\" and \"C\" (the pink circles in Figure 6), and let their tangent points with the three given circles be denoted as A, A, A, and B, B, B, respectively. Gergonne's solution aims to locate these six points, and thus solve for the two solution circles.\n\nGergonne's insight was that if a line \"L\" could be constructed such that A and B were guaranteed to fall on it, those two points could be identified as the intersection points of \"L\" with the given circle \"C\" (Figure 6). The remaining four tangent points would be located similarly, by finding lines \"L\" and \"L\" that contained A and B, and A and B, respectively. To construct a line such as \"L\", two points must be identified that lie on it; but these points need not be the tangent points. Gergonne was able to identify two other points for each of the three lines. One of the two points has already been identified: the radical center G lies on all three lines (Figure 6).\n\nTo locate a second point on the lines \"L\", \"L\" and \"L\", Gergonne noted a reciprocal relationship between those lines and the radical axis \"R\" of the solution circles, \"C\" and \"C\". To understand this reciprocal relationship, consider the two tangent lines to the circle \"C\" drawn at its tangent points A and B with the solution circles; the intersection of these tangent lines is the pole point of \"L\" in \"C\". Since the distances from that pole point to the tangent points A and B are equal, this pole point must also lie on the radical axis \"R\" of the solution circles, by definition (Figure 9). The relationship between pole points and their polar lines is reciprocal; if the pole of \"L\" in \"C\" lies on \"R\", the pole of \"R\" in \"C\" must conversely lie on \"L\". Thus, if we can construct \"R\", we can find its pole P in \"C\", giving the needed second point on \"L\" (Figure 10).\n\nGergonne found the radical axis \"R\" of the unknown solution circles as follows. Any pair of circles has two centers of similarity; these two points are the two possible intersections of two tangent lines to the two circles. Therefore, the three given circles have six centers of similarity, two for each distinct pair of given circles. Remarkably, these six points lie on four lines, three points on each line; moreover, each line corresponds to the radical axis of a potential pair of solution circles. To show this, Gergonne considered lines through corresponding points of tangency on two of the given circles, e.g., the line defined by A/A and the line defined by B/B. Let X be a center of similitude for the two circles \"C\" and \"C\"; then, A/A and B/B are pairs of antihomologous points, and their lines intersect at X. It follows, therefore, that the products of distances are equal\n\nwhich implies that X lies on the radical axis of the two solution circles. The same argument can be applied to the other pairs of circles, so that three centers of similitude for the given three circles must lie on the radical axes of pairs of solution circles.\n\nIn summary, the desired line \"L\" is defined by two points: the radical center G of the three given circles and the pole in \"C\" of one of the four lines connecting the homothetic centers. Finding the same pole in \"C\" and \"C\" gives \"L\" and \"L\", respectively; thus, all six points can be located, from which one pair of solution circles can be found. Repeating this procedure for the remaining three homothetic-center lines yields six more solutions, giving eight solutions in all. However, if a line \"L\" does not intersect its circle \"C\" for some \"k\", there is no pair of solutions for that homothetic-center line.\n\nThe techniques of modern algebraic geometry, and in particular intersection theory, can be used to solve Apollonius's problem. In this approach, the problem is reinterpreted as a statement about circles in the complex projective plane. Solutions involving complex numbers are allowed and degenerate situations are counted with multiplicity. When this is done, there are always eight solutions to the problem.\n\nEvery quadratic equation in , , and determines a unique conic, its vanishing locus. Conversely, every conic in the complex projective plane has an equation, and that equation is unique up to an overall scaling factor (because rescaling an equation does not change its vanishing locus). Therefore, the set of all conics may be parametrized by five-dimensional projective space , where the correspondence is\nA \"circle\" in the complex projective plane is defined to be a conic that passes through the two points and , where denotes a square root of . The points and are called the \"circular points\". The projective variety of all circles is the subvariety of consisting of those points which correspond to conics passing through the circular points. Substituting the circular points into the equation for a generic conic yields the two equations\nTaking the sum and difference of these equations shows that it is equivalent to impose the conditions\nTherefore, the variety of all circles is a three-dimensional linear subspace of . After rescaling and completing the square, these equations also demonstrate that every conic passing through the circular points has an equation of the form\nwhich is the homogenization of the usual equation of a circle in the affine plane. Therefore, studying circles in the above sense is nearly equivalent to studying circles in the conventional sense. The only difference is that the above sense permits degenerate circles which are the union of two lines. The non-degenerate circles are called \"smooth circles\", while the degenerate ones are called \"singular circles.\" There are two types of singular circles. One is the union of the line at infinity with another line in the projective plane (possibly the line at infinity again), and the other is union of two lines in the projective plane, one through each of the two circular points. These are the limits of smooth circles as the radius tends to and , respectively. In the latter case, no point on either of the two lines has real coordinates except for the origin .\n\nLet be a fixed smooth circle. If is any other circle, then, by the definition of a circle, and intersect at the circular points and . Because and are conics, Bézout's theorem implies and intersect in four points total, when those points are counted with the proper intersection multiplicity. That is, there are four points of intersection , , , and , but some of these points might collide. Appolonius' problem is concerned with the situation where , meaning that the intersection multiplicity at that point is ; if is also equal to a circular point, this should be interpreted as the intersection multiplicity being .\n\nLet be the variety of circles tangent to . This variety is a quadric cone in the of all circles. To see this, consider the incidence correspondence\nFor a curve that is the vanishing locus of a single equation , the condition that the curve meets at with multiplicity means that the Taylor series expansion of vanishes to order at ; it is therefore linear conditions on the coefficients of . This shows that, for each , the fiber of over is a cut out by two linear equations in the space of circles. Consequently, is irreducible of dimension . Since it is possible to exhibit a circle that is tangent to at only a single point, a generic element of must be tangent at only a single point. Therefore, the projection sending to is a birational morphism. It follows that the image of , which is , is also irreducible and two dimensional.\n\nTo determine the shape of , fix two distinct circles and , not necessarily tangent to . These two circles determine a pencil, meaning a line in the of circles. If the equations of and are and , respectively, then the points on correspond to the circles whose equations are , where is a point of . The points where meets are precisely the circles in the pencil that are tangent to .\n\nThere are two possibilities for the number of points of intersections. One is that either or , say , is the equation for . In this case, is a line through . If is tangent to , then so is every circle in the pencil, and therefore is contained in . The other possibility is that neither nor is the equation for . In this case, the function is a quotient of quadratics, neither of which vanishes identically. Therefore, it vanishes at two points and has poles at two points. These are the points in and , respectively, counted with multiplicity and with the circular points deducted. The rational function determines a morphism of degree two. The fiber over is the set of points for which . These are precisely the points at which the circle whose equation is meets . The branch points of this morphism are the circles tangent to . By the Riemann–Hurwitz formula, there are precisely two branch points, and therefore meets in two points. Together, these two possibilities for the intersection of and demonstrate that is a quadric cone. All such cones in are the same up to a change of coordinates, so this completely determines the shape of .\n\nTo conclude the argument, let , , and be three circles. If the intersection is finite, then it has degree , and therefore there are eight solutions to the problem of Apollonius, counted with multiplicity. To prove that the intersection is generically finite, consider the incidence correspondence\nThere is a morphism which projects onto its final factor of . The fiber over is . This has dimension , so has dimension . Because also has dimension , the generic fiber of the projection from to the first three factors cannot have positive dimension. This proves that generically, there are eight solutions counted with multiplicity. Since it is possible to exhibit a configuration where the eight solutions are distinct, the generic configuration must have all eight solutions distinct.\n\nIn the generic problem with eight solution circles, The reciprocals of the radii of four of the solution circles sum to the same value as do the reciprocals of the radii of the other four solution circles.\n\nApollonius problem is to construct one or more circles tangent to three given objects in a plane, which may be circles, points, or lines. This gives rise to ten types of Apollonius' problem, one corresponding to each combination of circles, lines and points, which may be labeled with three letters, either C, L, or P, to denote whether the given elements are a circle, line or point, respectively (Table 1). As an example, the type of Apollonius problem with a given circle, line, and point is denoted as CLP.\n\nSome of these special cases are much easier to solve than the general case of three given circles. The two simplest cases are the problems of drawing a circle through three given points (PPP) or tangent to three lines (LLL), which were solved first by Euclid in his \"Elements\". For example, the PPP problem can be solved as follows. The center of the solution circle is equally distant from all three points, and therefore must lie on the perpendicular bisector line of any two. Hence, the center is the point of intersection of any two perpendicular bisectors. Similarly, in the LLL case, the center must lie on a line bisecting the angle at the three intersection points between the three given lines; hence, the center lies at the intersection point of two such angle bisectors. Since there are two such bisectors at every intersection point of the three given lines, there are four solutions to the general LLL problem.\n\nPoints and lines may be viewed as special cases of circles; a point can be considered as a circle of infinitely small radius, and a line may be thought of an infinitely large circle whose center is also at infinity. From this perspective, the general Apollonius problem is that of constructing circles tangent to three given circles. The nine other cases involving points and lines may be viewed as limiting cases of the general problem. These limiting cases often have fewer solutions than the general problem; for example, the replacement of a given circle by a given point halves the number of solutions, since a point can be construed as an infinitesimal circle that is either internally or externally tangent.\n\nThe problem of counting the number of solutions to different types of Apollonius' problem belongs to the field of enumerative geometry. The general number of solutions for each of the ten types of Apollonius' problem is given in Table 1 above. However, special arrangements of the given elements may change the number of solutions. For illustration, Apollonius' problem has no solution if one circle separates the two (Figure 11); to touch both the solid given circles, the solution circle would have to cross the dashed given circle; but that it cannot do, if it is to touch the dashed circle tangentially. Conversely, if three given circles are all tangent at the same point, then \"any\" circle tangent at the same point is a solution; such Apollonius problems have an infinite number of solutions. If any of the given circles are identical, there is likewise an infinity of solutions. If only two given circles are identical, there are only two distinct given circles; the centers of the solution circles form a hyperbola, as used in one solution to Apollonius' problem.\n\nAn exhaustive enumeration of the number of solutions for all possible configurations of three given circles, points or lines was first undertaken by Muirhead in 1896, although earlier work had been done by Stoll and Study. However, Muirhead's work was incomplete; it was extended in 1974 and a definitive enumeration, with 33 distinct cases, was published in 1983. Although solutions to Apollonius' problem generally occur in pairs related by inversion, an odd number of solutions is possible in some cases, e.g., the single solution for PPP, or when one or three of the given circles are themselves solutions. (An example of the latter is given in the on Descartes' theorem.) However, there are no Apollonius problems with seven solutions. Alternative solutions based on the geometry of circles and spheres have been developed and used in higher dimensions.\n\nIf the three given circles are mutually tangent, Apollonius' problem has five solutions. Three solutions are the given circles themselves, since each is tangent to itself and to the other two given circles. The remaining two solutions (shown in red in Figure 12) correspond to the inscribed and circumscribed circles, and are called \"Soddy's circles\". This special case of Apollonius' problem is also known as the four coins problem. The three given circles of this Apollonius problem form a Steiner chain tangent to the two Soddy's circles.\n\nEither Soddy circle, when taken together with the three given circles, produces a set of four circles that are mutually tangent at six points. The radii of these four circles are related by an equation known as Descartes' theorem. In a 1643 letter to Princess Elizabeth of Bohemia, René Descartes showed that\n\nwhere \"k\" = 1/\"r\" and \"r\" are the curvature and radius of the solution circle, respectively, and similarly for the curvatures \"k\", \"k\" and \"k\" and radii \"r\", \"r\" and \"r\" of the three given circles. For every set of four mutually tangent circles, there is a second set of four mutually tangent circles that are tangent at the same six points.\n\nDescartes' theorem was rediscovered independently in 1826 by Jakob Steiner, in 1842 by Philip Beecroft, and again in 1936 by Frederick Soddy. Soddy published his findings in the scientific journal \"Nature\" as a poem, \"The Kiss Precise\", of which the first two stanzas are reproduced below. The first stanza describes Soddy's circles, whereas the second stanza gives Descartes' theorem. In Soddy's poem, two circles are said to \"kiss\" if they are tangent, whereas the term \"bend\" refers to the curvature \"k\" of the circle.\n\nSundry extensions of Descartes' theorem have been derived by Daniel Pedoe.\n\nApollonius' problem can be extended to construct all the circles that intersect three given circles at a precise angle θ, or at three specified crossing angles θ, θ and θ; the ordinary Apollonius' problem corresponds to a special case in which the crossing angle is zero for all three given circles. Another generalization is the dual of the first extension, namely, to construct circles with three specified tangential distances from the three given circles.\n\nApollonius' problem can be extended from the plane to the sphere and other quadratic surfaces. For the sphere, the problem is to construct all the circles (the boundaries of spherical caps) that are tangent to three given circles on the sphere. This spherical problem can be rendered into a corresponding planar problem using stereographic projection. Once the solutions to the planar problem have been constructed, the corresponding solutions to the spherical problem can be determined by inverting the stereographic projection. Even more generally, one can consider the problem of four tangent curves that result from the intersections of an arbitrary quadratic surface and four planes, a problem first considered by Charles Dupin.\n\nBy solving Apollonius' problem repeatedly to find the inscribed circle, the interstices between mutually tangential circles can be filled arbitrarily finely, forming an Apollonian gasket, also known as a \"Leibniz packing\" or an \"Apollonian packing\". This gasket is a fractal, being self-similar and having a dimension \"d\" that is not known exactly but is roughly 1.3, which is higher than that of a regular (or rectifiable) curve (\"d\" = 1) but less than that of a plane (\"d\" = 2). The Apollonian gasket was first described by Gottfried Leibniz in the 17th century, and is a curved precursor of the 20th-century Sierpiński triangle. The Apollonian gasket also has deep connections to other fields of mathematics; for example, it is the limit set of Kleinian groups.\n\nThe configuration of a circle tangent to \"four\" circles in the plane has special properties, which have been elucidated by Larmor (1891) and Lachlan (1893). Such a configuration is also the basis for Casey's theorem, itself a generalization of Ptolemy's theorem.\n\nThe extension of Apollonius' problem to three dimensions, namely, the problem of finding a fifth sphere that is tangent to four given spheres, can be solved by analogous methods. For example, the given and solution spheres can be resized so that one given sphere is shrunk to point while maintaining tangency. Inversion in this point reduces Apollonius' problem to finding a plane that is tangent to three given spheres. There are in general eight such planes, which become the solutions to the original problem by reversing the inversion and the resizing. This problem was first considered by Pierre de Fermat, and many alternative solution methods have been developed over the centuries.\n\nApollonius' problem can even be extended to \"d\" dimensions, to construct the hyperspheres tangent to a given set of hyperspheres. Following the publication of Frederick Soddy's re-derivation of the Descartes theorem in 1936, several people solved (independently) the mutually tangent case corresponding to Soddy's circles in \"d\" dimensions.\n\nThe principal application of Apollonius' problem, as formulated by Isaac Newton, is hyperbolic trilateration, which seeks to determine a position from the \"differences\" in distances to at least three points. For example, a ship may seek to determine its position from the differences in arrival times of signals from three synchronized transmitters. Solutions to Apollonius' problem were used in World War I to determine the location of an artillery piece from the time a gunshot was heard at three different positions, and hyperbolic trilateration is the principle used by the Decca Navigator System and LORAN. Similarly, the location of an aircraft may be determined from the difference in arrival times of its transponder signal at four receiving stations. This multilateration problem is equivalent to the three-dimensional generalization of Apollonius' problem and applies to global navigation satellite systems (see GPS#Geometric interpretation). It is also used to determine the position of calling animals (such as birds and whales), although Apollonius' problem does not pertain if the speed of sound varies with direction (i.e., the transmission medium not isotropic).\n\nApollonius' problem has other applications. In Book 1, Proposition 21 in his \"Principia\", Isaac Newton used his solution of Apollonius' problem to construct an orbit in celestial mechanics from the center of attraction and observations of tangent lines to the orbit corresponding to instantaneous velocity. The special case of the problem of Apollonius when all three circles are tangent is used in the Hardy–Littlewood circle method of analytic number theory to construct Hans Rademacher's contour for complex integration, given by the boundaries of an infinite set of Ford circles each of which touches several others. Finally, Apollonius' problem has been applied to some types of packing problems, which arise in disparate fields such as the error-correcting codes used on DVDs and the design of pharmaceuticals that bind in a particular enzyme of a pathogenic bacterium.\n\n\n\n"}
{"id": "47332071", "url": "https://en.wikipedia.org/wiki?curid=47332071", "title": "Proof School", "text": "Proof School\n\nProof School is a school in San Francisco for pupils who love mathematics. The school opened in the fall of 2015 with 45 students in grades 6–10. Currently, 94.4 students in grades 6–12 are enrolled in Proof School for the academic year (2018–2019).\n\nProof School is a full-curriculum day school that emphasizes communication, collaboration, and problem solving. The school is accredited by Western Association of Schools and Colleges.\n\nThe school year is divided into 5 'blocks', each of which consist of 6 normal academic weeks and a build week.\n\nEach student has 5 courses: 4 'morning' courses that vary across grades, and a math class. The morning courses meet twice a week: once for 70 minutes, and once for 90 minutes. The math courses meet for two hours and ten minutes every day. \n\nThe (non-post-calculus) math classes focus on a different subject each block: Block 1 is Combinatorics, Block 2 is Algebra, Block 3 is Geometry, Block 4 is Algebra and Pre-Calculus, and Block 5 is Number Theory.\n"}
{"id": "46487145", "url": "https://en.wikipedia.org/wiki?curid=46487145", "title": "Robert Finn (mathematician)", "text": "Robert Finn (mathematician)\n\nRobert Finn (born August 8, 1922) is an American mathematician.\n\nFinn was born in Buffalo, New York. He received in 1951 his PhD from Syracuse University under Abe Gelbart with the thesis \"On some properties of the solution of a class of non-linear partial differential equations\". As a postdoc he was in 1953 at the Institute for Advanced Study and in 1953/54 at the Institute for Hydrodynamics at the University of Maryland. He became in 1954 an assistant professor at the University of Southern California and in 1956 an associate professor at California Institute of Technology. Since 1959 he has been a professor at Stanford University.\n\nAt the beginning of his career Finn did research on minimal surfaces and quasiconformal mappings and later in his career on mathematical problems of hydrodynamics, such as mathematically rigorous treatments of capillary action. He was a visiting professor at the University of Bonn and several other universities. He was an exchange scientist in 1978 at the Soviet Academy of Sciences and in 1987 at the Akademie der Wissenschaften der DDR. In 1994 he received an honorary doctorate from the University of Leipzig. For the academic years 1958/58 und 1965/66 he held Guggenheim Fellowships. Since 1979 he has been an editor of the \"Pacific Journal of Mathematics\".\n\n\n"}
{"id": "1354446", "url": "https://en.wikipedia.org/wiki?curid=1354446", "title": "Schönhage–Strassen algorithm", "text": "Schönhage–Strassen algorithm\n\nThe Schönhage–Strassen algorithm is an asymptotically fast multiplication algorithm for large integers. It was developed by Arnold Schönhage and Volker Strassen in 1971. The run-time bit complexity is, in Big O notation,formula_1 for two \"n\"-digit numbers. The algorithm uses recursive Fast Fourier transforms in rings with 2+1 elements, a specific type of number theoretic transform.\n\nThe Schönhage–Strassen algorithm was the asymptotically fastest multiplication method known from 1971 until 2007, when a new method, Fürer's algorithm, was announced with lower asymptotic complexity; however, Fürer's algorithm currently only achieves an advantage for astronomically large values and is not used in practice.\n\nIn practice the Schönhage–Strassen algorithm starts to outperform older methods such as Karatsuba and Toom–Cook multiplication for numbers beyond 2 to 2 (10,000 to 40,000 decimal digits). The GNU Multi-Precision Library uses it for values of at least 1728 to 7808 64-bit words (33,000 to 150,000 decimal digits), depending on architecture. There is a Java implementation of Schönhage–Strassen which uses it above 74,000 decimal digits.\n\nApplications of the Schönhage–Strassen algorithm include mathematical empiricism, such as the Great Internet Mersenne Prime Search and computing approximations of \"π\", as well as practical applications such as Kronecker substitution, in which multiplication of polynomials with integer coefficients can be efficiently reduced to large integer multiplication; this is used in practice by GMP-ECM for Lenstra elliptic curve factorization.\n\nThis section explains in detail how Schönhage–Strassen is implemented. It is based primarily on an overview of the method by Crandall and Pomerance in their \"Prime Numbers: A Computational Perspective\". This variant differs somewhat from Schönhage's original method in that it exploits the discrete weighted transform to perform negacyclic convolutions more efficiently. Another source for detailed information is Knuth's \"The Art of Computer Programming\". The section begins by discussing the building blocks and concludes with a step-by-step description of the algorithm itself. \n\nSuppose we are multiplying two numbers like 123 and 456 using long multiplication with base \"B\" digits, but without performing any carrying. The result might look something like this:\n\nThis sequence (4, 13, 28, 27, 18) is called the \"acyclic\" or \"linear convolution\" of the two original sequences (1,2,3) and (4,5,6). Once you have the acyclic convolution of two sequences, computing the product of the original numbers is easy: you just perform the carrying (for example, in the rightmost column, you'd keep the 8 and add the 1 to the column containing 27). In the example this yields the correct product 56088.\n\nThere are two other types of convolutions that will be useful. Suppose the input sequences have \"n\" elements (here 3). Then the acyclic convolution has \"n\"+\"n\"−1 elements; if we take the rightmost \"n\" elements and add the leftmost \"n\"−1 elements, this produces the cyclic convolution:\n\nIf we perform carrying on the cyclic convolution, the result is equivalent to the product of the inputs mod B − 1. In the example, 10 − 1 = 999, performing carrying on (28, 31, 31) yields 3141, and 3141 ≡ 56088 (mod 999).\n\nConversely, if we take the rightmost \"n\" elements and \"subtract\" the leftmost \"n\"−1 elements, this produces the negacyclic convolution:\n\nIf we perform carrying on the negacyclic convolution, the result is equivalent to the product of the inputs mod B + 1. In the example, 10 + 1 = 1001, performing carrying on (28, 23, 5) yields 3035, and 3035 ≡ 56088 (mod 1001). The negacyclic convolution can contain negative numbers, which can be eliminated during carrying using borrowing, as is done in long subtraction.\n\nLike other multiplication methods based on the Fast Fourier transform, Schönhage–Strassen depends fundamentally on the convolution theorem, which provides an efficient way to compute the cyclic convolution of two sequences. It states that:\n\nOr in symbols:\n\nIf we compute the DFT and IDFT using a fast Fourier transform algorithm, and invoke our multiplication algorithm recursively to multiply the entries of the transformed vectors DFT(\"X\") and DFT(\"Y\"), this yields an efficient algorithm for computing the cyclic convolution.\n\nIn this algorithm, it will be more useful to compute the \"negacyclic\" convolution; as it turns out, a slightly modified version of the convolution theorem (see discrete weighted transform) can enable this as well. Suppose the vectors X and Y have length \"n\", and \"a\" is a primitive root of unity of order 2\"n\" (that is, \"a\" = 1 and \"a\" to all smaller powers is not 1). Then we can define a third vector \"A\", called the \"weight vector\", as:\n\nNow, we can state:\n\nIn other words, it's the same as before except that the inputs are first multiplied by \"A\", and the result is multiplied by \"A\".\n\nThe discrete Fourier transform is an abstract operation that can be performed in any algebraic ring; typically it's performed in the complex numbers, but actually performing complex arithmetic to sufficient precision to ensure accurate results for multiplication is slow and error-prone. Instead, we will use the approach of the number theoretic transform, which is to perform the transform in the integers mod \"N\" for some integer \"N\".\n\nJust like there are primitive roots of unity of every order in the complex plane, given any order \"n\" we can choose a suitable N such that \"b\" is a primitive root of unity of order \"n\" in the integers mod \"N\" (in other words, \"b\" ≡ 1 (mod \"N\"), and no smaller power of \"b\" is equivalent to 1 mod \"N\").\n\nThe algorithm will spend most of its time performing recursive multiplications of smaller numbers; with a naive algorithm, these occur in a number of places:\n\n\nThe key insight to Schönhage–Strassen is to choose N, the modulus, to be equal to 2 + 1 for some integer \"n\" that is a multiple of the number of pieces \"P\"=2 being transformed. This has a number of benefits in standard systems that represent large integers in binary form:\n\n\nTo make the recursive multiplications convenient, we will frame Schönhage–Strassen as being a specialized multiplication algorithm for computing not just the product of two numbers, but the product of two numbers mod 2 + 1 for some given \"n\". This is not a loss of generality, since one can always choose \"n\" large enough so that the product mod 2 + 1 is simply the product.\n\nIn the course of the algorithm, there are many cases in which multiplication or division by a power of two (including all roots of unity) can be profitably replaced by a small number of shifts and adds. This makes use of the observation that:\n\nNote that a \"k\"-digit number in base 2 written in positional notation can be expressed as (\"d\"...,\"d\",\"d\"). It represents the number formula_2. Also note that for each \"d\" we have 0≤\"d\" < 2.\n\nThis makes it simple to reduce a number represented in binary mod 2 + 1: take the rightmost (least significant) \"n\" bits, subtract the next \"n\" bits, add the next \"n\" bits, and so on until the bits are exhausted. If the resulting value is still not between 0 and 2, normalize it by adding or subtracting a multiple of the modulus 2 + 1. For example, if \"n\"=3 (and so the modulus is 2+1 = 9) and the number being reduced is 656, we have:\n\nMoreover, it's possible to effect very large shifts without ever constructing the shifted result. Suppose we have a number A between 0 and 2, and wish to multiply it by 2. Dividing \"k\" by \"n\" we find \"k\" = \"qn\" + \"r\" with \"r\" < \"n\". It follows that:\n\nSince A is ≤ 2 and \"r\" < \"n\", A shift-left \"r\" has at most 2\"n\"−1 bits, and so only one shift and subtraction (followed by normalization) is needed.\n\nFinally, to divide by 2, observe that squaring the first equivalence above yields:\n\nHence,\n\nThe algorithm follows a split, evaluate (forward FFT), pointwise multiply, interpolate (inverse FFT), and combine phases similar to Karatsuba and Toom-Cook methods.\n\nGiven input numbers \"x\" and \"y\", and an integer \"N\", the following algorithm computes the product \"xy\" mod 2 + 1. Provided N is sufficiently large this is simply the product.\n\n\nThe optimal number of pieces to divide the input into is proportional to formula_3, where \"N\" is the number of input bits, and this setting achieves the running time of O(\"N\" log \"N\" log log \"N\"), so the parameter \"k\" should be set accordingly. In practice, it is set empirically based on the input sizes and the architecture, typically to a value between 4 and 16.\n\nIn step 2, the observation is used that:\n\nThis section explains a number of important practical optimizations that have been considered when implementing Schönhage–Strassen in real systems. It is based primarily on a 2007 work by Gaudry, Kruppa, and Zimmermann describing enhancements to the GNU Multi-Precision Library.\n\nBelow a certain cutoff point, it's more efficient to perform the recursive multiplications using other algorithms, such as Toom–Cook multiplication. The results must be reduced mod 2 + 1, which can be done efficiently as explained above in Shift optimizations with shifts and adds/subtracts.\n\nComputing the IDFT involves dividing each entry by the primitive root of unity 2, an operation that is frequently combined with multiplying the vector by A afterwards, since both involve division by a power of two.\n\nIn a system where a large number is represented as an array of 2-bit words, it's useful to ensure that the vector size 2 is also a multiple of the bits per word by choosing \"k\" ≥ \"w\" (e.g. choose \"k\" ≥ 5 on a 32-bit computer and \"k\" ≥ 6 on a 64-bit computer); this allows the inputs to be broken up into pieces without bit shifts, and provides a uniform representation for values mod 2 + 1 where the high word can only be zero or one.\n\nNormalization involves adding or subtracting the modulus 2 + 1; this value has only two bits set, which means this can be done in constant time on average with a specialized operation.\n\nIterative FFT algorithms such as the Cooley–Tukey FFT algorithm, although frequently used for FFTs on vectors of complex numbers, tend to exhibit very poor cache locality with the large vector entries used in Schönhage–Strassen. The straightforward recursive, not in-place implementation of FFT is more successful, with all operations fitting in the cache beyond a certain point in the call depth, but still makes suboptimal use of the cache in higher call depths. Gaudry, Kruppa, and Zimmerman used a technique combining Bailey's 4-step algorithm with higher radix transforms that combine multiple recursive steps. They also mix phases, going as far into the algorithm as possible on each element of the vector before moving on to the next one.\n\nThe \"square root of 2 trick\", first described by Schönhage, is to note that, provided \"k\" ≥ 2, 2−2 is a square root of 2 mod 2+1, and so a 4\"n\"-th root of unity (since 2 ≡ 1). This allows the transform length to be extended from 2 to 2.\n\nFinally, the authors are careful to choose the right value of \"k\" for different ranges of input numbers, noting that the optimal value of \"k\" may go back and forth between the same values several times as the input size increases.\n"}
{"id": "6591796", "url": "https://en.wikipedia.org/wiki?curid=6591796", "title": "Support (measure theory)", "text": "Support (measure theory)\n\nIn mathematics, the support (sometimes topological support or spectrum) of a measure \"μ\" on a measurable topological space (\"X\", Borel(\"X\")) is a precise notion of where in the space \"X\" the measure \"lives\". It is defined to be the largest (closed) subset of \"X\" for which every open neighbourhood of every point of the set has positive measure.\n\nA (non-negative) measure \"μ\" on a measurable space (\"X\", Σ) is really a function \"μ\" : Σ → [0, +∞]. Therefore, in terms of the usual definition of support, the support of \"μ\" is a subset of the σ-algebra Σ:\n\nwhere the overbar denotes set closure. However, this definition is somewhat unsatisfactory: we use the notion of closure, but we do not even have a topology on Σ. What we really want to know is where in the space \"X\" the measure \"μ\" is non-zero. Consider two examples:\n\nIn light of these two examples, we can reject the following candidate definitions in favour of the one in the next section:\n\nHowever, the idea of \"local strict positivity\" is not too far from a workable definition:\n\nLet (\"X\", \"T\") be a topological space; let B(\"T\") denote the Borel σ-algebra on \"X\", i.e. the smallest sigma algebra on \"X\" that contains all open sets \"U\" ∈ \"T\". Let \"μ\" be a measure on (\"X\", B(\"T\")). Then the support (or spectrum) of \"μ\" is defined as the set of all points \"x\" in \"X\" for which every open neighbourhood \"N\" of \"x\" has positive measure:\n\nSome authors prefer to take the closure of the above set. However, this is not necessary: see \"Properties\" below.\n\nAn equivalent definition of support is as the largest \"C\" ⊆ \"X\" (with respect to inclusion) such that every open set which has non-empty intersection with \"C\" has positive measure, i.e. the largest C such that:\n\n\nIn the case of Lebesgue measure \"λ\" on the real line R, consider an arbitrary point \"x\" ∈ R. Then any open neighbourhood \"N\" of \"x\" must contain some open interval (\"x\" − \"ε\", \"x\" + \"ε\") for some \"ε\" > 0. This interval has Lebesgue measure 2\"ε\" > 0, so \"λ\"(\"N\") ≥ 2\"ε\" > 0. Since \"x\" ∈ R was arbitrary, supp(\"λ\") = R.\n\nIn the case of Dirac measure \"δ\", let \"x\" ∈ R and consider two cases:\nWe conclude that supp(\"δ\") is the closure of the singleton set {\"p\"}, which is {\"p\"} itself.\n\nIn fact, a measure \"μ\" on the real line is a Dirac measure \"δ\" for some point \"p\" if and only if the support of \"μ\" is the singleton set {\"p\"}. Consequently, Dirac measure on the real line is the unique measure with zero variance [provided that the measure has variance at all].\n\nConsider the measure \"μ\" on the real line R defined by\ni.e. a uniform measure on the open interval (0, 1). A similar argument to the Dirac measure example shows that supp(\"μ\") = [0, 1]. Note that the boundary points 0 and 1 lie in the support: any open set containing 0 (or 1) contains an open interval about 0 (or 1), which must intersect (0, 1), and so must have positive \"μ\"-measure.\n\nThe space of all countable ordinals with the topology generated by \"open intervals\", is a locally compact Hausdorff space. The measure (\"Dieudonné measure\") that assigns measure 1 to Borel sets containing an unbounded closed subset and assigns 0 to other Borel sets is a Borel probability measure whose support is empty.\n\nOn a compact Hausdorff space the support of a non-zero measure is always non-empty, but may have measure 0. An example of this is given by adding the first uncountable ordinal Ω to the previous example: the support of the measure is the single point Ω, which has measure 0.\n\nSuppose that \"μ\" : Σ → [−∞, +∞] is a signed measure. Use the Hahn decomposition theorem to write\n\nwhere \"μ\" are both non-negative measures. Then the support of \"μ\" is defined to be\n\nSimilarly, if \"μ\" : Σ → C is a complex measure, the support of \"μ\" is defined to be the union of the supports of its real and imaginary parts.\n\n"}
{"id": "10995628", "url": "https://en.wikipedia.org/wiki?curid=10995628", "title": "Tempered representation", "text": "Tempered representation\n\nIn mathematics, a tempered representation of a linear semisimple Lie group is a representation that has a basis whose matrix coefficients lie in the L space\n\nfor any ε > 0.\n\nThis condition, as just given, is slightly weaker than the condition that the matrix coefficients are square-integrable, in other words lie in\n\nwhich would be the definition of a discrete series representation. If \"G\" is a linear semisimple Lie group with a maximal compact subgroup \"K\", an admissible representation ρ of \"G\" is tempered if the above condition holds for the \"K\"-finite matrix coefficients of ρ.\n\nThe definition above is also used for more general groups, such as \"p\"-adic Lie groups and finite central extensions of semisimple real algebraic groups. The definition of \"tempered representation\" makes sense for arbitrary unimodular locally compact groups, but on groups with infinite centers such as infinite central extensions of semisimple Lie groups it does not behave well and is usually replaced by a slightly different definition. More precisely, an irreducible representation is called tempered if it is unitary when restricted to the center \"Z\", and the absolute values of the matrix coefficients are in \"L\"(\"G\"/\"Z\").\n\nTempered representations on semisimple Lie groups were first defined and studied by Harish-Chandra (using a different but equivalent definition), who showed that they are exactly the representations needed for the Plancherel theorem. They were classified by Knapp and Zuckerman, and used by Langlands in the Langlands classification of irreducible representations of a reductive Lie group \"G\" in terms of the tempered representations of smaller groups.\n\nIrreducible tempered representations were identified by Harish-Chandra in his work on harmonic analysis on a semisimple Lie group as those representations that contribute to the Plancherel measure. The original definition of a tempered representation, which has certain technical advantages, is that its Harish-Chandra character should be a \"tempered distribution\" (see the section about this below). It follows from Harish-Chandra's results that it is equivalent to the more elementary definition given above. Tempered representations also seem to play a fundamental role in the theory of automorphic forms. This connection was probably first realized by Satake (in the context of the Ramanujan-Petersson conjecture) and Robert Langlands and served as a motivation for Langlands to develop his classification scheme for irreducible admissible representations of real and \"p\"-adic reductive algebraic groups in terms of the tempered representations of smaller groups. The precise conjectures identifying the place of tempered representations in the automorphic spectrum were formulated later by James Arthur and constitute one of the most actively developing parts of the modern theory of automorphic forms.\n\nTempered representations play an important role in the harmonic analysis on semisimple Lie groups. An irreducible unitary representation of a semisimple Lie group \"G\" is tempered if and only if it is in the support of the Plancherel measure of \"G\". In other words, tempered representations are precisely the class of representations of \"G\" appearing in the spectral decomposition of L functions on the group (while discrete series representations have a stronger property that an individual representation has a positive spectral measure). This stands in contrast with the situation for abelian and more general solvable Lie groups, where a different class of representations is needed to fully account for the spectral decomposition. This can be seen already in the simplest example of the additive group R of the real numbers, for which the matrix elements of the irreducible representations do not fall off to 0 at infinity.\n\nIn the Langlands program, tempered representations of real Lie groups are those coming from unitary characters of tori by Langlands functoriality.\n\n\nThe irreducible tempered representations of a semisimple Lie group were classified by . \nIn fact they classified a more general class of representations called basic representations. If \"P=MAN\" is the Langlands decomposition of a cuspidal parabolic subgroup, then a basic representation is defined to be \nthe parabolically induced representation associated to a limit of discrete series representation of \"M\" and a unitary representation of the abelian group \"A\". If the limit of discrete series representation is in fact a discrete series representation, then the basic representation is called an induced discrete series representation. Any irreducible tempered representation is a basic representation, and conversely any basic representation is the sum of a finite number of irreducible tempered representations. More precisely, it is a direct sum of 2 irreducible tempered representations indexed by the characters of an elementary abelian group \"R\" of order 2 (called the R-group). \nAny basic representation, and consequently any irreducible tempered representation, is a summand of an induced discrete series representation. However it is not always possible to represent an irreducible tempered representation as an induced discrete series representation, which is why one considers the more general class of basic representations.\n\nSo the irreducible tempered representations are just the irreducible basic representations, and can be classified by listing all basic representations and picking out those that are irreducible, in other words those that have trivial R-group.\n\nFix a semisimple Lie group \"G\" with maximal compact subgroup \"K\". defined a distribution on \"G\" to be tempered if it is defined on the Schwartz space of \"G\". The Schwartz space is in turn defined to be the space of smooth functions \"f\" on \"G\" such that for any real \"r\" and any function \"g\" obtained from \"f\" by acting on the left or right by elements of the universal enveloping algebra of the Lie algebra of \"G\", the function \nis bounded. \nHere Ξ is a certain spherical function on \"G\", invariant under left and right multiplication by \"K\",\nand σ is the norm of the log of \"p\", where an element \"g\" of \"G\" is written as : \"g\"=\"kp\"\nfor \"k\" in \"K\" and \"p\" in \"P\".\n\n"}
{"id": "44787", "url": "https://en.wikipedia.org/wiki?curid=44787", "title": "Up to", "text": "Up to\n\nIn mathematics, the phrase up to appears in discussions about the elements of a set (say \"S\"), and the conditions under which subsets of those elements may be considered equivalent. The statement \"elements \"a\" and \"b\" of set S are equivalent up to \"X\"\" means that \"a\" and \"b\" are equivalent if criterion \"X\" (such as rotation or permutation) is ignored. That is, \"a\" and \"b\" can be transformed into one another if a transform corresponding to \"X\" (rotation, permutation etc.) is applied.\n\nLooking at the entire set \"S\", when \"X\" is ignored the elements can be arranged in subsets whose elements are equivalent (\"equivalent up to \"X\"\"). Such subsets are called \"equivalence classes\". \n\nIf X is some property or process, the phrase \"up to X\" means \"disregarding a possible difference in X\". For instance the statement \"an integer's prime factorization is unique \"up to ordering\"\", means that the prime factorization is unique if we disregard the order of the factors. We might say \"the solution to an indefinite integral is , \"up to addition by a constant\"\", meaning that the added constant is not the focus here, the solution is, and that the addition of a constant is to be regarded as a background, of secondary focus. Further examples concerning \"up to isomorphism\", \"up to permutations\" and \"up to rotations\" are described below.\n\nIn informal contexts, mathematicians often use the word \"modulo\" (or simply \"mod\") for similar purposes, as in \"modulo isomorphism\".\n\nA simple example is \"there are seven reflecting tetrominos, up to rotations\", which makes reference to the seven possible contiguous arrangements of tetrominoes (collections of four unit squares arranged to connect on at least one side) which are frequently thought of as the seven Tetris pieces (O, I, L, J, T, S, Z.) This could also be written \"there are five tetrominos, up to reflections and rotations\", which would take account of the perspective that L and J could be thought of as the same piece, reflected, as well as that S and Z could be seen as the same. The Tetris game does not allow reflections, so the former notation is likely to seem more natural.\n\nTo add in the exhaustive count, there is no formal notation. However, it is common to write \"there are seven reflecting tetrominos (= 19 total) up to rotations\". In this, Tetris provides an excellent example, as a reader might simply count 7 pieces × 4 rotations as 28, where some pieces (the 2×2 O being the obvious example) have fewer than four rotation states.\n\nIn the eight queens puzzle, if the eight queens are considered to be distinct, there are 3 709 440 distinct solutions. Normally however, the queens are considered to be identical, and one says \"there are 92 ( unique solutions \"up to\" permutations of the queens\", or \"there are 92 solutions \"mod\" the names of the queens\", signifying that two different arrangements of the queens are considered equivalent if the queens have been permuted, but the same squares on the chessboard are occupied by them.\n\nIf, in addition to treating the queens as identical, rotations and reflections of the board were allowed, we would have only 12 distinct solutions \"up to symmetry and the naming of the queens\", signifying that two arrangements that are symmetrical to each other are considered equivalent.\n\nThe regular \"n\"-gon, for given \"n\", is unique up to similarity. In other words, if all similar \"n\"-gons are considered instances of the same \"n\"-gon, then there is only one regular \"n\"-gon.\n\nIn group theory, for example, we may have a group \"G\" acting on a set \"X\", in which case we say that two elements of \"X\" are equivalent \"up to the group action\" if they lie in the same orbit.\n\nAnother typical example is the statement that \"there are two different groups of order 4 \"up to\" isomorphism\", or \"\"modulo\" isomorphism, there are two groups of order 4\". This means that there are two equivalence classes of groups of order 4, assuming we consider groups to be equivalent if they are isomorphic.\n\nA hyperreal \"x\" and its standard part st(\"x\") are equal up to an infinitesimal difference.\n\nIn computer science, the term \"up-to techniques\" is a precisely defined notion that refers to certain proof techniques for (weak) bisimulation, to relate processes that only behave similarly up to unobservable steps.\n\n\n"}
