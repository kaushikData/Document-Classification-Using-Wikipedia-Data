{"id": "45222866", "url": "https://en.wikipedia.org/wiki?curid=45222866", "title": "Behavioral game theory", "text": "Behavioral game theory\n\nBehavioral game theory analyzes interactive strategic decisions and behavior using the methods of game theory, experimental economics, and experimental psychology. Experiments include testing deviations from typical simplifications of economic theory such as the independence axiom and neglect of altruism, fairness, and framing effects. As a research program, the subject is a development of the last three decades. Traditional game theory focuses on mathematical equilibriums, utility maximizing, and rational choice; in contrast, behavioral game theory focuses on choices made by participants in studies and is game theory applied to experiments. Choices studied in behavioral game theory are not always rational and do not always represent the utility maximizing choice.\n\nBehavioral game theory began with the work of Allais in 1953 and Ellsberg in 1961. They discovered the Allais paradox and the Ellsberg paradox, respectively. Both paradoxes show that choices made by participants in a game do not reflect the benefit they expect to receive from making those choices. In the 1970s the work of Vernon Smith showed that economic markets could be examined experimentally rather than only theoretically. At the same time, several economists conducted experiments that discovered variations of traditional decision-making models such as regret theory, prospect theory, and hyperbolic discounting. These discoveries showed that actual decision makers consider many factors when making choices. For example, a person may seek to minimize the amount of regret they will feel after making a decision and weigh their options based on the amount of regret they anticipate from each. Because they were not previously examined by traditional economic theory, factors such as regret along with many others fueled further research.\n\nBeginning in the 1980s experimenters started examining the conditions that cause divergence from rational choice. Ultimatum and bargaining games examined the effect of emotions on predictions of opponent behavior. One of the most well known examples of an ultimatum game is the television show Deal or No Deal in which participants must make decisions to sell or continue playing based on monetary ultimatums given to them by “the banker.” These games also explored the effect of trust on decision-making outcomes and utility maximizing behavior. Common resource games were used to experimentally test how cooperation and social desirability affect subject's choices. A real life example of a common resource game might be a party guest's decision to take from a food platter. The guests decisions would not only be affected by how hungry they are, but they would also be affected by how much of the shared resource, the food, is left and if the guest believes others would judge them for taking more. Experimenters during this period regarded behavior that did not maximize utility as the result of participant's flawed reasoning. By the turn of the century economists and psychologists expanded this research. Models based on the rational choice theory were adapted to reflect decision maker preferences and attempt to rationalize choices that did not maximize utility.\n\nTraditional game theory uses theoretical models to determine the most beneficial choice of all players in a game. Game theory uses rational choice theory along with assumptions of players' common knowledge in order to predict utility-maximizing decisions. It also allows for players to predict their opponents' strategies. Traditional game theory is a primarily normative theory as it seeks to pinpoint the decision rational players should choose, but does not attempt to explain why that decision was made. Rationality is a primary assumption of game theory, so there are not explanations for different forms of rational decisions or irrational decisions.\n\nBehavioral game theory is a primarily positive theory rather than a normative theory. A positive theory seeks to describe phenomena rather than prescribe a correct action. Positive theories must be testable and can be proven true or false. A normative theory is subjective and based on opinions. Because of this, normative theories cannot be proven true or false. Behavioral game theory attempts to explain decision making using experimental data. The theory allows for rational and irrational decisions because both are examined using real-life experiments. Specifically, behavioral game theory attempts to explain factors that influence real world decisions. These factors are not explored in the area of traditional game theory, but can be postulated and observed using empirical data. Findings from behavioral game theory will tend to have higher external validity and can be better applied to real world decision-making behavior.\n\n\nBeliefs about other people in a decision-making game are expected to influence ones ability to make rational choices. However, beliefs of others can also cause experimental results to deviate from equilibrium, utility-maximizing decisions. In an experiment by Costa-Gomez (2008) participants were questioned about their first order beliefs about their opponent's actions prior to completing a series of normal-form games with other participants. Participants complied with Nash Equilibrium only 35% of the time. Further, participants only stated beliefs that their opponents would comply with traditional game theory equilibrium 15% of the time. This means participants believed their opponents would be less rational than they really were. The results of this study show that participants do not choose the utility-maximizing action and they expect their opponents to do the same. Also, the results show that participants did not choose the utility-maximizing action that corresponded to their beliefs about their opponent's action. While participants may have believed their opponent was more likely to make a certain decision, they still made decisions as if their opponent was choosing randomly. Another study that examined participants from the TV show Deal or No Deal found divergence from rational choice. Participants were more likely to base their decisions on previous outcomes when progressing through the game. Risk aversion decreased when participants' expectations were not met within the game. For example a subject that experienced a string of positive outcomes was less likely to accept the deal and end the game. The same was true for a subject that experienced primarily negative outcomes early in the game.\n\nSocial behavior and cooperation with other participants are two factors that are not modeled in traditional game theory, but are often seen in an experimental setting. The evolution of social norms has been neglected in decision-making models, but these norms influence the ways in which real people interact with one another and make choices. One tendency is for a person to be a strong reciprocator. This type of person enters a game with the predisposition to cooperate with other players. They will increase their cooperation levels in response to cooperation from other players and decrease their cooperation levels, even at their own expense, to punish players who do not cooperate. This is not utility-maximizing behavior, as a strong reciprocator is willing to reduce their payoff in order to encourage cooperation from others.\nDufwenberg and Kirchsteiger (2004) developed a model based on reciprocity called the sequential reciprocity equilibrium. This model adapts traditional game theory logic to the idea that players reciprocate actions in order to cooperate. The model had been used to more accurately predict experimental outcomes of classic games such as the prisoner's dilemma and the centipede game. Rabin (1993) also created a fairness equilibrium that measures altruism's effect on choices. He found that when a player is altruistic to another player the second player is more likely to reciprocate that altruism. This is due to the idea of fairness. Fairness equilibriums take the form of mutual maximum, where both players choose an outcome that benefits both of them the most, or mutual minimum, where both players choose an outcome that hurts both of them the most. These equilibriums are also Nash equilibriums, but they incorporate the willingness of participants to cooperate and play fair.\n\nThe role of incentives and consequences in decision-making is interesting to behavioral game theorists because it affects rational behavior. Post (2008) analyzed Deal or no Deal contestant behavior in order to reach conclusions about decision-making when stakes are high. Studying the contestant's choices formed the conclusion that in a sequential game with high stakes decisions were based on previous outcomes rather than rationality. Players who face a succession of good outcomes, in this case they eliminate the low-value cases from play, or players who face a succession of poor outcomes become less risk averse. This means that players who are having exceptionally good or exceptionally bad outcomes are more likely to gamble and continue playing than average players. The lucky or unlucky players were willing to reject offers of over one hundred percent of the expected value of their case in order to continue playing. This shows a shift from risk avoiding behavior to risk seeking behavior. This study highlights behavioral biases that are not accounted for by traditional game theory. Riskier behavior in unlucky contestants can be attributed to the break-even effect, which states that gamblers will continue to make risky decisions in order to win back money. On the other hand, riskier behavior in lucky contestants can be explained by the house-money effect, which states that winning gamblers are more likely to make risky decisions because they perceive that they are not gambling with their own money. This analysis shows that incentives influence rational choice, especially when players make a series of decisions.\n\nIncentives and consequences also play a large role in deception in games. Gneezy (2005) studied deception using a cheap talk sender-receiver game. In this type of game player one receives information about the payouts of option A and option B. Then, player one gives a recommendation to player two about which option to take. Player one can choose to deceive player two, and player two can choose to reject player one's advice. Gneezy found that participants were more sensitive to their gain from lying than to their opponent's loss. He also found that participants were not wholly selfish and cared about how much their opponents lost from their deception, but this effect diminished as their own payout increased. These findings show that decision makers examine both incentives to lie and consequences of lying in order to decide whether or not to lie. In general people are averse to lying, but given the right incentives they tend to ignore consequences. Wang (2009) also used a cheap talk game to study deception in participants with an incentive to deceive. Using eye tracking he found that participants who received information about payoffs focused on their own payoff twice as often as their opponents. This suggests minimal strategic thinking. Further, participants' pupils dilated when they sent a deceiving, and they dilated more when telling a bigger lie. Through these physical cues Wang concluded that deception is cognitively difficult. These findings show that factors such as incentives, consequences, and deception can create irrational decisions and affect the way games unfold.\n\nBehavioral game theory considers the effects of groups on rationality. In the real world many decisions are made by teams, yet traditional game theory uses an individual as a decision maker. This created a need to model group decision-making behavior. Bornstein and Yaniv (1998) examined the difference in rationality between groups and individuals in an ultimatum game. In this game player one (or group one) decides what percentage of a payout to give to player two (or group two) and then player two decides whether to accept or reject this offer. Participants in the group condition were put in groups of three and allowed to deliberate on their decisions. Perfect rationality in this game would be player one offering player two none of the payout, but that is almost never the case in observed offers. Bornstein and Yaniv found that groups were less generous, willing to give up a smaller portion of the payoff, in the player one condition and more accepting of low offers in the player two condition than individuals. These results suggest that groups are more rational than individuals.\n\nKocher and Sutter (2005) used a beauty contest game to study and compare individual and group behavior. A beauty contest game is one where all participants choose a number between zero and one hundred. The winner is the participant who chooses a number closest to two thirds of the average number. In the first round the rational choice would be thirty-three, as it is two thirds of the average number, fifty. Given an infinite number of rounds all participants should choose zero according to game theory. Kocher and Sutter found that groups did not perform more rationally than individuals in the first round of the game. However, groups performed more rationally than individuals in subsequent rounds. This shows that groups are able to learn the game and adapt their strategy faster than individuals.\n\n"}
{"id": "53009580", "url": "https://en.wikipedia.org/wiki?curid=53009580", "title": "Brown–Gitler spectrum", "text": "Brown–Gitler spectrum\n\nIn topology, a discipline within mathematics, the Brown–Gitler spectrum is a spectrum whose cohomology is a certain cyclic module over the Steenrod algebra.\n\nBrown–Gitler spectra are defined by the isomorphism:\n\nThe concept was introduced by mathematicians Edgar H. Brown and Samuel Gitler in a 1973 paper.\n\nIn topology, Brown–Gitler spectrum is related to the concepts of Segal conjecture and Burnside ring.\n\nBrown–Gitler spectra have had many important applications in homotopy theory.\n"}
{"id": "41121490", "url": "https://en.wikipedia.org/wiki?curid=41121490", "title": "Bunch–Nielsen–Sorensen formula", "text": "Bunch–Nielsen–Sorensen formula\n\nIn mathematics, in particular linear algebra, the Bunch–Nielsen–Sorensen formula, named after James R. Bunch, Christopher P. Nielsen and Danny C. Sorensen, expresses the eigenvectors of the sum of a symmetric matrix formula_1 and the outer product, formula_2, of vector formula_3 with itself.\n\nLet formula_4 denote the eigenvalues of formula_1 and formula_6 denote the eigenvalues of the updated matrix formula_7. In the special case when formula_1 is diagonal, the eigenvectors formula_9 of formula_10 can be written\n\nwhere formula_12 is a number that makes the vector formula_9 normalized.\n\nThis formula can be derived from the Sherman–Morrison formula by examining the poles of formula_14.\n\nThe eigenvalues of formula_10 were studied by Golub.\n\nNumerical stability of the computation is studied by Gu and Eisenstadt.\n\n\n"}
{"id": "2522243", "url": "https://en.wikipedia.org/wiki?curid=2522243", "title": "Chow variety", "text": "Chow variety\n\nIn mathematics, and more particularly in the field of algebraic geometry, Chow coordinates are a generalization of Plücker coordinates, applying to \"m\"-dimensional algebraic varieties of degree \"d\" in formula_1, that is, \"n\"-dimensional projective space. They are named for Wei-Liang Chow.\n\nA Chow variety is a variety whose points correspond to all cycles of a given projective space of given dimension and degree.\n\nTo define the Chow coordinates, take the intersection of an algebraic variety \"Z\" of degree \"d\" and dimension \"m\" by linear subspaces \"U\" of codimension \"m\". When \"U\" is in general position, the intersection will be a finite set of \"d\" distinct points.\n\nThen the coordinates of the \"d\" points of intersection are algebraic functions of the Plücker coordinates of U, and by taking a symmetric function of the algebraic functions, a homogeneous polynomial known as the Chow form (or Cayley form) of Z is obtained. \n\nThe Chow coordinates are then the coefficients of the Chow form. Chow coordinates can generate the smallest field of definition of a divisor. The Chow coordinates define a point in the projective space corresponding to all forms.\n\nThe closure of the possible Chow coordinates is called the Chow variety.\n\nThe Hilbert scheme is a variant of the Chow varieties. There is always a map (called the cycle map)\nfrom the Hilbert scheme to the Chow variety.\n\nA Chow quotient parametrizes closures of generic orbits. It is constructed as a closed subvariety of a Chow variety.\n\nKapranov's theorem says that the moduli space formula_3 of stable genus-zero curves with \"n\" marked points is the Chow quotient of Grassmannian formula_4 by the standard maximal torus.\n\n\n"}
{"id": "6320997", "url": "https://en.wikipedia.org/wiki?curid=6320997", "title": "Dense order", "text": "Dense order\n\nIn mathematics, a partial order or total order < on a set \"X\" is said to be dense if, for all \"x\" and \"y\" in \"X\" for which \"x\" < \"y\", there is a \"z\" in \"X\" such that \"x\" < \"z\" < \"y\".\n\nThe rational numbers with the ordinary ordering are a densely ordered set in this sense, as are the real numbers. On the other hand, the ordinary ordering on the integers is not dense.\n\nGeorg Cantor proved that every two densely totally ordered countable sets without lower or upper bounds are order-isomorphic. In particular, there exists an order-isomorphism between the rational numbers and other densely ordered countable sets including the dyadic rationals and the algebraic numbers. The proof of this result uses the back-and-forth method.\n\nMinkowski's question mark function can be used to determine the order isomorphisms between the quadratic algebraic numbers and the rational numbers, and between the rationals and the dyadic rationals.\n\nAny binary relation \"R\" is said to be \"dense\" if, for all \"R\"-related \"x\" and \"y\", there is a \"z\" such that \"x\" and \"z\" and also \"z\" and \"y\" are \"R\"-related. Formally:\n\nSufficient conditions for a binary relation \"R\" in a set \"X\" to be dense are:\nNone if them is necessary.\nA non-empty and dense relation cannot be antitransitive.\n\nA strict partial order < is a dense order iff < is a dense relation. A dense relation that is also transitive is said to be idempotent.\n\n\n"}
{"id": "1077078", "url": "https://en.wikipedia.org/wiki?curid=1077078", "title": "E. W. Hobson", "text": "E. W. Hobson\n\nErnest William Hobson FRS (27 October 1856 – 19 April 1933) was an English mathematician, now remembered mostly for his books, some of which broke new ground in their coverage in English of topics from mathematical analysis. He was Sadleirian Professor of Pure Mathematics at the University of Cambridge from 1910 to 1931.\n\nHe was born in Derby, and was educated at Derby School, the Royal School of Mines, and Christ's College, Cambridge, graduating Senior wrangler in 1878. \nHe was the brother of the economist John A. Hobson.\nHe became a Fellow of Christ's almost immediately after graduation. He made his way into research mathematics only gradually, becoming an expert in the theory of spherical harmonics.\n\nHis 1907 work on real analysis was something of a watershed in the British mathematical tradition; and was lauded by G. H. Hardy. It included material on general topology and Fourier series that was topical at the time; and included mistakes that were picked up later (for example by R. L. Moore).\n\nFrom 1924 to 1927 Robert Pollock Gillespie studied under him.\n\nHe is buried in the Parish of the Ascension Burial Ground in Cambridge, with his wife Seline, born 25 March 1860, died 10 June 1940, by whom he had four sons, one of whom Walter William (1894 - 1930) is buried with them in the same grave.\n\n\n\n"}
{"id": "54985664", "url": "https://en.wikipedia.org/wiki?curid=54985664", "title": "EURO Journal on Transportation and Logistics", "text": "EURO Journal on Transportation and Logistics\n\nEURO Journal on Transportation and Logistics is a peer-reviewed scientific journal that was established in 2012 and is published by Springer Science+Business Media. It is an official journal of the Association of European Operational Research Societies, publishing original mathematical models, methodologies and computational results focusing on advanced applications in transportation and logistics. The editor-in-chief is Michel Bierlaire (EPFL).\n\nThe journal is abstracted and indexed in the following databases:\n"}
{"id": "8117054", "url": "https://en.wikipedia.org/wiki?curid=8117054", "title": "Event symmetry", "text": "Event symmetry\n\nIn physics, event symmetry includes invariance principles that have been used in some discrete approaches to quantum gravity where the diffeomorphism invariance of general relativity can be extended to a covariance under every permutation of spacetime events.\n\nSince general relativity was discovered by Albert Einstein in 1915, observation and experiment have demonstrated that it is an accurate gravitation theory up to cosmic scales. On small scales, the laws of quantum mechanics have likewise been found to describe nature in a way consistent with every experiment performed, so far. To describe the laws of the universe fully a synthesis of general relativity and quantum mechanics must be found. Only then can physicists hope to understand the realms where gravity and quantum come together. The big bang is one such place.\n\nThe task to find such a theory of quantum gravity is one of the major scientific endeavours of our time. Many physicists believe that string theory is the leading candidate, but string theory has so far failed to provide an adequate description of the big bang, and its success is just as incomplete in other ways. That could be because physicists do not really know what the correct underlying principles of string theory are, so they do not have the right formulation that would allow them to answer the important questions. In particular, string theory treats spacetime in quite an old fashioned way even though it indicates that spacetime must be very different at small scales from what we are familiar with.\n\nGeneral relativity by contrast, is a model theory based on a geometric symmetry principle from which its dynamics can be elegantly derived. The symmetry is called general covariance or diffeomorphism invariance. It says that the dynamical equations of the gravitational field and any matter must be unchanged in form under any smooth transformation of spacetime coordinates. To understand what that means you have to think of a region of spacetime as a set of \"events\", each one labelled by unique values of four coordinate values x,y,z, and t. The first three tell us \"where\" in space the event happened, while the fourth is time and tells us \"when\" it happened. But the choice of coordinates that are used is arbitrary, so the laws of physics should not depend on what the choice is. It follows that if any smooth mathematical function is used to map one coordinate system to any other, the equations of dynamics must transform in such a way that they look the same as they did before. This symmetry principle is a strong constraint on the possible range of equations and can be used to derive the laws of gravity almost uniquely.\n\nThe principle of general covariance works on the assumption that spacetime is smooth and continuous. Although this fits in with our normal experience, there are reasons to suspect that it may not be a suitable assumption for quantum gravity. In quantum field theory, continuous fields are replaced with a more complex structure that has a dual particle-wave nature as if they can be both continuous and discrete depending on how you measure them. Research in string theory and several other approaches to quantum gravity suggest that spacetime must also have a dual continuous and discrete nature, but without the power to probe spacetime at sufficient energies it is difficult to measure its properties directly to find out how such a quantised spacetime should work.\n\nThis is where event symmetry comes in. In a discrete spacetime treated as a disordered set of events it is natural to extend the symmetry of general covariance to a discrete event symmetry in which any function mapping the set of events to itself replaces the smooth functions used in general relativity. Such a function is also called a permutation, so the principle of event symmetry states that the equations governing the laws of physics must be unchanged when transformed by any permutation of spacetime events.\n\nIt is not immediately obvious how event symmetry could work. It seems to say that taking one part of space time and swapping it with another part a long distance away is a valid physical operation, and that the laws of physics must be written to support this. Clearly this symmetry can only be correct if it is hidden or broken. To get this in perspective consider what the symmetry of general relativity seems to say. A smooth coordinate transformation or diffeomorphism can stretch and twist spacetime in any way so long as it is not torn. The laws of general relativity are unchanged in form under such a transformation. Yet this does not mean that objects can be stretched or bent without being opposed by a physical force. Likewise, event symmetry does not mean that objects can be torn apart in the way the permutations of spacetime would make us believe. In the case of general relativity the gravitational force acts as a background field that controls the measurement properties of spacetime. In ordinary circumstances the geometry of space is flat and Euclidean and the diffeomorphism invariance of general relativity is hidden thanks to this background field. Only in the extreme proximity of a violent collision of black holes would the flexibility of spacetime become apparent. In a similar way, event symmetry could be hidden by a background field that determines not just the geometry of spacetime, but also its topology.\n\nGeneral relativity is often explained in terms of curved spacetime. We can picture the universe as the curved surface of a membrane like a soap film that changes dynamically in time. The same picture can help us understand how event symmetry would be broken. A soap bubble is made from molecules that interact via forces that depend on the orientations of the molecules and the distance between them. If you wrote down the equations of motion for all the molecules in terms of their positions, velocities and orientations, then those equations would be unchanged in form under any permutation of the molecules (which we will assume are all the same). This is mathematically analogous to the event symmetry of spacetime events. The equations may be different, and unlike the molecules on the surface of a bubble, the events of spacetime are not embedded in a higher-dimensional space, yet the mathematical principle is the same.\n\nPhysicists do not presently know if event symmetry is a correct symmetry of nature, but the example of a soap bubble shows that it is a logical possibility. If it can be used to explain real physical observations then it merits serious consideration.\n\nAmerican philosopher of physics John Stachel has used permutability of spacetime events to generalize Einstein's hole argument. Stachel uses the term quiddity to describe the universal qualities of an entity and haecceity to describe its individuality. He makes use of the analogy with quantum mechanical particles, that have quiddity but no haecceity. The permutation symmetry of systems of particles leaves the equations of motion and the description of the system invariant. This is generalised to a \"principle of maximal permutability\" that should be applied to physical entities. In an approach to quantum gravity where spacetime events are discrete, the principle implies that physics must be symmetric under any permutations of events, so the principle of event symmetry is a special case of the principle of maximal permutability.\n\nStachel's view builds on the work of philosophers such as Gottfried Leibniz whose monadology proposed that the world should be viewed only in terms of relations between objects rather than their absolute positions. Ernst Mach used this to formulate his relational principle, which influenced Einstein in his formulation of general relativity. Some quantum gravity physicists believe that the true theory of quantum gravity will be a relational theory with no spacetime. The events of spacetime are then no longer a background in which physics happens. Instead they are just the set of events where an interaction between entities took place. Characteristics of spacetime that we are familiar with (such as distance, continuity and dimension) should be emergent in such a theory, rather than put in by hand.\n\nIn a random graph model of spacetime, points in space or events in spacetime are represented by nodes of a graph. Each node may be connected to any other node by a link. In mathematical terms this structure is called a graph. The smallest number of links that it takes to go between two nodes of the graph can be interpreted as a measure of the distance between them in space. The dynamics can be represented either by using a Hamiltonian formalism if the nodes are points in space, or a Lagrangian formalism if the nodes are events in spacetime. Either way, the dynamics allow the links to connect or disconnect in a random way according to specified probability rule. The model is event-symmetric if the rules are invariant under any permutation of the graph nodes.\n\nThe mathematical discipline of random graph theory was founded in the 1950s by Paul Erdős and Alfréd Rényi. They proved the existence of sudden changes in characteristics of a random graph as parameters of the model varied. These are similar to phase transitions in physical systems. The subject has been extensively studied since with applications in many areas including computation and biology. A standard text is \"Random Graphs\" by Béla Bollobás.\n\nApplication to quantum gravity came later. Early random graph models of space-time have been proposed by Frank Antonsen (1993), Manfred Requardt (1996) and Thomas Filk (2000). Tomasz Konopka, Fotini Markopoulou-Kalamara, Simone Severini, and Lee Smolin of the Canadian Perimeter Institute for Theoretical Physics introduced a graph model that they called Quantum Graphity. An argument based on quantum graphity combined with the holographic principle can resolve the horizon problem and explain the observed scale invariance of\ncosmic background radiation fluctuations without the need for cosmic inflation.\n\nIn the quantum graphity model, points in spacetime are represented by nodes on a graph connected by links that can be \"on\" or \"off\". This indicates whether or not the two points are directly connected as if they are next to each other in spacetime. When they are \"on\" the links have additional state variables that define the random dynamics of the graph under the influence of quantum fluctuations and temperature. At high temperature the graph is in \"Phase I\" where all the points are randomly connected to each other and no concept of spacetime as we know it exists. As the temperature drops and the graph cools, it is conjectured to undergo a phase transition to a \"Phase II\" where spacetime forms. It will then look like a spacetime manifold on large scales with only near-neighbour points being connected in the graph. The hypothesis of quantum graphity is that this \"geometrogenesis\" models the condensation of spacetime in the big bang.\n\nString theory is formulated on a background spacetime just as quantum field theory is. Such a background fixes spacetime curvature, which in general relativity is like saying that the gravitational field is fixed. However, analysis shows that the excitations of the string fields act as gravitons, which can perturb the gravitational field away from the fixed background. So, string theory actually includes dynamic quantised gravity. More detailed studies have shown that different string theories in different background spacetimes can be related by dualities. There is also good evidence that string theory supports changes in topology of spacetime. Relativists have therefore criticised string theory for not being formulated in a background independent way, so that changes of spacetime geometry and topology can be more directly expressed in terms of the fundamental degrees of freedom of the strings.\n\nThe difficulty in achieving a truly background independent formulation for string theory is demonstrated by a problem known as Witten's Puzzle. Ed Witten asked the question \"What could the full symmetry group of string theory be if it includes diffeomorphism invariance on a spacetime with changing topology?\". This is hard to answer because the diffeomorphism group for each spacetime topology is different and there is no natural way to form a larger group containing them all such that the action of the group on continuous spacetime events makes sense. This puzzle is solved if the spacetime is regarded as a discrete set of events with different topologies formed dynamically as different string field configurations. Then the full symmetry need only contain the permutation group of spacetime events. Since any diffeomorphism for any topology is a special kind of permutation on the discrete events, the permutation group does contain all the different diffeomorphism groups for all possible topologies.\n\nThere is some evidence from Matrix Models that event-symmetry is included in string theory. A random matrix model can be formed from a random graph model by taking the variables on the links of the graph and arranging them in a N by N square matrix, where N is the number of nodes on the graph. The element of the matrix in the n column and m row gives the variable on the link joining the n nodes to the m node. The event-symmetry can then be extended to a larger N dimensional rotational symmetry.\n\nIn string theory, random matrix models were introduced to provide a non-perturbative formulation of M-Theory using noncommutative geometry. Coordinates of spacetime are normally commutative but in noncommutative geometry they are replaced by matrix operators that do not commute. In the original M(atrix) Theory these matrices were interpreted as connections between instantons (also known as D0-branes), and the matrix rotations were a gauge symmetry. Later, Iso and Kawai reinterpreted this as a permutation symmetry of space-time events and argued that diffeomorphism invariance was included in this symmetry. The two interpretations are equivalent if no distinction is made between instantons and events, which is what would be expected in a relational theory. This shows that Event Symmetry can already be regarded as part of string theory.\n\nThe first known publication of the idea of event symmetry is in a work of science fiction rather than a journal of science. Greg Egan used the idea in a short story called \"Dust\" in 1992 and expanded it into the novel Permutation City in 1994. Egan used dust theory as a way of exploring the question of whether a perfect computer simulation of a person differs from the real thing. However, his description of the dust theory as an extension of general relativity is also a consistent statement of the principle of event symmetry as used in quantum gravity.\n\nThe essence of the argument can be found in chapter 12 of \"Permutation City\". Paul, the main character of the story set in the future, has created a copy of himself in a computer simulator. The simulation runs on a distributed network sufficiently powerful to emulate his thoughts and experiences. Paul argues that the events of his simulated world have been remapped to events in the real world by the computer in a way that resembles a coordinate transformation in relativity. General relativity only allows for covariance under continuous transformations whereas the computer network has formed a discontinuous mapping that permutes events like \"a cosmic anagram\". Yet Paul's copy in the simulator experiences physics as if it were unchanged. Paul realises that this is \"Like […] gravity and acceleration in General Relativity — it all depends on what you can't tell apart. This is a new Principle of Equivalence, a new symmetry between observers.\"\n"}
{"id": "31702086", "url": "https://en.wikipedia.org/wiki?curid=31702086", "title": "Exponential random graph models", "text": "Exponential random graph models\n\nExponential random graph models (ERGMs) are a family of statistical models for analyzing data about social and other networks.\n\nMany metrics exist to describe the structural features of an observed network such as the density, centrality, or assortativity. However, these metrics describe the observed network which is only one instance of a large number of possible alternative networks. This set of alternative networks may have similar or dissimilar structural features. To support statistical inference on the processes influencing the formation of network structure, a statistical model should consider the set of all possible alternative networks weighted on their similarity to an observed network. However because network data is inherently relational, it violates the assumptions of independence and identical distribution of standard statistical models like linear regression. Alternative statistical models should reflect the uncertainty associated with a given observation, permit inference about the relative frequency about network substructures of theoretical interest, disambiguating the influence of confounding processes, efficiently representing complex structures, and linking local-level processes to global-level properties. Degree-preserving randomization, for example, is a specific way in which an observed network could be considered in terms of multiple alternative networks.\n\nThe Exponential family is a broad family of models for covering many types of data, not just networks. An ERGM is a model from this family which describes networks.\n\nFormally a random graph formula_1 consists of a set of formula_2 nodes and formula_3 dyads (edges) formula_4 where formula_5 if the nodes formula_6 are connected and formula_7 otherwise.\n\nThe basic assumption of these models is that the structure in an observed graph formula_8 can be explained by any statistics formula_9 depending on the observed network and nodal attributes. This way, it is possible to describe any kind of dependence between the undyadic variables:\n\nformula_10\n\nwhere formula_11 is a vector of model parameters associated with formula_9 and formula_13 is a normalising constant.\n\nThese models represent a probability distribution on each possible network on formula_2 nodes. However, the size of the set of possible networks for an undirected network (simple graph) of size formula_2 is formula_16. Because the number of possible networks in the set vastly outnumbers the number of parameters which can constrain the model, the ideal probability distribution is the one which maximizes the Gibbs entropy.\n\n"}
{"id": "6212640", "url": "https://en.wikipedia.org/wiki?curid=6212640", "title": "Faugère's F4 and F5 algorithms", "text": "Faugère's F4 and F5 algorithms\n\nIn computer algebra, the Faugère F4 algorithm, by Jean-Charles Faugère, computes the Gröbner basis of an ideal of a multivariate polynomial ring. The algorithm uses the same mathematical principles as the Buchberger algorithm, but computes many normal forms in one go by forming a generally sparse matrix and using fast linear algebra to do the reductions in parallel.\n\nThe Faugère F5 algorithm first calculates the Gröbner basis of a pair of generator polynomials of the ideal. Then it uses this basis to reduce the size of the initial matrices of generators for the next larger basis: \n\nIf \"G\" is an already computed Gröbner basis (\"f\", …, \"f\") and we want to compute a Gröbner basis of (\"f\") + \"G\" then we will construct matrices whose rows are \"m\" \"f\" such that \"m\" is a monomial not divisible by the leading term of an element of \"G\".\n\nThis strategy allows the algorithm to apply two new criteria based on what Faugère calls \"signatures\" of polynomials. Thanks to these criteria, the algorithm can compute Gröbner bases for a large class of interesting polynomial systems, called \"regular sequences\", without ever simplifying a single polynomial to zero—the most time-consuming operation in algorithms that compute Gröbner bases. It is also very effective for a large number of non-regular sequences.\n\nThe Faugère F4 algorithm is implemented \n\nStudy versions of the Faugère F5 algorithm is implemented in\n\nThe previously intractable \"cyclic 10\" problem was solved by F5, as were a number of systems related to cryptography; for example HFE and C.\n\n\n"}
{"id": "3832917", "url": "https://en.wikipedia.org/wiki?curid=3832917", "title": "Formal derivative", "text": "Formal derivative\n\nIn mathematics, the formal derivative is an operation on elements of a polynomial ring or a ring of formal power series that mimics the form of the derivative from calculus. Though they appear similar, the algebraic advantage of a formal derivative is that it does not rely on the notion of a limit, which is in general impossible to define for a ring. Many of the properties of the derivative are true of the formal derivative, but some, especially those that make numerical statements, are not.\n\nFormal differentiation is used in algebra to test for multiple roots of a polynomial.\n\nThe definition of formal derivative is as follows: fix a ring \"R\" (not necessarily commutative) and let \"A\" = \"R\"[\"x\"] be the ring of polynomials over \"R\". Then the formal derivative is an operation on elements of \"A\", where if\n\nthen its formal derivative is\n\njust as for polynomials over the real or complex numbers. Here formula_3 does not mean multiplication in the ring, but rather formula_4 where formula_5 is never used inside the sum.\n\nThere is a problem with this definition for noncommutative rings. The formula itself is correct, but there is no standard form of a polynomial. Therefore using this definition it is difficult to prove formula_6\n\nLet for formula_7 holds formula_8 let formula_9\nLet us define derivative for expressions, such that formula_10 and formula_11\n\nWe must prove that this definition gives the same result for an expression independent on the method the expression was evaluated, therefore that it is compatible with the axioms of equality.\nand the distributivity from the other side from symmetry.\n\nLinearity naturally follows from the definition.\n\nFormula for derivative of a polynomial (in standard shape for commutative rings) is direct consequence of the definition:\nformula_19\n\nIt can be verified that:\n\n\n\nThese two properties make \"D\" a derivation on \"A\" (see also module of relative differential forms for a discussion of a generalization).\n\nAs in calculus, the derivative detects multiple roots: if \"R\" is a field then \"R\"[\"x\"] is a Euclidean domain, and in this situation we can define multiplicity of roots; namely, for every polynomial \"f\"(\"x\") and every element \"r\" of \"R\", there exists a nonnegative integer \"m\" and a polynomial \"g\"(\"x\") such that\n\nwhere \"g\"(\"r\") is not equal to \"0\". \"m\" is the multiplicity of \"r\" as a root of \"f\". It follows from the Leibniz rule that in this situation, \"m\" is also the number of differentiations that must be performed on \"f\"(\"x\") before \"r\" is not a root of the resulting polynomial. The utility of this observation is that although in general not every polynomial of degree \"n\" in \"R\"[\"x\"] has \"n\" roots counting multiplicity (this is the maximum, by the above theorem), we may pass to field extensions in which this is true (namely, algebraic closures). Once we do, we may uncover a multiple root that was not a root at all simply over \"R\". For example, if \"R\" is the field with three elements, the polynomial\n\nhas no roots in \"R\"; however, its formal derivative is zero since 3 = 0 in \"R\" and in any extension of \"R\", so when we pass to the algebraic closure it has a multiple root that could not have been detected by factorization in \"R\" itself. Thus, formal differentiation allows an effective notion of multiplicity. This is important in Galois theory, where the distinction is made between separable field extensions (defined by polynomials with no multiple roots) and inseparable ones.\n\nWhen the ring \"R\" of scalars is commutative, there is an alternative and equivalent definition of the formal derivative, which resembles the one seen in differential calculus. The element Y–X of the ring \"R\"[X,Y] divides Y – X for any nonnegative integer \"n\", and therefore divides \"f\"(Y) – \"f\"(X) for any polynomial \"f\" in one indeterminate. If the quotient (in \"R\"[X,Y]) is denoted by \"g\":\n\nthen it is not hard to verify that \"g\"(X,X) (in \"R\"[X]) coincides with the formal derivative of \"f\" as it was defined above.\n\nThis formulation of the derivative works equally well for a formal power series, assuming only that the ring of scalars is commutative.\n\nActually, if the division in this definition is carried out in the class of functions of formula_25 continuous at formula_26, it will recapture the classical definition of the derivative. If it is carried out in the class of functions continuous in both formula_26 and formula_25, we get uniform differentiability, and our function formula_29 will be continuously differentiable. Likewise, by choosing different classes of functions (say, the Lipschitz class), we get different flavors of differentiability. This way differentiation becomes a part of algebra of functions.\n\n\n"}
{"id": "50215912", "url": "https://en.wikipedia.org/wiki?curid=50215912", "title": "Gianfranco Cimmino", "text": "Gianfranco Cimmino\n\nGianfranco Cimmino (12 March 1908 – 30 May 1989) was an Italian mathematician, working mathematical analysis, numerical analysis, and theory of elliptic partial differential equations: he is known for being the first mathematician generalizing in a weak sense the notion of boundary value in a boundary value problem, and for doing an influential work in numerical analysis.\n\n\n\n\n\n\n"}
{"id": "20591151", "url": "https://en.wikipedia.org/wiki?curid=20591151", "title": "Hierarchical decision process", "text": "Hierarchical decision process\n\nThe hierarchical decision process (HDP) refines the classical analytic hierarchy process (AHP) a step further in eliciting and evaluating subjective judgements. These improvements, proposed initially by Dr. Jang Ra (a student of Dr. Thomas L. Saaty who developed and refined AHP) include the constant-sum measurement scale (1–99 scale) for comparing two elements, the logarithmic least squares method (LLSM) for computing normalized values, the sum of inverse column sums (SICS) for measuring the degree of (in)consistency, and sensitivity analysis of pairwise comparisons matrices. These subtle modifications address issues concerning normal AHP consistency and applicability in the process of constructing hierarchies: generating criteria, classifying/selecting criteria, and screening/selecting decision alternatives.\n"}
{"id": "713946", "url": "https://en.wikipedia.org/wiki?curid=713946", "title": "Hyperplane at infinity", "text": "Hyperplane at infinity\n\nIn geometry, any hyperplane \"H\" of a projective space \"P\" may be taken as a hyperplane at infinity. Then the set complement is called an affine space. For instance, if are homogeneous coordinates for \"n\"-dimensional projective space, then the equation defines a hyperplane at infinity for the \"n\"-dimensional affine space with coordinates . \"H\" is also called the ideal hyperplane.\n\nSimilarly, starting from an affine space \"A\", every class of parallel lines can be associated with a point at infinity. The union over all classes of parallels constitute the points of the hyperplane at infinity. Adjoining the points of this hyperplane (called ideal points) to \"A\" converts it into an \"n\"-dimensional projective space, such as the real projective space .\n\nBy adding these ideal points, the entire affine space \"A\" is completed to a projective space \"P\", which may be called the projective completion of \"A\". Each affine subspace \"S\" of \"A\" is completed to a projective subspace of \"P\" by adding to \"S\" all the ideal points corresponding to the directions of the lines contained in \"S\". The resulting projective subspaces are often called \"affine subspaces\" of the projective space \"P\", as opposed to the infinite or ideal subspaces, which are the subspaces of the hyperplane at infinity (however, they are projective spaces, not affine spaces).\n\nIn the projective space, each projective subspace of dimension \"k\" intersects the ideal hyperplane in a projective subspace \"at infinity\" whose dimension is .\n\nA pair of non-parallel affine hyperplanes intersect at an affine subspace of dimension , but a parallel pair of affine hyperplanes intersect at a projective subspace of the ideal hyperplane (the intersection \"lies on\" the ideal hyperplane). Thus, parallel hyperplanes, which did not meet in the affine space, intersect in the projective completion due to the addition of the hyperplane at infinity.\n\n\n"}
{"id": "39351494", "url": "https://en.wikipedia.org/wiki?curid=39351494", "title": "In silico medicine", "text": "In silico medicine\n\n\"In silico\" medicine (also known as \"computational medicine\") is the application of \"in silico\" research to problems involving health and medicine. It is the direct use of computer simulation in the diagnosis, treatment, or prevention of a disease. More specifically, \"in silico\" medicine is characterized by modeling, simulation, and visualization of biological and medical processes in computers with the goal of simulating real biological processes in a virtual environment.\n\nThe term \"in silico\" was first used in 1989 at a workshop \"Cellular Automata: Theory and Applications\" by a mathematician from National Autonomous University of Mexico (UNAM). The term \"in silico radiation oncology\", a precursor of generic \"in silico\" medicine was coined and first introduced by G. Stamatakos in Proceedings of the IEEE in 2002. The same researcher coined and introduced the more generic term \"in silico oncology\". \"In silico\" medicine is considered an extension of previous work using mathematical models of biological systems. It became apparent that the techniques used to model biological systems has utility to explain and predict dynamics in the medical field. The first fields in medicine to use \"in silico\" modeling were genetics, physiology and biochemistry. The field saw a dramatic influx of data when the human genome was sequenced in the 1980s and 1990s. Concurrently the increase in available computational power allowed for modeling of complex systems that were previously impractical.\n\nThere are numerous reasons why \"in silico\" medicine is used. For example, \"in silico\" medical modeling can allow for early prediction of success of a compound for a medicinal purpose and elucidate potential adverse effects early in the drug discovery process. \"In silico\" modeling can also provide a humane alternative to animal testing. It has been purported by a company in the field, that computer-aided models will make the use of testing on living organisms obsolete.\n\nThe term \"in silico\" medicine is exemplified in initiatives such as the Virtual Physiological Human by the European Commission and in institutes such as the VPH Institute and the INSIGNEO Institute at the University of Sheffield.\n\nThe In Silico Oncology Group (ISOG) at the Institute of Communication and Computer Systems, National Technical Institute of Athens (ICCS-NTUA) aims at developing clinically driven and oriented multiscale simulation models of malignant tumors (cancer) to be utilized as patient individualized decision support and treatment planning systems following completion of clinical adaptation and validation. An additional aim of the Group's research is to simulate oncological clinical trials which would otherwise be too costly or time intensive and to this direction, grid computing infrastructures have been exploited, such as the European Grid Infrastructure, to increase the performance and effectiveness of the simulations. ISOG has led the development of the first technologically integrated \"Oncosimulator\", a joint Euro-Japanese research venture.\n\nIn 2003, the first vaccine based solely off of genomic information was developed. The technique of developing the vaccine, termed \"reverse vaccinology,\" used the genomic information and not the infectious bacteria itself to develop the vaccine.\n\nAs modeling of human, social, behavioral, and cultural (HSBC) characteristics of patient behavior becomes more sophisticated, there is speculation that virtual patients may replace patient actors in medical school curriculum. Additionally, there are projects underway that utilize virtual cadavers, computer simulated models of human anatomy based on CT images of real people.\n\n"}
{"id": "618086", "url": "https://en.wikipedia.org/wiki?curid=618086", "title": "Interpretability logic", "text": "Interpretability logic\n\nInterpretability logics comprise a family of modal logics that extend provability logic to describe interpretability or various related metamathematical properties and relations such as weak interpretability, Π-conservativity, cointerpretability, tolerance, cotolerance, and arithmetic complexities. \n\nMain contributors to the field are Alessandro Berarducci, Petr Hájek, Konstantin Ignatiev, Giorgi Japaridze, Franco Montagna, Vladimir Shavrukov, Rineke Verbrugge, Albert Visser, and Domenico Zambella.\n\n\nAxiom schemata: \n\n1. All classical tautologies\n\n2. formula_10\n\n3. formula_11\n\n4. formula_12\n\n5. formula_13\n\n6. formula_14\n\n7. formula_15\n\n8. formula_16\n\n9. formula_17\n\nRules inference: \n\n1. “From formula_6 and formula_19 conclude formula_20” \n\n2. “From formula_6 conclude formula_5”.\n\nThe completeness of ILM with respect to its arithmetical interpretation was independently proven by Alessandro Berarducci and Vladimir Shavrukov. \nAxioms (with formula_26 standing for any formulas, formula_27 for any sequences of formulas, and formula_28 identified with ⊤): \n\n1. All classical tautologies\n\n2. formula_29\n\n3. formula_30\n\n4. formula_31\n\n5. formula_32\n\n6. formula_33\n\n7. formula_34\n\nRules inference: \n\n1. “From formula_6 and formula_19 conclude formula_20” \n\n2. “From formula_38 conclude formula_39”.\n\nThe completeness of TOL with respect to its arithmetical interpretation was proven by Giorgi Japaridze. \n\n"}
{"id": "10452186", "url": "https://en.wikipedia.org/wiki?curid=10452186", "title": "Inverse problem for Lagrangian mechanics", "text": "Inverse problem for Lagrangian mechanics\n\nIn mathematics, the inverse problem for Lagrangian mechanics is the problem of determining whether a given system of ordinary differential equations can arise as the Euler–Lagrange equations for some Lagrangian function.\n\nThere has been a great deal of activity in the study of this problem since the early 20th century. A notable advance in this field was a 1941 paper by the American mathematician Jesse Douglas, in which he provided necessary and sufficient conditions for the problem to have a solution; these conditions are now known as the Helmholtz conditions, after the German physicist Hermann von Helmholtz.\n\nThe usual set-up of Lagrangian mechanics on \"n\"-dimensional Euclidean space R is as follows. Consider a differentiable path \"u\" : [0, \"T\"] → R. The action of the path \"u\", denoted \"S\"(\"u\"), is given by\nwhere \"L\" is a function of time, position and velocity known as the Lagrangian. The principle of least action states that, given an initial state \"x\" and a final state \"x\" in R, the trajectory that the system determined by \"L\" will actually follow must be a minimizer of the action functional \"S\" satisfying the boundary conditions \"u\"(0) = \"x\", \"u\"(T) = \"x\". Furthermore, the critical points (and hence minimizers) of \"S\" must satisfy the Euler–Lagrange equations for \"S\":\n\nwhere the upper indices \"i\" denote the components of \"u\" = (\"u\", ..., \"u\").\n\nIn the classical case\nthe Euler–Lagrange equations are the second-order ordinary differential equations better known as Newton's laws of motion:\n\nThe inverse problem of Lagrangian mechanics is as follows: given a system of second-order ordinary differential equations\nthat holds for times 0 ≤ \"t\" ≤ \"T\", does there exist a Lagrangian \"L\" : [0, \"T\"] × R × R → R for which these ordinary differential equations (E) are the Euler–Lagrange equations? In general, this problem is posed not on Euclidean space R, but on an \"n\"-dimensional manifold \"M\", and the Lagrangian is a function \"L\" : [0, \"T\"] × T\"M\" → R, where T\"M\" denotes the tangent bundle of \"M\".\n\nTo simplify the notation, let\n\nand define a collection of \"n\" functions Φ by\n\nTheorem. (Douglas 1941) There exists a Lagrangian \"L\" : [0, \"T\"] × T\"M\" → R such that the equations (E) are its Euler–Lagrange equations if and only if there exists a non-singular symmetric matrix \"g\" with entries \"g\" depending on both \"u\" and \"v\" satisfying the following three Helmholtz conditions:\n\nAt first glance, solving the Helmholtz equations (H1)–(H3) seems to be an extremely difficult task. Condition (H1) is the easiest to solve: it is always possible to find a \"g\" that satisfies (H1), and it alone will not imply that the Lagrangian is singular. Equation (H2) is a system of ordinary differential equations: the usual theorems on the existence and uniqueness of solutions to ordinary differential equations imply that it is, \"in principle\", possible to solve (H2). Integration does not yield additional constants but instead first integrals of the system (E), so this step becomes difficult \"in practice\" unless (E) has enough explicit first integrals. In certain well-behaved cases (e.g. the geodesic flow for the canonical connection on a Lie group), this condition is satisfied.\n\nThe final and most difficult step is to solve equation (H3), called the \"closure conditions\" since (H3) is the condition that the differential 1-form \"g\" is a closed form for each \"i\". The reason why this is so daunting is that (H3) constitutes a large system of coupled partial differential equations: for \"n\" degrees of freedom, (H3) constitutes a system of\n\npartial differential equations in the 2\"n\" independent variables that are the components \"g\" of \"g\", where\n\ndenotes the binomial coefficient. In order to construct the most general possible Lagrangian, one must solve this huge system!\n\nFortunately, there are some auxiliary conditions that can be imposed in order to help in solving the Helmholtz conditions. First, (H1) is a purely algebraic condition on the unknown matrix \"g\". Auxiliary algebraic conditions on \"g\" can be given as follows: define functions\n\nby\n\nThe auxiliary condition on \"g\" is then\n\nIn fact, the equations (H2) and (A) are just the first in an infinite hierarchy of similar algebraic conditions. In the case of a parallel connection (such as the canonical connection on a Lie group), the higher order conditions are always satisfied, so only (H2) and (A) are of interest. Note that (A) comprises\n\nconditions whereas (H1) comprises\n\nconditions. Thus, it is possible that (H1) and (A) together imply that the Lagrangian function is singular. As of 2006, there is no general theorem to circumvent this difficulty in arbitrary dimension, although certain special cases have been resolved.\n\nA second avenue of attack is to see whether the system (E) admits a submersion onto a lower-dimensional system and to try to \"lift\" a Lagrangian for the lower-dimensional system up to the higher-dimensional one. This is not really an attempt to solve the Helmholtz conditions so much as it is an attempt to construct a Lagrangian and then show that its Euler–Lagrange equations are indeed the system (E).\n\n"}
{"id": "29080987", "url": "https://en.wikipedia.org/wiki?curid=29080987", "title": "Julio Rey Pastor", "text": "Julio Rey Pastor\n\nJulio Rey Pastor (14 August 1888 – 21 February 1962) was a Spanish mathematician and historian of science.\n\nJulio Rey Pastor studied high school in his hometown, and began his studies in Sciences in Vitoria. He moved to the University of Saragossa, where he found a stimulating environment in mathematics. Zoel García de Galdeano, Professor of Analytical Geometry and Calculus, was the professor who most influenced Rey Pastor’s scientific work. He graduated with honors in 1908. Rey Pastor earned his doctorate from Complutense University of Madrid in 1909, under supervision of Eduardo Torroja Caballé. Between 1911 and 1914, he studied at the University of Berlin and the University of Göttingen, under the supervision of Felix Klein. During that period, he also studied under the supervision of Professors Hermann Schwarz, Friedrich Hermann Schottky (father of Walter Schottky, Nobel Prize in Physics in 1911), and Ferdinand Georg Frobenius.\n\nHis report sent to the Junta para Ampliación de Estudios (JAE) allows us to assess the significance of his studies in Germany. He especially liked Schwarz’s lectures on analytic functions and synthetic geometry, not only because of their innovations but also because Schwarz’s teaching method. In this report, Rey proposed the creation of a \"seminar in mathematics to arouse the research spirit of our school children.\" His proposal was accepted and in 1915 the JAE created the Mathematics Laboratory and Seminar, an important institution for the development of research on this field in Spain.\n\nUndoubtedly, the creation of the laboratory was a result of Rey Pastor’s studies in Germany, and it was intended to overcome the isolation and individualism of the Spanish mathematicians. This laboratory, under the National Institute of Sciences, was first installed in the basement of the National Library, then moved to a modest apartment on Santa Teresa St., then to the building of the Center for Historical Studies and, finally, became part of the Consejo Superior de Investigaciones Científicas (CSIC), renamed Instituto Jorge Juan de Matemáticas in 1939.\n\nRey Pastor’s scientific work involved research, textbooks, and articles for the general public. They reflected the changes that were taking place in mathematics. He was also interested in the history of science and, specifically, mathematics in Spain.\n\nIn 1951, he was appointed director of the Instituto Jorge Juan de Matemáticas in the CSIC. His plans in Spain included two projects: the creation, within the CSIC, of an Institute of Applied Mathematics, and the foundation of a Seminar on the History of Science at the university.\n\nIn 1954, he entered the Royal Spanish Academy, proposed by Gregorio Marañón, and Francisco Javier Sánchez Cantón, and delivered an acceptance speech on the algebra of language. (He had become a member of the Academy of Sciences in Madrid in 1920, and of the Academy of Sciences in Buenos Aires in 1932).\n\nRey Pastor showed his passion for mathematics as a researcher, as promoter of new studies, and as creator of agencies and institutions that enhanced the development of mathematics in Spain. A lunar crater Faraday G was called \"Reypastor\" by Hugh Percy Wilkins and Antonio Paluzie-Borrell, mappers of Earth's Moon, but the designation was not adopted by the International Astronomical Union.\n\nIn 1956, he went back to Argentina and only returned to Spain on the occasion of the entry of his disciple, Sixto Ríos into the Academy of Sciences, on June 21, 1961. In his speech, Rey Pastor recalled the process of creation of the laboratory and the support from the JAE.\n\nRey Pastor occupied a seat in the Real Academia Española between 1953 and 1962. He was honored with a Spanish stamp in 2000.\n\n\n\n\n\n"}
{"id": "1497098", "url": "https://en.wikipedia.org/wiki?curid=1497098", "title": "Kaprekar's routine", "text": "Kaprekar's routine\n\nKaprekar's routine is an algorithm in recreational mathematics devised by the Indian mathematician D. R. Kaprekar which produces a sequence of numbers which either converges to a constant value or results in a repeating cycle. The algorithm is as follows:\n\n\nFor example, if we start with 3524 then we get:\n\nA sequence like this is sometimes called a Kaprekar sequence and the function = is the Kaprekar mapping.\n\nSome numbers map to themselves: for instance if = 0, then 0 − 0 = 0; if = 495, then 954 − 459 = 495; if = 6174, then 7641 − 1467 = 6174. These are the fixed points of the Kaprekar mapping. If the sequence reaches one of these numbers, it will repeat indefinitely.\n\nAll Kaprekar sequences will either reach one of these fixed points or will result in a repeating cycle. Either way, the end result is reached in a fairly small number of steps.\n\nNote that the numbers and have the same digit sum and hence the same remainder modulo 9 (if we are using base 10). Therefore, each number in a Kaprekar sequence of base 10 numbers (other than possibly the first) is a multiple of 9. In the general case of base , the numbers in the sequences will be multiples of .\n\nIn 1949 D. R. Kaprekar discovered that if the above process is applied to base 10 numbers of 4 digits, the resulting sequence will almost always converge to the value 6174 in at most 8 iterations, except for a small set of initial numbers which converge instead to 0. The number 6174 is sometimes known as Kaprekar's constant.\n\nThe set of numbers that converge to zero depends on whether leading zeros are discarded (the usual formulation) or are retained (as in Kaprekar's original formulation).\n\nIn the usual formulation, there are 77 four-digit numbers that converge to zero, for example 2111. However, in Kaprekar's original formulation the leading zeros are retained, and only repdigits such as 1111 or 2222 map to zero. This contrast is illustrated below:\n\nBelow is a flowchart. Leading zeros are retained, however the only difference when leading zeros are discarded is that instead of 0999 connecting to 8991, we get 999 connecting to 0.\n\nIf the Kaprekar routine is applied to numbers of 3 digits in base 10, the resulting sequence will almost always converge to the value 495 in at most 6 iterations, except for a small set of initial numbers which converge instead to 0.\n\nThe set of numbers that converge to zero depends on whether leading zeros are discarded (the usual formulation) or are retained (as in Kaprekar's original formulation). In the usual formulation, there are 60 three-digit numbers that converge to zero, for example 211. However, in Kaprekar's original formulation the leading zeros are retained, and only repdigits such as 111 or 222 map to zero.\n\nBelow is a flowchart. Leading zeros are retained, however the only difference when leading zeros are discarded is that instead of 099 connecting to 891, we get 99 connecting to 0.\n\nFor digit lengths other than three or four (in base 10), the routine may terminate at one of several constant values or may enter one of several cycles instead, depending on the starting value of the sequence.\n\nThe following table is applicable when leading zeros are retained. If leading zeros are discarded, the only difference is that the 09→81→63→27→45 cycle for digit length 2 does not occur, because then 54 − 45 = 9 (instead of 09), and 9 − 9 = 0, so all 2-digit sequences will instead end in 0. When leading zeros are retained, only repdigits will result in 0.\n\nIt can easily be shown that all numbers of the form 6174, 631764, 63317664, 6...333...17...666...4 (where the length of the \"3\" sequence and the length of the \"6\" sequence are the same) are fixed points of the Kaprekar mapping in base 10. Other such \"families\" also exist, for instance 495, 549945, 554999445, 5(n)499(n)4(n)5, where d(n) means n repetitions of the digit \"d\".\n\nThe number of cycles increases rapidly with larger digit lengths, and all but a small handful of these cycles are of length three. For example, for 20-digit numbers in base 10, there are fourteen constants (cycles of length one) and ninety-six cycles of length greater than one, all but two of which are of length three. Odd digit lengths produce fewer different end results than even digit lengths.\n\nKaprekar sequences can also be calculated in bases other than 10. For instance, the table below is applicable to hexadecimal, and leading zeros are retained. If leading zeros are discarded, the only difference is that the 0f→e1→c3→87 cycle for digit length 2 does not occur, because then 87 − 78 = f (instead of 0f), and f − f = 0, so all 2-digit sequences will instead end in 0. When leading zeros are retained, only repdigits will result in 0.\n\nNote that each number in a Kaprekar sequence in base 16 is a multiple of F (decimal 15 = 16−1).\n\n\n"}
{"id": "258733", "url": "https://en.wikipedia.org/wiki?curid=258733", "title": "Knuth's up-arrow notation", "text": "Knuth's up-arrow notation\n\nIn mathematics, Knuth's up-arrow notation is a method of notation for very large integers, introduced by Donald Knuth in 1976. It is closely related to the Ackermann function and especially to the hyperoperation sequence. The idea is based on the fact that multiplication can be viewed as iterated addition and exponentiation as iterated multiplication. Continuing in this manner leads to tetration (iterated exponentiation) and to the remainder of the hyperoperation sequence, which is commonly denoted using Knuth arrow notation. This notation allows for a simple description of numbers far larger than can be explicitly written out.\n\nA single arrow means exponentiation (iterated multiplication); more than one arrow means iterating the operation associated with one fewer arrow.\n\nFor example:\n\nThe general definition of the notation (by recursion) is as follows (for integer \"a\" and non-negative integers \"b\" and \"n\"):\n\nHere, ↑ stands for \"n\" arrows, so for example\n\nThe ordinary arithmetical operations of addition, multiplication, and exponentiation are naturally extended into a sequence of hyperoperations as follows.\n\nAddition by a natural number is defined as iterated counting:\nMultiplication by a natural number is defined as iterated addition:\n\nFor example,\n\nExponentiation for a natural power formula_9 is defined as iterated multiplication, which Knuth denoted by a single up-arrow:\n\nFor example,\n\nTo extend the sequence of operations beyond exponentiation, Knuth defined a “double arrow” operator to denote iterated exponentiation (tetration):\n\nFor example,\n\nHere and below evaluation is to take place from right to left, as Knuth's arrow operators (just like exponentiation) are defined to be right-associative.\n\nAccording to this definition,\n\nThis already leads to some fairly large numbers, but Knuth extended the notation. He went on to define a “triple arrow” operator for iterated tetration (pentation):\n\nfollowed by a “quadruple arrow“ operator for iterated pentation (hexation):\n\nand so on. The general rule is that an formula_20-arrow operator expands into a right-associative series of (formula_21)-arrow operators. Symbolically,\n\nExamples:\n\nThe notation formula_25 is commonly used to denote formula_26 with \"n\" arrows. In fact, formula_25 is \"a\" [\"n\"+2] \"b\" with hyperoperation. For example, formula_28 can also be written as 39 [4] 14 (the \"[4]\" means tetration), but it is not equal to 39 [2] 14 = 39 × 14 = 546; similarly, formula_29 is equal to 77 [79] 77, instead of 77 [77] 77.\n\nIn expressions such as formula_30, the notation for exponentiation is usually to write the exponent formula_9 as a superscript to the base number formula_32. But many environments — such as programming languages and plain-text e-mail — do not support superscript typesetting. People have adopted the linear notation formula_33 for such environments; the up-arrow suggests 'raising to the power of'. If the character set does not contain an up arrow, the caret (^) is used instead.\n\nThe superscript notation formula_30 doesn't lend itself well to generalization, which explains why Knuth chose to work from the inline notation formula_33 instead.\n\nformula_25 is a shorter alternative notation for n uparrows. Thus formula_37.\n\nAttempting to write formula_38 using the familiar superscript notation gives a power tower.\n\nIf \"b\" is a variable (or is too large), the power tower might be written using dots and a note indicating the height of the tower.\n\nContinuing with this notation, formula_41 could be written with a stack of such power towers, each describing the size of the one above it.\n\nAgain, if \"b\" is a variable or is too large, the stack might be written using dots and a note indicating its height.\n\nFurthermore, formula_44 might be written using several columns of such stacks of power towers, each column describing the number of power towers in the stack to its left:\n\nAnd more generally:\n\nThis might be carried out indefinitely to represent formula_25 as iterated exponentiation of iterated exponentiation for any \"a\", \"n\" and \"b\" (although it clearly becomes rather cumbersome).\n\nThe tetration notation formula_48 allows us to make these diagrams slightly simpler while still employing a geometric representation (we could call these \"tetration towers\").\n\nFinally, as an example, the fourth Ackermann number formula_52 could be represented as:\n\nSome numbers are so large that multiple arrows of Knuth's up-arrow notation become too cumbersome; then an \"n\"-arrow operator formula_54 is useful (and also for descriptions with a variable number of arrows), or equivalently, hyper operators.\n\nSome numbers are so large that even that notation is not sufficient. The Conway chained arrow notation can then be used: a chain of three elements is equivalent with the other notations, but a chain of four or more is even more powerful.\n\nIt is generally suggested that Knuth's arrow should be used for smaller magnitude numbers, and the chained arrow or hyper operators for larger ones.\n\nThe up-arrow notation is formally defined by\n\nfor all integers formula_57 with formula_58.\n\nThis definition takes multiplication as the basic operation formula_59, then yields exponentiation formula_60 as repeated multiplication, tetration formula_61 as repeated exponentiation, etc. (This is equivalent to the hyperoperation sequence except it omits the two more-basic operations of successor and addition, inclusion of which requires additional starting values that somewhat complicate the definition.)\n\nAll up-arrow operators (including normal exponentiation, formula_33) are defined to be right associative, i.e. evaluated from right to left in an expression. \nformula_63 —— not formula_64.\nformula_65 is formula_66 —— not formula_67\n\nNote that due to right-associativity we have, for formula_68,\n\nformula_69\n\nwhere each formula_32 appears as a left operand of the arrow operator (which is significant because the arrow operators are not commutative), and we have written formula_71 for the \"b\"th functional power of the function formula_72. Because formula_73, the original definition can therefore be written more concisely as follows:\nfor all integers formula_57 with formula_58.\n\nComputing formula_77 can be restated in terms of an infinite table. We place the numbers formula_78 in the top row, and fill the left column with values 2. To determine a number in the table, take the number immediately to the left, then look up the required number in the previous row, at the position given by the number just taken.\n\nThe table is the same as that of the Ackermann function, except for a shift in formula_79 and formula_20, and an addition of 3 to all values.\n\nWe place the numbers formula_81 in the top row, and fill the left column with values 3. To determine a number in the table, take the number immediately to the left, then look up the required number in the previous row, at the position given by the number just taken.\n\nWe place the numbers formula_82 in the top row, and fill the left column with values 4. To determine a number in the table, take the number immediately to the left, then look up the required number in the previous row, at the position given by the number just taken.\n\nWe place the numbers formula_83 in the top row, and fill the left column with values 10. To determine a number in the table, take the number immediately to the left, then look up the required number in the previous row, at the position given by the number just taken.\n\nNote that for 2 ≤ \"n\" ≤ 9 the numerical order of the numbers formula_84 is the lexicographical order with \"m\" as the most significant number, so for the numbers of these 8 columns the numerical order is simply line-by-line. The same applies for the numbers in the 97 columns with 3 ≤ \"n\" ≤ 99, and if we start from \"m\" = 1 even for 3 ≤ \"n\" ≤ 9,999,999,999.\n\nR. L. Goodstein, with a system of notation different from Knuth arrows, used the sequence of hyperoperators here denoted by formula_85 to create systems of numeration for the nonnegative integers. Letting square brackets ([1], [2], [3], [4], ... ) denote the respective hyperoperators formula_85, the so-called \"complete hereditary representation\" of integer \"n\", at level \"k\" and base \"b\", can be expressed as follows using only the first \"k\" hyperoperators and using as digits only 0, 1, ..., \"b\" − 1, together with the base \"b\" itself:\n\n\nThe remainder of this section will use the superscripts to denote the hyperoperators.\nUnnecessary parentheses can be avoided by giving higher-level operators higher precedence in the order of evaluation; thus,\n\nlevel-1 representations have the form b [1] X, with \"X\" also of this form;\n\nlevel-2 representations have the form b [2] X [1] Y, with \"X\",\"Y\" also of this form;\n\nlevel-3 representations have the form b [3] X [2] Y [1] Z, with \"X\",\"Y\",\"Z\" also of this form;\n\nlevel-4 representations have the form b [4] X [3] Y [2] Z [1] W, with \"X\",\"Y\",\"Z\",\"W\" also of this form;\n\nand so on. \n\nNote that in this type of base-\"b\" \"hereditary\" representation, the base itself appears in the expressions, as well as \"digits\" from the set {0, 1, ..., \"b\"-1}. This compares to \"ordinary\" base-2 representation when the latter is written out in terms of the base \"b\"; e.g., in ordinary base-2 notation, 6 = (110) = 2 [3] 2 [2] 1 [1] 2 [3] 1 [2] 1 [1] 2 [3] 0 [2] 0, whereas the level-3 base-2 hereditary representation is 6 = 2 [3] (2 [3] 1 [2] 1 [1] 0) [2] 1 [1] (2 [3] 1 [2] 1 [1] 0). The hereditary representations can be abbreviated by omitting any instances of [1] 0, [2] 1, [3] 1, [4] 1, etc.; for example, the above level-3 base-2 representation of 6 abbreviates to 2 [3] 2 [1] 2. \n\nExamples:\nThe unique base-2 representations of the number 266, at levels 1, 2, 3, 4, and 5 are as follows:\n\n\n"}
{"id": "24410049", "url": "https://en.wikipedia.org/wiki?curid=24410049", "title": "Laplace–Carson transform", "text": "Laplace–Carson transform\n\nIn mathematics, the Laplace–Carson transform, named after Pierre Simon Laplace and John Renshaw Carson, is an integral transform with significant applications in the field of physics and engineering, particularly in the field of railway engineering.\n\nLet formula_1 be a function and formula_2 a complex variable. The Laplace–Carson transform is defined as:\n\nThe inverse Laplace–Carson transform is:\n\nwhere formula_5 is a real-valued constant, formula_6 refers to the imaginary axis, which indicates the integral is carried out along a straight line parallel to the imaginary axis lying to the right of all the singularities of the following expression:\n\n"}
{"id": "18974136", "url": "https://en.wikipedia.org/wiki?curid=18974136", "title": "Mathematical beauty", "text": "Mathematical beauty\n\nMathematical beauty describes the notion that some mathematicians may derive aesthetic pleasure from their work, and from mathematics in general. They express this pleasure by describing mathematics (or, at least, some aspect of mathematics) as \"beautiful\". Mathematicians describe mathematics as an art form or, at a minimum, as a creative activity. Comparisons are often made with music and poetry.\n\nBertrand Russell expressed his sense of mathematical beauty in these words:\n\nMathematics, rightly viewed, possesses not only truth, but supreme beauty—a beauty cold and austere, like that of sculpture, without appeal to any part of our weaker nature, without the gorgeous trappings of painting or music, yet sublimely pure, and capable of a stern perfection such as only the greatest art can show. The true spirit of delight, the exaltation, the sense of being more than Man, which is the touchstone of the highest excellence, is to be found in mathematics as surely as poetry.\nPaul Erdős expressed his views on the ineffability of mathematics when he said, \"Why are numbers beautiful? It's like asking why is Beethoven's Ninth Symphony beautiful. If you don't see why, someone can't tell you. I \"know\" numbers are beautiful. If they aren't beautiful, nothing is\".\n\nMathematicians describe an especially pleasing method of proof as \"elegant\". Depending on context, this may mean:\n\n\nIn the search for an elegant proof, mathematicians often look for different independent ways to prove a result—the first proof that is found may not be the best. The theorem for which the greatest number of different proofs have been discovered is possibly the Pythagorean theorem, with hundreds of proofs having been published. Another theorem that has been proved in many different ways is the theorem of quadratic reciprocity—Carl Friedrich Gauss alone published eight different proofs of this theorem.\n\nConversely, results that are logically correct but involve laborious calculations, over-elaborate methods, very conventional approaches, or that rely on a large number of particularly powerful axioms or previous results are not usually considered to be elegant, and may be called \"ugly\" or \"clumsy\".\n\nSome mathematicians see beauty in mathematical results that establish connections between two areas of mathematics that at first sight appear to be unrelated. These results are often described as \"deep\".\n\nWhile it is difficult to find universal agreement on whether a result is deep, some examples are often cited. One is Euler's identity:\n\nformula_1\n\nThis is a special case of Euler's formula, which the physicist Richard Feynman called \"our jewel\" and \"the most remarkable formula in mathematics\". Modern examples include the modularity theorem, which establishes an important connection between elliptic curves and modular forms (work on which led to the awarding of the Wolf Prize to Andrew Wiles and Robert Langlands), and \"monstrous moonshine\", which connects the Monster group to modular functions via string theory for which Richard Borcherds was awarded the Fields Medal.\n\nOther examples of deep results include unexpected insights into mathematical structures. For example, Gauss's Theorema Egregium is a deep theorem which relates a local phenomenon (curvature) to a global phenomenon (area) in a surprising way. In particular, the area of a triangle on a curved surface is proportional to the excess of the triangle and the proportionality is curvature. Another example is the fundamental theorem of calculus (and its vector versions including Green's theorem and Stokes' theorem).\n\nThe opposite of \"deep\" is \"trivial\". A trivial theorem may be a result that can be derived in an obvious and straightforward way from other known results, or which applies only to a specific set of particular objects such as the empty set. Sometimes, however, a statement of a theorem can be original enough to be considered deep, even though its proof is fairly obvious.\n\nIn his \"A Mathematician's Apology\", Hardy suggests that a beautiful proof or result possesses \"inevitability\", \"unexpectedness\", and \"economy\".\n\nRota, however, disagrees with unexpectedness as a sufficient condition for beauty and proposes a counterexample:\n\nPerhaps ironically, Monastyrsky writes:\n\nThis disagreement illustrates both the subjective nature of mathematical beauty and its connection with mathematical results: in this case, not only the existence of exotic spheres, but also a particular realization of them.\n\nInterest in pure mathematics separate from empirical study has been part of the experience of various civilizations, including that of the ancient Greeks, who \"did mathematics for the beauty of it\". The aesthetic pleasure that mathematical physicists tend to experience in Einstein's theory of general relativity has been attributed (by Paul Dirac, among others) to its \"great mathematical beauty\". The beauty of mathematics is experienced when the physical reality of objects are represented by mathematical models. Group theory, developed in the early 1800s for the sole purpose of solving polynomial equations, became a fruitful way of categorizing elementary particles—the building blocks of matter. Similarly, the study of knots provides important insights into string theory and loop quantum gravity.\n\nSome believe that in order to appreciate mathematics, one must engage in doing mathematics.\nThere are some teachers that encourage student engagement by teaching mathematics in a kinesthetic way (see kinesthetic learning). For example, Math Circle is an afterschool enrichment program where students do mathematics through games and activities; in a general Math Circle lesson, students use pattern finding, observation, and exploration to make their own mathematical discoveries. For example, mathematical beauty arises in a Math Circle activity on symmetry designed for 2nd and 3rd graders. In this activity, students create their own snowflakes by folding a square piece of paper and cutting out designs of their choice along the edges of the folded paper. When the paper is unfolded, a symmetrical design reveals itself. In a day to day elementary school mathematics class, symmetry can be presented as such in an artistic manner where students see aesthetically pleasing results in mathematics.\n\nSome teachers prefer to use mathematical manipulatives to present mathematics in an aesthetically pleasing way. Examples of a manipulative include algebra tiles, cuisenaire rods, and pattern blocks. For example, one can teach the method of completing the square by using algebra tiles. Cuisenaire rods can be used to teach fractions, and pattern blocks can be used to teach geometry. Using mathematical manipulatives helps students gain a conceptual understanding that might not be seen immediately in written mathematical formulas. \n\nAnother example involves origami. Origami, the art of paper folding, has aesthetic qualities and many mathematical connections. One can study the mathematics of paper folding by observing the crease pattern on unfolded origami pieces.\n\nCombinatorics (the study of counting) has artistic representations that some find mathematically beautiful. There are many visual examples that illustrate combinatorial concepts. Here are some topics and objects seen in combinatorics courses with visual representations:\n\nSome mathematicians are of the opinion that the doing of mathematics is closer to discovery than invention, for example:\n\nThese mathematicians believe that the detailed and precise results of mathematics may be reasonably taken to be true without any dependence on the universe in which we live. For example, they would argue that the theory of the natural numbers is fundamentally valid, in a way that does not require any specific context. Some mathematicians have extrapolated this viewpoint that mathematical beauty is truth further, in some cases becoming mysticism.\n\nPythagorean mathematicians believed in the literal reality of numbers. The discovery of the existence of irrational numbers was a shock to them, since they considered the existence of numbers not expressible as the ratio of two natural numbers to be a flaw in nature (the Pythagorean world view did not contemplate the limits of infinite sequences of ratios of natural numbers—the modern notion of a real number). From a modern perspective, their mystical approach to numbers may be viewed as numerology.\n\nIn Plato's philosophy there were two worlds, the physical one in which we live and another abstract world which contained unchanging truth, including mathematics. He believed that the physical world was a mere reflection of the more perfect abstract world.\n\nHungarian mathematician Paul Erdős spoke of an imaginary book, in which God has written down all the most beautiful mathematical proofs. When Erdős wanted to express particular appreciation of a proof, he would exclaim \"This one's from The Book!\"\n\nTwentieth-century French philosopher Alain Badiou claims that ontology is mathematics. Badiou also believes in deep connections between mathematics, poetry and philosophy.\n\nIn some cases, natural philosophers and other scientists who have made extensive use of mathematics have made leaps of inference between beauty and physical truth in ways that turned out to be erroneous. For example, at one stage in his life, Johannes Kepler believed that the proportions of the orbits of the then-known planets in the Solar System have been arranged by God to correspond to a concentric arrangement of the five Platonic solids, each orbit lying on the circumsphere of one polyhedron and the insphere of another. As there are exactly five Platonic solids, Kepler's hypothesis could only accommodate six planetary orbits and was disproved by the subsequent discovery of Uranus.\n\nIn the 1970s, Abraham Moles and Frieder Nake analyzed links between beauty, information processing, and information theory. In the 1990s, Jürgen Schmidhuber formulated a mathematical theory of observer-dependent subjective beauty based on algorithmic information theory: the most beautiful objects among subjectively comparable objects have short algorithmic descriptions (i.e., Kolmogorov complexity) relative to what the observer already knows. Schmidhuber explicitly distinguishes between beautiful and interesting. The latter corresponds to the first derivative of subjectively perceived beauty:\nthe observer continually tries to improve the predictability and compressibility of the observations by discovering regularities such as repetitions and symmetries and fractal self-similarity. Whenever the observer's learning process (possibly a predictive artificial neural network) leads to improved data compression such that the observation sequence can be described by fewer bits than before, the temporary interestingness of the data corresponds to the compression progress, and is proportional to the observer's internal curiosity reward.\n\nExamples of the use of mathematics in music include the stochastic music of Iannis Xenakis, Fibonacci in Tool's Lateralus, counterpoint of Johann Sebastian Bach, polyrhythmic structures (as in Igor Stravinsky's \"The Rite of Spring\"), the Metric modulation of Elliott Carter, permutation theory in serialism beginning with Arnold Schoenberg, and application of Shepard tones in Karlheinz Stockhausen's \"Hymnen\".\n\nExamples of the use of mathematics in the visual arts include applications of chaos theory and fractal geometry to computer-generated art, symmetry studies of Leonardo da Vinci, projective geometries in development of the perspective theory of Renaissance art, grids in Op art, optical geometry in the camera obscura of Giambattista della Porta, and multiple perspective in analytic cubism and futurism.\n\nThe Dutch graphic designer M. C. Escher created mathematically inspired woodcuts, lithographs, and mezzotints. These feature impossible constructions, explorations of infinity, architecture, visual paradoxes and tessellations. British constructionist artist John Ernest created reliefs and paintings inspired by group theory. A number of other British artists of the constructionist and systems schools also draw on mathematics models and structures as a source of inspiration, including Anthony Hill and Peter Lowe. Computer-generated art is based on mathematical algorithms.\n\n\n"}
{"id": "290931", "url": "https://en.wikipedia.org/wiki?curid=290931", "title": "Maurice René Fréchet", "text": "Maurice René Fréchet\n\nMaurice Fréchet (; 2 September 1878 – 4 June 1973) was a French mathematician. He made major contributions to the topology of point sets and introduced the entire concept of metric spaces. He also made several important contributions to the field of statistics and probability, as well as calculus. His dissertation opened the entire field of functionals on metric spaces and introduced the notion of compactness. Independently of Riesz, he discovered the representation theorem in the space of Lebesgue square integrable functions.\n\nHe was born to a Protestant family in Maligny, Yonne to Jacques and Zoé Fréchet. At the time of his birth, his father was a director of a Protestant orphanage in Maligny and was later in his youth appointed a head of a Protestant school. However, the newly established Third Republic was not sympathetic to religious education and so the laws were enacted requiring all education to be secular. As a result, his father lost his job. To generate some income his mother set up a boarding house for foreigners in Paris. His father was able later to obtain another teaching position within the secular system – it was not a job of a headship, however, and the family could not expect as high standards as they might have otherwise.\n\nMaurice attended the secondary school Lycée Buffon in Paris where he was taught mathematics by Jacques Hadamard. Hadamard recognised the potential of young Maurice and decided to tutor him on an individual basis. After Hadamard moved to the University of Bordeaux in 1894, Hadamard continuously wrote to Fréchet, setting him mathematical problems and harshly criticising his errors. Much later Fréchet admitted that the problems caused him to live in a continual fear of not being able to solve some of them, even though he was very grateful for the special relationship with Hadamard he was privileged to enjoy.\n\nAfter completing high-school Fréchet was required to enroll in the military service. This is the time when he was deciding whether to study mathematics or physics – he chose mathematics out of dislike of chemistry classes he would have had to take otherwise. Thus in 1900 he enrolled to École Normale Supérieure to study mathematics.\n\nHe started publishing quite early, having published four papers in 1903. He also published some of his early papers in the American Mathematical Society due to his contact with American mathematicians in Paris—particularly Edwin Wilson.\n\nFréchet served at many different institutions during his academic career. From 1907–1908 he served as a professor of mathematics at the Lycée in Besançon, then moved in 1908 to the Lycée in Nantes to stay there for a year. After that he served at the University of Poitiers between 1910–1919.\n\nHe married in 1908 to Suzanne Carrive (1881-1945) and had four children: Hélène, Henri, Denise and Alain.\n\nFréchet was planning to spend a year in the United States at the University of Illinois but his plan was disrupted when the First World War broke out in 1914. He was mobilised on 4 August the same year. Because of his diverse language skills, gained when his mother ran the establishment for foreigners, he served as an interpreter for the British Army. However, this was not a safe job; he spent two and a half years very near to or at the front. French egalitarian ideals caused many academics to be mobilised. They served in the trenches and many of them were lost during the war. It is remarkable that during his service in the war, he still managed to produce cutting edge mathematical papers frequently, despite having little time to devote to mathematics.\n\nAfter the end of the war, Fréchet was chosen to go to Strasbourg to help with the reestablishment of the university. He served as a professor of higher analysis and Director of the Mathematics Institute. Despite being burdened with administrative work, he was again able to produce a large amount of high quality research.\n\nIn 1928 Fréchet decided to move back to Paris, thanks to encouragement from Borel, who was then Chair in the Calculus of Probabilities and Mathematical Physics at the Sorbonne. Fréchet briefly held a position of lecturer at the Sorbonne's Rockefeller Foundation and from 1928 was a Professor (without a Chair). Fréchet was promoted to tenured Chair of General Mathematics in 1933 and to Chair of Differential and Integral Calculus in 1935. In 1941 Fréchet succeeded Borel as Chair in the Calculus of Probabilities and Mathematical Physics, a position Fréchet held until he retired in 1949. From 1928 to 1935 Fréchet was also put in charge of lectures at the École Normale Supérieure; in this latter capacity Fréchet was able to direct a significant number of young mathematicians toward research in probability, including Doeblin, Fortet, Loeve, and Ville.\n\nDespite his major achievements, Fréchet was not overly appreciated in France. As an illustration, while being nominated numerous times, he was not elected a member of the Academy of Sciences until the age of 78.. In 1929 he became foreign member of the Polish Academy of Science and Arts and in 1950 foreign member of the Royal Netherlands Academy of Arts and Sciences.\n\nFréchet was an Esperantist, publishing some papers and articles in that constructed language. He also served as president of the \"Internacia Scienca Asocio Esperantista\" (\"International Scientific Esperantist Association\") from 1950–53.\n\nHis first major work was his outstanding 1906 PhD thesis \"Sur quelques points du calcul fonctionnel\", on the calculus of functionals. Here Fréchet introduced the concept of a metric space, although the name is due to Hausdorff. Fréchet's level of abstraction is similar to that in group theory, proving theorems within a carefully chosen axiomatic system which can then be applied to a large array of particular cases. \n\nHere is a list of his most important works, in chronological order:\n\nFréchet also developed ideas from the article \"Deux types fondamentaux de distribution statistique\" (1938; an English translation \"The Two Fundamental Types of Statistical Distribution\") of Czech geographer, demographer and statistician Jaromír Korčák.\n\nFréchet is sometimes credited with the introduction of what is now known as the Cramér–Rao bound, but Fréchet's 1940s lecture notes on the topic appear to have been lost.\n\nIn 1908 he married Suzanne Carrive.\n\n\n\n "}
{"id": "48064477", "url": "https://en.wikipedia.org/wiki?curid=48064477", "title": "Maxima of a point set", "text": "Maxima of a point set\n\nIn computational geometry, a point in a finite set of points is said to be \"maximal\" or \"non-dominated\" if there is no other point in whose coordinates are all greater than or equal to the corresponding coordinates of . The maxima of a point set are all the maximal points of .\nThe problem of finding all maximal points, sometimes called the problem of the maxima or maxima set problem, has been studied as a variant of the convex hull and orthogonal convex hull problems. It is equivalent to finding the Pareto frontier of a collection of points, and was called the floating-currency problem by Herbert Freeman based on an application involving comparing the relative wealth of individuals with different holdings of multiple currencies.\n\nFor points in two dimensions, this problem can be solved in time by an algorithm that performs the following steps:\n\nIf the coordinates of the points are assumed to be integers, this can be sped up using integer sorting algorithms, to have the same asymptotic running time as the sorting algorithms.\n\nFor points in three dimensions, it is again possible to find the maximal points in time using an algorithm similar to the two-dimensional one that performs the following steps:\nThis method reduces the problem of computing the maximal points of a static three-dimensional point set to one of maintaining the maximal points of a dynamic two-dimensional point set.\nThe two-dimensional subproblem can be solved efficiently by using a balanced binary search tree to maintain the set of maxima of a dynamic point set.\nUsing this data structure, it is possible to test whether a new point is dominated by the existing points, to find and remove the previously-undominated points that are dominated by a new point, and to add a new point to the set of maximal points, in logarithmic time per point. The number of search tree operations is linear over the course of the algorithm, so the total time is .\n\nFor points with integer coordinates the first part of the algorithm, sorting the points, can again be sped up by integer sorting. If the points are sorted separately by all three of their dimensions, the range of values of their coordinates can be reduced to the range from to without changing the relative order of any two coordinates and without changing the identities of the maximal points. After this reduction in the coordinate space, the problem of maintaining a dynamic two-dimensional set of maximal points may be solved by using a van Emde Boas tree in place of the balanced binary search tree. These changes to the algorithm speed up its running time to .\n\nIn any dimension the problem can be solved in time by testing all pairs of points for whether one dominates another, and reporting as output the points that are not dominated. However, when is a constant greater than three, this can be improved to . For point sets that are generated randomly, it is possible to solve the problem in linear time.\n"}
{"id": "49244", "url": "https://en.wikipedia.org/wiki?curid=49244", "title": "NaN", "text": "NaN\n\nIn computing, NaN, standing for not a number, is a numeric data type value representing an undefined or unrepresentable value, especially in floating-point calculations. Systematic use of NaNs was introduced by the IEEE 754 floating-point standard in 1985, along with the representation of other non-finite quantities like infinities.\n\nTwo separate kinds of NaNs are provided, termed \"quiet NaNs\" and \"signaling NaNs\". Quiet NaNs are used to propagate errors resulting from invalid operations or values, whereas signaling NaNs can support advanced features such as mixing numerical and symbolic computation or other extensions to basic floating-point arithmetic. For example, 0/0 is undefined as a real number, and so represented by NaN; the square root of a negative number is imaginary, and thus not representable as a real floating-point number, and so is represented by NaN; and NaNs may be used to represent missing values in computations.\n\nIn floating-point calculations, NaN is not the same as infinity, although both are typically handled as special cases in floating-point representations of real numbers as well as in floating-point operations. An invalid operation is also not the same as an arithmetic overflow (which might return an infinity) or an arithmetic underflow (which would return the smallest normal number, a denormal number, or zero).\n\nIEEE 754 NaNs are represented with the exponent field filled with ones (like infinity values), and some non-zero number in the significand (to make them distinct from infinity values); this representation allows the definition of multiple distinct NaN values, depending on which bits are set in the significand, but also on the value of the leading sign bit (but applications are not required to provide distinct semantics for those distinct NaN values).\n\nFor example, a bit-wise IEEE floating-point standard single precision (32-bit) NaN would be: s111 1111 1xxx xxxx xxxx xxxx xxxx xxxx where \"s\" is the sign (most often ignored in applications) and the \"x\" sequence represents a non-zero number (the value zero encodes infinities). The first bit from \"x\" is used to determine the type of NaN: \"quiet NaN\" or \"signaling NaN\". The remaining bits encode a \"payload\" (most often ignored in applications).\n\nFloating-point operations other than ordered comparisons normally propagate a quiet NaN (\"qNaN\"). Most floating-point operations on a signaling NaN (\"sNaN\") signal the invalid operation exception, the default exception action is then the same as for qNaN operands and they produce a qNaN if producing a floating-point result.\n\nThe propagation of quiet NaNs through arithmetic operations allows errors to be detected at the end of a sequence of operations without extensive testing during intermediate stages. However, note that depending on the language and the function, NaNs can silently be removed in expressions that would give a constant result for all other floating-point values e.g. NaN^0, which may be defined as 1, so in general a later test for a set \"invalid\" flag is needed to detect all cases where NaNs are introduced (see Function definition below for further details).\n\nIn section 6.2 of the current IEEE 754-2008 standard there are two anomalous functions (the \"maxnum\" and \"minnum\" functions that return the maximum of two operands that are expected to be numbers) that favor numbers — if just one of the operands is a NaN then the value of the other operand is returned. For the next revision of the IEEE 754 standard, it is planned to replace these functions as they are not associative (when a signaling NaN appears in an operand).\n\nA comparison with a NaN always returns an \"unordered result\" even when comparing with itself. The comparison predicates are either signaling or non-signaling on quiet NaN operands; the signaling versions signal the invalid operation exception for such comparisons. The equality and inequality predicates are non-signaling so \"x\" = \"x\" returning false can be used to test if \"x\" is a quiet NaN. The other standard comparison predicates are all signaling if they receive a NaN operand, the standard also provides non-signaling versions of these other predicates. The predicate \"isNaN(x)\" determines if a value is a NaN and never signals an exception, even if \"x\" is a signaling NaN.\n\nThere are three kinds of operations that can return NaN:\n\n\nNaNs may also be explicitly assigned to variables, typically as a representation for missing values. Prior to the IEEE standard, programmers often used a special value (such as −99999999) to represent undefined or missing values, but there was no guarantee that they would be handled consistently or correctly.\n\nNaNs are not necessarily generated in all the above cases. If an operation can produce an exception condition and traps are not masked then the operation will cause a trap instead. If an operand is a quiet NaN, and there isn't also a signaling NaN operand, then there is no exception condition and the result is a quiet NaN. Explicit assignments will not cause an exception even for signaling NaNs.\n\nQuiet NaNs, or qNaNs, do not raise any additional exceptions as they propagate through most operations. The exceptions are where the NaN cannot simply be passed through unchanged to the output, such as in format conversions or certain comparison operations (which do not \"expect\" a NaN input).\n\nSignaling NaNs, or sNaNs, are special forms of a NaN that when consumed by most operations should raise the invalid operation exception and then, if appropriate, be \"quieted\" into a qNaN that may then propagate. They were introduced in IEEE 754. There have been several ideas for how these might be used:\n\nWhen encountered, a trap handler could decode the sNaN and return an index to the computed result. In practice, this approach is faced with many complications. The treatment of the sign bit of NaNs for some simple operations (such as absolute value) is different from that for arithmetic operations. Traps are not required by the standard. There are other approaches to this sort of problem that would be more portable.\n\nThere are differences of opinion about the proper definition for the result of a numeric function that receives a quiet NaN as input. One view is that the NaN should propagate to the output of the function in all cases to propagate the indication of an error. Another view, and the one taken by the ISO C99 and IEEE 754-2008 standards in general, is that if the function has multiple arguments and the output is uniquely determined by all the non-NaN inputs (including infinity), then that value should be the result. Thus for example the value returned by hypot(±∞,qNaN) and hypot(qNaN,±∞) is +∞.\n\nThe problem is particularly acute for the exponentiation function pow(x,y) = x. The expressions 0, ∞ and 1 are considered indeterminate forms when they occur as limits (just like ∞ × 0), and the question of whether zero to the zero power should be defined as 1 has divided opinion.\n\nIf the output is considered as undefined when a parameter is undefined, then pow(1,qNaN) should produce a qNaN. However, math libraries have typically returned 1 for pow(1,y) for any real number y, and even when y is an infinity. Similarly, they produce 1 for pow(x,0) even when x is 0 or an infinity. The rationale for returning the value 1 for the indeterminate forms was that the value of functions at singular points can be taken as a particular value if that value is in the limit the value for all but a vanishingly small part of a ball around the limit value of the parameters. The 2008 version of the IEEE 754 standard says that pow(1,qNaN) and pow(qNaN,0) should both return 1 since they return 1 whatever else is used instead of quiet NaN. Moreover, ISO C99, and later IEEE 754-2008, chose to specify pow(−1,±∞) = 1 instead of qNaN; the reason of this choice is given in the C rationale: \"Generally, C99 eschews a NaN result where a numerical value is useful. [...] The result of pow(−2,∞) is +∞, because all large positive floating-point values are even integers.\"\n\nTo satisfy those wishing a more strict interpretation of how the power function should act, the 2008 standard defines two additional power functions: pown(x,n), where the exponent must be an integer, and powr(x,y), which returns a NaN whenever a parameter is a NaN or the exponentiation would give an indeterminate form.\n\nMost fixed-size integer formats do not have any way of explicitly indicating invalid data. Converting NaN to an integer type, or performing an integer operation whose floating-point equivalent would produce NaN, usually throws an exception. In Java, such operations throw instances of codice_1. In C, they lead to undefined behavior.\n\nPerl's package uses \"NaN\" for the result of strings that don't represent valid integers.\nDifferent operating systems and programming languages may have different string representations of NaN.\n\nSince, in practice, encoded NaNs have a sign, a quiet/signaling bit and optional 'diagnostic information' (sometimes called a \"payload\"), these will often be found in string representations of NaNs, too, for example:\n\n(other variants exist).\n\nIn IEEE 754 standard-conforming floating-point storage formats, NaNs are identified by specific, pre-defined bit patterns unique to NaNs. The sign bit does not matter. Binary format NaNs are represented with the exponential field filled with ones (like infinity values), and some non-zero number in the significand field (to make them distinct from infinity values). The original IEEE 754 standard from 1985 (IEEE 754-1985) only described binary floating-point formats, and did not specify how the signaling/quiet state was to be tagged. In practice, the most significant bit of the significand field determined whether a NaN is signaling or quiet. Two different implementations, with reversed meanings, resulted:\nThe former choice has been preferred as it allows the implementation to quiet a signaling NaN by just setting the signaling/quiet bit to 1. The reverse is not possible with the latter choice because setting the signaling/quiet bit to 0 could yield an infinity.\n\nThe 2008 revision of the IEEE 754 standard (IEEE 754-2008) makes formal recommendations for the encoding of the signaling/quiet state.\n\nFor IEEE 754-2008 conformance, the meaning of the signaling/quiet bit in recent MIPS processors is now configurable via the NAN2008 field of the FCSR register. This support is optional in MIPS Release 3 and required in Release 5.\n\nThe state/value of the remaining bits of the significand field are not defined by the standard. This value is called the 'payload' of the NaN. If an operation has a single NaN input and propagates it to the output, the result NaN's payload should be that of the input NaN (this is not always possible for binary formats when the signaling/quiet state is encoded by an 'is_signaling' flag, as explained above). If there are multiple NaN inputs, the result NaN's payload should be from one of the input NaNs; the standard does not specify which.\n\n"}
{"id": "4849201", "url": "https://en.wikipedia.org/wiki?curid=4849201", "title": "Object of the mind", "text": "Object of the mind\n\nAn object of the mind is an object that exists in the imagination, but which, in the real world, can only be represented or modeled. Some such objects are abstractions, literary concepts, or fictional scenarios.\n\nClosely related are intentional objects, which are what thoughts and feelings are about, even if they are not about anything real (such as thoughts \nabout unicorns, or feelings of apprehension about a dental appointment which is subsequently cancelled). However, intentional objects may coincide with real objects (as in thoughts about horses, or a feeling of regret about a missed appointment).\n\nMathematics and geometry describe abstract objects that sometimes correspond to familiar shapes, and sometimes do not. Circles, triangles, rectangles, and so forth describe two-dimensional shapes that are often found in the real world. However, mathematical formulas do not describe individual physical circles, triangles, or rectangles. They describe ideal shapes that are objects of the mind. The incredible precision of mathematical expression permits a vast applicability of mental abstractions to real life situations.\n\nMany more mathematical formulas describe shapes that are unfamiliar, or do not necessarily correspond to objects in the real world. For example, the Klein bottle is a one-sided, sealed surface with no inside or outside (in other words, it is the three-dimensional equivalent of the Möbius strip). Such objects can be represented by twisting and cutting or taping pieces of paper together, as well as by computer simulations. To hold them in the imagination, abstractions such as extra or fewer dimensions are necessary.\n\nIf-then arguments posit logical sequences that sometimes include objects of the mind. For example, a counterfactual argument proposes a hypothetical or subjunctive possibility which \"could\" or \"would\" be true, but \"might not\" be false. Conditional sequences involving subjunctives use intensional language, which is studied by modal logic, whereas classical logic studies the extensional language of necessary and sufficient conditions.\n\nIn general, a logical antecedent is a sufficient condition, and a logical consequent is a necessary condition(or the contingency) in a logical conditional. But logical conditionals accounting only for necessity and sufficiency do not always reflect every day if-then reasoning, and for this reason they are sometimes known as material conditionals. In contrast, indicative conditionals, sometimes known as non-material conditionals, attempt to describe if-then reasoning involving hypotheticals, fictions, or counterfactuals.\n\nTruth tables for if-then statements identify four unique combinations of premises and conclusions: true premises and true conclusions; false premises and true conclusions; true premises and false conclusions; false premises and false conclusions. Strict conditionals assign a positive truth-value to every case except the case of a true premise and a false conclusion. This is sometimes regarded as counterintuitive, but makes more sense when false conditions are understood as objects of the mind.\n\nA false antecedent is a premise known to be false, fictional, imaginary, or unnecessary. In a conditional sequence, a false antecedent may be the basis for any consequence, true or false.\n\nThe subjects of literature are sometimes false antecedents. For example, the contents of false documents, the origins of stand-alone phenomena, or the implications of loaded words. Also, artificial sources, personalities, events, and histories. False antecedents are sometimes referred to as \"nothing\", or \"nonexistent\", whereas nonexistent referents are not referred to.\n\nArt and acting often portray scenarios without any antecedent except an artist's imagination. For example, mythical heroes, legendary creatures, gods, and goddesses.\n\nA false consequent, in contrast, is a conclusion known to be false, fictional, imaginary, or insufficient. In a conditional statement, a fictional conclusion is known as a non sequitur, which literally means \"out of sequence\". A conclusion that is out of sequence is not contingent on any premises that precede it, and it does not follow from them, so such a sequence is not conditional. A conditional sequence is a connected series of statements. A false consequent cannot follow from true premises in a connected sequence. But, on the other hand, a false consequent can follow from a false antecedent.\n\nAs an example, the name of a team, a genre, or a nation is a collective term applied ex post facto to a group of distinct individuals. None of the individuals on a sports team is the team itself, nor is any musical chord a genre, nor any person America. The name is an identity for a collection that is connected by consensus or reference, but not by sequence. A different name could equally follow, but it would have different social or political significance.\n\nIn philosophy, mind-body dualism is the doctrine that mental activities exist apart from the physical body, notably posited by René Descartes in \"Meditations on First Philosophy\".\n\nMany objects in fiction follow the example of false antecedents or false consequents. For example, \"The Lord of the Rings\" by J.R.R. Tolkien is based on an imaginary book. In the \"Appendices\" to \"The Lord of the Rings\", Tolkien's characters name the \"Red Book of Westmarch\" as the source material for \"The Lord of the Rings\", which they describe as a translation. But the \"Red Book of Westmarch\" is a fictional document that chronicles events in an imaginary world. One might imagine a different translation, by another author.\n\nSocial reality is composed of many standards and inventions that facilitate communication, but which are ultimately objects of the mind. For example, money is an object of the mind which currency represents. Similarly, languages signify ideas and thoughts.\n\nObjects of the mind are frequently involved in the roles that people play. For example, acting is a profession which predicates real jobs on fictional premises. Charades is a game people play by guessing imaginary objects from short play-acts.\n\nImaginary personalities and histories are sometimes invented to enhance the verisimilitude of fictional universes, and/or the immersion of role-playing games. In the sense that they exist independently of extant personalities and histories, they are believed to be fictional characters and fictional time frames.\n\nScience fiction is abundant with future times, alternate times, and past times that are objects of the mind. For example, in the novel \"Nineteen Eighty-Four\" by George Orwell, the number 1984 represented a year that had not yet passed.\n\nCalendar dates also represent objects of the mind, specifically, past and future times. In \"\", which was released in 1986, the narration opens with the statement, \"It is the year 2005.\" In 1986, that statement was futuristic. During the year 2005, that reference to the year 2005 was factual. Now, \"The Transformers: The Movie\" is retro-futuristic. The number 2005 did not change, but the object of the mind that it represents did change.\n\nDeliberate invention also may reference an object of the mind. The intentional invention of fiction for the purpose of deception is usually referred to as lying, in contrast to invention for entertainment or art. Invention is also often applied to problem solving. In this sense the physical invention of materials is associated with the mental invention of fictions.\n\nConvenient fictions also occur in science.\n\nThe theoretical posits of one era's scientific theories may be demoted to mere objects of the mind by subsequent discoveries: some standard examples include phlogiston and ptolemaic epicycles.\n\nThis raises questions, in the debate between scientific realism and instrumentalism about the status of current posits, such as black holes and quarks. Are they still merely intentional, even if the theory is correct?\n\nThe situation is further complicated by the existence in scientific practice of entities which are explicitly held not to be real, but which nonetheless serve a purpose—convenient fictions. Examples include field lines, centers of gravity, and electron holes in semiconductor theory.\n\nA reference that names an imaginary source is in some sense also a self-reference. A self-reference automatically makes a comment about itself. Premises that name themselves as premises are premises by self-reference; conclusions that name themselves as conclusions are conclusions by self-reference.\n\nIn their respective imaginary worlds the \"Necronomicon\", \"The Hitchhiker's Guide to the Galaxy\", and the \"Red Book of Westmarch\" are realities, but only because they are referred to as real. Authors use this technique to invite readers to pretend or to make-believe that their imaginary world is real. In the sense that the stories that quote these books are true, the quoted books exist; in the sense that the stories are fiction, the quoted books do not exist.\n\nAustrian philosopher Alexius Meinong (1853–1920) advanced nonexistent objects in the 19th and 20th century within a “theory of objects”. He was interested in intentional states which are directed at nonexistent objects. Starting with the “principle of intentionality”, mental phenomena are intentionally directed towards an object. People may imagine, desire or fear something that does not exist. Other philosophers concluded that intentionality is not a real relation and therefore does not require the existence of an object, while Meinong concluded there is an object for every mental state whatsoever—if not an existent then at least a nonexistent one.\n\n\n"}
{"id": "49460892", "url": "https://en.wikipedia.org/wiki?curid=49460892", "title": "Onofrio Puglisi", "text": "Onofrio Puglisi\n\nOnofrio Puglisi (died 1679) was a Sicilian mathematician from Palermo.\n"}
{"id": "794350", "url": "https://en.wikipedia.org/wiki?curid=794350", "title": "Predictive validity", "text": "Predictive validity\n\nIn psychometrics, predictive validity is the extent to which a score on a scale or test predicts scores on some criterion measure.\n\nFor example, the validity of a cognitive test for job performance is the correlation between test scores and, for example, supervisor performance ratings. Such a cognitive test would have \"predictive validity\" if the observed correlation were statistically significant. \n\nPredictive validity shares similarities with concurrent validity in that both are generally measured as correlations between a test and some criterion measure. In a study of concurrent validity the test is administered at the same time as the criterion is collected. This is a common method of developing validity evidence for employment tests: A test is administered to incumbent employees, then a rating of those employees' job performance is, or has already been, obtained independently of the test (often, as noted above, in the form of a supervisor rating). Note the possibility for restriction of range both in test scores and performance scores: The incumbent employees are likely to be a more homogeneous and higher performing group than the applicant pool at large.\n\nIn a strict study of predictive validity, the test scores are collected first; then at some later time the criterion measure is collected. For predictive validity, the example is slightly different: Tests are administered, perhaps to job applicants, and then after those individuals work in the job for a year, their test scores are correlated with their first year job performance scores. Another relevant example is SAT scores: These are validated by collecting the scores during the examinee's senior year and high school and then waiting a year (or more) to correlate the scores with their first year college grade point average. Thus predictive validity provides somewhat more useful data about test validity because it has greater fidelity to the real situation in which the test will be used. After all, most tests are administered to find out something about future behavior.\n\nAs with many aspects of social science, the magnitude of the correlations obtained from predictive validity studies is usually not high. A typical predictive validity for an employment test might obtain a correlation in the neighborhood of \"r\"=.35. Higher values are occasionally seen and lower values are very common. Nonetheless the utility (that is the benefit obtained by making decisions using the test) provided by a test with a correlation of .35 can be quite substantial. More information, and an explanation of the relationship between variance and predictive validity, can be found here.\n\nThe latest \"Standards for Educational and Psychological Testing\" reflect Samuel Messick's model of validity and do not use the term \"predictive validity.\" Rather, the \"Standards\" describe validity-supporting \"Evidence Based on Relationships [between the test scores and] Other Variables.\"\n\nPredictive validity involves testing a group of subjects for a certain construct, and then comparing them with results obtained at some point in the future.\n"}
{"id": "6083265", "url": "https://en.wikipedia.org/wiki?curid=6083265", "title": "Recursive indexing", "text": "Recursive indexing\n\nWhen number (generally large number) is represented in a finite alphabet set, and it cannot be represented by just one member of the set, Recursive indexing is used.\n\nRecursive indexing itself is a method to write the successive differences of the number after extracting the maximum value of the alphabet set from the number, and continuing recursively till the difference falls in the range of the set.\n\nRecursive indexing with a 2-letter alphabet is called unary code.\n\nTo encode a number \"N\", keep reducing the maximum element of this set (\"S\") from \"N\" and output \"S\"max for each such difference, stopping when the number lies in the half closed half open\nrange [0 – \"S\").\n\nLet \"S\" = [0 1 2 3 4 … 10], be an 11-element set, and we have to recursively index the value N=49.\n\nAccording to this method, we need to keep removing 10 from 49, and keep proceeding till we reach a number in the 0–10 range.\n\nSo the values are 10 (\"N\" = 49 – 10 = 39), 10 (\"N\" = 39 – 10 = 29), 10 (\"N\" = 29 – 10 = 19), 10 (\"N\" = 19 – 10 = 9), 9.\nHence the recursively indexed sequence for \"N\" = 49 with set \"S\", is 10, 10, 10, 10, 9.\n\nKeep adding all the elements of the index, stopping when the index value is between (inclusive of ends) the least and penultimate elements of the set \"S\".\n\nContinuing from above example we have 10 + 10 + 10 + 10 + 9 = 49.\n\nThis technique is most commonly used in run-length encoding systems to encode longer runs than the alphabet sizes permit.\n\n"}
{"id": "8629753", "url": "https://en.wikipedia.org/wiki?curid=8629753", "title": "Regular Polytopes (book)", "text": "Regular Polytopes (book)\n\nRegular Polytopes is a mathematical geometry book written by Canadian mathematician Harold Scott MacDonald Coxeter. Originally published in 1947, the book was updated and republished in 1963 and 1973.\n\nThe book is a comprehensive survey of the geometry of regular polytopes, the generalisation of regular polygons and regular polyhedra to higher dimensions. Originating with an essay entitled \"Dimensional Analogy\" written in 1923, the first edition of the book took Coxeter twenty-four years to complete.\n\n\"Regular Polytopes\" is a standard reference work on regular polygons, polyhedra and their higher dimensional analogues. It is unusual in the breadth of its coverage; its combination of mathematical rigour with geometric insight; and the clarity of its diagrams and illustrations.\n\nCoxeter starts by introducing two-dimensional polygons and three-dimensional polyhedra. He then gives a rigorous combinatorial definition of \"regularity\" and uses it to show that there are no other convex regular polyhedra apart from the five Platonic solids. The concept of \"regularity\" is extended to non-convex shapes such as star polygons and star polyhedra; to tessellations and honeycombs and to polytopes in higher dimensions. Coxeter introduces and uses the groups generated by reflections that became known as Coxeter groups.\n\nThe book combines algebraic rigour with clear explanations, many of which are illustrated with diagrams, and with a diagramatic notation for Wythoff constructions. The black and white plates in the book show solid models of three-dimensional polyhedra, and wire-frame models of projections of some higher-dimensional polytopes. At the end of each chapter Coxeter includes an \"Historical remarks\" section which provides an historical perspective of the development of the subject.\n\nThe challenge of comprehending higher dimensions was addressed by Coxeter on page 118: \"There are three ways of approaching the Euclidean geometry of four or more dimensions: the axiomatic, the algebraic (or analytical) and the intuitive. The first two have been admirably expounded by Sommerville and Neville, and we shall presuppose some familiarity with such treatises.\" Concerning the third, Poincaré wrote: \"A man who really pursues it, will end up holding on to the fourth dimension\".\n\nThe contents of the third edition (1973) of \"Regular Polytopes\" are as follows:\n\nIn a brief review of the 1963 Dover reprint in Math Science Network () an anonymous reviewer writes that “anyone interested in the relationship of group theory to geometry should own a copy.” The original 1948 edition received a more complete review by M. Goldberg in , and the third edition was reviewed telegraphically in .\n\n"}
{"id": "6825924", "url": "https://en.wikipedia.org/wiki?curid=6825924", "title": "Root datum", "text": "Root datum\n\nIn mathematical group theory, the root datum of a connected split reductive algebraic group over a field is a generalization of a root system that determines the group up to isomorphism. They were introduced by Michel Demazure in SGA III, published in 1970.\n\nA root datum consists of a quadruple\nwhere \n\nThe elements of formula_5 are called the roots of the root datum, and the elements of formula_7 are called the coroots.\n\nIf formula_5 does not contain formula_24 for any formula_25, then the root datum is called reduced.\n\nIf \"G\" is a reductive algebraic group over an algebraically closed field \"K\" with a split maximal torus \"T\" then its root datum is a quadruple \nwhere \n\nA connected split reductive algebraic group over \"K\" is uniquely determined (up to isomorphism) by its root datum, which is always reduced. Conversely for any root datum there is a reductive algebraic group. A root datum contains slightly more information than the Dynkin diagram, because it also determines the center of the group.\n\nFor any root datum (\"X\", Φ,\"X\", Φ), we can define a dual root datum (\"X\", Φ,\"X\", Φ) by switching the characters with the 1-parameter subgroups, and switching the roots with the coroots.\n\nIf \"G\" is a connected reductive algebraic group over the algebraically closed field \"K\", then its Langlands dual group \"G\" is the complex connected reductive group whose root datum is dual to that of \"G\".\n\n"}
{"id": "33074892", "url": "https://en.wikipedia.org/wiki?curid=33074892", "title": "Sauer–Shelah lemma", "text": "Sauer–Shelah lemma\n\nIn combinatorial mathematics and extremal set theory, the Sauer–Shelah lemma states that every family of sets with small VC dimension consists of a small number of sets. It is named after Norbert Sauer and Saharon Shelah, who published it independently of each other in 1972. The same result was also published slightly earlier and again independently, by Vladimir Vapnik and Alexey Chervonenkis, after whom the VC dimension is named. In his paper containing the lemma, Shelah gives credit also to Micha Perles, and for this reason the lemma has also been called the Perles–Sauer–Shelah lemma.\n\nBuzaglo et al. call this lemma \"one of the most fundamental results on VC-dimension\", and it has applications in many areas. Sauer's motivation was in the combinatorics of set systems, while Shelah's was in model theory and that of Vapnik and Chervonenkis was in statistics. It has also been applied in discrete geometry and graph theory.\n\nIf formula_1 is a family of sets, and formula_2 is another set, then formula_2 is said to be shattered by formula_4 if every subset of formula_2 (including the empty set and formula_2 itself) can be obtained as an intersection formula_7 between formula_2 and a set in the family. The VC dimension of formula_4 is the largest cardinality of a set shattered by formula_4.\n\nIn terms of these definitions, the Sauer–Shelah lemma states that if formula_4 is a family of sets with formula_12 distinct elements such that\nformula_13, then formula_4 shatters a set of size formula_15. Equivalently, if the VC dimension of formula_4 is formula_17 then formula_4 can consist of at most formula_19 sets.\n\nThe bound of the lemma is tight: Let the family formula_4 be composed of all subsets of formula_21 with size less than formula_15. Then the size of formula_4 is exactly formula_24 but it does not shatter any set of size formula_15.\n\nA strengthening of the Sauer–Shelah lemma, due to , states that every finite set family formula_4shatters at least formula_27 sets. This immediately implies the Sauer–Shelah lemma, because only formula_28 of the subsets of an formula_12-item universe have cardinality less than formula_15. Thus, when formula_31, there are not enough small sets to be shattered, so one of the shattered sets must have cardinality at least formula_15.\n\nFor a restricted type of shattered set, called an order-shattered set, the number of shattered sets always equals the cardinality of the set family.\n\nPajor's variant of the Sauer–Shelah lemma may be proved by mathematical induction; the proof has variously been credited to Noga Alon or to Ron Aharoni and Ron Holzman. \n\nBase: every family of only one set shatters the empty set. \n\nStep: Assume the lemma is true for all families of size less than formula_27 and let formula_4 be a family of two or more sets. Let formula_35 be an element that belongs to some but not all of the sets in formula_4. Split formula_4 into two subfamilies, of the sets that contain formula_35 and the sets that do not contain formula_35. \n\nBy the induction assumption, these two subfamilies shatter two collections of sets whose sizes add to at least formula_27. \n\nNone of these shattered sets contain formula_35, since a set that contains formula_35 cannot be shattered by a family in which all sets contain formula_35 or all sets do not contain formula_35.\n\nSome of the shattered sets may be shattered by both subfamilies. When a set formula_45 is shattered by only one of the two subfamilies, it contributes one unit both to the number of shattered sets of the subfamily and to the number of shattered sets of formula_4. When a set formula_45 is shattered by both subfamilies, both formula_45 and formula_49 are shattered by formula_4, so formula_45 contributes two units to the number of shattered sets of the subfamilies and of formula_4. Therefore, the number of shattered sets of formula_4 is at least equal to the number shattered by the two subfamilies of formula_4, which is at least formula_27.\n\nA different proof of the Sauer–Shelah lemma in its original form, by Péter Frankl and János Pach, is based on linear algebra and the inclusion–exclusion principle.\n\nThe original application of the lemma, by Vapnik and Chervonenkis, was in showing that every probability distribution can be approximated (with respect to a family of events of a given VC dimension) by a finite set of sample points whose cardinality depends only on the VC dimension of the family of events. In this context, there are two important notions of approximation, both parameterized by a number ε: a set \"S\" of samples, and a probability distribution on \"S\", is said to be an ε-approximation of the original distribution if the probability of each event with respect to \"S\" differs from its original probability by at most ε. A set \"S\" of (unweighted) samples is said to be an ε-net if every event with probability at least ε includes at least one point of \"S\". An ε-approximation must also be an ε-net but not necessarily vice versa.\n\nVapnik and Chervonenkis used the lemma to show that set systems of VC dimension \"d\" always have ε-approximations of cardinality formula_56. Later authors including and similarly showed that there always exist ε-nets of cardinality formula_57, and more precisely of cardinality at most formula_58. The main idea of the proof of the existence of small ε-nets is to choose a random sample \"x\" of cardinality formula_57 and a second independent random sample \"y\" of cardinality formula_60, and to bound the probability that \"x\" is missed by some large event \"E\" by the probability that \"x\" is missed and simultaneously the intersection of \"y\" with \"E\" is larger than its median value. For any particular \"E\", the probability that \"x\" is missed while \"y\" is larger than its median is very small,\nand the Sauer–Shelah lemma (applied to formula_61) shows that only a small number of distinct events \"E\" need to be considered, so by the union bound, with nonzero probability, \"x\" is an ε-net.\n\nIn turn, ε-nets and ε-approximations, and the likelihood that a random sample of large enough cardinality has these properties, have important applications in machine learning, in the area of probably approximately correct learning. In computational geometry, they have been applied to range searching, derandomization, and approximation algorithms.\n\n"}
{"id": "40117302", "url": "https://en.wikipedia.org/wiki?curid=40117302", "title": "Separation axiom", "text": "Separation axiom\n\nIn topology and related fields of mathematics, there are several restrictions that one often makes on the kinds of topological spaces that one wishes to consider. Some of these restrictions are given by the separation axioms. These are sometimes called \"Tychonoff separation axioms\", after Andrey Tychonoff.\n\nThe separation axioms are axioms only in the sense that, when defining the notion of topological space, one could add these conditions as extra axioms to get a more restricted notion of what a topological space is. The modern approach is to fix once and for all the axiomatization of topological space and then speak of \"kinds\" of topological spaces.\nHowever, the term \"separation axiom\" has stuck. The separation axioms are denoted with the letter \"T\" after the German \"Trennungsaxiom\", which means \"separation axiom.\"\n\nThe precise meanings of the terms associated with the separation axioms has varied over time, as explained in History of the separation axioms. It is important to understand the authors' definition of each condition mentioned to know exactly what they mean, especially when reading older literature.\n\nBefore we define the separation axioms themselves, we give concrete meaning to the concept of separated sets (and points) in topological spaces. (Separated sets are not the same as \"separated spaces\", defined in the next section.)\n\nThe separation axioms are about the use of topological means to distinguish disjoint sets and distinct points. It's not enough for elements of a topological space to be distinct (that is, unequal); we may want them to be \"topologically distinguishable\". Similarly, it's not enough for subsets of a topological space to be disjoint; we may want them to be \"separated\" (in any of various ways). The separation axioms all say, in one way or another, that points or sets that are distinguishable or separated in some weak sense must also be distinguishable or separated in some stronger sense.\n\nLet \"X\" be a topological space. Then two points \"x\" and \"y\" in \"X\" are topologically distinguishable if they do not have exactly the same neighbourhoods (or equivalently the same open neighbourhoods); that is, at least one of them has a neighbourhood that is not a neighbourhood of the other (or equivalently there is an open set that one point belongs to but the other point does not).\n\nTwo points \"x\" and \"y\" are separated if each of them has a neighbourhood that is not a neighbourhood of the other; that is, neither belongs to the other's closure. More generally, two subsets \"A\" and \"B\" of \"X\" are separated if each is disjoint from the other's closure. (The closures themselves do not have to be disjoint.) All of the remaining conditions for separation of sets may also be applied to points (or to a point and a set) by using singleton sets. Points \"x\" and \"y\" will be considered separated, by neighbourhoods, by closed neighbourhoods, by a continuous function, precisely by a function, if and only if their singleton sets {\"x\"} and {\"y\"} are separated according to the corresponding criterion.\n\nSubsets \"A\" and \"B\" are separated by neighbourhoods if they have disjoint neighbourhoods. They are separated by closed neighbourhoods if they have disjoint closed neighbourhoods. They are separated by a continuous function if there exists a continuous function \"f\" from the space \"X\" to the real line R such that the image \"f\"(\"A\") equals {0} and \"f\"(\"B\") equals {1}. Finally, they are precisely separated by a continuous function if there exists a continuous function \"f\" from \"X\" to R such that the preimage \"f\"({0}) equals \"A\" and \"f\"({1}) equals \"B\".\n\nThese conditions are given in order of increasing strength: Any two topologically distinguishable points must be distinct, and any two separated points must be topologically distinguishable. Any two separated sets must be disjoint, any two sets separated by neighbourhoods must be separated, and so on.\n\nFor more on these conditions (including their use outside the separation axioms), see the articles Separated sets and Topological distinguishability.\n\nThese definitions all use essentially the preliminary definitions above.\n\nMany of these names have alternative meanings in some of mathematical literature, as explained on History of the separation axioms; for example, the meanings of \"normal\" and \"T\" are sometimes interchanged, similarly \"regular\" and \"T\", etc. Many of the concepts also have several names; however, the one listed first is always least likely to be ambiguous.\n\nMost of these axioms have alternative definitions with the same meaning; the definitions given here fall into a consistent pattern that relates the various notions of separation defined in the previous section. Other possible definitions can be found in the individual articles.\n\nIn all of the following definitions, \"X\" is again a topological space.\n\n\nThe T axiom is special in that it can be not only added to a property (so that completely regular plus T is Tychonoff) but also subtracted from a property (so that Hausdorff minus T is R), in a fairly precise sense; see Kolmogorov quotient for more information. When applied to the separation axioms, this leads to the relationships in the table to the left below. In this table, you go from the right side to the left side by adding the requirement of T, and you go from the left side to the right side by removing that requirement, using the Kolmogorov quotient operation. (The names in parentheses given on the left side of this table are generally ambiguous or at least less well known; but they are used in the diagram below.)\n\nOther than the inclusion or exclusion of T, the relationships between the separation axioms are indicated in the diagram to the right. In this diagram, the non-T version of a condition is on the left side of the slash, and the T version is on the right side. Letters are used for abbreviation as follows:\n\"P\" = \"perfectly\", \"C\" = \"completely\", \"N\" = \"normal\", and \"R\" (without a subscript) = \"regular\". A bullet indicates that there is no special name for a space at that spot. The dash at the bottom indicates no condition.\n\nYou can combine two properties using this diagram by following the diagram upwards until both branches meet. For example, if a space is both completely normal (\"CN\") and completely Hausdorff (\"CT\"), then following both branches up, you find the spot \"•/T\".\nSince completely Hausdorff spaces are T (even though completely normal spaces may not be), you take the T side of the slash, so a completely normal completely Hausdorff space is the same as a T space (less ambiguously known as a completely normal Hausdorff space, as you can see in the table above).\n\nAs you can see from the diagram, normal and R together imply a host of other properties, since combining the two properties leads you to follow a path through the many nodes on the rightside branch. Since regularity is the most well known of these, spaces that are both normal and R are typically called \"normal regular spaces\". In a somewhat similar fashion, spaces that are both normal and T are often called \"normal Hausdorff spaces\" by people that wish to avoid the ambiguous \"T\" notation. These conventions can be generalised to other regular spaces and Hausdorff spaces.\n\nThere are some other conditions on topological spaces that are sometimes classified with the separation axioms, but these don't fit in with the usual separation axioms as completely. Other than their definitions, they aren't discussed here; see their individual articles.\n\n\n\n"}
{"id": "632632", "url": "https://en.wikipedia.org/wiki?curid=632632", "title": "Shimon Even", "text": "Shimon Even\n\nShimon Even (; June 15, 1935 – May 1, 2004) was an Israeli computer science researcher. His main topics of interest included algorithms, graph theory and cryptography. He was a member of the Computer Science Department at the Technion since 1974. Shimon Even was the PhD advisor of Oded Goldreich, a prominent cryptographer.\n\n\n\n"}
{"id": "41985", "url": "https://en.wikipedia.org/wiki?curid=41985", "title": "Shortest path problem", "text": "Shortest path problem\n\nIn graph theory, the shortest path problem is the problem of finding a path between two vertices (or nodes) in a graph such that the sum of the weights of its constituent edges is minimized.\n\nThe problem of finding the shortest path between two intersections on a road map may be modeled as a special case of the shortest path problem in graphs, where the vertices correspond to intersections and the edges correspond to road segments, each weighted by the length of the segment.\n\nThe shortest path problem can be defined for graphs whether undirected, directed, or mixed.\nIt is defined here for undirected graphs; for directed graphs the definition of path\nrequires that consecutive vertices be connected by an appropriate directed edge.\n\nTwo vertices are adjacent when they are both incident to a common edge.\nA path in an undirected graph is a sequence of vertices formula_1\nsuch that formula_2 is adjacent to formula_3 for formula_4.\nSuch a path formula_5 is called a path of length formula_6\nfrom formula_7 to formula_8.\n\nLet formula_10 be the edge incident to both formula_2 and formula_12. Given a real-valued weight function formula_13, and an undirected (simple) graph formula_14, the shortest path from formula_15 to formula_16 is the path formula_17 (where formula_18 and formula_19) that over all possible formula_20 minimizes the sum formula_21 When each edge in the graph has unit weight or formula_22, this is equivalent to finding the path with fewest edges.\n\nThe problem is also sometimes called the single-pair shortest path problem, to distinguish it from the following variations:\n\nThese generalizations have significantly more efficient algorithms than the simplistic approach of running a single-pair shortest path algorithm on all relevant pairs of vertices.\n\nThe most important algorithms for solving this problem are:\n\nAdditional algorithms and associated evaluations may be found in .\n\nAn algorithm using topological sorting can solve the single-source shortest path problem in linear time, , in weighted DAGs.\n\nThe following table is taken from , with some corrections and additions.\nA green background indicates an asymptotically best bound in the table; \"L\" is the maximum length (or weight) among all edges, assuming integer edge weights.\n\nThe all-pairs shortest path problem finds the shortest paths between every pair of vertices , in the graph. The all-pairs shortest paths problem for unweighted directed graphs was introduced by , who observed that it could be solved by a linear number of matrix multiplications that takes a total time of .\n\nShortest path algorithms are applied to automatically find directions between physical locations, such as driving directions on web mapping websites like MapQuest or Google Maps. For this application fast specialized algorithms are available.\n\nIf one represents a nondeterministic abstract machine as a graph where vertices describe states and edges describe possible transitions, shortest path algorithms can be used to find an optimal sequence of choices to reach a certain goal state, or to establish lower bounds on the time needed to reach a given state. For example, if vertices represent the states of a puzzle like a Rubik's Cube and each directed edge corresponds to a single move or turn, shortest path algorithms can be used to find a solution that uses the minimum possible number of moves.\n\nIn a networking or telecommunications mindset, this shortest path problem is sometimes called the min-delay path problem and usually tied with a widest path problem. For example, the algorithm may seek the shortest (min-delay) widest path, or widest shortest (min-delay) path.\n\nA more lighthearted application is the games of \"six degrees of separation\" that try to find the shortest path in graphs like movie stars appearing in the same film.\n\nOther applications, often studied in operations research, include plant and facility layout, robotics, transportation, and VLSI design.\n\nA road network can be considered as a graph with positive weights. The nodes represent road junctions and each edge of the graph is associated with a road segment between two junctions. The weight of an edge may correspond to the length of the associated road segment, the time needed to traverse the segment, or the cost of traversing the segment. Using directed edges it is also possible to model one-way streets. Such graphs are special in the sense that some edges are more important than others for long distance travel (e.g. highways). This property has been formalized using the notion of highway dimension. There are a great number of algorithms that exploit this property and are therefore able to compute the shortest path a lot quicker than would be possible on general graphs.\n\nAll of these algorithms work in two phases. In the first phase, the graph is preprocessed without knowing the source or target node. The second phase is the query phase. In this phase, source and target node are known.The idea is that the road network is static, so the preprocessing phase can be done once and used for a large number of queries on the same road network.\n\nThe algorithm with the fastest known query time is called hub labeling and is able to compute shortest path on the road networks of Europe or the USA in a fraction of a microsecond. Other techniques that have been used are:\n\n\nFor shortest path problems in computational geometry, see Euclidean shortest path.\n\nThe travelling salesman problem is the problem of finding the shortest path that goes through every vertex exactly once, and returns to the start. Unlike the shortest path problem, which can be solved in polynomial time in graphs without negative cycles, the travelling salesman problem is NP-complete and, as such, is believed not to be efficiently solvable for large sets of data (see P = NP problem). The problem of finding the longest path in a graph is also NP-complete.\n\nThe Canadian traveller problem and the stochastic shortest path problem are generalizations where either the graph isn't completely known to the mover, changes over time, or where actions (traversals) are probabilistic.\n\nThe shortest multiple disconnected path is a representation of the primitive path network within the framework of Reptation theory.\n\nThe widest path problem seeks a path so that the minimum label of any edge is as large as possible.\n\nSometimes, the edges in a graph have personalities: each edge has its own selfish interest. An example is a communication network, in which each edge is a computer that possibly belongs to a different person. Different computers have different transmission speeds, so every edge in the network has a numeric weight equal to the number of milliseconds it takes to transmit a message. Our goal is to send a message between two points in the network in the shortest time possible. If we know the transmission-time of each computer (the weight of each edge), then we can use a standard shortest-paths algorithm. If we do not know the transmission times, then we have to ask each computer to tell us its transmission-time. But, the computers may be selfish: a computer might tell us that its transmission time is very long, so that we will not bother it with our messages. A possible solution to this problem is to use a variant of the VCG mechanism, which gives the computers an incentive to reveal their true weights.\n\nThere is a natural linear programming formulation for the shortest path problem, given below. It is very simple compared to most other uses of linear programs in discrete optimization, however it illustrates connections to other concepts.\n\nGiven a directed graph (\"V\", \"A\") with source node \"s\", target node \"t\", and cost \"w\" for each edge (\"i\", \"j\") in \"A\", consider the program with variables \"x\"\n\nThe intuition behind this is that formula_26 is an indicator variable for whether edge (\"i\", \"j\") is part of the shortest path: 1 when it is, and 0 if it is not. We wish to select the set of edges with minimal weight, subject to the constraint that this set forms a path from \"s\" to \"t\" (represented by the equality constraint: for all vertices except \"s\" and \"t\" the number of incoming and outcoming edges that are part of the path must be the same (i.e., that it should be a path from s to t).\n\nThis LP has the special property that it is integral; more specifically, every basic optimal solution (when one exists) has all variables equal to 0 or 1, and the set of edges whose variables equal 1 form an \"s\"-\"t\" dipath. See Ahuja et al. for one proof, although the origin of this approach dates back to mid-20th century.\n\nThe dual for this linear program is\nand feasible duals correspond to the concept of a consistent heuristic for the A* algorithm for shortest paths. For any feasible dual \"y\" the reduced costs formula_27 are nonnegative and A* essentially runs Dijkstra's algorithm on these reduced costs.\n\nMany problems can be framed as a form of the shortest path for some suitably substituted notions of addition along a path and taking the minimum. The general approach to these is to consider the two operations to be those of a semiring. Semiring multiplication is done along the path, and the addition is between paths. This general framework is known as the algebraic path problem.\n\nMost of the classic shortest-path algorithms (and new ones) can be formulated as solving linear systems over such algebraic structures.\n\nMore recently, an even more general framework for solving these (and much less obviously related problems) has been developed under the banner of valuation algebras.\n\nIn real-life situations, the transportation network is usually stochastic and time-dependent. In fact, a traveler traversing a link daily may experiences different travel times on that link due not only to the fluctuations in travel demand (origin-destination matrix) but also due to such incidents as work zones, bad weather conditions, accidents and vehicle breakdowns. As a result, a stochastic time-dependent (STD) network is a more realistic representation of an actual road network compared with the deterministic one.\n\nDespite considerable progress during the course of the past decade, it remains a controversial question how an optimal path should be defined and identified in stochastic road networks. In other words, there is no unique definition of an optimal path under uncertainty. One possible and common answer to this question is to find a path with the minimum expected travel time. The main advantage of using this approach is that efficient shortest path algorithms introduced for the deterministic networks can be readily employed to identify the path with the minimum expected travel time in a stochastic network. However, the resulting optimal path identified by this approach may not be reliable, because this approach fails to address travel time variability. To tackle this issue some researchers use distribution of travel time instead of expected value of it so they find the probability distribution of total travelling time using different optimization methods such as dynamic programming and Dijkstra's algorithm . These methods use stochastic optimization, specifically stochastic dynamic programming to find the shortest path in networks with probabilistic arc length. It should be noted that the concept of travel time reliability is used interchangeably with travel time variability in the transportation research literature, so that, in general, one can say that the higher the variability in travel time, the lower the reliability would be, and vice versa.\n\nIn order to account for travel time reliability more accurately, two common alternative definitions for an optimal path under uncertainty have been suggested. Some have introduced the concept of the most reliable path, aiming to maximize the probability of arriving on time or earlier than a given travel time budget. Others, alternatively, have put forward the concept of an α-reliable path based on which they intended to minimize the travel time budget required to ensure a pre-specified on-time arrival probability.\n\n\n"}
{"id": "32754586", "url": "https://en.wikipedia.org/wiki?curid=32754586", "title": "Sister Celine's polynomials", "text": "Sister Celine's polynomials\n\nIn mathematics, Sister Celine's polynomials are a family of hypergeometric polynomials introduced by . They include Legendre polynomials, Jacobi polynomials, and Bateman polynomials as special cases.\n"}
{"id": "28359539", "url": "https://en.wikipedia.org/wiki?curid=28359539", "title": "Some Remarks on Logical Form", "text": "Some Remarks on Logical Form\n\nSome Remarks on Logical Form () was the only academic paper ever published by Ludwig Wittgenstein, and contained Wittgenstein's thinking on logic and the philosophy of mathematics immediately before the rupture that divided the early Wittgenstein of the Tractatus Logico-Philosophicus from the later Wittgenstein. The approach to logical form in the paper reflected Frank P. Ramsey's critique of Wittgenstein's account in the Tractatus, and has been analyzed by G.E.M. Anscombe and Jaakko Hintikka, among others.\n"}
{"id": "32022697", "url": "https://en.wikipedia.org/wiki?curid=32022697", "title": "Stone algebra", "text": "Stone algebra\n\nIn mathematics, a Stone algebra, or Stone lattice, is a pseudo-complemented distributive lattices such that \"a\"* ∨\"a\"** = 1. They were introduced by and named after Marshall Harvey Stone.\n\nBoolean algebras are Stone algebras, and Stone algebras are Ockham algebras.\n\nExamples:\n\n\n"}
{"id": "46597851", "url": "https://en.wikipedia.org/wiki?curid=46597851", "title": "Strict initial object", "text": "Strict initial object\n\nIn the mathematical discipline of category theory, a strict initial object is an initial object 0 of a category \"C\" with the property that every morphism in \"C\" with codomain 0 is an isomorphism. If \"C\" is a Cartesian closed category, then any initial object 0 of \"C\" is strict. Also, if \"C\" is a distributive or extensive category, then the initial object 0 of \"C\" is strict.\n"}
{"id": "1399856", "url": "https://en.wikipedia.org/wiki?curid=1399856", "title": "Strongly regular graph", "text": "Strongly regular graph\n\nIn graph theory, a strongly regular graph is defined as follows. Let \"G\" = (\"V\", \"E\") be a regular graph with \"v\" vertices and degree \"k\". \"G\" is said to be strongly regular if there are also integers λ and μ such that:\n\n\nA graph of this kind is sometimes said to be an srg(\"v\", \"k\", λ, μ). Strongly regular graphs were introduced by Raj Chandra Bose in 1963.\n\nSome authors exclude graphs which satisfy the definition trivially, namely those graphs which are the disjoint union of one or more equal-sized complete graphs, and their complements, the complete multipartite graphs with equal-sized independent sets.\n\nThe complement of an srg(\"v\", \"k\", λ, μ) is also strongly regular. It is an srg(\"v\", \"v−k\"−1, \"v\"−2−2\"k\"+μ, \"v\"−2\"k\"+λ).\n\nA strongly regular graph is a distance-regular graph with diameter 2 whenever μ is non-zero.\n\nThe four parameters in an srg(\"v\", \"k\", λ, μ) are not independent and must obey the following relation:\nThe above relation can be derived very easily through a counting argument as follows:\n\nLet \"I\" denote the identity matrix and let \"J\" denote the matrix of ones, both matrices of order \"v\". The adjacency matrix \"A\" of a strongly regular graph satisfies two equations. First:\nwhich is a trivial restatement of the regularity requirement. This shows that \"k\" is an eigenvalue of the adjacency matrix with the all-ones eigenvector. Second is a quadratic equation,\n\nwhich expresses strong regularity. The \"ij\"-th element of the left hand side gives the number of two-step paths from \"i\" to \"j\". The first term of the RHS gives the number of self-paths from \"i\" to \"i\", namely \"k\" edges out and back in. The second term gives the number of two-step paths when \"i\" and \"j\" are directly connected. The third term gives the corresponding value when \"i\" and \"j\" are not connected. Since the three cases are mutually exclusive and collectively exhaustive, the simple additive equality follows.\n\nConversely, a graph whose adjacency matrix satisfies both of the above conditions and which is not a complete or null graph is a strongly regular graph.\n\nThe adjacency matrix of the graph has exactly three eigenvalues:\nAs the multiplicities must be integers, their expressions provide further constraints on the values of \"v\", \"k\", \"μ\", and \"λ\", related to the so-called \"Krein conditions\".\n\nStrongly regular graphs for which formula_12 are called conference graphs because of their connection with symmetric conference matrices. Their parameters reduce to\n\nStrongly regular graphs for which formula_14 have integer eigenvalues with unequal multiplicities.\n\nConversely, a connected regular graph with only three eigenvalues is strongly regular.\n\n\nA strongly regular graph is called primitive if both the graph and its complement are connected. All the above graphs are primitive, as otherwise μ=0 or λ=k.\n\nThe strongly regular graphs with λ = 0 are triangle free. Apart from the complete graphs on less than 3 vertices and all complete bipartite graphs the seven listed above are the only known ones. Strongly regular graphs with λ = 0 and μ = 1 are Moore graphs with girth 5. Again the three graphs given above, with parameters (5, 2, 0, 1), (10, 3, 0, 1) and (50, 7, 0, 1), are the only known ones. The only other possible set of parameters yielding a Moore graph is (3250, 57, 0, 1); it is unknown if such a graph exists, and if so, whether or not it is unique.\n\n\n\n"}
{"id": "14528980", "url": "https://en.wikipedia.org/wiki?curid=14528980", "title": "Structural break", "text": "Structural break\n\nIn econometrics, a structural break, or structural change, is an unexpected shift in a time series that can lead to huge forecasting errors and unreliability of the model in general. This issue was popularised by David Hendry, who argued that lack of stability of coefficients frequently caused forecast failure, and therefore we must routinely test for structural stability. Structural stability − i.e., the time-invariance of regression coefficients − is a central issue in all applications of linear regression models.\n\nFor linear regression models, the Chow test is often used to test for a single break in mean at a known time period for . This test assesses whether the coefficients in a regression model are the same for periods and .\n\nOther challenges occur where there are:\n\nThe Chow test is not applicable in these situations, since it only applies to models with a known breakpoint and where the error variance remains constant before and after the break.\n\nIn general, the CUSUM (cumulative sum) and CUSUM-sq (CUSUM squared) tests can be used to test the constancy of the coefficients in a model. The bounds test can also be used. For cases 1 and 2, the sup-Wald (i.e., the supremum of a set of Wald statistics), sup-LM (i.e., the supremum of a set of Lagrange multiplier statistics), and sup-LR (i.e., the supremum of a set of likelihood ratio statistics) tests developed by Andrews (1993, 2003) may be used to test for parameter instability when the number and location of structural breaks are unknown. These tests were shown to be superior to the CUSUM test in terms of statistical power, and are the most commonly used tests for the detection of structural change involving an unknown number of breaks in mean with unknown break points. The sup-Wald, sup-LM, and sup-LR tests are asymptotic in general (i.e., the asymptotic critical values for these tests are applicable for sample size as ), and involve the assumption of homoskedasticity across break points for finite samples; however, an exact test with the sup-Wald statistic may be obtained for a linear regression model with a fixed number of regressors and independent and identically distributed (IID) normal errors. A method developed by Bai and Perron (2003) also allows for the detection of multiple structural breaks from data.\n\nThe MZ test developed by Maasoumi, Zaman, and Ahmed (2010) allows for the simultaneous detection of one or more breaks in both mean and variance at a \"known\" break point. The sup-MZ test developed by Ahmed, Haider, and Zaman (2016) is a generalization of the MZ test which allows for the detection of breaks in mean and variance at an \"unknown\" break point. \n\nFor a cointegration model, the Gregory–Hansen test (1996) can be used for one unknown structural break, and the Hatemi–J test (2006) can be used for two unknown breaks.\n\nThere are several statistical packages that can be used to find structural breaks, including R, GAUSS, and Stata, among others.\n\n"}
{"id": "30875123", "url": "https://en.wikipedia.org/wiki?curid=30875123", "title": "Stuttering equivalence", "text": "Stuttering equivalence\n\nIn theoretical computer science, stuttering equivalence, a relation written as\n\ncan be seen as a partitioning of path formula_2 and formula_3 into blocks, so that states in the formula_4 block of one path are labeled (formula_5) the same as states in the formula_4 block of the other path. Corresponding blocks may have different lengths.\n\nFormally, this can be expressed as two infinite paths formula_7 and formula_8 which are stuttering equivalent (formula_9) if there are two infinite sequences of integers formula_10 and formula_11 such that for every block formula_12 holds formula_13.\n\nStuttering equivalence is not the same as bisimulation, since bisimulation cannot capture the semantics of the 'eventually' (or 'finally') operator found in linear temporal/computation tree logic(branching time logic)(modal logic). So-called \"branching bisimulation\" has to be used.\n"}
{"id": "8016560", "url": "https://en.wikipedia.org/wiki?curid=8016560", "title": "Symmetry element", "text": "Symmetry element\n\nA symmetry element is a point of reference about which symmetry operations can take place. In particular, symmetry elements can be identities, mirror planes, axes of rotation (both proper and improper), and centers of inversion. A symmetry element corresponds to a symmetry operation that generates the same representation of an object.\n\nThe identity symmetry element is found in all objects and is denoted E. It corresponds to an operation of doing nothing to the object.\n\nMirror planes are denoted by σ. In particular, a vertical mirror plane is denoted σ.\n\nCyclic symmetry, also known as radial symmetry, is represented by an axis about which the object rotates in its corresponding symmetry operation. For proper axes of rotation, cyclic symmetry is denoted as C, where n is the order of rotation.\nImproper rotation, also known as screw axis symmetry, is denoted by S. \nImproper rotation is the composition of a rotation axis and a mirror plane. In its symmetry operation, the object is rotated about the axis, then reflected across a mirror plane that is normal to the axis of rotation.\n\nFor inversion, denoted i, there must be a point in the center of an object that is the inversion center. In the inversion operation for 3D coordinates, the inversion center is the origin (0,0,0). When an object is inverted, the position vector of a point in an object, ⟨x,y,z⟩, is inverted to ⟨-x,-y,-z⟩.\n\n"}
{"id": "499010", "url": "https://en.wikipedia.org/wiki?curid=499010", "title": "Tensor calculus", "text": "Tensor calculus\n\nIn mathematics, tensor calculus or tensor analysis is an extension of vector calculus to tensor fields (tensors that may vary over a manifold, e.g. in spacetime).\n\nDeveloped by Gregorio Ricci-Curbastro and his student Tullio Levi-Civita, it was used by Albert Einstein to develop his theory of general relativity. Contrasted with the infinitesimal calculus, tensor calculus allows presentation of physics equations in a form that is independent of the choice of coordinates on the manifold.\n\nTensor calculus has many real-life applications in physics and engineering, including elasticity, continuum mechanics, electromagnetism (see mathematical descriptions of the electromagnetic field), general relativity (see mathematics of general relativity),quantum field theory.\n\n\n"}
{"id": "1681091", "url": "https://en.wikipedia.org/wiki?curid=1681091", "title": "Turing jump", "text": "Turing jump\n\nIn computability theory, the Turing jump or Turing jump operator, named for Alan Turing, is an operation that assigns to each decision problem a successively harder decision problem with the property that is not decidable by an oracle machine with an oracle for .\n\nThe operator is called a \"jump operator\" because it increases the Turing degree of the problem . That is, the problem is not Turing reducible to . Post's theorem establishes a relationship between the Turing jump operator and the arithmetical hierarchy of sets of natural numbers. Informally, given a problem, the Turing jump returns the set of Turing machines which halt when given access to an oracle that solves that problem.\n\nThe Turing jump of X can be thought of as an oracle to the halting problem for oracle machines with an oracle to X.\n\nFormally, given a set and a Gödel numbering of the -computable functions, the Turing jump of is defined as\n\nThe th Turing jump is defined inductively by\n\nThe jump of is the effective join of the sequence of sets for : \n\nwhere denotes the th prime.\n\nThe notation or is often used for the Turing jump of the empty set. It is read \"zero-jump\" or sometimes \"zero-prime\".\n\nSimilarly, is the th jump of the empty set. For finite , these sets are closely related to the arithmetic hierarchy.\n\nThe jump can be iterated into transfinite ordinals: the sets for , where is the Church–Kleene ordinal, are closely related to the hyperarithmetic hierarchy. Beyond , the process can be continued through the countable ordinals of the constructible universe, using set-theoretic methods (Hodes 1980). The concept has also been generalized to extend to uncountable regular cardinals (Lubarsky 1987).\n\n\n\nMany properties of the Turing jump operator are discussed in the article on Turing degrees.\n\n"}
{"id": "333996", "url": "https://en.wikipedia.org/wiki?curid=333996", "title": "Ultrametric space", "text": "Ultrametric space\n\nIn mathematics, an ultrametric space is a metric space in which the triangle inequality is strengthened to formula_1. Sometimes the associated metric is also called a non-Archimedean metric or super-metric. Although some of the theorems for ultrametric spaces may seem strange at a first glance, they appear naturally in many applications.\n\nFormally, an ultrametric space is a set of points formula_2 with an associated distance function (also called a metric)\n\n(where formula_4 is the set of real numbers), such that for all formula_5, one has:\n\nIn the case when formula_2 is a group (written additively) and formula_12 is generated by a length function formula_13 (so that formula_14), the last property can be made stronger using the Krull sharpening to:\n\nWe want to prove that if formula_17, then the equality occurs if formula_16. Without loss of generality, let us assume that formula_19. This implies that formula_20. But we can also compute formula_21. Now, the value of formula_22 cannot be formula_23, for if that is the case, we have formula_24 contrary to the initial assumption. Thus, formula_25, and formula_26. Using the initial inequality, we have formula_27 and therefore formula_28.\n\nFrom the above definition, one can conclude several typical properties of ultrametrics. For example, in an ultrametric space, for all formula_5 and formula_30, at least one of the three equalities formula_31 or formula_32 or formula_33 holds. That is, every triple of points in the space forms an isosceles triangle, so the whole space is an isosceles set.\n\nIn the following, the concept and notation of an (open) ball is the same as in the article about metric spaces, i.e.\n\nProving these statements is an instructive exercise. All directly derive from the ultrametric triangle inequality. Note that, by the second statement, a ball may have several center points that have non-zero distance. The intuition behind such seemingly strange effects is that, due to the strong triangle inequality, distances in ultrametrics do not add up.\n\n\n"}
{"id": "24438021", "url": "https://en.wikipedia.org/wiki?curid=24438021", "title": "Urquhart graph", "text": "Urquhart graph\n\nIn computational geometry, the Urquhart graph of a set of points in the plane, named after Roderick B. Urquhart, is obtained by removing the longest edge from each triangle in the Delaunay triangulation.\n\nThe Urquhart graph was described by , who suggested that removing the longest edge from each Delaunay triangle would be a fast way of constructing the relative neighborhood graph (the graph connecting pairs of points \"p\" and \"q\" when there does not exist any third point \"r\" that is closer to both \"p\" and \"q\" than they are to each other). Since Delaunay triangulations can be constructed in time O(\"n\" log \"n\"), the same time bound holds for the Urquhart graph as well. Although it was later shown that the Urquhart graph is not exactly the same as the relative neighborhood graph, it can be used as a good approximation to it. The problem of constructing relative neighborhood graphs in O(\"n\" log \"n\") time, left open by the mismatch between the Urquhart graph and the relative neighborhood graph, was solved by .\n\nLike the relative neighborhood graph, the Urquhart graph of a set of points in general position contains the Euclidean minimum spanning tree of its points, from which it follows that it is a connected graph.\n"}
{"id": "48578990", "url": "https://en.wikipedia.org/wiki?curid=48578990", "title": "Vyjayanthi Chari", "text": "Vyjayanthi Chari\n\nVyjayanthi Chari (born 1958) is an Indian–American professor of mathematics at the University of California, Riverside, known for her research in representation theory and quantum algebra.\n\nChari has a bachelor's, master's, and doctoral degree from the University of Mumbai. Her dissertation was supervised by Rajagopalan Parthasarathy.\n\nWith Andrew N. Pressley, she is the author of the book \"A Guide to Quantum Groups\" (Cambridge University Press, 1994).\n\nIn 2015 she was elected as a fellow of the American Mathematical Society.\n\n"}
{"id": "34535", "url": "https://en.wikipedia.org/wiki?curid=34535", "title": "Zeno's paradoxes", "text": "Zeno's paradoxes\n\nZeno's paradoxes are a set of philosophical problems generally thought to have been devised by Greek philosopher Zeno of Elea (c. 490–430 BC) to support Parmenides' doctrine that contrary to the evidence of one's senses, the belief in plurality and change is mistaken, and in particular that motion is nothing but an illusion. It is usually assumed, based on Plato's \"Parmenides\" (128a–d), that Zeno took on the project of creating these paradoxes because other philosophers had created paradoxes against Parmenides' view. Thus Plato has Zeno say the purpose of the paradoxes \"is to show that their hypothesis that existences are many, if properly followed up, leads to still more absurd results than the hypothesis that they are one.\" Plato has Socrates claim that Zeno and Parmenides were essentially arguing exactly the same point.\n\nSome of Zeno's nine surviving paradoxes (preserved in Aristotle's \"Physics\"\nand Simplicius's commentary thereon) are essentially equivalent to one another. Aristotle offered a refutation of some of them. Three of the strongest and most famous—that of Achilles and the tortoise, the Dichotomy argument, and that of an arrow in flight—are presented in detail below.\n\nZeno's arguments are perhaps the first examples of a method of proof called \"reductio ad absurdum\" also known as proof by contradiction. They are also credited as a source of the dialectic method used by Socrates.\n\nSome mathematicians and historians, such as Carl Boyer, hold that Zeno's paradoxes are simply mathematical problems, for which modern calculus provides a mathematical solution.\nSome philosophers, however, say that Zeno's paradoxes and their variations (see Thomson's lamp) remain relevant metaphysical problems.\n\nThe origins of the paradoxes are somewhat unclear. Diogenes Laertius, a fourth source for information about Zeno and his teachings, citing Favorinus, says that Zeno's teacher Parmenides was the first to introduce the Achilles and the tortoise paradox. But in a later passage, Laertius attributes the origin of the paradox to Zeno, explaining that Favorinus disagrees.\n\nIn the paradox of Achilles and the tortoise, Achilles is in a footrace with the tortoise. Achilles allows the tortoise a head start of 100 meters, for example. Supposing that each racer starts running at some constant speed (one very fast and one very slow), then after some finite time, Achilles will have run 100 meters, bringing him to the tortoise's starting point. During this time, the tortoise has run a much shorter distance, say, 10 meters. It will then take Achilles some further time to run that distance, by which time the tortoise will have advanced farther; and then more time still to reach this third point, while the tortoise moves ahead. Thus, whenever Achilles arrives somewhere the tortoise has been, he still has some distance to go before he can even reach the tortoise.\n\nSuppose Homer wishes to walk to the end of a path. Before he can get there, he must get halfway there. Before he can get halfway there, he must get a quarter of the way there. Before traveling a quarter, he must travel one-eighth; before an eighth, one-sixteenth; and so on.\n\nThe resulting sequence can be represented as:\n\nThis description requires one to complete an infinite number of tasks, which Zeno maintains is an impossibility.\n\nThis sequence also presents a second problem in that it contains no first distance to run, for any possible (finite) first distance could be divided in half, and hence would not be first after all. Hence, the trip cannot even begin. The paradoxical conclusion then would be that travel over any finite distance can neither be completed nor begun, and so all motion must be an illusion. An alternative conclusion, proposed by Henri Bergson, is that motion (time and distance) is not actually divisible.\n\nThis argument is called the \"Dichotomy\" because it involves repeatedly splitting a distance into two parts. It contains some of the same elements as the \"Achilles and the Tortoise\" paradox, but with a more apparent conclusion of motionlessness. It is also known as the Race Course paradox. Some, like Aristotle, regard the Dichotomy as really just another version of \"Achilles and the Tortoise\".\n\nIn the arrow paradox (also known as the fletcher's paradox), Zeno states that for motion to occur, an object must change the position which it occupies. He gives an example of an arrow in flight. He states that in any one (duration-less) instant of time, the arrow is neither moving to where it is, nor to where it is not.\nIt cannot move to where it is not, because no time elapses for it to move there; it cannot move to where it is, because it is already there. In other words, at every instant of time there is no motion occurring. If everything is motionless at every instant, and time is entirely composed of instants, then motion is impossible.\n\nWhereas the first two paradoxes divide space, this paradox starts by dividing time—and not into segments, but into points.\n\nFrom Aristotle:\nDescription of the paradox from the \"Routledge Dictionary of Philosophy\":\nAristotle's refutation:\nDescription from Nick Huggett:\nFrom Aristotle:\nFor an expanded account of Zeno's arguments as presented by Aristotle, see Simplicius' commentary \"On Aristotle's Physics\".\n\nAccording to Simplicius, Diogenes the Cynic said nothing upon hearing Zeno's arguments, but stood up and walked, in order to demonstrate the falsity of Zeno's conclusions (see \"solvitur ambulando\"). To fully solve any of the paradoxes, however, one needs to show what is wrong with the argument, not just the conclusions. Through history, several solutions have been proposed, among the earliest recorded being those of Aristotle and Archimedes.\n\nAristotle (384 BC−322 BC) remarked that as the distance decreases, the time needed to cover those distances also decreases, so that the time needed also becomes increasingly small.\nAristotle also distinguished \"things infinite in respect of divisibility\" (such as a unit of space that can be mentally divided into ever smaller units while remaining spatially the same) from things (or distances) that are infinite in extension (\"with respect to their extremities\").\nAristotle's objection to the arrow paradox was that \"Time is not composed of indivisible nows any more than any other magnitude is composed of indivisibles.\"\n\nThomas Aquinas, commenting on Aristotle's objection, wrote \"Instants are not parts of time, for time is not made up of instants any more than a magnitude is made of points, as we have already proved. Hence it does not follow that a thing is not in motion in a given time, just because it is not in motion in any instant of that time.\"\n\nBefore 212 BC, Archimedes had developed a method to derive a finite answer for the sum of infinitely many terms that get progressively smaller. (See: Geometric series, 1/4 + 1/16 + 1/64 + 1/256 + · · ·, \"The Quadrature of the Parabola\".) Modern calculus achieves the same result, using more rigorous methods (see convergent series, where the \"reciprocals of powers of 2\" series, equivalent to the Dichotomy Paradox, is listed as convergent). These methods allow the construction of solutions based on the conditions stipulated by Zeno, i.e. the amount of time taken at each step is geometrically decreasing.\n\nBertrand Russell offered what is known as the \"at-at theory of motion\". It agrees that there can be no motion \"during\" a durationless instant, and contends that all that is required for motion is that the arrow be at one point at one time, at another point another time, and at appropriate points between those two points for intervening times. In this view motion is a function of position with respect to time.\n\nNick Huggett argues that Zeno is assuming the conclusion when he says that objects that occupy the same space as they do at rest must be at rest.\n\nPeter Lynds has argued that all of Zeno's motion paradoxes are resolved by the conclusion that instants in time and instantaneous magnitudes do not physically exist.\nLynds argues that an object in relative motion cannot have an instantaneous or determined relative position (for if it did, it could not be in motion), and so cannot have its motion fractionally dissected as if it does, as is assumed by the paradoxes. For more about the inability to know both speed and location, see Heisenberg uncertainty principle.\n\nAnother proposed solution is to question one of the assumptions Zeno used in his paradoxes (particularly the Dichotomy), which is that between any two different points in space (or time), there is always another point. Without this assumption there are only a finite number of distances between two points, hence there is no infinite sequence of movements, and the paradox is resolved. The ideas of Planck length and Planck time in modern physics place a limit on the measurement of time and space, if not on time and space themselves. According to Hermann Weyl, the assumption that space is made of finite and discrete units is subject to a further problem, given by the \"tile argument\" or \"distance function problem\".\nAccording to this, the length of the hypotenuse of a right angled triangle in discretized space is always equal to the length of one of the two sides, in contradiction to geometry. Jean Paul Van Bendegem has argued that the Tile Argument can be resolved, and that discretization can therefore remove the paradox.\nInfinite processes remained theoretically troublesome in mathematics until the late 19th century. The epsilon-delta version of Weierstrass and Cauchy developed a rigorous formulation of the logic and calculus involved. These works resolved the mathematics involving infinite processes.\n\nWhile mathematics can calculate where and when the moving Achilles will overtake the Tortoise of Zeno's paradox, philosophers such as Brown and Moorcroft\nclaim that mathematics does not address the central point in Zeno's argument, and that solving the mathematical issues does not solve every issue the paradoxes raise.\n\nPopular literature often misrepresents Zeno's arguments. For example, Zeno is often said to have argued that the sum of an infinite number of terms must itself be infinite–with the result that not only the time, but also the distance to be travelled, become infinite. A humorous take is offered by Tom Stoppard in his play Jumpers (1972), in which the principal protagonist, the philosophy professor George Moore, suggests that according to Zeno’s paradox, Saint Sebastian, a 3rd Century Christian saint supposedly martyred by being shot with arrows, died of fright. However, none of the original ancient sources has Zeno discussing the sum of any infinite series. Simplicius has Zeno saying \"it is impossible to traverse an infinite number of things in a finite time\". This presents Zeno's problem not with finding the \"sum\", but rather with \"finishing\" a task with an infinite number of steps: how can one ever get from A to B, if an infinite number of (non-instantaneous) events can be identified that need to precede the arrival at B, and one cannot reach even the beginning of a \"last event\"?\n\nDebate continues on the question of whether or not Zeno's paradoxes have been resolved. In \"The History of Mathematics: An Introduction\" (2010) Burton writes, \"Although Zeno's argument confounded his contemporaries, a satisfactory explanation incorporates a now-familiar idea, the notion of a 'convergent infinite series.'\".\n\nBertrand Russell offered a \"solution\" to the paradoxes based on the work of Georg Cantor, but Brown concludes \"Given the history of 'final resolutions', from Aristotle onwards, it's probably foolhardy to think we've reached the end. It may be that Zeno's arguments on motion, because of their simplicity and universality, will always serve as a kind of 'Rorschach image' onto which people can project their most fundamental phenomenological concerns (if they have any).\"\n\nAncient Han Chinese philosophers from the Mohist School of Names during the Warring States period of China (479-221 BCE) independently developed equivalents to some of Zeno's paradoxes. The scientist and historian Sir Joseph Needham, in his \"Science and Civilisation in China\", describes an ancient Chinese paradox from the surviving Mohist School of Names book of logic which states, in the archaic ancient Chinese script, \"a one-foot stick, every day take away half of it, in a myriad ages it will not be exhausted.\" Several other paradoxes from this philosophical school (more precisely, movement) are known, but their modern interpretation is more speculative.\n\nIn 1977, physicists E. C. George Sudarshan and B. Misra discovered that the dynamical evolution (motion) of a quantum system can be hindered (or even inhibited) through observation of the system. This effect is usually called the \"quantum Zeno effect\" as it is strongly reminiscent of Zeno's arrow paradox. This effect was first theorized in 1958.\n\nIn the field of verification and design of timed and hybrid systems, the system behaviour is called \"Zeno\" if it includes an infinite number of discrete steps in a finite amount of time. Some formal verification techniques exclude these behaviours from analysis, if they are not equivalent to non-Zeno behaviour.\n\nIn systems design these behaviours will also often be excluded from system models, since they cannot be implemented with a digital controller.\n\n\n"}
{"id": "7913127", "url": "https://en.wikipedia.org/wiki?curid=7913127", "title": "Zvi Galil", "text": "Zvi Galil\n\nZvi Galil (; born 1947) is an Israeli computer scientist and mathematician. He is the dean of the Georgia Institute of Technology College of Computing. His research interests include the design and analysis of algorithms, computational complexity and cryptography. He has been credited with coining the terms stringology and sparsification. He has published over 170 scientific papers and is listed as an ISI highly cited researcher.\n\nZvi Galil was born in Tel Aviv in 1947. He completed both his B.Sc. (1970) and his M.Sc. (1971) in Applied Mathematics at Tel Aviv University before earning his Ph.D. in Computer Science at Cornell in 1975 under the supervision of John Hopcroft. He then spent a year working as a post-doctorate researcher at IBM's Thomas J. Watson Research Center in Yorktown Heights, New York.\n\nFrom 1976 until 1995 he worked in the computer science department of Tel Aviv University, serving as its chair from 1979 to 1982. In 1982 he joined the faculty of Columbia University, serving as the chair of the Computer Science Department from 1989-1994. From 1995-2007, he served as the dean of the Fu Foundation School of Engineering & Applied Science. In this position he oversaw the renaming of the school in honor of Chinese businessman Z. Y. Fu after a large donation was given in his name. At Columbia, he was appointed the Julian Clarence Levi Professor of Mathematical Methods and Computer Science in 1987, and the Morris and Alma A. Schapiro Dean of Engineering in 1995.\n\nFrom 1983 to 1987, Galil served as the chairman of ACM SIGACT, an organization that promotes research in theoretical computer science.\n\nGalil served as the President of Tel Aviv University starting in 2007, but resigned and returned to the faculty in 2009. He was named as the dean of Georgia Tech's College of Computing on April 9, 2010. At Georgia Tech, together with Udacity founder Sebastian Thrun, Galil conceived of the College of Computing’s Online Master of Science in Computer Science (OMSCS) program, and he led the faculty creation of the program.\n\nGalil's research is in the areas of algorithms, particularly string and graph algorithms, complexity, cryptography and experimental design. Among his most highly cited work are the following:\n\nIn 1995, he was inducted as a Fellow of the Association for Computing Machinery, for \"fundamental contributions to the design and analysis of algorithms and outstanding service to the theoretical computer science community,\" and in 2004 he was elected to the National Academy of Engineering for \"contributions to the design and analysis of algorithms and for leadership in computer science and engineering.\" In 2005 he was selected as a Fellow of the American Academy of Arts and Sciences. In 2009 the Columbia Society of Graduates awarded him the Great Teacher Award. In 2012, The University of Waterloo awarded Galil with an honorary Doctor of Mathematics degree for his \"fundamental contributions in the areas of graph algorithms and string matching.\"\n\n"}
