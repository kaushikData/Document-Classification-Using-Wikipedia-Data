{"id": "399312", "url": "https://en.wikipedia.org/wiki?curid=399312", "title": "58 (number)", "text": "58 (number)\n\n58 (fifty-eight) is the natural number following 57 and preceding 59.\n\nFifty-eight is the sum of the first seven prime numbers, an 11-gonal number, and a Smith number.\nGiven 58, the Mertens function returns 0.\n\nThere is no solution to the equation \"x\" – φ(\"x\") = 58, making 58 a noncototient. However, the sum of the totient function for the first thirteen integers is 58.\n\n\n\n\nIn the NBA, the most points ever scored in a fourth quarter was 58 by the Buffalo Braves (at Boston Celtics), Oct. 20, 1972. The most points in a game by a rookie player: Wilt Chamberlain, 58: Philadelphia vs. Detroit, Jan. 25, 1960, and Philadelphia vs. New York Knicks, Feb. 21, 1960.\n\nIn MotoGP, 58 was the number of Marco Simoncelli who died in an accident at the Malaysian Round of the 2011 MotoGP season. MotoGP's governing body, the FIM, are considering to retire number 58 from use in MotoGP as they did before with the numbers 74 and 48 of Daijiro Kato and Shoya Tomizawa, respectively. The retirement, from all motorcycle racing classes, eventually occurred in 2016, joining Kato's 74, the 34 of inaugural MotoGP champion Kevin Schwantz and the 65 of Loris Capirossi.\n\nOn the PGA Tour, 58 is the lowest score in an 18 hole round, achieved by Jim Furyk in the final round of the 2016 Travelers Championship at TPC River Highlands.\n\nIn Formula One, 58 is the number of laps of the Australian Grand Prix since 1996, when the Grand Prix held in Albert Park.\n\nThe number 58 was commonly associated with misfortune in many civilizations native to either Central America or Southern America. Due to their beliefs in the original 58 sins, the number came to symbolize curses and ill-luck. Aztec oracles supposedly stumbled across the number an unnaturally high number of times before disaster fell. One famous recording of this, though largely discredited as mere folktale, concerned the oracle of Moctezuma II, who allegedly counted 58 pieces of gold scattered before a sacrificial pit the day prior to the arrival of Hernán Cortés.\n\n"}
{"id": "38449", "url": "https://en.wikipedia.org/wiki?curid=38449", "title": "Affine transformation", "text": "Affine transformation\n\nIn geometry, an affine transformation, affine map or an affinity (from the Latin, \"affinis\", \"connected with\") is a function between affine spaces which preserves points, straight lines and planes. Also, sets of parallel lines remain parallel after an affine transformation. An affine transformation does not necessarily preserve angles between lines or distances between points, though it does preserve ratios of distances between points lying on a straight line.\n\nExamples of affine transformations include translation, scaling, homothety, similarity transformation, reflection, rotation, shear mapping, and compositions of them in any combination and sequence.\n\nIf formula_1 and formula_2 are affine spaces, then every affine transformation formula_3 is of the form formula_4, where formula_5 is a linear transformation on the space formula_1, formula_7 is a vector in formula_1, and formula_9 is a vector in formula_2. Unlike a purely linear transformation, an affine map need not preserve the zero point in a linear space. Thus, every linear transformation is affine, but not every affine transformation is linear.\n\nAll Euclidean spaces are affine, but there are affine spaces that are non-Euclidean. In affine coordinates, which include Cartesian coordinates in Euclidean spaces, each output coordinate of an affine map is a linear function (in the sense of calculus) of all input coordinates. Another way to deal with affine transformations systematically is to select a point as the origin; then, any affine transformation is equivalent to a linear transformation (of position vectors) followed by a translation.\n\nAn affine map formula_11 between two affine spaces is a map on the points that acts linearly on the vectors (that is, the vectors between points of the space). In symbols, \"formula_12\" determines a linear transformation \"formula_13\" such that, for any pair of points formula_14:\n\nor\n\nWe can interpret this definition in a few other ways, as follows.\n\nIf an origin formula_17 is chosen, and formula_18 denotes its image formula_19, then this means that for any vector formula_20:\n\nIf an origin formula_22 is also chosen, this can be decomposed as an affine transformation formula_23 that sends formula_24, namely\n\nfollowed by the translation by a vector formula_26.\n\nThe conclusion is that, intuitively, formula_12 consists of a translation and a linear map.\n\nGiven two affine spaces formula_28 and formula_29, over the same field, a function formula_30 is an affine map if and only if for every family formula_31 of weighted points in formula_28 such that \n\nwe have\n\nIn other words, formula_12 preserves barycenters.\n\nAs shown above, an affine map is the composition of two functions: a translation and a linear map. Ordinary vector algebra uses matrix multiplication to represent linear maps, and vector addition to represent translations. Formally, in the finite-dimensional case, if the linear map is represented as a multiplication by a matrix formula_36 and the translation as the addition of a vector formula_37, an affine map formula_12 acting on a vector formula_20 can be represented as\n\nUsing an augmented matrix and an augmented vector, it is possible to represent both the translation and the linear map using a single matrix multiplication. The technique requires that all vectors are augmented with a \"1\" at the end, and all matrices are augmented with an extra row of zeros at the bottom, an extra column—the translation vector—to the right, and a \"1\" in the lower right corner. If formula_36 is a matrix,\n\nis equivalent to the following\n\nThe above-mentioned augmented matrix is called an \"affine transformation matrix\", or \"projective transformation matrix\" (as it can also be used to perform projective transformations).\n\nThis representation exhibits the set of all invertible affine transformations as the semidirect product of formula_44 and formula_45. This is a group under the operation of composition of functions, called the affine group.\n\nOrdinary matrix-vector multiplication always maps the origin to the origin, and could therefore never represent a translation, in which the origin must necessarily be mapped to some other point. By appending the additional coordinate \"1\" to every vector, one essentially considers the space to be mapped as a subset of a space with an additional dimension. In that space, the original space occupies the subset in which the additional coordinate is 1. Thus the origin of the original space can be found at formula_46. A translation within the original space by means of a linear transformation of the higher-dimensional space is then possible (specifically, a shear transformation). The coordinates in the higher-dimensional space are an example of homogeneous coordinates. If the original space is Euclidean, the higher dimensional space is a real projective space.\n\nThe advantage of using homogeneous coordinates is that one can combine any number of affine transformations into one by multiplying the respective matrices. This property is used extensively in computer graphics, computer vision and robotics.\n\nIf the vectors formula_47 are a basis of the domain's projective vector space and if formula_48 are the corresponding vectors in the codomain vector space then the augmented matrix formula_5 that achieves this affine transformation\nis\n\nThis formulation works irrespective of whether any of the domain, codomain and image vector spaces have the same number of dimensions.\n\nFor example, the affine transformation of a vector plane is uniquely determined from the knowledge of where the three vertices of a non-degenerate triangle are mapped to.\n\nAn affine transformation preserves:\n\nAn affine transformation is invertible if and only if formula_36 is invertible. In the matrix representation, the inverse is:\n\nThe invertible affine transformations (of an affine space onto itself) form the affine group, which has the general linear group of degree formula_61 as subgroup and is itself a subgroup of the general linear group of degree formula_62.\n\nThe similarity transformations form the subgroup where formula_36 is a scalar times an orthogonal matrix. For example, if the affine transformation acts on the plane and if the determinant of formula_36 is 1 or −1 then the transformation is an equiareal mapping. Such transformations form a subgroup called the \"equi-affine group\". A transformation that is both equi-affine and a similarity is an isometry of the plane taken with Euclidean distance.\n\nEach of these groups has a subgroup of \"orientation-preserving\" or \"positive\" affine transformations: those where the determinant of formula_36 is positive. In the last case this is in 3D the group of rigid body motions (proper rotations and pure translations).\n\nIf there is a fixed point, we can take that as the origin, and the affine transformation reduces to a linear transformation. This may make it easier to classify and understand the transformation. For example, describing a transformation as a rotation by a certain angle with respect to a certain axis may give a clearer idea of the overall behavior of the transformation than describing it as a combination of a translation and a rotation. However, this depends on application and context.\n\nIn their applications to digital image processing, the affine transformations are analogous to printing on a sheet of rubber and stretching the sheet's edges parallel to the plane. This transform relocates pixels requiring intensity interpolation to approximate the value of moved pixels, bicubic interpolation is the standard for image transformations in image processing applications. Affine transformations scale, rotate, translate, mirror and shear images as shown in the following examples: \n\nThe affine transforms are applicable to the registration process where two or more images are aligned (registered). An example of image registration is the generation of panoramic images that are the product of multiple images stitched together.\n\nThe affine transform preserves parallel lines. However, the stretching and shearing transformations warp shapes, as the following example shows: \n\nThis is an example of image warping. However, the affine transformations do not facilitate projection onto a curved surface or radial distortions.\n\nAffine transformations in two real dimensions include:\n\nTo visualise the general affine transformation of the Euclidean plane, take labelled parallelograms \"ABCD\" and \"A′B′C′D′\". Whatever the choices of points, there is an affine transformation \"T\" of the plane taking \"A\" to \"A′\", and each vertex similarly. Supposing we exclude the degenerate case where \"ABCD\" has zero area, there is a unique such affine transformation \"T\". Drawing out a whole grid of parallelograms based on \"ABCD\", the image \"T\"(\"P\") of any point \"P\" is determined by noting that \"T\"(\"A\") = \"A′\", \"T\" applied to the line segment \"AB\" is \"A′B′\", \"T\" applied to the line segment \"AC\" is \"A′C′\", and \"T\" respects scalar multiples of vectors based at \"A\". [If \"A\", \"E\", \"F\" are collinear then the ratio length(\"AF\")/length(\"AE\") is equal to length(\"A\"′\"F\"′)/length(\"A\"′\"E\"′).] Geometrically \"T\" transforms the grid based on \"ABCD\" to that based in \"A′B′C′D′\".\n\nAffine transformations do not respect lengths or angles; they multiply area by a constant factor\n\nA given \"T\" may either be \"direct\" (respect orientation), or \"indirect\" (reverse orientation), and this may be determined by its effect on \"signed\" areas (as defined, for example, by the cross product of vectors).\n\nFunctions formula_66 with formula_67 and formula_68 constant, are commonplace affine transformations.\n\nThe following equation expresses an affine transformation in GF(2):\n\nFor instance, the affine transformation of the element formula_70 in big-endian binary notation = formula_71 in big-endian hexadecimal notation, is calculated as follows:\n\nThus, formula_80.\n\nIn ℝ, the transformation shown at left is accomplished using the map given by:\n\nTransforming the three corner points of the original triangle (in red) gives three new points which form the new triangle (in blue). This transformation skews and translates the original triangle.\n\nIn fact, all triangles are related to one another by affine transformations. This is also true for all parallelograms, but not for all quadrilaterals.\n\n\n"}
{"id": "8532185", "url": "https://en.wikipedia.org/wiki?curid=8532185", "title": "Automath", "text": "Automath\n\nAutomath (\"automating mathematics\") was a formal language, devised by Nicolaas Govert de Bruijn starting in 1967, for expressing complete mathematical theories in such a way that an included automated proof checker can verify their correctness.\n\nThe Automath system included many novel notions that were later adopted and/or reinvented in areas such as typed lambda calculus and explicit substitution. Dependent types is one outstanding example. Automath was also the first practical system that exploited the Curry–Howard correspondence. Propositions were represented as sets (called \"categories\") of their proofs, and the question of provability became a question of non-emptiness (type inhabitation); de Bruijn was unaware of Howard's work, and stated the correspondence independently. \n\nL. S. van Benthem Jutting, as part of this Ph.D. thesis in 1976, translated Edmund Landau's \"Foundations of Analysis\" into Automath and checked its correctness.\nAutomath was never widely publicized at the time, however, and so never achieved widespread use; nonetheless, it proved very influential in the later development of logical frameworks and proof assistants. The Mizar system, a system of writing and checking formalized mathematics that is still in active use, was influenced by Automath.\n\n\n"}
{"id": "19624442", "url": "https://en.wikipedia.org/wiki?curid=19624442", "title": "Benjamin Cowie", "text": "Benjamin Cowie\n\nBenjamin Morgan Cowie was Dean of Manchester and then Exeter in the last quarter of the 19th century.\n\nBorn on 8 June 1816, he was educated at St John's College, Cambridge and graduated Senior Wrangler in 1839. Ordained in 1841 he was successively Tutor, Lecturer and Fellow at his old college. Afterwards he was Vicar of St Lawrence Jewry followed by an 11-year spell in Manchester, followed by a further 17 at Exeter. He died on 3 May 1900.\n"}
{"id": "382805", "url": "https://en.wikipedia.org/wiki?curid=382805", "title": "Cantor–Bernstein theorem", "text": "Cantor–Bernstein theorem\n\nIn set theory and order theory, the Cantor–Bernstein theorem states that the cardinality of the second type class, the class of countable order types, equals the cardinality of the continuum. It was used by Felix Hausdorff and named by him after Georg Cantor and Felix Bernstein. Cantor constructed a family of countable order types with the cardinality of the continuum, and in his 1901 inaugural dissertation Bernstein proved that such a family can have no higher cardinality.\n\nBecause the second type class contains the countable ordinal numbers, which have cardinality formula_1, this result proves (by an inclusion of naturally defined sets) that formula_2, a relation between these two aleph numbers that (without assuming the axiom of choice) was not previously known.\n"}
{"id": "30667652", "url": "https://en.wikipedia.org/wiki?curid=30667652", "title": "Chinese Annals of Mathematics, Series B", "text": "Chinese Annals of Mathematics, Series B\n\nThe journal was founded in 1983 when it was split from \"Chinese Annals of Mathematics\". It is indexed by \"Mathematical Reviews\" and Zentralblatt MATH.\nThe journal's 2009 MCQ was 0.39. According to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 0.362.\n"}
{"id": "1401020", "url": "https://en.wikipedia.org/wiki?curid=1401020", "title": "Christoffel symbols", "text": "Christoffel symbols\n\nIn mathematics and physics, the Christoffel symbols are an array of numbers describing a metric connection. The metric connection is a specialization of the affine connection to surfaces or other manifolds endowed with a metric, allowing distances to be measured on that surface. In differential geometry, an affine connection can be defined without any reference to a metric, and many additional concepts follow: parallel transport, covariant derivatives, geodesics, etc. also do not require the concept of a metric. However, when a metric is available, these concepts can be directly tied to the \"shape\" of the manifold itself; that shape is determined by how the tangent space is attached to the cotangent space by the metric tensor. Abstractly, one would say that the manifold has an associated (orthonormal) frame bundle, with each \"frame\" being a possible choice of a coordinate frame. An invariant metric implies that the structure group of the frame bundle is the orthogonal group . As a result, such a manifold is necessarily a (pseudo-)Riemannian manifold. The Christoffel symbols provide a concrete representation of the connection of (pseudo-)Riemannian geometry in terms of coordinates on the manifold. Additional concepts, such as parallel transport, geodesics, etc. can then be expressed in terms of Christoffel symbols.\n\nIn general, there are an infinite number of metric connections for a given metric tensor; however, there is one, unique connection, the Levi-Civita connection, that is free of any torsion. It is very common in physics and general relativity to work almost exclusively with the Levi-Civita connection, by working in coordinate frames (called holonomic coordinates) where the torsion vanishes. For example, in Euclidean spaces, the Christoffel symbols describe how the local coordinate bases change from point to point.\n\nAt each point of the underlying -dimensional manifold, for any local coordinate system around that point, the Christoffel symbols are denoted for . Each entry of this array is a real number. Under \"linear\" coordinate transformations on the manifold, the Christoffel symbols transform like the components of a tensor, but under general coordinate transformations (diffeomorphisms) they do not. Most of the algebraic properties of the Christoffel symbols follow from their relationship to the affine connection; only a few follow from the fact that the structure group is the orthogonal group (or the Lorentz group for general relativity).\n\nChristoffel symbols are used for performing practical calculations. For example, the Riemann curvature tensor can be expressed entirely in terms of the Christoffel symbols and their first partial derivatives. In general relativity, the connection plays the role of the gravitational force field with the corresponding gravitational potential being the metric tensor. When the coordinate system and the metric tensor share some symmetry, many of the are zero.\n\nThe Christoffel symbols are named for Elwin Bruno Christoffel (1829–1900).\n\nThe definitions given below are valid for both Riemannian manifolds and pseudo-Riemannian manifolds, such as those of general relativity, with careful distinction being made between upper and lower indices (contra-variant and co-variant indices). The formulas hold for either sign convention, unless otherwise noted.\n\nEinstein summation convention is used in this article, with vectors indicated by bold font. The connection coefficients of the Levi-Civita connection (or pseudo-Riemannian connection) expressed in a coordinate basis are called \"Christoffel symbols\".\n\nGiven a coordinate system for on an -manifold , the tangent vectors\nwhere is the position vector, define what is referred to as the local basis of the tangent space to at each point of its domain. These can be used to define the metric tensor:\n\nand its inverse:\n\nwhich can in turn be used to define the dual basis:\n\nIn Euclidean space, the general definition given below for the Christoffel symbols of the second kind can be proven to be equivalent to:\n\nChristoffel symbols of the first kind can then be found via index juggling:\n\nRearranging, we see that:\n\nIn words, the arrays represented by the Christoffel symbols track how the basis changes from point to point. Symbols of the second kind decompose the change with respect to the basis, while symbols of the first kind decompose it with respect to the dual basis. These expressions fail as definitions when such decompositions are not possible - in particular, when the direction of change does not lie in the tangent space, which can occur on a curved surface. In this form, it easy to see the symmetry of the lower or last two indices:\n\nformula_8 and formula_9,\n\nfrom the definition of formula_10 and the fact that partial derivatives commute (as long as the manifold and coordinate system are well behaved).\n\nThe same numerical values for Christoffel symbols of the second kind also relate to derivatives of the dual basis, as seen in the expression:\n\nwhich we can rearrange as:\n\nThe Christoffel symbols of the first kind can be derived either from the Christoffel symbols of the second kind and the metric,\nor from the metric alone,\n\nAs an alternative notation one also finds\n\nIt is worth noting that .\n\nThe Christoffel symbols of the second kind are the connection coefficients—in a coordinate basis—of the Levi-Civita connection, and since this connection has zero torsion, then in this basis the connection coefficients are symmetric, i.e., . For this reason, a torsion-free connection is often called \"symmetric\".\n\nIn other words, the Christoffel symbols of the second kind\nholds, where is the Levi-Civita connection on taken in the coordinate direction (i.e., ) and where is a local coordinate (holonomic) basis.\n\nThe Christoffel symbols can be derived from the vanishing of the covariant derivative of the metric tensor :\n\nAs a shorthand notation, the nabla symbol and the partial derivative symbols are frequently dropped, and instead a semicolon and a comma are used to set off the index that is being used for the derivative. Thus, the above is sometimes written as\n\nUsing that the symbols are symmetric in the lower two indices, one can solve explicitly for the Christoffel symbols as a function of the metric tensor by permuting the indices and resumming:\n\nwhere is the inverse of the matrix , defined as (using the Kronecker delta, and Einstein notation for summation) . Although the Christoffel symbols are written in the same notation as tensors with index notation, they are not tensors,\nsince they do not transform like tensors under a change of coordinates.\n\nThe Christoffel symbols are most typically defined in a coordinate basis, which is the convention followed here. In other words, the name Christoffel symbols is reserved only for coordinate (i.e., holonomic) frames. However, the connection coefficients can also be defined in an arbitrary (i.e., nonholonomic) basis of tangent vectors by\nExplicitly, in terms of the metric tensor, this is\n\nwhere are the commutation coefficients of the basis; that is,\nwhere are the basis vectors and is the Lie bracket. The standard unit vectors in spherical and cylindrical coordinates furnish an example of a basis with non-vanishing commutation coefficients. The difference between the connection in such a frame, and the Levi-Civita connection is known as the contorsion tensor.\n\nWhen we choose the basis orthonormal: then . This implies that\nand the connection coefficients become antisymmetric in the first two indices:\nwhere\n\nIn this case, the connection coefficients are called the Ricci rotation coefficients.\n\nEquivalently, one can define Ricci rotation coefficients as follows:\nwhere is an orthonormal nonholonomic basis and its \"co-basis\".\n\nLet and be vector fields with components and . Then the th component of the covariant derivative of with respect to is given by\n\nHere, the Einstein notation is used, so repeated indices indicate summation over indices and contraction with the metric tensor serves to raise and lower indices:\n\nKeep in mind that and that , the Kronecker delta. The convention is that the metric tensor is the one with the lower indices; the correct way to obtain from is to solve the linear equations .\n\nThe statement that the connection is torsion-free, namely that\nis equivalent to the statement that—in a coordinate basis—the Christoffel symbol is symmetric in the lower two indices:\n\nThe index-less transformation properties of a tensor are given by pullbacks for covariant indices, and pushforwards for contravariant indices. The article on covariant derivatives provides additional discussion of the correspondence between index-free notation and indexed notation.\n\nThe covariant derivative of a vector field is\n\nThe covariant derivative of a scalar field is just\n\nand the covariant derivative of a covector field is\n\nThe symmetry of the Christoffel symbol now implies\n\nfor any scalar field, but in general the covariant derivatives of higher order tensor fields do not commute (see curvature tensor).\n\nThe covariant derivative of a type (2,0) tensor field is\n\nthat is,\n\nIf the tensor field is mixed then its covariant derivative is\n\nand if the tensor field is of type then its covariant derivative is\n\nTo find the contravariant derivative of a vector field, we must first transform \nit into a covariant derivative using the metric tensor\n\nUnder a change of variable from to , vectors transform as\n\nand so\n\nwhere the overline denotes the Christoffel symbols in the coordinate system. Note that the Christoffel symbol does not transform as a tensor, but rather as an object in the jet bundle. More precisely, the Christoffel symbols can be considered as functions on the jet bundle of the frame bundle of , independent of any local coordinate system. Choosing a local coordinate system determines a local section of this bundle, which can then be used to pull back the Christoffel symbols to functions on , though of course these functions then depend on the choice of local coordinate system.\n\nAt each point, there exist coordinate systems in which the Christoffel symbols vanish at the point. These are called (geodesic) normal coordinates, and are often used in Riemannian geometry.\n\nThe Christoffel symbols find frequent use in Einstein's theory of general relativity, where spacetime is represented by a curved 4-dimensional Lorentz manifold with a Levi-Civita connection. The Einstein field equations—which determine the geometry of spacetime in the presence of matter—contain the Ricci tensor, and so calculating the Christoffel symbols is essential. Once the geometry is determined, the paths of particles and light beams are calculated by solving the geodesic equations in which the Christoffel symbols explicitly appear.\n\n\n"}
{"id": "3367262", "url": "https://en.wikipedia.org/wiki?curid=3367262", "title": "Circular convolution", "text": "Circular convolution\n\nThe circular convolution, also known as cyclic convolution, of two aperiodic functions (i.e. Schwartz functions) occurs when one of them is convolved in the normal way with a periodic summation of the other function. That situation arises in the context of the circular convolution theorem. The identical operation can also be expressed in terms of the periodic summations of both functions, if the infinite integration interval is reduced to just one period.  That situation arises in the context of the discrete-time Fourier transform (DTFT) and is also called periodic convolution.  In particular, the DTFT of the product of two discrete sequences is the periodic convolution of the DTFTs of the individual sequences.\n\nLet \"x\" be a function with a well-defined periodic summation, \"x\", where:\n\nIf \"h\" is any other function for which the convolution \"x\" ∗ \"h\" exists, then the convolution \"x\" ∗ \"h\" is periodic and identical to:\n\nwhere \"t\" is an arbitrary parameter and \"h\" is a periodic summation of \"h\".\n\nThe second integral is called the periodic convolution of functions \"x\" and \"h\" and is sometimes normalized by 1/\"T\". When \"x\" is expressed as the periodic summation of another function, \"x\", the same operation may also be referred to as a circular convolution of functions \"h\" and \"x\".\n\nSimilarly, for discrete sequences and period N, we can write the circular convolution of functions \"h\" and \"x\" as:\n\nFor the special case that the non-zero extent of both \"x\" and \"h\" are \"≤ N\", this is reducible to matrix multiplication where the kernel of the integral transform is a circulant matrix.\n\nA case of great practical interest is illustrated in the figure. The duration of the x sequence is N (or less), and the duration of the h sequence is significantly less. Then many of the values of the circular convolution are identical to values of x∗h,  which is actually the desired result when the h sequence is a finite impulse response (FIR) filter. Furthermore, the circular convolution is very efficient to compute, using a fast Fourier transform (FFT) algorithm and the circular convolution theorem.\n\nThere are also methods for dealing with an x sequence that is longer than a practical value for N. The sequence is divided into segments (\"blocks\") and processed piecewise. Then the filtered segments are carefully pieced back together. Edge effects are eliminated by overlapping either the input blocks or the output blocks. To help explain and compare the methods, we discuss them both in the context of an h sequence of length 201 and an FFT size of \"N\" = 1024.\n\nOverlapping input blocks\n\nThis method uses a block size equal to the FFT size (1024). We describe it first in terms of normal or \"linear\" convolution. When a normal convolution is performed on each block, there are start-up and decay transients at the block edges, due to the filter \"latency\" (200-samples). Only 824 of the convolution outputs are unaffected by edge effects. The others are discarded, or simply not computed. That would cause gaps in the output if the input blocks are contiguous. The gaps are avoided by overlapping the input blocks by 200 samples. In a sense, 200 elements from each input block are \"saved\" and carried over to the next block. This method is referred to as overlap-save, although the method we describe next requires a similar \"save\" with the output samples.\n\nWhen an FFT is used to compute the 824 unaffected DFT samples, we don't have the option of not computing the affected samples, but the leading and trailing edge-effects are overlapped and added because of circular convolution. Consequently, the 1024-point inverse FFT (IFFT) output contains only 200 samples of edge effects (which are discarded) and the 824 unaffected samples (which are kept). To illustrate this, the fourth frame of the figure at right depicts a block that has been periodically (or \"circularly\") extended, and the fifth frame depicts the individual components of a linear convolution performed on the entire sequence. The edge effects are where the contributions from the extended blocks overlap the contributions from the original block. The last frame is the composite output, and the section colored green represents the unaffected portion.\n\nOverlapping output blocks\n\nThis method is known as overlap-add. In our example, it uses contiguous input blocks of size 824 and pads each one with 200 zero-valued samples. Then it overlaps and adds the 1024-element output blocks. Nothing is discarded, but 200 values of each output block must be \"saved\" for the addition with the next block. Both methods advance only 824 samples per 1024-point IFFT, but overlap-save avoids the initial zero-padding and final addition.\n\n\n"}
{"id": "8536059", "url": "https://en.wikipedia.org/wiki?curid=8536059", "title": "Clutching construction", "text": "Clutching construction\n\nIn topology, a branch of mathematics, the clutching construction is a way of constructing fiber bundles, particularly vector bundles on spheres.\n\nConsider the sphere formula_1 as the union of the upper and lower hemispheres formula_2 and formula_3 along their intersection, the equator, an formula_4.\n\nGiven trivialized fiber bundles with fiber formula_5 and structure group formula_6 over the two disks, then given a map formula_7 (called the \"clutching map\"), glue the two trivial bundles together via \"f\".\n\nFormally, it is the coequalizer of the inclusions formula_8 via formula_9 and formula_10: glue the two bundles together on the boundary, with a twist.\n\nThus we have a map formula_11: clutching information on the equator yields a fiber bundle on the total space.\n\nIn the case of vector bundles, this yields formula_12, and indeed this map is an isomorphism (under connect sum of spheres on the right).\n\nThe above can be generalized by replacing the disks and sphere with any closed triad formula_13, that is, a space \"X\", together with two closed subsets \"A\" and \"B\" whose union is \"X\". Then a clutching map on formula_14 gives a vector bundle on \"X\".\n\nLet formula_15 be a fibre bundle with fibre formula_5. Let formula_17 be a collection of pairs formula_18 such that formula_19 is a local trivialization of formula_20 over formula_21. Moreover, we demand that the union of all the sets formula_22 is formula_23 (i.e. the collection is an atlas of trivializations formula_24). \n\nConsider the space formula_25 modulo the equivalence relation formula_26 is equivalent to formula_27 if and only if formula_28 and formula_29. By design, the local trivializations formula_30 give a fibrewise equivalence between this quotient space and the fibre bundle formula_20.\n\nConsider the space formula_32 modulo the equivalence relation formula_33 is equivalent to formula_34 if and only if formula_28 and consider formula_36 to be a map formula_37 then we demand that formula_38. That is, in our re-construction of formula_20 we are replacing the fibre formula_5 by the topological group of homeomorphisms of the fibre, formula_41. If the structure group of the bundle is known to reduce, you could replace formula_41 with the reduced structure group. This is a bundle over formula_43 with fibre formula_41 and is a principal bundle. Denote it by formula_45. The relation to the previous bundle is induced from the principal bundle: formula_46. \n\nSo we have a principal bundle formula_47. The theory of classifying spaces gives us an induced push-forward fibration formula_48 where formula_49 is the classifying space of formula_41. Here is an outline:\n\nGiven a formula_6-principal bundle formula_52, consider the space formula_53. This space is a fibration in two different ways:\n\n1) Project onto the first factor: formula_54. The fibre in this case is formula_55, which is a contractible space by the definition of a classifying space. \n\n2) Project onto the second factor: formula_56. The fibre in this case is formula_57. \n\nThus we have a fibration formula_58. This map is called the classifying map of the fibre bundle formula_15 since 1) the principal bundle formula_52 is the pull-back of the bundle formula_61 along the classifying map and 2) The bundle formula_20 is induced from the principal bundle as above.\n\nTwisted spheres are sometimes referred to as a \"clutching-type\" construction, but this is misleading: the clutching construction is properly about fiber bundles.\n\n\n"}
{"id": "2702039", "url": "https://en.wikipedia.org/wiki?curid=2702039", "title": "Composition operator", "text": "Composition operator\n\nIn mathematics, the composition operator formula_1 with symbol formula_2 is a linear operator defined by the rule\n\nwhere formula_4 denotes function composition.\n\nThe study of composition operators is covered by AMS category 47B33.\n\nIn physics, and especially the area of dynamical systems, the composition operator is usually referred to as the Koopman operator (and its wild surge in popularity is sometimes jokingly called \"Koopmania\"), named after Bernard Koopman. It is the left-adjoint of the transfer operator of Frobenius–Perron.\n\nUsing the language of category theory, the composition operator is a pull-back on the space of measurable functions; it is adjoint to the transfer operator in the same way that the pull-back is adjoint to the push-forward; the composition operator is the inverse image functor.\n\nSince the domain considered here is that of Borel functions, the above describes the Koopman operator as it appears in Borel functional calculus.\n\nThe domain of a composition operator can be taken more narrowly, as some Banach space, often consisting of holomorphic functions: for example, some Hardy space or Bergman space. In this case, the composition operator lies in the realm of some functional calculus, such as the holomorphic functional calculus.\n\nInteresting questions posed in the study of composition operators often relate to how the spectral properties of the operator depend on the function space. Other questions include whether formula_1 is compact or trace-class; answers typically depend on how the function \"φ\" behaves on the boundary of some domain.\n\nWhen the transfer operator is a left-shift operator, the Koopman operator, as its adjoint, can be taken to be the right-shift operator. An appropriate basis, explicitly manifesting the shift, can often be found in the orthogonal polynomials. When these are orthogonal on the real number line, the shift is given by the Jacobi operator. When the polynomials are orthogonal on some region of the complex plane (viz, in Bergman space), the Jacobi operator is replaced by a Hessenberg operator\n\nIn mathematics, composition operators commonly occur in the study of shift operators, for example, in the Beurling–Lax theorem and the Wold decomposition. Shift operators can be studied as one-dimensional spin lattices. Composition operators appear in the theory of Aleksandrov–Clark measures.\n\nThe eigenvalue equation of the composition operator is Schröder's equation, and the principal eigenfunction \"f(x)\" is often called Schröder's function or Koenigs function.\n\n\n"}
{"id": "2815048", "url": "https://en.wikipedia.org/wiki?curid=2815048", "title": "Computational model", "text": "Computational model\n\nA computational model is a mathematical model in computational science that requires extensive computational resources to study the behavior of a complex system by computer simulation.\n\nThe system under study is often a complex nonlinear system for which simple, intuitive analytical solutions are not readily available. Rather than deriving a mathematical analytical solution to the problem, experimentation with the model is done by adjusting the parameters of the system in the computer, and studying the differences in the outcome of the experiments. Operation theories of the model can be derived/deduced from these computational experiments.\n\nExamples of common computational models are weather forecasting models, earth simulator models, flight simulator models, molecular protein folding models, and neural network models.\n\n"}
{"id": "7852887", "url": "https://en.wikipedia.org/wiki?curid=7852887", "title": "Cylindric algebra", "text": "Cylindric algebra\n\nThe notion of cylindric algebra, invented by Alfred Tarski, arises naturally in the algebraization of equational first-order logic. This is comparable to the role Boolean algebras play for propositional logic. Indeed, cylindric algebras are Boolean algebras equipped with additional cylindrification operations that model quantification and equality. They differ from polyadic algebras in that the latter do not model equality.\n\nA cylindric algebra of dimension formula_1 (where formula_1 is any ordinal number) is an algebraic structure formula_3 such that formula_4 is a Boolean algebra, formula_5 a unary operator on formula_6 for every formula_7 (called a \"cylindrification\"), and formula_8 a distinguished element of formula_6 for every formula_7 and formula_11 (called a \"diagonal\"), such that the following hold:\n\n(C1) formula_12\n\n(C2) formula_13\n\n(C3) formula_14\n\n(C4) formula_15\n\n(C5) formula_16\n\n(C6) If formula_17, then formula_18\n\n(C7) If formula_19, then formula_20\n\nAssuming a presentation of first-order logic without function symbols, \nthe operator formula_21 models existential quantification over variable formula_7 in formula formula_23 while the operator formula_8 models the equality of variables formula_7 and formula_11. Henceforth, reformulated using standard logical notations, the axioms read as\n\n(C1) formula_27\n\n(C2) formula_28\n\n(C3) formula_29\n\n(C4) formula_30\n\n(C5) formula_31\n\n(C6) If formula_7 is a variable different from both formula_11 and formula_34, then formula_35\n\n(C7) If formula_7 and formula_11 are different variables, then formula_38\n\nA cylindric set algebra of dimension formula_1 is an algebraic structure formula_40 such that formula_41 is a field of sets. Its axioms are the axioms C1–C7 of a cylindric algebra but with formula_42 instead of formula_43, formula_44 instead of formula_45, set complement for complement, empty set as 0, formula_46 as the unit, and formula_47 instead of formula_48. The set \"X\" is the domain of each of the variables and it is called the \"base\".\n\nAny cylindric algebra has a representation as a cylindric set algebra, due to Stone's representation theorem. It is easier to connect the semantics of first-order predicate logic with cylindric set algebra. (For more details, see the \"Further reading\" section.)\n\nCylindric algebras have been generalized to the case of many-sorted logic (Caleiro and Gonçalves 2006), which allows for a better modeling of the duality between first-order formulas and terms.\n\nWhen formula_49 and formula_50 are restricted to being only 0, then formula_5 becomes formula_52, the diagonals can be dropped out, and the following theorem of cylindric algebra (Pinter 1973):\nturns into the axiom\nof monadic Boolean algebra. The axiom (C4) drops out. Thus monadic Boolean algebra can be seen as a restriction of cylindric algebra to the one variable case.\n\n\n\n"}
{"id": "17349502", "url": "https://en.wikipedia.org/wiki?curid=17349502", "title": "Digital Morse theory", "text": "Digital Morse theory\n\nIn mathematics, Digital Morse theory is a digital adaptation of continuum Morse theory for scalar volume data. This is not about the Samuel Morse's Morse Code of long and short clicks or tones used in manual electric telegraphy. The term was first promulgated by DB Karron based on the work of JL Cox and DB Karron.\n\nThe main utility of a digital Morse theory is that it serves to provide a theoretical basis for isosurfaces (a kind of embedded manifold submanifold ), and perpendicular streamlines in a digital context. The intended main application of DMT is in the rapid semiautomatic segmentation objects such as organs and anatomic structures from stacks of medical images such as produced by Three Dimensional Computer Tomography by CT or MRI technology.\n\nA DMT Tree is a digital version of a Reeb graph or contour tree graph, showing the relationship and connectivity of one isovalued defined object to another. Typically, these are nested objects, one inside another, giving a parent-child relationship, or two objects standing alone with a peer relationship.\n\nThe essential insight of Morse theory can be given in a little parable.\n\nThe Fish Tank thought experiment: Counting islands as the water level changes\n\nThe essential insight of continuous Morse theory can be intuited by a thought experiment. Consider a rectangular glass fish tank. Into this tank, we pour a small quantity of sand such that we have two smoothly sloping small hills, one taller than the other. Now, we fill this tank to the brim with water. We now start a count of the number of island objects as we very slowly drain the tank.\n\nOur initial observation is that there are no island features in our tank scene. As the water level drops, we observe the water level just coincident with the peak of the tallest sand hill. \nWe next observe the behavior of the water at the critical peak of the hill. We see a degenerate point island contour, with zero area, zero perimeter, and infinite curvature. A vanishing small change in the water level and this point contour expand into a tiny island. \nWe now increment our island object count by +1. \nWe continue to drain water from the tank.\nWe next observe the creation of the second island at the peak of the second little hill. We again increment our island object count by +1 to two objects. Our little sea has two island objects in it. \nAs we continue to slowly lower the water level in our little tank sea.\nWe now observe the two island contours gradually expand and grow toward each other. As the water level reaches the level of the critical saddle point between the two hills the island contours touch at precisely the saddle point. \nWe observe that our object count decrements by –1 to give a total island count of one. \nThe essential feature of this rubric is that we only need to count the peaks and passes to inventory all of the islands in our sea, or objects in our scene. This approach works even as we increase the complexity of the scene.\n\nWe can use the same idea of enumerating peak, pits and pass criticalities in a very complex archipelago of island features, at any size scale, or any range of size scales, including noise at any size scale.\n\nThe relationship between island features can be\n\nDigital Morse Theory relates Peaks, Pits and Passes to Parents, Peers and Progeny. This gives a cute mnemonic: PPP → ppp.\n\nAs the topology does not care about geometry or dimensionality (directly), complex optimizations in infinite dimensional Hilbert spaces are amenable to this kind of analysis.\n\n"}
{"id": "26828544", "url": "https://en.wikipedia.org/wiki?curid=26828544", "title": "Dirk van Dalen", "text": "Dirk van Dalen\n\nDirk van Dalen (born 20 December 1932, Amsterdam) is a Dutch mathematician and historian of science.\n\nVan Dalen studied mathematics and physics and astronomy at the University of Amsterdam. Inspired by the work of LEJ Brouwer and Arend Heyting, he received his Ph.D. in 1963 from the University of Amsterdam for the thesis \"Extension problems in intuitionistic plane Projective geometry.\" From 1964 to 1966 Van Dalen taught logic and mathematics at MIT, and later Oxford. From 1967 he was professor at the University of Utrecht. In 2003 Dirk van Dalen was awarded the Academy Medal 2003 of the Royal Dutch Academy of Sciences for bringing the works of Brouwer to international attention.\n\n\n\n\n"}
{"id": "351853", "url": "https://en.wikipedia.org/wiki?curid=351853", "title": "Dominated convergence theorem", "text": "Dominated convergence theorem\n\nIn measure theory, Lebesgue's dominated convergence theorem provides sufficient conditions under which almost everywhere convergence of a sequence of functions implies convergence in the \"L\" norm. Its power and utility are two of the primary theoretical advantages of Lebesgue integration over Riemann integration.\n\nIt is widely used in probability theory, since it gives a sufficient condition for the convergence of expected values of random variables.\n\nLebesgue's Dominated Convergence Theorem. Let {\"f\"} be a sequence of real-valued measurable functions on a measure space . Suppose that the sequence converges pointwise to a function \"f\" and is dominated by some integrable function \"g\" in the sense that\nfor all numbers \"n\" in the index set of the sequence and all points \"x\" ∈ \"S\".\nThen \"f\" is integrable and\nwhich also implies\n\nRemark 1. The statement \"\"g\" is integrable\" is meant in the sense of Lebesgue; i.e\n\nRemark 2. The convergence of the sequence and domination by \"g\" can be relaxed to hold only almost everywhere provided the measure space is complete or \"f\" is chosen as a measurable function which agrees everywhere with the everywhere existing pointwise limit. (These precautions are necessary, because otherwise there might exist a non-measurable subset of a set , hence \"f\" might not be measurable.)<br>\n\nRemark 3. If μ(\"S\") < ∞, the condition that there is a dominating integrable function \"g\" can be relaxed to uniform integrability of the sequence {\"f\"}, see Vitali convergence theorem.\n\nLebesgue's dominated convergence theorem is a special case of the Fatou–Lebesgue theorem. Below, however, is a direct proof that uses Fatou’s lemma as the essential tool.\n\nSince \"f\" is the pointwise limit of the sequence \"(f)\" of measurable functions that are dominated by \"g\", it is also measurable and dominated by \"g\", hence it is integrable. Furthermore (these will be needed later),\nfor all \"n\" and\nThe second of these is trivially true (by the very definition of \"f\"). Using linearity and monotonicity of the Lebesgue integral,\nBy the reverse Fatou lemma (it is here that we use the fact that |\"f\"−\"f\"| is bounded above by an integrable function)\nwhich implies that the limit exists and vanishes i.e.\nFinally, since\nwe have that\nThe theorem now follows.\n\nIf the assumptions hold only everywhere, then there exists a set such that the functions \"f\" 1 satisfy the assumptions everywhere on \"S\". Then \"f\"(\"x\") is the pointwise limit of \"f\"(\"x\") for and for , hence \"f\" is measurable. The values of the integrals are not influenced by this μ-null set \"N\".\n\nDCT holds even if f converges to f in measure (finite measure) and the dominating function is non-negative almost everywhere.\n\nThe assumption that the sequence is dominated by some integrable \"g\" cannot be dispensed with. This may be seen as follows: define for \"x\" in the interval and otherwise. Any \"g\" which dominates the sequence must also dominate the pointwise supremum . Observe that\nby the divergence of the harmonic series. Hence, the monotonicity of the Lebesgue integral tells us that there exists no integrable function which dominates the sequence on [0,1]. A direct calculation shows that integration and pointwise limit do not commute for this sequence:\nbecause the pointwise limit of the sequence is the zero function. Note that the sequence {\"f\"} is not even uniformly integrable, hence also the Vitali convergence theorem is not applicable.\n\nOne corollary to the dominated convergence theorem is the bounded convergence theorem, which states that if {\"f\"} is a sequence of uniformly bounded real-valued measurable functions which converges pointwise on a bounded measure space (i.e. one in which μ(\"S\") is finite) to a function \"f\", then the limit \"f\" is an integrable function and\n\nRemark: The pointwise convergence and uniform boundedness of the sequence can be relaxed to hold only almost everywhere, provided the measure space is complete or \"f\" is chosen as a measurable function which agrees μ-almost everywhere with the everywhere existing pointwise limit.\n\nSince the sequence is uniformly bounded, there is a real number \"M\" such that for all and for all \"n\". Define for all . Then the sequence is dominated by \"g\". Furthermore, \"g\" is integrable since it is a constant function on a set of finite measure. Therefore the result follows from the dominated convergence theorem.\n\nIf the assumptions hold only everywhere, then there exists a set such that the functions \"f\"1 satisfy the assumptions everywhere on \"S\".\n\nLet formula_15 be a measure space, a real number and {\"f\"} a sequence of formula_16-measurable functions formula_17.\n\nAssume the sequence {\"f\"} converges μ-almost everywhere to an formula_16-measurable function \"f\", and is dominated by a formula_19 (cf. Lp space), i.e., for every natural number \"n\" we have: |\"f\"| ≤ \"g\", μ-almost everywhere.\n\nThen all \"f\" as well as \"f\" are in formula_20 and the sequence {\"f\"} converges to \"f\" in the sense of formula_20, i.e.:\n\nIdea of the proof: Apply the original theorem to the function sequence formula_23 with the dominating function formula_24.\n\nThe dominated convergence theorem applies also to measurable functions with values in a Banach space, with the dominating function still being non-negative and integrable as above. The assumption of convergence almost everywhere can be weakened to require only convergence in measure.\n\n\n"}
{"id": "5902964", "url": "https://en.wikipedia.org/wiki?curid=5902964", "title": "Elasticity of a function", "text": "Elasticity of a function\n\nIn mathematics, the elasticity or point elasticity of a positive differentiable function \"f\" of a positive variable (positive input, positive output) at point \"a\" is defined as\n\nor equivalently\nIt is thus the ratio of the relative (percentage) change in the function's output formula_4 with respect to the relative change in its input formula_5, for infinitesimal changes from a point formula_6. Equivalently, it is the ratio of the infinitesimal change of the logarithm of a function with respect to the infinitesimal change of the logarithm of the argument. Generalisations to multi-input-multi-output cases also exist in the literature.\n\nThe elasticity of a function is a constant formula_7 if and only if the function has the form formula_8 for a constant formula_9.\n\nThe elasticity at a point is the limit of the arc elasticity between two points as the separation between those two points approaches zero.\n\nThe concept of elasticity is widely used in economics; see elasticity (economics) for details.\n\nRules for finding the elasticity of products and quotients are simpler than those for derivatives. Let \"f, g\" be differentiable. Then\n\nThe derivative can be expressed in terms of elasticity as\nLet \"a\" and \"b\" be constants. Then\n\nIn economics, the price elasticity of demand refers to the elasticity of a demand function \"Q\"(\"P\"), and can be expressed as (dQ/dP)/(Q(P)/P) or the ratio of the value of the marginal function (dQ/dP) to the value of the average function (Q(P)/P). This relationship provides an easy way of determining whether a demand curve is elastic or inelastic at a particular point. First, suppose one follows the usual convention in mathematics of plotting the independent variable (P) horizontally and the dependent variable (Q) vertically. Then the slope of a line tangent to the curve at that point is the value of the marginal function at that point. The slope of a ray drawn from the origin through the point is the value of the average function. If the absolute value of the slope of the tangent is greater than the slope of the ray then the function is elastic at the point; if the slope of the secant is greater than the absolute value of the slope of the tangent then the curve is inelastic at the point. If the tangent line is extended to the horizontal axis the problem is simply a matter of comparing angles created by the lines and the horizontal axis. If the marginal angle is greater than the average angle then the function is elastic at the point; if the marginal angle is less than the average angle then the function is inelastic at that point. If, however, one follows the convention adopted by economists and plots the independent variable \"P\" on the vertical axis and the dependent variable \"Q\" on the horizontal axis, then the opposite rules would apply.\n\nThe same graphical procedure can also be applied to a supply function or other functions.\n\nA semi-elasticity (or semielasticity) gives the percentage change in \"f(x)\" in terms of a change (not percentage-wise) in \"x\". Algebraically, the semi-elasticity S of a function \"f\" at point \"x\" is \n\nThe semi-elasticity will be constant for exponential functions of the form, formula_19 since,\n\nAn example of semi-elasticity is modified duration in bond trading.\n\nThe term \"semi-elasticity\" is also sometimes used for the change if \"f(x)\" in terms of a percentage change in \"x\" which would be\n\n\n"}
{"id": "1535731", "url": "https://en.wikipedia.org/wiki?curid=1535731", "title": "Fictionalism", "text": "Fictionalism\n\nFictionalism is the view in philosophy according to which statements that appear to be descriptions of the world should not be construed as such, but should instead be understood as cases of \"make believe\", of pretending to treat something as literally true (a \"useful fiction\"). Two important strands of fictionalism are modal fictionalism developed by Gideon Rosen, which states that possible worlds, regardless of whether they exist or not, may be a part of a useful discourse, and mathematical fictionalism advocated by Hartry Field, which states that talk of numbers and other mathematical objects is nothing more than a convenience for doing science. Also in meta-ethics, there is an equivalent position called moral fictionalism (championed by Richard Joyce). Many modern versions of fictionalism are influenced by the work of Kendall Walton in aesthetics.\n\nFictionalism consists in at least the following three theses:\n\n\n\n"}
{"id": "10305032", "url": "https://en.wikipedia.org/wiki?curid=10305032", "title": "Gordon Royle", "text": "Gordon Royle\n\nGordon F. Royle is a Professor at the School of Mathematics and Statistics at The University of Western Australia.\n\nRoyle is the co-author (with Chris Godsil) of the book \"Algebraic Graph Theory\" (Springer Verlag, 2001, ).\nRoyle is also known for his research into the mathematics of Sudoku and his search for the Sudoku puzzle with the smallest number of entries that has a unique solution.\n\nRoyle earned his Ph.D. in 1987 from the University of Western Australia under the supervision of Cheryl Praeger and Brendan McKay.\n"}
{"id": "600373", "url": "https://en.wikipedia.org/wiki?curid=600373", "title": "Gradient conjecture", "text": "Gradient conjecture\n\nIn mathematics, the gradient conjecture, due to René Thom (1989), was proved in 2000 by three Polish mathematicians, Krzysztof Kurdyka (University of Savoie, France), Tadeusz Mostowski (Warsaw University, Poland) and Adam Parusiński (University of Angers, France). It states that given a real-valued analytic function \"f\" defined on R and a trajectory \"x\"(\"t\") of the gradient vector field of \"f\" having a limit point \"x\" ∈ R, where \"f\" has an isolated critical point at \"x\", there exists a limit (in the projective space PR) for the secant lines from \"x\"(\"t\") to \"x\", as \"t\" tends to zero.\n\n"}
{"id": "45152804", "url": "https://en.wikipedia.org/wiki?curid=45152804", "title": "Growth and underinvestment", "text": "Growth and underinvestment\n\nThe Growth and Underinvestment Archetype is one of the common system archetype patterns defined as part of the system dynamics discipline.\n\nSystem dynamics is an approach which strives to understand, describe and optimize nonlinear behaviors of complex systems over time, using tools such as feedback loops in order to find a leverage point of the system. As part of this discipline, several commonly found patterns of system behavior were found, named and described in detail. The Growth and Underinvestment Archetype is one of such patterns.\n\nThe system described in the Growth and Underinvestment Archetype consists of three feedback loops. Each feedback loop can be one of two types:\n\nThe reinforcing loop consists of a growing action, such as units of a specific product shipped to a customer, and a current state, such as current demand for a specific product. The growing action causes a positive increase in the current state. The increase of the current state then in turn causes a positive increase of the growing action, thereby creating the reinforcing characteristic of the loop.\n\nAs discussed above, this reinforcing loop would have exponential behavior in time, if its growth wouldn’t be bound by the combination of the two balancing loops present in the system.\n\nThe first balancing loop is directly connected to the reinforcing loop via the current state variable. The first balancing loop consists of a current state and a slowing action (for example exceeding capacity limits). The growth of the current state causes the growth of the slowing action. The growth of the slowing action in turn reduces the current state, thereby creating a balancing loop.\n\nOne example of this balancing loop is a situation where a number of units manufactured is increasing (current state), which causes the manufacturing utilization to increase (end eventually exceed capacity). This will make each additional unit of manufacturing more expensive, reducing the growth in units manufacture. One can note that a rubber-banding effect occurs, since the more units are manufactured, the more expensive the manufacturing is. This loop taken in isolation would eventually find a stable state, independently of its beginning state.\n\nThe second balancing loop is what differentiates the Growth and Underinvestment Archetype from other archetypes. It is directly connected to the first balancing loop via the slowing action variable. The balancing loop consists of several elements:\n\n\nFirst, the growth of the slowing action causes growth of the perceived need for investment (e.g. building additional manufacturing capacity). Another factor that can positively contribute to the perceived need to invest is the failure to uphold the performance standard (for example manufacturing error rate). The perceived need to invest positively translates into actually making the investment. The investment made then negatively influences the slowing action (e.g. removal of capacity limits).\n\nThe last element of the second balancing loop is the delay in investment, which happens for a variety of reasons, for example hesitation of management to invest in additional capacity.\n\nThe key to understanding the Growth and Underinvestment Archetype is in the delay in investment. This delay causes the second balancing loop to have longer cycle times than the first balancing loop. That in turn has the following effect:\n\nSince the second balancing loop has a shorter loop cycle, it causes the current state to be reduced, which in turn decreases the slowing action. This happens before an investment is made, in effect reducing the perceived need for investment.\n\nIn effect, the first and second reinforcing loop act together as a reinforcing loop to restrict growth.\n\nIf it were not for the delay, the whole system would work optimally thanks to timely investments.\n\nAt least two factors can contribute to the difficulty of identifying the Growth and Underinvestment Archetype in real-world complex systems.\n\nFirst, the archetype can be temporarily covered up by shifting the burden, that is, by trying to solve the underlying problem by a symptomatic solution, instead of a fundamental one. This leads to further delaying the investment decision, narrowing the window for effective and timely investment or missing it entirely.\n\nSecond, in order to recognize the archetype, a holistic view of the system is required. This can be difficult, since the Growth and Underinvestment Archetype can create many issues that management must attend to, in effect preventing them from stepping back and seeing the bigger picture.\n\nWhen discussing how to optimize the system, it can be beneficial to discuss what a leverage point is.\n\nThe leverage point in the system is a place where structural changes can lead to significant and lasting improvements to the system. There are two kinds of leverage points:\n\nWhen dealing with this archetype, several generic strategies can be considered in order to solve the problem the archetype presents.\n\nThe first strategy to consider is whether it is possible to shorten the delay between the perceived need to invest and actually making an investment.\n\nOne tool one can utilize in order to shorten the delay is Business Process Management, a field of study focusing on improving the efficiency of business processes. With its help, we might be able to identify the excessive delays in the investment process and shorten the delays or eliminate the parts of process that cause it entirely.\n\nWhen the reduction of investment delay is not possible, consider having a plan in advance. This includes monitoring the right key performance indicators (some KPIs such as utilization rate might act as an inhibitor for investment, since they frown upon unused capacity) and have an investment plan prepared in advance.\n\nSuch plan can also include a stop-gap solution that can temporarily weaken the growth inhibitor, such as hiring outside help in the form of contractors or lending additional capacity. But beware to not let the stop-gap solution become a permanent one, which could become a Shifting the Burden archetype.\n\nA new home delivery-focused pizzeria opens up in the neighborhood. At first, the demand is low, but the pizza’s quality is excellent, as well as the delivery times. After a while, the pizzeria gets noticed and is featured in a local online food blog. As a result, the demand for the pizza rises sharply. But the pizzeria owners are reluctant to purchase more delivery capacity (pizza delivery vehicles and personnel) along with higher pizza production capacity (additional pizza ovens). That results in higher delivery times and a larger percentage of undercooked pizzas, in turn lowering the number of returning customers. As a result, the pressure for additional investment in both delivery and production capacity is eliminated. The pizzeria owners are happy that they held off on the additional investment.\n\nSuch an example clearly represents a missed opportunity for further growth. It could have been avoided in two ways:\n\nThe application of the Growth and Underinvestment Archetype can be especially crucial for startup businesses, which need to grow fast or might have to face failure to raise additional funds. For them, the growing concern is a going concern.\n\nA new startup company focused on developing mobile gaming experiences has recently released its first game after successfully completing the first round of raising capital from investors. The game is initially priced at $5.99 in the Store. After the initial release, the game starts gaining a little bit of traction, but not enough to be considered a success. The company operates as usual, adding more content into the game and fixing bugs. Also, the game has an online component that is sized well for the current audience.\n\nAfter several weeks, the company comes to a major decision. It will re-release the game as free, instead focusing on selling additional content on the form in in-app purchases. The strategy works and many new users start playing the game. This has two effects:\n\nShortly after the free version of the game comes out, the influx of players starts to affect the online component, which occasionally crashes and disconnects users, causing them to save progress they have made in the game. The company redeploys its resources and tries to mitigate the situation by incrementally improving the online component. It is clear, however, that a complete rewrite of the online component is needed on order to eliminate the problem entirely. Therefore the company contacts its investors in order to raise additional funds to rebuild the online component.\n\nMeanwhile, the number of active players dwindles. In response to this fact, as well as the weak cash flow generated by the game, the investor decides to take time to make the investment decision. Unfortunately, the cash flow from the game is not improving, since the remaining user base purchased the content they were interested in and new content is delayed, since most of the developers have been reassigned to solving the online component woes. In response to this, the investor sees the ever-flattening sales and dwindling user base and decides not to invest further resources into the company. A few weeks later, the company runs out of funds and declares bankruptcy.\n\nHow such a situation could be prevented:\n\nThe Growth and Underinvestment with a Drifting Standard is a special case of the archetype.\n\nIt adds an additional relationship between the slowing action and the performance standard. When the slowing action is growing (e.g. the backlog of order is increasing in size), it has a negative effect on the performance standard (e.g. raising the maximum permitted time it takes to deliver an order). The rest of the system behaves in the same way as the original archetype.\n\nThis additional relationship can have severe consequences, since in some cases the performance standard can have major contribution to pressure exerted on individuals deciding whether to make the investment. With the slowing action actively undermining the performance standard, it can be harder to find the incentive to invest into additional resources.\n\nThe Growth and Underinvestment Archetype can be considered to be an elaboration o the Limits to Success archetype. It adds another feedback loop which effectively elaborates the Limiting State part of the Limits to Success archetype.\n"}
{"id": "40965653", "url": "https://en.wikipedia.org/wiki?curid=40965653", "title": "Homotopy hypothesis", "text": "Homotopy hypothesis\n\nIn category theory, a branch of mathematics, Grothendieck's homotopy hypothesis states that the ∞-groupoids are equivalent to the topological spaces. If we shall to model our ∞-groupoids as Kan complexes, then the homotopy types of the geometric realizations of these sets give models for every homotopy type. It is conjectured that there are many different \"equivalent\" models for ∞-groupoids all which can be realized as homotopy types.\n\n\n\n"}
{"id": "322170", "url": "https://en.wikipedia.org/wiki?curid=322170", "title": "Hyperperfect number", "text": "Hyperperfect number\n\nIn mathematics, a \"k\"-hyperperfect number is a natural number \"n\" for which the equality \"n\" = 1 + \"k\"(\"σ\"(\"n\") − \"n\" − 1) holds, where \"σ\"(\"n\") is the divisor function (i.e., the sum of all positive divisors of \"n\"). A hyperperfect number is a \"k\"-hyperperfect number for some integer \"k\". Hyperperfect numbers generalize perfect numbers, which are 1-hyperperfect.\n\nThe first few numbers in the sequence of \"k\"-hyperperfect numbers are 6, 21, 28, 301, 325, 496, 697, ... , with the corresponding values of \"k\" being 1, 2, 1, 6, 3, 1, 12, ... . The first few \"k\"-hyperperfect numbers that are not perfect are 21, 301, 325, 697, 1333, ... .\n\nThe following table lists the first few \"k\"-hyperperfect numbers for some values of \"k\", together with the sequence number in the On-Line Encyclopedia of Integer Sequences (OEIS) of the sequence of \"k\"-hyperperfect numbers:\n\nIt can be shown that if \"k\" > 1 is an odd integer and \"p\" = (3\"k\" + 1) / 2 and \"q\" = 3\"k\" + 4 are prime numbers, then \"p\"²\"q\" is \"k\"-hyperperfect; Judson S. McCranie has conjectured in 2000 that all \"k\"-hyperperfect numbers for odd \"k\" > 1 are of this form, but the hypothesis has not been proven so far. Furthermore, it can be proven that if \"p\" ≠ \"q\" are odd primes and \"k\" is an integer such that \"k\"(\"p\" + \"q\") = \"pq\" - 1, then \"pq\" is \"k\"-hyperperfect.\n\nIt is also possible to show that if \"k\" > 0 and \"p\" = \"k\" + 1 is prime, then for all \"i\" > 1 such that \"q\" = \"p\" − \"p\" + 1 is prime, \"n\" = \"p\"\"q\" is \"k\"-hyperperfect. The following table lists known values of \"k\" and corresponding values of \"i\" for which \"n\" is \"k\"-hyperperfect:\n\nThe newly introduced mathematical concept of hyperdeficiency is related to the hyperperfect numbers.\n\nDefinition (Minoli 2010): For any integer \"n\" and for integer \"k\", formula_1, define the k-hyperdeficiency (or simply the hyperdeficiency) for the number \"n\" as\n\nA number \"n\" is said to be k-hyperdeficient if δ(\"n\") > 0.\n\nNote that for \"k\"=1 one gets δ(\"n\")= 2\"n\"–σ(\"n\"), which is the standard traditional definition of deficiency.\n\nLemma: A number \"n\" is k-hyperperfect (including \"k\"=1) if and only if the k-hyperdeficiency of \"n\", δ(\"n\") = 0.\nLemma: A number \"n\" is k-hyperperfect (including \"k\"=1) if and only if for some \"k\", δ(\"n\") = -δ(\"n\") for at least one \"j\" > 0.\n\n\n\n"}
{"id": "1085606", "url": "https://en.wikipedia.org/wiki?curid=1085606", "title": "Hypervalent molecule", "text": "Hypervalent molecule\n\nA hypervalent molecule (the phenomenon is sometimes colloquially known as expanded octet) is a molecule that contains one or more main group elements apparently bearing more than eight electrons in their valence shells. Phosphorus pentachloride (PCl), sulfur hexafluoride (SF), chlorine trifluoride (ClF), the chlorite (ClO) ion, and the triiodide (I) ion are examples of hypervalent molecules.\n\nHypervalent molecules were first formally defined by Jeremy I. Musher in 1969 as molecules having central atoms of group 15–18 in any valence other than the lowest (i.e. 3, 2, 1, 0 for Groups 15, 16, 17, 18 respectively, based on the octet rule).\n\nSeveral specific classes of hypervalent molecules exist:\n\nN-X-L nomenclature, introduced collaboratively by the research groups of Martin, Arduengo, and Kochi in 1980, is often used to classify hypervalent compounds of main group elements, where:\nExamples of N-X-L nomenclature include: \n\nThe debate over the nature and classification of hypervalent molecules goes back to Gilbert N. Lewis and Irving Langmuir and the debate over the nature of the chemical bond in the 1920s. Lewis maintained the importance of the two-center two-electron (2c-2e) bond in describing hypervalence, thus using expanded octets to account for such molecules. Using the language of orbital hybridization, the bonds of molecules like PF and SF were said to be constructed from spd orbitals on the central atom. Langmuir, on the other hand, upheld the dominance of the octet rule and preferred the use of ionic bonds to account for hypervalence without violating the rule (e.g. \"SF 2F\" for SF).\n\nIn the late 1920s and 1930s, Sugden argued for the existence of a two-center one-electron (2c-1e) bond and thus rationalized bonding in hypervalent molecules without the need for expanded octets or ionic bond character; this was poorly accepted at the time. In the 1940s and 1950s, Rundle and Pimentel popularized the idea of the three-center four-electron bond, which is essentially the same concept which Sugden attempted to advance decades earlier; the three-center four-electron bond can be alternatively viewed as consisting of two collinear two-center one-electron bonds, with the remaining two nonbonding electrons localized to the ligands.\n\nThe attempt to actually prepare hypervalent organic molecules began with Hermann Staudinger and Georg Wittig in the first half of the twentieth century, who sought to challenge the extant valence theory and successfully prepare nitrogen and phosphorus-centered hypervalent molecules. The theoretical basis for hypervalency was not delineated until J.I. Musher's work in 1969.\n\nIn 1990, Magnusson published a seminal work definitively excluding the significance of d-orbital hybridization in the bonding of hypervalent compounds of second-row elements. This had long been a point of contention and confusion in describing these molecules using molecular orbital theory. Part of the confusion here originates from the fact that one must include d-functions in the basis sets used to describe these compounds (or else unreasonably high energies and distorted geometries result), and the contribution of the d-function to the molecular wavefunction is large. These facts were historically interpreted to mean that d-orbitals must be involved in bonding. However, Magnusson concludes in his work that d-orbital involvement is not implicated in hypervalency.\n\nNevertheless, a 2013 study showed that although the Pimentel ionic model best accounts for the bonding of hypervalent species, the energetic contribution of an expanded octet structure is also not null. In this modern valence bond theory study of the bonding of xenon difluoride, it was found that ionic structures account for about 81% of the overall wavefunction, of which 70% arises from ionic structures employing only the p orbital on xenon while 11% arises from ionic structures employing an formula_1hybrid on xenon. The contribution of a formally hypervalent structure employing an orbital of spd hydridization on xenon accounts for 11% of the wavefunction, with a diradical contribution making up the remaining 8%. The 11% spd contribution results in a net stabilization of the molecule by mol, a minor but significant fraction of the total energy of the total bond energy ( mol). Other studies have similarly found minor but non-negligible energetic contributions from expanded octet structures in SF (17%) and XeF (14%). \n\nBoth the term and concept of hypervalency still fall under criticism. In 1984, in response to this general controversy, Paul von Ragué Schleyer proposed the replacement of 'hypervalency' with use of the term hypercoordination because this term does not imply any mode of chemical bonding and the question could thus be avoided altogether.\n\nThe concept itself has been criticized by Ronald Gillespie who, based on an analysis of electron localization functions, wrote in 2002 that \"as there is no fundamental difference between the bonds in hypervalent and non-hypervalent (Lewis octet) molecules there is no reason to continue to use the term hypervalent.\"\n\nFor hypercoordinated molecules with electronegative ligands such as PF it has been demonstrated that the ligands can pull away enough electron density from the central atom so that its net content is again 8 electrons or fewer. Consistent with this alternative view is the finding that hypercoordinated molecules based on fluorine ligands, for example PF do not have hydride counterparts e.g. phosphorane PH which is unknown.\n\nThe ionic model holds up well in thermochemical calculations. It predicts favorable exothermic formation of PFF from phosphorus trifluoride PF and fluorine F whereas a similar reaction forming PHH is not favorable.\n\nDurrant has proposed an alternative definition of hypervalency, based on the analysis of atomic charge maps obtained from Atoms in molecules theory. This approach defines a parameter called the valence electron equivalent, γ, as “the formal shared electron count at a given atom, obtained by any combination of valid ionic and covalent resonance forms that reproduces the observed charge distribution”. For any particular atom X, if the value of γ(X) is greater than 8, that atom is hypervalent. Using this alternative definition, many species such as PCl, SO, and XeF, that are hypervalent by Musher's definition, are reclassified as hypercoordinate but not hypervalent, due to strongly ionic bonding that draws electrons away from the central atom. On the other hand, some compounds that are normally written with ionic bonds in order to conform to the octet rule, such as ozone O, nitrous oxide NNO, and trimethylamine N-oxide (CH)NO, are found to be genuinely hypervalent. Examples of γ calculations for phosphate PO (γ(P) = 2.6, non-hypervalent) and orthonitrate NO (γ(N) = 8.5, hypervalent) are shown below.\n\nEarly considerations of the geometry of hypervalent molecules returned familiar arrangements that were well explained by the VSEPR model for atomic bonding. Accordingly, AB and AB type molecules would possess a trigonal bi-pyramidal and octahedral geometry, respectively. However, in order to account for the observed bond angles, bond lengths and apparent violation of the Lewis octet rule, several alternative models have been proposed.\n\nIn the 1950s an expanded valence shell treatment of hypervalent bonding was adduced to explain the molecular architecture, where the central atom of penta- and hexacoordinated molecules would utilize d AOs in addition to s and p AOs. However, advances in the study of \"ab initio\" calculations have revealed that the contribution of d-orbitals to hypervalent bonding is too small to describe the bonding properties, and this description is now regarded as much less important. It was shown that in the case of hexacoordinated SF, d-orbitals are not involved in S-F bond formation, but charge transfer between the sulfur and fluorine atoms and the apposite resonance structures were able to account for the hypervalency (See below).\n\nAdditional modifications to the octet rule have been attempted to involve ionic characteristics in hypervalent bonding. As one of these modifications, in 1951, the concept of the 3-center 4-electron (3c-4e) bond, which described hypervalent bonding with a qualitative molecular orbital, was proposed. The 3c-4e bond is described as three molecular orbitals given by the combination of a p atomic orbital on the central atom and an atomic orbital from each of the two ligands on opposite sides of the central atom. Only one of the two pairs of electrons is occupying a molecular orbital that involves bonding to the central atom, the second pair being non-bonding and occupying a molecular orbital composed of only atomic orbitals from the two ligands. This model in which the octet rule is preserved was also advocated by Musher. \nA complete description of hypervalent molecules arises from consideration of molecular orbital theory through quantum mechanical methods. A LCAO in, for example, sulfur hexafluoride, taking a basis set of the one sulfur 3s-orbital, the three sulfur 3p-orbitals, and six octahedral geometry symmetry-adapted linear combinations (SALCs) of fluorine orbitals, a total of ten molecular orbitals are obtained (four fully occupied bonding MOs of the lowest energy, two fully occupied intermediate energy non-bonding MOs and four vacant antibonding MOs with the highest energy) providing room for all 12 valence electrons. This is a stable configuration only for S\"X\" molecules containing electronegative ligand atoms like fluorine, which explains why SH is not a stable molecule. In the bonding model, the two non-bonding MOs (1e) are localized equally on all six fluorine atoms.\n\nFor hypervalent compounds in which the ligands are more electronegative than the central, hypervalent atom, resonance structures can be drawn with no more than four covalent electron pair bonds and completed with ionic bonds to obey the octet rule. For example, in phosphorus pentafluoride (PF), 5 resonance structures can be generated each with four covalent bonds and one ionic bond with greater weight in the structures placing ionic character in the axial bonds, thus satisfying the octet rule and explaining both the observed trigonal bipyramidal molecular geometry and the fact that the axial bond length (158 pm) is longer than the equatorial (154 pm).\n\nFor a hexacoordinate molecule such as sulfur hexafluoride, each of the six bonds is the same length. The rationalization described above can be applied to generate 15 resonance structures each with four covalent bonds and two ionic bonds, such that the ionic character is distributed equally across each of the sulfur-fluorine bonds.\n\nSpin-coupled valence bond theory has been applied to diazomethane and the resulting orbital analysis was interpreted in terms of a chemical structure in which the central nitrogen has five covalent bonds;\n\nThis led the authors to the interesting conclusion that \"Contrary to what we were all taught as undergraduates, the nitrogen atom does indeed form five covalent linkages and the availability or otherwise of d-orbitals has nothing to do with this state of affairs.\"\n\nHexacoordinate phosphorus molecules involving nitrogen, oxygen, or sulfur ligands provide examples of Lewis acid-Lewis base hexacoordination. For the two similar complexes shown below, the length of the C-P bond increases with decreasing length of the N-P bond; the strength of the C-P bond decreases with increasing strength of the N-P Lewis acid-Lewis base interaction.\n\nThis trend is also generally true of pentacoordinated main-group elements with one or more lone-pair-containing ligand, including the oxygen-pentacoordinated silicon examples shown below.\n\nThe Si-halogen bonds range from close to the expected van der Waals value in A (a weak bond) almost to the expected covalent single bond value in C (a strong bond).\n\nCorriu and coworkers performed early work characterizing reactions thought to proceed through a hypervalent transition state. Measurements of the reaction rates of hydrolysis of tetravalent chlorosilanes incubated with catalytic amounts of water returned a rate that is first order in chlorosilane and second order in water. This indicated that two water molecules interacted with the silane during hydrolysis and from this a binucleophilic reaction mechanism was proposed. Corriu and coworkers then measured the rates of hydrolysis in the presence of nucleophilic catalyst HMPT, DMSO or DMF. It was shown that the rate of hydrolysis was again first order in chlorosilane, first order in catalyst and now first order in water. Appropriately, the rates of hydrolysis also exhibited a dependence on the magnitude of charge on the oxygen of the nucleophile.\n\nTaken together this led the group to propose a reaction mechanism in which there is a pre-rate determining nucleophilic attack of the tetracoordinated silane by the nucleophile (or water) in which a hypervalent pentacoordinated silane is formed. This is followed by a nucleophilic attack of the intermediate by water in a rate determining step leading to hexacoordinated species that quickly decomposes giving the hydroxysilane.\n\nSilane hydrolysis was further investigated by Holmes and coworkers in which tetracoordinated MesSiF (Mes = mesityl) and pentacoordinated MesSiF were reacted with two equivalents of water. Following twenty-four hours, almost no hydrolysis of the tetracoordinated silane was observed, while the pentacoordinated silane was completely hydrolyzed after fifteen minutes. Additionally, X-ray diffraction data collected for the tetraethylammonium salts of the fluorosilanes showed the formation of hydrogen bisilonate lattice supporting a hexacoordinated intermediate from which HF is quickly displaced leading to the hydroxylated product. This reaction and crystallographic data support the mechanism proposed by Corriu \"et al.\".\n\nThe apparent increased reactivity of hypervalent molecules, contrasted with tetravalent analogues, has also been observed for Grignard reactions. The Corriu group measured Grignard reaction half-times by NMR for related 18-crown-6 potassium salts of a variety of tetra- and pentacoordinated fluorosilanes in the presence of catalytic amounts of nucleophile.\n\nThough the half reaction method is imprecise, the magnitudinal differences in reactions rates allowed for a proposed reaction scheme wherein, a pre-rate determining attack of the tetravalent silane by the nucleophile results in an equilibrium between the neutral tetracoordinated species and the anionic pentavalent compound. This is followed by nucleophilic coordination by two Grignard reagents as normally seen, forming a hexacoordinated transition state and yielding the expected product.\nThe mechanistic implications of this are extended to a hexacoordinated silicon species that is thought to be active as a transition state in some reactions. The reaction of allyl- or crotyl-trifluorosilanes with aldehydes and ketones only precedes with fluoride activation to give a pentacoordinated silicon. This intermediate then acts as a Lewis acid to coordinate with the carbonyl oxygen atom. The further weakening of the silicon–carbon bond as the silicon becomes hexacoordinate helps drive this reaction.\n\nSimilar reactivity has also been observed for other hypervalent structures such as the miscellany of phosphorus compounds, for which hexacoordinated transition states have been proposed.\nHydrolysis of phosphoranes and oxyphosphoranes have been studied and shown to be second order in water. Bel'skii \"et al.\". have proposed a prerate determining nucleophilic attack by water resulting in an equilibrium between the penta- and hexacoordinated phosphorus species, which is followed by a proton transfer involving the second water molecule in a rate determining ring-opening step, leading to the hydroxlyated product. \nAlcoholysis of pentacoordinated phosphorus compounds, such as trimethoxyphospholene with benzyl alcohol, have also been postulated to occur through a similar octahedral transition state, as in hydrolysis, however without ring opening.\n\nIt can be understood from these experiments that the increased reactivity observed for hypervalent molecules, contrasted with analogous nonhypervalent compounds, can be attributed to the congruence of these species to the hypercoordinated activated states normally formed during the course of the reaction.\n\nThe enhanced reactivity at pentacoordinated silicon is not fully understood. Corriu and coworkers suggested that greater electropositive character at the pentavalent silicon atom may be responsible for its increased reactivity. Preliminary ab initio calculations supported this hypothesis to some degree, but used a small basis set.\n\nA software program for ab initio calculations, Gaussian 86, was used by Dieters and coworkers to compare tetracoordinated silicon and phosphorus to their pentacoordinate analogues. This ab initio approach is used as a supplement to determine why reactivity improves in nucleophilic reactions with pentacoordinated compounds. For silicon, the 6-31+G* basis set was used because of its pentacoordinated anionic character and for phosphorus, the 6-31G* basis set was used.\n\nPentacoordinated compounds should theoretically be less electrophilic than tetracoordinated analogues due to steric hindrance and greater electron density from the ligands, yet experimentally show greater reactivity with nucleophiles than their tetracoordinated analogues. Advanced ab initio calculations were performed on series of tetracoordinated and pentacoordinated species to further understand this reactivity phenomenon. Each series varied by degree of fluorination. Bond lengths and charge densities are shown as functions of how many hydride ligands are on the central atoms. For every new hydride, there is one less fluoride.\n\nFor silicon and phosphorus bond lengths, charge densities, and Mulliken bond overlap, populations were calculated for tetra and pentacoordinated species by this ab initio approach. Addition of a fluoride ion to tetracoordinated silicon shows an overall average increase of 0.1 electron charge, which is considered insignificant. In general, bond lengths in trigonal bipyramidal pentacoordinate species are longer than those in tetracoordinate analogues. Si-F bonds and Si-H bonds both increase in length upon pentacoordination and related effects are seen in phosphorus species, but to a lesser degree. The reason for the greater magnitude in bond length change for silicon species over phosphorus species is the increased effective nuclear charge at phosphorus. Therefore, silicon is concluded to be more loosely bound to its ligands.\n\nIn addition Dieters and coworkers show an inverse correlation between bond length and bond overlap for all series. Pentacoordinated species are concluded to be more reactive because of their looser bonds as trigonal-bipyramidal structures.\n\nBy calculating the energies for the addition and removal of a fluoride ion in various silicon and phosphorus species, several trends were found. In particular, the tetracoordinated species have much higher energy requirements for ligand removal than do pentacoordinated species. Further, silicon species have lower energy requirements for ligand removal than do phosphorus species, which is an indication of weaker bonds in silicon.\n\n"}
{"id": "3431943", "url": "https://en.wikipedia.org/wiki?curid=3431943", "title": "Joseph Diez Gergonne", "text": "Joseph Diez Gergonne\n\nJoseph Diez Gergonne (19 June 1771 at Nancy, France – 4 May 1859 at Montpellier, France) was a French mathematician and logician.\n\nIn 1791, Gergonne enlisted in the French army as a captain. That army was undergoing rapid expansion because the French government feared a foreign invasion intended to undo the French Revolution and restore Louis XVI to the throne of France. He saw action in the major battle of Valmy on 20 September 1792. He then returned to civilian life but soon was called up again and took part in the French invasion of Spain in 1794.\n\nIn 1795, Gergonne and his regiment were sent to Nîmes. At this point, he made a definitive transition to civilian life by taking up the chair of \"transcendental mathematics\" at the new École centrale. He came under the influence of Gaspard Monge, the Director of the new École polytechnique in Paris.\n\nIn 1810, in response to difficulties he encountered in trying to publish his work, Gergonne founded his own mathematics journal, officially named the \"Annales de mathématiques pures et appliquées\" but generally referred to as the \"Annales de Gergonne\". The most common subject of articles in his journal was geometry, Gergonne's specialty. Over a period of 22 years, the \"Annales de Gergonne\" published about 200 articles by Gergonne himself, and other articles by many distinguished mathematicians, including Poncelet, Servois, Bobillier, Steiner, Plücker, Chasles, Brianchon, Dupin, Lamé, even Galois.\n\nGergonne was appointed to the chair of astronomy at the University of Montpellier in 1816. In 1830, he was appointed Rector of the University of Montpellier, at which time he ceased publishing his journal. He retired in 1844.\n\nGergonne was among the first mathematicians to employ the word \"polar\". In a series of papers beginning in 1810, he contributed to elaborating the \"principle of duality\" in projective geometry, by noticing that every theorem in the plane connecting points and lines corresponds to another theorem in which points and lines are interchanged, provided that the theorem embodied no metrical notions. Gergonne was an early proponent of the techniques of analytical geometry and in 1816, he devised an elegant coordinate solution to the classical problem of Apollonius: to find a circle which touches three given circles, thus demonstrating the power of the new methods.\n\nIn 1813, Gergonne wrote the prize-winning essay for the Bordeaux Academy, \"Methods of synthesis and analysis in mathematics\", unpublished to this day and known only via a summary. The essay is very revealing of Gergonne's philosophical ideas. He called for the abandonment of the words \"analysis\" and \"synthesis\", claiming they lacked clear meanings. Surprisingly for a geometer, he suggested that algebra is more important than geometry at a time when algebra consisted almost entirely of the elementary algebra of the real field. He predicted that one day quasi-mechanical methods would be used to discover new results.\n\nIn 1815, Gergonne wrote the first paper on the optimal design of experiments for polynomial regression. According to S. M. Stigler, Gergonne is the pioneer of optimal design as well as response surface methodology. \n\nHe published his \"Essai sur la théorie des définitions\" (An essay on the theory of definition) in his \"Annales\" in 1818. This essay is generally credited for first recognizing and naming the construct of \"implicit definition\".\n\n\n\n"}
{"id": "2469940", "url": "https://en.wikipedia.org/wiki?curid=2469940", "title": "Keith Medal", "text": "Keith Medal\n\nThe Keith Medal was a prize awarded by the Royal Society of Edinburgh, Scotland's national academy, for a scientific paper published in the society's scientific journals, preference being given to a paper containing a discovery, either in mathematics or earth sciences.\n\nThe Medal was inaugurated in 1827 as a result of a gift from Alexander Keith of Dunnottar, the first Treasurer of the Society. It was awarded quadrennially, alternately for a paper published in: Proceedings A (Mathematics) or Transactions (Earth and Environmental Sciences). The medal bears the head of John Napier of Merchiston.\n\nThe medal is no longer awarded. \n\nSource (1827 to 1913): Proceedings of the Royal Society of Edinburgh\n\n"}
{"id": "217523", "url": "https://en.wikipedia.org/wiki?curid=217523", "title": "Lagrange polynomial", "text": "Lagrange polynomial\n\nIn numerical analysis, Lagrange polynomials are used for polynomial interpolation. For a given set of points formula_1 with no two formula_2 values equal, the Lagrange polynomial is the polynomial of lowest degree that assumes at each value formula_2 the corresponding value formula_4 (i.e. the functions coincide at each point).\nThe interpolating polynomial of the least degree is unique, however, and since it can be arrived at through multiple methods, referring to \"the Lagrange polynomial\" is perhaps not as correct as referring to \"the Lagrange form\" of that unique polynomial.\n\nAlthough named after Joseph Louis Lagrange, who published it in 1795, the method was first discovered in 1779 by Edward Waring It is also an easy consequence of a formula published in 1783 by Leonhard Euler.\n\nUses of Lagrange polynomials include the Newton–Cotes method of numerical integration and Shamir's secret sharing scheme in cryptography.\n\nLagrange interpolation is susceptible to Runge's phenomenon of large oscillation. As changing the points formula_2 requires recalculating the entire interpolant, it is often easier to use Newton polynomials instead.\n\nGiven a set of \"k\" + 1 data points\nwhere no two formula_2 are the same, the interpolation polynomial in the Lagrange form is a linear combination\nof Lagrange basis polynomials\n\nwhere formula_10. Note how, given the initial assumption that no two formula_2 are the same, formula_12, so this expression is always well-defined. The reason pairs formula_13 with formula_14 are not allowed is that no interpolation function formula_15 such that formula_16 would exist; a function can only get one value for each argument formula_17. On the other hand, if also formula_18, then those two points would actually be one single point.\n\nFor all formula_19, formula_20 includes the term formula_21 in the numerator, so the whole product will be zero at formula_22:\n\nOn the other hand,\n\nIn other words, all basis polynomials are zero at formula_22, except formula_26, for which it holds that formula_27, because it lacks the formula_21 term.\n\nIt follows that formula_29, so at each point formula_17, formula_31, showing that formula_15 interpolates the function exactly.\n\nThe function \"L\"(\"x\") being sought is a polynomial in formula_33 of the least degree that interpolates the given data set; that is, assumes value formula_4 at the corresponding formula_2 for all data points formula_36:\n\nObserve that:\n\nWe consider what happens when this product is expanded. Because the product skips formula_40, if formula_41 then all terms are formula_42 (except where formula_43, but that case is impossible, as pointed out in the definition section—in that term, formula_44, and since formula_45, formula_19, contrary to formula_47).\nAlso if formula_48 then since formula_49 does not preclude it, one term in the product will be for formula_50, i.e. formula_51, zeroing the entire product. So\n\nwhere formula_53 is the Kronecker delta. So:\n\nThus the function \"L\"(\"x\") is a polynomial with degree at most \"k\" and where formula_55.\n\nAdditionally, the interpolating polynomial is unique, as shown by the unisolvence theorem at the polynomial interpolation article.\n\nSolving an interpolation problem leads to a problem in linear algebra amounting to inversion of a matrix. Using a standard monomial basis for our interpolation polynomial formula_56, we must invert the Vandermonde matrix formula_57 to solve formula_55 for the coefficients formula_59 of formula_60. By choosing a better basis, the Lagrange basis, formula_61, we merely get the identity matrix, formula_53, which is its own inverse: the Lagrange basis automatically \"inverts\" the analog of the Vandermonde matrix.\n\nThis construction is analogous to the Chinese Remainder Theorem. Instead of checking for remainders of integers modulo prime numbers, we are checking for remainders of polynomials when divided by linears.\nFurthermore, when the order is large, Fast Fourier Transformation can be used to solve for the coefficients of the interpolated polynomial.\n\nWe wish to interpolate \"ƒ\"(\"x\") = \"x\" over the range 1 ≤ \"x\" ≤ 3, given these three points:\n\nThe interpolating polynomial is:\n\nWe wish to interpolate \"ƒ\"(\"x\") = \"x\" over the range 1 ≤ \"x\" ≤ 3, given these three points:\nThe interpolating polynomial is:\n\nThe Lagrange form of the interpolation polynomial shows the linear character of polynomial interpolation and the uniqueness of the interpolation polynomial. Therefore, it is preferred in proofs and theoretical arguments. Uniqueness can also be seen from the invertibility of the Vandermonde matrix, due to the non-vanishing of the Vandermonde determinant.\n\nBut, as can be seen from the construction, each time a node \"x\" changes, all Lagrange basis polynomials have to be recalculated. A better form of the interpolation polynomial for practical (or computational) purposes is the barycentric form of the Lagrange interpolation (see below) or Newton polynomials. \n\nLagrange and other interpolation at equally spaced points, as in the example above, yield a polynomial oscillating above and below the true function. This behaviour tends to grow with the number of points, leading to a divergence known as Runge's phenomenon; the problem may be eliminated by choosing interpolation points at Chebyshev nodes.\n\nThe Lagrange basis polynomials can be used in numerical integration to derive the Newton–Cotes formulas.\n\nUsing\n\nwe can rewrite the Lagrange basis polynomials as\n\nor, by defining the \"barycentric weights\"\n\nwe can simply write\n\nwhich is commonly referred to as the \"first form\" of the barycentric interpolation formula.\n\nThe advantage of this representation is that the interpolation polynomial may now be evaluated as\n\nwhich, if the weights formula_72 have been pre-computed, requires only formula_73 operations (evaluating formula_74 and the weights formula_75) as opposed to formula_76 for evaluating the Lagrange basis polynomials formula_20 individually.\n\nThe barycentric interpolation formula can also easily be updated to incorporate a new node formula_78 by dividing each of the formula_72, formula_80 by formula_81 and constructing the new formula_82 as above.\n\nWe can further simplify the first form by first considering the barycentric interpolation of the constant function formula_83:\n\nDividing formula_60 by formula_86 does not modify the interpolation, yet yields\n\nwhich is referred to as the \"second form\" or \"true form\" of the barycentric interpolation formula. This second form has the advantage that formula_74 need not be evaluated for each evaluation of formula_60.\n\nWhen interpolating a given function \"f\" by a polynomial of degree at the nodes \"x\"...,\"x\" we get the remainder formula_90 which can be expressed as\n\nwhere formula_92 is the notation for divided differences. Alternatively, the remainder can be expressed as a contour integral in complex domain as\n\nThe remainder can be bound as\n\nThe formula_95th derivatives of the Lagrange polynomial can be written as\n\nFor the first derivative, the coefficients are given by\n\nand for the second derivative \n\nThrough recursion, one can compute formulas for higher derivatives.\n\nThe Lagrange polynomial can also be computed in finite fields. This has applications in cryptography, such as in Shamir's Secret Sharing scheme.\n\n\n\n"}
{"id": "18203", "url": "https://en.wikipedia.org/wiki?curid=18203", "title": "Lambda calculus", "text": "Lambda calculus\n\nLambda calculus (also written as λ-calculus) is a formal system in mathematical logic for expressing computation based on function abstraction and application using variable binding and substitution. It is a universal model of computation that can be used to simulate any Turing machine. It was first introduced by mathematician Alonzo Church in the 1930s as part of his research of the foundations of mathematics.\n\nLambda calculus consists of constructing lambda terms and performing reduction operations on them. In the simplest form of lambda calculus, terms are built using only the following rules:\nproducing expressions such as: (λ\"x\".λ\"y\".(λ\"z\".(λ\"x\".\"z x\") (λ\"y.z y\")) (\"x y\")). Parentheses can be dropped if the expression is unambiguous. For some applications, terms for logical and mathematical constants and operations may be included.\n\nThe reduction operations include:\n\nIf De Bruijn indexing is used then α-conversion is no longer required as there will be no name collisions. If repeated application of the reduction steps eventually terminates then by the Church-Rosser theorem it will produce a beta normal form.\n\nLambda calculus is Turing complete, that is, it is a universal model of computation that can be used to simulate any Turing machine. Its namesake, the Greek letter lambda (λ), is used in lambda expressions and lambda terms to denote binding a variable in a function.\n\nLambda calculus may be \"untyped\" or \"typed\". In typed lambda calculus, functions can be applied only if they are capable of accepting the given input's \"type\" of data. Typed lambda calculi are \"weaker\" than the untyped lambda calculus that is the primary subject of this article, in the sense that \"typed lambda calculi can express less\" than the untyped calculus can, but on the other hand typed lambda calculi allow more things to be proved; in the simply typed lambda calculus it is, for example, a theorem that every evaluation strategy terminates for every simply typed lambda-term, whereas evaluation of untyped lambda-terms need not terminate. One reason there are many different typed lambda calculi has been the desire to do more (of what the untyped calculus can do) without giving up on being able to prove strong theorems about the calculus.\n\nLambda calculus has applications in many different areas in mathematics, philosophy, linguistics, and computer science. Lambda calculus has played an important role in the development of the theory of programming languages. Functional programming languages implement the lambda calculus. Lambda calculus is also a current research topic in Category theory.\n\nThe lambda calculus was introduced by mathematician Alonzo Church in the 1930s as part of an investigation into the foundations of mathematics. The original system was shown to be logically inconsistent in 1935 when Stephen Kleene and J. B. Rosser developed the Kleene–Rosser paradox.\n\nSubsequently, in 1936 Church isolated and published just the portion relevant to computation, what is now called the untyped lambda calculus. In 1940, he also introduced a computationally weaker, but logically consistent system, known as the simply typed lambda calculus.\n\nUntil the 1960s when its relation to programming languages was clarified, the λ-calculus was only a formalism. Thanks to Richard Montague and other linguists' applications in the semantics of natural language, the λ-calculus has begun to enjoy a respectable place in both linguistics and computer science.\n\nComputable functions are a fundamental concept within computer science and mathematics. The λ-calculus provides a simple semantics for computation, enabling properties of computation to be studied formally. The λ-calculus incorporates two simplifications that make this semantics simple.\nThe first simplification is that the λ-calculus treats functions \"anonymously\", without giving them explicit names. For example, the function \ncan be rewritten in \"anonymous form\" as \n(read as \"a tuple of and is mapped to formula_3\"). Similarly, \ncan be rewritten in anonymous form as\nwhere the input is simply mapped to itself.\n\nThe second simplification is that the λ-calculus only uses functions of a single input. An ordinary function that requires two inputs, for instance the formula_6 function, can be reworked into an equivalent function that accepts a single input, and as output returns \"another\" function, that in turn accepts a single input. For example, \ncan be reworked into \nThis method, known as currying, transforms a function that takes multiple arguments into a chain of functions each with a single argument.\n\nFunction application of the formula_6 function to the arguments (5, 2), yields at once\nwhereas evaluation of the curried version requires one more step\nto arrive at the same result.\n\nThe lambda calculus consists of a language of lambda terms, which is defined by a certain formal syntax, and a set of transformation rules, which allow manipulation of the lambda terms. These transformation rules can be viewed as an equational theory or as an operational definition.\n\nAs described above, all functions in the lambda calculus are anonymous functions, having no names. They only accept one input variable, with currying used to implement functions with several variables.\n\nThe syntax of the lambda calculus defines some expressions as valid lambda calculus expressions and some as invalid, just as some strings of characters are valid C programs and some are not. A valid lambda calculus expression is called a \"lambda term\".\n\nThe following three rules give an inductive definition that can be applied to build all syntactically valid lambda terms:\nNothing else is a lambda term. Thus a lambda term is valid if and only if it can be obtained by repeated application of these three rules. However, some parentheses can be omitted according to certain rules. For example, the outermost parentheses are usually not written. See \"Notation\", below.\n\nA lambda abstraction formula_28 is a definition of an anonymous function that is capable of taking a single input formula_15 and substituting it into the expression formula_22. \nIt thus defines an anonymous function that takes formula_15 and returns formula_22. For example, formula_33 is a lambda abstraction for the function formula_34 using the term formula_35 for formula_22. The definition of a function with a lambda abstraction merely \"sets up\" the function but does not invoke it. The abstraction binds the variable formula_15 in the term formula_22.\n\nAn application formula_39 represents the application of a function formula_22 to an input formula_26, that is, it represents the act of calling function formula_22 on input formula_26 to produce formula_44.\n\nThere is no concept in lambda calculus of variable declaration. In a definition such as formula_45 (i.e. formula_46), the lambda calculus treats formula_18 as a variable that is not yet defined. The lambda abstraction formula_45 is syntactically valid, and represents a function that adds its input to the yet-unknown formula_18.\n\nBracketing may be used and may be needed to disambiguate terms. For example, formula_50 and formula_51 denote different terms (although they coincidentally reduce to the same value). Here the first example defines a function that defines a function and returns the result of applying x to the child-function (apply function then return), while the second example defines a function that returns a function for any input and then returns it on application of x (return function then apply).\n\nIn lambda calculus, functions are taken to be 'first class values', so functions may be used as the inputs, or be returned as outputs from other functions.\n\nFor example, formula_52 represents the identity function, formula_5, and formula_54 represents the identity function applied to formula_18. Further, formula_56 represents the constant function formula_57, the function that always returns formula_18, no matter the input. In lambda calculus, function application is regarded as left-associative, so that formula_59 means formula_60.\n\nThere are several notions of \"equivalence\" and \"reduction\" that allow lambda terms to be \"reduced\" to \"equivalent\" lambda terms.\n\nA basic form of equivalence, definable on lambda terms, is alpha equivalence. It captures the intuition that the particular choice of a bound variable, in a lambda abstraction, does not (usually) matter.\nFor instance, formula_52 and formula_62 are alpha-equivalent lambda terms, and they both represent the same function (the identity function). \nThe terms formula_15 and formula_18 are not alpha-equivalent, because they are not bound in a lambda abstraction.\nIn many presentations, it is usual to identify alpha-equivalent lambda terms.\n\nThe following definitions are necessary in order to be able to define beta reduction:\n\nThe free variables of a term are those variables not bound by a lambda abstraction. The set of free variables of an expression is defined inductively:\n\nFor example, the lambda term representing the identity formula_52 has no free variables, but the function formula_74 has a single free variable, formula_18.\n\nSuppose formula_22, formula_26 and formula_78 are lambda terms and formula_15 and formula_18 are variables.\nThe notation formula_81 indicates substitution of formula_78 for formula_15 in formula_22 in a \"capture-avoiding\" manner. This is defined so that:\n\nFor example, formula_96, and formula_97.\n\nThe freshness condition (requiring that formula_18 is not in the free variables of formula_78) is crucial in order to ensure that substitution does not change the meaning of functions.\nFor example, a substitution is made that ignores the freshness condition: formula_100. This substitution turns the constant function formula_101 into the identity formula_52 by substitution.\n\nIn general, failure to meet the freshness condition can be remedied by alpha-renaming with a suitable fresh variable.\nFor example, switching back to our correct notion of substitution, in formula_103 the lambda abstraction can be renamed with a fresh variable formula_104, to obtain formula_105, and the meaning of the function is preserved by substitution.\n\nThe beta reduction rule states that an application of the form formula_106 reduces to the term formula_107. The notation formula_108 is used to indicate that formula_109 beta reduces to formula_110.\nFor example, for every formula_26, formula_112. This demonstrates that formula_113 really is the identity.\nSimilarly, formula_114, which demonstrates that formula_115 is a constant function.\n\nThe lambda calculus may be seen as an idealised version of a functional programming language, like Haskell or Standard ML.\nUnder this view, beta reduction corresponds to a computational step. This step can be repeated by additional beta conversions until there are no more applications left to reduce. In the untyped lambda calculus, as presented here, this reduction process may not terminate.\nFor instance, consider the term formula_116.\nHere formula_117.\nThat is, the term reduces to itself in a single beta reduction, and therefore the reduction process will never terminate.\n\nAnother aspect of the untyped lambda calculus is that it does not distinguish between different kinds of data.\nFor instance, it may be desirable to write a function that only operates on numbers. However, in the untyped lambda calculus, there is no way to prevent a function from being applied to truth values, strings, or other non-number objects.\n\nLambda expressions are composed of:\nThe set of lambda expressions, Λ, can be defined inductively:\nInstances of rule 2 are known as abstractions and instances of rule 3 are known as applications.\n\nTo keep the notation of lambda expressions uncluttered, the following conventions are usually applied:\n\nThe abstraction operator, λ, is said to bind its variable wherever it occurs in the body of the abstraction. Variables that fall within the scope of an abstraction are said to be \"bound\". All other variables are called \"free\". For example, in the expression λ\"y\".\"x\" \"x\" \"y\", y is a bound variable and x is free. Also note that a variable is bound by its \"nearest\" abstraction. In the following example the single occurrence of x in the expression is bound by the second lambda: λ\"x\".\"y\" (λ\"x\".\"z\" \"x\")\n\nThe set of \"free variables\" of a lambda expression, M, is denoted as FV(M) and is defined by recursion on the structure of the terms, as follows:\n\nAn expression that contains no free variables is said to be \"closed\". Closed lambda expressions are also known as combinators and are equivalent to terms in combinatory logic.\n\nThe meaning of lambda expressions is defined by how expressions can be reduced.\n\nThere are three kinds of reduction:\nWe also speak of the resulting equivalences: two expressions are \"β-equivalent\", if they can be β-converted into the same expression, and α/η-equivalence are defined similarly.\n\nThe term \"redex\", short for \"reducible expression\", refers to subterms that can be reduced by one of the reduction rules. For example, (λ\"x\".M) N is a beta-redex in expressing the substitution of N for x in M. The expression to which a redex reduces is called its reduct; the reduct of (λ\"x\".M) N is M[\"x\":=N].\n\nIf \"x\" is not free in M, λ\"x\".M \"x\" is also an eta-redex, with a reduct of M.\n\nAlpha-conversion, sometimes known as alpha-renaming, allows bound variable names to be changed. For example, alpha-conversion of λ\"x\".\"x\" might yield λ\"y\".\"y\". Terms that differ only by alpha-conversion are called \"α-equivalent\". Frequently, in uses of lambda calculus, α-equivalent terms are considered to be equivalent.\n\nThe precise rules for alpha-conversion are not completely trivial. First, when alpha-converting an abstraction, the only variable occurrences that are renamed are those that are bound to the same abstraction. For example, an alpha-conversion of λ\"x\".λ\"x\".\"x\" could result in λ\"y\".λ\"x\".\"x\", but it could \"not\" result in λ\"y\".λ\"x\".\"y\". The latter has a different meaning from the original. This is analogous to the programming notion of variable shadowing.\n\nSecond, alpha-conversion is not possible if it would result in a variable getting captured by a different abstraction. For example, if we replace \"x\" with \"y\" in λ\"x\".λ\"y\".\"x\", we get λ\"y\".λ\"y\".\"y\", which is not at all the same.\n\nIn programming languages with static scope, alpha-conversion can be used to make name resolution simpler by ensuring that no variable name masks a name in a containing scope (see alpha renaming to make name resolution trivial).\n\nIn the De Bruijn index notation, any two alpha-equivalent terms are syntactically identical.\n\nSubstitution, written , is the process of replacing all free occurrences of the variable in the expression with expression .\nSubstitution on terms of the λ-calculus is defined by recursion on the structure of terms, as follows (note: x and y are only variables while M and N are any λ expression).\n\nTo substitute into a lambda abstraction, it is sometimes necessary to α-convert the expression. For example, it is not correct for to result in , because the substituted was supposed to be free but ended up being bound. The correct substitution in this case is , up to α-equivalence. Notice that substitution is defined uniquely up to α-equivalence.\n\nBeta-reduction captures the idea of function application. Beta-reduction is defined in terms of substitution: the beta-reduction of  ((λ\"V\".\"E\") \"E′\")  is \"E\"[\"V\" := \"E′\"].\n\nFor example, assuming some encoding of 2, 7, ×, we have the following β-reduction: ((λ\"n\".\"n\"×2) 7) → 7×2.\n\nEta-conversion expresses the idea of extensionality, which in this context is that two functions are the same if and only if they give the same result for all arguments. Eta-conversion converts between λ\"x\".(\"f\" \"x\") and \"f\" whenever \"x\" does not appear free in \"f\".\n\nFor the untyped lambda calculus, β-reduction as a rewriting rule is neither strongly normalising nor weakly normalising.\n\nHowever, it can be shown that β-reduction is confluent. (Of course, we are working up to α-conversion, i.e. we consider two normal forms to be equal, if it is possible to α-convert one into the other.)\n\nTherefore, both strongly normalising terms and weakly normalising terms have a unique normal form. For strongly normalising terms, any reduction strategy is guaranteed to yield the normal form, whereas for weakly normalising terms, some reduction strategies may fail to find it.\n\nThe basic lambda calculus may be used to model booleans, arithmetic, data structures and recursion, as illustrated in the following sub-sections.\n\nThere are several possible ways to define the natural numbers in lambda calculus, but by far the most common are the Church numerals, which can be defined as follows:\nand so on. Or using the alternative syntax presented above in \"Notation\":\n\nA Church numeral is a higher-order function—it takes a single-argument function \"f\", and returns another single-argument function. The Church numeral \"n\" is a function that takes a function \"f\" as argument and returns the \"n\"-th composition of \"f\", i.e. the function \"f\" composed with itself \"n\" times. This is denoted \"f\" and is in fact the \"n\"-th power of \"f\" (considered as an operator); \"f\" is defined to be the identity function. Such repeated compositions (of a single function \"f\") obey the laws of exponents, which is why these numerals can be used for arithmetic. (In Church's original lambda calculus, the formal parameter of a lambda expression was required to occur at least once in the function body, which made the above definition of 0 impossible.)\n\nOne way of thinking about the Church numeral \"n\", which is often useful when analysing programs, is as an instruction 'repeat \"n\" times'. For example, using the PAIR and NIL functions defined below, one can define a function that constructs a (linked) list of \"n\" elements all equal to \"x\" by repeating 'prepend another \"x\" element' \"n\" times, starting from an empty list. The lambda term is\nBy varying what is being repeated, and varying what argument that function being repeated is applied to, a great many different effects can be achieved.\n\nWe can define a successor function, which takes a Church numeral \"n\" and returns \"n\" + 1 by adding another application of \"f\", where '(mf)x' means the function 'f' is applied 'm' times on 'x':\nBecause the \"m\"-th composition of \"f\" composed with the \"n\"-th composition of \"f\" gives the \"m\"+\"n\"-th composition of \"f\", addition can be defined as follows:\nPLUS can be thought of as a function taking two natural numbers as arguments and returning a natural number; it can be verified that\nand\nare β-equivalent lambda expressions. Since adding \"m\" to a number \"n\" can be accomplished by adding 1 \"m\" times, an alternative definition is:\nSimilarly, multiplication can be defined as\nAlternatively\nsince multiplying \"m\" and \"n\" is the same as repeating the add \"n\" function \"m\" times and then applying it to zero.\nExponentiation has a rather simple rendering in Church numerals, namely\nThe predecessor function defined by PRED \"n\" = \"n\" − 1 for a positive integer \"n\" and PRED 0 = 0 is considerably more difficult. The formula\ncan be validated by showing inductively that if \"T\" denotes (λ\"g\".λ\"h\".\"h\" (\"g\" \"f\")), then T(λ\"u\".\"x\") = (λ\"h\".\"h\"(\"f\"(\"x\"))) for \"n\" > 0. Two other definitions of PRED are given below, one using conditionals and the other using pairs. With the predecessor function, subtraction is straightforward. Defining\nSUB \"m\" \"n\" yields \"m\" − \"n\" when \"m\" > \"n\" and 0 otherwise.\n\nBy convention, the following two definitions (known as Church booleans) are used for the boolean values TRUE and FALSE:\nThen, with these two λ-terms, we can define some logic operators (these are just possible formulations; other expressions are equally correct):\nWe are now able to compute some logic functions, for example:\n\nand we see that AND TRUE FALSE is equivalent to FALSE.\n\nA \"predicate\" is a function that returns a boolean value. The most fundamental predicate is ISZERO, which returns TRUE if its argument is the Church numeral 0, and FALSE if its argument is any other Church numeral:\nThe following predicate tests whether the first argument is less-than-or-equal-to the second:\nand since \"m\" = \"n\", if LEQ \"m\" \"n\" and LEQ \"n\" \"m\", it is straightforward to build a predicate for numerical equality.\n\nThe availability of predicates and the above definition of TRUE and FALSE make it convenient to write \"if-then-else\" expressions in lambda calculus. For example, the predecessor function can be defined as:\nwhich can be verified by showing inductively that \"n\" (λ\"g\".λ\"k\".ISZERO (\"g\" 1) \"k\" (PLUS (\"g\" \"k\") 1)) (λ\"v\".0) is the add \"n\" − 1 function for \"n\" > 0.\n\nA pair (2-tuple) can be defined in terms of TRUE and FALSE, by using the Church encoding for pairs. For example, PAIR encapsulates the pair (\"x\",\"y\"), FIRST returns the first element of the pair, and SECOND returns the second.\n\nA linked list can be defined as either NIL for the empty list, or the PAIR of an element and a smaller list. The predicate NULL tests for the value NIL. (Alternatively, with NIL := FALSE, the construct \"l\" (λ\"h\".λ\"t\".λ\"z\".deal_with_head_\"h\"_and_tail_\"t\") (deal_with_nil) obviates the need for an explicit NULL test).\n\nAs an example of the use of pairs, the shift-and-increment function that maps (\"m\", \"n\") to (\"n\", \"n\" + 1) can be defined as\nwhich allows us to give perhaps the most transparent version of the predecessor function:\n\nThere is a considerable body of programming idioms for lambda calculus. Many of these were originally developed in the context of using lambda calculus as a foundation for programming language semantics, effectively using lambda calculus as a low-level programming language. Because several programming languages include the lambda calculus (or something very similar) as a fragment, these techniques also see use in practical programming, but may then be perceived as obscure or foreign.\n\nIn lambda calculus, a library would take the form of a collection of previously defined functions, which as lambda-terms are merely particular constants. The pure lambda calculus does not have a concept of named constants since all atomic lambda-terms are variables, but one can emulate having named constants by setting aside a variable as the name of the constant, using lambda-abstraction to bind that variable in the main body, and apply that lambda-abstraction to the intended definition. Thus to use \"f\" to mean \"M\" (some explicit lambda-term) in \"N\" (another lambda-term, the \"main program\"), one can say\nAuthors often introduce syntactic sugar, such as let, to permit writing the above in the more intuitive order\nBy chaining such definitions, one can write a lambda calculus \"program\" as zero or more function definitions, followed by one lambda-term using those functions that constitutes the main body of the program.\n\nA notable restriction of this let is that the name \"f\" is not defined in \"M\", since \"M\" is outside the scope of the lambda-abstraction binding \"f\"; this means a recursive function definition cannot be used as the \"M\" with let. The more advanced letrec syntactic sugar construction that allows writing recursive function definitions in that naive style instead additionally employs fixed-point combinators.\n\nRecursion is the definition of a function using the function itself. Lambda calculus cannot express this as directly as some other notations: all functions are anonymous in lambda calculus, so we can't refer to a value which is yet to be defined, inside the lambda term defining that same value. However, recursion can still be achieved by arranging for a lambda expression to receive itself as its argument value, for example in  (λ\"x\".\"x\" \"x\") \"E\".\n\nConsider the factorial function F(\"n\") recursively defined by\n\nIn the lambda expression which is to represent this function, a \"parameter\" (typically the first one) will be assumed to receive the lambda expression itself as its value, so that calling it – applying it to an argument – will amount to recursion. Thus to achieve recursion, the intended-as-self-referencing argument (called \"r\" here) must always be passed to itself within the function body, at a call point:\n\nThe self-application achieves replication here, passing the function's lambda expression on to the next invocation as an argument value, making it available to be referenced and called there.\n\nThis solves it but requires re-writing each recursive call as self-application. We would like to have a generic solution, without a need for any re-writes:\n\nGiven a lambda term with first argument representing recursive call (e.g. G here), the \"fixed-point\" combinator FIX will return a self-replicating lambda expression representing the recursive function (here, F). The function does not need to be explicitly passed to itself at any point, for the self-replication is arranged in advance, when it is created, to be done each time it is called. Thus the original lambda expression (FIX G) is re-created inside itself, at call-point, achieving self-reference.\n\nIn fact, there are many possible definitions for this FIX operator, the simplest of them being:\n\nIn the lambda calculus, Y \"g\"  is a fixed-point of \"g\", as it expands to:\n\nNow, to perform our recursive call to the factorial function, we would simply call (Y G) \"n\",  where \"n\" is the number we are calculating the factorial of. Given \"n\" = 4, for example, this gives:\n\nEvery recursively defined function can be seen as a fixed point of some suitably defined function closing over the recursive call with an extra argument, and therefore, using Y, every recursively defined function can be expressed as a lambda expression. In particular, we can now cleanly define the subtraction, multiplication and comparison predicate of natural numbers recursively.\n\nCertain terms have commonly accepted names:\nSeveral of these have direct applications in the \"elimination of lambda-abstraction\" that turns lambda terms into combinator calculus terms.\n\nIf \"N\" is a lambda-term without lambda-abstraction, but possibly containing named constants (combinators), then there exists a lambda-term \"T\"(\"x\",\"N\") which is equivalent to λ\"x\".\"N\" but lacks lambda-abstraction (except as part of the named constants, if these are considered non-atomic). This can also be viewed as anonymising variables, as \"T\"(\"x\",\"N\") removes all occurrences of \"x\" from \"N\", while still allowing argument values to be substituted into the positions where \"N\" contains an \"x\". The conversion function \"T\" can be defined by:\nIn either case, a term of the form \"T\"(\"x\",\"N\") \"P\" can reduce by having the initial combinator I, K, or S grab the argument \"P\", just like β-reduction of (λ\"x\".\"N\") \"P\" would do. I returns that argument. K throws the argument away, just like (λ\"x\".\"N\") would do if \"x\" has no free occurrence in \"N\". S passes the argument on to both subterms of the application, and then applies the result of the first to the result of the second.\n\nThe combinators B and C are similar to S, but pass the argument on to only one subterm of an application (B to the \"argument\" subterm and C to the \"function\" subterm), thus saving a subsequent K if there is no occurrence of \"x\" in one subterm. In comparison to B and C, the S combinator actually conflates two functionalities: rearranging arguments, and duplicating an argument so that it may be used in two places. The W combinator does only the latter, yielding the B, C, K, W system as an alternative to SKI combinator calculus.\n\nA typed lambda calculus is a typed formalism that uses the lambda-symbol (formula_118) to denote anonymous function abstraction. In this context, types are usually objects of a syntactic nature that are assigned to lambda terms; the exact nature of a type depends on the calculus considered (see kinds below). From a certain point of view, typed lambda calculi can be seen as refinements of the untyped lambda calculus but from another point of view, they can also be considered the more fundamental theory and \"untyped lambda calculus\" a special case with only one type.\n\nTyped lambda calculi are foundational programming languages and are the base of typed functional programming languages such as ML and Haskell and, more indirectly, typed imperative programming languages. Typed lambda calculi play an important role in the design of type systems for programming languages; here typability usually captures desirable properties of the program, e.g. the program will not cause a memory access violation.\n\nTyped lambda calculi are closely related to mathematical logic and proof theory via the Curry–Howard isomorphism and they can be considered as the internal language of classes of categories, e.g. the simply typed lambda calculus is the language of Cartesian closed categories (CCCs).\n\nA function \"F\": N → N of natural numbers is a computable function if and only if there exists a lambda expression \"f\" such that for every pair of \"x\", \"y\" in N, \"F\"(\"x\")=\"y\" if and only if \"f\" \"x\" = \"y\",  where \"x\" and \"y\" are the Church numerals corresponding to \"x\" and \"y\", respectively and = meaning equivalence with beta reduction. This is one of the many ways to define computability; see the Church–Turing thesis for a discussion of other approaches and their equivalence.\n\nThere is no algorithm that takes as input two lambda expressions and outputs TRUE or FALSE depending on whether or not the two expressions are equivalent. This was historically the first problem for which undecidability could be proven. As is common for a proof of undecidability, the proof shows that no computable function can decide the equivalence. Church's thesis is then invoked to show that no algorithm can do so.\n\nChurch's proof first reduces the problem to determining whether a given lambda expression has a \"normal form\". A normal form is an equivalent expression that cannot be reduced any further under the rules imposed by the form. Then he assumes that this predicate is computable, and can hence be expressed in lambda calculus. Building on earlier work by Kleene and constructing a Gödel numbering for lambda expressions, he constructs a lambda expression \"e\" that closely follows the proof of Gödel's first incompleteness theorem. If \"e\" is applied to its own Gödel number, a contradiction results.\n\nAs pointed out by Peter Landin's 1965 paper \"A Correspondence between ALGOL 60 and Church's Lambda-notation\", sequential procedural programming languages can be understood in terms of the lambda calculus, which provides the basic mechanisms for procedural abstraction and procedure (subprogram) application.\n\nFor example, in Lisp the \"square\" function can be expressed as a lambda expression as follows:\n\nThe above example is an expression that evaluates to a first-class function. The symbol codice_1 creates an anonymous function, given a list of parameter names, codice_2 – just a single argument in this case, and an expression that is evaluated as the body of the function, codice_3. Anonymous functions are sometimes called lambda expressions.\n\nFor example, Pascal and many other imperative languages have long supported passing subprograms as arguments to other subprograms through the mechanism of function pointers. However, function pointers are not a sufficient condition for functions to be first class datatypes, because a function is a first class datatype if and only if new instances of the function can be created at run-time. And this run-time creation of functions is supported in Smalltalk, JavaScript, and more recently in Scala, Eiffel (\"agents\"), C# (\"delegates\") and C++11, among others.\n\nWhether a term is normalising or not, and how much work needs to be done in normalising it if it is, depends to a large extent on the reduction strategy used. The distinction between reduction strategies relates to the distinction in functional programming languages between eager evaluation and lazy evaluation.\n\n\nApplicative order is not a normalising strategy. The usual counterexample is as follows: define Ω = ωω where ω = λ\"x\".\"xx\". This entire expression contains only one redex, namely the whole expression; its reduct is again Ω. Since this is the only available reduction, Ω has no normal form (under any evaluation strategy). Using applicative order, the expression KIΩ = (λ\"x\".λ\"y\".\"x\") (λ\"x\".\"x\")Ω is reduced by first reducing Ω to normal form (since it is the rightmost redex), but since Ω has no normal form, applicative order fails to find a normal form for KIΩ.\n\nIn contrast, normal order is so called because it always finds a normalising reduction, if one exists. In the above example, KIΩ reduces under normal order to \"I\", a normal form. A drawback is that redexes in the arguments may be copied, resulting in duplicated computation (for example, (λ\"x\".\"xx\") ((λ\"x\".\"x\")\"y\") reduces to ((λ\"x\".\"x\")\"y\") ((λ\"x\".\"x\")\"y\") using this strategy; now there are two redexes, so full evaluation needs two more steps, but if the argument had been reduced first, there would now be none).\n\nThe positive tradeoff of using applicative order is that it does not cause unnecessary computation, if all arguments are used, because it never substitutes arguments containing redexes and hence never needs to copy them (which would duplicate work). In the above example, in applicative order (λ\"x\".\"xx\") ((λ\"x\".\"x\")\"y\") reduces first to (λ\"x\".\"xx\")\"y\" and then to the normal order \"yy\", taking two steps instead of three.\n\nMost \"purely\" functional programming languages (notably Miranda and its descendents, including Haskell), and the proof languages of theorem provers, use \"lazy evaluation\", which is essentially the same as call by need. This is like normal order reduction, but call by need manages to avoid the duplication of work inherent in normal order reduction using \"sharing\". In the example given above, (λ\"x\".\"xx\") ((λ\"x\".\"x\")\"y\") reduces to ((λ\"x\".\"x\")\"y\") ((λ\"x\".\"x\")\"y\"), which has two redexes, but in call by need they are represented using the same object rather than copied, so when one is reduced the other is too.\n\nWhile the idea of beta reduction seems simple enough, it is not an atomic step, in that it must have a non-trivial cost when estimating computational complexity. To be precise, one must somehow find the location of all of the occurrences of the bound variable \"V\" in the expression \"E\", implying a time cost, or one must keep track of these locations in some way, implying a space cost. A naïve search for the locations of \"V\" in \"E\" is \"O\"(\"n\") in the length \"n\" of \"E\". This has led to the study of systems that use explicit substitution. Sinot's director strings offer a way of tracking the locations of free variables in expressions.\n\nThe Church–Rosser property of the lambda calculus means that evaluation (β-reduction) can be carried out in \"any order\", even in parallel. This means that various nondeterministic evaluation strategies are relevant. However, the lambda calculus does not offer any explicit constructs for parallelism. One can add constructs such as Futures to the lambda calculus. Other process calculi have been developed for describing communication and concurrency.\n\nIn Lévy's 1988 paper \"Sharing in the Evaluation of lambda Expressions\", he defines a notion of optimal sharing, such that no work is \"duplicated\". For example, performing a beta reduction in normal order on (λ\"x\".\"xx\") (II) reduces it to II (II). The argument II is duplicated by the application to the first lambda term. If the reduction was done in an applicative order first, we save work because work is not duplicated: (λ\"x\".\"xx\") (II) reduces to (λ\"x\".\"xx\") I. On the other hand, using applicative order can result in redundant reductions or even possibly never reduce to normal form. For example, performing a beta reduction in normal order on (λ\"f\".f I) (λy.(λ\"x\".\"xx\") (y I)) yields (λy.(λ\"x\".\"xx\") (y I)) I, (λ\"x\".\"xx\") (II) which we know we can do without duplicating work. Doing the same but in applicative order yields (λ\"f\".f I) (λy.y I (y I)), (λy.y I (y I)) I, I I (I I), and now work is duplicated.\n\nLévy shows the existence of lambda terms where there \"does not exist\" a sequence of reductions which reduces them without duplicating work. The below lambda term is such an example.\n\nIt is composed of three similar terms, x=((λg. ... ) (λh.y)) and y=((λf. ...) (λw.z) ), and finally z=λw.(h(w(λy.y))). There are only two possible beta reductions to be done here, on x and on y. Reducing the outer x term first results in the inner y term being duplicated, and each copy will have to be reduced, but reducing the inner y term first will duplicate its argument z, which will cause work to be duplicated when the values of h and w are made known. Incidentally, the above term reduces to the identity function (λy.y), and is constructed by making wrappers which make the identity function available to the binders g=λh..., f=λw..., h=λx.x (at first), and w=λz.z (at first), all of which are applied to the innermost term λy.y.\n\nThe precise notion of duplicated work relies on noticing that after the first reduction of I I is done, the value of the other I I can be determined, because they have the same structure (and in fact they have exactly the same values), and result from a common ancestor. Such similar structures can each be assigned a label that can be tracked across reductions. If a name is assigned to the redex that produces all the resulting II terms, and then all duplicated occurrences of II can be tracked and reduced in one go. However, it is not obvious that a redex will produce the II term. Identifying the structures that are similar in different parts of a lambda term can involve a complex algorithm and can possibly have a complexity equal to the history of the reduction itself.\n\nWhile Lévy defines the notion of optimal sharing, he does not provide an algorithm to do it. In Vincent van Oostrom, Kees-Jan van de Looij, and Marijn Zwitserlood's paper \"Lambdascope: Another optimal implementation of the lambda-calculus\", they provide such an algorithm by transforming lambda terms into interaction nets, which are then reduced. Roughly speaking, the resulting reduction is optimal because every term that would have the same labels as per Lévy's paper would also be the same graph in the interaction net. In the paper, they mention that their prototype implementation of Lambdascope performs as well as the \"optimised\" version of the reference optimal higher order machine BOHM.\n\nMore details can be found in the short article About the efficient reduction of lambda terms.\n\nThe fact that lambda calculus terms act as functions on other lambda calculus terms, and even on themselves, led to questions about the semantics of the lambda calculus. Could a sensible meaning be assigned to lambda calculus terms? The natural semantics was to find a set \"D\" isomorphic to the function space \"D\" → \"D\", of functions on itself. However, no nontrivial such \"D\" can exist, by cardinality constraints because the set of all functions from \"D\" to \"D\" has greater cardinality than \"D\", unless \"D\" is a singleton set.\n\nIn the 1970s, Dana Scott showed that, if only continuous functions were considered, a set or domain \"D\" with the required property could be found, thus providing a model for the lambda calculus.\n\nThis work also formed the basis for the denotational semantics of programming languages.\n\nMonographs/textbooks for graduate students:\n\n\"Some parts of this article are based on material from FOLDOC, used with .\"\n\n"}
{"id": "2038304", "url": "https://en.wikipedia.org/wiki?curid=2038304", "title": "Lazy caterer's sequence", "text": "Lazy caterer's sequence\n\nThe lazy caterer's sequence, more formally known as the central polygonal numbers, describes the maximum number of pieces of a disk (a pancake or pizza is usually used to describe the situation) that can be made with a given number of straight cuts. For example, three cuts across a pancake will produce six pieces if the cuts all meet at a common point inside the circle, but up to seven if they do not. This problem can be formalized mathematically as one of counting the cells in an arrangement of lines; for generalizations to higher dimensions, \"see\" arrangement of hyperplanes.\n\nThe analogue of this sequence in three dimensions is the cake number.\n\nThe maximum number \"p\" of pieces that can be created with a given number of cuts \"n\", where \"n\" ≥ 0, is given by the formula\n\nUsing binomial coefficients, the formula can be expressed as\n\nThis sequence , starting with formula_3, results in\n\nEach number equals 1 plus a triangular number.\n\nWhen a circle is cut \"n\" times to produce the maximum number of pieces, represented as \"p\" = \"ƒ\"(\"n\"), the \"n\"th cut must be considered; the number of pieces before the last cut is \"ƒ\"(\"n\" − 1), while the number of pieces added by the last cut is \"n\".\n\nTo obtain the maximum number of pieces, the \"n\"th cut line should cross all the other previous cut lines inside the circle, but not cross any intersection of previous cut lines. Thus, the \"n\"th line itself is cut in \"n\" − 1 places, and into \"n\" line segments. Each segment divides one piece of the (\"n\" − 1)-cut pancake into 2 parts, adding exactly \"n\" to the number of pieces. The new line can't have any more segments since it can only cross each previous line once. A cut line can always cross over all previous cut lines, as rotating the knife at a small angle around a point that is not an existing intersection will, if the angle is small enough, intersect all the previous lines including the last one added.\n\nThus, the total number of pieces after \"n\" cuts is\n\nThis recurrence relation can be solved. If \"ƒ\"(\"n\" − 1) is expanded one term the relation becomes\n\nExpansion of the term \"ƒ\"(\"n\" − 2) can continue until the last term is reduced to \"ƒ\"(0), thus,\n\nSince formula_7, because there is one piece before any cuts are made, this can be rewritten as\n\nThis can be simplified, using the formula for the sum of an arithmetic progression:\n\n"}
{"id": "18634", "url": "https://en.wikipedia.org/wiki?curid=18634", "title": "Lemma (mathematics)", "text": "Lemma (mathematics)\n\nIn mathematics, a \"helping theorem\" or lemma (plural lemmas or lemmata) is a proven proposition which is used as a stepping stone to a larger result rather than as a statement of interest by itself. The word derives from the Ancient Greek λῆμμα (\"anything which is received, such as a gift, profit, or a bribe\"). \n\nThere is no formal distinction between a lemma and a theorem, only one of intention – see Theorem terminology. However, a lemma can be considered a minor result whose sole purpose is to help prove a theorem  – a step in the direction of proof – or a short theorem appearing at an intermediate stage in a proof.\n\nA good stepping stone can lead to many others. Some powerful results in mathematics are known as lemmas, such as Bézout's lemma, Dehn's lemma, Euclid's lemma, Farkas' lemma, Fatou's lemma, Gauss's lemma, Greendlinger's lemma, Itō's lemma, Jordan's lemma, Nakayama's lemma, Poincaré's lemma, Riesz's lemma, Schur's lemma, Schwarz's lemma, Urysohn's lemma, Vitali covering lemma, Yoneda's lemma and Zorn's lemma. While these results originally seemed too simple or too technical to warrant independent interest, they have turned out to be central to the theories in which they occur.\n\n\n"}
{"id": "2174332", "url": "https://en.wikipedia.org/wiki?curid=2174332", "title": "Loop space", "text": "Loop space\n\nIn topology, a branch of mathematics, the loop space Ω\"X\" of a pointed topological space \"X\" is the space of (based) loops in \"X\", maps from the circle \"S\" to \"X\", equipped with the compact-open topology. Two loops can be multiplied by concatenation. With this operation, the loop space is an \"A\"-space. That is, the multiplication is homotopy coherently associative.\n\nThe set of path components of Ω\"X\", i.e. the set of based homotopy of based loops in \"X\", is a group, the fundamental group \"π\"(\"X\").\n\nThe iterated loop spaces of \"X\" are formed by applying Ω a number of times.\n\nThere is an analogous construction for topological spaces without basepoint. The free loop space of a topological space \"X\" is the space of maps from the circle \"S\" to \"X\" with the compact-open topology. The free loop space of \"X\" is often denoted by formula_1. \nAs a functor, the free loop space construction is right adjoint to cartesian product with the circle, while the loop space construction is right adjoint to the reduced suspension. This adjunction accounts for much of the importance of loop spaces in stable homotopy theory. (A related phenomenon in computer science is currying, where the cartesian product is adjoint to the hom functor.) Informally this is all referred to as Eckmann–Hilton duality.\n\nThe loop space is dual to the suspension of the same space; this duality is sometimes called Eckmann–Hilton duality. The basic observation is that\nwhere formula_3 is the set of homotopy classes of maps formula_4,\nand formula_5 is the suspension of A, and formula_6 denotes the natural homeomorphism. This homeomorphism is essentially that of currying, modulo the quotients needed to convert the products to reduced products.\n\nIn general, formula_7 does not have a group structure for arbitrary spaces formula_8 and formula_9. However, it can be shown that formula_10 and formula_11 do have natural group structures when formula_12 and formula_13 are pointed, and the aforesaid isomorphism is of those groups. \n\nThis follows, since the homotopy group is defined as formula_17, and the spheres can be obtained via suspensions of each-other: that is, formula_18. \n\n\n"}
{"id": "39723141", "url": "https://en.wikipedia.org/wiki?curid=39723141", "title": "M. T. Naraniengar", "text": "M. T. Naraniengar\n\nMandyam Tondanur Naraniengar (1871–1940) was an Indian mathematician. He first proved in 1909 the Morley's trisector theorem after it was posed in 1899 by Frank Morley.\n\nHe was the president of Indian Mathematical Society from 1930 to 1932 and the editor of the \"Journal of the Indian Mathematical Society\" from its founding in 1909 until 1927.\n\n"}
{"id": "36378173", "url": "https://en.wikipedia.org/wiki?curid=36378173", "title": "Martin Bridson", "text": "Martin Bridson\n\nMartin Robert Bridson is a Manx mathematician. He is \nthe Whitehead Professor of Pure Mathematics and Head of the Mathematical Institute at the University of Oxford. He is a Fellow of Magdalen College, Oxford, and an Honorary Fellow of Hertford College, Oxford. Specializing in geometry, topology and group theory, Bridson is best known for his work in Geometric Group Theory. \n\nBridson is a native of the Isle of Man. He was educated at St Ninian's High School, Douglas, then Hertford College, Oxford, and Cornell University,\nreceiving a Master of Arts degree from Oxford in 1986, and a Master of Science degree in 1988 followed by a PhD in 1991 from Cornell. His PhD thesis was supervised by Karen Vogtmann, and was entitled \"Geodesics and Curvature in Metric Simplicial Complexes\".\n\nHe was an Assistant Professor at Princeton University until 1996, was twice a Visiting Professor at the University of Geneva (1992 and 2006), and was Professor of Mathematics at Imperial College London from 2002 to 2007. From 1993 to 2002 he was a Tutorial Fellow of Pembroke College, Oxford, and Reader (1996) then Professor of Topology (2000) in the University of Oxford. He remains a Supernumerary Fellow of Pembroke College.\n\nBridson was an Invited Lecturer at the International Congress of Mathematicians in 2006.\n"}
{"id": "31383296", "url": "https://en.wikipedia.org/wiki?curid=31383296", "title": "MathMagic", "text": "MathMagic\n\nMathMagic is a mathematical WYSIWYG equation editor, available for Windows, macOS, Android and iOS since its debut in 1998. MathMagic is known for its DTP quality equations and widely used by Adobe InDesign and QuarkXPress users. MathMagic is a stand-alone multi-purpose equation editor application so its equation can be used by most software, such as word processors, presentation software, DTP layout software, and graphic software, via Copy and Paste, Drag and Drop, or by exporting to one of its supported formats.\n\nMathMagic product line includes \"MathMagic Personal Edition\", \"MathMagic Pro for Adobe InDesign\", \"MathMagic Pro for QuarkXPress\", and \"MathMagic Prime Edition\", depending on the configuration and their target market.\n\nIn June 2012, \"MathMagic Lite Edition\", their complimentary version, was introduced for Mac OS X and Android platforms, with some limited features according to their Feature Comparison Table. In September 2014, \"MathMagic Lite for Windows\" was released. Now, their Lite Edition is available on all Android OS, iOS, macOS, and Windows platforms.\n\nMathMagic supports MathML, LaTeX, Plain TeX, SVG, ASCIIMath, EPS, PDF, PICT, WMF, JPEG, GIF, BMP, PNG, TIFF, MathType equations, MS Equation Editor equations, MS Word 2007 equation, Google Docs equation, Zoho Writer equation, Math-To-Speech, and others. Some formats are platform specific. Although it is a WYSIWYG equation editor, MathMagic allows you type or paste LaTeX expressions directly into the editor window. MathML, ASCIIMathML, and other formats can also be pasted in or copied out. It also supports Text To Speech to read out mathematical expressions via the OS built-in TTS engine or a few internet based remote TTS services.\n\nMathMagic understands a certain level of natural English expressions to convert spoken language based Math reading into equation. For example, pasting \"y equals 3x plus 2a minus 2.5 squareroot b\" will form\n\nMathMagic Pro comes with all the features of Personal Edition, plus extra features and fonts for high-end users, and Plug-ins or XTensions to work directly with Adobe InDesign or QuarkXPress. MathMagic Pro for Adobe InDesign works with InDesign CS ~ CS6 and CC ~ CC2017. MathMagic Pro for QuarkXPress works with QuarkXPress 6.x ~ 9.x.\n\nMathMagic equation can be pasted into MS Word 2007 or newer's document in MathML format because MS Word's new built-in Equation editor can display and edit MathML.\n\nMathMagic does not support computation.\n\n\n\n"}
{"id": "2912292", "url": "https://en.wikipedia.org/wiki?curid=2912292", "title": "Mathematics of Sudoku", "text": "Mathematics of Sudoku\n\nThe class of Sudoku puzzles consists of a partially completed row-column grid of cells partitioned into \"N\" regions each of size \"N\" cells, to be filled in (\"solved\") using a prescribed set of \"N\" distinct symbols (typically the numbers {1, ..., \"N\"}), so that each row, column and region contains exactly one of each element of the set. The properties of Sudoku puzzles and their solutions can be investigated using mathematics and algorithms.\n\nThe analysis of Sudoku falls into two main areas: analyzing the properties of (1) completed grids and (2) puzzles. Also studied are computer algorithms to solve Sudokus, and to develop (or search for) new Sudokus. Analysis has largely focused on enumerating solutions, with results first appearing in 2004. There are many Sudoku variants, partially characterized by size (\"N\"), and the shape of their \"regions\". Unless noted, discussion in this article assumes classic Sudoku, i.e. \"N\"=9 (a 9×9 grid and 3×3 regions). A rectangular Sudoku uses rectangular regions of row-column dimension \"R\"×\"C\". Other variants include those with irregularly-shaped regions or with additional constraints (hypercube) or different constraint types (Samunamupure).\n\nA \"puzzle\" is a partially completed \"grid\", and the initial values are \"givens\" or \"clues\". \"Regions\" are also called \"blocks\" or \"boxes\". Horizontally adjacent \"rows\" are a \"band\", and vertically adjacent \"columns\" are a \"stack\". A \"proper\" puzzle has a unique solution. See Glossary of Sudoku for other terminology. Solving Sudokus from the viewpoint of a player has been explored in Denis Berthier's book \"The Hidden Logic of Sudoku\" (2007) which considers strategies such as \"hidden xy-chains\".\n\nThe general problem of solving Sudoku puzzles on \"n\"×\"n\" grids of \"n\"×\"n\" blocks is known to be NP-complete. For \"n\"=3 (classical Sudoku), however, this result is of little relevance: algorithms such as Dancing Links can solve puzzles in fractions of a second.\n\nA puzzle can be expressed as a graph coloring problem. The aim is to construct a 9-coloring of a particular graph, given a partial 9-coloring. The graph has 81 vertices, one vertex for each cell. The vertices are labeled with ordered pairs (\"x\", \"y\"), where \"x\" and \"y\" are integers between 1 and 9. In this case, two distinct vertices labeled by (\"x\", \"y\") and (\"x\"′, \"y\"′) are joined by an edge if and only if:\nThe puzzle is then completed by assigning an integer between 1 and 9 to each vertex, in such a way that vertices that are joined by an edge do not have the same integer assigned to them.\n\nA Sudoku solution grid is also a Latin square. There are significantly fewer Sudoku grids than Latin squares because Sudoku imposes the additional regional constraint. Nonetheless, the number of Sudoku grids was calculated by Bertram Felgenhauer and Frazer Jarvis in 2005 to be 6,670,903,752,021,072,936,960.  The number computed by Felgenhauer and Jarvis confirmed a result first identified by QSCGZ in September 2003. This number is equal to 9! × 72 × 2 × 27,704,267,971, the last factor of which is prime. The result was derived through logic and brute force computation. Russell and Jarvis also showed that when symmetries were taken into account, there were 5,472,730,538 solutions . The number of grids for 16×16 Sudoku is not known.\n\nThe number of minimal Sudokus (Sudokus in which no clue can be deleted without losing uniqueness of the solution) is not precisely known. However, statistical techniques combined with a generator (), show that there are approximately (with 0.065% relative error):\nOther authors used faster methods and calculated additional precise distribution statistics.\n\nAs in the case of Latin squares the (addition- or) multiplication tables (Cayley tables) of finite groups can be used to construct Sudokus and related tables of numbers. Namely, one has to take subgroups and quotient groups into account:\n\nTake for example formula_1 the group of pairs, adding each component separately modulo some formula_2.\nBy omitting one of the components, we suddenly find ourselves in formula_3 (and this mapping is obviously compatible with the respective additions, i.e. it is a group homomorphism).\nOne also says that the latter is a quotient group of the former, because some once different elements become equal in the new group.\nHowever, it is also a subgroup, because we can simply fill the missing component with formula_4 to get back to formula_1. \n\nUnder this view, we write down the example, Grid 1, for formula_6.\n\nEach Sudoku region looks the same on the second component (namely like the subgroup formula_7), because these are added regardless of the first one.\nOn the other hand, the first components are equal in each block, and if we imagine each block as one cell, these first components show the same pattern (namely the quotient group formula_7). As outlined in the article of Latin squares, this is a Latin square of order formula_9.\n\nNow, to yield a Sudoku, let us permute the rows (or equivalently the columns) in such a way, that each block is redistributed exactly once into each block – for example order them formula_10.\nThis of course preserves the Latin square property. Furthermore, in each block the lines have distinct first component by construction\nand each line in a block has distinct entries via the second component, because the blocks' second components originally formed a Latin square of order formula_11 (from the subgroup formula_7). Thus we arrive at a Sudoku (rename the pairs to numbers 1...9 if you wish). With the example and the row permutation above, we arrive at Grid 2.\n\nFor this method to work, one generally does not need a product of two equally-sized groups. A so-called short exact sequence of finite groups\nof appropriate size already does the job. Try for example the group formula_13 with quotient- and subgroup formula_14.\nIt seems clear (already from enumeration arguments), that not all Sudokus can be generated this way.\n\nThe answer to the question 'How many Sudoku grids are there?' depends on the definition of when similar solutions are considered different.\n\nFor the enumeration of \"all\" possible solutions, two solutions are considered distinct if any of their corresponding (81) cell values differ. Symmetry relations between similar solutions are ignored., e.g. the rotations of a solution are considered distinct. Symmetries play a significant role in the enumeration strategy, but not in the count of \"all\" possible solutions.\n\nThe first known solution to complete enumeration was posted by QSCGZ [Guenter Stertenbrink] to the \"rec.puzzles\" newsgroup in 2003, obtaining 6,670,903,752,021,072,936,960 () distinct solutions.\n\nIn a 2005 study, Felgenhauer and Jarvis analyzed the permutations of the top band used in valid solutions. Once the Band1 symmetries and equivalence classes for the partial grid solutions were identified, the completions of the lower two bands were constructed and counted for each equivalence class. Summing completions over the equivalence classes, weighted by class size, gives the total number of solutions as 6,670,903,752,021,072,936,960, confirming the value obtained by QSCGZ. The value was subsequently confirmed numerous times independently. A second enumeration technique based on band generation was later developed that is significantly less computationally intensive.\n\nTwo valid grids are \"essentially\" the same if one can be derived from the other. The following operations are Sudoku preserving symmetries and always translate a valid grid into another valid grid:\n\n\nThese operations define a symmetry relation between equivalent grids. Excluding relabeling, and with respect to the 81 grid cell values, the operations form a subgroup of the symmetric group S, of order 3!×2 = 3,359,232. Applying the above operations on majority of the grids results in 3!×2×9! essentially equivalent grids. Exceptions are Automorphic Sudokus which due to additional non-trivial symmetry generate fewer grids.\n\nThe number of distinct solutions can also be identified with Burnside's Lemma. For a solution, the set of equivalent solutions which can be reached using these operations (excluding relabeling), form an orbit of the symmetric group. The number of essentially different solutions is then the number of orbits, which can be computed using Burnside's lemma. The Burnside \"fixed points\" are solutions that differ only by relabeling. Using this technique, Jarvis/Russell computed the number of essentially different (symmetrically distinct) solutions as 5,472,730,538.\n\nEnumeration results for many Sudoku variants have been calculated: these are summarised below.\n\nIn the table, \"Dimensions\" are those of the regions (e.g. 3x3 in normal Sudoku). The \"Rel Err\" column indicates how a simple approximation, using the generalised method of Kevin Kilfoil, compares to the true grid count: it is an underestimate in all cases evaluated so far.\n\nFor large (\"R\",\"C\"), the method of Kevin Kilfoil (generalised method:) is used to estimate the number of grid completions. The method asserts that the Sudoku row and column constraints are, to first approximation, conditionally independent given the box constraint. Omitting a little algebra, this gives the Kilfoil-Silver-Pettersen formula:\nwhere \"b\" is the number of ways of completing a Sudoku band of \"R\" horizontally adjacent \"R\"×\"C\" boxes. Petersen's algorithm, as implemented by Silver, is currently the fastest known technique for exact evaluation of these \"b\".\n\nThe band counts for problems whose full Sudoku grid-count is unknown are listed below. As in the previous section, \"Dimensions\" are those of the regions.\n\nThe expression for the 4×C case is:\nformula_16\n\nwhere:\n\nThe following are all restrictions of 3x3 Sudokus. The type names have not been standardised: click on the attribution links to see the definitions.\n\nA Sudoku will remain valid under the actions of the Sudoku preserving symmetries (see also Jarvis). Some Sudokus are special in that some operations merely have the effect of relabelling the digits; several of these are listed below.\n\nFurther calculations of this ilk combine to show that the number of essentially different Sudoku grids is 5,472,730,538, for example as demonstrated by Jarvis / Russell and verified by Pettersen. Similar methods have been applied to the 2x3 case, where Jarvis / Russell showed that there are 49 essentially different grids (see also the article by Bailey, Cameron and Connelly), to the 2x4 case, where Russell showed that there are 1,673,187 essentially different grids (verified by Pettersen), and to the 2x5 case where Pettersen showed that there are 4,743,933,602,050,718 essentially different grids (not verified).\n\nOrdinary Sudokus (\"proper\" puzzles) have a unique solution. A \"minimal\" Sudoku is a Sudoku from which no clue can be removed leaving it a proper Sudoku. Different minimal Sudokus can have a different number of clues. This section discusses the minimum number of givens for proper puzzles.\n\nMany Sudokus have been found with 17 clues, although finding them is not a trivial task. A paper by Gary McGuire, Bastian Tugemann, and Gilles Civario, released on 1 January 2012, explains how it was proved through an exhaustive computer search that the minimum number of clues in any proper Sudoku is 17, and this was independently confirmed in September 2013. A few 17-clue puzzles with diagonal symmetry were provided by Ed Russell, after a search through equivalence transformations of Gordon Royle's database of 17-clue puzzles. Sudoku puzzles with 18 clues have been found with 180° rotational symmetry, and others with orthogonal symmetry, although it is not known if this number of clues is minimal in either case. Sudoku puzzles with 19 clues have been found with two-way orthogonal symmetry, and again it is unknown if this number of clues is minimal for this case.\n\nA Sudoku with 24 clues, dihedral symmetry (symmetry on both orthogonal axis, 90° rotational symmetry, 180° rotational symmetry, and diagonal symmetry) is known to exist, and is also automorphic. Again here, it is not known if this number of clues is minimal for this class of Sudoku. The fewest clues in a Sudoku with two-way diagonal symmetry is believed to be 18, and in at least one case such a Sudoku also exhibits automorphism.\n\n\nAdditional constraints (here, on 3×3 Sudokus) lead to a smaller minimum number of clues.\n\n\"Du-sum-oh\" (a.k.a. \"geometry number place\") puzzles replace the 3×3 (or \"R\"×\"C\") regions of Sudoku with irregular shapes of a fixed size. Bob Harris has proved that it is always possible to create (\"N\" − 1)-clue du-sum-ohs on an \"N\"×\"N\" grid, and has constructed several examples. Johan de Ruiter has proved that for any \"N\">3 there exist polyomino tilings that can not be turned into a Sudoku puzzle with \"N\" irregular shapes of size \"N\".\n\nIn sum number place (Samunampure), the regions are of irregular shape and various sizes. The usual constraints of no repeated value in any row, column or region apply. The clues are given as sums of values within regions (e.g. a 4-cell region with sum 10 must consist of values 1,2,3,4 in some order). The minimum number of clues for Samunampure is not known, nor even conjectured. A variant on Miyuki Misawa's web site replaces sums with relations: the clues are symbols =, < and > showing the relative values of (some but not all) adjacent region sums. She demonstrates an example with only eight relations. It is not known whether this is the best possible.\n\nThe most clues for a \"minimal\" Sudoku is believed to be 40, of which only two are known. If any clue is removed from either of these Sudokus, the puzzle would have more than one solution (and thus not be a proper Sudoku). In the work to find these Sudokus, other high-clue puzzles were catalogued, including more than 6,500,000,000 minimal puzzles with 36 clues. About 2600 minimal Sudokus with 39 clues were also found.\n\nIf dropping the requirement for the uniqueness of the solution, 41-clue minimal pseudo-puzzles are known to exist, but they can be completed to more than one solution grid. Removal of any clue increases the number of the completions and from this perspective none of the 41 clues is redundant. With slightly more than half the grid filled with givens (41 of 81 cells), the \"uniqueness\" of the solution constraint still dominates over the \"minimality\" constraint.\n\nAs for the \"most\" clues possible in a Sudoku while still \"not\" rendering a unique solution, it is four short of a full grid (77). If two instances of two numbers each are missing and the cells they are to occupy are the corners of an orthogonal rectangle, and exactly two of these cells are within one region, there are two ways the last digits can be added (two solutions).\n\nIt has been conjectured that no proper Sudoku can have clues limited to the range of positions in the pattern above (first image). The largest rectangular orthogonal \"hole\" (region with no clues) in a proper Sudoku is believed to be a rectangle of 30 cells (a 5 x 6 rectangular area). One example is a Sudoku with 22 clues (second image). The largest total number of empty groups (rows, columns, and boxes) in a Sudoku is believed to be nine. One example is a Sudoku with 3 empty rows, 3 empty columns, and 3 empty boxes (third image).\n\nA Sudoku grid is automorphic if it can be transformed in a way that leads back to the original grid, when that same transformation would not otherwise lead back to the original grid. One example of a grid which is automorphic would be a grid which can be rotated 180 degrees resulting in a new grid where the new cell values are a permutation of the original grid. Automorphic Sudokus are Sudoku puzzles which solve to an automorphic grid. Two examples of automorphic Sudokus, and an automorphic grid are shown below.\n\nIn the first two examples, notice that if the Sudoku is rotated 180 degrees, and the clues relabeled with the permutation (123456789) -> (987654321), it returns to the same Sudoku. Expressed another way, these Sudokus have the property that every 180 degree rotational pair of clues (a, b) follows the rule (a) + (b) = 10.\n\nSince these Sudokus are automorphic, so too their solutions grids are automorphic. Furthermore, every cell which is solved has a symmetrical partner which is solved with the same technique (and the pair would take the form a + b = 10). Notice that in the second example, the Sudouku also exhibits translational (or repetition) symmetry; clues are clustered in groups, with the clues in each group ordered sequentially (i.e., n, n+1, n+2, and n+3).\n\nThe third image is the \"Most Canonical\" solution grid. This grid has 648 automorphisms and contributes to all ~ solution grids by factor of 1/648 compared to any non-automorphic grid.\n\nIn these examples the automorphisms are easy to identify, but in general automorphism is not always obvious. The table at right shows the number of the essentially different Sudoku solution grids for all existing automorphisms.\n\nAn enumeration technique based on \"band generation\" was developed that is significantly less computationally intensive. The strategy begins by analyzing the permutations of the top band used in valid solutions. Once the Band1 symmetries and equivalence class for the partial solutions are identified, the completions of the lower two bands are constructed and counted for each equivalence class.\n\nThe Band1 algorithm proceeds as follows:\nThe Band1 symmetries (above) are \"solution permutation symmetries\" defined so that a permuted solution is also a solution. For the purpose of enumerating solutions, a \"counting symmetry\" for grid completion can be used to define band equivalence classes that yield a minimal number of classes.\n\n\"Counting symmetry\" partitions valid Band1 permutations into classes that place the same completion constraints on lower bands; all members of a band \"counting symmetry\" equivalence class must have the same number of grid completions since the completion constraints are equivalent. Counting symmetry constraints are identified by the Band1 column triplets (a column value set, no implied element order). Using band counting symmetry, a minimal generating set of 44 equivalence classes was established.\n\nThe following sequence demonstrates mapping a band configuration to a counting symmetry equivalence class. Begin with a valid band configuration (1). Build column triplets by ordering the column values within each column. This is not a valid Sudoku band, but does place the same constraints on the lower bands as the example (2). Construct an equivalence class ID from the B2, B3 column triplet values. Use column and box swaps to achieve the lowest lexicographical ID. The last figure shows the column and box ordering for the ID: 124 369 578 138 267 459. All Band1 permutations with this counting symmetry ID will have the same number of grid completions as the original example. An extension of this process can be used to build the largest possible \"band counting symmetry\" equivalence classes (3).\n\nNote, while column triplets are used to construct and identify the equivalence classes, the class members themselves are the valid Band1 permutations: class size (Sb.z) reflects column triplet permutations compatible with the \"One Rule\" solution requirements. \"Counting symmetry\" is a completion property and applies only to a partial grid (band or stack). \"Solution symmetry\" for preserving solutions can be applied to either partial grids (bands, stacks) or full grid solutions. Lastly note, \"counting symmetry\" is more restrictive than simple numeric completion count equality: two (distinct) bands belong to the same \"counting symmetry\" equivalence class only if they impose equivalent completion constraints.\n\nSymmetries group similar object into equivalence classes. Two numbers need to be distinguished for equivalence classes, and band symmetries as used here, a third:\n\nThe Band1 (6) symmetries divide the (56×6) Band1 valid permutations into (not less than) 336 (56×6) equivalence classes with (up to) 6^5 permutations each.\nThe \"not less than\" and \"up to\" caveats are necessary, since some combinations of the transformations may not produce distinct results, when relabeling is required (see below). Consequently, some equivalence classes may contain less than 6 distinct permutations and the theoretical minimum number of classes may not be achieved.\n\nEach of the valid Band1 permutations can be expanded (completed) into a specific number of solutions with the Band2,3 permutations. By virtue of their similarity, each member of an equivalence class will have the same number of completions. Consequently, we only need to construct the solutions for one member of each equivalence class and then multiply the number of solutions by the size of the equivalence class. We are still left with the task of identifying and calculating the size of each equivalence class. Further progress requires the dexterous application of computational techniques to catalogue (classify and count) the permutations into equivalence classes.\n\nFelgenhauer/Jarvis catalogued the Band1 permutations using lexicographical ordered IDs based on the ordered digits from blocks B2,3. Block 1 uses a canonical digit assignment and is not needed for a unique ID. Equivalence class identification and linkage uses the lowest ID within the class.\n\nApplication of the (2×6) B2,3 symmetry permutations produces 36288 (28×6) equivalence classes, each of size 72. Since the size is fixed, the computation only needs to find the 36288 equivalence class IDs. (Note: in this case,\nfor any Band1 permutation, applying these permutations to achieve the lowest ID provides an index to the associated equivalence class.)\n\nApplication of the rest of the block, column and row symmetries provided further reduction, i.e. allocation of the 36288 IDs into fewer, larger equivalence classes.\nWhen the B1 canonical labeling is lost through a transformation, the result is relabeled to the canonical B1 usage and then catalogued under this ID. This approach generated 416 equivalence classes, somewhat less effective than the theoretical 336 minimum limit for a full reduction. Application of \"counting symmetry\" patterns for \"duplicate paired\" digits achieved reduction to 174 and then to 71 equivalence classes. The introduction of equivalence classes based on \"band counting symmetry\" (subsequent to Felgenhauer/Jarvis by Russell) reduced the equivalence classes to a minimum generating set of 44.\n\nThe diversity of the ~, 56×6 Band1 permutations can be reduced to a set of 44 Band1 equivalence classes. Each of the 44 equivalence classes can be expanded to millions of distinct full solutions, but the entire solution space has a common origin in these 44. The 44 equivalence classes play a central role in other enumeration approaches as well, and speculation will return to the characteristics of the 44 classes when puzzle properties are explored later.\n\nEnumerating the Sudoku solutions breaks into an initial setup stage and then into two nested loops. Initially all the valid Band1 permutations are grouped into equivalence classes, who each impose a common constraint on the Band2,3 completions.\nFor each of the Band1 equivalence classes, all possible Band2,3 solutions need to be enumerated. An outer Band1 loop iterates over the 44 equivalence classes. In the inner loop, all lower band completions for each of the Band1 equivalence class are found and counted.\n\nThe computation required for the lower band solution search can be minimised by the same type of symmetry application used for Band1. There are 6! (720) permutations for the 6 values in column 1 of Band2,3. Applying the lower band (2) and row within band (6×6) permutations creates 10 equivalence classes of size 72. At this point, completing 10 sets of solutions for the remaining 48 cells with a recursive descent, backtracking algorithm is feasible with 2 GHz class PC so further simplification is not required to carry out the enumeration. Using this approach, the number of ways of filling in a blank Sudoku grid has been shown to be 6,670,903,752,021,072,936,960 ().\n\nThe result, as confirmed by Russell, also contains the distribution of solution counts for the 44 equivalence classes. The listed values are before application of the 9! factor for labeling and the two 72 factors (72 = 5184) for each of Stack 2,3 and Band2,3 permutations. The number of completions for each class is consistently on the order of 100,000,000, while the number of Band1 permutations covered by each class however varies from 4 – 3240. Within this wide size range, there are clearly two clusters. Ranked by size, the lower 33 classes average ~400 permutations/class, while the upper 11 average ~2100. The disparity in consistency between the distributions for size and number of completions or the separation into two clusters by size is yet to be examined.\n\nSudoku regions are polyominoes. Although the classic 9×9 Sudoku is made of square nonominoes, it is possible to apply the rules of Sudoku to puzzles of other sizes – the 2×2 and 4×4 square puzzles, for example. Only \"N\"×\"N\" Sudoku puzzles can be tiled with square polyominoes. Another popular variant is made of rectangular regions – for example, 2×3 hexominoes tiled in a 6×6 grid. The following notation is used for discussing this variant.\n\nPuzzles of size \"N\"×\"N\", where \"N\" is prime can only be tiled with irregular \"N\"-ominoes. Each \"N\"×\"N\" grid can be tiled multiple ways with \"N\"-ominoes. Before enumerating the number of solutions to a Sudoku grid of size \"N\"x\"N\", it is necessary to determine how many \"N\"-omino tilings exist for a given size (including the standard square tilings, as well as the rectangular tilings).\n\nThe size ordering of Sudoku puzzle can be used to define an integer series, e.g. for square Sudoku, the integer series of possible solutions . Sudoku with square \"N\"×\"N\" regions are more symmetrical than immediately obvious from the \"One Rule\". Each row and column intersects \"N\" regions and shares \"N\" cells with each. The number of bands and stacks also equals \"N\". Rectangular Sudoku do not have these properties. The \"3×3\" Sudoku is additionally unique: \"N\" is also the number of row-column-region constraints from the \"One Rule\" (i.e. there are \"N\"=3 types of \"units\"). See the Glossary of Sudoku for an expanded list of variants.\n\n\n"}
{"id": "375380", "url": "https://en.wikipedia.org/wiki?curid=375380", "title": "Mereology", "text": "Mereology\n\nIn philosophy and mathematical logic, mereology (from the Greek μέρος \"meros\" (root: μερε- \"mere-\", \"part\" [but in Greek: \"μερολογία\"]) and the suffix -logy \"study, discussion, science\") is the study of parts and the wholes they form. Whereas set theory is founded on the membership relation between a set and its elements, mereology emphasizes the meronomic relation between entities, which—from a set-theoretic perspective—is closer to the concept of inclusion between sets.\n\nMereology has been explored in various ways as applications of predicate logic to formal ontology, in each of which mereology is an important part. Each of these fields provides its own axiomatic definition of mereology. A common element of such axiomatizations is the assumption, shared with inclusion, that the part-whole relation orders its universe, meaning that everything is a part of itself (reflexivity), that a part of a part of a whole is itself a part of that whole (transitivity), and that two distinct entities cannot each be a part of the other (antisymmetry), thus forming a poset. A variant of this axiomatization denies that anything is ever part of itself (irreflexivity) while accepting transitivity, from which antisymmetry follows automatically.\n\nAlthough mereology is an application of mathematical logic, what could be argued to be a sort of \"proto-geometry\", it has been wholly developed by logicians, ontologists, linguists, engineers, and computer scientists, especially those working in artificial intelligence. In particular, mereology is also on the basis for a point-free foundation of geometry (see for example the quoted pioniering paper of Alfred Tarski and the review paper by Gerla 1995).\n\n\"Mereology\" can also refer to formal work in general systems theory on system decomposition and parts, wholes and boundaries (by, e.g., Mihajlo D. Mesarovic (1970), Gabriel Kron (1963), or Maurice Jessel (see Bowden (1989, 1998)). A hierarchical version of Gabriel Kron's Network Tearing was published by Keith Bowden (1991), reflecting David Lewis's ideas on gunk. Such ideas appear in theoretical computer science and physics, often in combination with sheaf theory, topos, or category theory. See also the work of Steve Vickers on (parts of) specifications in computer science, Joseph Goguen on physical systems, and Tom Etter (1996, 1998) on link theory and quantum mechanics.\n\nInformal part-whole reasoning was consciously invoked in metaphysics and ontology from Plato (in particular, in the second half of the \"Parmenides\") and Aristotle onwards, and more or less unwittingly in 19th-century mathematics until the triumph of set theory around 1910. Ivor Grattan-Guinness (2001) sheds much light on part-whole reasoning during the 19th and early 20th centuries, and reviews how Cantor and Peano devised set theory. In seventh century India, parts and wholes were studied extensively by Dharmakirti (see ). In Europe, however, it appears that the first to reason consciously and at length about parts and wholes was Edmund Husserl, in 1901, in the second volume of \"Logical Investigations\" – Third Investigation: \"On the Theory of Wholes and Parts\" (Husserl 1970 is the English translation). However, the word \"mereology\" is absent from his writings, and he employed no symbolism even though his doctorate was in mathematics.\n\nStanisław Leśniewski coined \"mereology\" in 1927, from the Greek word μέρος (\"méros\", \"part\"), to refer to a formal theory of part-whole he devised in a series of highly technical papers published between 1916 and 1931, and translated in Leśniewski (1992). Leśniewski's student Alfred Tarski, in his Appendix E to Woodger (1937) and the paper translated as Tarski (1984), greatly simplified Leśniewski's formalism. Other students (and students of students) of Lesniewski elaborated this \"Polish mereology\" over the course of the 20th century. For a good selection of the literature on Polish mereology, see Srzednicki and Rickey (1984). For a survey of Polish mereology, see Simons (1987). Since 1980 or so, however, research on Polish mereology has been almost entirely historical in nature.\n\nA. N. Whitehead planned a fourth volume of \"Principia Mathematica\", on geometry, but never wrote it. His 1914 correspondence with Bertrand Russell reveals that his intended approach to geometry can be seen, with the benefit of hindsight, as mereological in essence. This work culminated in Whitehead (1916) and the mereological systems of Whitehead (1919, 1920).\n\nIn 1930, Henry Leonard completed a Harvard Ph.D. dissertation in philosophy, setting out a formal theory of the part-whole relation. This evolved into the \"calculus of individuals\" of Goodman and Leonard (1940). Goodman revised and elaborated this calculus in the three editions of Goodman (1951). The calculus of individuals is the starting point for the post-1970 revival of mereology among logicians, ontologists, and computer scientists, a revival well-surveyed in Simons (1987) and Casati and Varzi (1999).\n\nReflexivity: A basic choice in defining a mereological system, is whether to consider things to be parts of themselves. In naive set theory a similar question arises: whether a set is to be considered a \"subset\" of itself. In both cases, \"yes\" gives rise to paradoxes analogous to Russell's paradox: Let there be an object O such that every object that is not a proper part of itself is a proper part of O. Is O a proper part of itself? No, because no object is a proper part of itself; and yes, because it meets the specified requirement for inclusion as a proper part of O. In set theory, a set is often termed an \"improper\" subset of itself. Given such paradoxes, mereology requires an axiomatic formulation.\n\nA mereological \"system\" is a first-order theory (with identity) whose universe of discourse consists of wholes and their respective parts, collectively called \"objects\". Mereology is a collection of nested and non-nested axiomatic systems, not unlike the case with modal logic.\n\nThe treatment, terminology, and hierarchical organization below follow Casati and Varzi (1999: Ch. 3) closely. For a more recent treatment, correcting certain misconceptions, see Hovda (2008). Lower-case letters denote variables ranging over objects. Following each symbolic axiom or definition is the number of the corresponding formula in Casati and Varzi, written in bold.\n\nA mereological system requires at least one primitive binary relation (dyadic predicate). The most conventional choice for such a relation is parthood (also called \"inclusion\"), \"\"x\" is a \"part\" of \"y\"\", written \"Pxy\". Nearly all systems require that parthood partially order the universe. The following defined relations, required for the axioms below, follow immediately from parthood alone:\nOverlap and Underlap are reflexive, symmetric, and intransitive.\n\nSystems vary in what relations they take as primitive and as defined. For example, in extensional mereologies (defined below), \"parthood\" can be defined from Overlap as follows:\n\nThe axioms are:\n\n\n\n\n\n\n\nSimons (1987), Casati and Varzi (1999) and Hovda (2008) describe many mereological systems whose axioms are taken from the above list. We adopt the boldface nomenclature of Casati and Varzi. The best-known such system is the one called \"classical extensional mereology\", hereinafter abbreviated CEM (other abbreviations are explained below). In CEM, P.1 through P.8' hold as axioms or are theorems. M9, \"Top\", and \"Bottom\" are optional.\n\nThe systems in the table below are partially ordered by inclusion, in the sense that, if all the theorems of system A are also theorems of system B, but the converse is not necessarily true, then B \"includes\" A. The resulting Hasse diagram is similar to that in Fig. 2, and Fig. 3.2 in Casati and Varzi (1999: 48).\n\nThere are two equivalent ways of asserting that the universe is partially ordered: Assume either M1–M3, or that Proper \"Parthood\" is transitive and asymmetric, hence a strict partial order. Either axiomatization results in the system M. M2 rules out closed loops formed using \"Parthood\", so that the part relation is well-founded. Sets are well-founded if the axiom of regularity is assumed. The literature contains occasional philosophical and common-sense objections to the transitivity of \"Parthood\".\n\nM4 and M5 are two ways of asserting \"supplementation\", the mereological analog of set complementation, with M5 being stronger because M4 is derivable from M5. M and M4 yield \"minimal\" mereology, MM. MM, reformulated in terms of Proper Part, is Simons's (1987) preferred minimal system.\n\nIn any system in which M5 or M5' are assumed or can be derived, then it can be proved that two objects having the same proper parts are identical. This property is known as \"Extensionality\", a term borrowed from set theory, for which extensionality is the defining axiom. Mereological systems in which Extensionality holds are termed \"extensional\", a fact denoted by including the letter E in their symbolic names.\n\nM6 asserts that any two underlapping objects have a unique sum; M7 asserts that any two overlapping objects have a unique product. If the universe is finite or if \"Top\" is assumed, then the universe is closed under \"sum\". Universal closure of \"Product\" and of supplementation relative to \"W\" requires \"Bottom\". \"W\" and \"N\" are, evidently, the mereological analog of the universal and empty sets, and \"Sum\" and \"Product\" are, likewise, the analogs of set-theoretical \"union\" and \"intersection\". If M6 and M7 are either assumed or derivable, the result is a mereology with \"closure\".\n\nBecause \"Sum\" and \"Product\" are binary operations, M6 and M7 admit the sum and product of only a finite number of objects. The \"fusion\" axiom, M8, enables taking the sum of infinitely many objects. The same holds for \"Product\", when defined. At this point, mereology often invokes set theory, but any recourse to set theory is eliminable by replacing a formula with a quantified variable ranging over a universe of sets by a schematic formula with one free variable. The formula comes out true (is satisfied) whenever the name of an object that would be a member of the set (if it existed) replaces the free variable. Hence any axiom with sets can be replaced by an axiom schema with monadic atomic subformulae. M8 and M8' are schemas of just this sort. The syntax of a first-order theory can describe only a denumerable number of sets; hence, only denumerably many sets may be eliminated in this fashion, but this limitation is not binding for the sort of mathematics contemplated here.\n\nIf M8 holds, then \"W\" exists for infinite universes. Hence, \"Top\" need be assumed only if the universe is infinite and M8 does not hold. \"Top\" (postulating \"W\") is not controversial, but \"Bottom\" (postulating \"N\") is. Leśniewski rejected \"Bottom\", and most mereological systems follow his example (an exception is the work of Richard Milton Martin). Hence, while the universe is closed under sum, the product of objects that do not overlap is typically undefined. A system with \"W\" but not \"N\" is isomorphic to:\nPostulating \"N\" renders all possible products definable, but also transforms classical extensional mereology into a set-free model of Boolean algebra.\n\nIf sets are admitted, M8 asserts the existence of the fusion of all members of any nonempty set. Any mereological system in which M8 holds is called \"general\", and its name includes G. In any general mereology, M6 and M7 are provable. Adding M8 to an extensional mereology results in \"general extensional mereology\", abbreviated GEM; moreover, the extensionality renders the fusion unique. On the converse, however, if the fusion asserted by M8 is assumed unique, so that M8' replaces M8, then—as Tarski (1929) had shown—M3 and M8' suffice to axiomatize GEM, a remarkably economical result. Simons (1987: 38–41) lists a number of GEM theorems.\n\nM2 and a finite universe necessarily imply \"Atomicity\", namely that everything either is an atom or includes atoms among its proper parts. If the universe is infinite, \"Atomicity\" requires M9. Adding M9 to any mereological system, X results in the atomistic variant thereof, denoted AX. \"Atomicity\" permits economies, for instance, assuming that M5' implies \"Atomicity\" and extensionality, and yields an alternative axiomatization of AGEM.\n\nThe notion of \"subset\" in set theory is not entirely the same as the notion of \"subpart\" in mereology Stanisław Leśniewski rejected set theory (a related to but not the same as nominalism (see https://plato.stanford.edu/entries/nominalism-metaphysics/). For a long time, nearly all philosophers and mathematicians avoided mereology, seeing it as tantamount to a rejection of set theory. Goodman too was a nominalist, and his fellow nominalist Richard Milton Martin employed a version of the calculus of individuals throughout his career, starting in 1941.\n\nMuch early work on mereology was motivated by a suspicion that set theory was ontologically suspect, and that Occam's razor requires that one minimise the number of posits in one's theory of the world and of mathematics. Mereology replaces talk of \"sets\" of objects with talk of \"sums\" of objects, objects being no more than the various things that make up wholes.\n\nMany logicians and philosophers reject these motivations, on such grounds as: \nFor a survey of attempts to found mathematics without using set theory, see Burgess and Rosen (1997).\n\nIn the 1970s, thanks in part to Eberle (1970), it gradually came to be understood that one can employ mereology regardless of one's ontological stance regarding sets. This understanding is called the \"ontological innocence\" of mereology. This innocence stems from mereology being formalizable in either of two equivalent ways:\nOnce it became clear that mereology is not tantamount to a denial of set theory, mereology became largely accepted as a useful tool for formal ontology and metaphysics.\n\nIn set theory, singletons are \"atoms\" that have no (non-empty) proper parts; many consider set theory useless or incoherent (not \"well-founded\") if sets cannot be built up from unit sets. The calculus of individuals was thought to require that an object either have no proper parts, in which case it is an \"atom\", or be the mereological sum of atoms. Eberle (1970), however, showed how to construct a calculus of individuals lacking \"atoms\", i.e., one where every object has a \"proper part\" (defined below) so that the universe is infinite.\n\nThere are analogies between the axioms of mereology and those of standard Zermelo–Fraenkel set theory (ZF), if \"Parthood\" is taken as analogous to subset in set theory. On the relation of mereology and ZF, also see Bunt (1985). One of the very few contemporary set theorists to discuss mereology is Potter (2004).\n\nLewis (1991) went further, showing informally that mereology, augmented by a few ontological assumptions and plural quantification, and some novel reasoning about singletons, yields a system in which a given individual can be both a member and a subset of another individual. In the resulting system, the axioms of ZFC (and of Peano arithmetic) are theorems.\n\nForrest (2002) revises Lewis's analysis by first formulating a generalization of CEM, called \"Heyting mereology\", whose sole nonlogical primitive is \"Proper Part\", assumed transitive and antireflexive. There exists a \"fictitious\" null individual that is a proper part of every individual. Two schemas assert that every lattice join exists (lattices are complete) and that meet distributes over join. On this Heyting mereology, Forrest erects a theory of \"pseudosets\", adequate for all purposes to which sets have been put.\n\nHusserl never claimed that mathematics could or should be grounded in part-whole rather than set theory. Lesniewski consciously derived his mereology as an alternative to set theory as a foundation of mathematics, but did not work out the details. Goodman and Quine (1947) tried to develop the natural and real numbers using the calculus of individuals, but were mostly unsuccessful; Quine did not reprint that article in his \"Selected Logic Papers\". In a series of chapters in the books he published in the last decade of his life, Richard Milton Martin set out to do what Goodman and Quine had abandoned 30 years prior. A recurring problem with attempts to ground mathematics in mereology is how to build up the theory of relations while abstaining from set-theoretic definitions of the ordered pair. Martin argued that Eberle's (1970) theory of relational individuals solved this problem.\n\nTopological notions of boundaries and connection can be married to mereology, resulting in mereotopology; see Casati and Varzi (1999: chpts. 4,5). Whitehead's 1929 \"Process and Reality\" contains a good deal of informal mereotopology.\n\nBunt (1985), a study of the semantics of natural language, shows how mereology can help understand such phenomena as the mass–count distinction and verb aspect. But Nicolas (2008) argues that a different logical framework, called plural logic, should be used for that purpose.\nAlso, natural language often employs \"part of\" in ambiguous ways (Simons 1987 discusses this at length). Hence, it is unclear how, if at all, one can translate certain natural language expressions into mereological predicates. Steering clear of such difficulties may require limiting the interpretation of mereology to mathematics and natural science. Casati and Varzi (1999), for example, limit the scope of mereology to physical objects.\n\nIn metaphysics there are many troubling questions pertaining to parts and wholes. One question addresses constitution and persistence, another asks about composition.\n\nIn metaphysics, there are several puzzles concerning cases of mereological constitution. That is, what makes up a whole. We are still concerned with parts and wholes, but instead of looking at what parts make up a whole, we are wondering what a thing is made of, such as its materials: e.g. the bronze in a bronze statue. Below are two of the main puzzles that philosophers use to discuss constitution.\n\n\"Ship of Theseus:\" Briefly, the puzzle goes something like this. There is a ship called the Ship of Theseus. Overtime the boards start to rot so we remove the boards and place them in a pile. First question, is the ship made of the new boards the same as the ship that had all the old boards? Second, if we reconstruct a ship using all of the old planks, etc. from the Ship of Theseus, and we also have a ship that was built out of new boards (each added one-by-one over time to replace old decaying boards), which ship is the real Ship of Theseus?\n\n\"Statue and Lump of Clay:\" Roughly, a sculptor decides to mold a statue out of a lump of clay. At time t1 the sculptor has a lump of clay. After many manipulations at time t2 there is a statue. The question asked is, is the lump of clay and the statue (numerically) identical? If so, how and why?\n\nConstitution typically has implications for views on persistence: how does an object persist over time if any of its parts (materials) change or are removed, as is the case with humans who lose cells, change height, hair color, memories, and yet we are said to be the same person today as we were when we were first born. For example, Ted Sider is the same today as he was when he was born—he just changed. But how can this be if many parts of Ted today did not exist when Ted was just born? Is it possible for things, such as organisms to persist? And if so, how? There are several views that attempt to answer this question. Some of the views are as follows (note, there are several other views):\n\n(a) Constitution View. This view accepts cohabitation. That is, two objects share exactly the same matter. Here, it follows, that there are no temporal parts.\n\n(b) Mereological essentialism, which states that the only objects that exist are quantities of matter, which are things defined by their parts. The object persists if matter is removed (or the form changes); but the object ceases to exist if any matter is destroyed.\n\n(c) Dominant Sorts. This is the view that tracing is determined by which sort is dominant; they reject cohabitation. For example, lump does not equal statue because they're different \"sorts\".\n\n(d) Nihilism—which makes the claim that no objects exist, except simples, so there is no persistence problem.\n\n(e) 4 Dimensionalism, or Temporal Parts (may also go by the names Perdurantism or Exdurantism), which roughly states that aggregates of temporal parts are intimately related. For example, two roads merging, momentarily and spatially, are still one road, because they share a part.\n\n(f) 3 Dimensionalism (may also go by the name Endurantism), where the object is wholly present. That is, the persisting object retains numerical identity.\n\nOne question that is addressed by philosophers is which is more fundamental: parts, wholes, or neither? Another pressing question is called the Special Composition Question (SCQ): For any Xs, when is it the case that there is a Y such that the Xs compose Y? This question has caused philosophers to run in three different directions: nihilism, universal composition (UC), or a moderate view (restricted composition). The first two views are considered extreme since the first denies composition, and the second allows any and all non-spatially overlapping objects to compose another object. The moderate view encompasses several theories that try to make sense of SCQ without saying 'no' to composition or 'yes' to unrestricted composition.\n\nThere are philosophers who are concerned with the question of fundamentality. That is, which is more ontologically fundamental the parts or their wholes. There are several responses to this question, though one of the default assumptions is that the parts are more fundamental. That is, the whole is grounded in its parts. This is the mainstream view. Another view, explored by Shaffer (2010) is monism, where the parts are grounded in the whole. Shaffer does not just mean that, say, the parts that make up my body are grounded in my body. Rather, Shaffer argues that the whole \"cosmos\" is more fundamental and everything else is a part of the cosmos. Then, there is the identity theory which claims that there is no hierarchy or fundamentality to parts and wholes. Instead wholes \"are just\" (or equivalent to) their parts. There can also be a two-object view which says that the wholes are not equal to the parts—they are numerically distinct from one another. Each of these theories has benefits and costs associated with them.\n\nPhilosophers want to know when some Xs compose something Y. There are several kinds of responses:\n\n\n(a) Contact—the Xs compose a complex Y if and only if the Xs are in contact;\n\n(b) Fastenation—the Xs compose a complex Y if and only if the Xs are fastened;\n\n(c) Cohesion—the Xs compose a complex Y if and only if the Xs cohere (cannot be pulled apart or moved in relation to each other without breaking);\n\n(d) Fusion—the Xs compose a complex Y if and only if the Xs are fused (fusion is when the Xs are joined together such that there is no boundary);\n\n(e) VIPA—van Inwagen's Proposed Answer—the Xs compose a complex Y if and only if either the activities of the Xs constitute a life or there is only one of the Xs; and\n\n(f) Brutal Composition—\"It's just the way things are.\" There is no true, nontrivial, and finitely long answer.\n\nThis is not an exhaustive list as many more hypotheses continue to be explored. However, a common problem with these theories is that they are vague. It remains unclear what \"fastened\" or \"life\" mean, for example. But there are many other issues within the restricted composition responses—though many of them are subject to which theory is being discussed.\n\nThe books by Simons (1987) and Casati and Varzi (1999) differ in their strengths:\n\nSimons devotes considerable effort to elucidating historical notations. The notation of Casati and Varzi is often used. Both books include excellent bibliographies. To these works should be added Hovda (2008), which presents the latest state of the art on the axiomatization of mereology.\n\n\n\n"}
{"id": "12779248", "url": "https://en.wikipedia.org/wiki?curid=12779248", "title": "Multiplicative cascade", "text": "Multiplicative cascade\n\nIn mathematics, a multiplicative cascade is a fractal/multifractal distribution of points produced via an iterative and multiplicative random process.\nModel I (left plot):\nModel II (middle plot):\nModel III (right plot):\n\nThe plots above are examples of multiplicative cascade multifractals.\nTo create these distributions there are a few steps to take. Firstly, we must create a lattice of cells which will be our underlying probability density field. \n\nSecondly, an iterative process is followed to create multiple levels of the lattice: at each iteration the cells are split into four equal parts (cells). Each new cell is then assigned a probability randomly from the set formula_4 without replacement, where formula_5. This process is continued to the \"N\"th level. For example, in constructing such a model down to level 8 we produce a 4 array of cells. \n\nThirdly, the cells are filled as follows: We take the probability of a cell being occupied as the product of the cell's own \"p\" and those of all its parents (up to level 1). A Monte Carlo rejection scheme is used repeatedly until the desired cell population is obtained, as follows: \"x\" and \"y\" cell coordinates are chosen randomly, and a random number between 0 and 1 is assigned; the (\"x\", \"y\") cell is then populated depending on whether the assigned number is lesser than (outcome: not populated) or greater or equal to (outcome: populated) the cell's occupation probability.\n\nTo produce the plots above we filled the probability density field with 5,000 points in a space of 256 × 256.\n\nAn example of the probability density field:\nThe fractals are generally not scale-invariant and therefore cannot be considered \"standard\" fractals. They can however be considered multifractals. The Rényi (generalized) dimensions can be theoretically predicted. It can be shown that as formula_6,\n\nwhere N is the level of the grid refinement and,\n\n"}
{"id": "32123297", "url": "https://en.wikipedia.org/wiki?curid=32123297", "title": "Ordinal logic", "text": "Ordinal logic\n\nIn mathematics, ordinal logic is a logic associated with an ordinal number by recursively adding elements to a sequence of previous logics. The concept was introduced in 1938 by Alan Turing in his PhD dissertation at Princeton in view of Gödel's incompleteness theorems.\n\nWhile Gödel showed that every system of logic suffers from some form of incompleteness, Turing focused on a method so that from a given system of logic a more complete system may be constructed. By repeating the process a sequence L1, L2, … of logics is obtained, each more complete than the previous one. A logic L can then be constructed in which the provable theorems are the totality of theorems provable with the help of the L1, L2, … etc. Thus Turing showed how one can associate a logic with any constructive ordinal. \n"}
{"id": "17887653", "url": "https://en.wikipedia.org/wiki?curid=17887653", "title": "Orthogonal array", "text": "Orthogonal array\n\nIn mathematics, in the area of combinatorial designs, an orthogonal array is a \"table\" (array) whose entries come from a fixed finite set of symbols (typically, {1,2...,\"n\"}), arranged in such a way that there is an integer \"t\" so that for every selection of \"t\" columns of the table, all ordered \"t\"-tuples of the symbols, formed by taking the entries in each row restricted to these columns, appear the same number of times. The number \"t\" is called the \"strength\" of the orthogonal array. Here is a simple example of an orthogonal array with symbol set {1,2} and strength 2:\n\nNotice that the four ordered pairs (2-tuples) formed by the rows restricted to the first and third columns, namely (1,1), (2,1), (1,2) and (2,2) are all the possible ordered pairs of the two element set and each appears exactly once. The second and third columns would give, (1,1), (2,1), (2,2) and (1,2); again, all possible ordered pairs each appearing once. The same statement would hold had the first and second columns been used. This is thus an orthogonal array of strength two.\n\nOrthogonal arrays generalize the idea of mutually orthogonal latin squares in a tabular form. These arrays have many connections to other combinatorial designs and have applications in the statistical design of experiments, coding theory, cryptography and various types of software testing.\n\nA t\"-(\"v\",\"k\",λ) \"orthogonal array (\"t\" ≤ \"k\") is a λ\"v\" × \"k\" array whose entries are chosen from a set \"X\" with \"v\" points such that in every subset of \"t\" columns of the array, every \"t\"-tuple of points of \"X\" appears in exactly λ rows.\n\nIn this formal definition, provision is made for repetition of the \"t\"-tuples (λ is the number of repeats) and the number of rows is determined by the other parameters.\n\nIn many applications these parameters are given the following names:\n\nAn orthogonal array is \"simple\" if it does not contain any repeated rows.\n\nAn orthogonal array is \"linear\" if \"X\" is a finite field of order \"q\", F (\"q\" a prime power) and the rows of the array form a subspace of the vector space (F).\n\nEvery linear orthogonal array is simple.\n\nAn example of a 2-(4, 5, 1) orthogonal array; a strength 2, 4 level design of index 1 with 16 runs.\n\nAn example of a 2-(3,5,3) orthogonal array (written as its transpose for ease of viewing):\n\nAny \"t\"-(\"v\", \"t\", λ) orthogonal array would be considered \"trivial\" since they are easily constructed by simply listing all the \"t\"-tuples of the \"v\"-set λ times.\n\nA 2-(\"v\",\"k\",1) orthogonal array is equivalent to a set of \"k\" − 2 mutually orthogonal latin squares of order \"v\".\n\nIndex one, strength 2 orthogonal arrays are also known as \"Hyper-Graeco-Latin square designs\" in the statistical literature.\n\nLet \"A\" be a strength 2, index 1 orthogonal array on a \"v\"-set of elements, identified with the set of natural numbers {1...,\"v\"}. Chose and fix, in order, two columns of \"A\", called the \"indexing columns\". All ordered pairs (\"i\", \"j\") with 1 ≤ \"i\", \"j\" ≤ \"v\" appear exactly once in the rows of the indexing columns. Take any other column of \"A\" and create a square array whose entry in position (\"i\",\"j\") is the entry of \"A\" in this column in the row that contains (\"i\", \"j\") in the indexing columns of \"A\". The resulting square is a latin square of order \"v\". For example, consider the 2-(3,4,1) orthogonal array:\n\nBy choosing columns 3 and 4 (in that order) as the indexing columns, the first column produces the latin square,\n\nwhile the second column produces the latin square,\nThe latin squares produced in this way from an orthogonal array will be orthogonal latin squares, so the \"k\" − 2 columns other than the indexing columns will produce a set of \"k\" − 2 mutually orthogonal latin squares.\n\nThis construction is completely reversible and so strength 2, index 1 orthogonal arrays can be constructed from sets of mutually orthogonal latin squares.\n\nOrthogonal arrays provide a uniform way to describe these diverse objects which are of interest in the statistical design of experiments.\n\nAs mentioned in the previous section a latin square of order \"n\" can be thought of as a 2-(\"n\", 3, 1) orthogonal array. Actually, the orthogonal array can lead to six latin squares since any ordered pair of distinct columns can be used as the indexing columns. However, these are all isotopic and are considered equivalent. For concreteness we shall always assume that the first two columns in their natural order are used as the indexing columns.\n\nIn the statistics literature, a latin cube is an \"n\" × \"n\" × \"n\" three-dimensional matrix consisting of \"n\" layers, each having \"n\" rows and \"n\" columns such that the \"n\" distinct elements which appear are repeated \"n\" times and arranged so that in each layer parallel to each of the three pairs of opposite faces of the cube all the \"n\" distinct elements appear and each is repeated exactly \"n\" times in that layer.\n\nNote that with this definition a layer of a latin cube need not be a latin square. In fact, no row, column or file (the cells of a particular position in the different layers) need be a permutation of the \"n\" symbols.\n\nA latin cube of order \"n\" is equivalent to a 2-(\"n\", 4, \"n\") orthogonal array.\n\nTwo latin cubes of order \"n\" are \"orthogonal\" if, among the \"n\" pairs of elements chosen from corresponding cells of the two cubes, each distinct ordered pair of the elements occurs exactly \"n\" times.\n\nA set of \"k\" − 3 mutually orthogonal latin cubes of order \"n\" is equivalent to a 2-(\"n\", \"k\", \"n\") orthogonal array.\n\nAn example of a pair of mutually orthogonal latin cubes of order three was given as the 2-(3,5,3) orthogonal array in the Examples section above.\n\nUnlike the case with latin squares, in which there are no constraints, the indexing columns of the orthogonal array representation of a latin cube must be selected so as to form a 3-(\"n\",3,1) orthogonal array.\n\nAn \"m\"-dimensional latin hypercube of order \"n\" of the \"r\"th class is an \"n\" × \"n\" × ... ×\"n\" \"m\"-dimensional matrix having \"n\" distinct elements, each repeated \"n\" times, and such that each element occurs exactly \"n\" times in each of its \"m\" sets of \"n\" parallel (\"m\" − 1)-dimensional linear subspaces (or \"layers\"). Two such latin hypercubes of the same order \"n\" and class \"r\" with the property that, when one is superimposed on the other, every element of the one occurs exactly \"n\" times with every element of the other, are said to be \"orthogonal\".\n\nA set of \"k\" − \"m\" mutually orthogonal \"m\"-dimensional latin hypercubes of order \"n\" is equivalent to a 2-(\"n\", \"k\", \"n\") orthogonal array, where the indexing columns form an \"m\"-(\"n\", \"m\", 1) orthogonal array.\n\nThe concepts of latin squares and mutually orthogonal latin squares were generalized to latin cubes and hypercubes, and orthogonal latin cubes and hypercubes by . generalized these results to strength \"t\". The present notion of orthogonal array as a generalization of these ideas, due to C. R. Rao, appears in .\n\nIf there exists an Hadamard matrix of order 4\"m\", then there exists a 2-(2, 4\"m\" − 1, \"m\") orthogonal array.\n\nLet \"H\" be an Hadamard matrix of order 4\"m\" in standardized form (first row and column entries are all +1). Delete the first row and take the transpose to obtain the desired orthogonal array.\n\nThe order 8 standardized Hadamard matrix below (±1 entries indicated only by sign),\nproduces the 2-(2,7,2) orthogonal array:\n\nUsing columns 1, 2 and 4 as indexing columns, the remaining columns produce four mutually orthogonal latin cubes of order 2.\n\nLet \"C\" ⊆ (F), be a linear code of dimension \"m\" with minimum distance \"d\". Then \"C\" (the orthogonal complement of the vector subspace \"C\") is a (linear) (\"d\" − 1)-(\"n\", \"q\", λ) orthogonal array where <br> λ = \"q\".\n\nSecret sharing (also called secret splitting) consists of methods for distributing a \"secret\" amongst a group of participants, each of whom is allocated a \"share\" of the secret. The secret can be reconstructed only when a sufficient number of shares, of possibly different types, are combined together; individual shares are of no use on their own. A secret sharing scheme is \"perfect\" if every collection of participants that does not meet the criteria for obtaining the secret, has no additional knowledge of what the secret is than does an individual with no share.\n\nIn one type of secret sharing scheme there is one \"dealer\" and \"n\" \"players\". The dealer gives shares of a secret to the players, but only when specific conditions are fulfilled will the players be able to reconstruct the secret. The dealer accomplishes this by giving each player a share in such a way that any group of \"t\" (for \"threshold\") or more players can together reconstruct the secret but no group of fewer than \"t\" players can. Such a system is called a (\"t\", \"n\")-threshold scheme.\n\nA \"t\"-(\"v\", \"n\" + 1, 1) orthogonal array may be used to construct a perfect (\"t\", \"n\")-threshold scheme.\n\nA factorial experiment is a statistically structured experiment in which several \"factors\" (watering levels, antibiotics, fertilizers, etc.) are applied to each experimental unit at varying (but integral) \"levels\" (high, low, or various intermediate levels). In a \"full factorial experiment\" all combinations of levels of the factors need to be tested, but to minimize confounding influences the levels should be varied within any experimental run.\n\nAn orthogonal array of strength 2 can be used to design a factorial experiment. The columns represent the various factors and the entries are the levels that the factors can be applied at (assuming that all factors can be applied at the same number of levels). An experimental run is a row of the orthogonal array, that is, apply the corresponding factors at the levels which appear in the row. When using one of these designs, the treatment units and trial order should be randomized as much as the design allows. For example, one recommendation is that an appropriately sized orthogonal array be randomly selected from those available, then randomize the run order.\n\nOrthogonal arrays played a central role in the development of Taguchi methods by Genichi Taguchi, which took place during his visit to Indian Statistical Institute in early 1950s. His methods were successfully applied and adopted by Japanese and Indian industries and subsequently were also embraced by US industry albeit with some reservations.\n\nOrthogonal array testing is a black box testing technique which is a systematic, statistical way of software testing. It is used when the number of inputs to the system is relatively small, but too large to allow for exhaustive testing of every possible input to the systems. It is particularly effective in finding errors associated with faulty logic within computer software systems. Orthogonal arrays can be applied in user interface testing, system testing, regression testing and performance testing.\nThe permutations of factor levels comprising a single treatment are so chosen that their responses are uncorrelated and hence each treatment gives a unique piece of information. The net effect of organizing the experiment in such treatments is that the same piece of information is gathered in the minimum number of experiments.\n\n\n\n"}
{"id": "45097232", "url": "https://en.wikipedia.org/wiki?curid=45097232", "title": "Photolith film", "text": "Photolith film\n\nA photolith film is a transparent film, made with some sort of transparent plastic (formerly made of acetate). Nowadays, with the use of laser printers and computers, the photolith film can be based on polyester, vegetable paper or laser film paper. It is mainly used in all photolithography processes.\n\nA color image, or polychromatic, is divided into four basic colors: cyan, the magenta, the yellow and black (the so-called system CMYK (short name from \"cyan\", \" magenta \", \" yellow \" and \" black \"), generating four photolith film images, a photo filtered with each of the three basic colors plus a B&W film (addition of the three). For black-and-white images, such as text or simple logos, only one photolith film is needed.\n\nThe \"photolith film\" it is sometimes recorded by an optical laser process on an \"imagesetter\" machine, coming from a digital file, or by a photographic process in a contact copier, if a physical copy of the original already exist. In the old offset printing plates acquire text or images to be printed after being sensitized from a photolith film.\n\nThe photolith films, as well as vegetable and the \"laser\" films, are used to store plates, screens or other media sensitive to light as a backup for repeating their processes in the future. They normally store the information of the three or four separated colours on monochrome photolith films.\n\n"}
{"id": "5987264", "url": "https://en.wikipedia.org/wiki?curid=5987264", "title": "Recursive function", "text": "Recursive function\n\nRecursive function may refer to:\n\n"}
{"id": "505218", "url": "https://en.wikipedia.org/wiki?curid=505218", "title": "Register machine", "text": "Register machine\n\nIn mathematical logic and theoretical computer science a register machine is a generic class of abstract machines used in a manner similar to a Turing machine. All the models are Turing equivalent.\n\nThe register machine gets its name from its use of one or more \"registers\". In contrast to the tape and head used by a Turing machine, the model uses multiple, uniquely addressed registers, each of which holds a single positive integer.\n\nThere are at least four sub-classes found in literature, here listed from most primitive to the most like a computer:\n\nAny properly defined register machine model is Turing equivalent. Computational speed is very dependent on the model specifics.\n\nIn practical computer science, a similar concept known as a virtual machine is sometimes used to minimise dependencies on underlying machine architectures. Such machines are also used for teaching. The term \"register machine\" is sometimes used to refer to a virtual machine in textbooks.\n\nA register machine consists of:\n\n\nTwo trends appeared in the early 1950s—the first to characterize the computer as a Turing machine, the second to define computer-like models—models with sequential instruction sequences and conditional jumps—with the power of a Turing machine, i.e. a so-called Turing equivalence. Need for this work was carried out in context of two \"hard\" problems: the unsolvable word problem posed by Emil Post—his problem of \"tag\"—and the very \"hard\" problem of Hilbert's problems—the 10th question around Diophantine equations. Researchers were questing for Turing-equivalent models that were less \"logical\" in nature and more \"arithmetic\" (cf Melzak (1961) p. 281, Shepherdson–Sturgis (1963) p. 218).\n\nThe first trend—toward characterizing computers—seems to have originated with Hans Hermes (1954), Rózsa Péter (1958), and Heinz Kaphengst (1959), the second trend with Hao Wang (1954, 1957) and, as noted above, furthered along by Zdzislaw Alexander Melzak (1961), Joachim Lambek (1961), Marvin Minsky (1961, 1967), and John Shepherdson and Howard E. Sturgis (1963).\n\nThe last five names are listed explicitly in that order by Yuri Matiyasevich. He follows up with:\n\nIt appears that Lambek, Melzak, Minsky and Shepherdson and Sturgis independently anticipated the same idea at the same time. See Note On Precedence below.\n\nThe history begins with Wang's model.\n\nWang's work followed from Emil Post's (1936) paper and led Wang to his definition of his Wang B-machine—a two-symbol Post–Turing machine computation model with only four atomic instructions:\n\nTo these four both Wang (1954, 1957) and then C.Y. Lee (1961) added another instruction from the Post set { ERASE }, and then a Post's unconditional jump { JUMP_to_ instruction_z } (or to make things easier, the conditional jump JUMP_IF_blank_to_instruction_z, or both. Lee named this a \"W-machine\" model:\n\nWang expressed hope that his model would be \"a rapprochement\" (p. 63) between the theory of Turing machines and the practical world of the computer.\n\nWang's work was highly influential. We find him referenced by Minsky (1961) and (1967), Melzak (1961), Shepherdson and Sturgis (1963). Indeed, Shepherdson and Sturgis (1963) remark that:\n\nMartin Davis eventually evolved this model into the (2-symbol) Post–Turing machine.\n\nDifficulties with the Wang/Post–Turing model:\n\nExcept there was a problem: the Wang model (the six instructions of the 7-instruction Post–Turing machine) was still a single-tape Turing-like device, however nice its \"sequential program instruction-flow\" might be. Both Melzak (1961) and Shepherdson and Sturgis (1963) observed this (in the context of certain proofs and investigations):\n\nIndeed, as examples at Turing machine examples, Post–Turing machine and partial function show, the work can be \"complicated\".\nSo why not 'cut the tape' so each is infinitely long (to accommodate any size integer) but left-ended, and call these three tapes \"Post–Turing (i.e. Wang-like) tapes\"? The individual heads will move left (for decrement) and right (for increment). In one sense the heads indicate \"the tops of the stack\" of concatenated marks. Or in Minsky (1961) and Hopcroft and Ullman (1979, p. 171ff) the tape is always blank except for a mark at the left end—at no time does a head ever print or erase.\n\nWe just have to be careful to write our instructions so that a test-for-zero and jump occurs \"before\" we decrement otherwise our machine will \"fall off the end\" or \"bump against the end\"—we will have an instance of a partial function. Before a decrement our machine must always ask the question: \"Is the tape/counter empty? If so then I can't decrement, otherwise I can.\"\n\nMinsky (1961) and Shepherdson–Sturgis (1963) prove that only a few tapes—as few as one—still allow the machine to be Turing equivalent \"IF\" the data on the tape is represented as a Gödel number (or some other uniquely encodable-decodable number); this number will evolve as the computation proceeds. In the one tape version with Gödel number encoding the counter machine must be able to (i) multiply the Gödel number by a constant (numbers \"2\" or \"3\"), and (ii) divide by a constant (numbers \"2\" or \"3\") and jump if the remainder is zero. Minsky (1967) shows that the need for this bizarre instruction set can be relaxed to { INC (r), JZDEC (r, z) } and the convenience instructions { CLR (r), J (r) } if two tapes are available. A simple Gödelization is still required, however. A similar result appears in Elgot–Robinson (1964) with respect to their RASP model.\nMelzak's (1961) model is significantly different. He took his own model, flipped the tapes vertically, called them \"holes in the ground\" to be filled with \"pebble counters\". Unlike Minsky's \"increment\" and \"decrement\", Melzak allowed for proper subtraction of any count of pebbles and \"adds\" of any count of pebbles.\n\nHe defines indirect addressing for his model (p. 288) and provides two examples of its use (p. 89); his \"proof\" (p. 290-292) that his model is Turing equivalent is so sketchy that the reader cannot tell whether or not he intended the indirect addressing to be a requirement for the proof.\n\nLegacy of Melzak's model is Lambek's simplification and the reappearance of his mnemonic conventions in Cook and Reckhow 1973.\n\nLambek (1961) took Melzak's ternary model and atomized it down to the two unary instructions—X+, X- if possible else jump—exactly the same two that Minsky (1961) had come up with.\n\nHowever, like the Minsky (1961) model, the Lambek model does execute its instructions in a default-sequential manner—both X+ and X- carry the identifier of the next instruction, and X- also carries the jump-to instruction if the zero-test is successful.\n\nA RASP or random-access stored-program machine begins as a counter machine with its \"program of instruction\" placed in its \"registers\". Analogous to, but independent of, the finite state machine's \"Instruction Register\", at least one of the registers (nicknamed the \"program counter\" (PC)) and one or more \"temporary\" registers maintain a record of, and operate on, the current instruction's number. The finite state machine's TABLE of instructions is responsible for (i) fetching the current \"program\" instruction from the proper register, (ii) parsing the \"program\" instruction, (iii) fetching operands specified by the \"program \" instruction, and (iv) executing the \"program\" instruction.\n\nExcept there is a problem: If based on the \"counter machine\" chassis this computer-like, von Neumann machine will not be Turing equivalent. It cannot compute everything that is computable. Intrinsically the model is bounded by the size of its (very-) \"finite\" state machine's instructions. The counter machine based RASP can compute any primitive recursive function (e.g. multiplication) but not all mu recursive functions (e.g. the Ackermann function ).\n\nElgot–Robinson investigate the possibility of allowing their RASP model to \"self modify\" its program instructions. The idea was an old one, proposed by Burks-Goldstine-von Neumann (1946-7), and sometimes called \"the computed goto.\" Melzak (1961) specifically mentions the \"computed goto\" by name but instead provides his model with indirect addressing.\n\nComputed goto: A RASP \"program\" of instructions that modifies the \"goto address\" in a conditional- or unconditional-jump \"program\" instruction.\n\nBut this does not solve the problem (unless one resorts to Gödel numbers). What is necessary is a method to fetch the address of a program instruction that lies (far) \"beyond/above\" the upper bound of the \"finite\" state machine's instruction register and TABLE.\n\nMinsky (1967) hints at the issue in his investigation of a counter machine (he calls them \"program computer models\") equipped with the instructions { CLR (r), INC (r), and RPT (\"a\" times the instructions m to n) }. He doesn't tell us how to fix the problem, but he does observe that:\n\nBut Elgot and Robinson solve the problem: They augment their P RASP with an indexed set of instructions—a somewhat more complicated (but more flexible) form of indirect addressing. Their P' model addresses the registers by adding the contents of the \"base\" register (specified in the instruction) to the \"index\" specified explicitly in the instruction (or vice versa, swapping \"base\" and \"index\"). Thus the indexing P' instructions have one more parameter than the non-indexing P instructions:\n\nBy 1971 Hartmanis has simplified the indexing to indirection for use in his RASP model.\n\nIndirect addressing: A pointer-register supplies the finite state machine with the address of the target register required for the instruction. Said another way: The \"contents\" of the pointer-register is the \"address\" of the \"target\" register to be used by the instruction. If the pointer-register is unbounded, the RAM, and a suitable RASP built on its chassis, will be Turing equivalent. The target register can serve either as a source or destination register, as specified by the instruction.\n\nNote that the finite state machine does not have to explicitly specify this target register's address. It just says to the rest of the machine: Get me the contents of the register pointed to by my pointer-register and then do xyz with it. It must specify explicitly by name, via its instruction, this pointer-register (e.g. \"N\", or \"72\" or \"PC\", etc.) but it doesn't have to know what number the pointer-register actually contains (perhaps 279,431).\n\nCook and Reckhow (1973) cite Hartmanis (1971) and simplify his model to what they call a random-access machine (RAM—i.e. a machine with indirection and the Harvard architecture). In a sense we are back to Melzak (1961) but with a much simpler model than Melzak's.\n\nMinsky was working at the MIT Lincoln Laboratory and published his work there; his paper was received for publishing in the \"Annals of Mathematics\" on August 15, 1960 but not published until November 1961. While receipt occurred a full year before the work of Melzak and Lambek was received and published (received, respectively, May and June 15, 1961 and published side-by-side September 1961). That (i) both were Canadians and published in the Canadian Mathematical Bulletin, (ii) neither would have had reference to Minsky's work because it was not yet published in a peer-reviewed journal, but (iii) Melzak references Wang, and Lambek references Melzak, leads one to hypothesize that their work occurred simultaneously and independently.\n\nAlmost exactly the same thing happened to Shepherdson and Sturgis. Their paper was received in December 1961—just a few months after Melzak and Lambek's work was received. Again, they had little (at most 1 month) or no benefit of reviewing the work of Minsky. They were careful to observe in footnotes that papers by Ershov, Kaphengst and Peter had \"recently appeared\" (p. 219). These were published much earlier but appeared in the German language in German journals so issues of accessibility present themselves.\n\nThe final paper of Shepherdson and Sturgis did not appear in a peer-reviewed journal until 1963. And as they fairly and honestly note in their Appendix A, the 'systems' of Kaphengst (1959), Ershov (1958), Peter (1958) are all so similar to what results were obtained later as to be indistinguishable to a set of the following:\n\nIndeed, Shepherson and Sturgis conclude\n\nBy order of \"publishing\" date the work of Kaphengst (1959), Ershov (1958), Peter (1958) were first.\n\nBackground texts: The following bibliography of source papers includes a number of texts to be used as background. The mathematics that led to the flurry of papers about abstract machines in the 1950s and 1960s can be found in van Heijenoort (1967)—an assemblage of original papers spanning the 50 years from Frege (1879) to Gödel (1931). Davis (ed.) \"The Undecidable\" (1965) carries the torch onward beginning with Gödel (1931) through Gödel's (1964) postscriptum (p. 71); the original papers of Alan Turing (1936-7) and Emil Post (1936) are included in \"The Undecidable\". The mathematics of Church, Rosser and Kleene that appear as reprints of original papers in \"The Undecidable\" is carried further in Kleene (1952), a mandatory text for anyone pursuing a deeper understanding of the mathematics behind the machines. Both Kleene (1952) and Davis (1958) are referenced by a number of the papers.\n\nFor a good treatment of the counter machine see Minsky (1967) Chapter 11 \"Models similar to Digital Computers\"—he calls the counter machine a \"program computer\". A recent overview is found at van Emde Boas (1990). A recent treatment of the Minsky (1961)/Lambek (1961) model can be found Boolos-Burgess-Jeffrey (2002); they reincarnate Lambek's \"abacus model\" to demonstrate equivalence of Turing machines and partial recursive functions, and they provide a graduate-level introduction to both abstract machine models (counter- and Turing-) and the mathematics of recursion theory. Beginning with the first edition Boolos-Burgess (1970) this model appeared with virtually the same treatment.\n\nThe papers: The papers begin with Wang (1957) and his dramatic simplification of the Turing machine. Turing (1936), Kleene (1952), Davis (1958) and in particular Post (1936) are cited in Wang (1957); in turn, Wang is referenced by Melzak (1961), Minsky (1961) and Shepherdson–Sturgis (1961-3) as they independently reduce the Turing tapes to \"counters\". Melzak (1961) provides his pebble-in-holes counter machine model with indirection but doesn't carry the treatment further. The work of Elgot–Robinson (1964) define the RASP—the computer-like random-access stored-program machines—and appear to be the first to investigate the failure of the bounded counter machine to calculate the mu-recursive functions. This failure—except with the draconian use of Gödel numbers in the manner of Minsky (1961))—leads to their definition of \"indexed\" instructions (i.e. indirect addressing) for their RASP model. Elgot–Robinson (1964) and more so Hartmanis (1971) investigate RASPs with self-modifying programs. Hartmanis (1971) specifies an instruction set with indirection, citing lecture notes of Cook (1970). For use in investigations of computational complexity Cook and his graduate student Reckhow (1973) provide the definition of a RAM (their model and mnemonic convention are similar to Melzak's, but offer him no reference in the paper). The pointer machines are an offshoot of Knuth (1968, 1973) and independently Schönhage (1980).\n\nFor the most part the papers contain mathematics beyond the undergraduate level—in particular the primitive recursive functions and mu recursive functions presented elegantly in Kleene (1952) and less in depth, but useful nonetheless, in Boolos-Burgess-Jeffrey (2002).\n\nAll texts and papers excepting the four starred have been witnessed. These four are written in German and appear as references in Shepherdson–Sturgis (1963) and Elgot–Robinson (1964); Shepherdson–Sturgis (1963) offer a brief discussion of their results in Shepherdson–Sturgis' Appendix A. The terminology of at least one paper (Kaphengst (1959) seems to hark back to the Burke-Goldstine-von Neumann (1946-7) analysis of computer architecture.\n\nNotes\n\nSources\n\n"}
{"id": "19378200", "url": "https://en.wikipedia.org/wiki?curid=19378200", "title": "Representation theory", "text": "Representation theory\n\nRepresentation theory is a branch of mathematics that studies abstract algebraic structures by \"representing\" their elements as linear transformations of vector spaces, and studies \nmodules over these abstract algebraic structures. In essence, a representation makes an abstract algebraic object more concrete by describing its elements by matrices and the algebraic operations in terms of matrix addition and matrix multiplication. The algebraic objects amenable to such a description include groups, associative algebras and Lie algebras. The most prominent of these (and historically the first) is the representation theory of groups, in which elements of a group are represented by invertible matrices in such a way that the group operation is matrix multiplication.\n\nRepresentation theory is a useful method because it reduces problems in abstract algebra to problems in linear algebra, a subject that is well understood. Furthermore, the vector space on which a group (for example) is represented can be infinite-dimensional, and by allowing it to be, for instance, a Hilbert space, methods of analysis can be applied to the theory of groups. Representation theory is also important in physics because, for example, it describes how the symmetry group of a physical system affects the solutions of equations describing that system.\n\nRepresentation theory is pervasive across fields of mathematics, for two reasons. First, the applications of representation theory are diverse: in addition to its impact on algebra, representation theory:\n\nSecondly, there are diverse approaches to representation theory. The same objects can be studied using methods from algebraic geometry, module theory, analytic number theory, differential geometry, operator theory, algebraic combinatorics and topology.\n\nThe success of representation theory has led to numerous generalizations. One of the most general is in category theory. The algebraic objects to which representation theory applies can be viewed as particular kinds of categories, and the representations as functors from the object category to the category of vector spaces. This description points to two obvious generalizations: first, the algebraic objects can be replaced by more general categories; second, the target category of vector spaces can be replaced by other well-understood categories.\n\nLet \"V\" be a vector space over a field F. For instance, suppose \"V\" is R or C, the standard \"n\"-dimensional space of column vectors over the real or complex numbers respectively. In this case, the idea of representation theory is to do abstract algebra concretely by using \"n\" × \"n\" matrices of real or complex numbers.\n\nThere are three main sorts of algebraic objects for which this can be done: groups, associative algebras and Lie algebras.\n\nThis generalizes to any field F and any vector space \"V\" over F, with linear maps replacing matrices and composition replacing matrix multiplication: there is a group GL(\"V\",F) of automorphisms of \"V\", an associative algebra End(\"V\") of all endomorphisms of \"V\", and a corresponding Lie algebra gl(\"V\",F).\n\nThere are two ways to say what a representation is. The first uses the idea of an action, generalizing the way that matrices act on column vectors by matrix multiplication. A representation of a group \"G\" or (associative or Lie) algebra \"A\" on a vector space \"V\" is a map\nwith two properties. First, for any \"g\" in \"G\" (or \"a\" in \"A\"), the map\nis linear (over F). Second, if we introduce the notation \"g\" · \"v\" for formula_3 (\"g\", \"v\"), then for any \"g\", \"g\" in \"G\" and \"v\" in \"V\":\nwhere \"e\" is the identity element of \"G\" and \"g\"\"g\" is the product in \"G\". The requirement for associative algebras is analogous, except that associative algebras do not always have an identity element, in which case equation (1) is ignored. Equation (2) is an abstract expression of the associativity of matrix multiplication. This doesn't hold for the matrix commutator and also there is no identity element for the commutator. Hence for Lie algebras, the only requirement is that for any \"x\", \"x\" in \"A\" and \"v\" in \"V\":\nwhere [\"x\", \"x\"] is the Lie bracket, which generalizes the matrix commutator \"MN\" − \"NM\".\n\nThe second way to define a representation focuses on the map \"φ\" sending \"g\" in \"G\" to a linear map \"φ\"(\"g\"): \"V\" → \"V\", which satisfies\nand similarly in the other cases. This approach is both more concise and more abstract.\nFrom this point of view:\n\nThe vector space \"V\" is called the representation space of \"φ\" and its dimension (if finite) is called the dimension of the representation (sometimes \"degree\", as in ). It is also common practice to refer to \"V\" itself as the representation when the homomorphism \"φ\" is clear from the context; otherwise the notation (\"V\",\"φ\") can be used to denote a representation.\n\nWhen \"V\" is of finite dimension \"n\", one can choose a basis for \"V\" to identify \"V\" with F and hence recover a matrix representation with entries in the field F.\n\nAn effective or faithful representation is a representation (\"V\",\"φ\") for which the homomorphism \"φ\" is injective.\n\nIf \"V\" and \"W\" are vector spaces over F, equipped with representations \"φ\" and \"ψ\" of a group \"G\", then an equivariant map from \"V\" to \"W\" is a linear map \"α\": \"V\" → \"W\" such that\nfor all \"g\" in \"G\" and \"v\" in \"V\". In terms of \"φ\": \"G\" → GL(\"V\") and \"ψ\": \"G\" → GL(\"W\"), this means\nfor all \"g\" in \"G\", i.e. the following diagram commutes:\n\nEquivariant maps for representations of an associative or Lie algebra are defined similarly. If \"α\" is invertible, then it is said to be an isomorphism, in which case \"V\" and \"W\" (or, more precisely, \"φ\" and \"ψ\") are \"isomorphic representations\", also phrased as \"equivalent representations\". An equivariant map is often called an \"intertwining map\" of representations. Also, in the case of a group , it is on occasion called a -map.\n\nIsomorphic representations are, for practical purposes, \"the same\"; they provide the same information about the group or algebra being represented. Representation theory therefore seeks to classify representations up to isomorphism.\n\nIf (\"V\",\"ψ\") is a representation of (say) a group \"G\", and \"W\" is a linear subspace of \"V\" that is preserved by the action of \"G\" in the sense that \"g\" · \"w\" ∈ \"W\" for all \"w\" ∈ \"W\" (Serre calls these \"W\" \"stable under G\"), then \"W\" is called a \"subrepresentation\": by defining \"φ\"(\"g\") to be the restriction of \"ψ\"(\"g\") to \"W\", (\"W\", \"φ\") is a representation of \"G\" and the inclusion of \"W\" into \"V\" is an equivariant map. The quotient space \"V\"/\"W\" can also be made into a representation of \"G\".\n\nIf \"V\" has exactly two subrepresentations, namely the trivial subspace {0} and \"V\" itself, then the representation is said to be \"irreducible\"; if \"V\" has a proper nontrivial subrepresentation, the representation is said to be \"reducible\".\n\nThe definition of an irreducible representation implies Schur's lemma: an equivariant map \"α\": \"V\" → \"W\" between irreducible representations is either the zero map or an isomorphism, since its kernel and image are subrepresentations. In particular, when \"V\" = \"W\", this shows that the equivariant endomorphisms of \"V\" form an associative division algebra over the underlying field F. If F is algebraically closed, the only equivariant endomorphisms of an irreducible representation are the scalar multiples of the identity.\n\nIrreducible representations are the building blocks of representation theory: if a representation \"V\" is not irreducible then it is built from a subrepresentation and a quotient that are both \"simpler\" in some sense; for instance, if \"V\" is finite-dimensional, then both the subrepresentation and the quotient have smaller dimension.\n\nIf (\"V\",\"φ\") and (\"W\",\"ψ\") are representations of (say) a group \"G\", then the direct sum of \"V\" and \"W\" is a representation, in a canonical way, via the equation\n\nThe direct sum of two representations carries no more information about the group \"G\" than the two representations do individually. If a representation is the direct sum of two proper nontrivial subrepresentations, it is said to be decomposable. Otherwise, it is said to be indecomposable.\n\nIn favorable circumstances, every finite-dimensional representation is a direct sum of irreducible representations: such representations are said to be semisimple. In this case, it suffices to understand only the irreducible representations. Examples where this \"complete reducibility\" phenomenon occur include finite and compact groups, and semisimple Lie algebras. \n\nIn cases where complete reducibility does not hold, one must understand how indecomposable representations can be built from irreducible representations as extensions of a quotient by a subrepresentation.\n\nSuppose formula_11 and formula_12 are representations of a group formula_13. Then we can form a representation formula_14 of G acting on the tensor product vector space formula_15 as follows:\nIf formula_17 and formula_18 are representations of a Lie algebra, then the correct formula to use is\n\nIn general, the tensor product of irreducible representations is \"not\" irreducible; the process of decomposing a tensor product as a direct sum of irreducible representations is known as Clebsch–Gordan theory.\n\nIn the case of the representation theory of the group SU(2) (or equivalently, of its complexified Lie algebra formula_20), the decomposition is easy to work out. The irreducible representations are labeled by a parameter formula_21 that is a non-negative integer or half integer; the representation then has dimension formula_22. Suppose we take the tensor product of the representation of two representations, with labels formula_23 and formula_24 where we assume formula_25. Then the tensor product decomposes as a direct sum of one copy of each representation with label formula_21, where formula_21 ranges from formula_28 to formula_29 in increments of 1. If, for example, formula_30, then the values of formula_21 that occur are 0, 1, and 2. Thus, the tensor product representation of dimension formula_32 decomposes as a direct sum of a 1-dimensional representation formula_33 a 3-dimensional representation formula_34 and a 5-dimensional representation formula_35.\n\nRepresentation theory is notable for the number of branches it has, and the diversity of the approaches to studying representations of groups and algebras. Although, all the theories have in common the basic concepts discussed already, they differ considerably in detail. The differences are at least 3-fold:\n\nGroup representations are a very important tool in the study of finite groups. They also arise in the applications of finite group theory to geometry and crystallography. Representations of finite groups exhibit many of the features of the general theory and point the way to other branches and topics in representation theory.\n\nOver a field of characteristic zero, the representation of a finite group \"G\" has a number of convenient properties. First, the representations of \"G\" are semisimple (completely reducible). This is a consequence of Maschke's theorem, which states that any subrepresentation \"V\" of a \"G\"-representation \"W\" has a \"G\"-invariant complement. One proof is to choose any projection \"π\" from \"W\" to \"V\" and replace it by its average \"π\" defined by\n\"π\" is equivariant, and its kernel is the required complement.\n\nThe finite-dimensional \"G\"-representations can be understood using character theory: the character of a representation \"φ\": \"G\" → GL(\"V\") is the class function \"χ\": \"G\" → F defined by\nwhere formula_38 is the trace. An irreducible representation of \"G\" is completely determined by its character.\n\nMaschke's theorem holds more generally for fields of positive characteristic \"p\", such as the finite fields, as long as the prime \"p\" is coprime to the order of \"G\". When \"p\" and |\"G\"| have a common factor, there are \"G\"-representations that are not semisimple, which are studied in a subbranch called modular representation theory.\n\nAveraging techniques also show that if F is the real or complex numbers, then any \"G\"-representation preserves an inner product formula_39 on \"V\" in the sense that\nfor all \"g\" in \"G\" and \"v\", \"w\" in \"W\". Hence any \"G\"-representation is unitary.\n\nUnitary representations are automatically semisimple, since Maschke's result can be proven by taking the orthogonal complement of a subrepresentation. When studying representations of groups that are not finite, the unitary representations provide a good generalization of the real and complex representations of a finite group.\n\nResults such as Maschke's theorem and the unitary property that rely on averaging can be generalized to more general groups by replacing the average with an integral, provided that a suitable notion of integral can be defined. This can be done for compact topological groups (including compact Lie groups), using Haar measure, and the resulting theory is known as abstract harmonic analysis.\n\nOver arbitrary fields, another class of finite groups that have a good representation theory are the finite groups of Lie type. Important examples are linear algebraic groups over finite fields. The representation theory of linear algebraic groups and Lie groups extends these examples to infinite-dimensional groups, the latter being intimately related to Lie algebra representations. The importance of character theory for finite groups has an analogue in the theory of weights for representations of Lie groups and Lie algebras.\n\nRepresentations of a finite group \"G\" are also linked directly to algebra representations via the group algebra F[\"G\"], which is a vector space over F with the elements of \"G\" as a basis, equipped with the multiplication operation defined by the group operation, linearity, and the requirement that the group operation and scalar multiplication commute.\n\nModular representations of a finite group \"G\" are representations over a field whose characteristic is not coprime to |\"G\"|, so that Maschke's theorem no longer holds (because |\"G\"| is not invertible in F and so one cannot divide by it). Nevertheless, Richard Brauer extended much of character theory to modular representations, and this theory played an important role in early progress towards the classification of finite simple groups, especially for simple groups whose characterization was not amenable to purely group-theoretic methods because their Sylow 2-subgroups were \"too small\".\n\nAs well as having applications to group theory, modular representations arise naturally in other branches of mathematics, such as algebraic geometry, coding theory, combinatorics and number theory.\n\nA unitary representation of a group \"G\" is a linear representation \"φ\" of \"G\" on a real or (usually) complex Hilbert space \"V\" such that \"φ\"(\"g\") is a unitary operator for every \"g\" ∈ \"G\". Such representations have been widely applied in quantum mechanics since the 1920s, thanks in particular to the influence of Hermann Weyl, and this has inspired the development of the theory, most notably through the analysis of representations of the Poincaré group by Eugene Wigner. One of the pioneers in constructing a general theory of unitary representations (for any group \"G\" rather than just for particular groups useful in applications) was George Mackey, and an extensive theory was developed by Harish-Chandra and others in the 1950s and 1960s.\n\nA major goal is to describe the \"unitary dual\", the space of irreducible unitary representations of \"G\". The theory is most well-developed in the case that \"G\" is a locally compact (Hausdorff) topological group and the representations are strongly continuous. For \"G\" abelian, the unitary dual is just the space of characters, while for \"G\" compact, the Peter–Weyl theorem shows that the irreducible unitary representations are finite-dimensional and the unitary dual is discrete. For example, if \"G\" is the circle group \"S\", then the characters are given by integers, and the unitary dual is Z.\n\nFor non-compact \"G\", the question of which representations are unitary is a subtle one. Although irreducible unitary representations must be \"admissible\" (as Harish-Chandra modules) and it is easy to detect which admissible representations have a nondegenerate invariant sesquilinear form, it is hard to determine when this form is positive definite. An effective description of the unitary dual, even for relatively well-behaved groups such as real reductive Lie groups (discussed below), remains an important open problem in representation theory. It has been solved for many particular groups, such as SL(2,R) and the Lorentz group.\n\nThe duality between the circle group \"S\" and the integers Z, or more generally, between a torus \"T\" and Z is well known in analysis as the theory of Fourier series, and the Fourier transform similarly expresses the fact that the space of characters on a real vector space is the dual vector space. Thus unitary representation theory and harmonic analysis are intimately related, and abstract harmonic analysis exploits this relationship, by developing the analysis of functions on locally compact topological groups and related spaces.\n\nA major goal is to provide a general form of the Fourier transform and the Plancherel theorem. This is done by constructing a measure on the unitary dual and an isomorphism between the regular representation of \"G\" on the space L(\"G\") of square integrable functions on \"G\" and its representation on the space of L functions on the unitary dual. Pontrjagin duality and the Peter–Weyl theorem achieve this for abelian and compact \"G\" respectively.\n\nAnother approach involves considering all unitary representations, not just the irreducible ones. These form a category, and Tannaka–Krein duality provides a way to recover a compact group from its category of unitary representations.\n\nIf the group is neither abelian nor compact, no general theory is known with an analogue of the Plancherel theorem or Fourier inversion, although Alexander Grothendieck extended Tannaka–Krein duality to a relationship between linear algebraic groups and tannakian categories.\n\nHarmonic analysis has also been extended from the analysis of functions on a group \"G\" to functions on homogeneous spaces for \"G\". The theory is particularly well developed for symmetric spaces and provides a theory of automorphic forms (discussed below).\n\nA Lie group is a group that is also a smooth manifold. Many classical groups of matrices over the real or complex numbers are Lie groups. Many of the groups important in physics and chemistry are Lie groups, and their representation theory is crucial to the application of group theory in those fields.\n\nThe representation theory of Lie groups can be developed first by considering the compact groups, to which results of compact representation theory apply. This theory can be extended to finite-dimensional representations of semisimple Lie groups using Weyl's unitary trick: each semisimple real Lie group \"G\" has a complexification, which is a complex Lie group \"G\", and this complex Lie group has a maximal compact subgroup \"K\". The finite-dimensional representations of \"G\" closely correspond to those of \"K\".\n\nA general Lie group is a semidirect product of a solvable Lie group and a semisimple Lie group (the Levi decomposition). The classification of representations of solvable Lie groups is intractable in general, but often easy in practical cases. Representations of semidirect products can then be analysed by means of general results called \"Mackey theory\", which is a generalization of the methods used in Wigner's classification of representations of the Poincaré group.\n\nA Lie algebra over a field F is a vector space over F equipped with a skew-symmetric bilinear operation called the Lie bracket, which satisfies the Jacobi identity. Lie algebras arise in particular as tangent spaces to Lie groups at the identity element, leading to their interpretation as \"infinitesimal symmetries\". An important approach to the representation theory of Lie groups is to study the corresponding representation theory of Lie algebras, but representations of Lie algebras also have an intrinsic interest.\n\nLie algebras, like Lie groups, have a Levi decomposition into semisimple and solvable parts, with the representation theory of solvable Lie algebras being intractable in general. In contrast, the finite-dimensional representations of semisimple Lie algebras are completely understood, after work of Élie Cartan. A representation of a semisimple Lie algebra g is analysed by choosing a Cartan subalgebra, which is essentially a generic maximal subalgebra h of g on which the Lie bracket is zero (\"abelian\"). The representation of g can be decomposed into weight spaces that are eigenspaces for the action of h and the infinitesimal analogue of characters. The structure of semisimple Lie algebras then reduces the analysis of representations to easily understood combinatorics of the possible weights that can occur.\n\nThere are many classes of infinite-dimensional Lie algebras whose representations have been studied. Among these, an important class are the Kac–Moody algebras. They are named after Victor Kac and Robert Moody, who independently discovered them. These algebras form a generalization of finite-dimensional semisimple Lie algebras, and share many of their combinatorial properties. This means that they have a class of representations that can be understood in the same way as representations of semisimple Lie algebras.\n\nAffine Lie algebras are a special case of Kac–Moody algebras, which have particular importance in mathematics and theoretical physics, especially conformal field theory and the theory of exactly solvable models. Kac discovered an elegant proof of certain combinatorial identities, Macdonald identities, which is based on the representation theory of affine Kac–Moody algebras.\n\nLie superalgebras are generalizations of Lie algebras in which the underlying vector space has a Z-grading, and skew-symmetry and Jacobi identity properties of the Lie bracket are modified by signs. Their representation theory is similar to the representation theory of Lie algebras.\n\nLinear algebraic groups (or more generally, affine group schemes) are analogues in algebraic geometry of Lie groups, but over more general fields than just R or C. In particular, over finite fields, they give rise to finite groups of Lie type. Although linear algebraic groups have a classification that is very similar to that of Lie groups, their representation theory is rather different (and much less well understood) and requires different techniques, since the Zariski topology is relatively weak, and techniques from analysis are no longer available.\n\nInvariant theory studies actions on algebraic varieties from the point of view of their effect on functions, which form representations of the group. Classically, the theory dealt with the question of explicit description of polynomial functions that do not change, or are \"invariant\", under the transformations from a given linear group. The modern approach analyses the decomposition of these representations into irreducibles.\n\nInvariant theory of infinite groups is inextricably linked with the development of linear algebra, especially, the theories of quadratic forms and determinants. Another subject with strong mutual influence is projective geometry, where invariant theory can be used to organize the subject, and during the 1960s, new life was breathed into the subject by David Mumford in the form of his geometric invariant theory.\n\nThe representation theory of semisimple Lie groups has its roots in invariant theory and the strong links between representation theory and algebraic geometry have many parallels in differential geometry, beginning with Felix Klein's Erlangen program and Élie Cartan's connections, which place groups and symmetry at the heart of geometry. Modern developments link representation theory and invariant theory to areas as diverse as holonomy, differential operators and the theory of several complex variables.\n\nAutomorphic forms are a generalization of modular forms to more general analytic functions, perhaps of several complex variables, with similar transformation properties. The generalization involves replacing the modular group PSL (R) and a chosen congruence subgroup by a semisimple Lie group \"G\" and a discrete subgroup \"Γ\". Just as modular forms can be viewed as differential forms on a quotient of the upper half space H = PSL (R)/SO(2), automorphic forms can be viewed as differential forms (or similar objects) on \"Γ\"\\\"G\"/\"K\", where \"K\" is (typically) a maximal compact subgroup of \"G\". Some care is required, however, as the quotient typically has singularities. The quotient of a semisimple Lie group by a compact subgroup is a symmetric space and so the theory of automorphic forms is intimately related to harmonic analysis on symmetric spaces.\n\nBefore the development of the general theory, many important special cases were worked out in detail, including the Hilbert modular forms and Siegel modular forms. Important results in the theory include the Selberg trace formula and the realization by Robert Langlands that the Riemann-Roch theorem could be applied to calculate the dimension of the space of automorphic forms. The subsequent notion of \"automorphic representation\" has proved of great technical value for dealing with the case that \"G\" is an algebraic group, treated as an adelic algebraic group. As a result, an entire philosophy, the Langlands program has developed around the relation between representation and number theoretic properties of automorphic forms.\n\nIn one sense, associative algebra representations generalize both representations of groups and Lie algebras. A representation of a group induces a representation of a corresponding group ring or group algebra, while representations of a Lie algebra correspond bijectively to representations of its universal enveloping algebra. However, the representation theory of general associative algebras does not have all of the nice properties of the representation theory of groups and Lie algebras.\n\nWhen considering representations of an associative algebra, one can forget the underlying field, and simply regard the associative algebra as a ring, and its representations as modules. This approach is surprisingly fruitful: many results in representation theory can be interpreted as special cases of results about modules over a ring.\n\nHopf algebras provide a way to improve the representation theory of associative algebras, while retaining the representation theory of groups and Lie algebras as special cases. In particular, the tensor product of two representations is a representation, as is the dual vector space.\n\nThe Hopf algebras associated to groups have a commutative algebra structure, and so general Hopf algebras are known as quantum groups, although this term is often restricted to certain Hopf algebras arising as deformations of groups or their universal enveloping algebras. The representation theory of quantum groups has added surprising insights to the representation theory of Lie groups and Lie algebras, for instance through the crystal basis of Kashiwara.\n\nA \"set-theoretic representation\" (also known as a group action or \"permutation representation\") of a group \"G\" on a set \"X\" is given by a function ρ from \"G\" to \"X\", the set of functions from \"X\" to \"X\", such that for all \"g\", \"g\" in \"G\" and all \"x\" in \"X\":\n\nThis condition and the axioms for a group imply that ρ(\"g\") is a bijection (or permutation) for all \"g\" in \"G\". Thus we may equivalently define a permutation representation to be a group homomorphism from G to the symmetric group S of \"X\".\n\nEvery group \"G\" can be viewed as a category with a single object; morphisms in this category are just the elements of \"G\". Given an arbitrary category \"C\", a \"representation\" of \"G\" in \"C\" is a functor from \"G\" to \"C\". Such a functor selects an object \"X\" in \"C\" and a group homomorphism from \"G\" to Aut(\"X\"), the automorphism group of \"X\".\n\nIn the case where \"C\" is Vect, the category of vector spaces over a field F, this definition is equivalent to a linear representation. Likewise, a set-theoretic representation is just a representation of \"G\" in the category of sets.\n\nFor another example consider the category of topological spaces, Top. Representations in Top are homomorphisms from \"G\" to the homeomorphism group of a topological space \"X\".\n\nTwo types of representations closely related to linear representations are:\n\nSince groups are categories, one can also consider representation of other categories. The simplest generalization is to monoids, which are categories with one object. Groups are monoids for which every morphism is invertible. General monoids have representations in any category. In the category of sets, these are monoid actions, but monoid representations on vector spaces and other objects can be studied.\n\nMore generally, one can relax the assumption that the category being represented has only one object. In full generality, this is simply the theory of functors between categories, and little can be said.\n\nOne special case has had a significant impact on representation theory, namely the representation theory of quivers. A quiver is simply a directed graph (with loops and multiple arrows allowed), but it can be made into a category (and also an algebra) by considering paths in the graph. Representations of such categories/algebras have illuminated several aspects of representation theory, for instance by allowing non-semisimple representation theory questions about a group to be reduced in some cases to semisimple representation theory questions about a quiver.\n\n\n\n"}
{"id": "19937535", "url": "https://en.wikipedia.org/wiki?curid=19937535", "title": "Shimshon Amitsur", "text": "Shimshon Amitsur\n\nShimshon Avraham Amitsur (born Kaplan; ; August 26, 1921 – September 5, 1994) was an Israeli mathematician. He is best known for his work in ring theory, in particular PI rings, an area of abstract algebra.\n\nAmitsur was born in Jerusalem and studied at the Hebrew University under the supervision of Jacob Levitzki. His studies were repeatedly interrupted, first by World War II and then by the Israel's War of Independence. He received his M.Sc. degree in 1946, and his Ph.D. in 1950. Later, for his joint work with Levitzki, he received the first Israel Prize in Exact Sciences. He worked at the Hebrew University until his retirement in 1989. Amitsur was a visiting scholar at the Institute for Advanced Study from 1952 to 1954. He was an Invited Speaker at the ICM in 1970 in Nice. He was a member of the Israel Academy of Sciences, where he was the Head for Experimental Science Section. He was one of the founding editors of the \"Israel Journal of Mathematics\", and the mathematical editor of the Hebrew Encyclopedia. Amitsur received a number of awards, including the honorary doctorate from Ben-Gurion University in 1990. His students included Avinoam Mann, Amitai Regev, Eliyahu Rips and Aner Shalev.\n\nAmitsur and Jacob Levitzki were each awarded the Israel Prize in exact sciences, in 1953, its inaugural year.\n\n\n\n\n"}
{"id": "53596978", "url": "https://en.wikipedia.org/wiki?curid=53596978", "title": "Teo Mora", "text": "Teo Mora\n\nFerdinando 'Teo' Mora is an Italian mathematician, and since 1990, a professor of algebra at the University of Genoa.\n\nMora's degree is in mathematics from the University of Genoa in 1974. Mora's publications span forty years; his notable contributions in computer algebra are the \ntangent cone algorithm and its extension of Buchberger theory of Gröbner bases and related algorithm earlier to non-commutative polynomial rings and more recently to effective rings; less significant the notion of Gröbner fan; marginal, with respect to the other authors, his contribution to the FGLM algorithm.\n\nMora is on the managing-editorial-board of the journal \"AAECC\" published by Springer, and was also formerly an editor of the \"Bulletin of the Iranian Mathematical Society\".\n\nHe is the author of the tetralogy \"Solving Polynomial Equation Systems\":\n\nMora lives in Genoa. Mora published a book trilogy in 1977-1978 (reprinted 2001-2003) called \"\" on the history of horror films. Italian television said in 2014 that the books are an \"authoritative guide with in-depth detailed descriptions and analysis.\"\n\n\n\n"}
{"id": "27936530", "url": "https://en.wikipedia.org/wiki?curid=27936530", "title": "Toilet paper orientation", "text": "Toilet paper orientation\n\nToilet paper when used with a toilet roll holder with a horizontal axle parallel to the floor and also parallel to the wall has two possible orientations: the toilet paper may hang \"over\" (in front of) or \"under\" (behind) the roll; if perpendicular to the wall, the two orientations are right-left or near-away. The choice is largely a matter of personal preference, dictated by habit. In surveys of US consumers and of bath and kitchen specialists, 60–70 percent of respondents prefer \"over\".\n\nSome people hold strong opinions on the matter; advice columnist Ann Landers said that the subject was the most responded to (15,000 letters in 1986) and controversial issue in her column's history. Defenders of either position cite advantages ranging from aesthetics, hospitality, and cleanliness to paper conservation, the ease of detaching individual sheets, and compatibility with setting specifics such as recreational vehicles or having pets. Some writers have proposed connections to age, sex, or political philosophy, and survey evidence has shown a correlation with socioeconomic status. \n\nSolutions range from compromise, to using separate dispensers or separate bathrooms entirely, or simply ignoring the issue altogether. One man advocates a plan under which his country will standardize on a single forced orientation, and at least one inventor hopes to popularize a new kind of toilet roll holder which swivels from one orientation to the other.\n\nIn the article \"Bathroom Politics: Introducing Students to Sociological Thinking from the Bottom Up\", Eastern Institute of Technology sociology professor Edgar Alan Burns describes some reasons toilet paper politics is worthy of examination. On the first day of Burns' introductory course in sociology, he asks his students, \"Which way do you think a roll of toilet paper should hang?\" In the following fifty minutes, the students examine why they picked their answers, exploring the social construction of \"rules and practices which they have never consciously thought about before\". \n\nBurns' activity has been adopted by a social psychology course at the University of Notre Dame, where it is used to illustrate the principles of Berger and Luckmann's 1966 classic \"The Social Construction of Reality\". \n\nChristopher Peterson, a professor of psychology at the University of Michigan, classifies the choice of toilet paper orientation under \"tastes, preferences, and interests\" as opposed to either values or \"attitudes, traits, norms, and needs\". Other personal interests include one's favorite cola or baseball team. Interests are an important part of identity; one expects and prefers that different people have different interests, which serves one's \"sense of uniqueness\". Differences in interests usually lead at most to teasing and gentle chiding. For most people, interests don't cause the serious divisions caused by conflicts of values; a possible exception is what Peterson calls \"the 'get a life' folks among us\" who elevate interests into moral issues.\n\nMorton Ann Gernsbacher, a professor of psychology at the University of Wisconsin–Madison, compares the orientation of toilet paper to the orientation of cutlery in a dishwasher, the choice of which drawer in a chest of drawers to place one's socks, and the order of shampooing one's hair and lathering one's body in the shower. In each choice, there is a prototypical solution chosen by the majority, and it is tempting to offer simplistic explanations of how the minority must be different. She warns that neuroimaging experiments—which as of 2007 were beginning to probe behaviors from mental rotation and facial expressions to grocery shopping and tickling—must strive to avoid such cultural bias and stereotypes.\n\nIn his book \"Conversational Capital\", Bertrand Cesvet gives toilet paper placement as an example of ritualized behavior—one of the ways designers and marketers can create a memorable experience around a product that leads to word-of-mouth momentum. Cesvet's other examples include shaking a box of Tic Tacs and dissecting Oreo cookies.\n\nBroadcaster Jim Bohannon has said that such issues are good for talk radio: \"It is an interactive medium, a certain kind of clash, it doesn't have to be a violent clash, but at least a disagreement would certainly be at the top of the list. It has to be something that's of general interest.\"\n\nThe main reasons given by people to explain why they hang their toilet paper a given way are ease of grabbing and habit. Some particular advantages cited for each orientation include:\n\nPartisans have claimed that each method makes it easier to tear the toilet paper on a perforated sheet boundary, depending on the direction of pulling and the use of a second hand to stabilize the roll. (A traveller from the U.S. to China in 1991 noted a different setup: non-perforated paper with a metal cutter above the roll, which obliges the \"over\" direction.)\n\nIt is unclear if one orientation is more economical than the other. The \"Centralian Advocate\" attributes a claim that \"over\" saves on paper usage to Planet Green.\n\nIn the academic field of evaluation, Michael Scriven writes that the question of the correct way to insert toilet paper is a \"one-item aptitude test\" for measuring one's evaluation skills. These skills include the evaluative attitude, practical logical analysis, empathy, teaching, and being a quick study. To prove one's competence, one may either derive the \"one right answer\" or prove that the test is or is not culturally biased.\n\nThe question \"Do you prefer that your toilet tissue unwinds over or under the spool?\" is featured on the cover of Barry Sinrod and Mel Poretz's 1989 book \"The First Really Important Survey of American Habits\". The overall result: 68 percent chose over. Sinrod explained, \"To me, the essence of the book is the toilet paper question ... Either people don't care, or they care so much that they practically cause bodily injury to one another.\" Poretz observed, \"The toilet-paper question galvanizes people almost like the Miller Lite tastes-great/less-filling commercial.\"\n\nIn Bernice Kanner's 1995 book \"Are You Normal?\", 53 percent of survey respondents prefer \"over\", while \"a fourth\" prefer \"under\" and 8 percent do not know or care.\n\n\"Sitting Pretty: The History of the Toilet\", a travelling exhibition that tours Canadian museums, asks visitors to register their preferred roll direction. When the exhibition reached Huntsville, Ontario, in June 2001, 13,000 visitors had taken the survey, with 67 percent preferring \"over\". At the Saint Boniface Museum in Winnipeg in February 2005, a voting machine registered 5,831 \"over\" versus 5,679 \"under\", or 51 percent \"over\". Saint Boniface's director noted, \"I think there's been some cheating, though.\"\n\nGeorgia-Pacific commissioned a survey of Americans' bathroom habits in 1993 to launch its new Quilted Northern brand, and more surveys followed:\n\nIn 1993, American Standard Brands conducted a poll of \"designers, contractors, dealers, distributors and other bath and kitchen reps\" at the Kitchen/Bath Industry Show & Conference in Atlanta. The question: \"What is the correct and only way to hang the toilet paper – under or over?\" \"Over\" won 59 percent of the vote, 1,826 to 1,256. American Standard spokeswoman Nora Monroe observed, \"The bathroom is a territorial place. You'd be surprised how many people have definite opinions on this issue.\" In 2008, American Standard commissioned the 2008 Bathroom Habits Survey, a more traditional format conducted by Opinion Research Corporation with 1,001 respondents. This time, \"three-quarters\" answered \"over\".\n\nIn 1995, a survey by Scott Paper Company's \"Cottonelle College of Freshness Knowledge\" had \"most Americans over 50\" preferring \"over\". In another Cottonelle survey in 1999, 68 percent of respondents preferred \"over\" to 25 percent \"under\". Columnist Bonnie Henry hypothesizes of the others: \"Meanwhile, 7 percent – no doubt bored beyond belief at this point by the inane questioning – had slipped into a deep, irreversible coma.\"\n\nOn January 27, 2010, the 100th anniversary of Thomas Crapper's death, Cottonelle launched a \"Great Debate\" advertising campaign, inviting American consumers to vote their preference at a Kimberly-Clark website. The result was announced during the 82nd Academy Awards: 72 percent had voted \"over\". In a more traditional preliminary survey of 1,000 Americans, Cottonelle found that \"overs\" are more likely than \"unders\" to notice a roll's direction (74 percent), to be annoyed when the direction is incorrect (24 percent), and to have flipped the direction at a friend's home (27 percent).\n\nPoretz and Sinrod break down the results of their 1989 survey by sex and age. These are the percentages of respondents who roll their paper \"under\":\nThe book does not note the number of respondents in each segment, so it is difficult to say whether any of the deviations are statistically significant, but there does not seem to be a difference between men's and women's preferences. Nonetheless, such a difference has been claimed by other authors, in both directions. The American Standard conference poll concluded: \"Many men voted for over, saying it made the paper easier to reach.\" Inventor Curtis Batts arrives at a different conclusion from his personal experience: \"Women like it over, and men like it under. I think it bugs women when it touches the wall.\" Advice columnist Ms Maud of \"The Press\" asserts that women prefer \"over\" because they are \"logical thinkers\".\n\nA Cottonelle survey indicated that men were more likely than women to notice, and become annoyed with, a toilet roll hung against their preference.\n\nA popular-culture occurrence of a gender theory is found in the \"Weekly World News\", a supermarket tabloid that runs outlandish stories for comedic effect. In the 2003 story \"North Korea Shocker!\", the \"WWN\" claimed that North Korean leader Kim Jong-il was secretly female. As supporting evidence, Kim supposedly watched the Home Shopping Network, is a member of Oprah's Book Club, and \"Yells at staffers who leave the toilet seat up and hang toilet paper rolls outward instead of inward.\"\n\nAccording to \"W. C. Privy's Original Bathroom Companion, Number 2\", \"By more than 4 to 1, older folks prefer to have their toilet paper dispense over the front.\" The same claim is made by James Buckley's \"The Bathroom Companion\" for people older than 50.\n\nSinrod observed of his survey, \"60 percent of those who earn $50,000 or more prefer it to be over and 73 percent of those who earn less than $20,000 prefer under\". On what that proves: \"I don't know, but it's sure interesting.\"\n\nIn one local election in Saskatoon, Saskatchewan, new voting machines were given a trial run by asking the question, \"Are you in favor of toilet paper in all public washrooms being installed with the loose end coming up and over the front of the roll?\" The answer was yes: 768 to 196, or 80 percent \"over\". It was thought to be a question \"which carried no political association\". Yet one teenager's science project at the Southern Appalachian Science and Engineering Fair, and a favorite of the fair's coordinator, was a survey concluding that liberals roll over while conservatives roll under.\n\nAdvice columnist Ann Landers (Eppie Lederer) was once asked which way toilet paper should hang. She answered \"under\", prompting thousands of letters in protest; she then recommended \"over\", prompting thousands more. She reflected that the 15,000 letters made toilet paper the most controversial issue in her column's 31-year history, wondering, \"With so many problems in the world, why were thousands of people making an issue of tissue?\"\n\nIn November 1986, Landers told the Canadian Commercial Travellers Association that \"Fine-quality toilet paper has designs that are right side up\" in the \"over\" position. In 1996, she explained the issue on \"The Oprah Winfrey Show\", where 68 percent of the studio audience favored \"over\"; Oprah suggested that \"under\" uses more paper. In 1998, she wrote that the issue \"seems destined to go on forever\", insisting, \"In spite of the fact that an overwhelming number of people prefer the roll hung so that the paper comes over the top, I still prefer to have the paper hanging close to the wall.\" On the day of her last column in 2002, Landers wrote, \"P.S. The toilet paper hangs over the top.\" Her published commentary on the issue has even continued after her death. 2005 saw the premiere of a one-woman play written by David Rambo: a character study of Ann Landers titled \"The Lady with All the Answers\". Toilet paper comes up once again, and the actress surveys the audience for their opinions.\n\nIn his article in \"Teaching Sociology\", Burns writes that the toilet paper hanging exercise is valuable in part because \"[the] subject matter is familiar to everybody; everyone is an expert, and everyone has an opinion.\" Many entertainers, celebrities and businesspeople have publicized their opinion on the topic. \n\nToilet paper orientation is often mentioned as a hurdle for married couples. The issue may also arise in businesses and public places.\n\nEven at the Amundsen–Scott Research Station at the South Pole, complaints have been raised over which way to install toilet paper. During the six-month-long polar night, a few dozen residents are stuck living together, and while many of the headaches of modern life are far away, food and hygiene are not. Despite the challenges posed by the hostile Antarctic climate, \"It is in the more mundane trials of everyday life that personality clashes are revealed.\"\n\nSome of the proposed solutions to this problem involve more or better technology, while others concentrate on human behavior.\n\nThe Tilt-A-Roll is a swiveling toilet paper dispenser invented by Curtis Batts in 1996, a Dallas-native industrial engineer. His patents on the invention, summarize its design as \"An adjustable angle coupling secures the yoke to the mounting assembly and permits rotation of the yoke about an axis directed orthogonally through the spindle such that the paper roll can be oriented to unroll paper either from over or from under the roll as desired.\" An inventor named Rocky Hutson demonstrated a similar device he called the T.P. Swivel to the producers of the television program \"PitchMen\" in late 2009. \n\nAnother solution is to install two toilet paper dispensers, as is more common in public restrooms and hotels. A reader of the \"Annie's Mailbox\" column recommends using a holder large enough to fit two rolls, noting that the roll mounted \"over\" is more popular. Another reader sidesteps the issue by foregoing the holder, instead piling five or six rolls in a big wicker basket. Even using separate bathrooms can help. Other solutions include vertical holders.\n\nToilet paper orientation has been used rhetorically as the ultimate issue that government has no business dictating, in letters to the editor protesting the regulation of noise pollution and stricter requirements to get a divorce. In 2006, protesting New Hampshire's ban on smoking in restaurants and bars, representative Ralph Boehm (R–Litchfield) asked \"Will we soon be told which direction the toilet paper must hang from the roll?\"\n\nDavid O'Connor's 2005 book \"Henderson's House Rules: The Official Guide to Replacing the Toilet Paper and Other Domestic Topics of Great Dispute\" aims to solve disagreements with a minimum of debate or compromise by offering authoritative, reasonable rules. The \"House Rule\" for toilet paper is \"over and out\", and a full page is dedicated to a diagram of this orientation. But O'Connor writes that \"if a female household member has a strong preference for the toilet paper to hang over and in, against the wall, that preference prevails. It is admittedly an odd preference, but women use toilet paper far more often than men—hence the rule.\"\n\n"}
{"id": "3757117", "url": "https://en.wikipedia.org/wiki?curid=3757117", "title": "Transitive reduction", "text": "Transitive reduction\n\nIn mathematics, a transitive reduction of a directed graph \"D\" is another directed graph with the same vertices and as few edges as possible, such that if there is a (directed) path from vertex \"v\" to vertex \"w\" in \"D\", then there is also such a path in the reduction. Transitive reductions were introduced by , who provided tight bounds on the computational complexity of constructing them.\n\nMore technically, the reduction is a directed graph that has the same reachability relation as \"D\". Equivalently, \"D\" and its transitive reduction should have the same transitive closure as each other, and its transitive reduction should have as few edges as possible among all graphs with this property. \n\nThe transitive reduction of a finite directed acyclic graph (a directed graph without directed cycles) is unique and is a subgraph of the given graph. However, uniqueness fails for graphs with (directed) cycles, and for infinite graphs not even existence is guaranteed. \n\nThe closely related concept of a minimum equivalent graph is a subgraph of \"D\" that has the same reachability relation and as few edges as possible. The difference is that a transitive reduction does not have to be a subgraph of \"D\". For finite directed acyclic graphs, the minimum equivalent graph is the same as the transitive reduction. However, for graphs that may contain cycles, minimum equivalent graphs are NP-hard to construct, while transitive reductions can be constructed in polynomial time. \n\nTransitive reduction can be defined for an abstract binary relation on a set, by interpreting the pairs of the relation as arcs in a directed graph.\n\nThe transitive reduction of a finite directed graph \"G\" is a graph with the fewest possible edges that has the same reachability relation as the original graph. That is, if there is a path from a vertex \"x\" to a vertex \"y\" in graph \"G\", there must also be a path from \"x\" to \"y\" in the transitive reduction of \"G\", and vice versa. The following image displays drawings of graphs corresponding to a non-transitive binary relation (on the left) and its transitive reduction (on the right).\n\nThe transitive reduction of a finite directed acyclic graph \"G\" is unique, and consists of the edges of \"G\" that form the only path between their endpoints. In particular, it is always a subgraph of the given graph. For this reason, the transitive reduction coincides with the minimum equivalent graph in this case.\n\nIn the mathematical theory of binary relations, any relation \"R\" on a set \"X\" may be thought of as a directed graph that has the set \"X\" as its vertex set and that has an arc \"xy\" for every ordered pair of elements that are related in \"R\". In particular, this method lets partially ordered sets be reinterpreted as directed acyclic graphs, in which there is an arc \"xy\" in the graph whenever there is an order relation \"x\" < \"y\" between the given pair of elements of the partial order. When the transitive reduction operation is applied to a directed acyclic graph that has been constructed in this way, it generates the covering relation of the partial order, which is frequently given visual expression by means of a Hasse diagram.\n\nTransitive reduction has been used on networks which can be represented as directed acyclic graphs (e.g. citation graphs or citation networks) to reveal structural differences between networks.\n\nIn a finite graph that may have cycles, the transitive reduction is not unique: there may be more than one graph on the same vertex set that has a minimum number of edges and has the same reachability relation as the given graph. Additionally, it may be the case that none of these minimum graphs is a subgraph of the given graph. Nevertheless, it is straightforward to characterize the minimum graphs with the same reachability relation as the given graph \"G\". If \"G\" is an arbitrary directed graph, and \"H\" is a graph with the minimum possible number of edges having the same reachability relation as \"G\", then \"H\" consists of\nThe total number of edges in this type of transitive reduction is then equal to the number of edges in the transitive reduction of the condensation, plus the number of vertices in nontrivial strongly connected components (components with more than one vertex).\n\nThe edges of the transitive reduction that correspond to condensation edges can always be chosen to be a subgraph of the given graph \"G\". However, the cycle within each strongly connected component can only be chosen to be a subgraph of \"G\" if that component has a Hamiltonian cycle, something that is not always true and is difficult to check. Because of this difficulty, it is NP-hard to find the smallest subgraph of a given graph \"G\" with the same reachability (its minimum equivalent graph).\n\nAs Aho et al. show, when the time complexity of graph algorithms is measured only as a function of the number \"n\" of vertices in the graph, and not as a function of the number of edges, transitive closure and transitive reduction of directed acyclic graphs have the same complexity. It had already been shown that transitive closure and multiplication of Boolean matrices of size \"n\" × \"n\" had the same complexity as each other, so this result put transitive reduction into the same class. The fastest known exact algorithms for matrix multiplication, as of 2015, take time O(\"n\"), and this gives the fastest known worst-case time bound for transitive reduction in dense graphs.\n\nTo prove that transitive reduction is as easy as transitive closure, Aho et al. rely on the already-known equivalence with Boolean matrix multiplication. They let \"A\" be the adjacency matrix of the given directed acyclic graph, and \"B\" be the adjacency matrix of its transitive closure (computed using any standard transitive closure algorithm). Then an edge \"uv\" belongs to the transitive reduction if and only if there is a nonzero entry in row \"u\" and column \"v\" of matrix \"A\", and there is a zero entry in the same position of the matrix product \"AB\". In this construction, the nonzero elements of the matrix \"AB\" represent pairs of vertices connected by paths of length two or more.\n\nTo prove that transitive reduction is as hard as transitive closure, Aho et al. construct from a given directed acyclic graph \"G\" another graph \"H\", in which each vertex of \"G\" is replaced by a path of three vertices, and each edge of \"G\" corresponds to an edge in \"H\" connecting the corresponding middle vertices of these paths. In addition, in the graph \"H\", Aho et al. add an edge from every path start to every path end. In the transitive reduction of \"H\", there is an edge from the path start for \"u\" to the path end for \"v\", if and only if edge \"uv\" does not belong to the transitive closure of \"G\". Therefore, if the transitive reduction of \"H\" can be computed efficiently, the transitive closure of \"G\" can be read off directly from it.\n\nWhen measured both in terms of the number \"n\" of vertices and the number \"m\" of edges in a directed acyclic graph, transitive reductions can also be found in time O(\"nm\"), a bound that may be faster than the matrix multiplication methods for sparse graphs. To do so, collect edges (\"u\",\"v\") such that the longest-path distance from \"u\" to \"v\" is one, calculating those distances by linear-time search from each possible starting vertex, \"u\". This O(\"nm\") time bound matches the complexity of constructing transitive closures by using depth first search or breadth first search to find the vertices reachable from every choice of starting vertex, so again with these assumptions transitive closures and transitive reductions can be found in the same amount of time.\n\n"}
{"id": "53880784", "url": "https://en.wikipedia.org/wiki?curid=53880784", "title": "Valery Alexeev (mathematician)", "text": "Valery Alexeev (mathematician)\n\nValery Alexeev is an American mathematician currently the David C. Barrow Professor at University of Georgia and an Elected Fellow of the American Mathematical Society.\n"}
{"id": "24993240", "url": "https://en.wikipedia.org/wiki?curid=24993240", "title": "Van der Corput lemma (harmonic analysis)", "text": "Van der Corput lemma (harmonic analysis)\n\nIn mathematics, in the field of harmonic analysis,\nthe van der Corput lemma is an estimate for oscillatory integrals\nnamed after the Dutch mathematician J. G. van der Corput.\n\nThe following result\nis stated by E. Stein:\n\nSuppose that a real-valued function formula_1 is smooth in an open interval formula_2,\nand that formula_3 for all formula_4.\nAssume that either formula_5, or that\nformula_6 and formula_7 is monotone for formula_8.\nThere is a constant formula_9, which does not depend on formula_10,\nsuch that\n\nfor any formula_12.\n\nThe van der Corput lemma is closely related to the sublevel set estimates\n(see for example\nwhich give the upper bound on the measure of the set\nwhere a function takes values not larger than formula_13.\n\nSuppose that a real-valued function formula_1 is smooth\non a finite or infinite interval formula_15,\nand that formula_3 for all formula_17.\nThere is a constant formula_9, which does not depend on formula_10,\nsuch that\nfor any formula_20\nthe measure of the sublevel set\nformula_21\nis bounded by formula_22.\n"}
{"id": "3103326", "url": "https://en.wikipedia.org/wiki?curid=3103326", "title": "Vector area", "text": "Vector area\n\nIn 3-dimensional geometry, for a finite planar surface of scalar area and unit normal , the vector area is defined as the unit normal scaled by the area:\n\nFor an orientable surface composed of a set of flat facet areas, the vector area of the surface is given by\n\nwhere is the unit normal vector to the area .\n\nFor bounded, oriented curved surfaces that are sufficiently well-behaved, we can still define vector area. First, we split the surface into infinitesimal elements, each of which is effectively flat. For each infinitesimal element of area, we have an area vector, also infinitesimal.\n\nwhere is the local unit vector perpendicular to . Integrating gives the vector area for the surface.\n\nFor a curved or faceted surface, the vector area is smaller in magnitude than the area. As an extreme example, a closed surface can possess arbitrarily large area, but its vector area is necessarily zero. Surfaces that share a boundary may have very different areas, but they must have the same vector area—the vector area is entirely determined by the boundary. These are consequences of Stokes' theorem.\n\nThe concept of an area vector simplifies the equation for determining the flux through the surface. Consider a planar surface in a uniform field. The flux can be written as the dot product of the field and area vector. This is much simpler than multiplying the field strength by the surface area and the cosine of the angle between the field and the surface normal.\n\nThe projected area onto (for example) the -plane is equivalent to the -component of the vector area, and is given as\n\nwhere is the angle between the plane normal and the -axis.\n\n"}
