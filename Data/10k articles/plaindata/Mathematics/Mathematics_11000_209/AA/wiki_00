{"id": "644032", "url": "https://en.wikipedia.org/wiki?curid=644032", "title": "Adjoint", "text": "Adjoint\n\nIn mathematics, the term adjoint applies in several situations. Several of these share a similar formalism: if \"A\" is adjoint to \"B\", then there is typically some formula of the type\n\nSpecifically, adjoint or adjunction may mean:\n"}
{"id": "298834", "url": "https://en.wikipedia.org/wiki?curid=298834", "title": "Affine space", "text": "Affine space\n\nIn mathematics, an affine space is a geometric structure that generalizes some of the properties of Euclidean spaces in such a way that these are independent of the concepts of distance and measure of angles, keeping only the properties related to parallelism and ratio of lengths for parallel line segments.\n\nIn an affine space, there is no distinguished point that serves as an origin. Hence, no vector has a fixed origin and no vector can be uniquely associated to a point. In an affine space, there are instead \"displacement vectors\", also called \"translation\" vectors or simply \"translations\", between two points of the space. Thus it makes sense to subtract two points of the space, giving a translation vector, but it does not make sense to add two points of the space. Likewise, it makes sense to add a displacement vector to a point of an affine space, resulting in a new point translated from the starting point by that vector.\n\nAny vector space may be considered as an affine space, and this amounts to forgetting the special role played by the zero vector. In this case, the elements of the vector space may be viewed either as \"points\" of the affine space or as \"displacement vectors\" or \"translations\". When considered as a point, the zero vector is called the \"origin\". Adding a fixed vector to the elements of a linear subspace of a vector space produces an \"affine subspace\". One commonly says that this affine subspace has been obtained by translating (away from the origin) the linear subspace by the translation vector. In finite dimensions, such an \"affine subspace\" is the solution set of an inhomogeneous linear system. The displacement vectors for that affine space are the solutions of the corresponding \"homogeneous\" linear system, which is a linear subspace. Linear subspaces, in contrast, always contain the origin of the vector space.\n\nThe \"dimension\" of an affine space is defined as the dimension of the vector space of its translations. An affine space of dimension one is an affine line. An affine space of dimension 2 is an affine plane. An affine subspace of dimension in an affine space or a vector space of dimension \"n\" is an affine hyperplane.\n\nThe following characterization may be easier to understand than the usual formal definition: an affine space is what is left of a vector space after you've forgotten which point is the origin (or, in the words of the French mathematician Marcel Berger, \"An affine space is nothing more than a vector space whose origin we try to forget about, by adding translations to the linear maps\"). Imagine that Alice knows that a certain point is the actual origin, but Bob believes that another point — call it — is the origin. Two vectors, and , are to be added. Bob draws an arrow from point to point and another arrow from point to point , and completes the parallelogram to find what Bob thinks is , but Alice knows that he has actually computed\n\nSimilarly, Alice and Bob may evaluate any linear combination of and , or of any finite set of vectors, and will generally get different answers. However, if the sum of the coefficients in a linear combination is 1, then Alice and Bob will arrive at the same answer.\n\nIf Alice travels to\n\nthen Bob can similarly travel to\n\nUnder this condition, for all coefficients , Alice and Bob describe the same point with the same linear combination, despite using different origins.\n\nWhile only Alice knows the \"linear structure\", both Alice and Bob know the \"affine structure\"—i.e. the values of affine combinations, defined as linear combinations in which the sum of the coefficients is 1. A set with an affine structure is an affine space.\n\nAn \"affine space\" is a set together with a vector space formula_1, and a transitive and free action of the additive group of formula_1 on the set . The elements of the affine space are called \"points\", and the elements of the associated vector space formula_1 are called \"vectors\", \"translations\", or sometimes \"free vectors\". Explicitly, the definition above means that the action is a mapping, generally denoted as an addition,\n\nthat has the following properties.\n\nThe first two properties are simply defining properties of a (right) group action. The third property characterizes free and transitive actions, the onto character coming from transitivity, and then the injective character follows from the action being free. There is a fourth property that follows from 1, 2, 3 above:\n\nProperty 3 is often used in the following equivalent form.\nAnother way to express the definition is that an affine space is a principal homogeneous space for the action of the additive group of a vector space. Homogeneous spaces are by definition endowed with a transitive group action, and for a principal homogeneous space such a transitive action is by definition free.\n\nThe properties of the group action allows for the definition of subtraction for any given ordered pair of points in , producing a vector of formula_15 This vector, denoted formula_16 (or formula_17) is by definition the unique vector in formula_1 such that\n\nExistence follows from the transitivity of the action, and uniqueness follows because the action is free.\n\nThis subtraction has the two following properties, called Weyl's axioms:\n\nIn Euclidean geometry, the second Weyl's axiom is commonly called the \"parallelogram rule\".\n\nAffine spaces can be equivalently defined as a point set , together with a vector space formula_24 and a subtraction satisfying Weyl's axioms. In this case, the addition of a vector to a point is defined from the first Weyl's axioms.\n\nAn \"affine subspace\" (also called, in some contexts, a \"linear variety\", a flat, or, over the real numbers, a \"linear manifold\") of an affine space is a subset of such that, given a point formula_25 the set of vectors formula_26 is a linear subspace of formula_15 This property, which does not depend on the choice of , implies that is an affine space, which has formula_28 as associated vector space.\n\nThe affine subspaces of are the subsets of of the form\nwhere is a point of , and a linear subspace of formula_1.\n\nThe linear subspace associated with an affine subspace is often called its \"\", and two subspaces that share the same direction are said to be \"parallel\".\n\nThis implies the following generalization of Playfair's axiom: Given a direction , for any point of there is one and only one affine subspace of direction , which passes through , namely the subspace .\n\nEvery translation formula_31 maps any affine subspace to a parallel subspace.\n\nThe term \"parallel\" is also used for two affine subspaces such that the direction of one is included in the direction of the other.\n\nGiven two affine spaces and whose associated vector spaces are formula_1 and formula_33 an \"affine map\" or \"affine homomorphism\" from to is a map\n\nsuch that\n\nis a well defined linear map. By formula_36 being well defined is meant that implies .\n\nThis implies that, for a point formula_37 and a vector formula_38 one has\n\nTherefore, since for any given in , for a unique , is completely defined by its value on a single point and the associated linear map formula_40\n\nEvery vector space may be considered as an affine space over itself. This means that every element of may be considered either as a point or as a vector. This affine set is sometimes denoted for emphasizing the double role of the elements of . When considered as a point, the zero vector is commonly denoted (or , when upper-case letters are used for points) and called the \"origin\".\n\nIf is another affine space over the same vector space (that is formula_41) the choice of any point in defines a unique affine isomorphism, which is the identity of and maps to . In other words, the choice of an origin in allows us to identify and up to a canonical isomorphism. The counterpart of this property is that the affine space may be identified with the vector space in which \"the place of the origin has been forgotten\".\n\nEuclidean spaces (including the one-dimensional line, two-dimensional plane, and three-dimensional space commonly studied in elementary geometry, as well as higher-dimensional analogues) are affine spaces.\n\nIndeed, in most modern definitions, a Euclidean space is defined to be an affine space, such that the associated vector space is a real inner product space of finite dimension, that is a vector space over the reals with a positive-definite quadratic form . The inner product of two vectors and is the value of the symmetric bilinear form\nThe usual Euclidean distance between two points and is \n\nIn older definition of Euclidean spaces through synthetic geometry, vectors are defined as equivalence classes of ordered pairs of points under equipollence (the pairs and are \"equipollent\" if the points (in this order) form a parallelogram). It is straightforward to verify that the vectors form a vector space, the square of the Euclidean distance is a quadratic form on the space of vectors, and the two definitions of Euclidean spaces are equivalent.\n\nIn Euclidean geometry, the common phrase \"affine property\" refers to a property that can be proved in affine spaces, that is, it can be proved without using the quadratic form and its associated inner product. In other words, an affine property is a property that does not involve lengths and angles. Typical examples are parallelism, and the definition of a tangent. A non-example is the definition of a normal.\n\nEquivalently, an affine property is a property that is invariant under affine transformations of the Euclidean space.\n\nLet us consider, in an affine space, points , and elements formula_44 of the ground field. For clarity, we use formula_17 to denote the vector .\n\nIf formula_46 for any two points and one has\n\nThus this sum is independent of the choice of the origin, and the resulting vector is denoted\n\nIn particular, when formula_49 one retrieves the definition of the subtraction of points.\n\nIf formula_50 let us denote by formula_51 the unique point such that\n\nfor some choice of an origin formula_53.\n\nOne can show that formula_51 is independent from the choice of the origin formula_53. Therefore, if\n\none writes\n\nThe point formula_51 is called the barycenter of the formula_59 for the weights formula_60 One says also that formula_51 is an affine combination of the formula_59 with coefficients formula_60\n\n\nFor any subset of an affine space , there is a smallest affine subspace that contains it, called the affine span of . It is the intersection of all affine subspaces containing , and its direction is the intersection of the directions of the affine subspaces that contain .\n\nThe affine span of is the set of all (finite) affine combinations of points of , and its direction is the linear span of the for and in . If one chooses a particular point , the direction of the affine span of is also the linear span of the </sub> for in .\n\nOne says also that the affine span of is generated by and that is a generating set of its affine span.\n\nA set of points of an affine space is said affinely independent or, simply, independent, if the affine span of any strict subset of is a strict subset of the affine span of . An , or barycentric frame (see , below) of an affine space is a generating set that is also independent (that is a minimal generating set).\n\nRecall the \"dimension\" of an affine space is the dimension of its associated vector space. The bases of an affine space of finite dimension are the independent subsets of elements, or, equivalently, the generating subsets of elements. Equivalently, } is an affine basis of an affine space if and only if } is a linear basis of the associated vector space.\n\nThere are two strongly related kinds of coordinate systems that may be defined on affine spaces.\n\nLet be an affine space over a field of dimension , and formula_65 be an affine basis of . The properties of an affine basis imply that for every in there is a unique -tuple formula_66 of elements of such that\n\nand\n\nThe formula_69 are called the barycentric coordinates of over the affine basis formula_70 If the are viewed as bodies that have weights (or masses) formula_71 the point is thus the barycenter of the , and this explains the origin of the term \"barycentric coordinates\".\n\nThe barycentric coordinates define an affine isomorphism between the affine space and the affine subspace of defined by the equation formula_72\n\nFor affine spaces of infinite dimension, the same definition applies, using only finite sums. This means that for each point, only a finite number of coordinates are non-zero.\n\nAn affine frame of an affine space consists of a point, called the \"origin\", and a linear basis of the associated vector space. More precisely, for an affine space with associated vector space formula_24 the origin belongs to , and the linear basis is a basis of formula_1 (for simplicity of the notation, we consider only the case of finite dimension, the general case is similar).\n\nFor each point of , there is a unique sequence formula_44 of elements of the ground field such that\n\nor equivalently\n\nThe formula_69 are called the affine coordinates of over the frame .\n\nExample: In Euclidean geometry, Cartesian coordinates are affine coordinates relative to an orthonormal frame, that is an affine frame such that is an orthonormal basis.\n\nBarycentric coordinates and affine coordinates are strongly related, and may be considered as equivalent.\n\nIn fact, given a barycentric frame\n\none deduces immediately the affine frame\n\nand, if\n\nare the barycentric coordinates of a point over the barycentric frame, then the affine coordinates of the same point over the affine frame are\n\nConversely, if\n\nis an affine frame, then\n\nis a barycentric frame. If\n\nare the affine coordinates of a point over the affine frame, then its barycentric coordinates over the barycentric frame are\n\nTherefore, barycentric and affine coordinates are almost equivalent. In most applications, affine coordinates are preferred, as involving less coordinates that are independent. However, in the situations where the important points of the studied problem are affinity independent, barycentric coordinates may lead to simpler computation, as in the following example.\n\nThe vertices of a non-flat triangle form an affine basis of the Euclidean plane. The barycentric coordinates allows easy characterization of the elements of the triangle that do not involve angles or distance:\n\nThe vertices are the points of barycentric coordinates , and . The lines supporting the edges are the points that have a zero coordinate. The edges themselves are the points that have a zero coordinate and two nonnegative coordinates. The interior of the triangle are the points whose all coordinates are positive. The medians are the points that have two equal coordinates, and the centroid is the point of coordinates .\n\nLet\n\nbe an affine homomorphism, with\n\nas associated linear map.\n\nThe image of is the affine subspace of , which has formula_89 as associated vector space. As an affine space does not have a zero element, an affine homomorphism does not have a kernel. However, for any point of , the inverse image of is an affine subspace of , of direction formula_90. This affine subspace is called the fiber of .\n\nAn important example is the projection parallel to some direction onto an affine subspace. The importance of this example lies in the fact that Euclidean spaces are affine spaces, and that this kind of projections is fundamental in Euclidean geometry.\n\nMore precisely, given an affine space with associated vector space formula_91, let be an affine subspace of direction formula_92, and be a complementary subspace of formula_92 in formula_91 (this means that every vector of formula_91 may be decomposed in a unique way as the sum of an element of formula_92 and an element of ). For every point of , its projection to parallel to is the unique point in such that\n\nThis is an affine homomorphism whose associated linear map formula_98 is defined by\nfor and in .\n\nThe image of this projection is , and its fibers are the subspaces of direction .\n\nAlthough kernels are not defined for affine spaces, quotient spaces are defined. This results from the fact that \"belonging to the same fiber of an affine homomorphism\" is an equivalence relation.\n\nLet be an affine space, and be a linear subspace of the associated vector space formula_91. The quotient of by is the quotient of by the equivalence relation\n\nThis quotient is an affine space, which has formula_102 as associated vector space.\n\nFor every affine homomorphism formula_103, the image is isomorphic to the quotient of by the kernel of the associated linear map. This is the first isomorphism theorem for affine spaces.\n\nAffine space is usually studied as analytic geometry using coordinates, or equivalently vector spaces. It can also be studied as synthetic geometry by writing down axioms, though this approach is much less common. There are several different systems of axioms for affine space.\n\nAffine planes satisfy the following axioms :\n(in which two lines are called parallel if they are equal or\ndisjoint):\nAs well as affine planes over fields (or division rings), there are also many non-Desarguesian planes satisfying these axioms. gives axioms for higher-dimensional affine spaces.\n\nAffine spaces are subspaces of projective spaces: an affine plane can be obtained from any projective plane by removing a line and all the points on it, and conversely any affine plane can be used to construct a projective plane as a closure by adding a line at infinity whose points correspond to equivalence classes of parallel lines.\n\nFurther, transformations of projective space that preserve affine space (equivalently, that leave the hyperplane at infinity invariant as a set) yield transformations of affine space. Conversely, any affine linear transformation extends uniquely to a projective linear transformation, so the affine group is a subgroup of the projective group. For instance, Möbius transformations (transformations of the complex projective line, or Riemann sphere) are affine (transformations of the complex plane) if and only if they fix the point at infinity.\n\nIn algebraic geometry, an affine variety (or, more generally, an affine algebraic set) is defined as the subset of an affine space that is the set of the common zeros of a set of so-called \"polynomial functions over the affine space\". For defining a \"polynomial function over the affine space\", one has to choose an affine coordinate system. Then, a polynomial function is a function such that the image of any point is the value of some multivariate polynomial function of the coordinates of the point. As a change of affine coordinates may be expressed by linear functions (more precisely affine functions) of the coordinates, this definition is independent of a particular choice of coordinates.\n\nThe choice of a system of affine coordinates for an affine space formula_104 of dimension over a field induces an affine isomorphism between formula_104 and the affine coordinate space . This explain why, for simplification, many textbooks write formula_106, and introduce affine algebraic varieties as the common zeros of polynomial functions over .\n\nAs the whole affine space is the set of the common zeros of the zero polynomial, affine spaces are affine algebraic varieties.\n\nBy the definition above, the choice of an affine coordinate system of an affine space formula_104 allows one to identify the polynomial functions on formula_104 with polynomials in variables, the \"i\"th variable representing the function that maps a point to its \"i\"th coordinate. It follows that the set of polynomial functions over formula_104 is a -algebra, denoted formula_110 which is isomorphic to the polynomial ring formula_111\n\nWhen one changes coordinates, the isomorphism between formula_112 and formula_113 changes accordingly, and this induces an automorphism of formula_114, which maps each indeterminate to a polynomial of degree one. It follows that the total degree defines a filtration of formula_115 which is independent from the choice of coordinates. The total degree defines also a graduation, but it depends on the choice of coordinates, as a change of affine coordinates may map indeterminates on non-homogeneous polynomials.\n\nAffine spaces over topological fields, such as the real or the complex numbers, have a natural topology. The Zariski topology, which is defined for affine spaces over any field, allows use of topological methods in any case. Zariski topology is the unique topology on an affine space whose closed sets are affine algebraic sets (that is sets of the common zeros of polynomials functions over the affine set). As, over a topological field, polynomial functions are continuous, every Zariski closed set is closed for the usual topology, if any. In other words, over a topological field, Zariski topology is coarser than the natural topology.\n\nThere is a natural injective function from an affine space into the set of prime ideals (that is the spectrum) of its ring of polynomial functions. When affine coordinates have been chosen, this function maps the point of coordinates formula_116 to the maximal ideal formula_117. This function is a homeomorphism (for the Zariski topology of the affine space and of the spectrum of the ring of polynomial functions) of the affine space onto the image of the function.\n\nThe case of an algebraically closed ground field is especially important in algebraic geometry, because, in this case, the homeomorphism above is a map between the affine space and the set of all maximal ideals of the ring of functions (this is Hilbert's Nullstellensatz).\n\nThis is the starting idea of scheme theory of Grothendieck, which consists, for studying algebraic varieties, of considering as \"points\", not only the points of the affine space, but also all the prime ideals of the spectrum. This allows gluing together algebraic varieties in a similar way as, for manifolds, charts are glued together for building a manifold.\n\nLike all affine varieties, local data on an affine space can always be patched together globally: the cohomology of affine space is trivial. More precisely, formula_118 for all coherent sheaves F, and integers formula_119. This property is also enjoyed by all other affine varieties. But also all of the etale cohomology groups on affine space are trivial. In particular, every line bundle is trivial. More generally, the Quillen–Suslin theorem implies that \"every\" algebraic vector bundle over an affine space is trivial.\n\n\n"}
{"id": "23715422", "url": "https://en.wikipedia.org/wiki?curid=23715422", "title": "African Journal of Educational Studies in Mathematics and Sciences", "text": "African Journal of Educational Studies in Mathematics and Sciences\n\nThe African Journal of Educational Studies in Mathematics and Sciences aims at generating fresh scholarly inquiry and exposition in the fields of mathematics, science education and related disciplines.\n\nRestricted access\nFulltext online: vol. 1 (2001) -\n\n\n"}
{"id": "13505688", "url": "https://en.wikipedia.org/wiki?curid=13505688", "title": "Analytic semigroup", "text": "Analytic semigroup\n\nIn mathematics, an analytic semigroup is particular kind of strongly continuous semigroup. Analytic semigroups are used in the solution of partial differential equations; compared to strongly continuous semigroups, analytic semigroups provide better regularity of solutions to initial value problems, better results concerning perturbations of the infinitesimal generator, and a relationship between the type of the semigroup and the spectrum of the infinitesimal generator.\n\nLet Γ(\"t\") = exp(\"At\") be a strongly continuous one-parameter semigroup on a Banach space (\"X\", ||·||) with infinitesimal generator \"A\". Γ is said to be an analytic semigroup if\n\n\n\nThe infinitesimal generators of analytic semigroups have the following characterization:\n\nA closed, densely defined linear operator \"A\" on a Banach space \"X\" is the generator of an analytic semigroup if and only if there exists an \"ω\" ∈ R such that the half-plane Re(\"λ\") > \"ω\" is contained in the resolvent set of \"A\" and, moreover, there is a constant \"C\" such that\n\nfor Re(\"λ\") > \"ω\" and where formula_3 is the resolvent of the operator \"A\". Such operators are called \"sectorial\". If this is the case, then the resolvent set actually contains a sector of the form\n\nfor some \"δ\" > 0, and an analogous resolvent estimate holds in this sector. Moreover, the semigroup is represented by\n\nwhere \"γ\" is any curve from \"e\"∞ to \"e\"∞ such that \"γ\" lies entirely in the sector\n\nwith \"π\" ⁄ 2 < \"θ\" < \"π\" ⁄ 2 + \"δ\".\n"}
{"id": "17995149", "url": "https://en.wikipedia.org/wiki?curid=17995149", "title": "Aperiodic finite state automaton", "text": "Aperiodic finite state automaton\n\nAn aperiodic finite-state automaton is a finite-state automaton whose transition monoid is aperiodic.\n\nA regular language is star-free if and only if it is accepted by an automaton with a finite and aperiodic transition monoid. This result of algebraic automata theory is due to Marcel-Paul Schützenberger.\n\nA counter-free language is a regular language for which there is an integer \"n\" such that for all words \"x\", \"y\", \"z\" and integers \"m\" ≥ \"n\" we have \"xy\"\"z\" in \"L\" if and only if \"xy\"\"z\" in \"L\". A counter-free automaton is a finite-state automaton which accepts a counter-free language. A finite-state automaton is counter-free if and only if it is aperiodic.\n\nAn aperiodic automaton satisfies the Černý conjecture.\n\n"}
{"id": "55959362", "url": "https://en.wikipedia.org/wiki?curid=55959362", "title": "Autocorrelation (words)", "text": "Autocorrelation (words)\n\nIn combinatorics, a branch of mathematics, the autocorrelation of a word is the set of periods of this word. More precisely, it is a sequence of values which indicate how much the end of a word looks likes the beginning of a word. This value can be used to compute, for example, the average value of the first occurrence of this word in a random string.\n\nIn this article, \"A\" is an alphabet, and formula_1 a word on \"A\" of length \"n\". The autocorrelation of formula_2 can be defined as the correlation of formula_2 with itself. However, we redefine this notion below.\n\nThe autocorrelation vector of formula_2 is formula_5, with formula_6 being 1 if the prefix of length formula_7 equals the suffix of length formula_7, and with formula_6 being 0 otherwise. That is formula_6 indicates whether formula_11.\n\nFor example, the autocorrelation vector of formula_12 is formula_13 since, clearly, for formula_14 being 0, 1 or 2, the prefix of length formula_7 is equal to the suffix of length formula_7. The autocorrelation vector of formula_17 is formula_18 since no strict prefix is equal to a strict suffix. Finally, the autocorrelation vector of formula_19 is 100011, as shown in the following table:\nNote that formula_20 is always equal to 1, since the prefix and the suffix of length formula_21 are both equal to the word formula_2. Similarly, formula_23 is 1 if and only if the first and the last letters are the same.\n\nThe autocorrelation polynomial of formula_2 is defined as formula_25. It is a polynomial of degree at most formula_26. \n\nFor example, the autocorrelation polynomial of formula_12 is formula_28 and the autocorrelation polynomial of formula_17 is formula_30. Finally, the autocorrelation polynomial of formula_19 is formula_32.\n\nWe now indicate some properties which can be computed using the autocorrelation polynomial.\n\nSuppose that you choose an infinite sequence formula_33 of letters of formula_34, randomly, each letter with probability formula_35, where formula_36 is the number of letters of formula_34. Let us call formula_38 the expectation of the first occurrence of formula_39 in formula_33. Then formula_38 equals formula_42. That is, each subword formula_43 of formula_2 which is both a prefix and a suffix causes the average value of the first occurrence of formula_2 to occur formula_46 letters later. Here formula_47 is the length of formula_47.\n\nFor example, over the binary alphabet formula_49, the first occurrence of formula_50 is at position formula_51 while the average first occurrence of formula_52 is at position formula_53. Intuitively, the fact that the first occurrence of formula_50 is later than the first occurrence of formula_52 can be explained in two ways:\nAutocorrelation polynomials allows to give simple equations for the ordinary generating functions (OGF) of many natural questions.\n\n\n"}
{"id": "340136", "url": "https://en.wikipedia.org/wiki?curid=340136", "title": "Bernstein polynomial", "text": "Bernstein polynomial\n\nIn the mathematical field of numerical analysis, a Bernstein polynomial, named after Sergei Natanovich Bernstein, is a polynomial in the Bernstein form, that is a linear combination of Bernstein basis polynomials.\n\nA numerically stable way to evaluate polynomials in Bernstein form is de Casteljau's algorithm.\n\nPolynomials in Bernstein form were first used by Bernstein in a constructive proof for the Stone–Weierstrass approximation theorem. With the advent of computer graphics, Bernstein polynomials, restricted to the interval [0, 1], became important in the form of Bézier curves.\n\nThe \"n\" + 1 Bernstein basis polynomials of degree \"n\" are defined as\n\nwhere formula_2 is a binomial coefficient.\n\nThe Bernstein basis polynomials of degree \"n\" form a basis for the vector space Π of polynomials of degree at most \"n\".\n\nA linear combination of Bernstein basis polynomials\n\nis called a Bernstein polynomial or polynomial in Bernstein form of degree \"n\".\n\nThe coefficients formula_4 are called Bernstein coefficients or Bézier coefficients.\n\nThe first few Bernstein basis polynomials are:\n\nThe Bernstein basis polynomials have the following properties:\nand by the inverse binomial transformation the reverse transformation is\n\nLet \"ƒ\" be a continuous function on the interval [0, 1]. Consider the Bernstein polynomial\n\nIt can be shown that\n\nuniformly on the interval [0, 1]. This is a stronger statement than the proposition that the limit holds for each value of \"x\" separately; that would be pointwise convergence rather than uniform convergence. Specifically, the word \"uniformly\" signifies that\n\nBernstein polynomials thus afford one way to prove the Weierstrass approximation theorem that every real-valued continuous function on a real interval [\"a\", \"b\"] can be uniformly approximated by polynomial functions over R.\n\nA more general statement for a function with continuous \"k\" derivative is\n\nwhere additionally\n\nis an eigenvalue of \"B\"; the corresponding eigenfunction is a polynomial of degree \"k\".\n\nSuppose \"K\" is a random variable distributed as the number of successes in \"n\" independent Bernoulli trials with probability \"x\" of success on each trial; in other words, \"K\" has a binomial distribution with parameters \"n\" and \"x\". Then we have the expected value E(\"K\"/\"n\") = \"x\".\n\nBy the weak law of large numbers of probability theory,\nfor every \"δ\" > 0. Moreover, this relation holds uniformly in \"x\", which can be seen from its proof via Chebyshev's inequality, taking into account that the variance of \"K\"/\"n\", equal to \"x\"(1-\"x\")/\"n\", is bounded from above by 1/(4\"n\") irrespective of \"x\".\n\nBecause \"ƒ\", being continuous on a closed bounded interval, must be uniformly continuous on that interval, one infers a statement of the form\nuniformly in \"x\". Taking into account that \"ƒ\" is bounded (on the given interval) one gets for the expectation\nuniformly in \"x\". To this end one splits the sum for the expectation in two parts. On one part the difference does not exceed ε; this part cannot contribute more than ε.\nOn the other part the difference exceeds ε, but does not exceed 2\"M\", where \"M\" is an upper bound for |\"ƒ\"(x)|; this part cannot contribute more than 2\"M\" times the small probability that the difference exceeds ε.\n\nFinally, one observes that the absolute value of the difference between expectations never exceeds the expectation of the absolute value of the difference, and that E(\"ƒ\"(\"K\"/\"n\")) is just the Bernstein polynomial \"B\"(\"ƒ\", \"x\").\n\nSee for instance.\n\n\n\n"}
{"id": "5126167", "url": "https://en.wikipedia.org/wiki?curid=5126167", "title": "Berry mechanism", "text": "Berry mechanism\n\nThe Berry mechanism, or Berry pseudorotation mechanism, is a type of vibration causing molecules of certain geometries to isomerize by exchanging the two axial ligands (see Figure at right) for two of the equatorial ones. It is the most widely accepted mechanism for pseudorotation and most commonly occurs in trigonal bipyramidal molecules such as PF, though it can also occur in molecules with a square pyramidal geometry. The Berry mechanism is named after R. Stephen Berry, who first described this mechanism in 1960.\n\nThe process of pseudorotation occurs when the two axial ligands close like a pair of scissors pushing their way in between two of the equatorial groups which scissor out to accommodate them. Both the axial and equatorial constituents move at the same rate of increasing the angle between the other axial or equatorial constituent. This forms a square based pyramid where the base is the four interchanging ligands and the tip is the \"pivot ligand\", which has not moved. The two originally equatorial ligands then open out until they are 180 degrees apart, becoming axial groups perpendicular to where the axial groups were before the pseudorotation. This requires about 3.6 kcal/mol in PF.\n\nThis rapid exchange of axial and equatorial ligands renders complexes with this geometry unresolvable (unlike carbon atoms with four distinct substituents), except at low temperatures or when one or more of the ligands is bi- or poly-dentate.\n\nThe Berry mechanism in square pyramidal molecules (such as IF) is somewhat like the inverse of the mechanism in bipyramidal molecules. Starting at the \"transition phase\" of bipyramidal pseudorotation, one pair of fluorines scissors back and forth with a third fluorine, causing the molecule to vibrate. Unlike with pseudorotation in bipyramidal molecules, the atoms and ligands which are not actively vibrating in the \"scissor\" motion are still participating in the process of pseudorotation; they make general adjustment based on the movement of the actively vibrating atoms and ligands. However, this geometry requires a significant amount of energy to occur of about 26.7 kcal/mol.\n\n"}
{"id": "1806503", "url": "https://en.wikipedia.org/wiki?curid=1806503", "title": "Canadian Society for History and Philosophy of Mathematics", "text": "Canadian Society for History and Philosophy of Mathematics\n\nThe Canadian Society for History and Philosophy of Mathematics (CSHPM) is dedicated to the study of the history and philosophy of mathematics in Canada. It was proposed by Kenneth O. May, in conjunction with the journal \"Historia Mathematica\", and was founded in 1974.\n\n"}
{"id": "27750331", "url": "https://en.wikipedia.org/wiki?curid=27750331", "title": "Circuits over sets of natural numbers", "text": "Circuits over sets of natural numbers\n\nCircuits over natural numbers are a mathematical model used in studying computational complexity theory. They are a special case of circuits. The object is a labeled directed acyclic graph the nodes of which evaluate to sets of natural numbers, the leaves are finite sets, and the gates are set operations or arithmetic operations.\n\nAs an algorithmic problem, the problem is to find if a given natural number is an element of the output node or if two circuits compute the same set. Decidability is still an open question.\n\nA natural number circuit is a circuit, i.e. a labelled directed acyclic graph of in-degree at most 2. The nodes of in-degree 0, the leaves, are finite sets of natural numbers, the labels of the nodes of in-degree 1 are −, where formula_1 and the labels of the nodes of in-degree 2 are +, ×, ∪ and ∩, where formula_2, formula_3 and ∪ and ∩ with the usual set meaning.\n\nThe subset of circuits which do not use all of the possible labels are also studied.\n\nOne can ask:\n\nFor circuits which use all the labels, all these problems are equivalent.\n\nThe first problem is reducible to the second one, by taking the intersection of the output gate and \"n\". Indeed, the new output get will be empty if and only if \"n\" was not an element of the former output gate.\n\nThe first problem is reducible to the third one, by asking if the node \"n\" is a subset of the output node.\n\nThe second problem is reducible to the first one, it suffices to multiply the output gate by 0, then 0 will be in the output gate if and only if the former output gate were not empty.\n\nThe third problem is reducible to the second one, checking if A is a subset of B is equivalent to ask if there is an element in formula_5.\n\nLet O be a subset of {∪,∩,−,+,×}, then we call MC(O) the problem of finding if a natural number is inside the output gate of a circuit the gates' labels of which are in O, and MF(O) the same problem with the added constraint that the circuit must be a tree.\n\nOne difficulty comes from the fact that the complement of a finite set is infinite, and a computer has got only a finite memory. But even without complementation, one can create double exponential numbers. Let formula_6, then one can easily prove by induction on formula_7 that formula_8, indeed formula_9 and by induction formula_10.\n\nAnd even double exponential—sized sets: let formula_11, then formula_12, i.e. formula_13 contains the formula_14 firsts number. Once again this can be proved by induction on formula_7, it is true for formula_16 by definition and let formula_17, dividing formula_18 by formula_14 we see that it can be written as formula_20 where formula_21, and by induction, formula_22 and formula_23 are in formula_13, so indeed formula_25.\n\nThese examples explains why addition and multiplication are enough to create problems of high complexity.\n\nThe membership problem asks if, given an element \"n\" and a circuit, \"n\" is in the output gate of the circuit.\n\nWhen the class of authorized gates is restricted, the membership problem lies inside well known complexity classes. Note that the size variable here is the size of the circuit or tree; the value of \"n\" is assumed to be fixed.\n\nThe equivalence problem asks if, given two gates of a circuit, they evaluate to the same set.\n\nWhen the class of authorized gates is restricted, the equivalence problem lies inside well known complexity classes. We call EC(O) and EF(O) the problem of equivalence over circuits and formulae the gates of which are in O.\n\n"}
{"id": "59047031", "url": "https://en.wikipedia.org/wiki?curid=59047031", "title": "Combinatorial matrix theory", "text": "Combinatorial matrix theory\n\nCombinatorial matrix theory is a branch of linear algebra and combinatorics that studies matrices in terms of the patterns of nonzeros and of positive and negative values in their coefficients.\n\nConcepts and topics studied within combinatorial matrix theory include:\n\nResearchers in combinatorial matrix theory include Richard A. Brualdi and Pauline van den Driessche.\n"}
{"id": "6471621", "url": "https://en.wikipedia.org/wiki?curid=6471621", "title": "Control variable", "text": "Control variable\n\nThe control variable (or scientific constant) in scientific experimentation is the experimental element which is constant and unchanged throughout the course of the investigation. The control variable strongly influences experimental results, and it is held constant during the experiment in order to test the relative relationship of the dependent and independent variables. The control variable itself is not of primary interest to the experimenter.\n\nA variable in an experiment which is held constant in order to assess the relationship between multiple variables, is the control variable. A control variable is the one element that is not changed throughout an experiment, because its unchanging state allows the relationship between the other variables being tested to be better understood. \n\nEssentially, a control variable is what is kept the same throughout the experiment, and it is not of primary concern in the experimental outcome. Any change in a control variable in an experiment would invalidate the correlation of dependent variables (DV) to the independent variable (IV), thus skewing the results.\n\nIn any system existing in a natural state, many variables may be interdependent, with each affecting the other. Scientific experiments tests the relationship of an IV –that element which is manipulated by the experimenter– to the DV –that element affected by the manipulation of the IV. Any additional independent variable can be a control variable.\n\nTake, for example, the combined gas law, which is stated mathematically as:\n\nwhere:\n\nIn an experimental verification of one part of the combined gas law, Boyle's law, ( * = ), where Pressure, Temperature, and Volume are all variables, to test the resultant changes to any of these variables requires at least one to be kept constant. This is in order to see \"comparable experimental results\" in the remaining variables. \n\nIf Volume is made the control variable and it is not allowed to change throughout the course of the experiment, the relationship between dependent variables, Pressure and Temperature, can quickly be established by changing the value for one or the other. For instance, if the Pressure is raised then the Temperature must increase.\n\nIf, however, Temperature is made the control variable and it is not allowed to change throughout the course of the experiment, the relationship between the dependent variables, Pressure and Volume, can quickly be established by changing the value for one or the other. For instance, the Pressure is raised then the Volume must decrease.\n\n"}
{"id": "886966", "url": "https://en.wikipedia.org/wiki?curid=886966", "title": "Cryptographic Module Validation Program", "text": "Cryptographic Module Validation Program\n\nThe Cryptographic Module Validation Program (CMVP) is a joint American and Canadian security accreditation program for cryptographic modules. The program is available to any vendors who seek to have their products certified for use by the U.S. Government and regulated industries (such as financial and health-care institutions) that collect, store, transfer, share and disseminate \"sensitive, but not classified\" information. All of the tests under the CMVP are handled by third-party laboratories that are accredited as Cryptographic Module Testing Laboratories by the National Voluntary Laboratory Accreditation Program (NVLAP). Product certifications under the CMVP are performed in accordance with the requirements of FIPS 140-2.\n\nThe CMVP was established by the U.S. National Institute of Standards and Technology (NIST) and the Communications Security Establishment (CSE) of the Government of Canada in July 1995.\n\nThe Cryptographic Algorithm Validation Program (CAVP), which provides guidelines for validation testing for FIPS approved and NIST recommended cryptographic algorithms and components of algorithms, is a prerequisite for CMVP.\n\n"}
{"id": "22920002", "url": "https://en.wikipedia.org/wiki?curid=22920002", "title": "Determinantal variety", "text": "Determinantal variety\n\nIn algebraic geometry, determinantal varieties are spaces of matrices with a given upper bound on their ranks. Their significance comes from the fact that many examples in algebraic geometry are of this form, such as the Segre embedding of a product of two projective spaces.\n\nGiven \"m\" and \"n\" and \"r\" < min(\"m\", \"n\"), the determinantal variety \"Y\" is the set of all \"m\" × \"n\" matrices (over a field \"k\") with rank ≤ \"r\". This is naturally an algebraic variety as the condition that a matrix have rank ≤ \"r\" is given by the vanishing of all of its (\"r\" + 1) × (\"r\" + 1) minors. Considering the generic \"m\" × \"n\" matrix whose entries are algebraically independent variables \"x\", these minors are polynomials of degree \"r\" + 1. The ideal of \"k\"[\"x\"] generated by these polynomials is a determinantal ideal. Since the equations defining minors are homogeneous, one can consider \"Y\" either as an affine variety in \"mn\"-dimensional affine space, or as a projective variety in (\"mn\" − 1)-dimensional projective space.\n\nThe radical ideal defining the determinantal variety is generated by the (\"r\" + 1) × (\"r\" + 1) minors of the matrix (Bruns-Vetter, Theorem 2.10).\n\nAssuming that we consider \"Y\" as an affine variety, its dimension is \"r\"(\"m\" + \"n\" − \"r\"). One way to see this is as follows: form the product space formula_1 over formula_2 where formula_3 is the Grassmannian of \"r\"-planes in an \"m\"-dimensional vector space, and consider the subspace formula_4, which is a desingularization of formula_5 (over the open set of matrices with rank exactly \"r\", this map is an isomorphism), and formula_6 is a vector bundle over formula_3 which is isomorphic to formula_8 where formula_9 is the tautological bundle over the Grassmannian. So formula_10 since they are birationally equivalent, and formula_11 since the fiber of formula_8 has dimension \"nr\".\n\nThe above shows that the matrices of rank <\"r\" contains the singular locus of formula_5, and in fact one has equality. This fact can be verified using that the radical ideal is given by the minors along with the Jacobian criterion for nonsingularity.\n\nThe variety \"Y\" naturally has an action of formula_14, a product of general linear groups. The problem of determining the syzygies of formula_5, when the characteristic of the field is zero, was solved by Alain Lascoux, using the natural action of \"G\".\n\nOne can \"globalize\" the notion of determinantal varieties by considering the space of linear maps between two vector bundles on an algebraic variety. Then the determinantal varieties fall into the general study of degeneracy loci. An expression for the cohomology class of these degeneracy loci is given by the Thom-Porteous formula, see (Fulton-Pragacz).\n\n"}
{"id": "28371257", "url": "https://en.wikipedia.org/wiki?curid=28371257", "title": "Elon Lindenstrauss", "text": "Elon Lindenstrauss\n\nElon Lindenstrauss (, born August 1, 1970) is an Israeli mathematician, and a winner of the 2010 Fields Medal.\n\nSince 2004, he has been a professor at Princeton University. In 2009, he was appointed to Professor at the Mathematics Institute at the Hebrew University.\n\nLindenstrauss was born into an Israeli-Jewish family with German origins. He was also born into a mathematical family, the son of the mathematician Joram Lindenstrauss, the namesake of the Johnson–Lindenstrauss lemma, and computer scientist Naomi Lindenstrauss, both professors at the Hebrew University. His sister Ayelet Lindenstrauss is also a mathematician. He attended the Hebrew University Secondary School. He enlisted to the IDF's Talpiot program, and studied at the Hebrew University, where he earned his BSc in Mathematics and Physics in 1991 and his master's degree in Mathematics in 1995. In 1999 he finished his Ph.D., his thesis being \"Entropy properties of dynamical systems\", under the guidance of Prof. Benjamin Weiss. He was a member at the Institute for Advanced Study in Princeton, New Jersey, then a Szego Assistant Prof. at Stanford University. From 2003 to 2005, he was a Long Term Prize Fellow at the Clay Mathematics Institute.\n\nIn Fall 2014, he was Visiting Miller Professor at the University of California, Berkeley.\n\nLindenstrauss works in the area of dynamics, particularly in the area of ergodic theory and its applications in number theory. With Anatole Katok and Manfred Einsiedler, he made progress on the Littlewood conjecture.\n\nIn a series of two papers (one co-authored with Jean Bourgain) he made major progress on Peter Sarnak's Arithmetic Quantum Unique Ergodicity conjecture. The proof of the conjecture was completed by Kannan Soundararajan.\n\nRecently with Manfred Einsiedler, Philippe Michel and Akshay Venkatesh, he studied distributions of torus periodic orbits in some arithmetic spaces, generalizing theorems by Hermann Minkowski and Yuri Linnik.\n\nTogether with Benjamin Weiss he developed and studied systematically the invariant of mean dimension introduced in 1999 by Mikhail Gromov. In related work he introduced and studied the small boundary property.\n\nAmong his co-authors are Jean Bourgain, Manfred Einsiedler, Philippe Michel, Shahar Mozes, Akshay Venkatesh and Barak Weiss.\n\n\n"}
{"id": "5517556", "url": "https://en.wikipedia.org/wiki?curid=5517556", "title": "Extension topology", "text": "Extension topology\n\nIn topology, a branch of mathematics, an extension topology is a topology placed on the disjoint union of a topological space and another set.\n\nThere are various types of extension topology, described in the sections below.\n\nLet X be a topological space and P a set disjoint from X. Consider in X ∪ P the topology whose open sets are of the form: A ∪ Q, where A is an open set of X and Q is a subset of P.\n\nNote that the closed sets of X ∪ P are of the form: B ∪ Q, where B is a closed set of X and Q is a subset of P.\n\nFor these reasons this topology is called the extension topology of X plus P, with which one extends to X ∪ P the open and the closed sets of X. Note that the subspace topology of X as a subset of X ∪ P is the original topology of X, while the subspace topology of P as a subset of X ∪ P is the discrete topology.\n\nBeing Y a topological space and R a subset of Y, one might ask whether the extension topology of Y - R plus R is the same as the original topology of Y, and the answer is in general no.\n\nNote the similitude of this extension topology construction and the Alexandroff one-point compactification, in which case, having a topological space X which one wishes to compactify by adding a point ∞ in infinity, one considers the closed sets of X ∪ {∞} to be the sets of the form: K, where K is a closed compact set of X, or B ∪ {∞}, where B is a closed set of X.\n\nLet X be a topological space and P a set disjoint from X. Consider in X ∪ P the topology whose open sets are of the form: X ∪ Q, where Q is a subset of P, or A, where A is an open set of X.\n\nFor this reason this topology is called the open extension topology of X plus P, with which one extends to X ∪ P the open sets of X. Note that the subspace topology of X as a subset of X ∪ P is the original topology of X, while the subspace topology of P as a subset of X ∪ P is the discrete topology.\n\nNote that the closed sets of X ∪ P are of the form: Q, where Q is a subset of P, or B ∪ P, where B is a closed set of X.\n\nBeing Y a topological space and R a subset of Y, one might ask whether the extension topology of Y - R plus R is the same as the original topology of Y, and the answer is in general no.\n\nNote that the open extension topology of X ∪ P is smaller than the extension topology of X ∪ P.\n\nBeing Z a set and p a point in Z, one obtains the excluded point topology construction by considering in Z the discrete topology and applying the open extension topology construction to Z - {p} plus p.\n\nLet X be a topological space and P a set disjoint from X. Consider in X ∪ P the topology whose closed sets are of the form: X ∪ Q, where Q is a subset of P, or B, where B is a closed set of X.\n\nFor this reason this topology is called the closed extension topology of X plus P, with which one extends to X ∪ P the closed sets of X. Note that the subspace topology of X as a subset of X ∪ P is the original topology of X, while the subspace topology of P as a subset of X ∪ P is the discrete topology.\n\nNote that the open sets of X ∪ P are of the form: Q, where Q is a subset of P, or A ∪ P, where A is an open set of X.\n\nBeing Y a topological space and R a subset of Y, one might ask whether the extension topology of Y - R plus R is the same as the original topology of Y, and the answer is in general no.\n\nNote that the closed extension topology of X ∪ P is smaller than the extension topology of X ∪ P.\n\nBeing Z a set and p a point in Z, one obtains the particular point topology construction by considering in Z the discrete topology and applying the closed extension topology construction to Z - {p} plus p.\n"}
{"id": "3250254", "url": "https://en.wikipedia.org/wiki?curid=3250254", "title": "Firefly (key exchange protocol)", "text": "Firefly (key exchange protocol)\n\nFirefly is a U.S. National Security Agency public-key key exchange protocol, used in EKMS, the STU-III secure telephone, and several other U.S. cryptographic systems.\n\n"}
{"id": "26013198", "url": "https://en.wikipedia.org/wiki?curid=26013198", "title": "Genaille–Lucas rulers", "text": "Genaille–Lucas rulers\n\nGenaille–Lucas rulers (also known as Genaille's rods) are an arithmetic tool invented by Henri Genaille, a French railway engineer, in 1891. The device is a variant of Napier's bones. By representing the carry graphically, the user can read off the results of simple multiplication problems directly, with no intermediate mental calculations.\n\nIn 1885, French mathematician Édouard Lucas posed an arithmetic problem during a session of the Académie française. Genaille, already known for having invented a number of arithmetic tools, created his rulers in the course of solving the problem. He presented his invention to the Académie française in 1891. The popularity of Genaille's rods was widespread but short-lived, as mechanical calculators soon began to displace manual arithmetic methods.\n\nA full set of Genaille–Lucas rulers consists of eleven strips of wood or metal. On each strip is printed a column of triangles and a column of numbers:\n\nBy arranging these rulers in the proper order, the user can solve multiplication problems.\n\nConsider multiplying 52749 by 4. Five rulers, one for each digit of 52749, are arranged side-by-side, next to the \"index\" ruler:\n\nThe second multiplicand is 4, so we look at the fourth row:\n\nWe start from the top number in the last column of the selected row:\n\nThe grey triangle points the way to the next number:\n\nWe follow the triangles from right to left, until we reach the first column.\n\nThen we simply read off the digits that we visited. The product, shown in red, is 210996.\n\nSoon after their development by Genaille, the rulers were adapted to a set of rods that can perform division. The division rods are aligned similarly to the multiplication rods, with the index rod on the left denoting the divisor, and the following rods spelling out the digits of the dividend. After these, a special \"remainder\" rod is placed on the right. The quotient is read from left to right, following the lines from one rod to the next. The path of digits ends with a number on the remainder rod, which is the remainder given by the division. \n\n\n"}
{"id": "862226", "url": "https://en.wikipedia.org/wiki?curid=862226", "title": "Gerd Faltings", "text": "Gerd Faltings\n\nGerd Faltings (; born 28 July 1954) is a German mathematician known for his work in arithmetic algebraic geometry.\n\nFrom 1972 to 1978, Faltings studied mathematics and physics at the University of Münster. In 1978 he received his PhD in mathematics.\n\nIn 1981 he obtained the \"venia legendi\" (Habilitation) in mathematics, both from the University of Münster. During this time he was an assistant professor at the University of Münster. From 1982 to 1984, he was professor at the University of Wuppertal. \n\nFrom 1985 to 1994, he was professor at Princeton University. In the fall of 1988 and in the academic year 1992–1993 he was a visiting scholar at the Institute for Advanced Study.\n\nIn 1986 he was awarded the Fields Medal at the ICM at Berkeley for proving the Tate conjecture for abelian varieties over number fields, the Shafarevich conjecture for abelian varieties over number fields and the Mordell conjecture, which states that any non-singular projective curve of genus \"g\" > 1 defined over a number field \"K\" contains only finitely many \"K\"-rational points. As a Fields Medalist he gave an ICM plenary talk \"Recent progress in arithmetic algebraic geometry\". \n\nIn 1994 as an ICM invited speaker in Zurich he gave a talk \"Mumford-Stabilität in der algebraischen Geometrie.\" Extending methods of Paul Vojta, he proved the Mordell–Lang conjecture, which is a generalization of the Mordell conjecture. Together with Gisbert Wüstholz, he reproved Roth's theorem, for which Roth had been awarded the Fields medal in 1958.\n\nIn 1994, he returned to Germany and from 1994 to 2018, he was a director of the Max Planck Institute for Mathematics in Bonn. \nIn 1996, he received the Gottfried Wilhelm Leibniz Prize of the Deutsche Forschungsgemeinschaft, which is the highest honour awarded in German research.\n\nFaltings was the formal supervisor of Shinichi Mochizuki, Wieslawa Niziol, Nikolai Dourov.\n\n"}
{"id": "767892", "url": "https://en.wikipedia.org/wiki?curid=767892", "title": "Grand coalition", "text": "Grand coalition\n\nA grand coalition is an arrangement in a multi-party parliamentary system in which the two largest political parties of opposing political ideologies unite in a coalition government. The term is most commonly used in countries where there are two dominant parties with different ideological orientations, and a number of smaller parties that have passed the election threshold to secure representation in the parliament. The two large parties will each try to secure enough seats in any election to have a majority government alone, and if this fails each will attempt to form a coalition with smaller parties that have a similar ideological orientation. Because the two large parties will tend to differ on major ideological issues, and portray themselves as rivals, or even sometimes enemies, they will usually find it more difficult to agree on a common direction for a combined government with each other than with smaller parties.\n\nOccasionally circumstances arise where normally opposing parties may find it desirable to form a government. One is a national crisis such as a war or depression, where people feel a need for national unity and stability that overcomes ordinary ideological differences. This is especially true where there is broad agreement about the best policy to deal with the crisis. In this case, a grand coalition may occur even when one party has enough seats to govern alone. An example would be the British national governments during World War I and before and during World War II.\n\nAnother possibility is that the major parties may find they have more in common ideologically with each other than with the smaller parties, or that the fragmentation of the smaller parties is so great that no other coalition is stable. Examples include Austria, where the mainstream parties of the left and right have often formed grand coalitions to keep parties of the far left or far right out of government (an example of a \"cordon sanitaire\"), or Israel, where no single party has \"ever\" won enough seats to govern alone, and, in some parliaments, the fragmentation and intransigence of some of the smaller parties has made it easier to maintain a coherent platform with a grand coalition than with a narrow one. This is often done out of political necessity, to prevent an early election.\n\nIn post-war Austria, a \"grand coalition\" () between conservative Austrian People's Party (ÖVP) and the Social Democratic Party of Austria (SPÖ) has been the standard case of governance. Notable exceptions were Josef Klaus's second government (ÖVP, 1966–70), the era of Chancellor Bruno Kreisky who governed from 1970 until 1983 with an SPÖ majority and from 1983 until 1987 in coalition with the FPÖ, ÖVP Chancellor Wolfgang Schüssel's coalition governments with right-wing populist Freedom Party of Austria (FPÖ) from 2000 until 2005 and Alliance for the Future of Austria (BZÖ) from 2005 until 2007, and ÖVP Chancellor Sebastian Kurz's current coalition government with the FPÖ since 2017.\n\nIn post-war Germany, \"grand coalition\" () refers to a governing coalition of the two largest parties, usually the Christian Democrats (CDU/CSU) with the Social Democrats (SPD). While Germany has historically tended to favor narrow coalitions of one of the two largest parties with the FDP or with the Greens, four grand coalitions have been formed on a federal level: the Kiesinger cabinet (1966–1969), the First Merkel cabinet (2005–2009), the Third Merkel cabinet (2013-2018), and the Fourth Merkel cabinet (since 2018). Under the Weimar Republic, a grand coalition was one including all of the major parties of the left, center, and center-right who formed the basis of most governments - the SPD, the Catholic Centre Party, the German Democratic Party (DDP), and the German People's Party (DVP). The two examples were the first and second Stresemann cabinets (August-November 1923) and, less ephemerally, the second Müller cabinet (1928-1930).\n\nIceland, a country in Europe has a grand coalition since 30 November 2017 between the largest parties of the centre-right to right-wing Independence Party (16), the left-wing Left-Green Movement (9), and the centre-right Progressive Party (8).\n\nIn Italy, \"grand coalition\" () refers to the only supermajority government formed in April 2013 between center-left Democratic Party (PD), center-right The People of Freedom (PdL) party, and the centrist Civic Choice (SC) and Union of the Centre (UdC) parties. In November 2013, The People of Freedom (later renamed as Forza Italia) however dropped out and broke apart, leaving the Letta Cabinet and further Renzi Cabinet (Coalition between PD, NCD, SC and UdC) with a small majority.\n\nThe UK has had grand coalitions in central government during periods of wartime. They are referred to as the \"National Government\".\n\nThe Northern Ireland Executive, the devolved administration of Northern Ireland, combines the largest Republican (also predominantly left of centre) and Unionist (also predominantly right of centre) parties. The chief post, of First Minister and deputy First Minister, is a diarchy. Most recently, this coalition was led by the Democratic Unionist Party and Sinn Féin. \n\nAll parties, major and minor, are offered posts in the executive, although may now opt to form an opposition.\n\nThe Cayman Islands, a British overseas territory, has an incumbent coalition between the largest parties; the centre-left Progressives and centre-right Democrats.\n\nIn the Netherlands, there have been several cabinets which can be described as grand coalitions. The Roman/Red coalitions of the 1940s and 1950s under Prime Minister Willem Drees were composed of the catholic Catholic People's Party (KVP) and the social-democratic Labour Party (PvdA) at its core and several smaller parties as backup (Drees–Van Schaik). The Purple coalitions in the 1990s under Prime Minister Wim Kok were between the Labour Party (PvdA), the conservative liberal People's Party for Freedom and Democracy (VVD) and the social-liberal Democrats 66 (D66) party (First Kok cabinet). The Second Rutte cabinet a grand coalition cabinet which also can be described as a purple coalition was composed of the People's Party for Freedom and Democracy (VVD) and the Labour Party (PvdA). A more traditional grand coalition cabinet was the Third Lubbers cabinet, comprising the christian-democratic Christian Democratic Appeal (CDA) and the Labour Party (PvdA).\n\nIn Spain, the term \"grand coalition\" is typically used to refer to any hypothetical government formed between the centre-right to right-wing People's Party (PP) and the centre-left Spanish Socialist Workers' Party (PSOE). No such a coalition government as ever been formed at the national level, though it was proposed by then-Prime Minister Mariano Rajoy during the 2015–2016 government formation process. Rajoy's own investiture on 29 October 2016 was allowed through the abstention of PSOE's MPs in what was dubbed as a \"covert grand coalition\", in reference to PSOE's tolerance of Rajoy's minority government through punctual agreements until the re-election of Pedro Sánchez as party leader in June 2017.\n\nAt the regional level, grand coalitions between the two largest parliamentary forces have been rare, but examples exist:\n\nAdditionally, both PSOE and PP formed a joint coalition government—which also included other parties—following a successful vote of no confidence on the Cantabrian regional government of Juan Hormaechea in 1990, enduring until the 1991 regional election. At the time, however, the PP was not among the two largest political parties in the regional assembly.\n\n\n\n"}
{"id": "2561202", "url": "https://en.wikipedia.org/wiki?curid=2561202", "title": "Grundzüge der Mengenlehre", "text": "Grundzüge der Mengenlehre\n\nGrundzüge der Mengenlehre (German for \"Basics of Set Theory\") is an influential book on set theory written by Felix Hausdorff.\n\nFirst published in April 1914, \"Grundzüge der Mengenlehre\" was the first comprehensive introduction to set theory. Besides the systematic treatment of known results in set theory, the book also contains chapters on measure theory and topology, which were then still considered parts of set theory. Hausdorff presented and developed original material which was later to become the basis for those areas. In 1927 Hausdorff published an extensively revised second edition under the title \"Mengenlehre\" (German for \"Set Theory\"), with many of the topics of the first edition omitted. In 1935 there was a third German edition, which in 1957 was translated by John R. Aumann \"et al.\" into English under the title \"Set Theory\".\n\n"}
{"id": "41316662", "url": "https://en.wikipedia.org/wiki?curid=41316662", "title": "Heinrich Scholz", "text": "Heinrich Scholz\n\nHeinrich Scholz (; December 17, 1884 – December 30, 1956) was a German logician, philosopher, and Protestant theologian who was a peer of Alan Turing, who wrote in his memoirs that he on the inclusion of his essay from 1936 \"On Computable Numbers, with an Application to the Entscheidungsproblem\": \"[I was disappointed that only] two people could have understood it, and would have responded [had I been asked] – Heinrich Scholz and Richard Bevan Braithwaite.\"\n\nScholz had an extraordinary career but was not considered a brilliant logician, for example on the same level as Gottlob Frege or Rudolf Carnap, but was considered an outstanding scientist of national importance. He provided a suitable academic environment for his students to thrive. He founded the Institute of Mathematical Logic and Fundamental Research at the University of Münster in 1936, which can be said enabled the study of logic at the highest international level after World War II up until the present day.\n\nHerman Scholz father was a minister at St. Mary's Church, Berlin. From 1903 to 1907 he studied philosophy and theology at Erlangen University and Berlin University achieving a Licentiate in theology (Lic. theol.). He was a student of Adolf von Harnack, in philosophy with peers Alois Riehl and Friedrich Paulsen. On 28 July 1910, Scholz habilitated in the subjects of religious philosophy and systematic theology in Berlin, and was promoted to full professor, therein working as a lecturer. In 1913, at Erlangen, Herman Sholz took his examination for promotion of Dr. phil. with Richard Falckenberg Richard Falckenberg, studying the work of Schleiermacher and Goethe, his thesis title: \"A contribution from the history of the German spirit.\" In 1917 was appointed to the chair of Philosophy of Religion at the Breslau succeeding Rudolf Otto to teach religious philosophy and systematic theology. In the same year he married his fiancée, Elisabeth Orth. Due to 8 years of continuous gastric trouble, he was exempted from military service. In 1919, he underwent an operation in which he believed to be a large part of his stomach was removed. That year he took the call to Kiel University, as the chair of philosophy. It was while at Kiel, in 1924, that Scholz's first wife, Elisabeth Orth died.\n\nFrom October 1928 onwards, he taught in Münster University, first as \"Professor of Philosophy\". In 1938, this was changed to \"Professor of Philosophy of Mathematics and Science\" and again in 1943 to \"Chair of Mathematical Logic and Fundamental Questions in Mathematics\" working as head of the Institute for Mathematical Logic and Fundamental Research at Münster until he retired in 1952 as professor emeritus.\n\nScholz was survived by his second wife, Erna. Scholz grave is located on the Park Cemetery Eichhof near Kiel.\n\nFrom his own account, in 1921, having by accident came across Principia Mathematica by Bertrand Russell and Alfred North Whitehead he began studying logic, which he had abandoned in his youth to study theology, leading later to a study of mathematics and theoretical physics by taking an undergraduate degree at Kiel. However another factor in his change of focus was the mathematician Otto Toeplitz. Toeplitz's broad research interests including Hilbert spaces and spectral theory encouraged Scholz interest in mathematics. Indeed, Segal suggests that Scholz love of structure was also an important factor in his move into mathematical logic, describing it this:\n\n\"Scholz's feeling for structure was no small thing. He apparently felt that when having guests for dinner: (1) no more than six people should be invited; (2) there must be an excellent menu; (3) a discussion theme must be planned; and (4) the guests should have prepared themselves as much as possible beforehand on this theme.\"\n\nIn 1925, he was a peer of Karl Barth at Münster University, in which he taught Protestant theology. Under the influence of conversations with Scholz, Barth later wrote in 1930/31. his book about the Anselm of Canterbury proof of God \"\"fides quaerens intellectum\".\"\n\nIn the 1930s, he continued to maintain contact with Alan Turing, who later wrote in his memoirs that he on the inclusion of his essay from 1936 \"On Computable Numbers, with an Application to the Entscheidungsproblem\" was disappointed that only two people could have understood it, and would have responded – Heinrich Scholz and Richard Bevan Braithwaite.\n\nAt the University of Münster, his study into mathematical logic and basic research, provided many of the critical insights, that contributed to the foundations of theoretical computer science. Right from the time he arrived at Münster, Scholz worked towards building a school of mathematical logic. By 1935, his research team at Münster were being referred to as the \"Münster school of mathematical logic\". Scholz names 1936, as the year the \"Münster School\" was born. His professorship was rededicated in 1936 to a lectureship for mathematical logic and fundamental research and in 1943 the first chair in Germany for mathematical logic and fundamental research. The Münster Chair is still regarded as one of the best in Germany.\n\nScholz was considered a Platonist, and in that sense, he regarded the mathematical logic as the foundation of knowledge. In 1936 he was awarded a grant from the DFG, for the production of three volumes of research in logic and for the editing of the Gottlob Frege papers. He is considered the discoverer of the estate of Gottlob Frege.\n\nGisbert Hasenjaeger whose thesis had been supervised by Scholtz, produced a book \"Grundzüge der mathematischen Logik\" in 1961 which was jointly authored with Scholz despite being published five years after Scholz's death.\n\nInitially Scholz was pleased with the rise of Nazi power in Germany. Describing himself a conservative nationalist, describing himself as such \"We felt like Prussians right to the bone,\"\" and described by his friend Heinrich Behnke as a \"small-minded Prussian nationalist\". Behnke found discussing political issues difficult. In the beginning the Nazi laws helped establish Münster as an important centre for Logic as other university staff at Göttingen and Berlin Universities were being obliterated.\n\nOn 14 March 1940, Scholz sent a letter to the Education department of occupied Poland, seeking the release of Jan Salamucha, who had been professor of theology at Kraków University. Salamucha was sent to Sachsenhausen concentration camp in 1940. In October 1940, Scholz received a reply for the education minister which stated he had \"injured the national honour\" and was forbidden to send further petitions. Salamucha was later released but killed by the Nazis in 1944 However, Scholz persisted, first helping Alfred Tarski, who had fled Poland to the United States, to correspond with his wife who remained in Poland and later helping the Polish Logician Jan Lukasiewicz, who he had been corresponding since 1938, to leave Poland with his wife and hide in Germany.\n\nAlthough Scholz' recognized the true nature of the Nazis and abhorred them from mid-1942 onwards, he remained on good terms with Nazi academics like Ludwig Bieberbach. During the period of National Socialism, Max Steck, who championed the \"German Mathematics\" which rejected the formalist approach to mathematics, deeply opposed Hilbert's approach which he described as Jewish - the worst possible insult in Germany at this time. Max Steck acknowledged the \"per se outstanding achievement of formalism\" (\"an sich betrachtet einmaligen Leistung des Formalismus\"), but criticized the \"missing epistemological component\" (\"Jede eigentliche Erkenntnistheorie fehlt im Formalismus\") and on the only page of his main work where he connects formalism and Jews he mentions that \"Jews were the actual trendsetters of formalism\" (\"die eigentlichen Schrittmacher des Formalismus\"). In response to this, Bieberbach asked Scholz to write an article for Deutsche Mathematik, to answer the attacks on mathematical formalism by Steck, which was surprising since Bieberbach led the Nazi mathematicians' attack on Jewish mathematics. Ensuring that Hilbert was not considered \"Jewish\", Scholz wrote \"What does formalised study of the foundations of mathematics aim at?.\" Scholz had received funding from Bieberbach as early as 1937, which prompted an annoyed Steck to write in his 1942 book:\n\nThere were three other articles by Heinrich Scholz in the journal \"German Mathematics\": \"Ein neuer Vollständigkeitsbeweis für das reduzierte Fregesche Axiomensystem des Aussagenkalküls\" (1936), a review of the Nazi philosopher Wolfgang Cramer's book \"Das Problem der reinen Anschauung\" (1938) and a review of Andreas Speiser's \"Ein Parmenideskommentar\" (1938).\n\nIn the late 2000s, Achim Clausing was tasked with going through the remaining estate of Scholz at Münster University, and while going through the archive papers in the basement of the Institute of Computer Science, Clausing discovered two original prints of the most important publication of Alan Turing, which had been missing since 1945. In this case, the work \"On Computable Numbers, with an Application to the Entscheidungsproblem\" from 1936, which Scholz had requested, and a postcard from Turing. Based on the work by Turing and conversations with Scholz, Clausing stated \"[it was] the world's first seminar on computer science.\" The second work, which was a Mind (journal) article, dates from 1950 and is a treatise on the development of artificial intelligence, Turing provided them with a handwritten comment. \"This is probably my last copy\". At Sotheby's recently, comparable prints of Turing, with no attached dedication, sold for 180,000 euros.\n\n\n\n\n"}
{"id": "20675409", "url": "https://en.wikipedia.org/wiki?curid=20675409", "title": "Infinity and the Mind", "text": "Infinity and the Mind\n\nInfinity and the Mind: The Science and Philosophy of the Infinite is a theoretical mathematics book by American mathematician, computer scientist, and science fiction writer Rudy Rucker.\n\nThe dust jacket of the original (Birkhäuser) edition had a picture of a star over a mountain and appeared to be a religious or mystical work. Subsequent paperback reprintings had covers suggesting that the publishers wished to appeal to a \"new age\" market. The most recent editions, from Princeton University Press, have very austere covers suggesting an academic work.\n\nThe book contains accessible popular expositions on the mathematical theory of infinity, and a number of related topics. These include Gödel's incompleteness theorems and their relationship to concepts of artificial intelligence and the human mind, as well as the conceivability of some unconventional cosmological models. The material is approached from a variety of viewpoints, some more conventionally mathematical and others being nearly mystical. There is a brief account of the author's personal contact with Kurt Gödel.\n\nAn appendix contains one of the few popular expositions on set theory research on what are known as \"strong axioms of infinity.\"\n\n"}
{"id": "43646359", "url": "https://en.wikipedia.org/wiki?curid=43646359", "title": "Insider investment strategy", "text": "Insider investment strategy\n\nThe insider investment strategy is an investment strategy that follows the buying and selling decisions of insiders. The primary insiders have information advantage and the proven theory is that they as a group over time will do better than the average investor on the Stock Exchange. However, in the world there are only a few investment funds that follow the insider trades, both of them were established in 2011.\nIn America Catalyst Capital Advisors LLC manages Catalyst Insider Buying Fund. This fund is a large-cap, long-only equity fund that only invests in companies where corporate insiders are buying their own company's stock on the open market.\nIn Europe Dovre Forvaltning UAB manages Dovre Inside Nordic fund. This fund is a Nordic equity fund, which strategy is to follow the primary insiders in listed companies and/or shareholders with assumed close relationship to company, board and management.\n\nA Lorie-Niederhoffer study indicates that proper and prompt analysis of data on insider trading can be profitable.\n\nGlass examines 14 different calendar months and selects the eight securities with the greatest excess of buyers to sellers among insiders within a month. He finds that the average return on these securities is 10 percent above the return on the stock market as a whole in the 7 months following the individual months of intensive buying.\n\nIn 2014 Dovre Forvaltning shared there analysis on insiders influence in the Nordic region. The company analyzed these different yearly portfolios (both for Purchases and Sales):\n\nOnly transactions above 80.000 SEK were included (33% of all insider trades were excluded because they were too small). If there were no Purchases/Sales in 1,3,6 months after a company's inclusion, it was excluded from the portfolio. All stocks are equally weighted.\nThe analysis showed that:\n\n"}
{"id": "4368061", "url": "https://en.wikipedia.org/wiki?curid=4368061", "title": "International Joint Conference on Automated Reasoning", "text": "International Joint Conference on Automated Reasoning\n\nThe International Joint Conference on Automated Reasoning (IJCAR) is a series of conferences on the topics of automated reasoning, automated deduction, and related fields. It is organized semi-regularly as a merger of other meetings. IJCAR replaces those independent conferences in the years it takes place. The conference is organized by CADE Inc., and CADE has always been one of the conferences partaking in IJCAR.\n\n"}
{"id": "691927", "url": "https://en.wikipedia.org/wiki?curid=691927", "title": "Invariant subspace problem", "text": "Invariant subspace problem\n\nIn the field of mathematics known as functional analysis, the invariant subspace problem is a partially unresolved problem asking whether every bounded operator on a complex Banach space sends some non-trivial closed subspace to itself. The original form of the problem as posed by Paul Halmos was in the special case of polynomials with compact square. This was resolved affirmatively, for a more general class of polynomially compact operators, by Allen R. Bernstein and Abraham Robinson in 1966 (see for a summary of the proof).\n\nMore formally, the invariant subspace problem for a complex Banach space \"H\" of dimension > 1 is the question whether every bounded linear operator \"T\" : \"H\" → \"H\" has a non-trivial closed \"T\"-invariant subspace (a closed linear subspace \"W\" of \"H\" which is different from {0} and \"H\" such that \"T\"(\"W\") ⊆ \"W\").\n\nTo find a counterexample to the invariant subspace problem means to answer affirmatively the following equivalent question: Does there exist a bounded linear operator \"T\" : \"H\" → \"H\" such that for every non-zero vector \"x\", the vector space generated by the sequence {\"T\"(\"x\") : \"n\" ≥ 0} is norm dense in \"H\"? Such operators are called cyclic.\n\nThe problem seems to have been stated in the mid-1900s after work by Beurling and von Neumann.\n\nFor Banach spaces, the first example of an operator without an invariant subspace was constructed by Enflo. (For Hilbert spaces, the invariant subspace problem remains open.)\n\nPer Enflo proposed a counterexample to the invariant subspace problem in 1975, publishing an outline in 1976. Enflo submitted the full article in 1981 and the article's complexity and length delayed its publication to 1987 Enflo's long \"manuscript had a world-wide circulation among mathematicians\" and some of its ideas were described in publications besides Enflo (1976). Enflo's works inspired a similar construction of an operator without an invariant subspace for example by Beauzamy, who acknowledged Enflo's ideas.\n\nIn the 1990s, Enflo developed a \"constructive\" approach to the invariant subspace problem on Hilbert spaces.\n\nWhile the general case of the invariant subspace problem is still open, several special cases have been settled for topological vector spaces (over the field of complex numbers):\n\n\n \n"}
{"id": "206507", "url": "https://en.wikipedia.org/wiki?curid=206507", "title": "Isaac Todhunter", "text": "Isaac Todhunter\n\nIsaac Todhunter FRS (23 November 1820 – 1 March 1884), was an English mathematician who is best known today for the books he wrote on mathematics and its history.\n\nThe son of George Todhunter, a Nonconformist minister, and Mary née Hume, he was born at Rye, Sussex. He was educated at Hastings, where his mother had opened a school after the death of his father in 1826. He became an assistant master at a school at Peckham, attending at the same time evening classes at the University College, London where he was influenced by Augustus De Morgan. In 1842 he obtained a mathematical scholarship and graduated as B.A. at London University, where he was awarded the gold medal on the M.A. examination. About this time he became mathematical master at a school at Wimbledon.\n\nIn 1844 Todhunter entered St John's College, Cambridge, where he was senior wrangler in 1848, and gained the first Smith's Prize and the Burney Prize; and in 1849 he was elected to a fellowship, and began his life of college lecturer and private tutor. In 1862 he was made a fellow of the Royal Society, and in 1865 a member of the Mathematical Society of London. In 1871 he gained the Adams Prize and was elected to the council of the Royal Society. He was elected honorary fellow of St John's in 1874, having resigned his fellowship on his marriage in 1864. In 1880 his eyesight began to fail, and shortly afterwards he was attacked with paralysis.\n\nHe is buried in the Mill Road cemetery, Cambridge.\n\nTodhunter married 13 August 1864 Louisa Anna Maria, eldest daughter of Captain (afterwards Admiral) George Davies, R.N. (at that time head of the county constabulary force). He died on 1 March 1884, at his residence, 6 Brookside, Cambridge. A mural tablet and medallion portrait were placed in the ante-chapel of his college by his widow, who, with four sons and one daughter, survived him.\n\nHe was a sound Latin and Greek scholar, familiar with French, German, Spanish, Italian, and also Russian, Hebrew, and Sanskrit. He was well versed in the history of philosophy, and on three occasions acted as examiner for the moral sciences tripos.\n\nAn unfinished work, \"The History of the Theory of Elasticity\", was edited and published posthumously in 1886 by Karl Pearson. A biographical work on William Whewell was published in 1876, in addition to many original papers in scientific journals.\n\nTodhunter wrote textbooks on algebra and trigonometry, and a revision of the translation by Robert Simson of Euclid's Elements, which, with an introduction by Thomas Little Heath, was republished by Everyman in 1933.\n\nTodhunter's major historical works include a history of the Probability theory from the time of Blaise Pascal to that of Pierre-Simon Laplace first published in 1865.\n\nSome of these are available at Isaac Todhunter's publications at Google Books.\n\n\n"}
{"id": "1591333", "url": "https://en.wikipedia.org/wiki?curid=1591333", "title": "Java Cryptography Architecture", "text": "Java Cryptography Architecture\n\nIn computing, the Java Cryptography Architecture (JCA) is a framework for working with cryptography using the Java programming language. It forms part of the Java security API, and was first introduced in JDK 1.1 in the package.\n\nThe JCA uses a \"provider\"-based architecture and contains a set of APIs for various purposes, such as encryption, key generation and management, secure random-number generation, certificate validation, etc. These APIs provide an easy way for developers to integrate security into application code.\n\n\n"}
{"id": "624666", "url": "https://en.wikipedia.org/wiki?curid=624666", "title": "List of partition topics", "text": "List of partition topics\n\nGenerally, a partition is a division of a whole into non-overlapping parts. Among the kinds of partitions considered in mathematics are\n\n\n\n"}
{"id": "18263758", "url": "https://en.wikipedia.org/wiki?curid=18263758", "title": "Locally normal space", "text": "Locally normal space\n\nIn mathematics, particularly topology, a topological space \"X\" is locally normal if intuitively it looks locally like a normal space. More precisely, a locally normal space satisfies the property that each point of the space belongs to a neighbourhood of the space that is normal under the subspace topology. \n\nA topological space \"X\" is said to be locally normal if and only if each point, \"x\", of \"X\" has a neighbourhood that is normal under the subspace topology.\n\nNote that not every neighbourhood of \"x\" has to be normal, but at least one neighbourhood of \"x\" has to be normal (under the subspace topology).\n\nNote however, that if a space were called locally normal if and only if each point of the space belonged to a subset of the space that was normal under the subspace topology, then every topological space would be locally normal. This is because, the singleton {\"x\"} is vacuously normal and contains \"x\". Therefore, the definition is more restrictive.\n\n\nTheorem 1\n\nIf \"X\" is homeomorphic to \"Y\" and \"X\" is locally normal, then so is \"Y\".\n\nProof\n\nThis follows from the fact that the image of a normal space under a homeomorphism is always normal.\n\n"}
{"id": "43111866", "url": "https://en.wikipedia.org/wiki?curid=43111866", "title": "Luigi Amerio", "text": "Luigi Amerio\n\nLuigi Amerio (15 August 1912 – 28 September 2004), was an Italian electrical engineer and mathematician. He is known for his work on almost periodic functions, on Laplace transforms in one and several dimensions, and on the theory of elliptic partial differential equations.\n\nA selection of Luigi Amerio's scientific papers is published in the two volumes of his \"\"Selecta\" : he is also the author of several university textbooks and, jointly with his pupil Giovanni Prouse, he wrote the influential monograph on almost periodic functions .\n\n\n\n\n\n\n"}
{"id": "5106151", "url": "https://en.wikipedia.org/wiki?curid=5106151", "title": "Metatheorem", "text": "Metatheorem\n\nIn logic, a metatheorem is a statement about a formal system proven in a metalanguage. Unlike theorems proved within a given formal system, a metatheorem is proved within a metatheory, and may reference concepts that are present in the metatheory but not the object theory.\n\nA formal system is determined by a formal language and a deductive system (axioms and rules of inference). The formal system can be used to prove particular sentences of the formal language with that system. Metatheorems, however, are proved externally to the system in question, in its metatheory. Common metatheories used in logic are set theory (especially in model theory) and primitive recursive arithmetic (especially in proof theory). Rather than demonstrating particular sentences to be provable, metatheorems may show that each of a broad class of sentences can be proved, or show that certain sentences cannot be proved.\n\nExamples of metatheorems include:\n\n\n\n"}
{"id": "22833956", "url": "https://en.wikipedia.org/wiki?curid=22833956", "title": "Molecular models of DNA", "text": "Molecular models of DNA\n\nMolecular models of DNA structures are representations of the molecular geometry and topology of deoxyribonucleic acid (DNA) molecules using one of several means, with the aim of simplifying and presenting the essential, physical and chemical, properties of DNA molecular structures either \"in vivo\" or \"in vitro\". These representations include closely packed spheres (CPK models) made of plastic, metal wires for \"skeletal models\", graphic computations and animations by computers, artistic rendering. Computer molecular models also allow animations and molecular dynamics simulations that are very important for understanding how DNA functions \"in vivo\".\n\nThe more advanced, computer-based molecular models of DNA involve molecular dynamics simulations and quantum mechanics computations of vibro-rotations, delocalized molecular orbitals (MOs), electric dipole moments, hydrogen-bonding, and so on. \"DNA molecular dynamics modeling\" involves simulating deoxyribonucleic acid (DNA) molecular geometry and topology changes with time as a result of both intra- and inter- molecular interactions of DNA. Whereas molecular models of DNA molecules such as closely packed spheres (CPK models) made of plastic or metal wires for \"skeletal models\" are useful representations of static DNA structures, their usefulness is very limited for representing complex DNA dynamics. Computer molecular modeling allows both animations and molecular dynamics simulations that are very important to understand how DNA functions \"in vivo\".\n\nFrom the very early stages of structural studies of DNA by X-ray diffraction and biochemical means, molecular models such as the Watson-Crick nucleic acid double helix model were successfully employed to solve the 'puzzle' of DNA structure, and also find how the latter relates to its key functions in living cells. The first high quality X-ray diffraction patterns\nof A-DNA were reported by Rosalind Franklin and Raymond Gosling in 1953. Rosalind Franklin made the critical observation that DNA exists in two distinct forms, A and B, and produced the sharpest pictures of both through X-ray diffraction technique. The first calculations of the Fourier transform of an atomic helix were reported one year earlier by Cochran, Crick and Vand, and were followed in 1953 by the computation of the Fourier transform of a coiled-coil by Crick.\n\nStructural information is generated from X-ray diffraction studies of oriented DNA fibers with the help of molecular models of DNA that are combined with crystallographic and mathematical analysis of the X-ray patterns.\n\nThe first reports of a double helix molecular model of B-DNA structure were made by James Watson and Francis Crick in 1953. That same year, Maurice F. Wilkins,\nA. Stokes and H.R. Wilson, reported the first X-ray patterns\nof \"in vivo\" B-DNA in partially oriented salmon sperm heads.\n\nThe development of the first correct double helix molecular model of DNA by Crick and Watson may not have been possible without the biochemical evidence for the nucleotide base-pairing ([A---T]; [C---G]), or Chargaff's rules. Although such initial studies of DNA structures with the help of molecular models were essentially static, their consequences for explaining the \"in vivo\" functions of DNA were significant in the areas of protein biosynthesis and the quasi-universality of the genetic code. Epigenetic transformation studies of DNA \"in vivo\" were however much slower to develop despite their importance for embryology, morphogenesis and cancer research. Such chemical dynamics and biochemical reactions of DNA are much more complex than the molecular dynamics of DNA physical interactions with water, ions and proteins/enzymes in living cells.\n\nAn old standing dynamic problem is how DNA \"self-replication\" takes place in living cells that should involve transient uncoiling of supercoiled DNA fibers. Although DNA consists of relatively rigid, very large elongated biopolymer molecules called \"fibers\" or chains (that are made of repeating nucleotide units of four basic types, attached to deoxyribose and phosphate groups), its molecular structure \"in vivo\" undergoes dynamic configuration changes that involve dynamically attached water molecules and ions. Supercoiling, packing with histones in chromosome structures, and other such supramolecular aspects also involve \"in vivo\" DNA topology which is even more complex than DNA molecular geometry, thus turning molecular modeling of DNA into an especially challenging problem for both molecular biologists and biotechnologists. Like other large molecules and biopolymers, DNA often exists in multiple stable geometries (that is, it exhibits conformational isomerism) and configurational, quantum states which are close to each other in energy on the potential energy surface of the DNA molecule.\n\nSuch varying molecular geometries can also be computed, at least in principle, by employing \"ab initio\" quantum chemistry methods that can attain high accuracy for small molecules, although claims that acceptable accuracy can be also achieved for polynuclelotides, and DNA conformations, were recently made on the basis of vibrational circular dichroism (VCD) spectral data. Such quantum geometries define an important class of \"ab initio\" molecular models of DNA which exploration has barely started, especially related to results obtained by VCD in solutions. More detailed comparisons with such \"ab initio\" quantum computations are in principle obtainable through 2D-FT NMR spectroscopy and relaxation studies of polynucleotide solutions or specifically labeled DNA, as for example with deuterium labels.\n\nIn an interesting twist of roles, the DNA molecule was proposed to be used for quantum computing via DNA. Both DNA nanostructures and DNA computing biochips have been built.\n\nThe chemical structure of DNA is insufficient to understand the complexity of the 3D structures of DNA. In contrast, animated molecular models allow one to visually explore the three-dimensional (3D) structure of DNA. The DNA model shown (far right) is a space-filling, or CPK, model of the DNA double helix. Animated molecular models, such as the wire, or skeletal, type shown at the top of this article, allow one to visually explore the three-dimensional (3D) structure of DNA. Another type of DNA model is the space-filling, or CPK, model.\n\nThe hydrogen bonding dynamics and proton exchange is very different by many orders of magnitude between the two systems of fully hydrated DNA and water molecules in ice. Thus, the DNA dynamics is complex, involving nanosecond and several tens of picosecond time scales, whereas that of liquid ice is on the picosecond time scale, and that of proton exchange in ice is on the millisecond time scale. The proton exchange rates in DNA and attached proteins may vary from picosecond to nanosecond, minutes or years, depending on the exact locations of the exchanged protons in the large biopolymers.\n\nA simple harmonic oscillator 'vibration' is only an oversimplified dynamic representation of the longitudinal vibrations of the DNA intertwined helices which were found to be anharmonic rather than harmonic as often assumed in quantum dynamic simulations of DNA.\n\nThe structure of DNA shows a variety of forms, both double-stranded and single-stranded. The mechanical properties of DNA, which are directly related to its structure, are a significant problem for cells. Every process which binds or reads DNA is able to use or modify the mechanical properties of DNA for purposes of recognition, packaging and modification. The extreme length (a chromosome may contain a 10 cm long DNA strand), relative rigidity and helical structure of DNA has led to the evolution of histones and of enzymes such as topoisomerases and helicases to manage a cell's DNA. The properties of DNA are closely related to its molecular structure and sequence, particularly the weakness of the hydrogen bonds and electronic interactions that hold strands of DNA together compared to the strength of the bonds within each strand.\n\nExperimental methods which can directly measure the mechanical properties of DNA are relatively new, and high-resolution visualization in solution is often difficult. Nevertheless, scientists have uncovered large amount of data on the mechanical properties of this polymer, and the implications of DNA's mechanical properties on cellular processes is a topic of active current research.\n\nThe DNA found in many cells can be macroscopic in length: a few centimetres long for each human chromosome. Consequently, cells must compact or \"package\" DNA to carry it within them. In eukaryotes this is carried by spool-like proteins named histones, around which DNA winds. It is the further compaction of this DNA-protein complex which produces the well known mitotic eukaryotic chromosomes.\n\nIn the late 1970s, alternate non-helical models of DNA structure were briefly considered as a potential solution to problems in DNA replication in plasmids and chromatin. However, the models were set aside in favor of the double-helical model due to subsequent experimental advances such as X-ray crystallography of DNA duplexes, and later the nucleosome core particle, and the discovery of topoisomerases. Such non-double-helical models are not currently accepted by the mainstream scientific community.\n\nAfter DNA has been separated and purified by standard biochemical methods, one has a sample in a jar much like in the figure at the top of this article. Below are the main steps involved in generating structural information from X-ray diffraction studies of oriented DNA fibers that are drawn from the hydrated DNA sample with the help of molecular models of DNA that are combined with crystallographic and mathematical analysis of the X-ray patterns.\n\nA paracrystalline lattice, or paracrystal, is a molecular or atomic lattice with significant amounts (e.g., larger than a few percent) of partial disordering of molecular arrangements. Limiting cases of the paracrystal model are nanostructures, such as glasses, liquids, etc., that may possess only local ordering and no global order. A simple example of a paracrystalline lattice is shown in the following figure for a silica glass:\n\nLiquid crystals also have paracrystalline rather than crystalline structures.\n\nHighly hydrated B-DNA occurs naturally in living cells in such a paracrystalline state, which is a dynamic one despite the relatively rigid DNA double helix stabilized by parallel hydrogen bonds between the nucleotide base-pairs in the two complementary, helical DNA chains (see figures). For simplicity most DNA molecular models omit both water and ions dynamically bound to B-DNA, and are thus less useful for understanding the dynamic behaviors of B-DNA \"in vivo\". The physical and mathematical analysis of X-ray and spectroscopic data for paracrystalline B-DNA is thus far more complex than that of crystalline, A-DNA X-ray diffraction patterns. The paracrystal model is also important for DNA technological applications such as DNA nanotechnology. Novel methods that combine X-ray diffraction of DNA with X-ray microscopy in hydrated living cells are now also being developed.\n\nThere are various uses of DNA molecular modeling in Genomics and Biotechnology research applications, from DNA repair to PCR and DNA nanostructures. Two-dimensional DNA junction arrays have been visualized by Atomic force microscopy.\n\nDNA molecular modeling has various uses in genomics and biotechnology, with research applications ranging from DNA repair to PCR and DNA nanostructures. These include computer molecular models of molecules as varied as RNA polymerase, an E. coli, bacterial DNA primase template suggesting very complex dynamics at the interfaces between the enzymes and the DNA template, and molecular models of the mutagenic, chemical interaction of potent carcinogen molecules with DNA. These are all represented in the gallery below.\n\nTechnological application include a DNA biochip and DNA nanostructures designed for DNA computing and other dynamic applications of DNA nanotechnology.\nThe image at right is of self-assembled DNA nanostructures. The DNA \"tile\" structure in this image consists of four branched junctions oriented at 90° angles. Each tile consists of nine DNA oligonucleotides as shown; such tiles serve as the primary \"building block\" for the assembly of the DNA nanogrids shown in the AFM micrograph.\n\nQuadruplex DNA may be involved in certain cancers. Images of quadruplex DNA are in the gallery below.\n\n\n\n\n\n\n\n\n\n"}
{"id": "2623887", "url": "https://en.wikipedia.org/wiki?curid=2623887", "title": "Nomological network", "text": "Nomological network\n\nNomological network is a representation of the concepts (constructs) of interest in a study, their observable manifestations, and the interrelationships among and between these. The term \"Nomology\" has been derived from the Greek, meaning \"lawful\", or in philosophy of science terms, \"lawlike\". It was Cronbach and Meehl's view of construct validity that in order to provide evidence that a measure has construct validity, a nomological network has to be developed for its measure. \n\nThe elements of a nomological network are:\n\nValidity evidence based on nomological validity is a form of construct validity. It is the degree to which a construct behaves as it should within a system of related constructs (the nomological network).\n\n"}
{"id": "1171759", "url": "https://en.wikipedia.org/wiki?curid=1171759", "title": "Pasch's theorem", "text": "Pasch's theorem\n\nIn geometry, Pasch's theorem, stated in 1882 by the German mathematician Moritz Pasch, is a result in plane geometry which cannot be derived from Euclid's postulates.\n\nThe statement is as follows: [Here, for example, (, , ) means that point lies between points and .]\n\n"}
{"id": "31593159", "url": "https://en.wikipedia.org/wiki?curid=31593159", "title": "Pierre Dolbeault", "text": "Pierre Dolbeault\n\nPierre Dolbeault (October 10, 1924 – June 12, 2015) was a French mathematician.\n\nDolbeault studied with Henri Cartan and graduated in 1944 from the École Normale Supérieure. He completed his Ph.D. at the University of Paris in 1955 under the supervision of Cartan, with a dissertation titled \"Formes différentielles et cohomologie sur une variété analytique complexe\".\n\nHe taught in the 1950s at the University of Montpellier and the University of Bordeaux, and later at the Université Pierre et Marie Curie (Jussieu). Together with Pierre Lelong and Henri Skoda he held an Analysis seminar in Paris.\n\nDolbeault cohomology is named after him, and so is the Dolbeault theorem.\n\n"}
{"id": "45307225", "url": "https://en.wikipedia.org/wiki?curid=45307225", "title": "Plethystic substitution", "text": "Plethystic substitution\n\nPlethystic substitution is a shorthand notation for a common kind of substitution in the algebra of symmetric functions and that of symmetric polynomials. It is essentially basic substitution of variables, but allows for a change in the number of variables used.\n\nThe formal definition of plethystic substitution relies on the fact that the ring of symmetric functions formula_1 is generated as an \"R\"-algebra by the power sum symmetric functions\n\nformula_2\n\nFor any symmetric function formula_3 and any formal sum of monomials formula_4, the \"plethystic substitution\" f[A] is the formal series obtained by making the substitutions\n\nformula_5\n\nin the decomposition of formula_3 as a polynomial in the \"p\"'s.\n\nIf formula_7 denotes the formal sum formula_8, then formula_9.\n\nOne can write formula_10 to denote the formal sum formula_11, and so the plethystic substitution formula_12 is simply the result of setting formula_13 for each i. That is,\n\nformula_14.\n\nPlethystic substitution can also be used to change the number of variables: if formula_15, then formula_16 is the corresponding symmetric function in the ring formula_17 of symmetric functions in \"n\" variables.\n\nSeveral other common substitutions are listed below. In all of the following examples, formula_8 and formula_19 are formal sums.\n\nformula_22\nformula_25, \nwhere formula_26 is the well-known involution on symmetric functions that sends a Schur function formula_27 to the conjugate Schur function formula_28.\n\n\n"}
{"id": "33909868", "url": "https://en.wikipedia.org/wiki?curid=33909868", "title": "Pomeau–Manneville scenario", "text": "Pomeau–Manneville scenario\n\nIn the theory of dynamical systems (or turbulent flow), the Pomeau–Manneville scenario is the transition to chaos (turbulence) due to intermittency.\n"}
{"id": "10155033", "url": "https://en.wikipedia.org/wiki?curid=10155033", "title": "R. C. T. Lee", "text": "R. C. T. Lee\n\nR. C. T. Lee (Lee Chia-Tung ; born 1939 in Shanghai, China), also known as Richard C. T. Lee, received his B.Sc. degree from the Department of Electrical Engineering of National Taiwan University and Ph.D. degree from the Department of Electrical Engineering and Computer Science from University of California, Berkeley.\n\nHe worked for NCR from 1963 to 1964 after he got his M.S. degree. After getting his Ph.D. degree, he joined National Institutes of Health, Bethesda, Maryland in 1967 and later worked in Naval Research Laboratory, Washington, D.C. in 1974.\n\nHe returned to Taiwan in 1975 and started his teaching career in National Tsing Hua University, Hsinchu, Taiwan. In this university, he had been the chairperson of Department of Computer Science and Department of Electrical Engineering. In 1984, after he became the Dean of College of Engineering and in 1988, he was appointed as the Provost. In 1994, he was the Acting President of National Tsing Hua University. From 1994 to 1999, he was the President of Providence University in Shalu, Taiwan and in 1999, he was the President of National Chi Nan University, Puli, Taiwan. He is now a professor of Chi Nan University under the joint appointment of four departments: the Department of Computer Science, the Department of Information Management, the Department of Communication and the Department of Medical Science.\n\nProfessor Lee has published roughly 80 papers, all in prestigious academic journals. He has been editors for ten journals. In 1989, he became an IEEE fellow. He received the Distinguished Research Awards from the National Science Council, Republic of China, five times and the Ministry of Education Engineering Academic Achievement Award in 1989. He is a Micronix Chair Professor. Professor Lee and R.C.Chang coauthored the book “Symbolic Logic and Mechanical Theorem Proving” which was published by Academic Press in 1973. This book was translated into Japanese, Russian and Italian. In 2005, McGraw-Hill published his “Introduction to the Design and Analysis of Algorithms, a Strategic Approach”, which he coauthored with S.S. Tseng, R.C. Chang and Y.T.Tsai. In addition to publishing technical papers, Professor Lee has also been an author of short stories. His four books, “Let the Wall Come Down”, “The Stranger”, “The Curtain Never Falls” and “The Bell Rings Again” have been all popular in Taiwan. “Let the Wall Come Down” has been sold more than 300,000 copies within a short period of seven years. He also published a book advising young people to pay attention to basics, entitled “Let Us Go Back to Basics”.\nLee is a professor of both the Computer Science and Information Engineering Department and Information Management Department of National Chi Nan University.\n\nProfessor Lee is an IEEE fellow. He co-authored \"Symbolic Logic and Mechanical Theorem Proving\", which has been translated into Japanese, Russian and Italian.\n\nEmployment:\n\n1. Engineer, National Cash Register Company, U.S.A., 1962–1963.\n\n2. Senior Research Fellow, Occupational Safety and Health Administration, U.S. Department of Labor, October 1967 – June, 1974.\n\n3. Research Fellow, Research Institute of U.S.A. Navy, Aug. 1974 – August 1975.\n\n4. Chairman, Institute of Applied Mathematics, National Tsing Hua University, August 1975 – July 1977.\n\n5. Chairman, Institute of Computer Management, National Tsing Hua University, August 1977 – July 1983.\n\n6. Chairman, Department of Electronic Engineering, National Tsing Hua University, August 1983 – July 1984.\n\n7. Dean, College of Engineering, National Tsing Hua University, August 1984 – July 1988.\n\n8. Provost and Acting President, National Tsing Hua University, August 1988 – January 1994.\n\n9. President, Providence University, February 1994 – June 1999.\n\n10. President, National Chi Nan University, July 1999 – November 1999.\n\n11. Professor, Department of Computer Science and Information Engineering, National Chi Nan University, December 1999 – Present.\n\nAcademic Awards:\n\n1. 1989: IEEE Fellow.\n\n2. Distinguished Research Awards of the National Science Council of the R.O.C., 1989-1993.\n\n3. Academic Engineering Research Award of Ministry of Education of the R.O.C., 1989.\n\n4. Outstanding Research Award of the Hou Jin-Dui Foundation, 1993.\n\n5. 1995 TECO Technology Award of the TECO Technology Foundation, 1995.\n\n6. Distinguished Research Fellow of National Science Council of the R.O.C., Since 1986.\n\n7. Medal of Honor of the Institute of Information and Computing Machinery, 2001.\n\n8. Chair Professor of Macronix International Co. Ltd, Since 2001.\n\n9. Heritage Prize of The Phi Tau Phi Scholastic Honor Society of the R. O. C., 2004.\n\n10. Honorary Doctorate Degree, ed Ph. D. , Christian Chung Yuan University, Taiwan, 2005.\n\n11. Second Class Economics Medal of Honor from the Ministry of Economics Affairs, 2005.\n\n\n"}
{"id": "27419729", "url": "https://en.wikipedia.org/wiki?curid=27419729", "title": "Rational difference equation", "text": "Rational difference equation\n\nA rational difference equation is a nonlinear difference equation of the form\nwhere the initial conditions formula_2 are such that the denominator never vanishes for any .\n\nA first-order rational difference equation is a nonlinear difference equation of the form\n\nWhen formula_4 and the initial condition formula_5 are real numbers, this difference equation is called a Riccati difference equation.\n\nSuch an equation can be solved by writing formula_6 as a nonlinear transformation of another variable formula_7 which itself evolves linearly. Then standard methods can be used to solve the linear difference equation in formula_7.\n\nOne approach to developing the transformed variable formula_7, when formula_10, is to write\nwhere formula_12 and formula_13 and where formula_14.\n\nFurther writing formula_15 can be shown to yield\n\nThis approach gives a first-order difference equation for formula_7 instead of a second-order one, for the case in which formula_18 is non-negative. Write formula_19 implying formula_20, where formula_21 is given by formula_22 and where formula_23. Then it can be shown that formula_7 evolves according to\n\nThe equation\n\ncan also be solved by treating it as a special case of the \n\nwhere all of \"A, B, C, E,\" and \"X\" are \"n\"×\"n\" matrices (in this case \"n\"=1); the solution of this is\n\nwhere\n\nIt was shown in that a dynamic matrix Riccati equation of the form\n\nwhich can arise in some discrete-time optimal control problems, can be solved using the second approach above if the matrix \"C\" has only one more row than column.\n\n"}
{"id": "2111994", "url": "https://en.wikipedia.org/wiki?curid=2111994", "title": "Rayleigh–Ritz method", "text": "Rayleigh–Ritz method\n\nThe Rayleigh–Ritz method is a method of finding approximations to eigenvalue equations that cannot be solved easily (or at all) analytically. The name is a common misnomer used to describe the method that is more appropriately termed the Ritz method or the Galerkin method. This method was invented by Walter Ritz in 1909, but it bears some similarity to the Rayleigh quotient and so the misnomer persists.\n\nThe Rayleigh–Ritz method allows for the computation of Ritz pairs formula_1 which approximate the solutions to the eigenvalue problem\n\nwhere formula_3.\n\nThe procedure is as follows:\n\nOne can always compute the accuracy of such an approximation via formula_8\n\nIf a Krylov subspace is used and A is a general matrix, then this is the Arnoldi algorithm.\n\nIn this technique, we approximate the variational problem and end up with a finite dimensional problem. So let us start with the problem of seeking a function formula_9 that extremizes an integral formula_10. Assume that we are able to approximate y(x) by a linear combination of certain linearly independent functions of the type:\n\nformula_11\n\nwhere formula_12 are constants to be determined by a variational method, such as one which will be described below.\n\nThe selection of which approximating functions formula_13 to use is arbitrary except for the following considerations:\n\na) If the problem has boundary conditions such as fixed end points, then formula_14 is chosen to satisfy the problem’s boundary conditions, and all other formula_13 vanish at the boundary.\n\nb) If the form of the solution is known, then formula_13 can be chosen so that formula_9 will have that form.\n\nThe expansion of formula_9 in terms of approximating functions replaces the variational problem of extremising the functional integral formula_10 to a problem of finding a set of constants formula_12 that extremizes formula_21. We can now solve this by setting the partial derivatives to zero. For each value of i,\n\nformula_22\n\nThe procedure is to first determine an initial estimate of formula_23 by the approximation formula_24. Next, the approximation formula_25 is used (with formula_23 being redetermined). The process continues with formula_27 as the third approximation and so on. At each stage the following two items are true:\n\nConvergence of the procedure means that as i tends to infinity, the approximation will tend towards the exact function formula_9 that extremizes an integral formula_10.\n\nIn many cases one uses a complete set of functions e. g. polynomials or sines and cosines. A set of functions formula_13 is called complete over [a, b] if for each Riemann integrable function formula_35, there is a set of values of coefficients formula_12 that reproduces formula_35.\n\nThe above outlined procedure can be extended to cases with more than one independent variable.\n\nThe Rayleigh–Ritz method is often used in mechanical engineering for finding the approximate real resonant frequencies of multi degree of freedom systems, such as spring mass systems or flywheels on a shaft with varying cross section. It is an extension of Rayleigh's method. It can also be used for finding buckling loads and post-buckling behaviour for columns.\n\nConsider the case whereby we want to find the resonant frequency of oscillation of a system. First, write the oscillation in the form,\n\nformula_38\n\nwith an unknown mode shape formula_39. Next, find the total energy of the system, consisting of a kinetic energy term and a potential energy term. The kinetic energy term involves the square of the time derivative of formula_40 and thus gains a factor of formula_41. Thus, we can calculate the total energy of the system and express it in the following form:\n\nformula_42\n\nBy conservation of energy, the average kinetic energy must be equal to the average potential energy. Thus,\n\nformula_43\n\nwhich is also known as the Rayleigh quotient. Thus, if we knew the mode shape formula_39, we would be able to calculate formula_45 and formula_46, and in turn get the eigenfrequency. However, we do not yet know the mode shape. In order to find this, we can approximate formula_39 as a combination of a few approximating functions formula_48\n\nformula_49\n\nwhere formula_12 are constants to be determined. In general, if we choose a random set of formula_12, it will describe a superposition of the actual eigenmodes of the system. However, if we seek formula_12 such that the eigenfrequency formula_53 is minimised, then the mode described by this set of formula_12 will be close to the lowest possible actual eigenmode of the system. Thus, this finds the lowest eigenfrequency. If we find eigenmodes orthogonal to this approximated lowest eigenmode, we can approximately find the next few eigenfrequencies as well.\n\nIn general, we can express formula_45 and formula_46 as a collection of terms quadratic in the coefficients formula_57:\n\nformula_58\n\nformula_59\n\nThe minimization of formula_53 becomes:\n\nformula_61\n\nSolving this,\n\nformula_62\n\nformula_63\n\nformula_64\n\nFor a non-trivial solution of c, we require determinant of the matrix coefficient of c to be zero.\n\nformula_65\n\nThis gives a solution for the first N eigenfrequencies and eigenmodes of the system, with N being the number of approximating functions.\n\nThe following discussion uses the simplest case, where the system has two lumped springs and two lumped masses, and only two mode shapes are assumed. Hence \"M\" = [\"m\", \"m\"] and \"K\" = [\"k\", \"k\"].\n\nA mode shape is assumed for the system, with two terms, one of which is weighted by a factor \"B\", e.g. \"Y\" = [1, 1] + \"B\"[1, −1].\nSimple harmonic motion theory says that the velocity at the time when deflection is zero, is the angular frequency formula_66 times the deflection (y) at time of maximum deflection. In this example the kinetic energy (KE) for each mass is formula_67 etc., and the potential energy (PE) for each spring is formula_68 etc.\n\nWe also know that without damping, the maximal KE equals the maximal PE. Thus,\n\nNote that the overall amplitude of the mode shape cancels out from each side, always. That is, the actual size of the assumed deflection does not matter, just the mode \"shape\".\n\nMathematical manipulations then obtain an expression for formula_66, in terms of B, which can be differentiated with respect to B, to find the minimum, i.e. when formula_71. This gives the value of B for which formula_66 is lowest. This is an upper bound solution for formula_66 if formula_66 is hoped to be the predicted fundamental frequency of the system because the mode shape is \"assumed\", but we have found the lowest value of that upper bound, given our assumptions, because B is used to find the optimal 'mix' of the two assumed mode shape functions.\n\nThere are many tricks with this method, the most important is to try and choose realistic assumed mode shapes. For example, in the case of beam deflection problems it is wise to use a deformed shape that is analytically similar to the expected solution. A quartic may fit most of the easy problems of simply linked beams even if the order of the deformed solution may be lower. The springs and masses do not have to be discrete, they can be continuous (or a mixture), and this method can be easily used in a spreadsheet to find the natural frequencies of quite complex distributed systems, if you can describe the distributed KE and PE terms easily, or else break the continuous elements up into discrete parts.\n\nThis method could be used iteratively, adding additional mode shapes to the previous best solution, or you can build up a long expression with many Bs and many mode shapes, and then differentiate them partially.\n\n\n"}
{"id": "1736264", "url": "https://en.wikipedia.org/wiki?curid=1736264", "title": "Second moment of area", "text": "Second moment of area\n\nThe 2nd moment of area, also known as moment of inertia of plane area, area moment of inertia, or second area moment, is a geometrical property of an area which reflects how its points are distributed with regard to an arbitrary axis. The second moment of area is typically denoted with either an formula_1 for an axis that lies in the plane or with a formula_2 for an axis perpendicular to the plane. In both cases, it is calculated with a multiple integral over the object in question. Its dimension is L (length) to the fourth power. Its unit of dimension when working with the International System of Units is meters to the fourth power, m, or inches to the fourth power, in, when working in the Imperial System of Units.\n\nIn structural engineering, the second moment of area of a beam is an important property used in the calculation of the beam's deflection and the calculation of stress caused by a moment applied to the beam. The planar second moment of area provides insight into a beam's resistance to bending due to an applied moment, force, or distributed load perpendicular to its neutral axis, as a function of its shape. The polar second moment of area provides insight into a beam's resistance to torsional deflection, due to an applied moment parallel to its cross-section, as a function of its shape. \n\nThe second moment of area for an arbitrary shape  with respect to an arbitrary axis formula_6 is defined as\n\nwhere\n\nFor example, when the desired reference axis is the x-axis, the second moment of area formula_12 (often denoted as formula_13) can be computed in Cartesian coordinates as\n\nThe second moment of the area is crucial in Euler–Bernoulli theory of slender beams.\n\nMore generally, the product moment of area is defined as\n\nIt is sometimes necessary to calculate the second moment of area of a shape with respect to an formula_16 axis different to the centroidal axis of the shape. However, it is often easier to derive the second moment of area with respect to its centroidal axis, formula_17, and use the parallel axis theorem to derive the second moment of area with respect to the formula_16 axis. The parallel axis theorem states\n\nwhere\n\nA similar statement can be made about a formula_24 axis and the parallel centroidal formula_25 axis. Or, in general, any centroidal formula_26 axis and a parallel formula_27 axis.\n\nFor the simplicity of calculation, it is often desired to define the polar moment of area (with respect to a perpendicular axis) in terms of two area moments of inertia (both with respect to in-plane axes). The simplest case relates formula_28 to formula_13 and formula_30.\n\nThis relationship relies on the Pythagorean theorem which relates formula_17 and formula_25 to formula_9 and on the linearity of integration.\n\nFor more complex areas, it is often easier to divide the area into a series of \"simpler\" shapes. The second moment of area for the entire shape is the sum of the second moment of areas of all of its parts about a common axis. This can include shapes that are \"missing\" (i.e. holes, hollow shapes, etc.), in which case the second moment of area of the \"missing\" areas are subtracted, rather than added. In other words, the second moment of area of \"missing\" parts are considered negative for the method of composite shapes.\n\nSee list of second moments of area for other shapes.\n\nConsider a rectangle with base formula_35 and height formula_36 whose centroid is located at the origin. formula_13 represents the second moment of area with respect to the x-axis; formula_30 represents the second moment of area with respect to the y-axis; formula_28 represents the polar moment of inertia with respect to the z-axis.\n\nUsing the perpendicular axis theorem we get the value of formula_28.\n\nConsider an annulus whose center is at the origin, outside radius is formula_43, and inside radius is formula_44. Because of the symmetry of the annulus, the centroid also lies at the origin. We can determine the polar moment of inertia, formula_28, about the formula_46 axis by the method of composite shapes. This polar moment of inertia is equivalent to the polar moment of inertia of a circle with radius formula_43 minus the polar moment of inertia of a circle with radius formula_44, both centered at the origin. First, let us derive the polar moment of inertia of a circle with radius formula_49 with respect to the origin. In this case, it is easier to directly calculate formula_28 as we already have formula_51, which has both an formula_17 and formula_25 component. Instead of obtaining the second moment of area from Cartesian coordinates as done in the previous section, we shall calculate formula_13 and formula_28 directly using polar coordinates.\n\nNow, the polar moment of inertia about the formula_46 axis for an annulus is simply, as stated above, the difference of the second moments of area of a circle with radius formula_43 and a circle with radius formula_44.\n\nAlternatively, we could change the limits on the formula_61 integral the first time around to reflect the fact that there is a hole. This would be done like this.\n\nThe second moment of area for any simple polygon on the XY-plane can be computed in general by summing contributions from each segment of the polygon after essentially chopping the area up into a set of triangles.\nA polygon is assumed to have formula_63 vertices, numbered in counter-clockwise fashion. If polygon vertices are numbered clockwise, returned values will be negative, but absolute values will be correct.\n\nAttention, the following formulae must be trusted\n\nwhere formula_65 are the coordinates of the formula_66-th polygon vertex, for formula_67. Also, formula_68 are assumed to be equal to the coordinates of the first vertex, i.e., formula_69 and formula_70.\n\n"}
{"id": "1908302", "url": "https://en.wikipedia.org/wiki?curid=1908302", "title": "Slewing", "text": "Slewing\n\nSlewing is the rotation of an object around an axis, usually the z axis. An example is a radar scanning 360 degrees by slewing around the z axis. This is also common terminology in astronomy. The process of rotating a telescope to observe a different region of the sky is referred to as slewing.\n\nThe term slewing is also found in motion control applications. Often the slew axis is combined with other axis to form a motion profile.\n\nIn crane terminology, slewing is the angular movement of a crane boom or crane jib in a horizontal plane.\n\nThe term is also used in the computer game Microsoft Flight Simulator wherein the user presses a key and he or she can rotate and move the virtual aircraft along all three spatial planes.\n\nIn the modern day use of CNC programs, \"slewing\" is a vital part of the process.\n"}
{"id": "943637", "url": "https://en.wikipedia.org/wiki?curid=943637", "title": "Sub-Riemannian manifold", "text": "Sub-Riemannian manifold\n\nIn mathematics, a sub-Riemannian manifold is a certain type of generalization of a Riemannian manifold. Roughly speaking, to measure distances in a sub-Riemannian manifold, you are allowed to go only along curves tangent to so-called \"horizontal subspaces\".\nSub-Riemannian manifolds (and so, \"a fortiori\", Riemannian manifolds) carry a natural intrinsic metric called the metric of Carnot–Carathéodory. The Hausdorff dimension of such metric spaces is always an integer and larger than its topological dimension (unless it is actually a Riemannian manifold).\n\nSub-Riemannian manifolds often occur in the study of constrained systems in classical mechanics, such as the motion of vehicles on a surface, the motion of robot arms, and the orbital dynamics of satellites. Geometric quantities such as the Berry phase may be understood in the language of sub-Riemannian geometry. The Heisenberg group, important to quantum mechanics, carries a natural sub-Riemannian structure.\n\nBy a \"distribution\" on formula_1 we mean a subbundle of the tangent bundle of formula_1.\nGiven a distribution formula_3 a vector field in formula_4 is called horizontal. A curve formula_5 on formula_1 is called horizontal if formula_7 for any \nformula_8.\n\nA distribution on formula_4 is called completely non-integrable if for any formula_10 we have that any tangent vector can be presented as a linear combination of vectors of the following types formula_11 where all vector fields formula_12 are horizontal.\n\nA sub-Riemannian manifold is a triple formula_13, where formula_1 is a differentiable manifold, formula_15 is a \"completely non-integrable\" \"horizontal\" distribution and formula_16 is a smooth section of positive-definite quadratic forms on formula_15.\n\nAny sub-Riemannian manifold carries the natural intrinsic metric, called the metric of Carnot–Carathéodory, defined as \nwhere infimum is taken along all \"horizontal curves\" formula_19 such that formula_20, formula_21.\n\nA position of a car on the plane is determined by three parameters: two coordinates formula_22 and formula_23 for the location and an angle formula_24 which describes the orientation of the car. Therefore, the position of the car can be described by a point in a manifold \n\nOne can ask, what is the minimal distance one should drive to get from one position to another? This defines a Carnot–Carathéodory metric on the manifold \n\nA closely related example of a sub-Riemannian metric can be constructed on a Heisenberg group: Take two elements formula_24 and formula_28 in the corresponding Lie algebra such that \n\nspans the entire algebra. The horizontal distribution formula_15 spanned by left shifts of formula_24 and formula_28 is \"completely non-integrable\". Then choosing any smooth positive quadratic form on formula_15 gives a sub-Riemannian metric on the group.\n\nFor every sub-Riemannian manifold, there exists a Hamiltonian, called the sub-Riemannian Hamiltonian, constructed out of the metric for the manifold. Conversely, every such quadratic Hamiltonian induces a sub-Riemannian manifold. The existence of geodesics of the corresponding Hamilton–Jacobi equations for the sub-Riemannian Hamiltonian is given by the Chow–Rashevskii theorem.\n\n\n"}
{"id": "4776914", "url": "https://en.wikipedia.org/wiki?curid=4776914", "title": "Szekeres snark", "text": "Szekeres snark\n\nIn the mathematical field of graph theory, the Szekeres snark is a snark with 50 vertices and 75 edges. It was the fifth known snark, discovered by George Szekeres in 1973.\n\nAs a snark, the Szekeres graph is a connected, bridgeless cubic graph with chromatic index equal to 4. The Szekeres snark is non-planar and non-hamiltonian but is hypohamiltonian. It has book thickness 3 and queue number 2.\n\nAnother well known snark on 50 vertices is the Watkins snark discovered by John J. Watkins in 1989.\n"}
{"id": "18267021", "url": "https://en.wikipedia.org/wiki?curid=18267021", "title": "The Annotated Turing", "text": "The Annotated Turing\n\nThe Annotated Turing: A Guided Tour Through Alan Turing’s Historic Paper on Computability and the Turing Machine is a book by Charles Petzold, published in 2008 by John Wiley & Sons, Inc. Petzold annotates Alan Turing's paper \"On Computable Numbers, with an Application to the Entscheidungsproblem\". The book takes readers sentence by sentence through Turing's paper, providing explanations, further examples, corrections, and biographical information.\n\n\n\n"}
{"id": "38382619", "url": "https://en.wikipedia.org/wiki?curid=38382619", "title": "Thrombodynamics test", "text": "Thrombodynamics test\n\nThrombodynamics is a method for blood coagulation monitoring and anticoagulant control. This test is based on imitation of coagulation processes occurring \"in vivo\", is sensitive both to pro- and anticoagulant changes in the hemostatic balance. Highly sensitive to thrombosis.\n\nThe method was developed in the Physical Biochemistry Laboratory under the direction of Prof. Fazly Ataullakhanov.\n\nThrombodynamics designed to investigate the \"in vitro\" spatial-temporal dynamics of blood coagulation initiated by localized coagulation activator under conditions similar to the conditions of the blood clotting\" in vivo\". Thrombodynamics takes into account the spatial heterogeneity trombodinamiki processes in blood coagulation. The test is performed without mixing in a thin layer of plasma.\n\nThe measurement cuvette with the blood plasma sample is placed inside the water thermostat. Clotting starts when activator with immobilized TF is immersed into the cuvette. The clot then propagates from the activating surface into the bulk of plasma. Image of growing clot is registered via the CCD camera using a time-lapse microscopy mode in scattered light and then parameters of coagulation are calculated on the computer. Thrombodynamics analyser T-2 device also supports measurement of spatial dynamics of thrombin propagation during the process of clot growth via usage of the fluorogenic substrate for thrombin. Blood plasma sample is periodically irradiated with the excitation light and the emission of the fluorophore is registered by CCD camera.\n\nMathematical methods are used to restore spatio-temporal distribution of the thrombin from the fluorophore signal. This experimental model worked well in research and has demonstrated good sensitivity to various disorders of the coagulation system.\n\n\n\n"}
{"id": "26945226", "url": "https://en.wikipedia.org/wiki?curid=26945226", "title": "Uncertainty coefficient", "text": "Uncertainty coefficient\n\nIn statistics, the uncertainty coefficient, also called proficiency, entropy coefficient or Theil's U, is a measure of nominal association. It was first introduced by Henri Theil and is based on the concept of information entropy.\n\nSuppose we have samples of two discrete random variables, \"X\" and \"Y\". By constructing the joint distribution, , from which we can calculate the conditional distributions, and , and calculating the various entropies, we can determine the degree of association between the two variables.\n\nThe entropy of a single distribution is given as:\n\nwhile the conditional entropy is given as:\n\nThe uncertainty coefficient or proficiency is defined as:\n\nand tells us: given \"Y\", what fraction of the bits of \"X\" can we predict? (The above expression makes clear that the uncertainty coefficient is a normalised mutual information \"I(X;Y)\".) In this case we can think of \"X\" as containing the \"true\" values. \nNote that the value of \"U\" (but not \"H\"!) is independent of the base of the \"log\" since all logarithms are proportional.\n\nThe uncertainty coefficient is useful for measuring the validity of a statistical classification algorithm and has the advantage over simpler accuracy measures such as precision and recall in that it is not affected by the relative fractions of the different classes, i.e., \"P\"(\"x\")\nIt also has the unique property that it won't penalize an algorithm for predicting the wrong classes, so long as it does so consistently (i.e., it simply rearranges the classes). This is useful in evaluating clustering algorithms since cluster labels typically have no particular ordering.\n\nThe uncertainty coefficient is not symmetric with respect to the roles of \"X\" and \"Y\". The roles can be reversed and a symmetrical measure thus defined as a weighted average between the two:\n\nAlthough normally applied to discrete variables, the uncertainty coefficient can be extended to continuous variables using density estimation.\n\n\n"}
{"id": "1258941", "url": "https://en.wikipedia.org/wiki?curid=1258941", "title": "Whiteprint", "text": "Whiteprint\n\nWhiteprint describes a document reproduction produced by using the diazo chemical process. It is also known as the blue-line process since the result is blue lines on a white background. It is a contact printing process which accurately reproduces the original in size, but cannot reproduce continuous tones or colors. The light-sensitivity of the chemicals used was known in the 1890s and several related printing processes were patented at that time. Whiteprinting replaced the blueprint process for reproducing architectural and engineering drawings because the process was simpler and involved fewer toxic chemicals. A blue-line print is not permanent and will fade if exposed to light for weeks or months, but a drawing print that lasts only a few months is sufficient for many purposes.\n\nThere are two components in this process:\n\nIn a variety of combinations and strengths, these two chemicals are mixed together in water and coated onto paper. The resulting coating is then dried yielding the specially treated paper commercially sold as Diazo paper. This solution can also be applied to polyester film or to vellum.\n\nThe process starts with original documents that have been created on a translucent medium. Such media include polyester films, vellums, linens, and translucent bond papers (bonds). Any media that allows some light to pass through typically works as a master; the desired durability of the master determines the choice. Depending on the thickness and type of the master, the intensity of the UV exposure light is adjusted using an intensity knob typically pre-marked for media types commonly used for masters in any particular shop. Similarly, the speed control (for setting the speed at which the sheets are pulled through the machine) is likewise typically pre-marked in any particular shop, having been optimized based on trial runs.\n\nThe original document is laid on top of the chemically-coated side of a sheet of diazo paper, which is retrieved from a light-protected flat file, and the two sheets are fed into the diazo duplicator, being pulled into the machine by rotating rubber friction wheels. There are two chambers inside the machine. The first is the exposure area, where the sandwich of the two sheets (the master and the diazo paper) pass in front of an ultraviolet lamp. Ultraviolet light penetrates the original and neutralizes the light sensitive diazonium salt wherever there was no image on the master. These areas become the white areas on the copy. Once this process is complete, the undeveloped image at the locations where the UV light could not penetrate can often be seen as very light yellow or white marks/lines on the diazo sheet. This completes the exposure phase.\n\nNext, the original is peeled from the diazo paper as the sandwich of master and diazo exits the machine, and the diazo sheet alone is fed into the developing chamber. Here, fumes of ammonium hydroxide create an extremely alkaline environment. Under these conditions, the azodyes (couplers) react with the remaining diazonium salt and undergo a chemical reaction that results in the unexposed lines changing color from invisible (or yellow) to a visible dark color. The range of colors for these lines is usually blue or black, but sepia (a brownish hue) is also quite popular. When making multiple copies of an original no more than four or five copies can typically be made at a time, due to the build-up of ammonia fumes, even with ventilation fans in the duplication room. A slight delay of perhaps five minutes is often required for the fumes to subside enough to permit making additional copies if no ventilation exists. Many blueprint shops ran ventilation ducts from the machines to outside. Smaller and mid-size blueprint machines were often outfitted with neutralizers which absorbed some of the ammonia for a period of time.\n\nIf the lines are too light, it is also possible to run the blue-line through the developing chamber once more, which often increases the contrast of the lines relative to the base media. Repeated lack of contrast and light prints is also a tip-off that the operator needs to adjust the speed or amount of ammonia. Sometimes both the master and the diazo print are inadvertently fed through the developing chamber together. If this occurs, one simply peels the master from the diazo paper and runs the diazo sheet through the developer once more to more fully develop the lines.\n\nDiazo printing was one of the most economic methods to reproduce large engineering and architectural drawings.\n\nA quirk of diazo blueline prints is that with continued exposure to ultraviolet light, either from natural sunlight or from typical office fluorescent lighting, a blueline copy can fade over a span of months (indoors) or just days (outdoors), becoming illegible. This fading process thus requires reduplication of the original documents every few months in a typical office for any project using bluelines. Hence, blueline drawings that are used as engineering working copy prints have to be protected when not in use by storing them in flat files in the dark. Incandescent lighting was often used in areas where blueline engineering prints needed to be posted on a wall for long periods to hinder rapid fading.\n\nImproperly exposed bluelines are more likely to fade at an increased rate since the chemical reaction in the ammonia phase continues until the process is completed. But also properly exposed bluelines should not be exposed to the elements, and bluelines kept in flat files or hanging on racks in a cool, dry room often retain the majority of their lines and are able to be subsequently scanned into a digital format for various purposes.\n\nDuring the 20th century, elaborate color-coding schemes were somewhat standardized in each engineering shop for indicating changes to blueline drawings. For example, revisions could be shown by using contrasting color on the bluelines: red markup of a blueprint copy by the engineer, then yellow markup on the copy by the draftsman who implemented the changes on the original drawing, then brown markup by the checker on a check-print (a \"brown-line\"). Finally, the architect or engineer, draftsman, checker and supervisor would sign the original drawing, making it a legal document.\n\nAn alternative revision scheme was to use red-lines to indicate additions, yellow-lines to indicate deletions, and green-line tracings for checking (e.g., to confirm conductivities or to add notes to draftsmen on how to make corrections), entered on the copies of blue-line prints that were under engineering control.\n\nWhatever scheme was standardized in each shop had to be documented. Colored lines were entered on the blue-line drawing using colored pencils and identified there. Once the modification process was complete, the red-lined blue-line prints were taken to document control, where the original master was modified to reflect the changes made during the engineering process. New blue-line prints would then be made of the modified master, and the process continued in this iterative manner until completion of the project. The master drawing, incorporating all changes and including the authorizing signatures, was finally archived in a vault.\n\nThe blueline print process was largely abandoned within the architectural/engineering community around the early 2000s. Contributing factors were the development of computer aided drafting and printing, the speed of machine printing, and the introduction of larger xerographic machines from companies like Ricoh and Xerox. The cost of blueline production materials and equipment, the fact that the prints themselves faded in sunlight, and the need to use the pungent chemical ammonia as a developer speeded up its replacement.\n\n"}
{"id": "38308915", "url": "https://en.wikipedia.org/wiki?curid=38308915", "title": "William J. Cook", "text": "William J. Cook\n\nWilliam John Cook (born October 18, 1957 in New Jersey) is an American operations researcher and mathematician, and Professor of Applied Mathematics and Statistics at Johns Hopkins University, where he joined the faculty in 2018. He is a member of the National Academy of Engineering. He is known for his work on the traveling salesman problem and is one of the authors of the Concorde TSP Solver.\n\nCook did his undergraduate studies at Rutgers University, graduating in 1979 with a bachelor's degree in mathematics. After earning a master's degree in operations research from Stanford University in 1980, he moved to the University of Waterloo, where he earned a Ph.D. in combinatorics and optimization in 1983 under the supervision of U. S. R. Murty. After postdoctoral studies at the University of Bonn, he joined the Cornell University faculty in 1985, moved to Columbia University in 1987, and in 1988 joined the research staff of Bell Communications Research. In 1994 he returned to academia as John von Neumann Professor at the University of Bonn, and in 1996 he moved to Rice University as Noah Harding Professor of Computational and Applied Mathematics. In 2002 he took his position at Georgia Tech. In January 2013, he moved to the University of Pittsburgh as the John Swanson Professor of Industrial Engineering, before returning to the University of Waterloo in June 2013 as a professor in the Department of Combinatorics and Optimization, and subsequently University Professor. In 2018 he moved to Johns Hopkins University as Professor of Applied Mathematics and Statistics.\n\nHe is the founding editor-in-chief of the journal \"Mathematical Programming Computation\" (since 2008), and the former editor-in-chief of \"Mathematical Programming\" (Series B from 1993 to 2003, and Series A from 2003 to 2007).\n\nHe won the Beale–Orchard-Hays Prize of the Mathematical Programming Society\nin 2000, and his book \"The Traveling Salesman Problem: A Computational Study\" won the Frederick W. Lanchester Prize of INFORMS in 2007.\n\nHe became a fellow of the Society for Industrial and Applied Mathematics in 2009, and of INFORMS in 2010. He was elected to the National Academy of Engineering in 2011. In 2012 he became a fellow of the American Mathematical Society.\n\n\n"}
