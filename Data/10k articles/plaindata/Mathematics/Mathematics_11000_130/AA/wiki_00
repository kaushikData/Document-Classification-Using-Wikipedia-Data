{"id": "498596", "url": "https://en.wikipedia.org/wiki?curid=498596", "title": "116 (number)", "text": "116 (number)\n\n116 (one hundred [and] sixteen) is the natural number following 115 and preceding 117.\n\n116 is a noncototient, meaning that there is no solution to the equation , where stands for Euler's totient function.\n\n116! + 1 is a factorial prime.\n\nThere are 116 ternary Lyndon words of length six, and 116 irreducible polynomials of degree six over a three-element field, which form the basis of a free Lie algebra of dimension 116.\n\nThere are 116 different ways of partitioning the numbers from 1 through 5 into subsets in such a way that, for every \"k\", the union of the first \"k\" subsets is a consecutive sequence of integers.\n\nThere are 116 different 6×6 Costas arrays.\n\nOne hundred sixteen is also:\n\n"}
{"id": "2695751", "url": "https://en.wikipedia.org/wiki?curid=2695751", "title": "158 (number)", "text": "158 (number)\n\n158 (one hundred [and] fifty-eight) is the natural number following 157 and preceding 159.\n\n158 is a nontotient, since there is no integer with 158 coprimes below it. 158 is a Perrin number, appearing after 68, 90, 119.\n\n158 is the number of digits in the decimal expansion of 100!, the product of all the natural numbers up to and including 100.\n\n\n\n\n158 is also:\n\n\n"}
{"id": "44751671", "url": "https://en.wikipedia.org/wiki?curid=44751671", "title": "1961 Census of India", "text": "1961 Census of India\n\nThe 1961 Census of India was the 10th in a series of censuses held in India every decade since 1871.\n\nThe population of India was counted as 438,936,918 people.\n\nThe 1961 census recognized 1,652 \"mother tongues\", counting all declarations made by any individual at the time when the census was conducted. However, the declaring individuals often mixed names of languages with those of dialects, sub-dialects and dialect clusters or even castes, professions, religions, localities, regions, countries and nationalities. The list therefore includes \"languages\" with barely a few individual speakers as well as 530 unclassified \"mother tongues\" and more than 100 idioms that are non-native to India, including linguistically unspecific demonyms such as \"African\", \"Canadian\" or \"Belgian\". Modifications were done by bringing in two additional components- place of birth i.e. village or town and duration of stay ( if born elsewhere).\n\n"}
{"id": "3326054", "url": "https://en.wikipedia.org/wiki?curid=3326054", "title": "Abraham Adrian Albert", "text": "Abraham Adrian Albert\n\nAbraham Adrian Albert (November 9, 1905 – June 6, 1972) was an American mathematician. In 1939, he received the American Mathematical Society's Cole Prize in Algebra for his work on Riemann matrices. He is best known for his work on the Albert–Brauer–Hasse–Noether theorem on finite-dimensional division algebras over number fields and as the developer of Albert algebras, which are also known as exceptional Jordan algebras.\n\nA second generation American, he was born in Chicago and most associated with that city. He received his Bachelor of Science in 1926, Masters in 1927, and PhD in 1928, at the age of 22. All degrees were obtained from the University of Chicago. He married around the same time as his graduation. He spent his postdoctoral year at Princeton University and then from 1929 to 1931 he was an instructor at Columbia University. During this period he worked on Abelian varieties and their endomorphism algebras. He returned to Princeton for the opening year of the Institute for Advanced Study in 1933-34 and spent another year in Princeton in 1961-62 as the first Director of the Communications Research Division of IDA (the Institute for Defense Analyses).\n\nFrom 1931 to 1972, he served on the mathematics faculty at the University of Chicago, where he became chair of the Mathematics Department in 1958 and Dean of the Physical Sciences Division in 1961.\n\nAs a research mathematician, he is primarily known for his work as one of the principal developers of the theory of linear associative algebras and as a pioneer in the development of linear non-associative algebras, although all of this grew out of his work on endomorphism algebras of Abelian varieties.\n\nAs an applied mathematician, he also did work for the military during World War II and thereafter. One of his most notable achievements was his groundbreaking work on cryptography. He prepared a manuscript, \"Some Mathematical Aspects of Cryptography,\" for his invited address at a meeting of the American Mathematical Society in November 1941. The theory that developed from this work can be seen in digital communications technologies.\n\nAfter WWII, he became a forceful advocate favoring government support for research in mathematics on a par with other physical sciences. He served on policy-making bodies at the Office of Naval Research, the United States National Research Council, and the National Science Foundation that funneled research grants into mathematics, giving many young mathematicians career opportunities previously unavailable. Due to his success in helping to give mathematical research a sound financial footing, he earned a reputation as a \"statesman for mathematics.\" Albert was elected a Fellow of the American Academy of Arts and Sciences in 1968.\n\n\n\n\n"}
{"id": "1342156", "url": "https://en.wikipedia.org/wiki?curid=1342156", "title": "Artin–Mazur zeta function", "text": "Artin–Mazur zeta function\n\nIn mathematics, the Artin–Mazur zeta function, named after Michael Artin and Barry Mazur, is a function that is used for studying the iterated functions that occur in dynamical systems and fractals.\n\nIt is defined as the formal power series\n\nwhere Fix(\"ƒ\") is the set of fixed points of the \"n\"th iterate of the function \"ƒ\", and card(Fix(\"ƒ\")) is the number of fixed points (i.e. the cardinality of that set).\n\nNote that the zeta function is defined only if the set of fixed points is finite for each \"n\". This definition is formal in that the series does not always have a positive radius of convergence.\n\nThe Artin–Mazur zeta function is invariant under topological conjugation.\n\nThe Milnor–Thurston theorem states that the Artin–Mazur zeta function is the inverse of the kneading determinant of \"ƒ\".\n\nThe Artin–Mazur zeta function is formally similar to the local zeta function, when a diffeomorphism on a compact manifold replaces the Frobenius mapping for an algebraic variety over a finite field.\n\nThe Ihara zeta function of a graph can be interpreted as an example of the Artin–Mazur zeta function.\n\n\n"}
{"id": "2233526", "url": "https://en.wikipedia.org/wiki?curid=2233526", "title": "Basic hypergeometric series", "text": "Basic hypergeometric series\n\nIn mathematics, basic hypergeometric series, or hypergeometric \"q\"-series, are \"q\"-analogue generalizations of generalized hypergeometric series, and are in turn generalized by elliptic hypergeometric series. \nA series \"x\" is called hypergeometric if the ratio of successive terms \"x\"/\"x\" is a rational function of \"n\". If the ratio of successive terms is a rational function of \"q\", then the series is called a basic hypergeometric series. The number \"q\" is called the base. \n\nThe basic hypergeometric series φ(\"q\",\"q\";\"q\";\"q\",\"x\") was first considered by . It becomes the hypergeometric series \"F\"(α,β;γ;\"x\") in the limit when the base \"q\" is 1.\n\nThere are two forms of basic hypergeometric series, the unilateral basic hypergeometric series φ, and the more general bilateral basic hypergeometric series ψ.\nThe unilateral basic hypergeometric series is defined as\n\nwhere \nand \nis the \"q\"-shifted factorial.\nThe most important special case is when \"j\" = \"k\" + 1, when it becomes\nThis series is called \"balanced\" if \"a\" ... \"a\" = \"b\" ...\"b\"\"q\".\nThis series is called \"well poised\" if \"a\"\"q\" = \"a\"\"b\" = ... = \"a\"\"b\", and \"very well poised\" if in addition \"a\" = −\"a\" = \"qa\". \nThe unilateral basic hypergeometric series is a q-analog of the hypergeometric series since\nholds ().<br>\nThe bilateral basic hypergeometric series, corresponding to the bilateral hypergeometric series, is defined as\n\nThe most important special case is when \"j\" = \"k\", when it becomes\n\nThe unilateral series can be obtained as a special case of the bilateral one by setting one of the \"b\" variables equal to \"q\", at least when none of the \"a\" variables is a power of \"q\", as all the terms with \"n\" < 0 then vanish.\n\nSome simple series expressions include\n\nand \n\nand \n\nThe \"q\"-binomial theorem (first published in 1811 by Heinrich August Rothe) states that\n\nwhich follows by repeatedly applying the identity\n\nThe special case of \"a\" = 0 is closely related to the q-exponential.\nCauchy binomial theorem is a special case of the q-binomial theorem.\n\nSrinivasa Ramanujan gave the identity\n\nvalid for |\"q\"| < 1 and |\"b\"/\"a\"| < |\"z\"| < 1. Similar identities for formula_15 have been given by Bailey. Such identities can be understood to be generalizations of the Jacobi triple product theorem, which can be written using q-series as\n\nKen Ono gives a related formal power series\n\nAs an analogue of the Barnes integral for the hypergeometric series, Watson showed that\nwhere the poles of formula_19 lie to the left of the contour and the remaining poles lie to the right. There is a similar contour integral for φ. This contour integral gives an analytic continuation of the basic hypergeometric function in \"z\".\nThe basic hypergeometric matrix function can be defined as follows:\nThe ratio test shows that this matrix function is absolutely convergent.\n\n\n"}
{"id": "50635919", "url": "https://en.wikipedia.org/wiki?curid=50635919", "title": "Benjamin Abram Bernstein", "text": "Benjamin Abram Bernstein\n\nBenjamin Abram Bernstein (20 May 1881, Pasvalys, Lithuania – 25 September 1964, Berkeley, California) was an American mathematician, specializing in mathematical logic.\n\nWith his family, Bernstein immigrated as a child to the United States. After completing public primary education in 1897 in Baltimore, he completed in 1902 his secondary education at Baltimore City College, and then received in 1905 his A.B. degree from Johns Hopkins University. After completing two years of graduate study at Johns Hopkins University, he became in 1907 an instructor and continuing graduate student in mathematics at the University of California, Berkeley. There he received in 1913, with supervisor Mellen W. Haskell, his Ph.D. At Berkeley, Bernstein became in 1918 an assistant professor, in 1923 an associate professor, and in 1928 a full professor of mathematics, retiring in 1951 as professor emeritus.\n\nHe was an Invited Speaker at the ICM in 1924 in Toronto. His doctoral students include Robert Levit and J.C.C. McKinsey.\n\nIn June 1920 in New York City, Professor Bernstein married Rose Davidson; her brother was the famous sculptor Jo Davidson. Bernstein was predeceased by his wife and upon his death was survived by a daughter and a granddaughter.\n\n\n"}
{"id": "4677", "url": "https://en.wikipedia.org/wiki?curid=4677", "title": "Binomial theorem", "text": "Binomial theorem\n\nIn elementary algebra, the binomial theorem (or binomial expansion) describes the algebraic expansion of powers of a binomial. According to the theorem, it is possible to expand the polynomial into a sum involving terms of the form , where the exponents and are nonnegative integers with , and the coefficient of each term is a specific positive integer depending on and . For example (for ),\n\nThe coefficient in the term of is known as the binomial coefficient formula_2 or formula_3 (the two have the same value). These coefficients for varying and can be arranged to form Pascal's triangle. These numbers also arise in combinatorics, where formula_2 gives the number of different combinations of elements that can be chosen from an -element set.\n\nSpecial cases of the binomial theorem were known since at least the 4th century B.C. when Greek mathematician Euclid mentioned the special case of the binomial theorem for exponent 2. There is evidence that the binomial theorem for cubes was known by the 6th century in India.\n\nBinomial coefficients, as combinatorial quantities expressing the number of ways of selecting \"k\" objects out of \"n\" without replacement, were of interest to ancient Indian mathematicians. The earliest known reference to this combinatorial problem is the \"Chandaḥśāstra\" by the Indian lyricist Pingala (c. 200 B.C.), which contains a method for its solution. The commentator Halayudha from the 10th century A.D. explains this method using what is now known as Pascal's triangle. By the 6th century A.D., the Indian mathematicians probably knew how to express this as a quotient formula_5, and a clear statement of this rule can be found in the 12th century text \"Lilavati\" by Bhaskara.\n\nThe first formulation of the binomial theorem and the table of binomial coefficients, to our knowledge, can be found in a work by Al-Karaji, quoted by Al-Samaw'al in his \"al-Bahir\". Al-Karaji described the triangular pattern of the binomial coefficients and also provided a mathematical proof of both the binomial theorem and Pascal's triangle, using an early form of mathematical induction. The Persian poet and mathematician Omar Khayyam was probably familiar with the formula to higher orders, although many of his mathematical works are lost. The binomial expansions of small degrees were known in the 13th century mathematical works of Yang Hui and also Chu Shih-Chieh. Yang Hui attributes the method to a much earlier 11th century text of Jia Xian, although those writings are now also lost.\n\nIn 1544, Michael Stifel introduced the term \"binomial coefficient\" and showed how to use them to express formula_6 in terms of formula_7, via \"Pascal's triangle\". Blaise Pascal studied the eponymous triangle comprehensively in the treatise \"Traité du triangle arithmétique\" (1653). However, the pattern of numbers was already known to the European mathematicians of the late Renaissance, including Stifel, Niccolò Fontana Tartaglia, and Simon Stevin.\n\nIsaac Newton is generally credited with the generalized binomial theorem, valid for any rational exponent.\n\nAccording to the theorem, it is possible\nto expand any power of \"x\" + \"y\" into a sum of the form\nwhere each formula_9 is a specific positive integer known as a binomial coefficient. (When an exponent is zero, the corresponding power expression is taken to be 1 and this multiplicative factor is often omitted from the term. Hence one often sees the right side written as formula_10.) This formula is also referred to as the binomial formula or the binomial identity. Using summation notation, it can be written as\nThe final expression follows from the previous one by the symmetry of \"x\" and \"y\" in the first expression, and by comparison it follows that the sequence of binomial coefficients in the formula is symmetrical.\nA simple variant of the binomial formula is obtained by substituting 1 for \"y\", so that it involves only a single variable. In this form, the formula reads\nor equivalently\n\nThe most basic example of the binomial theorem is the formula for the square of :\n\nThe binomial coefficients 1, 2, 1 appearing in this expansion correspond to the second row of Pascal's triangle. (Note that the top \"1\" of the triangle is considered to be row 0, by convention.) The coefficients of higher powers of correspond to lower rows of the triangle:\n\nSeveral patterns can be observed from these examples. In general, for the expansion :\n\nThe binomial theorem can be applied to the powers of any binomial. For example,\n\nFor a binomial involving subtraction, the theorem can be applied by using the form . This has the effect of changing the sign of every other term in the expansion:\n\nFor positive values of \"a\" and \"b\", the binomial theorem with \"n\" = 2 is the geometrically evident fact that a square of side can be cut into a square of side \"a\", a square of side \"b\", and two rectangles with sides \"a\" and \"b\". With \"n\" = 3, the theorem states that a cube of side can be cut into a cube of side \"a\", a cube of side \"b\", three \"a\"×\"a\"×\"b\" rectangular boxes, and three \"a\"×\"b\"×\"b\" rectangular boxes.\n\nIn calculus, this picture also gives a geometric proof of the derivative formula_18 if one sets formula_19 and formula_20 interpreting \"b\" as an infinitesimal change in \"a,\" then this picture shows the infinitesimal change in the volume of an \"n\"-dimensional hypercube, formula_21 where the coefficient of the linear term (in formula_22) is formula_23 the area of the \"n\" faces, each of dimension formula_24\nSubstituting this into the definition of the derivative via a difference quotient and taking limits means that the higher order terms, formula_26 and higher, become negligible, and yields the formula formula_27 interpreted as\nIf one integrates this picture, which corresponds to applying the fundamental theorem of calculus, one obtains Cavalieri's quadrature formula, the integral formula_29 – see proof of Cavalieri's quadrature formula for details.\n\nThe coefficients that appear in the binomial expansion are called binomial coefficients. These are usually written formula_9, and pronounced “\"n\" choose \"k\"”.\n\nThe coefficient of \"x\"\"y\" is given by the formula\n\nwhich is defined in terms of the factorial function \"n\"!. Equivalently, this formula can be written\n\nwith \"k\" factors in both the numerator and denominator of the fraction. Note that, although this formula involves a fraction, the binomial coefficient formula_9 is actually an integer.\n\nThe binomial coefficient formula_9 can be interpreted as the number of ways to choose \"k\" elements from an \"n\"-element set. This is related to binomials for the following reason: if we write (\"x\" + \"y\") as a product\nthen, according to the distributive law, there will be one term in the expansion for each choice of either \"x\" or \"y\" from each of the binomials of the product. For example, there will only be one term \"x\", corresponding to choosing \"x\" from each binomial. However, there will be several terms of the form \"x\"\"y\", one for each way of choosing exactly two binomials to contribute a \"y\". Therefore, after combining like terms, the coefficient of \"x\"\"y\" will be equal to the number of ways to choose exactly 2 elements from an \"n\"-element set.\n\nThe coefficient of \"xy\" in\n\nequals formula_37 because there are three \"x\",\"y\" strings of length 3 with exactly two \"y\"'s, namely,\n\ncorresponding to the three 2-element subsets of { 1, 2, 3 }, namely,\n\nwhere each subset specifies the positions of the \"y\" in a corresponding string.\n\nExpanding (\"x\" + \"y\") yields the sum of the 2 products of the form \"e\"\"e\" ... \"e\" where each \"e\" is \"x\" or \"y\". Rearranging factors shows that each product equals \"x\"\"y\" for some \"k\" between 0 and \"n\". For a given \"k\", the following are proved equal in succession:\nThis proves the binomial theorem.\n\nInduction yields another proof of the binomial theorem. When \"n\" = 0, both sides equal 1, since \"x\" = 1 and formula_43.\nNow suppose that the equality holds for a given \"n\"; we will prove it for \"n\" + 1.\nFor \"j\", \"k\" ≥ 0, let [\"ƒ\"(\"x\", \"y\")] denote the coefficient of \"x\"\"y\" in the polynomial \"ƒ\"(\"x\", \"y\").\nBy the inductive hypothesis, (\"x\" + \"y\") is a polynomial in \"x\" and \"y\" such that [(\"x\" + \"y\")] is formula_44 if \"j\" + \"k\" = \"n\", and 0 otherwise.\nThe identity\n\nshows that (\"x\" + \"y\") also is a polynomial in \"x\" and \"y\", and\n\nsince if \"j\" + \"k\" = \"n\" + 1, then (\"j\" − 1) + \"k\" = \"n\" and \"j\" + (\"k\" − 1) = \"n\". Now, the right hand side is\n\nby Pascal's identity. On the other hand, if \"j\" +\"k\" ≠ \"n\" + 1, then (\"j\" – 1) + \"k\" ≠ \"n\" and \"j\" +(\"k\" – 1) ≠ \"n\", so we get 0 + 0 = 0. Thus\n\nwhich is the inductive hypothesis with \"n\" + 1 substituted for \"n\" and so completes the inductive step.\n\nAround 1665, Isaac Newton generalized the binomial theorem to allow real exponents other than nonnegative integers. (The same generalization also applies to complex exponents.) In this generalization, the finite sum is replaced by an infinite series. In order to do this, one needs to give meaning to binomial coefficients with an arbitrary upper index, which cannot be done using the usual formula with factorials. However, for an arbitrary number \"r\", one can define\n\nwhere formula_50 is the Pochhammer symbol, here standing for a falling factorial. This agrees with the usual definitions when \"r\" is a nonnegative integer. Then, if \"x\" and \"y\" are real numbers with |\"x\"| > |\"y\"|, and \"r\" is any complex number, one has\n\nWhen \"r\" is a nonnegative integer, the binomial coefficients for \"k\" > \"r\" are zero, so this equation reduces to the usual binomial theorem, and there are at most \"r\" + 1 nonzero terms. For other values of \"r\", the series typically has infinitely many nonzero terms.\n\nFor example, \"r\" = 1/2 gives the following series for the square root:\n\nTaking formula_53, the generalized binomial series gives the geometric series formula, valid for formula_54:\n\nMore generally, with \"r\" = −\"s\":\n\nSo, for instance, when formula_57,\n\nThe generalized binomial theorem can be extended to the case where \"x\" and \"y\" are complex numbers. For this version, one should again assume |\"x\"| > |\"y\"| and define the powers of \"x\" + \"y\" and \"x\" using a holomorphic branch of log defined on an open disk of radius |\"x\"| centered at \"x\".\nThe generalized binomial theorem is valid also for elements \"x\" and \"y\" of a Banach algebra as long as \"xy\" = \"yx\", \"x\" is invertible, and ||\"y/x\"|| < 1.\n\nA version of the binomial theorem is valid for the following Pochhammer symbol-like family of polynomials: for a given real constant \"c\", define formula_59 and formula_60 for formula_61. Then\nformula_62\nThe case recovers the usual binomial theorem.\n\nMore generally, a sequence formula_63 of polynomials is said to be binomial if\nAn operator formula_71 on the space of polynomials is said to be the \"basis operator\" of the sequence formula_63 if formula_73 and formula_74 for all formula_75. A sequence formula_63 is binomial if and only if its basis operator is a Delta operator. Writing formula_77 for the shift by formula_78 operator, the Delta operators corresponding to the above \"Pochhammer\" families of polynomials are the backward difference formula_79 for formula_80, the ordinary derivative for formula_81, and the forward difference formula_82 for formula_83.\n\nThe binomial theorem can be generalized to include powers of sums with more than two terms. The general version is\n\nwhere the summation is taken over all sequences of nonnegative integer indices \"k\" through \"k\" such that the sum of all \"k\" is \"n\". (For each term in the expansion, the exponents must add up to \"n\"). The coefficients formula_85 are known as multinomial coefficients, and can be computed by the formula\n\nCombinatorially, the multinomial coefficient formula_87 counts the number of different ways to partition an \"n\"-element set into disjoint subsets of sizes \"k\", ..., \"k\".\n\nIt is often useful when working in more dimensions, to deal with products of binomial expressions. By the binomial theorem this is equal to\n\nThis may be written more concisely, by multi-index notation, as\n\nThe general Leibniz rule gives the th derivative of a product of two functions in a form similar to that of the binomial theorem:\n\nHere, the superscript indicates the th derivative of a function. If one sets and , and then cancels the common factor of from both sides of the result, the ordinary binomial theorem is recovered.\n\nFor the complex numbers the binomial theorem can be combined with De Moivre's formula to yield multiple-angle formulas for the sine and cosine. According to De Moivre's formula,\n\nUsing the binomial theorem, the expression on the right can be expanded, and then the real and imaginary parts can be taken to yield formulas for cos(\"nx\") and sin(\"nx\"). For example, since\nDe Moivre's formula tells us that\nwhich are the usual double-angle identities. Similarly, since\nDe Moivre's formula yields\nIn general,\nand\n\nThe number \"e\" is often defined by the formula\n\nApplying the binomial theorem to this expression yields the usual infinite series for \"e\". In particular:\n\nThe \"k\"th term of this sum is\n\nAs \"n\" → ∞, the rational expression on the right approaches one, and therefore\n\nThis indicates that \"e\" can be written as a series:\n\nIndeed, since each term of the binomial expansion is an increasing function of \"n\", it follows from the monotone convergence theorem for series that the sum of this infinite series is equal to \"e\".\n\nThe binomial theorem is closely related to the probability mass function of the negative binomial distribution. The probability of a (countable) collection of independent Bernoulli trials formula_103 with probability of success formula_104 all not happening is formula_105A useful upper bound for this quantity is formula_106. \n\nFormula (1) is valid more generally for any elements \"x\" and \"y\" of a semiring satisfying \"xy\" = \"yx\". The theorem is true even more generally: alternativity suffices in place of associativity.\n\nThe binomial theorem can be stated by saying that the polynomial sequence { 1, \"x\", \"x\", \"x\", ... } is of binomial type.\n\n\n\n\n"}
{"id": "3365", "url": "https://en.wikipedia.org/wiki?curid=3365", "title": "Byte", "text": "Byte\n\nThe byte is a unit of digital information that most commonly consists of eight bits, representing a binary number. Historically, the byte was the number of bits used to encode a single character of text in a computer and for this reason it is the smallest addressable unit of memory in many computer architectures.\n\nThe size of the byte has historically been hardware dependent and no definitive standards existed that mandated the size – byte-sizes from 1 to 48 bits are known to have been used in the past. Early character encoding systems often used six bits, and machines using six-bit and nine-bit bytes were common into the 1960s. These machines most commonly had memory words of 12, 24, 36, 48 or 60 bits, corresponding to two, four, six, eight or 10 six-bit bytes. In this era, bytes in the instruction stream were often referred to as \"syllables\", before the term byte became common.\n\nThe modern \"de-facto\" standard of eight bits, as documented in ISO/IEC 2382-1:1993, is a convenient power of two permitting the values 0 through 255 for one byte (2 in power of 8 = 256, where zero signifies number as well). The international standard IEC 80000-13 codified this common meaning. Many types of applications use information representable in eight or fewer bits and processor designers optimize for this common usage. The popularity of major commercial computing architectures has aided in the ubiquitous acceptance of the eight-bit size. Modern architectures typically use 32- or 64-bit words, built of four or eight bytes.\n\nThe unit symbol for the byte was designated as the upper-case letter \"B\" by the International Electrotechnical Commission (IEC) and Institute of Electrical and Electronics Engineers (IEEE) in contrast to the bit, whose IEEE symbol is a lower-case \"b\". Internationally, the unit \"octet\", symbol \"o\", explicitly denotes a sequence of eight bits, eliminating the ambiguity of the byte.\n\nThe term \"byte\" was coined by Werner Buchholz in June 1956, during the early design phase for the IBM Stretch computer, which had addressing to the bit and variable field length (VFL) instructions with a byte size encoded in the instruction.\nIt is a deliberate respelling of \"bite\" to avoid accidental mutation to \"bit\".\n\nAnother origin of \"byte\" for bit groups smaller than a machine's word size (and in particular groups of four bits) is on record by Louis G. Dooley, who claimed he coined the term while working with Jules Schwartz and Dick Beeler on an air defense system called SAGE at MIT Lincoln Laboratory in ca. 1956/1957, which was jointly developed by Rand, MIT, and IBM. Later on, Schwartz's language JOVIAL actually used the term, but he recalled vaguely that it was derived from AN/FSQ-31.\n\nEarly computers used a variety of four-bit binary coded decimal (BCD) representations and the six-bit codes for printable graphic patterns common in the U.S. Army (FIELDATA) and Navy. These representations included alphanumeric characters and special graphical symbols. These sets were expanded in 1963 to seven bits of coding, called the American Standard Code for Information Interchange (ASCII) as the Federal Information Processing Standard, which replaced the incompatible teleprinter codes in use by different branches of the U.S. government and universities during the 1960s. ASCII included the distinction of upper- and lowercase alphabets and a set of control characters to facilitate the transmission of written language as well as printing device functions, such as page advance and line feed, and the physical or logical control of data flow over the transmission media. During the early 1960s, while also active in ASCII standardization, IBM simultaneously introduced in its product line of System/360 the eight-bit Extended Binary Coded Decimal Interchange Code (EBCDIC), an expansion of their six-bit binary-coded decimal (BCDIC) representation used in earlier card punches.\nThe prominence of the System/360 led to the ubiquitous adoption of the eight-bit storage size, while in detail the EBCDIC and ASCII encoding schemes are different.\n\nIn the early 1960s, AT&T introduced digital telephony first on long-distance trunk lines. These used the eight-bit µ-law encoding. This large investment promised to reduce transmission costs for eight-bit data.\n\nThe development of eight-bit microprocessors in the 1970s popularized this storage size. Microprocessors such as the Intel 8008, the direct predecessor of the 8080 and the 8086, used in early personal computers, could also perform a small number of operations on the four-bit pairs in a byte, such as the decimal-add-adjust (DAA) instruction. A four-bit quantity is often called a nibble, also \"nybble\", which is conveniently represented by a single hexadecimal digit.\n\nThe term \"octet\" is used to unambiguously specify a size of eight bits. It is used extensively in protocol definitions.\n\nHistorically, the term \"octad\" or \"octade\" was used to denote eight bits as well at least in Western Europe; however, this usage is no longer common. The exact origin of the term is unclear, but it can be found in British, Dutch, and German sources of the 1960s and 1970s, and throughout the documentation of Philips mainframe computers.\n\nThe unit symbol for the byte is specified in IEC 80000-13, IEEE 1541 and the Metric Interchange Format as the upper-case character \"B\". In contrast, IEEE 1541 specifies the lower case character \"b\" as the symbol for the bit, but IEC 80000-13 and Metric-Interchange-Format specify the symbol as \"bit\", providing disambiguation from B for byte.\n\nIn the International System of Quantities (ISQ), B is the symbol of the \"bel\", a unit of logarithmic power ratios named after Alexander Graham Bell, creating a conflict with the IEC specification. However, little danger of confusion exists, because the bel is a rarely used unit. It is used primarily in its decadic fraction, the decibel (dB), for signal strength and sound pressure level measurements, while a unit for one tenth of a byte, the decibyte, and other fractions, are only used in derived units, such as transmission rates.\n\nThe lowercase letter o for octet is defined as the symbol for octet in IEC 80000-13 and is commonly used in languages such as French and Romanian, and is also combined with metric prefixes for multiples, for example ko and Mo.\n\nThe usage of the term \"octad(e)\" for eight bits is no longer common.\n\nDespite standardization efforts, ambiguity still exists in the meanings of the SI (or metric) prefixes used with the unit byte, especially concerning the prefixes \"kilo\" (k or K), \"mega\" (M), and \"giga\" (G). Computer memory has a binary architecture in which multiples are expressed in powers of 2. In some fields of the software and computer hardware industries a binary prefix is used for bytes and bits, while producers of computer storage devices practice adherence to decimal SI multiples. For example, a computer disk drive capacity of 100 gigabytes is specified when the disk contains 100 billion bytes (93 gibibytes) of storage space.\n\nWhile the numerical difference between the decimal and binary interpretations is relatively small for the prefixes kilo and mega, it grows to over 20% for prefix yotta. The linear-log graph at right illustrates the difference versus storage size up to an exabyte.\n\nMany programming languages defined the data type \"byte\".\n\nThe C and C++ programming languages define \"byte\" as an \"addressable unit of data storage large enough to hold any member of the basic character set of the execution environment\" (clause 3.6 of the C standard). The C standard requires that the integral data type \"unsigned char\" must hold at least 256 different values, and is represented by at least eight bits (clause 5.2.4.2.1). Various implementations of C and C++ reserve 8, 9, 16, 32, or 36 bits for the storage of a byte. In addition, the C and C++ standards require that there are no \"gaps\" between two bytes. This means every bit in memory is part of a byte.\n\nJava's primitive codice_1 data type is always defined as consisting of 8 bits and being a signed data type, holding values from −128 to 127.\n\n.NET programming languages, such as C#, define both an unsigned codice_1 and a signed codice_3, holding values from 0 to 255, and −128 to 127, respectively.\n\nIn data transmission systems, the byte is defined as a contiguous sequence of bits in a serial data stream representing the smallest distinguished unit of data. A transmission unit might include start bits, stop bits, or parity bits, and thus could vary from 7 to 12 bits to contain a single 7-bit ASCII code.\n\n"}
{"id": "22405720", "url": "https://en.wikipedia.org/wiki?curid=22405720", "title": "CS-BLAST", "text": "CS-BLAST\n\nCS-BLAST\n(Context-Specific BLAST) is a tool that searches a protein sequence that extends BLAST (Basic Local Alignment Search Tool), (Basic Local Alignment Search Tool) using context-specific mutation probabilities. More specifically, CS-BLAST derives context-specific amino-acid similarities on each query sequence from short windows on the query sequences [4]. Using CS-BLAST doubles sensitivity and significantly improves alignment quality without a loss of speed in comparison to BLAST. CSI-BLAST (Context-Specific Iterated) BLAST is the context-specific analog of PSI-BLAST (Position-Specific Iterated) BLAST which computes the mutation profile with substitution probabilities and mixes it with the query profile [2]. CSI-BLAST (Context-Specific Iterated BLAST) is the context specific analog of PSI-BLAST (Position-Specific Iterated) BLAST. Both of these programs are available as web-server and are available for free download.\n\nHomology is the relationship between biological structures or sequences derived from a common ancestor. Homologous proteins (proteins who have common ancestry) are inferred from their sequence similarity. Inferring homologous relationships involves calculating scores of aligned pairs minus penalties for gaps. Aligning pairs of proteins identify regions of similarity indicating a relationship between the two, or more, proteins. In order to have a homologous relationship, the sum of scores over all the aligned pairs of amino acids or nucleotides must be sufficiently high [2]. Standard methods of sequence comparisons use a substitution matrix to accomplish this [4]. Similarities between amino acids or nucleotides are quantified in these substitution matrices. The substitution score (formula_1) of amino acids formula_2 and formula_3 can we written as follows:\n\nformula_4\n\nwhere formula_5 denotes the probability of amino acid formula_2 mutating into amino acid formula_3 [2]. In a large set of sequence alignments, counting the number of amino acids as well as the number of aligned pairs formula_8 will allow you to derive the probabilities formula_5 and formula_10.\n\nSince protein sequences need to maintain a stable structure, a residue’s substitution probabilities are largely determined by the structural context of where it is found. As a result, substitution matrices are trained for structural contexts. Since context information is encoded in transition probabilities between states, mixing mutation probabilities from substitution matrices weighted for corresponding states achieves improved alignment qualities when compared to standard substitution matrices. CS-BLAST improves further upon this concept. The figure illustrates the sequence to sequence and profile to sequence equivalence with the alignment matrix. The query profile results from the artificial mutations in which the bar heights are proportional to the corresponding amino acid probabilities [4].\n\n(A FIGURE NEEDS TO GO HERE THIS IS THE CAPTION) “Sequence search/alignment algorithms find the path that maximizes the sum of similarity scores (color-coded blue to red). Substitution matrix scores are equivalent to profile scores if the sequence profile (colored histogram) is generated from the query sequence by adding artificial mutations with the substitution matrix pseudocount scheme. Histogram bar heights represent the fraction of amino acids in profile columns” [4].\n\nCS-BLAST greatly improves alignment quality over the entire range of sequence identities and especially for difficult alignments in comparison to regular BLAST and PSI-BLAST. PSI-BLAST (Position-Specific Iterated BLAST) runs at about the same speed per iteration as regular BLAST, but is able to detect weaker sequence similarities that are still biologically relevant [3]. Alignment quality is based on alignment sensitivity and alignment precision [4].\n\nAlignment sensitivity is measured by correctly comparing predicted alignments of residue pairs to the total number of possible alignable pairs. This is calculated with the fraction: (pairs correctly aligned)/(pairs structurally alignable)\n\nAlignment precision is measured by the correctness of aligned residue pairs. This is calculated with the fraction: (pairs correctly aligned)/(pairs aligned)\n\nThe graph is the benchmark Biegert and Söding used to evaluate homology detection. The benchmark compares CS-BLAST to BLAST using true positives from the same superfamily versus false positive of pairs from different folds [4]. (A GRAPH NEEDS TO GO HERE)\n\nThe other graph uses detects true positives (with a different scale than the previous graph) and false positives of PSI-BLAST and CSI-BLAST and compares the two for one to five iterations [4]. (A DIFFERENT GRAPH NEEDS TO GO HERE)\n\nCS-BLAST offers improved sensitivity and alignment quality in sequence comparison. Sequence searches with CS-BLAST are more than twice as sensitive as BLAST [4]. It produces higher quality alignments and generates reliable E-values without a loss of speed. CS-BLAST detects 139% more homologous proteins at a cumulative error rate of 20% [2]. At a 10% error rate, 138% more homologs are detected, and for the easiest cases at a 1% error rate, CS-BLAST was still 96% more effective than BLAST [2]. Additionally, CS-BLAST in 2 iterations is more sensitive than 5 iterations of PSI-BLAST. About 15% more homologs were detected in comparison [4].\n\nThe CS-BLAST method derives similarities between sequence context-specific amino acids for 13 residue windows centered on each residue. CS-BLAST works by generating a sequence profile for a query sequence by using context-specific mutations and then jumpstarting a profile-to-sequence search method.\n\nCS-BLAST starts by predicting the expected mutation probabilities for each position. For a certain residue, a sequence window of ten total surrounding residues is selected as seen in the image. Then, Biegert and Söding compared the sequence window to a library with thousands of context profiles. The library is generated by clustering a representative set of sequence profile windows. The actual predicting of mutation probabilities is achieved by weighted mixing of the central columns of the most similar context profiles [4]. This aligns short profiles that are nonhomologous and ungapped which gives higher weight to better matching profiles, making them easier to detect [4]. A sequence profile represents a multiple alignment of homologous sequences and describes what amino acids are likely to occur at each position in related sequences. With this method substitution matrices are unnecessary. In addition, there is no need for transition probabilities as a result of the fact that context information is encoded within the context profiles. This makes computation simpler and allows for runtime to be scaled linearly instead of quadratically.\n\n(A FIGURE GOES HERE AND THIS IS THE CAPTION) “Computation of context-specific pseudocounts. The Expected mutations (i.e., pseudocounts) for a residue (highlighted in yellow) are calculated based on the sequence context around it (red box). Library profiles contribute to the context-specific sequence profile with weights determined by their similarity to the sequence context. The resulting profile can be used to jump-start PSI-BLAST, which will then perform a sequence-to-sequence search with context-specific amino acid similarities” [4].\n\nThe context specific mutation probability, the probability of observing a specific amino acid in a homologous sequence given a context, is calculated by a weighted mixing of the amino acids in the central columns of the most similar context profiles. The image illustrates the calculation of expected mutation probabilities for a specific residue at a certain position. As seen in the image, the library of context profiles all contribute based on similarity to the context specific sequence profile for the query sequence [4].\n\nIn predicting substitution probabilities using only the amino acid’s local sequence context, you gain the advantage of not needing to know the structure of the query protein while still allowing for the detection of more homologous proteins than standard substitution matrices [4]. Bigert and Söding’s approach to predicting substitution probabilities was based on a generative model. In another paper in collaboration with Angermüller, they develop a discriminative machine learning method that improves prediction accuracy [2].\n\nGiven an observed variable formula_11 and a target variable formula_12, a generative model defines the probabilities formula_13 and formula_14 separately. In order to predict the unobserved target variable, formula_12, Bayes’ theorem, formula_16\n\nis used. A generative model, as the name suggests, allows one to generate new data points formula_17. The joint distribution is described as formula_18. To train a generative model, the following equation is used to maximize the joint probability formula_19.\n\nThe discriminative model is a logistic regression maximum entropy classifier. With the discriminative model, the goal is to predict a context specific substitution probability given a query sequence. The discriminative approach for modeling substitution probabilities,formula_20 where formula_21 describes a sequence of amino acids around position formula_22 of a sequence, is based on formula_23 context states. Context states are characterized by parameters emission weight (formula_24), bias weight (formula_25), and context weight (formula_26) [2]. Emission probabilities from a context state are given by the emission weights as follows for formula_27 to formula_28: formula_29\n\nwhere formula_30 is the emission probability and is the context state. In the discriminative approach, probability for a context state formula_31 given context formula_21 is modeled directly by the exponential of an affine function of the context account profile where formula_33 is the context count profile with a normalization constant formula_34 normalizes the probability to 1. This equation is as follows where the first summation takes formula_35 to formula_36 and the second summation takes formula_37 to formula_28: formula_39.\n\nAs with the generative model, target distribution is obtained by mixing the emission probabilities of each context state weighted by the similarity.\n\nThe MPI Bioinformatics toolkit in an interactive website and service that allows anyone to do comprehensive and collaborative protein analysis with a variety of different tools including CS-BLAST as well as PSI-BLAST [1]. This tool allows for input of a protein and select options for you to customize your analysis. It also can forward the output to other tools as well.\n\n\n[1] Alva, Vikram, Seung-Zin Nam, Johannes Söding, and Andrei N. Lupas. “The MPI Bioinformatics Toolkit as an Integrative Platform for Advanced Protein Sequence and Structure Analysis.” \"Nucleic Acids Research\" 44.Web server Issue (2016): W410-415. \"NCBI\". Web. 2 Nov. 2016.\n\n[2] Angermüller, Christof, Andreas Biegert, and Johannes Söding. “Discriminative Modelling of Context-specific Amino Acid Substitution Properties” \"BIOINFORMATICS\" 28.24 (2012): 3240-247. \"Oxford Journals\". Web. 2 Nov. 2016.\n\n[3] Astschul, Stephen F., et al. “Gapped BLAST and PSI-BLAST: A New Generation of Protein Database Search Programs.” \"Nucleic Acids Research\" 25.17 (1997): 3389-402. \"Oxford University Press.\" Print\n\n[4] Bigert, A., and J. Söding. “Sequence Context-specific Profiles for Homology Searching.” \"Proceedings of the National Academy of Sciences\" 106.10 (2009): 3770-3775. PNAS. Web. 23 Oct. 2016.\n\n"}
{"id": "482522", "url": "https://en.wikipedia.org/wiki?curid=482522", "title": "Carl Wilhelm Borchardt", "text": "Carl Wilhelm Borchardt\n\nCarl Wilhelm Borchardt (22 February 1817 – 27 June 1880) was a German mathematician.\n\nBorchardt was born to a Jewish family in Berlin. His father, Moritz, was a respected merchant, and his mother was Emma Heilborn. Borchardt studied under a number of tutors, including Julius Plücker and Jakob Steiner. He studied at the University of Berlin under Lejeune Dirichlet in 1836 and at the University of Königsberg in 1839. In 1848 he began teaching at the University of Berlin.\n\nHe did research in the area of arithmetic-geometric mean, continuing work by Gauss and Lagrange. He generalised the results of Kummer diagonalising symmetric matrices, using determinants and Sturm functions. He was also an editor of \"Crelle's Journal\" from 1856–80, during which time it was known as \"Borchardt's Journal\".\n\nHe died in Rüdersdorf, Germany. His grave is preserved in the Protestant \"Friedhof III der Jerusalems- und Neuen Kirchengemeinde\" (Cemetery No. III of the congregations of Jerusalem's Church and New Church) in Berlin-Kreuzberg, south of Hallesches Tor.\n\n"}
{"id": "29095224", "url": "https://en.wikipedia.org/wiki?curid=29095224", "title": "Chebfun", "text": "Chebfun\n\nChebfun is a free/open-source software system written in MATLAB for numerical computation with functions of a real variable. It is based on the idea of overloading MATLAB's commands for vectors and matrices to analogous commands for functions and operators. Thus, for example, whereas the SUM command in MATLAB adds up the elements of a vector, the SUM command in Chebfun evaluates a definite integral. Similarly the backslash command in MATLAB becomes a Chebfun command for solving differential equations.\n\nThe mathematical basis of Chebfun is numerical algorithms involving piecewise polynomial interpolants and Chebyshev polynomials, and this is where the name \"Cheb\" comes from. The package aims to combine the feel of symbolic computing systems like Maple and Mathematica with the speed of floating-point numerics.\n\nThe Chebfun project is based in the Mathematical Institute at the University of Oxford and was initiated in 2002 by Lloyd N. Trefethen and his student Zachary Battles. The most recent version, Version 5.7.0, was released on June 2, 2017.\n\nChebfun2, a software system that extends Chebfun to two dimensions, was made publicly available on the 4th of March 2013. Following Chebfun2, Spherefun (extension to the unit sphere) and Chebfun3 (extension to three dimensions) were made publicly available in May and July 2016.\n\n\nA user may begin by initialising the variable x, on the interval [0,10], say.\n\nThis variable can now be used to perform further computations, for example, computing and plotting roots of a function:\n\nThe definite integral can be computed with:\n"}
{"id": "5042360", "url": "https://en.wikipedia.org/wiki?curid=5042360", "title": "Constructive set theory", "text": "Constructive set theory\n\nConstructive set theory is an approach to mathematical constructivism following the program of axiomatic set theory. That is, it uses the usual first-order language of classical set theory, and although of course the logic is constructive, there is no explicit use of constructive types. Rather, there are just sets, thus it can look very much like classical mathematics done on the most common foundations, namely the Zermelo–Fraenkel axioms (ZFC).\n\nIn 1973, John Myhill proposed a system of set theory based on intuitionistic logic taking the most common foundation, ZFC, and throwing away the axiom of choice (AC) and the law of the excluded middle (LEM), leaving everything else as is. However, different forms of some of the ZFC axioms which are equivalent in the classical setting are inequivalent in the constructive setting, and some forms imply LEM.\n\nThe system, which has come to be known as IZF, or Intuitionistic Zermelo–Fraenkel (ZF refers to ZFC without the axiom of choice), has the usual axioms of extensionality, pairing, union, infinity, separation and power set. The axiom of regularity is stated in the form of an axiom schema of set induction. Also, while Myhill used the axiom schema of replacement in his system, IZF usually stands for the version with collection.\n\nWhile the axiom of replacement requires the relation \"φ\" to be a function over the set \"A\" (that is, for every \"x\" in \"A\" there is associated exactly one \"y\"), the axiom of collection does not: it merely requires there be associated at least one \"y\", and it asserts the existence of a set which collects at least one such \"y\" for each such \"x\". The axiom of regularity as it is normally stated implies LEM, whereas the form of set induction does not. The formal statements of these two schemata are:\n\nformula_1\n\nformula_2\n\nAdding LEM back to IZF results in ZF, as LEM makes collection equivalent to replacement and set induction equivalent to regularity. Even without LEM, IZF's proof-theoretical power equals that of ZF.\n\nWhile IZF is based on intuitionistic rather than classical logic, it is considered impredicative. It allows formation of sets using the axiom of separation with any proposition, including ones which contain quantifiers which are not bounded. Thus new sets can be formed in terms of the universe of all sets. Additionally the power set axiom implies the existence of a set of truth values. In the presence of LEM, this set exists and has two elements. In the absence of it, the set of truth values is also considered impredicative.\n\nThe subject was begun by John Myhill to provide a formal foundation for Errett Bishop's program of constructive mathematics. As he presented it, Myhill's system CST is a constructive first-order logic with three sorts: natural numbers, functions, and sets. The system is:\n\n\nPeter Aczel's \"constructive Zermelo-Fraenkel\", or CZF, is essentially IZF with its impredicative features removed. It strengthens the collection scheme, and then drops the impredicative power set axiom and replaces it with another collection scheme. Finally the separation axiom is restricted, as in Myhill's CST. This theory has a relatively simple interpretation in a version of constructive type theory and has modest proof theoretic strength as well as a fairly direct constructive and predicative justification, while retaining the language of set theory. Adding LEM to this theory also recovers full ZF.\n\nThe collection axioms are:\n\nStrong collection schema: This is the constructive replacement for the axiom schema of replacement. It states that if φ is a binary relation between sets which is \"total\" over a certain domain set (that is, it has at least one image of every element in the domain), then there exists a set which contains at least one image under φ of every element of the domain, and only images of elements of the domain. Formally, for any formula φ:\n\nformula_3\n\nSubset collection schema: This is the constructive version of the power set axiom. Formally, for any formula φ:\n\nformula_4\n\nThis is equivalent to a single and somewhat clearer axiom of fullness: between any two sets \"a\" and \"b\", there is a set \"c\" which contains a total subrelation of any total relation between \"a\" and \"b\" that can be encoded as a set of ordered pairs. Formally:\n\nformula_5\n\nwhere the references to \"P(a,b)\" are defined by:\n\nformula_6\n\nformula_7\n\nand some set-encoding of the ordered pair <x,y> is assumed.\n\nThe axiom of fullness implies CST's axiom of exponentiation: given two sets, the collection of all total functions from one to the other is also in fact a set.\n\nThe remaining axioms of CZF are: the axioms of extensionality, pairing, union, and infinity are the same as in ZF; and set induction and predicative separation are the same as above.\n\nIn 1977 Aczel showed that CZF can be interpreted in Martin-Löf type theory, (using the propositions-as-types approach) providing what is now seen a standard model of CZF in type theory. In 1989 Ingrid Lindström showed that non-well-founded sets obtained by replacing the axiom of foundation in CZF with Aczel's anti-foundation axiom (CZFA) can also be interpreted in Martin-Löf type theory.\n\nPresheaf models for constructive set theory were introduced by Nicola Gambino in 2004. They are analogous to the Presheaf models for intuitionistic set theory developed by Dana Scott in the 1980s (which remained unpublished).\n\n\n"}
{"id": "7599249", "url": "https://en.wikipedia.org/wiki?curid=7599249", "title": "Counter-machine model", "text": "Counter-machine model\n\n\"This page supplements counter machine.\"\n\nAlthough some authors use the name \"register machine\" synonymously with \"counter machine\", this article will give details and examples of only of the most primitive speciesthe \"counter machine\"of the genus \"register machine.\"\n\nWithin the species \"counter machine\" there are a number of varieties: the models of Hermes (1954), Kaphengst (1957), Ershov (1958), Péter (1958), Minsky (1961) and Minsky (1967), Melzak (1961), Lambek (1961), Shepherdson and Sturgis (1963), and Schönhage (1980). These models will be described in more detail in the following.\n\nShepherdson and Sturgis observe that \"the proof of this universality [of digital computers to Turing machines] ... seems to have been first written down by Hermes, who showed in [7--their reference number] how an idealized computer could be programmed to duplicate the behavior of any Turing machine\" (Shepherdson and Sturgis, p. 219).\n\nShepherdson and Sturgis observe that:\n\nThe only two \"arithmetic\" instructions are\n\nThe rest of the operations are transfers from register-to-accumulator or accumulator-to-register or test-jumps.\n\nKaphengst's paper is written in German; Sheperdson and Sturgis' translation results in strange words such as \"mill\" and \"orders\".\n\nThe machine contains \"a mill\" (accumulator). Kaphengst designates his mill/accumulator with the \"infinity\" symbol but we will use \"A\" in the following description. It also contains an \"order register\" (\"order\" as in \"instruction\", not as in \"sequence\"). (This usage came from the report of Burks-Goldstine-von Neumann (1946) description of '...an Electronic Computing Instrument'.) The order/instruction register is register \"0\". And, although not clear from Sheperdson and Sturgis' exposition the model contains an \"extension register\" designated by Kaphengst \"infinity-prime\"; we will use \"E\". \nThe instructions are stored in the registers:\n\nThus this model is actually a random-access machine. In the following, \"[ r ]\" indicates \"contents of\" register r, etc.\n\nShepherdson and Sturgis remove the mill/accumulator A and reduce the Kaphengst instructions to register-to-register \"copy\", arithmetic operation \"Increment\", and \"register-to-register compare\". \"Observe that there is no decrement\". This model, almost verbatim, is to be found in Minsky (1967); see more in the section below.\n\nShepherdson and Sturgis (1963) observe that Ersov's model allows for storage of the program in the registers. They assert that Ersov's model is as follows:\n\nShepherdson and Sturgis (1963) observe that Péter's \"treatment\" (they are not too specific here) has an equivalence to the instructions shown in the following table. They comment specifically about these instructions, that:\n\nIn his inquiry into problems of Emil Post (the tag system) and Hilbert's 10th problem (Hilbert's problems, Diophantine equation) led Minsky to the following definition of:\n\nHis \"Theorem Ia\" asserts that any partial recursive function is represented by \"a program operating on \"two\" integers S1 and S2 using instructions Ij of the forms (cf Minsky (1961) p. 449):\n\nThe first theorem is the context of a second \"Theorem IIa\" that \nIn this second form the machine uses Gödel numbers to process \"the integer S\". He asserts that the first machine/model does not need to do this if it has 4 registers available to it.\n\nIf we use the context of his model, \"keeping tally\" means \"adding by successive increments\" (throwing a pebbles into) or \"subtracting by successive decrements\"; transferring means moving (not copying) the contents from hole A to hole B, and comparing numbers is self-evident. This appears to be a blend of the three base models.\n\nMelzak's physical model is holes { X, Y, Z, etc. } in the ground together with an unlimited supply of pebbles in a special hole S (Sink or Supply or both? Melzak doesn't say).\n\nThe phrases indefinitely large number of locations and finite number of counters here are important. This model is different than the Minsky model that allows for a \"finite\" number of locations with \"unbounded\" (effectively infinite) capacity for \"markers\".\n\nThe instruction is a \"single\" \"ternary operation\" he calls \"XYZ\": \n\nOf all the possible operations, some are not allowed, as shown in the table below:\n\nSome observations about the Melzak model:\n\nOriginal \"abacus\" model of Lambek (1962):\n\nLambek references Melzak's paper. He atomizes Melzak's single 3-parameter operation (really 4 if we count the instruction addresses) into a 2-parameter increment \"X+\" and 3-parameter decrement \"X-\". He also provides both an informal and \"formal\" definition of \"a program\". This form is virtually identical to the Minsky (1961) model, and has been adopted by Boolos-Burgess-Jeffrey (2002).\n\nAbacus model of Boolos-Burgess (1970, etc.), Boolos-Burgess-Jeffrey (2002):\n\nThe various editions beginning with 1970 the authors use the Lambek (1961) model of an \"infinite abacus\". This series of Wikipedia articles is using their symbolism, e.g. \" [ r ] +1 → r\" \"the contents of register identified as number 'r', plus 1, replaces the contents of [is put into] register number 'r' \".\n\nThey use Lambek's name \"abacus\" but follow Melzak's pebble-in-holes model, modified by them to a 'stones-in-boxes' model. Like the original abacus model of Lambek, their model retains the Minsky (1961) use of non-sequential instructionsunlike the \"conventional\" computer-like default sequential instruction execution, the next instruction I is contained within the instruction.\n\nObserve, however, that B-B and B-B-J do not use a variable \"X\" in the mnemonics with a specifying parameter (as shown in the Lambek version) --i.e. \"X+\" and \"X-\"but rather the instruction mnemonics specifies the registers themselves, e.g. \"2+\", or \"3-\":\n\nOn page 218 Shepherdson and Sturgis references Minsky (1961) as it appeared for them in the form of an MIT Lincoln Laboratory report:\n\nTheir model is strongly influenced by the model and the spirit of Hao Wang (1957) and his Wang B-machine (also see Post–Turing machine). They \"sum up by saying\":\n\nUnlimited Register Machine URM: This, their \"most flexible machine... consists of a denumerable sequence of registers numbered 1, 2, 3, ..., each of which can store any natural number...Each particular program, however involves only a finite number of these registers\" (p. 219). In other words, the number of registers is potentially infinite, and each register's \"size\" is infinite.\n\nThey offer the following instruction set (p. 219), and the following \"Notes\":\n\"Notes.\n\nIndeed, they show how to reduce this set further, to the following (for an infinite number of registers each of infinite size):\n\nLimited Register Machine LRM: Here they restrict the machine to a finite number of registers N, but they also allow more registers to \"be brought in\" or removed if empty (cf p. 228). They show that the remove-register instruction need not require an empty register.\n\nSingle-Register Machine SRM: Here they are implementing the tag system of Emil Post and thereby allow only writing to the end of the string and erasing from the beginning. This is shown in their Figure 1 as a tape with a read head on the left and a write head on the right, and it can only move the tape right. \"A\" is their \"word\" (p. 229):\n\nThey also provide a model as \"a stack of cards\" with the symbols { 0, 1 } (p. 232 and Appendix C p. 248):\n\nUltimately, in Problem 11.7-1 Minsky observes that many bases of computation can be formed from a tiny collection:\n\nThe following are definitions of the various instructions he treats:\nMinsky (1967) begins with a model that consists of the three operations plus HALT:\n\nHe observes that we can dispense with [ 0 ] if we allow for a specific register e.g. w already \"empty\" (Minsky (1967) p. 206). Later (pages 255ff) he compresses the three { [ 0 ], [ ' ], [ - ] }, into two { [ ' ], [ - ] }.\n\nBut he admits the model is easier if he adds some [pseudo]-instructions [ O- ] (combined [ 0 ] and [ - ]) and \"go(n)\". He builds \"go(n)\" out of the register w pre-set to 0, so that [O-] (w, (n)) is an unconditional jump.\n\nIn his section 11.5 \"The equivalence of Program Machines with General-recursive functions\" he introduces two new subroutines:\n\nHe proceeds to show how to replace the \"successor-predecessor\" set { [ 0 ], [ ' ], [ - ] } with the \"successor-equality\" set { [ 0 ], [ ' ], [ ≠ ] }. And then he defines his \"REPEAT\" [RPT] and shows that we can define any primitive recursive function by the \"successor-repeat\" set { [ 0 ], [ ' ], [RPT] } (where the range of the [ RPT ] cannot include itself. If it does, we get what is called the mu operator (see also mu recursive functions ) (p. 213)):\n\nSchönhage (1980) developed his computational model in context of a \"new\" model he called the Storage Machine Modification model (SMM), his variety of pointer machine. His development described a RAM (Random-access machine) model with a remarkable instruction set requiring no operands at all, excepting, perhaps, the \"conditional jump\" (and even that could be achieved without an operand):\n\nThe way Schönhage did this is of interest. He (i) atomizes the conventional register \"address:datum\" into its two parts: \"address\", and \"datum\", and (ii) generates the \"address\" in a specific register n to which the finite-state machine instructions (i.e. the \"machine code\") would have access, and (iii) provides an \"accumulator\" register z where all arithmetic operations are to occur.\n\nIn his particular RAM0 model has only two \"arithmetic operations\"\"Z\" for \"set contents of register z to zero\", and \"A\" for \"add one to contents of register z\". The only access to address-register n is via a copy-from-A-to-N instruction called \"set address n\". To store a \"datum\" in accumulator z in a given register, the machine uses the contents of n to specify the register's address and register z to supply the datum to be sent to the register.\nPeculiarities: A first peculiarity of the Schönhage RAM0 is how it \"loads\" something into register z: register z first supplies the register-address and then secondly, receives the datum from the registera form of indirect \"load\". The second peculiarity is the specification of the COMPARE operation. This is a \"jump if accumulator-register z=\"zero\" (not, for example, \"compare the contents of z to the contents of the register pointed to by n). Apparently if the test fails the machine skips over the next instruction which always must be in the form of \"goto λ\" where \"λ\" is the jump-to address. The instruction\"compare contents of z to \"zero\"\" is unlike the Schonhage successor-RAM1 model (or any other known successor-models) with the more conventional \"compare contents of register z to contents of register a for equality\".\n\nPrimarily for referencethis is a RAM model, not a counter-machine modelthe following is the Schönhage RAM0 instruction set:\n\nAgain, the above instruction set is for a \"random-access machine\", a \"RAM\"a counter machine with indirect addressing; instruction \"N\" allows for indirect storage of the accumulator, and instruction \"L\" allows for indirect load of the accumulator.\n\nWhile peculiar, Schönhage's model shows how the conventional counter-machine's \"register-to-register\" or \"read-modify-write\" instruction set can be atomized to its simplest 0-parameter form. \n"}
{"id": "14093130", "url": "https://en.wikipedia.org/wiki?curid=14093130", "title": "DEVS", "text": "DEVS\n\nDEVS abbreviating Discrete Event System Specification is a modular and hierarchical formalism for modeling and analyzing general systems that can be discrete event systems which might be described by state transition tables, and continuous state systems which might be described by differential equations, and hybrid continuous state and discrete event systems. DEVS is a timed event system.\n\nDEVS is a formalism for modeling and analysis of discrete event systems (DESs). The DEVS formalism was invented by Bernard P. Zeigler, who is emeritus professor at the University of Arizona. DEVS was introduced to the public in Zeigler's first book, \"Theory of Modeling and Simulation\", in 1976, while Zeigler was an associate professor at University of Michigan. DEVS can be seen as an extension of the Moore machine formalism, which is a finite state automaton where the outputs are determined by the current state alone (and do not depend directly on the input). The extension was done by \n\nSince the lifespan of each state is a real number (more precisely, non-negative real) or infinity, it is distinguished from discrete time systems, sequential machines, and Moore machines, in which time is determined by a tick time multiplied by non-negative integers. Moreover, the lifespan can be a random variable; for example the lifespan of a given state can be distributed exponentially or uniformly. The state transition and output functions of DEVS can also be stochastic.\n\nZeigler proposed a hierarchical algorithm for DEVS model simulation in 1984 [Zeigler84] which was published in \"Simulation\" journal in 1987. Since then, many extended formalism from DEVS have been introduced with their own purposes: DESS/DEVS for combined continuous and discrete event systems, P-DEVS for parallel DESs, G-DEVS for piecewise continuous state trajectory modeling of DESs, RT-DEVS for realtime DESs, Cell-DEVS for cellular DESs, Fuzzy-DEVS for fuzzy DESs, Dynamic Structuring DEVS for DESs changing their coupling structures dynamically, and so on. In addition to its extensions, there are some subclasses such as SP-DEVS and FD-DEVS have been researched for achieving decidability of system properties.\n\nDue to the modular and hierarchical modeling views, as well as its simulation-based analysis capability, the DEVS formalism and its variations have been used in many application of engineering (such as hardware design, hardware/software codesign, communications systems, manufacturing systems) and science (such as biology, and sociology)\n\nDEVS defines system behavior as well as system structure. System behavior in DEVS formalism is described using input and output events as well as states. For example, for the ping-pong player of Fig. 1, the input event is \"?receive\", and the output event is \"!send\". Each player, \"A\", \"B\", has its states: \"Send\" and \"Wait\". \"Send\" state takes 0.1 seconds to send back the ball that is the output event \"!send\", while the \"Wait\" state lasts until the player receives the ball that is the input event \"?receive\".\n\nThe structure of ping-pong game is to connect two players: Player \"A\" 's output event \"!send\" is transmitted to Player \"B\" 's input event \"?receive\", and vice versa.\n\nIn the classic DEVS formalism, \"Atomic DEVS\" captures the system behavior, while \"Coupled DEVS\" describes the structure of system.\n\nThe following formal definition is for Classic DEVS [ZKP00]. In this article, we will use the time base, formula_1 that is the set of non-negative real numbers; the extended time base,formula_2 that is the set of non-negative real numbers plus infinity.\n\nAn atomic DEVS model is defined as a 7-tuple \n\nwhere \n\n\n\nThe atomic DEVS model for player A of Fig. 1 is given \nPlayer=formula_16 \nsuch that \n\nformula_17\nBoth Player A and Player B are atomic DEVS models.\n\nSimply speaking, there are two cases that an atomic DEVS model formula_18 can change its state formula_19: (1) when an external input formula_20 comes into the system formula_18; (2) when the elapsed time formula_22 reaches the lifespan of formula_23 which is defined by formula_24. (At the same time of (2), formula_18 generates an output formula_26 which is defined by formula_27.) .\n\nFor formal behavior description of given an Atomic DEVS model, refer to the page Behavior of DEVS. Computer algorithms to implement the behavior of a given Atomic DEVS model are available at Simulation Algorithms for Atomic DEVS.\n\nThe coupled DEVS defines which sub-components belong to it and how they are connected with each other. A coupled DEVS model is defined as an 8-tuple\n\nwhere\n\nThe ping-pong game of Fig. 1 can be modeled as a coupled DEVS model formula_38 where formula_39;formula_40;formula_41; formula_42 is described as above; formula_43; formula_44; and formula_45.\n\nSimply speaking, like the behavior of the atomic DEVS class, a coupled DEVS model formula_46 changes its components' states (1) when an external event formula_47 comes into formula_46; (2) when one of components formula_49 where formula_50 executes its internal state transition and generates its output formula_51. In both cases (1) and (2), a triggering event is transmitted to all influencees which are defined by coupling sets formula_52 and formula_53.\n\nFor formal definition of behavior of the coupled DEVS, you can refer to Behavior of Coupled DEVS. Computer algorithms to implement the behavior of a given coupled DEVS mode are available at Simulation Algorithms for Coupled DEVS.\n\nThe simulation algorithm of DEVS models considers two issues: time synchronization and message propagation. \"Time synchronization\" of DEVS is to control all models to have the identical current time. However, for an efficient execution, the algorithm makes the current time jump to the most urgent time when an event is scheduled to execute its internal state transition as well as its output generation. \"Message propagation\" is to transmit a triggering message which can be either an input or output event along the associated couplings which are defined in a coupled DEVS model. For more detailed information, the reader can refer to Simulation Algorithms for Atomic DEVS and Simulation Algorithms for Coupled DEVS.\n\nBy introducing a quantization method which abstracts a continuous segment as a piecewise const segment, DEVS can simulate behaviors of continuous state systems which are described by networks of differential algebraic equations. This research has been initiated by Zeigler in 90's and many properties have been clarified by Prof. Kofman in 2000's and Dr. Nutaro. In 2006, Prof. Cellier who is the author of \"Continuous System Modeling\"[Cellier91], and Prof. Kofman wrote a text book, \"Continuous System Simulation\"[CK06] in which Chapters 11 and 12 cover how DEVS simulates continuous state systems. Dr. Nutaro's book [Nutaro10], covers the discrete event simulation of continuous state systems too.\n\nAs an alternative analysis method against the sampling-based simulation method, an exhaustive generating behavior approach, generally called \"verification\" has been applied for analysis of DEVS models. It is proven that infinite states of a given DEVS model (especially a coupled DEVS model ) can be abstracted by behaviorally isomorphic finite structure, called a \"reachability graph\" when the given DEVS model is a sub-class of DEVS such as Schedule-Preserving DEVS (SP-DEVS), Finite & Deterministic DEVS (FD-DEVS) [HZ09], and Finite & Real-time DEVS (FRT-DEVS) [Hwang12]. As a result, based on the rechability graph, (1) dead-lock and live-lock freeness as qualitative properties are decidable with SP-DEVS [Hwang05], FD-DEVS [HZ06], and FRT-DEVS [Hwang12]; and (2) min/max processing time bounds as a quantitative property are decidable with SP-DEVS so far by 2012.\n\nNumerous extensions of the classic DEVS formalism have been developed in the last decades.\nAmong them formalisms which allow to have changing model structures while the simulation time evolves.\n\nG-DEVS [Giambiasi01][Zacharewicz08], Parallel DEVS, Dynamic Structuring DEVS, Cell-DEVS [Wainer09], dynDEVS, Fuzzy-DEVS, GK-DEVS, ml-DEVS, Symbolic DEVS, Real-Time DEVS, rho-DEVS\n\nThere are some sub-classes known as Schedule-Preserving DEVS (SP-DEVS) and Finite and Deterministic DEVS (FD-DEVS) which were designated to support verification analysis.\nSP-DEVS and FD-DEVS whose expressiveness are \"E\"(SP-DEVS) formula_54 \"E\"(FD-DEVS)formula_54 \"E\"(DEVS) where \"E\"(\"formalism\") denotes the expressiveness of \"formalism\".\n\n\n\n"}
{"id": "52815653", "url": "https://en.wikipedia.org/wiki?curid=52815653", "title": "Fangcheng (mathematics)", "text": "Fangcheng (mathematics)\n\nFangcheng (sometimes written as fang-cheng or fang cheng) () is the title of the eighth chapter of the Chinese mathematical classic Jiuzhang suanshu (The Nine Chapters on the Mathematical Art) composed by several generations of scholars who flourished during the period from the 10th to the 2nd century BCE. This text is one of the earliest surviving mathematical texts from China. Several historians of Chinese mathematics have observed that the term \"fangcheng\" is not easy to translate exactly. However, as a first approximation it has been translated as \"rectangular arrays\" or \"square arrays\". The term is also used to refer to a particular procedure for solving a certain class of problems discussed in the Chapter 8 of The Nine Chapters book.\n\nThe procedure referred to by the term \"fangcheng\" and explained in the eighth chapter of The Nine Chapters, is essentially a procedure to find the solution of systems of \"n\" equations in \"n\" unknowns and is equivalent to certain similar procedures in modern linear algebra. The earliest recorded \"fangcheng\" procedure is similar to what we now call Gaussian elimination.\n\nThe \"fangcheng\" procedure was popular in ancient China and was transmitted to Japan. It is possible that this procedure was transmitted to Europe also and served as precursors of the modern theory of matrices, Gaussian elimination, and determinants. It is well known that there was not much work on linear algebra in Greece or Europe prior to Gottfried Leibniz’s studies of elimination and determinants, beginning in 1678. Moreover Leibniz was a Sinophile and was interested in the translations of such Chinese texts as were available to him.\n\nThere is no ambiguity in the meaning of the first character \"fang\". It means “rectangle” or “square.” But different interpretations are given to the second character \"cheng\":\n\n\nSince the end of the 19th century, in Chinese mathematical literature the term \"fangcheng\" has been used to denote an \"equation.\" However, as already been noted, the traditional meaning of the term is very different from \"equation.\"\n\nThe eighth chapter titled \"Fangcheng\" of the \"Nine Chapters\" book contains 18 problems. (There are a total of 288 problems in the whole book.) Each of these 18 problems reduces to a problem of solving a system of simultaneous linear equations. Except for one problem, namely Problem 13, all the problems are determinate in the sense that the number of unknowns is same as the number of equations. There are problems involving 2, 3, 4 and 5 unknowns. The table below shows how many unknowns are there in the various problems:\nTable showing the number of unknowns and number of equations <br> in the various problems in Chapter 8 of \"Nine Chapters\" \n\nThe presentations of all the 18 problems (except Problem 1 and Problem 3) follow a common pattern:\n\n\nThe presentation of Problem 1 contains a description (not a crisp indication) of the procedure for obtaining the solution. The procedure has been referred to as \"fangcheng shu\", which means \"\"fangcheng\" procedure.\" The remaining problems all give the instruction \"follow the \"fangcheng\"\" procedure sometimes followed by the instruction to use the \"procedure for positive and negative numbers\".\n\nThere is also a special procedure, called \"procedure for positive and negative numbers\" (\"zhenh fu shu\") for handling negative numbers. This procedure is explained as part of the method for solving Problem 3.\n\nIn the collection of these 18 problems Problem 13 is very special. In it there are 6 unknowns but only 5 equations and so Problem 13 is indeterminate and does not have a unique solution. This is the earliest known reference to a system of linear equations in which the number of unknowns exceeds the number of equations. As per a suggestion of Jean-Claude Martzloff, a historian of Chinese mathematics, Roger Hart has named this problem \"the well problem.\"\n\n"}
{"id": "1548123", "url": "https://en.wikipedia.org/wiki?curid=1548123", "title": "Fractional-order integrator", "text": "Fractional-order integrator\n\nA fractional-order integrator or just simply fractional integrator is an integrator device that calculates the fractional-order integral or derivative (usually called a differintegral) of an input. Differentiation or integration is a real or complex parameter. The fractional integrator is useful in fractional-order control where the history of the system under control is important to the control system output.\n\nThe differintegral function,\n\nincludes the integer order differentiation and integration functions, and allows a continuous range of functions around them. The differintegral parameters are \"a\", \"t\", and \"q\". The parameters \"a\" and \"t\" describe the range over which to compute the result. The differintegral parameter \"q\" may be any real number or complex number. If \"q\" is greater than zero, the differintegral computes a derivative. If \"q\" is less than zero, the differintegral computes an integral.\nThe integer order integration can be computed as a Riemann–Liouville differintegral, where the weight of each element in the sum is the constant unit value 1, which is equivalent to the Riemann sum. To compute an integer order derivative, the weights in the summation would be zero, with the exception of the most recent data points, where (in the case of the first unit derivative) the weight of the data point at \"t\" − 1 is −1 and the weight of the data point at \"t\" is 1. The sum of the points in the input function using these weights results in the difference of the most recent data points.\nThese weights are computed using ratios of the Gamma function incorporating the number of data points in the range [\"a\",\"t\"], and the parameter \"q\".\n\nDigital devices have the advantage of being versatile, and are not susceptible to unexpected output variation due to heat or noise. The discrete nature of a computer however, does not allow for all of history to be computed. Some finite range [a,t] must exist. Therefore, the number of data points that can be stored in memory (\"N\"), determines the oldest data point in memory, so that the value a is never more than \"N\" samples old. The effect is that any history older than a is \"completely\" forgotten, and no longer influences the output.\n\nA solution to this problem is the Coopmans approximation, which allows old data to be forgotten more gracefully (though still with exponential decay, rather than with the power law decay of a purely analog device).\n\nAnalog devices have the ability to retain history over longer intervals. This translates into the parameter a staying constant, while \"t\" increases. \n\nThere is no error due to round-off, as in the case of digital devices, but there may be error in the device due to leakages, and also unexpected variations in behavior caused by heat and noise.\n\nAn example fractional-order integrator is a modification of the standard integrator circuit, where a capacitor is used as the feedback impedance on an opamp. By replacing the capacitor with an RC Ladder circuit, a half order integrator, that is, with\n\ncan be constructed.\n\n"}
{"id": "318413", "url": "https://en.wikipedia.org/wiki?curid=318413", "title": "French Institute for Research in Computer Science and Automation", "text": "French Institute for Research in Computer Science and Automation\n\nThe National Institute for Research in Computer Science and Automation (INRIA) () is a French national research institution focusing on computer science and applied mathematics.\nIt was created under the name \"Institut de recherche en informatique et en automatique\" (IRIA) in 1967 at Rocquencourt near Paris, part of Plan Calcul. Its first site was the historical premises of SHAPE (central command of NATO military forces). In 1979 IRIA became INRIA. Since 2011, it has been styled \"inria\".\n\nInria is a Public Scientific and Technical Research Establishment (EPST) under the double supervision of the French Ministry of National Education, Advanced Instruction and Research and the Ministry of Economy, Finance and Industry.\n\nInria has 8 research centers (in Bordeaux, Grenoble-Inovallée, Lille, Nancy, Paris-Rocquencourt, Rennes, Saclay, and Sophia Antipolis) and also contributes to academic research teams outside of those centers.\n\nBefore December 2007, the three centers of Bordeaux, Lille and Saclay formed a single research center called INRIA Futurs.\n\nIn October 2010, INRIA, with Pierre and Marie Curie University and Paris Diderot University started IRILL, a center for innovation and research initiative for free software.\n\nInria employs 3800 people. Among them are 1300 researchers, 1000 Ph.D. students and 500 postdoctorates.\n\nInria does both theoretical and applied research in computer science. In the process, it has produced many widely used programs, such as \n\n"}
{"id": "2336226", "url": "https://en.wikipedia.org/wiki?curid=2336226", "title": "Hilbert's nineteenth problem", "text": "Hilbert's nineteenth problem\n\nHilbert's nineteenth problem is one of the 23 Hilbert problems, set out in a list compiled in 1900 by David Hilbert. It asks whether the solutions of regular problems in the calculus of variations are always analytic. Informally, and perhaps less directly, since Hilbert's concept of a \"regular variational problem\"\" identifies precisely a variational problem whose Euler–Lagrange equation is an elliptic partial differential equation with analytic coefficients. Hilbert's nineteenth problem, despite its seemingly technical statement, simply asks whether, in this class of partial differential equations, any solution function inherits the relatively simple and well understood structure from the solved equation.\n\nDavid Hilbert presented the now called Hilbert's nineteen problem in his speech at the second International Congress of Mathematicians. In he states that, in his opinion, one of the most remarkable facts of the theory of analytic functions is that there exist classes of partial differential equations which admit only such kind of functions as solutions, adducing Laplace's equation, Liouville's equation, the minimal surface equation and a class of linear partial differential equations studied by Émile Picard as examples. He then notes the fact that most of the partial differential equations sharing this property are the Euler–Lagrange equation of a well defined kind of variational problem, featuring the following three properties:\nHilbert calls this kind of variational problem a \"\"regular variational problem\": property means that such kind of variational problems are minimum problems, property is the ellipticity condition on the Euler–Lagrange equations associated to the given functional, while property is a simple regularity assumption the function . Having identified the class of problems to deal with, he then poses the following question:-\"\"... does every Lagrangian partial differential equation of a regular variation problem have the property of admitting analytic integrals exclusively?\" and asks further if this is the case even when the function is required to assume, as it happens for Dirichlet's problem on the potential function, boundary values which are continuous, but not analytic.\n\nHilbert stated his nineteenth problem as a regularity problem for a class of elliptic partial differential equation with analytic coefficients, therefore the first efforts of the researchers who sought to solve it were directed to study the regularity of classical solutions for equations belonging to this class. For solutions Hilbert's problem was answered positively by in his thesis: he showed that solutions of nonlinear elliptic analytic equations in 2 variables are analytic. Bernstein's result was improved over the years by several authors, such as , who reduced the differentiability requirements on the solution needed to prove that it is analytic. On the other hand, direct methods in the calculus of variations showed the existence of solutions with very weak differentiability properties. For many years there was a gap between these results: the solutions that could be constructed were known to have square integrable second derivatives, which was not quite strong enough to feed into the machinery that could prove they were analytic, which needed continuity of first derivatives. This gap was filled independently by , and . They were able to show the solutions had first derivatives that were Hölder continuous, which by previous results implied that the solutions are analytic whenever the differential equation has analytic coefficients, thus completing the solution of Hilbert's nineteenth problem.\n\nThe affirmative answer to Hilbert's nineteenth problem given by Ennio De Giorgi and John Forbes Nash raised the question if the same conclusion holds also for Euler-lagrange equations of more general functionals: at the end of the sixties, , and constructed independently several counterexamples, showing that in general there is no hope to prove such kind of regularity results without adding further hypotheses.\n\nPrecisely, gave several counterexamples involving a single elliptic equation of order greater than two with analytic coefficients: for experts, the fact that such kind of equations could have nonanalytic and even nonsmooth solutions created a sensation.\n\nThe key theorem proved by De Giorgi is an a priori estimate stating that if \"u\" is a solution of a suitable linear second order strictly elliptic PDE of the form\nand \"u\" has square integrable first derivatives, then \"u\" is Hölder continuous.\n\nHilbert's problem asks whether the minimizers formula_4 of an energy functional such as\nare analytic. Here formula_4 is a function on some compact set formula_7 of R, formula_8 is its gradient vector, and formula_9 is the Lagrangian, a function of the derivatives of formula_4 that satisfies certain growth, smoothness, and convexity conditions. The smoothness of formula_4 can be shown using De Giorgi's theorem\nas follows. The Euler–Lagrange equation for this variational problem is the non-linear equation\nand differentiating this with respect to \"x\" gives\nThis means that \"u\"=\"w\" satisfies the linear equation\nwith \nso by De Giorgi's result the solution \"w\" has Hölder continuous first derivatives.\n\nOnce \"w\" is known to have Hölder continuous (\"n\"+1)st derivatives for some \"n\" ≥ 0, then the coefficients \"a\" have Hölder continuous \"n\"th derivatives, so a theorem of Schauder implies that the (\"n\"+2)nd derivatives are also Hölder continuous, so repeating this infinitely often shows that the solution \"w\" is smooth.\n\nNash gave a continuity estimate for solutions of the parabolic equation\nwhere \"u\" is a bounded function of \"x\"...,\"x\", \"t\" defined for \"t\" ≥ 0. From his estimate Nash was able to deduce a continuity estimate for solutions of the elliptic equation \n\n"}
{"id": "33800100", "url": "https://en.wikipedia.org/wiki?curid=33800100", "title": "Hilbert–Burch theorem", "text": "Hilbert–Burch theorem\n\nIn mathematics, the Hilbert–Burch theorem describes the structure of some free resolutions of a quotient of a local or graded ring in the case that the quotient has projective dimension 2. proved a version of this theorem for polynomial rings, and proved a more general version. Several other authors later rediscovered and published variations of this theorem. gives a statement and proof.\n\nIf \"R\" is a local ring with an ideal \"I\" and \nis a free resolution of the \"R\"-module \"R\"/\"I\", then \"m\" = \"n\" – 1 and the ideal \"I\" is \"aJ\" where \"a\" is a regular element of \"R\" and \"J\", a depth-2 ideal, is the first Fitting ideal formula_2 of \"I\", i.e., the ideal generated by the determinants of the minors of size \"m\" of the matrix of \"f\".\n\n"}
{"id": "523968", "url": "https://en.wikipedia.org/wiki?curid=523968", "title": "Ideal number", "text": "Ideal number\n\nIn number theory an ideal number is an algebraic integer which represents an ideal in the ring of integers of a number field; the idea was developed by Ernst Kummer, and led to Richard Dedekind's definition of ideals for rings. An ideal in the ring of integers of an algebraic number field is \"principal\" if it consists of multiples of a single element of the ring, and \"nonprincipal\" otherwise. By the principal ideal theorem any nonprincipal ideal becomes principal when extended to an ideal of the Hilbert class field. This means that there is an element of the ring of integers of the Hilbert class field, which is an ideal number, such that the original nonprincipal ideal is equal to the collection of all multiples of this ideal number by elements of this ring of integers that lie in the original field's ring of integers.\n\nFor instance, let \"y\" be a root of \"y\" + \"y\" + 6 = 0, then the ring of integers of the field formula_1 is formula_2, which means all \"a\" + \"by\" with \"a\" and \"b\" integers form the ring of integers. An example of a nonprincipal ideal in this ring is the set of all 2\"a\" + \"yb\" where \"a\" and \"b\" are integers; the cube of this ideal is principal, and in fact the class group is cyclic of order three. The corresponding class field is obtained by adjoining an element \"w\" satisfying \"w\" − \"w\" − 1 = 0 to formula_1, giving formula_4. An ideal number for the nonprincipal ideal 2\"a\" + \"yb\" is formula_5. Since this satisfies the equation\nformula_6 it is an algebraic integer.\n\nAll elements of the ring of integers of the class field which when multiplied by ι give a result in formula_2 are of the form \"a\"α + \"b\"β, where\n\nand\n\nThe coefficients α and β are also algebraic integers, satisfying\n\nand\n\nrespectively. Multiplying \"a\"α + \"b\"β by the ideal number ι gives 2\"a\" + \"by\", which is the nonprincipal ideal.\n\nKummer first published the failure of unique factorization in cyclotomic fields in 1844 in an obscure journal; it was reprinted in 1847 in Liouville's journal. In subsequent papers in 1846 and 1847 he published his main theorem, the unique factorization into (actual and ideal) primes.\n\nIt is widely believed that Kummer was led to his \"ideal complex numbers\" by his interest in Fermat's Last Theorem; there is even a story often told that Kummer, like Lamé, believed he had proven Fermat's Last Theorem until Lejeune Dirichlet told him his argument relied on unique factorization; but the story was first told by Kurt Hensel in 1910 and the evidence indicates it likely derives from a confusion by one of Hensel's sources. Harold Edwards says the belief that Kummer was mainly interested in Fermat's Last Theorem \"is surely mistaken\" (Edwards 1977, p. 79). Kummer's use of the letter λ to represent a prime number, α to denote a λth root of unity, and his study of the factorization of prime number formula_12 into \"complex numbers composed of formula_13th roots of unity\" all derive directly from a paper of Jacobi which is concerned with higher reciprocity laws. Kummer's 1844 memoir was in honor of the jubilee celebration of the University of Königsberg and was meant as a tribute to Jacobi. Although Kummer had studied Fermat's Last Theorem in the 1830s and was probably aware that his theory would have implications for its study, it is more likely that the subject of Jacobi's (and Gauss's) interest, higher reciprocity laws, held more importance for him. Kummer referred to his own partial proof of Fermat's Last Theorem for regular primes as \"a curiosity of number theory rather than a major item\" and to the higher reciprocity law (which he stated as a conjecture) as \"the principal subject and the pinnacle of contemporary number theory.\" On the other hand, this latter pronouncement was made when Kummer was still excited about the success of his work on reciprocity and when his work on Fermat's Last Theorem was running out of steam, so it may perhaps be taken with some skepticism.\n\nThe extension of Kummer's ideas to the general case was accomplished independently by Kronecker and Dedekind during the next forty years. A direct generalization encountered formidable difficulties, and it eventually led Dedekind to the creation of the theory of modules and ideals. Kronecker dealt with the difficulties by developing a theory of forms (a generalization of quadratic forms) and a theory of divisors. Dedekind's contribution would become the basis of ring theory and abstract algebra, while Kronecker's would become major tools in algebraic geometry.\n\n\n"}
{"id": "506177", "url": "https://en.wikipedia.org/wiki?curid=506177", "title": "Index of fractal-related articles", "text": "Index of fractal-related articles\n\nThis is a list of fractal topics, by Wikipedia page, See also list of dynamical systems and differential equations topics.\n\n"}
{"id": "13516635", "url": "https://en.wikipedia.org/wiki?curid=13516635", "title": "International Symposium on Graph Drawing", "text": "International Symposium on Graph Drawing\n\nThe International Symposium on Graph Drawing (GD) is an annual academic conference in which researchers present peer reviewed papers on graph drawing, information visualization of network information, geometric graph theory, and related topics.\n\nThe Graph Drawing symposia have been central to the growth and development of graph drawing as a research area: as Herman et al. write, \"the Graph Drawing community grew around the yearly Symposia.\" Nguyen lists Graph Drawing as one of \"several good conferences which directly or indirectly concern with information visualization\", and Wong et al. report that its proceedings \"provide a wealth of information\". In a 2003 study the symposium was among the top 30% of computer science research publication venues, ranked by impact factor.\n\nThe first symposium was held in Marino, near Rome, Italy, in 1992, organized by Giuseppe Di Battista, Peter Eades, Pierre Rosenstiehl, and Roberto Tamassia. The first two symposia did not publish proceedings, but reports are available online. Since 1994, the proceedings of the symposia have been published by Springer-Verlag's Lecture Notes in Computer Science series.\n\nCountries in which the conference has been held include Australia, Austria, Canada, the Czech Republic, France, Germany (twice), Greece, Ireland, Italy (three times), and the United States (five times).\n\nA citation graph having vertices representing the papers in the 1994–2000 Graph Drawing symposia and having edges representing citations between these papers was made available as part of the graph drawing contest associated with the 2001 symposium. \nThe largest connected component of this graph consists of 249 vertices and 642 edges; clustering analysis reveals several prominent subtopics within graph drawing that are more tightly connected, including three-dimensional graph drawing and orthogonal graph drawing.\n\n\n"}
{"id": "1232678", "url": "https://en.wikipedia.org/wiki?curid=1232678", "title": "Johannes Widmann", "text": "Johannes Widmann\n\nJohannes Widmann (c. 1460 – after 1498) was a German mathematician. The + and - symbols first appeared in print in his book \"Mercantile Arithmetic or Behende und hüpsche Rechenung auff allen Kauffmanschafft\" published in Leipzig in 1489 in reference to surpluses and deficits in business problems.\n\nBorn in Eger, Bohemia, Widmann attended the University of Leipzig in the 1480s. In 1482 he earned his \"Baccalaureus\" (Bachelor of Art degree) and in 1485 his \"Magister\" (doctorate).\n\nWidman published \"Behende und hübsche Rechenung auff allen Kauffmanschafft\" (German; i.e. Nimble and neat calculation in all trades), his work making use of the signs, in Leipzig in 1489. Further editions were published in Pforzheim, Hagenau, and Augsburg.\nHandwritten entries in a surviving collection show that after earning his \"Magister\" Widman announced holding lectures on e.g. calculating on the lines of a calculating board and on algebra. There is evidence that the lecture on algebra actually took place, making it the first known university lecture on this topic.\n\nAround 1495 Widmann published the Latin writings \"Algorithmus integrorum cum probis annexis\", \"Algorithmus linealis\", \"Algorithmus minutiarum phisicarum\", \"Algorithmus minutiarum vulgarium\", \"Regula falsi apud philosophantes augmenti et decrementi appellata und Tractatus proportionum plusquam aureus\".\n\nHe died in Leipzig.\n\nWhen Adam Ries was in Erfurt between 1518 and 1522 he got to know Widmann's algebra lecture script (today in the Saxon State Library) wherefrom he took examples for his own writings.\n\n"}
{"id": "10804313", "url": "https://en.wikipedia.org/wiki?curid=10804313", "title": "John Reif", "text": "John Reif\n\nJohn H. Reif (born 1951) is an American academic, and Professor of Computer Science at Duke University, who has made contributions to large number of fields in computer science: ranging from algorithms and computational complexity theory to robotics and to game theory.\n\nJohn Reif received a B.S. (magna cum laude) from Tufts University in 1973, a M.S. from Harvard University in 1975 and a Ph.D. from Harvard University in 1977.\n\nFrom 1983 to 1986 he was Associate Professor of Harvard University, and since 1986 he has been Professor of Computer Science at Duke University. Currently he holds the Hollis Edens Distinguished Professor, Trinity College of Arts and Sciences, Duke University. From 2011-2014 he was Distinguished Adjunct Professor, Faculty of Computing and Information Technology (FCIT), King Abdulaziz University (KAU), Jeddah, Saudi Arabia.\n\nJohn Reif is President of Eagle Eye Research, Inc., which specializes in defense applications of DNA biotechnology. He has also contributed to bringing together various disjoint research communities working in different areas of nano-sciences by organizing (as General Chairman) annual Conferences on \"Foundations of Nanoscience: Self-assembled architectures and devices\" (FNANO) for last 15 years.\n\nHe has been awarded Fellow of the following organizations: American Association for the Advancement of Science, IEEE, ACM, and the Institute of Combinatorics.\n\nHe is the son of Arnold E. Reif.\n\nJohn Reif has made contributions to large number of fields in computer science: ranging from algorithms and computational complexity theory to robotics and to game theory. He developed efficient randomized algorithms and parallel algorithms for a wide variety of graph, geometric, numeric, algebraic, and logical problems. His Google Scholar H-index is 68.\n\nIn the area of robotics, he gave the first hardness proofs for robotic motion planning as well as efficient algorithms for a wide variety of motion planning problems.\n\nHe also has led applied research projects: parallel programming languages (Proteus System for parallel programming), parallel architectures (Blitzen, a massively parallel machine), data compression (massively parallel loss-less compression hardware), and optical computing (free-space holographic routing). His papers on these algorithmic topics can be downloaded here.\n\nMore recently, he has centered his research in nanoscience and in particular DNA nanotechnology, DNA computing, and DNA nanorobotics. In the last dozen years his group at Duke has designed and experimentally demonstrated in the lab a variety of novel self-assembled DNA nanostructures and DNA lattices, including the first experimental demonstrations of molecular scale computation and patterning using DNA assembly. His group also experimentally demonstrated various molecular robotic devices composed of DNA, including one of the first autonomous unidirectional DNA walker that walked on a DNA track. He also has done significant work on controlling errors in self-assembly and the stochastic analysis of self-assembly.\n\n\nHe is the author of over 200 publications. A selection:\n\n\n"}
{"id": "1516033", "url": "https://en.wikipedia.org/wiki?curid=1516033", "title": "Kummer's function", "text": "Kummer's function\n\nIn mathematics, there are several functions known as Kummer's function. One is known as the confluent hypergeometric function of Kummer. Another one, defined below, is related to the polylogarithm. Both are named for Ernst Kummer.\n\nKummer's function is defined by\n\nThe duplication formula is\n\nCompare this to the duplication formula for the polylogarithm:\n\nAn explicit link to the polylogarithm is given by\n"}
{"id": "42074715", "url": "https://en.wikipedia.org/wiki?curid=42074715", "title": "Malmquist's theorem", "text": "Malmquist's theorem\n\nIn mathematics, Malmquist's theorem, is the name of any of the three theorems proved by . These theorems restrict the forms of first order algebraic differential equations which have transcendental meromorphic or algebroid solutions.\n\nTheorem (1913). If the differential equation\nwhere \"R\"(\"z\",\"w\") is a rational function, has a transcendental meromorphic solution, then \"R\" is a polynomial of degree at most 2 with respect to \"w\"; in other words the differential equation is a Riccati equation, or linear.\n\nTheorem (1920). If an irreducible differential equation\nwhere \"F\" is a polynomial, has a transcendental meromorphic solution, then the equation has no movable singularities. Moreover, it can be algebraically reduced either to a Riccati equation or to\nwhere \"P\" is a polynomial of degree \"3\" with respect to \"w\".\n\nTheorem (1941). If an irreducible differential equation\nwhere \"F\" is a polynomial, has a transcendental algebroid solution, then it can be algebraically reduced to an equation that has no movable singularities.\n\nA modern account of theorems 1913, 1920 is given in the paper of A. Eremenko(1982)\n\n"}
{"id": "13398693", "url": "https://en.wikipedia.org/wiki?curid=13398693", "title": "Marginal model", "text": "Marginal model\n\nIn statistics, marginal models (Heagerty & Zeger, 2000) are a technique for obtaining regression estimates in multilevel modeling, also called hierarchical linear models.\nPeople often want to know the effect of a predictor/explanatory variable \"X\", on a response variable \"Y\". One way to get an estimate for such effects is through regression analysis.\n\nIn a typical multilevel model, there are level 1 & 2 residuals (R and U variables). The two variables form a joint distribution for the response variable (formula_1). In a marginal model, we collapse over the level 1 & 2 residuals and thus \"marginalize\" (see also conditional probability) the joint distribution into a univariate normal distribution. We then fit the marginal model to data.\n\nFor example, for the following hierarchical model,\n\nThus, the marginal model is,\n\nThis model is what is used to fit to data in order to get regression estimates.\n\nHeagerty, P. J., & Zeger, S. L. (2000). Marginalized multilevel models and likelihood inference. \"Statistical Science, 15(1)\", 1-26.\n"}
{"id": "9318403", "url": "https://en.wikipedia.org/wiki?curid=9318403", "title": "Markov brothers' inequality", "text": "Markov brothers' inequality\n\nIn mathematics, the Markov brothers' inequality is an inequality proved in the 1890s by brothers Andrey Markov and Vladimir Markov, two Russian mathematicians. This inequality bounds the maximum of the derivatives of a polynomial on an interval in terms of the maximum of the polynomial. For \"k\" = 1 it was proved by Andrey Markov, and for \"k\" = 2,3... by his brother Vladimir Markov.\n\nLet \"P\" be a polynomial of degree ≤ \"n\". Then\n\nEquality is attained for Chebyshev polynomials of the first kind.\n\n\nMarkov's inequality is used to obtain lower bounds in computational complexity theory via the so-called \"Polynomial Method\".\n"}
{"id": "26702096", "url": "https://en.wikipedia.org/wiki?curid=26702096", "title": "Matsumoto zeta function", "text": "Matsumoto zeta function\n\nIn mathematics, Matsumoto zeta functions are a type of zeta function introduced by Kohji Matsumoto in 1990. They are functions of the form\nwhere \"p\" is a prime and \"A\" is a polynomial.\n"}
{"id": "27082137", "url": "https://en.wikipedia.org/wiki?curid=27082137", "title": "Method of steepest descent", "text": "Method of steepest descent\n\nIn mathematics, the method of steepest descent or stationary-phase method or saddle-point method is an extension of Laplace's method for approximating an integral, where one deforms a contour integral in the complex plane to pass near a stationary point (saddle point), in roughly the direction of steepest descent or stationary phase. The saddle-point approximation is used with integrals in the complex plane, whereas Laplace’s method is used with real integrals.\n\nThe integral to be estimated is often of the form\nwhere \"C\" is a contour, and λ is large. One version of the method of steepest descent deforms the contour of integration \"C\" into a new path integration \"C′\" so that the following conditions hold:\n\nThe method of steepest descent was first published by , who used it to estimate Bessel functions and pointed out that it occurred in the unpublished note about hypergeometric functions. The contour of steepest descent has a minimax property, see . described some other unpublished notes of Riemann, where he used this method to derive the Riemann–Siegel formula.\n\nLet and . If\n\nwhere formula_3 denotes the real part, and there exists a positive real number such that\n\nthen the following estimate holds:\n\nLet be a complex -dimensional vector, and\n\ndenote the Hessian matrix for a function . If\n\nis a vector function, then its Jacobian matrix is defined as\n\nA non-degenerate saddle point, , of a holomorphic function is a critical point of the function (i.e., ) where the function's Hessian matrix has a non-vanishing determinant (i.e., formula_9).\n\nThe following is the main tool for constructing the asymptotics of integrals in the case of a non-degenerate saddle point:\n\nThe Morse lemma for real-valued functions generalizes as follows for holomorphic functions: near a non-degenerate saddle point of a holomorphic function , there exist coordinates in terms of which is exactly quadratic. To make this precise, let be a holomorphic function with domain , and let in be a non-degenerate saddle point of , that is, and formula_9. Then there exist neighborhoods of and of , and a bijective holomorphic function with such that\n\nHere, the are the eigenvalues of the matrix formula_12.\n\nAssume\n\nThen, the following asymptotic holds\nwhere are eigenvalues of the Hessian formula_17 and formula_18 are defined with arguments\nThis statement is a special case of more general results presented in Fedoryuk (1987).\n\nEquation (8) can also be written as\n\nwhere the branch of\n\nis selected as follows\n\nConsider important special cases:\n\n\n\nIf the function has multiple isolated non-degenerate saddle points, i.e.,\n\nwhere\n\nis an open cover of , then the calculation of the integral asymptotic is reduced to the case of a single saddle point by employing the partition of unity. The partition of unity allows us to construct a set of continuous functions such that\n\nWhence,\n\nTherefore as we have:\n\nwhere equation (13) was utilized at the last stage, and the pre-exponential function at least must be continuous.\n\nWhen and formula_33, the point is called a degenerate saddle point of a function .\n\nCalculating the asymptotic of\n\nwhen is continuous, and has a degenerate saddle point, is a very rich problem, whose solution heavily relies on the catastrophe theory. Here, the catastrophe theory replaces the Morse lemma, valid only in the non-degenerate case, to transform the function into one of the multitude of canonical representations. For further details see, e.g., and .\n\nIntegrals with degenerate saddle points naturally appear in many applications including optical caustics and the multidimensional WKB approximation in quantum mechanics.\n\nThe other cases such as, e.g., and/or are discontinuous or when an extremum of lies at the integration region's boundary, require special care (see, e.g., and ).\n\nAn extension of the steepest descent method is the so-called \"nonlinear stationary phase/steepest descent method\". Here, instead of integrals, one needs to evaluate asymptotically solutions of Riemann–Hilbert factorization problems.\n\nGiven a contour \"C\" in the complex sphere, a function \"f\" defined on that contour and a special point, say infinity, one seeks a function \"M\" holomorphic away from the contour \"C\", with prescribed jump across \"C\", and with a given normalization at infinity. If \"f\" and hence \"M\" are matrices rather than scalars this is a problem that in general does not admit an explicit solution.\n\nAn asymptotic evaluation is then possible along the lines of the linear stationary phase/steepest descent method. The idea is to reduce asymptotically the solution of the given Riemann–Hilbert problem to that of a simpler, explicitly solvable, Riemann–Hilbert problem. Cauchy's theorem is used to justify deformations of the jump contour.\n\nThe nonlinear stationary phase was introduced by Deift and Zhou in 1993, based on earlier work of the Russian mathematician Alexander Its. A (properly speaking) nonlinear steepest descent method was introduced by Kamvissis, K. McLaughlin and P. Miller in 2003, based on previous work of Lax, Levermore, Deift, Venakides and Zhou. As in the linear case, steepest descent contours solve a min-max problem. In the nonlinear case they turn out to be \"S-curves\" (defined in a different context back in the 80s by Stahl, Gonchar and Rakhmanov).\n\nThe nonlinear stationary phase/steepest descent method has applications to the theory of soliton equations and integrable models, random matrices and combinatorics.\n\nPearcey integral\n\n"}
{"id": "16593557", "url": "https://en.wikipedia.org/wiki?curid=16593557", "title": "Natural pseudodistance", "text": "Natural pseudodistance\n\nIn size theory, the natural pseudodistance between two size pairs formula_1, formula_2 is the value formula_3, where formula_4 varies in the set of all homeomorphisms from the manifold formula_5 to the manifold formula_6 and formula_7 is the supremum norm. If formula_5 and formula_6 are not homeomorphic, then the natural pseudodistance is defined to be formula_10.\nIt is usually assumed that formula_5, formula_6 are formula_13 closed manifolds and the measuring functions formula_14 are formula_13. Put another way, the natural pseudodistance measures the infimum of the change of the measuring function induced by the homeomorphisms from formula_5 to formula_6.\n\nThe concept of natural pseudodistance can be easily extended to size pairs where the measuring function formula_18 takes values in formula_19\n\nIt can be proved \nthat the natural pseudodistance always equals the Euclidean distance between two critical values of the measuring functions (possibly, of the \"same\" measuring function) divided by a suitable positive integer formula_20.\nIf formula_5 and formula_6 are surfaces, the number formula_20 can be assumed to be formula_24, formula_25 or formula_26. If formula_5 and formula_6 are curves, the number formula_20 can be assumed to be formula_24 or formula_25.\nIf an optimal homeomorphism formula_32 exists (i.e., formula_33), then formula_20 can be assumed to be formula_24.\n\n"}
{"id": "18401725", "url": "https://en.wikipedia.org/wiki?curid=18401725", "title": "Nielsen–Schreier theorem", "text": "Nielsen–Schreier theorem\n\nIn group theory, a branch of mathematics, the Nielsen–Schreier theorem states that every subgroup of a free group is itself free. It is named after Jakob Nielsen and Otto Schreier.\n\nA free group may be defined from a group presentation consisting of a set of generators and the empty set of relations (equations that the generators satisfy). That is, it is the unique group in which every element is a product of some sequence of generators and their inverses, and in which there are no equations between group elements that do not follow in a trivial way from the equations describing the relation between a generator and its inverse. The elements of a free group may be described as all of the possible reduced words; these are strings of generators and their inverses, in which no generator is adjacent to its own inverse. Two reduced words may be multiplied by concatenating them and then removing any generator-inverse pairs that result from the concatenation.\n\nThe Nielsen–Schreier theorem states that if is a subgroup of a free group, then is itself isomorphic to a free group. That is, there exists a subset of elements of such that every element in is a product of members of and their inverses, and such that satisfies no nontrivial relations.\n\nThe Nielsen–Schreier formula, or Schreier index formula, quantifies the result in the case where the subgroup has finite index: if is a free group on generators, and is a subgroup of finite index , then is free of rank\n\nLet be the free group with two generators, and , and let be the subgroup consisting of all reduced words that are products of evenly many generators or their inverses. Then is itself generated by the six elements , , , , , and . A factorization of any reduced word in into these generators and their inverses may be constructed simply by taking consecutive pairs of symbols in the reduced word. However, this is not a free presentation of because it satisfies the relations and . Instead, is generated as a free group by the three elements , , and . Any factorization of a word into a product of generators from the six-element generating set } can be transformed into a product of generators from this smaller set by replacing with , replacing with , and replacing with . There are no additional relations satisfied by these three generators, so is the free group generated by , , and . The Nielsen–Schreier theorem states that this example is not a coincidence: like , every subgroup of a free group can be generated as a free group, possibly with a larger set of generators.\n\nIt is possible to prove the Nielsen–Schreier theorem using algebraic topology. A free group on a set of generators is the fundamental group of a bouquet of circles, a topological graph with a single vertex and with an edge for each generator. Any subgroup of the fundamental group is itself a fundamental group of a covering space of the bouquet, a (possibly infinite) topological Schreier coset graph that has one vertex for each coset of the subgroup. And in any topological graph, it is possible to shrink the edges of a spanning tree of the graph, producing a bouquet of circles that has the same fundamental group . Since is the fundamental group of a bouquet of circles, it is itself free. This proof is due to ; the original proof by Schreier forms the Schreier graph in a different way as a quotient of the Cayley graph of modulo the action of .\n\nAccording to Schreier's subgroup lemma, a set of generators for a free presentation of may be constructed from cycles in the covering graph formed by concatenating a spanning tree path from a base point (the coset of the identity) to one of the cosets, a single non-tree edge, and an inverse spanning tree path from the other endpoint of the edge back to the base point.\n\nAlthough several different proofs of the Nielsen–Schreier theorem are known, they all depend on the axiom of choice. In the proof based on fundamental groups of bouquets, for instance, the axiom of choice appears in the guise of the statement that every connected graph has a spanning tree. The use of this axiom is necessary, as there exist models of Zermelo–Fraenkel set theory in which the axiom of choice and the Nielsen–Schreier theorem are both false. The Nielsen–Schreier theorem in turn implies a weaker version of the axiom of choice, for finite sets.\n\nThe Nielsen–Schreier theorem is a non-abelian analogue of an older result of Richard Dedekind, that every subgroup of a free abelian group is free abelian.\n\nThe topological proof based on fundamental groups of bouquets of circles is due to . Another topological proof, based on the Bass–Serre theory of group actions on trees, was published by .\n\n\n"}
{"id": "57028478", "url": "https://en.wikipedia.org/wiki?curid=57028478", "title": "Paul Benioff", "text": "Paul Benioff\n\nPaul A. Benioff is an American physicist who helped pioneer the field of quantum computing. Benioff is best known for his research in quantum information theory that demonstrated the theoretical possibility of quantum computers.\n\nBenioff was born on May 1, 1930, in Pasadena, California. His father was a professor of seismology at the California Institute of Technology, and his mother received a master’s degree in English from the University of California, Berkeley.\n\nBenioff also attended Berkeley, where he earned an undergraduate degree in botany, in 1951. After a two-year stint working in nuclear chemistry for Tracerlab, he returned to Berkeley, in 1959, to obtain his Ph.D. in nuclear chemistry.\n\nIn 1960, Benioff spent a year at the Weizmann Institute of Science in Israel as a postdoctoral fellow. He then spent six months at the Niels Bohr Institute in Copenhagen, as a Ford Fellow. In 1961, he began a long career at Argonne National Laboratory, first with its Chemistry Division and later in 1978 in the lab’s Environmental Impact Division. Benioff remained at Argonne until he retired in 1995. He continues to conduct research at the laboratory as a post-retirement research participant for the Physics Division. In 1979, Benioff taught the foundations of quantum mechanics as a visiting professor at Tel Aviv University, and worked as a visiting scientist at CNRS Marseilles, in 1979 and 1982.\n\nIn the 1970s, Benioff began to research the theoretical feasibility of quantum computing. His early research culminated in a paper, published in 1980 that described a quantum mechanical model of Turing Machines. This work was based on a classical description in 1973 of reversible Turing machines by Bennett. Benioff’s model of a quantum computer was reversible, and did not dissipate energy. At the time, there were several papers arguing that the creation of a reversible model of quantum computing was impossible. Benioff’s paper was the first to show that reversible quantum computing was theoretically possible. This work and other later work by several authors initiated the field of quantum computing. After publishing several more papers on quantum computers, the idea began to gain traction with industry, banking, and government agencies. The field is now a fast-growing area of research.\n\nThroughout his career at Argonne, Benioff conducted research in many fields, including mathematics, physics and chemistry. While in the Chemistry Division, he conducted research on nuclear reaction theory, as well as the relationship between the foundations of physics and mathematics.\n\nAfter joining Argonne’s Environmental Impact Division in 1978, Benioff continued work on quantum computing and on foundational issues. This included descriptions of quantum robots, quantum mechanical models of different types of numbers, and other topics. More recently, his work has been on the effects of number scaling and local mathematics on physics and geometry. As an emeritus, he continues to work on these and other foundational topics.\n\nIn 2000, Benioff received the Quantum Communication Award of the International Organization for Quantum Communication, Computing, and Measurement, as well as the Quantum Computing and Communication Prize from Tamagawa University in Japan. He became a fellow of the American Physical Society in 2001. The following year, he was awarded the Special University of Chicago Medal for Distinguished Performance at Argonne National Laboratory. In 2016, Argonne held a conference in honor of his quantum computing work.\n\n\n"}
{"id": "357788", "url": "https://en.wikipedia.org/wiki?curid=357788", "title": "Per capita", "text": "Per capita\n\nPer capita is a Latin prepositional phrase: \"per\" (preposition, taking the accusative case, meaning \"by means of\") and \"capita\" (accusative plural of the noun \"caput\", \"head\"). The phrase thus means \"by heads\" or \"for each head\", i.e., per individual/person. The term is used in a wide variety of social sciences and statistical research contexts, including government statistics, economic indicators, and built environment studies.\n\nIt is commonly and usually used in the field of statistics in place of saying \"per person\" (although \"per caput\" is the Latin for \"per head\").\nIt is also used in wills to indicate that each of the named beneficiaries should receive, by devise or bequest, equal shares of the estate. This is in contrast to a \"per stirpes\" division, in which each branch (Latin \"stirps\", plural \"stirpes\") of the inheriting family inherits an equal share of the estate.\n\n"}
{"id": "40216580", "url": "https://en.wikipedia.org/wiki?curid=40216580", "title": "Pfister's sixteen-square identity", "text": "Pfister's sixteen-square identity\n\nIn algebra, Pfister's sixteen-square identity is a non-bilinear identity of form\n\nIt was first proven to exist by H. Zassenhaus and W. Eichhorn in the 1960s, and independently by Pfister around the same time. There are several versions, a concise one of which is\n\nIf all formula_18 and formula_19 with formula_20 are set equal to zero, then it reduces to Degen's eight-square identity (in blue). The formula_21 are\n\nand,\n\nThe identity shows that, in general, the product of two sums of sixteen squares is the sum of sixteen rational squares. Incidentally, the formula_21 also obey,\n\nNo sixteen-square identity exists involving only bilinear functions since Hurwitz's theorem states an identity of the form\n\nwith the formula_34 bilinear functions of the formula_18 and formula_19 is possible only for \"n\" ∈ {1, 2, 4, 8} . However, the more general Pfister's theorem (1965) shows that if the formula_34 are rational functions of one set of variables, hence has a denominator, then it is possible for all formula_38. There are also non-bilinear versions of Euler's four-square and Degen's eight-square identities.\n\n\n"}
{"id": "1363880", "url": "https://en.wikipedia.org/wiki?curid=1363880", "title": "Random forest", "text": "Random forest\n\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n\nThe first algorithm for random decision forests was created by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the \"stochastic discrimination\" approach to classification proposed by Eugene Kleinberg.\n\nAn extension of the algorithm was developed by Leo Breiman and Adele Cutler, and \"Random Forests\" is their trademark. The extension combines Breiman's \"bagging\" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.\n\nThe general method of random decision forests was first proposed by Ho in 1995. Ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow without suffering from overtraining, as long as the forests are randomly restricted to be sensitive to only selected feature dimensions. A subsequent work along the same lines concluded that other splitting methods, as long as they are randomly forced to be insensitive to some feature dimensions, behave similarly. Note that this observation of a more complex classifier (a larger forest) getting more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level of accuracy before being hurt by overfitting. The explanation of the forest method's resistance to overtraining can be found in Kleinberg's theory of stochastic discrimination.\nThe early development of Breiman's notion of random forests was influenced by the work of Amit and\nGeman who introduced the idea of searching over a random subset of the\navailable decisions when splitting a node, in the context of growing a single\ntree. The idea of random subspace selection from Ho was also influential in the design of random forests. In this method a forest of trees is grown,\nand variation among the trees is introduced by projecting the training data\ninto a randomly chosen subspace before fitting each tree or each node. Finally, the idea of\nrandomized node optimization, where the decision at each node is selected by a\nrandomized procedure, rather than a deterministic optimization was first\nintroduced by Dietterich.\n\nThe introduction of random forests proper was first made in a paper\nby Leo Breiman. This paper describes a method of building a forest of\nuncorrelated trees using a CART like procedure, combined with randomized node\noptimization and bagging. In addition, this paper combines several\ningredients, some previously known and some novel, which form the basis of the\nmodern practice of random forests, in particular:\n\n\nThe report also offers the first theoretical result for random forests in the\nform of a bound on the generalization error which depends on the strength of the\ntrees in the forest and their correlation.\n\nDecision trees are a popular method for various machine learning tasks. Tree learning \"come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining\", say Hastie \"et al.\", \"because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate\".\n\nIn particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, i.e. have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance in the final model.\n\nThe training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set = , ..., with responses = , ..., , bagging repeatedly (\"B\" times) selects a random sample with replacement of the training set and fits trees to these samples:\n\nAfter training, predictions for unseen samples can be made by averaging the predictions from all the individual regression trees on :\n\nor by taking the majority vote in the case of classification trees.\n\nThis bootstrapping procedure leads to better model performance because it decreases the variance of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets.\n\nAdditionally, an estimate of the uncertainty of the prediction can be made as the standard deviation of the predictions from all the individual regression trees on :\n\nThe number of samples/trees, , is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. An optimal number of trees can be found using cross-validation, or by observing the \"out-of-bag error\": the mean prediction error on each training sample , using only the trees that did not have in their bootstrap sample.\nThe training and test error tend to level off after some number of trees have been fit.\n\nThe above procedure describes the original bagging algorithm for trees. Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called \"feature bagging\". The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the trees, causing them to become correlated. An analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by Ho.\n\nTypically, for a classification problem with features, (rounded down) features are used in each split. For regression problems the inventors recommend (rounded down) with a minimum node size of 5 as the default.\n\nAdding one further step of randomization yields \"extremely randomized trees\", or ExtraTrees. While similar to ordinary random forests in that they are an ensemble of individual trees, there are two main differences: first, each tree is trained using the whole learning sample (rather than a bootstrap sample), and second, the top-down splitting in the tree learner is randomized. Instead of computing the locally \"optimal\" cut-point for each feature under consideration (based on, e.g., information gain or the Gini impurity), a \"random\" cut-point is selected. This value is selected from a uniform distribution within the feature's empirical range (in the tree's training set). Then, of all the randomly generated splits, the split that yields the highest score is chosen to split the node. Similar to ordinary random forests, the number of randomly selected features to be considered at each node can be specified. Default values for this parameter are formula_3 for classification and formula_4 for regression, where formula_4 is the number of features in the model. \n\nRandom forests can be used to rank the importance of variables in a regression or classification problem in a natural way. The following technique was described in Breiman's original paper and is implemented in the R package randomForest.\n\nThe first step in measuring the variable importance in a data set formula_6 is to fit a random forest to the data. During the fitting process the out-of-bag error for each data point is recorded and averaged over the forest (errors on an independent test set can be substituted if bagging is not used during training).\n\nTo measure the importance of the formula_7-th feature after training, the values of the formula_7-th feature are permuted among the training data and the out-of-bag error is again computed on this perturbed data set. The importance score for the formula_7-th feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees. The score is normalized by the standard deviation of these differences.\n\nFeatures which produce large values for this score are ranked as more important than features which produce small values. The statistical definition of the variable importance measure was given and analyzed by Zhu \"et al.\"\n\nThis method of determining variable importance has some drawbacks. For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. Methods such as partial permutations\nand growing unbiased treescan be used to solve the problem. If the data contain groups of correlated features of similar relevance for the output, then smaller groups are favored over larger groups.\n\nA relationship between random forests and the -nearest neighbor algorithm (-NN) was pointed out by Lin and Jeon in 2002. It turns out that both can be viewed as so-called \"weighted neighborhoods schemes\". These are models built from a training set formula_10 that make predictions formula_11 for new points by looking at the \"neighborhood\" of the point, formalized by a weight function :\n\nHere, formula_13 is the non-negative weight of the 'th training point relative to the new point in the same tree. For any particular , the weights for points formula_14 must sum to one. Weight functions are given as follows:\n\n\nSince a forest averages the predictions of a set of trees with individual weight functions formula_17, its predictions are\n\nThis shows that the whole forest is again a weighted neighborhood scheme, with weights that average those of the individual trees. The neighbors of in this interpretation are the points formula_14 sharing the same leaf in any tree formula_7. In this way, the neighborhood of depends in a complex way on the structure of the trees, and thus on the structure of the training set. Lin and Jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature.\n\nAs part of their construction, random forest predictors naturally lead to a dissimilarity measure among the observations. One can also define a random forest dissimilarity measure between unlabeled data: the idea is to construct a random forest predictor that distinguishes the “observed” data from suitably generated synthetic data.\nThe observed data are the original unlabeled data and the synthetic data are drawn from a reference distribution. A random forest dissimilarity can be attractive because it handles mixed variable types very well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. The random forest dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the \"Addcl 1\" random forest dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. The random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.\n\nInstead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes classifiers.\n\nIn machine learning, kernel random forests establish the connection between random forests and kernel methods. By slightly modifying their definition, random forests can be rewritten as kernel methods, which are more interpretable and easier to analyze.\n\nLeo Breiman was the first person to notice the link between random forest and kernel methods. He pointed out that random forests which are grown using i.i.d. random vectors in the tree construction are equivalent to a kernel acting on the true margin. Lin and Jeon established the connection between random forests and adaptive nearest neighbor, implying that random forests can be seen as adaptive kernel estimates. Davies and Ghahramani proposed Random Forest Kernel and show that it can empirically outperform state-of-art kernel methods. Scornet first defined KeRF estimates and gave the explicit link between KeRF estimates and random forest. He also gave explicit expressions for kernels based on centered random forest and uniform random forest, two simplified models of random forest. He named these two KeRFs Centered KeRF and Uniform KeRF, and proved upper bounds on their rates of consistency.\n\nCentered forest is a simplified model for Breiman's original random forest, which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. The algorithm stops when a fully binary tree of level formula_21 is built, where formula_22 is a parameter of the algorithm.\n\nUniform forest is another simplified model for Breiman's original random forest, which uniformly selects a feature among all features and performs splits at a point uniformly drawn on the side of the cell, along the preselected feature.\n\nGiven a training sample formula_23 of formula_24-valued independent random variables distributed as the independent prototype pair formula_25, where formula_26. We aim at predicting the response formula_27, associated with the random variable formula_28, by estimating the regression function formula_29. A random regression forest is an ensemble of formula_30 randomized regression trees. Denote formula_31 the predicted value at point formula_32 by the formula_7-th tree, where formula_34 are independent random variables, distributed as a generic random variable formula_35, independent of the sample formula_36. This random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate formula_37.\nFor regression trees, we have formula_38, where formula_39 is the cell containing formula_32, designed with randomness formula_41 and dataset formula_36, and formula_43.\n\nThus random forest estimates satisfy, for all formula_44, formula_45. Random regression forest has two level of averaging, first over the samples in the target cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the random forest methods and compensate the misestimation, Scornet defined KeRF by\n\nwhich is equal to the mean of the formula_47's falling in the cells containing formula_32 in the forest. If we define the connection function of the formula_30 finite forest as formula_50, i.e. the proportion of cells shared between formula_32 and formula_52, then almost surely we have formula_53, which defines the KeRF.\n\nThe construction of Centered KeRF of level formula_21 is the same as for centered forest, except that predictions are made by formula_55, the corresponding kernel function, or connection function is\n\nUniform KeRF is built in the same way as uniform forest, except that predictions are made by formula_55, the corresponding kernel function, or connection function is\n\nPredictions given by KeRF and random forests are close if the number of points in each cell is controlled:\nAssume that there exist sequences formula_59 such that, almost surely,\nThen almost surely,\nWhen the number of trees formula_30 goes to infinity, then we have infinite random forest and infinite KeRF. Their estimates are close if the number of observations in each cell is bounded:\nAssume that there exist sequences formula_63 such that, almost surely\nThen almost surely,\nAssume that formula_68, where formula_69 is a centered Gaussian noise, independent of formula_28, with finite variance formula_71. Moreover, formula_28 is uniformly distributed on formula_73 and formula_74 is Lipschitz. Scornet proved upper bounds on the rates of consistency for centered KeRF and uniform KeRF.\n\nProviding formula_75 and formula_76, there exists a constant formula_77 such that, for all formula_4,\nformula_79.\n\nProviding formula_75 and formula_76, there exists a constant formula_82 such that,\nformula_83.\n\nThe algorithm is often used in scientific works because of its advantages. For example, it can be used for quality assessment of Wikipedia articles.\n\n\n\n"}
{"id": "2660383", "url": "https://en.wikipedia.org/wiki?curid=2660383", "title": "Shortlex order", "text": "Shortlex order\n\nIn mathematics, and particularly in the theory of formal languages, shortlex is a total ordering for finite sequences of objects that can themselves be totally ordered. In the shortlex ordering, sequences are primarily sorted by cardinality (length) with the shortest sequences first, and sequences of the same length are sorted into lexicographical order. Shortlex ordering is also called radix, length-lexicographic, military, or genealogical ordering.\n\nIn the context of strings on a totally ordered alphabet, the shortlex order is identical to the lexicographical order, except that shorter strings precede longer strings. E.g., the shortlex order of the set of strings on the English alphabet (in its usual order) is \"[ε, a, b, c, ..., z, aa, ab, ac, ..., zz, aaa, aab, aac, ..., zzz, ...]\", where ε denotes the empty string.\n\nThe strings in this ordering over a fixed alphabet can be placed into one-to-one correspondence with the non-negative integers, giving the bijective numeration system for representing numbers. The shortlex ordering is also important in the theory of automatic groups.\n"}
{"id": "25824263", "url": "https://en.wikipedia.org/wiki?curid=25824263", "title": "Side-approximation theorem", "text": "Side-approximation theorem\n\nIn geometric topology, the side-approximation theorem was proved by . It implies that a 2-sphere in R can be approximated by polyhedral 2-spheres.\n\n"}
{"id": "3349272", "url": "https://en.wikipedia.org/wiki?curid=3349272", "title": "Sign extension", "text": "Sign extension\n\nSign extension is the operation, in computer arithmetic, of increasing the number of bits of a binary number while preserving the number's sign (positive/negative) and value. This is done by appending digits to the most significant side of the number, following a procedure dependent on the particular signed number representation used.\n\nFor example, if six bits are used to represent the number \"codice_1\" (decimal positive 10) and the sign extend operation increases the word length to 16 bits, then the new representation is simply \"codice_2\". Thus, both the value and the fact that the value was positive are maintained.\n\nIf ten bits are used to represent the value \"codice_3\" (decimal negative 15) using two's complement, and this is sign extended to 16 bits, the new representation is \"codice_4\". Thus, by padding the left side with ones, the negative sign and the value of the original number are maintained.\n\nIn the Intel x86 instruction set, for example, there are two ways of doing sign extension:\n\nA similar concept is zero extension. In a move or convert operation, zero extension refers to setting the high bits of the destination to zero, rather than setting them to a copy of the most significant bit of the source. If the source of the operation is an unsigned number, then zero extension is usually the correct way to move it to a larger field while preserving its numeric value, while sign extension is correct for signed numbers. \n\nIn the x86 and x64 instruction sets, the codice_5 instruction (\"move with zero extension\") performs this function. For example, codice_6 copies a byte from the codice_7 register to the low-order byte of codice_8 and then fills the remaining bytes of codice_8 with zeroes. \n\nOn x64, most instructions that write to the lower 32 bits of any of the general-purpose registers will zero the upper half of the destination register. For example, the instruction codice_10 will clear the upper 32 bits of the codice_11 register.\n\n"}
{"id": "41305278", "url": "https://en.wikipedia.org/wiki?curid=41305278", "title": "Solid partition", "text": "Solid partition\n\nIn mathematics, solid partitions are natural generalizations of partitions and plane partitions defined by Percy Alexander MacMahon. A solid partition of formula_1 is a three-dimensional array, formula_2, of non-negative integers (the indices formula_3) such that \nand\nLet formula_6 denote the number of solid partitions of formula_7. As the definition of solid partitions involves three-dimensional arrays of numbers, they are also called three-dimensional partitions in notation where plane partitions are two-dimensional partitions and partitions are one-dimensional partitions. Solid partitions and their higher-dimensional generalizations are discussed in the book by Andrews.\n\nAnother representation for solid partitions is in the form of Ferrers diagrams. The Ferrers diagram of a solid partition of formula_7 is a collection of formula_7 points or \"nodes\", formula_10, with formula_11 satisfying the condition:\n\nFor instance, the Ferrers diagram\nwhere each column is a node, represents a solid partition of formula_17. There is a natural action of the permutation group formula_18 on a Ferrers diagram – this corresponds to permuting the four coordinates of all nodes. This generalises the operation denoted by conjugation on usual partitions.\n\nGiven a Ferrers diagram, one constructs the solid partition (as in the main definition) as follows.\n\nGiven a set of formula_19 that form a solid partition, one obtains the corresponding Ferrers diagram as follows.\n\nFor example, the Ferrers diagram with formula_17 nodes given above corresponds to the solid partition with \nwith all other formula_19 vanishing.\n\nLet formula_31. Define the generating function of solid partitions, formula_32, by\nThe generating functions of partitions and plane partitions have simple formulae due to Euler and MacMahon respectively. However, a guess of MacMahon fails to correctly reproduce the solid partitions of 6 as shown by Atkin et al. It appears that there is no simple formula for the generating function of solid partitions. Somewhat confusingly, Atkin et al. refer to solid partitions as four-dimensional partitions as that is the dimension of the Ferrers diagram.\n\nGiven the lack of an explicitly known generating function, the enumerations of the numbers of solid partitions for larger integers have been carried out numerically. There are two algorithms that are used to enumerate solid partitions and their higher-dimensional generalizations. The work of Atkin. et al. used an algorithm due to Bratley and McKay. In 1970, Knuth proposed a different algorithm to enumerate topological sequences that he used to evaluate numbers of solid partitions of all integers formula_34. Mustonen and Rajesh extended the enumeration for all integers formula_35. In 2010, S. Balakrishnan proposed a parallel version of Knuth's algorithm that has been used to extend the enumeration to all integers formula_36. One finds \nwhich is a 19 digit number illustrating the difficulty in carrying out such exact enumerations.\n\nIt is known that from the work of Bhatia et al. that\nThe value of this constant was estimated using Monte-Carlo simulations by Mustonen and Rajesh to be formula_39.\n\n"}
{"id": "21669817", "url": "https://en.wikipedia.org/wiki?curid=21669817", "title": "Solvency ratio", "text": "Solvency ratio\n\nA solvency ratio measures the extent to which assets cover commitments for future payments, the liabilities. \n\nThe solvency ratio of an insurance company is the size of its capital relative to all risks it has taken. The solvency ratio is most often defined as: \n\nThe solvency ratio is a measure of the risk an insurer faces of claims that it cannot absorb. The amount of premium written is a better measure than the total amount insured because the level of premiums is linked to the likelihood of claims. \n\nDifferent countries use different methodologies to calculate the solvency ratio, and have different requirements. For example, in India insurers are required to maintain a minimum ratio of 1.5.\n\nFor pension plans, the solvency ratio is the ratio of pension plan assets to liabilities (the pensions to be paid). Another measure of the pension plan's ability to pay all pensions in perpetuity is the going concern ratio, which measures the cost of pensions if the pension plan continues to operate. For the solvency ratio, the pension liabilities are measured using stringent rules including the assumption that the plan will be close immediately so must purchase of annuities to transfer responsibility of the pensions to another party. This is more expensive so the solvency ratio is usually lower than the going concern ratio, which measures the pension plan's ability to pay pensions if it continues to operate.\n\nIn finance, the solvency ratio measures a company's cash flow compared to its liabilities:\n\nSolvency ratio = (net income + depreciation) / liabilities\n"}
{"id": "84539", "url": "https://en.wikipedia.org/wiki?curid=84539", "title": "Tomaž Pisanski", "text": "Tomaž Pisanski\n\nTomaž (Tomo) Pisanski (born May 24, 1949 in Ljubljana, Slovenia) is a Slovenian mathematician working mainly in discrete mathematics and graph theory.\nIn 1980 he calculated the genus of the Cartesian product of any pair of connected, bipartite, \"d\"-valent graphs using a method that was later called the \"White–Pisanski method\".\nIn 1982 Vladimir Batagelj and Pisanski proved that the Cartesian product of a tree and a cycle is Hamiltonian if and only if no degree of the tree exceeds the length of the cycle. They also proposed a conjecture concerning \"cyclic Hamiltonicity\" of graphs. Their conjecture was proved in 2005.\n\nPisanski studied at the University of Ljubljana where he obtained a B.Sc., M.Sc and Ph.D. in mathematics. He competed in the 1966 and 1967 International Mathematical Olympiads, winning a bronze medal in 1967. His Ph.D. thesis from topological graph theory was written under the guidance of Torrence Parsons. During his undergraduate studies he was an exchange student at the University of Nancy, France. He also obtained a M.Sc. in computer science from the Pennsylvania State University.\n\nCurrently, Pisanski is a professor of discrete and computational mathematics at Head of the Department of Information Sciences and Technology at University of Primorska in Koper. He has taught undergraduate and graduate courses in mathematics and computer science at the University of Ljubljana, University of Zagreb, University of Udine, University of Leoben, California State University, Chico, Simon Fraser University, University of Auckland and Colgate University. His students include John Shawe-Taylor (BSc in Ljubljana), Vladimir Batagelj, Bojan Mohar, Sandi Klavžar, Sandra Sattolo (MSc in Udine).\n\nFrom 1998-1999 he was chairman of the Society of Mathematicians, Physicists and Astronomers of Slovenia. In 2005 he was decorated with the Order of Merit (Slovenia)\n\n. He is a founding member of the International Academy of Mathematical Chemistry and from 2007-2011 its Vice President. He is a founding editor of the journal \"Ars Mathematica Contemporanea\". In 2012 he was elected to the Academia Europaea.\n\nWith Brigitte Servatius he is the co-author of a book on configurations.\n\nIn 2015 Pisanski received the Zois award for exceptional contributions to discrete mathematics and its applications.\n\n"}
{"id": "17633708", "url": "https://en.wikipedia.org/wiki?curid=17633708", "title": "Tucker's lemma", "text": "Tucker's lemma\n\nIn mathematics, Tucker's lemma is a combinatorial analog of the Borsuk–Ulam theorem, named after Albert W. Tucker.\n\nLet T be a triangulation of the closed \"n\"-dimensional ball formula_1. Assume T is antipodally symmetric on the boundary sphere formula_2. That means that the subset of simplices of T which are in formula_2 provides a triangulation of formula_2 where if σ is a simplex then so is −σ.\nLet formula_5 be a labeling of the vertices of T which is an odd function on formula_2, i.e, formula_7 for every vertex formula_8.\nThen Tucker's lemma states that T contains a \"complementary edge\" - an edge (a 1-simplex) whose vertices are labelled by the same number but with opposite signs.\n\nThe first proofs were non-constructive, by way of contradiction.\n\nLater, constructive proofs were found, which also supplied algorithms for finding the complementary edge. Basically, the algorithms are path-based: they start at a certain point or edge of the triangulation, then go from simplex to simplex according to prescribed rules, until it is not possible to proceed any more. It can be proved that the path must end in a simplex which contains a complementary edge.\n\nAn easier proof of Tucker's lemma uses the more general Ky Fan lemma, which has a simple algorithmic proof.\n\nThe following description illustrates the algorithm for formula_9. Note that in this case formula_1 is a disc and there are 4 possible labels: formula_11, like the figure at the top-right.\n\nStart outside the ball and consider the labels of the boundary vertices. Because the labeling is an odd function on the boundary, the boundary must have both positive and negative labels:\n\nSelect an formula_14 edge and go through it. There are three cases:\n\nThe last case can take you outside the ball. However, since the number of formula_14 edges on the boundary must be odd, there must be a new, unvisited formula_14 edge on the boundary. Go through it and continue.\n\nThis walk must end inside the ball, either in a formula_17 or in a formula_18 simplex. Done.\n\nThe run-time of the algorithm described above is polynomial in the triangulation size. This is considered bad, since the triangulations might be very large. It would be desirable to find an algorithm which is logarithmic in the triangulation size. However, the problem of finding a complementary edge is PPA-complete even for formula_9 dimensions. This implies that there is not too much hope for finding a fast algorithm.\n\n"}
{"id": "32182870", "url": "https://en.wikipedia.org/wiki?curid=32182870", "title": "Twisted Poincaré duality", "text": "Twisted Poincaré duality\n\nIn mathematics, the twisted Poincaré duality is a theorem removing the restriction on Poincaré duality to oriented manifolds. The existence of a global orientation is replaced by carrying along local information, by means of a local coefficient system.\n\nAnother version of the theorem with real coefficients features the de Rham cohomology with values in the orientation bundle. This is the flat real line bundle denoted formula_1, that is trivialized by coordinate charts of the manifold \"NM\", with transition maps the sign of the Jacobian determinant of the charts transition maps. As a flat line bundle, it has a de Rham cohomology, denoted by\n\nFor \"M\" a \"compact\" manifold, the top degree cohomology is equipped with a so-called trace morphism\nthat is to be interpreted as integration on \"M\", \"ie.\" evaluating against the fundamental class.\n\nThe Poincaré duality for differential forms is then the conjunction, for \"M\" connected, of the following two statements:\nis non-degenerate.\n\nThe oriented Poincaré duality is contained in this statement, as understood from the fact that the orientation bundle \"o(M)\" is trivial if the manifold is oriented, an orientation being a global trivialization, \"ie.\" a nowhere vanishing parallel section.\n\n\n"}
{"id": "602650", "url": "https://en.wikipedia.org/wiki?curid=602650", "title": "Type safety", "text": "Type safety\n\nIn computer science, type safety is the extent to which a programming language discourages or prevents type errors. A type error is erroneous or undesirable program behaviour caused by a discrepancy between differing data types for the program's constants, variables, and methods (functions), e.g., treating an integer (int) as a floating-point number (float). Type safety is sometimes alternatively considered to be a property of a computer program rather than the language in which that program is written; that is, some languages have type-safe facilities that can be circumvented by programmers who adopt practices that exhibit poor type safety. The formal type-theoretic definition of type safety is considerably stronger than what is understood by most programmers.\n\nType enforcement can be static, catching potential errors at compile time, or dynamic, associating type information with values at run-time and consulting them as needed to detect imminent errors, or a combination of both.\n\nThe behaviors classified as type errors by a given programming language are usually those that result from attempts to perform operations on values that are not of the appropriate data type. This classification is partly based on opinion; it may imply that any operation not leading to program crashes, security flaws or other obvious failures is legitimate and need not be considered an error, or it may imply that any contravention of the programmer's explicit intent (as communicated via typing annotations) to be erroneous and not \"type-safe\".\n\nIn the context of static (compile-time) type systems, type safety usually involves (among other things) a guarantee that the eventual value of any expression will be a legitimate member of that expression's static type. The precise requirement is more subtle than this — see, for example, subtype and polymorphism for complications.\n\nType safety is closely linked to \"memory safety\", a restriction on the ability to copy arbitrary bit patterns from one memory location to another. For instance, in an implementation of a language that has some type formula_1, such that some sequence of bits (of the appropriate length) does not represent a legitimate member of formula_1, if that language allows data to be copied into a variable of type formula_1, then it is not type-safe because such an operation might assign a non-formula_1 value to that variable. Conversely, if the language is type-unsafe to the extent of allowing an arbitrary integer to be used as a pointer, then it is not memory-safe.\n\nMost statically typed languages provide a degree of type safety that is strictly stronger than memory safety, because their type systems enforce the proper use of abstract data types defined by programmers even when this is not strictly necessary for memory safety or for the prevention of any kind of catastrophic failure.\n\nType-safe code accesses only the memory locations it is authorized to access. (For this discussion, type safety specifically refers to memory type safety and should not be confused with type safety in a broader respect.) For example, type-safe code cannot read values from another object's private fields.\n\nRobin Milner provided the following slogan to describe type safety:\nThe appropriate formalization of this slogan depends on the style of formal semantics used for a particular language. In the context of denotational semantics, type safety means that the value of an expression that is well-typed, say with type τ, is a \"bona fide\" member of the set corresponding to τ.\n\nIn 1994, Andrew Wright and Matthias Felleisen formulated what is now the standard definition and proof technique for type safety in languages defined by operational semantics. Under this approach, type safety is determined by two properties of the semantics of the programming language:\n\n\nThese properties do not exist in a vacuum; they are linked to the semantics of the programming language they describe, and there is a large space of varied languages that can fit these criteria, since the notion of \"well typed\" program is part of the static semantics of the programming language and the notion of \"getting stuck\" (or \"going wrong\") is a property of its dynamic semantics.\n\nVijay Saraswat provides the following definition:\n\nType safety is ultimately aimed at excluding other problems, e.g.:-\n\nType safety is usually a requirement for any toy language proposed in academic programming language research. Many languages, on the other hand, are too big for human-generated type safety proofs, as they often require checking thousands of cases. Nevertheless, some languages such as Standard ML, which has rigorously defined semantics, have been proved to meet one definition of type safety. Some other languages such as Haskell are \"believed\" to meet some definition of type safety, provided certain \"escape\" features are not used (for example Haskell's unsafePerformIO, used to \"escape\" from the usual restricted environment in which I/O is possible, circumvents the type system and so can be used to break type safety.) Type punning is another example of such an \"escape\" feature. Regardless of the properties of the language definition, certain errors may occur at run-time due to bugs in the implementation, or in linked libraries written in other languages; such errors could render a given implementation type unsafe in certain circumstances. An early version of Sun's Java virtual machine was vulnerable to this sort of problem.\n\nProgramming languages are often colloquially classified as strongly typed or weakly typed (also loosely typed) to refer to certain aspects of type safety. In 1974, Liskov and Zilles defined a strongly-typed language as one in which \"whenever an object is passed from a calling function to a called function, its type must be compatible with the type declared in the called function.\"\nIn 1977, Jackson wrote, \"In a strongly typed language each data area will have a distinct type and each process will state its communication requirements in terms of these types.\"\nIn contrast, a weakly typed language may produce unpredictable results or may perform implicit type conversion.\n\nIn object oriented languages type safety is usually intrinsic in the fact that a type system is in place. This is expressed in terms of class definitions.\n\nA class essentially defines the structure of the objects derived from it and an API as a \"contract\" for handling these objects.\nEach time a new object is created it will \"comply\" with that contract.\n\nEach function that exchanges objects derived from a specific class, or implementing a specific interface, will adhere to that contract: hence in that function the operations permitted on that object will be only those defined by the methods of the class the object implements.\nThis will guarantee that the object integrity will be preserved.\n\nExceptions to this are object oriented languages that allow dynamic modification of the object structure, or the use of reflection to modify the content of an object to overcome the constraints imposed by the class methods definitions.\n\nAda was designed to be suitable for embedded systems, device drivers and other forms of system programming, but also to encourage type-safe programming. To resolve these conflicting goals, Ada confines type-unsafety to a certain set of special constructs whose names usually begin with the string Unchecked_. Unchecked_Deallocation can be effectively banned from a unit of Ada text by applying pragma Pure to this unit. It is expected that programmers will use Unchecked_ constructs very carefully and only when necessary; programs that do not use them are type-safe.\n\nThe SPARK programming language is a subset of Ada eliminating all its potential ambiguities and insecurities while at the same time adding statically checked contracts to the language features available. SPARK avoids the issues with dangling pointers by disallowing allocation at run time entirely.\n\nAda2012 adds statically checked contracts to the language itself (in form of pre-, and post-conditions, as well as type invariants).\n\nThe C programming language is type-safe in limited contexts; for example, a compile-time error is generated when an attempt is made to convert a pointer to one type of structure to a pointer to another type of structure, unless an explicit cast is used. However, a number of very common operations are non-type-safe; for example, the usual way to print an integer is something like codice_2, where the codice_3 tells codice_4 at run-time to expect an integer argument. (Something like codice_5, which erroneously tells the function to expect a pointer to a character-string, may be accepted by compilers, but will produce undefined results.) This is partially mitigated by some compilers (such as gcc) checking type correspondences between printf arguments and format strings.\n\nIn addition, C, like Ada, provides unspecified or undefined explicit conversions; and unlike in Ada, idioms that use these conversions are very common, and have helped to give C a type-unsafe reputation. For example, the standard way to allocate memory on the heap is to invoke a memory allocation function, such as codice_6, with an argument indicating how many bytes are required. The function returns an untyped pointer (type codice_7), which the calling code must explicitly or implicitly cast to the appropriate pointer type. Pre-standardized implementations of C required an explicit cast to do so, therefore the code codice_8 became the accepted practice. However, this practice is discouraged in ISO C as it can mask a failure to include the header file in which codice_6 is defined, resulting in downstream errors on machines where the int and pointer types are of different sizes, such as most common implementations of C for the now-ubiquitous x86-64 architecture. A conflict arises in code that is required to compile as C++, since the cast is necessary in that language.\n\nSome features of C++ that promote more type-safe code:\n\nC# is type-safe (but not statically type-safe). It has support for untyped pointers, but this must be accessed using the \"unsafe\" keyword which can be prohibited at the compiler level. It has inherent support for run-time cast validation. Casts can be validated by using the \"as\" keyword that will return a null reference if the cast is invalid, or by using a C-style cast that will throw an exception if the cast is invalid. See C Sharp conversion operators.\n\nUndue reliance on the object type (from which all other types are derived) runs the risk of defeating the purpose of the C# type system. It is usually better practice to abandon object references in favour of generics, similar to templates in C++ and generics in Java.\n\nThe Java language is designed to enforce type safety. \nAnything in Java \"happens\" inside an object\nand each object is an instance of a class.\n\nTo implement the \"type safety\" enforcement, each object, before usage, needs to be allocated.\nJava allows usage of primitive types but only inside properly allocated objects.\n\nSometimes a part of the type safety is implemented indirectly: e.g. the class BigDecimal represents a floating point number of arbitrary precision, but handles only numbers that can be expressed with a finite representation.\nThe operation BigDecimal.divide() calculates a new object as the division of two numbers expressed as BigDecimal.\n\nIn this case if the division has no finite representation, as when one computes e.g. 1/3=0.33333..., the divide() method can raise an exception if no rounding mode is defined for the operation.\nHence the library, rather than the language, guarantees that the object respects the contract implicit in the class definition.\n\nSML has rigorously defined semantics and is known to be type-safe. However, some implementations of SML, including Standard ML of New Jersey (SML/NJ), its syntactic variant Mythryl and Mlton, provide libraries that offer certain unsafe operations. These facilities are often used in conjunction with those implementations' foreign function interfaces to interact with non-ML code (such as C libraries) that may require data laid out in specific ways. Another example is the SML/NJ interactive toplevel itself, which must use unsafe operations to execute ML code entered by the user.\n\nModula-2 is a strongly typed language with a design philosophy to require any unsafe facilities to be explicitly marked as unsafe. This is achieved by \"moving\" such facilities into a built-in pseudo-library called SYSTEM from where they must be imported before they can be used. The import thus makes it visible when such facilities are used. Unfortunately, this was not consequently implemented in the original language report and its implementation. There still remained unsafe facilities such as the type cast syntax and variant records (inherited from Pascal) that could be used without prior import. The difficulty in moving these facilities into the SYSTEM pseudo-module was the lack of any identifier for the facility that could then be imported since only identifiers can be imported, but not syntax.\n\nThe ISO Modula-2 standard corrected this for the type cast facility by changing the type cast syntax into a function called CAST which has to be imported from pseudo-module SYSTEM. However, other unsafe facilities such as variant records remained available without any import from pseudo-module SYSTEM.\n\nA recent revision of the language applied the original design philosophy rigorously. First, pseudo-module SYSTEM was renamed to UNSAFE to make the unsafe nature of facilities imported from there more explicit. Then all remaining unsafe facilities where either removed altogether (for example variant records) or moved to pseudo-module UNSAFE. For facilities where there is no identifier that could be imported, enabling identifiers were introduced. In order to enable such a facility, its corresponding enabling identifier must be imported from pseudo-module UNSAFE. No unsafe facilities remain in the language that do not require import from UNSAFE.\n\nPascal has had a number of type safety requirements, some of which are kept in some compilers. Where a Pascal compiler dictates \"strict typing\", two variables cannot be assigned to each other unless they are either compatible (such as conversion of integer to real) or assigned to the identical subtype. For example, if you have the following code fragment:\n\nUnder strict typing, a variable defined as TwoTypes is \"not compatible\" with DualTypes (because they are not identical, even though the components of that user defined type are identical) and an assignment of T1 := D2; is illegal. An assignment of T1 := T2; would be legal because the subtypes they are defined to \"are\" identical. However, an assignment such as T1.Q := D1.Q; would be legal.\n\nIn general, Common Lisp is a type-safe language. A Common Lisp compiler is responsible for inserting dynamic checks for operations whose type safety cannot be proven statically. However, a programmer may indicate that a program should be compiled with a lower level of dynamic type-checking. A program compiled in such a mode cannot be considered type-safe.\n\nThe following examples illustrates how C++ cast operators can break type safety when used incorrectly. The first example shows how basic data types can be incorrectly casted:\n\nIn this example, codice_10 explicitly prevents the compiler from performing a safe conversion from integer to floating-point value. When the program runs it will output a garbage floating-point value. The problem could have been avoided by instead writing codice_11\n\nThe next example shows how object references can be incorrectly downcasted:\n\nThe two child classes have members of different types. When downcasting a parent class pointer to a child class pointer, then the resulting pointer may not point to a valid object of correct type. In the example, this leads to garbage value being printed. The problem could have been avoided by replacing codice_12 with codice_13 that throws an exception on invalid casts.\n\n\n"}
{"id": "338331", "url": "https://en.wikipedia.org/wiki?curid=338331", "title": "Value (computer science)", "text": "Value (computer science)\n\nIn computer science, a value is the representation of some entity that can be manipulated by a program. The members of a type are the values of that type. \n\nThe \"value of a variable\" is given by the corresponding mapping in the environment. In languages with assignable variables it becomes necessary to distinguish between the \"r-value\" (or contents) and the \"l-value\" (or location) of a variable.\n\nIn declarative (high-level) languages, values have to be referentially transparent. This means that the resulting value is independent of the location in which a (sub-)expression needed to compute the value is stored. Only the contents of the location (the bits, whether they are 1 or 0) and their interpretation are significant.\n\nSome languages use the idea of l-values and r-values, deriving from the typical mode of evaluation on the left and right hand side of an assignment statement. An lvalue refers to an object that persists beyond a single expression. An rvalue is a temporary value that does not persist beyond the expression that uses it.\n\nThe notion of l-values and r-values was introduced by Combined Programming Language (CPL). The notions in an expression of r-value, l-value, and r-value/l-value are analogous to the parameter modes of input parameter (has a value), output parameter (can be assigned), and input/output parameter (has a value and can be assigned), though the technical details differ between contexts and languages.\n\nIn many languages, notably the C family, l-values have storage addresses that are programmatically accessible to the running program (e.g., via some address-of operator like \"&\" in C/C++), meaning that they are variables or dereferenced references to a certain memory location. R-values can be l-values (see below) or non-l-values—a term only used to distinguish from l-values. Consider the C expression codice_1. When executed, the computer generates an integer value of 13, but because the program has not explicitly designated where in the computer this 13 is stored, the expression is a non l-value. On the other hand, if a C program declares a variable x and assigns the value of 13 to x, then the expression codice_2 has a value of 13 and is an l-value.\n\nIn C, the term l-value originally meant something that could be assigned to (hence the name, indicating it is on the left side of the assignment operator), but since the reserved word codice_3 (constant) was added to the language, the term is now 'modifiable l-value'. In C++11 a special semantic-glyph codice_4 exists, to denote the \"use/access of the expression's address for the \"compiler\" only\"; i.e., the address cannot be retrieved using the address-of, codice_5, operator during the \"run-time\" of the program (see the use of move semantics).\n\nThis type of reference can be applied to \"all\" r-values including non-l-values as well as l-values. Some processors provide one or more instructions which take an immediate value, sometimes referred to as \"immediate\" for short. An immediate value is stored as part of the instruction which employs it, usually to load into, add to, or subtract from, a register. The other parts of the instruction are the opcode, and destination. The latter may be implicit. (A non-immediate value may reside in a register, or be stored elsewhere in memory, requiring the instruction to contain a direct or indirect address [e.g., index register address] to the value.)\n\nThe l-value expression designates (refers to) an object. A non-modifiable l-value is addressable, but not assignable. A modifiable l-value allows the designated object to be changed as well as examined. An r-value is any expression, a non-l-value is any expression that is not an l-value. One example is an \"immediate value\" (look below) and consequently not addressable..\n\nA value can be virtually any kind of data by a given data type, for instance a string, a digit, a single letter.\n\nProcessors often support more than one size of immediate data, e.g. 8 or 16 bit, employing a unique opcode and mnemonic for each instruction variant. If a programmer supplies a data value that will not fit, the assembler issues an \"Out of range\" error message. Most assemblers allow an immediate value to be expressed as ASCII, decimal, hexadecimal, octal, or binary data. Thus, the ASCII character 'A' is the same as 65 or 0x41. The byte order of strings may differ between processors, depending on the assembler and computer architecture.\n\n\n"}
{"id": "5734612", "url": "https://en.wikipedia.org/wiki?curid=5734612", "title": "Visual modeling", "text": "Visual modeling\n\nVisual modeling is the graphic representation of objects and systems of interest using graphical languages. Visual modeling languages may be General-Purpose Modeling (GPM) languages (e.g., UML, Southbeach Notation, IDEF) or Domain-Specific Modeling (DSM) languages (e.g., SysML). They include industry open standards (e.g., UML, SysML, Modelica), as well as proprietary standards, such as the visual languages associated with VisSim, MATLAB and Simulink, OPNET, NetSim, NI Multisim, and Reactive Blocks. Both VisSim and Reactive Blocks provide a royalty-free, downloadable viewer that lets anyone open and interactively simulate their models. The community edition of Reactive Blocks also allows full editing of the models as well as compilation, as long as the work is published under the Eclipse Public License. Visual modeling languages are an area of active research that continues to evolve, as evidenced by increasing interest in DSM languages, visual requirements, and visual OWL (Web Ontology Language).\n\n\n"}
{"id": "404582", "url": "https://en.wikipedia.org/wiki?curid=404582", "title": "Well-formed formula", "text": "Well-formed formula\n\nIn mathematical logic, propositional logic and predicate logic, a well-formed formula, abbreviated WFF or wff, often simply formula, is a finite sequence of symbols from a given alphabet that is part of a formal language. A formal language can be identified with the set of formulas in the language.\n\nA formula is a syntactic object that can be given a semantic meaning by means of an interpretation. Two key uses of formulas are in propositional logic and predicate logic.\n\nA key use of formulas is in propositional logic and predicate logics such as first-order logic. In those contexts, a formula is a string of symbols φ for which it makes sense to ask \"is φ true?\", once any free variables in φ have been instantiated. In formal logic, proofs can be represented by sequences of formulas with certain properties, and the final formula in the sequence is what is proven.\n\nAlthough the term \"formula\" may be used for written marks (for instance, on a piece of paper or chalkboard), it is more precisely understood as the sequence of symbols being expressed, with the marks being a token instance of formula. Thus the same formula may be written more than once, and a formula might in principle be so long that it cannot be written at all within the physical universe.\n\nFormulas themselves are syntactic objects. They are given meanings by interpretations. For example, in a propositional formula, each propositional variable may be interpreted as a concrete proposition, so that the overall formula expresses a relationship between these propositions. A formula need not be interpreted, however, to be considered solely as a formula.\n\nThe formulas of propositional calculus, also called propositional formulas, are expressions such as formula_1. Their definition begins with the arbitrary choice of a set \"V\" of propositional variables. The alphabet consists of the letters in \"V\" along with the symbols for the propositional connectives and parentheses \"(\" and \")\", all of which are assumed to not be in \"V\". The formulas will be certain expressions (that is, strings of symbols) over this alphabet.\n\nThe formulas are inductively defined as follows:\n\nThis definition can also be written as a formal grammar in Backus–Naur form, provided the set of variables is finite:\n\nUsing this grammar, the sequence of symbols\nis a formula, because it is grammatically correct. The sequence of symbols\nis not a formula, because it does not conform to the grammar.\n\nA complex formula may be difficult to read, owing to, for example, the proliferation of parentheses. To alleviate this last phenomenon, precedence rules (akin to the standard mathematical order of operations) are assumed among the operators, making some operators more binding than others. For example, assuming the precedence (from most binding to least binding) 1. ¬   2. →  3. ∧  4. ∨. Then the formula\nmay be abbreviated as\nThis is, however, only a convention used to simplify the written representation of a formula. If the precedence was assumed, for example, to be left-right associative, in following order: 1. ¬   2. ∧  3. ∨  4. →, then the same formula above (without parentheses) would be rewritten as\n\nThe definition of a formula in first-order logic formula_2 is relative to the signature of the theory at hand. This signature specifies the constant symbols, relation symbols, and function symbols of the theory at hand, along with the arities of the function and relation symbols.\n\nThe definition of a formula comes in several parts. First, the set of terms is defined recursively. Terms, informally, are expressions that represent objects from the domain of discourse.\n\nThe next step is to define the atomic formulas.\n\nFinally, the set of formulas is defined to be the smallest set containing the set of atomic formulas such that the following holds:\n\nIf a formula has no occurrences of formula_17 or formula_18, for any variable formula_10, then it is called \"quantifier-free\". An \"existential formula\" is a formula starting with a sequence of existential quantification followed by a quantifier-free formula.\n\nAn \"atomic formula\" is a formula that contains no logical connectives nor quantifiers, or equivalently a formula that has no strict subformulas.\nThe precise form of atomic formulas depends on the formal system under consideration; for propositional logic, for example, the atomic formulas are the propositional variables. For predicate logic, the atoms are predicate symbols together with their arguments, each argument being a term.\n\nAccording to some terminology, an \"open formula\" is formed by combining atomic formulas using only logical connectives, to the exclusion of quantifiers. This has not to be confused with a formula which is not closed.\n\nA \"closed formula\", also \"ground formula\" or \"sentence\", is a formula in which there are no free occurrences of any variable. If A is a formula of a first-order language in which the variables \"v\", ..., \"v\" have free occurrences, then A preceded by \"v\" ... \"v\" is a closure of A.\n\n\nIn earlier works on mathematical logic (e.g. by Church), formulas referred to any strings of symbols and among these strings, well-formed formulas were the strings that followed the formation rules of (correct) formulas.\n\nSeveral authors simply say formula. Modern usages (especially in the context of computer science with mathematical software such as model checkers, automated theorem provers, interactive theorem provers) tend to retain of the notion of formula only the algebraic concept and to leave the question of well-formedness, i.e. of the concrete string representation of formulas (using this or that symbol for connectives and quantifiers, using this or that parenthesizing convention, using Polish or infix notation, etc.) as a mere notational problem.\n\nWhile the expression \"well-formed formula\" is still in use, these authors do not necessarily use it in contradistinction to the old sense of \"formula\", which is no longer common in mathematical logic.\n\nThe expression \"well-formed formulas\" (WFF) also crept into popular culture. \"WFF\" is part of an esoteric pun used in the name of the academic game \"WFF 'N PROOF: The Game of Modern Logic,\" by Layman Allen, developed while he was at Yale Law School (he was later a professor at the University of Michigan). The suite of games is designed to teach the principles of symbolic logic to children (in Polish notation). Its name is an echo of \"whiffenpoof\", a nonsense word used as a cheer at Yale University made popular in \"The Whiffenpoof Song\" and The Whiffenpoofs.\n\n\n\n"}
