{"id": "64516", "url": "https://en.wikipedia.org/wiki?curid=64516", "title": "2", "text": "2\n\n2 (two) is a number, numeral, and glyph. It is the natural number following 1 and preceding 3.\n\nAn integer is called \"even\" if it is divisible by 2. For integers written in a numeral system based on an even number, such as decimal, hexadecimal, or in any other base that is even, divisibility by 2 is easily tested by merely looking at the last digit. If it is even, then the whole number is even. In particular, when written in the decimal system, all multiples of 2 will end in 0, 2, 4, 6, or 8.\n\nTwo is the smallest prime number, and the only even prime number (for this reason it is sometimes called \"the oddest prime\"). The next prime is three. Two and three are the only two consecutive prime numbers. 2 is the first Sophie Germain prime, the first factorial prime, the first Lucas prime, and the first Ramanujan prime.\n\nTwo is the third (or fourth) Fibonacci number.\n\nTwo is the base of the binary system, the numeral system with the least number of tokens allowing to denote a natural number substantially more concise ( tokens), compared to a direct representation by the corresponding count of a single token ( tokens). This binary number system is used extensively in computing.\n\nFor any number \"x\":\n\nExtending this sequence of operations by introducing the notion of hyperoperations, here denoted by \"hyper(\"a\",\"b\",\"c\")\" with \"a\" and \"c\" being the first and second operand, and \"b\" being the \"level\" in the above sketched sequence of operations, the following holds in general:\n\nTwo has therefore the unique property that , disregarding the level of the hyperoperation, here denoted by Knuth's up-arrow notation. The number of up-arrows refers to the level of the hyperoperation.\n\nTwo is the only number \"x\" such that the sum of the reciprocals of the powers of \"x\" equals itself. In symbols\n\nThis comes from the fact that:\n\nPowers of two are central to the concept of Mersenne primes, and important to computer science. Two is the first Mersenne prime exponent.\n\nTaking the square root of a number is such a common mathematical operation, that the spot on the root sign where the exponent would normally be written for cubic and other roots, may simply be left blank for square roots, as it is tacitly understood.\n\nThe square root of 2 was the first known irrational number.\n\nThe smallest field has two elements.\n\nIn a set-theoretical construction of the natural numbers, 2 is identified with the set <nowiki>\n"}
{"id": "12437648", "url": "https://en.wikipedia.org/wiki?curid=12437648", "title": "Antisymmetrizer", "text": "Antisymmetrizer\n\nIn quantum mechanics, an antisymmetrizer formula_1 (also known as antisymmetrizing operator) is a linear operator that makes a wave function of \"N\" identical fermions antisymmetric under the exchange of the coordinates of any pair of fermions. After application of formula_1 the wave function satisfies the Pauli principle. Since formula_1 is a projection operator, application of the antisymmetrizer to a wave function that is already totally antisymmetric has no effect, acting as the identity operator.\n\nConsider a wave function depending on the space and spin coordinates of \"N\" fermions:\nwhere the position vector r of particle \"i\" is a vector in formula_5 and σ takes on 2\"s\"+1 values, where \"s\" is the half-integral intrinsic spin of the fermion. For electrons \"s\" = 1/2 and σ can have two values (\"spin-up\": 1/2 and \"spin-down\": −1/2). It is assumed that the positions of the coordinates in the notation for Ψ have a well-defined meaning. For instance, the 2-fermion function Ψ(1,2) will in general be not the same as Ψ(2,1). This implies that in general formula_6 and therefore we can define meaningfully a \"transposition operator\" formula_7 that interchanges the coordinates of particle \"i\" and \"j\". In general this operator will not be equal to the identity operator (although in special cases it may be). \n\nA transposition has the\nparity (also known as signature) −1. The Pauli principle postulates that a wave function of identical fermions must be an eigenfunction of a transposition operator with its parity as eigenvalue\nHere we associated the transposition operator formula_7 with the permutation of coordinates \"π\" that acts on the set of \"N\" coordinates. In this case \"π\" = (\"ij\"), where (\"ij\") is the cycle notation for the transposition of the coordinates of particle \"i\" and \"j\".\n\nTranspositions may be composed (applied in sequence). This defines a product between the transpositions that is associative. \nIt can be shown that an arbitrary permutation of \"N\" objects can be written as a product of transpositions and that the number of transposition in this decomposition is of fixed parity. That is, either a permutation is always decomposed in an even number of transpositions (the permutation is called even and has the parity +1), or a permutation is always decomposed in an odd number of transpositions and then it is an odd permutation with parity −1. Denoting the parity of an arbitrary permutation \"π\" by (−1), it follows that an antisymmetric wave function satisfies\n\nwhere we associated the linear operator formula_11 with the permutation π.\n\nThe set of all \"N\"! permutations with the associative product: \"apply one permutation after the other\", is a group, known as the permutation group or symmetric group, denoted by \"S\". We define the antisymmetrizer as\n\nIn the representation theory of finite groups the antisymmetrizer is a well-known object, because the set of parities formula_13 forms a one-dimensional (and hence irreducible) representation of the permutation group known as the \"antisymmetric representation\". The representation being one-dimensional, the set of parities form the character of the antisymmetric representation. The antisymmetrizer is in fact a character projection operator and is quasi-idempotent,\nThis has the consequence that for \"any\" \"N\"-particle wave function Ψ(1, ...,\"N\") we have\nEither Ψ does not have an antisymmetric component, and then the antisymmetrizer projects onto zero, or it has one and then the antisymmetrizer projects out this antisymmetric component Ψ'.\nThe antisymmetrizer carries a left and a right representation of the group:\nwith the operator formula_11 representing the coordinate permutation π.\nNow it holds, for \"any\" \"N\"-particle wave function Ψ(1, ...,\"N\") with a non-vanishing antisymmetric component, that\nshowing that the non-vanishing component is indeed antisymmetric.\n\nIf a wave function is symmetric under any odd parity permutation it has no antisymmetric component. Indeed, assume that the permutation π, represented by the operator formula_11, has odd parity and that Ψ is symmetric, then\nAs an example of an application of this result, we assume that Ψ is a spin-orbital product. Assume further that a spin-orbital occurs twice (is \"doubly occupied\") in this product, once with coordinate \"k\" and once with coordinate \"q\". Then the product is symmetric under the transposition (\"k\", \"q\") and hence vanishes. Notice that this result gives the original formulation of the Pauli principle: no two electrons can have the same set of quantum numbers (be in the same spin-orbital).\n\nPermutations of identical particles are unitary, (the Hermitian adjoint is equal to the inverse of the operator), and since π and π have the same parity, it follows that the antisymmetrizer is Hermitian,\n\nThe antisymmetrizer commutes with any observable formula_22 (Hermitian operator corresponding to a physical—observable—quantity)\nIf it were otherwise, measurement of formula_22 could distinguish the particles, in contradiction with the assumption that only the coordinates of indistinguishable particles are affected by the antisymmetrizer.\n\nIn the special case that the wave function to be antisymmetrized is a product of spin-orbitals\nthe Slater determinant is created by the antisymmetrizer operating on the product of spin-orbitals, as below:\n\nThe correspondence follows immediately from the Leibniz formula for determinants, which reads\nwhere B is the matrix\nTo see the correspondence we notice that the fermion labels, permuted by the terms in the antisymmetrizer, label different columns (are second indices). The first indices are orbital indices, \"n\", ..., \"n\" labeling the rows.\n\nBy the definition of the antisymmetrizer\n\nConsider the Slater determinant\n\nBy the Laplace expansion along the first row of \"D\"\n\nso that \n\nBy comparing terms we see that \n\nOne often meets a wave function of the product form\nformula_34 where the total wave function is not antisymmetric, but the factors are antisymmetric,\nand\nHere formula_37 antisymmetrizes the first \"N\" particles and formula_38 antisymmetrizes the second set of \"N\" particles.\nThe operators appearing in these two antisymmetrizers represent the elements of the subgroups \"S\" and \"S\", respectively, of \"S\". \n\nTypically, one meets such partially antisymmetric wave functions in the theory of intermolecular forces, where formula_39 is the electronic wave function of molecule \"A\" and formula_40 is the wave function of molecule \"B\". When \"A\" and \"B\" interact, the Pauli principle requires the antisymmetry of the total wave function, also under intermolecular permutations.\n\nThe total system can be antisymmetrized by the total antisymmetrizer formula_41 which consists of the (\"N\" + \"N\")! terms in the group \"S\". However, in this way one does not take advantage of the partial antisymmetry that is already present. It is more economic to use the fact that the product of the two subgroups is also a subgroup, and to consider the left cosets of this product group in \"S\":\n\nwhere τ is a left coset representative. Since \nwe can write\nThe operator formula_45 represents the coset representative τ (an intermolecular coordinate permutation). Obviously the intermolecular antisymmetrizer formula_46 has a factor \"N\"! \"N\"! fewer terms then the total antisymmetrizer.\nFinally,\nso that we see that it suffices to act with formula_46 if the wave functions of the subsystems are already antisymmetric.\n\n"}
{"id": "1842", "url": "https://en.wikipedia.org/wiki?curid=1842", "title": "Augustin-Louis Cauchy", "text": "Augustin-Louis Cauchy\n\nBaron Augustin-Louis Cauchy (; ; 21 August 178923 May 1857) was a French mathematician, engineer and physicist who made pioneering contributions to several branches of mathematics, including mathematical analysis and continuum mechanics. He was one of the first to state and rigorously prove theorems of calculus, rejecting the heuristic principle of the generality of algebra of earlier authors. He almost singlehandedly founded complex analysis and the study of permutation groups in abstract algebra.\n\nA profound mathematician, Cauchy had a great influence over his contemporaries and successors; Hans Freudenthal stated: \"More concepts and theorems have been named for Cauchy than for any other mathematician (in elasticity alone there are sixteen concepts and theorems named for Cauchy).\" Cauchy was a prolific writer; he wrote approximately eight hundred research articles and five complete textbooks on a variety of topics in the fields of mathematics and mathematical physics.\n\nCauchy was the son of Louis François Cauchy (1760–1848) and Marie-Madeleine Desestre. Cauchy had two brothers: Alexandre Laurent Cauchy (1792–1857), who became a president of a division of the court of appeal in 1847 and a judge of the court of cassation in 1849, and Eugene François Cauchy (1802–1877), a publicist who also wrote several mathematical works.\n\nCauchy married Aloise de Bure in 1818. She was a close relative of the publisher who published most of Cauchy's works. They had two daughters, Marie Françoise Alicia (1819) and Marie Mathilde (1823).\n\nCauchy's father (Louis François Cauchy) was a high official in the Parisian Police of the New Régime, but lost this position due to the French Revolution (July 14, 1789), which broke out one month before Augustin-Louis was born. The Cauchy family survived the revolution and the following Reign of Terror (1793-4) by escaping to Arcueil, where Cauchy received his first education, from his father. After the execution of Robespierre (1794), it was safe for the family to return to Paris. There Louis-François Cauchy found himself a new bureaucratic job in 1800, and quickly moved up the ranks. When Napoleon Bonaparte came to power (1799), Louis-François Cauchy was further promoted, and became Secretary-General of the Senate, working directly under Laplace (who is now better known for his work on mathematical physics). The famous mathematician Lagrange was also a friend of the Cauchy family.\n\nOn Lagrange's advice, Augustin-Louis was enrolled in the École Centrale du Panthéon, the best secondary school of Paris at that time, in the fall of 1802. Most of the curriculum consisted of classical languages; the young and ambitious Cauchy, being a brilliant student, won many prizes in Latin and the humanities. In spite of these successes, Augustin-Louis chose an engineering career, and prepared himself for the entrance examination to the École Polytechnique.\n\nIn 1805, he placed second out of 293 applicants on this exam, and he was admitted. One of the main purposes of this school was to give future civil and military engineers a high-level scientific and mathematical education. The school functioned under military discipline, which caused the young and pious Cauchy some problems in adapting. Nevertheless, he finished the Polytechnique in 1807, at the age of 18, and went on to the École des Ponts et Chaussées (School for Bridges and Roads). He graduated in civil engineering, with the highest honors.\n\nAfter finishing school in 1810, Cauchy accepted a job as a junior engineer in Cherbourg, where Napoleon intended to build a naval base. Here Augustin-Louis stayed for three years, and was assigned the Ourcq Canal project and the Saint-Cloud Bridge project, and worked at the Harbor of Cherbourg. Although he had an extremely busy managerial job, he still found time to prepare three mathematical manuscripts, which he submitted to the \"Première Classe\" (First Class) of the Institut de France. Cauchy's first two manuscripts (on polyhedra) were accepted; the third one (on directrices of conic sections) was rejected.\n\nIn September 1812, now 23 years old, Cauchy returned to Paris after becoming ill from overwork. Another reason for his return to the capital was that he was losing his interest in his engineering job, being more and more attracted to the abstract beauty of mathematics; in Paris, he would have a much better chance to find a mathematics related position. Therefore, when his health improved in 1813, Cauchy chose to not return to Cherbourg. Although he formally kept his engineering position, he was transferred from the payroll of the Ministry of the Marine to the Ministry of the Interior. The next three years Augustin-Louis was mainly on unpaid sick leave, and spent his time quite fruitfully, working on mathematics (on the related topics of symmetric functions, the symmetric group and the theory of higher-order algebraic equations). He attempted admission to the First Class of the Institut de France but failed on three different occasions between 1813 and 1815. In 1815 Napoleon was defeated at Waterloo, and the newly installed Bourbon king Louis XVIII took the restoration in hand. The Académie des Sciences was re-established in March 1816; Lazare Carnot and Gaspard Monge were removed from this Academy for political reasons, and the king appointed Cauchy to take the place of one of them. The reaction of Cauchy's peers was harsh; they considered the acceptance of his membership in the Academy an outrage, and Cauchy thereby created many enemies in scientific circles.\n\nIn November 1815, Louis Poinsot, who was an associate professor at the École Polytechnique, asked to be exempted from his teaching duties for health reasons. Cauchy was by then a rising mathematical star, who certainly merited a professorship. One of his great successes at that time was the proof of Fermat's polygonal number theorem. However, the fact that Cauchy was known to be very loyal to the Bourbons, doubtless also helped him in becoming the successor of Poinsot. He finally quit his engineering job, and received a one-year contract for teaching mathematics to second-year students of the École Polytechnique. In 1816, this Bonapartist, non-religious school was reorganized, and several liberal professors were fired; the reactionary Cauchy was promoted to full professor.\n\nWhen Cauchy was 28 years old, he was still living with his parents. His father found it high time for his son to marry; he found him a suitable bride, Aloïse de Bure, five years his junior. The de Bure family were printers and booksellers, and published most of Cauchy's works. Aloïse and Augustin were married on April 4, 1818, with great Roman Catholic pomp and ceremony, in the Church of Saint-Sulpice. In 1819 the couple's first daughter, Marie Françoise Alicia, was born, and in 1823 the second and last daughter, Marie Mathilde. \n\nThe conservative political climate that lasted until 1830 suited Cauchy perfectly. In 1824 Louis XVIII died, and was succeeded by his even more reactionary brother Charles X. During these years Cauchy was highly productive, and published one important mathematical treatise after another. He received cross appointments at the Collège de France, and the .\n\nIn July 1830, the July Revolution occurred in France. Charles X fled the country, and was succeeded by the non-Bourbon king Louis-Philippe (of the House of Orléans). Riots, in which uniformed students of the École Polytechnique took an active part, raged close to Cauchy's home in Paris.\n\nThese events marked a turning point in Cauchy's life, and a break in his mathematical productivity. Cauchy, shaken by the fall of the government, and moved by a deep hatred of the liberals who were taking power, left Paris to go abroad, leaving his family behind. He spent a short time at Fribourg in Switzerland, where he had to decide whether he would swear a required oath of allegiance to the new regime. He refused to do this, and consequently lost all his positions in Paris, except his membership of the Academy, for which an oath was not required. In 1831 Cauchy went to the Italian city of Turin, and after some time there, he accepted an offer from the King of Sardinia (who ruled Turin and the surrounding Piedmont region) for a chair of theoretical physics, which was created especially for him. He taught in Turin during 1832–1833. In 1831, he was elected a foreign member of the Royal Swedish Academy of Sciences, and the following year a Foreign Honorary Member of the American Academy of Arts and Sciences.\n\nIn August 1833 Cauchy left Turin for Prague, to become the science tutor of the thirteen-year-old Duke of Bordeaux Henri d'Artois (1820–1883), the exiled Crown Prince and grandson of Charles X. As a professor of the École Polytechnique, Cauchy had been a notoriously bad lecturer, assuming levels of understanding that only a few of his best students could reach, and cramming his allotted time with too much material. The young Duke had neither taste nor talent for either mathematics or science, so student and teacher were a perfect mismatch. Although Cauchy took his mission very seriously, he did this with great clumsiness, and with surprising lack of authority over the Duke.\n\nDuring his civil engineering days, Cauchy once had been briefly in charge of repairing a few of the Parisian sewers, and he made the mistake of mentioning this to his pupil; with great malice, the young Duke went about saying Mister Cauchy started his career in the sewers of Paris. His role as tutor lasted until the Duke became eighteen years old, in September 1838. Cauchy did hardly any research during those five years, while the Duke acquired a lifelong dislike of mathematics. The only good that came out of this episode was Cauchy's promotion to baron, a title by which Cauchy set great store. In 1834, his wife and two daughters moved to Prague, and Cauchy was finally reunited with his family after four years in exile.\n\nCauchy returned to Paris and his position at the Academy of Sciences late in 1838. He could not regain his teaching positions, because he still refused to swear an oath of allegiance. However, he desperately wanted to regain a formal position in Parisian science.\nIn August 1839 a vacancy appeared in the Bureau des Longitudes. This Bureau had some resemblance to the Academy; for instance, it had the right to co-opt its members. Further, it was believed that members of the Bureau could \"forget\" about the oath of allegiance, although formally, unlike the Academicians, they were obliged to take it. The Bureau des Longitudes was an organization founded in 1795 to solve the problem of determining position on sea – mainly the longitudinal coordinate, since latitude is easily determined from the position of the sun. Since it was thought that position on sea was best determined by astronomical observations, the Bureau had developed into an organization resembling an academy of astronomical sciences.\n\nIn November 1839 Cauchy was elected to the Bureau, and discovered immediately that the matter of the oath was not so easily dispensed with. Without his oath, the king refused to approve his election. For four years Cauchy was in the position of being elected, but not being approved; hence, he was not a formal member of the Bureau, did not receive payment, could not participate in meetings, and could not submit papers. Still Cauchy refused to take any oaths; however, he did feel loyal enough to direct his research to celestial mechanics. In 1840, he presented a dozen papers on this topic to the Academy. He also described and illustrated the signed-digit representation of numbers, an innovation presented in England in 1727 by John Colson. The confounded membership of the Bureau lasted until the end of 1843, when Cauchy was finally replaced by Poinsot.\n\nThroughout the nineteenth century the French educational system struggled over the separation of Church and State. After losing control of the public education system, the Catholic Church sought to establish its own branch of education and found in Cauchy a staunch and illustrious ally. He lent his prestige and knowledge to the École Normale Écclésiastique, a school in Paris run by Jesuits, for training teachers for their colleges. He also took part in the founding of the Institut Catholique. The purpose of this institute was to counter the effects of the absence of Catholic university education in France. These activities did not make Cauchy popular with his colleagues who, on the whole, supported the Enlightenment ideals of the French Revolution. When a chair of mathematics became vacant at the Collège de France in 1843, Cauchy applied for it, but got just three out of 45 votes.\n\nThe year 1848 was the year of revolution all over Europe; revolutions broke out in numerous countries, beginning in France. King Louis-Philippe, fearful of sharing the fate of Louis XVI, fled to England. The oath of allegiance was abolished, and the road to an academic appointment was finally clear for Cauchy. On March 1, 1849, he was reinstated at the Faculté de Sciences, as a professor of mathematical astronomy. After political turmoil all through 1848, France chose to become a Republic, under the Presidency of Louis Napoleon Bonaparte, nephew of Napoleon Bonaparte, and son of Napoleon's brother, who had been installed as the first king of Holland. Soon (early 1852) the President made himself Emperor of France, and took the name Napoleon III.\n\nNot unexpectedly, the idea came up in bureaucratic circles that it would be useful to again require a loyalty oath from all state functionaries, including university professors. This time a cabinet minister was able to convince the Emperor to exempt Cauchy from the oath. Cauchy remained a professor at the University until his death at the age of 67. He received the Last Rites and died of a bronchial condition at 4 a.m. on May 23, 1857.\n\nHis name is one of the 72 names inscribed on the Eiffel Tower.\n\nThe genius of Cauchy was illustrated in his simple solution of the problem of Apollonius—describing a circle touching three given circles—which he discovered in 1805, his generalization of Euler's formula on polyhedra in 1811, and in several other elegant problems. More important is his memoir on wave propagation, which obtained the Grand Prix of the French Academy of Sciences in 1816. Cauchy's writings covered notable topics including: the theory of series, where he developed the notion of convergence and discovered many of the basic formulas for q-series. In the theory of numbers and complex quantities, he was the first to define complex numbers as pairs of real numbers. He also wrote on the theory of groups and substitutions, the theory of functions, differential equations and determinants.\n\nIn the theory of light he worked on Fresnel's wave theory and on the dispersion and polarization of light. He also contributed significant research in mechanics, substituting the notion of the continuity of geometrical displacements for the principle of the continuity of matter. He wrote on the equilibrium of rods and elastic membranes and on waves in elastic media. He introduced a 3 × 3 symmetric matrix of numbers that is now known as the Cauchy stress tensor. In elasticity, he originated the theory of stress, and his results are nearly as valuable as those of Siméon Poisson.\n\nOther significant contributions include being the first to prove the Fermat polygonal number theorem.\n\nCauchy is most famous for his single-handed development of complex function theory. The first pivotal theorem proved by Cauchy, now known as \"Cauchy's integral theorem\", was the following:\n\nwhere \"f\"(\"z\") is a complex-valued function holomorphic on and within the non-self-intersecting closed curve \"C\" (contour) lying in the complex plane. The \"contour integral\" is taken along the contour \"C\". The rudiments of this theorem can already be found in a paper that the 24-year-old Cauchy presented to the Académie des Sciences (then still called \"First Class of the Institute\") on August 11, 1814. In full form the theorem was given in 1825. The 1825 paper is seen by many as Cauchy's most important contribution to mathematics.\n\nIn 1826 Cauchy gave a formal definition of a residue of a function. This concept regards functions that have poles—isolated singularities, i.e., points where a function goes to positive or negative infinity. If the complex-valued function \"f\"(\"z\") can be expanded in the neighborhood of a singularity \"a\" as\n\nwhere φ(\"z\") is analytic (i.e., well-behaved without singularities), then \"f\" is said to have a pole of order \"n\" in the point \"a\". If \"n\" = 1, the pole is called simple.\nThe coefficient \"B\" is called by Cauchy the residue of function \"f\" at \"a\". If \"f\" is non-singular at \"a\" then the residue of \"f\" is zero at \"a\". Clearly the residue is in the case of a simple pole equal to,\nwhere we replaced \"B\" by the modern notation of the residue.\n\nIn 1831, while in Turin, Cauchy submitted two papers to the Academy of Sciences of Turin. In the first he proposed the formula now known as Cauchy's integral formula,\nwhere \"f\"(\"z\") is analytic on \"C\" and within the region bounded by the contour \"C\" and the complex number \"a\" is somewhere in this region. The contour integral is taken counter-clockwise. Clearly, the integrand has a simple pole at \"z\" = \"a\". In the second paper he presented the residue theorem,\nwhere the sum is over all the \"n\" poles of \"f\"(\"z\") on and within the contour \"C\". These results of Cauchy's still form the core of complex function theory as it is taught today to physicists and electrical engineers. For quite some time, contemporaries of Cauchy ignored his theory, believing it to be too complicated. Only in the 1840s the theory started to get response, with Pierre Alphonse Laurent being the first mathematician, besides Cauchy, making a substantial contribution (his Laurent series published in 1843).\n\n In his book \"Cours d'Analyse\" Cauchy stressed the importance of rigor in analysis. \"Rigor\" in this case meant the rejection of the principle of \"Generality of algebra\" (of earlier authors such as Euler and Lagrange) and its replacement by geometry and infinitesimals. Judith Grabiner wrote Cauchy was \"the man who taught rigorous analysis to all of Europe.\" The book is frequently noted as being the first place that inequalities, and formula_6 arguments were introduced into Calculus. Here Cauchy defined continuity as follows: \"The function f(x) is continuous with respect to x between the given limits if, between these limits, an infinitely small increment in the variable always produces an infinitely small increment in the function itself.\"\n\nM. Barany claims that the École mandated the inclusion of infinitesimal methods against Cauchy's better judgement . Gilain notes that when the portion of the curriculum devoted to \"Analyse Algébrique\" was reduced in 1825, Cauchy insisted on placing the topic of continuous functions (and therefore also infinitesimals) at the beginning of the Differential Calculus . Laugwitz (1989) and Benis-Sinaceur (1973) point out that Cauchy continued to use infinitesimals in his own research as late as 1853.\n\nCauchy gave an explicit definition of an infinitesimal in terms of a sequence tending to zero. There has been a vast body of literature written about Cauchy's notion of \"infinitesimally small quantities\", arguing they lead from everything from the usual \"epsilontic\" definitions or to the notions of non-standard analysis. The consensus is that Cauchy omitted or left implicit the important ideas to make clear the precise meaning of the infinitely small quantities he used. \n\nHe was the first to prove Taylor's theorem rigorously, establishing his well-known form of the remainder. He wrote a textbook (see the illustration) for his students at the École Polytechnique in which he developed the basic theorems of mathematical analysis as rigorously as possible. In this book he gave the necessary and sufficient condition for the existence of a limit in the form that is still taught. Also Cauchy's well-known test for absolute convergence stems from this book: Cauchy condensation test. In 1829 he defined for the first time a complex function of a complex variable in another textbook. In spite of these, Cauchy's own research papers often used intuitive, not rigorous, methods; thus one of his theorems was exposed to a \"counter-example\" by Abel, later fixed by the introduction of the notion of uniform continuity.\n\nIn a paper published in 1855, two years before Cauchy's death, he discussed some theorems, one of which is similar to the \"Argument Principle\" in many modern textbooks on complex analysis. In modern control theory textbooks, the Cauchy argument principle is quite frequently used to derive the Nyquist stability criterion, which can be used to predict the stability of negative feedback amplifier and negative feedback control systems. Thus Cauchy's work has a strong impact on both pure mathematics and practical engineering.\n\nCauchy was very productive, in number of papers second only to Leonhard Euler. It took almost a century to collect all his writings into 27 large volumes:\nHis greatest contributions to mathematical science are enveloped in the rigorous methods which he introduced; these are mainly embodied in his three great treatises:\nHis other works include:\n\nAugustin-Louis Cauchy grew up in the house of a staunch royalist. This made his father flee with the family to Arcueil during the French Revolution. Their life there during that time was apparently hard; Augustin-Louis's father, Louis François, spoke of living on rice, bread, and crackers during the period. A paragraph from an undated letter from Louis François to his mother in Rouen says:\n\nIn any event, he inherited his father's staunch royalism and hence refused to take oaths to any government after the overthrow of Charles X.\n\nHe was an equally staunch Catholic and a member of the Society of Saint Vincent de Paul. He also had links to the Society of Jesus and defended them at the Academy when it was politically unwise to do so. His zeal for his faith may have led to his caring for Charles Hermite during his illness and leading Hermite to become a faithful Catholic. It also inspired Cauchy to plead on behalf of the Irish during the Potato Famine.\n\nHis royalism and religious zeal also made him contentious, which caused difficulties with his colleagues. He felt that he was mistreated for his beliefs, but his opponents felt he intentionally provoked people by berating them over religious matters or by defending the Jesuits after they had been suppressed. Niels Henrik Abel called him a \"bigoted Catholic\" and added he was \"mad and there is nothing that can be done about him\", but at the same time praised him as a mathematician. Cauchy's views were widely unpopular among mathematicians and when Guglielmo Libri Carucci dalla Sommaja was made chair in mathematics before him he, and many others, felt his views were the cause. When Libri was accused of stealing books he was replaced by Joseph Liouville rather than Cauchy, which caused a rift between Liouville and Cauchy. Another dispute with political overtones concerned Jean Marie Constant Duhamel and a claim on inelastic shocks. Cauchy was later shown, by Jean-Victor Poncelet, to be wrong.\n\n\n\n"}
{"id": "762954", "url": "https://en.wikipedia.org/wiki?curid=762954", "title": "Barycentric coordinate system", "text": "Barycentric coordinate system\n\nIn geometry, the barycentric coordinate system is a coordinate system in which the location of a point of a simplex (a triangle, tetrahedron, etc.) is specified as the center of mass, or barycenter, of usually unequal masses placed at its vertices. Coordinates also extend outside the simplex, where one or more coordinates become negative. The system was introduced (1827) by August Ferdinand Möbius.\n\nLet formula_1 be the vertices of a simplex in an affine space \"A\". If, for some point formula_2 in \"A\",\n\nand at least one of formula_4 does not vanish\nthen we say that the coefficients (formula_4) are \"barycentric coordinates\" of formula_2 with respect to formula_1. The vertices themselves have the coordinates formula_8. Barycentric coordinates are not unique: for any \"b\" not equal to zero, (formula_9) are also barycentric coordinates of \"p\".\n\nWhen the coordinates are not negative, the point formula_2 lies in the convex hull of formula_1, that is, in the simplex which has those points as its vertices.\n\nBarycentric coordinates, as defined above, are a form of homogeneous coordinates: indeed, the \"usual\" homogeneous coordinates are the barycentric coordinates defined in the extended affine n-space on the simplex whose vertices are the points at infinity on the coordinate axes, plus the origin. Sometimes values of coordinates are restricted with a condition \nwhich makes them unique; then, they are affine coordinates. The classical terminology in this case is that of absolute barycentric coordinates.\n\nIn the context of a triangle, barycentric coordinates are also known as area coordinates or areal coordinates, because the coordinates of \"P\" with respect to triangle \"ABC\" are equivalent to the (signed) ratios of the areas of \"PBC\", \"PCA\" and \"PAB\" to the area of the reference triangle \"ABC\". Areal and trilinear coordinates are used for similar purposes in geometry.\n\nBarycentric or areal coordinates are extremely useful in engineering applications involving triangular subdomains. These make analytic integrals often easier to evaluate, and Gaussian quadrature tables are often presented in terms of area coordinates.\n\nConsider a triangle formula_13 defined by its three vertices, formula_14, formula_15 and formula_16. Each point formula_17 located inside this triangle can be written as a unique convex combination of the three vertices. In other words, for each formula_17 there is a unique sequence of three numbers, formula_19 such that formula_20 and\n\nThe three numbers formula_22 indicate the \"barycentric\" or \"area\" coordinates of the point formula_17 with respect to the triangle. They are often denoted as formula_24 instead of formula_22. Note that although there are three coordinates, there are only two degrees of freedom, since formula_20. Thus every point is uniquely defined by any two of the barycentric coordinates.\n\nTo explain why these coordinates are signed ratios of areas, let us assume that we work in the Euclidean space formula_27. Here, consider the Cartesian coordinate system formula_28 and its associated basis, namely formula_29. Consider also the positively oriented triangle formula_30 lying in the formula_31 plane. It is known that for any basis formula_32 of formula_27 and any free vector formula_34 one has\n\nwhere formula_36 stands for the mixed product of these three vectors.\n\nTake formula_37, where formula_38 is an arbitrary point in the plane formula_31, and remark that\n\nA subtle point regarding our choice of free vectors: formula_41 is, in fact, the equipollence class of the bound vector formula_42.\n\nWe have obtained that\n\nGiven the positive (counterclockwise) orientation of triangle formula_30, the denominator of both formula_45 and formula_46 is precisely the double of the area of the triangle formula_30. Also,\n\nand so the numerators of formula_45 and formula_46 are the doubles of the signed areas of triangles formula_51 and respectively formula_52.\n\nFurther, we deduce that\n\nwhich means that the numbers formula_54, formula_45 and formula_46 are the barycentric coordinates of formula_38. Similarly, the third barycentric coordinate reads as\n\nThis formula_59-letter notation of the barycentric coordinates comes from the fact that the point formula_38 may be interpreted as the center of mass for the masses formula_61, formula_45, formula_46 which are located in formula_64, formula_65 and formula_66.\n\nSwitching back and forth between the barycentric coordinates and other coordinate systems makes some problems much easier to solve.\n\nGiven a point formula_17 in a triangle's plane one can obtain the barycentric coordinates formula_68, formula_69 and formula_70 from the Cartesian coordinates formula_71 or vice versa.\n\nWe can write the Cartesian coordinates of the point formula_17 in terms of the Cartesian components of the triangle vertices formula_73, formula_74, formula_75 where formula_76 and in terms of the barycentric coordinates of formula_17 as\n\nThat is, the Cartesian coordinates of any point are a weighted average of the Cartesian coordinates of the triangle's vertices, with the weights being the point's barycentric coordinates summing to unity.\n\nTo find the reverse transformation, from Cartesian coordinates to barycentric coordinates, we first substitute formula_79 into the above to obtain\n\nRearranging, this is\n\nThis linear transformation may be written more succinctly as\n\nwhere formula_83 is the vector of the first two barycentric coordinates, formula_17 is the vector of Cartesian coordinates, and formula_85 is a matrix given by\n\nNow the matrix formula_85 is invertible, since formula_88 and formula_89 are linearly independent (if this were not the case, then formula_73, formula_74, and formula_75 would be collinear and would not form a triangle). Thus, we can rearrange the above equation to get\n\nFinding the barycentric coordinates has thus been reduced to finding the 2×2 inverse matrix of formula_85, an easy problem.\n\nExplicitly, the formulae for the barycentric coordinates of point formula_17 in terms of its Cartesian coordinates (\"x, y\") and in terms of the Cartesian coordinates of the triangle's vertices are:\n\nAnother way to solve the conversion from Cartesian to barycentric coordinates is to rewrite the problem in matrix form so that\n\nwith formula_100\nand\nformula_101.\nThen, the condition formula_102\nreads formula_103\nand the barycentric coordinates can be solved as the solution of\nthe linear system\n\nA point with trilinear coordinates \"x\" : \"y\" : \"z\" has barycentric coordinates \"ax\" : \"by\" : \"cz\" where \"a\", \"b\", \"c\" are the side lengths of the triangle. Conversely, a point with barycentrics formula_105 has trilinears formula_106\n\nThe sides \"a, b, c\" respectively have equations\n\nThe equation of a triangle's Euler line is\n\nUsing the previously given conversion between barycentric and trilinear coordinates, the various other equations given in Trilinear coordinates#Formulas can be rewritten in terms of barycentric coordinates.\n\nThe displacement vector of two normalized points formula_109 and formula_110 is\n\nThe distance formula_112 between formula_38 and formula_114, or the length of the displacement vector formula_115 is\n\nwhere \"a, b, c\" are the sidelengths of the triangle. The equivalence of the last two expressions follows from formula_117 which holds because formula_118\n\nThe barycentric coordinates of a point can be calculated based on distances \"d\" to the three triangle vertices by solving the equation\n\nAlthough barycentric coordinates are most commonly used to handle points inside a triangle, they can also be used to describe a point outside the triangle. If the point is not inside the triangle, then we can still use the formulas above to compute the barycentric coordinates. However, since the point is outside the triangle, at least one of the coordinates will violate our original assumption that formula_120. In fact, given any point in cartesian coordinates, we can use this fact to determine where this point is with respect to a triangle.\n\nIf a point lies in the interior of the triangle, all of the Barycentric coordinates lie in the open interval formula_121 If a point lies on an edge of the triangle but not at a vertex, one of the area coordinates formula_122 (the one associated with the opposite vertex) is zero, while the other two lie in the open interval formula_121 If the point lies on a vertex, the coordinate associated with that vertex equals 1 and the others equal zero. Finally, if the point lies outside the triangle at least one coordinate is negative.\n\nSummarizing,\n\nIn particular, if a point lies on the opposite side of a sideline from the vertex opposite that sideline, then that point's barycentric coordinate corresponding to that vertex is negative.\n\nIf formula_130 are known quantities, but the values of formula_131 inside the triangle defined by formula_132 is unknown, we can approximate these values using linear interpolation. Barycentric coordinates provide a convenient way to compute this interpolation. If formula_17 is a point inside the triangle with barycentric coordinates formula_68, formula_69, formula_70, then\n\nIn general, given any unstructured grid or polygon mesh, we can use this kind of technique to approximate the value of formula_131 at all points, as long as the function's value is known at all vertices of the mesh. In this case, we have many triangles, each corresponding to a different part of the space. To interpolate a function formula_131 at a point formula_17, we must first find a triangle that contains it. To do so, we first transform formula_17 into the barycentric coordinates of each triangle. If we find some triangle such that the coordinates satisfy formula_142, then the point lies in that triangle or on its edge (explained in the previous section). We can then interpolate the value of formula_143 as described above.\n\nThese methods have many applications, such as the finite element method (FEM).\n\nThe integral of a function over the domain of the triangle can be annoying to compute in a cartesian coordinate system. One generally has to split the triangle up into two halves, and great messiness follows. Instead, it is often easier to make a change of variables to any two barycentric coordinates, e.g. formula_144. Under this change of variables,\n\nwhere formula_64 is the area of the triangle. This result follows from the fact that a rectangle in barycentric coordinates corresponds to a quadrilateral in cartesian coordinates, and the ratio of the areas of the corresponding shapes in the corresponding coordinate systems is given by formula_147.\n\nThe three vertices of a triangle have barycentric coordinates formula_148\n\nThe centroid has barycentrics formula_149\n\nThe circumcenter of a triangle \"ABC\" has barycentric coordinates\n\nwhere are edge lengths respectively of the triangle.\n\nThe orthocenter has barycentric coordinates\n\nThe incenter has barycentric coordinates\n\nThe excenters' barycentrics are\n\nThe nine-point center has barycentric coordinates\n\nBarycentric coordinates may be easily extended to three dimensions. The 3D simplex is a tetrahedron, a polyhedron having four triangular faces and four vertices. Once again, the barycentric coordinates are defined so that the first vertex formula_73 maps to barycentric coordinates formula_159, formula_160, etc.\n\nThis is again a linear transformation, and we may extend the above procedure for triangles to find the barycentric coordinates of a point formula_17 with respect to a tetrahedron:\n\nwhere formula_85 is now a 3×3 matrix:\n\nOnce again, the problem of finding the barycentric coordinates has been reduced to inverting a 3×3 matrix. 3D barycentric coordinates may be used to decide if a point lies inside a tetrahedral volume, and to interpolate a function within a tetrahedral mesh, in an analogous manner to the 2D procedure. Tetrahedral meshes are often used in finite element analysis because the use of barycentric coordinates can greatly simplify 3D interpolation.\n\nBarycentric coordinates (\"a\", ..., \"a\") that are defined with respect to a polytope instead of a simplex are called \"generalized barycentric coordinates\". For these, the equation\n\nis still required to hold where \"x\", ..., \"x\" are the vertices of the given polytope. Thus, the definition is formally unchanged but while a simplex with \"n\" vertices needs to be embedded in a vector space of dimension of at least \"n-1\", a polytope may be embedded in a vector space of lower dimension. The simplest example is a quadrilateral in the plane. Consequently, even normalized generalized barycentric coordinates (i.e. coordinates such that the sum of the coefficients is 1) are in general not uniquely determined anymore while this is the case for normalized barycentric coordinates with respect to a simplex.\n\nMore abstractly, generalized barycentric coordinates express a polytope with \"n\" vertices, regardless of dimension, as the \"image\" of the standard formula_166-simplex, which has \"n\" vertices – the map is onto: formula_167 The map is one-to-one if and only if the polytope is a simplex, in which case the map is an isomorphism; this corresponds to a point not having \"unique\" generalized barycentric coordinates except when P is a simplex.\n\nDual to generalized barycentric coordinates are slack variables, which measure by how much margin a point satisfies the linear constraints, and gives an embedding formula_168 into the \"f\"-orthant, where \"f\" is the number of faces (dual to the vertices). This map is one-to-one (slack variables are uniquely determined) but not onto (not all combinations can be realized).\n\nThis use of the standard formula_166-simplex and \"f\"-orthant as standard objects that map to a polytope or that a polytope maps into should be contrasted with the use of the standard vector space formula_170 as the standard object for vector spaces, and the standard affine hyperplane formula_171 as the standard object for affine spaces, where in each case choosing a linear basis or affine basis provides an \"isomorphism,\" allowing all vector spaces and affine spaces to be thought of in terms of these standard spaces, rather than an onto or one-to-one map (not every polytope is a simplex). Further, the \"n\"-orthant is the standard object that maps \"to\" cones.\n\nGeneralized barycentric coordinates have applications in computer graphics and more specifically in geometric modelling. Often, a three-dimensional model can be approximated by a polyhedron such that the generalized barycentric coordinates with respect to that polyhedron have a geometric meaning. In this way, the processing of the model can be simplified by using these meaningful coordinates. Barycentric coordinates are also used in geophysics \n\n\n\n"}
{"id": "58468325", "url": "https://en.wikipedia.org/wiki?curid=58468325", "title": "Bicapped trigonal prismatic molecular geometry", "text": "Bicapped trigonal prismatic molecular geometry\n\nIn chemistry, the bicapped trigonal prismatic molecular geometry describes the shape of compounds where eight atoms or groups of atoms or ligands are arranged around a central atom defining the vertices of a biaugmented triangular prism. This shape has C symmetry and is one of the three common shapes for octacoordinate transition metal complexes, along with the square antiprism and the dodecahedron.\n\nOne example of the bicapped trigonal prismatic molecular geometry is the ZrF ion.\n"}
{"id": "19968510", "url": "https://en.wikipedia.org/wiki?curid=19968510", "title": "Bohr–van Leeuwen theorem", "text": "Bohr–van Leeuwen theorem\n\nThe Bohr–van Leeuwen theorem states that when statistical mechanics and classical mechanics are applied consistently, the thermal average of the magnetization is always zero. This makes magnetism in solids solely a quantum mechanical effect and means that classical physics cannot account for diamagnetism.\n\nWhat is today known as the Bohr–van Leeuwen theorem was discovered by Niels Bohr in 1911 in his doctoral dissertation and was later rediscovered by Hendrika Johanna van Leeuwen in her doctoral thesis in 1919. In 1932, van Vleck formalized and expanded upon Bohr's initial theorem in a book he wrote on electric and magnetic susceptibilities.\n\nThe significance of this discovery is that classical physics does not allow for such things as paramagnetism, diamagnetism and ferromagnetism and thus quantum physics are needed to explain the magnetic events. This result, \"perhaps the most deflationary publication of all time,\" may have contributed to Bohr's development of a quasi-classical theory of the hydrogen atom in 1913.\n\nThe Bohr–van Leeuwen theorem applies to an isolated system that cannot rotate (an isolated star could start rotating if exposed to a field). If, in addition, there is only one state of thermal equilibrium in a given temperature and field, and the system is allowed time to return to equilibrium after a field is applied, then there will be no magnetization.\n\nThe probability that the system will be in a given state of motion is predicted by Maxwell–Boltzmann statistics to be proportional to formula_1, where formula_2 is the energy of the system, formula_3 is the Boltzmann constant, and formula_4 is the absolute temperature. This energy is equal to the kinetic energy formula_5 for a particle with mass formula_6 and speed formula_7 and the potential energy.\n\nThe magnetic field does not contribute to the potential energy. The Lorentz force on a particle with charge formula_8 and velocity formula_9 is\nwhere formula_11 is the electric field and formula_12 is the magnetic flux density. The rate of work done is formula_13 and does not depend on formula_12. Therefore, the energy does not depend on the magnetic field, so the distribution of motions does not depend on the magnetic field.\n\nIn zero field, there will be no net motion of charged particles because the system is not able to rotate. There will therefore be an average magnetic moment of zero. Since the distribution of motions does not depend on the magnetic field, the moment in thermal equilibrium remains zero in any magnetic field.\n\nSo as to lower the complexity of the proof, a system with formula_15 electrons will be used.\n\nThis is appropriate, since most of the magnetism in a solid is carried by electrons, and the proof is easily generalized to more than one type of charged particle.\n\nEach electron has a negative charge formula_16 and mass formula_17.\n\nIf its position is formula_18 and velocity is formula_9, it produces a current formula_20 and a magnetic moment\n\nThe above equation shows that the magnetic moment is a linear function of the position coordinates, so the total magnetic moment in a given direction must be a linear function of the form\nwhere the dot represents a time derivative and formula_23 are vector coefficients depending on the position coordinates formula_24.\n\nMaxwell–Boltzmann statistics gives the probability that the nth particle has momentum formula_25 and coordinate formula_26 as\nwhere formula_28 is the Hamiltonian, the total energy of the system.\n\nThe thermal average of any function formula_29 of these generalized coordinates is then\n\nIn the presence of a magnetic field,\nwhere formula_32 is the magnetic vector potential and formula_33 is the electric scalar potential. \nFor each particle the components of the momentum formula_34 and position formula_35 are related by the equations of Hamiltonian mechanics:\nTherefore,\nso the moment formula_38 is a linear function of the momenta formula_34.\n\nThe thermally averaged moment,\nis the sum of terms proportional to integrals of the form\nwhere formula_42 represents one of the moment coordinates.\n\nThe integrand is an odd function of formula_42, so it vanishes.\n\nTherefore, formula_44.\n\nThe Bohr–van Leeuwen theorem is useful in several applications including plasma physics, \"All these references base their discussion of the Bohr–van Leeuwen theorem on Niels Bohr's physical model, in which perfectly reflecting walls are necessary to provide the currents that cancel the net contribution from the interior of an element of plasma, and result in zero net diamagnetism for the plasma element.\"\n\nDiamagnetism of a purely classical nature occurs in plasmas but is a consequence of thermal disequilibrium, such as a gradient in plasma density. Electromechanics and electrical engineering also see practical benefit from the Bohr–van Leeuwen theorem.\n\n\n\n"}
{"id": "28720055", "url": "https://en.wikipedia.org/wiki?curid=28720055", "title": "Bryan Shader", "text": "Bryan Shader\n\nBryan Lynn Shader (born 17 December 1961) is a professor of mathematics at the University of Wyoming. He received his Ph.D. from University of Wisconsin-Madison in 1990, his advisor was Professor Richard Brualdi. Shader is the Editor-in-chief of the \"Electronic Journal of Linear Algebra\". He is also Associate Editor of other two journals, \"Linear Algebra and its Applications\" (since 2003) and \"Linear & Multilinear Algebra\" (since 2009). He is one of the most active mathematicians working on Combinatorial Matrix Theory. He is also noted for his monograph on matrices of sign-solvable linear systems. Besides organizing many workshops he is a Co-PI of Math Teacher Leadership Program, an NSF project (2009–2014). Shader is Special Assistant to the Vice-President of Research of University of Wyoming.\n\nShader received the 2005 Burton W. Jones Distinguished Teaching Award from the Rocky Mountain Section of the Mathematical Association of America.\n\nBryan Shader has a daughter named Sarah Shader who is currently attending MIT and studying Computer Science and perhaps math.\n\n\n\n"}
{"id": "22672164", "url": "https://en.wikipedia.org/wiki?curid=22672164", "title": "Calkin–Wilf tree", "text": "Calkin–Wilf tree\n\nIn number theory, the Calkin–Wilf tree is a tree in which the vertices correspond 1-for-1 to the positive rational numbers. The tree is rooted at the number 1, and any rational number expressed in simplest terms as the fraction \"a\"/\"b\" has as its two children the numbers \"a\"/(\"a\" + \"b\") and (\"a\" + \"b\")/\"b\". Every positive rational number appears exactly once in the tree.\n\nThe sequence of rational numbers in a breadth-first traversal of the Calkin–Wilf tree is known as the Calkin–Wilf sequence. Its sequence of numerators (or, offset by one, denominators) is Stern's diatomic series, and can be computed by the fusc function.\n\nThe Calkin–Wilf tree is named after Neil Calkin and Herbert Wilf, who considered it in their 2000 paper. The tree was introduced earlier by Jean Berstel and Aldo de Luca as \"Raney tree\", since they drew some ideas from a paper by George N. Raney. Stern's diatomic series was formulated much earlier by Moritz Abraham Stern, a 19th-century German mathematician who also invented the closely related Stern–Brocot tree. Even earlier, a similar tree appears in Kepler's \"Harmonices Mundi\" (1619).\n\nThe Calkin–Wilf tree may be defined as a directed graph in which each positive rational number \"a\"/\"b\" occurs as a vertex and has one outgoing edge to another vertex, its parent. We assume that \"a\"/\"b\" is in simplest terms; that is, the greatest common divisor of \"a\" and \"b\" is 1. If \"a\"/\"b\" < 1, the parent of \"a\"/\"b\" is \"a\"/(\"b\" − \"a\"); if \"a\"/\"b\" is greater than one, the parent of \"a\"/\"b\" is (\"a\" − \"b\")/\"b\". Thus, in either case, the parent is a fraction with a smaller sum of numerator and denominator, so repeated reduction of this type must eventually reach the number 1. As a graph with one outgoing edge per vertex and one root reachable by all other vertices, the Calkin–Wilf tree must indeed be a tree.\n\nThe children of any vertex in the Calkin–Wilf tree may be computed by inverting the formula for the parents of a vertex. Each vertex \"a\"/\"b\" has one child whose value is less than 1, \"a\"/(\"a\" + \"b\"), because this is the only value less than 1 whose parent formula leads back to \"a\"/\"b\". Similarly, each vertex \"a\"/\"b\" has one child whose value is greater than 1, (\"a\" + \"b\")/\"b\".\n\nAlthough it is a binary tree (each vertex has two children), the Calkin–Wilf tree is not a binary search tree: its inorder does not coincide with the sorted order of its vertices. However, it is closely related to a different binary search tree on the same set of vertices, the Stern–Brocot tree: the vertices at each level of the two trees coincide, and are related to each other by a bit-reversal permutation.\n\nThe Calkin–Wilf sequence is the sequence of rational numbers generated by a breadth-first traversal of the Calkin–Wilf tree,\nBecause the Calkin–Wilf tree contains every positive rational number exactly once, so does this sequence. The denominator of each fraction equals the numerator of the next fraction in the sequence.\nThe Calkin–Wilf sequence can also be generated directly by the formula\n\nwhere formula_2 denotes the \"i\"th number in the sequence, starting from formula_3, and formula_4 represents the integral part.\n\nIt's also possible to calculate formula_2 directly from the run-length encoding of the binary representation of :\nthe number of consecutive 1-digits starting from the least significant bit, then the number of consecutive 0-digits starting from the first block of 1-digits, etc. The sequence of numbers generated in this way gives the continued fraction representation of formula_2.\n\nExample:<br>\ni = 1081 = formula_7: The continued fraction is [1;2,3,4,1] \nhence formula_8.<br>\ni = 1990 = formula_9: The continued fraction is [0;1,2,3,5] \nhence formula_10.<br>\n\nIn the other direction, using the continued fraction of any formula_2 as the run-length encoding of a binary number gives back itself.<br>\nExample:<br>\nformula_12: The continued fraction is [0;1,3] hence i = formula_13 = 14.<br>\nformula_14: The continued fraction is [1;3]. \nBut to use this method the length of the continued fraction must be an odd number. \nSo [1;3] should be replaced by the equivalent continued fraction [1;2,1]. Hence i = formula_15 = 9.\n\nA similar conversion between run-length-encoded binary numbers and continued fractions can also be used to evaluate Minkowski's question mark function; however,\nin the Calkin–Wilf tree the binary numbers are integers (positions in the breadth-first traversal) while in the question mark function they are real numbers between 0 and 1.\n\nStern's diatomic sequence is the integer sequence\nUsing zero-based numbering, the \"n\"th value in the sequence is the value fusc(\"n\") of the fusc function,\ndefined by the recurrence relations fusc(2\"n\") = fusc(\"n\") and fusc(2\"n\" + 1) = fusc(\"n\") + fusc(\"n\" + 1), with the base cases fusc(0) = 0 and fusc(1) = 1.\nThe \"n\"th rational number in a breadth-first traversal of the Calkin–Wilf tree is the number fusc(\"n\") / fusc(\"n\" + 1). Thus, the diatomic sequence forms both the sequence of numerators and the sequence of denominators of the numbers in the Calkin–Wilf sequence.\n\nThe function fusc(\"n\" + 1) is the number of odd binomial coefficients of the form formula_16 and\nalso counts the number of ways of writing \"n\" as a sum of powers of two in which each power occurs at most twice. This can be seen from the recurrence defining fusc: the expressions as a sum of powers of two for an even number 2\"n\" either have no 1's in them (in which case they are formed by doubling each term an expression for \"n\") or two 1's (in which case the rest of the expression is formed by doubling each term in an expression for \"n\" − 1), so the number of representations is the sum of the number of representations for \"n\" and for \"n\" − 1, matching the recurrence. Similarly, each representation for an odd number 2\"n\" + 1 is formed by doubling a representation for \"n\" and adding 1, again matching the recurrence. For instance,\nhas three representations as a sum of powers of two with at most two copies of each power, so fusc(6 + 1) = 3.\n\nThe Calkin–Wilf tree resembles the Stern–Brocot tree in that both are binary trees with each positive rational number appearing exactly once. Additionally, the top levels of the two trees appear very similar, and in both trees the same numbers appear at the same levels. One tree can be obtained from the other by performing a bit-reversal permutation on the numbers at each level of the trees. Alternatively, the number at a given node of the Calkin–Wilf tree can be converted into the number at the same position in the Stern–Brocot tree, and vice versa, by a process involving the reversal of the continued fraction representations of these numbers.\nHowever, in other ways they have different properties: for instance, the Stern–Brocot tree is a binary search tree: the left-to-right traversal order of the tree is the same as the numerical order of the numbers in it. This property is not true of the Calkin–Wilf tree.\n\n\n"}
{"id": "14599476", "url": "https://en.wikipedia.org/wiki?curid=14599476", "title": "Casey's theorem", "text": "Casey's theorem\n\nIn mathematics, Casey's theorem, also known as the generalized Ptolemy's theorem, is a theorem in Euclidean geometry named after the Irish mathematician John Casey.\n\nLet formula_1 be a circle of radius formula_2. Let formula_3 be (in that order) four non-intersecting circles that lie inside formula_1 and tangent to it. Denote by formula_5 the length of the exterior common bitangent of the circles formula_6. Then:\n\nNote that in the degenerate case, where all four circles reduce to points, this is exactly Ptolemy's theorem.\n\nThe following proof is attributable to Zacharias. Denote the radius of circle formula_8 by formula_9 and its tangency point with the circle formula_1 by formula_11. We will use the notation formula_12 for the centers of the circles.\nNote that from Pythagorean theorem,\n\nWe will try to express this length in terms of the points formula_14. By the law of cosines in triangle formula_15,\n\nSince the circles formula_17 tangent to each other:\n\nLet formula_19 be a point on the circle formula_1. According to the law of sines in triangle formula_21:\n\nTherefore,\n\nand substituting these in the formula above:\n\nAnd finally, the length we seek is\n\nWe can now evaluate the left hand side, with the help of the original Ptolemy's theorem applied to the inscribed quadrilateral formula_28:\n\nIt can be seen that the four circles need not lie inside the big circle. In fact, they may be tangent to it from the outside as well. In that case, the following change should be made:\nIf formula_6 are both tangent from the same side of formula_1 (both in or both out), formula_5 is the length of the exterior common tangent.\nIf formula_6 are tangent from different sides of formula_1 (one in and one out), formula_5 is the length of the interior common tangent.\nThe converse of Casey's theorem is also true. That is, if equality holds, the circles are tangent to a common circle.\n\nCasey's theorem and its converse can be used to prove a variety of statements in Euclidean geometry. For example, the shortest known proof of Feuerbach's theorem uses the converse theorem.\n\n"}
{"id": "1846827", "url": "https://en.wikipedia.org/wiki?curid=1846827", "title": "Catastrophe modeling", "text": "Catastrophe modeling\n\nCatastrophe modeling (also known as cat modeling) is the process of using computer-assisted calculations to estimate the losses that could be sustained due to a catastrophic event such as a hurricane or earthquake. Cat modeling is especially applicable to analyzing risks in the insurance industry and is at the confluence of actuarial science, engineering, meteorology, and seismology.\n\nNatural catastrophes (sometimes referred to as \"nat cat\") include: \nHuman catastrophes include: \n\n\nThe input into a typical cat modeling software package is information on the exposures being analyzed that are vulnerable to catastrophe risk. The exposure data can be categorized into three basic groups:\n\n\nThe output is an estimate of the losses that the model predicts would be associated with a particular event or set of events. When running a \"probabilistic\" model, the output is either a probabilistic loss distribution or a set of events that could be used to create a loss distribution; probable maximum losses (PMLs) and average annual losses (AALs) are calculated from the loss distribution. When running a \"deterministic\" model, losses caused by a specific event are calculated; for example, Hurricane Katrina or \"a magnitude 8.0 earthquake in downtown San Francisco\" could be analyzed against the portfolio of exposures.\n\n\nRecently, an effort to create and disseminate open multi-hazard cat risk modeling tools was initiated by the Alliance for Global Open Risk Analysis (AGORA).\n\nAlso, Oasis Loss Modelling Framework (LMF) http://www.oasislmf.org has been created. It has been founded as a not for profit organisation funded and owned by the Insurance Industry to promote open access to models and to promote transparency. Open source code for the framework is available at http://github.com/oasislmf .\n\nAdditionally, the insurance industry is currently working with the Association for Cooperative Operations Research and Development (ACORD) to develop an industry standard for collecting and sharing exposure data. To date, the industry has been operating on closed, proprietary data formats.\n\n\n"}
{"id": "32088502", "url": "https://en.wikipedia.org/wiki?curid=32088502", "title": "Coxeter notation", "text": "Coxeter notation\n\nIn geometry, Coxeter notation (also Coxeter symbol) is a system of classifying symmetry groups, describing the angles between fundamental reflections of a Coxeter group in a bracketed notation expressing the structure of a Coxeter-Dynkin diagram, with modifiers to indicate certain subgroups. The notation is named after H. S. M. Coxeter, and has been more comprehensively defined by Norman Johnson.\n\nFor Coxeter groups, defined by pure reflections, there is a direct correspondence between the bracket notation and Coxeter-Dynkin diagram. The numbers in the bracket notation represent the mirror reflection orders in the branches of the Coxeter diagram. It uses the same simplification, suppressing 2s between orthogonal mirrors.\n\nThe Coxeter notation is simplified with exponents to represent the number of branches in a row for linear diagram. So the \"A\" group is represented by [3], to imply \"n\" nodes connected by \"n-1\" order-3 branches. Example \"A\" = [3,3] = [3] or [3] represents diagrams or .\n\nCoxeter initially represented bifurcating diagrams with vertical positioning of numbers, but later abbreviated with an exponent notation, like [...,3] or [3], starting with [3] or [3,3] = or as D. Coxeter allowed for zeros as special cases to fit the \"A\" family, like \"A\" = [3,3,3,3] = [3] = [3] = [3] = [3], like = = .\n\nCoxeter groups formed by cyclic diagrams are represented by parenthesese inside of brackets, like [(p,q,r)] = for the triangle group (p q r). If the branch orders are equal, they can be grouped as an exponent as the length the cycle in brackets, like [(3,3,3,3)] = [3], representing Coxeter diagram or . can be represented as [3,(3,3,3)] or [3,3].\n\nMore complicated looping diagrams can also be expressed with care. The paracompact Coxeter group can be represented by Coxeter notation [(3,3,(3),3,3)], with nested/overlapping parentheses showing two adjacent [(3,3,3)] loops, and is also represented more compactly as [3], representing the rhombic symmetry of the Coxeter diagram. The paracompact complete graph diagram or , is represented as [3] with the superscript [3,3] as the symmetry of its regular tetrahedron coxeter diagram.\n\nThe Coxeter diagram usually leaves order-2 branches undrawn, but the bracket notation includes an explicit 2 to connect the subgraphs. So the Coxeter diagram = \"A\"×\"A\" = 2\"A\" can be represented by [3]×[3] = [3] = [3,2,3]. Sometimes explicit 2-branches may be included either with a 2 label, or with a line with a gap: or , as an identical presentation as [3,2,3].\n\nFor the affine and hyperbolic groups, the subscript is one less than the number of nodes in each case, since each of these groups was obtained by adding a node to a finite group's diagram.\n\nCoxeter's notation represents rotational/translational symmetry by adding a superscript operator outside the brackets, [X] which cuts the order of the group [X] in half, thus a index 2 subgroup. This operator implies an even number of operators must be applied, replacing reflections with rotations (or translations). When applied to a Coxeter group, this is called a direct subgroup because what remains are only direct isometries without reflective symmetry.\n\nThe operators can also be applied inside of the brackets, like [X,Y] or [X,(Y,Z)], and creates \"semidirect\" subgroups that may include both reflective and nonreflective generators. Semidirect subgroups can only apply to Coxeter group subgroups that have even order branches adjacent to it. Elements by parentheses inside of a Coxeter group can be give a superscript operator, having the effect of dividing adjacent ordered branches into half order, thus is usually only applied with even numbers. For example, [4,3] and [4,(3,3)] (). \n\nIf applied with adjacent odd branch, it doesn't create a subgroup of index 2, but instead creates overlapping fundamental domains, like [5,1] = [5/2], which can define doubly wrapped polygons like a pentagram, {5/2}, and [5,3] relates to Schwarz triangle [5/2,3,3], density 2.\n\nGroups without neighboring elements can be seen in ringed nodes Coxeter-Dynkin diagram for uniform polytopes and honeycomb are related to \"hole\" nodes around the elements, empty circles with the alternated nodes removed. So the snub cube, has symmetry [4,3] (), and the snub tetrahedron, has symmetry [4,3] (), and a demicube, h{4,3} = {3,3} ( or = ) has symmetry [1,4,3] = [3,3] ( or = = ).\n\nNote: Pyritohedral symmetry can be written as , separating the graph with gaps for clarity, with the generators {0,1,2} from the Coxeter group , producing pyritohedral generators {0,12}, a reflection and 3-fold rotation. And chiral tetrahedral symmetry can be written as or , [1,4,3] = [3,3], with generators {12,0120}.\n\nJohnson extends the operator to work with a placeholder 1 nodes, which removes mirrors, doubling the size of the fundamental domain and cuts the group order in half. In general this operation only applies to individual mirrors bounded by even-order branches. The 1 represents a mirror so [2p] can be seen as [2p,1], [1,2p], or [1,2p,1], like diagram or , with 2 mirrors related by an order-2p dihedral angle. The effect of a mirror removal is to duplicate connecting nodes, which can be seen in the Coxeter diagrams: = , or in bracket notation:[1,2p, 1] = [1,p,1] = [p].\n\nEach of these mirrors can be removed so h[2p] = [1,2p,1] = [1,2p,1] = [p], a reflective subgroup index 2. This can be shown in a Coxeter diagram by adding a symbol above the node: = = .\n\nIf both mirrors are removed, a quarter subgroup is generated, with the branch order becoming a gyration point of half the order: \n\nFor example, (with p=2): [4,1] = [1,4] = [2] = [ ]×[ ], order 4. [1,4,1] = [2], order 2.\n\nThe opposite to halving is doubling which adds a mirror, bisecting a fundamental domain, and doubling the group order.\n\nHalving operations apply for higher rank groups, like tetrahedral symmetry is a half group of octahedral group: h[4,3] = [1,4,3] = [3,3], removing half the mirrors at the 4-branch. The effect of a mirror removal is to duplicate all connecting nodes, which can be seen in the Coxeter diagrams: = , h[2p,3] = [1,2p,3] = [(p,3,3)].\n\nIf nodes are indexed, half subgroups can be labeled with new mirrors as composites. Like , generators {0,1} has subgroup = , generators {1,010}, where mirror 0 is removed, and replaced by a copy of mirror 1 reflected across mirror 0. Also given , generators {0,1,2}, it has half group = , generators {1,2,010}.\n\nDoubling by adding a mirror also applies in reversing the halving operation: = [4,3], or more generally = [2p,q].\n\nJohnson also added an asterisk or star * operator for \"radical\" subgroups, that acts similar to the operator, but removes rotational symmetry. The index of the radical subgroup is the order of the removed element. For example, [4,3*] ≅ [2,2]. The removed [3] subgroup is order 6 so [2,2] is an index 6 subgroup of [4,3].\n\nThe radical subgroups represent the inverse operation to an extended symmetry operation. For example, [4,3*] ≅ [2,2], and in reverse [2,2] can be extended as [3[2,2]] ≅ [4,3]. The subgroups can be expressed as a Coxeter diagram: or ≅ . The removed node (mirror) causes adjacent mirror virtual mirrors to become real mirrors.\n\nIf [4,3] has generators {0,1,2}, [4,3], index 2, has generators {0,12}; [1,4,3] ≅ [3,3], index 2 has generators {010,1,2}; while radical subgroup [4,3*] ≅ [2,2], index 6, has generators {01210, 2, (012)}; and finally [1,4,3*], index 12 has generators {0(12)0, (012)01}.\n\nA trionic subgroup is an index 3 subgroups. There are many\nJohnson defines a \"trionic subgroup\" with operator ⅄, index 3. For rank 2 Coxeter groups, [3], the trionic subgroup, [3] is [ ], a single mirror. And for [3\"p\"], the trionic subgroup is [3\"p\"] ≅ [\"p\"]. Given , with generators {0,1}, has 3 trionic subgroups. They can be differentiated by putting the ⅄ symbol next to the mirror generator to be removed, or on a branch for both: [3\"p\",1] = = , = , and [3\"p\"] = = with generators {0,10101}, {01010,1}, or {101,010}.\n\nTrionic subgroups of tetrahedral symmetry: [3,3] ≅ [2,4], relating the symmetry of the regular tetrahedron and tetragonal disphenoid.\n\nFor rank 3 Coxeter groups, [\"p\",3], there is a trionic subgroup [\"p\",3] ≅ [\"p\"/2,\"p\"], or = . For example the finite group [4,3] ≅ [2,4], and Euclidean group [6,3] ≅ [3,6], and hyperbolic group [8,3] ≅ [4,8]. \n\nAn odd-order adjacent branch, \"p\", will not lower the group order, but create overlapping fundamental domains. The group order stays the same, while the density increases. For example the icosahedral symmetry, [5,3], of the regular polyhedra icosahedron becomes [5/2,5], the symmetry of 2 regular star polyhedra. It also relates the hyperbolic tilings {p,3}, and star hyperbolic tilings {p/2,p} \n\nFor rank 4, [\"q\",2\"p\",3] = [2\"p\",((p,q,q))], = . \n\nFor example [3,4,3] = [4,3,3], or = , generators {0,1,2,3} in [3,4,3] with the trionic subgroup [4,3,3] generators {0,1,2,32123}. For hyperbolic groups, [3,6,3] = [6,3], and [4,4,3] = [4,4,4].\nJohnson identified two specific trionic subgroups of [3,3], first an index 3 subgroup [3,3] ≅ [2,4], with [3,3] ( = = ) generators {0,1,2}. It can also be written as [(3,3,2)] () as a reminder of its generators {02,1}. This symmetry reduction is the relationship between the regular tetrahedron and the tetragonal disphenoid, represent a stretching of a tetrahedron perpendicular to two opposite edges.\n\nSecondly he identifies a related index 6 subgroup [3,3] or [(3,3,2)] (), index 3 from [3,3] ≅ [2,2], with generators {02,1021}, from [3,3] and its generators {0,1,2}.\n\nThese subgroups also apply within larger Coxeter groups with [3,3] subgroup with neighboring branches all even order.\nFor example, [(3,3),4], [(3,3),4], and [(3,3),4] are subgroups of [3,3,4], index 2, 3 and 6 respectively. The generators of [(3,3),4] ≅ ≅ [8,2,8], order 128, are {02,1,3} from [3,3,4] generators {0,1,2,3}. And [(3,3),4] ≅ , order 64, has generators {02,1021,3}. As well, [3,4,3] ≅ [(3,3),4].\n\nAlso related [3] = [3,3,4,1] has trionic subgroups: [3] = [(3,3),4,1], order 64, and 1=[3] = [(3,3),4,1] ≅ 4,2,4, order 32.\n\nA central inversion, order 2, is operationally differently by dimension. The group [ ] = [2] represents \"n\" orthogonal mirrors in n-dimensional space, or an n-flat subspace of a higher dimensional space. The mirrors of the group [2] are numbered . The order of the mirrors doesn't matter in the case of an inversion. The matrix of a central inversion is , the Identity matrix with negative one on the diagonal.\n\nFrom that basis, the central inversion has a generator as the product of all the orthogonal mirrors. In Coxeter notation this inversion group is expressed by adding an alternation to each 2 branch. The alternation symmetry is marked on Coxeter diagram nodes as open nodes.\n\nA Coxeter-Dynkin diagram can be marked up with explicit 2 branches defining a linear sequence of mirrors, open-nodes, and shared double-open nodes to show the chaining of the reflection generators.\n\nFor example, [2,2] and [2,2] are subgroups index 2 of [2,2], , and are represented as (or ) and (or ) with generators {01,2} and {0,12} respectively. Their common subgroup index 4 is [2,2], and is represented by (or ), with the double-open marking a shared node in the two alternations, and a single rotoreflection generator {012}.\n\nRotations and rotary reflections are constructed by a single single-generator product of all the reflections of a prismatic group, [2\"p\"]×[2\"q\"]×... where gcd(\"p\",\"q\"...)=1, they are isomorphic to the abstract cyclic group Z, of order \"n\"=2\"pq\".\n\nThe 4-dimensional double rotations, [2\"p\",2,2\"q\"] (with gcd(\"p\",\"q\")=1), which include a central group, and are expressed by Conway as ±[C×C], order 2\"pq\". From Coxeter diagram , generators {0,1,2,3}, the single generator of [2\"p\",2,2\"q\"], is {0123}. The half group, [2\"p\",2,2\"q\"], or cyclc graph, [(2\"p\",2,2\"q\",2)], expressed by Conway is [C×C], order \"pq\", with generator {01230123}.\n\nIf there is a common factor \"f\", the double rotation can be written as [2\"pf\",2,2\"qf\"] (with gcd(\"p\",\"q\")=1), generator {0123}, order 2\"pqf\". For example, \"p\"=\"q\"=1, \"f\"=2, [4,2,4] is order 4. And [2\"pf\",2,2\"qf\"], generator {01230123}, is order \"pqf\". For example [4,2,4] is order 2, a central inversion.\nSimple groups with only odd-order branch elements have only a single rotational/translational subgroup of order 2, which is also the commutator subgroup, examples [3,3], [3,5], [3,3,3], [3,3,5]. For other Coxeter groups with even-order branches, the commutator subgroup has index 2, where c is the number of disconnected subgraphs when all the even-order branches are removed. For example, [4,4] has three independent nodes in the Coxeter diagram when the 4s are removed, so its commutator subgroup is index 2, and can have different representations, all with three operators: [4,4], [1,4,1,4,1], [1,4,4,1], or [(4,4,2)]. A general notation can be used with +\"c\" as a group exponent, like [4,4].\n\nDihedral symmetry groups with even-orders have a number of subgroups. This example shows two generator mirrors of [4] in red and green, and looks at all subgroups by halfing, rank-reduction, and their direct subgroups. The group [4], has two mirror generators 0, and 1. Each generate two virtual mirrors 101 and 010 by reflection across the other.\nThe [4,4] group has 15 small index subgroups. This table shows them all, with a yellow fundamental domain for pure reflective groups, and alternating white and blue domains which are paired up to make rotational domains. Cyan, red, and green mirror lines correspond to the same colored nodes in the Coxeter diagram. Subgroup generators can be expressed as products of the original 3 mirrors of the fundamental domain, {0,1,2}, corresponding to the 3 nodes of the Coxeter diagram, . A product of two intersecting reflection lines makes a rotation, like {012}, {12}, or {02}. Removing a mirror causes two copies of neighboring mirrors, across the removed mirror, like {010}, and {212}. Two rotations in series cut the rotation order in half, like {0101} or {(01)}, {1212} or {(02)}. A product of all three mirrors creates a transreflection, like {012} or {120}.\n\nThe same set of 15 small subgroups exists on all triangle groups with even order elements, like [6,4] in the hyperbolic plane:\n\nCoxeter's notation includes double square bracket notation, to express automorphic symmetry within a Coxeter diagram. Johnson added alternative of angled-bracket <[X]> or [X] option as equivalent to square brackets for doubling to distinguish diagram symmetry through the nodes versus through the branches. Johnson also added a prefix symmetry modifier [Y[X]], where Y can either represent symmetry of the Coxeter diagram of [X], or symmetry of the fundamental domain of [X].\n\nFor example, in 3D these equivalent rectangle and rhombic geometry diagrams of formula_1: and , the first doubled with square brackets, or twice doubled as [2[3]], with [2], order 4 higher symmetry. To differentiate the second, angled brackets are used for doubling, [3] and twice doubled as 2[3], also with a different [2], order 4 symmetry. Finally a full symmetry where all 4 nodes are equivalent can be represented by [4[3]], with the order 8, [4] symmetry of the square. But by considering the tetragonal disphenoid fundamental domain the [4] extended symmetry of the square graph can be marked more explicitly as [(2,4)[3]] or [2,4[3]].\n\nFurther symmetry exists in the cyclic formula_2 and branching formula_3, formula_4, and formula_5 diagrams. formula_2 has order 2\"n\" symmetry of a regular \"n\"-gon, {\"n\"}, and is represented by [\"n\"[3]]. formula_3 and formula_4 are represented by [3[3]] = [3,4,3] and [3[3]] respectively while formula_5 by [(3,3)[3]] = [3,3,4,3], with the diagram containing the order 24 symmetry of the regular tetrahedron, {3,3}. The paracompact hyperbolic group formula_10 = [3], , contains the symmetry of a 5-cell, {3,3,3}, and thus is represented by [(3,3,3)[3]] = [3,4,3,3,3].\n\nAn asterisk * superscript is effectively an inverse operation, creating \"radical subgroups\" removing connected of odd-ordered mirrors.\n\nExamples:\nLooking at generators, the double symmetry is seen as adding a new operator that maps symmetric positions in the Coxeter diagram, making some original generators redundant. For 3D space groups, and 4D point groups, Coxeter defines an index two subgroup of , , which he defines as the product of the original generators of [X] by the doubling generator. This looks similar to , which is the chiral subgroup of . So for example the 3D space groups (I432, 211) and (Pmn, \n223) are distinct subgroups of (Imm, 229).\n\nA Coxeter group, represented by Coxeter diagram , is given Coxeter notation [p,q] for the branch orders. Each node in the Coxeter diagram represents a mirror, by convention called ρ (and matrix R). The \"generators\" of this group [p,q] are reflections: ρ, ρ, and ρ. Rotational subsymmetry is given as products of reflections: By convention, σ (and matrix S) = ρρ represents a rotation of angle π/p, and σ = ρρ is a rotation of angle π/q, and σ = ρρ represents a rotation of angle π/2.\n\n[p,q], , is an index 2 subgroup represented by two rotation generators, each a products of two reflections: σ, σ, and representing rotations of π/\"p\", and π/\"q\" angles respectively.\n\nWith one even branch, [\"p\",2\"q\"], or , is another subgroup of index 2, represented by rotation generator σ, and reflectional ρ.\n\nWith even branches, [2\"p\",2\"q\"], , is a subgroup of index 4 with two generators, constructed as a product of all three reflection matrices: By convention as: ψ and ψ, which are rotary reflections, representing a reflection and rotation or reflection.\n\nIn the case of affine Coxeter groups like , or , one mirror, usually the last, is translated off the origin. A translation generator τ (and matrix T) is constructed as the product of two (or an even number of) reflections, including the affine reflection. A transreflection (reflection plus a translation) can be the product of an odd number of reflections φ (and matrix V), like the index 4 subgroup : [4,4] = .\n\nAnother composite generator, by convention as ζ (and matrix Z), represents the inversion, mapping a point to its inverse. For [4,3] and [5,3], ζ = (ρρρ), where \"h\" is 6 and 10 respectively, the Coxeter number for each family. For 3D Coxeter group [p,q] (), this subgroup is a rotary reflection [2,h].\n\nCoxeter groups are categorized by their rank, being the number of nodes in its Coxeter-Dynkin diagram. The structure of the groups are also given with their abstract group types: In this article, the abstract dihedral groups are represented as \"Dih\", and cyclic groups are represented by \"Z\", with \"Dih\"=\"Z\".\n\nExample, in 2D, the Coxeter group [p] () is represented by two reflection matrices R and R, The cyclic symmetry [p] () is represented by rotation generator of matrix S.\nThe finite rank 3 Coxeter groups are [1,\"p\"], [2,\"p\"], [3,3], [3,4], and [3,5].\n\nTo reflect a point through a plane formula_11 (which goes through the origin), one can use formula_12, where formula_13 is the 3x3 identity matrix and formula_14 is the three-dimensional unit vector for the vector normal of the plane. If the L2 norm of formula_15 and formula_16 is unity, the transformation matrix can be expressed as:\n\nThe reducible 3-dimensional finite reflective group is dihedral symmetry, [\"p\",2], order 4\"p\", . The reflection generators are matrices R, R, R. R=R=R=(R×R)=(R×R)=(R×R)=Identity. [\"p\",2] () is generated by 2 of 3 rotations: S, S, and S. An order \"p\" rotoreflection is generated by V, the product of all 3 reflections.\n\nThe simplest irreducible 3-dimensional finite reflective group is tetrahedral symmetry, [3,3], order 24, . The reflection generators, from a D=A construction, are matrices R, R, R. R=R=R=(R×R)=(R×R)=(R×R)=Identity. [3,3] () is generated by 2 of 3 rotations: S, S, and S. A trionic subgroup, isomorphic to [2,4], order 8, is generated by S and R. An order 4 rotoreflection is generated by V, the product of all 3 reflections.\n\nAnother irreducible 3-dimensional finite reflective group is octahedral symmetry, [4,3], order 48, . The reflection generators matrices are R, R, R. R=R=R=(R×R)=(R×R)=(R×R)=Identity. Chiral octahedral symmetry, [4,3], () is generated by 2 of 3 rotations: S, S, and S. Pyritohedral symmetry [4,3], () is generated by reflection R and rotation S. A 6-fold rotoreflection is generated by V, the product of all 3 reflections.\n\nA final irreducible 3-dimensional finite reflective group is icosahedral symmetry, [5,3], order 120, . The reflection generators matrices are R, R, R. R=R=R=(R×R)=(R×R)=(R×R)=Identity. [5,3] () is generated by 2 of 3 rotations: S, S, and S. A 10-fold rotoreflection is generated by V, the product of all 3 reflections.\n\nA simple example affine group is [4,4] () (p4m), can be given by three reflection matrices, constructed as a reflection across the x axis (y=0), a diagonal (x=y), and the affine reflection across the line (x=1). [4,4] () (p4) is generated by S S, and S. [4,4] () (pgg) is generated by 2-fold rotation S and transreflection V. [4,4] () (p4g) is generated by S and R. The group [(4,4,2)] () (cmm), is generated by 2-fold rotation S and reflection R.\nA irreducible 4-dimensional finite reflective group is hyperoctahedral group (or hexadecachoric group (for 16-cell), B=[4,3,3], order 384, . The reflection generators matrices are R, R, R, R. R=R=R=R=(R×R)=(R×R)=(R×R)=(R×R)=(R×R)=(R×R)=Identity. \n\nChiral hyperoctahedral symmetry, [4,3,3], () is generated by 3 of 6 rotations: S, S, S, S, S, and S. Hyperpyritohedral symmetry [4,(3,3)], () is generated by reflection R and rotations S and S. An 8-fold double rotation is generated by W, the product of all 4 reflections.\n\nA irreducible 4-dimensional finite reflective group is Icositetrachoric group (for 24-cell), F=[3,4,3], order 1152, . The reflection generators matrices are R, R, R, R. R=R=R=R=(R×R)=(R×R)=(R×R)=(R×R)=(R×R)=(R×R)=Identity. \n\nChiral icositetrachoric symmetry, [3,4,3], () is generated by 3 of 6 rotations: S, S, S, S, S, and S. Ionic diminished [3,4,3] group, () is generated by reflection R and rotations S and S. An 12-fold double rotation is generated by W, the product of all 4 reflections.\n\nThe hyper-icosahedral symmetry, [5,3,3], order 14400, . The reflection generators matrices are R, R, R, R. R=R=R=R=(R×R)=(R×R)=(R×R)=(R×R)=(R×R)=(R×R)=Identity. [5,3,3] () is generated by 3 rotations: S = R×R, S = R×R, S = R×R, etc.\n\nIn one dimension, the \"bilateral group\" [ ] represents a single mirror symmetry, abstract \"Dih\" or \"Z\", symmetry order 2. It is represented as a Coxeter–Dynkin diagram with a single node, . The \"identity group\" is the direct subgroup [ ], Z, symmetry order 1. The + superscript simply implies that alternate mirror reflections are ignored, leaving the identity group in this simplest case. Coxeter used a single open node to represent an alternation, .\n\nIn two dimensions, the \"rectangular group\" [2], abstract \"D\" or \"D\", also can be represented as a direct product [ ]×[ ], being the product of two bilateral groups, represents two orthogonal mirrors, with Coxeter diagram, , with order 4. The 2 in [2] comes from linearization of the orthogonal subgraphs in the Coxeter diagram, as with explicit branch order 2. The \"rhombic group\", [2] ( or ), half of the rectangular group, the point reflection symmetry, Z, order 2.\n\nCoxeter notation to allow a 1 place-holder for lower rank groups, so [1] is the same as [ ], and [1] or [1] is the same as [ ] and Coxeter diagram .\n\nThe \"full p-gonal group\" [p], abstract \"dihedral group\" D, (nonabelian for p>2), of order 2\"p\", is generated by two mirrors at angle \"π\"/\"p\", represented by Coxeter diagram . The \"p-gonal\" subgroup [p], \"cyclic group\" \"Z\", of order \"p\", generated by a rotation angle of \"π\"/\"p\".\n\nCoxeter notation uses double-bracking to represent an automorphic \"doubling\" of symmetry by adding a bisecting mirror to the fundamental domain. For example, <nowiki>p</nowiki> adds a bisecting mirror to [p], and is isomorphic to [2p].\n\nIn the limit, going down to one dimensions, the \"full apeirogonal group\" is obtained when the angle goes to zero, so [∞], abstractly the infinite dihedral group D, represents two parallel mirrors and has a Coxeter diagram . The \"apeirogonal group\" [∞], , abstractly the infinite cyclic group Z, isomorphic to the \"additive group\" of the integers, is generated by a single nonzero translation.\n\nIn the hyperbolic plane, there is a \"full pseudogonal group\" [\"iπ/λ\"], and \"pseudogonal subgroup\" [\"iπ/λ\"], . These groups exist in regular infinite-sided polygons, with edge length λ. The mirrors are all orthogonal to a single line.\n\nPoint groups in 3 dimensions can be expressed in bracket notation related to the rank 3 Coxeter groups:\nIn three dimensions, the \"full orthorhombic group\" or \"orthorectangular\" [2,2], abstractly \"D\"×\"D\", order 8, represents three orthogonal mirrors, (also represented by Coxeter diagram as three separate dots ). It can also can be represented as a direct product [ ]×[ ]×[ ], but the [2,2] expression allows subgroups to be defined:\n\nFirst there is a \"semidirect\" subgroup, the \"orthorhombic group\", [2,2] ( or ), abstractly \"D\"×\"Z\"=\"Z\"×\"Z\", of order 4. When the + superscript is given inside of the brackets, it means reflections generated only from the adjacent mirrors (as defined by the Coxeter diagram, ) are alternated. In general, the branch orders neighboring the + node must be even. In this case [2,2] and [2,2] represent two isomorphic subgroups that are geometrically distinct. The other subgroups are the \"pararhombic group\" [2,2] ( or ), also order 4, and finally the \"central group\" [2,2] ( or ) of order 2.\n\nNext there is the \"full ortho-\"p\"-gonal group\", [2,p] (), abstractly \"D\"×\"D\"=\"Z\"×\"D\", of order 4p, representing two mirrors at a dihedral angle π/\"p\", and both are orthogonal to a third mirror. It is also represented by Coxeter diagram as .\n\nThe direct subgroup is called the para-\"p\"-gonal group, [2,p] ( or ), abstractly \"D\", of order 2p, and another subgroup is [2,p] () abstractly \"D\"×\"Z\", also of order 2p.\n\nThe \"full gyro-p-gonal group\", [2,2\"p\"] ( or ), abstractly \"D\", of order 4\"p\". The gyro-\"p\"-gonal group, [2,2p] ( or ), abstractly \"Z\", of order 2\"p\" is a subgroup of both [2,2\"p\"] and [2,2\"p\"].\n\nThe polyhedral groups are based on the symmetry of platonic solids: the tetrahedron, octahedron, cube, icosahedron, and dodecahedron, with Schläfli symbols {3,3}, {3,4}, {4,3}, {3,5}, and {5,3} respectively. The Coxeter groups for these are: [3,3] (), [3,4] (), [3,5] () called full tetrahedral symmetry, octahedral symmetry, and icosahedral symmetry, with orders of 24, 48, and 120.\n\nIn all these symmetries, alternate reflections can be removed producing the rotational tetrahedral [3,3](), octahedral [3,4] (), and icosahedral [3,5] () groups of order 12, 24, and 60. The octahedral group also has a unique index 2 subgroup called the pyritohedral symmetry group, [3,4] ( or ), of order 12, with a mixture of rotational and reflectional symmetry. Pyritohedral symmetry is also an index 5 subgroup of icosahedral symmetry: --> , with virtual mirror 1 across 0, {010}, and 3-fold rotation {12}.\n\nThe tetrahedral group, [3,3] (), has a doubling (which can be represented by colored nodes ), mapping the first and last mirrors onto each other, and this produces the [3,4] ( or ) group. The subgroup [3,4,1] ( or ) is the same as [3,3], and [3,4,1] ( or ) is the same as [3,3].\n\nIn the Euclidean plane there's 3 fundamental reflective groups generated by 3 mirrors, represented by Coxeter diagrams , , and , and are given Coxeter notation as [4,4], [6,3], and [(3,3,3)]. The parentheses of the last group imply the diagram cycle, and also has a shorthand notation [3].\n\nDirect subgroups of rotational symmetry are: [4,4], [6,3], and [(3,3,3)]. [4,4] and [6,3] are semidirect subgroups.\n\nGiven in Coxeter notation (orbifold notation), some low index affine subgroups are:\nRank four groups defined the 4-dimensional point groups:\n\nRank four groups also defined the 3-dimensional line groups:\n\nRank four groups defined the 4-dimensional duoprismatic groups. In the limit as p and q go to infinity, they degenerate into 2 dimensions and the wallpaper groups.\nRank four groups also defined some of the 2-dimensional wallpaper groups, as limiting cases of the four-dimensional duoprism groups:\nSubgroups of [∞,2,∞], (*2222) can be expressed down to its index 16 commutator subgroup:\n\nCoxeter notation has been extended to Complex space, C where nodes are unitary reflections of period greater than 2. Nodes are labeled by an index, assumed to be 2 for ordinary real reflection if suppressed. Complex reflection groups are called Shephard groups rather than Coxeter groups, and can be used to construct complex polytopes.\n\nIn formula_18, a rank 1 shephard group , order \"p\", is represented as [], [] or ][. It has a single generator, representing a 2\"π\"/\"p\" radian rotation in the Complex plane: formula_19.\n\nCoxeter writes the rank 2 complex group, [\"q\"] represents Coxeter diagram . The \"p\" and \"r\" should only be suppressed if both are 2, which is the real case [\"q\"]. The order of a rank 2 group [\"q\"] is formula_20.\n\nThe rank 2 solutions that generate complex polygons are: [4] (\"p\" is 2,3,4...), [3], [6], [4], [3], [8], [6], [4], [5], [3], [10], [6], and [4] with Coxeter diagrams , , , , , , , , , , , , . \nInfinite groups are [12], [8], [6], [6], [4], [4], and [3] or , , , , , , .\n\nIndex 2 subgroups exists by removing a real reflection: [2\"q\"] → [\"q\"]. Also index \"r\" subgroups exist for 4 branches: [4] → [\"r\"].\n\nFor the infinite family [4], for any \"p\" = 2, 3, 4..., there are two subgroups: [4] → [\"p\"], index \"p\", while and [4] → []×[], index 2.\n"}
{"id": "40530379", "url": "https://en.wikipedia.org/wiki?curid=40530379", "title": "Cross-serial dependencies", "text": "Cross-serial dependencies\n\nIn linguistics, cross-serial dependencies (also called crossing dependencies by some authors) occur when the lines representing the dependency relations between two series of words cross over each other. They are of particular interest to linguists who wish to determine the syntactic structure of natural language; languages containing an arbitrary number of them are non-context-free. By this fact, Dutch and Swiss-German have been proved to be non-context-free.\n\nAs Swiss-German allows verbs and their arguments to be ordered cross-serially, we have the following example, taken from Shieber:\n\nThat is, \"we help Hans paint the house.\"\n\nNotice that the sequential noun phrases \"em Hans\" (\"Hans\") and \"es huus\" (\"the house\"), and the sequential verbs \"hälfed\" (\"help\") and \"aastriiche\" (\"paint\") both form two separate series of constituents. Notice also that the dative verb \"hälfed\" and the accusative verb \"aastriiche\" take the dative \"em Hans\" and accusative \"s huus\" as their arguments, respectively.\n\nIn Swiss-German sentences, the number of verbs of a grammatical case (dative or accusative) must match the number of objects of that case. Additionally, a sentence containing an arbitrary number of such objects is admissible (in principle). Hence, the following formal language is grammatical:\n\nformula_1\n\nIt can be seen that formula_2 is of the form formula_3. By taking another image to remove the formula_4, formula_5 and formula_6, the non-context-free language formula_7 may be observed. All spoken languages which contain cross-serial dependencies also contain a language of a form similar to formula_8.\n\nResearch in mildly context-sensitive language has attempted to identify a narrower and more computationally tractable subclass of context-sensitive languages that can capture context sensitivity as found in natural languages. For example cross-serial dependencies can be expressed in linear context-free rewriting systems (LCFRS); one can write a LCFRS grammar for {\"abcd\" | \"n\" ≥ 1} for example.\n"}
{"id": "1337370", "url": "https://en.wikipedia.org/wiki?curid=1337370", "title": "Cross section (geometry)", "text": "Cross section (geometry)\n\nIn geometry and science, a cross section is the non-empty intersection of a solid body in three-dimensional space with a plane, or the analog in higher-dimensional spaces. Cutting an object into slices creates many parallel cross sections. The boundary of a cross section in three-dimensional space that is parallel to two of the axes, that is, parallel to the plane determined by these axes, is a sometimes referred to as a contour line; for example, if a plane cuts through mountains of a raised-relief map parallel to the ground, the result is a contour line in two-dimensional space showing points on the surface of the mountains of equal elevation.\n\nIn technical drawing a cross-section, being a projection of an object onto a plane that intersects it, is a common tool used to depict the internal arrangement of a 3-dimensional object in two dimensions. It is traditionally crosshatched with the style of crosshatching often indicating the types of materials being used.\n\nWith computed axial tomography, computers construct cross-sections from x-ray data.\n\nIf a plane intersects a solid (a 3-dimensional object), then the region common to the plane and the solid is called a cross-section of the solid. A plane containing a cross-section of the solid may be referred to as a \"cutting plane\".\n\nThe shape of the cross-section of a solid may depend upon the orientation of the cutting plane to the solid. For instance, while all the cross-sections of a ball are disks, the cross-sections of a cube depend on how the cutting plane is related to the cube. If the cutting plane is perpendicular to a line joining the centers of two opposite faces of the cube, the cross-section will be a square, however, if the cutting plane is perpendicular to a diagonal of the cube joining opposite vertices, the cross-section can be either a point, a triangle or a hexagon.\n\nA related concept is that of a plane section, which is the curve of intersection of a plane with a \"surface\". Thus, a plane section is the boundary of a cross-section of a solid in a cutting plane. \n\nIf a surface in a three-dimensional space is defined by a function of two variables, i.e., , the plane sections by cutting planes that are parallel to a coordinate plane (a plane determined by two coordinate axes) are called level curves or isolines.\nMore specifically, cutting planes with equations of the form (planes parallel to the -plane) produce plane sections that are often called contour lines in application areas.\n\nA cross section of a polyhedron is a polygon.\n\nThe conic sections – circles, ellipses, parabolas, and hyperbolas – are plane sections of a cone with the cutting planes at various different angles, as seen in the diagram at left.\n\nAny cross-section passing through the center of an ellipsoid forms an elliptic region, while the corresponding plane sections are ellipses on its surface. These degenerate to disks and circles, respectively, when the cutting planes are perpendicular to a symmetry axis. In more generality, the plane sections of a quadric are conic sections.\n\nA cross-section of a solid right circular cylinder extending between two bases is a disk if the cross-section is parallel to the cylinder's base, or an elliptic region (see diagram at right) if it is neither parallel nor perpendicular to the base. If the cutting plane is perpendicular to the base it consists of a rectangle (not shown) unless it is just tangent to the cylinder, in which case it is a single line segment.\n\nThe term cylinder can also mean the lateral surface of a solid cylinder (see Cylinder (geometry)). If cylinder is used in this sense, the above paragraph would read as follows: A plane section of a right circular cylinder of finite length is a circle if the cutting plane is perpendicular to the cylinder's axis of symmetry, or an ellipse if it is neither parallel nor perpendicular to that axis. If the cutting plane is parallel to the axis the plane section consists of a pair of parallel line segments unless the cutting plane is tangent to the cylinder, in which case, the plane section is a single line segment.\nA plane section can be used to visualize the partial derivative of a function with respect to one of its arguments, as shown. Suppose . In taking the partial derivative of with respect to , one can take a plane section of the function at a fixed value of to plot the level curve of solely against ; then the partial derivative with respect to is the slope of the resulting two-dimensional graph.\n\nA plane section of a probability density function of two random variables in which the cutting plane is at a fixed value of one of the variables is a conditional density function of the other variable (conditional on the fixed value defining the plane section). If instead the plane section is taken for a fixed value of the density, the result is an iso-density contour. For the normal distribution, these contours are ellipses.\n\nIn economics, a production function specifies the output that can be produced by various quantities and of inputs, typically labor and physical capital. The production function of a firm or a society can be plotted in three-dimensional space. If a plane section is taken parallel to the -plane, the result is an isoquant showing the various combinations of labor and capital usage that would result in the level of output given by the height of the plane section. Alternatively, if a plane section of the production function is taken at a fixed level of —that is, parallel to the -plane—then the result is a two-dimensional graph showing how much output can be produced at each of various values of usage of one input combined with the fixed value of the other input .\n\nAlso in economics, a cardinal or ordinal utility function gives the degree of satisfaction of a consumer obtained by consuming quantities and of two goods. If a plane section of the utility function is taken at a given height (level of utility), the two-dimensional result is an indifference curve showing various alternative combinations of consumed amounts and of the two goods all of which give the specified level of utility.\n\nCavalieri's principle states that solids with corresponding cross sections of equal areas have equal volumes.\n\nThe cross-sectional area (formula_1) of an object when viewed from a particular angle is the total area of the orthographic projection of the object from that angle. For example, a cylinder of height \"h\" and radius \"r\" has formula_2 when viewed along its central axis, and formula_3 when viewed from an orthogonal direction. A sphere of radius \"r\" has formula_2 when viewed from any angle. More generically, formula_1 can be calculated by evaluating the following surface integral:\n\nwhere formula_7 is the unit vector pointing along the viewing direction toward the viewer, formula_8 is a surface element with an outward-pointing normal, and the integral is taken only over the top-most surface, that part of the surface that is \"visible\" from the perspective of the viewer. For a convex body, each ray through the object from the viewer's perspective crosses just two surfaces. For such objects, the integral may be taken over the entire surface (formula_9) by taking the absolute value of the integrand (so that the \"top\" and \"bottom\" of the object do not subtract away, as would be required by the Divergence Theorem applied to the constant vector field formula_7) and dividing by two:\n\nIn analogy with the cross-section of a solid, the cross-section of an -dimensional body in an -dimensional space is the non-empty intersection of the body with a hyperplane (an -dimensional subspace). This concept has sometimes been used to help visualize aspects of higher dimensional spaces. For instance, \nif a four-dimensional object passed through our three-dimensional space, we would see a three-dimensional cross-section of the four-dimensional object. In particular, a 4-ball (hypersphere) passing through 3-space would appear as a 3-ball that increased to a maximum and then decreased in size during the transition. This dynamic object (from the point of view of 3-space) is a sequence of cross-sections of the 4-ball.\n\nIn geology, the structure of the interior of a planet is often illustrated using a diagram of a cross section of the planet that passes through the planet's center, as in the cross section of Earth at right.\n\nCross-sections are often used in anatomy to illustrate the inner structure of an organ, as shown at left.\n\nA cross-section of a tree trunk, as shown at left, reveals growth rings that can be used to find the age of the tree and the temporal properties of its environment.\n\n\n"}
{"id": "1233054", "url": "https://en.wikipedia.org/wiki?curid=1233054", "title": "Cyrillic numerals", "text": "Cyrillic numerals\n\nCyrillic numerals are a numeral system derived from the Cyrillic script, developed in the First Bulgarian Empire in the late 10th century. It was used in the First Bulgarian Empire and by South and East Slavic peoples. The system was used in Russia as late as the early 18th century, when Peter the Great replaced it with Arabic numerals as part of his civil script reform initiative. Cyrillic numbers played a role in Peter the Great's currency reform plans, too, with silver wire kopecks issued after 1696 and mechanically minted coins issued between 1700 and 1722 inscribed with the date using Cyrillic numerals. By 1725, Russian Imperial coins had transitioned to Arabic numerals. The Cyrillic numerals may still be found in books written in the Church Slavonic language.\n\nThe system is a quasi-decimal alphabetic system, equivalent to the Ionian numeral system but written with the corresponding graphemes of the Cyrillic script. The order is based on the original Greek alphabet rather than the standard Cyrillic alphabetical order. \n\nA separate letter is assigned to each unit (1, 2, ... 9), each multiple of ten (10, 20, ... 90), and each multiple of one hundred (100, 200, ... 900). To distinguish numbers from text, a titlo (  ) is sometimes drawn over the numbers, or they are set apart with dots. The numbers are written as pronounced in Slavonic, generally from the high value position to the low value position, with the exception of 11 through 19, which are written and pronounced with the ones unit before the tens; for example, ЗІ (17) is \"семнадсять\" (literally \"seven-on-ten\", cf. the English \"seven-teen\").\n\nExamples:\n\nTo evaluate a Cyrillic number, the values of all the figures are added up: for example, ѰЗ is 700 + 7, making 707. If the number is greater than 999 (ЦЧѲ), the thousands sign (҂) is used to multiply the number's value: for example, ҂Ѕ is 6000, while ҂Л҂В is parsed as 30,000 + 2000, making 32,000. To produce larger numbers, a modifying sign is used to encircle the number being multiplied. Two scales existed in such cases (similar to the long and short scales), one (Малый счёт; Lesser count) giving a new name and sign every order of magnitude, the other (Великий счёт; Greater Count), each squaring except for the end (extending to 10 in the 49th power)\n\nGlagolitic numerals are similar to Cyrillic numerals except that numeric values are assigned according to the native alphabetic order of the Glagolitic alphabet. Glyphs for the ones, tens, and hundreds values are combined to form more precise numbers, for example, ⰗⰑⰂ is 500 + 80 + 3 or 583. As with Cyrillic numerals, the numbers 11 through 19 are typically written with the ones digit before the glyph for 10; for example ⰅⰊ is 6 + 10, making 16. Whereas Cyrillic numerals use modifying signs for numbers greater than 999, some documents attest to the use of Glagolitic letters for 1000 through 6000, although the validity of 3000 and greater is questioned.\n\n"}
{"id": "25699937", "url": "https://en.wikipedia.org/wiki?curid=25699937", "title": "David A. Lane", "text": "David A. Lane\n\nDavid A. Lane (born 1945) is an American economist, who developed the theory of artifact innovation. At the Santa Fe Institute he has been defining the notion of economic complexity and the Santa Fe approach. He is professor of economics at the University of Modena and Reggio Emilia and research fellow at the European Centre for Living Technology.\n\nIn 1987 he was elected as a Fellow of the American Statistical Association.\n\nLane is co-author of the book \"Foresight, Complexity and Strategy\".\n\nAmong his most cited papers is \"Artificial worlds and economics, part II\", published in the Journal of Evolutionary Economics, Volume 3, Number 3 / September, 1993 DOI 10.1007/BF01200867, and cited by 218 other works listed in Google Scholar.\n"}
{"id": "3504168", "url": "https://en.wikipedia.org/wiki?curid=3504168", "title": "Differential entropy", "text": "Differential entropy\n\nDifferential entropy (also referred to as continuous entropy) is a concept in information theory that began as an attempt by Shannon to extend the idea of (Shannon) entropy, a measure of average surprisal of a random variable, to continuous probability distributions. Unfortunately, Shannon did not derive this formula, and rather just assumed it was the correct continuous analogue of discrete entropy, but it is not. The actual continuous version of discrete entropy is the limiting density of discrete points (LDDP). Differential entropy (described here) is commonly encountered in the literature, but it is a limiting case of the LDDP, and one that loses its fundamental association with discrete entropy.\n\nLet formula_1 be a random variable with a probability density function formula_2 whose support is a set formula_3. The \"differential entropy\" formula_4 or formula_5 is defined as\n\nFor probability distributions which don't have an explicit density function expression, but have an explicit quantile function expression, formula_7, then formula_8 can be defined in terms of the derivative of formula_7 i.e. the quantile density function formula_10 as \n\nAs with its discrete analog, the units of differential entropy depend on the base of the logarithm, which is usually 2 (i.e., the units are bits). See logarithmic units for logarithms taken in different bases. Related concepts such as joint, conditional differential entropy, and relative entropy are defined in a similar fashion. Unlike the discrete analog, the differential entropy has an offset that depends on the units used to measure formula_1. For example, the differential entropy of a quantity measured in millimeters will be log(1000) more than the same quantity measured in meters; a dimensionless quantity will have differential entropy of log(1000) more than the same quantity divided by 1000.\n\nOne must take care in trying to apply properties of discrete entropy to differential entropy, since probability density functions can be greater than 1. For example, Uniform(0,1/2) has \"negative\" differential entropy\n\nThus, differential entropy does not share all properties of discrete entropy.\n\nNote that the continuous mutual information formula_14 has the distinction of retaining its fundamental significance as a measure of discrete information since it is actually the limit of the discrete mutual information of \"partitions\" of formula_1 and formula_16 as these partitions become finer and finer. Thus it is invariant under non-linear homeomorphisms (continuous and uniquely invertible maps)\n, including linear transformations of formula_1 and formula_16, and still represents the amount of discrete information that can be transmitted over a channel that admits a continuous space of values.\n\nFor the direct analogue of discrete entropy extended to the continuous space, see limiting density of discrete points.\n\n\nHowever, differential entropy does not have other desirable properties:\nA modification of differential entropy that addresses these drawbacks is the relative information entropy, also known as the Kullback–Leibler divergence, which includes an invariant measure factor (see limiting density of discrete points).\n\nWith a normal distribution, differential entropy is maximized for a given variance. A Gaussian random variable has the largest entropy amongst all random variables of equal variance, or, alternatively, the maximum entropy distribution under constraints of mean and variance is the Gaussian.\n\nLet formula_47 be a Gaussian PDF with mean μ and variance formula_48 and formula_49 an arbitrary PDF with the same variance. Since differential entropy is translation invariant we can assume that formula_49 has the same mean of formula_51 as formula_47.\n\nConsider the Kullback–Leibler divergence between the two distributions\nNow note that\nbecause the result does not depend on formula_49 other than through the variance. Combining the two results yields\nwith equality when formula_57 following from the properties of Kullback–Leibler divergence.\n\nThis result may also be demonstrated using the variational calculus. A Lagrangian function with two Lagrangian multipliers may be defined as:\n\nwhere \"g(x)\" is some function with mean μ. When the entropy of \"g(x)\" is at a maximum and the constraint equations, which consist of the normalization condition formula_59 and the requirement of fixed variance formula_60, are both satisfied, then a small variation δ\"g\"(\"x\") about \"g(x)\" will produce a variation δ\"L\" about \"L\" which is equal to zero:\n\nSince this must hold for any small δ\"g\"(\"x\"), the term in brackets must be zero, and solving for \"g(x)\" yields:\n\nUsing the constraint equations to solve for λ and λ yields the normal distribution:\n\nLet formula_1 be an exponentially distributed random variable with parameter formula_65, that is, with probability density function\n\nIts differential entropy is then\nHere, formula_67 was used rather than formula_4 to make it explicit that the logarithm was taken to base \"e\", to simplify the calculation.\n\nThe differential entropy yields a lower bound on the expected squared error of an estimator. For any random variable formula_1 and estimator formula_70 the following holds:\nwith equality if and only if formula_1 is a Gaussian random variable and formula_70 is the mean of formula_1.\n\nWarning: many of the results in the table below are known to be wrong; this was shown by Darbellay & Vajda, IEEE Trans Inf Theory 46 (2000) 709.\n\nIn the table below formula_75 is the gamma function, formula_76 is the digamma function, formula_77 is the beta function, and γ is Euler's constant.\nMany of the differential entropies are from.\n\nAs described above, differential entropy does not share all properties of discrete entropy. For example, the differential entropy can be negative; also it is not invariant under continuous coordinate transformations. Edwin Thompson Jaynes showed in fact that the expression above is not the correct limit of the expression for a finite set of probabilities.\n\nA modification of differential entropy adds an invariant measure factor to correct this, (see limiting density of discrete points). If formula_78 is further constrained to be a probability density, the resulting notion is called relative entropy in information theory:\n\nThe definition of differential entropy above can be obtained by partitioning the range of formula_1 into bins of length formula_81 with associated sample points formula_82 within the bins, for formula_1 Riemann integrable. This gives a quantized version of formula_1, defined by formula_85 if formula_86. Then the entropy of formula_85 is\n\nThe first term on the right approximates the differential entropy, while the second term is approximately formula_89. Note that this procedure suggests that the entropy in the discrete sense of a continuous random variable should be formula_90.\n\n\n"}
{"id": "526004", "url": "https://en.wikipedia.org/wiki?curid=526004", "title": "Emirp", "text": "Emirp\n\nAn emirp (\"prime\" spelled backwards) is a prime number that results in a different prime when its decimal digits are reversed. This definition excludes the related palindromic primes. The term reversible prime may be used to mean the same as emirp, but may also, ambiguously, include the palindromic primes. \n\nThe sequence of emirps begins 13, 17, 31, 37, 71, 73, 79, 97, 107, 113, 149, 157, 167, 179, 199, 311, 337, 347, 359, 389, 701, 709, 733, 739, 743, 751, 761, 769, 907, 937, 941, 953, 967, 971, 983, 991, ... .\n\nAll non-palindromic permutable primes are emirps.\n\n, the largest known emirp is 10+941992101×10+1, found by Jens Kruse Andersen in October 2007.\n\nThe term 'emirpimes' (singular) is used also in places to treat semiprimes in a similar way. That is, an emirpimes is a semiprime that is also a (distinct) semiprime upon reversing its digits.\n\nThe emirps in base 12 are (using reversed two and three for ten and eleven, respectively)\n\nThere is a subset of emirps \"x\", with mirror \"x\", such that \"x\" is the \"y\"th prime, and \"x\" is the \"y\"th prime. (E.g. 73 is the 21st prime number; its mirror, 37, is the 12th prime number; 12 is the mirror of 21.)\n"}
{"id": "22997927", "url": "https://en.wikipedia.org/wiki?curid=22997927", "title": "Evidential decision theory", "text": "Evidential decision theory\n\nEvidential decision theory is a school of thought within decision theory according to which the best action is the one which, conditional on one's having chosen it, gives one the best expectations for the outcome. It contrasts with causal decision theory, which requires a causal connection between one's actions and the desirable outcome.\n\nIn a 1981 article, Allan Gibbard and William Harper characterized evidential decision theory as maximization of the expected utility formula_1 of an action formula_2 \"calculated from conditional probabilities\":\nwhere formula_4 is the desirability of outcome formula_5 and formula_6 is the conditional probability of formula_5 given formula_2.\n\nDavid Lewis has characterized evidential decision theory as promoting \"an irrational \npolicy of managing the news\". James M. Joyce asserted, \"Rational agents choose acts on the basis of their \"causal efficacy\", not their auspiciousness; they act to \"bring about\" good results even when doing so might betoken bad news.\"\n\n"}
{"id": "866638", "url": "https://en.wikipedia.org/wiki?curid=866638", "title": "False precision", "text": "False precision\n\nFalse precision (also called overprecision, fake precision, misplaced precision and spurious precision) occurs when numerical data are presented in a manner that implies better precision than is justified; since precision is a limit to accuracy, this often leads to overconfidence in the accuracy, named precision bias.\n\nMadsen Pirie defines the term \"false precision\" in a more general way: when exact numbers are used for notions that cannot be expressed in exact terms. For example, \"We know that 90% of the difficulty in writing is getting started.\" Often false precision is abused to produce an unwarranted confidence in the claim: \"our mouthwash is twice as good as our competitor's\". \n\nIn science and engineering, convention dictates that unless a margin of error is explicitly stated, the number of significant figures used in the presentation of data should be limited to what is warranted by the precision of those data. For example, if an instrument can be read to tenths of a unit of measurement, results of calculations using data obtained from that instrument can only be confidently stated to the tenths place, regardless of what the raw calculation returns or whether other data used in the calculation are more accurate. Even outside these disciplines, there is a tendency to assume that all the non-zero digits of a number are meaningful; thus, providing excessive figures may lead the viewer to expect better precision than exists.\n\nHowever, in contrast, it is good practice to retain more significant figures than this in the intermediate stages of a calculation, in order to avoid accumulated rounding errors.\n\nFalse precision commonly arises when high-precision and low-precision data are combined, and in conversion of units.\n\nFalse precision is the gist of numerous variations of a joke which can be summarized as follows: A tour guide at a museum says a dinosaur skeleton is 100,000,005 years old, because an expert told him that it was 100 million years old when he started working there 5 years ago.\n\nIf a car's speedometer indicates the vehicle is travelling at 60 mph and that is converted to km/h, it would equal 96.5606 km/h. The conversion from the whole number in one system to the precise result in another makes it seem like the measurement was very precise, when in fact it was not.\n\nMeasures that rely on statistical sampling, such as IQ tests, are often reported with false precision.\n\n\n"}
{"id": "5705488", "url": "https://en.wikipedia.org/wiki?curid=5705488", "title": "Feebly compact space", "text": "Feebly compact space\n\nIn mathematics, a topological space is feebly compact if every locally finite cover by nonempty open sets is finite.\n\nSome facts:\n\n"}
{"id": "5087910", "url": "https://en.wikipedia.org/wiki?curid=5087910", "title": "Fracton", "text": "Fracton\n\nA fracton is a collective quantized vibration on a substrate with a fractal structure.\n\nFractons are the fractal analog of phonons. Phonons are the result of applying translational symmetry to the potential in a Schrödinger equation. Fractal self-similarity can be thought of as a symmetry somewhat comparable to translational symmetry. Translational symmetry is symmetry under displacement or change of position, and self-similarity is symmetry under change of scale. The quantum mechanical solutions to such a problem in general lead to a continuum of states with different frequencies. In other words, a fracton band is comparable to a phonon band. The vibrational modes are restricted to part of the substrate and are thus not fully delocalized, unlike phonon vibrational modes. Instead there is a hierarchy of vibrational modes which encompass smaller and smaller parts of the substrate.\n"}
{"id": "52041427", "url": "https://en.wikipedia.org/wiki?curid=52041427", "title": "G*Power", "text": "G*Power\n\nG*Power is a free-to use software used to calculate statistical power. The program offers the ability to calculate power for a wide variety of statistical tests including t-tests, F-tests, and chi-square-tests, among others. Additionally, the user must determine which of the many contexts this test is being used, such as a one-way ANOVA versus a multi-way ANOVA. In order to calculate power, the user must know four of five variables: either number of groups, number of observations, effect size, significance level (α), or power (1-β). G*Power has a built-in tool for determining effect size if it cannot be estimated from prior literature or is not easily calculable.\n\n\n"}
{"id": "33308777", "url": "https://en.wikipedia.org/wiki?curid=33308777", "title": "Good–deal bounds", "text": "Good–deal bounds\n\nGood–deal bounds are price bounds for a financial portfolio which depends on an individual trader's preferences. Mathematically, if formula_1 is a set of portfolios with future outcomes which are \"acceptable\" to the trader, then define the function formula_2 by\nwhere formula_4 is the set of final values for self-financing trading strategies. Then any price in the range formula_5 does not provide a good deal for this trader, and this range is called the \"no good-deal price bounds.\"\n\nIf formula_6 then the good-deal price bounds are the no-arbitrage price bounds, and correspond to the subhedging and superhedging prices. The no-arbitrage bounds are the greatest extremes that good-deal bounds can take.\n\nIf formula_7 where formula_8 is a utility function, then the good-deal price bounds correspond to the indifference price bounds.\n"}
{"id": "21318521", "url": "https://en.wikipedia.org/wiki?curid=21318521", "title": "Graph structure theorem", "text": "Graph structure theorem\n\nIn mathematics, the graph structure theorem is a major result in the area of graph theory. The result establishes a deep and fundamental connection between the theory of graph minors and topological embeddings. The theorem is stated in the seventeenth of a series of 23 papers by Neil Robertson and Paul Seymour. Its proof is very long and involved. and are surveys accessible to nonspecialists, describing the theorem and its consequences.\n\nA minor of a graph \"G\" is any graph \"H\" that is isomorphic to a graph that can be obtained from a subgraph of \"G\" by contracting some edges. If \"G\" does \"not\" have a graph \"H\" as a minor, then we say that \"G\" is \"H\"-free. Let \"H\" be a fixed graph. Intuitively, if \"G\" is a huge \"H\"-free graph, then there ought to be a \"good reason\" for this. The graph structure theorem provides such a \"good reason\" in the form of a rough description of the structure of \"G\". In essence, every \"H\"-free graph \"G\" suffers from one of two structural deficiencies: either \"G\" is \"too thin\" to have \"H\" as a minor, or \"G\" can be (almost) topologically embedded on a surface that is too simple to embed \"H\" upon. The first reason applies if \"H\" is a planar graph, and both reasons apply if \"H\" is not planar. We first make precise these notions.\n\nThe tree width of a graph \"G\" is a positive integer that specifies the \"thinness\" of \"G\". For example, a connected graph \"G\" has tree width one if and only if it is a tree, and \"G\" has tree width two if and only if it is a series-parallel graph. Intuitively, a huge graph \"G\" has small tree width if and only if \"G\" takes the structure of a huge tree whose nodes and edges have been replaced by small graphs. We give a precise definition of tree width in the subsection regarding clique-sums. It is a theorem that if \"H\" is a minor of \"G\", then the tree width of \"H\" is not greater than that of \"G\". Therefore, one \"good reason\" for \"G\" to be \"H\"-free is that the tree width of \"G\" is not very large. The graph structure theorem implies that this reason always applies in case \"H\" is planar.\n\n\"Corollary 1.\" For every planar graph \"H\", there exists a positive integer \"k\" such that every \"H\"-free graph has tree width less than \"k\".\n\nIt is unfortunate that the value of \"k\" in Corollary 1 is generally much larger than the tree width of \"H\" (a notable exception is when \"H\" = \"K\", the complete graph on four vertices, for which \"k\"=3). This is one reason that the graph structure theorem is said to describe the \"rough structure\" of \"H\"-free graphs.\n\nRoughly, a surface is a set of points with a local topological structure of a disc. Surfaces fall into two infinite families: the \"orientable\" surfaces include the sphere, the torus, the double torus and so on; the \"nonorientable\" surfaces include the real projective plane, the Klein bottle and so on. A graph embeds on a surface if the graph can be drawn on the surface as a set of points (vertices) and arcs (edges) that do not cross or touch each other, except where edges and vertices are incident or adjacent. A graph is planar if it embeds on the sphere. If a graph \"G\" embeds on a particular surface then every minor of \"G\" also embeds on that same surface. Therefore, a \"good reason\" for \"G\" to be \"H\"-free is that \"G\" embeds on a surface that \"H\" does not embed on.\n\nWhen \"H\" is not planar, the graph structure theorem may be looked at as a vast generalization of the Kuratowski theorem. A version of this theorem proved by states that if a graph \"G\" is both \"K\"-free and \"K\"-free, then \"G\" is planar. This theorem provides a \"good reason\" for a graph \"G\" not to have \"K\" or \"K\" as minors; specifically, \"G\" embeds on the sphere, whereas neither \"K\" nor \"K\" embed on the sphere. Unfortunately, this notion of \"good reason\" is not sophisticated enough for the graph structure theorem. Two more notions are required: \"clique-sums\" and \"vortices\".\n\nA clique in a graph \"G\" is any set of vertices that are pairwise adjacent in \"G\". For a non-negative integer \"k\", a \"k\"-clique-sum of two graphs \"G\" and \"K\" is any graph obtained by selecting a non-negative integer \"m\" ≤ \"k\", selecting clique of size \"m\" in each of \"G\" and \"K\", identifying the two cliques into a single clique of size \"m\", then deleting zero or more of the edges that join vertices in the new clique.\n\nIf \"G\", \"G\", ..., \"G\" is a list of graphs, then we may produce a new graph by joining the list of graphs \"via k-clique-sums\". That is, we take a \"k\"-clique-sum of \"G\" and \"G\", then take a \"k\"-clique-sum of \"G\" with the resulting graph, and so on. A graph has tree width at most \"k\" if it can be obtained via \"k\"-clique-sums from a list of graphs, where each graph in the list has at most \"k\" + 1 vertices.\n\nCorollary 1 indicates to us that \"k\"-clique-sums of small graphs describe the rough structure \"H\"-free graphs when \"H\" is planar. When \"H\" is nonplanar, we also need to consider \"k\"-clique-sums of a list of graphs, each of which is embedded on a surface. The following example with \"H\" = \"K\" illustrates this point. The graph \"K\" embeds on every surface except for the sphere. However there exist \"K\"-free graphs that are far from planar. In particular, the 3-clique-sum of any list of planar graphs results in a \"K\"-free graph. determined the precise structure of \"K\"-free graphs, as part of a cluster of results known as Wagner's theorem:\n\n\"Theorem 2.\" If \"G\" is \"K\"-free, then \"G\" can be obtained via 3-clique-sums from a list of planar graphs, and copies of one special non-planar graph having 8-vertices.\n\nWe point out that Theorem 2 is an \"exact structure theorem\" since the precise structure of \"K\"-free graphs is determined. Such results are rare within graph theory. The graph structure theorem is not precise in this sense because, for most graphs \"H\", the structural description of \"H\"-free graphs includes some graphs that are \"not\" \"H\"-free.\n\nOne might be tempted to conjecture that an analog of Theorem 2 holds for graphs \"H\" other than \"K\". Perhaps it is true that: \"for any non-planar graph H, there exists a positive integer k such that every H-free graph can be obtained via k-clique-sums from a list of graphs, each of which either has at most k vertices or embeds on some surface that H does not embed on\". Unfortunately, this statement is not yet sophisticated enough to be true. We must allow each embedded graph \"G\" to \"cheat\" in two limited ways. First, we must allow a bounded number of locations on the surface at which we may add some new vertices and edges that are permitted to cross each other in a manner of limited \"complexity\". Such locations are called vortices. The \"complexity\" of a vortex is limited by a parameter called its depth, closely related to pathwidth. The reader may prefer to defer reading the following precise description of a vortex of depth \"k\". Second, we must allow a limited number of new vertices to add to each of the embedded graphs with vortices.\n\nA face of an embedded graph is an open 2-cell in the surface that is disjoint from the graph, but whose boundary is the union of some of the edges of the embedded graph. Let \"F\" be a face of an embedded graph \"G\" and let \"v\", \"v\", ..., \"v\",\"v\" = \"v\" be the vertices lying on the boundary of \"F\" (in that circular order). A circular interval for \"F\" is a set of vertices of the form {\"v\", \"v\", ..., \"v\"} where \"a\" and \"s\" are integers and where subscripts are reduced modulo \"n\". Let Λ be a finite list of circular intervals for \"F\". We construct a new graph as follows. For each circular interval \"L\" in Λ we add a new vertex \"v\" that joins to zero or more of the vertices in \"L\". Finally, for each pair {\"L\", \"M\"} of intervals in Λ, we may add an edge joining \"v\" to \"v\" provided that \"L\" and \"M\" have nonempty intersection. The resulting graph is said to be obtained from \"G\" by adding a vortex of depth at most \"k\" (to the face \"F\") provided that no vertex on the boundary of \"F\" appears in more than \"k\" of the intervals in Λ.\n\nGraph structure theorem. \"For any graph H, there exists a positive integer k such that every H-free graph can be obtained as follows:\"\n\nNote that steps 1. and 2. result in an empty graph if \"H\" is planar, but the bounded number of vertices added in step 3. makes the statement consistent with Corollary 1.\n\nStrengthened versions of the graph structure theorem are possible depending on the set \"H\" of forbidden minors. For instance, when one of the graphs in \"H\" is planar, then every \"H\"-minor-free graph has a tree decomposition of bounded width; equivalently, it can be represented as a clique-sum of graphs of constant size When one of the graphs in \"H\" can be drawn in the plane with only a single crossing, then the \"H\"-minor-free graphs admit a decomposition as a clique-sum of graphs of constant size and graphs of bounded genus, without vortices.\nA different strengthening is also known when one of the graphs in \"H\" is an apex graph.\n\n\n"}
{"id": "53928", "url": "https://en.wikipedia.org/wiki?curid=53928", "title": "Greibach normal form", "text": "Greibach normal form\n\nIn formal language theory, a context-free grammar is in Greibach normal form (GNF) if the right-hand sides of all production rules start with a terminal symbol, optionally followed by some variables. A non-strict form allows one exception to this format restriction for allowing the empty word (epsilon, ε) to be a member of the described language. The normal form was established by Sheila Greibach and it bears her name.\n\nMore precisely, a context-free grammar is in Greibach normal form, if all production rules are of the form:\nor\nwhere formula_3 is a nonterminal symbol, formula_4 is a terminal symbol, \nformula_5 is a (possibly empty) sequence of nonterminal symbols not including the start symbol, formula_6 is the start symbol, and \"ε\" is the empty word.\n\nObserve that the grammar does not have left recursions.\n\nEvery context-free grammar can be transformed into an equivalent grammar in Greibach normal form. Various constructions exist. Some do not permit the second form of rule and cannot transform context-free grammars that can generate the empty word. For one such construction the size of the constructed grammar is \"O(n)\" in the general case and \"O(n)\" if no derivation of the original grammar consists of a single nonterminal symbol, where \"n\" is the size of the original grammar. This conversion can be used to prove that every context-free language can be accepted by a real-time pushdown automaton, i.e., the automaton reads a letter from its input every step. \n\nGiven a grammar in GNF and a derivable string in the grammar with length \"n\", any top-down parser will halt at depth \"n\".\n\n"}
{"id": "1104948", "url": "https://en.wikipedia.org/wiki?curid=1104948", "title": "Grigory Margulis", "text": "Grigory Margulis\n\nGregori Aleksandrovich Margulis (, first name often given as Gregory, Grigori or Grigory; born February 24, 1946) is a Russian-American mathematician known for his work on lattices in Lie groups, and the introduction of methods from ergodic theory into diophantine approximation. He was awarded a Fields Medal in 1978 and a Wolf Prize in Mathematics in 2005, becoming the seventh mathematician to receive both prizes. In 1991, he joined the faculty of Yale University, where he is currently the Erastus L. De Forest Professor of Mathematics.\n\nMargulis was born in Moscow, Soviet Union. He received his PhD in 1970 from the Moscow State University, starting research in ergodic theory under the supervision of Yakov Sinai. Early work with David Kazhdan produced the Kazhdan–Margulis theorem, a basic result on discrete groups. His superrigidity theorem from 1975 clarified an area of classical conjectures about the characterisation of arithmetic groups amongst lattices in Lie groups.\n\nHe was awarded the Fields Medal in 1978, but was not permitted to travel to Helsinki to accept it in person. His position improved, and in 1979 he visited Bonn, and was later able to travel freely, though he still worked in the Institute of Problems of Information Transmission, a research institute rather than a university. In 1991, Margulis accepted a professorial position at Yale University.\n\nMargulis was elected a member of the U.S. National Academy of Sciences in 2001. In 2012 he became a fellow of the American Mathematical Society.\n\nIn 2005, Margulis received the Wolf Prize for his contributions to theory of lattices and applications to ergodic theory, representation theory, number theory, combinatorics, and measure theory.\n\nMargulis's early work dealt with Kazhdan's property (T) and the questions of rigidity and arithmeticity of lattices in semisimple algebraic groups of higher rank over a local field. It had been known since the 1950s (Borel, Harish-Chandra) that a certain simple-minded way of constructing subgroups of semisimple Lie groups produces examples of lattices, called \"arithmetic lattices\". It is analogous to considering the subgroup \"SL\"(\"n\",Z) of the real special linear group \"SL\"(\"n\",R) that consists of matrices with \"integer\" entries. Margulis proved that under suitable assumptions on \"G\" (no compact factors and split rank greater or equal than two), \"any\" (irreducible) lattice \"Γ\" in it is arithmetic, i.e. can be obtained in this way. Thus \"Γ\" is commensurable with the subgroup \"G\"(Z) of \"G\", i.e. they agree on subgroups of finite index in both. Unlike general lattices, which are defined by their properties, arithmetic lattices are defined by a construction. Therefore, these results of Margulis pave a way for classification of lattices. Arithmeticity turned out to be closely related to another remarkable property of lattices discovered by Margulis. \"Superrigidity\" for a lattice \"Γ\" in \"G\" roughly means that any homomorphism of \"Γ\" into the group of real invertible \"n\" × \"n\" matrices extends to the whole \"G\". The name derives from the following variant:\n\n(The case when \"f\" is an isomorphism is known as the strong rigidity.) While certain rigidity phenomena had already been known, the approach of Margulis was at the same time novel, powerful, and very elegant.\n\nMargulis solved the Banach–Ruziewicz problem that asks whether the Lebesgue measure is the only normalized rotationally invariant finitely additive measure on the \"n\"-dimensional sphere. The affirmative solution for \"n\" ≥ 4, which was also independently and almost simultaneously obtained by Dennis Sullivan, follows from a construction of a certain dense subgroup of the orthogonal group that has property (T).\n\nMargulis gave the first construction of expander graphs, which was later generalized in the theory of Ramanujan graphs.\n\nIn 1986, Margulis gave a complete resolution of the Oppenheim conjecture on quadratic forms and diophantine approximation. This was a question that had been open for half a century, on which considerable progress had been made by the Hardy–Littlewood circle method; but to reduce the number of variables to the point of getting the best-possible results, the more structural methods from group theory proved decisive. He has formulated a further program of research in the same direction, that includes the Littlewood conjecture.\n\n\n\n\n\n"}
{"id": "97315", "url": "https://en.wikipedia.org/wiki?curid=97315", "title": "Heh (god)", "text": "Heh (god)\n\nḤeḥ (also Huh, Hah, Hauh, Huah, and Hehu) was the personification of infinity or eternity in the Ogdoad in Egyptian mythology. His name originally meant \"flood\", referring to the watery chaos that the Egyptians believed existed before the creation of the world. The Egyptians envisioned this chaos as infinite, in contrast with the finite created world, so Heh personified this aspect of the primordial waters. Heh's female counterpart was known as Hauhet, which is simply the feminine form of his name.\n\nLike the other concepts in the Ogdoad, his male form was often depicted as a frog, or a frog-headed human, and his female form as a snake or snake-headed human. The frog head symbolised fertility, creation, and regeneration, and was also possessed by the other Ogdoad males Kek, Amun, and Nun. The other common representation depicts him crouching, holding a palm stem in each hand (or just one), sometimes with a palm stem in his hair, as palm stems represented long life to the Egyptians, the years being represented by notches on it. Depictions of this form also had a shen ring at the base of each palm stem, which represented infinity. Depictions of Heh were also used in hieroglyphs to represent one million, which was essentially considered equivalent to infinity in Ancient Egyptian mathematics. Thus this deity is also known as the \"god of millions of years\". \n\nThe primary meaning of the Egyptian word \"ḥeḥ\" was \"million\" or \"millions\"; a personification of this concept, Ḥeḥ, was adopted as the Egyptian god of infinity. With his female counterpart Ḥauḥet (or Ḥeḥut), Ḥeḥ represented one of the four god-goddess pairs comprising the Ogdoad, a pantheon of eight primeval deities whose worship was centred at Hermopolis Magna.\nThe mythology of the Ogdoad describes its eight members, Heh and Hauhet, Nu and Naunet, Amun and Amaunet, and Kuk and Kauket, coming together in the cataclysmic event that gives rise to the sun (and its deific personification, Atum).\n\nThe god Ḥeḥ was usually depicted anthropomorphically, as in the hieroglyphic character, as a male figure with divine beard and lappet wig. Normally kneeling (one knee raised), sometimes in a basket—the sign for \"all\", the god typically holds in each hand a notched palm branch (palm rib). (These were employed in the temples for ceremonial time-keeping, which use explains the use of the palm branch as the hieroglyphic symbol for \"rnp.t\", \"year\"). Occasionally, an additional palm branch is worn on the god's head.\n\nIn Ancient Egyptian Numerology, Gods such as Heh were used to represent numbers in a decimal point system. Particularly, the number 1,000,000 is depicted in the hieroglyph of Heh, who is position in his normal seated position.\n\nThe personified, somewhat abstract god of eternity Ḥeḥ possessed no known cult centre or sanctuary; rather, his veneration revolved around symbolism and personal belief. The god's image and its iconographic elements reflected the wish for millions of years of life or rule; as such, the figure of Ḥeḥ finds frequent representation in amulets, prestige items and royal iconography from the late Old Kingdom period onwards. Heh became associated with the King and his quest for longevity. For instance, he appears on the tomb of King Tutankhamen, in two cartouches, where he is crowned with a winged scarab beetle, symbolizing existence and a sun disk. The placement of Heh in relation to King Tutankhamen's corpse means he will be granting him these \"millions of years\" into the afterlife.\n\n\n"}
{"id": "12809481", "url": "https://en.wikipedia.org/wiki?curid=12809481", "title": "Hennessy–Milner logic", "text": "Hennessy–Milner logic\n\nIn computer science, Hennessy–Milner logic (HML) is a dynamic logic used to specify properties of a labeled transition system (LTS), a structure similar to an automaton. It was introduced in 1980 by Matthew Hennessy and Robin Milner in their paper \"On observing nondeterminism and concurrency\" (ICALP). \n\nAnother variant of the HML involves the use of recursion to extend the expressibility of the logic, and is commonly referred to as 'Hennessy-Milner Logic with recursion'. Recursion is enabled with the use of maximum and minimum fixed points.\n\nA formula is defined by the following BNF grammar for \"Act\" some set of actions:\n\nThat is, a formula can be\n\n\nLet formula_6 be a labeled transition system, and let\nformula_7 be the set of HML formulae. The satisfiability\nrelation formula_8 relates states of the LTS\nto the formulae they satisfy, and is defined as the smallest relation such that, for all states formula_9\nand formulae formula_10,\n\n\n"}
{"id": "7731831", "url": "https://en.wikipedia.org/wiki?curid=7731831", "title": "Herbrand interpretation", "text": "Herbrand interpretation\n\nIn mathematical logic, a Herbrand interpretation is an interpretation in which all constants and function symbols are assigned very simple meanings. Specifically, every constant is interpreted as itself, and every function symbol is interpreted as the function that applies it. The interpretation also defines predicate symbols as denoting a subset of the relevant Herbrand base, effectively specifying which ground atoms are true in the interpretation. This allows the symbols in a set of clauses to be interpreted in a purely syntactic way, separated from any real instantiation.\n\nThe importance of Herbrand interpretations is that, if any interpretation satisfies a given set of clauses \"S\" then there is a Herbrand interpretation that satisfies them. Moreover, Herbrand's theorem states that if \"S\" is unsatisfiable then there is a finite unsatisfiable set of ground instances from the Herbrand universe defined by \"S\". Since this set is finite, its unsatisfiability can be verified in finite time. However there may be an infinite number of such sets to check. \n\nIt is named after Jacques Herbrand.\n\n"}
{"id": "44916914", "url": "https://en.wikipedia.org/wiki?curid=44916914", "title": "Irene M. Gamba", "text": "Irene M. Gamba\n\nIrene Martínez Gamba (born 1957) is an Argentine–American mathematician. She works as a professor of mathematics at the University of Texas at Austin, where she holds the W.A. Tex Moncreif, Jr. Chair in Computational Engineering and Sciences and is head of the Applied Mathematics Group in the Institute for Computational Engineering and Sciences.\n\nGamba graduated from the University of Buenos Aires in 1981. She went to the University of Chicago for her graduate studies, earning a master's degree in 1985 and a Ph.D. in 1989, under the supervision of Jim Douglas, Jr. After postdoctoral studies at Purdue University and the Courant Institute of Mathematical Sciences of New York University, she became an assistant professor at NYU in 1994, and moved to the University of Texas in 1997. At the University of Texas, she was the Joe B. and Louise Cook Professor from 2007 to 2013, the John T. Stuart III Centennial Professor from 2013 to 2014, and the W.A. Tex Moncreif, Jr. Chair since 2014.\n\nIn 2012, Gamba became a fellow of the Society for Industrial and Applied Mathematics, and one of the inaugural fellows of the American Mathematical Society. The Association for Women in Mathematics selected her as their 2014 Sonia Kovalevsky Lecturer.\n\n"}
{"id": "32376357", "url": "https://en.wikipedia.org/wiki?curid=32376357", "title": "Law of cotangents", "text": "Law of cotangents\n\nIn trigonometry, the law of cotangents is a relationship among the lengths of the sides of a triangle and the cotangents of the halves of the three angles.\n\nJust as three quantities whose equality is expressed by the law of sines are equal to the diameter of the circumscribed circle of the triangle (or to its reciprocal, depending on how the law is expressed), so also the law of cotangents relates the radius of the inscribed circle of a triangle (the inradius) to its sides and angles.\n\nUsing the usual notations for a triangle (see the figure at the upper right), where , , are the lengths of the three sides, , , are the vertices opposite those three respective sides, , , are the corresponding angles at those vertices, is the semi-perimeter, that is, , and is the radius of the inscribed circle, the law of cotangents states that\n\nand furthermore that the inradius is given by\n\nIn the upper figure, the points of tangency of the incircle with the sides of the triangle break the perimeter into 6 segments, in 3 pairs. In each pair the segments are of equal length. For example, the 2 segments adjacent to vertex are equal. If we pick one segment from each pair, their sum will be the semiperimeter . An example of this is the segments shown in color in the figure. The two segments making up the red line add up to , so the blue segment must be of length . Obviously, the other five segments must also have lengths , , or , as shown in the lower figure.\n\nBy inspection of the figure, using the definition of the cotangent function, we have\nand similarly for the other two angles, proving the first assertion.\n\nFor the second one—the inradius formula—we start from the general addition formula:\n\nApplying to , we obtain:\n\nSubstituting the values obtained in the first part, we get:\nMultiplying through by gives the value of , proving the second assertion.\n\nA number of other results can be derived from the law of cotangents.\n\n\n\n\n\n\n"}
{"id": "26090637", "url": "https://en.wikipedia.org/wiki?curid=26090637", "title": "Lehmer code", "text": "Lehmer code\n\nIn mathematics and in particular in combinatorics, the Lehmer code is a particular way to encode each possible permutation of a sequence of \"n\" numbers. It is an instance of a scheme for numbering permutations and is an example of an inversion table.\n\nThe Lehmer code is named in reference to Derrick Henry Lehmer, but the code had been known since 1888 at least.\n\nThe Lehmer code makes use of the fact that there are\npermutations of a sequence of \"n\" numbers. If a permutation \"σ\" is specified by the sequence (\"σ\", …, \"σ\") of its images of 1, …, \"n\", then it is encoded by a sequence of \"n\" numbers, but not all such sequences are valid since every number must be used only once. By contrast the encodings considered here choose the first number from a set of \"n\" values, the next number from a fixed set of values, and so forth decreasing the number of possibilities until the last number for which only a single fixed value is allowed; \"every\" sequence of numbers chosen from these sets encodes a single permutation. While several encodings can be defined, the Lehmer code has several additional useful properties; it is the sequence\nin other words the term \"L\"(\"σ\") counts the number of terms in (\"σ\", …, \"σ\") to the right of \"σ\" that are smaller than it, a number between 0 and , allowing for different values.\n\nA pair of indices (\"i\",\"j\") with and is called an inversion of \"σ\", and \"L\"(\"σ\") counts the number of inversions (\"i\",\"j\") with \"i\" fixed and varying \"j\". It follows that is the total number of inversions of \"σ\", which is also the number of adjacent transpositions that are needed to transform the permutation into the identity permutation. Other properties of the Lehmer code include that the lexicographical order of the encodings of two permutations is the same as that of their sequences (\"σ\", …, \"σ\"), that any value 0 in the code represents a right-to-left minimum in the permutation (i.e., a smaller than any to its right), and a value \nat position \"i\" similarly signifies a right-to-left maximum, and that the Lehmer code of \"σ\" coincides with the factorial number system representation of its position in the list of permutations of \"n\" in lexicographical order (numbering the positions starting from 0).\n\nVariations of this encoding can be obtained by counting inversions (\"i\",\"j\") for fixed \"j\" rather than fixed \"i\", by counting inversions with a fixed smaller \"value\" rather than smaller index \"i\", or by counting non-inversions rather than inversions; while this does not produce a fundamentally different type of encoding, some properties of the encoding will change correspondingly. In particular counting inversions with a fixed smaller value gives the inversion table of \"σ\", which can be seen to be the Lehmer code of the inverse permutation.\n\nThe usual way to prove that there are \"n\"! different permutations of \"n\" objects is to observe that the first object can be chosen in different ways, the next object in different ways (because choosing the same number as the first is forbidden), the next in different ways (because there are now 2 forbidden values), and so forth. Translating this freedom of choice at each step into a number, one obtains an encoding algorithm, one that finds the Lehmer code of a given permutation. One need not suppose the objects permuted to be numbers, but one needs a total ordering of the set of objects. Since the code numbers are to start from 0, the appropriate number to encode each object \"σ\" by is the number of objects that were available at that point (so they do not occur before position \"i\"), but which are smaller than the object \"σ\" actually chosen. (Inevitably such objects must appear at some position , and (\"i\",\"j\") will be an inversion, which shows that this number is indeed \"L\"(\"σ\").)\n\nThis number to encode each object can be found by direct counting, in several ways (directly counting inversions, or correcting the total number of objects smaller than a given one, which is its sequence number starting from 0 in the set, by those that are unavailable at its position). Another method which is in-place, but not really more efficient, is to start with the permutation of {0, 1, … } obtained by representing each object by its mentioned sequence number, and then for each entry \"x\", in order from left to right, correct the items to its right by subtracting 1 from all entries (still) greater than \"x\" (to reflect the fact that the object corresponding to \"x\" is no longer available). Concretely a Lehmer code for the permutation B,F,A,G,D,E,C of letters, ordered alphabetically, would first give the list of sequence numbers 1,5,0,6,3,4,2, which is successively transformed\nwhere the final line is the Lehmer code (at each line one subtracts 1 from the larger entries to the right of the boldface element to form the next line).\n\nFor decoding a Lehmer code into a permutation of a given set, the latter procedure may be reversed: for each entry \"x\", in order from right to left, correct the items to its right by adding 1 to all those (currently) greater than or equal to \"x\"; finally interpret the resulting permutation of {0, 1, … } as sequence numbers (which amounts to adding 1 to each entry if a permutation of {1, 2, … \"n\"} is sought). Alternatively the entries of the Lehmer code can be processed from left to right, and interpreted as a number determining the next choice of an element as indicated above; this requires maintaining a list of available elements, from which each chosen element is removed. In the example this would mean choosing element 1 from {A,B,C,D,E,F,G} (which is B) then element 4 from {A,C,D,E,F,G} (which is F), then element 0 from {A,C,D,E,G} (giving A) and so on, reconstructing the sequence B,F,A,G,D,E,C.\n\nThe Lehmer code defines a bijection from the symmetric group \"S\" to the Cartesian product formula_4, where [\"k\"] designates the \"k\"-element set formula_5. As a consequence, under the uniform distribution on \"S\", the component \"L\"(\"σ\") defines a uniformly distributed random variable on , and these random variables are mutually independent, because they are projections on different factors of a Cartesian product.\n\nDefinition : In a sequence \"u(u)\", there is right-to-left minimum (resp. maximum) at rank \"k\" if \"u\" is strictly smaller (resp. strictly bigger) than each element \"u\" with \"i>k\", i.e., to its right.\n\nLet \"B(k)\" (resp. \"H(k)\") be the event \"there is right-to-left minimum (resp. maximum) at rank \"k\"\", i.e. \"B(k)\" is the set of the permutations formula_6 which exhibit a right-to-left minimum (resp. maximum) at rank \"k\". We clearly have\nformula_7\nThus the number \"N(ω)\" (resp. \"N(ω)\") of right-to-left minimum (resp. maximum) for the permutation \"ω\" can be written as a sum of independent Bernoulli random variables each with a respective parameter of 1/k :\nformula_8\nIndeed, as \"L(k)\" follows the uniform law on formula_9 \nformula_10\nThe generating function for the Bernoulli random variable formula_11 is \nformula_12\ntherefore the generating function of \"N\" is\nformula_13\n(using the rising factorial notation),\nwhich allows us to recover the product formula for the generating function of the \nStirling numbers of the first kind (unsigned).\n\nThis is an optimal stop problem, a classic in decision theory, statistics and applied probabilities, where a random permutation is gradually revealed through the first elements of its Lehmer code, and where the goal is to stop exactly at the element k such as σ(k)=n, whereas the only available information (the k first values of the Lehmer code) is not sufficient to compute σ(k).\n\nIn less mathematical words : a series of n applicants are interviewed one after the other. The interviewer must hire the best applicant, but must make his decision (“Hire” or “Not hire”) on the spot, without interviewing the next applicant (and \"a fortiori\" without interviewing all applicants).\n\nThe interviewer thus knows the rank of the k applicant, therefore, at the moment of making his k decision, the interviewer knows only the k first elements of the Lehmer code whereas he would need to know all of them to make a well informed decision.\nTo determine the optimal strategies (i.e. the strategy maximizing the probability of a win), the statistical properties of the Lehmer code are crucial.\n\nAllegedly, Johannes Kepler clearly exposed this secretary problem to a friend of his at a time when he was trying to make up his mind and choose one out eleven prospective brides as his second wife. His first marriage had been an unhappy one, having been arranged without himself being consulted, and he was thus very concerned that he could reach the right decision.\n\n"}
{"id": "1118789", "url": "https://en.wikipedia.org/wiki?curid=1118789", "title": "Linearization", "text": "Linearization\n\nIn mathematics, linearization is finding the linear approximation to a function at a given point. The linear approximation of a function is the first order Taylor expansion around the point of interest. In the study of dynamical systems, linearization is a method for assessing the local stability of an equilibrium point of a system of nonlinear differential equations or discrete dynamical systems. This method is used in fields such as engineering, physics, economics, and ecology.\n\nLinearizations of a function are lines—usually lines that can be used for purposes of calculation. Linearization is an effective method for approximating the output of a function formula_1 at any formula_2 based on the value and slope of the function at formula_3, given that formula_4 is differentiable on formula_5 (or formula_6) and that formula_7 is close to formula_8. In short, linearization approximates the output of a function near formula_2.\n\nFor example, formula_10. However, what would be a good approximation of formula_11?\n\nFor any given function formula_1, formula_4 can be approximated if it is near a known differentiable point. The most basic requisite is that formula_14, where formula_15 is the linearization of formula_4 at formula_2. The point-slope form of an equation forms an equation of a line, given a point formula_18 and slope formula_19. The general form of this equation is: formula_20.\n\nUsing the point formula_21, formula_15 becomes formula_23. Because differentiable functions are locally linear, the best slope to substitute in would be the slope of the line tangent to formula_4 at formula_2.\n\nWhile the concept of local linearity applies the most to points arbitrarily close to formula_2, those relatively close work relatively well for linear approximations. The slope formula_19 should be, most accurately, the slope of the tangent line at formula_2.\nVisually, the accompanying diagram shows the tangent line of formula_4 at formula_30. At formula_31, where formula_32 is any small positive or negative value, formula_31 is very nearly the value of the tangent line at the point formula_34.\n\nThe final equation for the linearization of a function at formula_2 is:\n\nformula_36\n\nFor formula_2, formula_38. The derivative of formula_4 is formula_40, and the slope of formula_4 at formula_7 is formula_43.\n\nTo find formula_44, we can use the fact that formula_10. The linearization of formula_46 at formula_2 is formula_48, because the function formula_49 defines the slope of the function formula_46 at formula_30. Substituting in formula_52, the linearization at 4 is formula_53. In this case formula_54, so formula_44 is approximately formula_56. The true value is close to 2.00024998, so the linearization approximation has a relative error of less than 1 millionth of a percent.\n\nThe equation for the linearization of a function formula_57 at a point formula_58 is:\n\nformula_59\n\nThe general equation for the linearization of a multivariable function formula_60 at a point formula_61 is:\n\nformula_62\n\nwhere formula_63 is the vector of variables, and formula_61 is the linearization point of interest\n\nLinearization makes it possible to use tools for studying linear systems to analyze the behavior of a nonlinear function near a given point. The linearization of a function is the first order term of its Taylor expansion around the point of interest. For a system defined by the equation\n\nthe linearized system can be written as\n\nwhere formula_67 is the point of interest and formula_68 is the Jacobian of formula_69 evaluated at formula_67.\n\nIn stability analysis of autonomous systems, one can use the eigenvalues of the Jacobian matrix evaluated at a hyperbolic equilibrium point to determine the nature of that equilibrium. This is the content of linearization theorem. For time-varying systems, the linearization requires additional justification.\n\nIn microeconomics, decision rules may be approximated under the state-space approach to linearization. Under this approach, the Euler equations of the utility maximization problem are linearized around the stationary steady state. A unique solution to the resulting system of dynamic equations then is found.\n\nIn mathematical optimization, cost functions and non-linear components within can be linearized in order to apply a linear solving method such as the Simplex algorithm. The optimized result is reached much more efficiently and is deterministic as a global optimum.\n\nIn multiphysics systems—systems involving multiple physical fields that interact with one another—linearization with respect to each of the physical fields may be performed. This linearization of the system with respect to each of the fields results in a linearized monolithic equation system that can be solved using monolithic iterative solution procedures such as the Newton-Raphson method. Examples of this include MRI scanner systems which results in a system of electromagnetic, mechanical and acoustic fields.\n\n\n"}
{"id": "402719", "url": "https://en.wikipedia.org/wiki?curid=402719", "title": "List of equations", "text": "List of equations\n\nThis is a list of equations, by Wikipedia page under appropriate bands of maths, science and engineering.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "57528388", "url": "https://en.wikipedia.org/wiki?curid=57528388", "title": "María Manzano", "text": "María Manzano\n\nMaría Gracia Manzano Arjona (born 1950) is a Spanish mathematician specializing in mathematical logic and model theory.\n\nManzano earned her Ph.D. in 1977 from the University of Barcelona. Her dissertation, \"Sistemas generales de la lógica de segundo orden\" [General systems of second-order logic], was supervised by Jesús Mosterín. She is a professor of logic and the philosophy of science at the University of Salamanca.\n\nShe is the author of several books on logic and model theory:\n\n"}
{"id": "1640947", "url": "https://en.wikipedia.org/wiki?curid=1640947", "title": "Mathematical Kangaroo", "text": "Mathematical Kangaroo\n\nMathematical Kangaroo (also known as International Mathematical Kangaroo, or Kangourou sans frontières in French) is an international mathematical competition where over 50 countries are represented. There are twelve levels of participation, ranging from grade 1 to grade 12. The competition is held annually on the third Thursday of March. According to the organizers, the key competence tested by Mathematical Kangaroo is logical combination, not just pure knowledge of formulas.\n\nIt is the largest competition for school students in the world, with over 5 million participants from 47 countries in 2009, and 6 million by 2014.\n\nThe competition was established in 1991 by André Deledicq, a professor of mathematics at the University of Paris 7, and , professor of mathematics at Marseille. The idea comes from the Australian Mathematics Competition, initiated in 1978 by Peter O'Halloran. It is based on multiple-choice questions (MCQs), which were rarely used in France at that time, at least in mathematics. For this competition, Jean-Pierre Boudine and André Deledicq were awarded the 1994 d'Alembert prize of the Mathematical Society of France. \n\nThe competition has spread around the world. Pupils from Sweden first took part in 1999. By 2011, 860,000 pupils from 9,000 schools took part in Germany, having grown rapidly from 549,000 in 2007. In 2014, the competition was hosted in Latin America. In 2017, the Bulgarian association held a week-long Kangaroo summer camp In Canada, math contest clubs for elementary school children teach \"questions typical of the Math Kangaroo contest\", starting with those with a visual component and helping to develop logic and spatial reasoning. Students in Pakistan took part for the first time in 2005, the numbers increasing each year since. In 2009, the \"Pittsburgh Post-Gazette\" noted that the competition was very popular in Europe, and was \"finding its way into the United States\". Denmark first participated in 2015.\n\nThe competition is a multiple choice test that runs for 75 minutes. It consists of 24 questions for students up to 4th grade, and 30 questions for other students. The sections for 3 point, 4 point, and 5 point questions are equally divided. The minimum score is 0. The maximum score is 96 points for students up to 4th grade, and 120 points for other students. There is no penalty for any incorrect answer, and no penalty for skipping a question.\n\nEvaluation and collecting of results as well as the prizes are regulated and organized nationally. Special prizes are given for the “longest kangaroo jump” (i.e., the highest number of consecutive correct answers) for each school.\n\nElisabeth Mellroth has investigated the use of mathematical competencies in the mathematical kangaroo.\n\n\n"}
{"id": "41838122", "url": "https://en.wikipedia.org/wiki?curid=41838122", "title": "Michael A. Harrison", "text": "Michael A. Harrison\n\nMichael A. Harrison is a computer scientist, in particular a pioneer in the area of formal languages.\n\nMichael A. Harrison (born in Philadelphia, Pennsylvania, U.S.) studied electrical engineering and computing for BS and MS at the Case Institute of Technology, and then received a PhD from the University of Michigan in Communication Sciences. He was assistant professor from 1963 to 1966 at the University of Michigan, and then joined the faculty of the E.E. Dept at the University of California at Berkeley, where he was an associate professor from 1966 to 1971, and a full professor from 1971 to 1994.\nIn the 1960s, he worked with Sheila Greibach, Gene Rose, Ed Spanier, and Joe Ullian in a research group formed and led by Seymour Ginsburg, dedicated to formal language theory and the foundations of Computer Science. The work that came out of this group distinguished Computer Science theory from other fields. It also brought the field of formal language theory to bear on programming language research.\nIn 1975, he developed the HRU security model (named after its authors Harrison, Ruzzo, Ullman), an operating system level computer security model dealing with the integrity of access rights in the system.\n\nWith his Ph.D. student Pehong Chen at Berkeley, he founded the \"Gain Technology\" company (acquired by Sybase in 1992).\nCurrently, he is professor emeritus and also professor in the graduate school at Berkeley.\n\n"}
{"id": "54150419", "url": "https://en.wikipedia.org/wiki?curid=54150419", "title": "Multivariate Laplace distribution", "text": "Multivariate Laplace distribution\n\nIn the mathematical theory of probability, multivariate Laplace distributions are extensions of the Laplace distribution and the asymmetric Laplace distribution to multiple variables. The marginal distributions of symmetric multivariate Laplace distribution variables are Laplace distributions. The marginal distributions of asymmetric multivariate Laplace distribution variables are asymmetric Laplace distributions.\n\nA typical characterization of the symmetric multivariate Laplace distribution has the characteristic function:\n\nwhere formula_5 is the vector of means for each variable and formula_6 is the covariance matrix.\n\nUnlike the multivariate normal distribution, even if the covariance matrix has zero covariance and correlation the variables are not independent. The symmetric multivariate Laplace distribution is elliptical.\n\nIf formula_7, the probability density function (pdf) for a \"k\"-dimensional multivariate Laplace distribution becomes:\n\nwhere:\n\nformula_1 and formula_2 is the modified Bessel function of the second kind.\n\nIn the correlated bivariate case, i.e., \"k\" = 2, with formula_11 the pdf reduces to:\n\nwhere:\n\nformula_13 and formula_14 are the standard deviations of formula_15 and formula_16, respectively, and formula_17 is the correlation coefficient of formula_15 and formula_16.\n\nFor the independent bivariate Laplace case, that is \"k\" = 2, formula_20 and formula_21, the pdf becomes:\n\nA typical characterization of the asymmetric multivariate Laplace distribution has the characteristic function:\n\nAs with the symmetric multivariate Laplace distribution, the asymmetric multivariate Laplace distribution has mean formula_5, but the covariance becomes formula_25. The asymmetric multivariate Laplace distribution is not elliptical unless formula_7, in which case the distribution reduces to the symmetric multivariate Laplace distribution with formula_7.\n\nThe probability density function (pdf) for a \"k\"-dimensional asymmetric multivariate Laplace distribution is:\n\nwhere:\n\nformula_1 and formula_2 is the modified Bessel function of the second kind.\n\nThe asymmetric Laplace distribution, including the special case of formula_7, is an example of a geometric stable distribution. It represents the limiting distribution for a sum of independent, identically distributed random variables with finite variance and covariance where the number of elements to be summed is itself an independent random variable distributed according to a geometric distribution. Such geometric sums can arise in practical applications within biology, economics and insurance. The distribution may also be applicable in broader situations to model multivariate data with heavier tails than a normal distribution but finite moments.\n\nThe relationship between the exponential distribution and the Laplace distribution allows for a simple method for simulating bivariate asymmetric Laplace variables (including for the case of formula_7). Simulate a bivariate normal random variable vector formula_33 from a distribution with formula_34 and covariance matrix formula_6. Independently simulate an exponential random variables W from an Exp(1) distribution. formula_36 will be distributed (asymmetric) bivariate Laplace with mean formula_5 and covariance matrix formula_6.\n"}
{"id": "3063191", "url": "https://en.wikipedia.org/wiki?curid=3063191", "title": "Nielsen realization problem", "text": "Nielsen realization problem\n\nThe Nielsen realization problem is a question asked by about whether finite subgroups of mapping class groups can act on surfaces, that was answered positively by .\n\nGiven an oriented surface, we can divide the group Diff(\"S\"), the group of diffeomorphisms of the surface to itself, into isotopy classes to get the mapping class group π(Diff(\"S\")). The conjecture asks whether a finite subgroup of the mapping class group of a surface can be realized as the isometry group of a hyperbolic metric on the surface.\n\nThe mapping class group acts on Teichmüller space. An equivalent way of stating the question asks whether every finite subgroup of the mapping class group fixes some point of Teichmüller space.\n\n asked whether finite subgroups of mapping class groups can act on surfaces.\n\n"}
{"id": "910285", "url": "https://en.wikipedia.org/wiki?curid=910285", "title": "Nonagonal number", "text": "Nonagonal number\n\nA nonagonal number is a figurate number that extends the concept of triangular and square numbers to the nonagon (a nine-sided polygon). However, unlike the triangular and square numbers, the patterns involved in the construction of nonagonal numbers are not rotationally symmetrical. Specifically, the \"n\"th nonagonal numbers counts the number of dots in a pattern of \"n\" nested nonagons, all sharing a common corner, where the \"i\"th nonagon in the pattern has sides made of \"i\" dots spaced one unit apart from each other. The nonagonal number for \"n\" is given by the formula:\n\nThe first few nonagonal numbers are:\n\nThe parity of nonagonal numbers follows the pattern odd-odd-even-even.\n\nLetting \"N(n)\" give the \"n\"th nonagonal number and \"T(n)\" the \"n\"th triangular number,\n\nIf is an integer, then is the -th nonagonal number. If is not an integer, then is not nonagonal.\n\n"}
{"id": "24665031", "url": "https://en.wikipedia.org/wiki?curid=24665031", "title": "Project Mathematics!", "text": "Project Mathematics!\n\nProject Mathematics! (stylized as Project MATHEMATICS!), is a series of educational video modules and accompanying workbooks for teachers, developed at the California Institute of Technology to help teach basic principles of mathematics to high school students. In 2017, the entire series of videos was made available on YouTube.\n\nThe \"Project Mathematics!\" series of videos is a teaching aid for teachers to help students understand the basics of geometry and trigonometry. The series was developed by Tom M. Apostol and James F. Blinn, both from the California Institute of Technology. Apostol led the production of the series, while Blinn provided the computer animation used to depict the ideas beings discussed. Blinn mentioned that part of his inspiration was the Bell science series of films from the 1950s.\n\nThe material was designed for teachers to use in their curriculums and was aimed at grades 8 through 13. Workbooks are also available to accompany the videos and to assist teachers in presenting the material to their students. The videos are distributed as either 9 VHS videotapes or 3 DVDs, and include a history of mathematics and examples of how math is used in real world applications.\n\nA total of nine educational video modules were created between 1988 and 2000. Another two modules, \"Teachers Workshop\" and \"Project MATHEMATICS! Contest\", were created in 1991 for teachers and are only available on videotape. The content of the nine educational modules follows below.\n\nIn 1988, \"The Theorem of Pythagoras\" was the first video produced by the series and reviews the Pythagorean theorem. For all right triangles, the square of the hypotenuse is equal to the sum of the squares of the other two sides ( a + b = c ). The theorem is named after Pythagoras of ancient Greece. Pythagorean triples occur when all three sides of a right triangle are integers such as a = 3, b = 4 and c = 5. A clay tablet shows that the Babylonians knew of Pythagorean triples 1200 years before Pythagoras, but nobody knows if they knew the more-general Pythagorean theorem. The Chinese proof uses four similar triangles to prove the theorem. \n\nToday, we know of the Pythagorean theorem because of Euclid's Elements, a set of 13 books on mathematics—from around 300 BCE—and the knowledge it contained has been used for more than 2000 years. Euclid's proof is described in book 1, proposition 47 and uses the idea of equal areas along with shearing and rotating triangles. In the \"dissection proof\", the square of the hypotenuse is cut into pieces to fit into the other two squares. Proposition 31 in book 6 of Euclid's Elements describes the similarity proof, which states that the squares of each side can be replaced by shapes that are similar to each other and the proof still works.\n\nThe second module created was \"The Story of Pi\", in 1989, and describes the mathematical constant pi and its history. The first letter in the Greek word for \"perimeter\" (περίμετρος) is , known in English as \"pi\". Pi is the ratio of a circle's circumference to its diameter and is roughly equal to 3.14159. The circumference of a circle is formula_1 and its area is formula_2. The volume and surface area of a cylinder, cone sphere and torus are calculated using pi. Pi is also used in calculating planetary orbit times, gaussian curves and alternating current. In calculus, there are infinite series that involve pi and pi is used in trigonometry. Ancient cultures used different approximations for pi. The Babylonian's used formula_3 and the Egyptians used formula_4. \n\nPi is a fundamental constant of nature. Archimedes discovered that the area of the circle equals the square of its radius times pi. Archimedes was the first to accurately calculate pi by using polygons with 96 sides both inside and outside a circle then measuring the line segments and finding that pi was between formula_5 and formula_6. A Chinese calculation used polygons with 3,000 sides and calculated pi accurately to five decimal places. The Chinese also found that formula_7 was an accurate estimate of pi to within 6 decimal places and was the most accurate estimate for 1,000 years until arabic numerals were used for arithmetic. \n\nBy the end of the 19th century, formulas were discovered to calculate pi without the need for geometric diagrams. These formulas used infinite series and trigonometric functions to calculate pi to hundreds of decimal places. Computers were used in the 20th century to calculate pi and its value was known to one billion decimals places by 1989. One reason to accurately calculate pi is to test the performance of computers. Another reason is to determine if pi is a specific fraction, which is a ratio of two integers called a rational number that has a repeating pattern of digits when expressed in decimal form. In the 18th century, Johann Lambert found that pi cannot be a ratio and is therefore an irrational number. Pi shows up in many areas having no obvious connection to circles. For example; the fraction of points on a lattice viewable from an origin point is equal to formula_8.\n\nDiscusses how scaling objects does not change their shape and how angles stay the same. Also shows how ratios change for perimeters, areas and volumes.\n\nVisually depicts how sines and cosines are related to waves and a unit circle. Also reviews their relationship to the ratios of side lengths of right triangles.\n\nExplains the law of sines and cosines how they relate to sides and angles of a triangle. The module also gives some real life examples of their use.\n\nDescribes the addition formulas of sines and cosines and discusses the history of Ptolemy's Almagest. It also goes into details of Ptolemy's Theorem. Animation shows how sines and cosines relate to harmonic motion.\n\nHow polynomials can approximate sines and cosines. Includes information about cubic splines in design engineering.\n\nHow did the ancients dig the Tunnel of Samos from two opposite sides of a mountain in 500 BCE? And how were they able to meet under the mountain? Maybe they used geometry and trigonometry.\n\nReviews some of the major developments in mathematical history.\n\nThe \"Project Mathematics!\" series was created and directed by Tom M. Apostol and James F. Blinn, both from the California Institute of Technology. The project was originally titled \"Mathematica\" but was changed to avoid confusion with the mathematics software package. A total of four full-time employees and four part-time employees produce the episodes with help from several volunteers. Each episode took between four and five months to produce. Blinn headed the creation of the computer animation used in each episode, which was done on a network of computers donated by Hewlett-Packard.\n\nThe majority of the funding came from two grants from the National Science Foundation totaling $3.1 million. Free distribution of some of the modules was provided by a grant from Intel.\n\n\"Project Mathematics!\" video tapes, DVDs and workbooks are primarily distributed to teachers through the California Institute of Technology bookstore and were popular enough that the bookstore hired an extra person just for processing orders of the series. An estimated 140,000 of the tapes and DVDs were sent to educational institutions around the world, and have been viewed by approximately 10 million people over the last 20 years.\n\nThe series is also distributed through the Mathematical Association of America and NASA's Central Operation of Resources for Educators (CORE). In addition, over half of the states in the US have received master copies of the videotapes so they can produce and distribute copies to their various educational institutions. The videotapes may be freely copied for educational purposes with a few restrictions, but the DVD version is not freely reproducible.\n\nThe video segments for the first 3 modules can be viewed for free at the \"Project Mathematics!\" website as streaming video. Selected video segments of the remaining 6 modules are also available for free viewing.\n\nIn 2017, Caltech made the entirety of the series, as well as three SIGGRAPH demo videos, available on YouTube.\n\nThe videos have been translated into Hebrew, Portuguese, French, and Spanish with the DVD version being both English and Spanish. PAL versions of the videos are available as well and efforts are underway to translate the materials into Korean.\n\nAll of the following were published by the California Institute of Technology:\n\n\"Project Mathematics!\" has received numerous awards including the Gold Apple award in 1989 from the National Educational Film and Video Festival.\n\nA web-based version of the materials was funded by a third grant from the National Science Foundation and was in phase 1, .\n\n\n"}
{"id": "303368", "url": "https://en.wikipedia.org/wiki?curid=303368", "title": "Quintic function", "text": "Quintic function\n\nIn algebra, a quintic function is a function of the form\n\nwhere , , , , and are members of a field, typically the rational numbers, the real numbers or the complex numbers, and is nonzero. In other words, a quintic function is defined by a polynomial of degree five.\n\nIf is zero but one of the coefficients , , , or is non-zero, the function is classified as either a quartic function, cubic function, quadratic function or linear function.\n\nBecause they have an odd degree, normal quintic functions appear similar to normal cubic functions when graphed, except they may possess an additional local maximum and local minimum each. The derivative of a quintic function is a quartic function.\n\nSetting and assuming produces a quintic equation of the form:\nSolving quintic equations in terms of radicals was a major problem in algebra from the 16th century, when cubic and quartic equations were solved, until the first half of the 19th century, when the impossibility of such a general solution was proved with the Abel–Ruffini theorem.\n\nFinding the roots of a given polynomial has been a prominent mathematical problem.\n\nSolving linear, quadratic, cubic and quartic equations by factorization into radicals can always be done, no matter whether the roots are rational or irrational, real or complex; there are formulae that yield the required solutions. However, there is no algebraic expression for general quintic equations over the rationals in terms of radicals; this statement is known as the Abel–Ruffini theorem, first asserted in 1799 and completely proved in 1824. This result also holds for equations of higher degrees. An example of a quintic whose roots cannot be expressed in terms of radicals is . This quintic is in Bring–Jerrard normal form.\n\nSome quintics may be solved in terms of radicals. However, the solution is generally too complex to be used in practice. Instead, numerical approximations are calculated using root-finding algorithm for polynomials.\n\nSome quintic equations can be solved in terms of radicals. These include the quintic equations defined by a polynomial that is reducible, such as . For example, it has been shown that \n\nhas solutions in radicals if and only if it has an integer solution or \"r\" is one of ±15, ±22440, or ±2759640, in which cases the polynomial is reducible.\n\nAs solving reducible quintic equations reduces immediately to solving polynomials of lower degree, only irreducible quintic equations are considered in the remainder of this section, and the term \"quintic\" will refer only to irreducible quintics. A solvable quintic is thus an irreducible quintic polynomial whose roots may be expressed in terms of radicals.\n\nTo characterize solvable quintics, and more generally solvable polynomials of higher degree, Évariste Galois developed techniques which gave rise to group theory and Galois theory. Applying these techniques, Arthur Cayley found a general criterion for determining whether any given quintic is solvable. This criterion is the following.\n\nGiven the equation\nthe Tschirnhaus transformation , which depresses the quintic (that means removes the term of degree four), gives the equation\n\nwhere\n\nBoth quintics are solvable by radicals if and only if either they are factorisable in equations of lower degrees with rational coefficients or the polynomial , named \"Cayley's resolvent\", has a rational root in , where\n\nand\n\nCayley's result allows us to test if a quintic is solvable. If it is the case, finding its roots is a more difficult problem, which consists of expressing the roots in terms of radicals involving the coefficients of the quintic and the rational root of Cayley's resolvent.\n\nIn 1888, George Paxton Young described how to solve a solvable quintic equation, without providing an explicit formula; Daniel Lazard wrote out a three-page formula (Lazard (2004)).\n\nThere are several parametric representations of solvable quintics of the form , called the Bring–Jerrard form.\n\nDuring the second half of 19th century, John Stuart Glashan, George Paxton Young, and Carl Runge gave such a parameterization: an irreducible quintic with rational coefficients in Bring–Jerrard form\nis solvable if and only if either or it may be written\nwhere and are rational.\n\nIn 1994, Blair Spearman and Kenneth S. Williams gave an alternative,\n\nThe relationship between the 1885 and 1994 parameterizations can be seen by defining the expression\nwhere . Using the negative case of the square root yields, after scaling variables, the first parametrization while the positive case gives the second.\n\nThe substitution , in the Spearman-Williams parameterization allows one to not exclude the special case , giving the following result:\n\nIf and are rational numbers, the equation is solvable by radicals if either its left-hand side is a product of polynomials of degree less than 5 with rational coefficients or there exist two rational numbers and such that\n\nA polynomial equation is solvable by radicals if its Galois group is a solvable group. In the case of irreducible quintics, the Galois group is a subgroup of the symmetric group of all permutations of a five element set, which is solvable if and only if it is a subgroup of the group , of order , generated by the cyclic permutations and .\n\nIf the quintic is solvable, one of the solutions may be represented by an algebraic expression involving a fifth root and at most two square roots, generally nested. The other solutions may then be obtained either by changing of fifth root or by multiplying all the occurrences of the fifth root by the same power of a primitive 5th root of unity\n\nAll four primitive fifth roots of unity may be obtained by changing the signs of the square roots appropriately, namely:\n\nwhere formula_19, yielding the four distinct primitive fifth roots of unity.\n\nIt follows that one may need four different square roots for writing all the roots of a solvable quintic. Even for the first root that involves at most two square roots, the expression of the solutions in terms of radicals is usually huge. However, when no square root is needed, the form of the first solution may be rather simple, as for the equation , for which the only real solution is\n\nAn example of a more complex (although small enough to be written here) solution is the unique real root of . Let , , and , where is the golden ratio. Then the only real solution is given by\n\nor, equivalently, by\n\nwhere the are the four roots of the quartic equation\n\nMore generally, if an equation of prime degree with rational coefficients is solvable in radicals, then one can define an auxiliary equation of degree , also with rational coefficients, such that each root of is the sum of -th roots of the roots of . These -th roots have been introduced by Joseph-Louis Lagrange, and their product by are commonly called Lagrange resolvents. The computation of and its roots can be used to solve . However these -th roots may not be computed independently (this would provide roots instead of ). Thus a correct solution needs to express all these -roots in term of one of them. Galois theory shows that this is always theoretically possible, even if the resulting formula may be too large to be of any use.\n\nIt is possible that some of the roots of are rational (as in the first example of this section) or some are zero. In these cases, the formula for the roots is much simpler, as for the solvable de Moivre quintic\n\nwhere the auxiliary equation has two zero roots and reduces, by factoring them out, to the quadratic equation\n\nsuch that the five roots of the de Moivre quintic are given by\n\nwhere \"y\" is any root of the auxiliary quadratic equation and \"ω\" is any of the four primitive 5th roots of unity. This can be easily generalized to construct a solvable septic and other odd degrees, not necessarily prime.\n\nThere are infinitely many solvable quintics in Bring-Jerrard form which have been parameterized in a preceding section.\n\nUp to the scaling of the variable, there are exactly five solvable quintics of the shape formula_27, which are (where \"s\" is a scaling factor):\n\nPaxton Young (1888) gave a number of examples of solvable quintics:\n\nAn infinite sequence of solvable quintics may be constructed, whose roots are sums of \"n\"-th roots of unity, with \"n\" = 10\"k\" + 1 being a prime number:\n\nThere are also two parameterized families of solvable quintics:\nThe Kondo–Brumer quintic,\n\nand the family depending on the parameters formula_34\n\nwhere\n\nAnalogously to cubic equations, there are solvable quintics which have five real roots all of whose solutions in radicals involve roots of complex numbers. This is \"casus irreducibilis\" for the quintic, which is discussed in Dummit.\n\nAbout 1835, Jerrard demonstrated that quintics can be solved by using ultraradicals (also known as Bring radicals), the unique real root of for real numbers . In 1858 Charles Hermite showed that the Bring radical could be characterized in terms of the Jacobi theta functions and their associated elliptic modular functions, using an approach similar to the more familiar approach of solving cubic equations by means of trigonometric functions. At around the same time, Leopold Kronecker, using group theory, developed a simpler way of deriving Hermite's result, as had Francesco Brioschi. Later, Felix Klein came up with a method that relates the symmetries of the icosahedron, Galois theory, and the elliptic modular functions that are featured in Hermite's solution, giving an explanation for why they should appear at all, and developed his own solution in terms of generalized hypergeometric functions. Similar phenomena occur in degree (septic equations) and , as studied by Klein and discussed in icosahedral symmetry: related geometries.\n\nA Tschirnhaus transformation, which may be computed by solving a quartic equation, reduces the general quintic equation of the form \nto the Bring–Jerrard normal form .\n\nThe roots of this equation cannot be expressed by radicals. However, in 1858, Charles Hermite published the first known solution of this equation in terms of elliptic functions.\nAt around the same time Francesco Brioschi \nand Leopold Kronecker\ncame upon equivalent solutions.\n\nSee Bring radical for details on these solutions and some related ones.\n\nSolving for the locations of the Lagrangian points of an astronomical orbit in which the masses of both objects are non-negligible involves solving a quintic.\n\nMore precisely, the locations of L and L are the solutions to the following equations, where the gravitational forces of two masses on a third (e.g. Sun and Earth on satellites such as Gaia at L and SOHO at L) provide the satellite's centripetal force necessary to be in a synchronous orbit with Earth around the Sun:\n\nThe ± sign corresponds to L and L, respectively; G is the gravitational constant, \"ω\" the angular velocity, r the distance of the satellite to Earth, R the distance Sun to Earth (i.e. the semi-major axis of Earth's orbit), and m, M, and M are the respective masses of satellite, Earth, and Sun.\n\nUsing Kepler's Third Law formula_39 and rearranging all terms yields the quintic\n\nwith formula_41 , formula_42 , formula_43 , formula_44 (thus d = 0 for L), formula_45 , formula_46 .\n\nSolving these two quintics yields for L and for L. The Sun–Earth Lagrangian points L and L are usually given as 1.5 million km from Earth.\n\n\n\n"}
{"id": "651752", "url": "https://en.wikipedia.org/wiki?curid=651752", "title": "Scaling (geometry)", "text": "Scaling (geometry)\n\nIn Euclidean geometry, uniform scaling (or isotropic scaling) is a linear transformation that enlarges (increases) or shrinks (diminishes) objects by a scale factor that is the same in all directions. The result of uniform scaling is similar (in the geometric sense) to the original. A scale factor of 1 is normally allowed, so that congruent shapes are also classed as similar. Uniform scaling happens, for example, when enlarging or reducing a photograph, or when creating a scale model of a building, car, airplane, etc.\n\nMore general is scaling with a separate scale factor for each axis direction. Non-uniform scaling (anisotropic scaling) is obtained when at least one of the scaling factors is different from the others; a special case is directional scaling or stretching (in one direction). Non-uniform scaling changes the shape of the object; e.g. a square may change into a rectangle, or into a parallelogram if the sides of the square are not parallel to the scaling axes (the angles between lines parallel to the axes are preserved, but not all angles). It occurs, for example, when a faraway billboard is viewed from an oblique angle, or when the shadow of a flat object falls on a surface that is not parallel to it.\n\nWhen the scale factor is larger than 1, (uniform or non-uniform) scaling is sometimes also called dilation or enlargement. When the scale factor is a positive number smaller than 1, scaling is sometimes also called contraction.\n\nIn the most general sense, a scaling includes the case in which the directions of scaling are not perpendicular. It also includes the case in which one or more scale factors are equal to zero (projection), and the case of one or more negative scale factors (a directional scaling by -1 is equivalent to a reflection).\n\nScaling is a linear transformation, and a special case of homothetic transformation. In most cases, the homothetic transformations are non-linear transformations.\n\nA scaling can be represented by a scaling matrix. To scale an object by a vector \"v\" = (\"v, v, v\"), each point \"p\" = (\"p, p, p\") would need to be multiplied with this scaling matrix:\n\nAs shown below, the multiplication will give the expected result:\n\nSuch a scaling changes the diameter of an object by a factor between the scale factors, the area by a factor between the smallest and the largest product of two scale factors, and the volume by the product of all three.\n\nThe scaling is uniform if and only if the scaling factors are equal (\"v = v = v\"). If all except one of the scale factors are equal to 1, we have directional scaling.\n\nIn the case where \"v = v = v = k\", scaling increases the area of any surface by a factor of k and the volume of any solid object by a factor of k.\n\nIn formula_3-dimensional space formula_4, uniform scaling by a factor formula_5 is accomplished by scalar multiplication with formula_5, that is, multiplying each coordinate of each point by formula_5. As a special case of linear transformation, it can be achieved also by multiplying each point (viewed as a column vector) with a diagonal matrix whose entries on the diagonal are all equal to formula_5, namely formula_9 .\n\nNon-uniform scaling is accomplished by multiplication with any symmetric matrix. The eigenvalues of the matrix are the scale factors, and the corresponding eigenvectors are the axes along which each scale factor applies. A special case is a diagonal matrix, with arbitrary numbers formula_10 along the diagonal: the axes of scaling are then the coordinate axes, and the transformation scales along each axis formula_11 by the factor formula_12\n\nIn uniform scaling with a non-zero scale factor, all non-zero vectors retain their direction (as seen from the origin), or all have the direction reversed, depending on the sign of the scaling factor. In non-uniform scaling only the vectors that belong to an eigenspace will retain their direction. A vector that is the sum of two or more non-zero vectors belonging to different eigenspaces will be tilted towards the eigenspace with largest eigenvalue.\n\nIn projective geometry, often used in computer graphics, points are represented using homogeneous coordinates. To scale an object by a vector \"v\" = (\"v, v, v\"), each homogeneous coordinate vector \"p\" = (\"p, p, p\", 1) would need to be multiplied with this projective transformation matrix:\n\nAs shown below, the multiplication will give the expected result:\n\nSince the last component of a homogeneous coordinate can be viewed as the denominator of the other three components, a uniform scaling by a common factor \"s\" (uniform scaling) can be accomplished by using this scaling matrix:\n\nFor each vector \"p\" = (\"p, p, p\", 1) we would have\nwhich would be equivalent to\n\nGiven a point formula_18, the dilation associates it with the point formula_19 through the equations\nformula_20 for formula_21\n\nTherefore, given a function formula_22, the equation of the dilated function is\n\nformula_23\n\nIf formula_24, the transformation is horizontal; when formula_25, it is a dilation, when formula_26, it is a contraction.\n\nIf formula_27, the transformation is vertical; when formula_28 it is a dilation, when formula_29, it is a contraction.\n\n\n"}
{"id": "26339669", "url": "https://en.wikipedia.org/wiki?curid=26339669", "title": "Schur algebra", "text": "Schur algebra\n\nIn mathematics, Schur algebras, named after Issai Schur, are certain finite-dimensional algebras closely associated with Schur–Weyl duality between general linear and symmetric groups. They are used to relate the representation theories of those two groups. Their use was promoted by the influential monograph of J. A. Green first published in 1980. The name \"Schur algebra\" is due to Green. In the modular case (over infinite fields of positive characteristic) Schur algebras were used by Gordon James and Karin Erdmann to show that the (still open) problems of computing decomposition numbers for general linear groups and symmetric groups are actually equivalent. Schur algebras were used by Friedlander and Suslin to prove finite generation of cohomology of finite group schemes.\n\nThe Schur algebra formula_1 can be defined for any commutative ring formula_2 and integers formula_3. Consider the algebra formula_4 of polynomials (with coefficients in formula_2) in formula_6 commuting variables formula_7, 1 ≤ \"i\", \"j\" ≤ formula_8. Denote by formula_9 the homogeneous polynomials of degree formula_10. Elements of formula_9 are \"k\"-linear combinations of monomials formed by multiplying together formula_10 of the generators formula_7 (allowing repetition). Thus\n\nNow, formula_4 has a natural coalgebra structure with comultiplication formula_16 and counit formula_17 the algebra homomorphisms given on generators by\n\nSince comultiplication is an algebra homomorphism, formula_4 is a bialgebra. One easily\nchecks that formula_9 is a subcoalgebra of the bialgebra formula_4, for every \"r\" ≥ 0.\n\nDefinition. The Schur algebra (in degree formula_10) is the algebra formula_23. That is, formula_24 is the linear dual of formula_25.\n\nIt is a general fact that the linear dual of a coalgebra formula_26 is an algebra in a natural way, where the multiplication in the algebra is induced by dualizing the comultiplication in the coalgebra. To see this, let \nand, given linear functionals formula_28, formula_29 on formula_26, define their product to be the linear functional given by \nThe identity element for this multiplication of functionals is the counit in formula_26.\n\n\nThen the symmetric group formula_38 on formula_10 letters acts naturally on the tensor space by place permutation, and one has an isomorphism \nIn other words, formula_24 may be viewed as the algebra of endomorphisms of tensor space commuting with the action of the symmetric group.\n\n\n\n\nThe study of these various classes of generalizations forms an active area of contemporary research.\n\n"}
{"id": "5438948", "url": "https://en.wikipedia.org/wiki?curid=5438948", "title": "Substitution (logic)", "text": "Substitution (logic)\n\nSubstitution is a fundamental concept in logic.\nA substitution is a syntactic transformation on formal expressions.\nTo apply a substitution to an expression means to consistently replace its variable, or placeholder, symbols by other expressions.\nThe resulting expression is called a substitution instance, or short instance, of the original expression.\n\nWhere \"ψ\" and \"φ\" represent formulas of propositional logic, \"ψ\" is a substitution instance of \"φ\" if and only if \"ψ\" may be obtained from \"φ\" by substituting formulas for symbols in \"φ\", replacing each occurrence of the same symbol by an occurrence of the same formula. For example:\n\nis a substitution instance of:\n\nand\n\nis a substitution instance of:\n\nIn some deduction systems for propositional logic, a new expression (a proposition) may be entered on a line of a derivation if it is a substitution instance of a previous line of the derivation (Hunter 1971, p. 118). This is how new lines are introduced in some axiomatic systems. In systems that use rules of transformation, a rule may include the use of a \"substitution instance\" for the purpose of introducing certain variables into a derivation.\n\nIn first-order logic, every closed propositional formula that can be derived from an open propositional formula \"a\" by substitution is said to be a substitution instance of \"a\". If \"a\" is a closed propositional formula we count \"a\" itself as its only substitution instance.\n\nA propositional formula is a tautology if it is true under every valuation (or interpretation) of its predicate symbols. If Φ is a tautology, and Θ is a substitution instance of Φ, then Θ is again a tautology. This fact implies the soundness of the deduction rule described in the previous section.\n\nrefers to a substitution mapping each variable \"x\" to the corresponding term \"t\", for \"i\"=1...,\"k\", and every other variable to itself; the \"x\" must be pairwise distinct. Applying that substitution to a term \"t\" is written in postfix notation as \"t\" { \"x\" ↦ \"t\", ..., \"x\" ↦ \"t\" }; it means to (simultaneously) replace every occurrence of each \"x\" in \"t\" by \"t\". The result \"t\"σ of applying a substitution σ to a term \"t\" is called an instance of that term \"t\".\nFor example, applying the substitution { \"x\" ↦ \"z\", \"z\" ↦ \"h\"(\"a\",\"y\") } to the term \n\nThe domain \"dom\"(σ) of a substitution σ is commonly defined as the set of variables actually replaced, i.e. \"dom\"(σ) = { \"x\" ∈ \"V\" | \"x\"σ ≠ \"x\" }.\nA substitution is called a ground substitution if it maps all variables of its domain to ground, i.e. variable-free, terms.\nThe substitution instance \"t\"σ of a ground substitution is a ground term if all of \"t\"'s variables are in σ's domain, i.e. if \"vars\"(\"t\") ⊆ \"dom\"(σ).\nA substitution σ is called a linear substitution if \"t\"σ is a linear term for some (and hence every) linear term \"t\" containing precisely the variables of σ's domain, i.e. with \"vars\"(\"t\") = \"dom\"(σ).\nA substitution σ is called a flat substitution if \"x\"σ is a variable for every variable \"x\".\nA substitution σ is called a renaming substitution if it is a permutation on the set of all variables. Like every permutation, a renaming substitution σ always has an inverse substitution σ, such that \"t\"σσ = \"t\" = \"t\"σσ for every term \"t\". However, it is not possible to define an inverse for an arbitrary substitution.\n\nFor example, { \"x\" ↦ 2, \"y\" ↦ 3+4 } is a ground substitution, { \"x\" ↦ \"x\", \"y\" ↦ \"y\"+4 } is non-ground and non-flat, but linear,\n\nTwo substitutions are considered equal if they map each variable to structurally equal result terms, formally: σ = τ if \"x\"σ = \"x\"τ for each variable \"x\" ∈ \"V\".\nThe composition of two substitutions σ = { \"x\" ↦ \"t\", ..., \"x\" ↦ \"t\" } and τ = { \"y\" ↦ \"u\", ..., \"y\" ↦ u } is obtained by removing from the substitution { \"x\" ↦ \"t\"τ, ..., \"x\" ↦ \"t\"τ, \"y\" ↦ \"u\", ..., \"y\" ↦ \"u\" } those pairs \"y\" ↦ \"u\" for which \"y\" ∈ { \"x\", ..., \"x\" }.\nThe composition of σ and τ is denoted by στ. Composition is an associative operation, and is compatible with substitution application, i.e. (ρσ)τ = ρ(στ), and (\"t\"σ)τ = \"t\"(στ), respectively, for every substitutions ρ, σ, τ, and every term \"t\".\nThe identity substitution, which maps every variable to itself, is the neutral element of substitution composition. A substitution σ is called idempotent if σσ = σ, and hence \"t\"σσ = \"t\"σ for every term \"t\". The substitution { \"x\" ↦ \"t\", ..., \"x\" ↦ \"t\" } is idempotent if and only if none of the variables \"x\" occurs in any \"t\". Substitution composition is not commutative, that is, στ may be different from τσ, even if σ and τ are idempotent.\n\nFor example, { \"x\" ↦ 2, \"y\" ↦ 3+4 } is equal to { \"y\" ↦ 3+4, \"x\" ↦ 2 }, but different from { \"x\" ↦ 2, \"y\" ↦ 7 }. The substitution { \"x\" ↦ \"y\"+\"y\" } is idempotent, e.g. ((\"x\"+\"y\") {\"x\"↦\"y\"+\"y\"}) {\"x\"↦\"y\"+\"y\"} = ((\"y\"+\"y\")+\"y\") {\"x\"↦\"y\"+\"y\"} = (\"y\"+\"y\")+\"y\", while the substitution { \"x\" ↦ \"x\"+\"y\" } is non-idempotent, e.g. ((\"x\"+\"y\") {\"x\"↦\"x\"+\"y\"}) {\"x\"↦\"x\"+\"y\"} = ((\"x\"+\"y\")+\"y\") {\"x\"↦\"x\"+\"y\"} = ((\"x\"+\"y\")+\"y\")+\"y\". An example for non-commuting substitutions is { \"x\" ↦ \"y\" } { \"y\" ↦ \"z\" } = { \"x\" ↦ \"z\", \"y\" ↦ \"z\" }, but { \"y\" ↦ \"z\"} { \"x\" ↦ \"y\"} = { \"x\" ↦ \"y\", \"y\" ↦ \"z\" }.\n\n"}
{"id": "147298", "url": "https://en.wikipedia.org/wiki?curid=147298", "title": "Sum rule in integration", "text": "Sum rule in integration\n\nIn calculus, the sum rule in integration states that the integral of a sum of two functions is equal to the sum of their integrals. It is of particular use for the integration of sums, and is one part of the linearity of integration.\n\nAs with many properties of integrals in calculus, the sum rule applies both to definite integrals and indefinite integrals. For indefinite integrals, the sum rule states\n\nFor example, if you know that the integral of exp(x) is exp(x) from calculus with exponentials and that the integral of cos(x) is sin(x) from calculus with trigonometry then:\n\nSome other general results come from this rule. For example:\nThe proof above relied on the special case of the constant factor rule in integration with k=-1.\n\nThus, the sum rule might be written as:\n\nAnother basic application is that sigma and integral signs can be changed around. That is:\n\nThis is simply because:\n\nPassing from the case of indefinite integrals to the case of integrals over an interval [a,b], we get exactly the same form of rule (the arbitrary constant of integration disappears).\n\nFirst note that from the definition of integration as the antiderivative, the reverse process of differentiation:\n\nAdding these,\n\nNow take the sum rule in differentiation:\n\nIntegrate both sides with respect to x:\n\nSo we have, looking at (1) and (2):\n\nTherefore:\n\nNow substitute:\n"}
{"id": "433005", "url": "https://en.wikipedia.org/wiki?curid=433005", "title": "The Emperor's New Mind", "text": "The Emperor's New Mind\n\nThe Emperor's New Mind: Concerning Computers, Minds and The Laws of Physics is a 1989 book by mathematical physicist Sir Roger Penrose.\n\nPenrose argues that human consciousness is non-algorithmic, and thus is not capable of being modeled by a conventional Turing machine, which includes a digital computer. Penrose hypothesizes that quantum mechanics plays an essential role in the understanding of human consciousness. The collapse of the quantum wavefunction is seen as playing an important role in brain function.\n\nThe majority of the book is spent reviewing, for the scientifically minded layreader, a plethora of interrelated subjects such as Newtonian physics, special and general relativity, the philosophy and limitations of mathematics, quantum physics, cosmology, and the nature of time. Penrose intermittently describes how each of these bears on his developing theme: that consciousness is not \"algorithmic\". Only the later portions of the book address the thesis directly.\n\nPenrose states that his ideas on the nature of consciousness are speculative, and his thesis is considered erroneous by experts in the fields of philosophy, computer science, and robotics.\n\nFollowing the publication of this book, Penrose began to collaborate with Stuart Hameroff on a biological analog to quantum computation involving microtubules, which became the foundation for his subsequent book, \"Shadows of the Mind: A Search for the Missing Science of Consciousness\".\n\n\"The Emperor's New Mind\" attacks the claims of artificial intelligence using the physics of computing: Penrose notes that the present home of computing lies more in the tangible world of classical mechanics than in the imponderable realm of quantum mechanics. The modern computer is a deterministic system that for the most part simply executes algorithms. Penrose shows that, by reconfiguring the boundaries of a billiard table, one might make a computer in which the billiard balls act as message carriers and their interactions act as logical decisions. The billiard-ball computer was first designed some years ago by Edward Fredkin and Tommaso Toffoli of the Massachusetts Institute of Technology.\n\nPenrose won the Science Book Prize in 1990 for this book.\n\n"}
{"id": "54423048", "url": "https://en.wikipedia.org/wiki?curid=54423048", "title": "Tree transducer", "text": "Tree transducer\n\nIn theoretical computer science and formal language theory, a tree transducer (TT) is an abstract machine taking as input a tree, and generating output – generally other trees, but models producing words or other structures exist. Roughly speaking, tree transducers extend tree automata in the same way that word transducers extend word automata.\n\nManipulating tree structures instead of words enable TT to model syntax-directed transformations of formal or natural languages. However, TT are not as well-behaved as their word counterparts in terms of algorithmic complexity, closure properties, etcetera. In particular, most of the main classes are not closed under composition.\n\nThe main classes of tree transducers are:\n\nA TOP \"T\" is a tuple (\"Q\", Σ, Γ, \"I\", δ) such that:\n\n\nFor instance, \nis a rule – one customarily writes formula_5 instead of the pair formula_6 – and its intuitive semantics is that, under the action of \"q\", a tree with \"f\" at the root and three children is transformed into \nwhere, recursively, formula_8 and formula_9 are replaced, respectively, with the application of formula_10 on the first child and \nwith the application of formula_11 on the third.\n\nThe semantics of each state of the transducer \"T\", and of \"T\" itself, is a binary relation between input trees (on Σ) and output trees (on Γ).\n\nA way of defining the semantics formally is to see formula_1 as a term rewriting system, provided that in the right-hand sides the calls are written in the form formula_5, where states \"q\" are unary symbols. Then the semantics formula_14 of a state \"q\" is given by\nThe semantics of \"T\" is then defined as the union of the semantics of its initial states:\n\nAs with tree automata, a TOP is said to be deterministic (abbreviated DTOP) if no two rules of δ share the same left-hand side, and there is at most one initial state. In that case, the semantics of the DTOP is a partial function from input trees (on Σ) to output trees (on Γ), as are the semantics of each of the DTOP's states.\n\nThe domain of a transducer is the domain of its semantics. Likewise, the image of a transducer is the image of its semantics.\n\n\n\n\n\n\nAs in the simpler case of tree automata, bottom-up tree transducers are defined similarly to their top-down counterparts, but proceed from the leaves of the tree to the root, instead of from the root to the leaves. Thus the main difference is in the form of the rules, which are of the form formula_34.\n\n"}
{"id": "27190089", "url": "https://en.wikipedia.org/wiki?curid=27190089", "title": "Vinay V. Deodhar", "text": "Vinay V. Deodhar\n\nVinay Vithal Deodhar (3 December 1948 – 18 January 2015) was a Professor Emeritus with the Department of Mathematics at the Indiana University who worked in the area of algebraic groups and representation theory.\n\nDeodhar was born in Bombay, India in 1948.\n\nHe earned his Ph.D. from the University of Mumbai in 1974 for his work \"On Central Extensions of Rational Points of Algebraic Groups\" done under the supervision of M. S. Raghunathan.\n\nHe was a visiting scholar at the Institute for Advanced Study during 1975-77 and 1992-93.\n"}
