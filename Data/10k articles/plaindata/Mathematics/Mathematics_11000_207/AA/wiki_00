{"id": "426749", "url": "https://en.wikipedia.org/wiki?curid=426749", "title": "Adjusted present value", "text": "Adjusted present value\n\nThe method is to calculate the NPV of the project as if it is all-equity financed (so called base case). Then the base-case NPV is adjusted for the benefits of financing. Usually, the main benefit is a tax shield resulted from tax deductibility of interest payments. Another benefit can be a subsidized borrowing at sub-market rates. The APV method is especially effective when a leveraged buyout case is considered since the company is loaded with an extreme amount of debt, so the tax shield is substantial.\n\nTechnically, an APV valuation model looks similar to a standard DCF model. However, instead of WACC, cash flows would be discounted at the unlevered cost of equity, and tax shields at either the cost of debt (Myers) or following later academics also with the unlevered cost of equity. APV and the standard DCF approaches should give the identical result if the capital structure remains stable.\n\nAPV = Unlevered NPV of Free Cash Flows and assumed Terminal Value + NPV of Interest Tax Shield and assumed Terminal Value\n\nThe discount rate used in the first part is the return on assets or return on equity if unlevered.\nThe discount rate used in the second part is the cost of debt financing by period.\n\nIn detail:\n\nEBIT\n\n- Taxes on EBIT\n\n=Net Operating Profit After Tax (NOPAT)\n\n+ Non cash items in EBIT\n\n- Working Capital changes\n\n- Capital Expenditures and Other Operating Investments\n\n=Free Cash Flows\n\nTake Present Value (PV) of FCFs discounted by Return on Assets % (also Return on Unlevered Equity %)\n\n+ PV of terminal value\n\n=Value of Unlevered Assets\n\n+ Excess cash and other assets\n\n=Value of Unlevered Firm (i.e., firm value without financing effects or benefit of interest tax shield)\n\n+ Present Value of Debt's Periodic Interest Tax Shield discounted by Cost of Debt Financing %\n\n=Value of Levered Firm\n\n- Value of Debt\n\n=Value of Levered Equity or APV\n\nThe value from the interest tax shield assumes the company is profitable enough to deduct the interest expense. If not, adjust this part for when the interest can be deducted for tax purposes.\n"}
{"id": "2848519", "url": "https://en.wikipedia.org/wiki?curid=2848519", "title": "Algebraic semantics (mathematical logic)", "text": "Algebraic semantics (mathematical logic)\n\nIn mathematical logic, algebraic semantics is a formal semantics based on algebras studied as part of algebraic logic. For example, the modal logic S4 is characterized by the class of topological boolean algebras—that is, boolean algebras with an interior operator. Other modal logics are characterized by various other algebras with operators. The class of boolean algebras characterizes classical propositional logic, and the class of Heyting algebras propositional intuitionistic logic.\n\n"}
{"id": "2112", "url": "https://en.wikipedia.org/wiki?curid=2112", "title": "Associative algebra", "text": "Associative algebra\n\nIn mathematics, an associative algebra is an algebraic structure with compatible operations of addition, multiplication (assumed to be associative), and a scalar multiplication by elements in some field. The addition and multiplication operations together give \"A\" the structure of a ring; the addition and scalar multiplication operations together give \"A\" the structure of a vector space over \"K\". In this article we will also use the term \"K\"-algebra\"' to mean an associative algebra over the field \"K\". A standard first example of a \"K\"-algebra is a ring of square matrices over a field \"K\", with the usual matrix multiplication. \n\nIn this article associative algebras are assumed to have a multiplicative unit, denoted 1; they are sometimes called unital associative algebras for clarification. In some areas of mathematics this assumption is not made, and we will call such structures non-unital associative algebras. We will also assume that all rings are unital, and all ring homomorphisms are unital.\n\nMany authors consider the more general concept of an associative algebra over a commutative ring \"R\", instead of a field: An \"R\"-algebra is an \"R\"-module with an associative \"R\"-bilinear binary operation, which also contains a multiplicative identity. For examples of this concept, if \"S\" is any ring with center \"C\", then \"S\" is an associative \"C\"-algebra.\n\nLet \"R\" be a fixed commutative ring (so \"R\" could be a field). An associative \"R\"-algebra (or more simply, an \"R\"-algebra) is an additive abelian group \"A\" which has the structure of both a ring and an \"R\"-module in such a way that the scalar multiplication satisfies\nfor all \"r\" ∈ \"R\" and \"x\", \"y\" ∈ \"A\". Furthermore, \"A\" is assumed to be unital, which is to say it contains an element 1 such that\nfor all \"x\" ∈ \"A\". Note that such an element 1 must be unique.\n\nIn other words, \"A\" is an \"R\"-module together with (1) an \"R\"-bilinear map \"A\" × \"A\" → \"A\", called the multiplication, and (2) the multiplicative identity, such that the multiplication is associative:\nfor all \"x\", \"y\", and \"z\" in \"A\". (Technical note: the multiplicative identity is a datum, while associativity is a property. By the uniqueness of the multiplicative identity, \"unitarity\" is often treated like a property.) If one drops the requirement for the associativity, then one obtains a non-associative algebra.\n\nIf \"A\" itself is commutative (as a ring) then it is called a commutative \"R\"-algebra.\n\nThe definition is equivalent to saying that a unital associative \"R\"-algebra is a monoid object in \"R\"-Mod (the monoidal category of \"R\"-modules). By definition, a ring is a monoid object in the category of abelian groups; thus, the notion of an associative algebra is obtained by replacing the category of abelian groups with the category of modules.\n\nPushing this idea further, some authors have introduced a \"generalized ring\" as a monoid object in some other category that behaves like the category of modules. Indeed, this reinterpretation allows one to avoid making an explicit reference to elements of an algebra \"A\". For example, the associativity can be expressed as follows. By the universal property of a tensor product of modules, the multiplication (the \"R\"-bilinear map) corresponds to a unique \"R\"-linear map\nThe associativity then refers to the identity:\n\nAn associative algebra amounts to a ring homomorphism whose image lies in the center. Indeed, starting with a ring \"A\" and a ring homomorphism formula_6 whose image lies in the center of \"A\", we can make \"A\" an \"R\"-algebra by defining\nfor all \"r\" ∈ \"R\" and \"x\" ∈ \"A\". If \"A\" is an \"R\"-algebra, taking \"x\" = 1, the same formula in turn defines a ring homomorphism formula_6 whose image lies in the center.\n\nIf \"A\" is commutative then the center of \"A\" is equal to \"A\", so that a commutative \"R\"-algebra can be defined simply as a homomorphism formula_6 of commutative rings.\n\nThe ring homomorphism η appearing in the above is often called a structure map. In the commutative case, one can consider the category whose objects are ring homomorphisms \"R\" → \"A\"; i.e., commutative \"R\"-algebras and whose morphisms are ring homomorphisms \"A\" → \"A\" that are under \"R\"; i.e., \"R\" → \"A\" → \"A\" is \"R\" → \"A\" (i.e., the coslice category of the category of commutative rings under \"R\".) The prime spectrum functor Spec then determines an anti-equivalence of this category to the category of affine schemes over Spec \"R\".\n\nHow to weaken the commutativity assumption is a subject matter of noncommutative algebraic geometry and, more recently, of derived algebraic geometry. See also: generic matrix ring.\n\nA homomorphism between two \"R\"-algebras is an \"R\"-linear ring homomorphism. Explicitly, formula_10 is an associative algebra homomorphism if\n\nThe class of all \"R\"-algebras together with algebra homomorphisms between them form a category, sometimes denoted \"R\"-Alg.\n\nThe subcategory of commutative \"R\"-algebras can be characterized as the coslice category \"R\"/CRing where CRing is the category of commutative rings.\n\nThe most basic example is a ring itself; it is an algebra over its center or any subring lying in the center. In particular, any commutative ring is an algebra over any of its subrings. Other examples abound both from algebra and other fields of mathematics.\n\nAlgebra\n\nRepresentation theory\n\nAnalysis\n\nGeometry and combinatorics\n\n\nAn associative algebra over \"K\" is given by a \"K\"-vector space \"A\" endowed with a bilinear map \"A\"×\"A\"→\"A\" having 2 inputs (multiplicator and multiplicand) and one output (product), as well as a morphism \"K\"→\"A\" identifying the scalar multiples of the multiplicative identity. If the bilinear map \"A\"×\"A\"→\"A\" is reinterpreted as a linear map (i. e., morphism in the category of \"K\"-vector spaces) \"A\"⊗\"A\"→\"A\" (by the universal property of the tensor product), then we can view an associative algebra over \"K\" as a \"K\"-vector space \"A\" endowed with two morphisms (one of the form \"A\"⊗\"A\"→\"A\" and one of the form \"K\"→\"A\") satisfying certain conditions which boil down to the algebra axioms. These two morphisms can be dualized using categorial duality by reversing all arrows in the commutative diagrams which describe the algebra axioms; this defines the structure of a coalgebra.\n\nThere is also an abstract notion of F-coalgebra, where \"F\" is a functor. This is vaguely related to the notion of coalgebra discussed above.\n\nA representation of an algebra \"A\" is an algebra homomorphism ρ: \"A\" → End(\"V\") from \"A\" to the endomorphism algebra of some vector space (or module) \"V\". The property of ρ being an algebra homomorphism means that ρ preserves the multiplicative operation (that is, ρ(\"xy\")=ρ(\"x\")ρ(\"y\") for all \"x\" and \"y\" in \"A\"), and that ρ sends the unity of \"A\" to the unity of End(\"V\") (that is, to the identity endomorphism of \"V\").\n\nIf \"A\" and \"B\" are two algebras, and ρ: \"A\" → End(\"V\") and τ: \"B\" → End(\"W\") are two representations, then there is a (canonical) representation \"A formula_24 B\" → End(\"V formula_24 W\") of the tensor product algebra \"A formula_24 B\" on the vector space \"V formula_24 W\". However, there is no natural way of defining a tensor product of two representations of a single associative algebra in such a way that the result is still a representation of that same algebra (not of its tensor product with itself), without somehow imposing additional conditions. Here, by \"tensor product of representations\", the usual meaning is intended: the result should be a linear representation of the same algebra on the product vector space. Imposing such additional structure typically leads to the idea of a Hopf algebra or a Lie algebra, as demonstrated below.\n\nConsider, for example, two representations formula_28 and formula_29. One might try to form a tensor product representation formula_30 according to how it acts on the product vector space, so that\n\nHowever, such a map would not be linear, since one would have\n\nfor \"k\" ∈ \"K\". One can rescue this attempt and restore linearity by imposing additional structure, by defining an algebra homomorphism Δ: \"A\" → \"A\" ⊗ \"A\", and defining the tensor product representation as\n\nSuch a homomorphism Δ is called a comultiplication if it satisfies certain axioms. The resulting structure is called a bialgebra. To be consistent with the definitions of the associative algebra, the coalgebra must be co-associative, and, if the algebra is unital, then the co-algebra must be co-unital as well. A Hopf algebra is a bialgebra with an additional piece of structure (the so-called antipode), which allows not only to define the tensor product of two representations, but also the Hom module of two representations (again, similarly to how it is done in the representation theory of groups).\n\nOne can try to be more clever in defining a tensor product. Consider, for example,\n\nso that the action on the tensor product space is given by\n\nThis map is clearly linear in \"x\", and so it does not have the problem of the earlier definition. However, it fails to preserve multiplication:\n\nBut, in general, this does not equal\n\nThis shows that this definition of a tensor product is too naive; the obvious fix is to define it such that it is antisymmetric, so that the middle two terms cancel. This leads to the concept of a Lie algebra.\n\nSome authors use the term \"associative algebra\" to refer to structures which do not necessarily have a multiplicative identity, and hence consider homomorphisms which are not necessarily unital. \n\nAn example of a non-unital associative algebra is given by the set of all functions \"f\": R → R whose limit as \"x\" nears infinity is zero.\n\n\n"}
{"id": "20067629", "url": "https://en.wikipedia.org/wiki?curid=20067629", "title": "Bridging model", "text": "Bridging model\n\nIn computer science, a bridging model is an abstract model of a computer which provides a conceptual bridge between the physical implementation of the machine and the abstraction available to a programmer of that machine; in other words, it is intended to provide a common level of understanding between hardware and software engineers.\n\nA successful bridging model is one which can be efficiently implemented in reality and efficiently targeted by programmers; in particular, it should be possible for a compiler to produce good code from a typical high-level language. The term was introduced by Leslie Valiant's 1990 paper \"A Bridging Model for Parallel Computation\", which argued that the strength of the von Neumann model was largely responsible for the success of computing as a whole. The paper goes on to develop the bulk synchronous parallel model as an analogous model for parallel computing.\n"}
{"id": "13297554", "url": "https://en.wikipedia.org/wiki?curid=13297554", "title": "Bruce Irons (engineer)", "text": "Bruce Irons (engineer)\n\nBruce Irons (1924 – 5 December 1983) was an engineer and mathematician, known for his fundamental contribution to the finite element method, including the patch test, the frontal solver and, along with Ian C. Taig, the isoparametric element concept.\n\nHe developed multiple sclerosis; finding it difficult to accept anticipated relapses, he committed suicide on 5 December 1983, and his wife followed suit.\n\n"}
{"id": "43192161", "url": "https://en.wikipedia.org/wiki?curid=43192161", "title": "Cohen–Hewitt factorization theorem", "text": "Cohen–Hewitt factorization theorem\n\nIn mathematics, the Cohen–Hewitt factorization theorem states that if formula_1 is a left module over a Banach algebra formula_2 with a left approximate unit formula_3, then an element formula_4 of formula_1 can be factorized as a product formula_6 (for some formula_7 and formula_8) whenever formula_9. The theorem was introduced by and .\n\n"}
{"id": "2047150", "url": "https://en.wikipedia.org/wiki?curid=2047150", "title": "Coincidence point", "text": "Coincidence point\n\nIn mathematics, a coincidence point (or simply coincidence) of two mappings is a point in their common domain having the same image.\n\nFormally, given two mappings\nwe say that a point \"x\" in \"X\" is a \"coincidence point\" of \"f\" and \"g\" if \"f\"(\"x\") = \"g\"(\"x\").\n\nCoincidence theory (the study of coincidence points) is, in most settings, a generalization of fixed point theory, the study of points \"x\" with \"f\"(\"x\") = \"x\". Fixed point theory is the special case obtained from the above by letting \"X = Y\" and taking \"g\" to be the identity mapping.\n\nJust as fixed point theory has its fixed-point theorems, there are theorems that guarantee the existence of coincidence points for pairs of mappings. Notable among them, in the setting of manifolds, is the Lefschetz coincidence theorem, which is typically known only in its special case formulation for fixed points.\n\nCoincidence points, like fixed points, are today studied using many tools from mathematical analysis and topology. An equaliser is a generalization of the coincidence set.\n"}
{"id": "36650958", "url": "https://en.wikipedia.org/wiki?curid=36650958", "title": "Combining dimensions", "text": "Combining dimensions\n\nIn topology, a mathematical discipline, combining dimensions into a manifold consists of taking \"n\" dimensions and visualizing them in a smaller \"m\" dimension. Similar concepts apply when studying spacetime and Calabi–Yau manifolds. For example, in string theory, six dimensions are combined into a two dimensional manifold for simple visualization.\n"}
{"id": "7482029", "url": "https://en.wikipedia.org/wiki?curid=7482029", "title": "Completely distributive lattice", "text": "Completely distributive lattice\n\nIn the mathematical area of order theory, a completely distributive lattice is a complete lattice in which arbitrary joins distribute over arbitrary meets.\n\nFormally, a complete lattice \"L\" is said to be completely distributive if, for any doubly indexed family \nwhere \"F\" is the set of choice functions \"f\" choosing for each index \"j\" of \"J\" some index \"f\"(\"j\") in \"K\".\n\nComplete distributivity is a self-dual property, i.e. dualizing the above statement yields the same class of complete lattices.\n\nWithout the axiom of choice, no complete lattice with more than one element can ever satisfy the above property, as one can just let \"x\" equal the top element of \"L\" for all indices \"j\" and \"k\" with all of the sets \"K\" being nonempty but having no choice function.\n\nVarious different characterizations exist. For example, the following is an equivalent law that avoids the use of choice functions. For any set \"S\" of sets, we define the set \"S\" to be the set of all subsets \"X\" of the complete lattice that have non-empty intersection with all members of \"S\". We then can define complete distributivity via the statement\n\nThe operator ( ) might be called the crosscut operator. This version of complete distributivity only implies the original notion when admitting the Axiom of Choice.\n\nIn addition, it is known that the following statements are equivalent for any complete lattice \"L\":\n\n\nDirect products of [0,1], i.e. sets of all functions from some set \"X\" to [0,1] ordered pointwise, are also called \"cubes\".\n\nEvery poset \"C\" can be completed in a completely distributive lattice.\n\nA completely distributive lattice \"L\" is called the free completely distributive lattice over a poset \"C\" if and only if there is an order embedding formula_3 such that for every completely distributive lattice \"M\" and monotonic function formula_4, there is a unique complete homomorphism formula_5 satisfying formula_6. For every poset \"C\", the free completely distributive lattice over a poset \"C\" exists and is unique up to isomorphism.\n\nThis is an instance of the concept of free object. Since a set \"X\" can be considered as a poset with the discrete order, the above result guarantees the existence of the free completely distributive lattice over the set \"X\".\n\n\n"}
{"id": "1001490", "url": "https://en.wikipedia.org/wiki?curid=1001490", "title": "Convex conjugate", "text": "Convex conjugate\n\nIn mathematics and mathematical optimization, the convex conjugate of a function is a generalization of the Legendre transformation which applies to non-convex functions. It is also known as Legendre–Fenchel transformation or Fenchel transformation (after Adrien-Marie Legendre and Werner Fenchel). It is used to transform an optimization problem into its corresponding dual problem, which can often be simpler to solve.\n\nLet formula_1 be a real topological vector space, and let formula_2 be the dual space to formula_1. Denote the dual pairing by\n\nFor a function\n\ntaking values on the extended real number line, the convex conjugate\n\nis defined in terms of the supremum by\n\nor, equivalently, in terms of the infimum by\n\nThis definition can be interpreted as an encoding of the convex hull of the function's epigraph in terms of its supporting hyperplanes.\nThe convex conjugate of an affine function\nis\n\nThe convex conjugate of a power function\nwhere formula_12\n\nThe convex conjugate of the absolute value function\nis\n\nThe convex conjugate of the exponential function formula_15 is\nConvex conjugate and Legendre transform of the exponential function agree except that the domain of the convex conjugate is strictly larger as the Legendre transform is only defined for positive real numbers.\n\nLet \"F\" denote a cumulative distribution function of a random variable \"X\". Then (integrating by parts),\n\nhas the convex conjugate\n\nA particular interpretation has the transform\n\nas this is a nondecreasing rearrangement of the initial function f; in particular, formula_20 for \"ƒ\" nondecreasing.\n\nThe convex conjugate of a closed convex function is again a closed convex function. The convex conjugate of a polyhedral convex function (a convex function with polyhedral epigraph) is again a polyhedral convex function.\n\nConvex-conjugation is order-reversing: if formula_21 then formula_22. Here\n\nFor a family of functions formula_24 it follows from the fact that supremums may be interchanged that\n\nand from the max–min inequality that \n\nThe convex conjugate of a function is always lower semi-continuous. The biconjugate formula_27 (the convex conjugate of the convex conjugate) is also the closed convex hull, i.e. the largest lower semi-continuous convex function with formula_28. \nFor proper functions \"f\", \n\nFor any function and its convex conjugate , Fenchel's inequality (also known as the Fenchel–Young inequality) holds for every and :\n\nThe proof follows immediately from the definition of convex conjugate: formula_31.\n\nFor two functions formula_32 and formula_33 and a number formula_34 the convexity relation \nholds. The formula_36 operation is a convex mapping itself.\n\nThe infimal convolution (or epi-sum) of two functions \"f\" and \"g\" is defined as\n\nLet \"f\", …, \"f\" be proper, convex and lsc functions on R. Then the infimal convolution is convex and lsc (but not necessarily proper), and satisfies\n\nThe infimal convolution of two functions has a geometric interpretation: The (strict) epigraph of the infimal convolution of two functions is the Minkowski sum of the (strict) epigraphs of those functions.\n\nIf the function formula_39 is differentiable, then its derivative is the maximizing argument in the computation of the convex conjugate:\n"}
{"id": "2580999", "url": "https://en.wikipedia.org/wiki?curid=2580999", "title": "Criterion validity", "text": "Criterion validity\n\nIn psychometrics, criterion or concrete validity is the extent to which a measure is related to an outcome. Criterion validity is often divided into concurrent and predictive validity. Concurrent validity refers to a comparison between the measure in question and an outcome assessed at the same time. In \"Standards for Educational & Psychological Tests\", it states, \"concurrent validity reflects only the status quo at a particular time.\" Predictive validity, on the other hand, compares the measure in question with an outcome assessed at a later time. Although concurrent and predictive validity are similar, it is cautioned to keep the terms and findings separated. \"Concurrent validity should not be used as a substitute for predictive validity without an appropriate supporting rationale.\"\n\nAn example of concurrent validity is a comparison of the scores of the CLEP College Algebra exam with course grades in college algebra to determine the degree to which scores on the CLEP are related to performance in a college algebra class. An example of predictive validity is a comparison of scores on the SAT with first semester grade point average (GPA) in college; this assesses the degree to which SAT scores are predictive of college performance.\n\n\n"}
{"id": "295411", "url": "https://en.wikipedia.org/wiki?curid=295411", "title": "Cross-cap", "text": "Cross-cap\n\nIn mathematics, a cross-cap is a two-dimensional surface in 3-space that is one-sided and the continuous image of a Möbius strip that intersects itself in an interval. In the domain, the inverse image of this interval is a longer interval that the mapping into 3-space \"folds in half\". At the point where the longer interval is folded in half in the image, the nearby configuration is that of the Whitney umbrella. \n\nThe interval of self intersection precludes the cross-cap from being homeomorphic to the Möbius strip, but there are only two points in the image (the endpoints of the interval of self-intersection) where the image cannot be that of an immersion. The bounding edge of a cross-cap is a simple closed loop. Like certain versions of the Möbius strip, it may take the form of a symmetrical circle.\n\nA cross-cap that has been closed up by gluing a disc to its boundary is a model of the real projective plane P (again with an interval of self-intersection, and two points where this model is not an immersion of P).\n\nTwo cross-caps glued together at their boundaries form a model of the Klein bottle, this time with two intervals of self-intersection and four points where this model is not an immersion.\n\nAn important theorem of topology, the classification theorem for surfaces, states that each two-dimensional compact manifold without boundary is homeomorphic to a sphere with some number (possibly 0) of \"handles\" and 0, 1, or 2 cross-caps.\n\n"}
{"id": "26064582", "url": "https://en.wikipedia.org/wiki?curid=26064582", "title": "DYNAMO (programming language)", "text": "DYNAMO (programming language)\n\nDYNAMO (DYNAmic MOdels) is an historically important simulation language and accompanying graphical notation developed within the system dynamics analytical framework. It was originally for industrial dynamics but was soon extended to other applications, including population and resource studies\nand urban planning.\n\nDYNAMO was initially developed under the direction of Jay Wright Forrester in the late 1950s, by Dr. Phyllis Fox,\nAlexander L. Pugh III, Grace Duren,\nand others\nat the M.I.T. Computation Center.\n\nDYNAMO was used for the system dynamics simulations of global resource-depletion reported in the Club of Rome's Limits to Growth, but has since fallen into disuse.\n\nIn 1958, Forrester unwittingly instigated DYNAMO's development when he asked an MIT staff programmer to compute needed solutions to some equations, for a Harvard Business Review paper he was writing about industrial dynamics.\nThe programmer, Richard Bennett, chose to implement a system (SIMPLE - \"Simulation of Industrial Management Problems with Lots of Equations\") that took coded equations as symbolic input and computed solutions. SIMPLE became the proof-of-concept for DYNAMO: rather than have a specialist programmer \"hard-code\" a special-purpose solver in a general purpose programming language, users could specify a system's equations in a special simulation language and get simulation output from one program execution.\n\nDYNAMO was designed to emphasize the following:\n\n\nAmong the ways in which DYNAMO was above the standard of the time, it featured units checking of numerical types and relatively clear error messages.\n\nThe earliest versions were written in assembly language for the IBM 704, then for the IBM 709 and IBM 7090. DYNAMO II was written in AED-0, an extended version of Algol 60.\nDynamo II/F, in 1971, generated portable FORTRAN code\nand both Dynamo II/F and Dynamo III improved the system's portability by being written in FORTRAN.\n\nOriginally designed for batch processing on mainframe computers, it was made available on minicomputers in the late 1970s,\nand became available as \"micro-Dynamo\" on personal computers in the early 1980s.\nThe language went through several revisions from DYNAMO II up to DYNAMO IV in 1983,\n\nApart from its (indirectly felt) public impact in environmental issues raised by the controversy over \"Limits to Growth\", DYNAMO was influential in the history of discrete-event simulation even though it was essentially a package for continuous simulation specified through difference equations. It has been said by some to have opened opportunities for computer modelling even for users of relatively low mathematical sophistication. On the other hand, it has also been criticized as weak precisely where mathematical sophistication should be required and for relying only on Euler integration.\n\n\n"}
{"id": "27123668", "url": "https://en.wikipedia.org/wiki?curid=27123668", "title": "Digital manifold", "text": "Digital manifold\n\nIn mathematics, a digital manifold is a special kind of combinatorial manifold which is defined in digital space i.e. grid cell space. A combinatorial manifold is a kind of manifold which is a discretization of a manifold. It usually means a piecewise linear manifold made by simplicial complexes.\n\nParallel-move is used to extend an i-cell to (i+1)-cell. In other words, if A and B are two i-cells \nand A is a parallel-move of B, then {A,B} is an (i+1)-cell.\nTherefore, k-cells can be defined recursively.\n\nBasically, a connected set of grid points M can be viewed as a digital k-manifold if: \n(1) any two k-cells are (k-1)-connected, (2) every (k-1)-cell has\nonly one or two parallel-moves, and (3) M does not contain any (k+1)-cells.\n\n"}
{"id": "42083183", "url": "https://en.wikipedia.org/wiki?curid=42083183", "title": "Edmund Schuster", "text": "Edmund Schuster\n\nEdmund Schuster (7 September 1851 – 5 July 1932) was a German engineer and mathematician who contributed to the field of special functions and complex analysis being a pioneer in the field of harmonic analysis.\n\nSchuster was born in Munich in 1851. He studied in Bonn and Leipzig, and in 1875 he moved with his family to Chile. After a short period in Santiago de Chile he moved to Valdivia, in the south of the country, where he lived until his death, in 1932. In Valdivia, Schuster initiated a school of mathematics and founded the Acta Philosophica Valdiviana, in 1876.\n\nSchuster is known in the field of harmonic analysis for the function that receives his name, whose analytic structure exhibits interesting properties. In particular, its roots, usually denoted formula_1, are given by\n\nwhere formula_3 represents a real number that labels the order of the Schuster function. The zeros of the Schuster function can also be written in terms of a complex variable formula_4, namely\n\nwhere formula_6 is the complex conjugate of formula_7. This function is used also in physics.\n\nSchuster has also made contributions to civil engineering.\n\n"}
{"id": "4652353", "url": "https://en.wikipedia.org/wiki?curid=4652353", "title": "Ephemeral key", "text": "Ephemeral key\n\nA cryptographic key is called ephemeral if it is generated for each execution of a key establishment process. In some cases ephemeral keys are used more than once, within a single session (e.g., in broadcast applications) where the sender generates only one ephemeral key pair per message and the private key is combined separately with each recipient's public key. Contrast with a static key.\n\nPrivate (resp. public) ephemeral key agreement keys are the private (resp. public) keys of asymmetric key pairs that are used a single key establishment transaction to establish one or more keys (e.g., key wrapping keys, data encryption keys, or MAC keys) and, optionally, other keying material (e.g., initialization vectors).\n\n"}
{"id": "56167324", "url": "https://en.wikipedia.org/wiki?curid=56167324", "title": "Eugene P. Northrop", "text": "Eugene P. Northrop\n\nEugene P. Northrop (1908–1969) was an American research mathematician and a math popularizer. He held the William Rainey Harper Chair of Mathematics at the University of Chicago, and frequently served in administrative roles and on technical commissions. He is most remembered for his 1944 book \"Riddles in Mathematics\", which was well-received by the mathematical community and remains in print as a Dover book.\n"}
{"id": "27883420", "url": "https://en.wikipedia.org/wiki?curid=27883420", "title": "F(g) Scholar", "text": "F(g) Scholar\n\nf(g) Scholar also known as fg Scholar was a graphing, spreadsheet, and calculator software package published by Future Graph, Inc.. The software received a number of awards as well as extensive press coverage. Although originally targeted towards technical academia in the fields of math, science, and engineering primarily college students and teachers, the software did gain acceptance in the business world.\n\nfg Scholar featured a calculator, its own programming language with macro support, the ability to import graphics, an automated formula builder, math templates, a spreadsheet with graphing capability, a full featured vector drawing module, and the ability to export files in a number of formats.\n\nf(g) Scholar provided 12 types of charts including: bar, pie, area, three-dimensional area.\n\nf(g) Scholar supported the following graphic types: .WPG, .WMF, .CLP, .CGM, and .PIC.\n\n\n\n"}
{"id": "583532", "url": "https://en.wikipedia.org/wiki?curid=583532", "title": "Function type", "text": "Function type\n\nIn computer science, a function type (or arrow type or exponential) is the type of a variable or parameter to which a function has or can be assigned, or an argument or result type of a higher-order function taking or returning a function.\n\nA function type depends on the type of the parameters and the result type of the function (it, or more accurately the unapplied type constructor codice_1, is a higher-kinded type). In theoretical settings and programming languages where functions are defined in curried form, such as the simply typed lambda calculus, a function type depends on exactly two types, the domain \"A\" and the range \"B\". Here a function type is often denoted , following mathematical convention, or , based on there existing exactly (exponentially many) set-theoretic functions mappings \"A\" to \"B\" in the category of sets. The class of such maps or functions is called the exponential object. The act of currying makes the function type adjoint to the product type; this is explored in detail in the article on currying.\n\nThe function type can be considered to be a special case of the dependent product type, which among other properties, encompasses the idea of a polymorphic function.\n\nThe syntax used for function types in several programming languages can be summarized, including an example type signature for the higher-order function composition function:\nWhen looking at the example type signature of, for example C#, the type of the function is actually codice_2.\n\nDue to type erasure in C++11's codice_3, it is more common to use templates for higher order function parameters and type inference (codice_4) for closures.\n\nThe function type in programming languages does not correspond to the space of all set-theoretic functions. Given the countably infinite type of natural numbers as the domain and the booleans as range, then there are an uncountably infinite number (2 = c) of set-theoretic functions between them. Clearly this space of functions is larger than the number of functions that can be defined in any programming language, as there exist only countably many programs (a program being a finite sequence of a finite number of symbols) and one of the set-theoretic functions effectively solves the halting problem.\n\nDenotational semantics concerns itself with finding more appropriate models (called domains) to model programming language concepts such as function types. It turns out that restricting expression to the set of computable functions is not sufficient either if the programming language allows writing non-terminating computations (which is the case if the programming language is Turing complete). Expression must be restricted to the so-called \"continuous functions\" (corresponding to continuity in the Scott topology, not continuity in the real analytical sense). Even then, the set of continuous function contains the \"parallel-or\" function, which cannot be correctly defined in all programming languages.\n\n\n"}
{"id": "12695", "url": "https://en.wikipedia.org/wiki?curid=12695", "title": "Group representation", "text": "Group representation\n\nIn the mathematical field of representation theory, group representations describe abstract groups in terms of linear transformations of vector spaces; in particular, they can be used to represent group elements as matrices so that the group operation can be represented by matrix multiplication. Representations of groups are important because they allow many group-theoretic problems to be reduced to problems in linear algebra, which is well understood. They are also important in physics because, for example, they describe how the symmetry group of a physical system affects the solutions of equations describing that system.\n\nThe term \"representation of a group\" is also used in a more general sense to mean any \"description\" of a group as a group of transformations of some mathematical object. More formally, a \"representation\" means a homomorphism from the group to the automorphism group of an object. If the object is a vector space we have a \"linear representation\". Some people use \"realization\" for the general notion and reserve the term \"representation\" for the special case of linear representations. The bulk of this article describes linear representation theory; see the last section for generalizations.\n\nThe representation theory of groups divides into subtheories depending on the kind of group being represented. The various theories are quite different in detail, though some basic definitions and concepts are similar. The most important divisions are:\n\n\nRepresentation theory also depends heavily on the type of vector space on which the group acts. One distinguishes between finite-dimensional representations and infinite-dimensional ones. In the infinite-dimensional case, additional structures are important (e.g. whether or not the space is a Hilbert space, Banach space, etc.).\n\nOne must also consider the type of field over which the vector space is defined. The most important case is the field of complex numbers. The other important cases are the field of real numbers, finite fields, and fields of p-adic numbers. In general, algebraically closed fields are easier to handle than non-algebraically closed ones. The characteristic of the field is also significant; many theorems for finite groups depend on the characteristic of the field not dividing the order of the group.\n\nA representation of a group \"G\" on a vector space \"V\" over a field \"K\" is a group homomorphism from \"G\" to GL(\"V\"), the general linear group on \"V\". That is, a representation is a map \nsuch that\n\nHere \"V\" is called the representation space and the dimension of \"V\" is called the dimension of the representation. It is common practice to refer to \"V\" itself as the representation when the homomorphism is clear from the context.\n\nIn the case where \"V\" is of finite dimension \"n\" it is common to choose a basis for \"V\" and identify GL(\"V\") with , the group of \"n\"-by-\"n\" invertible matrices on the field \"K\".\n\n\n\nConsider the complex number \"u\" = e which has the property \"u\" = 1. The cyclic group \"C\" = {1, \"u\", \"u\"} has a representation ρ on C given by:\n\nThis representation is faithful because ρ is a one-to-one map.\n\nAnother representation for \"C\" on C, isomorphic to the previous one, is σ given by:\n\nThe group \"C\" may also be faithfully represented on R by τ given by:\n\nwhere\n\nA subspace \"W\" of \"V\" that is invariant under the group action is called a \"subrepresentation\". If \"V\" has exactly two subrepresentations, namely the zero-dimensional subspace and \"V\" itself, then the representation is said to be \"irreducible\"; if it has a proper subrepresentation of nonzero dimension, the representation is said to be \"reducible\". The representation of dimension zero is considered to be neither reducible nor irreducible, just like the number 1 is considered to be neither composite nor prime.\n\nUnder the assumption that the characteristic of the field \"K\" does not divide the size of the group, representations of finite groups can be decomposed into a direct sum of irreducible subrepresentations (see Maschke's theorem). This holds in particular for any representation of a finite group over the complex numbers, since the characteristic of the complex numbers is zero, which never divides the size of a group.\n\nIn the example above, the first two representations given (ρ and σ) are both decomposable into two 1-dimensional subrepresentations (given by span{(1,0)} and span{(0,1)}), while the third representation (τ) is irreducible.\n\nA \"set-theoretic representation\" (also known as a group action or \"permutation representation\") of a group \"G\" on a set \"X\" is given by a function ρ : \"G\" → \"X\", the set of functions from \"X\" to \"X\", such that for all \"g\", \"g\" in \"G\" and all \"x\" in \"X\":\n\nThis condition and the axioms for a group imply that ρ(\"g\") is a bijection (or permutation) for all \"g\" in \"G\". Thus we may equivalently define a permutation representation to be a group homomorphism from G to the symmetric group S of \"X\".\n\nFor more information on this topic see the article on group action.\n\nEvery group \"G\" can be viewed as a category with a single object; morphisms in this category are just the elements of \"G\". Given an arbitrary category \"C\", a \"representation\" of \"G\" in \"C\" is a functor from \"G\" to \"C\". Such a functor selects an object \"X\" in \"C\" and a group homomorphism from \"G\" to Aut(\"X\"), the automorphism group of \"X\".\n\nIn the case where \"C\" is Vect, the category of vector spaces over a field \"K\", this definition is equivalent to a linear representation. Likewise, a set-theoretic representation is just a representation of \"G\" in the category of sets.\n\nWhen \"C\" is Ab, the category of abelian groups, the objects obtained are called \"G\"-modules.\n\nFor another example consider the category of topological spaces, Top. Representations in Top are homomorphisms from \"G\" to the homeomorphism group of a topological space \"X\".\n\nTwo types of representations closely related to linear representations are:\n\n\n"}
{"id": "357328", "url": "https://en.wikipedia.org/wiki?curid=357328", "title": "Gröbner basis", "text": "Gröbner basis\n\nIn mathematics, and more specifically in computer algebra, computational algebraic geometry, and computational commutative algebra, a Gröbner basis is a particular kind of generating set of an ideal in a polynomial ring over a field . A Gröbner basis allows many important properties of the ideal and the associated algebraic variety to be deduced easily, such as the dimension and the number of zeros when it is finite. Gröbner basis computation is one of the main practical tools for solving systems of polynomial equations and computing the images of algebraic varieties under projections or rational maps.\n\nGröbner basis computation can be seen as a multivariate, non-linear generalization of both Euclid's algorithm for computing polynomial greatest common divisors, and\nGaussian elimination for linear systems.\n\nGröbner bases were introduced in 1965, together with an algorithm to compute them (Buchberger's algorithm), by Bruno Buchberger in his Ph.D. thesis. He named them after his advisor Wolfgang Gröbner. In 2007, Buchberger received the Association for Computing Machinery's Paris Kanellakis Theory and Practice Award for this work.\nHowever, the Russian mathematician Nikolai Günther had introduced a similar notion in 1913, published in various Russian mathematical journals. These papers were largely ignored by the mathematical community until their rediscovery in 1987 by Bodo Renschuch \"et al.\" An analogous concept for multivariate power series was developed independently by Heisuke Hironaka in 1964, who named them standard bases. This term has been used by some authors to also denote Gröbner bases.\n\nThe theory of Gröbner bases has been extended by many authors in various directions. It has been generalized to other structures such as polynomials over principal ideal rings or polynomial rings, and also some classes of non-commutative rings and algebras, like Ore algebras.\n\nGröbner bases are primarily defined for ideals in a polynomial ring formula_1 over a field . Although the theory works for any field, most Gröbner basis computations are done either when is the field of rationals or the integers modulo a prime number.\n\nA polynomial is a sum formula_2 where the formula_3 are nonzero elements of and the formula_4 are monomials or \"power products\" of the variables. This means that a monomial is a product formula_5 where the formula_6 are nonnegative integers. The vector formula_7 is called the exponent vector of . The notation is often abbreviated as formula_8. Monomials are uniquely defined by their exponent vectors so computers can represent monomials efficiently as exponent vectors, and polynomials as lists of exponent vectors and their coefficients.\n\nIf formula_9 is a finite set of polynomials in a polynomial ring , the ideal generated by is the set of linear combinations of elements from with coefficients in all of :\n\nAll operations related to Gröbner bases require the choice of a total order on the monomials, with the following properties of compatibility with multiplication. For all monomials , , ,\nA total order satisfying these condition is sometimes called an \"admissible ordering\".\n\nThese conditions imply Noetherianity, which means that every strictly decreasing sequence of monomials is finite.\n\nAlthough Gröbner basis theory does not depend on a particular choice of an admissible monomial ordering, three monomial orderings are specially important for the applications:\n\nGröbner basis theory was initially introduced for the lexicographical ordering. It was soon realised that the Gröbner basis for degrevlex is almost always much easier to compute, and that it is almost always easier to compute a lex Gröbner basis by first computing the degrevlex basis and then using a \"change of ordering algorithm\". When elimination is needed, degrevlex is not convenient; both lex and lexdeg may be used but, again, many computations are relatively easy with lexdeg and almost impossible with lex.\n\nOnce a monomial ordering is fixed, the \"terms\" of a polynomial (product of a monomial with its nonzero coefficient) are naturally ordered by decreasing monomials (for this order). This makes the representation of a polynomial as an ordered list of pairs coefficient–exponent vector a canonical representation of the polynomials. The first (greatest) term of a polynomial for this ordering and the corresponding monomial and coefficient are respectively called the \"leading term\", \"leading monomial\" and \"leading coefficient\" and denoted, in this article, lt(), lm() and lc().\n\nThe concept of reduction, also called multivariate division or normal form computation, is central to Gröbner basis theory. It is a multivariate generalization of the Euclidean division of univariate polynomials.\n\nIn this section we suppose a fixed monomial ordering, which will not be defined explicitly.\n\nGiven two polynomials \"f\" and \"g\", one says that \"f\" is \"reducible\" by \"g\" if some monomial \"m\" in \"f\" is a multiple of the leading monomial lm(\"g\") of \"g\". If \"m\" happens to be the leading monomial of \"f\" then one says that \"f\" is \"lead-reducible\" by \"g\". If \"c\" is the coefficient of \"m\" in \"f\" and \"m\" = \"q\" lm(\"g\"), the \"one-step reduction\" of \"f\" by \"g\" is the operation that associates to \"f\" the polynomial\nThe main properties of this operation are that the resulting polynomial does not contain the monomial \"m\" and that the monomials greater than \"m\" (for the monomial ordering) remain unchanged. This operation is not, in general, uniquely defined; if several monomials in \"f\" are multiples of lm(\"g\"), then one may choose arbitrarily which one to reduce. In practice, it is better to choose the greatest one for the monomial ordering, because otherwise subsequent reductions could reintroduce the monomial that has just been removed.\n\nGiven a finite set \"G\" of polynomials, one says that \"f\" is \"reducible\" or \"lead-reducible\" by \"G\" if it is reducible or lead-reducible, respectively, by an element g of \"G\". If it is the case, then one defines formula_14. The (complete) reduction of \"f\" by \"G\" consists in applying iteratively this operator formula_15 until getting a polynomial formula_16, which is irreducible by \"G\". It is called a \"normal form\" of \"f\" by \"G\". In general this form is not uniquely defined (this is not a canonical form); this non-uniqueness is the starting point of Gröbner basis theory.\n\nFor Gröbner basis computations, except at the end, it is not necessary to do a complete reduction: a \"lead-reduction\" is sufficient, which saves a large amount of computation.\n\nThe definition of the reduction shows immediately that, if \"h\" is a normal form of \"f\" by \"G\", then we have\nwhere the formula_18 are polynomials. In the case of univariate polynomials, if \"G\" consists of a single element \"g\", then \"h\" is the remainder of the Euclidean division of \"f\" by \"g\", \"q\" is the quotient and the division algorithm is exactly the process of lead-reduction. For this reason, some authors use the term \"multivariate division\" instead of reduction.\n\nA Gröbner basis G of an ideal I in a polynomial ring R over a field is a generating set of I characterized by any one of the following properties, stated relatively to some monomial order:\n\n\nAll these properties are equivalent; different authors use different definitions depending on the topic they choose. The last two properties allow calculations in the factor ring R/I with the same facility as modular arithmetic. It is a significant fact of commutative algebra that Gröbner bases always exist, and can be effectively computed for any ideal given by a finite generating subset.\n\nMultivariate division requires a monomial ordering, the basis depends on the monomial ordering chosen, and different orderings can give rise to radically different Gröbner bases. Two of the most commonly used orderings are lexicographic ordering, and \"degree reverse lexicographic order\" (also called \"graded reverse lexicographic order\" or simply \"total degree order\"). Lexicographic order eliminates variables, however the resulting Gröbner bases are often very large and expensive to compute. Degree reverse lexicographic order typically provides for the fastest Gröbner basis computations. In this order monomials are compared first by total degree, with ties broken by taking the smallest monomial with respect to lexicographic ordering with the variables reversed.\n\nIn most cases (polynomials in finitely many variables with complex coefficients or, more generally, coefficients over any field, for example), Gröbner bases exist for any monomial ordering. Buchberger's algorithm is the oldest and most well-known method for computing them. Other methods are the Faugère's F4 and F5 algorithms, based on the same mathematics as the Buchberger algorithm, and involutive approaches, based on ideas from differential algebra. There are also three algorithms for converting a Gröbner basis with respect to one monomial order to a Gröbner basis with respect to a different monomial order: the FGLM algorithm, the Hilbert Driven Algorithm and the Gröbner walk algorithm. These algorithms are often employed to compute (difficult) lexicographic Gröbner bases from (easier) total degree Gröbner bases.\n\nA Gröbner basis is termed \"reduced\" if the leading coefficient of each element of the basis is 1 and no monomial in any element of the basis is in the ideal generated by the leading terms of the other elements of the basis. In the worst case, computation of a Gröbner basis may require time that is exponential or even doubly exponential in the number of solutions of the polynomial system (for degree reverse lexicographic order and lexicographic order, respectively). Despite these complexity bounds, both standard and reduced Gröbner bases are often computable in practice, and most computer algebra systems contain routines to do so.\n\nThe concept and algorithms of Gröbner bases have been generalized to submodules of free modules over a polynomial ring. In fact, if \"L\" is a free module over a ring \"R\", then one may consider \"R\"⊕\"L\" as a ring by defining the product of two elements of \"L\" to be 0. This ring may be identified with formula_19 where formula_20 is a basis of \"L\". This allows to identify a submodule of \"L\" generated by formula_21 with the ideal of formula_22 generated by formula_21 and the products formula_24, formula_25. If \"R\" is a polynomial ring, this reduces the theory and the algorithms of Gröbner bases of modules to the theory and the algorithms of Gröbner bases of ideals.\n\nThe concept and algorithms of Gröbner bases have also been generalized to ideals over various rings, commutative or not, like polynomial rings over a principal ideal ring or Weyl algebras.\n\nLet \"R\" = Q[\"x\",\"y\"] be the ring of bivariate polynomials with rational coefficients and consider the ideal \"I\" = <\"f\",\"g\"> generated by the polynomials\n\nTwo other elements of \"I\" are the polynomials\n\nUnder lexicographic ordering with \"x\" > \"y\" we have\n\nThe ideal generated by {lt(\"f\"),lt(\"g\")} only contains polynomials that are divisible by \"x\" which\nexcludes lt(\"h\") = \"y\"; it follows that {\"f\", \"g\"} is not a Gröbner basis for \"I.\"\n\nOn the other hand, we can show that {\"f\", \"k\", \"h\"} is indeed a Gröbner basis for \"I.\"\n\nFirst note that \"f\" and \"g,\" and therefore also \"h, k\" and all the other polynomials in the ideal \"I\"\nhave the following three zeroes in the (\"x\",\"y\") plane in common, as indicated in the figure: {(1,1),(-1,1),(0,0)}.\nThose three points are not collinear, so \"I\" does not contain any polynomial of the first degree.\nNeither can \"I\" contain any polynomials of the special form\n\nwith \"c\" a nonzero rational number and \"p\" a polynomial in the variable \"y\" only; the reason being that\nsuch an \"m\" can never have two distinct zeroes with the same value for \"y\" (in this case,\nthe points (1,1) and (-1,1)).\n\nFrom the above it follows that \"I,\" apart from the zero polynomial, only contains polynomials whose leading term has degree greater than or equal to 2; therefore their leading terms are divisible by at least one of the three monomials\nThis means that {\"f\", \"k\", \"h\"} is a Gröbner basis for \"I\" with respect to lexicographic ordering with \"x\" > \"y.\"\n\nUnless explicitly stated, all the results that follow are true for any monomial ordering (see that article for the definitions of the different orders that are mentioned below).\n\nIt is a common misconception that the lexicographical order is needed for some of these results. On the contrary, the lexicographical order is, almost always, the most difficult to compute, and using it makes impractical many computations that are relatively easy with graded reverse lexicographic order (grevlex), or, when elimination is needed, the elimination order (lexdeg) which restricts to grevlex on each block of variables.\n\nReduced Gröbner bases are \"unique\" for any given ideal and any monomial ordering. Thus two ideals are equal if and only if they have the same (reduced) Gröbner basis (usually a Gröbner basis software always produces reduced Gröbner bases).\n\nThe reduction of a polynomial \"f\" by the Gröbner basis \"G\" of an ideal \"I\" yields 0 \"if and only if\" \"f\" is in \"I\". This allows to test the membership of an element in an ideal. Another method consists in verifying that the Gröbner basis of \"G\"∪{\"f\"} is equal to \"G\".\n\nTo test if the ideal \"I\" generated by \"f\", ...,\"f\" is contained in the ideal \"J\", it suffices to test that every \"f\" is in \"J\". One may also test the equality of the reduced Gröbner bases of \"J\" and \"J\"∪{\"f\", ...,\"f\"}.\n\nAny set of polynomials may be viewed as a system of polynomial equations by equating the polynomials to zero. The set of the solutions of such a system depends only on the generated ideal, and, therefore does not change when the given generating set is replaced by the Gröbner basis, for any ordering, of the generated ideal. Such a solution, with coordinates in an algebraically closed field containing the coefficients of the polynomials, is called a \"zero of the ideal\". In the usual case of rational coefficients, this algebraically closed field is chosen as the complex field.\n\nAn ideal does not have any zero (the system of equations is inconsistent) if and only if 1 belongs to the ideal (this is Hilbert's Nullstellensatz), or, equivalently, if its Gröbner basis (for any monomial ordering) contains 1, or, also, if the corresponding reduced Gröbner basis is [1].\n\nGiven the Gröbner basis \"G\" of an ideal \"I\", it has only a finite number of zeros, if and only if, for each variable \"x\", \"G\" contains a polynomial with a leading monomial that is a power of \"x\" (without any other variable appearing in the leading term). If it is the case the number of zeros, counted with multiplicity, is equal to the number of monomials that are not multiple of any leading monomial of \"G\". This number is called the \"degree\" of the ideal.\n\nWhen the number of zeros is finite, the Gröbner basis for a lexicographical monomial ordering provides, theoretically, a solution: the first coordinates of a solution is a root of the greatest common divisor of polynomials of the basis that depends only of the first variable. After substituting this root in the basis, the second coordinates of this solution is a root of the greatest common divisor of the resulting polynomials that depends only on this second variable, and so on. This solving process is only theoretical, because it implies GCD computation and root-finding of polynomials with approximate coefficients, which are not practicable because of numeric instability. Therefore, other methods have been developed to solve polynomial systems through Gröbner bases (see System of polynomial equations for more details).\n\nThe dimension of an ideal \"I\" in a polynomial ring \"R\" is the Krull dimension of the ring \"R\"/\"I\" and is equal to the dimension of the algebraic set of the zeros of \"I\". It is also equal to number of hyperplanes in general position which are needed to have an intersection with the algebraic set, which is a finite number of points. The degree of the ideal and of its associated algebraic set is the number of points of this finite intersection, counted with multiplicity. In particular, the degree of a hypersurface is equal to the degree of its definition polynomial.\n\nBoth degree and dimension depends only on the set of the leading monomials of the Gröbner basis of the ideal for any monomial ordering.\n\nThe dimension is the maximal size of a subset \"S\" of the variables such that there is no leading monomial depending only on the variables in \"S\". Thus, if the ideal has dimension 0, then for each variable \"x\" there is a leading monomial in the Gröbner basis that is a power of \"x\".\n\nBoth dimension and degree may be deduced from the Hilbert series of the ideal, which is the series formula_26, where formula_27 is the number of monomials of degree \"i\" that are not multiple of any leading monomial in the Gröbner basis. The Hilbert series may be summed into a rational fraction\nwhere \"d\" is the dimension of the ideal and formula_29 is a polynomial such that formula_30 is the degree of the ideal.\n\nAlthough the dimension and the degree do not depend on the choice of the monomial ordering, the Hilbert series and the polynomial formula_29 change when one changes of monomial ordering.\n\nMost computer algebra systems that provide functions to compute Gröbner bases provide also functions for computing the Hilbert series, and thus also the dimension and the degree.\n\nThe computation of Gröbner bases for an \"elimination\" monomial ordering allows computational elimination theory. This is based on the following theorem.\n\nLet us consider a polynomial ring formula_32 in which the variables are split into two subsets \"X\" and \"Y\". Let us also choose an elimination monomial ordering \"eliminating\" \"X\", that is a monomial ordering for which two monomials are compared by comparing first the \"X\"-parts, and, in case of equality only, considering the \"Y\"-parts. This implies that a monomial containing an \"X\"-variable is greater than every monomial independent of \"X\".\nIf \"G\" is a Gröbner basis of an ideal \"I\" for this monomial ordering, then formula_33 is a Gröbner basis of formula_34 (this ideal is often called the \"elimination ideal\"). Moreover, formula_33 consists exactly of the polynomials of whose leading terms belong to (this makes very easy the computation of formula_33, as only the leading monomials need to be checked).\n\nThis \"elimination property\" has many applications, some of them are reported in the next sections.\n\nAnother application, in algebraic geometry, is that \"elimination\" realizes the geometric operation of projection of an affine algebraic set into a subspace of the ambient space: with above notation, the (Zariski closure of) the projection of the algebraic set defined by the ideal \"I\" into the \"Y\"-subspace is defined by the ideal formula_37\n\nThe lexicographical ordering such that formula_38 is an elimination ordering for every partition formula_39 Thus a Gröbner basis for this ordering carries much more information than usually necessary. This may explain why Gröbner bases for the lexicographical ordering are usually the most difficult to compute.\n\nand {\"g\", ..., \"g\"}, then a single Gröbner basis computation produces a Gröbner basis of their intersection \"I\" ∩ \"J\". For this, one introduces a new indeterminate \"t\", and one uses an elimination ordering such that the first block contains only \"t\" and the other block contains all the other variables (this means that a monomial containing \"t\" is greater than every monomial that do not contain \"t\". With this monomial ordering, a Gröbner basis of \"I\" ∩ \"J\" consists in the polynomials that do not contain \"t\", in the Gröbner basis of the ideal\nIn other words, \"I\" ∩ \"J\" is obtained by \"eliminating\" \"t\" in \"K\".\nThis may be proven by remarking that the ideal \"K\" consists in the polynomials formula_41 such that formula_42 and formula_43. Such a polynomial is independent of \"t\" if and only \"a\"=\"b\", which means that formula_44\n\nA rational curve is an algebraic curve that has a parametric equation of the form\n\nwhere formula_46 and formula_47 are univariate polynomials for 1 ≤ \"i\" ≤ \"n\". One may (and will) suppose that formula_46 and formula_47 are coprime (they have no non-constant common factors).\n\nImplicitization consists in computing the implicit equations of such a curve. In case of \"n\" = 2, that is for plane curves, this may be computed with the resultant. The implicit equation is the following resultant:\n\nElimination with Gröbner bases allow to implicitize for any value of \"n\", simply by eliminating \"t\" in the ideal\nformula_51\nIf \"n\" = 2, the result is the same as with the resultant, if the map formula_52 is injective for almost every \"t\". In the other case, the resultant is a power of the result of the elimination.\n\nWhen modeling a problem by polynomial equations, it is highly frequent that some quantities are supposed to be non-zero, because, if they are zero, the problem becomes very different. For example, when dealing with triangles, many properties become false if the triangle is degenerated, that is if the length of one side is equal to the sum of the lengths of the other sides. In such situations, there is no hope of deducing relevant information from the polynomial system if the degenerate solutions are not dropped out. More precisely, the system of equations defines an algebraic set which may have several irreducible components, and one has to remove the components on which the degeneracy conditions are everywhere zero.\n\nThis is done by \"saturating\" the equations by the degeneracy conditions, which may be done by using the elimination property of Gröbner bases.\n\nThe localization of a ring consists in adjoining to it the formal inverses of some elements. This section concerns only the case of a single element, or equivalently a finite number of elements (adjoining the inverses of several elements is equivalent to adjoin the inverse of their product. The \"localization\" of a ring \"R\" by an element \"f\" is the ring formula_53 where \"t\" is a new indeterminate representing the inverse of \"f\". The \"localization\" of an ideal \"I\" of \"R\" is the ideal formula_54 of formula_55 When \"R\" is a polynomial ring, computing in formula_56 is not efficient, because of the need to manage the denominators. Therefore, the operation of \"localization\" is usually replaced by the operation of \"saturation\".\n\nThe saturation with respect to \"f\" of an ideal \"I\" in \"R\" is the inverse image of formula_54 under the canonical map from \"R\" to formula_55 It is the ideal formula_59 consisting in all elements of \"R\" whose products by some power of \"f\" belongs to \"I\".\n\nIf \"J\" is the ideal generated by \"I\" and 1-\"ft\" in \"R\"[\"t\"], then formula_60 It follows that, if \"R\" is a polynomial ring, a Gröbner basis computation eliminating \"t\" allows to compute a Gröbner basis of the saturation of an ideal by a polynomial.\n\nThe important property of the saturation, which ensures that it removes from the algebraic set defined by the ideal \"I\" the irreducible components on which the polynomial \"f\" is zero is the following: \"The primary decomposition of\" formula_61 \"consists in the components of the primary decomposition of I that do not contain any power of f\".\n\nA Gröbner basis of the saturation by \"f\" of a polynomial ideal generated by a finite set of polynomials \"F\", may be obtained by eliminating \"t\" in formula_62 that is by keeping the polynomials independent of \"t\" in the Gröbner basis of formula_63 for an elimination ordering eliminating \"t\".\n\nInstead of using \"F\", one may also start from a Gröbner basis of \"F\". Which method is most efficient depends on the problem. However, if the saturation does not remove any component, that is if the ideal is equal to its saturated ideal, computing first the Gröbner basis of \"F\" is usually faster. On the other hand, if the saturation removes some components, the direct computation may be dramatically faster.\n\nIf one wants to saturate with respect to several polynomials formula_64 or with respect to a single polynomial which is a product formula_65 there are three ways to proceed which give the same result but may have very different computation times (it depends on the problem which is the most efficient).\n\nHilbert's Nullstellensatz has two versions. The first one asserts that a set of polynomials has an empty set of common zeros in an algebraic closure of the field of the coefficients if and only if 1 belongs to the generated ideal. This is easily tested with a Gröbner basis computation, because 1 belongs to an ideal if and only if 1 belongs to the Gröbner basis of the ideal, for any monomial ordering.\n\nThe second version asserts that the set of common zeros (in an algebraic closure of the field of the coefficients) of an ideal is contained in the hypersurface of the zeros of a polynomial \"f\", if and only if a power of \"f\" belongs to the ideal. This may be tested by a saturating the ideal by \"f\"; in fact, a power of \"f\" belongs to the ideal if and only if the saturation by \"f\" provides a Gröbner basis containing 1.\n\nBy definition, an affine rational variety of dimension \"k\" may be described by parametric equations of the form\nwhere formula_72 are \"n\"+1 polynomials in the \"k\" variables (parameters of the parameterization) formula_73 Thus the parameters formula_74 and the coordinates formula_75 of the points of the variety are zeros of the ideal\n\nOne could guess that it suffices to eliminate the parameters to obtain the implicit equations of the variety, as it has been done in the case of curves. Unfortunately this is not always the case. If the formula_77 have a common zero (sometimes called \"base point\"), every irreducible component of the non-empty algebraic set defined by the formula_77 is an irreducible component of the algebraic set defined by \"I\". It follows that, in this case, the direct elimination of the formula_70 provides an empty set of polynomials.\n\nTherefore, if \"k\">1, two Gröbner basis computations are needed to implicitize:\n\n\n\n"}
{"id": "18329570", "url": "https://en.wikipedia.org/wiki?curid=18329570", "title": "Haïm Brezis", "text": "Haïm Brezis\n\nHaïm Brezis (born 1 June 1944) is a French mathematician who works in functional analysis and partial differential equations.\n\nBorn in Riom-ès-Montagnes, Cantal, France. Brezis is the son of a Romanian immigrant father, who came to France in the 1930s, and a Jewish mother who fled from the Netherlands. His wife, Michal Govrin, a native Israeli, works as a novelist, poet, and theater director. Brezis received his Ph.D. from the University of Paris in 1972 under the supervision of Gustave Choquet. He is currently a Professor at the Pierre and Marie Curie University and a Visiting Distinguished Professor at Rutgers University. He is a member of the Academia Europaea (1988) and a foreign associate of the United States National Academy of Sciences (2003). In 2012 he became a fellow of the American Mathematical Society. He holds honorary doctorates from several universities including National Technical University of Athens. Brezis is listed as an ISI highly cited researcher.\n\n\n\n"}
{"id": "2336214", "url": "https://en.wikipedia.org/wiki?curid=2336214", "title": "Hilbert's seventeenth problem", "text": "Hilbert's seventeenth problem\n\nHilbert's seventeenth problem is one of the 23 Hilbert problems set out in a celebrated list compiled in 1900 by David Hilbert. It concerns the expression of positive definite rational functions as sums of quotients of squares. The original question may be reformulated as:\n\n\nThe formulation of the question takes into account that there are non-negative polynomials, for example\n\nwhich cannot be represented as a sum of squares of other polynomials. In 1888, Hilbert showed that every homogeneous polynomial in \"n\" variables and degree 2\"d\" can be represented as sum of squares of other polynomials if and only if either (a)\n\"n\" = 2 or (b) 2\"d\" = 2 or (c) \"n\" = 3 and 2\"d\" = 4. Hilbert's proof did not exhibit an explicit example: only in 1967 the first explicit example was constructed by Motzkin.\n\nThe particular case of \"n\" = 2 was already solved by Hilbert in 1893. The general problem was solved in the affirmative, in 1927, by Emil Artin, for positive semidefinite functions over the reals or more generally real-closed fields. An algorithmic solution was found by Charles Delzell in 1984. A result of Albrecht Pfister shows that a positive semidefinite form in \"n\" variables can be expressed as a sum of 2 squares.\n\nDubois showed in 1967 that the answer is negative in general for ordered fields. In this case one can say that a positive polynomial is a sum of weighted squares of rational functions with positive coefficients.\n\nA generalization to the matrix case (matrices with polynomial function entries that are always positive semidefinite can be expressed as sum of squares of symmetric matrices with rational function entries) was given by Gondard, Ribenboim and Procesi, Schacher, with an elementary proof given by Hillar and Nie.\n\nIt is an open question what is the smallest number\n\nsuch that any \"n\"-variate, non-negative polynomial of degree \"d\" can be written as sum of at most formula_3 square rational functions over the reals.\n\nThe best known result () is\n\ndue to Pfister in 1967.\n\nIn complex analysis the Hermitian analogue, requiring the squares to be squared norms of holomorphic mappings, is somewhat more complicated, but true for positive polynomials by a result of Quillen. The result of Pfister on the other hand fails in the Hermitian case, that is there is no bound on the number of squares required, see D'Angelo–Lebl.\n\n\n"}
{"id": "15040455", "url": "https://en.wikipedia.org/wiki?curid=15040455", "title": "Inverse curve", "text": "Inverse curve\n\nIn inversive geometry, an inverse curve of a given curve is the result of applying an inverse operation to . Specifically, with respect to a fixed circle with center and radius the inverse of a point is the point for which lies on the ray and . The inverse of the curve is then the locus of as runs over . The point in this construction is called the center of inversion, the circle the circle of inversion, and the radius of inversion.\n\nAn inversion applied twice is the identity transformation, so the inverse of an inverse curve with respect to the same circle is the original curve. Points on the circle of inversion are fixed by the inversion, so its inverse is itself.\n\nThe inverse of the point with respect to the unit circle is where\n\nor equivalently\n\nSo the inverse of the curve determined by with respect to the unit circle is\n\nIt is clear from this that inverting an algebraic curve of degree with respect to a circle produces an algebraic curve of degree at most .\n\nSimilarly, the inverse of the curve defined parametrically by the equations\n\nwith respect to the unit circle is given parametrically as\n\nThis implies that the circular inverse of a rational curve is also rational.\n\nMore generally, the inverse of the curve determined by with respect to the circle with center and radius is\n\nThe inverse of the curve defined parametrically by\n\nwith respect to the same circle is given parametrically as\n\nIn polar coordinates, the equations are simpler when the circle of inversion is the unit circle. The inverse of the point with respect to the unit circle is where\n\nSo the inverse of the curve is determined by and the inverse of the curve is .\n\nAs noted above, the inverse with respect to a circle of a curve of degree has degree at most . The degree is exactly unless the original curve passes through the point of inversion or it is circular, meaning that it contains the circular points, , when considered as a curve in the complex projective plane. In general, inversion with respect to an arbitrary curve may produce an algebraic curve with proportionally larger degree.\n\nSpecifically, if is -circular of degree , and if the center of inversion is a singularity of order on , then the inverse curve will be an -circular curve of degree and the center of inversion is a singularity of order on the inverse curve. Here if the curve does not contain the center of inversion and if the center of inversion is a nonsingular point on it; similarly the circular points, , are singularities of order on . The value can be eliminated from these relations to show that the set of -circular curves of degree , where may vary but is a fixed positive integer, is invariant under inversion.\n\nApplying the above transformation to the lemniscate of Bernoulli\n\ngives us\n\nthe equation of a hyperbola; since inversion is a birational transformation and the hyperbola is a rational curve, this shows the lemniscate is also a rational curve, which is to say a curve of genus zero.\n\nIf we apply the transformation to the Fermat curve , where is odd, we obtain\n\nAny rational point on the Fermat curve has a corresponding rational point on this curve, giving an equivalent formulation of Fermat's Last Theorem.\n\nFor simplicity, the circle of inversion in the following cases will be the unit circle. Results for other circles of inversion can be found by translation and magnification of the original curve.\n\nFor a line passing through the origin, the polar equation is where is fixed. This remains unchanged under the inversion.\n\nThe polar equation for a line not passing through the origin is\n\nand the equation of the inverse curve is\n\nwhich defines a circle passing through the origin. Applying the inversion again shows that the inverse of a circle passing through the origin is a line.\n\nIn polar coordinates, the general equation for a circle that does not pass through the origin (the other cases having been covered) is\n\nwhere is the radius and are the polar coordinates of the center. The equation of the inverse curve is then\n\nor\n\nThis is the equation of a circle with radius\n\nand center whose polar coordinates are\n\nNote that may be negative.\n\nIf the original circle intersects with the unit circle, then the centers of the two circles and a point of intersection form a triangle with sides this is a right triangle, i.e. the radii are at right angles, exactly when\n\nBut from the equations above, the original circle is the same as the inverse circle exactly when\n\nSo the inverse of a circle is the same circle if and only if it intersects the unit circle at right angles.\n\nTo summarize and generalize this and the previous section:\n\nThe equation of a parabola is, up to similarity, translating so that the vertex is at the origin and rotating so that the axis is horizontal, . In polar coordinates this becomes\n\nThe inverse curve then has equation\n\nwhich is the cissoid of Diocles.\n\nThe polar equation of a conic section with one focus at the origin is, up to similarity\n\nwhere e is the eccentricity. The inverse of this curve will then be\n\nwhich is the equation of a limaçon of Pascal. When this is the circle of inversion. When the original curve is an ellipse and the inverse is a simple closed curve with an acnode at the origin. When the original curve is a parabola and the inverse is the cardioid which has a cusp at the origin. When the original curve is a hyperbola and the inverse forms two loops with a crunode at the origin.\n\nThe general equation of an ellipse or hyperbola is\nTranslating this so that the origin is one of the vertices gives\nand rearranging gives\nor, changing constants,\nNote that parabola above now fits into this scheme by putting and .\nThe equation of the inverse is\n\nor\n\nThis equation describes a family of curves called the conchoids of de Sluze. This family includes, in addition to the cissoid of Diocles listed above, the trisectrix of Maclaurin () and the right strophoid ().\n\nInverting the equation of an ellipse or hyperbola\n\ngives\n\nwhich is the hippopede. When this is the lemniscate of Bernoulli.\n\nApplying the degree formula above, the inverse of a conic (other than a circle) is a circular cubic if the center of inversion is on the curve, and a bicircular quartic otherwise. Conics are rational so the inverse curves are rational as well. Conversely, any rational circular cubic or rational bicircular quartic is the inverse of a conic. In fact, any such curve must have a real singularity and taking this point as a center of inversion, the inverse curve will be a conic by the degree formula.\n\nAn anallagmatic curve is one which inverts into itself. Examples include the circle, cardioid, oval of Cassini, strophoid, and trisectrix of Maclaurin.\n\n\n\n"}
{"id": "15387", "url": "https://en.wikipedia.org/wiki?curid=15387", "title": "Irreducible complexity", "text": "Irreducible complexity\n\nIrreducible complexity (IC) is the idea that certain biological systems cannot evolve by successive small modifications to pre-existing functional systems through natural selection. Irreducible complexity is central to the creationist concept of intelligent design, but it is rejected by the scientific community, which regards intelligent design as pseudoscience. Irreducible complexity is one of two main arguments used by intelligent design proponents, the other being specified complexity.\n\nThe theological argument from design was presented in creation science with assertions that evolution could not explain complex molecular mechanisms, and in 1993 Michael Behe, a professor of biochemistry at Lehigh University, presented these arguments in a revised version of \"Of Pandas and People\". In his 1996 book \"Darwin's Black Box\" he called this \"irreducible complexity\" and said it made evolution through natural selection of random mutations impossible. This was based on the mistaken assumption that evolution relies on improvement of existing functions, ignoring how complex adaptations originate from changes in function, and disregarding published research. Evolutionary biologists have published rebuttals showing how systems discussed by Behe can evolve, and examples documented through comparative genomics show that complex molecular systems are formed by the addition of components as revealed by different temporal origins of their proteins.\n\nIn the 2005 \"Kitzmiller v. Dover Area School District\" trial, Behe gave testimony on the subject of irreducible complexity. The court found that \"Professor Behe's claim for irreducible complexity has been refuted in peer-reviewed research papers and has been rejected by the scientific community at large.\"\n\nMichael Behe defined irreducible complexity in natural selection in his book \"Darwin's Black Box\":\n... a single system which is composed of several well-matched, interacting parts that contribute to the basic function, and where the removal of any one of the parts causes the system to effectively cease functioning.\n\nA second definition given by Behe (his \"evolutionary definition\") is as follows:\nAn irreducibly complex evolutionary pathway is one that contains one or more unselected steps (that is, one or more necessary-but-unselected mutations). The degree of irreducible complexity is the number of unselected steps in the pathway.\n\nIntelligent design advocate William A. Dembski gives this definition:\nA system performing a given basic function is irreducibly complex if it includes a set of well-matched, mutually interacting, nonarbitrarily individuated parts such that each part in the set is indispensable to maintaining the system's basic, and therefore original, function. The set of these indispensable parts is known as the irreducible core of the system.\n\nThe argument from irreducible complexity is a descendant of the teleological argument for God (the argument from design or from complexity). This states that because certain things in nature appear very complicated, they must have been designed. William Paley famously argued, in his 1802 watchmaker analogy, that complexity in nature implies a God for the same reason that the existence of a watch implies the existence of a watchmaker. This argument has a long history, and one can trace it back at least as far as Cicero's \"De Natura Deorum\" ii.34, written in 45 BC.\n\nGalen (1st and 2nd centuries AD) wrote about the large number of parts of the body and their relationships, which observation was cited as evidence for creation. The idea that the interdependence between parts would have implications for the origins of living things was raised by writers starting with Pierre Gassendi in the mid-17th century and by John Wilkins (1614-1672), who wrote (citing Galen), \"Now to imagine, that all these things, according to their several kinds, could be brought into this regular frame and order, to which such an infinite number of Intentions are required, without the contrivance of some wise Agent, must needs be irrational in the highest degree.\" In the late 17th-century, Thomas Burnet referred to \"a multitude of pieces aptly joyn'd\" to argue against the eternity of life. In the early 18th century, Nicolas Malebranche wrote \"An organized body contains an infinity of parts that mutually depend upon one another in relation to particular ends, all of which must be actually formed in order to work as a whole\", arguing in favor of preformation, rather than epigenesis, of the individual; and a similar argument about the origins of the individual was made by other 18th-century students of natural history. In his 1790 book, \"The Critique of Judgment\", Kant is said by Guyer to argue that \"we cannot conceive how a whole that comes into being only gradually from its parts can nevertheless be the cause of the properties of those parts\".\n\nChapter XV of Paley's \"Natural Theology\" discusses at length what he called \"relations\" of parts of living things as an indication of their design.\n\nGeorges Cuvier applied his principle of the \"correlation of parts\" to describe an animal from fragmentary remains. For Cuvier, this related to another principle of his, the \"conditions of existence\", which excluded the possibility of transmutation of species.\n\nWhile he did not originate the term, Charles Darwin identified the argument as a possible way to falsify a prediction of the theory of evolution at the outset. In \"The Origin of Species\" (1859), he wrote, \"If it could be demonstrated that any complex organ existed, which could not possibly have been formed by numerous, successive, slight modifications, my theory would absolutely break down. But I can find out no such case.\" Darwin's theory of evolution challenges the teleological argument by postulating an alternative explanation to that of an intelligent designer—namely, evolution by natural selection. By showing how simple unintelligent forces can ratchet up designs of extraordinary complexity without invoking outside design, Darwin showed that an intelligent designer was not the necessary conclusion to draw from complexity in nature. The argument from irreducible complexity attempts to demonstrate that certain biological features cannot be purely the product of Darwinian evolution.\n\nIn the late 19th century, in a dispute between supporters of the adequacy of natural selection and those who held for inheritance of acquired characteristics, one of the arguments made repeatedly by Herbert Spencer, and followed by others, depended on what Spencer referred to as \"co-adaptation\" of \"co-operative\" parts, as in: \"We come now to Professor Weismann's endeavour to disprove my second thesis — that it is impossible to explain by natural selection alone the co-adaptation of co-operative parts. It is thirty years since this was set forth in \"The Principles of Biology.\" In §166, I instanced the enormous horns of the extinct Irish elk, and contended that in this and in kindred cases, where for the efficient use of some one enlarged part many other parts have to be simultaneously enlarged, it is out of the question to suppose that they can have all spontaneously varied in the required proportions.\" Darwin responded to Spencer's objections in chapter XXV of \"The Variation of Animals and Plants under Domestication\" (1868). The history of this concept in the dispute has been characterized: \"An older and more religious tradition of idealist thinkers were committed to the explanation of complex adaptive contrivances by intelligent design. ... Another line of thinkers, unified by the recurrent publications of Herbert Spencer, also saw co-adaptation as a composed, irreducible whole, but sought to explain it by the inheritance of acquired characteristics.\"\n\nSt. George Jackson Mivart raised the objection to natural selection that \"Complex and simultaneous co-ordinations … until so far developed as to effect the requisite junctions, are useless\" which \"amounts to the concept of \"irreducible complexity\" as defined by … Michael Behe\".\n\nHermann Muller, in the early 20th century, discussed a concept similar to irreducible complexity. However, far from seeing this as a problem for evolution, he described the \"interlocking\" of biological features as a consequence to be expected of evolution, which would lead to irreversibility of some evolutionary changes. He wrote, \"Being thus finally woven, as it were, into the most intimate fabric of the organism, the once novel character can no longer be withdrawn with impunity, and may have become vitally necessary.\"\n\nIn 1974 the young Earth creationist Henry M. Morris introduced a similar concept in his book \"Scientific Creationism\", in which he wrote; \"This issue can actually be attacked quantitatively, using simple principles of mathematical probability. The problem is simply whether a complex system, in which many components function unitedly together, and in which each component is uniquely necessary to the efficient functioning of the whole, could ever arise by random processes.\"\n\nIn 1975 Thomas H. Frazzetta published a book-length study of a concept similar to irreducible complexity, explained by gradual, step-wise, non-teleological evolution. Frazzetta wrote: \"A complex adaptation is one constructed of \"several\" components that must blend together operationally to make the adaptation \"work\". It is analogous to a machine whose performance depends upon careful cooperation among its parts. In the case of the machine, no single part can greatly be altered without changing the performance of the entire machine.\" The machine that he chose as an analog is the Peaucellier–Lipkin linkage, and one biological system given extended description was the jaw apparatus of a python. The conclusion of this investigation, rather than that evolution of a complex adaptation was impossible, \"awed by the adaptations of living things, to be stunned by their complexity and suitability\", was \"to accept the inescapable but not humiliating fact that much of mankind can be seen in a tree or a lizard.\"\n\nIn 1981, Ariel Roth, in defense of the creation-science position in the trial \"McLean v. Arkansas\", said of \"complex integrated structures\": \"This system would not be functional until all the parts were there ... How did these parts survive during evolution ...?\"\n\nIn 1985 Cairns-Smith wrote of \"interlocking\": \"How can a complex collaboration between components evolve in small steps?\" and used the analogy of the scaffolding called centering - used to build an arch then removed afterwards: \"Surely there was 'scaffolding'. Before the multitudinous components of present biochemistry could come to lean together \"they had to lean on something else.\"\" However, neither Muller or Cairns-Smith claimed their ideas as evidence of something supernatural.\n\nAn essay in support of creationism published in 1994 referred to bacterial flagella as showing \"multiple, integrated components\", where \"nothing about them works unless every one of their complexly fashioned and integrated components are in place\". The author asked the reader to \"imagine the effects of natural selection on those organisms that fortuitously evolved the flagella ... without the concommitant control mechanisms\".\n\nAn early concept of irreducibly complex systems comes from Ludwig von Bertalanffy (1901-1972), an Austrian biologist. He believed that complex systems must be examined as complete, irreducible systems in order to fully understand how they work. He extended his work on biological complexity into a general theory of systems in a book titled \"General Systems Theory\".\n\nAfter James Watson and Francis Crick published the structure of DNA in the early 1950s, General Systems Theory lost many of its adherents in the physical and biological sciences.\nHowever, Systems theory remained popular in the social sciences long after its demise in the physical and biological sciences.\n\nMichael Behe developed his ideas on the concept around 1992, in the early days of the 'wedge movement', and first presented his ideas about \"irreducible complexity\" in June 1993 when the \"Johnson-Behe cadre of scholars\" met at Pajaro Dunes in California. He set out his ideas in the second edition of \"Of Pandas and People\" published in 1993, extensively revising Chapter 6 \"Biochemical Similarities\" with new sections on the complex mechanism of blood clotting and on the origin of proteins.\n\nHe first used the term \"irreducible complexity\" in his 1996 book \"Darwin's Black Box\", to refer to certain complex biochemical cellular systems. He posits that evolutionary mechanisms cannot explain the development of such \"irreducibly complex\" systems. Notably, Behe credits philosopher William Paley for the original concept (alone among the predecessors) and suggests that his application of the concept to biological systems is entirely original.\n\nIntelligent design advocates argue that irreducibly complex systems must have been deliberately engineered by some form of intelligence.\n\nIn 2001, Michael Behe wrote: \"[T]here is an asymmetry between my current definition of irreducible complexity and the task facing natural selection. I hope to repair this defect in future work.\" Behe specifically explained that the \"current definition puts the focus on removing a part from an already functioning system\", but the \"difficult task facing Darwinian evolution, however, would not be to remove parts from sophisticated pre-existing systems; it would be to bring together components to make a new system in the first place\". In the 2005 \"Kitzmiller v. Dover Area School District\" trial, Behe testified under oath that he \"did not judge [the asymmetry] serious enough to [have revised the book] yet.\"\n\nBehe additionally testified that the presence of irreducible complexity in organisms would not rule out the involvement of evolutionary mechanisms in the development of organic life. He further testified that he knew of no earlier \"peer reviewed articles in scientific journals discussing the intelligent design of the blood clotting cascade,\" but that there were \"probably a large number of peer reviewed articles in science journals that demonstrate that the blood clotting system is indeed a purposeful arrangement of parts of great complexity and sophistication.\" (The judge ruled that \"intelligent design is not science and is essentially religious in nature\".)\n\nAccording to the theory of evolution, genetic variations occur without specific design or intent. The environment \"selects\" the variants that have the highest fitness, which are then passed on to the next generation of organisms. Change occurs by the gradual operation of natural forces over time, perhaps slowly, perhaps more quickly (see punctuated equilibrium). This process is able to adapt complex structures from simpler beginnings, or convert complex structures from one function to another (see spandrel). Most intelligent design advocates accept that evolution occurs through mutation and natural selection at the \"micro level\", such as changing the relative frequency of various beak lengths in finches, but assert that it cannot account for irreducible complexity, because none of the parts of an irreducible system would be functional or advantageous until the entire system is in place.\n\nBehe uses the mousetrap as an illustrative example of this concept. A mousetrap consists of five interacting pieces: the base, the catch, the spring, the hammer, and the hold-down bar. All of these must be in place for the mousetrap to work, as the removal of any one piece destroys the function of the mousetrap. Likewise, he asserts that biological systems require multiple parts working together in order to function. Intelligent design advocates claim that natural selection could not create from scratch those systems for which science is currently unable to find a viable evolutionary pathway of successive, slight modifications, because the selectable function is only present when all parts are assembled.\n\nIn his 2008 book \"Only A Theory\", biologist Kenneth R. Miller challenges Behe's claim that the mousetrap is irreducibly complex. Miller observes that various subsets of the five components can be devised to form cooperative units, ones that have different functions from the mousetrap and so, in biological terms, could form functional spandrels before being adapted to the new function of catching mice. In an example taken from his high school experience, Miller recalls that one of his classmates...struck upon the brilliant idea of using an old, broken mousetrap as a spitball catapult, and it worked brilliantly... It had worked perfectly as something other than a mousetrap... my rowdy friend had pulled a couple of parts --probably the hold-down bar and catch-- off the trap to make it easier to conceal and more effective as a catapult... [leaving] the base, the spring, and the hammer. Not much of a mousetrap, but a helluva spitball launcher... I realized why [Behe's] mousetrap analogy had bothered me. It was wrong. The mousetrap is not irreducibly complex after all.\n\nOther systems identified by Miller that include mousetrap components include the following:\n\nThe point of the reduction is that - in biology - most or all of the components were already at hand, by the time it became necessary to build a mousetrap. As such, it required far fewer steps to develop a mousetrap than to design all the components from scratch.\n\nThus, the development of the mousetrap, said to consist of five different parts which had no function on their own, has been reduced to one step: the assembly from parts that are already present, performing other functions.\n\nThe Intelligent Design argument focuses on the functionality to catch mice. It skips over the case that many, if not all, parts are already available in their own right, at the time that the need for a mousetrap arises.\n\nSupporters of intelligent design argue that anything less than the complete form of such a system or organ would not work at all, or would in fact be a \"detriment\" to the organism, and would therefore never survive the process of natural selection. Although they accept that some complex systems and organs \"can\" be explained by evolution, they claim that organs and biological features which are \"irreducibly complex\" cannot be explained by current models, and that an intelligent designer must have created life or guided its evolution. Accordingly, the debate on irreducible complexity concerns two questions: whether irreducible complexity can be found in nature, and what significance it would have if it did exist in nature.\n\nBehe's original examples of irreducibly complex mechanisms included the bacterial flagellum of \"E. coli\", the blood clotting cascade, cilia, and the adaptive immune system.\n\nBehe argues that organs and biological features which are irreducibly complex cannot be wholly explained by current models of evolution. In explicating his definition of \"irreducible complexity\" he notes that:\nAn irreducibly complex system cannot be produced directly (that is, by continuously improving the initial function, which continues to work by the same mechanism) by slight, successive modifications of a precursor system, because any precursor to an irreducibly complex system that is missing a part is by definition nonfunctional.\n\nIrreducible complexity is not an argument that evolution does not occur, but rather an argument that it is \"incomplete\". In the last chapter of \"Darwin's Black Box\", Behe goes on to explain his view that irreducible complexity is evidence for intelligent design. Mainstream critics, however, argue that irreducible complexity, as defined by Behe, can be generated by known evolutionary mechanisms. Behe's claim that no scientific literature adequately modeled the origins of biochemical systems through evolutionary mechanisms has been challenged by TalkOrigins. The judge in the \"Dover\" trial wrote \"By defining irreducible complexity in the way that he has, Professor Behe attempts to exclude the phenomenon of exaptation by definitional fiat, ignoring as he does so abundant evidence which refutes his argument. Notably, the NAS has rejected Professor Behe's claim for irreducible complexity...\"\n\nBehe and others have suggested a number of biological features that they believe may be irreducibly complex.\n\nThe process of blood clotting or coagulation cascade in vertebrates is a complex biological pathway which is given as an example of apparent irreducible complexity.\n\nThe irreducible complexity argument assumes that the necessary parts of a system have always been necessary, and therefore could not have been added sequentially. However, in evolution, something which is at first merely advantageous can later become necessary. Natural selection can lead to complex biochemical systems being built up from simpler systems, or to existing functional systems being recombined as a new system with a different function. For example, one of the clotting factors that Behe listed as a part of the clotting cascade (Factor XII, also called Hageman factor) was later found to be absent in whales, demonstrating that it is not essential for a clotting system. Many purportedly irreducible structures can be found in other organisms as much simpler systems that utilize fewer parts. These systems, in turn, may have had even simpler precursors that are now extinct. Behe has responded to critics of his clotting cascade arguments by suggesting that homology is evidence for evolution, but not for natural selection.\n\nThe \"improbability argument\" also misrepresents natural selection. It is correct to say that a set of simultaneous mutations that form a complex protein structure is so unlikely as to be unfeasible, but that is not what Darwin advocated. His explanation is based on small accumulated changes that take place without a final goal. Each step must be advantageous in its own right, although biologists may not yet understand the reason behind all of them—for example, jawless fish accomplish blood clotting with just six proteins instead of the full ten.\n\nThe eye is an example of a supposedly irreducibly complex structure, due to its many elaborate and interlocking parts, seemingly all dependent upon one another. It is frequently cited by intelligent design and creationism advocates as an example of irreducible complexity. Behe used the \"development of the eye problem\" as evidence for intelligent design in \"Darwin's Black Box\". Although Behe acknowledged that the evolution of the larger anatomical features of the eye have been well-explained, he pointed out that the complexity of the minute biochemical reactions required at a molecular level for light sensitivity still defies explanation. Creationist Jonathan Sarfati has described the eye as evolutionary biologists' \"greatest challenge as an example of superb 'irreducible complexity' in God's creation\", specifically pointing to the supposed \"vast complexity\" required for transparency.\n\nIn an often misquoted passage from \"On the Origin of Species\", Charles Darwin appears to acknowledge the eye's development as a difficulty for his theory. However, the quote in context shows that Darwin actually had a very good understanding of the evolution of the eye (see fallacy of quoting out of context). He notes that \"to suppose that the eye ... could have been formed by natural selection, seems, I freely confess, absurd in the highest possible degree\". Yet this observation was merely a rhetorical device for Darwin. He goes on to explain that if gradual evolution of the eye could be shown to be possible, \"the difficulty of believing that a perfect and complex eye could be formed by natural selection ... can hardly be considered real\". He then proceeded to roughly map out a likely course for evolution using examples of gradually more complex eyes of various species.\n\nSince Darwin's day, the eye's ancestry has become much better understood. Although learning about the construction of ancient eyes through fossil evidence is problematic due to the soft tissues leaving no imprint or remains, genetic and comparative anatomical evidence has increasingly supported the idea of a common ancestry for all eyes.\n\nCurrent evidence does suggest possible evolutionary lineages for the origins of the anatomical features of the eye. One likely chain of development is that the eyes originated as simple patches of photoreceptor cells that could detect the presence or absence of light, but not its direction. When, via random mutation across the population, the photosensitive cells happened to have developed on a small depression, it endowed the organism with a better sense of the light's source. This small change gave the organism an advantage over those without the mutation. This genetic trait would then be \"selected for\" as those with the trait would have an increased chance of survival, and therefore progeny, over those without the trait. Individuals with deeper depressions would be able to discern changes in light over a wider field than those individuals with shallower depressions. As ever deeper depressions were advantageous to the organism, gradually, this depression would become a pit into which light would strike certain cells depending on its angle. The organism slowly gained increasingly precise visual information. And again, this gradual process continued as individuals having a slightly shrunken aperture of the eye had an advantage over those without the mutation as an aperture increases how collimated the light is at any one specific group of photoreceptors. As this trait developed, the eye became effectively a pinhole camera which allowed the organism to dimly make out shapes—the nautilus is a modern example of an animal with such an eye. Finally, via this same selection process, a protective layer of transparent cells over the aperture was differentiated into a crude lens, and the interior of the eye was filled with humours to assist in focusing images. In this way, eyes are recognized by modern biologists as actually a relatively unambiguous and simple structure to evolve, and many of the major developments of the eye's evolution are believed to have taken place over only a few million years, during the Cambrian explosion. Behe asserts that this is only an explanation of the gross anatomical steps, however, and not an explanation of the changes in discrete biochemical systems that would have needed to take place.\n\nBehe maintains that the complexity of light sensitivity at the molecular level and the minute biochemical reactions required for those first \"simple patches of photoreceptor[s]\" still defies explanation, and that the proposed series of infinitesimal steps to get from patches of photoreceptors to a fully functional eye would actually be considered great, complex leaps in evolution if viewed on the molecular scale. Other intelligent design proponents claim that the evolution of the entire visual system would be difficult rather than the eye alone.\n\nThe flagella of certain bacteria constitute a molecular motor requiring the interaction of about 40 different protein parts. Behe presents this as a prime example of an irreducibly complex structure defined as \"a single system composed of several well-matched, interacting parts that contribute to the basic function, wherein the removal of any one of the parts causes the system to effectively cease functioning\", and argues that since \"an irreducibly complex system that is missing a part is by definition nonfunctional\", it could not have evolved gradually through natural selection.\n\nReducible complexity. In contrast to Behe's claims, many proteins can be deleted or mutated and the flagellum still works, even though sometimes at reduced efficiency. In fact, the composition of flagella is surprisingly diverse across bacteria with many proteins only found in some species but not others. Hence the flagellar apparatus is clearly very flexible in evolutionary terms and perfectly able to lose or gain protein components. Further studies have shown that, contrary to claims of \"irreducible complexity\", flagella and related protein transport mechanisms show evidence of evolution through Darwinian processes, providing case studies in how complex systems can evolve from simpler components. Multiple processes were involved in the evolution of the flagellum, including horizontal gene transfer.\n\nEvolution from Type Three Secretion Systems. Scientists regard this argument as having been disproved in the light of research dating back to 1996 as well as more recent findings. They point out that the basal body of the flagella has been found to be similar to the Type III secretion system (TTSS), a needle-like structure that pathogenic germs such as \"Salmonella\" and \"Yersinia pestis\" use to inject toxins into living eucaryote cells. The needle's base has ten elements in common with the flagellum, but it is missing forty of the proteins that make a flagellum work. The TTSS system negates Behe's claim that taking away any one of the flagellum's parts would prevent the system from functioning. On this basis, Kenneth Miller notes that, \"The parts of this supposedly irreducibly complex system actually have functions of their own.\" Studies have also shown that similar parts of the flagellum in different bacterial species can have different functions despite showing evidence of common descent, and that certain parts of the flagellum can be removed without completely eliminating its functionality.\n\nDembski has argued that phylogenetically, the TTSS is found in a narrow range of bacteria which makes it seem to him to be a late innovation, whereas flagella are widespread throughout many bacterial groups, and he argues that it was an early innovation. Against Dembski's argument, different flagella use completely different mechanisms, and publications show a plausible path in which bacterial flagella could have evolved from a secretion system.\n\nThe cilium construction of axoneme microtubules movement by the sliding of dynein protein was cited by Behe as an example of irreducible complexity. He further said that the advances in knowledge in the subsequent 10 years had shown that the complexity of intraflagellar transport for two hundred components cilium and many other cellular structures is substantially greater than was known earlier.\n\nThe bombardier beetle is able to defend itself by directing a spray of hot fluid at an attacker. The mechanism involves a system for mixing hydroquinones and hydrogen peroxide, which react violently to attain a temperature near boiling point, and in some species a nozzle which allows the spray to be directed accurately in any direction.\n\nThe unique combination of features of the bombardier beetle's defense mechanism—strongly exothermic reactions, boiling-hot fluids, and explosive release—have been claimed by creationists]] and proponents of intelligent design to be examples of irreducible complexity. Biologists such as the taxonomist Mark Isaak note however that step-by-step evolution of the mechanism could readily have occurred. In particular, quinones are precursors to sclerotin, used to harden the skeleton of many insects, while peroxide is a common by-product of metabolism.\n\nLike intelligent design, the concept it seeks to support, irreducible complexity has failed to gain any notable acceptance within the scientific community. One science writer called it a \"full-blown intellectual surrender strategy\".\n\nResearchers have proposed potentially viable evolutionary pathways for allegedly irreducibly complex systems such as blood clotting, the immune system and the flagellum - the three examples Behe proposed. John H. McDonald even showed his example of a mousetrap to be reducible. If irreducible complexity is an insurmountable obstacle to evolution, it should not be possible to conceive of such pathways.\n\nNiall Shanks and Karl H. Joplin, both of East Tennessee State University, have shown that systems satisfying Behe's characterization of irreducible biochemical complexity can arise naturally and spontaneously as the result of self-organizing chemical processes. They also assert that what evolved biochemical and molecular systems actually exhibit is \"redundant complexity\"—a kind of complexity that is the product of an evolved biochemical process. They claim that Behe overestimated the significance of irreducible complexity because of his simple, linear view of biochemical reactions, resulting in his taking snapshots of selective features of biological systems, structures, and processes, while ignoring the redundant complexity of the context in which those features are naturally embedded. They also criticized his over-reliance of overly simplistic metaphors, such as his mousetrap.\n\nA computer model of the co-evolution of proteins binding to DNA in the peer-reviewed journal \"Nucleic Acids Research\" consisted of several parts (DNA binders and DNA binding sites) which contribute to the basic function; removal of either one leads immediately to the death of the organism. This model fits the definition of irreducible complexity exactly, yet it evolves. (The program can be run from Ev program.)\n\nIn addition, research published in the peer-reviewed journal \"Nature\" has shown that computer simulations of evolution demonstrate that it is possible for complex features to evolve naturally.\n\nOne can compare a mousetrap with a cat in this context. Both normally function so as to control the mouse population. The cat has many parts that can be removed leaving it still functional; for example, its tail can be bobbed, or it can lose an ear in a fight. Comparing the cat and the mousetrap, then, one sees that the mousetrap (which is not alive) offers better evidence, in terms of irreducible complexity, for intelligent design than the cat. Even looking at the mousetrap analogy, several critics have described ways in which the parts of the mousetrap could have independent uses or could develop in stages, demonstrating that it is not irreducibly complex.\n\nMoreover, even cases where removing a certain component in an organic system will cause the system to fail do not demonstrate that the system could not have been formed in a step-by-step, evolutionary process. By analogy, stone arches are irreducibly complex—if you remove any stone the arch will collapse—yet humans build them easily enough, one stone at a time, by building over centering that is removed afterward. Similarly, naturally occurring arches of stone form by the weathering away of bits of stone from a large concretion that has formed previously.\n\nEvolution can act to simplify as well as to complicate. This raises the possibility that seemingly irreducibly complex biological features may have been achieved with a period of increasing complexity, followed by a period of simplification.\n\nA team led by Joseph Thornton, assistant professor of biology at the University of Oregon's Center for Ecology and Evolutionary Biology, using techniques for resurrecting ancient genes, reconstructed the evolution of an apparently irreducibly complex molecular system. The April 7, 2006 issue of \"Science\" published this research.\n\nIrreducible complexity may not actually exist in nature, and the examples given by Behe and others may not in fact represent irreducible complexity, but can be explained in terms of simpler precursors. The theory of facilitated variation challenges irreducible complexity. Marc W. Kirschner, a professor and chair of Department of Systems Biology at Harvard Medical School, and John C. Gerhart, a professor in Molecular and Cell Biology, University of California, Berkeley, presented this theory in 2005. They describe how certain mutation and changes can cause apparent irreducible complexity. Thus, seemingly irreducibly complex structures are merely \"very complex\", or they are simply misunderstood or misrepresented.\n\nThe precursors of complex systems, when they are not useful in themselves, may be useful to perform other, unrelated functions. Evolutionary biologists argue that evolution often works in this kind of blind, haphazard manner in which the function of an early form is not necessarily the same as the function of the later form. The term used for this process is exaptation. The mammalian middle ear (derived from a jawbone) and the panda's thumb (derived from a wrist bone spur) provide classic examples. A 2006 article in \"Nature\" demonstrates intermediate states leading toward the development of the ear in a Devonian fish (about 360 million years ago). Furthermore, recent research shows that viruses play a heretofore unexpected role in evolution by mixing and matching genes from various hosts.\n\nArguments for irreducibility often assume that things started out the same way they ended up—as we see them now. However, that may not necessarily be the case. In the \"Dover\" trial an expert witness for the plaintiffs, Ken Miller, demonstrated this possibility using Behe's mousetrap analogy. By removing several parts, Miller made the object unusable as a mousetrap, but he pointed out that it was now a perfectly functional, if unstylish, tie clip.\n\nIrreducible complexity can be seen as equivalent to an \"uncrossable valley\" in a fitness landscape. A number of mathematical models of evolution have explored the circumstances under which such valleys can, nevertheless, be crossed.\n\nSome critics, such as Jerry Coyne (professor of evolutionary biology at the University of Chicago) and Eugenie Scott (a physical anthropologist and former executive director of the National Center for Science Education) have argued that the concept of irreducible complexity and, more generally, intelligent design is not falsifiable and, therefore, not scientific.\n\nBehe argues that the theory that irreducibly complex systems could not have evolved can be falsified by an experiment where such systems are evolved. For example, he posits taking bacteria with no flagellum and imposing a selective pressure for mobility. If, after a few thousand generations, the bacteria evolved the bacterial flagellum, then Behe believes that this would refute his theory.\n\nOther critics take a different approach, pointing to experimental evidence that they believe falsifies the argument for Intelligent Design from irreducible complexity. For example, Kenneth Miller cites the lab work of Barry G. Hall on E. coli, which he asserts is evidence that \"Behe is wrong\".\n\nOther evidence that irreducible complexity is not a problem for evolution comes from the field of computer science, which routinely uses computer analogues of the processes of evolution in order to automatically design complex solutions to problems. The results of such genetic algorithms are frequently irreducibly complex since the process, like evolution, both removes non-essential components over time as well as adding new components. The removal of unused components with no essential function, like the natural process where rock underneath a natural arch is removed, can produce irreducibly complex structures without requiring the intervention of a designer. Researchers applying these algorithms automatically produce human-competitive designs—but no human designer is required.\n\nIntelligent design proponents attribute to an intelligent designer those biological structures they believe are irreducibly complex and therefore they say a natural explanation is insufficient to account for them. However, critics view irreducible complexity as a special case of the \"complexity indicates design\" claim, and thus see it as an argument from ignorance and as a God-of-the-gaps argument.\n\nEugenie Scott, along with Glenn Branch and other critics, has argued that many points raised by intelligent-design proponents are arguments from ignorance. Behe has been accused by critics of using an \"argument by lack of imagination\".\n\nIrreducible complexity is at its core an argument against evolution. If truly irreducible systems are found, the argument goes, then intelligent design must be the correct explanation for their existence. However, this conclusion is based on the assumption that current evolutionary theory and intelligent design are the only two valid models to explain life, a false dilemma.\n\nWhile testifying during the 2005 \"Kitzmiller v. Dover Area School District\" trial, Behe conceded that there are no peer-reviewed papers supporting his claims that complex molecular systems, like the bacterial flagellum, the blood-clotting cascade, and the immune system, were intelligently designed nor are there any peer-reviewed articles supporting his argument that certain complex molecular structures are \"irreducibly complex.\"\n\nIn the final ruling of \"Kitzmiller v. Dover Area School District\", Judge Jones specifically singled out Behe and irreducible complexity:\n\n\n\n"}
{"id": "23436071", "url": "https://en.wikipedia.org/wiki?curid=23436071", "title": "Isserlis' theorem", "text": "Isserlis' theorem\n\nIn probability theory, Isserlis’ theorem or Wick’s theorem is a formula that allows one to compute higher-order moments of the multivariate normal distribution in terms of its covariance matrix. It is named after Leon Isserlis.\n\nThis theorem is particularly important in particle physics, where it is known as Wick's theorem after the work of . Other applications include the analysis of portfolio returns, quantum field theory and generation of colored noise.\n\nIf formula_1 is a zero-mean multivariate normal random vector, then\nwhere the notation ∑ ∏ means summing over all distinct ways of partitioning \"X\", …, \"X\" into pairs \"X\",\"X\" and each summand is the product of the \"n\" pairs. This yields formula_3 terms in the sum (see double factorial). For example, for fourth order moments (four variables) there are three terms. For sixth-order moments there are 3 × 5 = 15 terms, and for eighth-order moments there are 3 × 5 × 7 = 105 terms (as you can check in the examples below). \n\nIn his original paper, Leon Isserlis proves this theorem by mathematical induction, generalizing the formula for the fourth-order moments, which takes the appearance\nFor sixth-order moments, Isserlis' theorem is:\n\n\n"}
{"id": "6674772", "url": "https://en.wikipedia.org/wiki?curid=6674772", "title": "John H. Hubbard", "text": "John H. Hubbard\n\nJohn Hamal Hubbard (born October 6 or 7, 1945; the actual date is unknown) is an American mathematician and professor at Cornell University and the Université de Provence. He is well known for the mathematical contributions he made with Adrien Douady in the field of complex dynamics, including a study of the Mandelbrot set. One of their most important results is that the Mandelbrot set is connected.\n\nHubbard graduated with a Doctorat d'État from Université de Paris-Sud in 1973 under the direction of Adrien Douady; his thesis was entitled \"Sur Les Sections Analytiques de La Courbe Universelle de Teichmüller\" and was published by the American Mathematical Society. Hubbard has a variety of mathematical interests ranging from complex analysis to differential geometry. He has written many influential papers on complex dynamics, and he has written several books.\n\nIn 2006, he completed another book: the first volume of a series devoted to Teichmüller theory and applications to four revolutionary theorems of William Thurston. Hubbard is a former student of Harvard University's infamous Math 55, where he famously struggled initially because he \"just didn't know proofs.\" He later returned to Harvard to teach that same class. However, Hubbard developed a profound distaste for Math 55's method of teaching proofs largely centered on algebraic induction. In response, he and his wife Barbara wrote the book \"Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach\".\n\nHe is married to Barbara Burke Hubbard, the science writer. Together they have a son, Alexander, and three younger daughters, Eleanor, Judith and Diana. The children sometimes help them with their books, in illustration, writing answer keys and pointing out the minor errors.\n\n"}
{"id": "22606041", "url": "https://en.wikipedia.org/wiki?curid=22606041", "title": "Kostka number", "text": "Kostka number\n\nIn mathematics, the Kostka number \"K\" (depending on two integer partitions λ and μ) is a non-negative integer that is equal to the number of semistandard Young tableaux of shape λ and weight μ. They were introduced by the mathematician Carl Kostka in his study of symmetric functions ().\n\nFor example, if λ = (3, 2) and μ = (1, 1, 2, 1), the Kostka number \"K\" counts the number of ways to fill a left-aligned collection of boxes with 3 in the first row and 2 in the second row with 1 copy of the number 1, 1 copy of the number 2, 2 copies of the number 3 and 1 copy of the number 4 such that the entries increase along columns and do not decrease along rows. The three such tableaux are shown at right, and \"K\" = 3.\n\nFor any partition λ, the Kostka number \"K\" is equal to 1: the unique way to fill the Young diagram of shape λ = (λ, λ, ..., λ) with λ copies of 1, λ copies of 2, and so on, so that the resulting tableau is weakly increasing along rows and strictly increasing along columns is if all the 1s are placed in the first row, all the 2s are placed in the second row, and so on. (This tableau is sometimes called the Yamanouchi tableau of shape λ.)\n\nThe Kostka number \"K\" is positive (i.e., there exist semistandard Young tableaux of shape λ and weight μ) if and only if λ and μ are both partitions of the same integer \"n\" and λ is larger than μ in dominance order.\n\nIn general, there are no nice formulas known for the Kostka numbers. However, some special cases are known. For example, if μ = (1, 1, 1, ..., 1) is the partition whose parts are all 1 then a semistandard Young tableau of weight μ is a standard Young tableau; the number of standard Young tableaux of a given shape λ is given by the hook-length formula.\n\nAn important simple property of Kostka numbers is that \"K\" does not depend on the order of entries of μ. For example, \"K\" = \"K\". This is not immediately obvious from the definition but can be shown by establishing a bijection between the sets of semistandard Young tableaux of shape λ and weights μ and μ', where μ and μ' differ only by swapping two entries.\n\nIn addition to the purely combinatorial definition above, they can also be defined as the coefficients that arise when one expresses the Schur polynomial \"s\" as a linear combination of monomial symmetric functions \"m\":\n\nwhere λ and μ are both partitions of \"n\". Alternatively, Schur polynomials can also be expressed as\n\nwhere the sum is over all weak compositions α of \"n\" and \"x\" denotes the monomial \"x\"⋯\"x\".\n\nBecause of the connections between symmetric function theory and representation theory, Kostka numbers also express the decomposition of the permutation module \"M\" in terms of the representations \"V\" corresponding to the character \"s\", i.e.,\n\nOn the level of representations of the general linear group formula_4, the Kostka number \"K\" counts the dimension of the weight space corresponding to μ in the irreducible representation \"V\" (where we require μ and λ to have at most \"n\" parts).\n\nThe Kostka numbers for partitions of size at most 3 are as follows:\n\nThese values are exactly the coefficients in the expansions of Schur functions in terms of monomial symmetric functions:\n\nKostka numbers are special values of the 1 or 2 variable Kostka polynomials: \n\n"}
{"id": "1611766", "url": "https://en.wikipedia.org/wiki?curid=1611766", "title": "Law of total cumulance", "text": "Law of total cumulance\n\nIn probability theory and mathematical statistics, the law of total cumulance is a generalization to cumulants of the law of total probability, the law of total expectation, and the law of total variance. It has applications in the analysis of time series. It was introduced by David Brillinger.\n\nIt is most transparent when stated in its most general form, for \"joint\" cumulants, rather than for cumulants of a specified order for just one random variable. In general, we have\n\nwhere\n\n\nOnly in case \"n\" = either 2 or 3 is the \"n\"th cumulant the same as the \"n\"th central moment. The case \"n\" = 2 is well-known (see law of total variance). Below is the case \"n\" = 3. The notation \"μ\" means the third central moment.\n\nFor general 4th-order cumulants, the rule gives a sum of 15 terms, as follows:\n\nSuppose \"Y\" has a Poisson distribution with expected value 1, and \"X\" is the sum of \"Y\" copies of \"W\" that are independent of each other and of \"Y\".\n\nAll of the cumulants of the Poisson distribution are equal to each other, and so in this case are equal to 1. Also recall that if random variables \"W\", ..., \"W\" are independent, then the \"n\"th cumulant is additive:\n\nWe will find the 4th cumulant of \"X\". We have:\n\nWe recognize this last sum as the sum over all partitions of the set { 1, 2, 3, 4 }, of the product over all blocks of the partition, of cumulants of \"W\" of order equal to the size of the block. That is precisely the 4th raw moment of \"W\" (see cumulant for a more leisurely discussion of this fact). Hence the moments of \"W\" are the cumulants of \"X\".\n\nIn this way we see that every moment sequence is also a cumulant sequence (the converse cannot be true, since cumulants of even order ≥ 4 are in some cases negative, and also because the cumulant sequence of the normal distribution is not a moment sequence of any probability distribution).\n\nSuppose \"Y\" = 1 with probability \"p\" and \"Y\" = 0 with probability \"q\" = 1 − \"p\". Suppose the conditional probability distribution of \"X\" given \"Y\" is \"F\" if \"Y\" = 1 and \"G\" if \"Y\" = 0. Then we have\n\nwhere formula_9 means is a partition of the set { 1, ..., \"n\" } that is finer than the coarsest partition – the sum is over all partitions except that one. For example, if \"n\" = 3, then we have\n"}
{"id": "52933264", "url": "https://en.wikipedia.org/wiki?curid=52933264", "title": "List of things named after Anatoliy Skorokhod", "text": "List of things named after Anatoliy Skorokhod\n\nThese are things named after Anatoliy Skorokhod (1930-2011), a Ukrainian mathematician.\n\n"}
{"id": "2817337", "url": "https://en.wikipedia.org/wiki?curid=2817337", "title": "Marcel-Paul Schützenberger", "text": "Marcel-Paul Schützenberger\n\nMarcel-Paul \"Marco\" Schützenberger (October 24, 1920 – July 29, 1996) was a French mathematician and Doctor of Medicine. He worked in the fields of formal language, combinatorics, and information theory. In addition to his formal results in mathematics, he was \"deeply involved in [a] struggle against the votaries of [neo-]Darwinism\", a stance which has resulted in some mixed reactions from his peers and from critics of his stance on evolution. Several notable theorems and objects in mathematics as well as computer science bear his name (for example Schutzenberger group or the Chomsky–Schützenberger hierarchy). Paul Schützenberger was his great-grandfather.\n\nIn the late 1940s, he was briefly married to the psychologist Anne Ancelin Schützenberger.\n\nSchützenberger's first doctorate, in medicine, was awarded in 1948 from the \"Faculté de Médecine de Paris\". His doctoral thesis, on the statistical study of gender at birth, was distinguished by the Baron Larrey Prize from the French Academy of Medicine.\n\nBiologist Jaques Besson, a co-author with Schützenberger on a biological topic, while noting that Schützenberger is perhaps most remembered for work in pure mathematical fields, credits him for likely being responsible for the introduction of statistical sequential analysis in French hospital practice.\n\nSchützenberger's second doctorate was awarded in 1953 from Université Paris III. This work, developed from earlier results is counted amongst the early influential French academic work in information theory. His later impact in both linguistics and combinatorics is reflected by two theorems in formal linguistics (the Chomsky–Schützenberger enumeration theorem and the Chomsky–Schützenberger representation theorem), and one in combinatorics (the Schützenberger theorem). With Alain Lascoux, Schützenberger is credited with the foundation of the notion of the plactic monoid, reflected in the name of the combinatorial structure called by some the Lascoux–Schützenberger tree.\n\nThe mathematician Dominique Perrin credited Schützenberger with \"deeply [influencing] the theory of semigroups\", and \"deep results on rational functions and transducers,\" amongst other contributions to mathematics.\n\n\n\n\n\n\nAfter his death, two journals in theoretical mathematics dedicated issues to Schützenberger's memory. He was commemorated in this manner by \"Theoretical Computer Science\" in 1998 and again by the \"International Journal of Algebra and Computation\" in 1999.\n\nThe mathematician David Berlinski provided this dedication in his 2000 book \"The Advent of The Algorithm: The Idea that Rules the World\": À la mémoire de mon ami . . M. P. Schützenberger, 1921-1996.\n\n\nFor the complete list of his papers, see: Papers\n\n\nThe Complete Works of Marcel-Paul Schützenberger: Complete Works\n\n\n"}
{"id": "32377002", "url": "https://en.wikipedia.org/wiki?curid=32377002", "title": "MaxDDBS", "text": "MaxDDBS\n\nThe Maximum Degree-and-Diameter-Bounded Subgraph problem (MaxDDBS) is a problem in graph theory. \n\nGiven a connected \"host graph G\", an upper bound for the degree \"d\", and an upper bound for the diameter \"k\", we look for the largest subgraph \"S\" of \"G\" with maximum degree at most \"d\" and diameter at most \"k\". This problem is also referred to as the Degree-Diameter Subgraph Problem, as it contains the degree diameter problem as a special case (namely, by taking a sufficiently large complete graph as a host graph). Despite being a natural generalization of the Degree-Diameter Problem, MaxDDBS only began to be investigated in 2011, while research in the Degree-Diameter Problem has been active since the 1960s. Regarding its computational complexity, the problem is NP-hard, and not in APX (i.e. it cannot be approximated to within a constant factor in polynomial time).\n\n"}
{"id": "81959", "url": "https://en.wikipedia.org/wiki?curid=81959", "title": "Metacharacter", "text": "Metacharacter\n\nA metacharacter is a character that has a special meaning to a computer program, such as a shell interpreter or a regular expression (regex) engine.\n\nIn POSIX extended regular expressions, there are 14 metacharacters that must be \"escaped\" (preceded by a backslash (\"\\\")) in order to drop their special meaning and be treated literally inside an expression: opening and closing square brackets (\"[\" and \"]\"); backslash (\"\\\"); caret (\"^\"); dollar sign (\"$\"); period/full stop/dot (\".\"); vertical bar/pipe symbol (\"|\"); question mark (\"?\"); asterisk (\"*\"); plus sign (\"+\"); opening and closing curly brackets/braces (\"{\" and \"}\"); and opening and closing parentheses (\"(\" and \")\").\n\nFor example, to match the arithmetic expression \"(1+1)*3=6\" with a regex, the correct regex is \"\\(1\\+1\\)\\*3=6\"; otherwise, the parentheses, plus sign, and asterisk will have special meanings.\n\nSome other characters may have special meaning in some environments.\n\nThe term \"to escape a metacharacter\" means to make the metacharacter ineffective (to strip it of its special meaning), causing it to have its literal meaning. For example, in PCRE, a dot (\".\") stands for any single character. The regular expression \"A.C\" will match \"ABC\", \"A3C\", or even \"A C\". However, if the \".\" is escaped, it will lose its meaning as a metacharacter and will be interpreted literally as \".\", causing the regular expression \"A\\.C\" to only match the string \"A.C\".\n\nThe usual way to escape a character in a regex and elsewhere is by prefixing it with a backslash (\"\\\"). Other environments may employ different methods, like MS-DOS/Windows Command Prompt, where a caret (\"^\") is used instead.\n\n"}
{"id": "52897329", "url": "https://en.wikipedia.org/wiki?curid=52897329", "title": "Multi-time-step integration", "text": "Multi-time-step integration\n\nIn numerical analysis, multi-time-step integration, also referred to as multiple-step or asynchronous time integration, is a numerical time-integration method that uses different time-steps or time-integrators for different parts of the problem. There are different approaches to multi-time-step integration. They are based on domain decompostition and can be classified into strong (monolithic) or weak (staggered) schemes. Using different time-steps or time-integrators in the context of a weak algorithm is rather straightforward, because the numerical solvers operate independently. However, this is not the case in a strong algorithm. In the past few years a number of research articles have addressed the development of strong multi-time-step algorithms. In either case, strong or weak, the numerical accuracy and stability needs to be carefully studied. Other approaches to multi-time-step integration in the context of operator splitting methods have also been developed; i.e., multi-rate GARK method and multi-step methods for molecular dynamics simulations.\n"}
{"id": "58343144", "url": "https://en.wikipedia.org/wiki?curid=58343144", "title": "Nissan Deliatitz", "text": "Nissan Deliatitz\n\nNissan ben Avraham Deliatitz () was a nineteenth-century Russian rabbi and mathematician.\n\nHe wrote \"Keneh Ḥokhmah\", a manual of algebra in five parts, published in Vilna and Grodno in 1829. The work received approbations from Rabbi David, the \"av beit din\" of Novhardok, and Rabbi Avraham Abele ben Avraham Shlomo Poswoler, an eminent scholar who headed the Vilna \"beit din\".\n"}
{"id": "13548016", "url": "https://en.wikipedia.org/wiki?curid=13548016", "title": "Orthogonal Procrustes problem", "text": "Orthogonal Procrustes problem\n\nThe orthogonal Procrustes problem is a matrix approximation problem in linear algebra. In its classical form, one is given two matrices formula_1 and formula_2 and asked to find an orthogonal matrix formula_3 which most closely maps formula_1 to formula_2. Specifically,\n\nwhere formula_7 denotes the Frobenius norm. This is a special case of Wahba's problem (with identical weights; instead of considering two matrices, in Wahba's problem the columns of the matrices are considered as individual vectors).\n\nThe name Procrustes refers to a bandit from Greek mythology who made his victims fit his bed by either stretching their limbs or cutting them off.\n\nThis problem was originally solved by Peter Schönemann in a 1964 thesis, and shortly after appeared in the journal Psychometrika. A proof appeared in 1998. \n\nThis problem is equivalent to finding the nearest orthogonal matrix to a given matrix formula_8. To find this orthogonal matrix formula_3, one uses the singular value decomposition\nto write\n\nOne proof depends on basic properties of the matrix inner product that induces the Frobenius norm:\n\nThere are a number of related problems to the classical orthogonal Procrustes problem. One might generalize it by seeking the closest matrix in which the columns are orthogonal, but not necessarily orthonormal. \n\nAlternately, one might constrain it by only allowing rotation matrices (i.e. orthogonal matrices with determinant 1, also known as special orthogonal matrices). In this case, one can write (using the above decomposition formula_17)\n\nwhere formula_19 is a modified formula_20, with the smallest singular value replaced by formula_21 (+1 or -1), and the other singular values replaced by 1, so that the determinant of R is guaranteed to be positive. For more information, see the Kabsch algorithm. \n\n"}
{"id": "2231589", "url": "https://en.wikipedia.org/wiki?curid=2231589", "title": "Pseudorandom ensemble", "text": "Pseudorandom ensemble\n\nIn cryptography, a pseudorandom ensemble is a family of variables meeting the following criteria:\n\nLet formula_1 be a uniform ensemble\nand formula_2 be an ensemble. The ensemble formula_3 is called pseudorandom if formula_3 and formula_5\nare indistinguishable in polynomial time.\n\n\n "}
{"id": "44977826", "url": "https://en.wikipedia.org/wiki?curid=44977826", "title": "Quantization commutes with reduction", "text": "Quantization commutes with reduction\n\nIn mathematics, more specifically in the context of geometric quantization, quantization commutes with reduction states that the space of global sections of a line bundle satisfying the quantization condition on the symplectic quotient of a compact symplectic manifold is the space of invariant sections of the line bundle.\n\nThis was conjectured in 1980s by Guillemin and Sternberg and was proven in 1990s by Meinrenken (the second paper used symplectic cut) as well as Tian and Zhang. For the formulation due to Teleman, see C. Woodward's notes.\n\n\n"}
{"id": "46731998", "url": "https://en.wikipedia.org/wiki?curid=46731998", "title": "Quasi-Newton least squares method", "text": "Quasi-Newton least squares method\n\nIn numerical analysis, the quasi-Newton least squares method is a quasi-Newton method for finding roots in variables. It was originally described by Rob Haelterman et al. in 2009.\n\nNewton's method for solving uses the Jacobian matrix, , at every iteration. However, computing this Jacobian is a difficult (sometimes even impossible) and expensive operation. The idea behind the quasi-Newton least squares method is to build up an approximate Jacobian based on known input–output pairs of the function .\n\nHaelterman et al. also showed that when the quasi-Newton least squares method is applied to a linear system of size , it converges in at most steps, although like all quasi-Newton methods, it may not converge for nonlinear systems.\n\nThe method is closely related to the quasi-Newton inverse least squares method.\n"}
{"id": "2324711", "url": "https://en.wikipedia.org/wiki?curid=2324711", "title": "Rogers–Ramanujan identities", "text": "Rogers–Ramanujan identities\n\nIn mathematics, the Rogers–Ramanujan identities are two identities related to basic hypergeometric series, first discovered and proved by . They were subsequently rediscovered (without a proof) by Srinivasa Ramanujan some time before 1913. Ramanujan had no proof, but rediscovered Rogers's paper in 1917, and they then published a joint new proof . independently rediscovered and proved the identities.\n\nThe Rogers–Ramanujan identities are\n\nand\n\nHere, formula_3 denotes the q-Pochhammer symbol.\n\nConsider the following:\n\nThe Rogers–Ramanujan identities could be now interpreted in the following way. Let formula_5 be a non-negative integer.\n\nAlternatively,\n\nIf \"q\" = e, then \"q\"\"G\"(\"q\") and \"q\"\"H\"(\"q\") are modular functions of τ.\n\nThe Rogers–Ramanujan identities appeared in Baxter's solution of the hard hexagon model in statistical mechanics.\n\nRamanujan's continued fraction is \n\nJames Lepowsky and Robert Lee Wilson were the first to prove Rogers–Ramanujan identities using completely representation-theoretic techniques. They proved these identities using level 3 modules for the affine Lie algebra formula_24. In the course of this proof they invented and used what they called formula_25-algebras. \nLepowsky and Wilson's approach is universal, in that it is able to treat all affine Lie algebras at all levels.\nIt can be used to find (and prove) new partition identities. \nFirst such example is that of Capparelli's identities discovered by Stefano Capparelli using level 3 modules for \nthe affine Lie algebra formula_26.\n\n\n\n"}
{"id": "1073567", "url": "https://en.wikipedia.org/wiki?curid=1073567", "title": "Sigma additivity", "text": "Sigma additivity\n\nIn mathematics, additivity and sigma additivity (also called countable additivity) of a function defined on subsets of a given set are abstractions of the intuitive properties of size (length, area, volume) of a set.\n\nLet \"formula_1\" be a function defined on an algebra of sets formula_2 with values in [−∞, +∞] (see the extended real number line). The function formula_1 is called additive, or finitely additive, if, whenever \"A\" and \"B\" are disjoint sets in formula_2, one has\n\nOne can prove by mathematical induction that an additive function satisfies\n\nfor any formula_7 disjoint sets in formula_2.\n\nSuppose that formula_2 is a σ-algebra. If for any sequence formula_10 of pairwise disjoint sets in formula_2, one has\nwe say that μ is countably additive or σ-additive. \nAny σ-additive function is additive but not vice versa, as shown below.\n\nSuppose that in addition to a sigma algebra formula_2, we have a topology τ. If for any directed family of measurable open sets formula_14⊆formula_2∩τ,\nwe say that μ is τ-additive. In particular, if μ is inner regular (with respect to compact sets) then it is τ-additive.\n\nUseful properties of an additive function μ include the following:\n\nAn example of a σ-additive function is the function μ defined over the power set of the real numbers, such that \n\nIf formula_18 is a sequence of disjoint sets of real numbers, then either none of the sets contains 0, or precisely one of them does. In either case, the equality \nholds.\n\nSee measure and signed measure for more examples of σ-additive functions.\n\nAn example of an additive function which is not σ-additive is obtained by considering μ, defined over the Lebesgue sets of the real numbers by the formula\n\nwhere \"λ\" denotes the Lebesgue measure and \"lim\" the Banach limit.\n\nOne can check that this function is additive by using the linearity of the limit. That this function is not σ-additive follows by considering the sequence of disjoint sets\nfor \"n\"=0, 1, 2, ... The union of these sets is the positive reals, and μ applied to the union is then one, while μ applied to any of the individual sets is zero, so the sum of μ(\"A\") is also zero, which proves the counterexample.\n\nOne may define additive functions with values in any additive monoid (for example any group or more commonly a vector space). For sigma-additivity, one needs in addition that the concept of limit of a sequence be defined on that set. For example, spectral measures are sigma-additive functions with values in a Banach algebra. Another example, also from quantum mechanics, is the positive operator-valued measure.\n\n"}
{"id": "3928533", "url": "https://en.wikipedia.org/wiki?curid=3928533", "title": "Spherical law of cosines", "text": "Spherical law of cosines\n\nIn spherical trigonometry, the law of cosines (also called the cosine rule for sides) is a theorem relating the sides and angles of spherical triangles, analogous to the ordinary law of cosines from plane trigonometry.\nGiven a unit sphere, a \"spherical triangle\" on the surface of the sphere is defined by the great circles connecting three points , and on the sphere (shown at right). If the lengths of these three sides are (from to (from to ), and (from to ), and the angle of the corner opposite is , then the (first) spherical law of cosines states:\n\nSince this is a unit sphere, the lengths , and are simply equal to the angles (in radians) subtended by those sides from the center of the sphere. (For a non-unit sphere, the lengths are the subtended angles times the radius, and the formula still holds if and are reinterpreted as the subtended angles). As a special case, for , then , and one obtains the spherical analogue of the Pythagorean theorem:\n\nIf the law of cosines is used to solve for , the necessity of inverting the cosine magnifies rounding errors when is small. In this case, the alternative formulation of the law of haversines is preferable.\n\nA variation on the law of cosines, the second spherical law of cosines, (also called the cosine rule for angles) states:\n\nwhere and are the angles of the corners opposite to sides and , respectively. It can be obtained from consideration of a spherical triangle dual to the given one.\n\nLet , and denote the unit vectors from the center of the sphere to those corners of the triangle. The angles and distances do not change if the coordinate system is rotated, so we can rotate the coordinate system so that formula_4 is at the north pole and formula_5 is somewhere on the prime meridian (longitude of 0). With this rotation, the spherical coordinates for formula_5 are formula_7, where \"θ\" is the angle measured from the north pole not from the equator, and the spherical coordinates for formula_8 are formula_9. The Cartesian coordinates for formula_5 are formula_11 and the Cartesian coordinates for formula_8 are formula_13. The value of formula_14 is the dot product of the two Cartesian vectors, which is formula_15.\n\nLet , and denote the unit vectors from the center of the sphere to those corners of the triangle. We have , , , and . The vectors and have lengths and respectively and the angle between them is , so\nusing cross products, dot products, and the Binet–Cauchy identity .\n\nThe first and second spherical laws of cosines can be rearranged to put the sides () and angles () on opposite sides of the equations:\n\nFor \"small\" spherical triangles, i.e. for small , and , the spherical law of cosines is approximately the same as the ordinary planar law of cosines,\n\nTo prove this, we will use the small-angle approximation obtained from the Maclaurin series for the cosine and sine functions:\n\nSubstituting these expressions into the spherical law of cosines nets:\n\nor after simplifying:\n\nThe big O terms for and are dominated by as and get small, so we can write this last expression as:\n\n"}
{"id": "47704642", "url": "https://en.wikipedia.org/wiki?curid=47704642", "title": "St-planar graph", "text": "St-planar graph\n\nIn graph theory, an \"st\"-planar graph is a bipolar orientation of a plane graph for which both the source and the sink of the orientation are on the outer face of the graph. That is, it is a directed graph drawn without crossings in the plane, in such a way that there are no directed cycles in the graph, exactly one graph vertex has no incoming edges, exactly one graph vertex has no outgoing edges, and these two special vertices both lie on the outer face of the graph.\n\nWithin the drawing, each face of the graph must have the same structure: there is one vertex that acts as the source of the face, one vertex that acts as the sink of the face, and all edges within the face are directed along two paths from the source to the sink. If one draws an additional edge from the sink of an \"st\"-planar graph back to the source, through the outer face, and then constructs the dual graph (oriented each dual edge clockwise with respect to its primal edge) then the result is again an \"st\"-planar graph, augmented with an extra edge in the same way.\n\nThese graphs are closely related to partially ordered sets and lattices. The Hasse diagram of a partially ordered set is a directed acyclic graph whose vertices are the set elements, with an edge from \"x\" to \"y\" for each pair \"x\", \"y\" of elements for which \"x\" ≤ \"y\" in the partial order but for which there does not exist \"z\" with \"x\" ≤ \"y\" ≤ \"z\".\nA partially ordered set forms a complete lattice if and only if every subset of elements has a unique greatest lower bound and a unique least upper bound, and the order dimension of a partially ordered set is the least number of total orders on the same set of elements whose intersection is the given partial order.\nIf the vertices of an \"st\"-planar graph are partially ordered by reachability, then this ordering always forms a two-dimensional complete lattice, whose Hasse diagram is the transitive reduction of the given graph. Conversely, the Hasse diagram of every two-dimensional complete lattice is always an \"st\"-planar graph.\n\nBased on this two-dimensional partial order property, every \"st\"-planar graph can be given a dominance drawing, in which for every two vertices \"u\" and \"v\" there exists a path from \"u\" to \"v\" if and only if both coordinates of \"u\" are smaller than the corresponding coordinates of \"v\". The coordinates of such a drawing may also be used as a data structure that can be used to test whether one vertex of an \"st\"-planar graph can reach another in constant time per query. Rotating such a drawing by 45° gives an upward planar drawing of the graph. A directed acyclic graph \"G\" has an upward planar drawing if and only if \"G\" is a subgraph of an \"st\"-planar graph.\n"}
{"id": "40966970", "url": "https://en.wikipedia.org/wiki?curid=40966970", "title": "Stable model category", "text": "Stable model category\n\nIn category theory, a branch of mathematics, a stable model category is a pointed model category in which the suspension functor is an equivalence of the homotopy category with itself.\n\nThe prototypical examples are the category of spectra in the stable homotopy theory and the category of chain complex of \"R\"-modules. On the other hand, the category of pointed topological spaces and the category of pointed simplicial sets are not stable model categories.\n\nAny stable model category is equivalent to a category of presheaves of spectra.\n\n"}
{"id": "4499769", "url": "https://en.wikipedia.org/wiki?curid=4499769", "title": "Stanisław Gołąb", "text": "Stanisław Gołąb\n\nStanisław Gołąb (July 26, 1902 – April 30, 1980) was a Polish mathematician from Kraków, working in particular on the field of affine geometry.\n\nIn 1932, he proved that the perimeter of the unit disc respect to a given metric can take any value in between 6 and 8, and that these extremal values are obtained if and only if the unit disc is an affine regular hexagon resp. a parallelogram.\n\n\n"}
{"id": "8729683", "url": "https://en.wikipedia.org/wiki?curid=8729683", "title": "Stirling numbers and exponential generating functions in symbolic combinatorics", "text": "Stirling numbers and exponential generating functions in symbolic combinatorics\n\nThe use of exponential generating functions (EGFs) to study the properties of Stirling numbers is a classical exercise in combinatorial mathematics and possibly the canonical example of how symbolic combinatorics is used. It also illustrates the parallels in the construction of these two types of numbers, lending support to the binomial-style notation that is used for them.\n\nThis article uses the coefficient extraction operator formula_1 for formal power series, as well as the (labelled) operators formula_2 (for cycles) and formula_3 (for sets) on combinatorial classes, which are explained on the page for symbolic combinatorics. Given a combinatorial class, the cycle operator creates the class obtained by placing objects from the source class along a cycle of some length, where cyclical symmetries are taken into account, and the set operator creates the class obtained by placing objects from the source class in a set (symmetries from the symmetric group, i.e. an \"unstructured bag\".) The two combinatorial classes (shown without additional markers) are\n\nand\n\nwhere formula_6 is the singleton class.\n\nWarning: The notation used here for the Stirling numbers is not that of the Wikipedia articles on Stirling numbers; square brackets denote the signed Stirling numbers here.\n\nThe unsigned Stirling numbers of the first kind count the number of permutations of [\"n\"] with \"k\" cycles. A permutation is a set of cycles, and hence the set formula_7 of permutations is given by\n\nwhere the singleton formula_9 marks cycles. This decomposition is examined in some detail on the page on the statistics of random permutations.\n\nTranslating to generating functions we obtain the mixed generating function of the unsigned Stirling numbers of the first kind:\n\nNow the signed Stirling numbers of the first kind are obtained from the unsigned ones through the relation\nHence the generating function formula_12 of these numbers is\n\nA variety of identities may be derived by manipulating this generating function:\n\nIn particular, the order of summation may be exchanged, and derivatives taken, and then \"z\" or \"u\" may be fixed.\n\nA simple sum is\n\nThis formula holds because the exponential generating function of the sum is\n\nSome infinite sums include\n\nwhere formula_18 (the singularity nearest to formula_19\nof formula_20 is at formula_21)\n\nThis relation holds because\n\nThese numbers count the number of partitions of [\"n\"] into \"k\" nonempty subsets. First consider the total number of partitions, i.e. \"B\" where\n\ni.e. the Bell numbers. The Flajolet–Sedgewick fundamental theorem applies (labelled case).\nThe set formula_24 of partitions into non-empty subsets is given by (\"set of non-empty sets of singletons\")\n\nThis decomposition is entirely analogous to the construction of the set formula_7 of permutations from cycles, which is given by\n\nand yields the Stirling numbers of the first kind. Hence the name \"Stirling numbers of the second kind.\"\n\nThe decomposition is equivalent to the EGF\n\nDifferentiate to obtain\n\nwhich implies that\n\nby convolution of exponential generating functions and because differentiating an EGF drops the first coefficient and shifts \"B\" to \"z\"/\"n\"<nowiki>!</nowiki>.\n\nThe EGF of the Stirling numbers of the second kind is obtained by marking every subset that goes into the partition with the term formula_31, giving\n\nTranslating to generating functions, we obtain\n\nThis EGF yields the formula for the Stirling numbers of the second kind:\n\nor\n\nwhich simplifies to\n\n\n"}
{"id": "41355731", "url": "https://en.wikipedia.org/wiki?curid=41355731", "title": "The Game of Logic", "text": "The Game of Logic\n\nThe Game of Logic is a book written by Lewis Carroll, published in 1886. In addition to his well-known children's literature, Carroll was an academic mathematician who worked in mathematical logic. The book describes, in an informal and playful style, the use of a board game to represent logical propositions and inferences. Carroll incorporated the game into a longer and more formal introductory logic textbook titled \"Symbolic Logic\", published in 1897. The books are sometimes reprinted in a single volume.\n\n\n"}
{"id": "36044548", "url": "https://en.wikipedia.org/wiki?curid=36044548", "title": "Tompkins–Paige algorithm", "text": "Tompkins–Paige algorithm\n\nThe Tompkins–Paige algorithm is a computer algorithm for generating all permutations of a finite set of objects.\n\nLet \"P\" and \"c\" be arrays of length \"n\" with 1-based indexing (i.e. the first entry of an array has index 1). The algorithm for generating all \"n\"! permutations of the set {1,2...,\"n\"} is given by the following pseudocode:\n\nIn the above pseudocode, the statement \"yield \"P\"\" means to output or record the set of permuted indices \"P\". If the algorithm is implemented correctly, \"P\" will be yielded exactly \"n\"! times, each with a different set of permuted indices.\n\nThis algorithm is not the most efficient one among all existing permutation generation methods. Not only does it have to keep track of an auxiliary counting array (\"c\"), redundant permutations are also produced and ignored (because \"P\" is not yielded after left-rotation if \"c\"[\"i\"] ≥ \"i\") in the course of generation. For instance, when \"n\" = 4, the algorithm will first yield \"P\" = [1,2,3,4] and then generate the other 23 permutations in 40 iterations (i.e. in 17 iterations, there are redundant permutations and \"P\" is not yielded). The following lists, in the order of generation, all 41 values of \"P\", where the parenthesized ones are redundant:\n"}
{"id": "18204005", "url": "https://en.wikipedia.org/wiki?curid=18204005", "title": "Ulrich Kohlenbach", "text": "Ulrich Kohlenbach\n\nUlrich Wilhelm Kohlenbach (born July 27, 1962 in Frankfurt am Main) is a German professor of mathematics and a researcher in logic. His research interests lie in the field of proof mining.\n\nHe graduated ('Abitur') from Lessing-Gymnasium (High School) in 1980 and completed his studies of mathematics, philosophy, and linguistics with a master's degree ('Diplom') from the University of Frankfurt. In 1990, he received his Ph.D. under H. Luckhardt and passed his Habilitation ('venia legendi') in mathematics five years later at the University of Frankfurt. In 1998, he became an associate professor at the University of Aarhus in Denmark where he worked until 2004. Kohlenbach is now a full professor at the Technische Universität Darmstadt. He is married to Gabriele Bahl-Kohlenbach with whom he has a daughter. He is an invited speaker at the 2018 International Conference of Mathematicians in Rio. \n"}
{"id": "4166617", "url": "https://en.wikipedia.org/wiki?curid=4166617", "title": "Warren Ambrose", "text": "Warren Ambrose\n\nWarren Arthur Ambrose (October 25, 1914 – December 4, 1995) was Professor Emeritus of Mathematics at the Massachusetts Institute of Technology and at the University of Buenos Aires.\n\nHe was born in Virden, Illinois in 1914. He received his bachelor of science degree in 1935, his master's in 1936 and his Ph.D. in 1939, all from the University of Illinois at Urbana-Champaign.\n\nWarren Ambrose was a food and wine connoisseur, and also a fan of jazz saxophone player, Charlie \"Bird\" Parker. He is noted for his work with MIT colleague, Isadore Singer, both of whom helped to shape the pure mathematics department at MIT. He retired from teaching at MIT in 1985, thereafter moving to France. Ambrose died in 1995 in Paris. He is survived by his wife, Jeannette (Grillet) Ambrose of Paris, two children from an earlier marriage, Adam Ambrose of Bisbee, AZ, and Ellen Ambrose of Laurel, MD, and four grandchildren, David and Adam Holzsager, Ari Ambrose, and Jennifer Laurent.\n\nAmbrose became an assistant professor at MIT in 1947, an associate professor in 1950 and full professor in 1957. He was several times between 1939 and 1959 a visiting scholar at the Institute for Advanced Study in Princeton, New Jersey. Ambrose is often considered one of the fathers of modern geometry. He is noted for making changes in the pure mathematics undergraduate curriculum at MIT to reflect recent findings in differential geometry. For example, less than ten years after André Weil presented the differential form, Ambrose was using it in his undergraduate differential geometry courses. In the 1950s, Ambrose (together with I. M. Singer) made MIT into the only center in geometry in the United States outside the University of Chicago. His influence continued through his students, in particular Hung-Hsi Wu and John Rhodes, both of the University of California, Berkeley. In the 1960s Ambrose was a visiting professor at the University of Buenos Aires, Argentina. He is noted for his opposition of takeover of South American countries military regimes, specifically Argentina.\n\nIn the summer of 1966, while a visiting professor at the University of Buenos Aires in Argentina, Ambrose was severely beaten along with Argentinian faculty members and students by Argentinian military police. This occurred shortly after a military regime took over public universities in Argentina. Ambrose responded by bringing several of the best and brightest students from the University of Buenos Aires back to MIT with him. Ambrose is often renowned amongst Latin American intellectuals for bringing attention to right-wing dictatorships in South America.\n\nIn 1967, Ambrose signed a letter declaring his intention to refuse to pay taxes in protest against the U.S. war against Vietnam, and urging other people to also take this stand.\n"}
