{"id": "313741", "url": "https://en.wikipedia.org/wiki?curid=313741", "title": "3D projection", "text": "3D projection\n\n3D projection is any method of mapping three-dimensional points to a two-dimensional plane. As most current methods for displaying graphical data are based on planar (pixel information from several bitplanes) two-dimensional media, the use of this type of projection is widespread, especially in computer graphics, engineering and drafting.\n\nWhen the human eye looks at a scene, objects in the distance appear smaller than objects close by. Orthographic projection ignores this effect to allow the creation of to-scale drawings for construction and engineering.\n\nOrthographic projections are a small set of transforms often used to show profile, detail or precise measurements of a three dimensional object. Common names for orthographic projections include plane, cross-section, bird's-eye, and elevation.\n\nIf the normal of the viewing plane (the camera direction) is parallel to one of the primary axes (which is the \"x\", \"y\", or \"z\" axis), the mathematical transformation is as follows;\nTo project the 3D point formula_1, formula_2, formula_3 onto the 2D point formula_4, formula_5 using an orthographic projection parallel to the y axis (where positive \"y\" represents forward direction - profile view), the following equations can be used:\nwhere the vector s is an arbitrary scale factor, and c is an arbitrary offset. These constants are optional, and can be used to properly align the viewport. Using matrix multiplication, the equations become:\n\nWhile orthographically projected images represent the three dimensional nature of the object projected, they do not represent the object as it would be recorded photographically or perceived by a viewer observing it directly. In particular, parallel lengths at all points in an orthographically projected image are of the same scale regardless of whether they are far away or near to the virtual viewer. As a result, lengths are not foreshortened as they would be in a perspective projection.\n\nA \"weak\" perspective projection uses the same principles of an orthographic projection, but requires the scaling factor to be specified, thus ensuring that closer objects appear bigger in the projection, and vice versa. It can be seen as a hybrid between an orthographic and a perspective projection, and described either as a perspective projection with individual point depths formula_9 replaced by an average constant depth formula_10, or simply as an orthographic projection plus a scaling.\n\nThe weak-perspective model thus approximates perspective projection while using a simpler model, similar to the pure (unscaled) orthographic perspective.\nIt is a reasonable approximation when the depth of the object along the line of sight is small compared to the distance from the camera, and the field of view is small. With these conditions, it can be assumed that all points on a 3D object are at the same distance formula_10 from the camera without significant errors in the projection (compared to the full perspective model).\n\nEquation\nassuming focal length formula_13.\n\nWhen the human eye views a scene, objects in the distance appear smaller than objects close by - this is known as perspective. While orthographic projection ignores this effect to allow accurate measurements, perspective projection shows distant objects as smaller to provide additional realism.\n\nThe perspective projection requires a more involved definition as compared to orthographic projections. A conceptual aid to understanding the mechanics of this projection is to imagine the 2D projection as though the object(s) are being viewed through a camera viewfinder. The camera's position, orientation, and field of view control the behavior of the projection transformation. The following variables are defined to describe this transformation:\nWhich results in:\n\nWhen formula_20 and formula_21 the 3D vector formula_22 is projected to the 2D vector formula_23.\n\nOtherwise, to compute formula_18 we first define a vector formula_25 as the position of point \"A\" with respect to a coordinate system defined by the camera, with origin in \"C\" and rotated by formula_26 with respect to the initial coordinate system. This is achieved by subtracting formula_27 from formula_19 and then applying a rotation by formula_29 to the result. This transformation is often called a , and can be expressed as follows, expressing the rotation in terms of rotations about the \"x,\" \"y,\" and \"z\" axes (these calculations assume that the axes are ordered as a left-handed system of axes):\nThis representation corresponds to rotating by three Euler angles (more properly, Tait–Bryan angles), using the \"xyz\" convention, which can be interpreted either as \"rotate about the \"extrinsic\" axes (axes of the \"scene\") in the order \"z, y, x\" (reading right-to-left)\" or \"rotate about the \"intrinsic\" axes (axes of the \"camera\") in the order \"x, y, z\" (reading left-to-right)\". Note that if the camera is not rotated (formula_31), then the matrices drop out (as identities), and this reduces to simply a shift: formula_32\n\nAlternatively, without using matrices (let's replace (a-c) with x and so on, and abbreviate cosθ to \"c\" and sinθ to \"s\"):\nThis transformed point can then be projected onto the 2D plane using the formula (here, \"x/y\" is used as the projection plane; literature also may use \"x/z\"):\n\nOr, in matrix form using homogeneous coordinates, the system\nin conjunction with an argument using similar triangles, leads to division by the homogeneous coordinate, giving\n\nThe distance of the viewer from the display surface, formula_37, directly relates to the field of view, where formula_38 is the viewed angle. (Note: This assumes that you map the points (-1,-1) and (1,1) to the corners of your viewing surface)\n\nThe above equations can also be rewritten as:\nIn which formula_40 is the display size, formula_41 is the recording surface size (CCD or film), formula_42 is the distance from the recording surface to the entrance pupil (camera center), and formula_43 is the distance, from the 3D point being projected, to the entrance pupil.\n\nSubsequent clipping and scaling operations may be necessary to map the 2D plane onto any particular display media.\n\nTo determine which screen \"x\"-coordinate corresponds to a point at formula_44 multiply the point coordinates by:\nwhere \nBecause the camera is in 3D, the same works for the screen \"y\"-coordinate, substituting \"y\" for \"x\" in the above diagram and equation.\n\n\n\n"}
{"id": "283867", "url": "https://en.wikipedia.org/wiki?curid=283867", "title": "Algebraic data type", "text": "Algebraic data type\n\nIn computer programming, especially functional programming and type theory, an algebraic data type is a kind of composite type, i.e., a type formed by combining other types.\n\nTwo common classes of algebraic types are product types (i.e., tuples and records) and sum types (i.e., tagged or disjoint unions or \"variant types\").\n\nThe values of a product type typically contain several values, called \"fields\". All values of that type have the same combination of field types. The set of all possible values of a product type is the set-theoretic product, i.e., the Cartesian product, of the sets of all possible values of its field types.\n\nThe values of a sum type are typically grouped into several classes, called \"variants\". A value of a variant type is usually created with a quasi-functional entity called a \"constructor\". Each variant has its own constructor, which takes a specified number of arguments with specified types. The set of all possible values of a sum type is the set-theoretic sum, i.e., the disjoint union, of the sets of all possible values of its variants. Enumerated types are a special case of sum types in which the constructors take no arguments, as exactly one value is defined for each constructor.\n\nValues of algebraic types are analyzed with pattern matching, which identifies a value by its constructor or field names and extracts the data it contains.\n\nAlgebraic data types were introduced in Hope, a small functional programming language developed in the 1970s at the University of Edinburgh.\n\nOne of the most common examples of an algebraic data type is the singly linked list. A list type is a sum type with two variants, codice_1 for an empty list and codice_2 for the combination of a new element \"x\" with a list \"xs\" to create a new list. Here is an example of how a singly linked list would be declared in Haskell:\n\ncodice_3 is an abbreviation of \"cons\"truct. Many languages have special syntax for lists defined in this way. For example, Haskell and ML use codice_4 for codice_1, codice_6 or codice_7 for codice_3, respectively, and square brackets for entire lists. So codice_9 would normally be written as codice_10 or codice_11 in Haskell, or as codice_12 or codice_13 in ML.\n\nFor a slightly more complex example, binary trees may be implemented in Haskell as follows:\nHere, codice_14 represents an empty tree, codice_15 contains a piece of data, and codice_16 organizes the data into branches.\n\nIn most languages that support algebraic data types, it is possible to define parametric types. Examples are given later in this article.\n\nSomewhat similar to a function, a data constructor is applied to arguments of an appropriate type, yielding an instance of the data type to which the type constructor belongs. For example, the data constructor codice_15 is logically a function codice_18, meaning that giving an integer as an argument to codice_15 produces a value of the type codice_20. As codice_16 takes two arguments of the type codice_20 itself, the datatype is recursive.\n\nOperations on algebraic data types can be defined by using pattern matching to retrieve the arguments. For example, consider a function to find the depth of a codice_20, given here in Haskell:\n\nThus, a codice_20 given to codice_25 can be constructed using any of codice_14, codice_15, or codice_16 and must be matched for any of them respectively to deal with all cases. In case of codice_16, the pattern extracts the subtrees codice_30 and codice_31 for further processing.\n\nAlgebraic data types are highly suited to implementing abstract syntax. For example, the following algebraic data type describes a simple language representing numerical expressions:\n\nAn element of such a data type would have a form such as codice_32.\n\nWriting an evaluation function for this language is a simple exercise; however, more complex transformations also become feasible. For example, an optimization pass in a compiler might be written as a function taking an abstract expression as input and returning an optimized form.\n\nWhat is happening is that there is a datatype which can be \"one of several types of things\". Each \"type of thing\" is associated with an identifier called a \"constructor\", which can be viewed as a kind of tag for that kind of data. Each constructor can carry with it a different type of data. A constructor could carry no data (e.g., \"Empty\" in the example above), or one piece of data (e.g., “Leaf” has one Int value), or multiple pieces of data (e.g., “Node” has two Tree values).\n\nTo do something with a value of this Tree algebraic data type, it is \"deconstructed\" using a process termed \"pattern matching\". It involves \"matching\" the data with a series of \"patterns\". The example function \"depth\" above pattern-matches its argument with three patterns. When the function is called, it finds the first pattern that matches its argument, performs any variable bindings that are found in the pattern, and evaluates the expression corresponding to the pattern.\n\nEach pattern above has a form that resembles the structure of some possible value of this datatype. The first pattern simply matches values of the constructor \"Empty\". The second pattern matches values of the constructor \"Leaf\". Patterns are recursive, so then the data that is associated with that constructor is matched with the pattern \"n\". In this case, a lowercase identifier represents a pattern that matches any value, which then is bound to a variable of that name — in this case, a variable “codice_33” is bound to the integer value stored in the data type — to be used in the expression to evaluate.\n\nThe recursion in patterns in this example are trivial, but a possible more complex recursive pattern would be something like codice_34. Recursive patterns several layers deep are used for example in balancing red-black trees, which involve cases that require looking at colors several layers deep.\n\nThe example above is operationally equivalent to the following pseudocode:\n\nThe comparison of this with pattern matching will point out some of the advantages of algebraic data types and pattern matching. The first advantage is type safety. The pseudocode above relies on the diligence of the programmer to not access field2 when the constructor is a Leaf, for example. Also, the type of field1 is different for Leaf and Node (for Leaf it is Int; for Node it is Tree), so the type system would have difficulties assigning a static type to it in a safe way in a traditional record data structure. However, in pattern matching, the type of each extracted value is checked based on the types declared by the relevant constructor, and how many values can be extracted is known based on the constructor, so it does not face these problems.\n\nSecond, in pattern matching, the compiler statically checks that all cases are handled. If one of the cases of the \"depth\" function above were missing, the compiler would issue a warning, indicating that a case is not handled. This task may seem easy for the simple patterns above, but with many complex recursive patterns, the task becomes difficult for the average human (or compiler, if it must check arbitrary nested if-else constructs) to handle. Similarly, there may be patterns which never match (i.e., are already covered by prior patterns), and the compiler can also check and issue warnings for these, as they may indicate an error in reasoning.\n\nDo not confuse these patterns with regular expression patterns used in string pattern matching. The purpose is similar: to check whether a piece of data matches certain constraints, and if so, extract relevant parts of it for processing. However, the mechanism is very different. This kind of pattern matching on algebraic data types matches on the structural properties of an object rather than on the character sequence of strings.\n\nA general algebraic data type is a possibly recursive sum type of product types. Each constructor tags a product type to separate it from others, or if there is only one constructor, the data type is a product type. Further, the parameter types of a constructor are the factors of the product type. A parameterless constructor corresponds to the empty product. If a datatype is recursive, the entire sum of products is wrapped in a recursive type, and each constructor also rolls the datatype into the recursive type.\n\nFor example, the Haskell datatype:\n\nis represented in type theory as\nformula_1\nwith constructors formula_2 and formula_3.\n\nThe Haskell List datatype can also be represented in type theory in a slightly different form, thus:\nformula_4.\n(Note how the formula_5 and formula_6 constructs are reversed relative to the original.) The original formation specified a type function which body was a recursive type. The revised version specifies a recursive function on types. (The type variable formula_7 is used to suggest a function rather than a \"base type\" like formula_8, since formula_7 is like a Greek \"f\".) The function must also now be applied formula_7 to its argument type formula_11 in the body of the type.\n\nFor the purposes of the List example, these two formulations are not significantly different; but the second form allows expressing so-called nested data types, i.e., those where the recursive type differs parametrically from the original. (For more information on nested data types, see the works of Richard Bird, Lambert Meertens, and Ross Paterson.)\n\nIn set theory the equivalent of a sum type is a disjoint union, a set which elements are pairs consisting of a tag (equivalent to a constructor) and an object of a type corresponding to the tag (equivalent to the constructor arguments).\n\nMany programming languages incorporate algebraic data types as a first class notion, including:\n"}
{"id": "1799103", "url": "https://en.wikipedia.org/wiki?curid=1799103", "title": "All-pairs testing", "text": "All-pairs testing\n\nIn computer science, all-pairs testing or pairwise testing is a combinatorial method of software testing that, for \"each pair\" of input parameters to a system (typically, a software algorithm), tests all possible discrete combinations of those parameters. Using carefully chosen test vectors, this can be done much faster than an exhaustive search of all combinations of all parameters, by \"parallelizing\" the tests of parameter pairs.\n\nThe most common bugs in a program are generally triggered by either a single input parameter or an interaction between pairs of parameters. Bugs involving interactions between three or more parameters are both progressively less common and also progressively more expensive to find---such testing has as its limit the testing of all possible inputs. Thus, a combinatorial technique for picking test cases like all-pairs testing is a useful cost-benefit compromise that enables a significant reduction in the number of test cases without drastically compromising functional coverage.\n\nMore rigorously, if we assume that a test case has formula_1 parameters given in a set formula_2.\nThe range of the parameters are given by formula_3.\nLet's assume that formula_4.\nWe note that the number of all possible test cases is a formula_5. Imagining that the code deals with the conditions taking only two parameters at a time, might reduce the number of needed test cases.\n\nTo demonstrate, suppose there are X,Y,Z parameters.\nWe can use a predicate of the form formula_6 of order 3, which takes all 3 as input, or rather three different order 2 predicates of the form formula_7. formula_8 can be written in an equivalent form of formula_9 where comma denotes any combination. If the code is written as conditions taking \"pairs\" of parameters,\nthen the set of choices of ranges formula_10 can be a multiset, because there can be multiple parameters having same number of choices.\n\nformula_11 is one of the maximum of the multiset formula_12\nThe number of pair-wise test cases on this test function would be:-\nformula_13\n\nTherefore, if the formula_14 and formula_15 then the number of tests is typically O(\"nm\"), where \"n\" and \"m\" are the number of possibilities for each of the two parameters with the most choices, and it can be quite a lot less than the exhaustive formula_5·\n\nN-wise testing can be considered the generalized form of pair-wise testing.\n\nThe idea is to apply sorting to the set formula_10 so that formula_18 gets ordered too.\nLet the sorted set be a formula_1 tuple :-\n\nformula_20\n\nNow we can take the set formula_21 and call it the pairwise testing.\nGeneralizing further we can take the set formula_22 and call it the 3-wise testing.\nEventually, we can say formula_23 T-wise testing.\n\nThe N-wise testing then would just be, all possible combinations from the above formula.\n\nConsider the parameters shown in the table below.\n\n'Enabled', 'Choice Type' and 'Category' have a choice range of 2, 3 and 4, respectively. An exhaustive test would involve 24 tests (2 x 3 x 4). Multiplying the two largest values (3 and 4) indicates that a pair-wise tests would involve 12 tests. The pict tool generated pairwise test cases is shown below.\n\n\n"}
{"id": "50494257", "url": "https://en.wikipedia.org/wiki?curid=50494257", "title": "Annalisa Buffa", "text": "Annalisa Buffa\n\nAnnalisa Buffa (14 February 1973) is an Italian mathematician, specializing in numerical analysis and PDEs.\n\nBuffa received her master's degree in computer engineering in 1996 and in 2000 her Ph.D., with supervisor Franco Brezzi, from the University of Milan with thesis \"Some numerical and theoretical problems in computational electromagnetism\". She was from 2001 to 2004 a Researcher, from 2004 to 2013 a Research Director (rank equivalent to Professor), and from 2013 to 2016 she was the Director at the Istituto di matematica applicata e tecnologie informatiche \"E. Magenes\" (IMATI) of the CNR in Pavia. From 2016 to present she is Professor of Mathematics and holds the Chair of Numerical Modeling and Simulation at EPFL.\n\nShe has been a visiting scholar at many institutions, including the at the University of Paris VI, the École Polytechnique, the ETH Zürich, and the University of Texas at Austin (Institute for Computational Engineering and Sciences, ICES).\n\nBuffa's research deals with a wide range of topics in PDEs and numerical analysis: \"isogeometric analysis, fully compatible discretization of PDEs, linear and non linear elasticity, contact mechanics, integral equations on non-smooth manifolds, functional theory for Maxwell equations in non-smooth domains, finite element techniques for Maxwell equations, non-conforming domain decomposition methods, asymptotic analysis, stabilization techniques for finite element discretizations.\"\n\nBuffa was awarded in 2007 the Bartolozzi Prize and in 2015 the Collatz Prize \"for her spectacular use of deep and sophisticated mathematical concepts to obtain outstanding contributions to the development of computer simulations in science and industry\" (Laudatio). In 2014 she was an Invited Speaker at the International Congress of Mathematicians in Seoul with talk \"Spline differential forms\". In 2008 she received an ERC Starting Grant and in 2016 an ERC Advanced Grant.\n\n"}
{"id": "58980299", "url": "https://en.wikipedia.org/wiki?curid=58980299", "title": "Annette Imhausen", "text": "Annette Imhausen\n\nAnnette Imhausen (also known as Annette Warner, born June 12, 1970) is a German historian of mathematics known for her work on Ancient Egyptian mathematics. She is a professor in the Normative Orders Cluster of Excellence at Goethe University Frankfurt.\n\nImhausen studied chemistry at Johannes Gutenberg University Mainz, passing the Staatsexamen in 1996. She completed her doctorate in the history of mathematics at Mainz in 2002 under the joint supervision of David E. Rowe and James Ritter.\n\nAfter teaching the Coptic language at Mainz in 1998, she worked as an instructor at the University of Cambridge from 2003 to 2006.\nShe returned to Mainz as an assistant professor from 2006 to 2008, and became a professor at Frankfurt in 2009.\n\nImhausen is featured in the BBC TV series \"The Story of Maths\".\n\nHer dissertation, \"Ägyptische Algorithmen. Eine Untersuchung zu den mittelägyptischen mathematischen Aufgabentexten\", was published by Harrassowitz Verlag in 2002 (Ägyptologische Abhandlungen, vol. 65).\nShe is also the author of \"Mathematics in Ancient Egypt: A Contextual History\" (Princeton University Press, 2016).\n"}
{"id": "21337035", "url": "https://en.wikipedia.org/wiki?curid=21337035", "title": "Balayage", "text": "Balayage\n\nIn potential theory, a mathematical discipline, balayage (from French: \"balayage\" \"scanning, sweeping\") is a method devised by Henri Poincaré for reconstructing a harmonic function in a domain from its values on the boundary of the domain.\n\nIn modern terms, the balayage operator maps a measure \"μ\" on a closed domain \"D\" to a measure \"ν\" on the boundary \"∂ D\", so that the Newtonian potentials of \"μ\" and \"ν\" coincide outside \"D\". The procedure is called balayage since the mass is \"swept out\" from \"D\" onto the boundary.\n\nFor \"x\" in \"D\", the balayage of \"δ\" yields the harmonic measure \"ν\" corresponding to \"x\". Then the value of a harmonic function \"f\" at \"x\" is equal to\n"}
{"id": "34950733", "url": "https://en.wikipedia.org/wiki?curid=34950733", "title": "Bochner–Yano theorem", "text": "Bochner–Yano theorem\n\nIn differential geometry, the Bochner–Yano theorem states that the isometry group of a compact Riemannian manifold with negative Ricci curvature is finite. It is named after its publication by .\n"}
{"id": "14933760", "url": "https://en.wikipedia.org/wiki?curid=14933760", "title": "Certificate (complexity)", "text": "Certificate (complexity)\n\nIn computational complexity theory, a certificate (also called a witness) is a string that certifies the answer to a computation, or certifies the membership of some string in a language. A certificate is often thought of as a solution path within a verification process, which is used to check whether a problem gives the answer \"Yes\" or \"No\". \n\nIn the decision tree model of computation, certificate complexity is the minimum number of the formula_1 input variables of a decision tree that need to be assigned a value in order to definitely establish the value of the Boolean function formula_2.\n\nThe notion of certificate is used to define semi-decidability: \n\nL ∈ SD iff there is a two-place predicate R ⊆ Σ∗ × Σ∗ such that R is computable, and such that for all x ∈ Σ∗:\n\nIt can also be used to define the class NP:\n\nL ∈ NP iff there is a polytime verifier V such that:\n\n Show L ∈ NP.\n\n\n"}
{"id": "5916", "url": "https://en.wikipedia.org/wiki?curid=5916", "title": "Circumference", "text": "Circumference\n\nIn geometry, the circumference (from Latin \"circumferentia\", meaning \"carrying around\") of a circle is the (linear) distance around it. That is, the circumference would be the length of the circle if it were opened up and straightened out to a line segment. Since a circle is the edge (boundary) of a disk, circumference is a special case of perimeter. The perimeter is the length around any closed figure and is the term used for most figures excepting the circle and some circular-like figures such as ellipses.\nInformally, \"circumference\" may also refer to the edge itself rather than to the length of the edge.\n\nThe circumference of a circle is the distance around it, but if, as in many elementary treatments, distance is defined in terms of straight lines, this can not be used as a definition. Under these circumstances, the circumference of a circle may be defined as the limit of the perimeters of inscribed regular polygons as the number of sides increases without bound. The term circumference is used when measuring physical objects, as well as when considering abstract geometric forms.\n\nThe circumference of a circle is related to one of the most important mathematical constants. This constant, pi, is represented by the Greek letter . The first few decimal digits of the numerical value of are 3.141592653589793 ... Pi is defined as the ratio of a circle's circumference to its diameter :\n\nOr, equivalently, as the ratio of the circumference to twice the radius. The above formula can be rearranged to solve for the circumference:\n\nThe use of the mathematical constant is ubiquitous in mathematics, engineering, and science. \n\nIn \"Measurement of a Circle\" written circa 250 BCE, Archimedes showed that this ratio (, since he did not use the name ) was greater than 3 but less than 3 by calculating the perimeters of an inscribed and a circumscribed regular polygon of 96 sides. This method for approximating was used for centuries, obtaining more accuracy by using polygons of larger and larger number of sides. The last such calculation was performed in 1630 by Christoph Grienberger who used polygons with 10 sides.\n\nCircumference is used by some authors to denote the perimeter of an ellipse. There is no general formula for the circumference of an ellipse in terms of the semi-major and semi-minor axes of the ellipse that uses only elementary functions. However, there are approximate formulas in terms of these parameters. One such approximation, due to Euler (1773), for the canonical ellipse, \nis \nSome lower and upper bounds on the circumference of the canonical ellipse with formula_5 are\n\nHere the upper bound formula_9 is the circumference of a circumscribed concentric circle passing through the endpoints of the ellipse's major axis, and the lower bound formula_10 is the perimeter of an inscribed rhombus with vertices at the endpoints of the major and minor axes.\n\nThe circumference of an ellipse can be expressed exactly in terms of the complete elliptic integral of the second kind. More precisely, we have\nwhere again formula_12 is the length of the semi-major axis and formula_13 is the eccentricity formula_14\n\nIn graph theory the circumference of a graph refers to the longest (simple) cycle contained in that graph.\n\n\n"}
{"id": "26427352", "url": "https://en.wikipedia.org/wiki?curid=26427352", "title": "Colin McLarty", "text": "Colin McLarty\n\nColin McLarty is an American logician whose publications have ranged widely in philosophy and the foundations of mathematics, as well as in the history of science and of mathematics.\n\nMcLarty's \"Elementary Categories and Elementary Toposes\" describes category theory and topos theory at an elementary level.\n\nHe has written papers about Saunders Mac Lane, one of the founders of category theory.\n\nHe is a member of the Grothendieck Circle, which provides on-line and open access to many writings about the mathematician Alexandre Grothendieck, who revolutionized Banach-space theory and algebraic geometry and whose life has fascinated many biographers and mathematical scientists.\n\nMcLarty has also written about the German algebraist, Emmy Noether, who was a Jewish woman, and her involvement with German political history.\n\nMcLarty claims that can show a proof of \"Fermat Last Theorem\" in Peano's Arithmetic, in 2012.\n\nAt Case Western Reserve University (CWRU), Colin McLarty is the Truman P. Handy Professor of Philosophy and a previous Chair of the Philosophy Department.\n\nAt CWRU, he is also a professor of mathematics.\n\n\n\n\n \n"}
{"id": "4165882", "url": "https://en.wikipedia.org/wiki?curid=4165882", "title": "Convex geometry", "text": "Convex geometry\n\nIn mathematics, convex geometry is the branch of geometry studying convex sets, mainly in Euclidean space. Convex sets occur naturally in many areas: computational geometry, convex analysis, discrete geometry, functional analysis, geometry of numbers, integral geometry, linear programming, probability theory, etc.\n\nAccording to the Mathematics Subject Classification MSC2010, the mathematical discipline \"Convex and Discrete Geometry\" includes three major branches:\n(though only portions of the latter two are included in convex geometry).\n\nGeneral convexity is further subdivided as follows: \n\nThe term \"convex geometry\" is also used in combinatorics as an alternate name for an antimatroid, which is one of the abstract models of convex sets.\n\nConvex geometry is a relatively young mathematical discipline. Although the first known contributions to convex geometry date back to antiquity and can be traced in the works of Euclid and Archimedes, it became an independent branch of mathematics at the turn of the 20th century, mainly due to the works of Hermann Brunn and Hermann Minkowski in dimensions two and three. A big part of their results was soon generalized to spaces of higher dimensions, and in 1934 T. Bonnesen and W. Fenchel gave a comprehensive survey of convex geometry in Euclidean space R. Further development of convex geometry in the 20th century and its relations to numerous mathematical disciplines are summarized in the \"Handbook of convex geometry\" edited by P. M. Gruber and J. M. Wills.\n\n\nExpository articles on convex geometry \n\nBooks on convex geometry \n\nArticles on history of convex geometry \n"}
{"id": "57800229", "url": "https://en.wikipedia.org/wiki?curid=57800229", "title": "Cramér's theorem (large deviations)", "text": "Cramér's theorem (large deviations)\n\nCramér's theorem is a fundamental result in the theory of large deviations, a subdiscipline of probability theory. It determines the rate function of a series of iid random variables.\nA weak version of this result was first shown by Harald Cramér in 1938.\n\nThe logarithmic moment generating function (which is the cumulant-generating function) of a random variable is defined as:\n\nLet formula_2 be a series of iid real random variables with finite logarithmic moment generating function, e.g. formula_3 for all formula_4.\n\nThen the Legendre transform of formula_5:\n\nsatisfies,\n\nfor all formula_8\n\nIn the terminology of the theory of large deviations the result can be reformulated as follows:\n\nIf formula_2 is a series of iid random variables, then the distributions formula_10 satisfy a large deviation principle with rate function formula_11.\n\n"}
{"id": "53133381", "url": "https://en.wikipedia.org/wiki?curid=53133381", "title": "Crumpling", "text": "Crumpling\n\nIn geometry and topology, crumpling is the process whereby a sheet of paper or other two-dimensional manifold undergoes disordered deformation to yield a three-dimensional structure comprising a random network of ridges and facets with variable density. The geometry of crumpled structures is the subject of some interest the mathematical community within the discipline of topology. Crumpled paper balls have been studied and found to exhibit surprisingly complex structures with compressive strength resulting from frictional interactions at locally flat facets between folds. The unusually high compressive strength of crumpled structures relative to their density is of interest in the disciplines of materials science and mechanical engineering.\n\nThe packing of a sheet by crumpling is a complex phenomenon that depends on material parameters and the packing protocol. Thus the crumpling behaviour of foil, paper and poly-membranes differs significantly and can be interpreted on the basis of material foldability. The high compressive strength exhibited by dense crumple formed cellulose paper is of interest towards impact dissipation applications and has been proposed as an approach to utilising waste paper.\n"}
{"id": "64669", "url": "https://en.wikipedia.org/wiki?curid=64669", "title": "De Morgan's laws", "text": "De Morgan's laws\n\nIn propositional logic and boolean algebra, De Morgan's laws are a pair of transformation rules that are both valid rules of inference. They are named after Augustus De Morgan, a 19th-century British mathematician. The rules allow the expression of conjunctions and disjunctions purely in terms of each other via negation.\n\nThe rules can be expressed in English as:\nor\nor\n\nIn set theory and Boolean algebra, these are written formally as\nwhere\n\nIn formal language, the rules are written as\nand\nwhere\n\nApplications of the rules include simplification of logical expressions in computer programs and digital circuit designs. De Morgan's laws are an example of a more general concept of mathematical duality.\n\nThe \"negation of conjunction\" rule may be written in sequent notation:\n\nThe \"negation of disjunction\" rule may be written as:\n\nIn rule form: \"negation of conjunction\"\n\nand \"negation of disjunction\"\n\nand expressed as a truth-functional tautology or theorem of propositional logic:\n\nwhere formula_13 and formula_14 are propositions expressed in some formal system.\n\nDe Morgan's laws are normally shown in the compact form above, with negation of the output on the left and negation of the inputs on the right. A clearer form for substitution can be stated as:\n\nThis emphasizes the need to invert both the inputs and the output, as well as change the operator, when doing a substitution.\n\nIn set theory and Boolean algebra, it is often stated as \"union and intersection interchange under complementation\", which can be formally expressed as:\n\nwhere:\n\nThe generalized form is:\n\nwhere is some, possibly uncountable, indexing set.\n\nIn set notation, De Morgan's laws can be remembered using the mnemonic \"break the line, change the sign\".\n\nIn electrical and computer engineering, De Morgan's laws are commonly written as:\n\nand\n\nwhere:\n\nDe Morgan’s laws commonly apply to text searching using Boolean operators AND, OR, and NOT. Consider a set of documents containing the words “cars” and “trucks”. De Morgan’s laws hold that these two searches will return the same set of documents:\n\nThe corpus of documents containing “cars” or “trucks” can be represented by four documents:\n\nTo evaluate Search A, clearly the search “(cars OR trucks)” will hit on Documents 1, 2, and 3. So the negation of that search (which is Search A) will hit everything else, which is Document 4.\n\nEvaluating Search B, the search “(NOT cars)” will hit on documents that do not contain “cars”, which is Documents 2 and 4. Similarly the search “(NOT trucks)” will hit on Documents 1 and 4. Applying the AND operator to these two searches (which is Search B) will hit on the documents that are common to these two searches, which is Document 4.\n\nA similar evaluation can be applied to show that the following two searches will return the same set of documents (Documents 1, 2, 4):\n\nThe laws are named after Augustus De Morgan (1806–1871), who introduced a formal version of the laws to classical propositional logic. De Morgan's formulation was influenced by algebraization of logic undertaken by George Boole, which later cemented De Morgan's claim to the find. Nevertheless, a similar observation was made by Aristotle, and was known to Greek and Medieval logicians. For example, in the 14th century, William of Ockham wrote down the words that would result by reading the laws out. Jean Buridan, in his \"Summulae de Dialectica\", also describes rules of conversion that follow the lines of De Morgan's laws. Still, De Morgan is given credit for stating the laws in the terms of modern formal logic, and incorporating them into the language of logic. De Morgan's laws can be proved easily, and may even seem trivial. Nonetheless, these laws are helpful in making valid inferences in proofs and deductive arguments.\n\nDe Morgan's theorem may be applied to the negation of a disjunction or the negation of a conjunction in all or part of a formula.\n\nIn the case of its application to a disjunction, consider the following claim: \"it is false that either of A or B is true\", which is written as:\nIn that it has been established that \"neither\" A nor B is true, then it must follow that both A is not true and B is not true, which may be written directly as:\nIf either A or B \"were\" true, then the disjunction of A and B would be true, making its negation false. Presented in English, this follows the logic that \"since two things are both false, it is also false that either of them is true\".\n\nWorking in the opposite direction, the second expression asserts that A is false and B is false (or equivalently that \"not A\" and \"not B\" are true). Knowing this, a disjunction of A and B must be false also. The negation of said disjunction must thus be true, and the result is identical to the first claim.\n\nThe application of De Morgan's theorem to a conjunction is very similar to its application to a disjunction both in form and rationale. Consider the following claim: \"it is false that A and B are both true\", which is written as:\nIn order for this claim to be true, either or both of A or B must be false, for if they both were true, then the conjunction of A and B would be true, making its negation false. Thus, one (at least) or more of A and B must be false (or equivalently, one or more of \"not A\" and \"not B\" must be true). This may be written directly as,\nPresented in English, this follows the logic that \"since it is false that two things are both true, at least one of them must be false\".\n\nWorking in the opposite direction again, the second expression asserts that at least one of \"not A\" and \"not B\" must be true, or equivalently that at least one of A and B must be false. Since at least one of them must be false, then their conjunction would likewise be false. Negating said conjunction thus results in a true expression, and this expression is identical to the first claim.\n\nThe proof that formula_26 is completed in 2 steps by proving both formula_27 and formula_28.\n\nLet formula_29. Then, formula_30.\n\nBecause formula_31, it must be the case that formula_32 or formula_33.\n\nIf formula_32, then formula_35, so formula_36.\n\nSimilarly, if formula_33, then formula_38, so formula_39.\n\nThus, formula_40;\n\nthat is, formula_27.\n\nTo prove the reverse direction, let formula_36, and assume formula_43.\n\nUnder that assumption, it must be the case that formula_44,\n\nso it follows that formula_45 and formula_46, and thus formula_47 and formula_48.\n\nHowever, that means formula_49, in contradiction to the hypothesis that formula_36,\n\ntherefore, the assumption formula_43 must not be the case, meaning that formula_52.\n\nHence, formula_53,\n\nthat is, formula_28.\n\nIf formula_28 \"and\" formula_56, then formula_26; this concludes the proof of De Morgan's law.\n\nThe other De Morgan's law, formula_58, is proven similarly.\n\nIn extensions of classical propositional logic, the duality still holds (that is, to any logical operator one can always find its dual), since in the presence of the identities governing negation, one may always introduce an operator that is the De Morgan dual of another. This leads to an important property of logics based on classical logic, namely the existence of negation normal forms: any formula is equivalent to another formula where negations only occur applied to the non-logical atoms of the formula. The existence of negation normal forms drives many applications, for example in digital circuit design, where it is used to manipulate the types of logic gates, and in formal logic, where it is needed to find the conjunctive normal form and disjunctive normal form of a formula. Computer programmers use them to simplify or properly negate complicated logical conditions. They are also often useful in computations in elementary probability theory.\n\nLet one define the dual of any propositional operator P(\"p\", \"q\", ...) depending on elementary propositions \"p\", \"q\", ... to be the operator formula_59 defined by\n\nThis duality can be generalised to quantifiers, so for example the universal quantifier and existential quantifier are duals:\n\nTo relate these quantifier dualities to the De Morgan laws, set up a model with some small number of elements in its domain \"D\", such as\n\nThen\n\nand\n\nBut, using De Morgan's laws,\n\nand\n\nverifying the quantifier dualities in the model.\n\nThen, the quantifier dualities can be extended further to modal logic, relating the box (\"necessarily\") and diamond (\"possibly\") operators:\n\nIn its application to the alethic modalities of possibility and necessity, Aristotle observed this case, and in the case of normal modal logic, the relationship of these modal operators to the quantification can be understood by setting up models using Kripke semantics.\n\n\n"}
{"id": "677191", "url": "https://en.wikipedia.org/wiki?curid=677191", "title": "Differential (mathematics)", "text": "Differential (mathematics)\n\nIn mathematics, differential refers to infinitesimal differences or to the derivatives of functions. The term is used in various branches of mathematics such as calculus, differential geometry, algebraic geometry and algebraic topology.\n\n\nThe notion of a differential motivates several concepts in differential geometry (and differential topology).\n\nDifferentials are also important in algebraic geometry, and there are several important notions.\n\nThe term \"differential\" has also been adopted in homological algebra and algebraic topology, because of the role the exterior derivative plays in de Rham cohomology: in a cochain complex formula_1, the maps (or \"coboundary operators\") \"d\" are often called differentials. Dually, the boundary operators in a chain complex are sometimes called \"codifferentials\".\n\nThe properties of the differential also motivate the algebraic notions of a \"derivation\" and a \"differential algebra\".\n"}
{"id": "48126834", "url": "https://en.wikipedia.org/wiki?curid=48126834", "title": "Elizabeth Buchanan Cowley", "text": "Elizabeth Buchanan Cowley\n\nElizabeth Buchanan Cowley (1874–1945) was an American mathematician.\n\nCowley was born on May 22, 1874, in Allegheny, Pennsylvania. She had four siblings, but they and her father all died by 1900. Cowley's mother, Mary Junkin Buchanan Cowley, later became a member of the Board of Public Education of Pittsburgh, and was the namesake of the Mary J. Cowley School in Pittsburgh. Cowley's grandfather (Mary Cowley's father) was James Galloway Buchanan, a surgeon in the Union Army.\n\nCowley earned a bachelor's degree in 1893 from the Indiana State Normal School of Pennsylvania, and became a school teacher. She earned a second bachelor's degree in 1901 and a master's degree in 1902 from Vassar College, and became an instructor at Vassar, studying higher mathematics during the summers at the University of Chicago. In 1908 she completed a doctorate from Columbia University. Her dissertation, on algebraic curves, was supervised by Cassius Jackson Keyser; she became the fourth woman to earn a doctorate in mathematics from Columbia. Continuing to work at Vassar, Cowley was promoted to assistant professor in 1913, and associate professor in 1916. She went on leave in 1926 to assist her mother, and resigned her position at Vassar in 1929, instead becoming a high school teacher in Pittsburgh.\n\nShe retired from teaching in 1938, had a stroke in 1941, and died on April 13, 1945, in Fort Lauderdale, Florida.\n\nCowley and her co-author Ida Whiteside won a prize for a 1907 paper they wrote on the orbit of comet C/1825 V1. Another of her publications, in 1926, concerned liquid water pouring puzzles. She was the author of two textbooks on plane and solid geometry, published in 1932 and 1934, and advocated teaching solid geometry to high school students after many colleges had replaced the subject with freshman calculus. She published another book in 1941 about public education.\n\nCowley was an early member of the Mathematical Association of America, and became a member of its board of trustees when it incorporated in 1920.\nShe was an invited speaker at the International Congress of Mathematicians in 1932, speaking there about mathematics education. She also belonged to the American Mathematical Society, German Mathematical Society, and Circolo Matematico di Palermo.\n"}
{"id": "43415449", "url": "https://en.wikipedia.org/wiki?curid=43415449", "title": "Equivalent definitions of mathematical structures", "text": "Equivalent definitions of mathematical structures\n\nIn mathematics, equivalent definitions are used in two somewhat different ways. First, within a particular mathematical theory (for example, Euclidean geometry), a notion (for example, ellipse or minimal surface) may have more than one definition. These definitions are equivalent in the context of a given mathematical structure (Euclidean space, in this case). Second, a mathematical structure may have more than one definition (for example, topological space has at least seven definitions; ordered field has at least two definitions).\n\nIn the former case, equivalence of two definitions means that a mathematical object (for example, geometric body) satisfies one definition if and only if it satisfies the other definition.\n\nIn the latter case, the meaning of equivalence (between two definitions of a structure) is more complicated, since a structure is more abstract than an object. Many different objects may implement the same structure.\n\nNatural numbers may be implemented as 0 = , 1 = = , 2 = = , 3 = = and so on; or alternatively as 0 = , 1 = =, 2 = = and so on. These are two different but isomorphic implementations of natural numbers in set theory.\nThey are isomorphic as models of Peano axioms, that is, triples (\"N\",0,\"S\") where \"N\" is a set, 0 an element of \"N\", and \"S\" (called the successor function) a map of \"N\" to itself (satisfying appropriate conditions). In the first implementation \"S\"(\"n\") = \"n\" ∪ ; in the second implementation \"S\"(\"n\") = . As emphasized in Benacerraf's identification problem, the two implementations differ in their answer to the question whether 0 ∈ 2; however, this is not a legitimate question about natural numbers (since the relation ∈ is not stipulated by the relevant signature(s), see the next section). Similarly, different but isomorphic implementations are used for complex numbers.\n\nThe successor function \"S\" on natural numbers leads to arithmetic operations, addition and multiplication, and the total order, thus endowing \"N\" with an ordered semiring structure. This is an example of a deduced structure. The ordered semiring structure (\"N\", +, ·, ≤) is deduced from the Peano structure (\"N\", 0, \"S\") by the following procedure:\n\"n\" + 0 = \"n\",   \"m\" + S (\"n\") = S (\"m\" + \"n\"),   \"m\" · 0 = 0,   \"m\" · S (\"n\") = \"m\" + (\"m\" · \"n\"), and \"m\" ≤ \"n\" if and only if there exists \"k\" ∈ \"N\" such that \"m\" + \"k\" = \"n\". And conversely, the Peano structure is deduced from the ordered semiring structure as follows: \"S\" (\"n\") = \"n\" + 1, and 0 is defined by 0 + 0 = 0. It means that the two structures on \"N\" are equivalent by means of the two procedures.\n\nThe two isomorphic implementations of natural numbers, mentioned in the previous section, are isomorphic as triples (\"N\",0,\"S\"), that is, structures of the same signature (0,\"S\") consisting of a constant symbol 0 and a unary function \"S\". An ordered semiring structure (\"N\", +, ·, ≤) has another signature (+, ·, ≤) consisting of two binary functions and one binary relation. The notion of isomorphism does not apply to structures of different signatures. In particular, a Peano structure cannot be isomorphic to an ordered semiring. However, an ordered semiring deduced from a Peano structure may be isomorphic to another ordered semiring. Such relation between structures of different signatures is sometimes called a cryptomorphism.\n\nA structure may be implemented within a set theory ZFC, or another set theory such as NBG, NFU, ETCS. Alternatively, a structure may be treated in the framework of first-order logic, second-order logic, higher-order logic, a type theory, homotopy type theory etc.\n\nAccording to Bourbaki, the scale of sets on a given set \"X\" consists of all sets arising from \"X\" by taking Cartesian products and power sets, in any combination, a finite number of times. Examples: \"X\"; \"X\" × \"X\"; \"P\"(\"X\"); \"P\"(\"P\"(\"X\" × \"X\") × \"X\" × \"P\"(\"P\"(\"X\"))) × \"X\". (Here \"A\" × \"B\" is the product of \"A\" and \"B\", and \"P\"(\"A\") is the powerset of \"A\".) In particular, a pair (0,\"S\") consisting of an element 0 ∈ \"N\" and a unary function \"S\" : \"N\" → \"N\" belongs to \"N\" × \"P\"(\"N\" × \"N\") (since a function is a subset of the Cartesian product). A triple (+, ·, ≤) consisting of two binary functions \"N\" × \"N\" → \"N\" and one binary relation on \"N\" belongs to \"P\"(\"N\" × \"N\" × \"N\") × \"P\"(\"N\" × \"N\" × \"N\") × \"P\"(\"N\" × \"N\"). Similarly, every algebraic structure on a set belongs to the corresponding set in the scale of sets on \"X\".\n\nNon-algebraic structures on a set \"X\" often involve sets of subsets of \"X\" (that is, subsets of \"P\"(\"X\"), in other words, elements of \"P\"(\"P\"(\"X\"))). For example, the structure of a topological space, called a topology on \"X\", treated as the set of \"open\" sets; or the structure of a measurable space, treated as the σ-algebra of \"measurable\" sets; both are elements of \"P\"(\"P\"(\"X\")). These are second-order structures.\n\nMore complicated non-algebraic structures combine an algebraic component and a non-algebraic component. For example, the structure of a topological group consists of a topology and the structure of a group. Thus it belongs to the product of \"P\"(\"P\"(\"X\")) and another (\"algebraic\") set in the scale; this product is again a set in the scale.\n\nGiven two sets \"X\", \"Y\" and a bijection \"f\" : \"X\" → \"Y\", one constructs the corresponding bijections between scale sets. Namely, the bijection \"X\" × \"X\" → \"Y\" × \"Y\" sends (\"x\",\"x\") to (\"f\"(\"x\"),\"f\"(\"x\")); the bijection \"P\"(\"X\") → \"P\"(\"Y\") sends a subset \"A\" of \"X\" into its image \"f\"(\"A\") in \"Y\"; and so on, recursively: a scale set being either product of scale sets or power set of a scale set, one of the two constructions applies.\n\nLet (\"X\",\"U\") and (\"Y\",\"V\") be two structures of the same signature. Then \"U\" belongs to a scale set \"S\", and \"V\" belongs to the corresponding scale set \"S\". Using the bijection \"F\" : \"S\" → \"S\" constructed from a bijection \"f\" : \"X\" → \"Y\", one defines:\nThis general notion of isomorphism generalizes many less general notions listed below.\nIn fact, Bourbaki stipulates two additional features. First, several sets \"X\", ..., \"X\" (so-called principal base sets) may be used, rather than a single set \"X\". However, this feature is of little use. All the items listed above use a single principal base set. Second, so-called auxiliary base sets \"E\", ..., \"E\" may be used. This feature is widely used. Indeed, the structure of a vector space stipulates not only addition \"X\" × \"X\" → \"X\" but also scalar multiplication R × \"X\" → \"X\" (if R is the field of scalars). Thus, R is an auxiliary base set (called also \"external\"). The scale of sets consists of all sets arising from all base sets (both principal and auxiliary) by taking Cartesian products and power sets. Still, the map \"f\" (possibly an isomorphism) acts on \"X\" only; auxiliary sets are endowed by identity maps. (However, the case of \"n\" principal sets leads to \"n\" maps.)\n\nSeveral statements formulated by Bourbaki without mentioning categories can be reformulated readily in the language of category theory. First, some terminology. \n\n\"Proposition.\" Each echelon construction scheme leads to a functor from Set* to itself.\n\nIn particular, the permutation group of a set \"X\" acts on every scale set \"S\".\n\nIn order to formulate one more proposition, the notion \"species of structures\" is needed, since echelon construction scheme gives only preliminary information on a structure. For example, commutative groups and (arbitrary) groups are two different species of the same echelon construction scheme. Another example: topological spaces and measurable spaces. They differ in the so-called axiom of the species. This axiom is the conjunction of all required properties, such as \"multiplication is associative\" for groups, or \"the union of open sets is an open set\" for topological spaces.\n\n\"Proposition.\" Each species of structures leads to a functor from Set* to itself.\n\nExample. For the species of groups, the functor \"F\" maps a set \"X\" to the set \"F\"(\"X\") of all group structures on \"X\". For the species of topological spaces, the functor \"F\" maps a set \"X\" to the set \"F\"(\"X\") of all topologies on \"X\". The morphism \"F\"(\"f\") : \"F\"(\"X\") → \"F\"(\"Y\") corresponding to a bijection \"f\" : \"X\" → \"Y\" is the transport of structures. Topologies on \"Y\" correspond bijectively to topologies on \"X\". The same holds for group structures, etc.\n\nIn particular, the set of all structures of a given species on a given set is invariant under the action of the permutation group on the corresponding scale set \"S\", and is a fixed point of the action of the group on another scale set \"P\"(\"S\"). However, not all fixed points of this action correspond to species of structures.\n\nGiven two species, Bourbaki defines the notion \"procedure of deduction\" (of a structure of the second species from a structure of the first species). A pair of mutually inverse procedures of deduction leads to the notion \"equivalent species\".\n\nExample. The structure of a topological space may be defined as an open set topology or alternatively, a closed set topology. The two corresponding procedures of deduction coincide; each one replaces all given subsets of \"X\" with their complements. In this sense, these are two equivalent species.\n\nIn the general definition of Bourbaki, deduction procedure may include a change of the principal base set(s), but this case is not treated here. In the language of category theory one have the following result.\n\n\"Proposition.\" Equivalence between two species of structures leads to a natural isomorphism between the corresponding functors.\n\nHowever, in general, not all natural isomorphisms between these functors correspond to equivalences between the species.\n\nIn practice, one makes no distinction between equivalent species of structures.\n\nUsually, a text based on natural numbers (for example, the article \"prime number\") does not specify the used definition of natural numbers. Likewise, a text based on topological spaces (for example, the article \"homotopy\", or \"inductive dimension\") does not specify the used definition of a topological space. Thus, it is possible (and rather probable) that the reader and the author interpret the text differently, according to different definitions. Nevertheless, the communication is successful, which means that such different definitions may be thought of as equivalent.\n\nA person acquainted with topological spaces knows basic relations between neighborhoods, convergence, continuity, boundary, closure, interior, open sets, closed sets, and does not need to know that some of these notions are \"primary\", stipulated in the definition of a topological space, while others are \"secondary\", characterized in terms of \"primary\" notions. Moreover, knowing that subsets of a topological space are themselves topological spaces, as well as products of topological spaces, the person is able to construct some new topological spaces irrespective of the definition.\n\nThus, in practice a topology on a set is treated like an abstract data type that provides all needed notions (and constructors) but hides the distinction between \"primary\" and \"secondary\" notions. The same applies to other kinds of mathematical structures. \"Interestingly, the formalization of structures in set theory is a similar task as the formalization of structures for computers.\"\n\nAs was mentioned, equivalence between two species of structures leads to a natural isomorphism between the corresponding functors. However, \"natural\" does not mean \"canonical\". A natural transformation is generally non-unique.\n\nExample. Consider again the two equivalent structures for natural numbers. One is the \"Peano structure\" (0,\"S\"), the other is the structure (+, ·, ≤) of ordered semiring. If a set \"X\" is endowed by both structures then, on one hand, \"X\" = where \"S\"(\"a\") = \"a\" for all \"n\" and 0 = \"a\"; and on the other hand, \"X\" = where \"b\" = \"b\" + \"b\", \"b\" = \"b\" · \"b\", and \"b\" ≤\"b\" if and only if \"m\" ≤ \"n\". Requiring that \"a\" = \"b\" for all \"n\" one gets the canonical equivalence between the two structures. However, one may also require \"a\" = \"b\", \"a\" = \"b\", and \"a\" = \"b\" for all \"n\" > 1, thus getting another, non-canonical, natural isomorphism. Moreover, every permutation of the index set leads to a natural isomorphism; they are uncountably many!\n\nAnother example. A structure of a (simple) graph on a set \"V\" = of vertices may be described by means of its adjacency matrix, a (0,1)-matrix of size \"n\"×\"n\" (with zeros on the diagonal). More generally, for arbitrary \"V\" an adjacency function on \"V\" × \"V\" may be used. The canonical equivalence is given by the rule: \"1\" means \"connected\" (with an edge), \"0\" means \"not connected\". However, another rule, \"0\" means \"connected\", \"1\" means \"not\", may be used, and leads to another, natural but not canonical, equivalence. In this example, canonicity is rather a matter of convention. But here is a worse case. Instead of \"0\" and \"1\" one may use, say, the two possible orientations of the plane R (\"clockwise\" and \"counterclockwise\"). It is difficult to choose a canonical rule in this case!\n\n\"Natural\" is a well-defined mathematical notion, but it does not ensure uniqueness. \"Canonical\" does, but generally is more or less conventional. A consistent choice of canonical equivalences is an inevitable component of equivalent definitions of mathematical structures.\n\n\n\n"}
{"id": "12600886", "url": "https://en.wikipedia.org/wiki?curid=12600886", "title": "Erdős–Diophantine graph", "text": "Erdős–Diophantine graph\n\nAn Erdős–Diophantine graph is an object in the mathematical subject of Diophantine equations consisting of a set of integer points at integer distances in the plane that cannot be extended by any additional points. Equivalently,\nit can be described as a complete graph with vertices located on the integer square grid formula_1 such that all mutual distances between the vertices are integers, while all other grid points have a non-integer distance to at least one vertex.\n\nErdős–Diophantine graphs are named after Paul Erdős and Diophantus of Alexandria. They form a subset of the set of Diophantine figures, which are defined as complete graphs in the Diophantine plane for which the length of all edges are integers (unit distance graphs). Thus, Erdős–Diophantine graphs are exactly the Diophantine figures that cannot be extended. The existence of Erdős–Diophantine graphs follows from the Erdős–Anning theorem, according to which infinite Diophantine figures must be collinear in the Diophantine plane. Hence, any process of extending a non-collinear Diophantine figure by adding vertices must eventually reach a figure that can no longer be extended.\n\nAny set of zero or one point can be trivially extended, and any Diophantine set of two points can be extended by more points on the same line.Therefore, all Diophantine sets with fewer than three nodes can be extended, so Erdős–Diophantine graphs on fewer than three nodes cannot exist.\n\nBy numerical search, have shown that three-node Erdős–Diophantine graphs do exist. The smallest Erdős–Diophantine triangle is characterised by edge lengths 2066, 1803, and 505. The next larger Erdős–Diophantine triangle has edges 2549, 2307 and 1492. In both cases, the sum of the three edge-lengths is even. Brancheva has proven that this property holds for all Erdős–Diophantine triangles. More generally, the total length of any closed path in an Erdős–Diophantine graph is always even.\n\nAn example of a 4-node Erdős–Diophantine graph is provided by the complete graph formed by the four nodes located on the vertices of a rectangle with sides 4 and 3.\n\n"}
{"id": "19107713", "url": "https://en.wikipedia.org/wiki?curid=19107713", "title": "Euler spiral", "text": "Euler spiral\n\nA Euler spiral is a curve whose curvature changes linearly with its curve length (the curvature of a circular curve is equal to the reciprocal of the radius). Euler spirals are also commonly referred to as spiros, clothoids, or Cornu spirals.\n\nEuler spirals have applications to diffraction computations. They are also widely used as transition curves in railroad engineering/highway engineering for connecting and transiting the geometry between a tangent and a circular curve. A similar application is also found in photonic integrated circuits. The principle of linear variation of the curvature of the transition curve between a tangent and a circular curve defines the geometry of the Euler spiral: \n\nTo travel along a circular path, an object needs to be subject to a centripetal acceleration (e.g.: the moon circles around the earth because of gravity; a car turns its front wheels inward to generate a centripetal force). If a vehicle traveling on a straight path were to suddenly transition to a tangential circular path, it would require centripetal acceleration suddenly switching at the tangent point from zero to the required value; this would be difficult to achieve (think of a driver instantly moving the steering wheel from straight line to turning position, and the car actually doing it), putting mechanical stress on vehicle's parts, and causing much discomfort (causing jerk).\n\nOn early railroads this instant application of lateral force was not an issue since low speeds and wide-radius curves were employed (lateral forces on the passengers and the lateral sway was small and tolerable). As speeds of rail vehicles increased over the years, it became obvious that an easement is necessary so that the centripetal acceleration increases linearly with the traveled distance. Given the expression of centripetal acceleration , the obvious solution is to provide an easement curve whose curvature, , increases linearly with the traveled distance. This geometry is an Euler spiral.\n\nUnaware of the solution of the geometry by Leonhard Euler, Rankine cited the cubic curve (a polynomial curve of degree 3), which is an approximation of the Euler spiral for small angular changes in the same way that a parabola is an approximation to a circular curve.\n\nMarie Alfred Cornu (and later some civil engineers) also solved the calculus of Euler spiral independently. Euler spirals are now widely used in rail and highway engineering for providing a transition or an easement between a tangent and a horizontal circular curve.\n\nThe Cornu spiral can be used to describe a diffraction pattern.\n\nBends with continuously varying radius of curvature following the Euler spiral are also used to reduce losses in photonic integrated circuits, either in singlemode waveguides, to smoothen the abrupt change of curvature and coupling to radiation modes, or in multimode waveguides, in order to suppress coupling to higher order modes and ensure effective singlemode operation.\nA pioneering and very elegant application of the Euler spiral to waveguides had been made as early as 1957, with a hollow metal waveguide for microwaves. There the idea was to exploit the fact that a straight metal waveguide can be physically bent to naturally take a gradual bend shape resembling an Euler spiral.\n\nMotorsport author Adam Brouillard has shown the Euler spiral's use in optimizing the racing line during the corner entry portion of a turn.\n\nRaph Levien has released Spiro as a toolkit for curve design, especially font design, in 2007 under a free licence. This toolkit has been implemented quite quickly afterwards in the font design tool Fontforge and the digital vector drawing Inkscape.\n\nCutting a sphere along a spiral with width and flattening out the resulting shape yields a Euler spiral when tends to the infinity. If the sphere is the globe, this produces a map projection whose distortion tends to zero as tends to the infinity.\n\nIf \"a\" = 1, which is the case for normalized Euler curve, then the Cartesian coordinates are given by Fresnel integrals (or Euler integrals):\n\nFor a given Euler curve with:\n\nor\n\nthen\n\nwhere formula_6 and formula_7.\n\nThe process of obtaining solution of of an Euler spiral can thus be described as:\n\nIn the normalization process,\n\nThen\n\nGenerally the normalization reduces L' to a small value (<1) and results in good converging characteristics of the Fresnel integral manageable with only a few terms (at a price of increased numerical instability of the calculation, esp. for bigger \"formula_13\" values.).\n\nGiven:\nThen\nAnd\n\nWe scale down the Euler spiral by , i.e.100 to normalized Euler spiral that has:\n\nAnd\n\nThe two angles formula_20 are the same. This thus confirms that the original and normalized Euler spirals are geometrically similar. The locus of the normalized curve can be determined from Fresnel Integral, while the locus of the original Euler spiral can be obtained by scaling back / up or denormalizing.\n\nNormalized Euler spirals can be expressed as:\n\nOr expressed as power series:\n\nThe normalized Euler spiral will converge to a single point in the limit, which can be expressed as:\n\nNormalized Euler spirals have the following properties:\n\nAnd\n\nNote that formula_31 also means formula_32, in agreement with the last mathematical statement.\n\nThe following SageMath code produces the second graph above. The first four lines express the Euler spiral component. Fresnel functions could not be found. Instead, the integrals of two expanded Taylor series are adopted. The remaining code expresses respectively the tangent and the circle, including the computation for the center coordinates.\nvar('L')\np = integral(taylor(cos(L^2), L, 0, 12), L)\nq = integral(taylor(sin(L^2), L, 0, 12), L)\nr1 = parametric_plot([p, q], (L, 0, 1), color = 'red')\nr2 = line([(-1.0, 0), (0,0)], rgbcolor = 'blue')\nx1 = p.subs(L = 1)\ny1 = q.subs(L = 1)\nR = 0.5\nx2 = x1 - R*sin(1.0)\ny2 = y1 + R*cos(1.0)\nr3 = circle((x2, y2), R, rgbcolor = 'green')\nshow(r1 + r2 + r3, aspect_ratio = 1, axes=false)\nThe following is Mathematica code for the Euler spiral component (it works directly in wolframalpha.com):\nParametricPlot[{\n}, {t, -10, 10}]\nThe following is Xcas code for the Euler spiral component:\n\nThe following is SageMath code for the complete double ended Euler spiral:\ns = var('s')\nparametric_plot((lambda s: numerical_integral(cos(x**2),0,s)[0], lambda s: numerical_integral(sin(x**2),0,s)[0]), (-3*pi/2, 3*pi/2))\nThe following is JavaScript code for drawing an Euler spiral on a canvas element:\nfunction drawEulerSpiral(canvas, T, N, scale) {\ndrawEulerSpiral(document.getElementById(\"myCanvas\"),10,10000,100)\nThe following is Logo (programming language) code for drawing the Euler spiral using the turtle sprite.\nrt 90\nrepeat 720 [ fd 10 lt repcount ]\n\nNotes\n\nSources\n\n\n"}
{"id": "8307819", "url": "https://en.wikipedia.org/wiki?curid=8307819", "title": "Expected shortfall", "text": "Expected shortfall\n\nExpected shortfall (ES) is a risk measure—a concept used in the field of financial risk measurement to evaluate the market risk or credit risk of a portfolio. The \"expected shortfall at q% level\" is the expected return on the portfolio in the worst formula_1% of cases. ES is an alternative to value at risk that is more sensitive to the shape of the tail of the loss distribution.\n\nExpected shortfall is also called conditional value at risk (CVaR), average value at risk (AVaR), and expected tail loss (ETL).\n\nES estimates the risk of an investment in a conservative way, focusing on the less profitable outcomes. For high values of formula_1 it ignores the most profitable but unlikely possibilities, while for small values of formula_1 it focuses on the worst losses. On the other hand, unlike the discounted maximum loss, even for lower values of formula_1 the expected shortfall does not consider only the single most catastrophic outcome. A value of formula_1 often used in practice is 5%.\n\nExpected shortfall is considered a more useful risk measure than VaR because it is a coherent, and moreover a spectral, measure of financial portfolio risk. It is calculated for a given quantile-level formula_1, and is defined to be the mean loss of portfolio value given that a loss is occurring at or below the formula_1-quantile.\n\nIf formula_8 (an Lp space) is the payoff of a portfolio at some future time and formula_9 then we define the expected shortfall as\n\nwhere formula_11 is the Value at risk. This can be equivalently written as\n\nwhere formula_13 is the lower formula_14-quantile and formula_15 is the indicator function. The dual representation is\nwhere formula_17 is the set of probability measures which are absolutely continuous to the physical measure formula_18 such that formula_19 almost surely. Note that formula_20 is the Radon–Nikodym derivative of formula_21 with respect to formula_18.\n\nExpected Shortfall can be generalized to a general class of coherent risk measures on formula_23 spaces (Lp space) with a corresponding dual characterization in the corresopnding formula_24 dual space. The domain can be extended for more general Orlitz Hearts.\n\nIf the underlying distribution for formula_25 is a continuous distribution then the expected shortfall is equivalent to the tail conditional expectation defined by formula_26.\n\nInformally, and non rigorously, this equation amounts to saying \"in case of losses so severe that they occur only alpha percent of the time, what is our average loss\".\n\nExpected shortfall can also be written as a distortion risk measure given by the distortion function formula_27\n\nExample 1. If we believe our average loss on the worst 5% of the possible outcomes for our portfolio is EUR 1000, then we could say our expected shortfall is EUR 1000 for the 5% tail.\n\nExample 2. Consider a portfolio that will have the following possible values at the end of the period:\n\nNow assume that we paid 100 at the beginning of the period for this portfolio. Then the profit in each case is (\"ending value\"−100) or:\n\nFrom this table let us calculate the expected shortfall formula_28 for a few values of formula_1:\n\nTo see how these values were calculated, consider the calculation of formula_30, the expectation in the worst 5% of cases. These cases belong to (are a subset of) row 1 in the profit table, which have a profit of −100 (total loss of the 100 invested). The expected profit for these cases is −100.\n\nNow consider the calculation of formula_31, the expectation in the worst 20 out of 100 cases. These cases are as follows: 10 cases from row one, and 10 cases from row two (note that 10+10 equals the desired 20 cases). For row 1 there is a profit of −100, while for row 2 a profit of −20. Using the expected value formula we get\n\nSimilarly for any value of formula_1. We select as many rows starting from the top as are necessary to give a cumulative probability of formula_1 and then calculate an expectation over those cases. In general the last row selected may not be fully used (for example in calculating formula_35 we used only 10 of the 30 cases per 100 provided by row 2).\n\nAs a final example, calculate formula_36. This is the expectation over all cases, or\n\nThe Value at Risk (VaR) is given below for comparison.\n\nThe expected shortfall formula_28 increases as formula_1 decreases.\n\nThe 100%-quantile expected shortfall formula_40 equals the expected value of the portfolio.\n\nFor a given portfolio, the expected shortfall formula_28 is greater than or equal to the Value at Risk formula_42 at the same formula_1 level.\n\nThe conditional version of the expected shortfall at the time \"t\" is defined by\n\nwhere formula_45.\n\nThis is not a time-consistent risk measure. The time-consistent version is given by\nsuch that\n\n\nMethods of statistical estimation of VaR and ES can be found in \nEmbrechts et al. and Novak. When forecasting VaR and ES, or optimizing portfolios to minimize tail risk, it is important to account for asymmetric dependence and non-normalities in the distribution of stock returns such as auto-regression, asymmetric volatility, skewness, and kurtosis.\n\n"}
{"id": "2675917", "url": "https://en.wikipedia.org/wiki?curid=2675917", "title": "Extremal combinatorics", "text": "Extremal combinatorics\n\nExtremal combinatorics is a field of combinatorics, which is itself a part of mathematics. Extremal combinatorics studies how large or how small a collection of finite objects (numbers, graphs, vectors, sets, etc.) can be, if it has to satisfy certain restrictions.\n\nMuch of extremal combinatorics concerns classes of sets; this is called extremal set theory. For instance, in an \"n\"-element set, what is the largest number of \"k\"-element subsets that can pairwise intersect one another? What is the largest number of subsets of which none contains any other? The latter question is answered by Sperner's theorem, which gave rise to much of extremal set theory.\n\nAnother kind of example: How many people can we invite to a party where among each three people there are two who know each other and two who don't know each other? Ramsey theory shows that at most five persons can attend such a party. Or, suppose we are given a finite set of nonzero integers, and are asked to mark as large a subset as possible of this set under the restriction that the sum of any two marked integers cannot be marked. It appears that (independent of what the given integers actually are!) we can always mark at least one-third of them.\n\n"}
{"id": "31613712", "url": "https://en.wikipedia.org/wiki?curid=31613712", "title": "Fundamental Fysiks Group", "text": "Fundamental Fysiks Group\n\nThe Fundamental Fysiks Group was founded in San Francisco in May 1975 by two physicists, Elizabeth Rauscher and George Weissmann, at the time both graduate students at the University of California, Berkeley. The group held informal discussions on Friday afternoons to explore the philosophical implications of quantum theory. Leading members included Fritjof Capra, John Clauser, Philippe Eberhard, Nick Herbert, Jack Sarfatti, Saul-Paul Sirag, Henry Stapp, and Fred Alan Wolf.\n\nDavid Kaiser argues, in \"How the Hippies Saved Physics: Science, Counterculture, and the Quantum Revival\" (2011), that the group's meetings and papers helped to nurture the ideas in quantum physics that came to form the basis of quantum information science. Two reviewers wrote that Kaiser may have exaggerated the group's influence on the future of physics research, though one of them, Silvan Schweber, wrote that some of the group's contributions are easy to identify, such as Clauser's experimental evidence for non-locality attracting a share of the Wolf Prize in 2010, and the publication of Capra's \"The Tao of Physics\" (1975) and Zukav's \"The Dancing Wu Li Masters\" (1979) attracting the interest of a wider audience.\n\nKaiser writes that the group were \"very smart and very playful\", discussing quantum mysticism and becoming local celebrities in the Bay Area's counterculture. When Francis Ford Coppola bought \"City Magazine\" in 1975, one of its earliest features was on the Fundamental Fysiks Group, including a photo spread of Sirag, Wolf, Herbert, and Sarfatti.\n\nHugh Gusterson writes that several challenging ideas lie at the heart of quantum physics: that electrons behave like waves and particles; that you can know a particle's location or momentum, but not both; that observing a particle changes its behavior; and that particles appear to communicate with each other across great distances, known as nonlocality and quantum entanglement. It is these concepts that led to the development of quantum information science and quantum encryption, which has been experimentally used, for example, to transfer money and electronic votes. Kaiser argues that the Fundamental Fysiks Group saved physics by exploring these ideas, in three ways:\n\nSpecifically, in 1981, Nick Herbert, a member of the group, proposed a scheme for sending signals faster than the speed of light using quantum entanglement. Quantum computing pioneer Asher Peres writes that the refutation of Herbert's ideas led to the development of the no-cloning theorem by William Wootters, Wojciech Zurek, and Dennis Dieks.\n\nIn a review of Kaiser's book in \"Physics Today\", Silvan Schweber challenges Kaiser's views of the importance of the Fundamental Fysiks Group. He writes that Bell's Theorem was not obscure during the preceding decade, but was worked on by authors such as John Clauser (who was a member of the group) and Eugene Wigner. Schweber also mentioned the work of Alain Aspect, which preceded Nick Herbert's 1981 proposal.\n\nGiven quantum theory's perceived implications for the study of parapsychology and telepathy, the group cultivated patrons such as the Central Intelligence Agency, Defense Intelligence Agency, and the human potential movement. In 1972, the CIA and DIA set up a research program, jokingly called ESPionage, which financed experiments into remote viewing at the Stanford Research Institute, where the Fundamental Fysiks Group became what Kaiser calls its house theorists.\n\n\n\n"}
{"id": "245530", "url": "https://en.wikipedia.org/wiki?curid=245530", "title": "Generalized Fourier series", "text": "Generalized Fourier series\n\nIn mathematical analysis, many generalizations of Fourier series have proved to be useful.\nThey are all special cases of decompositions over an orthonormal basis of an inner product space.\nHere we consider that of square-integrable functions defined on an interval of the real line, which is important, among others, for interpolation theory.\n\nConsider a set of square-integrable functions with values in formula_1,\n\nwhich are pairwise orthogonal for the inner product\n\nwhere \"w\"(\"x\") is a weight function, and formula_4 represents complex conjugation, i.e. formula_5 for formula_6.\n\nThe generalized Fourier series of a square-integrable function \"f\": [\"a\", \"b\"] → formula_7,\nwith respect to Φ, is then\n\nwhere the coefficients are given by\n\nIf Φ is a complete set, i.e., an orthonormal basis of the space of all square-integrable functions on [\"a\", \"b\"], as opposed to a smaller orthonormal set,\nthe relation formula_10 becomes equality in the \"L²\" sense, more precisely modulo |·| (not necessarily pointwise, nor almost everywhere).\n\nThe Legendre polynomials are solutions to the Sturm–Liouville problem\n\nand because of Sturm-Liouville theory, these polynomials are eigenfunctions of the problem and are solutions orthogonal with respect to the inner product above with unit weight. So we can form a generalized Fourier series (known as a Fourier–Legendre series) involving the Legendre polynomials, and\n\nAs an example, let us calculate the Fourier–Legendre series for \"ƒ\"(\"x\") = cos \"x\" over [−1, 1]. Now,\n\nand a series involving these terms\n\nwhich differs from cos \"x\" by approximately 0.003, about 0. It may be advantageous to use such Fourier–Legendre series since the eigenfunctions are all polynomials and hence the integrals and thus the coefficients are easier to calculate.\n\nSome theorems on the coefficients \"c\" include:\n\nIf Φ is a complete set,\n\n"}
{"id": "19709056", "url": "https://en.wikipedia.org/wiki?curid=19709056", "title": "Gil Kalai", "text": "Gil Kalai\n\nGil Kalai (born 1955) is the Henry and Manya Noskwith Professor of Mathematics at the Hebrew University of Jerusalem, and adjunct professor of mathematics and of computer science at Yale University.\n\nGil Kalai received his Ph.D. from Hebrew University in 1983, under the supervision of Micha Perles, and joined the Hebrew University faculty in 1985 after a postdoctoral fellowship at the Massachusetts Institute of Technology. He was the recipient of the Pólya Prize in 1992, the Erdős Prize of the Israel Mathematical Society in 1993, and the Fulkerson Prize in 1994. He is known for finding variants of the simplex algorithm in linear programming that can be proven to run in subexponential time, for showing that every monotone property of graphs has a sharp phase transition, for solving Borsuk's problem (known as Borsuk's conjecture) on the number of pieces needed to partition convex sets into subsets of smaller diameter, and for his work on the Hirsch conjecture on the diameter of convex polytopes and in polyhedral combinatorics more generally.\n\nHe was the winner of the 2012 Rothschild Prize in mathematics. From 1995 to 2001, he was the Editor-in-Chief of the Israel Journal of Mathematics. In 2016, he was elected honorary member of the Hungarian Academy of Sciences. In 2018 he was a plenary speaker with talk \"Noise Stability, Noise Sensitivity and the Quantum Computer Puzzle\" at the International Congress of Mathematicians in Rio de Janeiro.\n\nConjecture 1 (No quantum error correction). The process for creating a quantum error-correcting code will necessarily lead to a mixture of the desired codewords with undesired codewords. The probability of the undesired codewords is uniformly bounded away from zero. (In every implementation of quantum error-correcting codes with one encoded qubit, the probability of not getting the intended qubit is at least some δ > 0, independently of the number of qubits used for encoding.)\n\nConjecture 2. A noisy quantum computer is subject to noise in which information leaks for two substantially entangled qubits have a substantial positive correlation.\n\nConjecture 3. In any quantum computer at a highly entangled state there will be a strong effect of error-synchronization.\n\nConjecture 4. Noisy quantum processes are subject to detrimental noise.\n\n\n"}
{"id": "44816377", "url": "https://en.wikipedia.org/wiki?curid=44816377", "title": "Government Statistical Service", "text": "Government Statistical Service\n\nThe Government Statistical Service (GSS) is the community of all civil servants who work in the collection, production and communication of UK official statistics. It includes not only statisticians, but also economists, social researchers, IT professionals, and secretarial and clerical staff. Members of the GSS work in the Office for National Statistics, most UK Government departments, and the devolved administrations. The GSS publishes around 2,000 sets of statistics each year, as well as providing professional advice and\nanalysis to decision-makers.\n\nThe National Statistician is the Head of the GSS.\n\nThe Government Statistician Group (GSG) is a subset of the GSS, and is the community of professional government statisticians who meet the standards for statistical qualifications and competence set by the National Statistician's Office.\n\nThe GSS was formed in 1968, in response to a series of recommendations made by Claus Moser, director of the Central Statistical Office, who recognised that 'society was going through radical changes, and social and economic policy was being made on incorrect and out of date statistics. Data was not being shared efficiently across government delaying its use in decisions. There was duplication of work in some areas and gaps in others. It lacked a joined-up approach across government.'\n\n"}
{"id": "242549", "url": "https://en.wikipedia.org/wiki?curid=242549", "title": "Hampshire College Summer Studies in Mathematics", "text": "Hampshire College Summer Studies in Mathematics\n\nThe Hampshire College Summer Studies in Mathematics (HCSSiM) is an American residential program for mathematically talented high school students. The program has been conducted each summer since 1971, with the exceptions of 1981 and 1996, and has more than 1500 alumni.\n\nThe program was created and is still headed by Professor David C. Kelly. \n\nThe program is housed at Hampshire College in Amherst, Massachusetts, and generally runs for six weeks from early July until mid-August. The program itself consists of lectures, study sessions, math workshops (general-knowledge classes), maxi-courses (three-week classes run by the senior staff members), and mini-courses (specialized shorter classes).\n\nOn a typical day, students spend four hours in the morning in class, have lunch together with the faculty, and then have several hours to use at their leisure. During this \"down time\" students and faculty members often host quasis, where they participate in an activity as a small group, such as juggling or making sushi. They return for the \"Prime Time Theorem\" (an hour-long talk on an interesting piece of mathematics given by a faculty member or a visitor), have dinner, and then spend three hours in a problem solving session. One of the instructors blogged the content of her class.\n\nMany students go on to professional careers in mathematics. An occasional publication has resulted from work done at the program. Well-known alumni of the program include two MacArthur Fellows, Eric Lander and Erik Winfree, as well as Lisa Randall, Dana Randall, and Eugene Volokh. Many alumni return to the campus for a few days around Yellow Pig's Day (July 17) of each year. This observance was formalized for 2006 in \"Yellow Pig Math Days,\" which was conducted in observance of 2006 being the 34th offering of the HCSSiM Program (34 being a multiple of 17). \n\nThe Summer Studies has been funded in the past by the American Mathematical Society and the U.S. National Science Foundation.\n\n\n"}
{"id": "13665", "url": "https://en.wikipedia.org/wiki?curid=13665", "title": "Hausdorff maximal principle", "text": "Hausdorff maximal principle\n\nIn mathematics, the Hausdorff maximal principle is an alternate and earlier formulation of Zorn's lemma proved by Felix Hausdorff in 1914 (Moore 1982:168). It states that in any partially ordered set, every totally ordered subset is contained in a maximal totally ordered subset.\n\nThe Hausdorff maximal principle is one of many statements equivalent to the axiom of choice over ZF (Zermelo–Fraenkel set theory without the axiom of choice). The principle is also called the Hausdorff maximality theorem or the Kuratowski lemma (Kelley 1955:33).\n\nThe Hausdorff maximal principle states that, in any partially ordered set, every totally ordered subset is contained in a maximal totally ordered subset. Here a maximal totally ordered subset is one that, if enlarged in any way, does not remain totally ordered. The maximal set produced by the principle is not unique, in general; there may be many maximal totally ordered subsets containing a given totally ordered subset.\n\nAn equivalent form of the principle is that in every partially ordered set there exists a maximal totally ordered subset.\n\nTo prove that it follows from the original form, let \"A\" be a poset. Then formula_1 is a totally ordered subset of \"A\", hence there exists a maximal totally ordered subset containing formula_1, in particular \"A\" contains a maximal totally ordered subset.\n\nFor the converse direction, let \"A\" be a partially ordered set and \"T\" a totally ordered subset of \"A\". Then\nis partially ordered by set inclusion formula_4, therefore it contains a maximal totally ordered subset \"P\". Then the set formula_5 satisfies the desired properties.\n\nThe proof that the Hausdorff maximal principle is equivalent to Zorn's lemma is very similar to this proof.\n\nEXAMPLE 1. If \"A\" is any collection of sets, the relation \"is a proper subset of\" is a strict partial order on \"A\". Suppose that \"A\" is the collection of all circular regions (interiors of circles) in the plane. One maximal totally ordered sub-collection of \"A\" consists of all circular regions with centers at the origin. Another maximal totally ordered sub-collection consists of all circular regions bounded by circles tangent from the right to the y-axis at the origin.\n\nEXAMPLE 2. If (x, y) and (x, y) are two points of the plane ℝ, define (x, y) < (x, y)\n\nif y = y and x < x. This is a partial ordering of ℝ under which two points are comparable only if they lie on the same horizontal line. The maximal totally ordered sets are horizontal lines in ℝ.\n\n\n"}
{"id": "2636072", "url": "https://en.wikipedia.org/wiki?curid=2636072", "title": "Higher-order abstract syntax", "text": "Higher-order abstract syntax\n\nIn computer science, higher-order abstract syntax (abbreviated HOAS) is a technique for the representation of abstract syntax trees for languages with variable binders.\n\nAn abstract syntax tree is \"abstract\" because it is a mathematical object that has certain structure by its very nature. For instance, in \"first-order abstract syntax\" (\"FOAS\") trees, as commonly used in compilers, the tree structure implies the subexpression relation, meaning that no parentheses are required to disambiguate programs (as they are in the concrete syntax). HOAS exposes additional structure: the relationship between variables and their binding sites. In FOAS representations, a variable is typically represented with an identifier, with the relation between binding site and use being indicated by using the \"same\" identifier. With HOAS, there is no name for the variable; each use of the variable refers directly to the binding site.\n\nThere are a number of reasons why this technique is useful. First, it makes the binding structure of a program explicit: just as there is no need to explain operator precedence in a FOAS representation, there is no need to have the rules of binding and scope at hand to interpret a HOAS representation. Second, programs that are \nalpha-equivalent (differing only in the names of bound variables) have identical representations in HOAS, which can make equivalence checking more efficient. \n\nOne mathematical object that could be used to implement HOAS is a graph where variables are associated with their binding sites via edges. Another popular way to implement HOAS (in, for example, compilers) is with de Bruijn indices.\n\nIn the domain of logical frameworks, the term higher-order abstract syntax is usually used to refer to a specific representation that uses the binders of the meta-language to encode the binding structure of the object language.\n\nFor instance, the logical framework LF has a λ-construct, which has arrow\n(→) type. A first-order encoding of an object language construct codice_1 would be (using Twelf\nsyntax):\n\nHere, codice_2 is the family of object language expressions. The family codice_3 is the representation of variables (implemented perhaps as natural numbers, which is not shown); the constant codice_4 witnesses the fact that variables are expressions. The constant codice_1 is an expression that takes three arguments: an expression (that is being bound), a variable (that it is bound to) and another expression (that the variable is bound within).\n\nThe canonical HOAS representation of the same object language would be:\n\nIn this representation, object level variables do not appear explicitly. The constant codice_1 takes an expression (that is being bound) and a meta-level function codice_2 → codice_2 \n(the body of the let). This function is the \"higher-order\" part: an expression with a free variable is\nrepresented as an expression with \"holes\" that are filled in by the meta-level function when applied. As a concrete example, we would construct the object level expression\n\n(assuming the natural constructors for numbers and addition) using the HOAS signature above as\n\nwhere codice_9 is Twelf's syntax for the function formula_1.\n\nThis specific representation has advantages beyond the ones above: for one, by reusing the meta-level notion of binding, the encoding enjoys properties such as type-preserving \"substitution\" without the need to define/prove them. In this way using HOAS can drastically reduce the amount of boilerplate code having to do with binding in an encoding.\n\nHigher-order abstract syntax is generally only applicable when object language variables can be understood as variables in the mathematical sense (that is, as stand-ins for arbitrary members of some domain). This is often, but not always, the case: for instance, there are no advantages to be gained from a HOAS encoding of dynamic scope as it appears in some dialects of Lisp because dynamically scoped variables do not act like mathematical variables.\n\n"}
{"id": "13849747", "url": "https://en.wikipedia.org/wiki?curid=13849747", "title": "Ian G. Enting", "text": "Ian G. Enting\n\nIan Enting (born 25 September 1948) is a mathematical physicist and the AMSI/MASCOS Professorial Fellow at the ARC Centre of Excellence for Mathematics and Statistics of Complex Systems (MASCOS) based at The University of Melbourne.\n\nEnting is the author of \"Twisted, The Distorted Mathematics of Greenhouse Denial\" in which he analyses the presentation and use of data by climate change deniers.\n\nMore recently he has been addressing the claims made in Ian Plimer's book \"Heaven and Earth\". He has published a critique, \"Ian Plimer’s ‘Heaven + Earth’ — Checking the Claims\", listing what Enting claims are numerous misrepresentations of the sources cited in the book.\n\nFrom 1980 to 2004 he worked in CSIRO Atmospheric Research, primarily on modelling the global carbon cycle.\n\nHe was one of the lead authors of the chapter \" and the Carbon Cycle\" in the 1994 IPCC report on \"Radiative Forcing of Climate\".\n\nEnting has published scientific papers, on mathematical physics and carbon cycle modelling, and a monograph on mathematical techniques for interpreting observations of carbon dioxide () and other trace gases.\n\n"}
{"id": "671882", "url": "https://en.wikipedia.org/wiki?curid=671882", "title": "Infrared fixed point", "text": "Infrared fixed point\n\nIn physics, an infrared fixed point is a set of coupling constants, or other parameters that evolve from initial values at very high energies (short distance), to fixed stable values, usually predictable, at low energies (large distance). This usually involves the use of the renormalization group, which specifically details the way parameters in a physical system (a quantum field theory) depend on the energy scale being probed.\n\nConversely, if the length-scale decreases and the physical parameters approach fixed values, then we have ultraviolet fixed points. The fixed points are generally independent of the initial values of the parameters over a large range of the initial values. This is known as universality.\n\nIn the statistical physics of second order phase transitions, the physical system approaches an infrared fixed point that is independent of the\ninitial short distance dynamics that defines the material. This determines the properties of the phase transition at the critical temperature, or critical point. Observables, such as critical exponents usually depend only upon dimension of space, and are independent of the atomic or molecular constituents.\n\nThere is a remarkable infrared fixed point of the coupling constants that determine the masses of very heavy quarks. In the Standard Model, quarks and leptons have \"Yukawa couplings\" to the Higgs boson. These determine the mass of the particle. All of the quarks' and leptons' Yukawa couplings are small compared to the top quark's Yukawa coupling. Yukawa couplings are not constants and their properties change depending on the energy scale at which they are measured, this is known as \"running\" of the constants. The dynamics of Yukawa couplings are determined by the renormalization group equation:\n\nformula_1,\n\nwhere formula_2 is the color gauge coupling (which is a function of formula_3 and associated with asymptotic freedom) and formula_4 is the Yukawa coupling. This equation describes how the Yukawa coupling changes with energy scale formula_3.\n\nThe Yukawa couplings of the up, down, charm, strange and bottom quarks, are small at the extremely high energy scale of grand unification, formula_6 GeV. Therefore, the formula_7 term can be neglected in the above equation. Solving, we then find that formula_4 is increased slightly at the low energy scales at which the quark masses are generated by the Higgs, formula_9 GeV.\n\nOn the other hand, solutions to this equation for large initial values formula_4 cause the \"rhs\" to quickly approach zero . This locks formula_4 to the QCD coupling formula_2 . This is known as a (quasi-infrared) fixed point of the renormalization group equation for the Yukawa coupling. No matter what the initial starting value of the coupling is, if it is sufficiently large it will reach this fixed point value, and the corresponding quark mass is predicted.\n\nThe value of the fixed point is fairly precisely determined in the Standard Model, leading to a predicted top quark mass of 230  GeV. If there is more than one Higgs doublet, the value will be reduced by Higgs mixing angle effects. The observed top quark mass is slightly lower, about 171 GeV.\n\nIn the minimal supersymmetric extension of the Standard Model (MSSM), there are two Higgs doublets and the renormalization group equation for the top quark Yukawa coupling is slightly modified. This leads to a fixed point where the top mass is smaller, 170–200 GeV. Some theorists believe this is supporting evidence for the MSSM.\n\nThe \"quasi-infrared fixed point\" was proposed in 1981 by C. T. Hill, B. Pendleton and G. G. Ross. The prevailing view at the time was that the top quark mass would lie in a range of 15 to 26 GeV. The quasi-infrared fixed point has formed the basis of top quark condensation theories of electroweak symmetry breaking in which the Higgs boson is composite at \"extremely\" short distance scales, composed of a pair of top and anti-top quarks. Many authors have explored other aspects of infrared fixed points to understand the anticipated spectrum of Higgs bosons in multi-Higgs models.\n\nAnother example of an infrared fixed point is the Banks-Zaks fixed point in which the coupling constant of a Yang-Mills theory evolves to a fixed value. The beta-function vanishes, and the theory possesses a symmetry known as conformal symmetry.\n\n"}
{"id": "8460224", "url": "https://en.wikipedia.org/wiki?curid=8460224", "title": "International Conference on Differential Geometric Methods in Theoretical Physics", "text": "International Conference on Differential Geometric Methods in Theoretical Physics\n\nInternational Conference on Differential Geometric Methods in Theoretical Physics are congresses held every few years on the subject of Differential geometric methods in Theoretical physics. Lectures, seminars, and discussions are held in different universities throughout the world, every few years, and a book compilation is published thereafter consisting of the papers submitted and discussed at the conference.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "160562", "url": "https://en.wikipedia.org/wiki?curid=160562", "title": "KY-57", "text": "KY-57\n\nThe Speech Security Equipment (VINSON), TSEC/KY-57, is a portable, tactical cryptographic device in the VINSON family, designed to provide voice encryption for a range of military communication devices such as radio or telephone.\n\nThe KY-57 was in use by NATO and its allies towards the end of the cold war. The device itself was classified as a CCI (controlled cryptographic item) when it was unkeyed. The classification of the device was temporarily raised to the classification of the key when the device was keyed. It was authorized for TOP SECRET information with the appropriate key. It is no longer authorized for handling classified information, and it has been de facto, but not officially, declassified. The details of its technical operation are still classified. The first unit, serial number 001 is still in operation at the NSA.\n\nThe KY-57 can accept signal fades of up to 12 seconds without losing synchronization with the transmitting station. There are storage positions for 6 keys. Keys 1 to 5 are traffic encryption keys (TEK). Key 6 is a key encryption key (KEK) used for over the air rekeying (OTAR) of the other 5 keys. Key 6 must be loaded manually using a fill device such as the AN/CYZ-10.\n\n"}
{"id": "9626825", "url": "https://en.wikipedia.org/wiki?curid=9626825", "title": "LILI-128", "text": "LILI-128\n\nLILI-128 is an LFSR based synchronous stream cipher with a 128-bit key. On 13 November 2000, LILI-128 was presented at the NESSIE workshop. It is designed to be simple to implement in both software and hardware.\n\nIn 2007, LILI-128 was totally broken by using a notebook running MATLAB in 1.61 hours.\n"}
{"id": "36423541", "url": "https://en.wikipedia.org/wiki?curid=36423541", "title": "List of definite integrals", "text": "List of definite integrals\n\nIn mathematics, the definite integral:\n\nis the area of the region in the \"xy\"-plane bounded by the graph of \"f\", the \"x\"-axis, and the lines \"x\" = \"a\" and \"x\" = \"b\", such that area above the \"x\"-axis adds to the total, and that below the \"x\"-axis subtracts from the total.\n\nThe fundamental theorem of calculus establishes the relationship between indefinite and definite integrals and introduces a technique for evaluating definite integrals.\n\nIf the interval is infinite the definite integral is called an \"improper integral\" and defined by using appropriate limiting procedures. for example:\n\nA constant, such pi, that may be defined by the integral of an algebraic function over an algebraic domain is known as a period.\n\nThe following is a list of the most common definite Integrals. For a list of indefinite integrals see \"List of indefinite integrals\"\n\nformula_73\n\nformula_74\n\nformula_75\n\nformula_76\n\nformula_77 holds if the integral exists and formula_78 is continuous.\n\n\n"}
{"id": "37580589", "url": "https://en.wikipedia.org/wiki?curid=37580589", "title": "Locally finite operator", "text": "Locally finite operator\n\nIn mathematics, a linear operator formula_1 is called locally finite if the space formula_2 is the union of a family of finite-dimensional formula_3-invariant subspaces.\n\nIn other words, there exists a family formula_4 of linear subspaces of formula_2, such that we have the following:\n\n"}
{"id": "3165509", "url": "https://en.wikipedia.org/wiki?curid=3165509", "title": "Lower convex envelope", "text": "Lower convex envelope\n\nIn mathematics, the lower convex envelope formula_1 of a function formula_2 defined on an interval formula_3 is defined at each point of the interval as the supremum of all convex functions that lie under that function, i.e.\n\n"}
{"id": "213597", "url": "https://en.wikipedia.org/wiki?curid=213597", "title": "Magic star", "text": "Magic star\n\nAn \"n\"-pointed magic star is a star polygon with Schläfli symbol {\"n\"/2} in which numbers are placed at each of the \"n\" vertices and \"n\" intersections, such that the four numbers on each line sum to the same magic constant. A normal magic star contains the consecutive integers 1 to 2\"n\". No numbers are ever repeated. The magic constant of an \"n\"-pointed normal magic star is \"M\" = 4\"n\" + 2.\n\nNo star polygons with fewer than 5 points exist, and the construction of a normal 5-pointed magic star turns out to be impossible. It can be proven that there exists no 4-pointed star that will satisfy the conditions here. The smallest examples of normal magic stars are therefore 6-pointed. Some examples are given below. Notice that for specific values of \"n\", the \"n\"-pointed magic stars are also known as \"magic hexagram\" etc.\n\n"}
{"id": "437979", "url": "https://en.wikipedia.org/wiki?curid=437979", "title": "Midpoint", "text": "Midpoint\n\nIn geometry, the midpoint is the middle point of a line segment. It is equidistant from both endpoints, and it is the centroid both of the segment and of the endpoints. It bisects the segment.\n\nThe midpoint of a segment in \"n\"-dimensional space whose endpoints are formula_1 and formula_2 is given by\n\nThat is, the \"i\" coordinate of the midpoint (\"i\" = 1, 2, ..., \"n\") is\n\nGiven two points of interest, finding the midpoint of the line segment they determine can be accomplished by a compass and straightedge construction. The midpoint of a line segment, embedded in a plane, can be located by first constructing a lens using circular arcs of equal (and large enough) radii centered at the two endpoints, then connecting the cusps of the lens (the two points where the arcs intersect). The point where the line connecting the cusps intersects the segment is then the midpoint of the segment. It is more challenging to locate the midpoint using only a compass, but it is still possible according to the Mohr-Mascheroni theorem.\n\nThe midpoint of any diameter of a circle is the center of the circle.\n\nAny line perpendicular to any chord of a circle and passing through its midpoint also passes through the circle's center.\n\nThe butterfly theorem states that, if \"M\" is the midpoint of a chord \"PQ\" of a circle, through which two other chords \"AB\" and \"CD\" are drawn, then \"AD\" and \"BC\" intersect chord \"PQ\" at \"X\" and \"Y\" respectively, such that \"M\" is the midpoint of \"XY\".\n\nThe midpoint of any segment which is an area bisector or perimeter bisector of an ellipse is the ellipse's center.\n\nThe ellipse's center is also the midpoint of a segment connecting the two foci of the ellipse.\n\nThe midpoint of a segment connecting a hyperbola's vertices is the center of the hyperbola.\n\nThe perpendicular bisector of a side of a triangle is the line that is perpendicular to that side and passes through its midpoint. The three perpendicular bisectors of a triangle's three sides intersect at the circumcenter (the center of the circle through the three vertices). \n\nThe median of a triangle's side passes through both the side's midpoint and the triangle's opposite vertex. The three medians of a triangle intersect at the triangle's centroid (the point on which the triangle would balance if it were made of a thin sheet of uniform-density metal).\n\nThe nine-point center of a triangle lies at the midpoint between the circumcenter and the orthocenter. These points are all on the Euler line.\n\nA \"midsegment\" (or \"midline\") of a triangle is a line segment that joins the midpoints of two sides of the triangle. It is parallel to the third side and has a length equal to one half of that third side.\n\nThe medial triangle of a given triangle has vertices at the midpoints of the given triangle's sides, therefore its sides are the three midsegments of the given triangle. It shares the same centroid and medians with the given triangle. The perimeter of the medial triangle equals the semiperimeter (half the perimeter) of the original triangle, and its area is one quarter of the area of the original triangle. The orthocenter (intersection of the altitudes) of the medial triangle coincides with the circumcenter (center of the circle through the vertices) of the original triangle.\n\nEvery triangle has an inscribed ellipse, called its Steiner inellipse, that is internally tangent to the triangle at the midpoints of all its sides. This ellipse is centered at the triangle's centroid, and it has the largest area of any ellipse inscribed in the triangle.\n\nIn a right triangle, the circumcenter is the midpoint of the hypotenuse.\n\nIn an isosceles triangle, the median, altitude, and perpendicular bisector from the base side and the angle bisector of the apex coincide with the Euler line and the axis of symmetry, and these coinciding lines go through the midpoint of the base side.\n\nThe two bimedians of a convex quadrilateral are the line segments that connect the midpoints of opposite sides, hence each bisecting two sides. The two bimedians and the line segment joining the midpoints of the diagonals are concurrent at (all intersect at)a point called the \"vertex centroid\", which is the midpoint of all three of these segments.\n\nThe four \"maltitudes\" of a convex quadrilateral are the perpendiculars to a side through the midpoint of the opposite side, hence bisecting the latter side. If the quadrilateral is cyclic (inscribed in a circle), these maltitudes all meet at a common point called the \"anticenter\".\n\nBrahmagupta's theorem states that if a cyclic quadrilateral is orthodiagonal (that is, has perpendicular diagonals), then the perpendicular to a side from the point of intersection of the diagonals always goes through the midpoint of the opposite side.\n\nVarignon's theorem states that the midpoints of the sides of an arbitrary quadrilateral form the vertices of a parallelogram, and if the quadrilateral is not self-intersecting then the area of the parallelogram is half the area of the quadrilateral.\n\nThe Newton line is the line that connects the midpoints of the two diagonals in a convex quadrilateral that is not a parallelogram. The line segments connecting the midpoints of opposite sides of a convex quadrilateral intersect in a point that lies on the Newton line.\n\nA regular polygon has an inscribed circle which is tangent to each side of the polygon at its midpoint.\n\nIn a regular polygon with an even number of sides, the midpoint of a diagonal between opposite vertices is the polygon's center.\n\nThe midpoint-stretching polygon of a cyclic polygon (a polygon whose vertices all fall on the same circle) is another cyclic polygon inscribed in the same circle, the polygon whose vertices are the midpoints of the circular arcs between the vertices of . Iterating the midpoint-stretching operation on an arbitrary initial polygon results in a sequence of polygons whose shapes converge to that of a regular polygon.\n\nThe abovementioned formulas for the midpoint of a segment implicitly use the lengths of segments. However, in the generalization to affine geometry, where segment lengths are not defined, the midpoint can still be defined since it is an affine invariant. The synthetic affine definition of the midpoint of a segment is the projective harmonic conjugate of the point at infinity, , of the line . That is, the point such that . When coordinates can be introduced in an affine geometry, the two definitions of midpoint will coincide.\n\nThe midpoint is not naturally defined in projective geometry since there is no distinguished point to play the role of the point at infinity (any point in a projective range may be projectively mapped to any other point in (the same or some other) projective range). However, fixing a point at infinity defines an affine structure on the projective line in question and the above definition can be applied. \n\nThe definition of the midpoint of a segment may be extended to geodesic arcs on a Riemannian manifold. Note that, unlike in the affine case, the \"midpoint\" between two points may not be uniquely determined.\n\n\n"}
{"id": "827305", "url": "https://en.wikipedia.org/wiki?curid=827305", "title": "Noncommutative quantum field theory", "text": "Noncommutative quantum field theory\n\nIn mathematical physics, noncommutative quantum field theory (or quantum field theory on noncommutative spacetime) is an application of noncommutative mathematics to the spacetime of quantum field theory that is an outgrowth of noncommutative geometry and index theory in which the coordinate functions are noncommutative. One commonly studied version of such theories has the \"canonical\" commutation relation:\n\nwhich means that (with any given set of axes), it is impossible to accurately measure the position of a particle with respect to more than one axis. In fact, this leads to an uncertainty relation for the coordinates analogous to the Heisenberg uncertainty principle.\n\nVarious lower limits have been claimed for the noncommutative scale, (i.e. how accurately positions can be measured) but there is currently no experimental evidence in favour of such a theory or grounds for ruling them out.\n\nOne of the novel features of noncommutative field theories is the UV/IR mixing phenomenon in which the physics at high energies affects the physics at low energies which does not occur in quantum field theories in which the coordinates commute.\n\nOther features include violation of Lorentz invariance due to the preferred direction of noncommutativity. Relativistic invariance can however be retained in the sense of twisted Poincaré invariance of the theory. The causality condition is modified from that of the commutative theories.\n\nHeisenberg was the first to suggest extending noncommutativity to the coordinates as a possible way of removing the infinite quantities appearing in field theories before the renormalization procedure was developed and had gained acceptance. The first paper on the subject was published in 1947 by Hartland Snyder. The success of the renormalization method resulted in little attention being paid to the subject for some time. In the 1980s, mathematicians, most notably Alain Connes, developed noncommutative geometry. Among other things, this work generalized the notion of differential structure to a noncommutative setting. This led to an operator algebraic description of noncommutative space-times, with the problem that it classically corresponds to a manifold with positively defined metric tensor, so that there is no description of (noncommutative) causality in this approach. However it also led to the development of a Yang–Mills theory on a noncommutative torus.\n\nThe particle physics community became interested in the noncommutative approach because of a paper by Nathan Seiberg and Edward Witten. They argued in the context of string theory that the coordinate functions of the endpoints of open strings constrained to a D-brane in the presence of a constant Neveu-Schwarz B-field—equivalent to a constant magnetic field on the brane—would satisfy the noncommutative algebra set out above. The implication is that a quantum field theory on noncommutative spacetime can be interpreted as a low energy limit of the theory of open strings.\n\nTwo papers, one by Sergio Doplicher, Klaus Fredenhagen and John Roberts \nand the other by D. V. Ahluwalia,\nset out another motivation for the possible noncommutativity of space-time. \nThe arguments go as follows: According to general relativity, when the energy density grows sufficiently large, a black hole is formed. On the other hand, according to the Heisenberg uncertainty principle, a measurement of a space-time separation causes an uncertainty in momentum inversely proportional to the extent of the separation. Thus energy whose scale corresponds to the uncertainty in momentum is localized in the system within a region corresponding to the uncertainty in position. When the separation is small enough, the Schwarzschild radius of the system is reached and a black hole is formed, which prevents any information from escaping the system. Thus there is a lower bound for the measurement of length. A sufficient condition for preventing gravitational collapse can be expressed as an uncertainty relation for the coordinates. This relation can in turn be derived from a commutation relation for the coordinates.\n\nIt is worth stressing that, differently from other approaches, in particular those relying upon Connes' ideas, here the noncommutative spacetime is a proper spacetime, i.e. it extends the idea of a four-dimensional pseudo-Riemannian manifold. On the other hand, differently from Connes' noncommutative geometry, the proposed model turns out to be coordinates dependent from scratch.\nIn Doplicher Fredenhagen Roberts' paper noncommutativity of coordinates concerns all four spacetime coordinates and not only spatial ones.\n\n\n"}
{"id": "379840", "url": "https://en.wikipedia.org/wiki?curid=379840", "title": "One-sided limit", "text": "One-sided limit\n\nIn calculus, a one-sided limit is either of the two limits of a function \"f\"(\"x\") of a real variable \"x\" as \"x\" approaches a specified point either from below or from above. One should write either:\n\nfor the limit as \"x\" decreases in value approaching \"a\" (\"x\" approaches \"a\" \"from the right\" or \"from above\"), and similarly\n\nfor the limit as \"x\" increases in value approaching \"a\" (\"x\" approaches \"a\" \"from the left\" or \"from below\"). In probability theory it is common to use the short notation\n\nThe two one-sided limits exist and are equal if the limit of \"f\"(\"x\") as \"x\" approaches \"a\" exists. In some cases in which the limit\n\ndoes not exist, the two one-sided limits nonetheless exist. Consequently, the limit as \"x\" approaches \"a\" is sometimes called a \"two-sided limit\". In some cases one of the two one-sided limits exists and the other does not, and in some cases neither exists.\n\nThe right-sided limit can be rigorously defined as\n\nand the left-sided limit can be rigorously defined as\n\nwhere represents some interval that is within the domain of .\n\nOne example of a function with different one-sided limits is the following:\n\nwhereas\n\nThe one-sided limit to a point \"p\" corresponds to the general definition of limit, with the domain of the function restricted to one side, by either allowing that the function domain is a subset of the topological space, or by considering a one-sided subspace, including \"p\". Alternatively, one may consider the domain with a half-open interval topology.\n\nA noteworthy theorem treating one-sided limits of certain power series at the boundaries of their intervals of convergence is Abel's theorem.\n\n"}
{"id": "49666935", "url": "https://en.wikipedia.org/wiki?curid=49666935", "title": "Order polynomial", "text": "Order polynomial\n\nThe order polynomial is a polynomial studied in mathematics, in particular in algebraic graph theory and algebraic combinatorics. The order polynomial counts the number of order-preserving maps from a poset to a chain of length formula_1. These order-preserving maps were first introduced by Richard P. Stanley while studying ordered structures and partitions as a Ph.D. student at Harvard University in 1971 under the guidance of Gian-Carlo Rota. \n\nLet formula_2 be a finite poset, formula_3 and formula_4 be points in formula_2 and formula_6 be a chain of length formula_1. A map formula_8 is order-preserving if formula_9 implies formula_10. The number of such maps grows polynomially with formula_1, and the function that counts their number as a function of formula_1 is the order polynomial formula_13.\n\nSimilarly, we can define an order polynomial that counts the number of strictly order-preserving maps. A map formula_14 is strictly order-preserving if formula_15 implies formula_16. The order polynomial that counts the number of these maps is denoted by formula_17.\n\nLet formula_18 be a chain of length formula_19. Then formula_20 and formula_21\n\nWe can also consider the poset formula_18 with formula_23 disjoint points. Then the number of order-preserving maps to a chain of length formula_24 is formula_25, and formula_26. \n\nThe reciprocity theorem shows that there is a relation between strictly order-preserving maps and order-preserving maps. In addition, this comes hand in hand with important properties that the chromatic polynomial and Ehrhart polynomial share. The relation can be stated as follows:\nIn the case that formula_2 is a chain, this recovers the negative binomial identity.\n\nThe chromatic polynomial counts the number of proper ways to color a graph. Let formula_29 be a finite graph and let formula_30 be an acyclic orientation of formula_31. Then there is a natural binary relation on the vertices formula_32 of formula_31 defined as formula_34 if formula_35. In particular, formula_36 is a partial ordering on the set of vertices formula_37 of formula_31. In addition, formula_39 is compatible with formula_30 if formula_41 is order-preserving. Then, we can conclude that \nwhere formula_30 are all acyclic orientations of \"G\".\n\nThe Ehrhart polynomial is a polynomial that associates the number of integer lattice points and the expansion of a polytope formula_2 by a factor of formula_45. In other words, consider the lattice formula_46 and a formula_47-dimensional polytope in formula_48 with integer vertices. Let formula_45 be a positive integer, and formula_50 be a dilation of formula_2 so \nis the number of lattice points in formula_50. Eugène Ehrhart showed that this was a rational polynomial of degree formula_47 in formula_45.\n\nThe order polytope associates a polytope with a partial order. Let formula_2 be a poset with formula_23 elements. Then the order polytope, denoted formula_58, is the set of points in the formula_23-dimensional unit cube such that the coordinates satisfy the partial order.\n\nMore formally, let formula_60 be the set of all functions from formula_2 to formula_62. The order polytope formula_58 of the poset formula_2 is the set of all maps formula_41 in formula_60 which satisfy the following two conditions. First, formula_67 for all formula_68, and second, formula_69 if formula_70 in the ordering of formula_2. Thus, the polytope can be created given a poset and a partial order.\n\nThe volume of the order polytope is given by the leading coefficient of the order polynomial, which is the number of linear extensions of the poset formula_2 divided by formula_73.\n"}
{"id": "5078522", "url": "https://en.wikipedia.org/wiki?curid=5078522", "title": "Regular tree grammar", "text": "Regular tree grammar\n\nIn theoretical computer science and formal language theory, a regular tree grammar (RTG) is a formal grammar that describes a set of directed trees, or terms. A regular word grammar can be seen as a special kind of regular tree grammar, describing a set of single-path trees.\n\nA regular tree grammar \"G\" is defined by the tuple\n\n\"G\" = (\"N\", Σ, \"Z\", \"P\"),\n\nwhere\n\n\nThe grammar \"G\" implicitly defines a set of trees: any tree that can be derived from \"Z\" using the rule set \"P\" is said to be described by \"G\".\nThis set of trees is known as the language of \"G\".\nMore formally, the relation ⇒ on the set \"T\"(\"N\") is defined as follows:\n\nA tree \"t\"∈ \"T\"(\"N\") can be derived in a single step into a tree \"t\" ∈ \"T\"(\"N\") \n(in short: \"t\" ⇒ \"t\"), if there is a context \"S\" and a production (\"A\"→\"t\") ∈ \"P\" such that:\n\n\nHere, a \"context\" means a tree with exactly one hole in it; if \"S\" is such a context, \"S\"[\"t\"] denotes the result of filling the tree \"t\" into the hole of \"S\".\n\nThe tree language generated by \"G\" is the language \"L\"(\"G\") = { \"t\" ∈ \"T\" | \"Z\" ⇒ \"t\" }.\n\nHere, \"T\" denotes the set of all trees composed from symbols of Σ, while ⇒ denotes successive applications of ⇒.\n\nA language generated by some regular tree grammar is called a regular tree language.\n\nLet \"G\" = (\"N\",Σ,\"Z\",\"P\"), where\n\nAn example derivation from the grammar \"G\" is\n\n\"BList\"\n⇒ \"cons\"(\"Bool\",\"BList\")\n⇒ \"cons\"(\"false\",\"cons\"(\"Bool\",\"BList\"))\n⇒ \"cons\"(\"false\",\"cons\"(\"true\",\"nil\")).\n\nThe image shows the corresponding derivation tree; it is a tree of trees (main picture), whereas a derivation tree in word grammars is a tree of strings (upper left table).\n\nThe tree language generated by \"G\" is the set of all finite lists of boolean values, that is, \"L\"(\"G\") happens to equal \"T\".\nThe grammar \"G\" corresponds to the algebraic data type declarations (in the Standard ML programming language):\nEvery member of \"L\"(\"G\") corresponds to a Standard-ML value of type BList.\n\nFor another example, let \"G\" = (\"N\",Σ,\"BList1\",\"P\" ∪ \"P\"), using the nonterminal set and the alphabet from above, but extending the production set by \"P\", consisting of the following productions:\nThe language \"L\"(\"G\") is the set of all finite lists of boolean values that contain \"true\" at least once. The set \"L\"(\"G\") has no datatype counterpart in Standard ML, nor in any other functional language.\nIt is a proper subset of \"L\"(\"G\").\nThe above example term happens to be in \"L\"(\"G\"), too, as the following derivation shows:\n\n\"BList1\"\n⇒ \"cons\"(\"false\",\"BList1\")\n⇒ \"cons\"(\"false\",\"cons\"(\"true\",\"BList\"))\n⇒ \"cons\"(\"false\",\"cons\"(\"true\",\"nil\")).\n\nIf \"L\", \"L\" both are regular tree languages, then the tree sets \"L\" ∩ \"L\", \"L\" ∪ \"L\", and \"L\" \\ \"L\" are also regular tree languages, and it is decidable whether \"L\" ⊆ \"L\", and whether \"L\" = \"L\".\n\n\nApplications of regular tree grammars include:\n\n\n"}
{"id": "35152264", "url": "https://en.wikipedia.org/wiki?curid=35152264", "title": "Shimura's reciprocity law", "text": "Shimura's reciprocity law\n\nIn mathematics, Shimura's reciprocity law, introduced by , describes the action of ideles of imaginary quadratic fields on the values of modular functions at singular moduli. It forms a part of the Kronecker Jugendtraum, explicit class field theory for such fields. There are also higher-dimensional generalizations.\n"}
{"id": "35194349", "url": "https://en.wikipedia.org/wiki?curid=35194349", "title": "Siegel parabolic subgroup", "text": "Siegel parabolic subgroup\n\nIn mathematics, the Siegel parabolic subgroup, named after Carl Ludwig Siegel, is the parabolic subgroup of the symplectic group with abelian radical, given by the matrices of the symplectic group whose lower left quadrant is 0 (for the standard symplectic form).\n"}
{"id": "8227451", "url": "https://en.wikipedia.org/wiki?curid=8227451", "title": "Symmetrization", "text": "Symmetrization\n\nIn mathematics, symmetrization is a process that converts any function in \"n\" variables to a symmetric function in \"n\" variables.\nConversely, anti-symmetrization converts any function in \"n\" variables into an antisymmetric function.\n\nLet formula_1 be a set and formula_2 an abelian group. Given a map formula_3, formula_4 is termed a symmetric map if formula_5 for all formula_6.\n\nThe symmetrization of a map formula_7 is the map formula_8.\n\nSimilarly, the anti-symmetrization or skew-symmetrization of a map formula_7 is the map formula_10.\n\nThe sum of the symmetrization and the anti-symmetrization of a map \"α\" is 2\"α\".\nThus, away from 2, meaning if 2 is invertible, such as for the real numbers, one can divide by 2 and express every function as a sum of a symmetric function and an anti-symmetric function.\n\nThe symmetrization of a symmetric map is its double, while the symmetrization of an alternating map is zero; similarly, the anti-symmetrization of a symmetric map is zero, while the anti-symmetrization of an anti-symmetric map is its double.\n\nThe symmetrization and anti-symmetrization of a bilinear map are bilinear; thus away from 2, every bilinear form is a sum of a symmetric form and a skew-symmetric form, and there is no difference between a symmetric form and a quadratic form.\n\nAt 2, not every form can be decomposed into a symmetric form and a skew-symmetric form – for instance, over the integers, the associated symmetric form (over the rationals) may take half-integer values, while over formula_11 a function is skew-symmetric if and only if it is symmetric (as ). \n\nThis leads to the notion of ε-quadratic forms and ε-symmetric forms.\n\nIn terms of representation theory:\n\nAs the symmetric group of order two equals the cyclic group of order two (formula_12), this corresponds to the discrete Fourier transform of order two.\n\nMore generally, given a function in \"n\" variables, one can symmetrize by taking the sum over all formula_13 permutations of the variables, or anti-symmetrize by taking the sum over all formula_14 even permutations and subtracting the sum over all formula_14 odd permutations (except that when , the only permutation is even).\n\nHere symmetrizing (respectively anti-symmetrizing) a symmetric function multiplies by formula_13 – thus if formula_13 is invertible, such as when working a field of characteristic formula_18 or formula_19 then these yield projections when divided by formula_13.\n\nIn terms of representation theory, these only yield the subrepresentations corresponding to the trivial and sign representation, but for formula_21 there are others – see representation theory of the symmetric group and symmetric polynomials.\n\nGiven a function in \"k\" variables, one can obtain a symmetric function in \"n\" variables by taking the sum over \"k\" element subsets of the variables. In statistics, this is referred to as bootstrapping, and the associated statistics are called U-statistics.\n"}
{"id": "7767296", "url": "https://en.wikipedia.org/wiki?curid=7767296", "title": "T-theory", "text": "T-theory\n\nT-theory is a branch of discrete mathematics dealing with analysis of trees and discrete metric spaces. \n\nT-theory originated from a question raised by Manfred Eigen in the late 1970s. He was trying to fit twenty distinct t-RNA molecules of the \"Escherichia coli\" bacterium into a tree.\n\nAn important concept of T-theory is the tight span of a metric space. If \"X\" is a metric space, the tight span \"T\"(\"X\") of \"X\" is, up to isomorphism, the unique minimal injective metric space that contains \"X\". John Isbell was the first to discover the tight span in 1964, which he called the injective envelope. Andreas Dress independently constructed the same construct, which he called the tight span.\n\n\n\n"}
{"id": "38959571", "url": "https://en.wikipedia.org/wiki?curid=38959571", "title": "Transcriptor", "text": "Transcriptor\n\nA transcriptor is a transistor-like device composed of DNA and RNA rather than a semiconducting material such as silicon. Prior to its invention in 2013, the transcriptor was considered an important component to build biological computers.\n\nTo function, a modern computer needs three different capabilities: It must be able to store information, transmit information between components, and possess a basic system of logic. Prior to March 2013, scientists had successfully demonstrated the ability to store and transmit data using biological components made of proteins and DNA. Simple two-terminal logic gates had been demonstrated, but required multiple layers of inputs and thus were impractical due to scaling difficulties.\n\nOn March 28, 2013, a team of bioengineers from Stanford University led by Drew Endy announced that they had created the biological equivalent of a transistor, which they named a \"transcriptor\". That is, they created a three-terminal device with a logic system that can control other components. The transcriptor regulates the flow of RNA polymerase across a strand of DNA using special combinations of enzymes to control movement. According to project member Jerome Bonnet, \"The choice of enzymes is important. We have been careful to select enzymes that function in bacteria, fungi, plants and animals, so that bio-computers can be engineered within a variety of organisms.\" \n\nTranscriptors can replicate traditional AND, OR, NOR, NAND, XOR, and XNOR gates with equivalents, which Endy dubbed \"Boolean Integrase Logic (BIL) gates\", in a single-layer process (i.e., without requiring multiple instances of the simpler gates to build up more complex ones). Like a traditional transistor, a transcriptor can amplify an input signal. A group of transcriptors can do almost any type of computing, including counting and comparison.\n\nStanford dedicated the BIL gate's design to the public domain, which may speed its adoption. According to Endy, other researchers were already using the gates to reprogram metabolism when the Stanford team published its research.\n\nComputing by transcriptor is still very slow; it can take a few hours between receiving an input signal and generating an output. Endy doubted that biocomputers would ever be as fast as traditional computers, but added that is not the goal of his research. \"We're building computers that will operate in a place where your cellphone isn't going to work\", he said. Medical devices with built-in biological computers could monitor, or even alter, cell behavior from inside a patient's body. \"ExtremeTech\" writes:\nUC Berkeley biochemical engineer Jay Keasling said the transcriptor \"clearly demonstrates the power of synthetic biology and could revolutionize how we compute in the future\".\n\n"}
{"id": "13797188", "url": "https://en.wikipedia.org/wiki?curid=13797188", "title": "Vaught conjecture", "text": "Vaught conjecture\n\nThe Vaught conjecture is a conjecture in the mathematical field of model theory originally proposed by Robert Lawson Vaught in 1961. It states that the number of countable models of a first-order complete theory in a countable language is finite or ℵ or 2. Morley showed that number of countable models is finite or ℵ or ℵ or 2, which solves the conjecture except for the case of ℵ models when the continuum hypothesis fails. For this remaining case, has announced a counterexample to the Vaught conjecture and the topological Vaught conjecture. As of 2016 the counterexample has not been verified.\n\nLet formula_1 be a first-order, countable, complete theory with infinite models. Let formula_2 denote the number of models of T of cardinality formula_3 up to isomorphism, the spectrum of the theory formula_1. Morley proved that if I(T,ℵ) is infinite then it must be ℵ or ℵ or the cardinality of the continuum. The Vaught conjecture is the statement that it is not possible for formula_5. The conjecture is a trivial consequence of the continuum hypothesis; so this axiom is often excluded in work on the conjecture. Alternatively there is a sharper form of the conjecture which states that any countable complete T with uncountably many countable models will have a perfect set of uncountable models (as pointed out by John Steel, in On Vaught's conjecture. Cabal Seminar 76—77 (Proc. Caltech-UCLA Logic Sem., 1976—77), pp. 193–208, Lecture Notes in Math., 689, Springer, Berlin, 1978, this form of the Vaught conjecture is equiprovable with the original).\n\nThe original formulation by Vaught was not stated as a conjecture, but as a problem: \"Can it be proved, without the use of the continuum hypothesis, that there exists a complete theory having exactly ℵ non-isomorphic denumerable models?\" By the result by Morley mentioned at the beginning, a positive solution to the conjecture essentially corresponds to a negative answer to Vaught's problem as originally stated.\n\nVaught proved that the number of countable models of a complete theory cannot be 2. It can be any finite number other than 2, for example:\n\nThe idea of the proof of Vaught's theorem is as follows. If there are at most countably many countable models, then there is a smallest one: the atomic model, and a largest one, the saturated model, which are different if there is more than one model. If they are different, the saturated model must realize some \"n\"-type omitted by the atomic model. Then one can show that an atomic model of the theory of structures realizing this \"n\"-type (in a language expanded by finitely many constants) is a third model, not isomorphic to either the atomic or the saturated model. In the example above with 3 models, the atomic model is the one where the sequence is unbounded, the saturated model is the one where the sequence does not converge, and an example of a type not realized by the atomic model is an element greater than all elements of the sequence.\n\nThe topological Vaught conjecture is the statement that whenever a Polish group acts continuously on a Polish space, there are either countably many orbits or continuum many orbits. The topological Vaught conjecture is more general than the original Vaught conjecture: Given a countable language we can form the space of all structures on the natural numbers for that language. If we equip this with the topology generated by first order formulas, then it is known from A. Gregorczyk, A. Mostowski, C. Ryll-Nardzewski, \"Definability of sets of models of axiomatic theories\", Bulletin of the Polish Academy of Sciences (series Mathematics, Astronomy, Physics), vol. 9(1961), pp. 163–7 that the resulting space is Polish. There is a continuous action of the infinite symmetric group (the collection of all permutations of the natural numbers with the topology of point wise convergence) which gives rise to the equivalence relation of isomorphism. Given a complete first order theory T, the set of structures satisfying T is a minimal, closed invariant set, and hence Polish in its own right.\n\n\n"}
{"id": "9017838", "url": "https://en.wikipedia.org/wiki?curid=9017838", "title": "Virtual fixture", "text": "Virtual fixture\n\nA virtual fixture is an overlay of augmented sensory information upon a user's perception of a real environment in order to improve human performance in both direct and remotely manipulated tasks. Developed in the early 1990s by Louis Rosenberg at the U.S. Air Force Research Laboratory (AFRL), Virtual Fixtures was a pioneering platform in virtual reality and augmented reality technologies.\n\nVirtual Fixtures was first developed by Louis Rosenberg in 1992 at the USAF Armstrong Labs, resulting in the first immersive augmented reality system ever built. Because 3D graphics were too slow in the early 1990s to present a photorealistic and spatially-registered augmented reality, Virtual Fixtures used two real physical robots, controlled by a full upper-body exoskeleton worn by the user. To create the immersive experience for the user, a unique optics configuration was employed that involved a pair of binocular magnifiers aligned so that the user's view of the robot arms were brought forward so as to appear registered in the exact location of the user's real physical arms. The result was a spatially-registered immersive experience in which the user moved his or her arms, while seeing robot arms in the place where his or her arms should be. The system also employed computer-generated virtual overlays in the form of simulated physical barriers, fields, and guides, designed to assist in the user while performing real physical tasks.\n\nFitts Law performance testing was conducted on batteries of human test subjects, demonstrating for the first time, that a significant enhancement in human performance of real-world dexterous tasks could be achieved by providing immersive augmented reality overlays to users.\n\nThe concept of virtual fixtures was first introduced by Rosenberg (1992) as an overlay of virtual sensory information on a workspace in order to improve human performance in direct and remotely manipulated tasks. The virtual sensory overlays can be presented as physically realistic structures, registered in space such that they are perceived by the user to be fully present in the real workspace environment. The virtual sensory overlays can also be abstractions that have properties not possible of real physical structures. The concept of sensory overlays is difficult to visualize and talk about, as a consequence the virtual fixture metaphor was introduced. To understand what a virtual fixture is an analogy with a real physical fixture such as a ruler is often used. A simple task such as drawing a straight line on a piece of paper free-hand is a task that most humans are unable to perform with good accuracy and high speed. However, the use of a simple device such as a ruler allows the task to be carried out quickly and with good accuracy. The use of a ruler helps the user by guiding the pen along the ruler reducing the tremor and mental load of the user, thus increasing the quality of the results.\n\nThe definition of virtual fixtures by Rosenberg is much broader than simply providing guidance of the end-effector. For example, auditory virtual fixtures are used to increase the user awareness by providing audio clues that helps the user by providing multi modal cues for localization of the end-effector. Rosenberg argues that the success of virtual fixtures is not only because the user is guided by the fixture, but that the user experiences a greater presence and better localization in the remote workspace. However, in the context of human-machine collaborative systems, the term virtual fixtures is often used to refer to a task dependent virtual aid that is overlaid upon a real environment and guides the user's motion along desired directions while preventing motion in undesired directions or regions of the workspace. This is the type of virtual fixtures that is described in detail in the next section of this article.\n\nVirtual fixtures can be either \"guiding virtual fixtures\" or \"forbidden regions virtual fixtures\". A forbidden regions virtual fixture could be used, for example, in a teleoperated setting where the operator has to drive a vehicle at a remote site to accomplish an objective. If there are pits at the remote site which would be harmful for the vehicle to fall into forbidden regions could be defined at the various pits locations, thus preventing the operator from issuing commands that would result in the vehicle ending up in such a pit.\n\nSuch illegal commands could easily be sent by an operator because of, for instance, delays in the teleoperation loop, poor telepresence or a number of other reasons.\n\nAn example of a guiding virtual fixture could be when the vehicle must follow a certain trajectory,\n\nThe operator is then able to control the progress along the \"preferred direction\" while motion along the \"non-preferred direction\" is constrained.\n\nWith both forbidden regions and guiding virtual fixtures the \"stiffness\", or its inverse the \"compliance\", of the fixture can be adjusted. If the compliance is high (low stiffness) the fixture is \"soft\". On the other hand, when the compliance is zero (maximum stiffness) the fixture is \"hard\".\n\nThis section describes how a control law that implements virtual fixtures can be derived. It is assumed that the robot is a purely kinematic device with end-effector position formula_1 and end-effector orientation formula_2 expressed in the robot's base frame formula_3. The input control signal formula_4 to the robot is assumed to be a desired end-effector velocity formula_5. In a tele-operated system it is often useful to scale the input velocity from the operator, formula_6 before feeding it to the robot controller. If the input from the user is of another form such as a force or position it must first be transformed to an input velocity, by for example scaling or differentiating.\n\nThus the control signal formula_4 would be computed from the operator's input velocity formula_6 as:\nIf formula_10 there exists a one-to-one mapping between the operator and the slave robot.\n\nIf the constant formula_11 is replaced by a diagonal matrix formula_12 it is\npossible to adjust the compliance independently for different\ndimensions of formula_13. For example, setting the first three\nelements on the diagonal of formula_12 to formula_11 and all other elements to zero\nwould result in a system that only permits translational motion and not\nrotation. This would be an example of a hard virtual fixture that\nconstrains the motion from formula_16 to \nformula_17. If the rest of the elements on the\ndiagonal were set to a small value, instead of zero, the fixture\nwould be soft, allowing some motion in the rotational directions.\n\nTo express more general constraints assume a time-varying matrix \nformula_18\nwhich represents the preferred direction at time formula_19. Thus if formula_20 the\npreferred direction is along a curve in formula_21. Likewise,\nformula_22 would give preferred directions that span a surface. From formula_23 two\nprojection operators can be defined (Marayong \"et al.\", 2003), the\nspan and kernel of the column space:\n\nIf formula_23 does not have full column rank the span can not be computed,\nconsequently it is better to compute the span by using the\npseudo-inverse (Marayong \"et al.\", 2003), thus in practice the span is computed as:\nwhere formula_27 denotes the pseudo-inverse of formula_23.\n\nIf the input velocity is split into two components as:\nit is possible to rewrite the control law as:\n\nNext introduce a new compliance that affects only the non-preferred\ncomponent of the velocity input and write the final control law as:\n\n"}
