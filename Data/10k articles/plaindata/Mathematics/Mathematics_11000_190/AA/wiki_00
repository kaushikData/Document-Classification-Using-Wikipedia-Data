{"id": "362193", "url": "https://en.wikipedia.org/wiki?curid=362193", "title": "22 (number)", "text": "22 (number)\n\n22 (twenty-two) is the natural number following 21 and preceding 23.\n\n\n\n\n\n\n\n\n\n\nTwenty-two may also refer to:\n\n"}
{"id": "894157", "url": "https://en.wikipedia.org/wiki?curid=894157", "title": "Academic Games", "text": "Academic Games\n\nAcademic Games is a competition in the U.S. in which players win by out-thinking each other in mathematics, language arts, and social studies. Formal tournaments are organized by local leagues, and on a national level by the Academic Games Leagues of America (AGLOA). Member leagues in eight states hold a national tournament every year, in which players in four divisions compete in eight different games covering math, English, and history. Some turn-based games require a kit consisting of a board and playing cubes, while other games have a central reader announcing questions or clues and each player answering individually.\n\nBefore the existence of AGLOA, tournaments were held by the National Academic Games Project founded by the creator of many of the games. The earliest tournaments, in the late 1960s, were held on or near the campus of Nova High School in Davie, Florida. Nova was the beta test site for the game \"Propaganda\" and others. Many AGLOA leaders were involved with NAGP. The new league was created partially because of personal conflict with Robert W. Allen. Allen later sued the AGLOA for copyright, trademark, and tradename infringement.\n\nAcademic Games Leagues of America was founded in 1991 to encourage the use of Academic Games as an educational tool and as a scholar competition. Many of the games used in tournaments, however, were created as early as in the 1960s and 1970s. Most of the games played at tournaments are available from Wff 'N Proof Learning Games. Brother Neal Golden of New Orleans is the current board president of AGLOA; Rod Beard of West Bloomfiend, MI is the current vice-president. Other board members represent Academic Games leagues in Florida, Georgia, Louisiana, Michigan, Pennsylvania, and West Virginia.\n\nAcademic Games players compete with other players in their own age group. These are the four age divisions in the league.\n\nHowever, there is no restriction against playing one of your players in a higher division. Several teams have won national championships in the senior division, even though half their players belonged, agewise, in the junior division.\n\nGames become more challenging as a player progresses through the divisions. There are often two variations of the games: basic and adventurous. Basic games have no variations or special demands players can make on game solutions. Adventurous games have a series of variation possibilities that may apply and increase in difficulty as players age.\n\nEight games are played in official AGLOA tournaments. Some local leagues also play other games such as On-Words (a simplified version of LinguiSHTIK).\n\nTwo math games, Equations and On-Sets are played at AGLOA tournaments.\n\nEquations is a mathematics game created in 1965 for 2-3 players. The game uses a playing mat with Forbidden, Permitted, and Required sections and 24 cubes, each labeled with numbers and mathematical operations. At the beginning of each \"shake\", one player uses up to six cubes to set a \"goal.\" All players must use the remaining cubes to devise a solution that equals the goal or win by challenging an impossible board + goal situation.\n\nGameplay can become more complicated through the use of \"variations\" called on the game. Applicable variations differ by the player's age division. The game progresses with each player moving one cube on their turn, or alternatively challenging that they can create a solution with the cubes in play, that a solution was possible on the last turn and the player before had missed it, or challenging that it is impossible to create a solution with the cubes available. When a player calls a challenge, it is called against the player who most recently completed their move.\n\nIn a three player game, the indifferent player may choose who he sides with in the case of a challenge. A player who correctly challenges another player wins the game. The loser of a game gains two points, The winner six, and the sider (if he sided with the winner) gains four or two (if he sided with the loser). Equations games become more intricate with the use of factorials, vulgar fractions, and even logarithms, in the Senior division.\n\nOn-Sets is a board and cube game that teaches basic logic and set theory. This game also uses a deck of 16 cards that is used to make the \"Universe\". Each card contains a different combination of colored dots. The cubes contain numbers, colors and logic operators.\n\nPlayers learn logic concepts such as union and intersection, and learn to use restrictions such as subset. Variations can be also be used in On-Sets games. A player wins by using the cubes in resources to create a logical statement which equals the goal set using the numeral cubes. Challenges and multiplayer games work in a similar way to Equations game.\n\nWFF 'N Proof is a board and cube game that was created by Professor Layman Allen in 1961 to teach the basics of symbolic logic.\nIt is played with 28 cubes that contain various letters, such as p, q, C, or N. The game board contains a forbidden section, a permitted section, and a required section. To win the game, you have to write a proof, using the cubes to create \"WFFs\" (Well-Formed Formulas). This game is now part of the AGLOA National Tournament, beginning in 2013.\n\nLinguiSHTIK is a technical game that teaches language arts and linguistics. The game has a playing mat and 23 cubes which are imprinted with the 26 letters of the alphabet.\n\nA player has to create a word using the letters available, and the word must be used in a sentence that matches the Demands called. A demand specifies something about the sentence or word, such as number of clauses, part of speech, number of letters, etc. Challenges in LinguiSHTIK work similarly as in the other cube games with the exception of a forceout, which is called when moving any cube would result in a Challenge Win. Some concepts taught in LinguiSHTIK include sentence patterns, clauses, grammar, and verbs.\n\nThe game has elements similar to the popular word game Scrabble but adds a different element of play through grammatical demands and the shared letter pool.\n\nIn Propaganda, clues are read to all players by a central reader. Each player must decide, from a list, which persuasion technique that clue used, if any. There are several different sections of Propaganda techniques; the reader also specifies which section the persuasion technique is listed in.\n\nDifferent leagues have different scoring methods, but the official AGLOA scoring involves a \"bold\" and \"cautious\" wager method. If you wager \"bold\", then you receive four points for a correct answer or lose two points for an incorrect answer. If you wager \"cautious,\" then you receive two points for a correct answer; however, you lose nothing for an incorrect answer. A round consists of nine questions, so the highest score possible per round is 36 points, while the lowest is -18 points.\n\nMost Propaganda clues involve statements that are likely to be heard in advertising or politics. There are six different Propaganda sections, but only four specific sections are used in each season. Sections A, B, D, and E are being used for the 2016-2017 season, and B, C, D, and F will be used for the 2017-2018 season. For 2018-2019, the sections will be A, C, D, and E. For 2019-2020, the sections will be A, B, C, and F. For 2020-2021, the sections will be B, C, D, and E. Here are all the Propaganda techniques, listed by section.\n\nMore complete definitions of the individual techniques can be found on agloa.org.\n\nA reader announces three clues about a particular U.S. President. Each player must individually write down which President the clue describes. Players who answer correctly on the earliest clue get more points than players that answer after more clues are given. The first clue is worth 6 points, the second is worth 4 points, and the third is worth 2 points. The 6 point clue is the hardest clue, while the 4 point and 2 point clues get progressively easier.\n\nIn the Elementary and Middle divisions, only a portion of presidents are used per season. For those divisions, ranges switch between presidents 1-24 and 25-45 every other year. In Junior and Senior divisions, however, all the presidents are used every season. During a tournament, players are assisted by a gazetteer which has each president's name, birth date, birthplace, and other basic information.\n\nThis event was part of the national tournament through 2016, after which its two rounds, Current Events and Theme, were each made an independent game.\n\nThis game was originally known as “World Card.”\n\nThis event concerns events from the past year, both foreign and domestic. It consists of a Wager Round, in which the players choose how many points they wish to wager, and a Lightning Round, in which the point values for each question are determined before the round.\n\nIn the Wager Round, players may wager two, four, or six points after being given a broad category (such as “international politics” or “arts and entertainment”). The player need not have any points to wager; thus, negative scores are possible. After the wager is made, the question is asked and the players answer. A correct answer earns as many points as were wagered, while an incorrect answer loses half that many.\n\nIn the Lightning Round, the questions are assigned point values (two, four, or six, with six questions of each value) by a panel of judges before the game starts. The questions are asked rapidly, and a correct answer earns its value; however, unlike the Wager Round, there is no penalty for answering incorrectly.\n\nThis event is very similar to its sister game, Current Events, which was once also a round of World Events. It is played and scored in the same fashion, with a Wager Round followed by a Lightning Round. The only notable difference is that this game concerns historical facts related to a theme chosen at the national tournament two years prior. Examples of past themes include the 1970s, the history of NASA, the Mesoamerican civilizations (Aztecs, Incas, Mayas), World War I, and the American Civil War.\n\nThe theme of the 2017-2018 season is Greek and Roman Mythology, which was chosen by vote at the 2015-2016 tournament.\n\nA spectator at an Academic Games tournament will hear a lot of jargon being thrown around that he or she may not be familiar with. Here are some of the most common AG-related words and their meanings.\n\n\n\n\n"}
{"id": "53283857", "url": "https://en.wikipedia.org/wiki?curid=53283857", "title": "Alpha Profiling", "text": "Alpha Profiling\n\nAlpha profiling is an application of machine learning to optimize the execution of large orders in financial markets by means of algorithmic trading. The purpose is to select an execution schedule that minimizes the expected implementation shortfall, or more generally, ensures compliance with a best execution mandate.\nAlpha profiling models learn statistically-significant patterns in the execution of orders from a particular trading strategy or portfolio manager and leverages these patterns to associate an optimal execution schedule to new orders. In this sense, it is an application of statistical arbitrage to best execution. For example, a portfolio manager specialized in value investing may have a behavioral bias to place orders to buy while an asset is still declining in value. In this case a slow or back-loaded execution schedule would provide better execution results than an urgent one. But this same portfolio manager will occasionally place an order after the asset price has already begun to rise in which case it should best be handled with urgency; this example illustrates the fact that Alpha Profiling must combine public information such as market data with private information including as the identity of the portfolio manager and the size and origin of the order, to identify the optimal execution schedule.\n\nLarge block orders can generally not be executed immediately because there is no available counterparty with the same size. Instead, they must be sliced into smaller pieces which are sent to the market over time. Each slice has some impact on the price, so on average the realized price for a buy order will be higher than at the time of the decision, or less for a sell order. The implementation shortfall is difference between the price at the time of the decision and the average expected price to be paid for executing the block, and is usually expressed in basis points as follows.\n\nThe alpha profile of an order is the expected impact-free price conditioned on the order and the state of the market, form the decision time to the required completion time. In other words, it is the price that one expects for the security would have over the execution horizon if the order were not executed. To estimate the cost of an execution strategy, market impact must be added to the impact-free price. It is well worth stressing that attempts to estimate the cost of alternative schedules without impact adjustments are counter-productive: high urgency strategies would capture more liquidity near the decision time and therefore would always be preferred if one did not account for their impact. In fact, front-loaded execution schedules have a higher average impact cost.\n\nOne way to compute an alpha profile is to use a classification technique such as Naive Bayes: find in the historical record a collection of orders with similar features, compute the impact-free price for each case, and take the simple average return from trade start over the next few days. This method is robust and transparent: each order is attached to a class of orders that share specific features that can be shown to the user as part of an explanation for the proposed optimal decision. However, an alpha profiling model based on classifying trades by similarity has limited generalization power. New orders do not always behave in the same way as other orders with similar features behaved in the past. A more accurate estimation of alpha profiles can be accomplished using Machine Learning (ML) methods to learn the probabilities of future price scenarios given the order and the state of the market. Alpha profiles are then computed as the statistical average of the security price under various scenarios, weighted by scenario probabilities.\n\nOptimal execution is the problem of identifying the execution schedule that minimizes a risk-adjusted cost function, where the cost term is the expected effect of trading costs on the portfolio value and the risk term is a measure of the effect of trade execution on risk. It is difficult to attribute the effect of trade execution on portfolio returns, and even more difficult to attribute its effect on risk, so in practice an alternate specification is often used: cost is defined as the implementation shortfall and risk is taken to be the variance of the same quantity. While this specification is commonly used, it is important to be aware of two shortcomings. First, the implementation shortfall as just defined is only a measure of the cost to the portfolio if all orders are entirely filled as originally entered; if portfolio managers edit the size of orders or some orders are left incomplete, opportunity costs must be considered. Second, execution risk as just defined is not directly related to portfolio risk and therefore has little practical value.\n\nA method for deriving optimal execution schedules that minimize a risk-adjusted cost function was proposed by Bertsimas and Lo. Almgren and Chriss provided closed-form solutions of the basic risk-adjusted cost optimization problem with a linear impact model and trivial alpha profile. More recent solutions have been proposed based on a propagator model for market impact, but here again the alpha profile is assumed to be trivial. In practice, impact is non-linear and the optimal schedule is sensitive to the alpha profile. A diffusion model yields a functional form of market impact including an estimate of the speed exponent at 0.25 (trading faster causes more impact). It is not difficult to derive optimal execution solutions numerically with non-trivial alpha profiles using such a functional form for market impact \n\n"}
{"id": "11585295", "url": "https://en.wikipedia.org/wiki?curid=11585295", "title": "Anthony Hill (artist)", "text": "Anthony Hill (artist)\n\nAnthony Hill (born 1930) is an English artist, painter and relief-maker, originally a member of the post-World War II British art movement termed the Constructionist Group whose work was essentially in the international constructivist tradition.\n\nHis fellow members in this group were Victor Pasmore, Adrian Heath, John Ernest, Kenneth Martin, Mary Martin, Gillian Wise (artist) and Stephen Gilbert. He was born on 23 April 1930 in London, and studied at the St Martin's and the Central Schools of Art 1948–51. He began painting in the style of Dada and Surrealism in 1948 but quickly moved on to geometric abstract idioms. He made his first relief in 1954 and abandoned painting for relief-making in 1956. One feature of these reliefs has been the use of non-traditional materials such as industrial aluminium and Perspex. His first one-man show of reliefs was held at the Institute of Contemporary Arts in 1958. He has participated in exhibitions of abstract and constructivist art in the UK, Paris, Germany, Holland, Poland, Switzerland and the USA. In 1978 he exhibited in the Arts Council's exhibition, Constructive Context, alongside a number if artists such as Jeffrey Steele and Peter Lowe who had begun working in a systematised constructive mode in the mid to late 1960s and came together in the Systems Group in December 1969. Hill, however, along with the Martins, declined membership of this group. In 1983 the Hayward Gallery held a major retrospective exhibition of Anthony Hill's constructivist work.\n\nAnthony Hill has had a lifelong fascination with mathematics, and there are many mathematicians among his circle of acquaintances. Together with his colleague John Ernest he made contributions to graph theory (crossing number) and in 1979, in recognition of a number of his mathematical papers, he was elected a member of the London Mathematical Society and made a visiting research associate in the Department of Mathematics at University College, London. But although almost all his reliefs have an underlying mathematical structure or logic, he was always insistent that in his art, in his own words, \"the mathematical thematic or mathematical process can only be a component: one is calculating or organising something which is clearly not mathematical.\" From the late 1980s onward, working in parallel with his systems-based work but in a very different mode, Anthony Hill exhibited dadaist pictures and collages under the pseudonym Achill Redo. The Tate Gallery, London has collections under both of the names Anthony Hill and Achill Redo.\n\nAn excellent summary of the life and constructivist work of Anthony Hill, together with that of the other British constructivists, is given in Alastair Grieve's authoritative book of 2005.\n\n\nThe Tate Gallery, London holds 11 of Hill's works. See their online listing for Anthony Hill: \nand for Achill Redo:\n"}
{"id": "2823", "url": "https://en.wikipedia.org/wiki?curid=2823", "title": "Antiderivative", "text": "Antiderivative\n\nIn calculus, an antiderivative, primitive function, primitive integral or indefinite integral of a function is a differentiable function whose derivative is equal to the original function . This can be stated symbolically as formula_1. The process of solving for antiderivatives is called antidifferentiation (or indefinite integration) and its opposite operation is called differentiation, which is the process of finding a derivative.\n\nAntiderivatives are related to definite integrals through the fundamental theorem of calculus: the definite integral of a function over an interval is equal to the difference between the values of an antiderivative evaluated at the endpoints of the interval.\n\nThe discrete equivalent of the notion of antiderivative is antidifference.\n\nThe function formula_2 is an antiderivative of formula_3, as the derivative of formula_4 is formula_5. As the derivative of a constant is zero, formula_5 will have an infinite number of antiderivatives, such as formula_4, formula_8, formula_9, etc. Thus, all the antiderivatives of formula_5 can be obtained by changing the value of in formula_11, where is an arbitrary constant known as the constant of integration. Essentially, the graphs of antiderivatives of a given function are vertical translations of each other; each graph's vertical location depending upon the value .\n\nIn physics, the integration of acceleration yields velocity plus a constant. The constant is the initial velocity term that would be lost upon taking the derivative of velocity because the derivative of a constant term is zero. This same pattern applies to further integrations and derivatives of motion (position, velocity, acceleration, and so on).\n\nAntiderivatives can be used to compute definite integrals, using the fundamental theorem of calculus: if is an antiderivative of the integrable function over the interval formula_12, then:\n\nBecause of this, each of the infinitely many antiderivatives of a given function is sometimes called the \"general integral\" or \"indefinite integral\" of \"f\" and is written using the integral symbol with no bounds:\n\nIf is an antiderivative of , and the function is defined on some interval, then every other antiderivative of differs from by a constant: there exists a number such that formula_15 for all . is called the constant of integration. If the domain of is a disjoint union of two or more (open) intervals, then a different constant of integration may be chosen for each of the intervals. For instance\n\nis the most general antiderivative of formula_17 on its natural domain formula_18\n\nEvery continuous function has an antiderivative, and one antiderivative is given by the definite integral of with variable upper boundary:\nVarying the lower boundary produces other antiderivatives (but not necessarily all possible antiderivatives). This is another formulation of the fundamental theorem of calculus.\n\nThere are many functions whose antiderivatives, even though they exist, cannot be expressed in terms of elementary functions (like polynomials, exponential functions, logarithms, trigonometric functions, inverse trigonometric functions and their combinations). Examples of these are\n\"From left to right, the first four are the error function, the Fresnel function, the trigonometric integral, and the logarithmic integral function.\"\n\nSee also Differential Galois theory for a more detailed discussion.\n\nFinding antiderivatives of elementary functions is often considerably harder than finding their derivatives. For some elementary functions, it is impossible to find an antiderivative in terms of other elementary functions. See the articles on elementary functions and nonelementary integral for further information.\n\nThere are various methods available:\n\n\nComputer algebra systems can be used to automate some or all of the work involved in the symbolic techniques above, which is particularly useful when the algebraic manipulations involved are very complex or lengthy. Integrals which have already been derived can be looked up in a table of integrals.\n\nNon-continuous functions can have antiderivatives. While there are still open questions in this area, it is known that:\n\nAssuming that the domains of the functions are open intervals:\n\n\n\n"}
{"id": "12768648", "url": "https://en.wikipedia.org/wiki?curid=12768648", "title": "Apeirotope", "text": "Apeirotope\n\nAn apeirotope or infinite polytope is a polytope which has infinitely many facets. There are two main geometric classes of apeirotope:\n\nIn general, a honeycomb in \"n\" dimensions is an infinite example of a polytope in \"n\" + 1 dimensions. \n\nTilings of the plane and close-packed space-fillings of polyhedra are examples of honeycombs in two and three dimensions respectively.\n\nA line divided into infinitely many finite segments is an example of an apeirogon. \n\nA skew apeirogon in two dimensions forms a zig-zag line in the plane. If the zig-zag is even and symmetrical, then the apeirogon is regular.\n\nSkew apeirogons can be constructed in any number of dimensions. In three dimensions, a regular skew apeirogon traces out a helical spiral and may be either left- or right-handed.\n\nThere are three regular skew apeirohedra, which look rather like polyhedral sponges:\n\nThere are thirty regular apeirohedra in Euclidean space. These include those listed above, as well as (in the plane) polytopes of type: {∞,3}, {∞,4}, {∞,6} and in 3-dimensional space, blends of these with either an apeirogon or a line segment, and the \"pure\" 3-dimensional apeirohedra (12 in number)\n\n"}
{"id": "314425", "url": "https://en.wikipedia.org/wiki?curid=314425", "title": "Barrel shifter", "text": "Barrel shifter\n\nA barrel shifter is a digital circuit that can shift a data word by a specified number of bits without the use of any sequential logic, only pure combinational logic. One way to implement it is as a sequence of multiplexers where the output of one multiplexer is connected to the input of the next multiplexer in a way that depends on the shift distance. A barrel shifter is often used to shift and rotate n-bits in modern microprocessors, typically within a single clock cycle.\n\nFor example, take a four-bit barrel shifter, with inputs A, B, C and D. The shifter can cycle the order of the bits \"ABCD\" as \"DABC\", \"CDAB\", or \"BCDA\"; in this case, no bits are lost. That is, it can shift all of the outputs up to three positions to the right (and thus make any cyclic combination of A, B, C and D). The barrel shifter has a variety of applications, including being a useful component in microprocessors (alongside the ALU).\n\nA barrel shifter is often implemented as a cascade of parallel 2×1 multiplexers. For an 8-bit barrel shifter, two intermediate signals are used which shifts by four and two bits, or passes the same data, based on the value of S[2] and S[1]. This signal is then shifted by another multiplexer, which is controlled by S[0]:\n\nLarger barrel shifters have additional stages.\n\nThe number of multiplexers required for an \"n\"-bit word is formula_1. Five common word sizes and the number of multiplexers needed are listed below:\n\nCost of critical path in FO4 (estimated, without wire delay):\n\nA common usage of a barrel shifter is in the hardware implementation of floating-point arithmetic. For a floating-point add or subtract operation, the significands of the two numbers must be aligned, which requires shifting the smaller number to the right, increasing its exponent, until it matches the exponent of the larger number. This is done by subtracting the exponents and using the barrel shifter to shift the smaller number to the right by the difference, in one cycle. If a simple shifter were used, shifting by \"n\" bit positions would require \"n\" clock cycles.\n\n\n"}
{"id": "9944209", "url": "https://en.wikipedia.org/wiki?curid=9944209", "title": "Cactus graph", "text": "Cactus graph\n\nIn graph theory, a cactus (sometimes called a cactus tree) is a connected graph in which any two simple cycles have at most one vertex in common. Equivalently, it is a connected graph in which every edge belongs to at most one simple cycle, or (for nontrivial cactus) in which every block (maximal subgraph without a cut-vertex) is an edge or a cycle.\n\nCacti are outerplanar graphs. Every pseudotree is a cactus. A nontrivial graph is a cactus if and only if every block is either a simple cycle or a single edge.\n\nThe family of graphs in which each component is a cactus is downwardly closed under graph minor operations. This graph family may be characterized by a single forbidden minor, the four-vertex diamond graph formed by removing an edge from the complete graph \"K\".\n\nA triangular cactus is a special type of cactus graph such that each cycle has length three. For instance, the friendship graphs, graphs formed from a collection of triangles joined together at a single shared vertex, are triangular cacti. As well as being cactus graphs the triangular cacti are also block graphs.\n\nThe largest triangular cactus in any graph may be found in polynomial time using an algorithm for the matroid parity problem. Since triangular cactus graphs are planar graphs, the largest triangular cactus can be used as an approximation to the largest planar subgraph, an important subproblem in planarization. As an approximation algorithm, this method has approximation ratio 4/9, the best known for the maximum planar subgraph problem.\n\nThe algorithm for finding the largest triangular cactus is associated with a theorem of Lovász and Plummer that characterizes the number of triangles in this largest cactus.\nLovász and Plummer consider pairs of partitions of the vertices and edges of the given graph into subsets, with the property that every triangle of the graph either has two vertices in a single class of the vertex partition or all three edges in a single class of the edge partition; they call a pair of partitions with this property \"valid\".\nThen the number of triangles in the largest triangular cactus equals the maximum, over pairs of valid partitions formula_1 and formula_2, of\nwhere formula_4 is the number of vertices in the given graph and formula_5 is the number of vertex classes met by edge class formula_6.\n\nAn important conjecture related to the triangular cactus is the Rosa's Conjecture, named after Alexander Rosa, which says that all triangular cacti are graceful or nearly-graceful. More precisely \n\n\"All triangular cacti with t ≡ 0, 1 mod 4 blocks are graceful, and those with t ≡ 2, 3 mod 4 are near graceful.\"\n\nSome facility location problems which are NP-hard for general graphs, as well as some other graph problems, may be solved in polynomial time for cacti.\n\nSince cacti are special cases of outerplanar graphs, a number of combinatorial optimization problems on graphs may be solved for them in polynomial time.\n\nCacti represent electrical circuits that have useful properties. An early application of cacti was associated with the representation of op-amps.\n\nCacti have also recently been used in comparative genomics as a way of representing the relationship between different genomes or parts of genomes.\n\nIf a cactus is connected, and each of its vertices belongs to at most two blocks, then it is called a Christmas cactus. Every polyhedral graph has a Christmas cactus subgraph that includes all of its vertices, a fact that plays an essential role in a proof by that every polyhedral graph has a greedy embedding in the Euclidean plane, an assignment of coordinates to the vertices for which greedy forwarding succeeds in routing messages between all pairs of vertices.\n\nIn topological graph theory, the graphs whose cellular embeddings are all planar are exactly the subfamily of the cactus graphs with the additional property that each vertex belongs to at most one cycle. These graphs have two forbidden minors, the diamond graph and the five-vertex friendship graph.\n\nCacti were first studied under the name of Husimi trees, bestowed on them by Frank Harary and George Eugene Uhlenbeck in honor of previous work on these graphs by Kôdi Husimi. The same Harary–Uhlenbeck paper reserves the name \"cactus\" for graphs of this type in which every cycle is a triangle, but now allowing cycles of all lengths is standard.\n\nMeanwhile, the name Husimi tree commonly came to refer to graphs in which every block is a complete graph (equivalently, the intersection graphs of the blocks in some other graph). This usage had little to do with the work of Husimi, and the more pertinent term block graph is now used for this family; however, because of this ambiguity this phrase has become less frequently used to refer to cactus graphs.\n\n"}
{"id": "51838760", "url": "https://en.wikipedia.org/wiki?curid=51838760", "title": "Chaos machine", "text": "Chaos machine\n\nIn mathematics, a chaos machine is a class of algorithms constructed on the base of chaos theory (mainly deterministic chaos) to produce pseudo-random oracle. It represents the idea of creating a universal scheme with modular design and customizable parameters, which can be applied wherever randomness and sensitiveness is needed.\n\nTheoretical model was published in early 2015 by Maciej A. Czyzewski. It was designed specifically to combine the benefits of hash function and pseudo-random function. However, it can be used to implement many cryptographic primitives, including cryptographic hashes, message authentication codes and randomness extractors.\n\n\n"}
{"id": "9031833", "url": "https://en.wikipedia.org/wiki?curid=9031833", "title": "Cheryl Praeger", "text": "Cheryl Praeger\n\nCheryl Elisabeth Praeger, AM, FAA, (born 7 September 1948, Toowoomba, Queensland) is an Australian mathematician. Praeger received BSc (1969) and MSc degrees from the University of Queensland (1974), and doctorate from the University of Oxford in 1973 under direction of Peter M. Neumann. She has published widely and has advised 27 PhD students (as of March 2018). She is currently Emeritus Professor of Mathematics at the University of Western Australia. She is best known for her works in group theory, algebraic graph theory and combinatorial designs.\n\nPraeger completed her high school education at Brisbane Girls Grammar School. After graduating high school, Praeger went to the government vocational guidance section to inquire about how she could further study mathematics. The vocational guidance officer she spoke with tried to discourage her from studying mathematics further, suggesting she become a teacher or a nurse because two other girls who came to him wanting to study math weren't able to pass their courses. He reluctantly showed her an engineering course description, but she felt it didn't have enough mathematics. So she left without getting much information that day, but did continue on to receive her bachelor's and master's degrees from the University of Queensland.\n\nHaving met several women on the mathematics staff during her undergraduate studies, the prospect of becoming a mathematician didn't seem strange to her. During her first and second years she did honours studies in mathematics and physics, choosing to continue in mathematics after her second year. After completing her education at University of Queensland she was offered a research scholarship at ANU but chose instead to take the Commonwealth Scholarship to the University of Oxford and attended St Anne's College. At that point she knew she wanted to study algebra.\n\nAfter earning her doctorate in 1973, she obtained a research fellowship at ANU. She had her first opportunity at teaching regular classes at the University of Virginia during the semester she worked there. Afterwards, she returned to ANU, where she met her future husband, John Henstridge, who was studying statistics. She was later offered a short-term position at the University of Western Australia, which turned into a long term position, where she currently works today. In 1989 she received the degree of Doctor of Science from the University of Western Australia for her work on permutation groups and algebraic graph theory.\n\nHer career has been largely the Department of Mathematics and Statistics at the University of Western Australia. She was appointed full Professor in 1983 and was Head of the Department of Mathematics 1992–1994, inaugural Dean of Postgraduate Research Studies 1996–1998, Chair Promotions and Tenure Committee 2000–2004, Deputy Dean of the Faculty of Engineering Computing and Mathematics 2003–2006, ARC Professorial Fellow 2007. and ARC Federation Fellow in 2009.\n\nDuring her career, Praeger has been invited to speak at many conferences, including ones in South Korea, Singapore, Hong Kong, Morocco, Slovakia, Slovenia, France, Germany, USSR, Belgium, Iran, Italy, the Philippines, and Japan.\n\nPraeger is a Fellow of the Australian Academy of Science, former president of the\nAustralian Mathematical Society (1992–1994 and first female President of the Society), and was appointed as a member of the Order of Australia in 1999 for her service to mathematics in Australia, especially through research and professional associations.\nOther awards include:\n\nSince 2014, the Women in Mathematics Special Interest Group of the Australian Mathematical Society bestows the Cheryl E. Praeger Travel Awards to female mathematicians. Since 2017 the Australian Mathematics Trust has awarded the Cheryl Praeger Medal to the best performing female contestants in the Australian Mathematics Competition. \n\nPraeger has also held memberships with the Combinatorial Mathematics Society of Australasia, Institute of Combinatorics and its Applications, Australian Mathematics Trust, American Mathematical Society, and the London Mathematical Society. Her past affiliations have not been limited to academia. \n\nShe has also been a member of the Curriculum Development Center of the Commonwealth Schools Commission, the Prime Minister's Science Advisory Council, WISET Advisory Committee to the Federal Minister for Science on participation of women in Science, Engineering, and Technology, UWA Academy of Young Mathematicians Lectures, the Western Australian School Mathematics Enrichment Course Tutor, and Data Analysis Australia Pty Ltd. She has also served on the Australian Federation of University Women (Western Australian Branch) and the Nedlands Primary School Council. Since 1992 she has been a board member of the Australian Mathematics Trust. Since 2001 she has chaired the Australian Mathematical Olympiad Committee.\n\nBetween 2007 and 2014 she was a member of the Executive Committee of the International Mathematical Union and between 2013 and 2016 a Vice President of the International Commission on Mathematical Instruction.\n\nBetween 2014 and 2018 she was Foreign Secretary of the Australian Academy of Science. Professor Praeger was elected as a Member-at-Large of the Executive Board of the Association of Academies and Societies of Sciences in Asia (AASSA) for 2016–18 and accepted an invitation to Chair the AASSA Committee of Women in Science and Engineering (WISE). She is a Member of the Executive Committee of the Inter Academy Partnership - Science, 2017-19.\n\nIn August 1975 she married John Henstridge in Brisbane. They have two children, James (1979) and Tim (1982).\n\nIn addition to holding a doctorate in mathematics, she also holds an AMusA in piano performance and is a member of the University of Western Australia Collegium Musicum. She has been a member of the Uniting Church in Australia, Nedlands Parish since 1977, functioned as an elder from 1981–1987, and as an organist since 1985. She lists keyboard music among her stronger interests along with sailing, hiking, and cycling.\n\nPraeger also promotes the involvement of women in mathematics by encouraging girls in primary and secondary schools with lectures, workshops, conferences and through Family Maths Program Australia (FAMPA), which she was key in implementing in local primary schools.\n\nPraeger's key research is in Group Theory and Combinatorics, including Analysis of algorithms and complexity, Discrete Mathematics and Geometry. She was first published in 1970 while still an undergraduate. As of September 2018, she has 395 publications total.\n\nShe has co-authored several papers on symmetric graphs and distance-transitive graphs with Tony Gardiner. She has also co-authored several papers with Peter Cameron, including the proof of Sims' Conjecture in 1983.\n\nWith Jan Saxl and Martin Liebeck, she has co-authored papers on many topics including: permutation groups, primitive permutation groups, simple groups, and almost simple groups. Together they co-authored \"On the O'Nan Scott Theorem for primitive permutation groups\". It pertains to the classification of finite simple groups, namely the classification of finite primitive permutation groups. The paper contains a complete self-contained proof of the theorem. Praeger later went on to generalise the O'Nan–Scott Theorem to quasiprimitive groups.\n\n\n"}
{"id": "36810753", "url": "https://en.wikipedia.org/wiki?curid=36810753", "title": "Clarence Raymond Adams", "text": "Clarence Raymond Adams\n\nClarence Raymond Adams (April 10, 1898 – October 15, 1965) was an American mathematician who worked on partial difference equations.\n\nHe entered Brown University in the fall of 1915 and graduated in 1918. Adams received his PhD in 1922 from Harvard University under the direction of G. D. Birkhoff. On August 17, 1922, he married Rachel Blodgett, who earned a PhD from Radcliffe College in 1921. As a Sheldon Traveling Fellow of Harvard University, he studied at the Sapienza University of Rome under Tullio Levi-Civita and at the University of Göttingen under Richard Courant. In 1923 Adams returned to Brown University as an instructor, then became a full professor in 1936 and eventually chair of the mathematics department from 1942 to 1960. In 1965 he retired and died on October 15 of that same year.\n\n\n"}
{"id": "572352", "url": "https://en.wikipedia.org/wiki?curid=572352", "title": "Complete partial order", "text": "Complete partial order\n\nIn mathematics, the phrase complete partial order is variously used to refer to at least three similar, but distinct, classes of partially ordered sets, characterized by particular completeness properties. Complete partial orders play a central role in theoretical computer science, in denotational semantics and domain theory.\n\nA complete partial order abbreviated cpo can, depending on context, refer to any of the following concepts. \n\n\n\n\nNote that \"complete partial order\" is never used to mean a poset in which \"all\" subsets have suprema; the terminology complete lattice is used for this concept.\n\nRequiring the existence of directed suprema can be motivated by viewing directed sets as generalized approximation sequences and suprema as \"limits\" of the respective (approximative) computations. This intuition, in the context of denotational semantics, was the motivation behind the development of domain theory.\n\nThe dual notion of a directed complete poset is called a filtered complete partial order. However, this concept occurs far less frequently in practice, since one usually can work on the dual order explicitly.\n\n\nAn ordered set \"P\" is a pointed dcpo if and only if every chain has a supremum in \"P\", i.e., \"P\" is chain-complete. Alternatively, an ordered set \"P\" is a pointed dcpo if and only if every order-preserving self-map of \"P\" has a least fixpoint. Every set \"S\" can be turned into a pointed dcpo by adding a least element ⊥ and introducing a flat order with ⊥ ≤ \"s\" and s ≤ \"s\" for every \"s\" ∈ \"S\" and no other order relations.\n\nA function \"f\" between two dcpos \"P\" and \"Q\" is called (Scott) continuous if it maps directed sets to directed sets while preserving their suprema:\nNote that every continuous function between dcpos is a monotone function. \nThis notion of continuity is equivalent to the topological continuity induced by the Scott topology.\n\nThe set of all continuous functions between two dcpos \"P\" and \"Q\" is denoted <nowiki>[</nowiki>\"P\" → \"Q\"<nowiki>]</nowiki>. Equipped with the pointwise order, this is again a dcpo, and a cpo whenever \"Q\" is a cpo.\nThus the complete partial orders with Scott-continuous maps form a cartesian closed category.\n\nEvery order-preserving self-map \"f\" of a cpo (\"P\", ⊥) has a least fixpoint. If \"f\" is continuous then this fixpoint is equal to the supremum of the iterates (⊥, \"f\"(⊥), \"f\"(\"f\"(⊥)), … \"f\"(⊥), …) of ⊥ (see also the Kleene fixpoint theorem).\n\nDirected completeness relates in various ways to other completeness notions such as chain completeness. Directed completeness alone is quite a basic property that occurs often in other order-theoretic investigations, using for instance algebraic posets and the Scott topology.\n"}
{"id": "11403316", "url": "https://en.wikipedia.org/wiki?curid=11403316", "title": "Compressed sensing", "text": "Compressed sensing\n\nCompressed sensing (also known as compressive sensing, compressive sampling, or sparse sampling) is a signal processing technique for efficiently acquiring and reconstructing a signal, by finding solutions to underdetermined linear systems. This is based on the principle that, through optimization, the sparsity of a signal can be exploited to recover it from far fewer samples than required by the Shannon-Nyquist sampling theorem. There are two conditions under which recovery is possible. The first one is sparsity which requires the signal to be sparse in some domain. The second one is incoherence which is applied through the isometric property which is sufficient for sparse signals.\n\nA common goal of the engineering field of signal processing is to reconstruct a signal from a series of sampling measurements. In general, this task is impossible because there is no way to reconstruct a signal during the times that the signal is not measured. Nevertheless, with prior knowledge or assumptions about the signal, it turns out to be possible to perfectly reconstruct a signal from a series of measurements (acquiring this series of measurements is called sampling). Over time, engineers have improved their understanding of which assumptions are practical and how they can be generalized.\n\nAn early breakthrough in signal processing was the Nyquist–Shannon sampling theorem. It states that if a real signal's highest frequency is less than half of the sampling rate (or less than the sampling rate, if the signal is complex), then the signal can be reconstructed perfectly by means of sinc interpolation. The main idea is that with prior knowledge about constraints on the signal's frequencies, fewer samples are needed to reconstruct the signal.\n\nAround 2004, Emmanuel Candès, Justin Romberg, Terence Tao, and David Donoho proved that given knowledge about a signal's sparsity, the signal may be reconstructed with even fewer samples than the sampling theorem requires. This idea is the basis of compressed sensing.\n\nCompressed sensing relies on L1 techniques, which several other scientific fields have used historically. In statistics, the least squares method was complemented by the formula_1-norm, which was introduced by Laplace. Following the introduction of linear programming and Dantzig's simplex algorithm, the formula_1-norm was used in computational statistics. In statistical theory, the formula_1-norm was used by George W. Brown and later writers on median-unbiased estimators. It was used by Peter J. Huber and others working on robust statistics. The formula_1-norm was also used in signal processing, for example, in the 1970s, when seismologists constructed images of reflective layers within the earth based on data that did not seem to satisfy the Nyquist–Shannon criterion. It was used in matching pursuit in 1993, the LASSO estimator by Robert Tibshirani in 1996 and basis pursuit in 1998. There were theoretical results describing when these algorithms recovered sparse solutions, but the required type and number of measurements were sub-optimal and subsequently greatly improved by compressed sensing.\n\nAt first glance, compressed sensing might seem to violate the sampling theorem, because compressed sensing depends on the sparsity of the signal in question and not its highest frequency. This is a misconception, because the sampling theorem guarantees perfect reconstruction given sufficient, not necessary, conditions. A sampling method fundamentally different from classical fixed-rate sampling cannot \"violate\" the sampling theorem. Sparse signals with high frequency components can be highly under-sampled using compressed sensing compared to classical fixed-rate sampling.\n\nAn underdetermined system of linear equations has more unknowns than equations and generally has an infinite number of solutions. The figure below shows such an equation system formula_5 where we want to find a solution for formula_6.\n\nIn order to choose a solution to such a system, one must impose extra constraints or conditions (such as smoothness) as appropriate. In compressed sensing, one adds the constraint of sparsity, allowing only solutions which have a small number of nonzero coefficients. Not all underdetermined systems of linear equations have a sparse solution. However, if there is a unique sparse solution to the underdetermined system, then the compressed sensing framework allows the recovery of that solution.\n\nCompressed sensing takes advantage of the redundancy in many interesting signals—they are not pure noise. In particular, many signals are sparse, that is, they contain many coefficients close to or equal to zero, when represented in some domain. This is the same insight used in many forms of lossy compression.\n\nCompressed sensing typically starts with taking a weighted linear combination of samples also called compressive measurements in a basis different from the basis in which the signal is known to be sparse. The results found by Emmanuel Candès, Justin Romberg, Terence Tao and David Donoho, showed that the number of these compressive measurements can be small and still contain nearly all the useful information. Therefore, the task of converting the image back into the intended domain involves solving an underdetermined matrix equation since the number of compressive measurements taken is smaller than the number of pixels in the full image. However, adding the constraint that the initial signal is sparse enables one to solve this underdetermined system of linear equations.\n\nThe least-squares solution to such problems is to minimize the formula_7 norm—that is, minimize the amount of energy in the system. This is usually simple mathematically (involving only a matrix multiplication by the pseudo-inverse of the basis sampled in). However, this leads to poor results for many practical applications, for which the unknown coefficients have nonzero energy.\n\nTo enforce the sparsity constraint when solving for the underdetermined system of linear equations, one can minimize the number of nonzero components of the solution. The function counting the number of non-zero components of a vector was called the formula_8 \"norm\" by David Donoho.\n\nCandès et al. proved that for many problems it is probable that the formula_1 norm is equivalent to the formula_8 norm, in a technical sense: This equivalence result allows one to solve the formula_1 problem, which is easier than the formula_8 problem. Finding the candidate with the smallest formula_1 norm can be expressed relatively easily as a linear program, for which efficient solution methods already exist. When measurements may contain a finite amount of noise, basis pursuit denoising is preferred over linear programming, since it preserves sparsity in the face of noise and can be solved faster than an exact linear program.\n\nTotal variation can be seen as a non-negative real-valued functional defined on the space of real-valued functions (for the case of functions of one variable) or on the space of integrable functions (for the case of functions of several variables). For signals, especially, total variation refers to the integral of the absolute gradient of the signal. In signal and image reconstruction, it is applied as total variation regularization where the underlying principle is that signals with excessive details have high total variation and that removing these details, while retaining important information such as edges, would reduce the total variation of the signal and make the signal subject closer to the original signal in the problem.\n\nFor the purpose of signal and image reconstruction, formula_14 minimization models are used. Other approaches also include the least-squares as has been discussed before in this article. These methods are extremely slow and return a not-so-perfect reconstruction of the signal. The current CS Regularization models attempt to address this problem by incorporating sparsity priors of the original image, one of which is the total variation (TV). Conventional TV approaches are designed to give piece-wise constant solutions. Some of these include (as discussed ahead) - constrained l1-minimization which uses an iterative scheme. This method, though fast, subsequently leads to over-smoothing of edges resulting in blurred image edges. TV methods with iterative re-weighting have been implemented to reduce the influence of large gradient value magnitudes in the images. This has been used in computed tomography (CT) reconstruction as a method known as edge-preserving total variation. However, as gradient magnitudes are used for estimation of relative penalty weights between the data fidelity and regularization terms, this method is not robust to noise and artifacts and accurate enough for CS image/signal reconstruction and, therefore, fails to preserve smaller structures.\n\nRecent progress on this problem involves using an iteratively directional TV refinement for CS reconstruction. This method would have 2 stages: the first stage would estimate and refine the initial orientation field - which is defined as a noisy point-wise initial estimate, through edge-detection, of the given image. In the second stage, the CS reconstruction model is presented by utilizing directional TV regularizer. More details about these TV-based approaches - iteratively reweighted l1 minimization, edge-preserving TV and iterative model using directional orientation field and TV- are provided below.\n\nIn the CS reconstruction models using constrained formula_15 minimization, larger coefficients are penalized heavily in the formula_15 norm. It was proposed to have a weighted formulation of formula_15 minimization designed to more democratically penalize nonzero coefficients. An iterative algorithm is used for constructing the appropriate weights. Each iteration requires solving one formula_15 minimization problem by finding the local minimum of a concave penalty function that more closely resembles the formula_20 norm. An additional parameter, usually to avoid any sharp transitions in the penalty function curve, is introduced into the iterative equation to ensure stability and so that a zero estimate in one iteration does not necessarily lead to a zero estimate in the next iteration. The method essentially involves using the current solution for computing the weights to be used in the next iteration.\n\nEarly iterations may find inaccurate sample estimates, however this method will down-sample these at a later stage to give more weight to the smaller non-zero signal estimates. One of the disadvantages is the need for defining a valid starting point as a global minimum might not be obtained every time due to the concavity of the function. Another disadvantage is that this method tends to uniformly penalize the image gradient irrespective of the underlying image structures. This causes over-smoothing of edges, especially those of low contrast regions, subsequently leading to loss of low contrast information. The advantages of this method include: reduction of the sampling rate for sparse signals; reconstruction of the image while being robust to the removal of noise and other artifacts; and use of very few iterations. This can also help in recovering images with sparse gradients.\n\nIn the figure shown below, P1 refers to the first-step of the iterative reconstruction process, of the projection matrix P of the fan-beam geometry, which is constrained by the data fidelity term. This may contain noise and artifacts as no regularization is performed. The minimization of P1 is solved through the conjugate gradient least squares method. P2 refers to the second step of the iterative reconstruction process wherein it utilizes the edge-preserving total variation regularization term to remove noise and artifacts, and thus improve the quality of the reconstructed image/signal. The minimization of P2 is done through a simple gradient descent method. Convergence is determined by testing, after each iteration, for image positivity, by checking if formula_21 for the case when formula_22 (Note that formula_23 refers to the different x-ray linear attenuation coefficients at different voxels of the patient image).\n\nThis is an iterative CT reconstruction algorithm with edge-preserving TV regularization to reconstruct CT images from highly undersampled data obtained at low dose CT through low current levels (milliampere). In order to reduce the imaging dose, one of the approaches used is to reduce the number of x-ray projections acquired by the scanner detectors. However, this insufficient projection data which is used to reconstruct the CT image can cause streaking artifacts. Furthermore, using these insufficient projections in standard TV algorithms end up making the problem under-determined and thus leading to infinitely many possible solutions. In this method, an additional penalty weighted function is assigned to the original TV norm. This allows for easier detection of sharp discontinuities in intensity in the images and thereby adapt the weight to store the recovered edge information during the process of signal/image reconstruction. The parameter formula_24 controls the amount of smoothing applied to the pixels at the edges to differentiate them from the non-edge pixels. The value of formula_24 is changed adaptively based on the values of the histogram of the gradient magnitude so that a certain percentage of pixels have gradient values larger than formula_24. The edge-preserving total variation term, thus, becomes sparser and this speeds up the implementation. A two-step iteration process known as forward-backward splitting algorithm is used. The optimization problem is split into two sub-problems which are then solved with the conjugate gradient least squares method and the simple gradient descent method respectively. The method is stopped when the desired convergence has been achieved or if the maximum number of iterations is reached.\n\nSome of the disadvantages of this method are the absence of smaller structures in the reconstructed image and degradation of image resolution. This edge preserving TV algorithm, however, requires fewer iterations than the conventional TV algorithm. Analyzing the horizontal and vertical intensity profiles of the reconstructed images, it can be seen that there are sharp jumps at edge points and negligible, minor fluctuation at non-edge points. Thus, this method leads to low relative error and higher correlation as compared to the TV method. It also effectively suppresses and removes any form of image noise and image artifacts such as streaking.\n\nTo prevent over-smoothing of edges and texture details and to obtain a reconstructed CS image which is accurate and robust to noise and artifacts, this method is used. First, an initial estimate of the noisy point-wise orientation field of the image formula_27, formula_28, is obtained. This noisy orientation field is defined so that it can be refined at a later stage to reduce the noise influences in orientation field estimation.A coarse orientation field estimation is then introduced based on structure tensor which is formulated as: formula_29. Here, formula_30 refers to the structure tensor related with the image pixel point (i,j) having standard deviation formula_31. formula_32 refers to the Gaussian kernel formula_33 with standard deviation formula_31. formula_24 refers to the manually defined parameter for the image formula_27 below which the edge detection is insensitive to noise. formula_37 refers to the gradient of the image formula_27 and formula_39 refers to the tensor product obtained by using this gradient.\n\nThe structure tensor obtained is convolved with a Gaussian kernel formula_32 to improve the accuracy of the orientation estimate with formula_24 being set to high values to account for the unknown noise levels. For every pixel (i,j) in the image, the structure tensor J is a symmetric and positive semi-definite matrix. Convolving all the pixels in the image with formula_32, gives orthonormal eigen vectors ω and υ of the formula_43 matrix. ω points in the direction of the dominant orientation having the largest contrast and υ points in the direction of the structure orientation having the smallest contrast. The orientation field coarse initial estimation formula_28 is defined as formula_28 = υ. This estimate is accurate at strong edges. However, at weak edges or on regions with noise, its reliability decreases.\n\nTo overcome this drawback, a refined orientation model is defined in which the data term reduces the effect of noise and improves accuracy while the second penalty term with the L2-norm is a fidelity term which ensures accuracy of initial coarse estimation.\n\nThis orientation field is introduced into the directional total variation optimization model for CS reconstruction through the equation: formula_46. formula_47 is the objective signal which needs to be recovered. Y is the corresponding measurement vector, d is the iterative refined orientation field and formula_48 is the CS measurement matrix. This method undergoes a few iterations ultimately leading to convergence.formula_28 is the orientation field approximate estimation of the reconstructed image formula_50 from the previous iteration (in order to check for convergence and the subsequent optical performance, the previous iteration is used). For the two vector fields represented by formula_47 and formula_52, formula_53 refers to the multiplication of respective horizontal and vertical vector elements of formula_47 and formula_52 followed by their subsequent addition. These equations are reduced to a series of convex minimization problems which are then solved with a combination of variable splitting and augmented Lagrangian (FFT-based fast solver with a closed form solution) methods. It (Augmented Lagrangian) is considered equivalent to the split Bregman iteration which ensures convergence of this method. The orientation field, d is defined as being equal to formula_56, where formula_57 define the horizontal and vertical estimates of formula_52.\n\nThe Augmented Lagrangian method for the orientation field, formula_46, involves initializing formula_60 and then finding the approximate minimizer of formula_61 with respect to these variables. The Lagrangian multipliers are then updated and the iterative process is stopped when convergence is achieved. For the iterative directional total variation refinement model, the augmented lagrangian method involves initializing formula_62.\n\nHere, formula_63 are newly introduced variables where formula_64 = formula_65, formula_66 = formula_67, formula_68 = formula_69, and formula_70 = formula_71. formula_72 are the Lagrangian multipliers for formula_63. For each iteration, the approximate minimizer of formula_74 with respect to variables (formula_75) is calculated. And as in the field refinement model, the lagrangian multipliers are updated and the iterative process is stopped when convergence is achieved.\n\nFor the orientation field refinement model, the Lagrangian multipliers are updated in the iterative process as follows:\n\nformula_76\n\nformula_77\n\nFor the iterative directional total variation refinement model, the Lagrangian multipliers are updated as follows:\n\nformula_78\n\nformula_79\n\nHere, formula_80 are positive constants.\n\nBased on Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) metrics and known ground-truth images for testing performance, it is concluded that iterative directional total variation has a better reconstructed performance than the non-iterative methods in preserving edge and texture areas. The orientation field refinement model plays a major role in this improvement in performance as it increases the number of directionless pixels in the flat area while enhancing the orientation field consistency in the regions with edges.\n\nThe field of compressive sensing is related to several topics in signal processing and computational mathematics, such as underdetermined linear-systems, group testing, heavy hitters, sparse coding, multiplexing, sparse sampling, and finite rate of innovation. Its broad scope and generality has enabled several innovative CS-enhanced approaches in signal processing and compression, solution of inverse problems, design of radiating systems, radar and through-the-wall imaging, and antenna characterization. Imaging techniques having a strong affinity with compressive sensing include coded aperture and computational photography. Implementations of compressive sensing in hardware at different technology readiness levels is available.\n\nConventional CS reconstruction uses sparse signals (usually sampled at a rate less than the Nyquist sampling rate) for reconstruction through constrained formula_15 minimization. One of the earliest applications of such an approach was in reflection seismology which used sparse reflected signals from band-limited data for tracking changes between sub-surface layers. When the LASSO model came into prominence in the 1990s as a statistical method for selection of sparse models, this method was further used in computational harmonic analysis for sparse signal representation from over-complete dictionaries. Some of the other applications include incoherent sampling of radar pulses. The work by \"Boyd et al.\" has applied the LASSO model- for selection of sparse models- towards analog to digital converters (the current ones use a sampling rate higher than the Nyquist rate along with the quantized Shannon representation). This would involve a parallel architecture in which the polarity of the analog signal changes at a high rate followed by digitizing the integral at the end of each time-interval to obtain the converted digital signal.\n\nCompressed sensing is used in a mobile phone camera sensor. The approach allows a reduction in image acquisition energy per image by as much as a factor of 15 at the cost of complex decompression algorithms; the computation may require an off-device implementation.\n\nCompressed sensing is used in single-pixel cameras from Rice University. Bell Labs employed the technique in a lensless single-pixel camera that takes stills using repeated snapshots of randomly chosen apertures from a grid. Image quality improves with the number of snapshots, and generally requires a small fraction of the data of conventional imaging, while eliminating lens/focus-related aberrations.\n\nCompressed sensing can be used to improve image reconstruction in holography by increasing the number of voxels one can infer from a single hologram. It is also used for image retrieval from undersampled measurements in optical and millimeter-wave holography.\n\nCompressed sensing is being used in facial recognition applications.\n\nCompressed sensing has been used to shorten magnetic resonance imaging scanning sessions on conventional hardware. Reconstruction methods include\n\nCompressed sensing addresses the issue of high scan time by enabling faster acquisition by measuring fewer Fourier coefficients. This produces a high-quality image with relatively lower scan time. Another application (also discussed ahead) is for CT reconstruction with fewer X-ray projections. Compressed sensing, in this case, removes the high spatial gradient parts - mainly, image noise and artifacts. This holds tremendous potential as one can obtain high-resolution CT images at low radiation doses (through lower current-mA settings).\n\nCompressed sensing has showed outstanding results in the application of network tomography to network management. Network delay estimation and network congestion detection can both be modeled as underdetermined systems of linear equations where the coefficient matrix is the network routing matrix. Moreover, in the Internet, network routing matrices usually satisfy the criterion for using compressed sensing.\n\nCommercial shortwave-infrared cameras based upon compressed sensing are available. These cameras have light sensitivity from 0.9 µm to 1.7 µm, which are wavelengths invisible to the human eye.\n\nIn the field of radio astronomy, compressed sensing has been proposed for deconvolving an interferometric image. In fact, the Högbom CLEAN algorithm that has been in use for the deconvolution of radio images since 1974, is similar to compressed sensing's matching pursuit algorithm.\n\nCompressed sensing combined with a moving aperture has been used to increase the acquisition rate of images in a transmission electron microscope. In scanning mode, compressive sensing combined with random scanning of the electron beam has enabled both faster acquisition and less electron dose, which allows for imaging of electron beam sensitive materials.\n\n\n"}
{"id": "155414", "url": "https://en.wikipedia.org/wiki?curid=155414", "title": "Computability theory", "text": "Computability theory\n\nComputability theory, also known as recursion theory, is a branch of mathematical logic, of computer science, and of the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability. In these areas, recursion theory overlaps with proof theory and effective descriptive set theory.\n\nBasic questions addressed by recursion theory include:\n\nAlthough there is considerable overlap in terms of knowledge and methods, mathematical recursion theorists study the theory of relative computability, reducibility notions, and degree structures; those in the computer science field focus on the theory of subrecursive hierarchies, formal methods, and formal languages.\n\nRecursion theory originated in the 1930s, with work of Kurt Gödel, Alonzo Church, Rózsa Péter, Alan Turing, Stephen Kleene, and Emil Post..\n\nThe fundamental results the researchers obtained established Turing computability as the correct formalization of the informal idea of effective calculation. These results led Stephen Kleene (1952) to coin the two names \"Church's thesis\" (Kleene 1952:300) and \"Turing's Thesis\" (Kleene 1952:376). Nowadays these are often considered as a single hypothesis, the Church–Turing thesis, which states that any function that is computable by an algorithm is a computable function. Although initially skeptical, by 1946 Gödel argued in favor of this thesis: \n\nWith a definition of effective calculation came the first proofs that there are problems in mathematics that cannot be effectively decided. Church (1936a, 1936b) and Turing (1936), inspired by techniques used by Gödel (1931) to prove his incompleteness theorems, independently demonstrated that the Entscheidungsproblem is not effectively decidable. This result showed that there is no algorithmic procedure that can correctly decide whether arbitrary mathematical propositions are true or false.\n\nMany problems in mathematics have been shown to be undecidable after these initial examples were established. In 1947, Markov and Post published independent papers showing that the word problem for semigroups cannot be effectively decided. Extending this result, Pyotr Novikov and William Boone showed independently in the 1950s that the word problem for groups is not effectively solvable: there is no effective procedure that, given a word in a finitely presented group, will decide whether the element represented by the word is the identity element of the group. In 1970, Yuri Matiyasevich proved (using results of Julia Robinson) Matiyasevich's theorem, which implies that Hilbert's tenth problem has no effective solution; this problem asked whether there is an effective procedure to decide whether a Diophantine equation over the integers has a solution in the integers. The list of undecidable problems gives additional examples of problems with no computable solution.\n\nThe study of which mathematical constructions can be effectively performed is sometimes called recursive mathematics; the \"Handbook of Recursive Mathematics\" (Ershov \"et al.\" 1998) covers many of the known results in this field.\n\nThe main form of computability studied in recursion theory was introduced by Turing (1936). A set of natural numbers is said to be a \"computable set\" (also called a \"decidable\", \"recursive\", or \"Turing computable\" set) if there is a Turing machine that, given a number \"n\", halts with output 1 if \"n\" is in the set and halts with output 0 if \"n\" is not in the set. A function \"f\" from the natural numbers to themselves is a \"recursive\" or \"(Turing) computable function\" if there is a Turing machine that, on input \"n\", halts and returns output \"f\"(\"n\"). The use of Turing machines here is not necessary; there are many other models of computation that have the same computing power as Turing machines; for example the μ-recursive functions obtained from primitive recursion and the μ operator.\n\nThe terminology for recursive functions and sets is not completely standardized. \nThe definition in terms of μ-recursive functions as well as a different definition of \"rekursiv\" functions by Gödel led to the traditional name \"recursive\" for sets and functions computable by a Turing machine. The word \"decidable\" stems from the German word \"Entscheidungsproblem\" which was used in the original papers of Turing and others. In contemporary use, the term \"computable function\" has various definitions: according to Cutland (1980), it is a partial recursive function (which can be undefined for some inputs), while according to Soare (1987) it is a total recursive (equivalently, general recursive) function. This article follows the second of these conventions. Soare (1996) gives additional comments about the terminology.\n\nNot every set of natural numbers is computable. The halting problem, which is the set of (descriptions of) Turing machines that halt on input 0, is a well-known example of a noncomputable set. The existence of many noncomputable sets follows from the facts that there are only countably many Turing machines, and thus only countably many computable sets, but there are uncountably many sets of natural numbers.\n\nAlthough the halting problem is not computable, it is possible to simulate program execution and produce an infinite list of the programs that do halt. Thus the halting problem is an example of a \"recursively enumerable set\", which is a set that can be enumerated by a Turing machine (other terms for recursively enumerable include \"computably enumerable\" and \"semidecidable\"). Equivalently, a set is recursively enumerable if and only if it is the range of some computable function. The recursively enumerable sets, although not decidable in general, have been studied in detail in recursion theory.\n\nBeginning with the theory of recursive sets and functions described above, the field of recursion theory has grown to include the study of many closely related topics. These are not independent areas of research: each of these areas draws ideas and results from the others, and most recursion theorists are familiar with the majority of them.\n\nRecursion theory in mathematical logic has traditionally focused on \"relative computability\", a generalization of Turing computability defined using oracle Turing machines, introduced by Turing (1939). An oracle Turing machine is a hypothetical device which, in addition to performing the actions of a regular Turing machine, is able to ask questions of an \"oracle\", which is a particular set of natural numbers. The oracle machine may only ask questions of the form \"Is \"n\" in the oracle set?\". Each question will be immediately answered correctly, even if the oracle set is not computable. Thus an oracle machine with a noncomputable oracle will be able to compute sets that a Turing machine without an oracle cannot.\n\nInformally, a set of natural numbers \"A\" is \"Turing reducible\" to a set \"B\" if there is an oracle machine that correctly tells whether numbers are in \"A\" when run with \"B\" as the oracle set (in this case, the set \"A\" is also said to be (\"relatively\") \"computable from\" \"B\" and \"recursive in\" \"B\"). If a set \"A\" is Turing reducible to a set \"B\" and \"B\" is Turing reducible to \"A\" then the sets are said to have the same \"Turing degree\" (also called \"degree of unsolvability\"). The Turing degree of a set gives a precise measure of how uncomputable the set is.\n\nThe natural examples of sets that are not computable, including many different sets that encode variants of the halting problem, have two properties in common:\n\nMany-one reductions are \"stronger\" than Turing reductions: if a set \"A\" is many-one reducible to a set \"B\", then \"A\" is Turing reducible to \"B\", but the converse does not always hold. Although the natural examples of noncomputable sets are all many-one equivalent, it is possible to construct recursively enumerable sets \"A\" and \"B\" such that \"A\" is Turing reducible to \"B\" but not many-one reducible to \"B\". It can be shown that every recursively enumerable set is many-one reducible to the halting problem, and thus the halting problem is the most complicated recursively enumerable set with respect to many-one reducibility and with respect to Turing reducibility. Post (1944) asked whether \"every\" recursively enumerable set is either computable or Turing equivalent to the halting problem, that is, whether there is no recursively enumerable set with a Turing degree intermediate between those two.\n\nAs intermediate results, Post defined natural types of recursively enumerable sets like the simple, hypersimple and hyperhypersimple sets. Post showed that these sets are strictly between the computable sets and the halting problem with respect to many-one reducibility. Post also showed that some of them are strictly intermediate under other reducibility notions stronger than Turing reducibility. But Post left open the main problem of the existence of recursively enumerable sets of intermediate Turing degree; this problem became known as \"Post's problem\". After ten years, Kleene and Post showed in 1954 that there are intermediate Turing degrees between those of the computable sets and the halting problem, but they failed to show that any of these degrees contains a recursively enumerable set. Very soon after this, Friedberg and Muchnik independently solved Post's problem by establishing the existence of recursively enumerable sets of intermediate degree. This groundbreaking result opened a wide study of the Turing degrees of the recursively enumerable sets which turned out to possess a very complicated and non-trivial structure.\n\nThere are uncountably many sets that are not recursively enumerable, and the investigation of the Turing degrees of all sets is as central in recursion theory as the investigation of the recursively enumerable Turing degrees. Many degrees with special properties were constructed: \"hyperimmune-free degrees\" where every function computable relative to that degree is majorized by a (unrelativized) computable function; \"high degrees\" relative to which one can compute a function \"f\" which dominates every computable function \"g\" in the sense that there is a constant \"c\" depending on \"g\" such that \"g(x) < f(x)\" for all \"x > c\"; \"random degrees\" containing algorithmically random sets; \"1-generic\" degrees of 1-generic sets; and the degrees below the halting problem of limit-recursive sets.\n\nThe study of arbitrary (not necessarily recursively enumerable) Turing degrees involves the study of the Turing jump. Given a set \"A\", the \"Turing jump\" of \"A\" is a set of natural numbers encoding a solution to the halting problem for oracle Turing machines running with oracle \"A\". The Turing jump of any set is always of higher Turing degree than the original set, and a theorem of Friedburg shows that any set that computes the Halting problem can be obtained as the Turing jump of another set. Post's theorem establishes a close relationship between the Turing jump operation and the arithmetical hierarchy, which is a classification of certain subsets of the natural numbers based on their definability in arithmetic.\n\nMuch recent research on Turing degrees has focused on the overall structure of the set of Turing degrees and the set of Turing degrees containing recursively enumerable sets. A deep theorem of Shore and Slaman (1999) states that the function mapping a degree \"x\" to the degree of its Turing jump is definable in the partial order of the Turing degrees. A recent survey by Ambos-Spies and Fejer (2006) gives an overview of this research and its historical progression.\n\nAn ongoing area of research in recursion theory studies reducibility relations other than Turing reducibility. Post (1944) introduced several \"strong reducibilities\", so named because they imply truth-table reducibility. A Turing machine implementing a strong reducibility will compute a total function regardless of which oracle it is presented with. \"Weak reducibilities\" are those where a reduction process may not terminate for all oracles; Turing reducibility is one example.\n\nThe strong reducibilities include:\nFurther reducibilities (positive, disjunctive, conjunctive, linear and their weak and bounded versions) are discussed in the article Reduction (recursion theory).\n\nThe major research on strong reducibilities has been to compare their theories, both for the class of all recursively enumerable sets as well as for the class of all subsets of the natural numbers. Furthermore, the relations between the reducibilities has been studied. For example, it is known that every Turing degree is either a truth-table degree or is the union of infinitely many truth-table degrees.\n\nReducibilities weaker than Turing reducibility (that is, reducibilities that are implied by Turing reducibility) have also been studied. The most well known are arithmetical reducibility and hyperarithmetical reducibility. These reducibilities are closely connected to definability over the standard model of arithmetic.\n\nRice showed that for every nontrivial class \"C\" (which contains some but not all r.e. sets) the index set \"E\" = {\"e\": the \"e\"th r.e. set \"W\" is in \"C\"} has the property that either the halting problem or its complement is many-one reducible to \"E\", that is, can be mapped using a many-one reduction to \"E\" (see Rice's theorem for more detail). But, many of these index sets are even more complicated than the halting problem. These type of sets can be classified using the arithmetical hierarchy. For example, the index set FIN of class of all finite sets is on the level Σ, the index set REC of the class of all recursive sets is on the level Σ, the index set COFIN of all cofinite sets is also on the level Σ and the index set COMP of the class of all Turing-complete sets Σ. These hierarchy levels are defined inductively, Σ contains just all sets which are recursively enumerable relative to Σ; Σ contains the recursively enumerable sets. The index sets given here are even complete for their levels, that is, all the sets in these levels can be many-one reduced to the given index sets.\n\nThe program of \"reverse mathematics\" asks which set-existence axioms are necessary to prove particular theorems of mathematics in subsystems of second-order arithmetic. This study was initiated by Harvey Friedman and was studied in detail by Stephen Simpson and others; Simpson (1999) gives a detailed discussion of the program. The set-existence axioms in question correspond informally to axioms saying that the powerset of the natural numbers is closed under various reducibility notions. The weakest such axiom studied in reverse mathematics is \"recursive comprehension\", which states that the powerset of the naturals is closed under Turing reducibility.\n\nA numbering is an enumeration of functions; it has two parameters, \"e\" and \"x\" and outputs the value of the \"e\"-th function in the numbering on the input \"x\". Numberings can be partial-recursive although some of its members are total recursive, that is, computable functions. Admissible numberings are those into which all others can be translated. A Friedberg numbering (named after its discoverer) is a one-one numbering of all partial-recursive functions; it is necessarily not an admissible numbering. Later research dealt also with numberings of other classes like classes of recursively enumerable sets. Goncharov discovered for example a class of recursively enumerable sets for which the numberings fall into exactly two classes with respect to recursive isomorphisms.\n\nPost's problem was solved with a method called the \"priority method\"; a proof using this method is called a \"priority argument\". This method is primarily used to construct recursively enumerable sets with particular properties. To use this method, the desired properties of the set to be constructed are broken up into an infinite list of goals, known as \"requirements\", so that satisfying all the requirements will cause the set constructed to have the desired properties. Each requirement is assigned to a natural number representing the priority of the requirement; so 0 is assigned to the most important priority, 1 to the second most important, and so on. The set is then constructed in stages, each stage attempting to satisfy one of more of the requirements by either adding numbers to the set or banning numbers from the set so that the final set will satisfy the requirement. It may happen that satisfying one requirement will cause another to become unsatisfied; the priority order is used to decide what to do in such an event.\n\nPriority arguments have been employed to solve many problems in recursion theory, and have been classified into a hierarchy based on their complexity (Soare 1987). Because complex priority arguments can be technical and difficult to follow, it has \ntraditionally been considered desirable to prove results without priority arguments, or to see if results proved with priority arguments can also be proved without them. \nFor example, Kummer published a paper on a proof for the existence of Friedberg numberings without using the priority method.\n\nWhen Post defined the notion of a simple set as an r.e. set with an infinite complement not containing any infinite r.e. set, he started to study the structure of the recursively enumerable sets under inclusion. This lattice became a well-studied structure. Recursive sets can be defined in this structure by the basic result that a set is recursive if and only if the set and its complement are both recursively enumerable. Infinite r.e. sets have always infinite recursive subsets; but on the other hand, simple sets exist but do not have a coinfinite recursive superset. Post (1944) introduced already hypersimple and hyperhypersimple sets; later maximal sets were constructed which are r.e. sets such that every r.e. superset is either a finite variant of the given maximal set or is co-finite. Post's original motivation in the study of this lattice was to find a structural notion such that every set which satisfies this property is neither in the Turing degree of the recursive sets nor in the Turing degree of the halting problem. Post did not find such a property and the solution to his problem applied priority methods instead; Harrington and Soare (1991) found eventually such a property.\n\nAnother important question is the existence of automorphisms in recursion-theoretic structures. One of these structures is that one of recursively enumerable sets under inclusion modulo finite difference; in this structure, \"A\" is below \"B\" if and only if the set difference \"B\" − \"A\" is finite. Maximal sets (as defined in the previous paragraph) have the property that they cannot be automorphic to non-maximal sets, that is, if there is an automorphism of the recursive enumerable sets under the structure just mentioned, then every maximal set is mapped to another maximal set. Soare (1974) showed that also the converse holds, that is, every two maximal sets are automorphic. So the maximal sets form an orbit, that is, every automorphism preserves maximality and any two maximal sets are transformed into each other by some automorphism. Harrington gave a further example of an automorphic property: that of the creative sets, the sets which are many-one equivalent to the halting problem.\n\nBesides the lattice of recursively enumerable sets, automorphisms are also studied for the structure of the Turing degrees of all sets as well as for the structure of the Turing degrees of r.e. sets. In both cases, Cooper claims to have constructed nontrivial automorphisms which map some degrees to other degrees; this construction has, however, not been verified and some colleagues believe that the construction contains errors and that the question of whether there is a nontrivial automorphism of the Turing degrees is still one of the main unsolved questions in this area (Slaman and Woodin 1986, Ambos-Spies and Fejer 2006).\n\nThe field of Kolmogorov complexity and algorithmic randomness was developed during the 1960s and 1970s by Chaitin, Kolmogorov, Levin, Martin-Löf and Solomonoff (the names are given here in alphabetical order; much of the research was independent, and the unity of the concept of randomness was not understood at the time). The main idea is to consider a universal Turing machine \"U\" and to measure the complexity of a number (or string) \"x\" as the length of the shortest input \"p\" such that \"U\"(\"p\") outputs \"x\". This approach revolutionized earlier ways to determine when an infinite sequence (equivalently, characteristic function of a subset of the natural numbers) is random or not by invoking a notion of randomness for finite objects. Kolmogorov complexity became not only a subject of independent study but is also applied to other subjects as a tool for obtaining proofs.\nThere are still many open problems in this area. For that reason, a recent research conference in this area was held in January 2007 and a list of open problems is maintained by Joseph Miller and Andre Nies.\n\nThis branch of recursion theory analyzed the following question: For fixed \"m\" and \"n\" with 0 < \"m\" < \"n\", for which functions \"A\" is it possible to compute for any different \"n\" inputs \"x\", \"x\", ..., \"x\" a tuple of \"n\" numbers \"y,y...,y\" such that at least \"m\" of the equations \"A\"(\"x\") = \"y\" are true. Such sets are known as (\"m\", \"n\")-recursive sets. The first major result in this branch of Recursion Theory is Trakhtenbrot's result that a set is computable if it is (\"m\", \"n\")-recursive for some \"m\", \"n\" with 2\"m\" > \"n\". On the other hand, Jockusch's semirecursive sets (which were already known informally before Jockusch introduced them 1968) are examples of a set which is (\"m\", \"n\")-recursive if and only if 2\"m\" < \"n\" + 1. There are uncountably many of these sets and also some recursively enumerable but noncomputable sets of this type. Later, Degtev established a hierarchy of recursively enumerable sets that are (1, \"n\" + 1)-recursive but not (1, \"n\")-recursive. After a long phase of research by Russian scientists, this subject became repopularized in the west by Beigel's thesis on bounded queries, which linked frequency computation to the above-mentioned bounded reducibilities and other related notions. One of the major results was Kummer's Cardinality Theory which states that a set \"A\" is computable if and only if there is an \"n\" such that some algorithm enumerates for each tuple of \"n\" different numbers up to \"n\" many possible choices of the cardinality of this set of \"n\" numbers intersected with \"A\"; these choices must contain the true cardinality but leave out at least one false one.\n\nThis is the recursion-theoretic branch of learning theory. It is based on Gold's model of learning in the limit from 1967 and has developed since then more and more models of learning. The general scenario is the following: Given a class \"S\" of computable functions, is there a learner (that is, recursive functional) which outputs for any input of the form (\"f\"(0),\"f\"(1)...,\"f\"(\"n\")) a hypothesis. A learner \"M\" learns a function \"f\" if almost all hypotheses are the same index \"e\" of \"f\" with respect to a previously agreed on acceptable numbering of all computable functions; \"M\" learns \"S\" if \"M\" learns every \"f\" in \"S\". Basic results are that all recursively enumerable classes of functions are learnable while the class REC of all computable functions is not learnable. Many related models have been considered and also the learning of classes of recursively enumerable sets from positive data is a topic studied from Gold's pioneering paper in 1967 onwards.\n\nRecursion theory includes the study of generalized notions of this field such as arithmetic reducibility, hyperarithmetical reducibility and α-recursion theory, as described by Sacks (1990). These generalized notions include reducibilities that cannot be executed by Turing machines but are nevertheless natural generalizations of Turing reducibility. These studies include approaches to investigate the analytical hierarchy which differs from the arithmetical hierarchy by permitting quantification over sets of natural numbers in addition to quantification over individual numbers. These areas are linked to the theories of well-orderings and trees; for example the set of all indices of recursive (nonbinary) trees without infinite branches is complete for level formula_1 of the analytical hierarchy. Both Turing reducibility and hyperarithmetical reducibility are important in the field of effective descriptive set theory. The even more general notion of degrees of constructibility is studied in set theory.\n\nComputability theory for digital computation is well developed. Computability theory is less well developed for analog computation that occurs in analog computers, analog signal processing, analog electronics, neural networks and continuous-time control theory, modelled by differential equations and continuous dynamical systems (Orponen 1997; Moore 1996).\n\nThere are close relationships between the Turing degree of a set of natural numbers and the difficulty (in terms of the arithmetical hierarchy) of defining that set using a first-order formula. One such relationship is made precise by Post's theorem. A weaker relationship was demonstrated by Kurt Gödel in the proofs of his completeness theorem and incompleteness theorems. Gödel's proofs show that the set of logical consequences of an effective first-order theory is a recursively enumerable set, and that if the theory is strong enough this set will be uncomputable. Similarly, Tarski's indefinability theorem can be interpreted both in terms of definability and in terms of computability.\n\nRecursion theory is also linked to second order arithmetic, a formal theory of natural numbers and sets of natural numbers. The fact that certain sets are computable or relatively computable often implies that these sets can be defined in weak subsystems of second order arithmetic. The program of reverse mathematics uses these subsystems to measure the noncomputability inherent in well known mathematical theorems. Simpson (1999) discusses many aspects of second-order arithmetic and reverse mathematics.\n\nThe field of proof theory includes the study of second-order arithmetic and Peano arithmetic, as well as formal theories of the natural numbers weaker than Peano arithmetic. One method of classifying the strength of these weak systems is by characterizing which computable functions the system can prove to be total (see Fairtlough and Wainer (1998)). For example, in primitive recursive arithmetic any computable function that is provably total is actually primitive recursive, while Peano arithmetic proves that functions like the Ackermann function, which are not primitive recursive, are total. Not every total computable function is provably total in Peano arithmetic, however; an example of such a function is provided by Goodstein's theorem.\n\nThe field of mathematical logic dealing with computability and its generalizations has been called \"recursion theory\" since its early days. Robert I. Soare, a prominent researcher in the field, has proposed (Soare 1996) that the field should be called \"computability theory\" instead. He argues that Turing's terminology using the word \"computable\" is more natural and more widely understood than the terminology using the word \"recursive\" introduced by Kleene. Many contemporary researchers have begun to use this alternate terminology. These researchers also use terminology such as \"partial computable function\" and \"computably enumerable \"(\"c.e.\")\" set\" instead of \"partial recursive function\" and \"recursively enumerable \"(\"r.e.\")\" set\". Not all researchers have been convinced, however, as explained by Fortnow and Simpson.\nSome commentators argue that both the names \"recursion theory\" and \"computability theory\" fail to convey the fact that most of the objects studied in recursion theory are not computable.\n\nRogers (1967) has suggested that a key property of recursion theory is that its results and structures should be invariant under computable bijections on the natural numbers (this suggestion draws on the ideas of the Erlangen program in geometry). The idea is that a computable bijection merely renames numbers in a set, rather than indicating any structure in the set, much as a rotation of the Euclidean plane does not change any geometric aspect of lines drawn on it. Since any two infinite computable sets are linked by a computable bijection, this proposal identifies all the infinite computable sets (the finite computable sets are viewed as trivial). According to Rogers, the sets of interest in recursion theory are the noncomputable sets, partitioned into equivalence classes by computable bijections of the natural numbers.\n\nThe main professional organization for recursion theory is the \"Association for Symbolic Logic\", which holds several research conferences each year. The interdisciplinary research Association \"Computability in Europe\" (\"CiE\") also organizes a series of annual conferences.\n\n\n\n\n\n\n"}
{"id": "302883", "url": "https://en.wikipedia.org/wiki?curid=302883", "title": "Crispin Wright", "text": "Crispin Wright\n\nCrispin James Garth Wright (; born 21 December 1942) is a British philosopher, who has written on neo-Fregean (neo-logicist) philosophy of mathematics, Wittgenstein's later philosophy, and on issues related to truth, realism, cognitivism, skepticism, knowledge, and objectivity. He is Professor of Philosophy at New York University and Professor of Philosophical Research at the University of Stirling, and taught previously at the University of St Andrews, University of Aberdeen, Princeton University and University of Michigan. \n\nHe was born in Surrey and was educated at Birkenhead School (1950–61) and at Trinity College, Cambridge, graduating in Moral Sciences in 1964 and taking a PhD in 1968. He took an Oxford BPhil in 1969 and was elected Prize Fellow and then Research Fellow at All Souls College, Oxford, where he worked until 1978. He then moved to the University of St. Andrews, where he was appointed Professor of Logic and Metaphysics and then the first Bishop Wardlaw University\nProfessorship in 1997. As of fall 2008, he is professor at New York University (NYU). He has also taught at the University of Michigan, Oxford University, Columbia University, and Princeton University. Crispin Wright is founder and director of Arché at the University of St. Andrews, which he left in September 2009 to take up leadership of the new Northern Institute of Philosophy (NIP) at the University of Aberdeen.\n\nIn the philosophy of mathematics, he is best known for his book \"Frege's Conception of Numbers as Objects\" (1983), where he argues that Frege's logicist project could be revived by removing the axiom schema of unrestricted comprehension (sometimes referred to as Basic Law V) from the formal system. Arithmetic is then derivable in second-order logic from Hume's principle. He gives informal arguments that (i) Hume's principle plus second-order logic is consistent, and (ii) from it one can produce the Dedekind–Peano axioms. Both results were proven informally by Gottlob Frege (Frege's Theorem), and would later be more rigorously proven by George Boolos and Richard Heck. Wright is one of the major proponents of neo-logicism, alongside his frequent collaborator Bob Hale. He has also written \"Wittgenstein and the Foundations of Mathematics\" (1980).\n\nIn general metaphysics, his most important work is \"Truth and Objectivity\" (Harvard University Press, 1992). He argues in this book that there need be no single, discourse-invariant thing in which truth consists, making an analogy with identity. There need only be some principles regarding how the truth predicate can be applied to a sentence, some 'platitudes' about true sentences. Wright also argues that in some contexts, probably including moral contexts, superassertibility will effectively function as a truth predicate. He defines a predicate as superassertible if and only if it is \"assertible\" in some state of information and then remains so no matter how that state of information is enlarged upon or improved. Assertiveness is warrant by whatever standards inform the discourse in question.\n\nMany of his most important papers in philosophy of language, epistemology, philosophical logic, meta-ethics, and the interpretation of Wittgenstein have been collected in two volumes published by Harvard University Press.\n\n\n\n"}
{"id": "39179243", "url": "https://en.wikipedia.org/wiki?curid=39179243", "title": "Dimension (graph theory)", "text": "Dimension (graph theory)\n\nIn mathematics, and particularly in graph theory, the dimension of a graph is the least integer such that there exists a \"classical representation\" of the graph in the Euclidean space of dimension with all the edges having unit length.\n\nIn a classical representation, the vertices must be distinct points, but the edges may cross one another.\n\nThe dimension of a graph is written: formula_1.\n\nFor example, the Petersen graph can be drawn with unit edges in formula_2, but not in formula_3: its dimension is therefore 2 (see the figure to the right).\n\nThis concept was introduced in 1965 by Paul Erdős, Frank Harary and William Tutte. It generalises the concept of unit distance graph to more than 2 dimensions.\n\nIn the worst case, every pair of vertices is connected, giving a complete graph.\n\nTo immerse the complete graph formula_4 with all the edges having unit length, we need the Euclidean space of dimension formula_5. For example, it takes two dimensions to immerse formula_6 (an equilateral triangle), and three to immerse formula_7 (a regular tetrahedron) as shown to the right.\n\nIn other words, the dimension of the complete graph is the same as that of the simplex having the same number of vertices.\n\nAll star graphs formula_9, for formula_10, have dimension 2, as shown in the figure to the left. Star graphs with equal to 1 or 2 need only dimension 1.\n\nThe dimension of a complete bipartite graph formula_11, for formula_10, can be drawn as in the figure to the right, by placing vertices on a circle whose radius is less than a unit, and the other two vertices one each side of the plane of the circle, at a suitable distance from it. formula_13 has dimension 2, as it can be drawn as a unit rhombus in the plane.\n\nTo summarise: \n\nThis proof also uses circles.\n\nWe write for the chromatic number of , and assign the integers formula_15 to the colours. In formula_16-dimensional Euclidean space, with its dimensions denoted formula_17, we arrange all the vertices of colour arbitrarily on the circle given by formula_18.\n\nThen the distance from a vertex of colour to a vertex of colour is given by formula_19.\n\nThe definition of the dimension of a graph given above says, of the minimal- representation:\n\nThis definition is rejected by some authors. A different definition was proposed in 1991 by Alexander Soifer, for what he termed the Euclidean dimension of a graph. Previously, in 1980, Paul Erdős and Miklós Simonovits had already proposed it with the name faithful dimension. By this definition, the minimal- representation is one such that two vertices of the graph are connected \"if and only if\" their representations are at distance 1.\n\nThe figures opposite show the difference between these definitions, in the case of a wheel graph having a central vertex and six peripheral vertices, with one spoke removed. Its representation in the plane allows two vertices at distance 1, but they are not connected.\n\nWe write this dimension as formula_20. It is never less than the dimension defined as above:\n\nPaul Erdős and Miklós Simonovits proved the following result in 1980:\n\nIt is NP-hard, and more specifically complete for the existential theory of the reals, to test whether the dimension or the Euclidean dimension of a given graph is at most a given value.\nThe problem remains hard even for testing whether the dimension or Euclidean dimension is two.\n"}
{"id": "22945970", "url": "https://en.wikipedia.org/wiki?curid=22945970", "title": "E. M. V. Krishnamurthy", "text": "E. M. V. Krishnamurthy\n\nEdayyathu Mangalam Venkatarama Krishnamurthy (18 June 1934 - 26 October 2012) was an Indian-born computer scientist. He was a professor at the Department of Computer science, Indian Institute of Science, Bangalore. He was an Emeritus Fellow, Computer Sciences Laboratory, Research School of Information Sciences and Engineering, Australian National University, Canberra.\n\nHe received the prestigious Shanti Swarup Bhatnagar Prize for Science and Technology (1978). He held several positions working for many institutions in India, Australia, USA, Europe and other nations.\n\n"}
{"id": "36201091", "url": "https://en.wikipedia.org/wiki?curid=36201091", "title": "Ehrenpreis conjecture", "text": "Ehrenpreis conjecture\n\nIn mathematics, the Ehrenpreis conjecture of Leon Ehrenpreis states that for any \"K\" greater than 1, any two closed Riemann surfaces of genus at least 2 have finite-degree covers which are \"K\"-quasiconformal: that is, the covers are arbitrarily close in the Teichmüller metric.\n\nA proof was announced by Jeremy Kahn and Vladimir Markovic in January 2011, using their proof of the Surface subgroup conjecture and a newly developed \"good pants homology\" theory. In June 2012, Kahn and Markovic were given the Clay Research Awards for their work on these two problems by the Clay Mathematics Institute at a ceremony at Oxford University.\n"}
{"id": "34873594", "url": "https://en.wikipedia.org/wiki?curid=34873594", "title": "Exsphere (polyhedra)", "text": "Exsphere (polyhedra)\n\nIn geometry, the exsphere of a face of a regular polyhedron is the sphere outside the polyhedron which touches the face and the planes defined by extending the adjacent faces outwards. It is tangent to the face externally and tangent to the adjacent faces internally.\n\nIt is the 3-dimensional equivalent of the excircle.\n\nThe sphere is more generally well-defined for any face which is a regular\npolygon and delimited by faces with the same dihedral angles\nat the shared edges. Faces of semi-regular polyhedra often \nhave different types of faces, which define exspheres of different size with each type of face.\n\nThe exsphere touches the face of the regular polyedron at the center\nof the incircle of that face. If the exsphere radius is denoted , the radius of this incircle \nand the dihedral angle between the face and the extension of the \nadjacent face , the center of the exsphere\nis located from the viewpoint at the middle of one edge of the\nface by bisecting the dihedral angle. Therefore\n\ninternal face-to-face angle.\n\nApplied to the geometry of the Tetrahedron of edge length ,\nwe have an incircle radius (derived by dividing twice the face area through the\nperimeter ), a dihedral angle , and in consequence .\n\nThe radius of the exspheres of the 6 faces of the Cube\nis the same as the radius of the inscribed\nsphere, since and its complement are the same, 90 degrees.\n\nThe dihedral angle applicable to the Icosahedron is derived by\nconsidering the coordinates of two triangles with a common edge,\nfor example one face with vertices\nat\n\nthe other at\n\nwhere is the golden ratio. Subtracting vertex coordinates\ndefines edge vectors,\n\nof the first face and\n\nof the other. Cross products of the edges of the first face and second\nface yield (not normalized) face normal vectors\n\nof the first and\nof the second face, using .\nThe dot product between these two face normals yields the cosine\nof the dihedral angle,\n\nFor an icosahedron of edge length , the incircle radius of the triangular faces is , and finally the radius of the 20 exspheres\n\n\n"}
{"id": "19719", "url": "https://en.wikipedia.org/wiki?curid=19719", "title": "Filter (mathematics)", "text": "Filter (mathematics)\n\nIn mathematics, a filter is a special subset of a partially ordered set. For example, the power set of some set, partially ordered by set inclusion, is a filter. Filters appear in order and lattice theory, but can also be found in topology from where they originate. The dual notion of a filter is an ideal.\n\nFilters were introduced by Henri Cartan in 1937 and subsequently used by Bourbaki in their book \"Topologie Générale\" as an alternative to the similar notion of a net developed in 1922 by E. H. Moore and H. L. Smith.\n\nIntuitively, a filter in a partially ordered set (\"poset\"), X, is a subset of X that includes as members those elements that are large enough to satisfy some criterion. For example, if \"x\" is an element of the poset, then the set of elements that are above \"x\" is a filter, called the principal filter at \"x\". (Notice that if \"x\" and \"y\" are incomparable elements of the poset, then neither of the principal filters at \"x\" and \"y\" is contained in the other one, and conversely.)\n\nSimilarly, a filter on a set contains those subsets that are sufficiently large to contain \"something\". For example, if the set is the real line and \"x\" is one of its points, then the family of sets that include \"x\" in their interior is a filter, called the filter of neighbourhoods of \"x\". (Notice that the \"thing\" in this case is slightly larger than \"x\", but it still doesn't contain any other specific point of the line.)\n\nThe above interpretations do not really, without elaboration, explain the condition 2. of the general definition of filter (see below). For, why should two \"large enough\" things contain a \"common\" \"large enough\" thing? (Note, however, that they do explain conditions 1 and 3: Clearly the empty set is not \"large enough\", and clearly the collection of \"large enough\" things should be \"upward closed\".)\n\nAlternatively, a filter can be viewed as a \"locating scheme\": Suppose we try to locate something (a point or a subset) in the space X. Call a filter the \"collection of subsets of X that might contain \"what we are looking for\".\" Then this \"filter\" should possess the following natural structure: 1. Empty set cannot contain anything so it will not belong to our filter. 2. If two subsets, E and F, both might contain \"what we are looking for\", then so might their intersection. Thus our filter should be closed with respect to finite intersection. 3. If a set E might contain \"what we are looking for\", so might any superset of it. Thus our filter is upward closed.\n\nAn ultrafilter can be viewed as a \"perfect locating scheme\" where \"each\" subset E of the space X can be used in deciding whether \"what we are looking for\" might lie in E.\n\nFrom this interpretation, compactness (see the mathematical characterization below) can be viewed as the property that \"no location scheme can end up with nothing\", or, to put it another way, \"we will always find something\".\n\nThe mathematical notion of filter provides a precise language to treat these situations in a rigorous and general way, which is useful in analysis, general topology and logic.\n\nA subset \"F\" of a partially ordered set (\"P\",≤) is a filter if the following conditions hold:\n\n\nA filter is proper if it is not equal to the whole set \"P\". This condition is sometimes added to the definition of a filter.\n\nWhile the above definition is the most general way to define a filter for arbitrary posets, it was originally defined for lattices only. In this case, the above definition can be characterized by the following equivalent statement:\nA subset \"F\" of a lattice (\"P\",≤) is a filter, if and only if it is an upper set that is closed under finite intersection (infima or meet), i.e., for all \"x\", \"y\" in \"F\", we find that \"x\" ∧ \"y\" is also in \"F\".\n\nThe smallest filter that contains a given element \"p\" is a principal filter and \"p\" is a principal element in this situation. The principal filter for \"p\" is just given by the set formula_1 and is denoted by prefixing \"p\" with an upward arrow: \n\nThe dual notion of a filter, i.e. the concept obtained by reversing all ≤ and exchanging ∧ with ∨, is ideal. Because of this duality, the discussion of filters usually boils down to the discussion of ideals. Hence, most additional information on this topic (including the definition of maximal filters and prime filters) is to be found in the article on ideals. There is a separate article on ultrafilters.\n\nA special case of a filter is a filter defined on a set. Given a set \"S\", a partial ordering ⊆ can be defined on the powerset P(\"S\") by subset inclusion, turning (P(\"S\"),⊆) into a lattice. Define a filter \"F\" on \"S\" as a nonempty subset of P(\"S\") with the following properties:\n\n\nIf the empty set is not in \"F\", we say \"F\" is a proper filter. \n\nThe first two properties imply that a filter on a set has the finite intersection property. With this definition, a filter on a set is indeed a filter. The only nonproper filter on \"S\" is P(\"S\").\n\nA filter base (or filter basis) is a subset \"B\" of P(\"S\") with the following properties:\n\nGiven a filter base \"B\", the filter generated or spanned by \"B\" is defined as the minimum filter containing \"B\". It is the family of all the subsets of \"S\" which contain a member of \"B\". Every filter is also a filter base, so the process of passing from filter base to filter may be viewed as a sort of completion.\n\nIf \"B\" and \"C\" are two filter bases on \"S\", one says \"C\" is finer than \"B\" (or that \"C\" is a refinement of \"B\") if for each \"B\" ∈ \"B\", there is a \"C\" ∈ \"C\" such that \"C\" ⊆ \"B\". If also \"B\" is finer than \"C\", one says that they are equivalent filter bases.\n\nFor any subset \"T\" of P(\"S\") there is a smallest (possibly nonproper) filter \"F\" containing \"T\", called the filter generated or spanned by \"T\". It is constructed by taking all finite intersections of \"T\", which then form a filter base for \"F\". This filter is proper if and only if any finite intersection of elements of \"T\" is non-empty, and in that case we say that \"T\" is a filter subbase.\n\n\nFor any filter \"F\" on a set \"S\", the set function defined by\nis finitely additive — a \"measure\" if that term is construed rather loosely. Therefore the statement\n\ncan be considered somewhat analogous to the statement that φ holds \"almost everywhere\". That interpretation of membership in a filter is used (for motivation, although it is not needed for actual \"proofs\") in the theory of ultraproducts in model theory, a branch of mathematical logic.\n\nIn topology and analysis, filters are used to define convergence in a manner similar to the role of sequences in a metric space.\n\nIn topology and related areas of mathematics, a filter is a generalization of a net. Both nets and filters provide very general contexts to unify the various notions of limit to arbitrary topological spaces.\n\nA sequence is usually indexed by the natural numbers, which are a totally ordered set. Thus, limits in first-countable spaces can be described by sequences. However, if the space is not first-countable, nets or filters must be used. Nets generalize the notion of a sequence by requiring the index set simply be a directed set. Filters can be thought of as sets built from multiple nets. Therefore, both the limit of a filter and the limit of a net are conceptually the same as the limit of a sequence.\n\nLet \"X\" be a topological space and \"x\" a point of \"X\".\n\n\nLet \"X\" be a topological space and \"x\" a point of \"X\".\n\n\nIndeed:\n\n(i) implies (ii): if \"F\" is a filter base satisfying the properties of (i), then the filter associated to \"F\" satisfies the properties of (ii).\n\n(ii) implies (iii): if \"U\" is any open neighborhood of \"x\" then by the definition of convergence \"U\" contains an element of \"F\"; since also \"Y\" is an element of \"F\", \n\"U\" and \"Y\" have nonempty intersection.\n\n(iii) implies (i): Define formula_9. Then \"F\" is a filter base satisfying the properties of (i).\n\nLet \"X\" be a topological space and \"x\" a point of \"X\".\n\n\nLet \"X\" be a topological space.\n\n\nLet formula_11, formula_12 be topological spaces. Let formula_13 be a filter base on formula_11 and formula_15 be a function. The image of formula_13 under formula_17 is defined as the set formula_18. The image is denoted formula_19 and forms a filter base on formula_12. \n\nLet formula_25 be a metric space.\n"}
{"id": "48831575", "url": "https://en.wikipedia.org/wiki?curid=48831575", "title": "Fueter–Pólya theorem", "text": "Fueter–Pólya theorem\n\nThe Fueter–Pólya theorem, first proved by Rudolf Fueter and George Pólya, states that the only quadratic pairing functions are the Cantor polynomials.\n\nIn 1873, Georg Cantor showed that the so-called Cantor polynomial\n\nis a bijective mapping from formula_2 to formula_3.\nThe polynomial given by swapping the variables is also a pairing function.\n\nFueter was investigating whether there are other quadratic polynomials with this property, and concluded that this is not the case assuming formula_4. He then wrote to Pólya, who showed the theorem does not require this condition.\n\nIf formula_5 is a real quadratic polynomial in two variables whose restriction to formula_2 is a bijection from formula_2 to formula_3 then it is\n\nor\n\nThe original proof is surprisingly difficult, using the Lindemann–Weierstrass theorem to prove the transcendence of\nformula_11 for a nonzero algebraic number formula_12.\nIn 2002, M. A. Vsemirnov published an elementary proof of this result.\n\nThe theorem states that the Cantor polynomial is the only quadratic paring polynomial of formula_2 and formula_3. The Cantor polynomial can be generalized to higher degree as bijection of ℕ with ℕ for \"k\" > 2. The conjecture is that these are the only such pairing polynomials.\n\nThe generalization of the Cantor polynomial in higher dimensions is as follows:\n\nThe sum of these binomial coefficients yields a polynomial of degree formula_16 in formula_16 variables. It is an open question whether every degree formula_16 polynomial which is a bijection formula_19 arises as a permutation of the variables of the polynomial formula_20.\n"}
{"id": "8354790", "url": "https://en.wikipedia.org/wiki?curid=8354790", "title": "George Jerrard", "text": "George Jerrard\n\nGeorge Birch Jerrard (25 November 1804 – 23 November 1863) was a British mathematician.\n\nHe studied at Trinity College, Dublin from 1821 to 1827. His main work was on the theory of equations, where he was reluctant to accept the validity of the work of Niels Henrik Abel on the insolubility of the quintic equation by radicals. He found a way of using Tschirnhaus transformations to eliminate three of the terms in an equation, which generalised work of Erland Bring (1736–1798), and is now called Bring–Jerrard normal form.\n\n"}
{"id": "35739443", "url": "https://en.wikipedia.org/wiki?curid=35739443", "title": "Growing context-sensitive grammar", "text": "Growing context-sensitive grammar\n\nIn formal language theory, a growing context-sensitive grammar is a context-sensitive grammar in which the productions increase the length of the sentences being generated. These grammars are thus noncontracting and context-sensitive. A growing context-sensitive language is a context-sensitive language generated by these grammars.\nIn these grammars the \"start symbol\" S does not appear on the right hand side of any production rule and the length of the right hand side of each production exceeds the length of the left side, unless the left side is S.\n\nThese grammars were introduced by Dahlhaus and Warmuth. They were later shown to be equivalent to the acyclic context-sensitive grammars. Membership in any growing context-sensitive language is polynomial time computable; however, the \"uniform\" problem of deciding whether a given string belongs to the language generated by a given growing or acyclic context-sensitive grammar is NP-complete.\n\n\n"}
{"id": "26640478", "url": "https://en.wikipedia.org/wiki?curid=26640478", "title": "György Elekes", "text": "György Elekes\n\nGyörgy Elekes ( – ) was a Hungarian mathematician and computer scientist who specialized in Combinatorial geometry and Combinatorial set theory. He may be best known for his work in the field that would eventually be called Additive Combinatorics. Particularly notable was his \"ingenious\" application of the Szemerédi–Trotter theorem to improve the best known lower bound for the sum-product problem. He also proved that any polynomial-time algorithm approximating the volume of convex bodies must have a multiplicative error, and the error grows exponentially on the dimension. With Micha Sharir he set up a framework which eventually led Guth and Katz to the solution of the Erdős distinct distances problem. (See below.)\n\nAfter graduating from the mathematics program at Fazekas Mihály Gimnázium (i.e., \"Fazekas Mihály high school\" in Budapest, which is known for its excellence, especially in mathematics), Elekes studied mathematics at the Eötvös Loránd University. Upon completing his degree, he joined the faculty in the Department of Analysis at the university. In 1984, he joined the newly forming Department of Computer Science, which was being headed by László Lovász. Elekes was promoted to full professor in 2005. He received the \"Doctor of Mathematical Sciences\" title from the Hungarian Academy of Sciences in 2001.\n\nElekes started his mathematical work in combinatorial set theory, answering some questions posed by Erdős and Hajnal. One of his results states that if the set of infinite subsets of the set of natural numbers is split into countably many parts, then in one of them, there is a solution of the equation \"A\"∪\"B\"=\"C\". His interest later switched to another favorite topic of Erdős, discrete geometry and geometric algorithm theory. In 1986 he proved that if a deterministic polynomial algorithm computes a number \"V\"(\"K\") for every convex body \"K\" in any Euclidean space given by a separation oracle such that \"V\"(\"K\") always at least vol(\"K\"), the volume of \"K\", then for every large enough dimension \"n\", there is a convex body in the \"n\"-dimensional Euclidean space such that \"V\"(\"K\")>2vol(\"K\"). That is, any polynomial-time estimate the volume of \"K\" must be inaccurate by at least an exponential factor.\n\nNot long before his death he developed new tools in Algebraic geometry and used them to obtain results in Discrete geometry, proving Purdy's Conjecture. Micha Sharir organized, extended and published Elekes's posthumous notes on these methods. Then Nets Katz and Larry Guth used them to solve (apart from a factor of (log n) ) the Erdős distinct distances problem, posed in 1946.\n\n"}
{"id": "42191418", "url": "https://en.wikipedia.org/wiki?curid=42191418", "title": "Imaging cycler microscopy", "text": "Imaging cycler microscopy\n\nAn imaging cycler microscope (ICM) is a fully automated (epi)fluorescence microscope which overcomes the spectral resolution limit resulting in parameter- and dimension-unlimited fluorescence imaging. The principle and robotic device was described by Walter Schubert in 1997 and ever since has been further developed with his co-workers within the human toponome project. The ICM runs robotically controlled repetitive incubation-imaging-bleaching cycles with dye-conjugated probe libraries recognizing target structures in situ (biomolecules in fixed cells or tissue sections). This results in the transmission of a randomly large number of distinct biological informations by re-using the same fluorescence channel after bleaching for the transmission of another biological information using the same dye which is conjugated to another specific probe, a.s.o. Thereby noise-reduced quasi-multi channel fluorescence images with reproducible physical, geometrical, and biophyscial stabilities are generated. The resulting power of combinatorial molecular discrimination (PCMD) per data point is given by 65,536, where 65,536 is the number of grey value levels (output of a 16-bit CCD camera) and \"k\" is the number of co-mapped biomolecules and/or subdomains per biomolecule(s). High PCMD has been shown for \"k\" = 100, and in principle can be expanded for much higher numbers of \"k\". In contrast to traditional multi-channel-few parameter fluorescence microscopy (Fig 1a) high PCMDs in an ICM lead to high functional and spatial resolution (Fig 1b). Systematic ICM analysis of biological systems reveals the supramolecular segregation law that describes the principle of order of large, hierarchically organized biomolecular networks in situ (toponome). The ICM is the core technology for the systematic mapping of the complete protein network code in tissues (human toponome project). The original ICM method includes any modification of the bleaching step. Corresponding modifications have been reported for antibody retrieval and chemical dye-quenching debated recently. The Toponome Imaging Systems (TIS) and Multi-Epitope-Ligand cartographs (MELC) represent different stages of the ICM technological development. Imaging Cycler Microscopy received the American ISAC best paper award in 2008 for the three symbol code of organized proteomes.\n\n\n"}
{"id": "12653871", "url": "https://en.wikipedia.org/wiki?curid=12653871", "title": "Infinite monkey theorem in popular culture", "text": "Infinite monkey theorem in popular culture\n\nThe infinite monkey theorem and its associated imagery is considered a popular and proverbial illustration of the mathematics of probability, widely known to the general public because of its transmission through popular culture rather than because of its transmission via the classroom.\n\nHowever, this popularity as either presented to or taken in the public's mind often oversimplifies or confuses important aspects of the different scales of the concepts involved: infinity, probability, and time—all of these are in measures beyond average human experience and practical comprehension or comparison.\n\nThe history of the imagery of \"typing monkeys\" dates back at least as far as Émile Borel's use of the metaphor in his essay in 1913, and this imagery has recurred many times since in a variety of media.\n\n\nToday, popular interest in the typing monkeys is sustained by numerous appearances in literature, television and radio, music, and the Internet, as well as graphic novels and stand-up comedy routines. Several collections of cultural references to the theorem have been published.\n\nThe following thematic timelines are based on these existing collections. The timelines are not comprehensive – instead, they document notable examples of references to the theorem appearing in various media. The initial timeline starts with some of the early history following Borel, and the later timelines record examples of the history, from the stories by Maloney and Borges in the 1940s, up to the present day.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "23897905", "url": "https://en.wikipedia.org/wiki?curid=23897905", "title": "International Congress on Mathematical Physics", "text": "International Congress on Mathematical Physics\n\nThe International Congress on Mathematical Physics (ICMP) is the largest research congress in mathematical physics. It is held every three years, on behalf of the International Association of Mathematical Physics (IAMP).\n\nThe Henri Poincaré Prize and the IAMP early career award are both delivered at the ICMP.\n\n1972: Moscow\n1974: Warsaw\n1975: Kyoto\n1977: Rome\n1979: Lausanne\n1981: Berlin\n1983: Boulder\n1986: Marseille\n1988: Swansea\n1991: Leipzig\n1994: Paris\n1997: Brisbane (website)\n2000: London\n2003: Lisbon (website)\n2006: Rio de Janeiro (website)\n2009: Prague (website)\n2012: Aalborg (website)\n2015: Santiago (website)\n"}
{"id": "27586152", "url": "https://en.wikipedia.org/wiki?curid=27586152", "title": "Isoline retrieval", "text": "Isoline retrieval\n\nIsoline retrieval is a remote sensing inverse method that retrieves one or more isolines of a trace atmospheric constituent or variable. When used to validate another contour, it is the most accurate method possible for the task. When used to retrieve a whole field, it is a general, nonlinear inverse method and a robust estimator.\n\nSuppose we have, as in contour advection, inferred knowledge of a\nsingle contour or isoline of an atmospheric constituent, \"q\"\nand we wish to validate this against satellite remote-sensing data.\nSince satellite instruments cannot measure the constituent directly,\nwe need to perform some sort of inversion.\nIn order to validate the contour, it is not necessary to know,\nat any given point, the exact value of the constituent. We only need to\nknow whether it falls inside or outside, that is, is it greater\nthan or less than the value of the contour, \"q\".\n\nThis is a classification problem. Let:\n\nbe the discretized variable.\nThis will be related to the satellite \"measurement vector\", formula_2,\nby some conditional probability, formula_3,\nwhich we approximate by collecting samples, called \"training data\", of both the\nmeasurement vector and the state variable, \"q\".\nBy generating classification results over the region of interest\nand using any contouring algorithm to separate the\ntwo classes, the isoline will have been \"retrieved.\"\n\nThe accuracy of a retrieval will be given by integrating\nthe conditional probability over the area of interest, \"A\":\n\nwhere \"c\" is the retrieved class at position, formula_5.\nWe can maximize this quantity by maximizing the value of the integrand\nat each point:\n\nSince this is the definition of maximum likelihood,\na classification algorithm based on maximum likelihood\nis the most accurate method possible of validating an advected contour.\nA good method for performing maximum likelihood classification\nfrom a set of training data is variable kernel density estimation.\n\nThere are two methods of generating the training data.\nThe most obvious is empirically, by simply matching measurements of\nthe variable, \"q\", with collocated\nmeasurements from the satellite instrument. In this case,\nno knowledge of the actual physics that produce the measurement\nis required and the retrieval algorithm is purely statistical.\nThe second is with a forward model:\n\nwhere formula_8 is the \"state vector\" and\n\"q = x\" is a single component.\nAn advantage of this method is that state vectors need not\nreflect actual atmospheric configurations, they need only\ntake on a state that could reasonably occur in the real atmosphere.\nThere are also none of the errors inherent in\nmost collocation procedures,\ne.g. because of offset errors in the locations of the paired samples\nand differences in the footprint sizes of the two instruments.\nSince retrievals will be biased towards more common states,\nhowever, the statistics ought to reflect those in the real world.\n\nThe conditional probabilities, formula_3, provide\nexcellent error characterization, therefore the classification\nalgorithm ought to return them.\nWe define the \"confidence rating\" by rescaling the conditional\nprobability:\n\nwhere \"n\" is the number of classes (in this case, two).\nIf \"C\" is zero, then the classification is little better than\nchance, while if it is one, then it should be perfect.\nTo transform the confidence rating to a statistical \"tolerance\",\nthe following line integral can be applied to an isoline retrieval\nfor which the true isoline is known:\n\nwhere \"s\" is the path, \"l\" is the length of the isoline\nand formula_12 is the retrieved confidence as a function\nof position.\nWhile it appears that the integral must be evaluated separately\nfor each value of the confidence rating, \"C\", in fact it may be\ndone for all values of \"C\" by sorting the confidence ratings of the\nresults, formula_12.\nThe function relates the threshold value of the confidence rating\nfor which the tolerance is applicable.\nThat is, it defines a region that contains a fraction of the true\nisoline equal to the tolerance.\n\nThe Advanced Microwave Sounding Unit (AMSU) series of satellite instruments\nare designed to detect temperature and water vapour. They have a high\nhorizontal resolution (as little as 15 km) and because they are\nmounted on more than one satellite, full global coverage can be\nobtained in less than one day.\nTraining data was generated using the second method from\nEuropean Centre for Medium-Range Weather Forecasts (ECMWF) ERA-40\ndata fed to a fast radiative transfer model called\nRTTOV.\nThe function, formula_14 has been generated from\nsimulated retrievals and is shown in the figure to the right.\nThis is then used to set the 90 percent tolerance in the figure\nbelow by shading all the confidence ratings less than 0.8.\nThus we expect the true isoline to fall within the shading\n90 percent of the time.\n\nIsoline retrieval is also useful for retrieving a continuum variable\nand constitutes a general, nonlinear inverse method.\nIt has the advantage over both a neural network, as well as iterative\nmethods such as optimal estimation that invert the forward model\ndirectly, in that there is no possibility of getting stuck in a\nlocal minimum.\n\nThere are a number of methods of reconstituting the continuum variable\nfrom the discretized one. Once a sufficient number of contours\nhave been retrieved, it is straightforward to interpolate between\nthem. Conditional probabilities make a good proxy for\nthe continuum value.\n\nConsider the transformation from a continuum to a discrete variable:\n\nSuppose that formula_17 is given by a Gaussian:\n\nwhere formula_19 is the expectation value and formula_20\nis the standard deviation, then the conditional probability is related to the\ncontinuum variable, \"q\", by the error function:\n\nThe figure shows conditional probability versus specific humidity for the example\nretrieval discussed above.\n\nThe location of \"q\" is found by setting the conditional probabilities\nof the two classes to be equal:\n\nIn other words, equal amounts of the \"zeroeth order moment\" lie on either side\nof \"q\". This type of formulation is characteristic of a robust estimator.\n\n"}
{"id": "2376138", "url": "https://en.wikipedia.org/wiki?curid=2376138", "title": "John Henry Michell", "text": "John Henry Michell\n\nJohn Henry Michell, FRS (26 October 1863 – 3 February 1940) was an Australian mathematician, Professor of Mathematics at the University of Melbourne.\n\nMichell was the son of John Michell (pronounced Mitchell), a miner, and his wife Grace, \"née\" Rowse and was born at Maldon, Victoria. His parents had migrated from Devonshire in 1854. Educated at first at Maldon, he went to Wesley College, Melbourne, in 1877, where he won the Draper and Walter Powell scholarships. In 1881 he began the arts course at the University of Melbourne, and qualified for the B.A. degree at the end of 1883. He had an outstanding course, heading the list with first-class honours each year, and winning the final honour scholarship in mathematics and physics.\n\nMichell then went to the University of Cambridge, obtained a major scholarship at Trinity College, and was bracketed senior wrangler with three others in the first part of the mathematical tripos in 1887. In the second part of the tripos in 1888, Michell was placed in division one of the first class.\n\nMichell was elected a fellow of Trinity in 1890, but returned to Melbourne later the same year, and was appointed lecturer in mathematics at Melbourne University. He held this position for over 30 years. His academic work occupied so much of his time that it was difficult to do original research. The first of his papers, \"On the theory of free streamlines\", which appeared in \"Transactions of the Royal Society\" in 1890, had drawn attention to his ability as a mathematician, and during the following 12 years about 15 papers were contributed to English mathematical journals. It was recognized that these were important contributions to the knowledge of hydrodynamics and elasticity, and in June 1902 he was elected a Fellow of the Royal Society (FRS), London. The number of his students at the University steadily increased, but there was no corresponding staff increase for a long while. Michell continued his research work but none of it was published. In 1923 he became professor of mathematics and, obtaining some increase of staff, established practice-classes and tutorials, thus considerably improving the efficiency of his department. Michell resigned the chair at the end of 1928 and was given the title of honorary research professor. He died after a short illness on 3 February 1940 at Camberwell. Michell did not marry. Michell published \"The Elements of Mathematical Analysis\" (1937), a substantial work in two volumes written in collaboration with Maurice Belz.\n\nMichell was regarded as a shy man and was one of the earliest graduates of an Australian university to be elected to the Royal Society. He was a good teacher, good-natured and patient with students, but his heart was really in his research work. His assistance was freely given to his engineering friends in clearing up their problems, and he did a good deal of physical experimentation including the devising and construction of several new forms of gyroscopes. He was continually at work, and it is not known why he did not choose to publish any papers after 1902. The value of his paper on \"The wave resistance of a ship\", published in 1898, was not realized until some 30 years later, when both English and German designers began to recognize its importance. Michell's brother, Anthony Michell (born 1870) made significant contributions to mechanical science, including the famous Michell thrust bearing.\n\nDuring a relatively short research career, Michell published 23 scientific papers that are some of the most important contributions ever made by an Australian mathematician.\nA mini-symposium has held at the 3rd Biennial Engineering Mathematics and Applications Conference (EMAC '98) celebrating the centenary of the publication of Michell's famous 1898 paper on ship hydrodynamics, \"The wave resistance of a ship\", Phil. Mag. (5) 45 (1898) 106-123.\n\nSince 1999, The JH Michell Medal has been awarded by ANZIAM in his honour.\n\n\n"}
{"id": "52066850", "url": "https://en.wikipedia.org/wiki?curid=52066850", "title": "Joseph Pérès", "text": "Joseph Pérès\n\nJoseph Pérès (31 October 1890 – 12 February 1962) was a French mathematician.\n\nPérès was born in Clermont-Ferrand on 31 October 1890. Former student of the Ecole Normale Superieure, he worked in Rome with Vito Volterra and defended his doctoral thesis in 1915. In 1920 he became a lecturer at the Faculty of Sciences of Strasbourg and in 1921 held the mechanics chair of the faculty of sciences of Marseille.\n\nIn 1932, he was appointed lecturer at the Faculty of Paris. He was elected member of the Academy of Sciences in 1942. He held the chair of mechanics in 1950 and Dean of the Faculty of Science in 1954, succeeding Albert Châtelet. During his deanship, he undertakes the creation of the Orsay campus. He was also one of the founders of the Institut des Hautes Études Scientifiques and its first président until his death.\n\n\n"}
{"id": "10282799", "url": "https://en.wikipedia.org/wiki?curid=10282799", "title": "Lie bracket of vector fields", "text": "Lie bracket of vector fields\n\nIn the mathematical field of differential topology, the Lie bracket of vector fields, also known as the Jacobi–Lie bracket or the commutator of vector fields, is an operator that assigns to any two vector fields \"X\" and \"Y\" on a smooth manifold \"M\" a third vector field denoted .\n\nConceptually, the Lie bracket is the derivative of \"Y\" along the flow generated by \"X\". A generalization of the Lie bracket is the Lie derivative, which allows differentiation of any tensor field along the flow generated by \"X\". The Lie bracket equals the Lie derivative of the vector \"Y\" (which is a tensor field) along \"X\", and is sometimes denoted formula_1 (read \"the Lie derivative of \"Y\" along \"X\"\").\n\nThe Lie bracket is an R-bilinear operation and turns the set of all smooth vector fields on the manifold \"M\" into an (infinite-dimensional) Lie algebra.\n\nThe Lie bracket plays an important role in differential geometry and differential topology, for instance in the Frobenius theorem, and is also fundamental in the geometric theory for nonlinear control systems (, nonholonomic systems; , feedback linearization).\n\nThere are three conceptually different but equivalent approaches to defining the Lie bracket:\n\nEach vector field \"X\" on a smooth manifold \"M\"\nmay be regarded as a differential operator acting on smooth\nfunctions on \"M\". Indeed, each\nsmooth vector field \"X\" becomes a derivation on the smooth \nfunctions \"C\"(\"M\") when we define \"X\"(\"f\") to be the element of \"C\"(\"M\") whose value at a point \"p\" is the directional derivative of \"f\" at \"p\" in the direction \"X\"(\"p\"). Furthermore, it is known that any derivation on \"C\"(\"M\") arises in this fashion from a uniquely determined smooth vector field \"X\".\n\nIn general, the commutator formula_2 of any two derivations formula_3 and formula_4 is again a derivation. This can be used to define the Lie bracket of vector fields as follows.\n\nThe Lie bracket, , of two smooth vector fields \n\"X\" and \"Y\" is the smooth vector field such that \n\nLet formula_6 be the flow associated with the vector field \"X\", and let d denote the tangent map derivative operator. Then the Lie bracket of \"X\" and \"Y\" at the point can be defined as\n\nor in terms of the Lie derivative\n\nwhich is also equivalent to\n\nThough neither definition of the Lie bracket depends on a choice of coordinates, in practice one often wants to compute the bracket with respect to a coordinate system.\n\nIf we have picked a coordinate chart on \"M\" with local coordinate functions formula_10, and we write formula_11 for the associated local basis for the tangent bundle, then the vector fields can be written as \nand \nwith smooth functions formula_14 and formula_15. Then the Lie bracket is given by\n\nIf \"M\" is (an open subset of) R, then the vector fields \"X\" and \"Y\" can be written as smooth maps of the form formula_17 and formula_18, and the Lie bracket formula_19 is given by\nwhere formula_21 and formula_22 are the Jacobian matrices of formula_23 and formula_24, respectively. These \"n\"-by-\"n\" matrices are multiplied by the \"n\"-vectors \"X\" and \"Y\".\n\nThe Lie bracket of vector fields equips the real vector space formula_25 of all vector fields on \"M\" (i.e., smooth sections of the tangent bundle formula_26 of formula_27) with the structure of a Lie algebra, i.e., [·,·] is a map formula_28 with the following properties\n\nAn immediate consequence of the second property is that formula_31 for any formula_24.\n\nFurthermore, there is a \"product rule\" for Lie brackets. Given a smooth real-valued function \"f\" defined on \"M\" and a vector field \"Y\" on \"M\", we have a new vector field \"fY\", defined by multiplying the vector \"Y\" with the number \"f\"(\"x\"), at each point . The Lie bracket of \"X\" and \"fY\" is then given by\nwhere on the right-hand side we multiply the function \"X\"(\"f\") with the vector field \"Y\", and the function \"f\" with the vector field .\nThis turns the vector fields with the Lie bracket into a Lie algebroid.\n\nWe also have the following fact:\n\nTheorem:\n\nformula_34 iff the flows of \"X\" and \"Y\" commute locally, i.e. iff for every and all sufficiently small real numbers \"s\", \"t\" we have formula_35.\n\nFor a Lie group \"G\", the corresponding Lie algebra is the tangent space at the identity, which can be identified with the left invariant vector fields on \"G\". The Lie bracket of the Lie algebra is then the Lie bracket of the left invariant vector fields, which is also left invariant.\n\nFor a matrix Lie group, smooth vector fields can be locally represented in the corresponding Lie algebra. Since the Lie algebra associated with a Lie group is isomorphic to the group's tangent space at the identity, elements of the Lie algebra of a matrix Lie group are also matrices. Hence the Jacobi–Lie bracket corresponds to the usual commutator for a matrix group:\n\nwhere juxtaposition indicates matrix multiplication.\n\nThe Jacobi–Lie bracket is essential to proving small-time local controllability (STLC) for driftless affine control systems.\n\nAs mentioned above, the Lie derivative can be seen as a generalization of the Lie bracket. Another generalization of the Lie bracket (to vector-valued differential forms) is the Frölicher–Nijenhuis bracket.\n\n"}
{"id": "12677528", "url": "https://en.wikipedia.org/wiki?curid=12677528", "title": "Liouville's equation", "text": "Liouville's equation\n\nIn differential geometry, Liouville's equation, named after Joseph Liouville, is the nonlinear partial differential equation satisfied by the conformal factor of a metric on a surface of constant Gaussian curvature :\n\nwhere is the flat Laplace operator\n\nLiouville's equation appears in the study of isothermal coordinates in differential geometry: the independent variables are the coordinates, while can be described as the conformal factor with respect to the flat metric. Occasionally it is the square that is referred to as the conformal factor, instead of itself.\n\nLiouville's equation was also taken as an example by David Hilbert in the formulation of his nineteenth problem.\n\nBy using the change of variables , another commonly found form of Liouville's equation is obtained:\n\nOther two forms of the equation, commonly found in the literature, are obtained by using the slight variant of the previous change of variables and Wirtinger calculus:\n\nNote that it is exactly in the first one of the preceding two forms that Liouville's equation was cited by David Hilbert in the formulation of his nineteenth problem.\n\nIn a more invariant fashion, the equation can be written in terms of the \"intrinsic\" Laplace-Beltrami operator\n\nas follows:\n\nLiouville's equation is a consequence of the Gauss–Codazzi equations when the metric is written in isothermal coordinates.\n\nIn a simply connected domain , the general solution of Liouville's equation can be found by using Wirtinger calculus. Its form is given by\n\nwhere is any meromorphic function such that\n\nLiouville's equation can be used to prove the following classification results for surfaces:\n\n. A surface in the Euclidean 3-space with metric , and with constant scalar curvature is locally isometric to:\n\n\n"}
{"id": "40071018", "url": "https://en.wikipedia.org/wiki?curid=40071018", "title": "Liouville–Arnold theorem", "text": "Liouville–Arnold theorem\n\nIn dynamical systems theory, the Liouville–Arnold theorem states that if, in a Hamiltonian dynamical system with \"n\" degrees of freedom, there are also known \"n\" first integrals of motion that are independent and in involution, then there exists a canonical transformation to action-angle coordinates in which the transformed Hamiltonian is dependent only upon the action coordinates and the angle coordinates evolve linearly in time. Thus the equations of motion for the system can be solved in quadratures if the canonical transform is explicitly known. The theorem is named after Joseph Liouville and Vladimir Arnold.\n"}
{"id": "353855", "url": "https://en.wikipedia.org/wiki?curid=353855", "title": "List of linear algebra topics", "text": "List of linear algebra topics\n\nThis is a list of linear algebra topics. See also:\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "7218498", "url": "https://en.wikipedia.org/wiki?curid=7218498", "title": "List of things named after Hermann Weyl", "text": "List of things named after Hermann Weyl\n\nThis is a list of topics named after Hermann Weyl, the influential German mathematician from the 20th century.\n\n"}
{"id": "30852653", "url": "https://en.wikipedia.org/wiki?curid=30852653", "title": "Monatshefte für Mathematik", "text": "Monatshefte für Mathematik\n\nThe journal was founded by Gustav von Escherich and Emil Weyr in 1890 as \"Monatshefte für Mathematik und Physik\" and published until 1941. In 1947 it was reestablished by Johann Radon under its current title. It is currently published by Springer in cooperation with the Austrian Mathematical Society. The journal is indexed by \"Mathematical Reviews\" and Zentralblatt MATH.\nIts 2009 MCQ was 0.58, and its 2009 impact factor was 0.764.\n\n"}
{"id": "4190631", "url": "https://en.wikipedia.org/wiki?curid=4190631", "title": "Moscow Mathematical Journal", "text": "Moscow Mathematical Journal\n\nThe Moscow Mathematical Journal is a mathematics journal published quarterly by the Independent University of Moscow and the HSE Faculty of Mathematics and distributed by the American Mathematical Society. The journal published its first issue in 2001. Its editors-in-chief are Yulij Ilyashenko (Independent University of Moscow and Cornell University), Michael Tsfasman (Independent University of Moscow and Aix-Marseille University), and Sabir Gusein-Zade (Moscow State University and the Independent University of Moscow).\n"}
{"id": "2540892", "url": "https://en.wikipedia.org/wiki?curid=2540892", "title": "Multiplication (music)", "text": "Multiplication (music)\n\nThe mathematical operations of multiplication have several applications to music. Other than its application to the frequency ratios of intervals (e.g., Just intonation, and the twelfth root of two in equal temperament), it has been used in other ways for twelve-tone technique, and musical set theory. Additionally ring modulation is an electrical audio process involving multiplication that has been used for musical effect.\n\nA multiplicative operation is a mapping in which the argument is multiplied . Multiplication originated intuitively in interval expansion, including tone row order number rotation, for example in the music of Béla Bartók and Alban Berg . Pitch number rotation, \"Fünferreihe\" or \"five-series\" and \"Siebenerreihe\" or \"seven-series\", was first described by Ernst Krenek in \"Über neue Musik\" (; ). Princeton-based theorists, including James K. , Godfrey , and Hubert S. \"were the first to discuss and adopt them, not only with regards to twelve-tone series\" .\n\nWhen dealing with pitch-class sets, multiplication modulo 12 is a common operation. Dealing with all twelve tones, or a tone row, there are only a few numbers which one may multiply a row by and still end up with a set of twelve distinct tones. Taking the prime or unaltered form as P, multiplication is indicated by formula_1, formula_2 being the multiplicator:\n\nThe following table lists all possible multiplications of a chromatic twelve-tone row:\n\nNote that only M, M, M, and M give a one-to-one mapping (a complete set of 12 unique tones). This is because each of these numbers is relatively prime to 12. Also interesting is that the chromatic scale is mapped to the circle of fourths with M, or fifths with M, and more generally under M all even numbers stay the same while odd numbers are transposed by a tritone. This kind of multiplication is frequently combined with a transposition operation. It was first described in print by Herbert Eimert, under the terms \"Quartverwandlung\" (fourth transformation) and \"Quintverwandlung\" (fifth transformation) , and has been used by the composers Milton Babbitt (; ), Robert , and Charles Wuorinen . This operation also accounts for certain harmonic transformations in jazz .\n\nThus multiplication by the two meaningful operations (5 & 7) may be designated with M(\"a\") and M(\"a\") or M and IM .\n\n\nPierre described an operation he called pitch multiplication, which is somewhat akin to the Cartesian product of pitch-class sets. Given two sets, the result of pitch multiplication will be the set of sums (modulo 12) of all possible pairings of elements between the original two sets. Its definition:\n\nFor example, if multiplying a C-major chord formula_5 with a dyad containing C,D formula_6, the result is:\n\nIn this example, a set of three pitches multiplied with a set of two pitches gives a new set of 3 × 2 pitches. Given the limited space of modulo 12 arithmetic, when using this procedure very often duplicate tones are produced, which are generally omitted. This technique was used most famously in Boulez's 1955 \"Le marteau sans maître\", as well as in his Third Piano Sonata, \"Pli selon pli\", \"Eclat\" (and \"Eclat multiples\"), \"Figures-Doubles-Prisms\", \"Domaines\", and \"Cummings ist der Dichter\", as well as the withdrawn choral work, \"Oubli signal lapidé\" (1952) (; ; ).\n\nHoward Hanson called this operation of commutative mathematical convolution \"superposition\" or \"@-projection\" and used the \"/\" notation interchangeably. Thus \"p@m\" or \"p/m\" means \"perfect fifth at major third\", e.g.: { C E G B }. He specifically noted that two triad forms could be so multiplied, or a triad multiplied by itself, to produce a resultant scale. The latter \"squaring\" of a triad produces a particular scale highly saturated in instances of the source triad . Thus \"pmn\", Hanson's name for common the major triad, when squared, is \"PMN\", e.g.: { C D E G G B }.\n\nNicholas Slonimsky used this operation, non-generalized, to form 1300 scales by multiplying the symmetric tritones, augmented chords, diminished seventh chords, and wholetone scales by the sum of 3 factors which he called interpolation, infrapolation, and ultrapolation . The combination of interpolation, infrapolation, and ultrapolation, forming obliquely infra-interpolation, infra-ultrapolation, and infra-inter-ultrapolation, additively sums to what is effectively a second sonority. This second sonority, multiplied by the first, gives his formula for generating scales and their harmonizations.\n\nJoseph Schillinger used the idea, undeveloped, to categorize common 19th- and early 20th-century harmonic styles as product of horizontal harmonic root-motion and vertical harmonic structure . Some of the composers' styles which he cites appear in the following multiplication table.\n\nThe approximation of the 12 pitches of Western music by modulus-12 math, forming the Circle of Halfsteps, means that musical intervals can also be thought of as angles in a polar coordinate system, stacking of identical intervals as functions of harmonic motion, and transposition as rotation around an axis. Thus, in the multiplication example above from Hanson, \"p@m\" or \"p/m\" (\"perfect 5th at major 3rd\", e.g.: { C E G B }) also means \"perfect fifth, superimposed upon perfect fifth rotated 1/3 of the circumference of the Circle of Halfsteps\". A conversion table of intervals to angular measure (taken as negative numbers for clockwise rotation) follows:\n\nThis angular interpretation of intervals is helpful to visualize a very practical example of multiplication in music: Euler-Fokker genera used in describing the Just intonation tuning of keyboard instruments . Each genus represents an harmonic function such as \"3 perfect fifths stacked\" or other sonority such as { C G D F }, which, when multiplied by the correct angle(s) of copy, approximately fills the 12TET circumferential space of the Circle of fifths. It would be possible, though not musically pretty, to tune an augmented triad of two perfect non-beating major thirds, then (multiplying) tune two tempered fifths above and 1 below each note of the augmented chord; this is Euler-Fokker genus [555]. A different result is obtained by starting with the \"3 perfect fifths stacked\", and from these non-beating notes tuning a tempered major third above and below; this is Euler-Fokker genus [333].\n\nJoseph Schillinger described an operation of \"polynomial time multiplication\" (\"polynomial\" refers to any rhythm consisting of more than one duration) corresponding roughly to that of Pitch multiplication above ( ). A theme, reduced to a consistent series of integers representing the quarter, 8th-, or 16th-note duration of each of the notes of the theme, could be multiplied by itself or the series of another theme to produce a coherent and related variation. Especially, a theme's series could be squared or cubed or taken to higher powers to produce a saturation of related material.\n\nHerbert Eimert described what he called the \"eight modes\" of the twelve-tone series, all mirror forms of one another. The inverse is obtained through a horizontal mirror, the retrograde through a vertical mirror, the retrograde-inverse through both a horizontal and a vertical mirror, and the \"cycle-of-fourths-transform\" or \"Quartverwandlung\" and \"cycle-of-fifths-transform\" or \"Quintverwandlung\" obtained through a slanting mirror . With the retrogrades of these transforms and the prime, there are eight permutations.\n\nJoseph Schillinger embraced not only contrapuntal inverse, retrograde, and retrograde-inverse—operations of matrix multiplication in Euclidean vector space—but also their rhythmic counterparts as well. Thus he could describe a variation of theme using the same pitches in same order, but employing its original time values in retrograde order. He saw the scope of this multiplicatory universe beyond simple reflection, to include transposition and rotation (possibly with projection back to source), as well as dilation which had formerly been limited in use to the time dimension (via augmentation and diminution) (). Thus he could describe another variation of theme, or even of a basic scale, by multiplying the halfstep counts between each successive pair of notes by some factor, possibly normalizing to the octave via Modulo-12 operation ().\n\nSome Z-related chords are connected by \"M\" or \"IM\" (multiplication by 5 or multiplication by 7), due to identical entries for 1 and 5 on the APIC vector .\n\n\n"}
{"id": "685158", "url": "https://en.wikipedia.org/wiki?curid=685158", "title": "Neyman–Pearson lemma", "text": "Neyman–Pearson lemma\n\nIn statistics, the Neyman–Pearson lemma was introduced by Jerzy Neyman and Egon Pearson in a paper in 1933.\n\nSuppose one is performing a hypothesis test between two simple hypotheses \"H\": \"θ\" = \"θ\" and \"H\": \"θ\" = \"θ\" using the likelihood-ratio test with threshold formula_1, which rejects \"H\" in favour of \"H\" at a significance level of \nwhere\nand formula_4 is the likelihood function.\nThen, the lemma states that formula_5 is the most powerful test at significance level \"α\".\n\nIf the test is most powerful for all formula_6, it is said to be uniformly most powerful (UMP) for alternatives in the set formula_7.\n\nIn practice, the likelihood ratio is often used directly to construct tests — see likelihood-ratio test. However it can also be used to suggest particular test-statistics that might be of interest or to suggest simplified tests — for this, one considers algebraic manipulation of the ratio to see if there are key statistics in it related to the size of the ratio (i.e. whether a large statistic corresponds to a small ratio or to a large one).\n\nDefine the rejection region of the null hypothesis for the Neyman–Pearson (NP) test as\n\nwhere formula_1 is chosen so that formula_10\n\nAny other test will have a different rejection region that we denote by formula_11. The probability of the data falling in region formula_12 given parameter formula_13 is\n\nFor the test with critical region formula_11 to have level formula_16, it must be true that formula_17, hence\n\nIt will be useful to break these down into integrals over distinct regions:\n\nSetting formula_20, these two expressions and the above inequality yield that\n\nThe powers of the two tests are formula_22 and formula_23, and we would like to prove that:\n\nHowever, as shown above this is equivalent to:\n\nformula_25\n\nin what follows we show that the above inequality holds:\n\nLet formula_27 be a random sample from the formula_28 distribution where the mean formula_29 is known, and suppose that we wish to test for formula_30 against formula_31. The likelihood for this set of normally distributed data is\n\nWe can compute the likelihood ratio to find the key statistic in this test and its effect on the test's outcome:\n\nThis ratio only depends on the data through formula_34. Therefore, by the Neyman–Pearson lemma, the most powerful test of this type of hypothesis for this data will depend only on formula_34. Also, by inspection, we can see that if formula_36, then formula_37 is a decreasing function of formula_34. So we should reject formula_39 if formula_34 is sufficiently large. The rejection threshold depends on the size of the test. In this example, the test statistic can be shown to be a scaled Chi-square distributed random variable and an exact critical value can be obtained.\n\nA variant of the Neyman–Pearson lemma has found an application in the seemingly unrelated domain of the economics of land value. One of the fundamental problems in consumer theory is calculating the demand function of the consumer given the prices. In particular, given a heterogeneous land-estate, a price measure over the land, and a subjective utility measure over the land, the consumer's problem is to calculate the best land parcel that he can buy – i.e. the land parcel with the largest utility, whose price is at most his budget. It turns out that this problem is very similar to the problem of finding the most powerful statistical test, and so the Neyman–Pearson lemma can be used.\n\nThe Neyman–Pearson lemma is quite useful in electronics engineering, namely in the design and use of radar systems, digital communication systems, and in signal processing systems. \nIn radar systems, the Neyman–Pearson lemma is used in first setting the rate of missed detections to a desired (low) level, and then minimizing the rate of false alarms, or vice versa.\nNeither false alarms nor missed detections can be set at arbitrarily low rates, including zero. All of the above goes also for many systems in signal processing.\n\nThe Neyman–Pearson lemma is applied to the construction of analysis-specific likelihood-ratios, used to e.g. test for signatures of new physics against the nominal Standard Model prediction in proton-proton collision datasets collected at the LHC.\n\n\n\n"}
{"id": "44465987", "url": "https://en.wikipedia.org/wiki?curid=44465987", "title": "Non-constructive algorithm existence proofs", "text": "Non-constructive algorithm existence proofs\n\nThe vast majority of positive results about computational problems are constructive proofs, i.e., a computational problem is proved to be solvable by showing an algorithm that solves it; a computational problem is shown to be in P (complexity) by showing an algorithm that solves it in time that is polynomial in the size of the input; etc.\n\nHowever, there are several non-constructive results, where an algorithm is proved to exist without showing the algorithm itself. Several techniques are used to provide such existence proofs.\n\nA simple example of a non-constructive algorithm was published in 1982 by Elwyn R. Berlekamp, John H. Conway, and Richard K. Guy, in their book \"Winning Ways for your Mathematical Plays\". It concerns the game of Sylver Coinage, in which players take turns specifying a positive integer that cannot be expressed as a sum of previously specified values, with a player losing when they are forced to specify the number 1. There exists an algorithm (given in the book as a flow chart) for determining whether a given first move is winning or losing: if it is a prime number greater than three, or one of a finite set of 3-smooth numbers, then it is a winning first move, and otherwise it is losing. However, the finite set is not known.\n\nNon-constructive algorithm proofs for problems in graph theory were studied beginning in 1988 by Michael Fellows and Michael Langston.\n\nA common question in graph theory is whether a certain input graph has a certain property. For example:\n\nThere is a highly exponential algorithm that decides whether two cycles embedded in a 3d-space are linked, and one could test all pairs of cycles in the graph, but it is not obvious how to account for all possible embeddings in a 3d-space. Thus, it is a-priori not clear at all if the linkedness problem is decidable.\n\nHowever, there is a non-constructive proof that shows that linkedness is decidable in polynomial time. The proof relies on the following facts:\n\n\nGiven an input graph G, the following \"algorithm\" solves the above problem:\n\nThe non-constructive part here is the Robertson–Seymour theorem. Although it guarantees that there is a finite number of minor-minimal elements it does not tell us what these elements are. Therefore, we cannot really execute the \"algorithm\" mentioned above. But, we do know that an algorithm exists and that its runtime is polynomial.\n\nThere are many more similar problems whose decidability can be proved in a similar way. In some cases, the knowledge that a problem can be proved in a polynomial time has led researchers to search and find an actual polynomial-time algorithm that solves the problem in an entirely different way. This shows that non-constructive proofs can have constructive outcomes.\n\nThe main idea is that a problem can be solved using an algorithm that uses, as a parameter, an unknown set. Although the set is unknown, we know that it must be finite, and thus a polynomial-time algorithm exists.\n\nThere are many other combinatorial problems that can be solved with a similar technique.\n\nSometimes the number of potential algorithms for a given problem is finite. We can count the number of possible algorithms and prove that only a bounded number of them are \"bad\", so at least one algorithm must be \"good\".\n\nAs an example, consider the following problem.\n\nI select a vector \"v\" composed of \"n\" elements which are integers between 0 and a certain constant \"d\".\n\nYou have to guess \"v\" by asking \"sum queries\", which are queries of the form: \"what is the sum of the elements with indices \"i\" and \"j\"?\". A sum query can relate to any number of indices from 1 to \"n\".\n\nHow many queries do you need? Obviously, \"n\" queries are always sufficient, because you can use \"n\" queries asking for the \"sum\" of a single element. But when \"d\" is sufficiently small, it is possible to do better. The general idea is as follows.\n\nEvery query can be represented as a 1-by-\"n\" vector whose elements are all in the set {0,1}. The response to the query is just the dot product of the query vector by \"v\". Every set of \"k\" queries can be represented by an \"k\"-by-\"n\" matrix over {0,1}; the set of responses is the product of the matrix by \"v\".\n\nA matrix \"M\" is \"good\" if it enables us to uniquely identify \"v\". This means that, for every vector \"v\", the product \"M v\" is different. A matrix \"M\" is \"bad\" if there are two different vectors, \"v\" and \"u\", such that \"M v\" = \"M u\".\n\nUsing some algebra, it is possible to bound the number of \"bad\" matrices. The bound is a function of \"d\" and \"k\". Thus, for a sufficiently small \"d\", there must be a \"good\" matrix with a small \"k\", which corresponds to an efficient algorithm for solving the identification problem.\n\nThis proof is non-constructive in two ways: it is not known how to find a good matrix; and even if a good matrix is supplied, it is not known how to efficiently re-construct the vector from the query replies.\n\nThere are many more similar problems which can be proved to be solvable in a similar way.\n\n\nThe references in this page were collected from the following Stack Exchange threads: \n\n"}
{"id": "1543837", "url": "https://en.wikipedia.org/wiki?curid=1543837", "title": "Phase response", "text": "Phase response\n\nIn signal processing, phase response is the relationship between the phase of a sinusoidal input and the output signal passing through any device that accepts input and produces an output signal, such as an amplifier or a filter.\n\nAmplifiers, filters, and other devices are often categorized by their amplitude and/or phase response. The amplitude response is the ratio of output amplitude to input, usually a function of the frequency. Similarly, phase response is the phase of the output with the input as reference. The input is defined as zero phase. A phase response is not limited to lying between 0° and 360°, as phase can accumulate to any amount of time.\n\n"}
{"id": "24656270", "url": "https://en.wikipedia.org/wiki?curid=24656270", "title": "Polyhedral graph", "text": "Polyhedral graph\n\nIn geometric graph theory, a branch of mathematics, a polyhedral graph is the undirected graph formed from the vertices and edges of a convex polyhedron. Alternatively, in purely graph-theoretic terms, the polyhedral graphs are the 3-vertex-connected planar graphs.\n\nThe Schlegel diagram of a convex polyhedron represents its vertices and edges as points and line segments in the Euclidean plane, forming a subdivision of an outer convex polygon into smaller convex polygons. It has no crossings, so every polyhedral graph is also a planar graph. Additionally, by Balinski's theorem, it is a 3-vertex-connected graph.\n\nAccording to Steinitz's theorem, these two graph-theoretic properties are enough to completely characterize the polyhedral graphs: they are exactly the 3-vertex-connected planar graphs. That is, whenever a graph is both planar and 3-vertex-connected, there exists a polyhedron whose vertices and edges form an isomorphic graph. Given such a graph, a representation of it as a subdivision of a convex polygon into smaller convex polygons may be found using the Tutte embedding.\n\nTait conjectured that every cubic polyhedral graph (that is, a polyhedral graph in which each vertex is incident to exactly three edges) has a Hamiltonian cycle, but this conjecture was disproved by a counterexample of W. T. Tutte, the polyhedral but non-Hamiltonian Tutte graph. If one relaxes the requirement that the graph be cubic, there are much smaller non-Hamiltonian polyhedral graphs; the one with the fewest vertices and edges is the 11-vertex and 18-edge Herschel graph, and there also exists an 11-vertex non-Hamiltonian polyhedral graph in which all faces are triangles, the Goldner–Harary graph.\n\nMore strongly, there exists a constant α < 1 (the shortness exponent) and an infinite family of polyhedral graphs such that the length of the longest simple path of an \"n\"-vertex graph in the family is O(\"n\").\n\nDuijvestijn provides a count of the polyhedral graphs with up to 26 edges; The number of these graphs with 6, 7, 8, ... edges is\nOne may also enumerate the polyhedral graphs by their numbers of vertices: for graphs with 4, 5, 6, ... vertices, the number of polyhedral graphs is\n\nA polyhedral graph is the graph of a simple polyhedron if it is cubic (every vertex has three edges), and it is the graph of a simplicial polyhedron if it is a maximal planar graph. The Halin graphs, graphs formed from a planar embedded tree by adding an outer cycle connecting all of the leaves of the tree, form another important subclass of the polyhedral graphs.\n"}
{"id": "15935860", "url": "https://en.wikipedia.org/wiki?curid=15935860", "title": "Regression validation", "text": "Regression validation\n\nIn statistics, regression validation is the process of deciding whether the numerical results quantifying hypothesized relationships between variables, obtained from regression analysis, are acceptable as descriptions of the data. The validation process can involve analyzing the goodness of fit of the regression, analyzing whether the regression residuals are random, and checking whether the model's predictive performance deteriorates substantially when applied to data that were not used in model estimation.\n\nOne measure of goodness of fit is the \"R\" (coefficient of determination), which in ordinary least squares with an intercept ranges between 0 and 1. While a low \"R\" implies that the model does not fit the data well, an \"R\" close to 1 does not guarantee that the model fits the data well: as Anscombe's quartet shows, a high \"R\" can occur in the presence of misspecification of the functional form of a relationship or in the presence of outliers that distort the true relationship.\n\nOne problem with the \"R\" as a measure of model validity is that it can always be increased by adding more variables into the model, except in the unlikely event that the additional variables are exactly uncorrelated with the dependent variable in the data sample being used. This problem can be avoided by doing an F-test of the statistical significance of the increase in the \"R\", or by instead using the adjusted \"R\"2.\n\nThe residuals from a fitted model are the differences between the responses observed at each combination of values of the explanatory variables and the corresponding prediction of the response computed using the regression function. Mathematically, the definition of the residual for the \"i\" observation in the data set is written\nwith \"y\" denoting the \"i\" response in the data set and \"x\" the vector of explanatory variables, each set at the corresponding values found in the \"i\" observation in the data set.\n\nIf the model fit to the data were correct, the residuals would approximate the random errors that make the relationship between the explanatory variables and the response variable a statistical relationship. Therefore, if the residuals appear to behave randomly, it suggests that the model fits the data well. On the other hand, if non-random structure is evident in the residuals, it is a clear sign that the model fits the data poorly. The next section details the types of plots to use to test different aspects of a model and gives the correct interpretations of different results that could be observed for each type of plot.\n\nA basic, though not quantitatively precise, way to check for problems that render a model inadequate is to conduct a visual examination of the residuals (the mispredictions of the data used in quantifying the model) to look for obvious deviations from randomness. If a visual examination suggests, for example, the possible presence of heteroskedasticity (a relationship between the variance of the model errors and the size of an independent variable's observations), then statistical tests can be performed to confirm or reject this hunch; if it is confirmed, different modeling procedures are called for.\n\nDifferent types of plots of the residuals from a fitted model provide information on the adequacy of different aspects of the model.\nGraphical methods have an advantage over numerical methods for model validation because they readily illustrate a broad range of complex aspects of the relationship between the model and the data.\n\nNumerical methods also play an important role in model validation. For example, the lack-of-fit test for assessing the correctness of the functional part of the model can aid in interpreting a borderline residual plot. One common situation when numerical validation methods take precedence over graphical methods is when the number of parameters being estimated is relatively close to the size of the data set. In this situation residual plots are often difficult to interpret due to constraints on the residuals imposed by the estimation of the unknown parameters. One area in which this typically happens is in optimization applications using designed experiments. Logistic regression with binary data is another area in which graphical residual analysis can be difficult.\n\nSerial correlation of the residuals can indicate model misspecification, and can be checked for with the Durbin–Watson statistic. The problem of heteroskedasticity can be checked for in any of several ways.\n\nCross-validation is the process of assessing how the results of a statistical analysis will generalize to an independent data set. If the model has been estimated over some, but not all, of the available data, then the model using the estimated parameters can be used to predict the held-back data. If, for example, the out-of-sample mean squared error, also known as the mean squared prediction error, is substantially higher than the in-sample mean square error, this is a sign of deficiency in the model. \n\nA development in medical statistics is the use of out-of-sample cross validation techniques in meta-analysis. It forms the basis of the \"validation statistic, Vn\", which is used to test the statistical validity of meta-analysis summary estimates. Essentially it measures a type of normalized prediction error and its distribution is a linear combination of \"χ\" variables of degree 1. \n\n\n\n"}
{"id": "298763", "url": "https://en.wikipedia.org/wiki?curid=298763", "title": "Richard M. Karp", "text": "Richard M. Karp\n\nRichard Manning Karp (born January 3, 1935) is an American computer scientist and computational theorist at the University of California, Berkeley. He is most notable for his research in the theory of algorithms, for which he received a Turing Award in 1985, The Benjamin Franklin Medal in Computer and Cognitive Science in 2004, and the Kyoto Prize in 2008.\n\nBorn to Abraham and Rose Karp in Boston, Massachusetts, Karp has three younger siblings: Robert, David, and Carolyn. He attended Harvard University, where he received his bachelor's degree in 1955, his master's degree in 1956, and his Ph.D. in applied mathematics in 1959.\n\nHe started working at IBM's Thomas J. Watson Research Center. In 1968, he became Professor of Computer Science, Mathematics, and Operations Research at the University of California, Berkeley. Apart from a 4-year period as a professor at the University of Washington, he has remained at Berkeley. From 1988 to 1995 and 1999 to the present he has also been a Research Scientist at the International Computer Science Institute in Berkeley, where he currently leads the Algorithms Group.\n\nRichard Karp was awarded the National Medal of Science, and was the recipient of the Harvey Prize of the Technion and the 2004 Benjamin Franklin Medal in Computer and Cognitive Science for his insights into computational complexity. In 1994 he was inducted as a Fellow of the Association for Computing Machinery. He is the recipient of several honorary degrees.\n\nIn 2012, Karp became the founding director of the Simons Institute for the Theory of Computing at the University of California, Berkeley.\n\nKarp has made many important discoveries in computer science, combinatorial algorithms, and operations research . His major current research interests include bioinformatics.\nIn 1971 he co-developed with Jack Edmonds the Edmonds–Karp algorithm for solving the max-flow problem on networks, and in 1972 he published a landmark paper in complexity theory, \"Reducibility Among Combinatorial Problems\", in which he proved 21 Problems to be NP-complete.\n\nIn 1973 he and John Hopcroft published the Hopcroft–Karp algorithm, still the fastest known method for finding maximum cardinality matchings in bipartite graphs.\n\nIn 1980, along with Richard J. Lipton, Karp proved the Karp-Lipton theorem (which proves that, if SAT can be solved by Boolean circuits with a polynomial number of logic gates, then the polynomial hierarchy collapses to its second level).\n\nIn 1987 he co-developed with Michael O. Rabin the Rabin-Karp string search algorithm.\n\nHis citation for the \"(1985)\" Turing Award was as follows: \n\n"}
{"id": "6419203", "url": "https://en.wikipedia.org/wiki?curid=6419203", "title": "Ruan Yuan", "text": "Ruan Yuan\n\nRuan Yuan (; 1764–1849) was a Chinese scholar official of the Qing Dynasty who was the most prominent Chinese scholar during the first half of the 19th century. He won the \"jinshi\" degree in the imperial examinations in 1789 and was subsequently appointed to the Hanlin Academy. He was known for his work \"Biographies of Astronomers and Mathematicians\" and for his editing the \"Shisan Jing Zhushu\" (Commentaries and Notes on the Thirteen Classics) for the Qing emperor.\n\nRuan Yuan was a successful official as well as a scholar. He was the Viceroy of Liangguang, the most important imperial official in Canton (Guangzhou), during the critical years 1817–1826, just before the First Opium War with Britain. It was a crucial time when Chinese trade with the outside world was allowed only through the Canton System, with all foreigners confined to Canton, the capital of Guangdong Province. During his tenure in Canton, Ruan is estimated to have earned more than 195,000 taels of silver.\n\nHe was widely recognized as an official, scholar, and patron of learning both by his contemporaries and by modern scholars. He was also praised as an honest official and an exemplary man of the ‘Confucian persuasion’. His name is mentioned in almost all works on Qing history or Chinese classics because of the wide range of his research and publications. A number of these publications are still reprinted. Ruan Yuan was a follower of the Han Learning tradition and as such, with the encouragement of Liu Fenglu, he edited and organized publication of the compendium of the imperial achievements in \"kaozheng\" scholarship, the \"Huang Qing Jingjie\" () published in 1829.\n\nKong Luhua (relative of the Duke Yansheng) was the second wife of Ruan Yuan.\n\n\n\n"}
{"id": "715594", "url": "https://en.wikipedia.org/wiki?curid=715594", "title": "Samuel L. Greitzer", "text": "Samuel L. Greitzer\n\nSamuel L. Greitzer (August 10, 1905 – February 22, 1988) was an American mathematician, the founding chairman of the United States of America Mathematical Olympiad, and the publisher of the precollege mathematics journal \"Arbelos\". Together with H.S.M. Coxeter in 1967, Greitzer coauthored the well-received textbook \"Geometry Revisited\", which has remained in print for more than 40 years.\n\nBorn in Russia, Greitzer moved to the United States in 1906, graduated from Stuyvesant High School, received his bachelor's degree in 1927 from City College of New York, and later earned a Ph.D. from Yeshiva University. He held academic positions at Yeshiva University, Brooklyn Polytechnic Institute, Columbia University, and Rutgers University. In the 1970s, he directed a National Science Foundation summer program at Rutgers for high-ability high-school math students.\n\nSamuel Greitzer and his wife Ethel had one son. Samuel died on February 22, 1988 in Metuchen, New Jersey.\n\n"}
{"id": "10851027", "url": "https://en.wikipedia.org/wiki?curid=10851027", "title": "Statistical geography", "text": "Statistical geography\n\nStatistical geography is the study and practice of collecting, analysing and presenting data that has a geographic or areal dimension, such as census or demographics data. It uses techniques from spatial analysis, but also encompasses geographical activities such as the defining and naming of geographical regions for statistical purposes. For example, for the purposes of statistical geography, the Australian Bureau of Statistics uses the Australian Standard Geographical Classification, a hierarchical regionalisation that divides Australia up into states and territories, then statistical divisions, statistical subdivisions, statistical local areas, and finally census collection districts.\n\nGeographers study how and why elements differ from place to place, as well as how spatial patterns change through time. Geographers begin with the question 'Where?', exploring how features are distributed on a physical or cultural landscape, observing spatial patterns and the variation of phenomena. Contemporary geographical analysis has shifted to 'Why?', determining why a specific spatial pattern exists, what spatial or ecological processes may have affected a pattern, and why such processes operate. Only by approaching the 'why?' questions can social scientists begin to appreciate the mechanisms of change, which are infinite in their complexity.\n\nStatistical techniques and procedures are applied in all fields of academic research; wherever data are collected and summarized or wherever any numerical information is analyzed or research is conducted, statistics are needed for sound analysis and interpretation of results. \n\nGeographers use statistics in numerous ways:\n\n\nThere are several potential difficulties associated with the analysis of spatial data, among these are boundary delineation, modifiable areal units, and the level of spatial aggregation or scale. In each of these cases, the absolute descriptive statistics of an area - the mean, median, mode, standard deviation, and variation - are changed through the manipulation of these spatial problems.\n\nThe location of a study area boundary and the positioning of internal boundaries affect various descriptive statistics. With respect to measures such as the mean or standard deviation, the study area size alone may have large implications; consider a study of per capita income within a city, if confined to the inner city, income levels are likely to be lower because of a less affluent population, if expanded to include the suburbs or surrounding communities, income levels will become greater with the influence of homeowner populations. Because of this problem, absolute descriptive statistics such as the mean, standard deviation, and variance should be evaluated comparatively only in relation to a particular study area. In the determination of internal boundaries this is also true, as these statistics may only have valid interpretations for the area and subarea configuration over which they are calculated.\n\n\"See also\": Modifiable areal unit problem\n\nIn many cases the subdivision of spatial data has already been determined, this is evident in demographic datasets, as the available information will be grouped into their respective counties or municipalities. For this type of data, analysts must use the same county or municipal boundaries delineated in the collected data for their subsequent analysis. When alternate boundaries are possible, an analyst must take into account that any new subdivision model may create different results.\n\nSocio-economic data may be available at a variety of scales, for example: municipalities, regional districts, census tracts, enumeration districts, or at the provincial/state level. When this data is aggregated at different scales, the resulting descriptive statistics may exhibit variations, either in a systematic, predictable way, or in a more uncertain fashion. If we are observing economic data, we may notice a distinct reduction in manufacturing productivity for a country (the USA) over a certain period; since this is a general model, individual states may experience these effects differently. The result of this aggregation is that the standard deviation of the data in question is increased due to the variability among states.\n\nFor summarizing point pattern analysis, a set of descriptive spatial statistics has been developed that are areal equivalents to nonspatial measures. Since geographers are particularly concerned with the analysis of locational data, these descriptive spatial statistics (geostatistics) are often applied to summarize point patterns and to describe the degree of spatial variability of some phenomena.\n\nAn example here is the idea of a center of population, of which a particular example is the mean center of U.S. population. Several different ways of defining a center are available:\n\nJust as the standard deviation indicates how closely the values in a data set are clustered around the mean, so standard distance in a spatial distribution indicates how closely the points are clustered around the mean centre.\n\n\nThe motivating insight behind topology is that some geometric problems depend not on the exact shape of the objects involved, but rather on the \"way they are connected together\". One of the first papers in topology was the demonstration, by Leonhard Euler, that it was impossible to find a route through the town of Königsberg (now Kaliningrad) that would cross each of its seven bridges exactly once. This result did not depend on the lengths of the bridges, nor on their distance from one another, but only on connectivity properties: which bridges are connected to which islands or riverbanks. This problem, the \"Seven Bridges of Königsberg\", is now a famous problem in introductory mathematics, and led to the branch of mathematics known as graph theory.\n\nTopology rules are particularly important within GIS, and are used for a variety of correction and analytical procedures. The primary shapes in GIS are the point, line, and polygon, each of which implies different spatial characteristics; for instance, the only shape which has a distinguishable inside and outside is the polygon. Principles of connectivity associated with topology lead to applications in hydrology, urban planning, and logistics, as well as other fields; as such, topological analyses offer unique modelling capabilities, defining the vector nature of topological features and correcting spatial data errors from digitizing.\n\nDue to the devolved nature of the United Kingdom, responsibility for managing statistical geographies often falls to the National Statistical Institute with jurisdiction for that devolved administration. For England and Wales this is the Office for National Statistics, for Scotland National Records of Scotland and for Northern Ireland the Northern Ireland Statistics and Research Agency.\n\nThe lowest form of statistical geography in England and Wales is the Output Area. These are small geographies of approximately 300 people and 100 households for which Census data is published. By containing roughly the same number of people and households it is possible to compare statistics for any two Output Areas in the country, and know that this is being done in a consistent way (unlike comparing statistics for Administrative geographies).\n\nThe Output Areas form the smallest part of a hierarchy that consists of Output Areas, Lower Layer Super Output Areas and Middle Layer Super Output Areas.\n\nEngland and Wales also have a statistical geography designed specifically for the publication of workplace statistics. This is because Output Areas are built around residential populations and make analysing workplace statistics difficult. Workplace Zones have been released as part of the 2011 Census.\n\nLike England and Wales, the lowest level of statistical geography in Scotland is the Output Area. Scottish OAs are smaller than those for England and Wales because smaller thresholds are applied, but the methodology for their creation is broadly similar to that used by ONS.\n\nThe higher levels are again similar to England and Wales but operate as Data Zones and Intermediate Zones rather than Lower and Middle Layer Super Output Areas.\n\nThere are no Workplace Zones for Scotland.\n\n"}
{"id": "57961238", "url": "https://en.wikipedia.org/wiki?curid=57961238", "title": "Thamsanqa Kambule", "text": "Thamsanqa Kambule\n\nThamsanqa Kambule (15 January 1921 – 7 August 2009) was a South African Mathematician and Educator. He was the first black professor at the University of the Witwatersrand, and was the first black person to be awarded honorary membership to the Actuarial Society of South Africa. He was awarded the Order of the Baobab in 2002 for his services to mathematics education.\n\nKambule was born in Aliwal North. His mother died when he was 18 months old, and his aunt was responsible for raising him. He did not attend school until he was 11 years old, when he joined Anglican St Peter's School in Johannesburg. He completed a Teachers Diploma at Adams College in 1946 and a Bachelor's degree at the University of South Africa in 1954.\n\nKambule taught in Zambia, Malawai as well as several schools in South Africa before being appointed Principal of Orlando High School in Soweto in 1958. He campaigned to ensure the children had the best education possible, despite the restrictions of the Bantu Education Act, 1953. Orlando High School had a library named after Robert Birley, a visiting professor at the University of the Witwatersrand. He led the Rand Bursary Fund, a support program that provided scholarships for pupils in need. The fund allowed more than 1,000 students to complete high school. His former pupils included Desmond Tutu and Jackie Selebi. In 1976 during the Soweto uprising, the schoolchildren revolted against being forced to learn in the Afrikaans language. An undetermined number of children were shot dead by police, and education in townships fell apart. Kambule resigned in 1977 to protest against the Department of Bantu Education, and became the head of Pace College. \n\nIn 1978 he joined the University of the Witwatersrand, where he became the first black professor. He published a series of maths textbooks for non-specialist teachers. He retired in 1976 and promptly became the Principal of \"O R T Step College of Technology\". He was awarded an honorary doctorate in 1997 and a doctorate of education in 2006. In 2002 he was awarded the Order of the Baobab from Thabo Mbeki. He became known as \"the Rock\" for his transparent principles. \n\nKambule died on 7 August 2009. He was a much loved teacher, and his former students Siphiwe Nyanda, Felicia Mabuza-Suttle and Mokotedi Mpshe attended his memorial service. His student Trevor Mdaka was his doctor at the Unitas Hospital in Centurion. \n\nIn 2017 the University of the Witwatersrand named their Mathematical Sciences Building after him. Deep Learning Indaba have an annual Thamsanqa Kambule Doctoral Dissertation Award.\n"}
