{"id": "939425", "url": "https://en.wikipedia.org/wiki?curid=939425", "title": "999 (number)", "text": "999 (number)\n\n999 (nine hundred ninety-nine or nine-nine-nine) is a natural number following 998 and preceding 1000.\n\n\nIn SubGenius numerology the number 999 represents Bob Dobbs.\n\n999 is the number used to contact the Emergency Services in many countries; See 999 (emergency telephone number).\n"}
{"id": "1601833", "url": "https://en.wikipedia.org/wiki?curid=1601833", "title": "Algebraic solution", "text": "Algebraic solution\n\nAn algebraic solution or solution in radicals is a closed-form expression, and more specifically a closed-form algebraic expression, that is the solution of an algebraic equation in terms of the coefficients, relying only on addition, subtraction, multiplication, division, raising to integer powers, and the extraction of nth roots (square roots, cube roots, and other integer roots). \n\nThe most well-known example is the solution \n\nintroduced in secondary school, of the quadratic equation\n\n(where \"a\" ≠ 0).\n\nThere exist more complicated algebraic solutions for the general cubic equation and quartic equation. The Abel–Ruffini theorem states that the general quintic equation lacks an algebraic solution, and this directly implies that the general polynomial equation of degree \"n\", for \"n\" ≥ 5, cannot be solved algebraically. However, for \"n\" ≥ 5, some polynomial equations have algebraic solutions; for example, the equation formula_3 can be solved as formula_4 See for various other examples in degree 5.\n\nÉvariste Galois introduced a criterion allowing one to decide which equations are solvable in radicals. See Radical extension for the precise formulation of his result.\n\nAlgebraic solutions form a subset of closed-form expressions, because the latter permit transcendental functions (non-algebraic functions) such as the exponential function, the logarithmic function, and the trigonometric functions and their inverses.\n\n"}
{"id": "20717382", "url": "https://en.wikipedia.org/wiki?curid=20717382", "title": "Alternating polynomial", "text": "Alternating polynomial\n\nIn algebra, an alternating polynomial is a polynomial formula_1 such that if one switches any two of the variables, the polynomial changes sign:\nEquivalently, if one permutes the variables, the polynomial changes in value by the sign of the permutation:\n\nMore generally, a polynomial formula_4 is said to be \"alternating in\" formula_5 if it changes sign if one switches any two of the formula_6, leaving the formula_7 fixed.\n\nProducts of symmetric and alternating polynomials (in the same variables formula_5) behave thus:\n\nThis is exactly the addition table for parity, with \"symmetric\" corresponding to \"even\" and \"alternating\" corresponding to \"odd\". Thus, the direct sum of the spaces of symmetric and alternating polynomials forms a superalgebra (a formula_9-graded algebra), where the symmetric polynomials are the even part, and the alternating polynomials are the odd part.\nThis grading is unrelated to the grading of polynomials by degree.\n\nIn particular, alternating polynomials form a module over the algebra of symmetric polynomials (the odd part of a superalgebra is a module over the even part); in fact it is a free module of rank 1, with as generator the Vandermonde polynomial in \"n\" variables.\n\nIf the characteristic of the coefficient ring is 2, there is no difference between the two concepts: the alternating polynomials are precisely the symmetric polynomials.\n\nThe basic alternating polynomial is the Vandermonde polynomial:\nThis is clearly alternating, as switching two variables changes the sign of one term and does not change the others.\n\nThe alternating polynomials are exactly the Vandermonde polynomial times a symmetric polynomial: formula_11 where formula_12 is symmetric.\nThis is because:\n\nConversely, the ratio of two alternating polynomials is a symmetric function, possibly rational (not necessarily a polynomial), though the ratio of an alternating polynomial over the Vandermonde polynomial is a polynomial.\nSchur polynomials are defined in this way, as an alternating polynomial divided by the Vandermonde polynomial.\n\nThus, denoting the ring of symmetric polynomials by Λ, the ring of symmetric and alternating polynomials is formula_20, or more precisely formula_21, where formula_22 is a symmetric polynomial, the discriminant.\n\nThat is, the ring of symmetric and alternating polynomials is a quadratic extension of the ring of symmetric polynomials, where one has adjoined a square root of the discriminant.\n\nAlternatively, it is:\n\nIf 2 is not invertible, the situation is somewhat different, and one must use a different polynomial formula_24, and obtains a different relation; see Romagny.\n\nFrom the perspective of representation theory, the symmetric and alternating polynomials are subrepresentations of the action of the symmetric group on \"n\" letters on the polynomial ring in \"n\" variables. (Formally, the symmetric group acts on \"n\" letters, and thus acts on derived objects, particularly free objects on \"n\" letters, such as the ring of polynomials.)\n\nThe symmetric group has two 1-dimensional representations: the trivial representation and the sign representation. The symmetric polynomials are the trivial representation, and the alternating polynomials are the sign representation. Formally, the scalar span of any symmetric (resp., alternating) polynomial is a trivial (resp., sign) representation of the symmetric group, and multiplying the polynomials tensors the representations.\n\nIn characteristic 2, these are not distinct representations, and the analysis is more complicated.\n\nIf formula_25, there are also other subrepresentations of the action of the symmetric group on the ring of polynomials, as discussed in representation theory of the symmetric group.\n\nAlternating polynomials are an unstable phenomenon (in the language of stable homotopy theory): the ring of symmetric polynomials in \"n\" variables can be obtained from the ring of symmetric polynomials in arbitrarily many variables by evaluating all variables above formula_26 to zero: symmetric polynomials are thus \"stable\" or \"compatibly defined.\" However, this is not the case for alternating polynomials, in particular the Vandermonde polynomial.\n\n\n"}
{"id": "19874619", "url": "https://en.wikipedia.org/wiki?curid=19874619", "title": "Carlos Ginzburg", "text": "Carlos Ginzburg\n\nCarlos Ginzburg is a conceptual artist and theoretician born in 1946 in La Plata, Argentina. He studied philosophy and social theory.\n\nGermano Celant, when writing about Arte Povera, invited Ginzburg by letter to join his movement.\n\nAs a conceptual artist interested in digital art, fractals chaos and fractal art, Ginzburg created what he calls \"homo fractalus\" – a concept about microcosm totality.\n\nHe has worked with the art critic Pierre Restany (with whom he developed the concept of \"Political Ecology\") and with Severo Sarduy who put him near Hokusai in \"Barroco\", one of the reference's books to \"Le Pli\" of Gilles Deleuze.\n\nHe lives in Paris and works, since 2005, with the art criticism and French artist KolbaSha/Tschann on chaos's destruction and advent paradise.\n\nPersonal exhibition\n\n\nCollectival exhibition\n\n\n\n\nPascale LE THOREL-DAVIOT\n\n\nGillo Dorfles\n\n\nPierre Restany\n\n\nSevero Sarduy\n\n\nSevero Sarduy et Klaus Ottman\n\n\nTim Jacobs\n\n\nJean-Claude Chirollet (philosophe, esthéticien, spécialiste de l'art fractaliste, Université de Strasbourg):\n\nla Forme\", éd. L'Harmattan, Paris, p. 283-295\néd. du Treize Mars, Paris, septembre 1998, p. 112-114\nUniversité du Québec, p. 103-132.\n\nPaul Ardenne\n\n\n\n\n\n\n\n"}
{"id": "46405381", "url": "https://en.wikipedia.org/wiki?curid=46405381", "title": "Chaotic cryptology", "text": "Chaotic cryptology\n\nChaotic cryptology includes two integral opposite parts: Chaotic cryptography and Chaotic cryptanalysis. Chaotic cryptography is the application of the mathematical chaos theory to the practice of the cryptography, the study or techniques used to privately and securely transmit information with the presence of a third-party or adversary. Since first being investigated by Robert Matthews in 1989 , the use of chaos in cryptography has attracted much interest; however, long-standing concerns about its security and implementation speed continue to limit its implementation.\n\nIn order to use chaos theory efficiently in cryptography, the chaotic maps should be implemented such that the entropy generated by the map can produce required Confusion and diffusion. Properties in chaotic systems and cryptographic primitives share unique characteristics that allow for the chaotic systems to be applied to cryptography. If chaotic parameters, as well as cryptographic keys, can be mapped symmetrically or mapped to produce acceptable and functional outputs, it will make it next to impossible for an adversary to find the outputs without any knowledge of the initial values. Since chaotic maps in a real life scenario require a set of numbers that are limited, they may, in fact, have no real purpose in a cryptosystem if the chaotic behavior can be predicted. One of the most important issues for any cryptographic primitive is the security of the system. However, in numerous cases, chaos-based cryptography algorithms are proved unsecure. The main issue in many of the cryptanalyzed algorithms is the inadequacy of the chaotic maps implemented in the system.\nThe concept of chaos cryptography or in the other words chaos-based cryptography can be divided into two major groups: the asymmetric and symmetric chaos-based cryptography. The majority of the symmetric chaos-based algorithms are based on the application of discrete chaotic maps in their process.\n\nBourbakis and Alexopoulos in 1991 proposed supposedly the earliest fully intended digital image encryption scheme which was based on SCAN language. Later on, with the emergence of chaos-based cryptography hundreds of new image encryption algorithms, all with the aim of improving the security of digital images were proposed. However, there were three main aspects of the design of an image encryption that was usually modified in different algorithms (chaotic map, application of the map and structure of algorithm). The initial and perhaps most crucial point was the chaotic map applied in the design of the algorithms. The speed of the cryptosystem is always an important parameter in the evaluation of the efficiency of a cryptography algorithm, therefore, the designers were initially interested in using simple chaotic maps such as tent map, and the logistic map. However, in 2006 and 2007, the new image encryption algorithms based on more sophisticated chaotic maps proved that application of chaotic map with higher dimension could improve the quality and security of the cryptosystems.\n\nThe unpredictable behavior of the chaotic maps can be used in the generation of random numbers. Some of the earliest chaos-based random number generators tried to directly generate random numbers from the logistic map.\n"}
{"id": "20844795", "url": "https://en.wikipedia.org/wiki?curid=20844795", "title": "Comparison of cryptographic hash functions", "text": "Comparison of cryptographic hash functions\n\nThe following tables compare general and technical information for a number of cryptographic hash functions. An overview of hash function security/cryptanalysis can be found at hash function security summary.\n\nBasic general information about the cryptographic hash functions: year, designer, references, etc. \n\nThe following tables compare technical information for compression functions of cryptographic hash functions. The information comes from the specifications, please refer to them for more details. \n\n"}
{"id": "14496121", "url": "https://en.wikipedia.org/wiki?curid=14496121", "title": "Conductance (graph)", "text": "Conductance (graph)\n\nIn graph theory the conductance of a graph \"G\"=(\"V\",\"E\") measures how \"well-knit\" the graph is: it controls how fast a random walk on \"G\" converges to a uniform distribution. The conductance of a graph is often called the Cheeger constant of a graph as the \nanalog of its counterpart in spectral geometry. Since electrical networks are intimately related to random walks\nwith a long history in the usage of the term \"conductance\", this alternative name helps avoid possible confusion.\n\nThe conductance of a cut formula_1 in a graph is defined as:\n\nwhere the formula_3 are the entries of the adjacency matrix for \"G\", so that\n\nis the total number (or weight) of the edges incident with \"S\". formula_5 is also called a volume of the set formula_6.\n\nThe conductance of the whole graph is the minimum conductance over all the possible cuts:\n\nEquivalently, conductance of a graph is defined as follows: \n\nFor a \"d\"-regular graph, the conductance is equal to the isoperimetric number divided by \"d\".\n\nIn practical applications, one often considers the conductance only over a cut. A common generalization of conductance is to handle the case of weights assigned to the edges: then the weights are added; if the weight is in the form of a resistance, then the reciprocal weights are added.\n\nThe notion of conductance underpins the study of percolation in physics and other applied areas; thus, for example, the permeability of petroleum through porous rock can be modeled in terms of the conductance of a graph, with weights given by pore sizes.\n\nConductance also helps measure the quality of a Spectral clustering. The maximum among the conductance of clusters provides a bound which can be used, along with inter-cluster edge weight, to define a measure on the quality of clustering. Intuitively, the conductance of a cluster(which can be seen as a set of vertices in a graph) should be low. Apart from this, the conductance of the subgraph induced by a cluster(called \"internal conductance\") can be used as well.\n\nFor an ergodic reversible Markov chain with an underlying graph \"G\", the conductance is a way to measure how hard it is to leave a small set of nodes. Formally, the conductance of a graph is defined as the minimum over all sets formula_9 of the capacity of formula_9 divided by the ergodic flow out of formula_9. Alistair Sinclair showed that conductance is closely tied to mixing time in ergodic reversible Markov chains. We can also view conductance in a more probabilistic way, as the minimal probability of leaving a small set of nodes given that we started in that set to begin with. Writing formula_12 for the conditional probability of leaving a set of nodes S given that we were in that set to begin with, the conductance is the minimal formula_12 over sets formula_9 that have a total stationary probability of at most 1/2. \n\nConductance is related to Markov chain mixing time in the reversible setting. \n\n\n"}
{"id": "39251785", "url": "https://en.wikipedia.org/wiki?curid=39251785", "title": "Cross Gramian", "text": "Cross Gramian\n\nIn control theory, the cross Gramian (formula_1, also referred to by formula_2) is a Gramian matrix used to determine how controllable and observable a linear system is.\n\nFor the stable time-invariant linear system \n\nthe cross Gramian is defined as:\n\nand thus also given by the solution to the Sylvester equation:\n\nThe triple formula_7 is controllable and observable if and only if the matrix formula_1 is nonsingular, (i.e. formula_1 has full rank, for any formula_10).\n\nIf the associated system formula_7 is furthermore symmetric, such that there exists a transformation formula_12 with\n\nthen the absolute value of the eigenvalues of the cross Gramian equal Hankel singular values:\n\nThus the direct truncation of the singular value decomposition of the cross Gramian allows model order reduction (see ) without a balancing procedure as opposed to balanced truncation.\n\n"}
{"id": "37590891", "url": "https://en.wikipedia.org/wiki?curid=37590891", "title": "Dianalytic manifold", "text": "Dianalytic manifold\n\nIn mathematics, dianalytic manifolds are possibly non-orientable generalizations of complex analytic manifolds. A dianalytic structure on a manifold is given by an atlas of \ncharts such that the transition maps are either complex analytic maps or complex conjugates of complex analytic maps. Every dianalytic manifold is given by the quotient of an analytic manifold (possibly non-connected) by a fixed-point-free involution changing the complex structure to its complex conjugate structure. Dianalytic manifolds were introduced by , and dianalytic manifolds of 1 complex dimension are sometimes called Klein surfaces.\n"}
{"id": "53853305", "url": "https://en.wikipedia.org/wiki?curid=53853305", "title": "Fernando Q. Gouvêa", "text": "Fernando Q. Gouvêa\n\nFernando Quadros Gouvêa is a Brazilian number theorist and historian of mathematics who won the Lester R. Ford Award of the Mathematical Association of America (MAA) in 1995 for his exposition of Wiles's proof of Fermat's Last Theorem.\nHe also won the Beckenbach Book Prize of the MAA in 2007 for his book with William P. Berlinghoff, \"Math through the Ages: A Gentle History for Teachers and Others\" (Oxton House, 2002; 2nd ed., 2014).\nHe is the Carter Professor of Mathematics at Colby College in Waterville, Maine.\n\nGouvêa grew up in São Paulo, the son of a lawyer and banker, and was educated there in an English-language primary school and then at the Colégio Bandeirantes de São Paulo. He earned a bachelor's degree from the University of São Paulo, and then a master's degree in 1981 under the supervision of César Polcino Milies.\nHe moved to Harvard University in 1983 for continuing graduate study in number theory, and completed his doctorate there in 1987; his dissertation, titled \"Arithmetic of\" -\"adic Modular Forms\", was supervised by Barry Mazur.\nHe became a faculty member at the University of São Paulo, took a visiting position at Queen's University in Kingston, Ontario in 1990, and was brought to Colby by Keith Devlin, who had recently been hired as department chair there.\n\nHe is the editor of the \"Carus Mathematical Monographs\" book series, and of \"MAA Reviews\", an online book review service published by the MAA.\n\n"}
{"id": "2240299", "url": "https://en.wikipedia.org/wiki?curid=2240299", "title": "Freiman's theorem", "text": "Freiman's theorem\n\nIn mathematics, Freiman's theorem is a combinatorial result in additive number theory. In a sense it accounts for the approximate structure of sets of integers that contain a high proportion of their internal sums, taken two at a time.\n\nThe formal statement is:\n\nLet \"A\" be a finite set of integers such that the sumset\n\nis small, in the sense that\n\nfor some constant formula_3. There exists an \"n\"-dimensional arithmetic progression of length\n\nthat contains \"A\", and such that \"c\"' and \"n\" depend only on \"c\".\n\nA simple instructive case is the following. We always have\n\nwith equality precisely when \"A\" is an arithmetic progression.\n\nThis result is due to Gregory Freiman (1964,1966). Much interest in it, and applications, stemmed from a new proof by Imre Z. Ruzsa (1994). Mei-Chu Chang proved new polynomial estimates for the size of arithmetic progressions arising in the theorem in 2002.\n\nGreen and Ruzsa (2007) generalized the theorem for arbitrary abelian groups: here \"A\" can be contained in the sum of a generalized arithmetic progression and a subgroup — the name of such sets is coset-progression.\n\n\n"}
{"id": "41320504", "url": "https://en.wikipedia.org/wiki?curid=41320504", "title": "Girth (functional analysis)", "text": "Girth (functional analysis)\n\nIn functional analysis, the girth of a Banach space is the infimum of lengths of centrally symmetric simple closed curves in the unit sphere of the space. Equivalently, it is twice the infimum of distances between opposite points of the sphere, as measured within the sphere.\n\nEvery finite-dimensional Banach space has a pair of opposite points on the unit sphere that achieves the minimum distance, and a centrally symmetric simple closed curve that achieves the minimum length. However, such a curve may not always exist in infinite-dimensional spaces.\n\nThe girth is always at least four, because the shortest path on the unit sphere between two opposite points cannot be shorter than the length-two line segment connecting them through the origin of the space. A Banach space for which it is exactly four is said to be \"flat\". There exist flat Banach spaces of infinite dimension in which the girth is achieved by a minimum-length curve; an example is the space \"C\"[0,1] of continuous functions from the unit interval to the real numbers, with the sup norm. The unit sphere of such a space has the counterintuitive property that certain pairs of opposite points have the same distance within the sphere that they do in the whole space.\n\nThe girth is a continuous function on the Banach–Mazur compactum, a space whose points correspond to the normed vector spaces of a given dimension. The girth of the dual space of a normed vector space is always equal to the girth of the original space.\n\n"}
{"id": "1041204", "url": "https://en.wikipedia.org/wiki?curid=1041204", "title": "Granular computing", "text": "Granular computing\n\nGranular computing (GrC) is an emerging computing paradigm of information processing that concerns the processing of complex information entities called \"information granules\", which arise in the process of data abstraction and derivation of knowledge from information or data. Generally speaking, information granules are collections of entities that usually originate at the numeric level and are arranged together due to their similarity, functional or physical adjacency, indistinguishability, coherency, or the like.\n\nAt present, granular computing is more a \"theoretical perspective\" than a coherent set of methods or principles. As a theoretical perspective, it encourages an approach to data that recognizes and exploits the knowledge present in data at various levels of resolution or scales. In this sense, it encompasses all methods which provide flexibility and adaptability in the resolution at which knowledge or information is extracted and represented.\n\n \nAs mentioned above, \"granular computing\" is not an algorithm or process; there is no particular method that is called \"granular computing\". It is rather an approach to looking at data that recognizes how different and interesting regularities in the data can appear at different levels of granularity, much as different features become salient in satellite images of greater or lesser resolution. On a low-resolution satellite image, for example, one might notice interesting cloud patterns representing cyclones or other large-scale weather phenomena, while in a higher-resolution image, one misses these large-scale atmospheric phenomena but instead notices smaller-scale phenomena, such as the interesting pattern that is the streets of Manhattan. The same is generally true of all data: At different resolutions or granularities, different features and relationships emerge. The aim of granular computing is to try to take advantage of this fact in designing more effective machine-learning and reasoning systems.\n\nThere are several types of granularity that are often encountered in data mining and machine learning, and we review them below:\n\nOne type of granulation is the quantization of variables. It is very common that in data mining or machine-learning applications the resolution of variables needs to be \"decreased\" in order to extract meaningful regularities. An example of this would be a variable such as \"outside temperature\" (formula_1), which in a given application might be recorded to several decimal places of precision (depending on the sensing apparatus). However, for purposes of extracting relationships between \"outside temperature\" and, say, \"number of health-club applications\" (formula_2), it will generally be advantageous to quantize \"outside temperature\" into a smaller number of intervals.\n\nThere are several interrelated reasons for granulating variables in this fashion:\nFor example, a simple learner or pattern recognition system may seek to extract regularities satisfying a conditional probability threshold such as formula_3. In the special case where formula_4, this recognition system is essentially detecting \"logical implication\" of the form formula_5 or, in words, \"if formula_6, then formula_7\". The system's ability to recognize such implications (or, in general, conditional probabilities exceeding threshold) is partially contingent on the resolution with which the system analyzes the variables.\n\nAs an example of this last point, consider the feature space shown to the right. The variables may each be regarded at two different resolutions. Variable formula_8 may be regarded at a high (quaternary) resolution wherein it takes on the four values formula_9 or at a lower (binary) resolution wherein it takes on the two values formula_10. Similarly, variable formula_11 may be regarded at a high (quaternary) resolution or at a lower (binary) resolution, where it takes on the values formula_12 or formula_13, respectively. It will be noted that at the high resolution, there are no detectable implications of the form formula_5, since every formula_15 is associated with more than one formula_16, and thus, for all formula_15, formula_18. However, at the low (binary) variable resolution, two bilateral implications become detectable: formula_19 and formula_20, since every formula_21 occurs \"iff\" formula_22 and formula_23 occurs \"iff\" formula_24. Thus, a pattern recognition system scanning for implications of this kind would find them at the binary variable resolution, but would fail to find them at the higher quaternary variable resolution.\n\nIt is not feasible to exhaustively test all possible discretization resolutions on all variables in order to see which combination of resolutions yields interesting or significant results. Instead, the feature space must be preprocessed (often by an entropy analysis of some kind) so that some guidance can be given as to how the discretization process should proceed. Moreover, one cannot generally achieve good results by naively analyzing and discretizing each variable independently, since this may obliterate the very interactions that we had hoped to discover.\n\nA sample of papers that address the problem of variable discretization in general, and multiple-variable discretization in particular, is as follows: , , , , , , , , , , , , , , , , \n\nVariable granulation is a term that could describe a variety of techniques, most of which are aimed at reducing dimensionality, redundancy, and storage requirements. We briefly describe some of the ideas here, and present pointers to the literature.\n\nA number of classical methods, such as principal component analysis, multidimensional scaling, factor analysis, and structural equation modeling, and their relatives, fall under the genus of \"variable transformation.\" Also in this category are more modern areas of study such as dimensionality reduction, projection pursuit, and independent component analysis. The common goal of these methods in general is to find a representation of the data in terms of new variables, which are a linear or nonlinear transformation of the original variables, and in which important statistical relationships emerge. The resulting variable sets are almost always smaller than the original variable set, and hence these methods can be loosely said to impose a granulation on the feature space. These dimensionality reduction methods are all reviewed in the standard texts, such as , , and .\n\nA different class of variable granulation methods derive more from data clustering methodologies than from the linear systems theory informing the above methods. It was noted fairly early that one may consider \"clustering\" related variables in just the same way that one considers clustering related data. In data clustering, one identifies a group of similar entities (using a \"measure of similarity\" suitable to the domain — ), and then in some sense \"replaces\" those entities with a prototype of some kind. The prototype may be the simple average of the data in the identified cluster, or some other representative measure. But the key idea is that in subsequent operations, we may be able to use the single prototype for the data cluster (along with perhaps a statistical model describing how exemplars are derived from the prototype) to \"stand in\" for the much larger set of exemplars. These prototypes are generally such as to capture most of the information of interest concerning the entities.\nSimilarly, it is reasonable to ask whether a large set of variables might be aggregated into a smaller set of \"prototype\" variables that capture the most salient relationships between the variables. Although variable clustering methods based on linear correlation have been proposed (;), more powerful methods of variable clustering are based on the mutual information between variables. Watanabe has shown (;) that for any set of variables one can construct a \"polytomic\" (i.e., n-ary) tree representing a series of variable agglomerations in which the ultimate \"total\" correlation among the complete variable set is the sum of the \"partial\" correlations exhibited by each agglomerating subset (see figure). Watanabe suggests that an observer might seek to thus partition a system in such a way as to minimize the interdependence between the parts \"... as if they were looking for a natural division or a hidden crack.\"\n\nOne practical approach to building such a tree is to successively choose for agglomeration the two variables (either atomic variables or previously agglomerated variables) which have the highest pairwise mutual information . The product of each agglomeration is a new (constructed) variable that reflects the local joint distribution of the two agglomerating variables, and thus possesses an entropy equal to their joint entropy.\n(From a procedural standpoint, this agglomeration step involves replacing two columns in the attribute-value table—representing the two agglomerating variables—with a single column that has a unique value for every unique combination of values in the replaced columns . No information is lost by such an operation; however, it should be noted that if one is exploring the data for inter-variable relationships, it would generally \"not\" be desirable to merge redundant variables in this way, since in such a context it is likely to be precisely the redundancy or \"dependency\" between variables that is of interest; and once redundant variables are merged, their relationship to one another can no longer be studied.\n\nIn database systems, aggregations (see e.g. OLAP aggregation and Business intelligence systems) result in transforming original data tables (often called information systems) into the tables with different semantics of rows and columns, wherein the rows correspond to the groups (granules) of original tuples and the columns express aggregated information about original values within each of the groups. Such aggregations are usually based on SQL and its extensions. The resulting granules usually correspond to the groups of original tuples with the same values (or ranges) over some pre-selected original columns.\n\nThere are also other approaches wherein the groups are defined basing on, e.g., physical adjacency of rows. For example, Infobright implemented a database engine wherein data was partitioned onto \"rough rows\", each consisting of 64K of physically consecutive (or almost consecutive) rows. Rough rows were automatically labeled with compact information about their values on data columns, often involving multi-column and multi-table relationships. It resulted in a higher layer of granulated information where objects corresponded to rough rows and attributes - to various aspects of rough information. Database operations could be efficiently supported within such a new framework, with an access to the original data pieces still available .\n\nThe origins of the \"granular computing\" ideology are to be found in the rough sets and fuzzy sets literatures. One of the key insights of rough set research—although by no means unique to it—is that, in general, the selection of different sets of features or variables will yield different \"concept\" granulations. Here, as in elementary rough set theory, by \"concept\" we mean a set of entities that are \"indistinguishable\" or \"indiscernible\" to the observer (i.e., a simple concept), or a set of entities that is composed from such simple concepts (i.e., a complex concept). To put it in other words, by projecting a data set (value-attribute system) onto different sets of variables, we recognize alternative sets of equivalence-class \"concepts\" in the data, and these different sets of concepts will in general be conducive to the extraction of different relationships and regularities.\n\nWe illustrate with an example. Consider the attribute-value system below:\n\nWhen the full set of attributes formula_25 is considered, we see that we have the following seven equivalence classes or primitive (simple) concepts:\n\nThus, the two objects within the first equivalence class, formula_27, cannot be distinguished from one another based on the available attributes, and the three objects within the second equivalence class, formula_28, cannot be distinguished from one another based on the available attributes. The remaining five objects are each discernible from all other objects. Now, let us imagine a projection of the attribute value system onto attribute formula_29 alone, which would represent, for example, the view from an observer which is only capable of detecting this single attribute. Then we obtain the following much coarser equivalence class structure.\n\nThis is in a certain regard the same structure as before, but at a lower degree of resolution (larger grain size). Just as in the case of value granulation (discretization/quantization), it is possible that relationships (dependencies) may emerge at one level of granularity that are not present at another. As an example of this, we can consider the effect of concept granulation on the measure known as \"attribute dependency\" (a simpler relative of the mutual information).\n\nTo establish this notion of dependency (see also rough sets), let formula_31 represent a particular concept granulation, where each formula_32 is an equivalence class from the concept structure induced by attribute set formula_33. For example, if the attribute set formula_33 consists of attribute formula_29 alone, as above, then the concept structure formula_36 will be composed of formula_37, formula_38, and formula_39. The dependency of attribute set formula_33 on another attribute set formula_41, formula_42, is given by\n\nThat is, for each equivalence class formula_32 in formula_36, we add up the size of its \"lower approximation\" (see rough sets) by the attributes in formula_41, i.e., formula_47. More simply, this approximation is the number of objects which on attribute set formula_41 can be positively identified as belonging to target set formula_32. Added across all equivalence classes in formula_36, the numerator above represents the total number of objects which—based on attribute set formula_41—can be positively categorized according to the classification induced by attributes formula_33. The dependency ratio therefore expresses the proportion (within the entire universe) of such classifiable objects, in a sense capturing the \"synchronization\" of the two concept structures formula_36 and formula_54. The dependency formula_42 \"can be interpreted as a proportion of such objects in the information system for which it suffices to know the values of attributes in formula_41 to determine the values of attributes in formula_33\" (Ziarko & Shan 1995).\n\nHaving gotten definitions now out of the way, we can make the simple observation that the choice of concept granularity (i.e., choice of attributes) will influence the detected dependencies among attributes. Consider again the attribute value table from above:\n\nLet us consider the dependency of attribute set formula_58\non attribute set formula_59. That is, we wish to know what proportion of objects can be correctly classified into classes of formula_36 based on knowledge of formula_54. The equivalence classes of formula_36 and of formula_54 are shown below.\n\nThe objects that can be \"definitively\" categorized according to concept structure formula_36 based on formula_54 are those in the set formula_66, and since there are six of these, the dependency of formula_33 on formula_41, formula_69. This might be considered an interesting dependency in its own right, but perhaps in a particular data mining application only stronger dependencies are desired.\n\nWe might then consider the dependency of the smaller attribute set formula_70\non the attribute set formula_59. The move from formula_58 to formula_70 induces a coarsening of the class structure formula_36, as will be seen shortly. We wish again to know what proportion of objects can be correctly classified into the (now larger) classes of formula_36 based on knowledge of formula_54. The equivalence classes of the new formula_36 and of formula_54 are shown below.\n\nClearly, formula_36 has a coarser granularity than it did earlier. The objects that can now be \"definitively\" categorized according to the concept structure formula_36 based on formula_54 constitute the complete universe formula_82, and thus the dependency of formula_33 on formula_41, formula_85. That is, knowledge of membership according to category set formula_54 is adequate to determine category membership in formula_36 with complete certainty; In this case we might say that formula_88. Thus, by coarsening the concept structure, we were able to find a stronger (deterministic) dependency. However, we also note that the classes induced in formula_36 from the reduction in resolution necessary to obtain this deterministic dependency are now themselves large and few in number; as a result, the dependency we found, while strong, may be less valuable to us than the weaker dependency found earlier under the higher resolution view of formula_36.\n\nIn general it is not possible to test all sets of attributes to see which induced concept structures yield the strongest dependencies, and this search must be therefore be guided with some intelligence. Papers which discuss this issue, and others relating to intelligent use of granulation, are those by Y.Y. Yao and Lotfi Zadeh listed in the #References below.\n\nAnother perspective on concept granulation may be obtained from work on parametric models of categories. In mixture model learning, for example, a set of data is explained as a mixture of distinct Gaussian (or other) distributions. Thus, a large amount of data is \"replaced\" by a small number of distributions. The choice of the number of these distributions, and their size, can again be viewed as a problem of \"concept granulation\". In general, a better fit to the data is obtained by a larger number of distributions or parameters, but in order to extract meaningful patterns, it is necessary to constrain the number of distributions, thus deliberately \"coarsening\" the concept resolution. Finding the \"right\" concept resolution is a tricky problem for which many methods have been proposed (e.g., AIC, BIC, MDL, etc.), and these are frequently considered under the rubric of \"model regularization\".\n\nGranular computing can be conceived as a framework of theories, methodologies, techniques, and tools that make use of information granules in the process of problem solving. In this sense, granular computing is used as an umbrella term to cover topics that have been studied in various fields in isolation. By examining all of these existing studies in light of the unified framework of granular computing and extracting their commonalities, it may be possible to develop a general theory for problem solving.\n\nIn a more philosophical sense, granular computing can describe a way of thinking that relies on the human ability to perceive the real world under various levels of granularity (i.e., abstraction) in order to abstract and consider only those things that serve a specific interest and to switch among different granularities. By focusing on different levels of granularity, one can obtain different levels of knowledge, as well as a greater understanding of the inherent knowledge structure. Granular computing is thus essential in human problem solving and hence has a very significant impact on the design and implementation of intelligent systems.\n\n\n"}
{"id": "8149170", "url": "https://en.wikipedia.org/wiki?curid=8149170", "title": "Graph embedding", "text": "Graph embedding\n\nIn topological graph theory, an embedding (also spelled imbedding) of a graph formula_1 on a surface formula_2 is a representation of formula_1 on formula_2 in which points of formula_2 are associated with vertices and simple arcs (homeomorphic images of formula_6) are associated with edges in such a way that:\nHere a surface is a compact, connected formula_9-manifold.\n\nInformally, an embedding of a graph into a surface is a drawing of the graph on the surface in such a way that its edges may intersect only at their endpoints. It is well known that any finite graph can be embedded in 3-dimensional Euclidean space formula_10 and planar graphs can be embedded in 2-dimensional Euclidean space formula_11\n\nOften, an embedding is regarded as an equivalence class (under homeomorphisms of formula_2) of representations of the kind just described.\n\nSome authors define a weaker version of the definition of \"graph embedding\" by omitting the non-intersection condition for edges. In such contexts the stricter definition is described as \"non-crossing graph embedding\".\n\nThis article deals only with the strict definition of graph embedding. The weaker definition is discussed in the articles \"graph drawing\" and \"crossing number\".\n\nIf a graph formula_1 is embedded on a closed surface formula_2, the complement of the union of the points and arcs associated with\nthe vertices and edges of formula_1 is a family of regions (or faces). A 2-cell embedding, cellular embedding or map is an embedding in which every face is homeomorphic to an open disk. A closed 2-cell embedding is an embedding in which the closure of every face is homeomorphic to a closed disk.\n\nThe genus of a graph is the minimal integer formula_16 such that the graph can be embedded in a surface of genus formula_16. In particular, a planar graph has genus formula_18, because it can be drawn on a sphere without self-crossing. The non-orientable genus of a graph is the minimal integer formula_16 such that the graph can be embedded in a non-orientable surface of (non-orientable) genus formula_16.\n\nThe Euler genus of a graph is the minimal integer formula_16 such that the graph can be embedded in an orientable surface of (orientable) genus formula_22 or in a non-orientable surface of (non-orientable) genus formula_16. A graph is orientably simple if its Euler genus is smaller than its non-orientable genus.\n\nThe maximum genus of a graph is the maximal integer formula_16 such that the graph can be formula_9-cell embedded in an orientable surface of genus formula_16.\n\nAn embedded graph uniquely defines cyclic orders of edges incident to the same vertex. The set of all these cyclic orders is called a rotation system. Embeddings with the same rotation system are considered to be equivalent and the corresponding equivalence class of embeddings is called combinatorial embedding (as opposed to the term topological embedding, which refers to the previous definition in terms of points and curves). Sometimes, the rotation system itself is called a \"combinatorial embedding\".\n\nAn embedded graph also defines natural cyclic orders of edges which constitutes the boundaries of the faces of the embedding. However handling these face-based orders is less straightforward, since in some cases some edges may be traversed twice along a face boundary. For example this is always the case for embeddings of trees, which have a single face. To overcome this combinatorial nuisance, one may consider that every edge is \"split\" lengthwise in two \"half-edges\", or \"sides\". Under this convention in all face boundary traversals each half-edge is traversed only once and the two half-edges of the same edge are always traversed in opposite directions.\n\nOther equivalent representations for cellular embeddings include the ribbon graph, a topological space formed by gluing together topological disks for the vertices and edges of an embedded graph, and the graph-encoded map, an edge-colored cubic graph with four vertices for each edge of the embedded graph.\n\nThe problem of finding the graph genus is NP-hard (the problem of determining whether an formula_16-vertex graph has genus formula_28 is NP-complete).\n\nAt the same time, the graph genus problem is fixed-parameter tractable, i.e., polynomial time algorithms are known to check whether a graph can be embedded into a surface of a given fixed genus as well as to find the embedding.\n\nThe first breakthrough in this respect happened in 1979, when algorithms of time complexity\n\"O\"(\"n\") were independently submitted to the Annual ACM Symposium on Theory of Computing: one by I. Filotti and G.L. Miller and another one by John Reif. Their approaches were quite different, but upon the suggestion of the program committee they presented a joint paper. However, Myrvold and Kocay proved in 2011 that the algorithm given by Filotti, Miller and Reif was incorrect.\n\nIn 1999 it was reported that the fixed-genus case can be solved in time linear in the graph size and doubly exponential in the genus.\n\nIt is known that any finite graph can be embedded into a three-dimensional space.\n\nOne method for doing this is to place the points on any line in space and to draw the edges as curves each of which lies in a distinct halfplane, with all halfplanes having that line as their common boundary. An embedding like this in which the edges are drawn on halfplanes is called a book embedding of the graph. This metaphor comes from imagining that each of the planes where an edge is drawn is like a page of a book. It was observed that in fact several edges may be drawn in the same \"page\"; the \"book thickness\" of the graph is the minimum number of halfplanes needed for such a drawing.\n\nAlternatively, any finite graph can be drawn with straight-line edges in three dimensions without crossings by placing its vertices in general position so that no four are coplanar. For instance, this may be achieved by placing the \"i\"th vertex at the point (\"i\",\"i\",\"i\") of the moment curve.\n\nAn embedding of a graph into three-dimensional space in which no two of the cycles are topologically linked is called a linkless embedding. A graph has a linkless embedding if and only if it does not have one of the seven graphs of the Petersen family as a minor.\n\n"}
{"id": "543082", "url": "https://en.wikipedia.org/wiki?curid=543082", "title": "Heinz Hopf", "text": "Heinz Hopf\n\nHeinz Hopf (19 November 1894 – 3 June 1971) was a German mathematician who worked on the fields of topology and geometry.\n\nHopf was born in Gräbschen, Germany (now , part of Wrocław, Poland), the son of Elizabeth (née Kirchner) and Wilhelm Hopf. His father was born Jewish and converted to Protestantism a year after Heinz was born; his mother was from a Protestant family.\n\nHopf attended Dr. Karl Mittelhaus' higher boys' school from 1901 to 1904, and then entered the König-Wilhelm-Gymnasium in Breslau. He showed mathematical talent from an early age. In 1913 he entered the Silesian Friedrich Wilhelm University where he attended lectures by Ernst Steinitz, Adolf Kneser, Max Dehn, Erhard Schmidt, and Rudolf Sturm. When World War I broke out in 1914, Hopf eagerly enlisted. He was wounded twice and received the iron cross (first class) in 1918.\n\nAfter the war Hopf continued his mathematical education in Heidelberg (winter 1919/20 and summer 1920) and Berlin (since winter 1920/21). He studied under Ludwig Bieberbach and receiving his doctorate in 1925.\n\nIn his dissertation, \"Connections between topology and metric of manifolds\" (German \"Über Zusammenhänge zwischen Topologie und Metrik von Mannigfaltigkeiten\"), he proved that any simply connected complete Riemannian 3-manifold of constant sectional curvature is globally isometric to Euclidean, spherical, or hyperbolic space. He also studied the indices of zeros of vector fields on hypersurfaces, and connected their sum to curvature. Some six months later he gave a new proof that the sum of the indices of the zeros of a vector field on a manifold is independent of the choice of vector field and equal to the Euler characteristic of the manifold. This theorem is now called the Poincaré–Hopf theorem.\n\nHopf spent the year after his doctorate at the University of Göttingen, where David Hilbert, Richard Courant, Carl Runge, and Emmy Noether were working. While there he met Paul Alexandrov and began a lifelong friendship.\n\nIn 1926 Hopf moved back to Berlin, where he gave a course in combinatorial topology. He spent the academic year 1927/28 at Princeton University on a Rockefeller fellowship with Alexandrov. Solomon Lefschetz, Oswald Veblen and J. W. Alexander were all at Princeton at the time. At this time Hopf discovered the Hopf invariant of maps formula_1 and proved that the Hopf fibration has invariant 1. In the summer of 1928 Hopf returned to Berlin and began working with Alexandrov, at the suggestion of Courant, on a book on topology. Three volumes were planned, but only one was finished. It was published in 1935.\n\nIn 1929, he declined a job offer from Princeton University. In 1931 Hopf took Hermann Weyl's position at ETH, in Zürich. Hopf received another invitation to Princeton in 1940, but he declined it. Two years later, however, he was forced to file for Swiss citizenship after his property was confiscated by Nazis, his father's conversion to Christianity having failed to convince German authorities that he was an \"Aryan.\"\n\nIn 1946/47 and 1955/56 Hopf visited the United States, staying at Princeton and giving lectures at New York University and Stanford University. He served as president of the International Mathematical Union from 1955 to 1958.\n\nIn October 1928 Hopf married Anja von Mickwitz (1891–1967).\n\nHe received honorary doctorates from Princeton University, the University of Freiburg, the University of Manchester, the University of Paris, the Free University of Brussels, and the University of Lausanne. In 1949 he was elected a corresponding member of the Heidelberg Academy of Sciences. He was an Invited Speaker at the International Congress of Mathematicians (ICM) in Zürich in 1932 and a Plenary Speaker at the ICM in Cambridge, Massachusetts in 1950.\n\nIn memory of Hopf, ETH Zürich awards the Heinz Hopf Prize for outstanding scientific work in the field of pure mathematics.\n\n\n\n\n"}
{"id": "309343", "url": "https://en.wikipedia.org/wiki?curid=309343", "title": "Heyting algebra", "text": "Heyting algebra\n\nIn mathematics, a Heyting algebra is a bounded lattice (with join and meet operations written ∨ and ∧ and with least element 0 and greatest element 1) equipped with a binary operation \"a\" → \"b\" of \"implication\" such that \"c\" ∧ \"a\" ≤ \"b\" is equivalent to \"c\" ≤ \"a\" → \"b\". From a logical standpoint, \"A\" → \"B\" is by this definition the weakest proposition for which modus ponens, the inference rule \"A\" → \"B\", \"A\" ⊢ \"B\", is sound. Equivalently a Heyting algebra is a residuated lattice whose monoid operation is ∧; yet another definition is as a posetal cartesian closed category with all finite sums. Like Boolean algebras, Heyting algebras form a variety axiomatizable with finitely many equations. Heyting algebras were introduced by to formalize intuitionistic logic.\n\nAs lattices, Heyting algebras are distributive. Every Boolean algebra is a Heyting algebra when \"a\" → \"b\" is defined as usual as ¬\"a\" ∨ \"b\", as is every complete distributive lattice satisfying a one-sided infinite distributive law when \"a\" → \"b\" is taken to be the supremum of the set of all \"c\" for which \"a\" ∧ \"c\" ≤ \"b\". The open sets of a topological space form such a lattice, and therefore a (complete) Heyting algebra. In the finite case every nonempty distributive lattice, in particular every nonempty finite chain, is automatically complete and completely distributive, and hence a Heyting algebra.\n\nIt follows from the definition that 1 ≤ 0 → \"a\", corresponding to the intuition that any proposition \"a\" is implied by a contradiction 0. Although the negation operation ¬\"a\" is not part of the definition, it is definable as \"a\" → 0. The definition implies that \"a\" ∧ ¬\"a\" = 0, making the intuitive content of ¬\"a\" the proposition that to assume \"a\" would lead to a contradiction, from which any other proposition would then follow. It can further be shown that \"a\" ≤ ¬¬\"a\", although the converse, ¬¬\"a\" ≤ \"a\", is not true in general, that is, double negation elimination does not hold in general in a Heyting algebra.\n\nHeyting algebras generalize Boolean algebras in the sense that a Heyting algebra satisfying \"a\" ∨ ¬\"a\" = 1 (excluded middle), equivalently ¬¬\"a\" = \"a\" (double negation elimination), is a Boolean algebra. Those elements of a Heyting algebra of the form ¬\"a\" comprise a Boolean lattice, but in general this is not a subalgebra of H (see below).\n\nHeyting algebras serve as the algebraic models of propositional intuitionistic logic in the same way Boolean algebras model propositional classical logic. Complete Heyting algebras are a central object of study in pointless topology. The internal logic of an elementary topos is based on the Heyting algebra of subobjects of the terminal object 1 ordered by inclusion, equivalently the morphisms from 1 to the subobject classifier Ω.\n\nEvery Heyting algebra whose set of non-greatest elements has a greatest element (and forms another Heyting algebra) is subdirectly irreducible, whence every Heyting algebra can be made an SI by adjoining a new greatest element. It follows that even among the finite Heyting algebras there exist infinitely many that are subdirectly irreducible, no two of which have the same equational theory. Hence no finite set of finite Heyting algebras can supply all the counterexamples to non-laws of Heyting algebra. This is in sharp contrast to Boolean algebras, whose only SI is the two-element one, which on its own therefore suffices for all counterexamples to non-laws of Boolean algebra, the basis for the simple truth table decision method. Nevertheless, it is decidable whether an equation holds of all Heyting algebras.\n\nHeyting algebras are less often called pseudo-Boolean algebras, or even Brouwer lattices, although the latter term may denote the dual definition, or have a slightly more general meaning.\n\nA Heyting algebra \"H\" is a bounded lattice such that for all \"a\" and \"b\" in \"H\" there is a greatest element \"x\" of \"H\" such that\n\nThis element is the relative pseudo-complement of \"a\" with respect to \"b\", and is denoted \"a\"→\"b\". We write 1 and 0 for the largest and the smallest element of \"H\", respectively.\n\nIn any Heyting algebra, one defines the pseudo-complement ¬\"a\" of any element \"a\" by setting ¬\"a\" = (\"a\"→0). By definition, formula_2, and ¬\"a\" is the largest element having this property. However, it is not in general true that formula_3, thus ¬ is only a pseudo-complement, not a true complement, as would be the case in a Boolean algebra.\n\nA complete Heyting algebra is a Heyting algebra that is a complete lattice.\n\nA subalgebra of a Heyting algebra \"H\" is a subset \"H\" of \"H\" containing 0 and 1 and closed under the operations ∧, ∨ and →. It follows that it is also closed under ¬. A subalgebra is made into a Heyting algebra by the induced operations.\n\nA Heyting algebra formula_4 is a bounded lattice that has all exponential objects.\n\nThe lattice formula_4 is regarded as a category where \nmeet, formula_6, is the product. The exponential condition means that for any objects formula_7 and formula_8 in formula_4 an exponential formula_10 uniquely exists as an object in formula_4.\n\nA Heyting implication (often written using formula_12 or formula_13 to avoid confusions such as the use of formula_14 to indicate a functor) is just an exponential: formula_15 is an alternative notation for formula_10. From the definition of exponentials we have that implication (formula_17) is right adjoint to meet (formula_18). This adjunction can be written as formula_19 or more fully as:\nformula_20\n\nAn equivalent definition of Heyting algebras can be given by considering the mappings:\n\nfor some fixed \"a\" in \"H\". A bounded lattice \"H\" is a Heyting algebra if and only if every mapping \"f\" is the lower adjoint of a monotone Galois connection. In this case the respective upper adjoint \"g\" is given by \"g\"(\"x\") = \"a\"→\"x\", where → is defined as above.\n\nYet another definition is as a residuated lattice whose monoid operation is ∧. The monoid unit must then be the top element 1. Commutativity of this monoid implies that the two residuals coincide as \"a\"→\"b\".\n\nGiven a bounded lattice \"A\" with largest and smallest elements 1 and 0, and a binary operation →, these together form a Heyting algebra if and only if the following hold:\nwhere 4 is the distributive law for →.\n\nThis characterization of Heyting algebras makes the proof of the basic facts concerning the relationship between intuitionist propositional calculus and Heyting algebras immediate. (For these facts, see the sections \"Provable identities\" and \"Universal constructions\".) One should think of the element formula_26 as meaning, intuitively, \"provably true.\" Compare with the axioms at Intuitionistic logic#Axiomatization ).\n\nGiven a set \"A\" with three binary operations →, ∧ and ∨, and two distinguished elements formula_27 and formula_26, then \"A\" is a Heyting algebra for these operations (and the relation ≤ defined by the condition that formula_29 when \"a\"→\"b\" = formula_26) if and only if the following conditions hold for any elements \"x\", \"y\" and \"z\" of \"A\":\n\nFinally, we define ¬\"x\" to be \"x\"→ formula_27.\n\nCondition 1 says that equivalent formulas should be identified. Condition 2 says that provably true formulas are closed under modus ponens. Conditions 3 and 4 are \"then\" conditions. Conditions 5, 6 and 7 are \"and\" conditions. Conditions 8, 9 and 10 are \"or\" conditions. Condition 11 is a \"false\" condition.\n\nOf course, if a different set of axioms were chosen for logic, we could modify ours accordingly.\n\nThe ordering formula_43 on a Heyting algebra \"H\" can be recovered from the operation → as follows: for any elements \"a\", \"b\" of \"H\", formula_29 if and only if \"a\"→\"b\" = 1.\n\nIn contrast to some many-valued logics, Heyting algebras share the following property with Boolean algebras: if negation has a fixed point (i.e. ¬\"a\" = \"a\" for some \"a\"), then the Heyting algebra is the trivial one-element Heyting algebra.\n\nGiven a formula formula_45 of propositional calculus (using, in addition to the variables, the connectives formula_46, and the constants 0 and 1), it is a fact, proved early on in any study of Heyting algebras, that the following two conditions are equivalent:\nThe metaimplication is extremely useful and is the principal practical method for proving identities in Heyting algebras. In practice, one frequently uses the deduction theorem in such proofs.\n\nSince for any \"a\" and \"b\" in a Heyting algebra \"H\" we have formula_29 if and only if \"a\"→\"b\" = 1, it follows from that whenever a formula \"F\"→\"G\" is provably true, we have formula_50 for any Heyting algebra \"H\", and any elements formula_48. (It follows from the deduction theorem that \"F\"→\"G\" is provable [from nothing] if and only if \"G\" is provable from \"F\", that is, if \"G\" is a provable consequence of \"F\".) In particular, if \"F\" and \"G\" are provably equivalent, then formula_52, since ≤ is an order relation.\n\n1 ⇒ 2 can be proved by examining the logical axioms of the system of proof and verifying that their value is 1 in any Heyting algebra, and then verifying that the application of the rules of inference to expressions with value 1 in a Heyting algebra results in expressions with value 1. For example, let us choose the system of proof having modus ponens as its sole rule of inference, and whose axioms are the Hilbert-style ones given at Intuitionistic logic#Axiomatization. Then the facts to be verified follow immediately from the axiom-like definition of Heyting algebras given above.\n\n1 ⇒ 2 also provides a method for proving that certain propositional formulas, though tautologies in classical logic, \"cannot\" be proved in intuitionist propositional logic. In order to prove that some formula formula_45 is not provable, it is enough to exhibit a Heyting algebra \"H\" and elements formula_48 such that formula_55.\n\nIf one wishes to avoid mention of logic, then in practice it becomes necessary to prove as a lemma a version of the deduction theorem valid for Heyting algebras: for any elements \"a\", \"b\" and \"c\" of a Heyting algebra \"H\", we have formula_56.\n\nFor more on the metaimplication 2 ⇒ 1, see the section \"Universal constructions\" below.\n\nHeyting algebras are always distributive. Specifically, we always have the identities\n\nThe distributive law is sometimes stated as an axiom, but in fact it follows from the existence of relative pseudo-complements. The reason is that, being the lower adjoint of a Galois connection, formula_6 preserves all existing suprema. Distributivity in turn is just the preservation of binary suprema by formula_6.\n\nBy a similar argument, the following infinite distributive law holds in any complete Heyting algebra:\n\nfor any element \"x\" in \"H\" and any subset \"Y\" of \"H\". Conversely, any complete lattice satisfying the above infinite distributive law is a complete Heyting algebra, with\nbeing its relative pseudo-complement operation.\n\nAn element \"x\" of a Heyting algebra \"H\" is called regular if either of the following equivalent conditions hold:\nThe equivalence of these conditions can be restated simply as the identity ¬¬¬\"x\" = ¬\"x\", valid for all \"x\" in \"H\".\n\nElements \"x\" and \"y\" of a Heyting algebra \"H\" are called complements to each other if \"x\"∧\"y\" = 0 and \"x\"∨\"y\" = 1. If it exists, any such \"y\" is unique and must in fact be equal to ¬\"x\". We call an element \"x\" complemented if it admits a complement. It is true that \"if\" \"x\" is complemented, then so is ¬\"x\", and then \"x\" and ¬\"x\" are complements to each other. However, confusingly, even if \"x\" is not complemented, ¬\"x\" may nonetheless have a complement (not equal to \"x\"). In any Heyting algebra, the elements 0 and 1 are complements to each other. For instance, it is possible that ¬\"x\" is 0 for every \"x\" different from 0, and 1 if \"x\" = 0, in which case 0 and 1 are the only regular elements.\n\nAny complemented element of a Heyting algebra is regular, though the converse is not true in general. In particular, 0 and 1 are always regular.\n\nFor any Heyting algebra \"H\", the following conditions are equivalent:\nIn this case, the element is equal to \n\nThe regular (resp. complemented) elements of any Heyting algebra \"H\" constitute a Boolean algebra \"H\" (resp. \"H\"), in which the operations ∧, ¬ and →, as well as the constants 0 and 1, coincide with those of \"H\". In the case of \"H\", the operation ∨ is also the same, hence \"H\" is a subalgebra of \"H\". In general however, \"H\" will not be a subalgebra of \"H\", because its join operation ∨ may be differ from ∨. For we have See below for necessary and sufficient conditions in order for ∨ to coincide with ∨.\n\nOne of the two De Morgan laws is satisfied in every Heyting algebra, namely\n\nHowever, the other De Morgan law does not always hold. We have instead a weak de Morgan law:\n\nThe following statements are equivalent for all Heyting algebras \"H\":\nCondition 2 is the other De Morgan law. Condition 6 says that the join operation ∨ on the Boolean algebra \"H\" of regular elements of \"H\" coincides with the operation ∨ of \"H\". Condition 7 states that every regular element is complemented, i.e., \"H\" = \"H\".\n\nWe prove the equivalence. Clearly the metaimplications and are trivial. Furthermore, and result simply from the first De Morgan law and the definition of regular elements. We show that by taking ¬\"x\" and ¬¬\"x\" in place of \"x\" and \"y\" in 6 and using the identity Notice that follows from the first De Morgan law, and results from the fact that the join operation ∨ on the subalgebra \"H\" is just the restriction of ∨ to \"H\", taking into account the characterizations we have given of conditions 6 and 7. The metaimplication is a trivial consequence of the weak De Morgan law, taking ¬\"x\" and ¬\"y\" in place of \"x\" and \"y\" in 5.\n\nHeyting algebras satisfying the above properties are related to De Morgan logic in the same way Heyting algebras in general are related to intuitionist logic.\n\nGiven two Heyting algebras \"H\" and \"H\" and a mapping we say that \"ƒ\" is a morphism of Heyting algebras if, for any elements \"x\" and \"y\" in \"H\", we have:\n\nIt follows from condition 4 (or 2 alone, or 3 alone) that \"f\" is an increasing function, that is, that whenever .\n\nAssume \"H\" and \"H\" are structures with operations →, ∧, ∨ (and possibly ¬) and constants 0 and 1, and \"f\" is a surjective mapping from \"H\" to \"H\" with properties 1 through 5 (or 1 through 6) above. Then if \"H\" is a Heyting algebra, so too is \"H\". This follows from the characterization of Heyting algebras as bounded lattices (thought of as algebraic structures rather than partially ordered sets) with an operation → satisfying certain identities.\n\nThe identity map from any Heyting algebra to itself is a morphism, and the composite of any two morphisms \"f\" and \"g\" is a morphism. Hence Heyting algebras form a category.\n\nGiven a Heyting algebra \"H\" and any subalgebra \"H\", the inclusion mapping is a morphism.\n\nFor any Heyting algebra \"H\", the map defines a morphism from \"H\" onto the Boolean algebra of its regular elements \"H\". This is \"not\" in general a morphism from \"H\" to itself, since the join operation of \"H\" may be different from that of \"H\".\n\nLet \"H\" be a Heyting algebra, and let We call \"F\" a filter on \"H\" if it satisfies the following properties:\n\nThe intersection of any set of filters on \"H\" is again a filter. Therefore, given any subset \"S\" of \"H\" there is a smallest filter containing \"S\". We call it the filter generated by \"S\". If \"S\" is empty, Otherwise, \"F\" is equal to the set of \"x\" in \"H\" such that there exist with \n\nIf \"H\" is a Heyting algebra and \"F\" is a filter on \"H\", we define a relation ∼ on \"H\" as follows: we write whenever and both belong to \"F\". Then ∼ is an equivalence relation; we write for the quotient set. There is a unique Heyting algebra structure on such that the canonical surjection becomes a Heyting algebra morphism. We call the Heyting algebra the quotient of \"H\" by \"F\".\n\nLet \"S\" be a subset of a Heyting algebra \"H\" and let \"F\" be the filter generated by \"S\". Then \"H\"/\"F\" satisfies the following universal property:\n\nLet be a morphism of Heyting algebras. The kernel of \"f\", written ker \"f\", is the set It is a filter on \"H\". (Care should be taken because this definition, if applied to a morphism of Boolean algebras, is dual to what would be called the kernel of the morphism viewed as a morphism of rings.) By the foregoing, \"f\" induces a morphism It is an isomorphism of onto the subalgebra \"f\"[\"H\"] of \"H\".\n\nThe metaimplication in the section \"Provable identities\" is proved by showing that the result of the following construction is itself a Heyting algebra:\n\nAs always under the axiom-like definition of Heyting algebras, we define ≤ on \"H\" by the condition that \"x\"≤\"y\" if and only if \"x\"→\"y\"=1. Since, by the deduction theorem, a formula \"F\"→\"G\" is provably true if and only if \"G\" is provable from \"F\", it follows that [\"F\"]≤[\"G\"] if and only if F≼G. In other words, ≤ is the order relation on \"L\"/∼ induced by the preorder ≼ on \"L\".\n\nIn fact, the preceding construction can be carried out for any set of variables {\"A\" : \"i\"∈\"I\"} (possibly infinite). One obtains in this way the \"free\" Heyting algebra on the variables {\"A\"}, which we will again denote by \"H\". It is free in the sense that given any Heyting algebra \"H\" given together with a family of its elements 〈\"a\": \"i\"∈\"I\" 〉, there is a unique morphism \"f\":\"H\"→\"H\" satisfying \"f\"([\"A\"])=\"a\". The uniqueness of \"f\" is not difficult to see, and its existence results essentially from the metaimplication of the section \"Provable identities\" above, in the form of its corollary that whenever \"F\" and \"G\" are provably equivalent formulas, \"F\"(〈\"a\"〉)=\"G\"(〈\"a\"〉) for any family of elements 〈\"a\"〉in \"H\".\n\nGiven a set of formulas \"T\" in the variables {\"A\"}, viewed as axioms, the same construction could have been carried out with respect to a relation \"F\"≼\"G\" defined on \"L\" to mean that \"G\" is a provable consequence of \"F\" and the set of axioms \"T\". Let us denote by \"H\" the Heyting algebra so obtained. Then \"H\" satisfies the same universal property as \"H\" above, but with respect to Heyting algebras \"H\" and families of elements 〈\"a\"〉 satisfying the property that \"J\"(〈\"a\"〉)=1 for any axiom \"J\"(〈\"A\"〉) in \"T\". (Let us note that \"H\", taken with the family of its elements 〈[\"A\"]〉, itself satisfies this property.) The existence and uniqueness of the morphism is proved the same way as for \"H\", except that one must modify the metaimplication in \"Provable identities\" so that 1 reads \"provably true \"from T\",\" and 2 reads \"any elements \"a\", \"a\"..., \"a\" in \"H\" \"satisfying the formulas of T\".\"\n\nThe Heyting algebra \"H\" that we have just defined can be viewed as a quotient of the free Heyting algebra \"H\" on the same set of variables, by applying the universal property of \"H\" with respect to \"H\", and the family of its elements 〈[\"A\"]〉.\n\nEvery Heyting algebra is isomorphic to one of the form \"H\". To see this, let \"H\" be any Heyting algebra, and let 〈\"a\": \"i\"∈I〉 be a family of elements generating \"H\" (for example, any surjective family). Now consider the set \"T\" of formulas \"J\"(〈\"A\"〉) in the variables 〈\"A\": \"i\"∈I〉 such that \"J\"(〈\"a\"〉)=1. Then we obtain a morphism \"f\":\"H\"→\"H\" by the universal property of \"H\", which is clearly surjective. It is not difficult to show that \"f\" is injective.\n\nThe constructions we have just given play an entirely analogous role with respect to Heyting algebras to that of Lindenbaum algebras with respect to Boolean algebras. In fact, The Lindenbaum algebra \"B\" in the variables {\"A\"} with respect to the axioms \"T\" is just our \"H\", where \"T\" is the set of all formulas of the form ¬¬\"F\"→\"F\", since the additional axioms of \"T\" are the only ones that need to be added in order to make all classical tautologies provable.\n\nIf one interprets the axioms of the intuitionistic propositional logic as terms of a Heyting algebra, then they will evaluate to the largest element, 1, in \"any\" Heyting algebra under any assignment of values to the formula's variables. For instance, (\"P\"∧\"Q\")→\"P\" is, by definition of the pseudo-complement, the largest element \"x\" such that formula_78. This inequation is satisfied for any \"x\", so the largest such \"x\" is 1.\n\nFurthermore, the rule of modus ponens allows us to derive the formula \"Q\" from the formulas \"P\" and \"P\"→\"Q\". But in any Heyting algebra, if \"P\" has the value 1, and \"P\"→\"Q\" has the value 1, then it means that formula_79, and so formula_80; it can only be that \"Q\" has the value 1.\n\nThis means that if a formula is deducible from the laws of intuitionistic logic, being derived from its axioms by way of the rule of modus ponens, then it will always have the value 1 in all Heyting algebras under any assignment of values to the formula's variables. However one can construct a Heyting algebra in which the value of Peirce's law is not always 1. Consider the 3-element algebra {0,½,1} as given above. If we assign ½ to \"P\" and 0 to \"Q\", then the value of Peirce's law ((\"P\"→\"Q\")→\"P\")→\"P\" is ½. It follows that Peirce's law cannot be intuitionistically derived. See Curry–Howard isomorphism for the general context of what this implies in type theory.\n\nThe converse can be proven as well: if a formula always has the value 1, then it is deducible from the laws of intuitionistic logic, so the \"intuitionistically valid\" formulas are exactly those that always have a value of 1. This is similar to the notion that \"classically valid\" formulas are those formulas that have a value of 1 in the two-element Boolean algebra under any possible assignment of true and false to the formula's variables — that is, they are formulas which are tautologies in the usual truth-table sense. A Heyting algebra, from the logical standpoint, is then a generalization of the usual system of truth values, and its largest element 1 is analogous to 'true'. The usual two-valued logic system is a special case of a Heyting algebra, and the smallest non-trivial one, in which the only elements of the algebra are 1 (true) and 0 (false).\n\nThe problem of whether a given equation holds in every Heyting algebra was shown to be decidable by S. Kripke in 1965. The precise computational complexity of the problem was established by R. Statman in 1979, who showed it was PSPACE-complete and hence at least as hard as deciding equations of Boolean algebra (shown coNP-complete in 1971 by S. Cook) and conjectured to be considerably harder. The elementary or first-order theory of Heyting algebras is undecidable. It remains open whether the universal Horn theory of Heyting algebras, or word problem, is decidable. À propos of the word problem it is known that Heyting algebras are not locally finite (no Heyting algebra generated by a finite nonempty set is finite), in contrast to Boolean algebras which are locally finite and whose word problem is decidable. It is unknown whether free complete Heyting algebras exist except in the case of a single generator where the free Heyting algebra on one generator is trivially completable by adjoining a new top.\n\n\n\n"}
{"id": "27540773", "url": "https://en.wikipedia.org/wiki?curid=27540773", "title": "Holomorphic tangent space", "text": "Holomorphic tangent space\n\nIn mathematics, specifically in the field of complex geometry, the holomorphic tangent space of a complex manifold \"M\" is the tangent space of the smooth manifold Σ underlying \"M\" viewed as complex vector space via the almost complex structure \"J\" of \"M\". The holomorphic tangent bundle (\"T\"Σ, \"J\") is isomorphic to a subbundle of the complexification of the real tangent bundle, namely the eigenbundle of \"J\" for the eigenvalue +\"i\" (compare linear complex structure). The holomorphic tangent bundle is often denoted by \"T\"\"M\".\n\n"}
{"id": "5735510", "url": "https://en.wikipedia.org/wiki?curid=5735510", "title": "Hydrostatic stress", "text": "Hydrostatic stress\n\nIn continuum mechanics, a hydrostatic stress is an isotropic stress that is given by the weight of water above a certain point. It is often used interchangeably with \"pressure\" and is also known as confining stress, particularly in the field of geomechanics. Its magnitude formula_1 can be given by:\n\nwhere formula_3 is an index denoting each distinct layer of material above the point of interest, formula_4 is the density of each layer, formula_5 is the gravitational acceleration (assumed constant here; this can be substituted with any acceleration that is important in defining weight), and formula_6 is the height (or thickness) of each given layer of material. For example, the magnitude of the hydrostatic stress felt at a point under ten meters of fresh water would be\n\nwhere the index formula_8 indicates \"water\".\n\nBecause the hydrostatic stress is isotropic, it acts equally in all directions. In tensor form, the hydrostatic stress is equal to\n\nwhere formula_10 is the 3-by-3 identity matrix.\n"}
{"id": "990454", "url": "https://en.wikipedia.org/wiki?curid=990454", "title": "Interior algebra", "text": "Interior algebra\n\nIn abstract algebra, an interior algebra is a certain type of algebraic structure that encodes the idea of the topological interior of a set. Interior algebras are to topology and the modal logic S4 what Boolean algebras are to set theory and ordinary propositional logic. Interior algebras form a variety of modal algebras.\n\nAn interior algebra is an algebraic structure with the signature\n\nwhere\n\nis a Boolean algebra and postfix designates a unary operator, the interior operator, satisfying the identities:\n\n\n\"x\" is called the interior of \"x\".\n\nThe dual of the interior operator is the closure operator defined by \"x\" = ((\"x\"′))′. \"x\" is called the closure of \"x\". By the principle of duality, the closure operator satisfies the identities:\n\n\nIf the closure operator is taken as primitive, the interior operator can be defined as \"x\" = ((\"x\"′))′. Thus the theory of interior algebras may be formulated using the closure operator instead of the interior operator, in which case one considers closure algebras of the form ⟨\"S\", ·, +, ′, 0, 1, ⟩, where ⟨\"S\", ·, +, ′, 0, 1⟩ is again a Boolean algebra and satisfies the above identities for the closure operator. Closure and interior algebras form dual pairs, and are paradigmatic instances of \"Boolean algebras with operators.\" The early literature on this subject (mainly Polish topology) invoked closure operators, but the interior operator formulation eventually became the norm following the work of Wim Blok.\n\nElements of an interior algebra satisfying the condition \"x\" = \"x\" are called open. The complements of open elements are called closed and are characterized by the condition \"x\" = \"x\". An interior of an element is always open and the closure of an element is always closed. Interiors of closed elements are called regular open and closures of open elements are called regular closed. Elements which are both open and closed are called clopen. 0 and 1 are clopen.\n\nAn interior algebra is called Boolean if all its elements are open (and hence clopen). Boolean interior algebras can be identified with ordinary Boolean algebras as their interior and closure operators provide no meaningful additional structure. A special case is the class of trivial interior algebras which are the single element interior algebras characterized by the identity 0 = 1.\n\nInterior algebras, by virtue of being algebraic structures, have homomorphisms. Given two interior algebras \"A\" and \"B\", a map \"f\" : \"A\" → \"B\" is an interior algebra homomorphism if and only if \"f\" is a homomorphism between the underlying Boolean algebras of \"A\" and \"B\", that also preserves interiors and closures. Hence:\n\nTopomorphisms are another important, and more general, class of morphisms between interior algebras. A map \"f\" : \"A\" → \"B\" is a topomorphism if and only if \"f\" is a homomorphism between the Boolean algebras underlying \"A\" and \"B\", that also preserves the open and closed elements of \"A\". Hence:\n(Such morphisms have also been called \"stable homomorphisms\" and \"closure algebra semi-homomorphisms\".) Every interior algebra homomorphism is a topomorphism, but not every topomorphism is an interior algebra homomorphism.\n\nEarly research often considered mappings between interior algebras which were homomorphisms of the underlying Boolean algebras but which did not necessarily preserve the interior or closure operator. Such mappings were called Boolean homomorphisms. (The terms \"closure homomorphism\" or \"topological homomorphism\" were used in the case where these were preserved, but this terminology is now redundant as the standard definition of a homomorphism in universal algebra requires that it preserves all operations.) Applications involving countably complete interior algebras (in which countable meets and joins always exist, also called \"σ-complete\") typically made use of countably complete Boolean homomorphisms also called Boolean σ-homomorphisms - these preserve countable meets and joins.\n\nThe earliest generalization of continuity to interior algebras was Sikorksi's based on the inverse image map of a continuous map. This is a Boolean homomorphism, preserves unions of sequences and includes the closure of an inverse image in the inverse image of the closure. Sikorski thus defined a \"continuous homomorphism\" as a Boolean σ-homomorphism \"f\" between two σ-complete interior algebras such that \"f\"(\"x\") ≤ \"f\"(\"x\"). This definition had several difficulties: The construction acts contravariantly producing a dual of a continuous map rather than a generalization. On the one hand σ-completeness is too weak to characterize inverse image maps (completeness is required), on the other hand it is too restrictive for a generalization. (Sikorski remarked on using non-σ-complete homomorphisms but included σ-completeness in his axioms for \"closure algebras\".) Later J. Schmid defined a continuous homomorphism or continuous morphism for interior algebras as a Boolean homomorphism \"f\" between two interior algebras satisfying \"f\"(\"x\") ≤ \"f\"(\"x\"). This generalizes the forward image map of a continuous map - the image of a closure is contained in the closure of the image. This construction is covariant but not suitable for category theoretic applications as it only allows construction of continuous morphisms from continuous maps in the case of bijections. (C. Naturman returned to Sikorski's approach while dropping σ-completeness to produce topomorphisms as defined above. In this terminology, Sikorski's original \"continuous homomorphisms\" are σ-complete topomorphisms between σ-complete interior algebras.)\n\nGiven a topological space X = ⟨\"X\", \"T\"⟩ one can form the power set Boolean algebra of \"X\":\n\nand extend it to an interior algebra\n\nwhere is the usual topological interior operator. For all \"S\" ⊆ \"X\" it is defined by\n\nFor all \"S\" ⊆ \"X\" the corresponding closure operator is given by\n\n\"S\" is the largest open subset of \"S\" and \"S\" is the smallest closed superset of \"S\" in X. The open, closed, regular open, regular closed and clopen elements of the interior algebra A(X) are just the open, closed, regular open, regular closed and clopen subsets of X respectively in the usual topological sense.\n\nEvery complete atomic interior algebra is isomorphic to an interior algebra of the form A(X) for some topological space X. Moreover, every interior algebra can be embedded in such an interior algebra giving a representation of an interior algebra as a topological field of sets. The properties of the structure A(X) are the very motivation for the definition of interior algebras. Because of this intimate connection with topology, interior algebras have also been called topo-Boolean algebras or topological Boolean algebras.\n\nGiven a continuous map between two topological spaces\n\nwe can define a complete topomorphism\n\nby\n\nfor all subsets \"S\" of Y. Every complete topomorphism between two complete atomic interior algebras can be derived in this way. If Top is the category of topological spaces and continuous maps and Cit is the category of complete atomic interior algebras and complete topomorphisms then Top and Cit are dually isomorphic and A : Top → Cit is a contravariant functor that is a dual isomorphism of categories. A(\"f\") is a homomorphism if and only if \"f\" is a continuous open map.\n\nUnder this dual isomorphism of categories many natural topological properties correspond to algebraic properties, in particular connectedness properties correspond to irreducibility properties:\n\n\nThe modern formulation of topological spaces in terms of topologies of open subsets, motivates an alternative formulation of interior algebras: A generalized topological space is an algebraic structure of the form\n\nwhere ⟨\"B\", ·, +, ′, 0, 1⟩ is a Boolean algebra as usual, and \"T\" is a unary relation on \"B\" (subset of \"B\") such that:\n\n\n\"T\" is said to be a generalized topology in the Boolean algebra.\n\nGiven an interior algebra its open elements form a generalized topology. Conversely given a generalized topological space\n\nwe can define an interior operator on \"B\" by \"b\" = ∑{\"a\" ∈\"T\" : \"a\" ≤ \"b\"} thereby producing an interior algebra whose open elements are precisely \"T\". Thus generalized topological spaces are equivalent to interior algebras.\n\nConsidering interior algebras to be generalized topological spaces, topomorphisms are then the standard homomorphisms of Boolean algebras with added relations, so that standard results from universal algebra apply.\n\nThe topological concept of neighbourhoods can be generalized to interior algebras: An element \"y\" of an interior algebra is said to be a neighbourhood of an element \"x\" if \"x\" ≤ \"y\". The set of neighbourhoods of \"x\" is denoted by \"N\"(\"x\") and forms a filter. This leads to another formulation of interior algebras:\n\nA neighbourhood function on a Boolean algebra is a mapping \"N\" from its underlying set \"B\" to its set of filters, such that:\n\n\nThe mapping \"N\" of elements of an interior algebra to their filters of neighbourhoods is a neighbourhood function on the underlying Boolean algebra of the interior algebra. Moreover, given a neighbourhood function \"N\" on a Boolean algebra with underlying set \"B\", we can define an interior operator by \"x\" = max {y ∈ \"B\" : \"x\" ∈ \"N(y)\"} thereby obtaining an interior algebra. \"N(x)\" will then be precisely the filter of neighbourhoods of \"x\" in this interior algebra. Thus interior algebras are equivalent to Boolean algebras with specified neighbourhood functions.\n\nIn terms of neighbourhood functions, the open elements are precisely those elements \"x\" such that \"x\" ∈ \"N(x)\". In terms of open elements \"x\" ∈ \"N(y)\" if and only if there is an open element \"z\" such that \"y\" ≤ \"z\" ≤ \"x\".\n\nNeighbourhood functions may be defined more generally on (meet)-semilattices producing the structures known as neighbourhood (semi)lattices. Interior algebras may thus be viewed as precisely the Boolean neighbourhood lattices i.e. those neighbourhood lattices whose underlying semilattice forms a Boolean algebra.\n\nGiven a theory (set of formal sentences) \"M\" in the modal logic S4, we can form its Lindenbaum–Tarski algebra:\n\nwhere ~ is the equivalence relation on sentences in \"M\" given by \"p\" ~ \"q\" if and only if \"p\" and \"q\" are logically equivalent in \"M\", and \"M\" / ~ is the set of equivalence classes under this relation. Then L(\"M\") is an interior algebra. The interior operator in this case corresponds to the modal operator □ (necessarily), while the closure operator corresponds to ◊ (possibly). This construction is a special case of a more general result for modal algebras and modal logic.\n\nThe open elements of L(\"M\") correspond to sentences that are only true if they are necessarily true, while the closed elements correspond to those that are only false if they are necessarily false.\n\nBecause of their relation to S4, interior algebras are sometimes called S4 algebras or Lewis algebras, after the logician C. I. Lewis, who first proposed the modal logics S4 and S5.\n\nSince interior algebras are (normal) Boolean algebras with operators, they can be represented by fields of sets on appropriate relational structures. In particular, since they are modal algebras, they can be represented as fields of sets on a set with a single binary relation, called a modal frame. The modal frames corresponding to interior algebras are precisely the preordered sets. Preordered sets (also called \"S4-frames\") provide the Kripke semantics of the modal logic S4, and the connection between interior algebras and preorders is deeply related to their connection with modal logic.\n\nGiven a preordered set X = ⟨\"X\", «⟩ we can construct an interior algebra\n\nfrom the power set Boolean algebra of \"X\" where the interior operator is given by\n\nThe corresponding closure operator is given by\n\n\"S\" is the set of all \"worlds\" inaccessible from \"worlds\" outside \"S\", and \"S\" is the set of all \"worlds\" accessible from some \"world\" in \"S\". Every interior algebra can be embedded in an interior algebra of the form B(X) for some preordered set X giving the above-mentioned representation as a field of sets (a preorder field).\n\nThis construction and representation theorem is a special case of the more general result for modal algebras and modal frames. In this regard, interior algebras are particularly interesting because of their connection to topology. The construction provides the preordered set X with a topology, the Alexandrov topology, producing a topological space T(X) whose open sets are:\n\nThe corresponding closed sets are:\n\nIn other words, the open sets are the ones whose \"worlds\" are inaccessible from outside (the up-sets), and the closed sets are the ones for which every outside \"world\" is inaccessible from inside (the down-sets). Moreover, B(X) = A(T(X)).\n\nAny monadic Boolean algebra can be considered to be an interior algebra where the interior operator is the universal quantifier and the closure operator is the existential quantifier. The monadic Boolean algebras are then precisely the variety of interior algebras satisfying the identity \"x\" = \"x\". In other words, they are precisely the interior algebras in which every open element is closed or equivalently, in which every closed element is open. Moreover, such interior algebras are precisely the semisimple interior algebras. They are also the interior algebras corresponding to the modal logic S5, and so have also been called S5 algebras.\n\nIn the relationship between preordered sets and interior algebras they correspond to the case where the preorder is an equivalence relation, reflecting the fact that such preordered sets provide the Kripke semantics for S5. This also reflects the relationship between the monadic logic of quantification (for which monadic Boolean algebras provide an algebraic description) and S5 where the modal operators □ (necessarily) and ◊ (possibly) can be interpreted in the Kripke semantics using monadic universal and existential quantification, respectively, without reference to an accessibility relation.\n\nThe open elements of an interior algebra form a Heyting algebra and the closed elements form a dual Heyting algebra. The regular open elements and regular closed elements correspond to the pseudo-complemented elements and dual pseudo-complemented elements of these algebras respectively and thus form Boolean algebras. The clopen elements correspond to the complemented elements and form a common subalgebra of these Boolean algebras as well as of the interior algebra itself. Every Heyting algebra can be represented as the open elements of an interior algebra and the latter may be chosen to an interior algebra generated by its open elements - such interior algebras correspond one to one with Heyting algebras (up to isomorphism) being the free Boolean extensions of the latter.\n\nHeyting algebras play the same role for intuitionistic logic that interior algebras play for the modal logic S4 and Boolean algebras play for propositional logic. The relation between Heyting algebras and interior algebras reflects the relationship between intuitionistic logic and S4, in which one can interpret theories of intuitionistic logic as S4 theories closed under necessity. The one to one correspondence between Heyting algebras and interior algebras generated by their open elements reflects the correspondence between extensions of intuitionistic logic and normal extensions of the modal logic S4.Grz.\n\nGiven an interior algebra A, the closure operator obeys the axioms of the derivative operator, . Hence we can form a derivative algebra D(A) with the same underlying Boolean algebra as A by using the closure operator as a derivative operator.\n\nThus interior algebras are derivative algebras. From this perspective, they are precisely the variety of derivative algebras satisfying the identity \"x\" ≥ \"x\". Derivative algebras provide the appropriate algebraic semantics for the modal logic WK4. Hence derivative algebras stand to topological derived sets and WK4 as interior/closure algebras stand to topological interiors/closures and S4.\nGiven a derivative algebra V with derivative operator , we can form an interior algebra I(V) with the same underlying Boolean algebra as V, with interior and closure operators defined by \"x\" = \"x\"·\"x\" ′  ′ and \"x\" = \"x\" + \"x\", respectively. Thus every derivative algebra can be regarded as an interior algebra. Moreover, given an interior algebra A, we have I(D(A)) = A. However, D(I(V)) = V does \"not\" necessarily hold for every derivative algebra V.\n\nStone duality provides a category theoretic duality between Boolean algebras and a class of topological spaces known as Boolean spaces. Building on nascent ideas of relational semantics (later formalized by Kripke) and a result of R. S. Pierce, Jónsson, Tarski and G. Hansoul extended Stone duality to Boolean algebras with operators by equipping Boolean spaces with relations that correspond to the operators via a power set construction. In the case of interior algebras the interior (or closure) operator corresponds to a pre-order on the Boolean space. Homomorphisms between interior algebras correspond to a class of continuous maps between the Boolean spaces known as pseudo-epimorphisms or p-morphisms for short. This generalization of Stone duality to interior algebras based on the Jónsson–Tarski representation was investigated by Leo Esakia and is also known as the \"Esakia duality for S4-algebras (interior algebras)\" and is closely related to the Esakia duality for Heyting algebras.\n\nWhereas the Jónsson–Tarski generalization of Stone duality applies to Boolean algebras with operators in general, the connection between interior algebras and topology allows for another method of generalizing Stone duality that is unique to interior algebras. An intermediate step in the development of Stone duality is Stone's representation theorem which represents a Boolean algebra as a field of sets. The Stone topology of the corresponding Boolean space is then generated using the field of sets as a topological basis. Building on the topological semantics introduced by Tang Tsao-Chen for Lewis's modal logic, McKinsey and Tarski showed that by generating a topology equivalent to using only the complexes that correspond to open elements as a basis, a representation of an interior algebra is obtained as a topological field of sets - a field of sets on a topological space that is closed with respect to taking interiors or closures. By equipping topological fields of sets with appropriate morphisms known as field maps C. Naturman showed that this approach can be formalized as a category theoretic Stone duality in which the usual Stone duality for Boolean algebras corresponds to the case of interior algebras having redundant interior operator (Boolean interior algebras).\n\nThe pre-order obtained in the Jónsson–Tarski approach corresponds to the accessibility relation in the Kripke semantics for an S4 theory, while the intermediate field of sets corresponds to a representation of the Lindenbaum–Tarski algebra for the theory using the sets of possible worlds in the Kripke semantics in which sentences of the theory hold. Moving from the field of sets to a Boolean space somewhat obfuscates this connection. By treating fields of sets on pre-orders as a category in its own right this deep connection can be formulated as a category theoretic duality that generalizes Stone representation without topology. R. Goldblatt had shown that with restrictions to appropriate homomorphisms such a duality can be formulated for arbitrary modal algebras and modal frames. Naturman showed that in the case of interior algebras this duality applies to more general topomorphisms and can be factored via a category theoretic functor through the duality with topological fields of sets. The latter represent the Lindenbaum–Tarski algebra using sets of points satisfying sentences of the S4 theory in the topological semantics. The pre-order can be obtained as the specialization pre-order of the McKinsey-Tarski topology. The Esakia duality can be recovered via a functor that replaces the field of sets with the Boolean space it generates. Via a functor that instead replaces the pre-order with its corresponding Alexandrov topology, an alternative representation of the interior algebra as a field of sets is obtained where the topology is the Alexandrov bico-reflection of the McKinsey-Tarski topology. The approach of formulating a topological duality for interior algebras using both the Stone topology of the Jónsson–Tarski approach and the Alexandrov topology of the pre-order to form a bi-topological space has been investigated by G. Bezhanishvili, R.Mines, and P.J. Morandi. The McKinsey-Tarski topology of an interior algebra is the intersection of the former two topologies.\n\nGrzegorczyk proved the elementary theory of closure algebras undecidable. Naturman demonstrated that the theory is hereditarily undecidable (all its subtheories are undecidable) and demonstrated an infinite chain of elementary classes of interior algebras with hereditarily undecidable theories.\n\n"}
{"id": "55559637", "url": "https://en.wikipedia.org/wiki?curid=55559637", "title": "James Mills Peirce", "text": "James Mills Peirce\n\nJames Mills Peirce (May 1, 1834 – March 21, 1906) was an American mathematician and educator. He taught at Harvard University for almost 50 years.\n\nHe was the eldest son of Sarah Hunt (Mills) Pierce and Benjamin Peirce (1809–1880), a professor of astronomy and mathematics at Harvard University. The family was considered part of the Boston Brahmin elite class. The surname is pronounced to rhyme with \"\". Benjamin Peirce's father, also named Benjamin, was librarian at Harvard. James had four younger siblings; one brother was philosopher, logician and professor Charles Sanders Peirce (1839–1914). Another brother was Herbert Henry Davis Peirce (1849–1916) who was the First Secretary of the American Embassy in Saint Petersburg, Russia, at the end of the 19th century.\n\nJ. M. Peirce graduated from Harvard College in 1853. While an undergraduate at Harvard, he was a member of the Hasty Pudding Club. He attended Harvard's law school for one year. In 1857, he enrolled at the university's Divinity School and graduated in 1859.\n\nLike his father, James Mills Peirce became a professor of mathematics and astronomy at Harvard. He was first a Tutor in Mathematics, then a proctor at Harvard. He was a preacher in Boston and Charleston, South Carolina, but eventually returned to academia, first as Assistant Professor of Mathematics in 1861. He was promoted to University Professor of Mathematics in 1869, then to Perkins Professor of Astronomy and Mathematics — the same position his father once held — in 1885. He was head of the Graduate Department at Harvard from 1872 to 1895 (becoming its dean when it was converted to the Graduate School). He was the Dean of the Faculty of Arts and Sciences from 1895 to 1898.\n\nAmong his publications are \"Mathematical Tables Chiefly to Four Figures\" (1896) and \"A Text-Book of Analytic Geometry; On the Basis of Professor Peirce’s Treatise\" (1857). He was considered a world authority on quaternions.\n"}
{"id": "841785", "url": "https://en.wikipedia.org/wiki?curid=841785", "title": "James Wilson (Archdeacon of Manchester)", "text": "James Wilson (Archdeacon of Manchester)\n\nJames Maurice Wilson (6 November 1836, Castletown, Isle of Man – 15 April 1931, Steep, Petersfield, Hampshire, England) was a British priest in the Church of England as well as a theologian, teacher and astronomer.\n\nWilson and his twin brother, Edward Pears Wilson, attended King William's College on the Isle of Man from August 1848 to midsummer 1853 (his twin died in December 1856). Their father Edward, vicar of Nocton in Lincolnshire, had earlier been headmaster there. According to his autobiography, Wilson had a rather unhappy time at King William's College. He later studied at Sedbergh School.\n\nWilson entered St John's College, Cambridge, in 1855, where he was Senior Wrangler in 1859. He received a Master of Arts degree in 1862 and was a fellow from 1859 to 1868.\n\nWilson was a major figure in the development and reform of Victorian public schools and promoted the teaching of science, which had until then been neglected. He was maths and science master at Rugby School from 1859 to 1879 and headmaster of Clifton College from 1879 to 1890.\n\nHe made astronomical observations (particularly of double stars) at Temple Observatory at Rugby with his former student George Mitchell Seabroke. Temple Observatory was named after Frederick Temple, headmaster of Rugby School, who later became Bishop of Exeter and Archbishop of Canterbury.\n\nWilson was encouraged by Temple to write the textbook \"Elementary Geometry\", which was published in 1868. Until that time, Euclid's \"Elements\" had remained the standard textbook used in British schools.\n\nWith Joseph Gledhill and Edward Crossley, Wilson co-wrote \"Handbook of Double Stars\" in 1879, which became a standard reference work in astronomy. His astronomical observations seem to have come to an end after he left Rugby and went to Clifton.\n\nWhile at Clifton, he successfully pushed for the creation of St Agnes Park in Bristol, as part of a plan to improve the lives of the urban poor.\n\nAfter his teaching career, he became Vicar of Rochdale, Archdeacon of Manchester from 1890 to 1905, a canon of Worcester Cathedral from 1905 to 1926 and vice-dean of the cathedral. He was Hulsean lecturer at Cambridge in 1898; Lady Margaret Preacher at Cambridge in 1900; and Lecturer in Pastoral Theology at Cambridge in 1902.\n\nHe wholeheartedly accepted the theory of evolution and its implications for the literal interpretation of the Bible. He gave two lectures in 1892 in which he accepted Darwinism and argued that it was compatible with a higher view of Christianity; the lectures were published by the Society for the Promotion of Christian Knowledge, which had a few years earlier strongly opposed Darwinian ideas.\n\nIn 1921, he served for one year as president of The Mathematical Association of the UK.\n\nIn 1925 he wrote an essay entitled \"The Religious Effect of the Idea of Evolution\". He wrote a number of books, including \"Life after Death\" \"with replies by Sir Arthur Conan Doyle\" in 1920. In addition to spiritual works, he co-wrote an astronomy book on double stars (mentioned above) and mathematical books on geometry and conic sections. He contributed the article \"On two fragments of geometrical treatises found in Worcester Cathedral\" to the \"Mathematical Gazette\" (March 1911, p. 19).\n\nIn 1868 he married his first wife, Annie Elizabeth Moore. Their first child was the leading civil servant Mona Wilson. His first wife died after giving birth to their fourth child in 1878. She was a cousin once removed of Arthur William Moore, a proponent of the Manx language.\n\nIn 1883 he married his second wife, Georgina Mary Talbot. Their sons included Sir Arnold Talbot Wilson, who became a British colonial administrator in Baghdad and was killed in action in World War II; 2nd Lt. Hugh Stanley Wilson (1885–1915), who died in World War I and is buried in the military cemetery at Hébuterne, Pas de Calais; and the tenor Sir Steuart Wilson. From his notes, Arnold and Steuart published the posthumous \"James M. Wilson: An Autobiography\" (London, Sidgwick & Jackson, 1932)\n\n\n"}
{"id": "50677472", "url": "https://en.wikipedia.org/wiki?curid=50677472", "title": "Karlsruhe Accurate Arithmetic", "text": "Karlsruhe Accurate Arithmetic\n\nKarlsruhe Accurate Arithmetic (KAA) or Karlsruhe Accurate Arithmetic Approach (KAAA), augments conventional floating-point arithmetic with good error behaviour with new operations to calculate scalar products with a single rounding error.\n\nThe foundations for KAA were developed at the University of Karlsruhe starting in the late 1960s.\n\n\n"}
{"id": "11386287", "url": "https://en.wikipedia.org/wiki?curid=11386287", "title": "Levy–Mises equations", "text": "Levy–Mises equations\n\nThe Levi–Mises equations (also called flow rules) describe the relationship between stress and strain for an ideal plastic solid where the elastic strains are negligible.\n\nThe generalized Levy–Mises equation can be written as:\n"}
{"id": "2994042", "url": "https://en.wikipedia.org/wiki?curid=2994042", "title": "Marek Karpinski", "text": "Marek Karpinski\n\nMarek Karpinski is a computer scientist and mathematician known for his research in the theory of algorithms and their applications, combinatorial optimization, computational complexity, and mathematical foundations. He is a recipient of several research prizes in the above areas.\n\nHe is currently a Professor of Computer Science, and the Head of the Algorithms Group at the University of Bonn. He is also a member of Bonn International Graduate School in Mathematics BIGS and the Hausdorff Center for Mathematics.\n\n"}
{"id": "2452874", "url": "https://en.wikipedia.org/wiki?curid=2452874", "title": "Mario Pieri", "text": "Mario Pieri\n\nMario Pieri (22 June 1860 – 1 March 1913) was an Italian mathematician who is known for his work on foundations of geometry.\n\nPieri was born in Lucca, Italy, the son of Pellegrino Pieri and Ermina Luporini. Pellegrino was a lawyer. Pieri began his higher education at University of Bologna where he drew the attention of Salvatore Pincherle. Obtaining a scholarship, Pieri transferred to \"Scuola Normale Superiore\" in Pisa. There he took his degree in 1884 and worked first at a technical secondary school in Pisa.\n\nWhen the opportunity to teach projective geometry at the military academy in Turin arose, Pieri moved there. By 1888 he was assisting in instructing that subject also at the University of Turin. In 1891 he became \"libero docente\" at the university, giving elective courses. Pieri continued teaching in Turin until 1900 when he won a competition for the position of \"extraordinary professor\" at University of Catania on the island of Sicily. In 1908 he moved to University of Parma, and in 1911 fell ill. Pieri died in Andrea di Compito (Capannori), not far from Lucca.\n\nVon Staudt's \"Geometrie der Lage\" (1847) was a much admired text on projective geometry. In 1889 Pieri translated it as \"Geometria di Posizione\", a publication that included a study of the life and work of von Staudt written by Corrado Segre, the initiator of the project.\n\nPieri also came under the influence of Giuseppe Peano at Turin. He contributed to the Formulario mathematico, and Peano placed nine of Pieri's papers for publication with the Academy of Sciences of Turin between 1895 and 1912. They shared a passion for reducing geometric ideas to their logical form and expressing these ideas symbolically.\n\nIn 1898 Pieri wrote \"I principii della geometria di posizione composti in un sistema logico-deduttivo\". According to J.T. Smith (2010) it is\n\nPieri was invited to address the International Congress of Philosophy in 1900 in Paris. Since this was also the year he moved from Turin to Sicily, he declined to attend but sent a paper \"Sur la Géométrie envisagée comme un système purement logique\" which was delivered by Louis Couturat. The ideas were also advanced by Alessandro Padoa at both that Congress and the International Congress of Mathematicians also held in Paris that year.\n\nIn 1900 Pieri wrote \"Monographia del punto e del moto\", which Smith calls the \"Point and Motion\" memoire. It is noteworthy as using only two primitive notions, point and motion to develop axioms for geometry. Alessandro Padoa shared in this expression of Peano's logico-geometrical program that reduced the number of primitive notions from the \"four\" used by Moritz Pasch.\n\nThe research into the foundations of geometry led to another formulation in 1908 in a \"Point and Sphere\" memoire. Smith (2010) describes it as\nThis memoire was translated into Polish in 1915 by S. Kwietniewski. A young Alfred Tarski encountered the text and carried forward Pieri's program, as recounted by Smith.\n\nIn 2002 Avellone, Brigaglia & Zappulla gave a modern evaluation of Pieri's contribution to geometry:\n\nGiuseppe Peano wrote this tribute to Pieri upon his death:\n\nMario Pieri's collected works were published by the Italian Mathematical Union in 1980 under the title \"Opere sui fondamenti della matematica\" (Edizioni Cremonese, Bologna).\n\n\n"}
{"id": "20942679", "url": "https://en.wikipedia.org/wiki?curid=20942679", "title": "Mark Jerrum", "text": "Mark Jerrum\n\nMark Richard Jerrum (born 1955) is a British computer scientist and computational theorist.\n\nJerrum received his Ph.D. in computer science 'On the complexity of evaluating multivariate polynomials' in 1981 from University of Edinburgh under the supervision of Leslie Valiant. He is professor of pure mathematics at Queen Mary, University of London.\n\nWith his student Alistair Sinclair, Jerrum investigated the mixing behaviour of Markov chains to construct approximation algorithms for counting problems such as the computing the permanent, with applications in diverse fields such as matching algorithms, geometric algorithms, mathematical programming, statistics, physics-inspired applications, and dynamical systems. This work has been highly influential in theoretical computer science and was recognised with the Gödel Prize in 1996. A refinement of these methods led to a fully polynomial-time randomised approximation algorithm for computing the permanent, for which Jerrum and his co-authors received the Fulkerson Prize in 2006.\n\n\n"}
{"id": "14920509", "url": "https://en.wikipedia.org/wiki?curid=14920509", "title": "Markov's principle", "text": "Markov's principle\n\nMarkov's principle, named after Andrey Markov Jr, is a specific statement in computability theory that is obviously true classically (i.e. it is a logical validity), but must be proved when using constructive mathematics. \nThere are many equivalent formulations of Markov's principle.\n\nIn the language of computability theory, Markov's principle is a formal expression of the claim that if it is impossible that an algorithm does not terminate, then it does terminate. This is equivalent to the claim that if a set and its complement are both computably enumerable, then the set is decidable. \n\nIn predicate logic, if \"P\" is a predicate over the natural numbers, it is expressed as:\n\nThat is, if \"P\" is decidable, and it cannot be false for every natural number \"n\", then it is true for some \"n\". (In general, a predicate \"P\" over some domain is called \"decidable\" if for every \"x\" in the domain, either \"P\"(\"x\") is true, or \"P\"(\"x\") is not true, which is not always the case constructively.)\n\nIt is equivalent in the language of arithmetic to:\nfor formula_3 a total recursive function on the natural numbers.\n\nIt is equivalent, in the language of real analysis, to the following principles:\n\n\nIf constructive arithmetic is translated using realizability into a classical meta-theory that proves the formula_4-consistency of the relevant classical theory (for example, Peano Arithmetic if we are studying Heyting arithmetic), then Markov's principle is justified: a realizer is the constant function that takes a realization that formula_5 is not everywhere false to the unbounded search that successively checks if formula_6 is true. If formula_5 is not everywhere false, then by formula_4-consistency there must be a term for which formula_5 holds, and each term will be checked by the search eventually. If however formula_5 does not hold anywhere, then the domain of the constant function must be empty, so although the search does not halt it still holds vacuously that the function is a realizer. By the Law of the Excluded Middle (in our classical metatheory), formula_5 must either hold nowhere or not hold nowhere, therefore this constant function is a realizer.\n\nIf instead the realizability interpretation is used in a constructive meta-theory, then it is not justified. Indeed, for first-order arithmetic, Markov's principle exactly captures the difference between a constructive and classical meta-theory. Specifically, a statement is provable in Heyting arithmetic with Extended Church's thesis if and only if there is a number that provably realizes it in Heyting arithmetic; and it is provable in Heyting arithmetic with Extended Church's thesis \"and Markov's principle\" if and only if there is a number that provably realizes it in Peano arithmetic. \n\nModified realizability does not justify Markov's principle, even if classical logic is used in the meta-theory: there is no realizer in the language of simply typed lambda calculus as this language is not Turing-complete and arbitrary loops cannot be defined in it.\n\nMarkov's rule is the formulation of Markov's principle as a rule. It states that formula_12 is derivable as soon as formula_13 is, for formula_5 decidable. It was proved by Anne S. Troelstra that Markov's rule is an admissible rule in Heyting arithmetic. Later on, the logician Harvey Friedman showed that Markov's rule is an admissible rule in all of intuitionistic logic, Heyting arithmetic, and various other intuitionistic theories, using the Friedman translation.\n\nA weaker form of Markov's principle may be stated in the language of analysis as\n\nThis form can be justified by Brouwer's continuity principles, whereas the stronger form contradicts them. Thus it can be derived from intuitionistic, realizability, and classical reasoning, in each case for different reasons, but this principle is not valid in the general constructive sense of Bishop.\n\n\n"}
{"id": "3203945", "url": "https://en.wikipedia.org/wiki?curid=3203945", "title": "Methoden der mathematischen Physik", "text": "Methoden der mathematischen Physik\n\nMethoden der mathematischen Physik (Methods of Mathematical Physics) is a 1924 book, in two volumes totalling around 1000 pages, published under the names of David Hilbert and Richard Courant. It was a comprehensive treatment of the \"methods of mathematical physics\" of the time. The second volume is devoted to the theory of partial differential equations. It contains presages of the finite element method, on which Courant would work subsequently, and which would eventually become basic to numerical analysis.\n\nThe material of the book was worked up from the content of Hilbert's lectures. While Courant played the major editorial role, many at the University of Göttingen were involved in the writing-up, and in that sense it was a collective production.\n\nOn its appearance in 1924 it apparently had little direct connection to the quantum theory questions at the centre of the theoretical physics of the time. That changed within two years, since the formulation of Schrödinger's equation made the Hilbert-Courant techniques of immediate relevance to the new wave mechanics.\n\nThere was a second edition (1931/7), wartime edition in the USA (1943), and a third German edition (1968). The English version \"Methods of Mathematical Physics\" (1953) was revised by Courant, and the second volume had extensive work done on it by the faculty of the Courant Institute. The books quickly gained the reputation as classics, and are among most highly referenced books in advanced mathematical physics courses.\n\n"}
{"id": "14479902", "url": "https://en.wikipedia.org/wiki?curid=14479902", "title": "Monogenic system", "text": "Monogenic system\n\nIn classical mechanics, a physical system is termed a monogenic system if the force acting on the system can be modelled in an especially convenient mathematical form (see mathematical definition below). In physics, among the most studied physical systems are monogenic systems. \n\nIn Lagrangian mechanics, the property of being monogenic is a necessary condition for the equivalence of different formulations of principle. If a physical system is both a holonomic system and a monogenic system, then it is possible to derive Lagrange's equations from d'Alembert's principle; it is also possible to derive Lagrange's equations from Hamilton's principle.\n\nThe term was introduced by Cornelius Lanczos in his book \"The Variational Principles of Mechanics\" (1970).\n\nMonogenic systems have excellent mathematical characteristics and are well suited for mathematical analysis. Pedagogically, within the discipline of mechanics, it is considered a logical starting point for any serious physics endeavour. \n\nIn a physical system, if all forces, with the exception of the constraint forces, are derivable from the generalized scalar potential, and this generalized scalar potential is a function of generalized coordinates, generalized velocities, or time, then, this system is a monogenic system.\n\nExpressed using equations, the exact relationship between generalized force formula_1 and generalized potential formula_2 is as follows:\n\nwhere formula_4 is generalized coordinate, formula_5 is generalized velocity, and formula_6 is time.\n\nIf the generalized potential in a monogenic system depends only on generalized coordinates, and not on generalized velocities and time, then, this system is a conservative system.The relationship between generalized force and generalized potential is as follows: \n\n"}
{"id": "69880", "url": "https://en.wikipedia.org/wiki?curid=69880", "title": "Napoleon", "text": "Napoleon\n\nNapoléon Bonaparte (; , ; 15 August 1769 – 5 May 1821) was a French statesman and military leader who rose to prominence during the French Revolution and led several successful campaigns during the French Revolutionary Wars. He was Emperor of the French from 1804 until 1814 and again briefly in 1815 during the Hundred Days. Napoleon dominated European and global affairs for more than a decade while leading France against a series of coalitions in the Napoleonic Wars. He won most of these wars and the vast majority of his battles, building a large empire that ruled over continental Europe before its final collapse in 1815. He is considered one of the greatest commanders in history, and his wars and campaigns are studied at military schools worldwide. Napoleon's political and cultural legacy has endured as one of the most celebrated and controversial leaders in human history.\n\nHe was born Napoleone di Buonaparte () in Corsica to a relatively modest family of Italian origin from the minor nobility. He was serving as an artillery officer in the French army when the French Revolution erupted in 1789. He rapidly rose through the ranks of the military, seizing the new opportunities presented by the Revolution and becoming a general at age 24. The French Directory eventually gave him command of the Army of Italy after he suppressed a revolt against the government from royalist insurgents. At age 26, he began his first military campaign against the Austrians and the Italian monarchs aligned with the Habsburgs—winning virtually every battle, conquering the Italian Peninsula in a year while establishing \"sister republics\" with local support, and becoming a war hero in France. In 1798, he led a military expedition to Egypt that served as a springboard to political power. He orchestrated a coup in November 1799 and became First Consul of the Republic. His ambition and public approval inspired him to go further, and he became the first Emperor of the French in 1804. Intractable differences with the British meant that the French were facing a Third Coalition by 1805. Napoleon shattered this coalition with decisive victories in the Ulm Campaign and a historic triumph over the Russian Empire and Austrian Empire at the Battle of Austerlitz which led to the dissolution of the Holy Roman Empire. In 1806, the Fourth Coalition took up arms against him because Prussia became worried about growing French influence on the continent. Napoleon quickly defeated Prussia at the battles of Jena and Auerstedt, then marched his \"Grande Armée\" deep into Eastern Europe and annihilated the Russians in June 1807 at the Battle of Friedland. France then forced the defeated nations of the Fourth Coalition to sign the Treaties of Tilsit in July 1807, bringing an uneasy peace to the continent. Tilsit signified the high-water mark of the French Empire. In 1809, the Austrians and the British challenged the French again during the War of the Fifth Coalition, but Napoleon solidified his grip over Europe after triumphing at the Battle of Wagram in July.\n\nNapoleon then invaded the Iberian Peninsula, hoping to extend the Continental System and choke off British trade with the European mainland, and declared his brother Joseph Bonaparte the King of Spain in 1808. The Spanish and the Portuguese revolted with British support. The Peninsular War lasted six years, featured extensive guerrilla warfare, and ended in victory for the Allies. The Continental System caused recurring diplomatic conflicts between France and its client states, especially Russia. The Russians were unwilling to bear the economic consequences of reduced trade and routinely violated the Continental System, enticing Napoleon into another war. The French launched a major invasion of Russia in the summer of 1812. The campaign destroyed Russian cities but resulted in the collapse of the \"Grande Armée\" and inspired a renewed push against Napoleon by his enemies. In 1813, Prussia and Austria joined Russian forces in the War of the Sixth Coalition against France. A lengthy military campaign culminated in a large Allied army defeating Napoleon at the Battle of Leipzig in October 1813, but his tactical victory at the minor Battle of Hanau allowed retreat onto French soil. The Allies then invaded France and captured Paris in the spring of 1814, forcing Napoleon to abdicate in April. He was exiled to the island of Elba off the coast of Tuscany, and the Bourbon dynasty was restored to power. However, Napoleon escaped from Elba in February 1815 and took control of France once again. The Allies responded by forming a Seventh Coalition which defeated him at the Battle of Waterloo in June. The British exiled him to the remote island of Saint Helena in the South Atlantic, where he died six years later at the age of 51.\n\nNapoleon's influence on the modern world brought liberal reforms to the numerous territories that he conquered and controlled, such as the Low Countries, Switzerland, and large parts of modern Italy and Germany. He implemented fundamental liberal policies in France and throughout Western Europe. His Napoleonic Code has influenced the legal systems of more than 70 nations around the world. British historian Andrew Roberts states: \"The ideas that underpin our modern world—meritocracy, equality before the law, property rights, religious toleration, modern secular education, sound finances, and so on—were championed, consolidated, codified and geographically extended by Napoleon. To them he added a rational and efficient local administration, an end to rural banditry, the encouragement of science and the arts, the abolition of feudalism and the greatest codification of laws since the fall of the Roman Empire\".\n\nThe ancestors of Napoleon descended from minor Italian nobility of Tuscan origin who had come to Corsica from Liguria in the 16th century. His parents Carlo Maria di Buonaparte and Maria Letizia Ramolino maintained an ancestral home called \"Casa Buonaparte\" in Ajaccio. Napoleon was born there on 15 August 1769, their fourth child and third son. A boy and girl were born first but died in infancy. He had an elder brother, Joseph, and younger siblings Lucien, Elisa, Louis, Pauline, Caroline, and Jérôme. Napoleon was baptised as a Catholic. Although he was born \"Napoleone di Buonaparte\" (), he changed his name to \"Napoléon Bonaparte\" () when he was 27 in 1796 upon his first marriage.\n\nNapoleon was born the same year the Republic of Genoa, a former commune of Italy, transferred Corsica to France. The state sold sovereign rights a year before his birth in 1768, and the island was conquered by France during the year of his birth and formally incorporated as a province in 1770, after 500 years under nominal Genoese rule and 14 years of independence. Napoleon's parents fought to maintain independence even when Maria was pregnant with him. His father was an attorney who went on to be named Corsica's representative to the court of Louis XVI in 1777. The dominant influence of Napoleon's childhood was his mother, whose firm discipline restrained a rambunctious child. Napoleon's maternal grandmother had married into the Swiss Fesch family in her second marriage, and Napoleon's uncle, the cardinal Joseph Fesch, would fulfill a role as protector of the Bonaparte family for some years. Napoleon's noble, moderately affluent background afforded him greater opportunities to study than were available to a typical Corsican of the time.\n\nWhen he turned 9 years old, he moved to the French mainland and enrolled at a religious school in Autun in January 1779. In May, he transferred with a scholarship to a military academy at Brienne-le-Château. In his youth he was an outspoken Corsican nationalist and supported the state's independence from France. Like many Corsicans, Napoleon spoke and read Corsican (as his mother tongue) and Italian (as the official language of Corsica). He began learning French in school at around age 10. Although he became fluent in French, he spoke with a distinctive Corsican accent and never learned how to spell French correctly. He was routinely bullied by his peers for his accent, birthplace, short stature, mannerisms and inability to speak French quickly. Bonaparte became reserved and melancholy applying himself to reading. An examiner observed that Napoleon \"has always been distinguished for his application in mathematics. He is fairly well acquainted with history and geography ... This boy would make an excellent sailor\". In early adulthood, he briefly intended to become a writer; he authored a history of Corsica and a romantic novella.\n\nOn completion of his studies at Brienne in 1784, Napoleon was admitted to the \"École Militaire\" in Paris. He trained to become an artillery officer and, when his father's death reduced his income, was forced to complete the two-year course in one year. He was the first Corsican to graduate from the \"École Militaire\". He was examined by the famed scientist Pierre-Simon Laplace.\n\nUpon graduating in September 1785, Bonaparte was commissioned a second lieutenant in \"La Fère\" artillery regiment. He served in Valence and Auxonne until after the outbreak of the Revolution in 1789, and took nearly two years' leave in Corsica and Paris during this period. At this time, he was a fervent Corsican nationalist, and wrote to Corsican leader Pasquale Paoli in May 1789, \"As the nation was perishing I was born. Thirty thousand Frenchmen were vomited on to our shores, drowning the throne of liberty in waves of blood. Such was the odious sight which was the first to strike me\".\n\nHe spent the early years of the Revolution in Corsica, fighting in a complex three-way struggle among royalists, revolutionaries, and Corsican nationalists. He was a supporter of the republican Jacobin movement, organising clubs in Corsica, and was given command over a battalion of volunteers. He was promoted to captain in the regular army in July 1792, despite exceeding his leave of absence and leading a riot against French troops.\n\nHe came into conflict with Paoli, who had decided to split with France and sabotage the Corsican contribution to the \"Expédition de Sardaigne\", by preventing a French assault on the Sardinian island of La Maddalena. Bonaparte and his family fled to the French mainland in June 1793 because of the split with Paoli.\n\nIn July 1793, Bonaparte published a pro-republican pamphlet entitled \"Le souper de Beaucaire\" (Supper at Beaucaire) which gained him the support of Augustin Robespierre, younger brother of the Revolutionary leader Maximilien Robespierre. With the help of his fellow Corsican Antoine Christophe Saliceti, Bonaparte was appointed artillery commander of the republican forces at the Siege of Toulon.\n\nHe adopted a plan to capture a hill where republican guns could dominate the city's harbour and force the British to evacuate. The assault on the position led to the capture of the city, but during it Bonaparte was wounded in the thigh. He was promoted to brigadier general at the age of 24. Catching the attention of the Committee of Public Safety, he was put in charge of the artillery of France's Army of Italy.\n\nNapoleon spent time as inspector of coastal fortifications on the Mediterranean coast near Marseille while he was waiting for confirmation of the Army of Italy post. He devised plans for attacking the Kingdom of Sardinia as part of France's campaign against the First Coalition. Augustin Robespierre and Saliceti were ready to listen to the freshly promoted artillery general.\n\nThe French army carried out Bonaparte's plan in the Battle of Saorgio in April 1794, and then advanced to seize Ormea in the mountains. From Ormea, they headed west to outflank the Austro-Sardinian positions around Saorge. After this campaign, Augustin Robespierre sent Bonaparte on a mission to the Republic of Genoa to determine that country's intentions towards France.\n\nSome contemporaries alleged that Bonaparte was put under house arrest at Nice for his association with the Robespierres following their fall in the Thermidorian Reaction in July 1794, but Napoleon's secretary Bourrienne disputed the allegation in his memoirs. According to Bourrienne, jealousy was responsible, between the Army of the Alps and the Army of Italy (with whom Napoleon was seconded at the time). Bonaparte dispatched an impassioned defense in a letter to the commissar Saliceti, and he was subsequently acquitted of any wrongdoing. He was released within two weeks and, due to his technical skills, was asked to draw up plans to attack Italian positions in the context of France's war with Austria. He also took part in an expedition to take back Corsica from the British, but the French were repulsed by the British Royal Navy.\n\nBy 1795, Bonaparte had become engaged to Désirée Clary, daughter of François Clary. Désirée's sister Julie Clary had married Bonaparte's elder brother Joseph. In April 1795, he was assigned to the Army of the West, which was engaged in the War in the Vendée—a civil war and royalist counter-revolution in Vendée, a region in west central France on the Atlantic Ocean. As an infantry command, it was a demotion from artillery general—for which the army already had a full quota—and he pleaded poor health to avoid the posting.\n\nHe was moved to the Bureau of Topography of the Committee of Public Safety and sought unsuccessfully to be transferred to Constantinople in order to offer his services to the Sultan. During this period, he wrote the romantic novella \"Clisson et Eugénie\", about a soldier and his lover, in a clear parallel to Bonaparte's own relationship with Désirée. On 15 September, Bonaparte was removed from the list of generals in regular service for his refusal to serve in the Vendée campaign. He faced a difficult financial situation and reduced career prospects.\n\nOn 3 October, royalists in Paris declared a rebellion against the National Convention. Paul Barras, a leader of the Thermidorian Reaction, knew of Bonaparte's military exploits at Toulon and gave him command of the improvised forces in defence of the Convention in the Tuileries Palace. Napoleon had seen the massacre of the King's Swiss Guard there three years earlier and realised that artillery would be the key to its defence.\n\nHe ordered a young cavalry officer named Joachim Murat to seize large cannons and used them to repel the attackers on 5 October 1795—\"13 Vendémiaire An IV\" in the French Republican Calendar; 1,400 royalists died and the rest fled. He had cleared the streets with \"a whiff of grapeshot\", according to 19th-century historian Thomas Carlyle in \"\".\n\nThe defeat of the royalist insurrection extinguished the threat to the Convention and earned Bonaparte sudden fame, wealth, and the patronage of the new government, the Directory. Murat married one of Napoleon's sisters, becoming his brother-in-law; he also served under Napoleon as one of his generals. Bonaparte was promoted to Commander of the Interior and given command of the Army of Italy.\n\nWithin weeks, he was romantically involved with Joséphine de Beauharnais, the former mistress of Barras. The couple married on 9 March 1796 in a civil ceremony.\n\nTwo days after the marriage, Bonaparte left Paris to take command of the Army of Italy. He immediately went on the offensive, hoping to defeat the forces of Piedmont before their Austrian allies could intervene. In a series of rapid victories during the Montenotte Campaign, he knocked Piedmont out of the war in two weeks. The French then focused on the Austrians for the remainder of the war, the highlight of which became the protracted struggle for Mantua. The Austrians launched a series of offensives against the French to break the siege, but Napoleon defeated every relief effort, scoring victories at the battles of Castiglione, Bassano, Arcole, and Rivoli. The decisive French triumph at Rivoli in January 1797 led to the collapse of the Austrian position in Italy. At Rivoli, the Austrians lost up to 14,000 men while the French lost about 5,000.\n\nThe next phase of the campaign featured the French invasion of the Habsburg heartlands. French forces in Southern Germany had been defeated by the Archduke Charles in 1796, but the Archduke withdrew his forces to protect Vienna after learning about Napoleon's assault. In the first encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797. The Austrians were alarmed by the French thrust that reached all the way to Leoben, about 100 km from Vienna, and finally decided to sue for peace. The Treaty of Leoben, followed by the more comprehensive Treaty of Campo Formio, gave France control of most of northern Italy and the Low Countries, and a secret clause promised the Republic of Venice to Austria. Bonaparte marched on Venice and forced its surrender, ending 1,100 years of independence. He also authorized the French to loot treasures such as the Horses of Saint Mark.\nHis application of conventional military ideas to real-world situations enabled his military triumphs, such as creative use of artillery as a mobile force to support his infantry. He stated later in life: \"I have fought sixty battles and I have learned nothing which I did not know at the beginning. Look at Caesar; he fought the first like the last\".\n\nBonaparte could win battles by concealment of troop deployments and concentration of his forces on the \"hinge\" of an enemy's weakened front. If he could not use his favourite envelopment strategy, he would take up the central position and attack two co-operating forces at their hinge, swing round to fight one until it fled, then turn to face the other. In this Italian campaign, Bonaparte's army captured 150,000 prisoners, 540 cannons, and 170 standards. The French army fought 67 actions and won 18 pitched battles through superior artillery technology and Bonaparte's tactics.\n\nDuring the campaign, Bonaparte became increasingly influential in French politics. He founded two newspapers: one for the troops in his army and another for circulation in France. The royalists attacked Bonaparte for looting Italy and warned that he might become a dictator. All told, Napoleon's forces extracted an estimated $45 million in funds from Italy during their campaign there, another $12 million in precious metals and jewels; atop that, his forces confiscated more than three-hundred priceless paintings and sculptures. Bonaparte sent General Pierre Augereau to Paris to lead a \"coup d'état\" and purge the royalists on 4 September—Coup of 18 Fructidor. This left Barras and his Republican allies in control again but dependent on Bonaparte, who proceeded to peace negotiations with Austria. These negotiations resulted in the Treaty of Campo Formio, and Bonaparte returned to Paris in December as a hero. He met Talleyrand, France's new Foreign Minister—who served in the same capacity for Emperor Napoleon—and they began to prepare for an invasion of Britain.\n\nAfter two months of planning, Bonaparte decided that France's naval power was not yet strong enough to confront the British Royal Navy. He decided on a military expedition to seize Egypt and thereby undermine Britain's access to its trade interests in India. Bonaparte wished to establish a French presence in the Middle East, linking with Tipu Sultan, a Muslim enemy of the British in India. Napoleon assured the Directory that \"as soon as he had conquered Egypt, he will establish relations with the Indian princes and, together with them, attack the English in their possessions\". The Directory agreed in order to secure a trade route to India.\n\nIn May 1798, Bonaparte was elected a member of the French Academy of Sciences. His Egyptian expedition included a group of 167 scientists, with mathematicians, naturalists, chemists, and geodesists among them. Their discoveries included the Rosetta Stone, and their work was published in the \"Description de l'Égypte\" in 1809.\n\nEn route to Egypt, Bonaparte reached Malta on 9 June 1798, then controlled by the Knights Hospitaller. Grand Master Ferdinand von Hompesch zu Bolheim surrendered after token resistance, and Bonaparte captured an important naval base with the loss of only three men.\n\nGeneral Bonaparte and his expedition eluded pursuit by the Royal Navy and landed at Alexandria on 1 July. He fought the Battle of Shubra Khit against the Mamluks, Egypt's ruling military caste. This helped the French practise their defensive tactic for the Battle of the Pyramids, fought on 21 July, about from the pyramids. General Bonaparte's forces of 25,000 roughly equalled those of the Mamluks' Egyptian cavalry. Twenty-nine French and approximately 2,000 Egyptians were killed. The victory boosted the morale of the French army.\n\nOn 1 August 1798, the British fleet under Sir Horatio Nelson captured or destroyed all but two French vessels in the Battle of the Nile, defeating Bonaparte's goal to strengthen the French position in the Mediterranean. His army had succeeded in a temporary increase of French power in Egypt, though it faced repeated uprisings. In early 1799, he moved an army into the Ottoman province of Damascus (Syria and Galilee). Bonaparte led these 13,000 French soldiers in the conquest of the coastal towns of Arish, Gaza, Jaffa, and Haifa. The attack on Jaffa was particularly brutal. Bonaparte discovered that many of the defenders were former prisoners of war, ostensibly on parole, so he ordered the garrison and 1,400 prisoners to be executed by bayonet or drowning to save bullets. Men, women, and children were robbed and murdered for three days.\n\nBonaparte began with an army of 13,000 men; 1,500 were reported missing, 1,200 died in combat, and thousands perished from disease—mostly bubonic plague. He failed to reduce the fortress of Acre, so he marched his army back to Egypt in May. To speed up the retreat, Bonaparte ordered plague-stricken men to be poisoned with opium; the number who died remains disputed, ranging from a low of 30 to a high of 580. He also brought out 1,000 wounded men. Back in Egypt on 25 July, Bonaparte defeated an Ottoman amphibious invasion at Abukir.\n\nWhile in Egypt, Bonaparte stayed informed of European affairs. He learned that France had suffered a series of defeats in the War of the Second Coalition. On 24 August 1799, he took advantage of the temporary departure of British ships from French coastal ports and set sail for France, despite the fact that he had received no explicit orders from Paris. The army was left in the charge of Jean Baptiste Kléber.\n\nUnknown to Bonaparte, the Directory had sent him orders to return to ward off possible invasions of French soil, but poor lines of communication prevented the delivery of these messages. By the time that he reached Paris in October, France's situation had been improved by a series of victories. The Republic, however, was bankrupt and the ineffective Directory was unpopular with the French population. The Directory discussed Bonaparte's \"desertion\" but was too weak to punish him.\n\nDespite the failures in Egypt, Napoleon returned to a hero's welcome. He drew together an alliance with director Emmanuel Joseph Sieyès, his brother Lucien, speaker of the Council of Five Hundred Roger Ducos, director Joseph Fouché, and Talleyrand, and they overthrew the Directory by a coup d'état on 9 November 1799 (\"the 18th Brumaire\" according to the revolutionary calendar), closing down the Council of Five Hundred. Napoleon became \"first consul\" for ten years, with two consuls appointed by him who had consultative voices only. His power was confirmed by the new \"Constitution of the Year VIII\", originally devised by Sieyès to give Napoleon a minor role, but rewritten by Napoleon, and accepted by direct popular vote (3,000,000 in favor, 1,567 opposed). The constitution preserved the appearance of a republic but in reality established a dictatorship.\n\nNapoleon established a political system that historian Martyn Lyons called \"dictatorship by plebiscite\". Worried by the democratic forces unleashed by the Revolution, but unwilling to ignore them entirely, Napoleon resorted to regular electoral consultations with the French people on his road to imperial power. He drafted the Constitution of the Year VIII and secured his own election as First Consul, taking up residence at the Tuileries. The constitution was approved in a rigged plebiscite held the following January, with 99.94 percent officially listed as voting \"yes\". Napoleon's brother, Lucien, had falsified the returns to show that 3 million people had participated in the plebiscite; the real number was 1.5 million. Political observers at the time assumed the eligible French voting public numbered about 5 million people, so the regime artificially doubled the participation rate to indicate popular enthusiasm for the Consulate. In the first few months of the Consulate, with war in Europe still raging and internal instability still plaguing the country, Napoleon's grip on power remained very tenuous.\n\nIn the spring of 1800, Napoleon and his troops crossed the Swiss Alps into Italy, aiming to surprise the Austrian armies that had reoccupied the peninsula when Napoleon was still in Egypt. After a difficult crossing over the Alps, the French army entered the plains of Northern Italy virtually unopposed. While one French army approached from the north, the Austrians were busy with another stationed in Genoa, which was besieged by a substantial force. The fierce resistance of this French army, under André Masséna, gave the northern force some time to carry out their operations with little interference.\n\nAfter spending several days looking for each other, the two armies collided at the Battle of Marengo on 14 June. General Melas had a numerical advantage, fielding about 30,000 Austrian soldiers while Napoleon commanded 24,000 French troops. The battle began favorably for the Austrians as their initial attack surprised the French and gradually drove them back. Melas stated that he'd won the battle and retired to his headquarters around 3 pm, leaving his subordinates in charge of pursuing the French. The French lines never broke during their tactical retreat; Napoleon constantly rode out among the troops urging them to stand and fight. Late in the afternoon, a full division under Desaix arrived on the field and reversed the tide of the battle. A series of artillery barrages and cavalry charges decimated the Austrian army, which fled over the Bormida River back to Alessandria, leaving behind 14,000 casualties. The following day, the Austrian army agreed to abandon Northern Italy once more with the Convention of Alessandria, which granted them safe passage to friendly soil in exchange for their fortresses throughout the region.\n\nAlthough critics have blamed Napoleon for several tactical mistakes preceding the battle, they have also praised his audacity for selecting a risky campaign strategy, choosing to invade the Italian peninsula from the north when the vast majority of French invasions came from the west, near or along the coastline. As Chandler points out, Napoleon spent almost a year getting the Austrians out of Italy in his first campaign; in 1800, it took him only a month to achieve the same goal. German strategist and field marshal Alfred von Schlieffen concluded that \"Bonaparte did not annihilate his enemy but eliminated him and rendered him harmless\" while \"[attaining] the object of the campaign: the conquest of North Italy\".\n\nNapoleon's triumph at Marengo secured his political authority and boosted his popularity back home, but it did not lead to an immediate peace. Bonaparte's brother, Joseph, led the complex negotiations in Lunéville and reported that Austria, emboldened by British support, would not acknowledge the new territory that France had acquired. As negotiations became increasingly fractious, Bonaparte gave orders to his general Moreau to strike Austria once more. Moreau and the French swept through Bavaria and scored an overwhelming victory at Hohenlinden in December 1800. As a result, the Austrians capitulated and signed the Treaty of Lunéville in February 1801. The treaty reaffirmed and expanded earlier French gains at Campo Formio.\n\nAfter a decade of constant warfare, France and Britain signed the Treaty of Amiens in March 1802, bringing the Revolutionary Wars to an end. Amiens called for the withdrawal of British troops from recently conquered colonial territories as well as for assurances to curtail the expansionary goals of the French Republic. With Europe at peace and the economy recovering, Napoleon's popularity soared to its highest levels under the Consulate, both domestically and abroad. In a new plebiscite during the spring of 1802, the French public came out in huge numbers to approve a constitution that made the Consulate permanent, essentially elevating Napoleon to dictator for life. Whereas the plebiscite two years earlier had brought out 1.5 million people to the polls, the new referendum enticed 3.6 million to go and vote (72 percent of all eligible voters). There was no secret ballot in 1802 and few people wanted to openly defy the regime; the constitution gained approval with over 99% of the vote. His broad powers were spelled out in the new constitution: \"Article 1. The French people name, and the Senate proclaims Napoleon-Bonaparte First Consul for Life.\" After 1802, he was generally referred to as Napoleon rather than Bonaparte.\nThe brief peace in Europe allowed Napoleon to focus on the French colonies abroad. Saint-Domingue had managed to acquire a high level of political autonomy during the Revolutionary Wars, with Toussaint Louverture installing himself as de facto dictator by 1801. Napoleon saw his chance to recuperate the formerly wealthy colony when he signed the Treaty of Amiens. During the Revolution, the National Convention voted to abolish slavery in February 1794. Under the terms of Amiens, however, Napoleon agreed to appease British demands by not abolishing slavery in any colonies where the 1794 decree had never been implemented. The resulting Law of 20 May never applied to colonies like Guadeloupe or Guyane, even though rogue generals and other officials used the pretext of peace as an opportunity to reinstate slavery in some of these places. The Law of 20 May officially restored the slave trade to the Caribbean colonies, not slavery itself. Napoleon sent an expedition under General Leclerc designed to reassert control over Sainte-Domingue. Although the French managed to capture Toussaint Louverture, the expedition failed when high rates of disease crippled the French army. In May 1803, the last 8000 French troops left the island and the slaves proclaimed an independent republic that they called Haïti in 1804. Seeing the failure of his colonial efforts, Napoleon decided in 1803 to sell the Louisiana Territory to the United States, instantly doubling the size of the U.S. The selling price in the Louisiana Purchase was less than three cents per acre, a total of $15 million.\n\nThe peace with Britain proved to be uneasy and controversial. Britain did not evacuate Malta as promised and protested against Bonaparte's annexation of Piedmont and his Act of Mediation, which established a new Swiss Confederation. Neither of these territories were covered by Amiens, but they inflamed tensions significantly. The dispute culminated in a declaration of war by Britain in May 1803; Napoleon responded by reassembling the invasion camp at Boulogne.\n\nDuring the Consulate, Napoleon faced several royalist and Jacobin assassination plots, including the \"Conspiration des poignards\" (Dagger plot) in October 1800 and the Plot of the Rue Saint-Nicaise (also known as the \"Infernal Machine\") two months later. In January 1804, his police uncovered an assassination plot against him that involved Moreau and which was ostensibly sponsored by the Bourbon family, the former rulers of France. On the advice of Talleyrand, Napoleon ordered the kidnapping of the Duke of Enghien, violating the sovereignty of Baden. The Duke was quickly executed after a secret military trial, even though he had not been involved in the plot. Enghien's execution infuriated royal courts throughout Europe, becoming one of the contributing political factors for the outbreak of the Napoleonic Wars.\n\nTo expand his power, Napoleon used these assassination plots to justify the creation of an imperial system based on the Roman model. He believed that a Bourbon restoration would be more difficult if his family's succession was entrenched in the constitution. Launching yet another referendum, Napoleon was elected as \"Emperor of the French\" by a tally exceeding 99%. As with the Life Consulate two years earlier, this referendum produced heavy participation, bringing out almost 3.6 million voters to the polls.\n\nA keen observer of Bonaparte's rise to absolute power, Madame de Rémusat, explains that \"men worn out by the turmoil of the Revolution … looked for the domination of an able ruler\" and that \"people believed quite sincerely that Bonaparte, whether as consul or emperor, would exert his authority and save [them] from the perils of anarchy.\"\nNapoleon's coronation took place on 2 December 1804. Two separate crowns were brought for the ceremony: a golden laurel wreath recalling the Roman Empire and a replica of Charlemagne's crown. Napoleon entered the ceremony wearing the laurel wreath and kept it on his head throughout the proceedings. For the official coronation, he raised the Charlemagne crown over his own head in a symbolic gesture, but never placed it on top because he was already wearing the golden wreath. Instead he placed the crown on Josephine's head, the event commemorated in the officially sanctioned painting by Jacques-Louis David. Napoleon was also crowned King of Italy, with the Iron Crown of Lombardy, at the Cathedral of Milan on 26 May 1805. He created eighteen Marshals of the Empire from among his top generals to secure the allegiance of the army on 18 May 1804, the official start of the Empire.\n\nGreat Britain had broken the Peace of Amiens by declaring war on France in May 1803. In December 1804, an Anglo-Swedish agreement became the first step towards the creation of the Third Coalition. By April 1805, Britain had also signed an alliance with Russia. Austria had been defeated by France twice in recent memory and wanted revenge, so it joined the coalition a few months later.\n\nBefore the formation of the Third Coalition, Napoleon had assembled an invasion force, the \"Armée d'Angleterre\", around six camps at Boulogne in Northern France. He intended to use this invasion force to strike at England. They never invaded, but Napoleon's troops received careful and invaluable training for future military operations. The men at Boulogne formed the core for what Napoleon later called \"La Grande Armée\". At the start, this French army had about 200,000 men organized into seven corps, which were large field units that contained 36–40 cannons each and were capable of independent action until other corps could come to the rescue. A single corps properly situated in a strong defensive position could survive at least a day without support, giving the \"Grande Armée\" countless strategic and tactical options on every campaign. On top of these forces, Napoleon created a cavalry reserve of 22,000 organized into two cuirassier divisions, four mounted dragoon divisions, one division of dismounted dragoons, and one of light cavalry, all supported by 24 artillery pieces. By 1805, the \"Grande Armée\" had grown to a force of 350,000 men, who were well equipped, well trained, and led by competent officers.\n\nNapoleon knew that the French fleet could not defeat the Royal Navy in a head-to-head battle, so he planned to lure it away from the English Channel through diversionary tactics. The main strategic idea involved the French Navy escaping from the British blockades of Toulon and Brest and threatening to attack the West Indies. In the face of this attack, it was hoped, the British would weaken their defense of the Western Approaches by sending ships to the Caribbean, allowing a combined Franco-Spanish fleet to take control of the channel long enough for French armies to cross and invade. However, the plan unraveled after the British victory at the Battle of Cape Finisterre in July 1805. French Admiral Villeneuve then retreated to Cádiz instead of linking up with French naval forces at Brest for an attack on the English Channel.\n\nBy August 1805, Napoleon had realised that the strategic situation had changed fundamentally. Facing a potential invasion from his continental enemies, he decided to strike first and turned his army's sights from the English Channel to the Rhine. His basic objective was to destroy the isolated Austrian armies in Southern Germany before their Russian allies could arrive. On 25 September, after great secrecy and feverish marching, 200,000 French troops began to cross the Rhine on a front of . Austrian commander Karl Mack had gathered the greater part of the Austrian army at the fortress of Ulm in Swabia. Napoleon swung his forces to the southeast and the \"Grande Armée\" performed an elaborate wheeling movement that outflanked the Austrian positions. The Ulm Maneuver completely surprised General Mack, who belatedly understood that his army had been cut off. After some minor engagements that culminated in the Battle of Ulm, Mack finally surrendered after realising that there was no way to break out of the French encirclement. For just 2,000 French casualties, Napoleon had managed to capture a total of 60,000 Austrian soldiers through his army's rapid marching. The Ulm Campaign is generally regarded as a strategic masterpiece and was influential in the development of the Schlieffen Plan in the late 19th century. For the French, this spectacular victory on land was soured by the decisive victory that the Royal Navy attained at the Battle of Trafalgar on 21 October. After Trafalgar, Britain had total domination of the seas for the duration of the Napoleonic Wars.\n\nFollowing the Ulm Campaign, French forces managed to capture Vienna in November. The fall of Vienna provided the French a huge bounty as they captured 100,000 muskets, 500 cannons, and the intact bridges across the Danube. At this critical juncture, both Tsar Alexander I and Holy Roman Emperor Francis II decided to engage Napoleon in battle, despite reservations from some of their subordinates. Napoleon sent his army north in pursuit of the Allies, but then ordered his forces to retreat so that he could feign a grave weakness. Desperate to lure the Allies into battle, Napoleon gave every indication in the days preceding the engagement that the French army was in a pitiful state, even abandoning the dominant Pratzen Heights near the village of Austerlitz. At the Battle of Austerlitz, in Moravia on 2 December, he deployed the French army below the Pratzen Heights and deliberately weakened his right flank, enticing the Allies to launch a major assault there in the hopes of rolling up the whole French line. A forced march from Vienna by Marshal Davout and his III Corps plugged the gap left by Napoleon just in time. Meanwhile, the heavy Allied deployment against the French right weakened their center on the Pratzen Heights, which was viciously attacked by the IV Corps of Marshal Soult. With the Allied center demolished, the French swept through both enemy flanks and sent the Allies fleeing chaotically, capturing thousands of prisoners in the process. The battle is often seen as a tactical masterpiece because of the near-perfect execution of a calibrated but dangerous plan – of the same stature as Cannae, the celebrated triumph by Hannibal some 2,000 years before.\n\nThe Allied disaster at Austerlitz significantly shook the faith of Emperor Francis in the British-led war effort. France and Austria agreed to an armistice immediately and the Treaty of Pressburg followed shortly after on 26 December. Pressburg took Austria out of both the war and the Coalition while reinforcing the earlier treaties of Campo Formio and of Lunéville between the two powers. The treaty confirmed the Austrian loss of lands to France in Italy and Bavaria, and lands in Germany to Napoleon's German allies. It also imposed an indemnity of 40 million francs on the defeated Habsburgs and allowed the fleeing Russian troops free passage through hostile territories and back to their home soil. Napoleon went on to say, \"The battle of Austerlitz is the finest of all I have fought\". Frank McLynn suggests that Napoleon was so successful at Austerlitz that he lost touch with reality, and what used to be French foreign policy became a \"personal Napoleonic one\". Vincent Cronin disagrees, stating that Napoleon was not overly ambitious for himself, \"he embodied the ambitions of thirty million Frenchmen\".\n\nNapoleon continued to entertain a grand scheme to establish a French presence in the Middle East in order to put pressure on Britain and Russia, and perhaps form an alliance with the Ottoman Empire. In February 1806, Ottoman Emperor Selim III finally recognised Napoleon as \"Emperor\". He also opted for an alliance with France, calling France \"our sincere and natural ally\". That decision brought the Ottoman Empire into a losing war against Russia and Britain. A Franco-Persian alliance was also formed between Napoleon and the Persian Empire of Fat′h-Ali Shah Qajar. It collapsed in 1807, when France and Russia themselves formed an unexpected alliance. In the end, Napoleon had made no effective alliances in the Middle East.\n\nAfter Austerlitz, Napoleon established the Confederation of the Rhine in 1806. A collection of German states intended to serve as a buffer zone between France and Central Europe, the creation of the Confederation spelled the end of the Holy Roman Empire and significantly alarmed the Prussians. The brazen reorganization of German territory by the French risked threatening Prussian influence in the region, if not eliminating it outright. War fever in Berlin rose steadily throughout the summer of 1806. At the insistence of his court, especially his wife Queen Louise, Frederick William III decided to challenge the French domination of Central Europe by going to war.\n\nThe initial military maneuvers began in September 1806. In a letter to Marshal Soult detailing the plan for the campaign, Napoleon described the essential features of Napoleonic warfare and introduced the phrase \"le bataillon-carré\" (\"square battalion\"). In the \"bataillon-carré\" system, the various corps of the \"Grande Armée\" would march uniformly together in close supporting distance. If any single corps was attacked, the others could quickly spring into action and arrive to help. Napoleon invaded Prussia with 180,000 troops, rapidly marching on the right bank of the River Saale. As in previous campaigns, his fundamental objective was to destroy one opponent before reinforcements from another could tip the balance of the war. Upon learning the whereabouts of the Prussian army, the French swung westwards and crossed the Saale with overwhelming force. At the twin battles of Jena and Auerstedt, fought on 14 October, the French convincingly defeated the Prussians and inflicted heavy casualties. With several major commanders dead or incapacitated, the Prussian king proved incapable of effectively commanding the army, which began to quickly disintegrate. In a vaunted pursuit that epitomized the \"peak of Napoleonic warfare\", according to historian Richard Brooks, the French managed to capture 140,000 soldiers, over 2,000 cannons and hundreds of ammunition wagons, all in a single month. Historian David Chandler wrote of the Prussian forces: \"Never has the morale of any army been more completely shattered\". Despite their overwhelming defeat, the Prussians refused to negotiate with the French until the Russians had an opportunity to enter the fight.\n\nFollowing his triumph, Napoleon imposed the first elements of the Continental System through the Berlin Decree issued in November 1806. The Continental System, which prohibited European nations from trading with Britain, was widely violated throughout his reign. In the next few months, Napoleon marched against the advancing Russian armies through Poland and was involved in the bloody stalemate at the Battle of Eylau in February 1807. After a period of rest and consolidation on both sides, the war restarted in June with an initial struggle at Heilsberg that proved indecisive. On 14 June, however, Napoleon finally obtained an overwhelming victory over the Russians at the Battle of Friedland, wiping out the majority of the Russian army in a very bloody struggle. The scale of their defeat convinced the Russians to make peace with the French. On 19 June, Czar Alexander sent an envoy to seek an armistice with Napoleon. The latter assured the envoy that the Vistula River represented the natural borders between French and Russian influence in Europe. On that basis, the two emperors began peace negotiations at the town of Tilsit after meeting on an iconic raft on the River Niemen. The very first thing Alexander said to Napoleon was probably well-calibrated: \"I hate the English as much as you do\".\n\nAlexander faced pressure from his brother, Duke Constantine, to make peace with Napoleon. Given the victory he had just achieved, the French emperor offered the Russians relatively lenient terms – demanding that Russia join the Continental System, withdraw its forces from Wallachia and Moldavia, and hand over the Ionian Islands to France. By contrast, Napoleon dictated very harsh peace terms for Prussia, despite the ceaseless exhortations of Queen Louise. Wiping out half of Prussian territories from the map, Napoleon created a new kingdom of 1,100 square miles called Westphalia and appointed his young brother Jérôme as its monarch. Prussia's humiliating treatment at Tilsit caused a deep and bitter antagonism which festered as the Napoleonic era progressed. Moreover, Alexander's pretensions at friendship with Napoleon led the latter to seriously misjudge the true intentions of his Russian counterpart, who would violate numerous provisions of the treaty in the next few years. Despite these problems, the Treaties of Tilsit at last gave Napoleon a respite from war and allowed him to return to France, which he had not seen in over 300 days.\n\nThe settlements at Tilsit gave Napoleon time to organize his empire. One of his major objectives became enforcing the Continental System against the British. He decided to focus his attention on the Kingdom of Portugal, which consistently violated his trade prohibitions. After defeat in the War of the Oranges in 1801, Portugal adopted a double-sided policy. At first, John VI agreed to close his ports to British trade. The situation changed dramatically after the Franco-Spanish defeat at Trafalgar; John grew bolder and officially resumed diplomatic and trade relations with Britain.\n\nUnhappy with this change of policy by the Portuguese government, Napoleon negotiated a secret treaty with Charles IV of Spain and sent an army to invade Portugal. On 17 October 1807, 24,000 French troops under General Junot crossed the Pyrenees with Spanish cooperation and headed towards Portugal to enforce Napoleon's orders. This attack was the first step in what would eventually become the Peninsular War, a six-year struggle that significantly sapped French strength. Throughout the winter of 1808, French agents became increasingly involved in Spanish internal affairs, attempting to incite discord between members of the Spanish royal family. On 16 February 1808, secret French machinations finally materialized when Napoleon announced that he would intervene to mediate between the rival political factions in the country. Marshal Murat led 120,000 troops into Spain and the French arrived in Madrid on 24 March, where wild riots against the occupation erupted just a few weeks later. Napoleon appointed his brother, Joseph Bonaparte, as the new King of Spain in the summer of 1808. The appointment enraged a heavily religious and conservative Spanish population. Resistance to French aggression soon spread throughout the country. The shocking French defeat at the Battle of Bailén in July gave hope to Napoleon's enemies and partly persuaded the French emperor to intervene in person.\n\nBefore going to Iberia, Napoleon decided to address several lingering issues with the Russians. At the Congress of Erfurt in October 1808, Napoleon hoped to keep Russia on his side during the upcoming struggle in Spain and during any potential conflict against Austria. The two sides reached an agreement, the Erfurt Convention, that called upon Britain to cease its war against France, that recognized the Russian conquest of Finland from Sweden, and that affirmed Russian support for France in a possible war against Austria \"to the best of its ability\". Napoleon then returned to France and prepared for war. The \"Grande Armée\", under the Emperor's personal command, rapidly crossed the Ebro River in November 1808 and inflicted a series of crushing defeats against the Spanish forces. After clearing the last Spanish force guarding the capital at Somosierra, Napoleon entered Madrid on 4 December with 80,000 troops. He then unleashed his soldiers against Moore and the British forces. The British were swiftly driven to the coast, and they withdrew from Spain entirely after a last stand at the Battle of Corunna in January 1809.\n\nNapoleon would end up leaving Iberia in order to deal with the Austrians in Central Europe, but the Peninsular War continued on long after his absence. He never returned to Spain after the 1808 campaign. Several months after Corunna, the British sent another army to the peninsula under the future Duke of Wellington. The war then settled into a complex and asymmetric strategic deadlock where all sides struggled to gain the upper hand. The highlight of the conflict became the brutal \"guerrilla warfare\" that engulfed much of the Spanish countryside. Both sides committed the worst atrocities of the Napoleonic Wars during this phase of the conflict. The vicious guerrilla fighting in Spain, largely absent from the French campaigns in Central Europe, severely disrupted the French lines of supply and communication. Although France maintained roughly 300,000 troops in Iberia during the Peninsular War, the vast majority were tied down to garrison duty and to intelligence operations. The French were never able to concentrate all of their forces effectively, prolonging the war until events elsewhere in Europe finally turned the tide in favor of the Allies. After the invasion of Russia in 1812, the number of French troops in Spain vastly declined as Napoleon needed reinforcements to conserve his strategic position in Europe. By 1814, after scores of battles and sieges throughout Iberia, the Allies had managed to push the French out of the peninsula.\n\nThe impact of the Napoleonic invasion of Spain and ousting of the Spanish Bourbon monarchy in favor of his brother Joseph had an enormous impact on the Spanish empire. In Spanish America many local elites formed juntas and set up mechanisms to rule in the name of Ferdinand VII of Spain, whom they considered the legitimate Spanish monarch. The outbreak of the Spanish American wars of independence in most of the empire was a result of Napoleon's destabilizing actions in Spain and led to the rise of strongmen in the wake of these wars.\n\nAfter four years on the sidelines, Austria sought another war with France to avenge its recent defeats. Austria could not count on Russian support because the latter was at war with Britain, Sweden, and the Ottoman Empire in 1809. Frederick William of Prussia initially promised to help the Austrians, but reneged before conflict began. A report from the Austrian finance minister suggested that the treasury would run out of money by the middle of 1809 if the large army that the Austrians had formed since the Third Coalition remained mobilized. Although Archduke Charles warned that the Austrians were not ready for another showdown with Napoleon, a stance that landed him in the so-called \"peace party\", he did not want to see the army demobilized either. On 8 February 1809, the advocates for war finally succeeded when the Imperial Government secretly decided on another confrontation against the French.\n\nIn the early morning of 10 April, leading elements of the Austrian army crossed the Inn River and invaded Bavaria. The early Austrian attack surprised the French; Napoleon himself was still in Paris when he heard about the invasion. He arrived at Donauwörth on the 17th to find the \"Grande Armée\" in a dangerous position, with its two wings separated by and joined together by a thin cordon of Bavarian troops. Charles pressed the left wing of the French army and hurled his men towards the III Corps of Marshal Davout. In response, Napoleon came up with a plan to cut off the Austrians in the celebrated \"Landshut Maneuver\". He realigned the axis of his army and marched his soldiers towards the town of Eckmühl. The French scored a convincing win in the resulting Battle of Eckmühl, forcing Charles to withdraw his forces over the Danube and into Bohemia. On 13 May, Vienna fell for the second time in four years, although the war continued since most of the Austrian army had survived the initial engagements in Southern Germany.\n\nBy 17 May, the main Austrian army under Charles had arrived on the Marchfeld. Charles kept the bulk of his troops several miles away from the river bank in hopes of concentrating them at the point where Napoleon decided to cross. On 21 May, the French made their first major effort to cross the Danube, precipitating the Battle of Aspern-Essling. The Austrians enjoyed a comfortable numerical superiority over the French throughout the battle; on the first day, Charles disposed of 110,000 soldiers against only 31,000 commanded by Napoleon. By the second day, reinforcements had boosted French numbers up to 70,000. The battle was characterized by a vicious back-and-forth struggle for the two villages of Aspern and Essling, the focal points of the French bridgehead. By the end of the fighting, the French had lost Aspern but still controlled Essling. A sustained Austrian artillery bombardment eventually convinced Napoleon to withdraw his forces back onto Lobau Island. Both sides inflicted about 23,000 casualties on each other. It was the first defeat Napoleon suffered in a major set-piece battle, and it caused excitement throughout many parts of Europe because it proved that he could be beaten on the battlefield.\n\nAfter the setback at Aspern-Essling, Napoleon took more than six weeks in planning and preparing for contingencies before he made another attempt at crossing the Danube. From 30 June to the early days of July, the French recrossed the Danube in strength, with more than 180,000 troops marching across the Marchfeld towards the Austrians. Charles received the French with 150,000 of his own men. In the ensuing Battle of Wagram, which also lasted two days, Napoleon commanded his forces in what was the largest battle of his career up until then. Napoleon finished off the battle with a concentrated central thrust that punctured a hole in the Austrian army and forced Charles to retreat. Austrian losses were very heavy, reaching well over 40,000 casualties. The French were too exhausted to pursue the Austrians immediately, but Napoleon eventually caught up with Charles at Znaim and the latter signed an armistice on 12 July.\n\nIn the Kingdom of Holland, the British launched the Walcheren Campaign to open up a second front in the war and to relieve the pressure on the Austrians. The British army only landed at Walcheren on 30 July, by which point the Austrians had already been defeated. The Walcheren Campaign was characterized by little fighting but heavy casualties thanks to the popularly dubbed \"Walcheren Fever\". Over 4000 British troops were lost in a bungled campaign, and the rest withdrew in December 1809. The main strategic result from the campaign became the delayed political settlement between the French and the Austrians. Emperor Francis wanted to wait and see how the British performed in their theater before entering into negotiations with Napoleon. Once it became apparent that the British were going nowhere, the Austrians agreed to peace talks.\n\nThe resulting Treaty of Schönbrunn in October 1809 was the harshest that France had imposed on Austria in recent memory. Metternich and Archduke Charles had the preservation of the Habsburg Empire as their fundamental goal, and to this end they succeeded by making Napoleon seek more modest goals in return for promises of friendship between the two powers. Nevertheless, while most of the hereditary lands remained a part of the Habsburg realm, France received Carinthia, Carniola, and the Adriatic ports, while Galicia was given to the Poles and the Salzburg area of the Tyrol went to the Bavarians. Austria lost over three million subjects, about one-fifth of her total population, as a result of these territorial changes. Although fighting in Iberia continued, the War of the Fifth Coalition would be the last major conflict on the European continent for the next three years.\n\nNapoleon turned his focus to domestic affairs after the war. Empress Joséphine had still not given birth to a child from Napoleon, who became worried about the future of his empire following his death. Desperate for a legitimate heir, Napoleon divorced Joséphine on 10 January 1810 and started looking for a new wife. Hoping to cement the recent alliance with Austria through a family connection, Napoleon married the Archduchess Marie Louise, who was 18 years old at the time. On 20 March 1811, Marie Louise gave birth to a baby boy, whom Napoleon made heir apparent and bestowed the title of \"King of Rome\". His son never actually ruled the empire, but historians still refer to him as \"Napoleon II\".\n\nIn 1808, Napoleon and Czar Alexander met at the Congress of Erfurt to preserve the Russo-French alliance. The leaders had a friendly personal relationship after their first meeting at Tilsit in 1807. By 1811, however, tensions had increased and Alexander was under pressure from the Russian nobility to break off the alliance. A major strain on the relationship between the two nations became the regular violations of the Continental System by the Russians, which led Napoleon to threaten Alexander with serious consequences if he formed an alliance with Britain.\n\nBy 1812, advisers to Alexander suggested the possibility of an invasion of the French Empire and the recapture of Poland. On receipt of intelligence reports on Russia's war preparations, Napoleon expanded his \"Grande Armée\" to more than 450,000 men. He ignored repeated advice against an invasion of the Russian heartland and prepared for an offensive campaign; on 24 June 1812 the invasion commenced.\n\nIn an attempt to gain increased support from Polish nationalists and patriots, Napoleon termed the war the \"Second Polish War\"—the \"First Polish War\" had been the Bar Confederation uprising by Polish nobles against Russia in 1768. Polish patriots wanted the Russian part of Poland to be joined with the Duchy of Warsaw and an independent Poland created. This was rejected by Napoleon, who stated he had promised his ally Austria this would not happen. Napoleon refused to manumit the Russian serfs because of concerns this might provoke a reaction in his army's rear. The serfs later committed atrocities against French soldiers during France's retreat.\n\nThe Russians avoided Napoleon's objective of a decisive engagement and instead retreated deeper into Russia. A brief attempt at resistance was made at Smolensk in August; the Russians were defeated in a series of battles, and Napoleon resumed his advance. The Russians again avoided battle, although in a few cases this was only achieved because Napoleon uncharacteristically hesitated to attack when the opportunity arose. Owing to the Russian army's scorched earth tactics, the French found it increasingly difficult to forage food for themselves and their horses.\n\nThe Russians eventually offered battle outside Moscow on 7 September: the Battle of Borodino resulted in approximately 44,000 Russian and 35,000 French dead, wounded or captured, and may have been the bloodiest day of battle in history up to that point in time. Although the French had won, the Russian army had accepted, and withstood, the major battle Napoleon had hoped would be decisive. Napoleon's own account was: \"The most terrible of all my battles was the one before Moscow. The French showed themselves to be worthy of victory, but the Russians showed themselves worthy of being invincible\".\n\nThe Russian army withdrew and retreated past Moscow. Napoleon entered the city, assuming its fall would end the war and Alexander would negotiate peace. However, on orders of the city's governor Feodor Rostopchin, rather than capitulation, Moscow was burned. After five weeks, Napoleon and his army left. In early November Napoleon got concerned about loss of control back in France after the Malet coup of 1812. His army walked through snow up to their knees, and nearly 10,000 men and horses froze to death on the night of 8/9 November alone. After the Battle of Berezina Napoleon managed to escape but had to abandon much of the remaining artillery and baggage train. On 5 December, shortly before arriving in Vilnius, Napoleon left the army in a sledge.\n\nThe French suffered in the course of a ruinous retreat, including from the harshness of the Russian Winter. The Armée had begun as over 400,000 frontline troops, with fewer than 40,000 crossing the Berezina River in November 1812. The Russians had lost 150,000 in battle and hundreds of thousands of civilians.\n\nThere was a lull in fighting over the winter of 1812–13 while both the Russians and the French rebuilt their forces; Napoleon was able to field 350,000 troops. Heartened by France's loss in Russia, Prussia joined with Austria, Sweden, Russia, Great Britain, Spain, and Portugal in a new coalition. Napoleon assumed command in Germany and inflicted a series of defeats on the Coalition culminating in the Battle of Dresden in August 1813.\n\nDespite these successes, the numbers continued to mount against Napoleon, and the French army was pinned down by a force twice its size and lost at the Battle of Leipzig. This was by far the largest battle of the Napoleonic Wars and cost more than 90,000 casualties in total.\n\nThe Allies offered peace terms in the Frankfurt proposals in November 1813. Napoleon would remain as Emperor of France, but it would be reduced to its \"natural frontiers\". That meant that France could retain control of Belgium, Savoy and the Rhineland (the west bank of the Rhine River), while giving up control of all the rest, including all of Spain and the Netherlands, and most of Italy and Germany. Metternich told Napoleon these were the best terms the Allies were likely to offer; after further victories, the terms would be harsher and harsher. Metternich's motivation was to maintain France as a balance against Russian threats, while ending the highly destabilizing series of wars.\n\nNapoleon, expecting to win the war, delayed too long and lost this opportunity; by December the Allies had withdrawn the offer. When his back was to the wall in 1814 he tried to reopen peace negotiations on the basis of accepting the Frankfurt proposals. The Allies now had new, harsher terms that included the retreat of France to its 1791 boundaries, which meant the loss of Belgium. Napoleon would remain Emperor, however he rejected the term. The British wanted Napoleon permanently removed, and they prevailed, but Napoleon adamantly refused.\n\nNapoleon withdrew back into France, his army reduced to 70,000 soldiers and little cavalry; he faced more than three times as many Allied troops. The French were surrounded: British armies pressed from the south, and other Coalition forces positioned to attack from the German states. Napoleon won a series of victories in the Six Days' Campaign, though these were not significant enough to turn the tide. The leaders of Paris surrendered to the Coalition in March 1814.\n\nOn 1 April, Alexander addressed the Sénat conservateur. Long docile to Napoleon, under Talleyrand's prodding it had turned against him. Alexander told the Sénat that the Allies were fighting against Napoleon, not France, and they were prepared to offer honorable peace terms if Napoleon were removed from power. The next day, the Sénat passed the Acte de déchéance de l'Empereur (\"Emperor's Demise Act\"), which declared Napoleon deposed. Napoleon had advanced as far as Fontainebleau when he learned that Paris was lost. When Napoleon proposed the army march on the capital, his senior officers and marshals mutinied. On 4 April, led by Ney, they confronted Napoleon. Napoleon asserted the army would follow him, and Ney replied the army would follow its generals. While the ordinary soldiers and regimental officers wanted to fight on, without any senior officers or marshals any prospective invasion of Paris would have been impossible. Bowing to the inevitable, on 4 April Napoleon abdicated in favour of his son, with Marie Louise as regent. However, the Allies refused to accept this under prodding from Alexander, who feared that Napoleon might find an excuse to retake the throne. Napoleon was then forced to announce his unconditional abdication only two days later.\n\nIn the Treaty of Fontainebleau, the Allies exiled Napoleon to Elba, an island of 12,000 inhabitants in the Mediterranean, off the Tuscan coast. They gave him sovereignty over the island and allowed him to retain the title of \"Emperor\". Napoleon attempted suicide with a pill he had carried after nearly being captured by the Russians during the retreat from Moscow. Its potency had weakened with age, however, and he survived to be exiled, while his wife and son took refuge in Austria. In the first few months on Elba he created a small navy and army, developed the iron mines, oversaw the construction of new roads, issued decrees on modern agricultural methods, and overhauled the island's legal and educational system.\n\nA few months into his exile, Napoleon learned that his ex-wife Josephine had died in France. He was devastated by the news, locking himself in his room and refusing to leave for two days.\n\nSeparated from his wife and son, who had returned to Austria, cut off from the allowance guaranteed to him by the Treaty of Fontainebleau, and aware of rumours he was about to be banished to a remote island in the Atlantic Ocean, Napoleon escaped from Elba, in the brig \"Inconstant\" on 26 February 1815 with 700 men. Two days later, he landed on the French mainland at Golfe-Juan and started heading north.\n\nThe 5th Regiment was sent to intercept him and made contact just south of Grenoble on 7 March 1815. Napoleon approached the regiment alone, dismounted his horse and, when he was within gunshot range, shouted to the soldiers, \"Here I am. Kill your Emperor, if you wish\". The soldiers quickly responded with, \"Vive L'Empereur!\" Ney, who had boasted to the restored Bourbon king, Louis XVIII, that he would bring Napoleon to Paris in an iron cage, affectionately kissed his former emperor and forgot his oath of allegiance to the Bourbon monarch. The two then marched together towards Paris with a growing army. The unpopular Louis XVIII fled to Belgium after realizing he had little political support. On 13 March, the powers at the Congress of Vienna declared Napoleon an outlaw. Four days later, Great Britain, Russia, Austria, and Prussia each pledged to put 150,000 men into the field to end his rule.\n\nNapoleon arrived in Paris on 20 March and governed for a period now called the Hundred Days. By the start of June the armed forces available to him had reached 200,000, and he decided to go on the offensive to attempt to drive a wedge between the oncoming British and Prussian armies. The French Army of the North crossed the frontier into the United Kingdom of the Netherlands, in modern-day Belgium.\n\nNapoleon's forces fought the Coalition armies, commanded by the Duke of Wellington and Gebhard Leberecht von Blücher, at the Battle of Waterloo on 18 June 1815. Wellington's army withstood repeated attacks by the French and drove them from the field while the Prussians arrived in force and broke through Napoleon's right flank.\n\nNapoleon returned to Paris and found that both the legislature and the people had turned against him. Realizing his position was untenable, he abdicated on 22 June in favour of his son. He left Paris three days later and settled at Josephine's former palace in Malmaison (on the western bank of the Seine about west of Paris). Even as Napoleon travelled to Paris, the Coalition forces swept through France (arriving in the vicinity of Paris on 29 June), with the stated intent of restoring Louis XVIII to the French throne.\n\nWhen Napoleon heard that Prussian troops had orders to capture him dead or alive, he fled to Rochefort, considering an escape to the United States. British ships were blocking every port. Napoleon demanded asylum from the British Captain Frederick Maitland on on 15 July 1815.\n\nThe British kept Napoleon on the island of Saint Helena in the Atlantic Ocean, from the west coast of Africa. They also took the precaution of sending a garrison of soldiers, with an experienced officer (Edward Nicolls), to uninhabited Ascension Island, which lay between St. Helena and Europe.\n\nNapoleon was moved to Longwood House on Saint Helena in December 1815; it had fallen into disrepair, and the location was damp, windswept and unhealthy. \"The Times\" published articles insinuating the British government was trying to hasten his death. Napoleon often complained of the living conditions in letters to the governor and his custodian, Hudson Lowe, while his attendants complained of \"colds, catarrhs, damp floors and poor provisions.\" It has been speculated by modern scientists that his later illness arose from arsenic poisoning caused by copper arsenite in the wallpaper at Longwood House.\n\nWith a small cadre of followers, Napoleon dictated his memoirs and grumbled about conditions. Lowe cut Napoleon's expenditure, ruled that no gifts were allowed if they mentioned his imperial status, and made his supporters sign a guarantee they would stay with the prisoner indefinitely.\n\nWhile in exile, Napoleon wrote a book about Julius Caesar, one of his great heroes. He also studied English under the tutelage of Count Emmanuel de Las Cases with the main aim of being able to read English newspapers and books, as access to French newspapers and books was heavily restricted to him on Saint Helena.\n\nThere were rumours of plots and even of his escape, but in reality no serious attempts were made. For English poet Lord Byron, Napoleon was the epitome of the Romantic hero, the persecuted, lonely, and flawed genius.\n\nNapoleon's personal physician, Barry O'Meara, warned London that his declining state of health was mainly caused by the harsh treatment. Napoleon confined himself for months on end in his damp and wretched habitation of Longwood.\n\nIn February 1821, Napoleon's health began to deteriorate rapidly, and he reconciled with the Catholic Church. He died on 5 May 1821, after confession, Extreme Unction and Viaticum in the presence of Father Ange Vignali. His last words were, \"France, l'armée, tête d'armée, Joséphine\" (\"France, the army, head of the army, Joséphine\").\n\nNapoleon's original death mask was created around 6 May, although it is not clear which doctor created it. In his will, he had asked to be buried on the banks of the Seine, but the British governor said he should be buried on Saint Helena, in the Valley of the Willows.\n\nIn 1840, Louis Philippe I obtained permission from the British to return Napoleon's remains to France. On 15 December 1840, a state funeral was held. The hearse proceeded from the Arc de Triomphe down the Champs-Élysées, across the Place de la Concorde to the Esplanade des Invalides and then to the cupola in St Jérôme's Chapel, where it remained until the tomb designed by Louis Visconti was completed.\n\nIn 1861, Napoleon's remains were entombed in a porphyry stone sarcophagus in the crypt under the dome at Les Invalides.\n\nThe cause of his death has been debated. Napoleon's physician, François Carlo Antommarchi, led the autopsy, which found the cause of death to be stomach cancer. Antommarchi did not sign the official report. Napoleon's father had died of stomach cancer, although this was apparently unknown at the time of the autopsy. Antommarchi found evidence of a stomach ulcer; this was the most convenient explanation for the British, who wanted to avoid criticism over their care of Napoleon.\nIn 1955, the diaries of Napoleon's valet, Louis Marchand, were published. His description of Napoleon in the months before his death led Sten Forshufvud in a 1961 paper in \"Nature\" to put forward other causes for his death, including deliberate arsenic poisoning. Arsenic was used as a poison during the era because it was undetectable when administered over a long period. Forshufvud, in a 1978 book with Ben Weider, noted that Napoleon's body was found to be well preserved when moved in 1840. Arsenic is a strong preservative, and therefore this supported the poisoning hypothesis. Forshufvud and Weider observed that Napoleon had attempted to quench abnormal thirst by drinking large amounts of orgeat syrup that contained cyanide compounds in the almonds used for flavouring.\n\nThey maintained that the potassium tartrate used in his treatment prevented his stomach from expelling these compounds and that his thirst was a symptom of the poison. Their hypothesis was that the calomel given to Napoleon became an overdose, which killed him and left extensive tissue damage behind. According to a 2007 article, the type of arsenic found in Napoleon's hair shafts was mineral, the most toxic, and according to toxicologist Patrick Kintz, this supported the conclusion that he was murdered.\n\nThere have been modern studies that have supported the original autopsy finding. In a 2008 study, researchers analysed samples of Napoleon's hair from throughout his life, as well as samples from his family and other contemporaries. All samples had high levels of arsenic, approximately 100 times higher than the current average. According to these researchers, Napoleon's body was already heavily contaminated with arsenic as a boy, and the high arsenic concentration in his hair was not caused by intentional poisoning; people were constantly exposed to arsenic from glues and dyes throughout their lives. Studies published in 2007 and 2008 dismissed evidence of arsenic poisoning, and confirmed evidence of peptic ulcer and gastric cancer as the cause of death.\n\nNapoleon's baptism took place in Ajaccio on 21 July 1771; he was piously raised as a Catholic but he never developed much faith. As an adult, Napoleon was a deist. Napoleon's deity was an absent and distant God. However he had a keen appreciation of the power of organised religion in social and political affairs, and paid a great deal of attention to bending it to his purposes. He noted the influence of Catholicism's rituals and splendors. Napoleon had a civil marriage with Joséphine de Beauharnais, without religious ceremony. Napoleon was crowned Emperor on 2 December 1804 at Notre-Dame de Paris in a ceremony presided over by Pope Pius VII. On the eve of the Coronation ceremony, and at the insistence of Pope Pius VII, a private religious wedding ceremony of Napoleon and Joséphine was celebrated. Cardinal Fesch performed the wedding. This marriage was annulled by tribunals under Napoleon's control in January 1810. On 1 April 1810, Napoleon married the Austrian princess Marie Louise in a Catholic ceremony. During his brother's rule in Spain, he abolished the Spanish Inquisition in 1813. Napoleon was excommunicated by the Catholic Church, but later reconciled with the Church before his death in 1821.\n\nSeeking national reconciliation between revolutionaries and Catholics, the Concordat of 1801 was signed on 15 July 1801 between Napoleon and Pope Pius VII. It solidified the Roman Catholic Church as the majority church of France and brought back most of its civil status. The hostility of devout Catholics against the state had now largely been resolved. It did not restore the vast church lands and endowments that had been seized during the revolution and sold off. As a part of the Concordat, he presented another set of laws called the Organic Articles.\n\nWhile the Concordat restored much power to the papacy, the balance of church–state relations had tilted firmly in Napoleon's favour. He selected the bishops and supervised church finances. Napoleon and the pope both found the Concordat useful. Similar arrangements were made with the Church in territories controlled by Napoleon, especially Italy and Germany. Now, Napoleon could win favor with the Catholics while also controlling Rome in a political sense. Napoleon said in April 1801, \"Skillful conquerors have not got entangled with priests. They can both contain them and use them\". French children were issued a catechism that taught them to love and respect Napoleon.\n\nIn 1809, under Napoleon's orders, Pope Pius VII was placed under arrest in Italy, and in 1812 the prisoner Pontiff was transferred to France, being held in the Palace of Fontainebleau. Because the arrest was made in a clandestine manner, some sources describe it as a kidnapping. The Pope was only released in 1814 when the Allies invaded France. In January 1813, Napoleon personally forced the Pope to sign a humiliating \"Concordat of Fontainebleau\". The 1813 document was later repudiated by the Pontiff.\n\nNapoleon emancipated Jews, as well as Protestants in Catholic countries and Catholics in Protestant countries, from laws which restricted them to ghettos, and he expanded their rights to property, worship, and careers. Despite the anti-semitic reaction to Napoleon's policies from foreign governments and within France, he believed emancipation would benefit France by attracting Jews to the country given the restrictions they faced elsewhere.\n\nIn 1806 an Assembly of Jewish notables was gathered by Napoleon to discuss 12 questions broadly dealing with the relations between Jews, Christians and other issues dealing with the Jewish ability to integrate into the general French society. Later, after the questions were answered in a satisfactory way according to the Emperor, a \"great Sanhedrin\" was brought together to transform the answers into decisions that would form the basis of the future status of the Jews in France and the rest of the Empire Napoleon was building.\n\nHe stated, \"I will never accept any proposals that will obligate the Jewish people to leave France, because to me the Jews are the same as any other citizen in our country. It takes weakness to chase them out of the country, but it takes strength to assimilate them\". He was seen as so favourable to the Jews that the Russian Orthodox Church formally condemned him as \"Antichrist and the Enemy of God\".\n\nOne year after the final meeting of the Sanhedrin, on 17 March 1808, Napoleon placed the Jews on probation. Several new laws restricting the citizenship the Jews had been offered 17 years previously were instituted at that time. However, despite pressure from leaders of a number of Christian communities to refrain from granting Jews emancipation, within one year of the issue of the new restrictions, they were once again lifted in response to the appeal of Jews from all over France.\n\nHistorians emphasize the strength of the ambition that took Napoleon from an obscure village to command of most of Europe. In-depth academic studies about his early life conclude that up until age 2, he had a \"gentle disposition\". His older brother, Joseph, frequently received their mother's attention which made Napoleon more assertive and approval-driven. During his early schooling years he would be harshly bullied by classmates for his Corsican identity and control of the French language. To withstand the stress he became domineering, eventually developing an inferiority complex.\n\nGeorge F. E. Rudé stresses his \"rare combination of will, intellect and physical vigour\". In one-on-one situations he typically had a hypnotic effect on people, seemingly bending the strongest leaders to his will. He understood military technology, but was not an innovator in that regard. He was an innovator in using the financial, bureaucratic, and diplomatic resources of France. He could rapidly dictate a series of complex commands to his subordinates, keeping in mind where major units were expected to be at each future point, and like a chess master, \"seeing\" the best plays moves ahead.\n\nNapoleon maintained strict, efficient work habits, prioritizing what needed to be done. He cheated at cards, but repaid the losses; he had to win at everything he attempted. He kept relays of staff and secretaries at work. Unlike many generals, Napoleon did not examine history to ask what Hannibal or Alexander or anyone else did in a similar situation. Critics said he won many battles simply because of luck; Napoleon responded, \"Give me lucky generals\", aware that \"luck\" comes to leaders who recognize opportunity, and seize it. Dwyer states that Napoleon's victories at Austerlitz and Jena in 1805–06 heightened his sense of self-grandiosity, leaving him even more certain of his destiny and invincibility.\n\nIn terms of influence on events, it was more than Napoleon's personality that took effect. He reorganized France itself to supply the men and money needed for wars. He inspired his men—Wellington said his presence on the battlefield was worth 40,000 soldiers, for he inspired confidence from privates to field marshals. He also unnerved the enemy. At the Battle of Auerstadt in 1806, King Frederick William III of Prussia outnumbered the French by 63,000 to 27,000; however, when he was told, mistakenly, that Napoleon was in command, he ordered a hasty retreat that turned into a rout. The force of his personality neutralized material difficulties as his soldiers fought with the confidence that with Napoleon in charge they would surely win.\n\nNapoleon has become a worldwide cultural icon who symbolises military genius and political power. Martin van Creveld described him as \"the most competent human being who ever lived\". Since his death, many towns, streets, ships, and even cartoon characters have been named after him. He has been portrayed in hundreds of films and discussed in hundreds of thousands of books and articles.\n\nWhen met in person, many of his contemporaries were surprised by his apparently unremarkable physical appearance in contrast to his significant deeds and reputation, especially in his youth, when he was consistently described as small and thin. Joseph Farington, who observed Napoleon personally in 1802, commented that \"Samuel Rogers stood a little way from me and ... seemed to be disappointed in the look of [Napoleon's] countenance [face] and said it was that of a little Italian.\" Farington said Napoleon's eyes were \"lighter, and more of a grey, than I should have expected from his complexion\", that \"His person is below middle size\", and that \"his general aspect was milder than I had before thought it.\" A personal friend of Napoleon's said that when he first met him in Brienne-le-Château as a young man, Napoleon was only notable \"for the dark color of his complexion, for his piercing and scrutinising glance, and for the style of his conversation\"; he also said that Napoleon was personally a serious and somber man: \"his conversation bore the appearance of ill-humor, and he was certainly not very amiable.\" Johann Ludwig Wurstemberger, who accompanied Napoleon from Camp Fornio in 1797 and on the Swiss campaign of 1798, noted that \"Bonaparte was rather slight and emaciated-looking; his face, too, was very thin, with a dark complexion ... his black, unpowdered hair hung down evenly over both shoulders\", but that, despite his slight and unkempt appearance, \"His looks and expression were earnest and powerful.\" Denis Davydov met him personally and considered him remarkably average in appearance: \"His face was slightly swarthy, with regular features. His nose was not very large, but straight, with a slight, hardly noticeable bend. The hair on his head was dark reddish-blond; his eyebrows and eyelashes were much darker than the colour of his hair, and his blue eyes, set off by the almost black lashes, gave him a most pleasing expression ... The man I saw was of short stature, just over five feet tall, rather heavy although he was only 37 years old.\"\n\nDuring the Napoleonic Wars he was taken seriously by the British press as a dangerous tyrant, poised to invade. Napoleon was mocked in British newspapers as a short tempered small man and he was nicknamed \"Little Boney in a strong fit\". A nursery rhyme warned children that Bonaparte ravenously ate naughty people; the \"bogeyman\". At , he was the height of an average French male but short for an aristocrat or officer (part of why he was assigned to the artillery, since at the time the infantry and cavalry required more commanding figures). It is possible he was taller at due to the difference in the French measurement of inches. Some historians believe that the reason for the mistake about his size at death came from use of an obsolete old French yardstick (a French foot equals 33 cm, while an English foot equals 30.47 cm). Napoleon was a champion of the metric system and had no use for the old yardsticks. It is more likely that he was , the height he was measured at on St. Helena (a British island), since he would have most likely been measured with an English yardstick rather than a yardstick of the Old French Regime. Napoleon surrounded himself with tall bodyguards and was affectionately nicknamed \"le petit caporal\" (the little corporal), reflecting his reported camaraderie with his soldiers rather than his height.\n\nWhen he became First Consul and later Emperor, Napoleon eschewed his general's uniform and habitually wore the green colonel uniform (non-Hussar) of a colonel of the Chasseur à Cheval of the Imperial Guard, the regiment that served as his personal escort many times, with a large bicorne. He also habitually wore (usually on Sundays) the blue uniform of a colonel of the Imperial Guard Foot Grenadiers (blue with white facings and red cuffs). He also wore his Légion d'honneur star, medal and ribbon, and the Order of the Iron Crown decorations, white French-style culottes and white stockings. This was in contrast to the complex uniforms with many decorations of his marshals and those around him.\n\nIn his later years he gained quite a bit of weight and had a complexion considered pale or sallow, something contemporaries took note of. Novelist Paul de Kock, who saw him in 1811 on the balcony of the Tuileries, called Napoleon \"yellow, obese, and bloated\". A British captain who met him in 1815 stated \"I felt very much disappointed, as I believe everyone else did, in his appearance ... He is fat, rather what we call pot-bellied, and although his leg is well shaped, it is rather clumsy ... He is very sallow, with light grey eyes, and rather thin, greasy-looking brown hair, and altogether a very nasty, priestlike-looking fellow.\"\n\nThe stock character of Napoleon is a comically short \"petty tyrant\" and this has become a cliché in popular culture. He is often portrayed wearing a large bicorne hat with a hand-in-waistcoat gesture—a reference to the painting produced in 1812 by Jacques-Louis David. In 1908 Alfred Adler, a psychologist, cited Napoleon to describe an inferiority complex in which short people adopt an over-aggressive behaviour to compensate for lack of height; this inspired the term \"Napoleon complex\".\n\nNapoleon instituted various reforms, such as higher education, a tax code, road and sewer systems, and established the Banque de France, the first central bank in French history. He negotiated the Concordat of 1801 with the Catholic Church, which sought to reconcile the mostly Catholic population to his regime. It was presented alongside the Organic Articles, which regulated public worship in France. He dissolved the Holy Roman Empire prior to German Unification later in the 19th century. The sale of the Louisiana Territory to the United States doubled the size of the United States.\n\nIn May 1802, he instituted the Legion of Honour, a substitute for the old royalist decorations and orders of chivalry, to encourage civilian and military achievements; the order is still the highest decoration in France.\n\nNapoleon's set of civil laws, the \"Code Civil\"—now often known as the Napoleonic Code—was prepared by committees of legal experts under the supervision of Jean Jacques Régis de Cambacérès, the \"Second Consul\". Napoleon participated actively in the sessions of the Council of State that revised the drafts. The development of the code was a fundamental change in the nature of the civil law legal system with its stress on clearly written and accessible law. Other codes (\"Les cinq codes\") were commissioned by Napoleon to codify criminal and commerce law; a Code of Criminal Instruction was published, which enacted rules of due process.\n\nThe Napoleonic code was adopted throughout much of Continental Europe, though only in the lands he conquered, and remained in force after Napoleon's defeat. Napoleon said: \"My true glory is not to have won forty battles ... Waterloo will erase the memory of so many victories. ... But ... what will live forever, is my Civil Code\". The Code influences a quarter of the world's jurisdictions such as that of in Continental Europe, the Americas and Africa.\n\nDieter Langewiesche described the code as a \"revolutionary project\" which spurred the development of bourgeois society in Germany by the extension of the right to own property and an acceleration towards the end of feudalism. Napoleon reorganised what had been the Holy Roman Empire, made up of more than a thousand entities, into a more streamlined forty-state Confederation of the Rhine; this helped promote the German Confederation and the unification of Germany in 1871.\n\nThe movement toward national unification in Italy was similarly precipitated by Napoleonic rule. These changes contributed to the development of nationalism and the nation state.\n\nNapoleon implemented a wide array of liberal reforms in France and across Continental Europe, especially in Italy and Germany, as summarized by British historian Andrew Roberts:\n\nNapoleon directly overthrew remnants of feudalism in much of western Continental Europe. He liberalised property laws, ended seigneurial dues, abolished the guild of merchants and craftsmen to facilitate entrepreneurship, legalised divorce, closed the Jewish ghettos and made Jews equal to everyone else. The Inquisition ended as did the Holy Roman Empire. The power of church courts and religious authority was sharply reduced and equality under the law was proclaimed for all men.\n\nIn the field of military organisation, Napoleon borrowed from previous theorists such as Jacques Antoine Hippolyte, Comte de Guibert, and from the reforms of preceding French governments, and then developed much of what was already in place. He continued the policy, which emerged from the Revolution, of promotion based primarily on merit.\n\nCorps replaced divisions as the largest army units, mobile artillery was integrated into reserve batteries, the staff system became more fluid and cavalry returned as an important formation in French military doctrine. These methods are now referred to as essential features of Napoleonic warfare. Though he consolidated the practice of modern conscription introduced by the Directory, one of the restored monarchy's first acts was to end it.\n\nHis opponents learned from Napoleon's innovations. The increased importance of artillery after 1807 stemmed from his creation of a highly mobile artillery force, the growth in artillery numbers, and changes in artillery practices. As a result of these factors, Napoleon, rather than relying on infantry to wear away the enemy's defenses, now could use massed artillery as a spearhead to pound a break in the enemy's line that was then exploited by supporting infantry and cavalry. McConachy rejects the alternative theory that growing reliance on artillery by the French army beginning in 1807 was an outgrowth of the declining quality of the French infantry and, later, France's inferiority in cavalry numbers. Weapons and other kinds of military technology remained static through the Revolutionary and Napoleonic eras, but 18th-century operational mobility underwent change.\n\nNapoleon's biggest influence was in the conduct of warfare. Antoine-Henri Jomini explained Napoleon's methods in a widely used textbook that influenced all European and American armies. Napoleon was regarded by the influential military theorist Carl von Clausewitz as a genius in the operational art of war, and historians rank him as a great military commander. Wellington, when asked who was the greatest general of the day, answered: \"In this age, in past ages, in any age, Napoleon\".\n\nUnder Napoleon, a new emphasis towards the destruction, not just outmanoeuvring, of enemy armies emerged. Invasions of enemy territory occurred over broader fronts which made wars costlier and more decisive. The political effect of war increased; defeat for a European power meant more than the loss of isolated enclaves. Near-Carthaginian peaces intertwined whole national efforts, intensifying the Revolutionary phenomenon of total war.\n\nThe official introduction of the metric system in September 1799 was unpopular in large sections of French society. Napoleon's rule greatly aided adoption of the new standard not only across France but also across the French sphere of influence. Napoleon took a retrograde step in 1812 when he passed legislation to introduce the \"mesures usuelles\" (traditional units of measurement) for retail trade, a system of measure that resembled the pre-revolutionary units but were based on the kilogram and the metre; for example, the \"livre metrique\" (metric pound) was 500 g, in contrast to the value of the \"livre du roi\" (the king's pound), 489.5 g. Other units of measure were rounded in a similar manner prior to the definitive introduction of the metric system across parts of Europe in the middle of the 19th century.\n\nNapoleon's educational reforms laid the foundation of a modern system of education in France and throughout much of Europe. Napoleon synthesized the best academic elements from the \"Ancien Régime\", The Enlightenment, and the Revolution, with the aim of establishing a stable, well-educated and prosperous society. He made French the only official language. He left some primary education in the hands of religious orders, but he offered public support to secondary education. Napoleon founded a number of state secondary schools (\"lycées\") designed to produce a standardized education that was uniform across France. All students were taught the sciences along with modern and classical languages. Unlike the system during the \"Ancien Régime\", religious topics did not dominate the curriculum, although they were present with the teachers from the clergy. Napoleon hoped to use religion to produce social stability. He gave special attention to the advanced centers, such as the École Polytechnique, that provided both military expertise and state-of-the-art research in science. Napoleon made some of the first efforts at establishing a system of secular and public education. The system featured scholarships and strict discipline, with the result being a French educational system that outperformed its European counterparts, many of which borrowed from the French system.\n\nIn the political realm, historians debate whether Napoleon was \"an enlightened despot who laid the foundations of modern Europe or, instead, a megalomaniac who wrought greater misery than any man before the coming of Hitler\". Many historians have concluded that he had grandiose foreign policy ambitions. The Continental powers as late as 1808 were willing to give him nearly all of his gains and titles, but some scholars maintain he was overly aggressive and pushed for too much, until his empire collapsed.\n\nNapoleon ended lawlessness and disorder in post-Revolutionary France. He was considered a tyrant and usurper by his opponents. His critics charge that he was not troubled when faced with the prospect of war and death for thousands, turned his search for undisputed rule into a series of conflicts throughout Europe and ignored treaties and conventions alike. His role in the Haitian Revolution and decision to reinstate slavery in France's overseas colonies are controversial and affect his reputation.\n\nNapoleon institutionalised plunder of conquered territories: French museums contain art stolen by Napoleon's forces from across Europe. Artefacts were brought to the Musée du Louvre for a grand central museum; his example would later serve as inspiration for more notorious imitators. He was compared to Adolf Hitler most famously by the historian Pieter Geyl in 1947 and Claude Ribbe in 2005. David G. Chandler, a foremost historian of Napoleonic warfare, wrote in 1973 that, \"Nothing could be more degrading to the former [Napoleon] and more flattering to the latter [Hitler]. The comparison is odious. On the whole Napoleon was inspired by a noble dream, wholly dissimilar from Hitler's ... Napoleon left great and lasting testimonies to his genius—in codes of law and national identities which survive to the present day. Adolf Hitler left nothing but destruction.\"\n\nCritics argue Napoleon's true legacy must reflect the loss of status for France and needless deaths brought by his rule: historian Victor Davis Hanson writes, \"After all, the military record is unquestioned—17 years of wars, perhaps six million Europeans dead, France bankrupt, her overseas colonies lost.\" McLynn states that, \"He can be viewed as the man who set back European economic life for a generation by the dislocating impact of his wars.\" Vincent Cronin replies that such criticism relies on the flawed premise that Napoleon was responsible for the wars which bear his name, when in fact France was the victim of a series of coalitions which aimed to destroy the ideals of the Revolution.\n\nBritish military historian Correlli Barnett calls him 'a social misfit' who exploited France for his personal megalomaniac goals. He says Napoleon's reputation is exaggerated. French scholar Jean Tulard provided an influential of his image as a savior. Louis Bergeron has praised the numerous changes he made to French society, especially regarding the law as well as education. His greatest failure was the Russian invasion. Many historians have blamed Napoleon's poor planning, but Russian scholars instead emphasize the Russian response, noting the notorious winter weather was just as hard on the defenders.\n\nThe large and growing historiography in French, English, Russian, Spanish and other languages has been summarized and evaluated by numerous scholars.\n\nNapoleon's use of propaganda contributed to his rise to power, legitimated his régime, and established his image for posterity. Strict censorship, controlling aspects of the press, books, theater, and art, was part of his propaganda scheme, aimed at portraying him as bringing desperately wanted peace and stability to France. The propagandistic rhetoric changed in relation to events and to the atmosphere of Napoleon's reign, focusing first on his role as a general in the army and identification as a soldier, and moving to his role as emperor and a civil leader. Specifically targeting his civilian audience, Napoleon fostered a relationship with the contemporary art community, taking an active role in commissioning and controlling different forms of art production to suit his propaganda goals.\n\nHazareesingh (2004) explores how Napoleon's image and memory are best understood. They played a key role in collective political defiance of the Bourbon restoration monarchy in 1815–1830. People from different walks of life and areas of France, particularly Napoleonic veterans, drew on the Napoleonic legacy and its connections with the ideals of the 1789 revolution.\n\nWidespread rumors of Napoleon's return from St. Helena and Napoleon as an inspiration for patriotism, individual and collective liberties, and political mobilization manifested themselves in seditious materials, displaying the tricolor and rosettes. There were also subversive activities celebrating anniversaries of Napoleon's life and reign and disrupting royal celebrations—they demonstrated the prevailing and successful goal of the varied supporters of Napoleon to constantly destabilize the Bourbon regime.\n\nDatta (2005) shows that, following the collapse of militaristic Boulangism in the late 1880s, the Napoleonic legend was divorced from party politics and revived in popular culture. Concentrating on two plays and two novels from the period—Victorien Sardou's \"Madame Sans-Gêne\" (1893), Maurice Barrès's \"Les Déracinés\" (1897), Edmond Rostand's \"L'Aiglon\" (1900), and André de Lorde and Gyp's \"Napoléonette\" (1913)—Datta examines how writers and critics of the \"Belle Époque\" exploited the Napoleonic legend for diverse political and cultural ends.\n\nReduced to a minor character, the new fictional Napoleon became not a world historical figure but an intimate one, fashioned by individuals' needs and consumed as popular entertainment. In their attempts to represent the emperor as a figure of national unity, proponents and detractors of the Third Republic used the legend as a vehicle for exploring anxieties about gender and fears about the processes of democratization that accompanied this new era of mass politics and culture.\n\nInternational Napoleonic Congresses take place regularly, with participation by members of the French and American military, French politicians and scholars from different countries. In January 2012, the mayor of Montereau-Fault-Yonne, near Paris—the site of a late victory of Napoleon—proposed development of Napoleon's Bivouac, a commemorative theme park at a projected cost of 200 million euros.\n\nNapoleon was responsible for spreading the values of the French Revolution to other countries, especially in legal reform and the abolition of serfdom.\n\nAfter the fall of Napoleon, not only was the Napoleonic Code retained by conquered countries including the Netherlands, Belgium, parts of Italy and Germany, but has been used as the basis of certain parts of law outside Europe including the Dominican Republic, the US state of Louisiana and the Canadian province of Quebec. The memory of Napoleon in Poland is favorable, for his support for independence and opposition to Russia, his legal code, the abolition of serfdom, and the introduction of modern middle class bureaucracies.\n\nNapoleon could be considered one of the founders of modern Germany. After dissolving the Holy Roman Empire, he reduced the number of German states from 300 to less than 50, prior to German Unification. A byproduct of the French occupation was a strong development in German nationalism. Napoleon also significantly aided the United States when he agreed to sell the territory of Louisiana for 15 million dollars during the presidency of Thomas Jefferson. That territory almost doubled the size of the United States, adding the equivalent of 13 states to the Union.\n\nNapoleon married Joséphine de Beauharnais in 1796, when he was 26; she was a 32-year-old widow whose first husband had been executed during the Revolution. Five days after Joséphine's first husband’s death, the Reign of Terror initiator Maximilien de Robespierre was executed, and, with the help of high-placed friends, Joséphine was freed. Until she met Bonaparte, she had been known as \"Rose\", a name which he disliked. He called her \"Joséphine\" instead, and she went by this name henceforth. Bonaparte often sent her love letters while on his campaigns. He formally adopted her son Eugène and cousin Stéphanie and arranged dynastic marriages for them. Joséphine had her daughter Hortense marry Napoleon's brother Louis.\n\nJoséphine had lovers, such as Lieutenant Hippolyte Charles, during Napoleon's Italian campaign. Napoleon learnt of that affair and a letter he wrote about it was intercepted by the British and published widely, to embarrass Napoleon. Napoleon had his own affairs too: during the Egyptian campaign he took Pauline Bellisle Foures, the wife of a junior officer, as his mistress. She became known as \"Cleopatra\".\n\nWhile Napoleon's mistresses had children by him, Joséphine did not produce an heir, possibly because of either the stresses of her imprisonment during the Reign of Terror or an abortion she may have had in her twenties. Napoleon chose divorce so he could remarry in search of an heir. Despite his divorce from Josephine, Napoleon showed his dedication to her for the rest of his life. When he heard the news of her death while on exile in Elba, he locked himself in his room and would not come out for two full days. Her name would also be his final word on his deathbed in 1821.\n\nOn 11 March 1810 by proxy, he married the 19-year-old Marie Louise, Archduchess of Austria, and a great niece of Marie Antoinette. Thus he had married into a German royal and imperial family. Louise was less than happy with the arrangement, at least at first, stating: \"Just to see the man would be the worst form of torture\". Her great-aunt had been executed in France, while Napoleon had fought numerous campaigns against Austria all throughout his military career. However, she seemed to warm up to him over time. After her wedding, she wrote to her father: \"He loves me very much. I respond to his love sincerely. There is something very fetching and very eager about him that is impossible to resist\".\n\nNapoleon and Marie Louise remained married until his death, though she did not join him in exile on Elba and thereafter never saw her husband again. The couple had one child, Napoleon Francis Joseph Charles (1811–1832), known from birth as the King of Rome. He became Napoleon II in 1814 and reigned for only two weeks. He was awarded the title of the Duke of Reichstadt in 1818 and died of tuberculosis aged 21, with no children.\n\nNapoleon acknowledged one illegitimate son: Charles Léon (1806–1881) by Eléonore Denuelle de La Plaigne. Alexandre Colonna-Walewski (1810–1868), the son of his mistress Maria Walewska, although acknowledged by Walewska's husband, was also widely known to be his child, and the DNA of his direct male descendant has been used to help confirm Napoleon's Y-chromosome haplotype. He may have had further unacknowledged illegitimate offspring as well, such as Eugen Megerle von Mühlfeld by Emilie Victoria Kraus and Hélène Napoleone Bonaparte (1816–1907) by Albine de Montholon.\n\n\n\n\n"}
{"id": "17117924", "url": "https://en.wikipedia.org/wiki?curid=17117924", "title": "Network probability matrix", "text": "Network probability matrix\n\nThe network probability matrix describes the probability structure of a network based on the historical presence or absence of edges in a network. For example, individuals in a social network are not connected to other individuals with uniform random probability. The probability structure is much more complex. Intuitively, there are some people whom a person will communicate with or be connected more closely than others. For this reason, real-world networks tend to have clusters or cliques of nodes that are more closely related than others (Albert and Barabasi, 2002, Carley [year], Newmann 2003). This can be simulated by varying the probabilities that certain nodes will communicate. The network probability matrix was originally proposed by Ian McCulloh.\n\n\n"}
{"id": "1254148", "url": "https://en.wikipedia.org/wiki?curid=1254148", "title": "Parallel coordinates", "text": "Parallel coordinates\n\nParallel coordinates are a common way of visualizing high-dimensional geometry and analyzing multivariate data.\n\nTo show a set of points in an \"n\"-dimensional space, a backdrop is drawn consisting of \"n\" parallel lines, typically vertical and equally spaced. A point in \"n\"-dimensional space is represented as a polyline with vertices on the parallel axes; the position of the vertex on the \"i\"-th axis corresponds to the \"i\"-th coordinate of the point.\n\nThis visualization is closely related to time series visualization, except that it is applied to data where the axes do not correspond to points in time, and therefore do not have a natural order. Therefore, different axis arrangements may be of interest.\n\nParallel coordinates were often said to be invented by Philbert Maurice d'Ocagne in 1885, but even though the words \"Coordonnées parallèles\" appear in the book title this work has nothing to do with the visualization techniques of the same name; the book only describes a method of coordinate transformation. But even before 1885, parallel coordinates were used, for example in Henry Gannetts \"General Summary, Showing the Rank of States, by Ratios, 1880\", or afterwards in Henry Gannetts \"Rank of States and Territories in Population at Each Census, 1790-1890\" in 1898. They were popularised again 79 years later by Alfred Inselberg in 1959 and systematically developed as a coordinate system starting from 1977. Some important applications are in collision avoidance algorithms for air traffic control (1987—3 USA patents), data mining (USA patent), computer vision (USA patent), Optimization, process control, more recently in intrusion detection and elsewhere.\n\nOn the plane with an xy cartesian coordinate system, adding more dimensions in parallel coordinates (often abbreviated ||-coords or PCP) involves adding more axes. The value of parallel coordinates is that certain geometrical properties in high dimensions transform into easily seen 2D patterns. For example, a set of points on a line in \"n\"-space transforms to a set of polylines in parallel coordinates all intersecting at \"n\" − 1 points. For \"n\" = 2 this yields a point-line duality pointing out why the mathematical foundations of parallel coordinates are developed in the projective rather than euclidean space. A pair of lines intersects at a unique point which has two coordinates and, therefore, can correspond to a unique line which is also specified by two parameters (or two points). By contrast, more than two points are required to specify a curve and also a pair of curves may not have a unique intersection. Hence by using curves in parallel coordinates instead of lines, the point line duality is lost together with all the other properties of projective geometry, and the known nice higher-dimensional patterns corresponding to (hyper)planes, curves, several smooth (hyper)surfaces, proximities, convexity and recently non-orientability. The goal is to map n-dimensional relations into 2D patterns. Hence, parallel coordinates is not a point-to-point mapping but rather a \"n\"D subset to 2D subset mapping, there is no loss of information. Note: even a point in nD is not mapped into a point in 2D, but to a polygonal line—a subset of 2D.\n\nWhen used for statistical data visualisation there are three important considerations: the order, the rotation, and the scaling of the axes.\n\nThe order of the axes is critical for finding features, and in typical data analysis many reorderings will need to be tried. Some authors have come up with ordering heuristics which may create illuminating orderings.\n\nThe rotation of the axes is a translation in the parallel coordinates and if the lines intersected outside the parallel axes it can be translated between them by rotations. The simplest example of this is rotating the axis by 180 degrees.\n\nScaling is necessary because the plot is based on interpolation (linear combination) of consecutive pairs of variables. Therefore, the variables must be in common scale, and there are many scaling methods to be considered as part of data preparation process that can reveal more informative views.\n\nA smooth parallel coordinate plot is achieved with splines. In the smooth plot, every observation is mapped into a parametric line (or curve), which is smooth, continuous on the axes, and orthogonal to each parallel axis. This design emphasizes the quantization level for each data attribute. If one uses the Fourier interpolation of degree equals to the data dimensionality, then an Andrews plot is achieved.\n\nInselberg () made a full review of how to visually read out parallel coords' relational patterns. When most lines between two parallel axis are somewhat parallel to each others, that suggests a positive relationship between these two dimensions. When lines cross in a kind of superposition of X-shapes, that's negative relationship. When lines cross randomly or are parallel, that show there is no particular relationship.\n\nIn parallel coordinates, each axis can have at most two neighboring axes (one on the left, and one on the right). For a d-dimensional data set, at most d-1 relationships can be shown at a time. In time series visualization, there exists a natural predecessor and successor; therefore in this special case, there exists a preferred arrangement. However, when the axes do not have a unique order, finding a good axis arrangement requires the use of heuristics and experimentation. In order to explore more complex relationships, axes must be reordered.\n\nBy arranging the axes in 3-dimensional space (however, still in parallel, like nails in a nail bed), an axis can have more than two neighbors in a circle around the central attribute, and the arrangement problem gets easier (for example by using a minimum spanning tree). A prototype of this visualization is available as extension to the data mining software ELKI. However, the visualization is harder to interpret and interact with than a linear order.\n\nWhile there are a large number of papers about parallel coordinates, there are only few notable software publicly available to convert databases into parallel coordinates graphics. Notable software are ELKI, GGobi, Macrofocus High-D, Mondrian, Orange and ROOT. Libraries include Protovis.js, D3.js provide basic examples, while more complex examples are also available. D3.Parcoords.js (a D3-based library) and Macrofocus High-D API (a Java library) specifically dedicated to ||-coords graphic creation have also been published. The Python data structure and analysis library Pandas implements parallel coordinates plotting, using the plotting library matplotlib. The R package GGally, among others, implements parallel coordinates plotting.\n\n\n\n"}
{"id": "166356", "url": "https://en.wikipedia.org/wiki?curid=166356", "title": "Pseudovector", "text": "Pseudovector\n\nIn physics and mathematics, a pseudovector (or axial vector) is a quantity that transforms like a vector under a proper rotation, but in three dimensions gains an additional sign flip under an improper rotation such as a reflection. Geometrically it is the opposite, of equal magnitude but in the opposite direction, of its mirror image. This is as opposed to a \"true\" vector, also known, in this context, as a polar vector, which on reflection matches its mirror image.\n\nIn three dimensions, the pseudovector p is associated with the curl of a polar vector or with the cross product of two polar vectors a and b:\n\nThe vector p calculated this way is a pseudovector. One example is the normal to an oriented plane. An oriented plane can be defined by two non-parallel vectors, a and b, that span the plane. The vector is a normal to the plane (there are two normals, one on each side – the right-hand rule will determine which), and is a pseudovector. This has consequences in computer graphics where it has to be considered when transforming surface normals.\n\nA number of quantities in physics behave as pseudovectors rather than polar vectors, including magnetic field and angular velocity. In mathematics pseudovectors are equivalent to three-dimensional bivectors, from which the transformation rules of pseudovectors can be derived. More generally in \"n\"-dimensional geometric algebra pseudovectors are the elements of the algebra with dimension , written ⋀R. The label \"pseudo\" can be further generalized to pseudoscalars and pseudotensors, both of which gain an extra sign flip under improper rotations compared to a true scalar or tensor.\n\nPhysical examples of pseudovectors include magnetic field, torque, angular velocity, and angular momentum.\n\nConsider the pseudovector angular momentum . Driving in a car, and looking forward, each of the wheels has an angular momentum vector pointing to the left. If the world is reflected in a mirror which switches the left and right side of the car, the \"reflection\" of this angular momentum \"vector\" (viewed as an ordinary vector) points to the right, but the \"actual\" angular momentum vector of the wheel (which is still turning forward in the reflection) still points to the left, corresponding to the extra minus sign in the reflection of a pseudovector.\n\nThe distinction between vectors and pseudovectors becomes important in understanding the effect of symmetry on the solution to physical systems. Consider an electric current loop in the plane that inside the loop generates a magnetic field oriented in the \"z\" direction. This system is symmetric (invariant) under mirror reflections through this plane, with the magnetic field unchanged by the reflection. But reflecting the magnetic field as a vector through that plane would be expected to reverse it; this expectation is corrected by realizing that the magnetic field is a pseudovector, with the extra sign flip leaving it unchanged.\n\nThe definition of a \"vector\" in physics (including both polar vectors and pseudovectors) is more specific than the mathematical definition of \"vector\" (namely, any element of an abstract vector space). Under the physics definition, a \"vector\" is required to have components that \"transform\" in a certain way under a proper rotation: In particular, if everything in the universe were rotated, the vector would rotate in exactly the same way. (The coordinate system is fixed in this discussion; in other words this is the perspective of active transformations.) Mathematically, if everything in the universe undergoes a rotation described by a rotation matrix \"R\", so that a displacement vector x is transformed to , then any \"vector\" v must be similarly transformed to . This important requirement is what distinguishes a \"vector\" (which might be composed of, for example, the \"x\"-, \"y\"-, and \"z\"-components of velocity) from any other triplet of physical quantities (For example, the length, width, and height of a rectangular box \"cannot\" be considered the three components of a vector, since rotating the box does not appropriately transform these three components.)\n\nThe discussion so far only relates to proper rotations, i.e. rotations about an axis. However, one can also consider improper rotations, i.e. a mirror-reflection possibly followed by a proper rotation. (One example of an improper rotation is inversion through a point in 3-dimensional space.) Suppose everything in the universe undergoes an improper rotation described by the rotation matrix \"R\", so that a position vector x is transformed to . If the vector v is a polar vector, it will be transformed to . If it is a pseudovector, it will be transformed to .\n\nThe transformation rules for polar vectors and pseudovectors can be compactly stated as\n\nwhere the symbols are as described above, and the rotation matrix \"R\" can be either proper or improper. The symbol det denotes determinant; this formula works because the determinant of proper and improper rotation matrices are +1 and −1, respectively.\n\nSuppose v and v are known pseudovectors, and v is defined to be their sum, . If the universe is transformed by a rotation matrix \"R\", then v is transformed to\n\nSo v is also a pseudovector. Similarly one can show that the difference between two pseudovectors is a pseudovector, that the sum or difference of two polar vectors is a polar vector, that multiplying a polar vector by any real number yields another polar vector, and that multiplying a pseudovector by any real number yields another pseudovector.\n\nOn the other hand, suppose v is known to be a polar vector, v is known to be a pseudovector, and v is defined to be their sum, . If the universe is transformed by a rotation matrix \"R\", then v is transformed to\n\nTherefore, v is neither a polar vector nor a pseudovector. For an improper rotation, v does not in general even keep the same magnitude:\n\nIf the magnitude of v were to describe a measurable physical quantity, that would mean that the laws of physics would not appear the same if the universe was viewed in a mirror. In fact, this is exactly what happens in the weak interaction: Certain radioactive decays treat \"left\" and \"right\" differently, a phenomenon which can be traced to the summation of a polar vector with a pseudovector in the underlying theory. (See parity violation.)\n\nFor a rotation matrix \"R\", either proper or improper, the following mathematical equation is always true:\nwhere v and v are any three-dimensional vectors. (This equation can be proven either through a geometric argument or through an algebraic calculation.)\n\nSuppose v and v are known polar vectors, and v is defined to be their cross product, . If the universe is transformed by a rotation matrix \"R\", then v is transformed to\nSo v is a pseudovector. Similarly, one can show:\nThis is isomorphic to addition modulo 2, where \"polar\" corresponds to 1 and \"pseudo\" to 0.\n\nFrom the definition, it is clear that a displacement vector is a polar vector. The velocity vector is a displacement vector (a polar vector) divided by time (a scalar), so is also a polar vector. Likewise, the momentum vector is the velocity vector (a polar vector) times mass (a scalar), so is a polar vector. Angular momentum is the cross product of a displacement (a polar vector) and momentum (a polar vector), and is therefore a pseudovector. Continuing this way, it is straightforward to classify any vector as either a pseudovector or polar vector.\n\nAbove, pseudovectors have been discussed using active transformations. An alternate approach, more along the lines of passive transformations, is to keep the universe fixed, but switch \"right-hand rule\" with \"left-hand rule\" everywhere in math and physics, including in the definition of the cross product. Any polar vector (e.g., a translation vector) would be unchanged, but pseudovectors (e.g., the magnetic field vector at a point) would switch signs. Nevertheless, there would be no physical consequences, apart from in the parity-violating phenomena such as certain radioactive decays.\n\nOne way to formalize pseudovectors is as follows: if \"V\" is an \"n\"-dimensional vector space, then a \"pseudovector\" of \"V\" is an element of the (\"n\" − 1)-th exterior power of \"V\": Λ(\"V\"). The pseudovectors of \"V\" form a vector space with the same dimension as \"V\".\n\nThis definition is not equivalent to that requiring a sign flip under improper rotations, but it is general to all vector spaces. In particular, when \"n\" is even, such a pseudovector does not experience a sign flip, and when the characteristic of the underlying field of \"V\" is 2, a sign flip has no effect. Otherwise, the definitions are equivalent, though it should be borne in mind that without additional structure (specifically, a volume form), there is no natural identification of ⋀(\"V\") with \"V\".\n\nIn geometric algebra the basic elements are vectors, and these are used to build a hierarchy of elements using the definitions of products in this algebra. In particular, the algebra builds pseudovectors from vectors.\n\nThe basic multiplication in the geometric algebra is the geometric product, denoted by simply juxtaposing two vectors as in ab. This product is expressed as:\n\nwhere the leading term is the customary vector dot product and the second term is called the wedge product. Using the postulates of the algebra, all combinations of dot and wedge products can be evaluated. A terminology to describe the various combinations is provided. For example, a multivector is a summation of \"k\"-fold wedge products of various \"k\"-values. A \"k\"-fold wedge product also is referred to as a \"k\"-blade.\n\nIn the present context the \"pseudovector\" is one of these combinations. This term is attached to a different multivector depending upon the dimensions of the space (that is, the number of linearly independent vectors in the space). In three dimensions, the most general 2-blade or bivector can be expressed as the wedge product of two vectors and is a pseudovector. In four dimensions, however, the pseudovectors are trivectors. In general, it is a -blade, where \"n\" is the dimension of the space and algebra. An \"n\"-dimensional space has \"n\" basis vectors and also \"n\" basis pseudovectors. Each basis pseudovector is formed from the outer (wedge) product of all but one of the \"n\" basis vectors. For instance, in four dimensions where the basis vectors are taken to be {e, e, e, e}, the pseudovectors can be written as: {e, e, e, e}.\n\nThe transformation properties of the pseudovector in three dimensions has been compared to that of the vector cross product by Baylis. He says: \"The terms \"axial vector\" and \"pseudovector\" are often treated as synonymous, but it is quite useful to be able to distinguish a bivector from its dual.\" To paraphrase Baylis: Given two polar vectors (that is, true vectors) a and b in three dimensions, the cross product composed from a and b is the vector normal to their plane given by . Given a set of right-handed orthonormal basis vectors , the cross product is expressed in terms of its components as:\n\nwhere superscripts label vector components. On the other hand, the plane of the two vectors is represented by the exterior product or wedge product, denoted by . In this context of geometric algebra, this bivector is called a pseudovector, and is the \"Hodge dual\" of the cross product. The \"dual\" of e is introduced as e ≡ ee = , and so forth. That is, the dual of e is the subspace perpendicular to e, namely the subspace spanned by e and e. With this understanding,\n\nFor details, see \"\". The cross product and wedge product are related by:\n\nwhere \"i\" = is called the \"unit pseudoscalar\". It has the property:\n\nUsing the above relations, it is seen that if the vectors a and b are inverted by changing the signs of their components while leaving the basis vectors fixed, both the pseudovector and the cross product are invariant. On the other hand, if the components are fixed and the basis vectors e are inverted, then the pseudovector is invariant, but the cross product changes sign. This behavior of cross products is consistent with their definition as vector-like elements that change sign under transformation from a right-handed to a left-handed coordinate system, unlike polar vectors.\n\nAs an aside, it may be noted that not all authors in the field of geometric algebra use the term pseudovector, and some authors follow the terminology that does not distinguish between the pseudovector and the cross product. However, because the cross product does not generalize to other than three dimensions,\nthe notion of pseudovector based upon the cross product also cannot be extended to a space of any other number of dimensions. The pseudovector as a -blade in an \"n\"-dimensional space is not restricted in this way.\n\nAnother important note is that pseudovectors, despite their name, are \"vectors\" in the sense of being elements of a vector space. The idea that \"a pseudovector is different from a vector\" is only true with a different and more specific definition of the term \"vector\" as discussed above.\n\n\n"}
{"id": "16649410", "url": "https://en.wikipedia.org/wiki?curid=16649410", "title": "Pushforward (homology)", "text": "Pushforward (homology)\n\nIn algebraic topology, the pushforward of a continuous function formula_1 : formula_2 between two topological spaces is a homomorphism formula_3 between the homology groups for formula_4.\n\nHomology is a functor which converts a topological space formula_5 into a sequence of homology groups formula_6. (Often, the collection of all such groups is referred to using the notation formula_7; this collection has the structure of a graded ring.) In any category, a functor must induce a corresponding morphism. The pushforward is the morphism corresponding to the homology functor.\n\nWe build the pushforward homomorphism as follows (for singular or simplicial homology):\n\nFirst we have an induced homomorphism between the singular or simplicial chain complex formula_8 and formula_9 defined by composing each singular n-simplex formula_10 : formula_11 with formula_1 to obtain a singular n-simplex of formula_13, formula_14 : formula_15. Then we extend formula_16 linearly via formula_17.\n\nThe maps formula_16 : formula_19 satisfy formula_20 where formula_21 is the boundary operator between chain groups, so formula_22 defines a chain map.\n\nWe have that formula_16 takes cycles to cycles, since formula_24 implies formula_25. Also formula_16 takes boundaries to boundaries since formula_27.\n\nHence formula_16 induces a homomorphism between the homology groups formula_29 for formula_30.\n\nTwo basic properties of the push-forward are:\n\nA main result about the push-forward is the homotopy invariance: if two maps formula_38 are homotopic, then they induce the same homomorphism formula_39.\n\nThis immediately implies that the homology groups of homotopy equivalent spaces are isomorphic:\n\nThe maps formula_40 induced by a homotopy equivalence formula_41 are isomorphisms for all formula_42.\n\n"}
{"id": "48803892", "url": "https://en.wikipedia.org/wiki?curid=48803892", "title": "Regularized least squares", "text": "Regularized least squares\n\nRegularized least squares (RLS) is a family of methods for solving the least-squares problem while using regularization to further constrain the resulting solution.\n\nRLS is used for two main reasons. The first comes up when the number of variables in the linear system exceeds the number of observations. In such settings, the ordinary least-squares problem is ill-posed and is therefore impossible to fit because the associated optimization problem has infinitely many solutions. RLS allows the introduction of further constraints that uniquely determine the solution.\n\nThe second reason that RLS is used occurs when the number of variables does not exceed the number of observations, but the learned model suffers from poor generalization. RLS can be used in such cases to improve the generalizability of the model by constraining it at training time. This constraint can either force the solution to be \"sparse\" in some way or to reflect other prior knowledge about the problem such as information about correlations between features. A Bayesian understanding of this can be reached by showing that RLS methods are often equivalent to priors on the solution to the least-squares problem.\n\nConsider a learning setting given by a probabilistic space formula_1, formula_2. Let formula_3 denote a training set of formula_4 pairs i.i.d. with respect to formula_5. Let formula_6 be a loss function. Define formula_7 as the space of the functions such that expected risk: \nis well defined. \nThe main goal is to minimize the expected risk:\nSince the problem cannot be solved exactly there is a need to specify how to measure the quality of a solution. A good learning algorithm should provide an estimator with a small risk.\n\nAs the joint distribution formula_5 is typically unknown, the empirical risk is taken. For regularized least squares the square loss function is introduced:\n\nHowever, if the functions are from a relatively unconstrained space, such as the set of square-integrable functions on formula_12, this approach may overfit the training data, and lead to poor generalization. Thus, it should somehow constrain or penalize the complexity of the function formula_13. In RLS, this is accomplished by choosing functions from a reproducing kernel Hilbert space (RKHS) formula_14, and adding a regularization term to the objective function, proportional to the norm of the function in formula_14:\n\nA RKHS can be defined by a symmetric positive-definite kernel function formula_17 with the reproducing property:\n\nwhere formula_19. The RKHS for a kernel formula_20 consists of the completion of the space of functions spanned by formula_21: formula_22, where all formula_23 are real numbers. Some commonly used kernels include the linear kernel, inducing the space of linear functions:\n\nthe polynomial kernel, inducing the space of polynomial functions of order formula_25:\n\nand the Gaussian kernel:\n\nNote that for an arbitrary loss function formula_28, this approach defines a general class of algorithms named Tikhonov regularization. For instance, using the hinge loss leads to the support vector machine algorithm, and using the epsilon-insensitive loss leads to support vector regression.\n\nThe representer theorem guarantees that the solution can be written as:\n\nThe minimization problem can be expressed as:\n\nwhere, with some abuse of notation, the formula_32 entry of kernel matrix formula_20 (as opposed to kernel function formula_34) is formula_35.\n\nFor such a function,\n\nThe following minimization problem can be obtained:\n\nAs the sum of convex functions is convex, the solution is unique and its minimum can be found by setting the gradient w.r.t formula_38 to formula_39:\nwhere formula_41.\n\nThe complexity of training is basically the cost of computing the kernel matrix plus the cost of solving the linear system which is roughly formula_42. The computation of the kernel matrix for the linear or Gaussian kernel is formula_43. The complexity of testing is formula_44.\n\nThe prediction at a new test point formula_45 is:\n\nFor convenience a vector notation is introduced. Let formula_12 be an formula_48 matrix, where the rows are input vectors, and formula_49 a formula_50 vector where the entries are corresponding outputs. In terms of vectors, the kernel matrix can be written as formula_51. The learning function can be written as:\n\nHere we define formula_53. The objective function can be rewritten as:\n\nThe first term is the objective function from ordinary least squares (OLS) regression, corresponding to the residual sum of squares. The second term is a regularization term, not present in OLS, which penalizes large formula_55 values.\nAs a smooth finite dimensional problem is considered and it is possible to apply standard calculus tools. In order to minimize the objective function, the gradient is calculated with respect to formula_55 and set it to zero:\n\nThis solution closely resembles that of standard linear regression, with an extra term formula_59. If the assumptions of OLS regression hold, the solution formula_60, with formula_61, is an unbiased estimator, and is the minimum-variance linear unbiased estimator, according to the Gauss–Markov theorem. The term formula_62 therefore leads to a biased solution; however, it also tends to reduce variance. This is easy to see, as the covariance matrix of the formula_55-values is proportional to formula_64, and therefore large values of formula_65 will lead to lower variance. Therefore, manipulating formula_65 corresponds to trading-off bias and variance. For problems with high-variance formula_55 estimates, such as cases with relatively small formula_4 or with correlated regressors, the optimal prediction accuracy may be obtained by using a nonzero formula_65, and thus introducing some bias to reduce variance. Furthermore, it is not uncommon in machine learning to have cases where formula_70, in which case formula_71 is rank-deficient, and a nonzero formula_65 is necessary to compute formula_73.\n\nThe parameter formula_65\ncontrols the invertibility of the matrix formula_75.\nSeveral methods can be used to solve the above linear system,\nCholesky decomposition being probably the method of choice, since the matrix formula_75 is symmetric and positive definite. The complexity of this method is formula_77 for training and\nformula_78 for testing. The cost formula_77 is essentially that of computing formula_80, whereas the inverse computation (or rather the solution of the linear system) is roughly formula_81.\n\nIn this section it will be shown how to extend RLS to any kind of reproducing kernel K. Instead of linear kernel a feature map is considered\nformula_82 for some Hilbert space formula_7, called the feature space. In this case the kernel is defined as: The matrix formula_12 is now replaced by the new data matrix formula_85, where formula_86, or the formula_87-th component of the formula_88.\n\nIt means that for a given training set formula_90. Thus, the objective function can be written as:\n\nThis approach is known as the kernel trick. This technique can significantly simplify the computational operations. If formula_7 is high dimensional, computing formula_93 may be rather intensive. If the explicit form of the kernel function is known, we just need to compute and store the formula_94 kernel matrix formula_95.\n\nIn fact, the Hilbert space formula_7 need not be isomorphic to formula_97, and can be infinite dimensional. This follows from Mercer's theorem, which states that a continuous, symmetric, positive definite kernel function can be expressed as:\n\nformula_98\n\nwhere formula_99 form an orthonormal basis for formula_100, and formula_101. If feature maps is defined formula_102 with components formula_103, it follows that formula_104. This demonstrates that any kernel can be associated with a feature map, and that RLS generally consists of linear RLS performed in some possibly higher-dimensional feature space. While Mercer's theorem shows how one feature map that can be associated with a kernel, in fact multiple feature maps can be associated with a given reproducing kernel. For instance, the map formula_105 satisfies the property formula_104 for an arbitrary reproducing kernel.\n\nLeast squares can be viewed as a likelihood maximization under an assumption of normally distributed residuals. This is because the exponent of the Gaussian distribution is quadratic in the data, and so is the least-squares objective function. In this framework, the regularization terms of RLS can be understood to be encoding priors on formula_55. For instance, Tikhonov regularization corresponds to a normally distributed prior on formula_55 that is centered at 0. To see this, first note that the OLS objective is proportional to the log-likelihood function when each sampled formula_109 is normally distributed around formula_110. Then observe that a normal prior on formula_55 centered at 0 has a log-probability of the form\nwhere formula_113 and formula_114 are constants that depend on the variance of the prior and are independent of formula_55. Thus, minimizing the logarithm of the likelihood times the prior is equivalent to minimizing the sum of the OLS loss function and the ridge regression regularization term.\n\nThis gives a more intuitive interpretation for why Tikhonov regularization leads to a unique solution to the least-squares problem: there are infinitely many vectors formula_55 satisfying the constraints obtained from the data, but since we come to the problem with a prior belief that formula_55 is normally distributed around the origin, we will end up choosing a solution with this constraint in mind.\n\nOther regularization methods correspond to different priors. See the list below for more details.\n\nOne particularly common choice for the penalty function formula_118 is the squared formula_119 norm, i.e.,\nThe most common names for this are called Tikhonov regularization and ridge regression. \nIt admits a closed-form solution for formula_55:\nThe name ridge regression alludes to the fact that the formula_124 term adds positive entries along the diagonal \"ridge\" of the sample covariance matrix formula_71.\n\nWhen formula_126, i.e., in the case of ordinary least squares, the condition that formula_127 causes the sample covariance matrix formula_71 to not have full rank and so it cannot be inverted to yield a unique solution. This is why there can be an infinitude of solutions to the ordinary least squares problem when formula_127. However, when formula_130, i.e., when ridge regression is used, the addition of formula_124 to the sample covariance matrix ensures that all of its eigenvalues will be strictly greater than 0. In other words, it becomes invertible, and the solution becomes unique.\n\nCompared to ordinary least squares, ridge regression is not unbiased. It accepts little bias to reduce variance and the mean square error, and helps to improve the prediction accuracy. Thus, ridge estimator yields more stable solutions by shrinking coefficients but suffers from the lack of sensitivity to the data.\n\nThe least absolute selection and shrinkage (LASSO) method is another popular choice. In lasso regression, the lasso penalty function formula_118 is the formula_133 norm, i.e.\n\nNote that the lasso penalty function is convex but not strictly convex. \nUnlike Tikhonov regularization, this scheme does not have a convenient closed-form solution: instead, the solution is typically found using quadratic programming or more general convex optimization methods, as well as by specific algorithms such as the least-angle regression algorithm.\n\nAn important difference between lasso regression and Tikhonov regularization is that lasso regression forces more entries of formula_55 to actually equal 0 than would otherwise. In contrast, while Tikhonov regularization forces entries of formula_55 to be small, it does not force more of them to be 0 than would be otherwise. Thus, LASSO regularization is more appropriate than Tikhonov regularization in cases in which we expect the number of non-zero entries of formula_55 to be small, and Tikhonov regularization is more appropriate when we expect that entries of formula_55 will generally be small but not necessarily zero. Which of these regimes is more relevant depends on the specific data set at hand.\n\nBesides feature selection described above, LASSO has some limitations. Ridge regression provides better accuracy in the case formula_140 for highly correlated variables. In another case, formula_141, LASSO selects at most formula_142 variables. Moreover, LASSO tends to select some arbitrary variables from group of highly correlated samples, so there is no grouping effect.\n\nThe most extreme way to enforce sparsity is to say that the actual magnitude of the coefficients of formula_55 does not matter; rather, the only thing that determines the complexity of formula_55 is the number of non-zero entries. This corresponds to setting formula_146 to be the formula_147 norm of formula_55. This regularization function, while attractive for the sparsity that it guarantees, is very difficult to solve because doing so requires optimization of a function that is not even weakly convex. Lasso regression is the minimal possible relaxation of formula_147 penalization that yields a weakly convex optimization problem.\n\nFor any non-negative formula_150 and formula_151 the objective has the following form:\n\nLet formula_153, then the solution of the minimization problem is described as:\n\nConsider formula_156 as an Elastic Net penalty function.\n\nWhen formula_157, elastic net becomes ridge regression, whereas formula_158 it becomes Lasso. formula_159 Elastic Net penalty function doesn't have the first derivative at 0 and it is strictly convex \nformula_160 taking the properties both lasso regression and ridge regression.\n\nOne of the main properties of the Elastic Net is that it can select groups of correlated variables. The difference between weight vectors of samples formula_161 and formula_162 is given by:\n\nIf formula_161 and formula_162 are highly correlated ( formula_167), the weight vectors are very close. In the case of negatively correlated samples ( formula_168) the samples formula_169 can be taken. To summarize, for highly correlated variables the weight vectors tend to be equal up to a sign in the case of negative correlated variables.\n\nThe following is a list of possible choices of the regularization function formula_170, along with the name for each one, the corresponding prior if there is a simple one, and ways for computing the solution to the resulting optimization problem.\n\n"}
{"id": "12766558", "url": "https://en.wikipedia.org/wiki?curid=12766558", "title": "Research Computing Services", "text": "Research Computing Services\n\nResearch Computing Services (separated in August 2007 from the former Manchester Computing at the University of Manchester), provides the focus for the University of Manchester's activities in supercomputing or high-performance computing, grid computing or e-science and computational science. Research Computing Services activities include services, training and research & development.\n\nThe University of Manchester has been home to many supercomputers, starting from the 1948 Manchester Baby - the world's first stored program computer. Others have included CDC7600 (1972, and a second in 1977), a CDC Cyber 205, VP1200, VPX and 240/10. The CSAR service (see below) supercomputers included a 576 PE Cray T3E-1200E (1998, upgraded to 816PE in 2000), and SGI Origin 3000 (2001) and Altix (2003) systems. More recently some large clusters (e.g., the 200 processor Dell EM64T cluster) have been installed.\n\nResearch Computing Services and its predecessors (Manchester Computing etc.) have been providing (high performance) computing services nationally in the UK since the 1970s. Manchester Computing operated the UK's 1998-2006 national supercomputer service CSAR with SGI and CSC Ltd. It currently operates other national computer services in the UK, including the Access Grid Support Centre (AGSC) and, as part of consortia, the UK National Grid Service (NGS) and North West Grid.\n\nResearch Computing Services is a part of several research centres including E-Science North West (ESNW), and the UK's National Centre for e-Social Science (NCeSS).\n\n"}
{"id": "31925388", "url": "https://en.wikipedia.org/wiki?curid=31925388", "title": "Schubert polynomial", "text": "Schubert polynomial\n\nIn mathematics, Schubert polynomials are generalizations of Schur polynomials that represent cohomology classes of Schubert cycles in flag varieties. \nThey were introduced by and are named after Hermann Schubert.\n\n described the history of Schubert polynomials.\n\nThe Schubert polynomials formula_1 are polynomials in the variables formula_2 depending on an element formula_3 of the infinite symmetric group formula_4 of all permutations of formula_5 fixing all but a finite number of elements. They form a basis for the polynomial ring formula_6 in infinitely many variables.\n\nThe cohomology of the flag manifold formula_7 is formula_8, where formula_9 is the ideal generated by homogeneous symmetric functions of positive degree. \nThe Schubert polynomial formula_1 is the unique homogeneous polynomial of degree formula_11 representing the Schubert cycle of formula_12 in the cohomology of the flag manifold formula_7 for all sufficiently large formula_14.\n\nSchubert polynomials can be calculated recursively from these two properties. In particular, this implies that formula_25.\n\nOther properties are\n\nAs an example formula_36.\n\nSince the Schubert polynomials form a basis, there are unique coefficients formula_37 \nsuch that formula_38.\nThese can be seen as a generalization of the Littlewood−Richardson coefficients described by the Littlewood–Richardson rule.\nFor representation-theoretical reasons, these coefficients are non-negative integers and it is an \noutstanding problem in representation theory and combinatorics to give a combinatorial rule for these numbers.\n\nDouble Schubert polynomials formula_39 are polynomials in two infinite sets of variables, parameterized by an element \"w\" of the infinite symmetric group, that becomes the usual Schubert polynomials when all the variables formula_40 are formula_41.\n\nThe double Schubert polynomial formula_39 are characterized by the properties\n\nThe double Schubert polynomials can also be defined as \nformula_48.\n\n introduced quantum Schubert polynomials, that have the same relation to the (small) quantum cohomology of flag manifolds that ordinary Schubert polynomials have to the ordinary cohomology.\n\n introduced universal Schubert polynomials, that generalize classical and quantum Schubert polynomials. He also described universal double Schubert polynomials generalizing double Schubert polynomials.\n\n\n"}
{"id": "710483", "url": "https://en.wikipedia.org/wiki?curid=710483", "title": "Scott domain", "text": "Scott domain\n\nIn the mathematical fields of order and domain theory, a Scott domain is an algebraic, bounded-complete cpo. It has been named in honour of Dana S. Scott, who was the first to study these structures at the advent of domain theory. Scott domains are very closely related to algebraic lattices, being different only in possibly lacking a greatest element. They are also closely related to Scott information systems, which constitute a \"syntactic\" representation of Scott domains.\n\nWhile the term \"Scott domain\" is widely used with the above definition, the term \"domain\" does not have such a generally accepted meaning and different authors will use different definitions; Scott himself used \"domain\" for the structures now called \"Scott domains\". Additionally, Scott domains appear with other names like \"algebraic semilattice\" in some publications.\n\nOriginally, Dana Scott demanded a complete lattice, and the Russian mathematician Yury Yershov constructed the isomorphic structure of cpo. But this was not recognized until after scientific communications improved after the fall of the Iron Curtain. In honour of their work, a number of mathematical papers now dub this fundamental construction a \"Scott–Ershov\" domain. \n\nFormally, a non-empty partially ordered set (\"D\", ≤) is called a \"Scott domain\" if the following hold:\n\n\nSince the empty set certainly has some upper bound, we can conclude the existence of a least element formula_1 (the supremum of the empty set) from bounded completeness.\n\nThe property of being bounded complete is equivalent to the existence of infima of all non-empty subsets of \"D\". It is well known that the existence of all infima implies the existence of all suprema and thus makes a partially ordered set into a complete lattice. Thus, when a top element (the infimum of the empty set) is adjoined to a Scott domain, one can conclude that:\nConsequently, Scott domains are in a sense \"almost\" algebraic lattices.\n\nScott domains become topological spaces by introducing the Scott topology.\n\nScott domains are intended to represent \"partial algebraic data\", ordered by information content. An element formula_2 is a piece of data that might not be fully defined. The statement formula_3 means \"formula_4 contains all the information that formula_5 does\".\n\nWith this interpretation we can see that the supremum formula_6 of a subset formula_7 is the element that contains all the information that \"any\" element of formula_8 contains, but \"no more\". Obviously such a supremum only exists (i.e., makes sense) provided formula_8 does not contain inconsistent information; hence the domain is directed and bounded complete, but not \"all\" suprema necessarily exist. The algebraicity axiom essentially ensures that all elements get all their information from (non-strictly) lower down in the ordering; in particular, the jump from compact or \"finite\" to non-compact or \"infinite\" elements does not covertly introduce any extra information that cannot be reached at some finite stage. The bottom element is the supremum of the empty set, i.e. the element containing no information at all; its existence is implied by bounded completeness, since, vacuously, the empty set has an upper bound in any non-empty poset.\n\nOn the other hand, the infimum formula_10 is the element that contains all the information that is shared by \"all\" elements of formula_8, and \"no less\". If formula_8 contains no consistent information, then its elements have no information in common and so its infimum is formula_1. In this way all infima exist, but not all infima are necessarily interesting.\n\nThis definition in terms of partial data allows an algebra to be defined as the limit of a sequence of increasingly more defined partial algebras—in other words a fixed point of an operator that adds progressively more information to the algebra. For more information, see Domain theory.\n\n\n\"See the literature given for domain theory.\"\n"}
{"id": "47565724", "url": "https://en.wikipedia.org/wiki?curid=47565724", "title": "Seiffert's spiral", "text": "Seiffert's spiral\n\nSeiffert's spherical spiral is a curve on a sphere made by moving on the sphere with constant speed and angular velocity with respect to a fixed diameter. (If one takes the diameter to be the line from the north pole to the south pole, then the requirement of constant angular velocity means that the longitude of the moving point is changing at a constant rate.) The cylindrical coordinates of the varying point on this curve turn out to be given by the Jacobian elliptic functions.\n\n"}
{"id": "2833933", "url": "https://en.wikipedia.org/wiki?curid=2833933", "title": "Separable state", "text": "Separable state\n\nIn quantum mechanics, separable quantum states are states without quantum entanglement.\n\nFor simplicity, the following assumes all relevant state spaces are finite-dimensional. First, consider separability for pure states. \n\nLet formula_1 and formula_2 be quantum mechanical state spaces, that is, finite-dimensional Hilbert spaces with basis states formula_3 and formula_4, respectively. By a postulate of quantum mechanics, the state space of the composite system is given by the tensor product\n\nwith base states formula_6, or in more compact notation formula_7. From the very definition of the tensor product, any vector of norm 1, i.e. a pure state of the composite system, can be written as\n\nwhere formula_9 is a constant. \nIf a pure state formula_10 can be written in the form formula_11 where formula_12 is a pure state of the i-th subsystem, it is said to be \"separable\". Otherwise it is called \"entangled\". When a system is in an entangled pure state, it is not possible to assign states to its subsystems. This will be true, in the appropriate sense, for the mixed state case as well.\n\nFormally, the embedding of a product of states into the product space is given by the Segre embedding. That is, a quantum-mechanical pure state is separable if and only if it is in the image of the Segre embedding.\n\nThe above discussion can be extended to the case of when the state space is infinite-dimensional with virtually nothing changed.\n\nConsider the mixed state case. A mixed state of the composite system is described by a density matrix formula_13 acting on formula_5. ρ is separable if there exist formula_15, formula_16 and formula_17 which are mixed states of the respective subsystems such that\n\nwhere\n\nOtherwise formula_13 is called an entangled state. We can assume without loss of generality in the above expression that formula_16 and formula_17 are all rank-1 projections, that is, they represent \"pure ensembles\" of the appropriate subsystems. It is clear from the definition that the family of separable states is a convex set.\n\nNotice that, again from the definition of the tensor product, any density matrix, indeed any matrix acting on the composite state space, can be trivially written in the desired form, if we drop the requirement that formula_16 and formula_17 are themselves states and formula_25 If these requirements are satisfied, then we can interpret the total state as a probability distribution over uncorrelated product states.\n\nIn terms of quantum channels, a separable state can be created from any other state using local actions and classical communication while an entangled state cannot.\n\nWhen the state spaces are infinite-dimensional, density matrices are replaced by positive trace class operators with trace 1, and a state is separable if it can be approximated, in trace norm, by states of the above form.\n\nIf there is only a single non-zero formula_26, then the state is called simply separable (or it is called a \"product state\").\n\nThe above discussion generalizes easily to the case of a quantum system consisting of more than two subsystems. Let a system have \"n\" subsystems and have state space formula_27. A pure state formula_28 is separable if it takes the form\n\nSimilarly, a mixed state ρ acting on \"H\" is separable if it is a convex sum\n\nOr, in the infinite-dimensional case, ρ is separable if it can be approximated in the trace norm by states of the above form.\n\nThe problem of deciding whether a state is separable in general is sometimes called the separability problem in quantum information theory. It is considered to be a difficult problem. It has been shown to be NP-hard. Some appreciation for this difficulty can be obtained if one attempts to solve the problem by employing the direct brute force approach, for a fixed dimension. We see that the problem quickly becomes intractable, even for low dimensions. Thus more sophisticated formulations are required. The separability problem is a subject of current research.\n\nA \"separability criterion\" is a necessary condition a state must satisfy to be separable. In the low-dimensional (\"2 X 2\" and \"2 X 3\") cases, the Peres-Horodecki criterion is actually a necessary and sufficient condition for separability. Other separability criteria include the range criterion and reduction criterion. See Ref. for a review of separability criteria in discrete variable systems.\n\nIn continuous variable systems, the Peres-Horodecki criterion also applies. Specifically, Simon formulated a particular version of the Peres-Horodecki criterion in terms of the second-order moments of canonical operators and showed that it is necessary and sufficient for formula_31-mode Gaussian states (see Ref. for a seemingly different but essentially equivalent approach). It was later found that Simon's condition is also necessary and sufficient for formula_32-mode Gaussian states, but no longer sufficient for formula_33-mode Gaussian states. Simon's condition can be generalized by taking into account the higher order moments of canonical operators or by using entropic measures.\n\nQuantum mechanics may be modelled on a projective Hilbert space, and the categorical product of two such spaces is the Segre embedding. In the bipartite case, a quantum state is separable if and only if it lies in the image of the Segre embedding.\nJon Magne Leinaas, Jan Myrheim and Eirik Ovrum in their paper \"Geometrical aspects of entanglement\" describe the problem and study the geometry of the separable states as a subset of the general state matrices. This subset have some intersection with the subset of states holding Peres-Horodecki criterion. In this paper, Leinaas et al. also give a numerical approach to test for separability in the general case.\n\nSince separability testing in a general case is an NP-hard. problem, in their paper, Leinaas et al. offer a numerical approach, iteratively refining an estimated separable state towards the target state to be tested, checking if the target state can indeed be reached. An implementation of the algorithm (including a built in Peres-Horodecki criterion testing) is brought in the \"StateSeparator\" web-app\n\n\n"}
{"id": "363540", "url": "https://en.wikipedia.org/wiki?curid=363540", "title": "Serre–Swan theorem", "text": "Serre–Swan theorem\n\nIn the mathematical fields of topology and K-theory, the Serre–Swan theorem, also called Swan's theorem, relates the geometric notion of vector bundles to the algebraic concept of projective modules and gives rise to a common intuition throughout mathematics: \"projective modules over commutative rings are like vector bundles on compact spaces\".\n\nThe two precise formulations of the theorems differ somewhat. The original theorem, as stated by Jean-Pierre Serre in 1955, is more algebraic in nature, and concerns vector bundles on an algebraic variety over an algebraically closed field (of any characteristic). The complementary variant stated by Richard Swan in 1962 is more analytic, and concerns (real, complex, or quaternionic) vector bundles on a smooth manifold or Hausdorff space.\n\nSuppose \"M\" is a smooth manifold (not necessarily compact), and a \"E\" is a smooth vector bundle over \"M\". Then \"Γ(E)\", the space of smooth sections of \"E\", is a module over C(\"M\") (the commutative algebra of smooth real-valued functions on \"M\"). Swan's theorem states that this module is finitely generated and projective over C(\"M\"). In other words, every vector bundle is a direct summand of some trivial bundle: formula_1 for some \"k\". The theorem can be proved by constructing a bundle epimorphism from a trivial bundle formula_2 This can be done by, for instance, exhibiting sections \"s\"...\"s\" with the property that for each point \"p\", {\"s\"(\"p\")} span the fiber over \"p\". \n\nWhen \"M\" is connected, the converse is also true: every finitely generated projective module over C(\"M\") arises in this way from some smooth vector bundle on \"M\". Such a module can be viewed as a smooth function \"f\" on \"M\" with values in the \"n\" × \"n\" idempotent matrices for some \"n\". The fiber of the corresponding vector bundle over \"x\" is then the range of \"f\"(\"x\"). If \"M\" is not connected, the converse does not hold unless one allows for vector bundles of non-constant rank (which means admitting manifolds of non-constant dimension). For example, if \"M\" is a zero-dimensional 2-point manifold, the module formula_3 is finitely-generated and projective over formula_4 but is not free, and so cannot correspond to the sections of any (constant-rank) vector bundle over \"M\" (all of which are trivial).\n\nAnother way of stating the above is that for any connected smooth manifold \"M\", the section functor \"Γ\" from the category of smooth vector bundles over \"M\" to the category of finitely generated, projective C(\"M\")-modules is full, faithful, and essentially surjective. Therefore the category of smooth vector bundles on \"M\" is equivalent to the category of finitely generated, projective C(\"M\")-modules. Details may be found in .\n\nSuppose \"X\" is a compact Hausdorff space, and C(\"X\") is the ring of continuous real-valued functions on \"X\". Analogous to the result above, the category of real vector bundles on \"X\" is equivalent to the category of finitely generated projective modules over C(\"X\"). The same result holds if one replaces \"real-valued\" by \"complex-valued\" and \"real vector bundle\" by \"complex vector bundle\", but it does not hold if one replace the field by a totally disconnected field like the rational numbers.\n\nIn detail, let Vec(\"X\") be the category of complex vector bundles over \"X\", and let ProjMod(C(\"X\")) be the category of finitely generated projective modules over the C*-algebra C(\"X\"). There is a functor Γ : Vec(\"X\") → ProjMod(C(\"X\")) which sends each complex vector bundle \"E\" over \"X\" to the C(\"X\")-module Γ(\"X\", \"E\") of sections. If formula_5 is a morphism of vector bundles over \"X\" then formula_6 and it follows that\n\ngiving the map \n\nwhich respects the module structure . Swan's theorem asserts that the functor Γ is an equivalence of categories.\n\nThe analogous result in algebraic geometry, due to applies to vector bundles in the category of affine varieties. Let \"X\" be an affine variety with structure sheaf formula_9 and formula_10 a coherent sheaf of formula_11 -modules on \"X\". Then formula_10 is the sheaf of germs of a finite-dimensional vector bundle if and only if formula_13 the space of sections of formula_14 is a projective module over the commutative ring formula_15\n\n"}
{"id": "21975332", "url": "https://en.wikipedia.org/wiki?curid=21975332", "title": "Soliton distribution", "text": "Soliton distribution\n\nA soliton distribution is a type of discrete probability distribution that arises in the theory of erasure correcting codes. A paper by Luby introduced two forms of such distributions, the ideal soliton distribution and the robust soliton distribution.\n\nThe ideal soliton distribution is a probability distribution on the integers from 1 to \"N\", where \"N\" is the single parameter of the distribution. The probability mass function is given by\n\nThe robust form of distribution is defined by adding an extra set of values to the elements of mass function of the ideal soliton distribution and then standardising so that the values add up to 1. The extra set of values, \"t\", are defined in terms of an additional real-valued parameter \"δ\" (which is interpreted as a failure probability) and an integer parameter \"M\" (\"M\" < \"N\") . Define \"R\" as \"R\"=\"N\"/\"M\". Then the values added to \"p\"(\"i\"), before the final standardisation, are\nWhile the ideal soliton distribution has a mode (or spike) at 1, the effect of the extra component in the robust distribution is to add an additional spike at the value \"M\".\n\n"}
{"id": "22457847", "url": "https://en.wikipedia.org/wiki?curid=22457847", "title": "Spectral signal-to-noise ratio", "text": "Spectral signal-to-noise ratio\n\nIn applied mathematics, the two-dimensional Spectral signal-to-noise ratio (SSNR) measures the normalised cross-correlation coefficient between several two-dimensional images over corresponding rings in Fourier space as a function of spatial frequency . It is a multi-particle extension of the Fourier ring correlation (FRC), which is related to the Fourier shell correlation. The SSNR is a popular method for finding the resolution of a class average in cryo-electron microscopy.\n\nwhere formula_2 is the complex structure Factor for image k for a pixel formula_3 at radius formula_4. It is possible convert the SSNR into an equivalent FRC using the following formula:\n\n\n"}
{"id": "767637", "url": "https://en.wikipedia.org/wiki?curid=767637", "title": "System F", "text": "System F\n\nSystem F, also known as the (Girard–Reynolds) polymorphic lambda calculus or the second-order lambda calculus, is a typed lambda calculus that differs from the simply typed lambda calculus by the introduction of a mechanism of universal quantification over types. System F thus formalizes the notion of parametric polymorphism in programming languages, and forms a theoretical basis for languages such as Haskell and ML. System F was discovered independently by logician Jean-Yves Girard (1972) and computer scientist John C. Reynolds (1974).\n\nWhereas simply typed lambda calculus has variables ranging over functions, and binders for them, System F additionally has variables ranging over \"types\", and binders for them. As an example, the fact that the identity function can have any type of the form A→ A would be formalized in System F as the judgment\n\nwhere formula_2 is a type variable. The upper-case formula_3 is traditionally used to denote type-level functions, as opposed to the lower-case formula_4 which is used for value-level functions. (The superscripted formula_2 means that the bound \"x\" is of type formula_2; the expression after the colon is the type of the lambda expression preceding it.)\n\nAs a term rewriting system, System F is strongly normalizing. However, type inference in System F (without explicit type annotations) is undecidable. Under the Curry–Howard isomorphism, System F corresponds to the fragment of second-order intuitionistic logic that uses only universal quantification. System F can be seen as part of the lambda cube, together with even more expressive typed lambda calculi, including those with dependent types.\n\nAccording to Girard, the \"F\" in \"System F\" was picked by chance.\n\nThe formula_7 type is defined as:\nformula_8, where formula_9 is a type variable. This means: formula_7 is the type of all functions which take as input a type α and two expressions of type α, and produce as output an expression of type α (note that we consider formula_11 to be right-associative.)\n\nThe following two definitions for the boolean values formula_12 and formula_13 are used, extending the definition of Church booleans:\n\nThen, with these two formula_21-terms, we can define some logic operators (which are of type formula_22):\n\nAs in Church encodings, there is no need for an IFTHENELSE function as one can just use raw formula_17-typed terms as decision functions. However, if one is requested:\nwill do.\nA \"predicate\" is a function which returns a formula_17-typed value. The most fundamental predicate is ISZERO which returns formula_12 if and only if its argument is the Church numeral 0:\n\nSystem F allows recursive constructions to be embedded in a natural manner, related to that in Martin-Löf's type theory. Abstract structures (S) are created using \"constructors\". These are functions typed as:\n\nRecursivity is manifested when formula_30 itself appears within one of the types formula_31. If you have formula_32 of these constructors, you can define the type of formula_30 as:\n\nFor instance, the natural numbers can be defined as an inductive datatype formula_35 with constructors\nThe System F type corresponding to this structure is\nformula_38. The terms of this type comprise a typed version of the Church numerals, the first few of which are:\n\nIf we reverse the order of the curried arguments (\"i.e.,\" formula_43), then the Church numeral for formula_44 is a function that takes a function \"f\" as argument and returns the formula_44 power of \"f\". That is to say, a Church numeral is a higher-order function – it takes a single-argument function \"f\", and returns another single-argument function.\n\nThe version of System F used in this article is as an explicitly typed, or Church-style, calculus. The typing information contained in λ-terms makes type-checking straightforward. Joe Wells (1994) settled an \"embarrassing open problem\" by proving that type checking is undecidable for a Curry-style variant of System F, that is, one that lacks explicit typing annotations.\n\nWells's result implies that type inference for System F is impossible.\nA restriction of System F known as \"Hindley–Milner\", or simply \"HM\", does have an easy type inference algorithm and is used for many statically typed functional programming languages such as Haskell 98 and ML. Over time, as the restrictions of HM-style type systems have become apparent, languages have steadily moved to more expressive logics for their type systems. As of 2008, GHC, a Haskell compiler, goes beyond HM, and now uses System F extended with non-syntactic type equality.\nF# is designed from scratch with System F in mind.\n\nWhile System F corresponds to the first axis of Barendregt's lambda cube, System F or the higher-order polymorphic lambda calculus combines the first axis (polymorphism) with the second axis (type operators); it is a different, more complex system.\n\nSystem F can be defined inductively on a family of systems, where induction is based on the kinds permitted in each system:\n\n\nIn the limit, we can define system formula_51 to be\n\n\nThat is, F is the system which allows functions from types to types where the argument (and result) may be of any order.\n\nNote that although F places no restrictions on the \"order\" of the arguments in these mappings, it does restrict the \"universe\" of the arguments for these mappings: they must be types rather than values. System F does not permit mappings from values to types (Dependent types), though it does permit mappings from values to values (formula_4 abstraction), mappings from types to values (formula_3 abstraction, sometimes written formula_55) and mappings from types to types (formula_4 abstraction at the level of types)\n\n\n\n\n"}
{"id": "9701718", "url": "https://en.wikipedia.org/wiki?curid=9701718", "title": "Sørensen–Dice coefficient", "text": "Sørensen–Dice coefficient\n\nThe Sørensen–Dice coefficient, also known by other names (see Name, below), is a statistic used for comparing the similarity of two samples. It was independently developed by the botanists Thorvald Sørensen and Lee Raymond Dice, who published in 1948 and 1945 respectively.\n\nThe index is known by several other names, especially \"Sørensen–Dice index\", \"Sørensen index\" and \"Dice's coefficient\". Other variations include the \"similarity coefficient\" or \"index\", such as \"Dice similarity coefficient (DSC)\". Common alternate spellings for Sørensen are Sorenson, Soerenson and Sörenson, and all three can also be seen with the –sen ending.\n\nOther names include:\n\nSørensen's original formula was intended to be applied to binary data. Given two sets, X and Y, it is defined as\n\nwhere |\"X\"| and |\"Y\"| are the cardinalities of the two sets (i.e. the number of elements in each set).\nThe Sørensen index equals twice the number of elements common to both sets divided by the sum of the number of elements in each set.\n\nWhen applied to boolean data, using the definition of true positive (TP), false positive (FP), and false negative (FN), it can be written as\n\nIt is different from the Jaccard index which only counts true positives once in both the numerator and denominator. DSC is the quotient of similarity and ranges between 0 and 1. It can be viewed as a similarity measure over sets.\n\nSimilarly to the Jaccard index, the set operations can be expressed in terms of vector operations over binary vectors a and b:\n\nis not a proper distance metric as it does not possess the property of triangle inequality. The simplest counterexample of this is given by the three sets {a}, {b}, and {a,b}, the distance between the first two being 1, and the difference between the third and each of the others being one-third. To satisfy the triangle inequality, the sum of \"any\" two of these three sides must be greater than or equal to the remaining side. However, the distance between {a} and {a,b} plus the distance between {b} and {a,b} equals 2/3 and is therefore less than the distance between {a} and {b} which is 1.\n\nThe Sørensen–Dice coefficient is useful for ecological community data (e.g. Looman & Campbell, 1960). Justification for its use is primarily empirical rather than theoretical (although it can be justified theoretically as the intersection of two fuzzy sets). As compared to Euclidean distance, the Sørensen distance retains sensitivity in more heterogeneous data sets and gives less weight to outliers. Recently the Dice score (and its variations, e.g. logDice taking a logarithm of it) has become popular in computer lexicography for measuring the lexical association score of two given words. It is also commonly used in image segmentation, in particular for comparing algorithm output against reference masks in medical applications.\n\nThe expression is easily extended to abundance instead of presence/absence of species. This quantitative version is known by several names:\n\n"}
{"id": "16267972", "url": "https://en.wikipedia.org/wiki?curid=16267972", "title": "Vectorial Mechanics", "text": "Vectorial Mechanics\n\nVectorial Mechanics (1948) is a book on vector manipulation (i.e., vector methods) by Edward Arthur Milne, a highly decorated (e.g., James Scott Prize Lectureship) British astrophysicist and mathematician. Milne states that the text was due to conversations (circa 1924) with his then-colleague and erstwhile teacher Sydney Chapman who viewed vectors not merely as a pretty toy but as a powerful weapon of applied mathematics. Milne states that he did not at first believe Chapman, holding on to the idea that \"vectors were like a pocket-rule, which needs to be unfolded before it can be applied and used.\" In time, however, Milne convinces himself that Chapman was right.\n\n\"Vectorial Mechanics\" has 18 chapters grouped into 3 parts. Part I is on \"vector algebra\" including chapters on a definition of a vector, products of vectors, elementary tensor analysis, and integral theorems. Part II is on \"systems of line vectors\" including chapters on line co-ordinates, systems of line vectors, statics of rigid bodies, the displacement of a rigid body, and the work of a system of line vectors. Part III is on \"dynamics\" including kinematics, particle dynamics, types of particle motion, dynamics of systems of particles, rigid bodies in motion, dynamics of rigid bodies, motion of a rigid body about its center of mass, gyrostatic problems, and impulsive motion.\n\nThere were significant reviews given near the time of original publication.\n\nG.J.Whitrow:\n\nDaniel C. Lewis: \n\nThe reviewer has long felt that the role of vector analysis in mechanics has been much overemphasized. It is true that the fundamental equations of motion in their various forms, especially in the case of rigid bodies, can be derived with greatest economy of thought by use of vectors (assuming that the requisite technique has already been developed); but once the equations have been set up, the usual procedure is to drop vector methods in their solution. If this position can be successfully refuted, this has been done in the present work, the most novel feature of which is to solve the vector differential equations by vector methods without ever writing down the corresponding scalar differential equations obtained by taking components. The author has certainly been successful in showing that this can be done in fairly simple, though nontrivial, cases. To give an example of a definitely nontrivial problem solved in this way, one might mention the nonholonomic problem afforded by the motion of a sphere rolling on a rough inclined plane or on a rough spherical surface. The author's methods are interesting and aesthetically satisfying and therefore deserve the widest publication even if they partake of the nature of a tour de force.\n"}
{"id": "168864", "url": "https://en.wikipedia.org/wiki?curid=168864", "title": "Well-ordering principle", "text": "Well-ordering principle\n\nIn mathematics, the well-ordering principle states that every non-empty set of positive integers contains a least element. In other words, the set of positive integers is well-ordered.\n\nThe phrase \"well-ordering principle\" is sometimes taken to be synonymous with the \"well-ordering theorem\". On other occasions it is understood to be the proposition that the set of integers {…, −2, −1, 0, 1, 2, 3, …} contains a well-ordered subset, called the natural numbers, in which every nonempty subset contains a least element.\n\nDepending on the framework in which the natural numbers are introduced, this (second order) property of the set of natural numbers is either an axiom or a provable theorem. For example:\n\nIn the second sense, this phrase is used when that proposition is relied on for the purpose of justifying proofs that take the following form: to prove that every natural number belongs to a specified set S, assume the contrary, which implies that the set of counterexamples is non-empty and thus contains a smallest counterexample. Then show that for any counterexample there is a still smaller counterexample, producing a contradiction. This mode of argument is the contrapositive of proof by complete induction. It is known light-heartedly as the \"minimal criminal\" method and is similar in its nature to Fermat's method of \"infinite descent\".\n\nGarrett Birkhoff and Saunders Mac Lane wrote in \"A Survey of Modern Algebra\" that this property, like the least upper bound axiom for real numbers, is non-algebraic; i.e., it cannot be deduced from the algebraic properties of the integers (which form an ordered integral domain).\n"}
{"id": "1964554", "url": "https://en.wikipedia.org/wiki?curid=1964554", "title": "Whitney umbrella", "text": "Whitney umbrella\n\nIn mathematics, the Whitney umbrella (or Whitney's umbrella, named after American mathematician Hassler Whitney, and sometimes called a Cayley umbrella) is a specific self-intersecting surface placed in three dimensions. It is the union of all straight lines that pass through points of a fixed parabola and are perpendicular to a fixed straight line, parallel to the axis of the parabola and lying on its perpendicular bisecting plane.\n\nWhitney's umbrella can be given by the parametric equations in Cartesian coordinates\nformula_1;\nformula_2;\nformula_3\nwhere the parameters \"u\" and \"v\" range over the real numbers. It is also given by the implicit equation\nThis formula also includes the negative \"z\" axis (which is called the \"handle\" of the umbrella).\n\nWhitney's umbrella is a ruled surface and a right conoid. It is important in the field of singularity theory, as a simple local model of a pinch point singularity. The pinch point and the fold singularity are the only stable local singularities of maps from R to R.\n\nIt is named after the American mathematician Hassler Whitney.\n\nIn string theory, a Whitney brane is a D7-brane wrapping a variety whose singularities are locally modeled by the Whitney umbrella. Whitney branes appear naturally when taking Sen's weak coupling limit of F-theory.\n\n\n"}
