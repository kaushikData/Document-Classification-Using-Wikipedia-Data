{"id": "3261224", "url": "https://en.wikipedia.org/wiki?curid=3261224", "title": "Abel's identity", "text": "Abel's identity\n\nIn mathematics, Abel's identity (also called as Abel's Formula or Abel's differential equation identity) is an equation that expresses the Wronskian of two solutions of a homogeneous second-order linear ordinary differential equation in terms of a coefficient of the original differential equation.\nThe relation can be generalised to \"n\"th-order linear ordinary differential equations. The identity is named after the Norwegian mathematician Niels Henrik Abel.\n\nSince Abel's identity relates the different linearly independent solutions of the differential equation, it can be used to find one solution from the other. It provides useful identities relating the solutions, and is also useful as a part of other techniques such as the method of variation of parameters. It is especially useful for equations such as Bessel's equation where the solutions do not have a simple analytical form, because in such cases the Wronskian is difficult to compute directly.\n\nA generalisation to first-order systems of homogeneous linear differential equations is given by Liouville's formula.\n\nConsider a homogeneous linear second-order ordinary differential equation\n\non an interval \"I\" of the real line with real- or complex-valued continuous functions \"p\" and \"q\". Abel's identity states that the Wronskian \"W\"(\"y\",\"y\") of two real- or complex-valued solutions \"y\" and \"y\" of this differential equation, that is the function defined by the determinant\n\nsatisfies the relation\n\nfor every point \"x\" in \"I\", where \"C\" is an arbitrary constant.\n\n\nDifferentiating the Wronskian using the product rule gives (writing \"W\" for \"W\"(\"y\",\"y\") and omitting the argument \"x\" for brevity)\n\nSolving for formula_5 in the original differential equation yields\n\nSubstituting this result into the derivative of the Wronskian function to replace the second derivatives of \"y\" and \"y\" gives\n\nThis is a first-order linear differential equation, and it remains to show that Abel's identity gives the unique solution, which attains the value \"W\"(\"x\") at \"x\". Since the function \"p\" is continuous on \"I\", it is bounded on every closed and bounded subinterval of \"I\" and therefore integrable, hence\n\nis a well-defined function. Differentiating both sides, using the product rule, the chain rule, the derivative of the exponential function and the fundamental theorem of calculus, one obtains\n\ndue to the differential equation for \"W\". Therefore, \"V\" has to be constant on \"I\", because otherwise we would obtain a contradiction to the mean value theorem (applied separately to the real and imaginary part in the complex-valued case). Since \"V\"(\"x\") = \"W\"(\"x\"), Abel's identity follows by solving the definition of \"V\" for \"W\"(\"x\").\n\nConsider a homogeneous linear \"n\"th-order (\"n\" ≥ 1) ordinary differential equation\n\non an interval \"I\" of the real line with a real- or complex-valued continuous function \"p\". The generalisation of Abel's identity states that the Wronskian \"W\"(\"y\",…,\"y\") of \"n\" real- or complex-valued solutions \"y\",…,\"y\" of this \"n\"th-order differential equation, that is the function defined by the determinant\n\nsatisfies the relation\n\nfor every point \"x\" in \"I\".\n\nFor brevity, we write \"W\" for \"W\"(\"y\",…,\"y\") and omit the argument \"x\". It suffices to show that the Wronskian solves the first-order linear differential equation\n\nbecause the remaining part of the proof then coincides with the one for the case \"n\" = 2.\n\nIn the case \"n\" = 1 we have \"W\" = \"y\" and the differential equation for \"W\" coincides with the one for \"y\". Therefore, assume \"n\" ≥ 2 in the following.\n\nThe derivative of the Wronskian \"W\" is the derivative of the defining determinant. It follows from the Leibniz formula for determinants that this derivative can be calculated by differentiating every row separately, hence\n\nHowever, note that every determinant from the expansion contains a pair of identical rows, except the last one. Since determinants with linearly dependent rows are equal to 0, one is only left with the last one:\n\nSince every \"y\" solves the ordinary differential equation, we have\n\nfor every \"i\" ∈ {1...,\"n\"}. Hence, adding to the last row of the above determinant \"p\" times its first row, \"p\" times its second row, and so on until \"p\" times its next to last row, the value of the determinant for the derivative of \"W\" is unchanged and we get\n\nThe solutions \"y\",…,\"y\" form the square-matrix valued solution\n\nof the \"n\"-dimensional first-order system of homogeneous linear differential equations\n\nThe trace of this matrix is −\"p\"(\"x\"), hence Abel's identity follows directly from Liouville's formula.\n\n"}
{"id": "38253307", "url": "https://en.wikipedia.org/wiki?curid=38253307", "title": "Bartlett's theorem", "text": "Bartlett's theorem\n\nIn queueing theory, Bartlett's theorem gives the distribution of the number of customers in a given part of a system at a fixed time.\n\nSuppose that customers arrive according to a non-stationary Poisson process with rate \"A\"(\"t\"), and that subsequently they move independently around a system of nodes. Write \"E\" for some particular part of the system and \"p\"(\"s\",\"t\") the probability that a customer who arrives at time \"s\" is in \"E\" at time \"t\". Then the number of customers in \"E\" at time \"t\" has a Poisson distribution with mean\n"}
{"id": "5164765", "url": "https://en.wikipedia.org/wiki?curid=5164765", "title": "Bifurcation locus", "text": "Bifurcation locus\n\nIn complex dynamics, the bifurcation locus of a family of holomorphic functions informally is a locus of those maps for which the dynamical behavior changes drastically under a small perturbation of the parameter. Thus the bifurcation locus can be thought of as an analog of the Julia set in parameter space. Without doubt, the most famous example of a bifurcation locus is the boundary of the Mandelbrot set.\n\nParameters in the complement of the bifurcation locus are called J-stable.\n\n\n"}
{"id": "38954098", "url": "https://en.wikipedia.org/wiki?curid=38954098", "title": "Blocking set", "text": "Blocking set\n\nIn geometry, specifically projective geometry, a blocking set is a set of points in a projective plane which every line intersects and which does not contain an entire line. The concept can be generalized in several ways. Instead of talking about points and lines, one could deal with \"n\"-dimensional subspaces and \"m\"-dimensional subspaces, or even more generally, objects of type 1 and objects of type 2 when some concept of intersection makes sense for these objects. A second way to generalize would be to move into more abstract settings than projective geometry. One can define a blocking set of a hypergraph as a set that meets all edges of the hypergraph.\n\nIn a finite projective plane π of order \"n\", a blocking set is a set of points of π which every line intersects and which contains no line completely. Under this definition, if \"B\" is a blocking set, then complementary set of points, π\\\"B\" is also a blocking set. A blocking set \"B\" is \"minimal\" if the removal of any point of \"B\" leaves a set which is not a blocking set. A blocking set of smallest size is called a \"committee\". Every committee is a minimal blocking set, but not all minimal blocking sets are committees. Blocking sets exist in all projective planes except for the smallest projective plane of order 2, the Fano plane.\n\nIt is sometimes useful to drop the condition that a blocking set does not contain a line. Under this extended definition, and since, in a projective plane every pair of lines meet, every line would be a blocking set. Blocking sets which contained lines would be called \"trivial\" blocking sets.\n\nIn any projective plane of order \"n\" (each line contains \"n\" + 1 points), the points on the lines forming a triangle without the vertices of the triangle (3(\"n\" - 1) points) form a minimal blocking set (if \"n\" = 2 this blocking set is trivial) which in general is not a committee.\n\nAnother general construction in an arbitrary projective plane of order \"n\" is to take all except one point, say \"P\", on a given line and then one point on each of the other lines through \"P\", making sure that these points are not all collinear (this last condition can not be satisfied if \"n\" = 2.) This produces a minimal blocking set of size 2\"n\".\n\nA \"projective triangle\" β of \"side m\" in PG(2,\"q\") consists of 3(\"m\" - 1) points, \"m\" on each side of a triangle, such that the vertices \"A\", \"B\" and \"C\" of the triangle are in β, and the following condition is satisfied: If point \"P\" on line \"AB\" and point \"Q\" on line \"BC\" are both in β, then the point of intersection of \"PQ\" and \"AC\" is in β.\n\nA \"projective triad\" δ of side m is a set of 3\"m\" - 2 points, \"m\" of which lie on each of three concurrent lines such that the point of concurrency \"C\" is in δ and the following condition is satisfied: If a point \"P\" on one of the lines and a point \"Q\" on another line are in δ, then the point of intersection of \"PQ\" with the third line is in δ.\n\n\"Theorem\": In PG(2,\"q\") with \"q\" odd, there exists a projective triangle of side (\"q\" + 3)/2 which is a blocking set of size 3(\"q\" + 1)/2.\n\n\"Theorem\": In PG(2,\"q\") with \"q\" even, there exists a projective triad of side (\"q\" + 2)/2 which is a blocking set of size (3\"q\" + 2)/2.\n\nOne typically searches for small blocking sets. The minimum size of a blocking set of formula_1 is called formula_2.\n\nIn the Desarguesian projective plane of order \"q\", PG(2,\"q\"), the size of a blocking set \"B\" is bounded:\nWhen \"q\" is a square the lower bound is achieved by any Baer subplane and the upper bound comes from the complement of a Baer subplane.\n\nA more general result can be proved,\n\nAny blocking set in a projective plane π of order \"n\" has at least formula_4 points. Moreover, if this lower bound is met, then \"n\" is necessarily a square and the blocking set consists of the points in some Baer subplane of π.\n\nAn upper bound for the size of a minimal blocking set has the same flavor,\n\nAny minimal blocking set in a projective plane π of order \"n\" has at most formula_5 points. Moreover, if this upper bound is reached, then \"n\" is necessarily a square and the blocking set consists of the points of some unital embedded in π.\n\nWhen \"n\" is not a square less can be said about the smallest sized nontrivial blocking sets. One well known result due to Aart Blokhuis is:\n\n\"Theorem\": A nontrivial blocking set in PG(2,\"p\"), \"p\" a prime, has size at least 3(\"p\" + 1)/2.\n\nIn these planes a projective triangle which meets this bound exists.\n\nBlocking sets originated in the context of economic game theory in a 1956 paper by Moses Richardson. Players were identified with points in a finite projective plane and minimal winning coalitions were lines. A \"blocking coalition\" was defined as a set of points containing no line but intersecting every line. In 1958, J. R. Isbell studied these games from a non-geometric viewpoint. Jane W. DiPaola studied the minimum blocking coalitions in all the projective planes of order formula_6 in 1969.\n\nLet formula_7 be a hypergraph, so that formula_8 is a set of elements, and formula_9 is a collection of subsets of formula_8, called (hyper)edges. A blocking set of formula_1 is a subset formula_12 of formula_8 that has nonempty intersection with each hyperedge.\n\nBlocking sets are sometimes also called \"hitting sets\" or \"vertex covers\".\nAlso the term \"transversal\" is used, but in some contexts a transversal of formula_1 is a subset formula_15 of formula_8 that meets each hyperedge in exactly one point.\n\nA \"two-coloring\" of formula_1 is a partition formula_18 of formula_8\ninto two subsets (color classes) such that no edge is monochromatic, i.e., no edge is contained entirely within formula_20 or within formula_21. Now both formula_20 and formula_21 are blocking sets.\n\nIn a projective plane a complete \"k\"-arc is a set of \"k\" points, no three collinear, which can not be extended to a larger arc (thus, every point not on the arc is on a secant line of the arc–a line meeting the arc in two points.)\n\n\"Theorem\": Let \"K\" be a complete \"k\"-arc in Π = PG(2,\"q\") with \"k\" < \"q\" + 2. The dual in Π of the set of secant lines of \"K\" is a blocking set, \"B\", of size \"k\"(\"k\" - 1)/2.\n\nIn any projective plane of order \"q\", for any nontrivial blocking set \"B\" (with \"b\" = |\"B\"|, the size of the blocking set) consider a line meeting \"B\" in \"n\" points. Since no line is contained in \"B\", there must be a point, \"P\", on this line which is not in \"B\". The \"q\" other lines though \"P\" must each contain at least one point of \"B\" in order to be blocked. Thus, formula_24 If for some line equality holds in this relation, the blocking set is called a \"blocking set of Rédei type\" and the line a \"Rédei line\" of the blocking set (note that \"n\" will be the largest number of collinear points in \"B\"). Not all blocking sets are of Rédei type, but many of the smaller ones are. These sets are named after László Rédei whose monograph on Lacunary polynomials over finite fields was influential in the study of these sets.\n\nA set of points in the finite Desarguesian affine space formula_25 that intersects every hyperplane non-trivially, i.e., every hyperplane is incident with some point of the set, is called an affine blocking set. Identify the space with formula_26 by fixing a coordinate system. Then it is easily shown that the set of points lying on the coordinate axes form a blocking set of size formula_27. Jean Doyen conjectured in a 1976 Oberwolfach conference that this is the least possible size of a blocking set. \nThis was proved by R. E. Jamison in 1977, and independently by A. E. Brouwer, A. Schrijver in 1978 using the so-called polynomial method. Jamison proved the following general covering result from which the bound on affine blocking sets follows using duality:\n\n\"Let formula_28 be an formula_29 dimensional vector space over formula_30. Then the number of formula_31-dimensional cosets required to cover all vectors except the zero vector is at least formula_32. Moreover, this bound is sharp.\"\n\n"}
{"id": "43125933", "url": "https://en.wikipedia.org/wiki?curid=43125933", "title": "Breakthrough Prize in Mathematics", "text": "Breakthrough Prize in Mathematics\n\nThe Breakthrough Prize in Mathematics is an annual award of the Breakthrough Prize series announced in 2013. It is funded by Yuri Milner and Mark Zuckerberg and others. The annual award comes with a cash gift of $3 million, and up to three laureates are chosen for the New Horizons in Mathematics Prize, at $100,000 which is intended for early-career researchers.\n\nThe founders of the prize have stated that they want to help scientists to be perceived as celebrities again, and to reverse a 50-year \"downward trend\". They hope that this may make \"more young students [...] aspire to be scientists\".\n\nThe first awards of the Prize, worth $3 million to each recipient, were made in 2014 (for the year 2015) to:\n\n\nThe 2016 prize was announced in November 2015 and made to:\n\n\nThe 2017 prize was announced in December 2016, and it was made to:\n\n\nThe 2018 prize was announced in December 2017, and it was made to:\n\n\nThe 2019 prize was announced in October 2018, and it was made to:\n\n\n\n"}
{"id": "18827986", "url": "https://en.wikipedia.org/wiki?curid=18827986", "title": "Circle of antisimilitude", "text": "Circle of antisimilitude\n\nIn inversive geometry, the circle of antisimilitude (also known as mid-circle) of two circles, \"α\" and \"β\", is a reference circle for which \"α\" and \"β\" are inverses of each other. If \"α\" and \"β\" are non-intersecting or tangent, a single circle of antisimilitude exists; if \"α\" and \"β\" intersect at two points, there are two circles of antisimilitude. When \"α\" and \"β\" are congruent, the circle of antisimilitude degenerates to a line of symmetry through which \"α\" and \"β\" are reflections of each other.\n\nIf the two circles \"α\" and \"β\" cross each other, another two circles \"γ\" and \"δ\" are each tangent to both \"α\" and \"β\", and in addition \"γ\" and \"δ\" are tangent to each other, then the point of tangency between \"γ\" and \"δ\" necessarily lies on one of the two circles of antisimilitude. If \"α\" and \"β\" are disjoint and non-concentric, then the locus of points of tangency of \"γ\" and \"δ\" again forms two circles, but only one of these is the (unique) circle of antisimilitude. If \"α\" and \"β\" are tangent or concentric, then the locus of points of tangency degenerates to a single circle, which again is the circle of antisimilitude.\n\nIf the two circles \"α\" and \"β\" cross each other, then their two circles of antisimilitude each pass through both crossing points, and bisect the angles formed by the arcs of \"α\" and \"β\" as they cross.\n\nIf a circle \"γ\" crosses circles \"α\" and \"β\" at equal angles, then \"γ\" is crossed orthogonally by one of the circles of antisimilitude of \"α\" and \"β\"; if \"γ\" crosses \"α\" and \"β\" in supplementary angles, it is crossed orthogonally by the other circle of antisimilitude, and if \"γ\" is orthogonal to both \"α\" and \"β\" then it is also orthogonal to both circles of antisimilitude.\n\nSuppose that, for three circles \"α\", \"β\", and \"γ\", there is a circle of antisimilitude for the pair (\"α\",\"β\") that crosses a second circle of antisimilitude for the pair (\"β\",\"γ\"). Then there is a third circle of antisimiltude for the third pair (\"α\",\"γ\") such that the three circles of antisimilitude cross each other in two triple intersection points. Altogether, at most eight triple crossing points may be generated in this way, for there are two ways of choosing each of the first two circles and two points where the two chosen circles cross. These eight or fewer triple crossing points are the centers of inversions that take all three circles \"α\", \"β\", and \"γ\" to become equal circles. For three circles that are mutually externally tangent, the (unique) circles of antisimilitude for each pair again cross each other at 120° angles in two triple intersection points that are the isodynamic points of the triangle formed by the three points of tangency.\n\n"}
{"id": "15535840", "url": "https://en.wikipedia.org/wiki?curid=15535840", "title": "Clone (algebra)", "text": "Clone (algebra)\n\nIn universal algebra, a clone is a set \"C\" of finitary operations on a set \"A\" such that\nThe question whether clones should contain nullary operations or not is not treated uniformly in the literature. The classical approach as evidenced by the standard monographs on clone theory considers clones only containing at least unary operations. However, with only minor modifications (related to the empty invariant relation) most of the usual theory can be lifted to clones allowing nullary operations. The more general concept includes all clones without nullary operations as subclones of the clone of all at least unary operations and is in accordance with the custom to allow nullary terms and nullary term operations in universal algebra. Typically, publications studying clones as abstract clones, e.g. in the category theoretic setting of Lawvere's algebraic theories, will include nullary operations.\n\nGiven an algebra in a signature \"σ\", the set of operations on its carrier definable by a \"σ\"-term (the \"term functions\") is a clone. Conversely, every clone can be realized as the clone of term functions in a suitable algebra by simply taking the clone itself as source for the signature \"σ\" so that the algebra has the whole clone as its fundamental operations.\n\nIf \"A\" and \"B\" are algebras with the same carrier such that every basic function of \"A\" is a term function in \"B\" and vice versa, then \"A\" and \"B\" have the same clone. For this reason, modern universal algebra often treats clones as a representation of algebras which abstracts from their signature.\n\nThere is only one clone on the one-element set (there are two if nullary operations are considered). The lattice of clones on a two-element set is countable, and has been completely described by Emil Post (see Post's lattice, which traditionally does not show clones with nullary operations). Clones on larger sets do not admit a simple classification; there are continuum clones on a finite set of size at least three, and 2 clones on an infinite set of cardinality \"κ\".\n\nPhilip Hall introduced the concept of \"abstract clone\". An abstract clone is different from a concrete clone in that the set \"A\" is not given.\nFormally, an abstract clone comprises \nsuch that \n\nAny concrete clone determines an abstract clone in the obvious manner. \n\nAny algebraic theory determines an abstract clone where \"C\" is the set of terms in \"n\" variables, are variables, and ∗ is substitution. Two theories determine isomorphic clones if and only if the corresponding categories of algebras are isomorphic. Conversely every abstract clone determines an algebraic theory with an \"n\"-ary operation for each element of \"C\". This gives a bijective correspondence between abstract clones and algebraic theories.\n\nEvery abstract clone \"C\" induces a Lawvere theory in which the morphisms \"m\" → \"n\" are elements of (\"C\"). This induces a bijective correspondence between Lawvere theories and abstract clones.\n\n\n"}
{"id": "702837", "url": "https://en.wikipedia.org/wiki?curid=702837", "title": "Cocoloring", "text": "Cocoloring\n\nIn graph theory, a cocoloring of a graph \"G\" is an assignment of colors to the vertices such that each color class forms an independent set in \"G\" or in the complement of \"G\". The cochromatic number z(\"G\") of \"G\" is the fewest colors needed in any cocolorings of \"G\". The graphs with cochromatic number 2 are exactly the bipartite graphs, complements of bipartite graphs, and split graphs.\n\nAs the requirement that each color class be a clique or independent is weaker than the requirement for coloring (in which each color class must be an independent set) and stronger than for subcoloring (in which each color class must be a disjoint union of cliques), it follows that the cochromatic number of \"G\" is less than or equal to the chromatic number of \"G\", and that it is greater than or equal to the subchromatic number of \"G\".\n\nCocoloring was named and first studied by . characterizes critical 3-cochromatic graphs, while describe algorithms for approximating the cochromatic number of a graph. defines a class of \"perfect cochromatic graphs\", analogous to the definition of perfect graphs via graph coloring, and provides a forbidden subgraph characterization of these graphs.\n\n"}
{"id": "42806211", "url": "https://en.wikipedia.org/wiki?curid=42806211", "title": "Conway criterion", "text": "Conway criterion\n\nIn the mathematical theory of tessellations, the Conway criterion, named for the English mathematician John Horton Conway, describes rules for when a prototile will tile the plane; it consists of the following requirements: The tile must be a closed topological disk with six consecutive points A, B, C, D, E, and F on the boundary such that: \n\nAny prototile satisfying Conway's criterion admits a periodic tiling of the plane—and does so using only translation and 180-degree rotations. The Conway criterion is a sufficient condition to prove that a prototile tiles the plane but not a necessary one; there are tiles that fail the criterion and still tile the plane.\n\nIn its simplest form the criterion states that any hexagon with a pair of opposite sides that are parallel and congruent will tessellate the plane by translation, called hexagonal parallelogons. But when some of the points coincide, the criterion can apply to other polygons and even to shapes with curved perimeters.\n\nThe Conway criterion discriminates many shapes, especially polyforms: except the two tiling nonominoes on the right, all tiling polyominos up to the nonominoes can form a patch of at least one tile which satisfies the criterion. These figures also show that the criterion is a sufficient but not necessary condition for a prototile to tile the plane.\n\n"}
{"id": "23110294", "url": "https://en.wikipedia.org/wiki?curid=23110294", "title": "Cynthia Dwork", "text": "Cynthia Dwork\n\nCynthia Dwork (born 1958) is an American computer scientist at Harvard University, where she is Gordon McKay Professor of Computer Science, Radcliffe Alumnae Professor at the Radcliffe Institute for Advanced Study, and Affiliated Professor, Harvard Law School. She is a distinguished scientist at Microsoft Research.\n\nDwork received her B.S.E. from Princeton University in 1979, graduating Cum Laude, and receiving the Charles Ira Young Award for Excellence in Independent Research.\nDwork received her Ph.D. from Cornell University in 1983 for research supervised by John Hopcroft. \n\nDwork is known for her research placing privacy-preserving data analysis on a mathematically rigorous foundation, including the co-invention of differential privacy, a strong privacy guarantee frequently permitting highly accurate data analysis (with McSherry, Nissim, and Smith, 2006). The differential privacy definition provides guidelines for preserving the privacy of people who may have contributed data to a dataset, by adding small amounts of noise either to the input data or to outputs of computations performed on the data. She uses a systems-based approach to studying fairness in algorithms including those used for placing ads. Dwork has also made contributions in cryptography and distributed computing, and is a recipient of the Edsger W. Dijkstra Prize for her early work on the foundations of fault-tolerant systems. \n\nHer contributions in cryptography include Nonmalleable Cryptography with Danny Dolev and Moni Naor in 1991, the first lattice-based cryptosystem with Miklós Ajtai in 1997, which was also the first public-key cryptosystem for which breaking a random instance is as hard as solving the hardest instance of the underlying mathematical problem (\"worst-case/average-case equivalence\"). With Naor she also first presented the idea of, and a technique for, combating e-mail spam by requiring a proof of computational effort, also known as proof-of-work - a key technology underlying hashcash and bitcoin.\nHer publications include:\nShe was elected as a Fellow of the American Academy of Arts and Sciences (AAAS) in 2008, as a member of the National Academy of Engineering in 2008, as a member of the National Academy of Sciences in 2014, as a fellow of the Association for Computing Machinery (ACM) in 2015, and as a member of the American Philosophical Society in 2016. She received the Dijkstra Prize in 2007 for her work on consensus problems together with Nancy Lynch and Larry Stockmeyer. In 2009 she won the PET Award for Outstanding Research in Privacy Enhancing Technologies. 2017 Gödel Prize was awarded to Cynthia Dwork, Frank McSherry, Kobbi Nissim and Adam Smith for their seminal paper that introduced differential privacy.\n\nDwork is the daughter of American mathematician Bernard Dwork, and sister of historian Debórah Dwork.\nShe has a black belt in taekwondo.\n\n"}
{"id": "25910555", "url": "https://en.wikipedia.org/wiki?curid=25910555", "title": "Demonic composition", "text": "Demonic composition\n\nIn mathematics, demonic composition is an operation on binary relations that is somewhat comparable to ordinary composition of relations but is robust to refinement of the relations into (partial) functions or injective relations.\n\nUnlike ordinary composition of relations, demonic composition is not associative.\n\nSuppose \"R\" is a binary relation between \"X\" and \"Y\" and \"S\" is a relation between \"Y\" and \"Z\". Their right demonic composition \"R\" ; \"S\" is a relation between \"X\" and \"Z\". Its graph is defined as\n\nConversely, their left demonic composition \"R\" ; \"S\" is defined by\n"}
{"id": "55275", "url": "https://en.wikipedia.org/wiki?curid=55275", "title": "Denotational semantics", "text": "Denotational semantics\n\nIn computer science, denotational semantics (initially known as mathematical semantics or Scott–Strachey semantics) is an approach of formalizing the meanings of programming languages by constructing mathematical objects (called \"denotations\") that describe the meanings of expressions from the languages. Other approaches provide formal semantics of programming languages including axiomatic semantics and operational semantics.\n\nBroadly speaking, denotational semantics is concerned with finding mathematical objects called domains that represent what programs do. For example, programs (or program phrases) might be represented by partial functions or by games between the environment and the system.\n\nAn important tenet of denotational semantics is that \"semantics should be compositional\": the denotation of a program phrase should be built out of the denotations of its subphrases.\n\nDenotational semantics originated in the work of Christopher Strachey and Dana Scott published in the early 1970s. As originally developed by Strachey and Scott, denotational semantics provided the denotation (meaning) of a computer program as a function that mapped input into output. To give denotations to recursively defined programs, Scott proposed working with continuous functions between domains, specifically complete partial orders. As described below, work has continued in investigating appropriate denotational semantics for aspects of programming languages such as sequentiality, concurrency, non-determinism and local state.\n\nDenotational semantics have been developed for modern programming languages that use capabilities like concurrency and exceptions, e.g., Concurrent ML, CSP, and Haskell. The semantics of these languages is compositional in that the denotation of a phrase depends on the denotations of its subphrases. For example, the meaning of the applicative expression f(E1,E2) is defined in terms of semantics of its subphrases f, E1 and E2. In a modern programming language, E1 and E2 can be evaluated concurrently and the execution of one of them might affect the other by interacting through shared objects causing their denotations to be defined in terms of each other. Also, E1 or E2 might throw an exception which could terminate the execution of the other one. The sections below describe special cases of the semantics of these modern programming languages.\n\nDenotational semantics are given to a program phrase as a function from an environment (that has the values of its free variables) to its denotation. For example, the phrase n*m produces a denotation when provided with an environment that has binding for its two free variables: n and m. If in the environment n has the value 3 and m has the value 5, then the denotation is 15.\n\nA function can be modeled as denoting a set of ordered pairs where each ordered pair in the set consists of two parts (1) an argument for the function and (2) the value of the function for that argument. For example, the set of order pairs {[0 1] [4 3]} is the denotation of a function with value 1 for argument 0, value 3 for the argument 4, and is otherwise undefined.\n\nThe problem to be solved is to provide denotations for recursive programs that are defined in terms of themselves such as the definition of the factorial function as\n\nA solution is to build up the denotation by approximation. The factorial function is a total function from ℕ to ℕ (defined everywhere in its domain), but we model it as a partial function. At the beginning, we start with the empty function (an empty set). Next, we add the ordered pair [0 1] to the function to result in another partial function that better approximates the factorial function. Afterwards, we add yet another ordered pair [1 1] to create an even better approximation.\n\nIt is instructive to think of this chain of iteration as \"F\", \"F\", \"F\", … where \"F\" indicates \"i\"-many applications of \"F\".\n\nThis iterative process builds a sequence of partial functions from ℕ to ℕ. Partial functions form a chain-complete partial order using ⊆ as the ordering. Furthermore, this iterative process of better approximations of the factorial function forms an expansive (also called progressive) mapping because each formula_1 using ⊆ as the ordering. So by a fixed-point theorem (specifically Bourbaki–Witt theorem), there exists a fixed point for this iterative process.\n\nIn this case, the fixed point is the least upper bound of this chain, which is the full factorial function, which can be expressed as the direct limit\nHere, the symbol \"⊔\" is the directed join (of directed sets), meaning \"least upper bound\". The directed join is essentially the join of directed sets.\n\nThe concept of power domains has been developed to give a denotational semantics to non-deterministic sequential programs. Writing \"P\" for a power-domain constructor, the domain \"P\"(\"D\") is the domain of non-deterministic computations of type denoted by \"D\".\n\nThere are difficulties with fairness and unboundedness in domain-theoretic models of non-determinism.\n\nMany researchers have argued that the domain-theoretic models given above do not suffice for the more general case of concurrent computation. For this reason various new models have been introduced. In the early 1980s, people began using the style of denotational semantics to give semantics for concurrent languages. Examples include Will Clinger's work with the actor model; Glynn Winskel's work with event structures and petri nets; and the work by Francez, Hoare, Lehmann, and de Roever (1979) on trace semantics for CSP. All these lines of inquiry remain under investigation (see e.g. the various denotational models for CSP).\n\nRecently, Winskel and others have proposed the category of profunctors as a domain theory for concurrency.\n\nState (such as a heap) and simple imperative features can be straightforwardly modeled in the denotational semantics described above. All the textbooks below have the details. The key idea is to consider a command as a partial function on some domain of states. The denotation of \"x:=3\" is then the function that takes a state to the state with 3 assigned to x. The sequencing operator \";\" is denoted by composition of functions. Fixed-point constructions are then used to give a semantics to looping constructs, such as \"while\".\n\nThings become more difficult in modelling programs with local variables. One approach is to no longer work with domains, but instead to interpret types as functors from some category of worlds to a category of domains. Programs are then denoted by natural continuous functions between these functors.\n\nMany programming languages allow users to define recursive data types. For example, the type of lists of numbers can be specified by\nThis section deals only with functional data structures that cannot change. Conventional imperative programming languages would typically allow the elements of such a recursive list to be changed.\n\nFor another example: the type of denotations of the untyped lambda calculus is\nThe problem of \"solving domain equations\" is concerned with finding domains that model these kinds of datatypes. One approach, roughly speaking, is to consider the collection of all domains as a domain itself, and then solve the recursive definition there. The textbooks below give more details.\n\nPolymorphic data types are data types that are defined with a parameter. For example, the type of α lists is defined by\nLists of natural numbers, then, are of type nat list, while lists of strings are of string list.\n\nSome researchers have developed domain theoretic models of polymorphism. Other researchers have also modeled parametric polymorphism within constructive set theories. Details are found in the textbooks listed below.\n\nA recent research area has involved denotational semantics for object and class based programming languages.\n\nFollowing the development of programming languages based on linear logic, denotational semantics have been given to languages for linear usage (see e.g. proof nets, coherence spaces) and also polynomial time complexity.\n\nThe problem of full abstraction for the sequential programming language PCF was, for a long time, a big open question in denotational semantics. The difficulty with PCF is that it is a very sequential language. For example, there is no way to define the parallel-or function in PCF. It is for this reason that the approach using domains, as introduced above, yields a denotational semantics that is not fully abstract.\n\nThis open question was mostly resolved in the 1990s with the development of game semantics and also with techniques involving logical relations. For more details, see the page on PCF.\n\nIt is often useful to translate one programming language into another. For example, a concurrent programming language might be translated into a process calculus; a high-level programming language might be translated into byte-code. (Indeed, conventional denotational semantics can be seen as the interpretation of programming languages into the internal language of the category of domains.)\n\nIn this context, notions from denotational semantics, such as full abstraction, help to satisfy security concerns.\n\nIt is often considered important to connect denotational semantics with operational semantics. This is especially important when the denotational semantics is rather mathematical and abstract, and the operational semantics is more concrete or closer to the computational intuitions. The following properties of a denotational semantics are often of interest.\n\nAdditional desirable properties we may wish to hold between operational and denotational semantics are:\n\nAn important aspect of denotational semantics of programming languages is compositionality, by which the denotation of a program is constructed from denotations of its parts. For example, consider the expression \"7 + 4\". Compositionality in this case is to provide a meaning for \"7 + 4\" in terms of the meanings of \"7\", \"4\" and \"+\".\n\nA basic denotational semantics in domain theory is compositional because it is given as follows. We start by considering program fragments, i.e. programs with free variables. A \"typing context\" assigns a type to each free variable. For instance, in the expression (\"x\" + \"y\") might be considered in a typing context (\"x\":nat,\"y\":nat). We now give a denotational semantics to program fragments, using the following scheme.\n\nNow, the meaning of the compound expression (7+4) is determined by composing the three functions 〚⊢7:nat〛:1→ℕ, 〚⊢4:nat〛:1→ℕ, and 〚\"x\":nat,\"y\":nat⊢\"x\"+\"y\":nat〛:ℕ×ℕ→ℕ.\n\nIn fact, this is a general scheme for compositional denotational semantics. There is nothing specific about domains and continuous functions here. One can work with a different category instead. For example, in game semantics, the category of games has games as objects and strategies as morphisms: we can interpret types as games, and programs as strategies. For a simple language without general recursion, we can make do with the category of sets and functions. For a language with side-effects, we can work in the Kleisli category for a monad. For a language with state, we can work in a functor category. Milner has advocated modelling location and interaction by working in a category with interfaces as objects and \"bigraphs\" as morphisms.\n\nAccording to Dana Scott (1980):\n\nAccording to Clinger (1981):\n\nSome work in denotational semantics has interpreted types as domains in the sense of domain theory, which can be seen as a branch of model theory, leading to connections with type theory and category theory. Within computer science, there are connections with abstract interpretation, program verification, and model checking.\n\n\n"}
{"id": "23699292", "url": "https://en.wikipedia.org/wiki?curid=23699292", "title": "Distributed source coding", "text": "Distributed source coding\n\nDistributed source coding (DSC) is an important problem in information theory and communication. DSC problems regard the compression of multiple correlated information sources that do not communicate with each other. By modeling the correlation between multiple sources at the decoder side together with channel codes, DSC is able to shift the computational complexity from encoder side to decoder side, therefore provide appropriate frameworks for applications with complexity-constrained sender, such as sensor networks and video/multimedia compression (see distributed video coding). One of the main properties of distributed source coding is that the computational burden in encoders is shifted to the joint decoder.\n\nIn 1973, David Slepian and Jack Keil Wolf proposed the information theoretical lossless compression bound on distributed compression of two correlated i.i.d. sources X and Y. After that, this bound was extended to cases with more than two sources by Thomas M. Cover in 1975, while the theoretical results in the lossy compression case are presented by Aaron D. Wyner and Jacob Ziv in 1976.\n\nAlthough the theorems on DSC were proposed on 1970s, it was after about 30 years that attempts were started for practical techniques, based on the idea that DSC is closely related to channel coding proposed in 1974 by Aaron D. Wyner. The asymmetric DSC problem was addressed by S. S. Pradhan and K. Ramchandran in 1999, which focused on statistically dependent binary and Gaussian sources and used scalar and trellis coset constructions to solve the problem. They further extended the work into the symmetric DSC case.\n\nSyndrome decoding technology was first used in distributed source coding by the DISCUS system of SS Pradhan and K Ramachandran (Distributed Source Coding Using Syndromes). They compress binary block data from one source into syndromes and transmit data from the other source uncompressed as side information. This kind of DSC scheme achieves asymmetric compression rates per source and results in \"asymmetric\" DSC. This asymmetric DSC scheme can be easily extended to the case of more than two correlated information sources. There are also some DSC schemes that use parity bits rather than syndrome bits.\n\nThe correlation between two sources in DSC has been modeled as a virtual channel which is usually referred as a binary symmetric channel.\n\nStarting from DISCUS, DSC has attracted significant research activity and more sophisticated channel coding techniques have been adopted into DSC frameworks, such as Turbo Code, LDPC Code, and so on.\n\nSimilar to the previous lossless coding framework based on Slepian–Wolf theorem, efforts have been taken on lossy cases based on the Wyner–Ziv theorem. Theoretical results on quantizer designs was provided by R. Zamir and S. Shamai, while different frameworks have been proposed based on this result, including a nested lattice quantizer and a trellis-coded quantizer.\n\nMoreover, DSC has been used in video compression for applications which require low complexity video encoding, such as sensor networks, multiview video camcorders, and so on.\n\nWith deterministic and probabilistic discussions of correlation model of two correlated information sources, DSC schemes with more general compressed rates have been developed. In these \"non-asymmetric\" schemes, both of two correlated sources are compressed.\n\nUnder a certain deterministic assumption of correlation between information sources, a DSC framework in which any number of information sources can be compressed in a distributed way has been demonstrated by X. Cao and M. Kuijper. This method performs non-asymmetric compression with flexible rates for each source, achieving the same overall compression rate as repeatedly applying asymmetric DSC for more than two sources. Then, by investigating the unique connection between syndromes and complementary codewords of linear codes, they have translated the major steps of DSC joint decoding into syndrome decoding followed by channel encoding via a linear block code and also via its complement code, which theoretically illustrated a method of assembling a DSC joint decoder from linear code encoders and decoders.\n\nThe information theoretical lossless compression bound on DSC (the Slepian–Wolf bound) was first purposed by David Slepian and Jack Keil Wolf in terms of entropies of correlated information sources in 1973. They also showed that two isolated sources can compress data as efficiently as if they were communicating with each other. This bound has been extended to the case of more than two correlated sources by Thomas M. Cover in 1975.\n\nSimilar results were obtained in 1976 by Aaron D. Wyner and Jacob Ziv with regard to lossy coding of joint Gaussian sources.\n\nDistributed Coding is the coding of two or more dependent sources with separate encoders and joint decoder. Given two statistically dependent i.i.d. finite-alphabet random sequences X and Y, Slepian–Wolf theorem includes theoretical bound for the lossless coding rate for distributed coding of the two sources as below:\n\nIf both the encoder and decoder of the two sources are independent, the lowest rate we can achieve for lossless compression is formula_4 and formula_5 for formula_6 and formula_7 respectively, where formula_4 and formula_5 are the entropies of formula_6 and formula_7. However, with joint decoding, if vanishing error probability for long sequences is accepted, the Slepian–Wolf theorem shows that much better compression rate can be achieved. As long as the total rate of formula_6 and formula_7 is larger than their joint entropy formula_14 and none of the sources is encoded with a rate larger than its entropy, distributed coding can achieve arbitrarily small error probability for long sequences.\n\nA special case of distributed coding is compression with decoder side information, where source formula_7 is available at the decoder side but not accessible at the encoder side. This can be treated as the condition that formula_16 has already been used to encode formula_7, while we intend to use formula_18 to encode formula_6. The whole system is operating in an asymmetric way (compression rate for the two sources are asymmetric).\n\nShortly after Slepian–Wolf theorem on lossless distributed compression was published, the extension to lossy compression with decoder side information was proposed as Wyner–Ziv theorem. Similarly to lossless case, two statistically dependent i.i.d. sources formula_6 and formula_7 are given, where formula_7 is available at the decoder side but not accessible at the encoder side. Instead of lossless compression in Slepian–Wolf theorem, Wyner–Ziv theorem looked into the lossy compression case.\n\nWyner–Ziv theorem presents the achievable lower bound for the bit rate of formula_6 at given distortion formula_24. It was found that for Gaussian memoryless sources and mean-squared error distortion, the lower bound for the bit rate of formula_6 remain the same no matter whether side information is available at the encoder or not.\n\nDeterministic model\n\nProbabilistic model\n\nAsymmetric DSC means that, different bitrates are used in coding the input sources, while same bitrate is used in symmetric DSC. Taking a DSC design with two sources for example, in this example formula_6 and formula_7 are two discrete, memoryless, uniformly distributed sources which generate set of variables formula_28 and formula_29 of length 7 bits and the Hamming distance between formula_28 and formula_29 is at most one. The Slepian–Wolf bound for them is:\n\nThis means, the theoretical bound is formula_35 and symmetric DSC means 5 bits for each source. Other pairs with formula_35 are asymmetric cases with different bit rate distributions between formula_6 and formula_7, where formula_39, formula_40 and formula_41, formula_42 represent two extreme cases called decoding with side information.\n\nIt was understood that Slepian–Wolf coding is closely related to channel coding in 1974, and after about 30 years, practical DSC started to be implemented by different channel codes. The motivation behind the use of channel codes is from two sources case, the correlation between input sources can be modeled as a virtual channel which has input as source formula_6 and output as source formula_7. The DISCUS system proposed by S. S. Pradhan and K. Ramchandran in 1999 implemented DSC with syndrome decoding, which worked for asymmetric case and was further extended to symmetric case.\n\nThe basic framework of syndrome based DSC is that, for each source, its input space is partitioned into several cosets according to the particular channel coding method used. Every input of each source gets an output indicating which coset the input belongs to, and the joint decoder can decode all inputs by received coset indices and dependence between sources. The design of channel codes should consider the correlation between input sources.\n\nA group of codes can be used to generate coset partitions, such as trellis codes and lattice codes. Pradhan and Ramchandran designed rules for construction of sub-codes for each source, and presented result of trellis-based coset constructions in DSC, which is based on convolution code and set-partitioning rules as in Trellis modulation, as well as lattice code based DSC. After this, embedded trellis code was proposed for asymmetric coding as an improvement over their results.\n\nAfter DISCUS system was proposed, more sophisticated channel codes have been adapted to the DSC system, such as Turbo Code, LDPC Code and Iterative Channel Code. The encoders of these codes are usually simple and easy to implement, while the decoders have much higher computational complexity and are able to get good performance by utilizing source statistics. With sophisticated channel codes which have performance approaching the capacity of the correlation channel, corresponding DSC system can approach the Slepian–Wolf bound.\n\nAlthough most research focused on DSC with two dependent sources, Slepian–Wolf coding has been extended to more than two input sources case, and sub-codes generation methods from one channel code was proposed by V. Stankovic, A. D. Liveris, etc. given particular correlation models.\n\nTheorem: Any pair of correlated uniformly distributed sources, formula_45, with formula_46, can be compressed separately at a rate pair formula_47 such that formula_48, where formula_49 and formula_50 are integers, and formula_51. This can be achieved using an formula_52 binary linear code.\n\n\"Proof\": The Hamming bound for an formula_52 binary linear code is formula_51, and we have Hamming code achieving this bound, therefore we have such a binary linear code formula_55 with formula_56 generator matrix formula_57. Next we will show how to construct syndrome encoding based on this linear code.\n\nLet formula_58 and formula_59 be formed by taking first formula_60 rows from formula_57, while formula_62 is formed using the remaining formula_63 rows of formula_57. formula_65 and formula_66 are the subcodes of the Hamming code generated by formula_59 and formula_62 respectively, with formula_69 and formula_70 as their parity check matrices.\n\nFor a pair of input formula_71, the encoder is given by formula_72 and formula_73. That means, we can represent formula_28 and formula_29 as formula_76, formula_77, where formula_78 are the representatives of the cosets of formula_79 with regard to formula_80 respectively. Since we have formula_81 with formula_82. We can get formula_83, where formula_84, formula_85.\n\nSuppose there are two different input pairs with the same syndromes, that means there are two different strings formula_86, such that formula_87 and formula_88. Thus we will have formula_89. Because minimum Hamming weight of the code formula_55 is formula_91, the distance between formula_92 and formula_93 is formula_94. On the other hand, according to formula_82 together with formula_87 and formula_88, we will have formula_98 and formula_99, which contradict with formula_100. Therefore, we cannot have more than one input pairs with the same syndromes.\n\nTherefore, we can successfully compress the two dependent sources with constructed subcodes from an formula_52 binary linear code, with rate pair formula_47 such that formula_48, where formula_49 and formula_50 are integers, and formula_51. \"Log\" indicates \"Log\".\n\nTake the same example as in the previous Asymmetric DSC vs. Symmetric DSC part, this part presents the corresponding DSC schemes with coset codes and syndromes including asymmetric case and symmetric case. The Slepian–Wolf bound for DSC design is shown in the previous part.\n\nIn the case where formula_39 and formula_40, the length of an input variable formula_29 from source formula_7 is 7 bits, therefore it can be sent lossless with 7 bits independent of any other bits. Based on the knowledge that formula_28 and formula_29 have Hamming distance at most one, for input formula_28 from source formula_6, since the receiver already has formula_29, the only possible formula_28 are those with at most 1 distance from formula_29. If we model the correlation between two sources as a virtual channel, which has input formula_28 and output formula_29, as long as we get formula_29, all we need to successfully \"decode\" formula_28 is \"parity bits\" with particular error correction ability, taking the difference between formula_28 and formula_29 as channel error. We can also model the problem with cosets partition. That is, we want to find a channel code, which is able to partition the space of input formula_6 into several cosets, where each coset has a unique syndrome associated with it. With a given coset and formula_29, there is only one formula_28 that is possible to be the input given the correlation between two sources.\n\nIn this example, we can use the formula_127 binary Hamming Code formula_55, with parity check matrix formula_129. For an input formula_28 from source formula_6, only the syndrome given by formula_132 is transmitted, which is 3 bits. With received formula_29 and formula_134, suppose there are two inputs formula_135 and formula_136 with same syndrome formula_134. That means formula_138, which is formula_139. Since the minimum Hamming weight of formula_140 Hamming Code is 3, formula_141. Therefore, the input formula_28 can be recovered since formula_143.\n\nSimilarly, the bits distribution with formula_42, formula_41 can be achieved by reversing the roles of formula_6 and formula_7.\n\nIn symmetric case, what we want is equal bitrate for the two sources: 5 bits each with separate encoder and joint decoder. We still use linear codes for this system, as we used for asymmetric case. The basic idea is similar, but in this case, we need to do coset partition for both sources, while for a pair of received syndromes (corresponds to one coset), only one pair of input variables are possible given the correlation between two sources.\n\nSuppose we have a pair of linear code formula_65 and formula_66 and an encoder-decoder pair based on linear codes which can achieve symmetric coding. The encoder output is given by: formula_72 and formula_73. If there exists two pair of valid inputs formula_152 and formula_153 generating the same syndromes, i.e. formula_154 and formula_155, we can get following(formula_156 represents Hamming weight):\n\nformula_157, where formula_158\n\nformula_159, where formula_160\n\nThus: formula_161\n\nformula_162\n\nwhere formula_163 and formula_164. That means, as long as we have the minimum distance between the two codes larger than formula_165, we can achieve error-free decoding.\n\nThe two codes formula_65 and formula_66 can be constructed as subcodes of the formula_168 Hamming code and thus has minimum distance of formula_165. Given the generator matrix formula_57 of the original Hamming code, the generator matrix formula_59 for formula_65 is constructed by taking any two rows from formula_57, and formula_62 is constructed by the remaining two rows of formula_57. The corresponding formula_176 parity-check matrix for each sub-code can be generated according to the generator matrix and used to generate syndrome bits.\n\nIn general, a Wyner–Ziv coding scheme is obtained by adding a quantizer and a de-quantizer to the Slepian–Wolf coding scheme. Therefore, a Wyner–Ziv coder design could focus on the quantizer and corresponding reconstruction method design. Several quantizer designs have been proposed, such as a nested lattice quantizer, trellis code quantizer and Lloyd quantization method.\n\nUnfortunately, the above approaches do not scale (in design or operational complexity requirements) to sensor networks of large sizes, a scenario where distributed compression is most helpful. If there are N sources transmitting at R bits each (with some distributed coding scheme), the number of possible reconstructions scales formula_177. Even for moderate values of N and R (say N=10, R = 2), prior design schemes become impractical. Recently, an approach, using ideas borrowed from Fusion Coding of Correlated Sources, has been proposed where design and operational complexity are traded against decoder performance. This has allowed distributed quantizer design for network sizes reaching 60 sources, with substantial gains over traditional approaches.\n\nThe central idea is the presence of a bit-subset selector which maintains a certain subset of the received (NR bits, in the above example) bits for each source. Let formula_178 be the set of all subsets of the NR bits i.e.\n\nThen, we define the bit-subset selector mapping to be\n\nNote that each choice of the bit-subset selector imposes a storage requirement (C) that is exponential in the cardinality of the set of chosen bits. \n\nThis allows a judicious choice of bits that minimize the distortion, given the constraints on decoder storage. Additional limitations on the set of allowable subsets are still needed. The effective cost function that needs to be minimized is a weighted sum of distortion and decoder storage\n\nThe system design is performed by iteratively (and incrementally) optimizing the encoders, decoder and bit-subset selector till convergence.\n\nThe syndrome approach can still be used for more than two sources. Let us consider formula_183 binary sources of length-formula_184 formula_185. Let formula_186 be the corresponding coding matrices of sizes formula_187. Then the input binary sources are compressed into formula_188 of total formula_189 bits. Apparently, two source tuples cannot be recovered at the same time if they share the same syndrome. In other words, if all source tuples of interest have different syndromes, then one can recover them losslessly.\n\nGeneral theoretical result does not seem to exist. However, for a restricted kind of source so-called Hamming source that only has at most one source different from the rest and at most one bit location not all identical, practical lossless DSC is shown to exist in some cases. For the case when there are more than two sources, the number of source tuple in a Hamming source is formula_190. Therefore, a packing bound that formula_191 obviously has to satisfy. When the packing bound is satisfied with equality, we may call such code to be perfect (an analogous of perfect code in error correcting code).\n\nA simplest set of formula_192 to satisfy the packing bound with equality is formula_193. However, it turns out that such syndrome code does not exist. The simplest (perfect) syndrome code with more than two sources have formula_194 and formula_195. Let\n\nformula_196\nformula_197\nformula_198\nformula_199,\nand \nformula_200\nsuch that formula_201 \nare any partition of formula_202.\n\nformula_203 \ncan compress a Hamming source (i.e., sources that have no more than one bit different will all have different syndromes). \nFor example, for the symmetric case, a possible set of coding matrices are\nformula_204\nformula_205\nformula_206\n\n"}
{"id": "8181025", "url": "https://en.wikipedia.org/wiki?curid=8181025", "title": "Elements of Algebra", "text": "Elements of Algebra\n\nElements of Algebra is an elementary mathematics textbook written by mathematician Leonhard Euler and originally published in 1770 in German. \"Elements of Algebra\" is one of the earliest books to set out algebra in the modern form we would recognize today (another early book being \"Elements of Algebra\" by Nicholas Saunderson, published in 1740), and is one of Euler's few writings, along with \"Letters to a German Princess\", that are accessible to the general public. Written in numbered paragraphs as was common practice till the 19th century, \"Elements\" begins with the definition of mathematics and builds on the fundamental operations of arithmetic and number systems, and gradually moves towards more abstract topics.\n\nIn 1771, Joseph-Louis Lagrange published an addendum titled \"Additions to Euler's Elements of Algebra\", which featured a number of important mathematical results.\n\nThe original German title of the book was \"Vollständige Anleitung zur Algebra\", which literally translates to \"Complete Instruction to Algebra\". Two English translations are now extant, one by John Hewlett (1822), and the other, which is translated to English from a French translation of the book, by Charles Tayler (1824). On the 300th birth anniversary of Euler in 2007, mathematician Christopher Sangwin working with Tarquin Publications published a digitized copy based on Hewlett's translation of the first four sections (or Part I) of the book.\n\nIn 2015, Scott Hecht published both print and Kindle versions of \"Elements of Algebra\" () with Euler's Part I (Containing the Analysis of Determinate Quantities), Part II (Containing the Analysis of Indeterminate Quantities), Lagrange's Additions, and footnotes by Johann Bernoulli and others.\n\n"}
{"id": "523076", "url": "https://en.wikipedia.org/wiki?curid=523076", "title": "Excitable medium", "text": "Excitable medium\n\nAn excitable medium is a nonlinear dynamical system which has the capacity to propagate a wave of some description, and which cannot support the passing of another wave until a certain amount of time has passed (known as the refractory time).\n\nA forest is an example of an excitable medium: if a wildfire burns through the forest, no fire can return to a burnt spot until the vegetation has gone through its refractory period and regrown. In chemistry, oscillating reactions are excitable media, for example the Belousov–Zhabotinsky reaction and the Briggs–Rauscher reaction. Pathological activities in the heart and brain can be modelled as excitable media. A group of spectators at a sporting event are an excitable medium, as can be observed in a Mexican wave (so-called from its initial appearance in the 1986 World Cup in Mexico).\n\nExcitable media can be modelled using both partial differential equations and cellular automata.\n\nCellular automata provide a simple model to aid in the understanding of excitable media. Perhaps the simplest such model is in. See Greenberg-Hastings cellular automaton for this model. \nEach cell of the automaton is made to represent some section of the medium being modelled (for example, a patch of trees in a forest, or a segment of heart tissue). Each cell can be in one of the three following states:\n\n\nAs in all cellular automata, the state of a particular cell in the next time step depends on the state of the cells around it—its neighbours—at the current time. In the forest fire example the simple rules given in Greenberg-Hastings cellular automaton might be modified as follows:\n\n\nThis function can be refined according to the particular medium. For example, the effect of wind can be added to the model of the forest fire.\n\nIt is most common for a one-dimensional medium to form a closed circuit, i.e. a ring. For example, the Mexican wave can be modeled as a ring going around the stadium. If the wave moves in one direction it will eventually return to where it started. If, upon a wave's return to the origin, the original spot has gone through its refractory period, then the wave will propagate along the ring again (and will do so indefinitely). If, however, the origin is still refractory upon the wave's return, the wave will be stopped.\n\nIn the Mexican wave, for example, if for some reason, the originators of the wave are still standing upon its return it will not continue. If the originators have sat back down then the wave can, in theory, continue.\n\nSeveral forms of waves can be observed in a two-dimensional medium.\n\nA \"spreading wave\" will originate at a single point in the medium and spread outwards. For example, a forest fire could start from a lightning strike at the centre of a forest and spread outwards.\n\nA \"spiral wave\" will again originate at a single point, but will spread in a spiral circuit. Spiral waves are believed to underlie phenomena such as tachycardia and fibrillation.\n\nSpiral waves constitute one of the mechanisms of fibrillation when they organize in long-lasting reentrant activities named rotors.\n\n\n"}
{"id": "6230931", "url": "https://en.wikipedia.org/wiki?curid=6230931", "title": "Expander mixing lemma", "text": "Expander mixing lemma\n\nThe expander mixing lemma states that, for any two subsets formula_1 of a d-regular expander graph formula_2 with formula_3 vertices, the number of edges between formula_4 and formula_5 is approximately what you would expect in a random \"d\"-regular graph, i.e. formula_6.\n\nLet formula_7 be a \"d\"-regular graph on \"n\" vertices with formula_8 the second-largest eigenvalue (in absolute value) of the adjacency matrix. For any two subsets formula_9, let formula_10 be the number of edges between \"S\" and \"T\" (counting edges contained in the intersection of \"S\" and \"T\" twice).\nThen\n\nthen its second-largest eigenvalue is formula_12.\n\n"}
{"id": "31759369", "url": "https://en.wikipedia.org/wiki?curid=31759369", "title": "Ferdinand Peper", "text": "Ferdinand Peper\n\nFerdinand Peper (born 1961) is a Dutch theoretical computer scientist.\n\nPeper obtained his PhD at the Delft University of Technology in 1989 with the thesis \"Efficient network topologies for extensible massively parallel computers\". He currently is working in a senior research position at Kobe Advanced ICT Research Center, and the National Institute of Information and Communications Technology. He is best known for his research on Nanocomputing, Asynchronous systems, Cellular automaton, Reconfigurable hardware and Instantaneous Noise-based logic. His research goals are to develop next-generation computing and communication architectures and also schemes enhanced by Nanotechnology and Nanoelectronics including single-electron transistors. Particular topics of his research include the reduction of energy requirement, the exploitation of noise and fluctuations for informatics,\nand the features of molecular self-organization and self-assembly. He was the Chair of the Fourth International Workshop on Natural Computing (2009) and acted as a co-editor of the book \"Natural Computing\" (Springer). He is a member of editorial board of the \"International Journal of Unconventional Computing.\" \n\n\n\n"}
{"id": "333835", "url": "https://en.wikipedia.org/wiki?curid=333835", "title": "Free object", "text": "Free object\n\nIn mathematics, the idea of a free object is one of the basic concepts of abstract algebra. It is a part of universal algebra, in the sense that it relates to all types of algebraic structure (with finitary operations). It also has a formulation in terms of category theory, although this is in yet more abstract terms. Examples include free groups, tensor algebras, or free lattices. Informally, a free object over a set \"A\" can be thought of as being a \"generic\" algebraic structure over \"A\": the only equations that hold between elements of the free object are those that follow from the defining axioms of the algebraic structure.\n\nFree objects are the direct generalization to categories of the notion of basis in a vector space. A linear function between vector spaces is entirely determined by its values on a basis of the vector space \"E\". The following definition translates this to any category.\n\nLet (\"C\",\"F\") be a concrete category (i.e. is a faithful functor), let \"X\" be a set (called \"basis\"), an object, and a map between sets (called \"canonical insertion\"). We say that \"A\" is the free object on \"X\" (with respect to \"i\") if and only if they satisfy this universal property:\n\nIn this way the free functor that builds the free object \"A\" from the set \"X\" becomes left adjoint to the forgetful functor.\n\nThe creation of free objects proceeds in two steps. For algebras that conform to the associative law, the first step is to consider the collection of all possible words formed from an alphabet. Then one imposes a set of equivalence relations upon the words, where the relations are the defining relations of the algebraic object at hand. The free object then consists of the set of equivalence classes.\n\nConsider, for example, the construction of the free group in two generators. One starts with an alphabet consisting of the five letters formula_2. In the first step, there is not yet any assigned meaning to the \"letters\" formula_3 or formula_4; these will be given later, in the second step. Thus, one could equally well start with the alphabet in five letters that is formula_5. In this example, the set of all words or strings formula_6 will include strings such as \"aebecede\" and \"abdc\", and so on, of arbitrary finite length, with the letters arranged in every possible order.\n\nIn the next step, one imposes a set of equivalence relations. The equivalence relations for a group are that of multiplication by the identity, formula_7, and the multiplication of inverses: formula_8. Applying these relations to the strings above, one obtains\n\nwhere it was understood that \"c\" is a stand-in for formula_3, and \"d\" is a stand-in for formula_4, while \"e\" is the identity element. Similarly, one has\n\nDenoting the equivalence relation or congruence by formula_13, the free object is then the collection of equivalence classes of words. Thus, in this example, the free group in two generators is the quotient\n\nThis is often written as\n\nwhere\n\nis the set of all words, and\n\nis the equivalence class of the identity, after the relations defining a group are imposed.\n\nA simpler example are the free monoids. The free monoid on a set \"X\", is the monoid of all finite strings using \"X\" as alphabet, with operation concatenation of strings. The identity is the empty string. In essence, the free monoid is simply the set of all words, with no equivalence relations imposed. This example is developed further in the article on the Kleene star.\n\nIn the general case, the algebraic relations need not be associative, in which case the starting point is not the set of all words, but rather, strings punctuated with parentheses, which are used to indicate the non-associative groupings of letters. Such a string may equivalently be represented by a binary tree or a free magma; the leaves of the tree are the letters from the alphabet.\n\nThe algebraic relations may then be general arities or finitary relations on the leaves of the tree. Rather than starting with the collection of all possible parenthesized strings, it can be more convenient to start with the Herbrand universe. Properly describing or enumerating the contents of a free object can be easy or difficult, depending on the particular algebraic object in question. For example, the free group in two generators is easily described. By contrast, little or nothing is known about the structure of free Heyting algebras in more than one generator. The problem of determining if two different strings belong to the same equivalence class is known as the word problem.\n\nAs the examples suggest, free objects look like constructions from syntax; one may reverse that to some extent by saying that major uses of syntax can be explained and characterised as free objects, in a way that makes apparently heavy 'punctuation' explicable (and more memorable).\n\nLet formula_18 be any set, let formula_19 be an algebraic structure of type formula_20 generated by formula_18. Let the underlying set of this algebraic structure formula_19, sometimes called its universe, be formula_23, and let formula_24 be a function. We say that formula_25formula_23,formula_27 (or informally just formula_19) is a \"free algebra\" (of type formula_20) on the set formula_18 of \"free generators\" if, for every algebra formula_31 of type formula_20 and function formula_33, where formula_34 is a universe of formula_31, there exists a unique homomorphism formula_36 such that formula_37.\n\nThe most general setting for a free object is in category theory, where one defines a functor, the free functor, that is the left adjoint to the forgetful functor.\n\nConsider the category C of algebraic structures; these can be thought of as sets plus operations, obeying some laws. This category has a functor, formula_38, the forgetful functor, which maps objects and functions in C to Set, the category of sets. The forgetful functor is very simple: it just ignores all of the operations.\n\nThe free functor \"F\", when it exists, is the left adjoint to \"U\". That is, formula_39 takes sets \"X\" in Set to their corresponding free objects \"F\"(\"X\") in the category C. The set \"X\" can be thought of as the set of \"generators\" of the free object \"F\"(\"X\").\n\nFor the free functor to be a left adjoint, one must also have a Set-morphism formula_40. More explicitly, \"F\" is, up to isomorphisms in C, characterized by the following universal property:\nConcretely, this sends a set into the free object on that set; it is the \"inclusion of a basis\". Abusing notation, formula_41 (this abuses notation because \"X\" is a set, while \"F\"(\"X\") is an algebra; correctly, it is formula_42).\n\nThe natural transformation formula_43 is called the unit; together with the counit formula_44, one may construct a T-algebra, and so a monad.\n\nThe cofree functor is the right adjoint to the forgetful functor.\n\nThere are general existence theorems that apply; the most basic of them guarantees that \nHere, a variety is a synonym for a finitary algebraic category, thus implying that the set of relations are finitary, and \"algebraic\" because it is monadic over Set.\n\nOther types of forgetfulness also give rise to objects quite like free objects, in that they are left adjoint to a forgetful functor, not necessarily to sets.\n\nFor example the tensor algebra construction on a vector space as left adjoint to the functor on associative algebras that ignores the algebra structure. It is therefore often also called a free algebra.\n\nLikewise the symmetric algebra and exterior algebra are free symmetric and anti-symmetric algebras on a vector space.\n\nSpecific kinds of free objects include:\n\n"}
{"id": "7354718", "url": "https://en.wikipedia.org/wiki?curid=7354718", "title": "Guy Terjanian", "text": "Guy Terjanian\n\nGuy Terjanian is a French mathematician who has worked on algebraic number theory. He achieved his Ph.D. under Claude Chevalley in 1966, and at that time published a counterexample to the original form of a conjecture of Emil Artin, which suitably modified had just been proved as the Ax-Kochen theorem.\n\nIn 1977, he proved that if \"p\" is an odd prime number, and the natural numbers \"x\", \"y\" and \"z\" satisfy formula_1, then \"2p\" must divide \"x\" or \"y\".\n\n\n"}
{"id": "4852151", "url": "https://en.wikipedia.org/wiki?curid=4852151", "title": "Hamilton's principle", "text": "Hamilton's principle\n\nIn physics, Hamilton's principle is William Rowan Hamilton's formulation of the principle of stationary action (see that article for historical formulations). It states that the dynamics of a physical system is determined by a variational problem for a functional based on a single function, the Lagrangian, which contains all physical information concerning the system and the forces acting on it. The variational problem is equivalent to and allows for the derivation of the \"differential\" equations of motion of the physical system. Although formulated originally for classical mechanics, Hamilton's principle also applies to classical fields such as the electromagnetic and gravitational fields, and plays an important role in quantum mechanics, quantum field theory and criticality theories.\n\nHamilton's principle states that the true evolution q(\"t\") of a system described by \"N\" generalized coordinates q = (\"q\", \"q\", ..., \"q\") between two specified states q = q(\"t\") and q = q(\"t\") at two specified times \"t\" and \"t\" is a stationary point (a point where the variation is zero), of the action functional\n\nwhere formula_2 is the Lagrangian function for the system. In other words, any \"first-order\" perturbation of the true evolution results in (at most) \"second-order\" changes in formula_3. The action formula_3 is a functional, i.e., something that takes as its input a function and returns a single number, a scalar. In terms of functional analysis, Hamilton's principle states that the true evolution of a physical system is a solution of the functional equation\n\n</math>\n\nRequiring that the true trajectory q(\"t\") be a stationary point of the action functional formula_3 is equivalent to a set of differential equations for q(\"t\") (the Euler–Lagrange equations), which may be derived as follows.\n\nLet q(\"t\") represent the true evolution of the system between two specified states q = q(\"t\") and q = q(\"t\") at two specified times \"t\" and \"t\", and let ε(\"t\") be a small perturbation that is zero at the endpoints of the trajectory\n\nTo first order in the perturbation ε(\"t\"), the change in the action functional formula_7 would be\n\nwhere we have expanded the Lagrangian \"L\" to first order in the perturbation ε(\"t\").\n\nApplying integration by parts to the last term results in\n\nThe boundary conditions formula_6 causes the first term to vanish\n\nHamilton's principle requires that this first-order change formula_12 is zero for all possible perturbations ε(\"t\"), i.e., the true path is a stationary point of the action functional formula_3 (either a minimum, maximum or saddle point). This requirement can be satisfied if and only if\n\nThese equations are called the Euler–Lagrange equations for the variational problem.\n\nThe conjugate momentum \"p\" for a generalized coordinate \"q\" is defined by the equation\n\nAn important special case of the Euler–Lagrange equation occurs when \"L\" does not contain a generalized coordinate \"q\" explicitly,\n\nthat is, the conjugate momentum is a \"constant of the motion\".\n\nIn such cases, the coordinate \"q\" is called a cyclic coordinate. For example, if we use polar coordinates \"t, r, θ\" to describe the planar motion of a particle, and if \"L\" does not depend on \"θ\", the conjugate momentum is the conserved angular momentum.\n\nTrivial examples help to appreciate the use of the action principle via the Euler–Lagrange equations. A free particle (mass \"m\" and velocity \"v\") in Euclidean space moves in a straight line. Using the Euler–Lagrange equations, this can be shown in polar coordinates as follows. In the absence of a potential, the Lagrangian is simply equal to the kinetic energy \nin orthonormal (\"x\",\"y\") coordinates, where the dot represents differentiation with respect to the curve parameter (usually the time, \"t\"). Therefore, upon application of the Euler–Lagrange equations,\n\nAnd likewise for \"y\". Thus the Euler–Lagrange formulation can be used to derive Newton's laws.\n\nIn polar coordinates (\"r\", φ) the kinetic energy and hence the Lagrangian becomes\n\nThe radial \"r\" and \"φ\" components of the Euler–Lagrange equations become, respectively\n\nThe solution of these two equations is given by\n\nfor a set of constants \"a, b, c, d\" determined by initial conditions.\nThus, indeed, \"the solution is a straight line\" given in polar coordinates: \"a\" is the velocity, \"c\" is the distance of the closest approach to the origin, and \"d\" is the angle of motion.\n\nHamilton's principle is an important variational principle in elastodynamics. As opposed to a system composed of rigid bodies, deformable bodies have an infinite number of degrees of freedom and occupy continuous regions of space; consequently, the state of the system is described by using continuous functions of space and time. The extended Hamilton Principle for such bodies is given by\n\nwhere \"T\" is the kinetic energy, \"U\" is the elastic energy, \"W\" is the work done by\nexternal loads on the body, and \"t\", \"t\" the initial and final times. If the system is conservative, the work done by external forces may be derived from a scalar potential \"V\". In this case,\n\nThis is called Hamilton's principle and it is invariant under coordinate transformations.\n\nHamilton's principle and Maupertuis' principle are occasionally confused and both have been called (incorrectly) the principle of least action. They differ in three important ways: \n\n\n\nThe action principle can be extended to obtain the equations of motion for fields, such as the electromagnetic field or gravity.\n\nThe Einstein equation utilizes the \"Einstein–Hilbert action\" as constrained by a variational principle.\n\nThe path of a body in a gravitational field (i.e. free fall in space time, a so-called geodesic) can be found using the action principle.\n\nIn quantum mechanics, the system does not follow a single path whose action is stationary, but the behavior of the system depends on all imaginable paths and the value of their action. The action corresponding to the various paths is used to calculate the path integral, that gives the probability amplitudes of the various outcomes.\n\nAlthough equivalent in classical mechanics with Newton's laws, the action principle is better suited for generalizations and plays an important role in modern physics. Indeed, this principle is one of the great generalizations in physical science. In particular, it is fully appreciated and best understood within quantum mechanics. Richard Feynman's path integral formulation of quantum mechanics is based on a stationary-action principle, using path integrals. Maxwell's equations can be derived as conditions of stationary action.\n\n\n"}
{"id": "14556621", "url": "https://en.wikipedia.org/wiki?curid=14556621", "title": "Idoneal number", "text": "Idoneal number\n\nIn mathematics, Euler's idoneal numbers (also called suitable numbers or convenient numbers) are the positive integers \"D\" such that any integer expressible in only one way as \"x\" ± \"Dy\" (where \"x\" is relatively prime to \"Dy\") is a prime, prime power, twice one of these, or a power of 2. In particular, a number that has two distinct representations as a sum of two squares is composite. Every idoneal number generates a set containing infinitely many primes and missing infinitely many other primes.\n\nA positive integer \"n\" is idoneal if and only if it cannot be written as \"ab\" + \"bc\" + \"ac\" for distinct positive integer \"a, b\", and \"c\".\n\nIt is sufficient to consider the set ; if all these numbers are of the form , , or \"2\" for some integer s, where is a prime, then is idoneal.\n\nThe 65 idoneal numbers found by Leonhard Euler and Carl Friedrich Gauss and conjectured to be the only such numbers are\nIn 1973, Peter J. Weinberger proved that at most one other idoneal number exists, and that the list above is complete if the generalized Riemann hypothesis holds.\n\n\n\n"}
{"id": "57407405", "url": "https://en.wikipedia.org/wiki?curid=57407405", "title": "Intensity (measure theory)", "text": "Intensity (measure theory)\n\nIn the mathematical discipline of measure theory, the intensity of a measure is the average value the measure assigns to an interval of length one.\n\nLet formula_1 be a measure on the real number. Then the intensity formula_2 of formula_1 is defined as\n\nif the limit exists and is independent of formula_5 for all formula_6\n\nLook at the Lebesgue measure formula_7. Then for a fixed formula_5, it is\n\nso\n\nTherefore the Lebesgue measure has intensity one.\n\nThe set of all measures formula_11 for which the intensity is well defined is a measurable subset of the set of all measures on formula_12. The mapping \ndefined by\n\nis measurable.\n"}
{"id": "40962833", "url": "https://en.wikipedia.org/wiki?curid=40962833", "title": "Joan Clarke", "text": "Joan Clarke\n\nJoan Elisabeth Lowther Murray, MBE (\"née\" Clarke; 24 June 1917 – 4 September 1996) was an English cryptanalyst and numismatist best known for her work as a code-breaker at Bletchley Park during the Second World War. Though she did not personally seek the spotlight, her role in the Enigma project that decrypted Nazi Germany's secret communications earned her awards and citations, such as appointment as a Member of the Order of the British Empire (MBE), in 1946.\n\nJoan Elisabeth Lowther Clarke was born on 24 June 1917 in West Norwood, London, England. She was the youngest child of Dorothy (née Fulford) and the Revd William Kemp Lowther Clarke, a clergyman. She had three brothers and one sister.\n\nClarke attended Dulwich High School for Girls in south London and won a scholarship in 1936, to attend Newnham College, Cambridge, where she gained a double first degree in mathematics and was a Wrangler. She was denied a full degree, as Cambridge only awarded these to men until 1948.\n\nClarke's mathematical abilities were first discovered by Gordon Welchman, in an undergraduate Geometry class at Cambridge. Welchman was one of the top four mathematicians to be recruited in 1939 to supervise decoding operations at Bletchley Park. After noticing Clarke's mathematical abilities he recruited her to join him at Bletchley Park and be a part of the 'Government Code and Cypher School' (GCCS).\n\nThe GCCS started up in 1939 with only one purpose, to break the German Enigma Code. The Enigma Code was a machine the Germans invented to encrypt their messages; they strongly believed their machine was unbreakable. Clarke first arrived at Bletchley Park on 17 June 1940. She was first placed in a group only made up of women referred to as \"The Girls\", that mainly did routine clerical work. At this time, cryptology was not a job for a woman. According to Clarke, she only knew of one other female cryptologist that worked at Bletchley Park.\n\nIn June 1940, Clarke was recruited by her former academic supervisor, Gordon Welchman, to the Government Code and Cypher School (GC&CS). She worked at Bletchley Park in the section known as Hut 8 and quickly became the only female practitioner of Banburismus, a cryptanalytic process developed by Alan Turing which reduced the need for bombes —electromechanical devices as used by British cryptologists Welchman and Turing to decipher German encrypted messages during World War II. Although Clarke had the same position as her male coworkers, she was being paid less due to her gender. Clarke's first work promotion was to Linguist Grade which was designed to earn her extra money despite the fact that she did not speak another language. This promotion was a recognition of her workload and contributions to the team.\n\nIn 1941, trawlers were captured as well as their cipher equipment and codes. Before this information was obtained, wolf packs had sunk 282,000 tons of shipping a month from March to June 1941. By November, Clarke and her team were able to reduce this number to 62,000 tons. Hugh Alexander, head of Hut 8 from 1943 to 1944, described her as \"one of the best Banburists in the section\". Alexander himself was regarded as the best of the Banburists. He and I. J. Good considered the process more an intellectual game than a job. It was \"not easy enough to be trivial, but not difficult enough to cause a nervous breakdown\".\n\nClarke became deputy head of Hut 8 in 1944, although she was prevented from progressing because of her gender, and was paid less than the men.\n\nClarke and Turing became very good friends at Bletchley Park. Turing arranged their shifts so they could work together, and they also spent much of their free time together. In early 1941, Turing proposed marriage to Clarke, and subsequently introduced her to his family. Although privately admitting his homosexuality to her—she was reportedly \"unfazed\" by the revelation—Turing decided that he could not go through with the marriage, and broke up with Clarke in mid-1941. Clarke later admitted that she suspected Turing's homosexuality for some time, and it was not much of a surprise when he made the admission to her.\n\nClarke and Turing had been close friends since soon after they met, and continued to be until Turing's death in 1954. They shared many hobbies and had similar personalities.\n\nAfter the war, Clarke worked for Government Communications Headquarters (GCHQ) where she met Lieutenant-Colonel John Kenneth Ronald Murray, a retired army officer who had served in India. They married on 26 July 1952 in Chichester Cathedral. Shortly after their marriage, John Murray retired from GCHQ due to ill health and the couple moved to Crail in Fife. They returned to work at GCHQ in 1962 where Clarke remained until 1977 when she retired aged 60.\n\nFollowing her husband's death in 1986, Clarke moved to Headington, Oxfordshire, where she continued her research into coinage. During the 1980s, she assisted Sir Harry Hinsley with the appendix to volume 3, part 2 of \"British Intelligence in the Second World War\". She also assisted historians studying war-time code breaking at Bletchley Park. Due to continuing secrecy among cryptanalysts, the full extent of her accomplishments remains unknown.\n\nAfter meeting her husband, who had published work on the Scottish coinage of the 16th and 17th centuries, Clarke developed an interest in numismatic history. She established the sequence of the complex series of gold unicorn and heavy groat coins that were in circulation in Scotland during the reigns of James III and James IV. In 1986, her research was recognised by the British Numismatic Society when she was awarded the Sanford Saltus Gold Medal. Issue No. 405 of the \"Numismatic Circular\" described her paper on the topic as \"magisterial.\"\n\nLittle is known about Clarke's personal interests or about her past. According to Kerry Howard, a researcher of the history of female World War II, Joan Clarke was a very private person. One of her neighbours said that she never talked about her personal background, and was quite awkward in social situations. What is known is that throughout her life, Clarke had a number of hobbies that became her passions, such as botanical work, chess, and knitting. \n\nIn the spring of 1941, Joan Clarke developed a close friendship with Alan Turing, her colleague from Hut 8. Turing and Clarke had met previously since Turing was friends with Clarke's older brother. For a time they were inseparable. Turing scheduled his shifts so that they worked and spent their free time together. Early in the friendship, Turing proposed marriage to Clarke and she accepted. A few days after the proposal, however, Turing told Clarke about his homosexuality and his concern about their marriage. Clarke and Turing remained engaged. In the late summer of 1941, their engagement ended because Turing continued to believe their marriage would be a failure.\n\nLater, in 1947 Clarke met Lieutenant Colonel John Kenneth Ronald Murray, a retired army officer who had served in India. Clarke and Murray were married on July 26, 1952 in the Chichester Cathedral.\n\nOn 4 September 1996, Clarke died at her home in Headington.\n\nClarke was portrayed by Keira Knightley in the film \"The Imitation Game\" (2014), opposite Benedict Cumberbatch as Alan Turing. Turing's surviving niece, Inagh Payne, described Clarke as \"rather plain\" and thought that Knightley was inappropriately cast as Clarke. Biographer Andrew Hodges also criticised the film, stating the script \"built up the relationship with Joan much more than it actually was.\"\n\nHowever, in contrast, an article by BBC journalist Joe Miller stated that Clarke's \"story has been immortalised.\" In terms of the film itself, director Morten Tyldum has argued that it shows how Clarke succeeded in her field despite working in a time \"when intelligence wasn't really appreciated in women.\"\n\nKnightley was nominated for the Academy Award for Best Supporting Actress at the 87th Academy Awards for her performance as Clarke.\n"}
{"id": "331597", "url": "https://en.wikipedia.org/wiki?curid=331597", "title": "Kuratowski closure axioms", "text": "Kuratowski closure axioms\n\nIn topology and related branches of mathematics, the Kuratowski closure axioms are a set of axioms that can be used to define a topological structure on a set. They are equivalent to the more commonly used open set definition. They were first introduced by Kazimierz Kuratowski.\n\nA similar set of axioms can be used to define a topological structure using only the dual notion of interior operator.\n\nLet formula_1 be a set and formula_2 its power set.\nA Kuratowski Closure Operator is an assignment formula_3 with the following properties:\n\nIf the last axiom, idempotence, is omitted, then the axioms define a preclosure operator.\n\nA consequence of the third axiom is: formula_8 (Preservation of Inclusion).\n\nThe four Kuratowski closure axioms can be replaced by a single condition, namely,\n\nConstruction\nA closure operator naturally induces a topology as follows:\nA subset formula_10 is called closed if and only if formula_11.\n\nEmpty Set and Entire Space are closed:\nBy extensitivity, formula_12 and since closure maps the power set of formula_1 into itself (that is, the image of any subset is a subset of formula_1), formula_15 we have formula_16. Thus formula_17 is closed.\nThe preservation of nullary unions states that formula_4. Thus formula_19 is closed.\n\nArbitrary intersections of closed sets are closed:\nLet formula_20 be an arbitrary set of indices and formula_21 closed for every formula_22.\nBy extensitivity, formula_23\nAlso, by preservation of inclusions, formula_24\nTherefore, formula_25. Thus formula_26 is closed.\n\nFinite unions of closed sets are closed:\nLet formula_20 be a finite set of indices and let formula_21 be closed for every formula_29.\nFrom the preservation of binary unions and using induction we have formula_30. Thus formula_31 is closed.\n\nIn any induced topology (relative to the subset \"A\") the closed sets induce a new closure operator that is just the original closure operator restricted to \"A\": formula_32\n\nCloseness\nA point formula_33 is close to a subset formula_34 iff formula_35.\n\nContinuity\nA function formula_36 is continuous at a point formula_33 iff formula_38.\n\n\n"}
{"id": "40124307", "url": "https://en.wikipedia.org/wiki?curid=40124307", "title": "List of Johnson solids", "text": "List of Johnson solids\n\nIn geometry, a Johnson solid is a strictly convex polyhedron, each face of which is a regular polygon, but which is not uniform, i.e., not a Platonic solid, Archimedean solid, prism or antiprism. In 1966, Norman Johnson published a list which included all 92 solids, and gave them their names and numbers. He did not prove that there were only 92, but he did conjecture that there were no others. Victor Zalgaller in 1969 proved that Johnson's list was complete.\n\nThe complete list is here with sorting by column. Other polyhedra can be constructed that are only approximately regular planar polygon faces, and are informally called near-miss Johnson solid; there can be no definitive count of them.\n\nLegend:\n\n\n"}
{"id": "25804104", "url": "https://en.wikipedia.org/wiki?curid=25804104", "title": "List of trigonometric identities", "text": "List of trigonometric identities\n\nIn mathematics, trigonometric identities are equalities that involve trigonometric functions and are true for every value of the occurring variables where both sides of the equality are defined. Geometrically, these are identities involving certain functions of one or more angles. They are distinct from triangle identities, which are identities potentially involving angles but also involving side lengths or other lengths of a triangle.\n\nThese identities are useful whenever expressions involving trigonometric functions need to be simplified. An important application is the integration of non-trigonometric functions: a common technique involves first using the substitution rule with a trigonometric function, and then simplifying the resulting integral with a trigonometric identity.\n\nThis article uses Greek letters such as alpha (), beta (), gamma (), and theta () to represent angles. Several different units of angle measure are widely used, including degree, radian, and gradian (gons):\n\nIf not specifically annotated by (°) for degree or (formula_1) for gradian, all values for angles in this article are assumed to be given in radian.\n\nThe following table shows for some common angles their conversions and the values of the basic trigonometric functions:\n\nResults for other angles can be found at Trigonometric constants expressed in real radicals. Per Niven's theorem, formula_2 are the only rational numbers that, taken in degrees, result in a rational sine-value for the corresponding angle within the first turn, which may account for their popularity in examples. The analogous condition for the unit radian requires that the argument divided by is rational, and yields the solutions 0, /6, /2, 5/6, , 7/6, 3/2, 11/6(, 2).\n\nThe functions sine, cosine and tangent of an angle are sometimes referred to as the \"primary\" or \"basic\" trigonometric functions. Their usual abbreviations are , and , respectively, where denotes the angle. The parentheses around the argument of the functions are often omitted, e.g., and , if an interpretation is unambiguously possible.\n\nThe sine of an angle is defined, in the context of a right triangle, as the ratio of the length of the side that is opposite to the angle divided by the length of the longest side of the triangle (the hypotenuse).\nThe cosine of an angle in this context is the ratio of the length of the side that is adjacent to the angle divided by the length of the hypotenuse.\nThe tangent of an angle in this context is the ratio of the length of the side that is opposite to the angle divided by the length of the side that is adjacent to the angle. This is the same as the ratio of the sine to the cosine of this angle, as can be seen by substituting the definitions of and from above:\n\nThe remaining trigonometric functions secant (), cosecant (), and cotangent () are defined as the reciprocal functions of cosine, sine, and tangent, respectively. Rarely, these are called the secondary trigonometric functions:\n\nThese definitions are sometimes referred to as ratio identities.\n\nThe inverse trigonometric functions are partial inverse functions for the trigonometric functions. For example, the inverse function for the sine, known as the inverse sine () or arcsine ( or ), satisfies\nand\n\nThis article uses the notation below for inverse trigonometric functions:\nIn trigonometry, the basic relationship between the sine and the cosine is given by the Pythagorean identity:\n\nwhere means and means .\n\nThis can be viewed as a version of the Pythagorean theorem, and follows from the equation for the unit circle. This equation can be solved for either the sine or the cosine:\n\nwhere the sign depends on the quadrant of .\n\nDividing this identity by either or yields the other two Pythagorean identities:\n\nUsing these identities together with the ratio identities, it is possible to express any trigonometric function in terms of any other (up to a plus or minus sign):\n\nThe versine, coversine, haversine, and exsecant were used in navigation. For example, the haversine formula was used to calculate the distance between two points on a sphere. They are rarely used today.\n\nBy examining the unit circle, the following properties of the trigonometric functions can be established.\n\nWhen a direction, represented by an angle formula_12 enclosed with the -direction, is reflected in a line with direction formula_13 then the angle formula_14 of this reflected direction has the value\n\nThis way, reflections in the directions and radian ( and ) generate equally looking results (see picture). The values of the trigonometric functions of these angles formula_16 for specific angles formula_17 satisfy simple identities: either they are equal, or have opposite signs, or employ the complementary trigonometric function. \n\nBy shifting round the arguments of trigonometric functions by certain angles, it is sometimes possible that changing the sign or applying complementary trigonometric functions express particular results more simply. Some examples of shifts are shown below in the table.\n\nThese are also known as the \"addition and subtraction theorems\" or \"formulae\".\nThe identities can be derived by combining right triangles such as in the adjacent diagram, or by considering the invariance of the length of a chord on a unit circle given a particular central angle. Furthermore, it is even possible to derive the identities using Euler's identity although this would be a more obscure approach given that complex numbers are used.\n\nFor acute angles and , whose sum is non-obtuse, a concise diagram (shown) illustrates the angle sum formulae for sine and cosine: The bold segment labeled \"1\" has unit length and serves as the hypotenuse of a right triangle with angle ; the opposite and adjacent legs for this angle have respective lengths and . The leg is itself the hypotenuse of a right triangle with angle ; that triangle's legs, therefore, have lengths given by and , multiplied by . The leg, as hypotenuse of another right triangle with angle , likewise leads to segments of length and . Now, we observe that the \"1\" segment is also the hypotenuse of a right triangle with angle ; the leg opposite this angle necessarily has length , while the leg adjacent has length . Consequently, as the opposing sides of the diagram's outer rectangle are equal, we deduce\nRelocating one of the named angles yields a variant of the diagram that demonstrates the angle difference formulae for sine and cosine. (The diagram admits further variants to accommodate angles and sums greater than a right angle.) Dividing all elements of the diagram by provides yet another variant (shown) illustrating the angle sum formula for tangent.\n\nThe sum and difference formulae for sine and cosine can be written in matrix form as:\n\nThe matrix inverse for a rotation is the rotation with the negative of the angle\nwhich is also the matrix transpose.\n\nThese formulae show that these matrices form a representation of the rotation group in the plane (technically, the special orthogonal group ), since the composition law is fulfilled and inverses exist. Furthermore, matrix multiplication of the rotation matrix for an angle with a column vector will rotate the column vector counterclockwise by the angle .\n\nWhen the series formula_21 converges absolutely then\n\nBecause the series formula_21 converges absolutely, it is necessarily the case that formula_25, formula_26, and formula_27. In particular, in these two identities an asymmetry appears that is not seen in the case of sums of finitely many angles: in each product, there are only finitely many sine factors but there are cofinitely many cosine factors. Terms with infinitely many sine factors would necessarily be equal to zero.\n\nWhen only finitely many of the angles are nonzero then only finitely many of the terms on the right side are nonzero because all but finitely many sine factors vanish. Furthermore, in each term all but finitely many of the cosine factors are unity.\n\nLet (for  = 0, 1, 2, 3, ...) be the th-degree elementary symmetric polynomial in the variables\n\nfor  = 0, 1, 2, 3, ..., i.e.,\n\nThen\n\nusing the sine and cosine sum formulae above.\n\nThe number of terms on the right side depends on the number of terms on the left side.\n\nFor example:\n\nand so on. The case of only finitely many terms can be proved by mathematical induction.\n\nwhere is the th-degree elementary symmetric polynomial in the variables , , and the number of terms in the denominator and the number of factors in the product in the numerator depend on the number of terms in the sum on the left. The case of only finitely many terms can be proved by mathematical induction on the number of such terms.\n\nFor example,\n\nAlso\n\nThese can be shown by using either the sum and difference identities or the multiple-angle formulae.\nThe fact that the triple-angle formula for sine and cosine only involves powers of a single function allows one to relate the geometric problem of a compass and straightedge construction of angle trisection to the algebraic problem of solving a cubic equation, which allows one to prove that trisection is in general impossible using the given tools, by field theory.\n\nA formula for computing the trigonometric identities for the one-third angle exists, but it requires finding the zeroes of the cubic equation , where is the value of the cosine function at the one-third angle and is the known value of the cosine function at the full angle. However, the discriminant of this equation is positive, so this equation has three real roots (of which only one is the solution for the cosine of the one-third angle). None of these solutions is reducible to a real algebraic expression, as they use intermediate complex numbers under the cube roots.\n\nFor specific multiples, these follow from the angle addition formulae, while the general formula was given by 16th-century French mathematician François Viète.\n\nfor nonnegative values of up through .\n\nIn each of these two equations, the first parenthesized term is a binomial coefficient, and the final trigonometric function equals one or minus one or zero so that half the entries in each of the sums are removed. The ratio of these formulae gives\n\nThe Chebyshev method is a recursive algorithm for finding the th multiple angle formula knowing the th and th values.\n\nThis can be proved by adding together the formulae\n\nSimilarly, can be computed from , , and with\nThis can be proved by adding formulae for and .\n\nServing a purpose similar to that of the Chebyshev method, for the tangent we can write:\n\nSetting either or to 0 gives the usual tangent half-angle formulae.\n\nObtained by solving the second and third versions of the cosine double-angle formula.\n\nand in general terms of powers of or the following is true, and can be deduced using De Moivre's formula, Euler's formula and the binomial theorem .\n\nThe product-to-sum identities or prosthaphaeresis formulae can be proven by expanding their right-hand sides using the angle addition theorems. See amplitude modulation for an application of the product-to-sum formulae, and beat (acoustics) and phase detector for applications of the sum-to-product formulae.\n\nThe following formulae apply to arbitrary plane triangles and follow from \"α\" + \"β\" + \"γ\" = 180°, as long as the functions occurring in the formulae are well-defined (the latter applies only to the formulae in which tangents and cotangents occur).\n\nThe Dirichlet kernel is the function occurring on both sides of the next identity:\n\nThe convolution of any integrable function of period 2 with the Dirichlet kernel coincides with the function's th-degree Fourier approximation. The same holds for any measure or generalized function.\n\nIf we set\n\nthen\n\nwhere , sometimes abbreviated to .\n\nWhen this substitution of for is used in calculus, it follows that is replaced by , is replaced by and the differential is replaced by . Thereby one converts rational functions of and to rational functions of in order to find their antiderivatives.\n\n\n"}
{"id": "2874081", "url": "https://en.wikipedia.org/wiki?curid=2874081", "title": "Lottery mathematics", "text": "Lottery mathematics\n\nLottery mathematics is used to calculate probabilities of winning or losing a lottery game. It is based heavily on combinatorics, particularly the twelvefold way and combinations without replacement.\n\nIn a typical 6/49 game, each player chooses six distinct numbers from a range of 1-49. If the six numbers on a ticket match the numbers drawn by the lottery, the ticket holder is a jackpot winner—regardless of the order of the numbers. The probability of this happening is 1 in 13,983,816.\n\nThe chance of winning can be demonstrated as follows: The first number drawn has a 1 in 49 chance of matching. When the draw comes to the second number, there are now only 48 balls left in the bag, because the balls are drawn without replacement. So there is now a 1 in 48 chance of predicting this number.\n\nThus for each of the 49 ways of choosing the first number there are 48 different ways of choosing the second. This means that the probability of correctly predicting 2 numbers drawn from 49 in the correct order is calculated as 1 in 49 × 48. On drawing the third number there are only 47 ways of choosing the number; but of course we could have arrived at this point in any of 49 × 48 ways, so the chances of correctly predicting 3 numbers drawn from 49, again in the correct order, is 1 in 49 × 48 × 47. This continues until the sixth number has been drawn, giving the final calculation, 49 × 48 × 47 × 46 × 45 × 44, which can also be written as formula_1 or 49 factorial divided by 43 factorial. This works out to 10,068,347,520, which is much bigger than the ~14 million stated above.\n\nHowever; the order of the 6 numbers is not significant. That is, if a ticket has the numbers 1, 2, 3, 4, 5, and 6, it wins as long as all the numbers 1 through 6 are drawn, no matter what order they come out in. Accordingly, given any set of 6 numbers, there are 6 × 5 × 4 × 3 × 2 × 1 = 6! or 720 orders in which they could be drawn. Dividing 10,068,347,520 by 720 gives 13,983,816, also written as formula_2, or more generally as\n\nThis function is called the combination function; in Microsoft Excel, this function is implemented as COMBIN(\"n\", \"k\"). For example, COMBIN(49, 6) (the calculation shown above), would return 13,983,816. For the rest of this article, we will use the notation formula_4. \"Combination\" means the group of numbers selected, irrespective of the order in which they are drawn.\n\nAn alternative method of calculating the odds is to note that the probability of the first ball corresponding to one of the six chosen is 6/49; the probability of the second ball corresponding to one of the remaining five chosen is 5/48; and so on. This yields a final formula of\n\nThe range of possible combinations for a given lottery can be referred to as the \"number space\". \"Coverage\" is the percentage of a lottery's number space that is in play for a given drawing.\n\nOne must divide the number of combinations producing the given result by the total number of possible combinations (for example, formula_6). The numerator equates to the number of ways to select the winning numbers multiplied by the number of ways to select the losing numbers.\n\nFor a score of \"n\" (for example, if 3 choices match three of the 6 balls drawn, then \"n\" = 3), formula_7 describes the odds of selecting \"n\" winning numbers from the 6 winning numbers. This means that there are 6 - n losing numbers, which are chosen from the 43 losing numbers in formula_8 ways. The total number of combinations giving that result is, as stated above, the first number multiplied by the second. The expression is therefore formula_9.\n\nThis can be written in a general form for all lotteries as:\n\nformula_10\n\nwhere formula_11 is the number of balls in lottery, formula_12 is the number of balls in a single ticket, and formula_13 is the number of matching balls for a winning ticket.\n\nThe generalisation of this formula is called the hypergeometric distribution.\n\nThis gives the following results:\nWhen a bonus number is included, the adjusted odds are:\nThe Pick8-32 Lottery game by Trillion Coins implements a lotto game ticket where 8 numbers from 01 to 32 are selected in any order and can be repeated. The odds of a play ticket matching all 8 numbers is simple to calculate and is illustrated by the following math.\n\nWhich means there is a 1 in 27,269,633 chance of matching all 8 numbers. The Trillion Coins lottery is a pay-out-on-every-drawing lotto game. The odds are dependent on the total number of tickets that are played. The player who matches the most of the picked numbers wins.\n\nMany lotteries have a Powerball (or \"bonus ball\"). If the powerball is drawn from a pool of numbers different from the main lottery, the odds are multiplied by the number of powerballs. For example, in the 6 from 49 lottery, given 10 powerball numbers, then the odds of getting a score of 3 and the powerball would be 1 in 56.66 × 10, or 566.6 (the \"probability\" would be divided by 10, to give an exact value of formula_15). Another example of such a game is Mega Millions, albeit with different jackpot odds.\n\nWhere more than 1 powerball is drawn from a separate pool of balls to the main lottery (for example, in the EuroMillions game), the odds of the different possible powerball matching scores are calculated using the method shown in the \"other scores\" section above (in other words, the powerballs are like a mini-lottery in their own right), and then multiplied by the odds of achieving the required main-lottery score.\n\nIf the powerball is drawn from the \"same\" pool of numbers as the main lottery, then, for a given target score, the number of winning combinations includes the powerball. For games based on the Canadian lottery (such as the lottery of the United Kingdom), after the 6 main balls are drawn, an extra ball is drawn from the same pool of balls, and this becomes the powerball (or \"bonus ball\"). An extra prize is given for matching 5 balls and the bonus ball. As described in the \"other scores\" section above, the number of ways one can obtain a score of 5 from a single ticket is formula_16. Since the number of remaining balls is 43, and the ticket has 1 unmatched number remaining, of these 258 combinations will match the next ball drawn (the powerball), leaving ways of achieving it. Therefore, the odds of getting a score of 5 and the powerball are formula_17.\n\nOf the 258 combinations that match 5 of the main 6 balls, in 42/43 of them the remaining number will not match the powerball, giving odds of formula_18 for obtaining a score of 5 without matching the powerball.\n\nUsing the same principle, the odds of getting a score of 2 and the powerball are formula_19 for the score of 2 multiplied by the probability of one of the remaining four numbers matching the bonus ball, which is . Since formula_20, the probability of obtaining the score of 2 and the bonus ball is formula_21, approximate decimal odds of 1 in 81.2.\n\nThe general formula for formula_13 matching balls in a formula_11 choose formula_12 lottery with one bonus ball from the formula_11 pool of balls is: \n\nformula_26\n\nThe general formula for formula_13 matching balls in a formula_11 choose formula_12 lottery with zero bonus ball from the formula_11 pool of balls is:\n\nformula_31\n\nThe general formula for formula_13 matching balls in a formula_11 choose formula_12 lottery with one bonus ball from a separate pool of formula_35 balls is:\n\nformula_36\n\nThe general formula for formula_13 matching balls in a formula_11 choose formula_12 lottery with no bonus ball from a separate pool of formula_35 balls is: \n\nformula_41\n\nIt is a hard (and often open) problem to calculate the minimum number of tickets one needs to purchase to guarantee that at least one of these tickets matches at least 2 numbers. In the 5-from-90 lotto, the minimum number of tickets that can guarantee a ticket with at least 2 matches is 100.\n\nAs a discrete probability space, the probability of any particular lottery outcome is atomic, meaning it greater than zero. Therefore, the probability of any event is the sum of probabilities of the outcomes of the event. This makes it easy to calculate quantities of interest from information theory. For example, the information content of any event easy to calculate, by the formula \n\nformula_42\n\nIn particular, the information content of outcome formula_43 of discrete random variable formula_44 is\n\nformula_45\n\nFor example, winning in the example above is a Bernoulli-distributed random variable formula_44 with a chance of winning (\"success\") We write formula_47 with formula_48 and formula_49. The information content of winning is \n\nformula_50shannons or bits of information. (See units of information for further explanation of terminology.) The information content of losing is \n\nformula_51 \n\nThe information entropy of a lottery probability distribution is also easy to calculate as the expected value of the information content. \n\nformula_52 \n\nOftentimes the random variable of interest in the lottery is a Bernoulli trial. In this case, the Bernoulli entropy function may be used. Using formula_44 representing winning the 6-of-49 lottery, the Shannon entropy of 6-of-49 above is \n\nformula_54\n\n"}
{"id": "21867246", "url": "https://en.wikipedia.org/wiki?curid=21867246", "title": "Mahler volume", "text": "Mahler volume\n\nIn convex geometry, the Mahler volume of a centrally symmetric convex body is a dimensionless quantity that is associated with the body and is invariant under linear transformations. It is named after German-English mathematician Kurt Mahler. It is known that the shapes with the largest possible Mahler volume are the balls and solid ellipsoids; this is now known as the Blaschke–Santaló inequality. The still-unsolved Mahler conjecture states that the minimum possible Mahler volume is attained by a hypercube.\n\nA convex body in Euclidean space is defined as a compact convex set with non-empty interior. If \"B\" is a centrally symmetric convex body in \"n\"-dimensional Euclidean space, the polar body \"B\" is another centrally symmetric body in the same space, defined as the set\n\nThe Mahler volume of \"B\" is the product of the volumes of \"B\" and \"B\".\n\nIf \"T\" is an invertible linear transformation, then formula_2; thus applying \"T\" to \"B\" changes its volume by formula_3 and changes the volume of \"B\" by formula_4. Thus the overall Mahler volume of \"B\" is preserved by linear transformations.\n\nThe polar body of an \"n\"-dimensional unit sphere is itself another unit sphere. Thus, its Mahler volume is just the square of its volume, \nHere Γ represents the Gamma function.\nBy affine invariance, any ellipsoid has the same Mahler volume.\n\nThe polar body of a polyhedron or polytope is its dual polyhedron or dual polytope. In particular, the polar body of a cube or hypercube is an octahedron or cross polytope. Its Mahler volume can be calculated as\n\nThe Mahler volume of the sphere is larger than the Mahler volume of the hypercube by a factor of approximately formula_7.\n\nThe Blaschke–Santaló inequality states that the shapes with maximum Mahler volume are the spheres and ellipsoids. The three-dimensional case of this result was proven by Wilhelm Blaschke; the full result was proven much later by using a technique known as Steiner symmetrization by which any centrally symmetric convex body can be replaced with a more sphere-like body without decreasing its Mahler volume.\n\nThe shapes with the minimum known Mahler volume are hypercubes, cross polytopes, and more generally the Hanner polytopes which include these two types of shapes, as well as their affine transformations. The Mahler conjecture states that the Mahler volume of these shapes is the smallest of any \"n\"-dimensional symmetric convex body; it remains unsolved. As Terry Tao writes:\n\n"}
{"id": "308357", "url": "https://en.wikipedia.org/wiki?curid=308357", "title": "Manuel Blum", "text": "Manuel Blum\n\nManuel Blum (Caracas, 26 April 1938) is a Venezuelan computer scientist who received the Turing Award in 1995 \"In recognition of his contributions to the foundations of computational complexity theory and its application to cryptography and program checking\".<ref name=\"doi10.1145/321386.321395\"></ref>\n\nBlum was educated at MIT, where he received his bachelor's degree and his master's degree in EECS in 1959 and 1961 respectively, and his Ph.D. in mathematics in 1964 supervised by Marvin Minsky.\n\nHe worked as a professor of computer science at the University of California, Berkeley until 1999. From 1999 to 2018, he was the Bruce Nelson Professor of Computer Science at Carnegie Mellon University, where his wife, Lenore Blum, was also a professor of Computer Science. In 2002 he was elected to the United States National Academy of Sciences.\n\nHe and his wife resigned from CMU in 2018 to protest against sexism.\n\nIn the 60s he developed an axiomatic complexity theory which was independent of concrete machine models. The theory is based on Gödel numberings and the Blum axioms. Even though the theory is not based on any machine model it yields concrete results like the compression theorem, the gap theorem, the honesty theorem and the Blum speedup theorem.\n\nSome of his other work includes a protocol for flipping a coin over a telephone, median of medians (a linear time selection algorithm), the Blum Blum Shub pseudorandom number generator, the Blum-Goldwasser cryptosystem, and more recently CAPTCHAs.\n\nBlum is also known as the advisor of many prominent researchers. Among his Ph.D. students are Leonard Adleman, Dana Angluin, Shafi Goldwasser, Mor Harchol-Balter, Russell Impagliazzo, Silvio Micali, Gary Miller, Moni Naor, Steven Rudich, Michael Sipser, Ronitt Rubinfeld, Umesh Vazirani, Vijay Vazirani, Luis von Ahn, and Ryan Williams.\n\n"}
{"id": "880710", "url": "https://en.wikipedia.org/wiki?curid=880710", "title": "Maximum modulus principle", "text": "Maximum modulus principle\n\nIn mathematics, the maximum modulus principle in complex analysis states that if \"f\" is a holomorphic function, then the modulus |\"f\" | cannot exhibit a true local maximum that is properly within the domain of \"f\". \n\nIn other words, either \"f\" is a constant function, or, for any point \"z\" inside the domain of \"f\" there exist other points arbitrarily close to \"z\" at which |\"f\" | takes larger values.\n\nLet \"f\" be a function holomorphic on some connected open subset \"D\" of the complex plane ℂ and taking complex values. If \"z\" is a point in \"D\" such that \nfor all \"z\" in a neighborhood of \"z\", then the function \"f\" is constant on \"D\".\n\nBy switching to the reciprocal, we can get the minimum modulus principle. It states that if \"f\" is holomorphic within a bounded domain \"D\", continuous up to the boundary of \"D\", and non-zero at all points, then |\"f\"(z)| takes its minimum value on the boundary of \"D\".\n\nAlternatively, the maximum modulus principle can be viewed as a special case of the open mapping theorem, which states that a nonconstant holomorphic function maps open sets to open sets. If |\"f\"| attains a local maximum at \"z\", then the image of a sufficiently small open neighborhood of \"z\" cannot be open. Therefore, \"f\" is constant.\n\nOne can use the equality\n\nfor complex natural logarithms to deduce that ln |\"f\"(\"z\")| is a harmonic function. Since \"z\" is a local maximum for this function also, it follows from the maximum principle that |\"f\"(\"z\")| is constant. Then, using the Cauchy–Riemann equations we show that \"\"(\"z\") = 0, and thus that \"f\"(\"z\") is constant as well. Similar reasoning shows that |\"f\"| can only have a local minimum (which necessarily has value 0) at an isolated zero of \"f(z)\".\n\nAnother proof works by using Gauss's mean value theorem to \"force\" all points within overlapping open disks to assume the same value. The disks are laid such that their centers form a polygonal path from the value where \"f\"(\"z\") is maximized to any other point in the domain, while being totally contained within the domain. Thus the existence of a maximum value implies that all the values in the domain are the same, thus \"f\"(\"z\") is constant.\n\nA physical interpretation of this principle comes from the heat equation. That is, since log |\"f\"(\"z\")| is harmonic, it is thus the steady state of a heat flow on the region \"D\". Suppose a strict maximum was attained on the interior of \"D\", the heat at this maximum would be dispersing to the points around it, which would contradict the assumption that this represents the steady state of a system.\n\nThe maximum modulus principle has many uses in complex analysis, and may be used to prove the following:\n\n"}
{"id": "22231180", "url": "https://en.wikipedia.org/wiki?curid=22231180", "title": "Minimum-weight triangulation", "text": "Minimum-weight triangulation\n\nIn computational geometry and computer science, the minimum-weight triangulation problem is the problem of finding a triangulation of minimal total edge length. That is, an input polygon or the convex hull of an input point set must be subdivided into triangles that meet edge-to-edge and vertex-to-vertex, in such a way as to minimize the sum of the perimeters of the triangles. The problem is NP-hard for point set inputs, but may be approximated to any desired degree of accuracy. For polygon inputs, it may be solved exactly in polynomial time. The minimum weight triangulation has also sometimes been called the optimal triangulation.\n\nThe problem of minimum weight triangulation of a point set was posed by , who suggested its application to the construction of triangulated irregular network models of land countours, and used a greedy heuristic to approximate it. conjectured that the minimum weight triangulation always coincided with the Delaunay triangulation, but this was quickly disproved by , and indeed showed that the weights of the two triangulations can differ by a linear factor.\n\nThe minimum-weight triangulation problem became notorious when included it in a list of open problems in their book on NP-completeness, and many subsequent authors published partial results on it. Finally, showed it to be NP-hard, and showed that accurate approximations to it can be constructed efficiently.\n\nThe weight of a triangulation of a set of points in the Euclidean plane is defined as the sum of lengths of its edges. Its decision variant is the problem of deciding whether there exists a triangulation of weight less than a given weight; it was proven to be NP-hard by . Their proof is by reduction from PLANAR-1-IN-3-SAT, a special case of the Boolean satisfiability problem in which a 3-CNF whose graph is planar is accepted when it has a truth assignment that satisfies exactly one literal in each clause. The proof uses complex gadgets, and involves computer assistance to verify the correct behavior of these gadgets.\n\nIt is not known whether the minimum-weight triangulation decision problem is NP-complete, since this depends on the known open problem whether the sum of radicals may be computed in polynomial time. However, Mulzer and Rote remark that the problem is NP-complete if the edge weights are rounded to integer values.\n\nAlthough NP-hard, the minimum weight triangulation may be constructed in subexponential time by a dynamic programming algorithm that considers all possible simple cycle separators of formula_1 points within the triangulation, recursively finds the optimal triangulation on each side of the cycle, and chooses the cycle separator leading to the smallest total weight. The total time for this method is formula_2.\n\nSeveral authors have proven results relating the minimum weight triangulation to other triangulations in terms of the approximation ratio, the worst-case ratio of the total edge length of the alternative triangulation to the total length of the minimum weight triangulation. In this vein, it is known that the Delaunay triangulation has an approximation ratio of formula_3, and that the greedy triangulation (the triangulation formed by adding edges in order from shortest to longest, at each step including an edge whenever it does not cross an earlier edge) has an approximation ratio of formula_4. Nevertheless, for randomly distributed point sets, both the Delaunay and greedy triangulations are within a logarithmic factor of the minimum weight.\n\nThe hardness result of Mulzer and Rote also implies the NP-hardness of finding an approximate solution with relative approximation error at most O(1/n). Thus, a fully polynomial approximation scheme for minimum weight triangulation is unlikely. However, a quasi-polynomial approximation scheme is possible: for any constant ε > 0, a solution with approximation ratio 1 + ε can be found in quasi-polynomial time exp(O((log \"n\")).\n\nBecause of the difficulty of finding the exact solutions of the minimum-weight triangulation, many authors have studied heuristics that may in some cases find the solution although they cannot be proven to work in all cases. In particular, much of this research has focused on the problem of finding sets of edges that are guaranteed to belong to the minimum-weight triangulation. If a subgraph of the minimum-weight triangulation found in this way turns out to be connected, then the remaining untriangulated space may be viewed as forming a simple polygon, and the entire triangulation may be found by using dynamic programming to find the optimal triangulation of this polygonal space. The same dynamic programming approach can also be extended to the case that the subgraph has a bounded number of connected components and the same approach of finding a connected graph and then applying dynamic programming to fill in the polygonal gaps surrounding the graph edges has also been used as part of heuristics for finding low-weight but not minimum-weight triangulations.\n\nThe graph formed by connecting two points whenever they are each other's nearest neighbors is necessarily a subgraph of the minimum-weight triangulation. However, this mutual nearest neighbor graph is a matching, and hence is never connected. A related line of research finds large subgraphs of the minimum-weight triangulation by using circle-based \"β\"-skeletons, the geometric graphs formed by including an edge between two points \"u\" and \"v\" whenever there does not exist a third point \"w\" forming an angle \"uwv\" greater than some parameter θ. It has been shown that, for sufficiently small values of θ, the graph formed in this way is a subgraph of the minimum-weight triangulation. However, the choice of θ needed to ensure this is smaller than the angle θ = 90ª for which the \"β\"-skeleton is always connected.\n\nA more sophisticated technique called the LMT-skeleton was proposed by . It is formed by an iterative process, in which two sets of edges are maintained, a set of edges known to belong to the minimum-weight triangulation and a set of edges that are candidates to belong to it. Initially, the set of known edges is initialized to the convex hull of the input, and all remaining pairs of vertices form candidate edges. Then, in each iteration of the construction process, candidate edges are removed whenever there is no pair of triangles formed by the remaining edges forming a quadrilateral for which the candidate edge is the shortest diagonal, and candidate edges are moved to the set of known edges when there is no other candidate edge that crosses them. The LMT-skeleton is defined to be the set of known edges produced after this process stops making any more changes. It is guaranteed to be a subgraph of the minimum-weight triangulation, can be constructed efficiently, and in experiments on sets of up to 200 points it was frequently connected. However it has been shown that on the average for large point sets it has a linear number of connected components.\n\nOther heuristics that have been applied to the minimum weight triangulation problem include genetic algorithms branch and bound, and ant colony optimization algorithms.\n\nA polygon triangulation of minimal weight may be constructed in cubic time using the dynamic programming approach, reported independently by and . In this method, the vertices are numbered consecutively around the boundary of the polygon, and for each diagonal from vertex \"i\" to vertex \"j\" that lies within the polygon, the optimal triangulation is calculated by considering all possible triangles \"ijk\" within the polygon, adding the weights of the optimal triangulations below the diagonals \"ik\" and \"jk\", and choosing the vertex \"k\" that leads to the minimum total weight. That is, if MWT(\"i\",\"j\") denotes the weight of the minimum-weight triangulation of the polygon below edge \"ij\", then the overall algorithm performs the following steps:\nAfter this iteration completes, MWT(1,\"n\") will store the total weight of the minimum weight triangulation. The triangulation itself may be obtained by recursively searching through this array, starting from MWT(1,\"n\"), at each step choosing the triangle \"ijk\" that leads to the minimum value for MWT(\"i\",\"j\") and recursively searching MWT(\"i\",\"k\") and MWT(\"j\",\"k\").\n\nSimilar dynamic programming methods may also be adapted to point set inputs where all but a constant number of points lie on the convex hull of the input, and to point sets that lie on a constant number of nested convex polygons or on a constant number of lines no two of which cross within the convex hull.\n\nIt is also possible to formulate a version of the point set or polygon triangulation problems in which one is allowed to add Steiner points, extra vertices, in order to reduce the total edge length of the resulting triangulations. In some cases, the resulting triangulation may be shorter than the minimum weight triangulation by as much as a linear factor. It is possible to approximate the minimum weight Steiner triangulation of a point set to within a constant factor of optimal, but the constant factor in the approximation is large.\n\nRelated problems that have also been studied include the construction of minimum-weight pseudotriangulations and the characterization of the graphs of minimum-weight triangulations.\n\n"}
{"id": "1371370", "url": "https://en.wikipedia.org/wiki?curid=1371370", "title": "NIPRNet", "text": "NIPRNet\n\nThe Non-classified Internet Protocol (IP) Router Network (NIPRNet) is a private IP network used to exchange unclassified information, including information subject to controls on distribution, among the private network's users. The NIPRNet also provides its users access to the Internet.\n\nNIPRNet is composed of Internet Protocol routers owned by the United States Department of Defense (DOD). It was created in the 1980s and managed by the Defense Information Systems Agency (DISA) to supersede the earlier MILNET.\n\nOver the last decades, NIPRNet has grown faster than the U.S. Department of Defense can monitor. DoD spent $10 million in 2010 to map out the current state of the NIPRNet, in an effort to analyze its expansion, and identify unauthorized users, who are suspected to have quietly joined the network. The NIPRNet survey, which uses IPSonar software developed by Lumeta Corporation, also looked for weakness in security caused by network configuration. The Department of Defense has made a major effort over the last few years, to improve network security. The Pentagon announced it was requesting $2.3 billion in the 2012 budget to bolster network security within the Defense Department and to strengthen ties with its counterparts at the Department of Homeland Security.\n\nSIPRNet and NIPRNet are referred to colloquially as \"sipper-net\" and \"nipper-net\" (or simply \"sipper\" and \"nipper\"), respectively.\n\n\n"}
{"id": "48365214", "url": "https://en.wikipedia.org/wiki?curid=48365214", "title": "Nancy Hingston", "text": "Nancy Hingston\n\nNancy Burgess Hingston is a mathematician whose research applies algebraic topology and functional analysis to differential geometry. She is a professor of mathematics at The College of New Jersey.\n\nHingston's father William Hingston was superintendent of the Central Bucks School District in Pennsylvania; her mother was a high school mathematics and computer science teacher. She graduated from the University of Pennsylvania with a double major in mathematics and physics. After a year studying physics as a graduate student, she switched to mathematics, and completed her Ph.D. in 1981 from Harvard University under the supervision of Raoul Bott. Before joining TCNJ, she taught at the University of Pennsylvania. She has also been a frequent visitor to the Institute for Advanced Study, and has been involved with the Program for Women and Mathematics at the Institute for Advanced Study since its founding in 1994.\n\nHingston made major contributions in geometry. In differential geometry, she proved that the growth rate of closed geodesics in Riemannian 2-spheres is at least the one of prime numbers. In symplectic geometry, she proved the long standing Conley conjecture: every Hamiltonian diffeomorphism of a standard symplectic torus of any even dimension possesses infinitely many periodic points.\n\nHingston was an invited speaker at the International Congress of Mathematicians in 2014.\n\nHer husband, Jovi Tenev, is a lawyer. She has three children.\n\nShe is a fellow of the American Mathematical Society, for \"contributions to differential geometry and the study of closed geodesics.\"\n"}
{"id": "10557570", "url": "https://en.wikipedia.org/wiki?curid=10557570", "title": "Parity-check matrix", "text": "Parity-check matrix\n\nIn coding theory, a parity-check matrix of a linear block code \"C\" is a matrix which describes the linear relations that the components of a codeword must satisfy. It can be used to decide whether a particular vector is a codeword and is also used in decoding algorithms.\n\nFormally, a parity check matrix, \"H\" of a linear code \"C\" is a generator matrix of the dual code, \"C\". This means that a codeword c is in \"C \"if and only if the matrix-vector product (some authors would write this in an equivalent form, c\"H\" = 0.)\n\nThe rows of a parity check matrix are the coefficients of the parity check equations. That is, they show how linear combinations of certain digits (components) of each codeword equal zero. For example, the parity check matrix\n\ncompactly represents the parity check equations,\nthat must be satisfied for the vector formula_3 to be a codeword of \"C\".\n\nFrom the definition of the parity-check matrix it directly follows the minimum distance of the code is the minimum number \"d\" such that every \"d - 1\" columns of a parity-check matrix \"H\" are linearly independent while there exist \"d\" columns of \"H\" that are linearly dependent.\n\nThe parity check matrix for a given code can be derived from its generator matrix (and vice versa). If the generator matrix for an [\"n\",\"k\"]-code is in standard form \nthen the parity check matrix is given by\nbecause \nNegation is performed in the finite field F. Note that if the characteristic of the underlying field is 2 (i.e., 1 + 1 = 0 in that field), as in binary codes, then -\"P\" = \"P\", so the negation is unnecessary.\n\nFor example, if a binary code has the generator matrix\n\nthen its parity check matrix is\n\nIt can be verified that G is a formula_9 matrix, while H is a formula_10 matrix.\n\nFor any (row) vector x of the ambient vector space, s = \"H\"x is called the syndrome of x. The vector x is a codeword if and only if s = 0. The calculation of syndromes is the basis for the syndrome decoding algorithm.\n\n\n"}
{"id": "12745947", "url": "https://en.wikipedia.org/wiki?curid=12745947", "title": "Permutohedron", "text": "Permutohedron\n\nIn mathematics, the permutohedron of order \"n\" (also spelled permutahedron) is an (\"n\" − 1)-dimensional polytope embedded in an \"n\"-dimensional space, the vertices of which are formed by permuting the coordinates of the vector (1, 2, 3, ..., \"n\").\n\nAccording to , permutohedra were first studied by . The name \"permutohedron\" (or rather its French version, \"permutoèdre\") was coined by . Regarding this coinage, they write that the word \"permutohedron\" is barbaric, but easy to remember, and that they submit it to the criticism of their readers.\n\nThe alternative spelling \"permutahedron\" is sometimes also used. Permutohedra are sometimes also called permutation polytopes, but this terminology is also used for a related polytope, the Birkhoff polytope, defined as the convex hull of permutation matrices. More generally, uses the phrase \"permutation polytope\" for any polytope whose vertices are in 1-1 correspondence with the permutations of some set.\n\nThe permutohedron of order \"n\" has \"n\"! vertices, each of which is adjacent to \"n\" − 1 others, so the total number of edges is (\"n\" − 1)\"n\"!/2. Each edge has length , and connects two vertices that differ by swapping two coordinates the values of which differ by one.\n\nThe permutohedron has one facet for each nonempty proper subset \"S\" of {1, 2, 3, ..., \"n\"}, consisting of the vertices in which all coordinates in positions in \"S\" are smaller than all coordinates in positions not in \"S\". Thus, the total number of facets is 2 − 2. More generally, the faces of the permutohedron (including the permutohedron itself, but not including the empty set) are in 1-1 correspondence with the strict weak orderings on a set of \"n\" items: a face of dimension \"d\" corresponds to a strict weak ordering in which there are \"n\" − \"d\" equivalence classes. Because of this correspondence, the number of faces is given by the ordered Bell numbers.\n\nThe number of formula_1-dimensional faces in a permutohedron of order formula_2 is found in the triangle \nwhere formula_4 are the Stirling numbers of the second kind\n\nThe permutohedron is vertex-transitive: the symmetric group \"S\" acts on the permutohedron by permutation of coordinates.\n\nThe permutohedron is a zonotope; a translated copy of the permutohedron can be generated as the Minkowski sum of the \"n\"(\"n\" − 1)/2 line segments that connect the pairs of the standard basis vectors .\n\nThe vertices and edges of the permutohedron are isomorphic as an undirected graph to one of the Cayley graphs of the symmetric group: The Cayley graph generated by the adjacent transpositions in the symmetric group (transpositions that swap consecutive elements). The Cayley graph of S, shown on the right, is generated by the transpositions (1,2), (2,3), and (3,4).<br>The Cayley graph labeling can be constructed by labeling each vertex by the inverse of the permutation given by its coordinates.\n\nThis Cayley graph is Hamiltonian; a Hamiltonian cycle may be found by the Steinhaus–Johnson–Trotter algorithm.\n\nThe permutohedron of order \"n\" lies entirely in the (\"n\" − 1)-dimensional hyperplane consisting of all points whose coordinates sum to the number\n\nMoreover, this hyperplane can be tiled by infinitely many translated copies of the permutohedron. Each of them differs from the basic permutohedron by an element of a certain (\"n\" − 1)-dimensional lattice, which consists of the \"n\"-tuples of integers that sum to zero and whose residues (modulo \"n\") are all equal:\n\nThus, the permutohedron of order 4 shown above tiles the 3-dimensional space by translation. Here the 3-dimensional space is the affine subspace of the 4-dimensional space R with coordinates \"x\", \"y\", \"z\", \"w\" that consists of the 4-tuples of real numbers whose sum is 10,\n\nOne easily checks that for each of the following four vectors,\n\nthe sum of the coordinates is zero and all coordinates are congruent to 1 (mod 4). Any three of these vectors generate the translation lattice.\n\nThe tessellations formed in this way from the order-2, order-3, and order-4 permutohedra, respectively, are the apeirogon, the regular hexagonal tiling, and the bitruncated cubic honeycomb. The dual tessellations contain all simplex facets, although they are not regular polytopes beyond order-3.\n\n\n\n\n"}
{"id": "25935424", "url": "https://en.wikipedia.org/wiki?curid=25935424", "title": "Plane of rotation", "text": "Plane of rotation\n\nIn geometry, a plane of rotation is an abstract object used to describe or visualize rotations in space. In three dimensions it is an alternative to the axis of rotation, but unlike the axis of rotation it can be used in other dimensions, such as two, four or more dimensions.\n\nMathematically such planes can be described in a number of ways. They can be described in terms of planes and angles of rotation. They can be associated with bivectors from geometric algebra. They are related to the eigenvalues and eigenvectors of a rotation matrix. And in particular dimensions they are related to other algebraic and geometric properties, which can then be generalised to other dimensions.\n\nPlanes of rotation are not used much in two and three dimensions, as in two dimensions there is only one plane so identifying the plane of rotation is trivial and rarely done, while in three dimensions the axis of rotation serves the same purpose and is the more established approach. The main use for them is in describing more complex rotations in higher dimensions, where they can be used to break down the rotations into simpler parts. This can be done using geometric algebra, with the planes of rotations associated with simple bivectors in the algebra.\n\nFor this article, all \"planes\" are planes through the origin, that is they contain the zero vector. Such a plane in -dimensional space is a two-dimensional linear subspace of the space. It is completely specified by any two non-zero and non-parallel vectors that lie in the plane, that is by any two vectors and , such that\n\nwhere is the exterior product from exterior algebra or geometric algebra (in three dimensions the cross product can be used). More precisely, the quantity is the bivector associated with the plane specified by and , and has magnitude , where is the angle between the vectors; hence the requirement that the vectors be nonzero and nonparallel.\n\nIf the bivector is written , then the condition that a point lies on the plane associated with is simply\n\nThis is true in all dimensions, and can be taken as the definition on the plane. In particular, from the properties of the exterior product it is satisfied by both and , and so by any vector of the form\n\nwith and real numbers. As and range over all real numbers, ranges over the whole plane, so this can be taken as another definition of the plane.\n\nA \"plane of rotation\" for a particular rotation is a plane that is mapped to itself by the rotation. The plane is not fixed, but all vectors in the plane are mapped to other vectors in the same plane by the rotation. This transformation of the plane to itself is always a rotation about the origin, through an angle which is the angle of rotation for the plane.\n\nEvery rotation except for the identity rotation (with matrix the identity matrix) has at least one plane of rotation, and up to\n\nplanes of rotation, where is the dimension. The maximum number of planes up to eight dimensions is shown in this table:\n\nWhen a rotation has multiple planes of rotation they are always orthogonal to each other, with only the origin in common. This is a stronger condition than to say the planes are at right angles; it instead means that the planes have no nonzero vectors in common, and that every vector in one plane is orthogonal to every vector in the other plane. This can only happen in four or more dimensions. In two dimensions there is only one plane, while in three dimensions all planes have at least one nonzero vector in common, along their line of intersection.\n\nIn more than three dimensions planes of rotation are not always unique. For example the negative of the identity matrix in four dimensions (the central inversion),\n\ndescribes a rotation in four dimensions in which every plane through the origin is a plane of rotation through an angle , so any pair of orthogonal planes generates the rotation. But for a general rotation it is at least theoretically possible to identify a unique set of orthogonal planes, in each of which points are rotated through an angle, so the set of planes and angles fully characterise the rotation.\n\nIn two-dimensional space there is only one plane of rotation, the plane of the space itself. In a Cartesian coordinate system it is the Cartesian plane, in complex numbers it is the complex plane. Any rotation therefore is of the whole plane, i.e. of the space, keeping only the origin fixed. It is specified completely by the signed angle of rotation, in the range for example − to . So if the angle is the rotation in the complex plane is given by Euler's formula:\n\nwhile the rotation in a Cartesian plane is given by the rotation matrix:\n\nIn three-dimensional space there are an infinite number of planes of rotation, only one of which is involved in any given rotation. That is, for a general rotation there is precisely one plane which is associated with it or which the rotation takes place in. The only exception is the trivial rotation, corresponding to the identity matrix, in which no rotation takes place.\n\nIn any rotation in three dimensions there is always a fixed axis, the axis of rotation. The rotation can be described by giving this axis, with the angle through which the rotation turns about it; this is the axis angle representation of a rotation. The plane of rotation is the plane orthogonal to this axis, so the axis is a surface normal of the plane. The rotation then rotates this plane through the same angle as it rotates around the axis, that is everything in the plane rotates by the same angle about the origin.\n\nOne example is shown in the diagram, where the rotation takes place about the -axis. The plane of rotation is the -plane, so everything in that plane it kept in the plane by the rotation. This could be described by a matrix like the following, with the rotation being through an angle (about the axis or in the plane):\nAnother example is the Earth's rotation. The axis of rotation is the line joining the North Pole and South Pole and the plane of rotation is the plane through the equator between the Northern and Southern Hemispheres. Other examples include mechanical devices like a gyroscope or flywheel which store rotational energy in mass usually along the plane of rotation.\n\nIn any three dimensional rotation the plane of rotation is uniquely defined. Together with the angle of rotation it fully describes the rotation. Or in a continuously rotating object the rotational properties such as the rate of rotation can be described in terms of the plane of rotation. It is perpendicular to, and so is defined by and defines, an axis of rotation, so any description of a rotation in terms of a plane of rotation can be described in terms of an axis of rotation, and vice versa. But unlike the axis of rotation the plane generalises into other, in particular higher, dimensions.\n\nA general rotation in four-dimensional space has only one fixed point, the origin. Therefore an axis of rotation cannot be used in four dimensions. But planes of rotation can be used, and each non-trivial rotation in four dimensions has one or two planes of rotation.\n\nA rotation with only one plane of rotation is a simple rotation. In a simple rotation there is a fixed plane, and rotation can be said to take place about this plane, so points as they rotate do not change their distance from this plane. The plane of rotation is orthogonal to this plane, and the rotation can be said to take place in this plane.\n\nFor example the following matrix fixes the -plane: points in that plane and only in that plane are unchanged. The plane of rotation is the -plane, points in this plane are rotated through an angle . A general point rotates only in the -plane, that is it rotates around the -plane by changing only its and coordinates.\n\nIn two and three dimensions all rotations are simple, in that they have only one plane of rotation. Only in four and more dimensions are there rotations that are not simple rotations. In particular in four dimensions there are also double and isoclinic rotations.\n\nIn a double rotation there are two planes of rotation, no fixed planes, and the only fixed point is the origin. The rotation can be said to take place in both planes of rotation, as points in them are rotated within the planes. These planes are orthogonal, that is they have no vectors in common so every vector in one plane is at right angles to every vector in the other plane. The two rotation planes span four-dimensional space, so every point in the space can be specified by two points, one on each of the planes.\n\nA double rotation has two angles of rotation, one for each plane of rotation. The rotation is specified by giving the two planes and two non-zero angles, and (if either angle is zero the rotation is simple). Points in the first plane rotate through , while points in the second plane rotate through . All other points rotate through an angle between and , so in a sense they together determine the amount of rotation. For a general double rotation the planes of rotation and angles are unique, and given a general rotation they can be calculated. For example a rotation of in the -plane and in the -plane is given by the matrix\n\nA special case of the double rotation is when the angles are equal, that is if . This is called an isoclinic rotation, and it differs from a general double rotation in a number of ways. For example in an isoclinic rotations all non-zero points rotate through the same angle, . Most importantly the planes of rotation are not uniquely identified. There are instead an infinite number of pairs of orthogonal planes that can be treated as planes of rotation. For example any point can be taken, and the plane it rotates in together with the plane orthogonal to it can be used as two planes of rotation.\n\nAs already noted the maximum number of planes of rotation in dimensions is\n\nso the complexity quickly increases with more than four dimensions and categorising rotations as above becomes too complex to be practical, but some observations can be made.\n\nSimple rotations can be identified in all dimensions, as rotations with just one plane of rotation. A simple rotation in dimensions takes place about (that is at a fixed distance from) an -dimensional subspace orthogonal to the plane of rotation.\n\nA general rotation is not simple, and has the maximum number of planes of rotation as given above. In the general case the angles of rotations in these planes are distinct and the planes are uniquely defined. If any of the angles are the same then the planes are not unique, as in four dimensions with an isoclinic rotation.\n\nIn even dimensions () there are up to planes of rotation span the space, so a general rotation rotates all points except the origin which is the only fixed point. In odd dimensions () there are planes and angles of rotation, the same as the even dimension one lower. These do not span the space, but leave a line which does not rotate – like the axis of rotation in three dimensions, except rotations do not take place about this line but in multiple planes orthogonal to it.\n\nThe examples given above were chosen to be clear and simple examples of rotations, with planes generally parallel to the coordinate axes in three and four dimensions. But this is not generally the case: planes are not usually parallel to the axes, and the matrices cannot simply be written down. In all dimensions the rotations are fully described by the planes of rotation and their associated angles, so it is useful to be able to determine them, or at least find ways to describe them mathematically.\n\nEvery simple rotation can be generated by two reflections. Reflections can be specified in dimensions by giving an -dimensional subspace to reflect in, so a two-dimensional reflection is in a line, a three-dimensional reflection is in a plane, and so on. But this becomes increasingly difficult to apply in higher dimensions, so it is better to use vectors instead, as follows.\n\nA reflection in dimensions is specified by a vector perpendicular to the -dimensional subspace. To generate simple rotations only reflections that fix the origin are needed, so the vector does not have a position, just direction. It does also not matter which way it is facing: it can be replaced with its negative without changing the result. Similarly unit vectors can be used to simplify the calculations.\n\nSo the reflection in a -dimensional space is given by the unit vector perpendicular to it, , thus:\n\nwhere the product is the geometric product from geometric algebra.\n\nIf is reflected in another, distinct, -dimensional space, described by a unit vector perpendicular to it, the result is\n\nThis is a simple rotation in dimensions, through twice the angle between the subspaces, which is also the angle between the vectors m and . It can be checked using geometric algebra that this is a rotation, and that it rotates all vectors as expected.\n\nThe quantity is a rotor, and is its inverse as\n\nSo the rotation can be written\n\nwhere is the rotor.\n\nThe plane of rotation is the plane containing and , which must be distinct otherwise the reflections are the same and no rotation takes place. As either vector can be replaced by its negative the angle between them can always be acute, or at most . The rotation is through \"twice\" the angle between the vectors, up to or a half-turn. The sense of the rotation is to rotate from towards : the geometric product is not commutative so the product is the inverse rotation, with sense from to .\n\nConversely all simple rotations can be generated this way, with two reflections, by two unit vectors in the plane of rotation separated by half the desired angle of rotation. These can be composed to produce more general rotations, using up to reflections if the dimension is even, if is odd, by choosing pairs of reflections given by two vectors in each plane of rotation.\n\nBivectors are quantities from geometric algebra, clifford algebra and the exterior algebra, which generalise the idea of vectors into two dimensions. As vectors are to lines, so are bivectors to planes. So every plane (in any dimension) can be associated with a bivector, and every simple bivector is associated with a plane. This makes them a good fit for describing planes of rotation.\n\nEvery rotation plane in a rotation has a simple bivector associated with it. This is parallel to the plane and has magnitude equal to the angle of rotation in the plane. These bivectors are summed to produce a single, generally non-simple, bivector for the whole rotation. This can generate a rotor through the exponential map, which can be used to rotate an object.\n\nBivectors are related to rotors through the exponential map (which applied to bivectors generates rotors and rotations using De Moivre's formula). In particular given any bivector the rotor associated with it is\n\nThis is a simple rotation if the bivector is simple, a more general rotation otherwise. When squared,\n\nit gives a rotor that rotates through twice the angle. If is simple then this is the same rotation as is generated by two reflections, as the product gives a rotation through twice the angle between the vectors. These can be equated,\n\nfrom which it follows that the bivector associated with the plane of rotation containing and that rotates to is\n\nThis is a simple bivector, associated with the simple rotation described. More general rotations in four or more dimensions are associated with sums of simple bivectors, one for each plane of rotation, calculated as above.\n\nExamples include the two rotations in four dimensions given above. The simple rotation in the -plane by an angle has bivector , a simple bivector. The double rotation by and in the -plane and -planes has bivector , the sum of two simple bivectors and which are parallel to the two planes of rotation and have magnitudes equal to the angles of rotation.\n\nGiven a rotor the bivector associated with it can be recovered by taking the logarithm of the rotor, which can then be split into simple bivectors to determine the planes of rotation, although in practice for all but the simplest of cases this may be impractical. But given the simple bivectors geometric algebra is a useful tool for studying planes of rotation using algebra like the above.\n\nThe planes of rotations for a particular rotation using the eigenvalues. Given a general rotation matrix in dimensions its characteristic equation has either one (in odd dimensions) or zero (in even dimensions) real roots. The other roots are in complex conjugate pairs, exactly\n\nsuch pairs. These correspond to the planes of rotation, the eigenplanes of the matrix, which can be calculated using algebraic techniques. In addition arguments of the complex roots are the magnitudes of the bivectors associated with the planes of rotations. The form of the characteristic equation is related to the planes, making it possible to relate its algebraic properties like repeated roots to the bivectors, where repeated bivector magnitudes have particular geometric interpretations.\n\n\n"}
{"id": "5967883", "url": "https://en.wikipedia.org/wiki?curid=5967883", "title": "Position tolerance", "text": "Position tolerance\n\nPosition Tolerance (symbol: ⌖) is a geometric dimensioning and tolerancing (GD&T) location control used on engineering drawings to specify desired location, as well as allowed deviation to the position of a feature on a part. Position tolerance must only be applied to features of size, which requires that the feature have at least two opposable points.\n\n"}
{"id": "2612376", "url": "https://en.wikipedia.org/wiki?curid=2612376", "title": "Product and manufacturing information", "text": "Product and manufacturing information\n\nProduct and manufacturing information, also abbreviated PMI, conveys non-geometric attributes in 3D computer-aided design (CAD) and Collaborative Product Development systems necessary for manufacturing product components and assemblies. PMI may include geometric dimensions and tolerances, 3D annotation (text) and dimensions, surface finish, and material specifications. PMI is used in conjunction with the 3D model within model-based definition to allow for the elimination of 2D drawings for data set utilization.\n\nThe PMI annotation is created on the 3D CAD model, associated to edges and faces, and can be exported into neutral formats such as ISO 10303 STEP and 3D PDF. This information can then be used by a number of down-stream processes. PMI can be used to generate annotation on a traditional 2D drawing the data. However, generally, PMI is used to visualized product definition within the 3D model, thus removing the need for drawings. Some 3D model formats enable computer-aided manufacturing software to access PMI directly for CNC programming. The PMI also may be used by tolerance analysis and Coordinate-measuring machine (CMM) software applications if the modeling application permits.\n\nPMI items are often organized within annotation views. Annotation views typically view including camera/view position, selected and also the particular state of the assembly (visibility, rendering mode, sometime even position of each element of the assembly). CAD applications have different notions of PMI Views (for instance \"Capture Views\" and \"Annotation \nViews\" are specific to Dassault Systems CATIA, etc.).\n\nFor anyone to be able to display any kind of PMI View, Adobe Systems has unified their format and added their description to the PDF format (version 1.7).\n\nIn an effort to unify the visualization of PMI across the different existing solutions, Adobe Systems has released a version of the Myriad CAD font that allows display of PMI information from almost any CAD application. Similarly, Siemens PLM Software offers downloadable font sets for multiple languages such as for Asian character sets. The ISO 10303 STEP standards also handle a wide range of PMI information.\n\nIndustry standards for defining PMI include ASME Y14.41-2003 Digital Product Data Definition Practices and ISO 1101:2004 Geometrical Product Specifications (GPS) -- Geometrical tolerancing—Tolerances of form, orientation, location and run-out.\n\n"}
{"id": "25666636", "url": "https://en.wikipedia.org/wiki?curid=25666636", "title": "Quantum affine algebra", "text": "Quantum affine algebra\n\nIn mathematics, a quantum affine algebra (or affine quantum group) is a Hopf algebra that is a \"q\"-deformation of the universal enveloping algebra of an affine Lie algebra. They were introduced independently by and as a special case of their general construction of a quantum group from a Cartan matrix. One of their principal applications has been to the theory of solvable lattice models in quantum statistical mechanics, where the Yang-Baxter equation occurs with a spectral parameter. Combinatorial aspects of the representation theory of quantum affine algebras can be described simply using crystal bases, which correspond to the degenerate case when the deformation parameter \"q\" vanishes and the Hamiltonian of the associated lattice model can be explicitly diagonalized.\n\n\n"}
{"id": "31601344", "url": "https://en.wikipedia.org/wiki?curid=31601344", "title": "Quantum dilogarithm", "text": "Quantum dilogarithm\n\nIn mathematics, the quantum dilogarithm also known as q-exponential is a special function defined by the formula\n\nThus in the notation of the page on q-exponential mentioned above, formula_2 .\n\nLet formula_3 be “q-commuting variables”, that is elements of a suitable\nnoncommutative algebra satisfying Weyl’s relation formula_4. Then, the quantum dilogarithm\nsatisfies Schützenberger’s identity\n\nFaddeev-Volkov's identity\n\nand Faddeev-Kashaev's identity\n\nThe latter is known to be a quantum generalization of Roger's five term dilogarithm identity.\n\nFaddeev's quantum dilogarithm formula_8 is defined by the following formula:\n\nwhere the contour of integration formula_10 goes along the real axis outside a small neighborhood of the origin and deviates into the upper half-plane near the origin. The same function can be described by the integral formula of Woronowicz:\n\nLudvig Faddeev discovered the quantum pentagon identity:\n\nwhere formula_13 and formula_14 are self-adjoint (normalized) quantum mechanical momentum and position operators satisfying Heisenberg's commutation relation\n\nand the inversion relation\n\nThe quantum dilogarithm finds applications in mathematical physics, quantum topology, cluster algebra theory.\n\nThe precise relationship between the q-exponential and formula_17 is expressed by the equality\n\nvalid for Im formula_19.\n\n"}
{"id": "35458775", "url": "https://en.wikipedia.org/wiki?curid=35458775", "title": "R. W. H. T. Hudson", "text": "R. W. H. T. Hudson\n\nRonald William Henry Turnbull Hudson (16 July 1876 – 20 September 1904) was a British mathematician. Son of W.H.H. Hudson (professor of mathematics, King's College London) Ronald W.H.T. Hudson was considered in his day to be the most gifted geometer in all of Cambridge. Hudson's life was cut short when he died in a mountaineering accident at the age of 28, but his posthumously-published book Kummer's Quartic Surface allows mathematicians today access to his work. Hudson's sister, Hilda Hudson was likewise a gifted mathematician, being a graduate of Newnham, a lecturer at the University of Berlin, and ultimately being awarded the O.B.E. in 1919.\n\n"}
{"id": "35194042", "url": "https://en.wikipedia.org/wiki?curid=35194042", "title": "Raynaud's isogeny theorem", "text": "Raynaud's isogeny theorem\n\nIn mathematics, Raynaud's isogeny theorem, proved by , relates the Faltings heights of two isogeneous elliptic curves.\n"}
{"id": "2672905", "url": "https://en.wikipedia.org/wiki?curid=2672905", "title": "Set Theory: An Introduction to Independence Proofs", "text": "Set Theory: An Introduction to Independence Proofs\n\nSet Theory: An Introduction to Independence Proofs is a textbook and reference work in set theory by Kenneth Kunen. It starts from basic notions, including the ZFC axioms, and quickly develops combinatorial notions such as trees, Suslin's problem, ◊, and Martin's axiom. It develops some basic model theory (rather specifically aimed at models of set theory) and the theory of Gödel's constructible universe L.\n\nThe book then proceeds to describe the method of forcing. Through exercises, the reader learns to apply the method to prove logical independence results in set theory.\n\nThis book is not suitable for beginners, but graduate students with some minimal experience in set theory and formal logic could find it a valuable self-teaching tool, particularly in regard to forcing. Some find it easier to read than a true reference work such as Thomas Jech's \"Set Theory\". It is the standard textbook from which to learn forcing, though it has the disadvantage that the exposition of forcing relies somewhat on the earlier presentation of Martin's axiom, and the style is perhaps overly concise. John L. Bell's \"Set Theory: Boolean-Valued Models and Independence Proofs\" is an alternative, though it presents the topic from the standpoint of the more conceptually elegant though less easily used technique of Boolean-valued models of set theory. Jech's presentation is a hybrid of the former two styles.\n\nKunen totally rewrote the book for the 2011 edition (under the title \"set theory\"), including more model theory.\n\n"}
{"id": "21535918", "url": "https://en.wikipedia.org/wiki?curid=21535918", "title": "Specker sequence", "text": "Specker sequence\n\nIn computability theory, a Specker sequence is a computable, monotonically increasing, bounded sequence of rational numbers whose supremum is not a computable real number. The first example of such a sequence was constructed by Ernst Specker (1949). \n\nThe existence of Specker sequences has consequences for computable analysis. The fact that such sequences exist means that the collection of all computable real numbers does not satisfy the least upper bound principle of real analysis, even when considering only computable sequences. A common way to resolve this difficulty is to consider only sequences that are accompanied by a modulus of convergence; no Specker sequence has a computable modulus of convergence. \n\nThe least upper bound principle has also been analyzed in the program of reverse mathematics, where the exact strength of this principle has been determined. In the terminology of that program, the least upper bound principle is equivalent to ACA over RCA.\n\nThe following construction is described by Kushner (1984). Let \"A\" be any recursively enumerable set of natural numbers that is not decidable, and let (\"a\") be a computable enumeration of \"A\" without repetition. Define a sequence (\"q\") of rational numbers with the rule\nIt is clear that each \"q\" is nonnegative and rational, and that the sequence \"q\" is strictly increasing. Moreover, because \"a\" has no repetition, it is possible to estimate each \"q\" against the series\nThus the sequence (\"q\") is bounded above by 1. Classically, this means that \"q\" has a supremum \"x\". \n\nIt is shown that \"x\" is not a computable real number. The proof uses a particular fact about computable real numbers. If \"x\" were computable then there would be a computable function \"r\"(\"n\") such that |\"q\" - \"q\"| < 1/\"n\" for all \"i\",\"j\" > \"r\"(\"n\"). To compute \"r\", compare the binary expansion of \"x\" with the binary expansion of \"q\" for larger and larger values of \"i\". The definition of \"q\" causes a single binary digit to go from 0 to 1 each time \"i\" increases by 1. Thus there will be some \"n\" where a large enough initial segment of \"x\" has already been determined by \"q\" that no additional binary digits in that segment could ever be turned on, which leads to an estimate on the distance between \"q\" and \"q\" for \"i\",\"j\" > \"n\". \n\nIf any such a function \"r\" were computable, it would lead to a decision procedure for \"A\", as follows. Given an input \"k\", compute \"r\"(2). Note that if \"k\" were to appear in the sequence (\"a\"), this would cause the sequence (\"q\") to increase by 2, but this cannot happen once all the elements of (\"q\") are within 2 of each other. Thus, if \"k\" is going to be enumerated into \"a\", it must be enumerated for a value of \"i\" less than \"r\"(2). This leaves a finite number of possible places where \"k\" could be enumerated. To complete the decision procedure, check these in an effective manner and the return 0 or 1 depending on whether \"k\" is found.\n\n\n"}
{"id": "534914", "url": "https://en.wikipedia.org/wiki?curid=534914", "title": "Spectral graph theory", "text": "Spectral graph theory\n\nIn mathematics, spectral graph theory is the study of the properties of a graph in relationship to the characteristic polynomial, eigenvalues, and eigenvectors of matrices associated with the graph, such as its adjacency matrix or Laplacian matrix.\n\nThe adjacency matrix of a simple graph is a real symmetric matrix and is therefore orthogonally diagonalizable; its eigenvalues are real algebraic integers.\n\nWhile the adjacency matrix depends on the vertex labeling, its spectrum is a graph invariant, although not a complete one.\n\nSpectral graph theory is also concerned with graph parameters that are defined via multiplicities of eigenvalues of matrices associated to the graph, such as the Colin de Verdière number.\n\nTwo graphs are called cospectral or isospectral if the adjacency matrices of the graphs have equal multisets of eigenvalues.\nCospectral graphs need not be isomorphic, but isomorphic graphs are always cospectral.\n\nA graph formula_1 is said to be determined by its spectrum if any other graph with the same spectrum as formula_1 is isomorphic to formula_1.\n\nSome first examples of families of graphs that are determined by their spectrum include:\n\nA pair of graphs are said to be cospectral mates if they have the same spectrum, but are non-isomorphic.\n\nThe smallest pair of cospectral mates is {\"K\", \"C\" U \"K\"}, comprising the 5-vertex star and the graph union of the 4-vertex cycle and the single-vertex graph, as reported by Collatz and Sinogowitz in 1957.\n\nThe smallest pair of polyhedral cospectral mates are enneahedra with eight vertices each.\n\nAlmost all trees are cospectral, i.e., the share of cospectral trees on \"n\" vertices tends to 1 as \"n\" grows.\n\nA pair of regular graphs are cospectral if and only if their complements are cospectral.\n\nA pair of distance-regular graphs are cospectral if and only if they have the same intersection array.\n\nCospectral graphs can also be constructed by means of the Sunada method.\n\nAnother important source of cospectral graphs are the point-collinearity graphs and the line-intersection graphs of point-line geometries. These graphs are always cospectral but are often non-isomorphic.\n\nThe famous Cheeger's inequality from Riemannian geometry has a discrete analogue involving the Laplacian matrix; this is perhaps the most important theorem in spectral graph theory and one of the most useful facts in algorithmic applications. It approximates the sparsest cut of a graph through the second eigenvalue of its Laplacian.\n\nThe Cheeger constant (also Cheeger number or isoperimetric number) of a graph is a numerical measure of whether or not a graph has a \"bottleneck\". The Cheeger constant as a measure of \"bottleneckedness\" is of great interest in many areas: for example, constructing well-connected networks of computers, card shuffling, and low-dimensional topology (in particular, the study of hyperbolic 3-manifolds).\n\nMore formally, the Cheeger constant \"h\"(\"G\") of a graph \"G\" on \"n\" vertices is defined as\nwhere the minimum is over all nonempty sets \"S\" of at most \"n\"/2 vertices and ∂(\"S\") is the \"edge boundary\" of \"S\", i.e., the set of edges with exactly one endpoint in \"S\".\n\nWhen the graph \"G\" is \"d\"-regular, there is a relationship between \"h\"(\"G\") and the spectral gap \"d\" − λ of \"G\". An inequality due to Dodziuk and independently Alon and Milman states that\n\nThis inequality is closely related to the Cheeger bound for Markov chains and can be seen as a discrete version of Cheeger's inequality in Riemannian geometry.\n\nThere is an eigenvalue bound for independent sets in regular graphs, originally due to Alan J. Hoffman and Philippe Delsarte.\n\nSuppose that formula_1 is a formula_7-regular graph on formula_8 vertices with least eigenvalue formula_9. Then:formula_10where formula_11 denotes its independence number.\n\nThis bound has been applied to establish e.g. algebraic proofs of the Erdős–Ko–Rado theorem and its analogue for intersecting families of subspaces over finite fields.\n\nSpectral graph theory emerged in the 1950s and 1960s. Besides graph theoretic research on the relationship between structural and spectral properties of graphs, another major source was research in quantum chemistry, but the connections between these two lines of work were not discovered until much later. The 1980 monograph \"Spectra of Graphs\" by Cvetković, Doob, and Sachs summarised nearly all research to date in the area. In 1988 it was updated by the survey \"Recent Results in the Theory of Graph Spectra\". The 3rd edition of \"Spectra of Graphs\" (1995) contains a summary of the further recent contributions to the subject. Discrete geometric analysis created and developed by Toshikazu Sunada in the 2000s deals with spectral graph theory in terms of discrete Laplacians associated with weighted graphs, and finds application in various fields, including shape analysis. In most recent years, the spectral graph theory has expanded to vertex-varying graphs often encountered in many real-life applications.\n\n\n"}
{"id": "744171", "url": "https://en.wikipedia.org/wiki?curid=744171", "title": "Strong perfect graph theorem", "text": "Strong perfect graph theorem\n\nIn graph theory, the strong perfect graph theorem is a forbidden graph characterization of the perfect graphs as being exactly the graphs that have neither odd holes (odd-length induced cycles) nor odd antiholes (complements of odd holes). It was conjectured by Claude Berge in 1961. A proof by Maria Chudnovsky, Neil Robertson, Paul Seymour, and Robin Thomas was announced in 2002 and published by them in 2006.\n\nThe proof of the strong perfect graph theorem won for its authors a $10,000 prize offered by Gérard Cornuéjols of Carnegie Mellon University and the 2009 Fulkerson Prize.\n\nA perfect graph is a graph in which, for every induced subgraph, the size of the maximum clique equals the minimum number of colors in a coloring of the graph; perfect graphs include many well-known graph classes including the bipartite graphs, chordal graphs, and comparability graphs. In his 1961 and 1963 works defining for the first time this class of graphs, Claude Berge observed that it is impossible for a perfect graph to contain an odd hole, an induced subgraph in the form of an odd-length cycle graph of length five or more, because odd holes have clique number two and chromatic number three. Similarly, he observed that perfect graphs cannot contain odd antiholes, induced subgraphs complementary to odd holes: an odd antihole with 2\"k\" + 1 vertices has clique number \"k\" and chromatic number \"k\" + 1, again impossible for a perfect graphs. The graphs having neither odd holes nor odd antiholes became known as the Berge graphs.\n\nBerge conjectured that every Berge graph is perfect, or equivalently that the perfect graphs and the Berge graphs define the same class of graphs. This became known as the strong perfect graph conjecture, until its proof in 2002, when it was renamed the strong perfect graph theorem.\n\nAnother conjecture of Berge, proved in 1972 by László Lovász, is that the complement of every perfect graph is also perfect. This became known as the perfect graph theorem, or (to distinguish it from the strong perfect graph conjecture/theorem) the weak perfect graph theorem. Because Berge's forbidden graph characterization is self-complementary, the weak perfect graph theorem follows immediately from the strong perfect graph theorem.\n\nThe proof of the strong perfect graph theorem by Chudnovsky et al. follows an outline conjectured in 2001 by Conforti, Cornuéjols, Robertson, Seymour, and Thomas, according to which every Berge graph either forms one of five types of basic building block (special classes of perfect graphs) or it has one of four different types of structural decomposition into simpler graphs. A minimally imperfect Berge graph cannot have any of these decompositions, from which it follows that no counterexample to the theorem can exist. This idea was based on previous conjectured structural decompositions of similar type that would have implied the strong perfect graph conjecture but turned out to be false.\n\nThe five basic classes of perfect graphs that form the base case of this structural decomposition are the bipartite graphs, line graphs of bipartite graphs, complementary graphs of bipartite graphs, complements of line graphs of bipartite graphs, and double split graphs. It is easy to see that bipartite graphs are perfect: in any nontrivial induced subgraph, the clique number and chromatic number are both two and therefore both equal. The perfection of complements of bipartite graphs, and of complements of line graphs of bipartite graphs, are both equivalent to Kőnig's theorem relating the sizes of maximum matchings, maximum independent sets, and minimum vertex covers in bipartite graphs. The perfection of line graphs of bipartite graphs can be stated equivalently as the fact that bipartite graphs have chromatic index equal to their maximum degree, proven by . Thus, all four of these basic classes are perfect. The double split graphs are a relative of the split graphs that can also be shown to be perfect.\n\nThe four types of decompositions considered in this proof are 2-joins, complements of 2-joins, balanced skew partitions, and homogeneous pairs.\n\nA 2-join is a partition of the vertices of a graph into two subsets, with the property that the edges spanning the cut between these two subsets form two vertex-disjoint complete bipartite graphs. When a graph has a 2-join, it may be decomposed into induced subgraphs called \"blocks\", by replacing one of the two subsets of vertices by a shortest path within that subset that connects one of the two complete bipartite graphs to the other; when no such path exists, the block is formed instead by replacing one of the two subsets of vertices by two vertices, one for each complete bipartite subgraph. A 2-join is perfect if and only if its two blocks are both perfect. Therefore, if a minimally imperfect graph has a 2-join, it must equal one of its blocks, from which it follows that it must be an odd cycle and not Berge. For the same reason, a minimally imperfect graph whose complement has a 2-join cannot be Berge.\n\nA skew partition is a partition of a graph's vertices into two subsets, one of which induces a disconnected subgraph and the other of which has a disconnected complement; had conjectured that no minimal counterexample to the strong perfect graph conjecture could have a skew partition. Chudnovsky et al. introduced some technical constraints on skew partitions, and were able to show that Chvátal's conjecture is true for the resulting \"balanced skew partitions\". The full conjecture is a corollary of the strong perfect graph theorem.\n\nA homogeneous pair is related to a modular decomposition of a graph. It is a partition of the graph into three subsets \"V\", \"V\", and \"V\" such that \"V\" and \"V\" together contain at least three vertices, \"V\" contains at least two vertices, and for each vertex \"v\" in \"V\" and each \"i\" in {1,2} either \"v\" is adjacent to all vertices in \"V\" or to none of them. It is not possible for a minimally imperfect graph to have a homogeneous pair. Subsequent to the proof of the strong perfect graph conjecture, simplified it by showing that homogeneous pairs could be eliminated from the set of decompositions used in the proof.\n\nThe proof that every Berge graph falls into one of the five basic classes or has one of the four types of decomposition follows a case analysis, according to whether certain configurations exist within the graph: a \"stretcher\", a subgraph that can be decomposed into three induced paths subject to certain additional constraints, the complement of a stretcher, and a \"proper wheel\", a configuration related to a wheel graph, consisting of an induced cycle together with a hub vertex adjacent to at least three cycle vertices and obeying several additional constraints. For each possible choice of whether a stretcher or its complement or a proper wheel exists within the given Berge graph, the graph can be shown to be in one of the basic classes or to be decomposable. This case analysis completes the proof.\n\n\n"}
{"id": "56628682", "url": "https://en.wikipedia.org/wiki?curid=56628682", "title": "Subfield of an algebra", "text": "Subfield of an algebra\n\nIn algebra, a subfield of an algebra \"A\" over a field \"F\" is an \"F\"-subalgebra that is also a field. A maximal subfield is a subfield that is not contained in a strictly larger subfield of \"A\".\n\nIf \"A\" is a finite-dimensional central simple algebra, then a subfield \"E\" of \"A\" is called a strictly maximal subfield if formula_1.\n\n"}
{"id": "53920738", "url": "https://en.wikipedia.org/wiki?curid=53920738", "title": "Total algebra", "text": "Total algebra\n\nIn abstract algebra, the total algebra of a monoid is a generalization of the monoid ring that allows for infinite sums of elements of a ring. Suppose that \"S\" is a monoid with the property that, for all formula_1, there exist only finitely many ordered pairs formula_2 for which formula_3. \nLet \"R\" be a ring. Then the total algebra of \"S\" over \"R\" is the set formula_4 of all functions formula_5 with the addition law given by the (pointwise) operation:\nand with the multiplication law given by:\nThe sum on the right-hand side has finite support, and so is well-defined in \"R\".\n\nThese operations turn formula_4 into a ring. There is an embedding of \"R\" into formula_4, given by the constant functions, which turns formula_4 into an \"R\"-algebra.\n\nAn example is the ring of formal power series, where the monoid \"S\" is the natural numbers. The product is then the Cauchy product.\n"}
{"id": "37303714", "url": "https://en.wikipedia.org/wiki?curid=37303714", "title": "Tree diagram (probability theory)", "text": "Tree diagram (probability theory)\n\nIn probability theory, a tree diagram may be used to represent a probability space.\n\nTree diagrams may represent a series of independent events (such as a set of coin flips) or conditional probabilities (such as drawing cards from a deck, without replacing the cards). Each node on the diagram represents an event and is associated with the probability of that event. The root node represents the certain event and therefore has probability 1. Each set of sibling nodes represents an exclusive and exhaustive partition of the parent event.\n\nThe probability associated with a node is the chance of that event occurring after the parent event occurs. The probability that the series of events leading to a particular node will occur is equal to the product of that node and its parents' probabilities.\n\n\n\n"}
{"id": "8082431", "url": "https://en.wikipedia.org/wiki?curid=8082431", "title": "Vaṭeśvara-siddhānta", "text": "Vaṭeśvara-siddhānta\n\nVaṭeśvara-siddhānta is a mathematical and astronomical treatise by Vaṭeśvara in India in 904. This treatise contains fifteen chapters on astronomy and applied mathematics.\n\nMathematical exercises are included for students to show their comprehension of the text.\n\n"}
