{"id": "208259", "url": "https://en.wikipedia.org/wiki?curid=208259", "title": "20 (number)", "text": "20 (number)\n\n20 (twenty) is the natural number following 19 and preceding 21. A group of twenty units may also be referred to as a score.\n\n\n\n\n\n\n\n\n"}
{"id": "33010942", "url": "https://en.wikipedia.org/wiki?curid=33010942", "title": "Actuarial polynomials", "text": "Actuarial polynomials\n\nIn mathematics, the actuarial polynomials \"a\"(\"x\") are polynomials studied by given by the generating function\n\n\n"}
{"id": "685665", "url": "https://en.wikipedia.org/wiki?curid=685665", "title": "Alexander horned sphere", "text": "Alexander horned sphere\n\nThe Alexander horned sphere is a pathological object in topology discovered by .\n\nThe Alexander horned sphere is the particular embedding of a sphere in 3-dimensional Euclidean space obtained by the following construction, starting with a standard torus:\nBy considering only the points of the tori that are not removed at some stage, an embedding results of the sphere with a Cantor set removed. This embedding extends to the whole sphere, since points approaching two different points of the Cantor set will be at least a fixed distance apart in the construction.\n\nThe horned sphere, together with its inside, is a topological 3-ball, the Alexander horned ball, and so is simply connected; i.e., every loop can be shrunk to a point while staying inside. The exterior is \"not\" simply connected, unlike the exterior of the usual round sphere; a loop linking a torus in the above construction cannot be shrunk to a point without touching the horned sphere. This shows that the Jordan–Schönflies theorem does not hold in three dimensions, as Alexander had originally thought. Alexander also proved that the theorem \"does\" hold in three dimensions for piecewise linear/smooth embeddings. This is one of the earliest examples where the need for distinction between the topological category of manifolds, and the categories of differentiable manifolds, and piecewise linear manifolds was noticed.\n\nNow consider Alexander's horned sphere as an embedding into the 3-sphere, considered as the one-point compactification of the 3-dimensional Euclidean space R. The closure of the non-simply connected domain is called the solid Alexander horned sphere. Although the solid horned sphere is not a manifold, R. H. Bing showed that its double (which is the 3-manifold obtained by gluing two copies of the horned sphere together along the corresponding points of their boundaries) is in fact the 3-sphere. One can consider other gluings of the solid horned sphere to a copy of itself, arising from different homeomorphisms of the boundary sphere to itself. This has also been shown to be the 3-sphere. The solid Alexander horned sphere is an example of a crumpled cube; i.e., a closed complementary domain of the embedding of a 2-sphere into the 3-sphere.\n\nOne can generalize Alexander's construction to generate other horned spheres by increasing the number of horns at each stage of Alexander's construction or considering the analogous construction in higher dimensions.\n\nOther substantially different constructions exist for constructing such \"wild\" spheres. Another example, also found by Alexander, is Antoine's horned sphere, which is based on Antoine's necklace, a pathological embedding of the Cantor set into the 3-sphere.\n\n\n\n"}
{"id": "826723", "url": "https://en.wikipedia.org/wiki?curid=826723", "title": "Angular diameter", "text": "Angular diameter\n\nThe angular diameter, angular size, apparent diameter, or apparent size is an angular measurement describing how large a sphere or circle appears from a given point of view. In the vision sciences, it is called the visual angle, and in optics, it is the angular aperture (of a lens). The angular diameter can alternatively be thought of as the angle through which an eye or camera must rotate to look from one side of an apparent circle to the opposite side. Angular radius equals half the angular diameter.\n\nThe angular diameter of a circle whose plane is perpendicular to the displacement vector between the point of view and the centre of said circle can be calculated using the formula\nin which formula_2 is the angular diameter, and formula_3 and formula_4 are the actual diameter of and the distance to the object. When formula_5, we have formula_6, and the result obtained is in radians.\n\nFor a spherical object whose \"actual\" diameter equals formula_7 and where formula_4 is the distance to the \"centre\" of the sphere, the angular diameter can be found by the formula\n\nThe difference is due to the fact that the apparent edges of a sphere are its tangent points, which are closer to the observer than the centre of the sphere. For practical use, the distinction is only significant for spherical objects that are relatively close, since the small-angle approximation holds for formula_10:\n\nEstimates of angular diameter may be obtained by holding the hand at right angles to a fully extended arm, as shown in the figure.\n\nIn astronomy, the sizes of celestial objects are often given in terms of their angular diameter as seen from Earth, rather than their actual sizes. Since these angular diameters are typically small, it is common to present them in arcseconds (″). An arcsecond is 1/3600th of one degree (1°), and a radian is 180/formula_12 degrees, so one radian equals 3,600*180/formula_12 arcseconds, which is about 206,265 arcseconds. Therefore, the angular diameter of an object with physical diameter \"d\" at a distance \"D\", expressed in arcseconds, is given by:\n\nThese objects have an angular diameter of 1″:\n\nThus, the angular diameter of Earth's orbit around the Sun as viewed from a distance of 1 pc is 2″, as 1 AU is the mean radius of Earth's orbit.\n\nThe angular diameter of the Sun, from a distance of one light-year, is 0.03″, and that of Earth 0.0003″. The angular diameter 0.03″ of the Sun given above is approximately the same as that of a person at a distance of the diameter of Earth.\n\nThis table shows the angular sizes of noteworthy celestial bodies as seen from Earth:\n\nThe table shows that the angular diameter of Sun, when seen from Earth is approximately 32′ (1920″ or 0.53°), as illustrated above.\n\nThus the angular diameter of the Sun is about 250,000 times that of Sirius. (Sirius has twice the diameter and its distance is 500,000 times as much; the Sun is 10 times as bright, corresponding to an angular diameter ratio of 10, so Sirius is roughly 6 times as bright per unit solid angle.)\n\nThe angular diameter of the Sun is also about 250,000 times that of Alpha Centauri A (it has about the same diameter and the distance is 250,000 times as much; the Sun is 4×10 times as bright, corresponding to an angular diameter ratio of 200,000, so Alpha Centauri A is a little brighter per unit solid angle).\n\nThe angular diameter of the Sun is about the same as that of the Moon. (The Sun's diameter is 400 times as large and its distance also; the Sun is 200,000 to 500,000 times as bright as the full Moon (figures vary), corresponding to an angular diameter ratio of 450 to 700, so a celestial body with a diameter of 2.5–4″ and the same brightness per unit solid angle would have the same brightness as the full Moon.)\n\nEven though Pluto is physically larger than Ceres, when viewed from Earth (e.g., through the Hubble Space Telescope) Ceres has a much larger apparent size.\n\nAngular sizes measured in degrees are useful for larger patches of sky. (For example, the three stars of the Belt cover about 4.5° of angular size.) However, much finer units are needed to measure the angular sizes of galaxies, nebulae, or other objects of the night sky.\n\nDegrees, therefore, are subdivided as follows:\n\nTo put this in perspective, the full Moon as viewed from Earth is about \n\n"}
{"id": "366719", "url": "https://en.wikipedia.org/wiki?curid=366719", "title": "Applied probability", "text": "Applied probability\n\nApplied probability is the application of probability theory to statistical problems and other scientific and engineering domains.\n\nMuch research involving probability is done under the auspices of applied probability. However, while such research is motivated (to some degree) by applied problems, it is usually the mathematical aspects of the problems that are of most interest to researchers (as is typical of applied mathematics in general).\n\nApplied probabilists are particularly concerned with the application of stochastic processes, and probability more generally, to the natural, applied and social sciences, including biology, physics (including astronomy), chemistry, medicine, computer science and information technology, and economics.\n\nAnother area of interest is in engineering: particularly in areas of uncertainty, risk management, probabilistic design, and Quality assurance.\n\n\n\n"}
{"id": "44969442", "url": "https://en.wikipedia.org/wiki?curid=44969442", "title": "Archimedean graph", "text": "Archimedean graph\n\nIn the mathematical field of graph theory, an Archimedean graph is a graph that forms the skeleton of one of the Archimedean solids. There are 13 Archimedean graphs, and all of them are regular, polyhedral (and therefore by necessity also 3-vertex-connected planar graphs), and also Hamiltonian graphs.\n\nAlong with the 13, the set of infinite prism graphs and antiprism graphs can also be considered Archimedean graphs.\n\n\n"}
{"id": "36255209", "url": "https://en.wikipedia.org/wiki?curid=36255209", "title": "Bauerian extension", "text": "Bauerian extension\n\nIn mathematics, in the field of algebraic number theory, a Bauerian extension is a field extension of an algebraic number field which is characterized by the prime ideals with inertial degree one in the extension.\n\nFor a finite degree extension \"L\"/\"K\" of an algebraic number field \"K\" we define \"P\"(\"L\"/\"K\") to be the set of primes p of \"K\" which have a factor P with inertial degree one (that is, the residue field of P has the same order as the residue field of p).\n\nBauer's theorem states that if \"M\"/\"K\" is a finite degree Galois extension, then \"P\"(\"M\"/\"K\") ⊇ \"P\"(\"L\"/\"K\") if and only if \"M\" ⊆ \"L\". In particular, finite degree Galois extensions \"N\" of \"K\" are characterised by set of prime ideals which split completely in \"N\".\n\nAn extension \"F\"/\"K\" is \"Bauerian\" if it obeys Bauer's theorem: that is, for every finite extension \"L\" of \"K\", we have \"P\"(\"F\"/\"K\") ⊇ \"P\"(\"L\"/\"K\") if and only if \"L\" contains a subfield \"K\"-isomorphic to \"F\".\n\nAll field extensions of degree at most 4 over Q are Bauerian.\nAn example of a non-Bauerian extension is the Galois extension of Q by the roots of 2\"x\" − 32\"x\" + 1, which has Galois group \"S\".\n\n\n"}
{"id": "37434162", "url": "https://en.wikipedia.org/wiki?curid=37434162", "title": "Born reciprocity", "text": "Born reciprocity\n\nIn physics, Born reciprocity, also called reciprocal relativity or Born–Green reciprocity, is a principle set up by theoretical physicist Max Born that calls for a duality-symmetry among space and momentum. Born and his co-workers expanded his principle to a framework that is also known as reciprocity theory.\n\nBorn noticed a symmetry among configuration space and momentum space representations of a free particle, in that its wave function description is invariant to a change of variables \"x\" → \"p\" and \"p\" → −\"x\". (It can also be worded such as to include scale factors, e.g. invariance to \"x\" → \"ap\" and \"p\" → −\"bx\" where \"a\", \"b\" are constants.) Born hypothesized that such symmetry should apply to the four-vectors of special relativity, that is, to the four-vector space coordinates\n\nand the four-vector momentum (four-momentum) coordinates\nBoth in classical and in quantum mechanics, the Born reciprocity conjecture postulates that the transformation \"x\" → \"p\" and \"p\" → −\"x\" leaves invariant the Hamilton equations:\n\nFrom his reciprocity approach, Max Born conjectured the invariance of a space-time-momentum-energy line element. Born and H.S. Green similarly introduced the notion an invariant (quantum) metric operator formula_5 as extension of the Minkowski metric of special relativity to an invariant metric on phase space coordinates. The metric is invariant under the group of quaplectic transformations.\n\nSuch a reciprocity as called for by Born can be observed in much, but not all, of the formalism of classical and quantum physics. Born's reciprocity theory was not developed much further for reason of difficulties in the mathematical foundations of the theory.\n\nHowever Born's idea of a quantum metric operator was later taken up by Hideki Yukawa when developing his nonlocal quantum theory in the 1950s. In 1981, Eduardo R. Caianiello proposed a \"maximal acceleration\", similarly as there is a minimal length at Planck scale, and this concept of maximal acceleration has been expanded upon by others. It has also been suggested that Born reciprocity may be the underlying physical reason for the T-duality symmetry in string theory, and that Born reciprocity may be of relevance to developing a quantum geometry.\n\nBorn chose the term \"reciprocity\" for the reason that in a crystal lattice, the motion of a particle can be described in \"p\"-space by means of the reciprocal lattice.\n\n"}
{"id": "27280274", "url": "https://en.wikipedia.org/wiki?curid=27280274", "title": "Boundary problem in geography", "text": "Boundary problem in geography\n\nNot to be mistaken with the demarcation problem or boundary problem in the philosophy of science, boundary problem in spatial analysis is a geographical phenomenon and one of the four major issues that interfere with an accurate estimation of the statistical parameter. The four issues include the boundary problem as well as the scale problem, pattern problem (or spatial autocorrelation), and modifiable areal unit problem (Barber 1988).\n\nThe boundary problem occurs because of the loss of neighbors in analyses that depend on the values of the neighbors. While geographic phenomena are measured and analyzed within a specific unit, identical spatial data can appear either dispersed or clustered depending on the boundary placed around the data. In analysis with point data, dispersion is evaluated as dependent of the boundary. In analysis with areal data, statistics should be interpreted based upon the boundary.\n\nIn geographical research, two types of areas are taken into consideration in relation to the boundary: an area surrounded by fixed natural boundaries (e.g., coastlines or streams), outside of which neighbors do not exist (Henley 1981), or an area included in a larger region defined by arbitrary artificial boundaries (e.g., an air pollution boundary in modeling studies or an urban boundary in population migration) (Haining 1990). In an area isolated by the natural boundaries, the spatial process discontinues at the boundaries. In contrast, if a study area is delineated by the artificial boundaries, the process continues beyond the area.\n\nIf a spatial process in an area occurs beyond a study area or has an interaction with neighbors outside artificial boundaries, the most common approach is to neglect the influence of the boundaries and assume that the process occurs at the internal area. However, such an approach leads to a significant model misspecification problem (Upton and Fingleton 1985).\n\nThat is, for measurement or administrative purposes, geographic boundaries are drawn, but the boundaries can bring about different spatial patterns in geographic phenomena (BESR 2002). It has been reported that the difference in the way of drawing the boundary significantly affects identification of the spatial distribution and estimation of the statistical parameters of the spatial process (Cressie 1992; Fotheringham and Rogerson 1993; Griffith 1983; Martin 1987). The difference is largely based on the fact that spatial processes are generally unbounded or fuzzy-bounded (Leung 1987) but the processes are expressed in data imposed within boundaries for analysis purposes (Miller 1999). Although the boundary problem was discussed in relation to artificial and arbitrary boundaries, the effect of the boundaries also occurs according to natural boundaries as long as it is ignored that properties at sites on the natural boundary such as streams are likely to differ from those at sites within the boundary (Martin 1989).\n\nThe boundary problem occurs with regard not only to horizontal boundaries but also to vertically drawn boundaries according to delineations of heights or depths (Pineda 1993). For example, biodiversity such as the density of species of plants and animals is high near the surface, so if the identically divided height or depth is used as a spatial unit, it is more likely to find fewer number of the plant and animal species as the height or depth increases.\n\nFigure 1. Boundary problem: urban sprawl in central Florida (an evaluation by land cover analysis with raster datasets vs. an evaluation by population density bounded in the census tract)\n\nFigure 2. Boundary problem: horizontal boundaries\n\nFigure 3. Boundary problem: vertical boundaries\n\nBy drawing a boundary around a study area, two types of problems in measurement and analysis takes place (Fotheringham and Rogerson 1993). The first is an edge effect. This effect originates from the ignorance of interdependences that occur outside the bounded region. Griffith (1980; 1983) and Griffith and Amrhein (1983) highlighted problems according to the edge effect. A typical example is a cross-boundary influence such as cross-border jobs, services and other resources located in a neighboring municipality (Macquire 1995).\n\nThe second is a shape effect that results from the artificial shape delineated by the boundary. As an illustration of the effect of the artificial shape, point pattern analysis tends to provide higher levels of clustering for the identical point pattern within a unit that is more elongated (Fotheringham and Rogerson 1993). Similarly, the shape can influence interaction and flow among spatial entities (Arlinghaus and Nystuen 1990; Ferguson and Kanaroglou 1998; Griffith 1982). For example, the shape can affect the measurement of origin-destination flows since these are often recorded when they cross an artificial boundary. Because of the effect set by the boundary, the shape and area information is used to estimate travel distances from surveys (Rogerson 1990) or to locate traffic counters, travel survey stations, or traffic monitoring systems (Kirby 1997). From the same perspective, Theobald (2001; retrieved from BESR 2002) argued that measures of urban sprawl should consider interdependences and interactions with nearby rural areas.\n\nIn spatial analysis, the boundary problem has been discussed along with the modifiable areal unit problem (MAUP) inasmuch as MAUP is associated with the arbitrary geographic unit and the unit is defined by the boundary (Rogerson 2006). For administrative purposes, data for policy indicators are usually aggregated within larger units (or enumeration units) such as census tracts, school districts, municipalities and counties. The artificial units serve the purposes of taxation and service provision. For example, municipalities can effectively respond to the need of the public in their jurisdictions. However, in such spatially aggregated units, spatial variations of detailed social variables cannot be identified. The problem is noted when the average degree of a variable and its unequal distribution over space are measured (BESR 2002).\n\nSeveral strategies for resolving geographic boundary problems in measurement and analysis have been proposed (Martin 1987; Wong and Fotheringham 1990). To identify the effectiveness of the strategies, Griffith (1983) reviewed traditional techniques that were developed to mitigate the edge effects: ignoring the effects, undertaking a torus mapping, construction of an empirical butter zone, construction of an artificial butter zone, extrapolation into a buffer zone, utilizing a correction factor, etc. The first method (i.e., the ignorance of the edge effects), assumes and infinite surface in which the edge effects do not occur. In fact, this approach has been used by traditional geographical theories (e.g., central place theory). Its main shortcoming is that empirical phenomena occur within a finite area, so an infinite and homogeneous surface is unrealistic (Griffith and Amrhein 1983). The remaining five approaches are similar in that they attempted to produce unbiased parameter estimation, that is, to provide a medium by which the edge effects are removed (Griffith 1983). (He called these operational solutions as opposed to statistical solutions to be discussed below.) Specifically, the techniques aim at a collection of data beyond the boundary of the study area and fit a larger model, that is, mapping over the area or over-bounding the study area (Ripley 1979; Wong and Fotheringham 1990). Through simulation analysis, however, Griffith and Amrhein (1983) identified the inadequacy of such an overbounding technique. Moreover, this technique can bring about issues related to large-area statistics, that is, ecological fallacy. By expanding the boundary of the study area, micro-scale variations within the boundary can be ignored.\n\nFigure 4. A solution to the boundary problem: overbounding\n\nAs alternatives to operational solutions, Griffith (1983) examined three correction techniques (i.e., statistical solutions) in removing boundary-induced bias from inference. They are (1) based on generalized least squares theory, (2) using dummy variables and a regression structure (as a way of creating a buffer zone), and (3) regarding the boundary problem as a missing values problem. However, these techniques require rather strict assumptions about the process of interest (Yoo and Kyriakidis 2008). For example, the solution according to the generalized least squares theory utilizes time-series modeling that needs an arbitrary transformation matrix to fit the multidirectional dependencies and multiple boundary units found in geographical data (Griffith 1980). Martin (1987) also argued that some of the underlying assumptions of the statistical techniques are unrealistic or unreasonably strict. Moreover, Griffith (1985) himself also identified the inferiority of the techniques through simulation analysis.\n\nAs particularly applicable using GIS technologies (Haslett et al. 1990; Openshaw et al. 1987), a possible solution for addressing both edge and shape effects is to an re-estimation of the spatial or process under repeated random realizations of the boundary. This solution provides an experimental distribution that can be subjected to statistical tests (Fotheringham and Rogerson 1993). As such, this strategy examines the sensitivity in the estimation result according to changes in the boundary assumptions. With GIS tools, boundaries can be systematically manipulated. The tools then conduct the measurement and analysis of the spatial process in such differentiated boundaries. Accordingly, such a sensitivity analysis allows the evaluation of the reliability and robustness of place-based measures that defined within artificial boundaries (BESR 2002). In the meantime, the changes in the boundary assumptions refer not only to altering or tilting the angles of the boundary, but also differentiating between the boundary and interior areas in examination and considering a possibility that isolated data collection points close to the boundary may show large variances.\n\nFigure 5. A solution to the boundary problem: sensitivity analysis\n\n\n"}
{"id": "6339382", "url": "https://en.wikipedia.org/wiki?curid=6339382", "title": "CJCSG", "text": "CJCSG\n\nIn cryptography, CJCSG is a stream cypher algorithm developed by Cees Jansen and Alexander Kolosha. It has been submitted to the eSTREAM Project of the eCRYPT network. It has been classified as an archival algorithm and will not be further considered. \n"}
{"id": "19284033", "url": "https://en.wikipedia.org/wiki?curid=19284033", "title": "CRM-Fields-PIMS prize", "text": "CRM-Fields-PIMS prize\n\nThe CRM-Fields-PIMS Prize is the premier Canadian research prize in the mathematical sciences. It is awarded in recognition of exceptional research achievement in the mathematical sciences and is given annually by three Canadian mathematics institutes: the Centre de Recherches Mathématiques (CRM), the Fields Institute, and the Pacific Institute for the Mathematical Sciences (PIMS).\n\nThe prize was established in 1994 by the CRM and the Fields Institute as the CRM-Fields Prize. The prize took its current name when PIMS became a partner in 2005.\n\nThe prize carries a monetary award of $10,000, funded jointly by the three institutes. The inaugural prize winner was H.S.M. Coxeter. \n\nSource: Centre de recherches mathématiques \n\n\n"}
{"id": "37342010", "url": "https://en.wikipedia.org/wiki?curid=37342010", "title": "Canon arithmeticus", "text": "Canon arithmeticus\n\nIn mathematics, Canon arithmeticus is a table of indices and powers with respect to primitive roots for prime powers less than 1000, originally published by . The tables were at one time used for arithmetical calculations modulo prime powers, though like many mathematical tables they have now been replaced by digital computers. Jacobi also reproduced Burkhardt's table of the periods of decimal fractions of 1/\"p\", and Ostrogradsky's tables of primitive roots of primes less than 200, and gave tables of indices of some odd numbers modulo powers of 2 with respect to the base 3 .\n\nAlthough the second edition of 1956 has Jacobi's name on the title, it has little in common with the first edition apart from the topic: the tables were completely recalculated, usually with a different choice of primitive root, by Wilhelm Patz. Jacobi's original tables use 10 or –10 or a number with a small power of this form as the primitive root whenever possible, while the second edition uses the smallest possible positive primitive root .\n\nThe term \"canon arithmeticus\" is occasionally used to mean any table of indices and powers of primitive roots.\n\n"}
{"id": "7821224", "url": "https://en.wikipedia.org/wiki?curid=7821224", "title": "Consequentia mirabilis", "text": "Consequentia mirabilis\n\nConsequentia mirabilis (Latin for \"admirable consequence\"), also known as Clavius's Law, is used in traditional and classical logic to establish the truth of a proposition from the inconsistency of its negation. It is thus similar to \"reductio ad absurdum\", but it can prove a proposition true using just its negation. It states that if a proposition is a consequence of its negation, then it is true, for consistency. It can thus be demonstrated without using any other principle, but that of consistency. (Barnes claims in passing that the term 'consequentia mirabilis' refers only to the inference of the proposition from the inconsistency of its negation, and that the term 'Lex Clavia' (or Clavius' Law) refers to the inference of the proposition's negation from the inconsistency of the proposition.) \n\nIn formal notation:\nformula_1\nwhich is equivalent to formula_2.\n\n\"Consequentia mirabilis\" was a pattern of argument popular in 17th century Europe that first appeared in a fragment of Aristotle's \"Protrepticus:\" \"If we ought to philosophise, then we ought to philosophise; and if we ought not to philosophise, then we ought to philosophise (i.e. in order to justify this view); in any case, therefore, we ought to philosophise.\"\n\nThe most famous example is perhaps the Cartesian \"cogito ergo sum\": Even if one can question the validity of the thinking, no one can deny that they are thinking.\n\n"}
{"id": "11589424", "url": "https://en.wikipedia.org/wiki?curid=11589424", "title": "Decimal floating point", "text": "Decimal floating point\n\nDecimal floating-point (DFP) arithmetic refers to both a representation and operations on decimal floating-point numbers. Working directly with decimal (base-10) fractions can avoid the rounding errors that otherwise typically occur when converting between decimal fractions (common in human-entered data, such as measurements or financial information) and binary (base-2) fractions.\n\nThe advantage of decimal floating-point representation over decimal fixed-point and integer representation is that it supports a much wider range of values. For example, while a fixed-point representation that allocates 8 decimal digits and 2 decimal places can represent the numbers 123456.78, 8765.43, 123.00, and so on, a floating-point representation with 8 decimal digits could also represent 1.2345678, 1234567.8, 0.000012345678, 12345678000000000, and so on. This wider range can dramatically slow the accumulation of rounding errors during successive calculations; for example, the Kahan summation algorithm can be used in floating point to add many numbers with no asymptotic accumulation of rounding error.\n\nEarly mechanical uses of decimal floating point are evident in the abacus, slide rule, the Smallwood calculator, and some other calculators that support entries in scientific notation. In the case of the mechanical calculators, the exponent is often treated as side information that is accounted for separately.\n\nThe otherwise binary Wang VS machine supported a 64-bit decimal floating-point format. The floating-point support library for the Motorola 68040 processor provided a 96-bit decimal floating-point storage format.\n\nSome computer languages have implementations of decimal floating-point arithmetic, including PL/I, C#, Java with big decimal, emacs with calc, and Python's decimal module.\nIn 1987, the IEEE released IEEE 854, a standard for computing with decimal floating point, which lacked a specification for how floating-point data should be encoded for interchange with other systems. This was subsequently addressed in IEEE 754-2008, which standardized the encoding of decimal floating-point data, albeit with two different alternative methods.\n\nIBM POWER6 and newer POWER processors include DFP in hardware, as does the IBM System z9 (and z10). SilMinds offers SilAx, a configurable vector DFP coprocessor. IEEE 754-2008 defines this in more detail. Fujitsu also has 64-bit Sparc processors with DFP in hardware.\n\nMicrosoft C#, or .NET, uses System.Decimal.\n\nThe IEEE 754-2008 standard defines 32-, 64- and 128-bit decimal floating-point representations. Like the binary floating-point formats, the number is divided into a sign, an exponent, and a significand. Unlike binary floating-point, numbers are not necessarily normalized; values with few significant digits have multiple possible representations: 1×10=0.1×10=0.01×10, etc. When the significand is zero, the exponent can be any value at all.\n\nThe exponent ranges were chosen so that the range available to normalized values is approximately symmetrical. Since this cannot be done exactly with an even number of possible exponent values, the extra value was given to Emax.\n\nTwo different representations are defined:\n\nBoth alternatives provide exactly the same range of representable values.\n\nThe most significant two bits of the exponent are limited to the range of 0−2, and the most significant 4 bits of the significand are limited to the range of 0−9. The 30 possible combinations are encoded in a 5-bit field, along with special forms for infinity and NaN.\n\nIf the most significant 4 bits of the significand are between 0 and 7, the encoded value begins as follows:\n\nIf the leading 4 bits of the significand are binary 1000 or 1001 (decimal 8 or 9), the number begins as follows:\n\nThe leading bit (s in the above) is a sign bit, and the following bits (xxx in the above) encode the additional exponent bits and the remainder of the most significant digit, but the details vary depending on the encoding alternative used.\n\nThe final combinations are used for infinities and NaNs, and are the same for both alternative encodings:\n\nIn the latter cases, all other bits of the encoding are ignored. Thus, it is possible to initialize an array to NaNs by filling it with a single byte value.\n\nThis format uses a binary significand from 0 to 10−1. For example, the Decimal32 significand can be up to 10−1 = = 98967F = . While the encoding can represent larger significands, they are illegal and the standard requires implementations to treat them as 0, if encountered on input.\n\nAs described above, the encoding varies depending on whether the most significant 4 bits of the significand are in the range 0 to 7 (0000 to 0111), or higher (1000 or 1001).\n\nIf the 2 bits after the sign bit are \"00\", \"01\", or \"10\", then the exponent field consists of the 8 bits following the sign bit (the 2 bits mentioned plus 6 bits of \"exponent continuation field\"), and the significand is the remaining 23 bits, with an implicit leading 0 bit, shown here in parentheses:\n\nThis includes subnormal numbers where the leading significand digit is 0.\n\nIf the 2 bits after the sign bit are \"11\", then the 8-bit exponent field is shifted 2 bits to the right (after both the sign bit and the \"11\" bits thereafter), and the represented significand is in the remaining 21 bits. In this case there is an implicit (that is, not stored) leading 3-bit sequence \"100\" in the true significand:\n\nThe \"11\" 2-bit sequence after the sign bit indicates that there is an \"implicit\" \"100\" 3-bit prefix to the significand.\n\nNote that the leading bits of the significand field do \"not\" encode the most significant decimal digit; they are simply part of a larger pure-binary number. For example, a significand of is encoded as binary , with the leading 4 bits encoding 7; the first significand which requires a 24th bit (and thus the second encoding form) is 2 = .\n\nIn the above cases, the value represented is:\n\nDecimal64 and Decimal128 operate analogously, but with larger exponent continuation and significand fields. For Decimal128, the second encoding form is actually never used; the largest valid significand of 10−1 = 1ED09BEAD87C0378D8E63FFFFFFFF can be represented in 113 bits.\n\nIn this version, the significand is stored as a series of decimal digits. The leading digit is between 0 and 9 (3 or 4 binary bits), and the rest of the significand uses the densely packed decimal (DPD) encoding.\n\nUnlike the binary integer significand version, where the exponent changed position and came before the significand, this encoding combines the leading 2 bits of the exponent and the leading digit (3 or 4 bits) of the significand into the five bits that follow the sign bit. This is followed by a fixed-offset exponent continuation field.\n\nFinally, the significand continuation field made of 2, 5, or 11 10-bit \"declets\", each encoding 3 decimal digits.\n\nIf the first two bits after the sign bit are \"00\", \"01\", or \"10\", then those are the leading bits of the exponent, and the three bits after that are interpreted as the leading decimal digit (0 to 7):\n\nIf the first two bits after the sign bit are \"11\", then the second two bits are the leading bits of the exponent, and the last bit is prefixed with \"100\" to form the leading decimal digit (8 or 9):\n\nThe remaining two combinations (11110 and 11111) of the 5-bit field are used to represent ±infinity and NaNs, respectively.\n\nThe usual rule for performing floating-point arithmetic is that the exact mathematical value is calculated, and the result is then rounded to the nearest representable value in the specified precision. This is in fact the behavior mandated for IEEE-compliant computer hardware, under normal rounding behavior and in the absence of exceptional conditions.\n\nFor ease of presentation and understanding, 7-digit precision will be used in the examples. The fundamental principles are the same in any precision.\n\nA simple method to add floating-point numbers is to first represent them with the same exponent. In the example below, the second number is shifted right by 3 digits. We proceed with the usual addition method:\n\nThe following example is decimal, which simply means the base is 10.\n\nThis is nothing other than converting to scientific notation.\nIn detail:\n\nThis is the true result, the exact sum of the operands. It will be rounded to 7 digits and then normalized if necessary. The final result is:\n\nNote that the low 3 digits of the second operand (654) are essentially lost. This is round-off error. In extreme cases, the sum of two non-zero numbers may be equal to one of them:\n\nAnother problem of loss of significance occurs when two close numbers are subtracted.\ne=5; s=1.234571 and e=5; s=1.234567 are representations of the rationals 123457.1467 and 123456.659.\n\nThe best representation of this difference is e=−1; s=4.877000, which differs more than 20% from e=−1; s=4.000000. In extreme cases, the final result may be zero even though an exact calculation may be several million. This \"cancellation\" illustrates the danger in assuming that all of the digits of a computed result are meaningful.\n\nDealing with the consequences of these errors are topics in numerical analysis.\n\nTo multiply, the significands are multiplied, while the exponents are added, and the result is rounded and normalized.\n\nDivision is done similarly, but that is more complicated.\n\nThere are no cancellation or absorption problems with multiplication or division, though small errors may accumulate as operations are performed repeatedly. In practice, the way these operations are carried out in digital logic can be quite complex.\n\n\n\n"}
{"id": "32010112", "url": "https://en.wikipedia.org/wiki?curid=32010112", "title": "Enterprise Dynamics", "text": "Enterprise Dynamics\n\nEnterprise Dynamics is a discrete event simulation software platform developed by INCONTROL Simulation Solutions. It is used to design and implement simulation solutions. The Enterprise Dynamics (ED) platform has several market-specific libraries to conform to customer requirements. These libraries are:\n\nEnterprise Dynamics is based on the Taylor II simulation software, which was developed by F&H Simulations. In 1998, the newly developed simulation platform Taylor Enterprise Dynamics was introduced and in 2000, F&H Simulations was acquired by the consulting company and simulation software distributor Incontrol Business Engineers. The new company name became INCONTROL Simulation Solutions and Taylor ED was renamed to Enterprise Dynamics. At this time F&H Simulations, now called Flexsim Software Products, became independent and developed a new simulation software called Flexsim.\n\nEnterprise Dynamics has been applied or is currently being applied in the following fields:\n\nEnterprise Dynamics is an object-oriented simulation platform combined with an event-oriented approach. The user can select standard simulation objects (the so-called 'Atoms'), in which the behavior of their real life equivalents is captured, from a library and create a model by clicking and dragging the objects into the model space. For each simulation object, parameters can be altered to change its behavior.\n\nSeveral add-ons are available that will extent the functionality of ED. These add-ons are:\n\n"}
{"id": "35112066", "url": "https://en.wikipedia.org/wiki?curid=35112066", "title": "Equivariant index theorem", "text": "Equivariant index theorem\n\nIn differential geometry, the equivariant index theorem, of which there are several variants, computes the (graded) trace of an element of a compact Lie group acting in given setting in terms of the integral over the fixed points of the element. If the element is neutral, then the theorem reduces to the usual index theorem.\n\nThe classical formula such as the Atiyah–Bott formula is a special case of the theorem.\n\nLet formula_1 be a clifford module bundle. Assume a compact Lie group \"G\" acts on both \"E\" and \"M\" so that formula_2 is equivariant. Let \"E\" be given a connection that is compatible with the action of \"G\". Finally, let \"D\" be a Dirac operator on \"E\" associated to the given data. In particular, \"D\" commutes with \"G\" and thus the kernel of \"D\" is a finite-dimensional representation of \"G\".\n\nThe equivariant index of \"E\" is a virtual character given by taking the supertrace:\n\n"}
{"id": "6920898", "url": "https://en.wikipedia.org/wiki?curid=6920898", "title": "Ernst G. Straus", "text": "Ernst G. Straus\n\nErnst Gabor Straus (February 25, 1922 – July 12, 1983) was a German-American mathematician of Jewish origin who helped found the theories of Euclidean Ramsey theory and of the arithmetic properties of analytic functions. His extensive list of co-authors includes Albert Einstein, Paul Erdős, Richard Bellman, Béla Bollobás, Sarvadaman Chowla, Ronald Graham, Mathukumalli V Subbarao, László Lovász, Carl Pomerance, and George Szekeres.\n\nStraus was born in Munich, February 25, 1922, the youngest of five children (Isa, Hana, Peter, Gabriella) of a prominent Zionist attorney, Elias (Eli) Straus, and his wife , a medical doctor and feminist. Ernst Gabor Straus came to be known as a mathematical prodigy from a very young age. Following the death of his father, the family fled the Nazi regime for Palestine in 1933, and Straus was educated at the Hebrew University in Jerusalem. Although he never received an undergraduate degree, Straus began graduate studies at Columbia University in New York, earning a PhD in 1948 under F. J. Murray. Two years later, he became the assistant of Albert Einstein. After a three-year stint at the Institute for Advanced Study, Straus took a position at the University of California, Los Angeles, which he kept for the rest of his life. Straus died July 12, 1983 of heart failure.\n\nStraus's interests ranged widely over his career, beginning with his early work on relativity with Einstein and continuing with deep work in analytic number theory, extremal graph theory, and combinatorics. One of his best known contributions in popular mathematics is the Erdős–Straus conjecture that every number of the form 4/\"n\" has a three-term Egyptian fraction.\n\n"}
{"id": "595708", "url": "https://en.wikipedia.org/wiki?curid=595708", "title": "Functional equation (L-function)", "text": "Functional equation (L-function)\n\nIn mathematics, the L-functions of number theory are expected to have several characteristic properties, one of which is that they satisfy certain functional equations. There is an elaborate theory of what these equations should be, much of which is still conjectural.\n\nA prototypical example, the Riemann zeta function has a functional equation relating its value at the complex number \"s\" with its value at 1 − \"s\". In every case this relates to some value ζ(\"s\") that is only defined by analytic continuation from the infinite series definition. That is, writingas is conventionalσ for the real part of \"s\", the functional equation relates the cases\n\nand also changes a case with\n\nin the \"critical strip\" to another such case, reflected in the line σ = ½. Therefore, use of the functional equation is basic, in order to study the zeta-function in the whole complex plane.\n\nThe functional equation in question for the Riemann zeta function takes the simple form\n\nwhere \"Z\"(\"s\") is ζ(\"s\") multiplied by a \"gamma-factor\", involving the gamma function. This is now read as an 'extra' factor in the Euler product for the zeta-function, corresponding to the infinite prime. Just the same shape of functional equation holds for the Dedekind zeta function of a number field \"K\", with an appropriate gamma-factor that depends only on the embeddings of \"K\" (in algebraic terms, on the tensor product of \"K\" with the real field).\n\nThere is a similar equation for the Dirichlet L-functions, but this time relating them in pairs:\n\nwith χ a primitive Dirichlet character, χ its complex conjugate, Λ the L-function multiplied by a gamma-factor, and ε a complex number of absolute value 1, of shape\n\nwhere \"G\"(χ) is a Gauss sum formed from χ. This equation has the same function on both sides if and only if χ is a \"real character\", taking values in {0,1,−1}. Then ε must be 1 or −1, and the case of the value −1 would imply a zero of \"Λ\"(\"s\") at \"s\" = ½. According to the theory (of Gauss, in effect) of Gauss sums, the value is always 1, so no such \"simple\" zero can exist (the function is \"even\" about the point).\n\nA unified theory of such functional equations was given by Erich Hecke, and the theory was taken up again in \"Tate's thesis\" by John Tate. Hecke found generalised characters of number fields, now called Hecke characters, for which his proof (based on theta functions) also worked. These characters and their associated L-functions are now understood to be strictly related to complex multiplication, as the Dirichlet characters are to cyclotomic fields.\n\nThere are also functional equations for the local zeta-functions, arising at a fundamental level for the (analogue of) Poincaré duality in étale cohomology. The Euler products of the Hasse–Weil zeta-function for an algebraic variety \"V\" over a number field \"K\", formed by reducing \"modulo\" prime ideals to get local zeta-functions, are conjectured to have a \"global\" functional equation; but this is currently considered out of reach except in special cases. The definition can be read directly out of étale cohomology theory, again; but in general some assumption coming from automorphic representation theory seems required to get the functional equation. The Taniyama–Shimura conjecture was a particular case of this as general theory. By relating the gamma-factor aspect to Hodge theory, and detailed studies of the expected ε factor, the theory as empirical has been brought to quite a refined state, even if proofs are missing.\n\n"}
{"id": "33670194", "url": "https://en.wikipedia.org/wiki?curid=33670194", "title": "Gauss's diary", "text": "Gauss's diary\n\nGauss's diary was a record of the mathematical discoveries of C. F. Gauss from 1796 to 1814. It was rediscovered in 1897 and published by , and reprinted in volume X of his collected works and in . There is an English translation with commentary given by , reprinted in the second edition of .\n\nMost of the entries consist of a brief and sometimes cryptic statement of a result in Latin.\n\nEntry 1, dated 1796, March 30, states \"Principia quibus innititur sectio circuli, ac divisibilitus eiusdem geometrica in septemdecim partes etc.\", which records Gauss's discovery of the construction of a heptadecagon by ruler and compass.\n\nEntry 18, dated 1796, July 10, states \"ΕΥΡΗΚΑ! num = Δ + Δ + Δ\" and records his discovery of a proof that any number is the sum of 3 triangular numbers, a special case of the Fermat polygonal number theorem.\nEntry 43, dated 1796, October 21, states \"Vicimus GEGAN\" (We have conquered GEGAN). The meaning of this was a mystery for many years. found a manuscript by Gauss suggesting that GEGAN is a reversal of the acronym NAGEG standing for Nexum medii Arithmetico-Geometricum Expectationibus Generalibus and refers to the connection between the arithmetic geometric mean and elliptic functions. \n\nEntry 146, dated 1814 July 9, is the last entry, and records an observation relating biquadratic residues and the lemniscate functions, later proved by Gauss and by . More precisely, Gauss observed that if \"a\"+\"bi\" is a (Gaussian) prime and \"a\"–1+\"bi\" is divisible by 2+2\"i\", then the number of solutions to the congruence 1=\"xx\"+\"yy\"+\"xxyy\" (mod \"a\"+\"bi\"), including \"x\"=∞, \"y\"=±\"i\" and \"x\"=±\"i\", \"y\"=∞, is (\"a\"–1)+\"b\".\n\n"}
{"id": "14717987", "url": "https://en.wikipedia.org/wiki?curid=14717987", "title": "Global element", "text": "Global element\n\nIn category theory, a global element of an object \"A\" from a category is a morphism\nwhere 1 is a terminal object of the category. Roughly speaking, global elements are a generalization of the notion of “elements” from the category of sets, and they can be used to import set-theoretic concepts into category theory. However, unlike a set, an object of a general category need not be determined by its global elements (not even up to isomorphism). For example, the terminal object of the category Grph of graph homomorphisms has one vertex and one edge, a self-loop, whence the global elements of a graph are its self-loops, conveying no information either about other kinds of edges, or about vertices having no self-loop, or about whether two self-loops share a vertex.\n\nIn an elementary topos the global elements of the subobject classifier Ω form a Heyting algebra when ordered by inclusion of the corresponding subobjects of the terminal object. For example, Grph happens to be a topos, whose subobject classifier Ω is a two-vertex directed clique with an additional self-loop (so five edges, three of which are self-loops and hence the global elements of Ω). The internal logic of Grph is therefore based on the three-element Heyting algebra as its truth values.\n\nA well-pointed category is a category that has enough global elements to distinguish every two arrows. That is, for each two different arrows in the category, there should exist a global element whose compositions with them are different from each other.\n"}
{"id": "304942", "url": "https://en.wikipedia.org/wiki?curid=304942", "title": "Heart rate", "text": "Heart rate\n\nHeart rate is the speed of the heartbeat measured by the number of contractions (beats) of the heart per minute (bpm). The heart rate can vary according to the body's physical needs, including the need to absorb oxygen and excrete carbon dioxide. It is usually equal or close to the pulse measured at any peripheral point. Activities that can provoke change include physical exercise, sleep, anxiety, stress, illness, and ingestion of drugs.\n\nThe American Heart Association states the normal resting adult human heart rate is 60–100 bpm. Tachycardia is a fast heart rate, defined as above 100 bpm at rest. Bradycardia is a slow heart rate, defined as below 60 bpm at rest. During sleep a slow heartbeat with rates around 40–50 bpm is common and is considered normal. When the heart is not beating in a regular pattern, this is referred to as an arrhythmia. Abnormalities of heart rate sometimes indicate disease.\n\nWhile heart rhythm is regulated entirely by the sinoatrial node under normal conditions, heart rate is regulated by sympathetic and parasympathetic input to the sinoatrial node. The accelerans nerve provides sympathetic input to the heart by releasing norepinephrine onto the cells of the sinoatrial node (SA node), and the vagus nerve provides parasympathetic input to the heart by releasing acetylcholine onto sinoatrial node cells. Therefore, stimulation of the accelerans nerve increases heart rate, while stimulation of the vagus nerve decreases it.\n\nDue to individuals having a constant blood volume, one of the physiological ways to deliver more oxygen to an organ is to increase heart rate to permit blood to pass by the organ more often. Normal resting heart rates range from 60-100 bpm. Bradycardia is defined as a resting heart rate below 60 bpm. However, heart rates from 50 to 60 bpm are common among healthy people and do not necessarily require special attention. Tachycardia is defined as a resting heart rate above 100 bpm, though persistent rest rates between 80–100 bpm, mainly if they are present during sleep, may be signs of hyperthyroidism or anemia (see below).\n\nThere are many ways in which the heart rate speeds up or slows down. Most involve stimulant-like endorphins and hormones being released in the brain, many of which are those that are 'forced'/'enticed' out by the ingestion and processing of drugs.\n\nThis section discusses target heart rates for healthy persons and are inappropriately high for most persons with coronary artery disease.\n\nThe heart rate is rhythmically generated by the sinoatrial node. It is also influenced by central factors through sympathetic and parasympathetic nerves. Nervous influence over the heartrate is centralized within the two paired cardiovascular centres of the medulla oblongata. The cardioaccelerator regions stimulate activity via sympathetic stimulation of the cardioaccelerator nerves, and the cardioinhibitory centers decrease heart activity via parasympathetic stimulation as one component of the vagus nerve. During rest, both centers provide slight stimulation to the heart, contributing to autonomic tone. This is a similar concept to tone in skeletal muscles. Normally, vagal stimulation predominates as, left unregulated, the SA node would initiate a sinus rhythm of approximately 100 bpm.\n\nBoth sympathetic and parasympathetic stimuli flow through the paired cardiac plexus near the base of the heart. The cardioaccelerator center also sends additional fibers, forming the cardiac nerves via sympathetic ganglia (the cervical ganglia plus superior thoracic ganglia T1–T4) to both the SA and AV nodes, plus additional fibers to the atria and ventricles. The ventricles are more richly innervated by sympathetic fibers than parasympathetic fibers. Sympathetic stimulation causes the release of the neurotransmitter norepinephrine (also known as noradrenaline) at the neuromuscular junction of the cardiac nerves. This shortens the repolarization period, thus speeding the rate of depolarization and contraction, which results in an increased heartrate. It opens chemical or ligand-gated sodium and calcium ion channels, allowing an influx of positively charged ions.\n\nNorepinephrine binds to the beta–1 receptor. High blood pressure medications are used to block these receptors and so reduce the heart rate.\n\nParasympathetic stimulation originates from the cardioinhibitory region with impulses traveling via the vagus nerve (cranial nerve X). The vagus nerve sends branches to both the SA and AV nodes, and to portions of both the atria and ventricles. Parasympathetic stimulation releases the neurotransmitter acetylcholine (ACh) at the neuromuscular junction. ACh slows HR by opening chemical- or ligand-gated potassium ion channels to slow the rate of spontaneous depolarization, which extends repolarization and increases the time before the next spontaneous depolarization occurs. Without any nervous stimulation, the SA node would establish a sinus rhythm of approximately 100 bpm. Since resting rates are considerably less than this, it becomes evident that parasympathetic stimulation normally slows HR. This is similar to an individual driving a car with one foot on the brake pedal. To speed up, one need merely remove one’s foot from the brake and let the engine increase speed. In the case of the heart, decreasing parasympathetic stimulation decreases the release of ACh, which allows HR to increase up to approximately 100 bpm. Any increases beyond this rate would require sympathetic stimulation.\nThe cardiovascular centres receive input from a series of visceral receptors with impulses traveling through visceral sensory fibers within the vagus and sympathetic nerves via the cardiac plexus. Among these receptors are various proprioreceptors, baroreceptors, and chemoreceptors, plus stimuli from the limbic system which normally enable the precise regulation of heart function, via cardiac reflexes. Increased physical activity results in increased rates of firing by various proprioreceptors located in muscles, joint capsules, and tendons. The cardiovascular centres monitor these increased rates of firing, suppressing parasympathetic stimulation or increasing sympathetic stimulation as needed in order to increase blood flow.\n\nSimilarly, baroreceptors are stretch receptors located in the aortic sinus, carotid bodies, the venae cavae, and other locations, including pulmonary vessels and the right side of the heart itself. Rates of firing from the baroreceptors represent blood pressure, level of physical activity, and the relative distribution of blood. The cardiac centers monitor baroreceptor firing to maintain cardiac homeostasis, a mechanism called the baroreceptor reflex. With increased pressure and stretch, the rate of baroreceptor firing increases, and the cardiac centers decrease sympathetic stimulation and increase parasympathetic stimulation. As pressure and stretch decrease, the rate of baroreceptor firing decreases, and the cardiac centers increase sympathetic stimulation and decrease parasympathetic stimulation.\n\nThere is a similar reflex, called the atrial reflex or Bainbridge reflex, associated with varying rates of blood flow to the atria. Increased venous return stretches the walls of the atria where specialized baroreceptors are located. However, as the atrial baroreceptors increase their rate of firing and as they stretch due to the increased blood pressure, the cardiac center responds by increasing sympathetic stimulation and inhibiting parasympathetic stimulation to increase HR. The opposite is also true.\n\nIncreased metabolic byproducts associated with increased activity, such as carbon dioxide, hydrogen ions, and lactic acid, plus falling oxygen levels, are detected by a suite of chemoreceptors innervated by the glossopharyngeal and vagus nerves. These chemoreceptors provide feedback to the cardiovascular centers about the need for increased or decreased blood flow, based on the relative levels of these substances.\n\nThe limbic system can also significantly impact HR related to emotional state. During periods of stress, it is not unusual to identify higher than normal HRs, often accompanied by a surge in the stress hormone cortisol. Individuals experiencing extreme anxiety may manifest panic attacks with symptoms that resemble those of heart attacks. These events are typically transient and treatable. Meditation techniques have been developed to ease anxiety and have been shown to lower HR effectively. Doing simple deep and slow breathing exercises with one’s eyes closed can also significantly reduce this anxiety and HR.\n\nUsing a combination of autorhythmicity and innervation, the cardiovascular center is able to provide relatively precise control over the heart rate, but other factors can impact on this. These include hormones, notably epinephrine, norepinephrine, and thyroid hormones; levels of various ions including calcium, potassium, and sodium; body temperature; hypoxia; and pH balance.\n\nThe catecholamines, epinephrine and norepinephrine, secreted by the adrenal medulla form one component of the extended fight-or-flight mechanism. The other component is sympathetic stimulation. Epinephrine and norepinephrine have similar effects: binding to the beta-1 adrenergic receptors, and opening sodium and calcium ion chemical- or ligand-gated channels. The rate of depolarization is increased by this additional influx of positively charged ions, so the threshold is reached more quickly and the period of repolarization is shortened. However, massive releases of these hormones coupled with sympathetic stimulation may actually lead to arrhythmias. There is no parasympathetic stimulation to the adrenal medulla.\n\nIn general, increased levels of the thyroid hormones (thyroxine(T4) and triiodothyronine (T3)), increase the heart rate; excessive levels can trigger tachycardia. The impact of thyroid hormones is typically of a much longer duration than that of the catecholamines. The physiologically active form of triiodothyronine, has been shown to directly enter cardiomyocytes and alter activity at the level of the genome. It also impacts the beta adrenergic response similar to epinephrine and norepinephrine.\n\nCalcium ion levels have a great impact on heart rate and contractility: increased calcium levels cause an increase in both. High levels of calcium ions result in hypercalcemia and excessive levels can induce cardiac arrest. Drugs known as calcium channel blockers slow HR by binding to these channels and blocking or slowing the inward movement of calcium ions.\n\nCaffeine and nicotine are both stimulants of the nervous system and of the cardiac centres causing an increased heart rate. Caffeine works by increasing the rates of depolarization at the SA node, whereas nicotine stimulates the activity of the sympathetic neurons that deliver impulses to the heart. Both stimulants are legal and unregulated, and are known to be very addictive.\n\nBoth surprise and stress induce physiological response: elevate heart rate substantially. In a study conducted on 8 female and male student actors ages 18 to 25, their reaction to an unforeseen occurrence (the cause of stress) during a performance was observed in terms of heart rate. In the data collected, there was a noticeable trend between the location of actors (onstage and offstage) and their elevation in heart rate in response to stress; the actors present offstage reacted to the stressor immediately, demonstrated by their immediate elevation in heart the minute the unexpected event occurred, but the actors present onstage at the time of the stressor reacted in the following 5 minute period (demonstrated by their increasingly elevated heart rate). This trend regarding stress and heart rate is supported by previous studies; negative emotion/stimulus has a prolonged effect on heart rate in individuals who are directly impacted. \nIn regard to the characters present onstage, a reduced startle response has been associated with a passive defense, and the diminished initial heart rate response has been predicted to have a greater tendency to dissociation. \nFurther, note that heart rate is an accurate measure of stress and the startle response which can be easily observed to determine the effects of certain stressors.\n\nThe heart rate can be slowed by altered sodium and potassium levels, hypoxia, acidosis, alkalosis, and hypothermia. The relationship between electrolytes and HR is complex, but maintaining electrolyte balance is critical to the normal wave of depolarization. Of the two ions, potassium has the greater clinical significance. Initially, both hyponatremia (low sodium levels) and hypernatremia (high sodium levels) may lead to tachycardia. Severely high hypernatremia may lead to fibrillation, which may cause CO to cease. Severe hyponatremia leads to both bradycardia and other arrhythmias. Hypokalemia (low potassium levels) also leads to arrhythmias, whereas hyperkalemia (high potassium levels) causes the heart to become weak and flaccid, and ultimately to fail.\n\nHeart muscle relies exclusively on aerobic metabolism for energy. Hypoxia (an insufficient supply of oxygen) leads to decreasing HRs, since metabolic reactions fueling heart contraction are restricted.\n\nAcidosis is a condition in which excess hydrogen ions are present, and the patient’s blood expresses a low pH value. Alkalosis is a condition in which there are too few hydrogen ions, and the patient’s blood has an elevated pH. Normal blood pH falls in the range of 7.35–7.45, so a number lower than this range represents acidosis and a higher number represents alkalosis. Enzymes, being the regulators or catalysts of virtually all biochemical reactions - are sensitive to pH and will change shape slightly with values outside their normal range. These variations in pH and accompanying slight physical changes to the active site on the enzyme decrease the rate of formation of the enzyme-substrate complex, subsequently decreasing the rate of many enzymatic reactions, which can have complex effects on HR. Severe changes in pH will lead to denaturation of the enzyme.\n\nThe last variable is body temperature. Elevated body temperature is called hyperthermia, and suppressed body temperature is called hypothermia. Slight hyperthermia results in increasing HR and strength of contraction. Hypothermia slows the rate and strength of heart contractions. This distinct slowing of the heart is one component of the larger diving reflex that diverts blood to essential organs while submerged. If sufficiently chilled, the heart will stop beating, a technique that may be employed during open heart surgery. In this case, the patient’s blood is normally diverted to an artificial heart-lung machine to maintain the body’s blood supply and gas exchange until the surgery is complete, and sinus rhythm can be restored. Excessive hyperthermia and hypothermia will both result in death, as enzymes drive the body systems to cease normal function, beginning with the central nervous system.\n\nHeart rate is not a stable value and it increases or decreases in response to the body's need in a way to maintain an equilibrium (basal metabolic rate) between requirement and delivery of oxygen and nutrients. The normal SA node firing rate is affected by autonomic nervous system activity: sympathetic stimulation increases and parasympathetic stimulation decreases the firing rate. A number of different metrics are used to describe heart rate.\n\nThe basal or resting heart rate (HR) is defined as the heart rate when a person is awake, in a neutrally temperate environment, and has not been subject to any recent exertion or stimulation, such as stress or surprise. A large body of evidence indicates that the normal range is 60-100 beats per minute. This resting heart rate is often correlated with mortality. For example, all-cause mortality is increased by 1.22 (hazard ratio) when heart rate exceeds 90 beats per minute. The mortality rate of patients with myocardial infarction increased from 15% to 41% if their admission heart rate was greater than 90 beats per minute. ECG of 46,129 individuals with low risk for cardiovascular disease revealed that 96% had resting heart rates ranging from 48-98 beats per minute. Finally, expert consensus reveals that 98% of cardiologists believe that the \"60 to 100\" range is too high, with a vast majority of them agreeing that 50 to 90 beats per minute is more appropriate. The normal resting heart rate is based on the at-rest firing rate of the heart's sinoatrial node, where the faster pacemaker cells driving the self-generated rhythmic firing and responsible for the heart's autorhythmicity are located. For endurance athletes at the elite level, it is not unusual to have a resting heart rate between 33 and 50 bpm.\n\nThe \"maximum heart rate\" (HR) is the highest heart rate an individual can achieve without severe problems through exercise stress, and generally decreases with age. Since HR varies by individual, the most accurate way of measuring any single person's HR is via a cardiac stress test. In this test, a person is subjected to controlled physiologic stress (generally by treadmill) while being monitored by an ECG. The intensity of exercise is periodically increased until certain changes in heart function are detected on the ECG monitor, at which point the subject is directed to stop. Typical duration of the test ranges ten to twenty minutes.\n\nAdults who are beginning a new exercise regimen are often advised to perform this test only in the presence of medical staff due to risks associated with high heart rates. For general purposes, a formula is often employed to estimate a person's maximum heart rate. However, these predictive formulas have been criticized as inaccurate because they generalized population-averages and usually focus on a person's age. It is well-established that there is a \"poor relationship between maximal heart rate and age\" and large standard deviations relative to predicted heart rates. (\"see Limitations of Estimation Formulas\").\n\nA number of formulas are used to estimate HR\n\nBased on measurements of 3320 healthy men and women aged between 19 and 89, and including the potential modifying effect of gender, body composition, and physical activity, Nes \"et al\" found\n\n\nThis relationship was found to hold substantially regardless of gender, physical activity status, maximal oxygen uptake, smoking, or body mass index. However, a standard error\nof the estimate of 10.8 beats/min must be accounted for when applying the formula to clinical settings, and the researchers concluded that actual measurement via a maximal test may be preferable whenever possible.\n\nFrom Tanaka, Monahan, & Seals (2001):\n\n\nTheir meta-analysis (of 351 prior studies involving 492 groups and 18,712 subjects) and laboratory study (of 514 healthy subjects) concluded that, using this equation, HRmax was very strongly correlated to age (r = −0.90). The regression equation that was obtained in the laboratory-based study (209 − 0.7 x age), was virtually identical to that of the meta-study. The results showed HRmax to be independent of gender and independent of wide variations in habitual physical activity levels. This study found a standard deviation of ~10 beats per minute for individuals of any age, meaning the HRmax formula given has an accuracy of ±20 beats per minute.\n\nIn 2007, researchers at the Oakland University analyzed maximum heart rates of 132 individuals recorded yearly over 25 years, and produced a linear equation very similar to the Tanaka formula, HR = 206.9 − (0.67 × age), and a nonlinear equation, HR = 191.5 − (0.007 × age). The linear equation had a confidence interval of ±5–8 bpm and the nonlinear equation had a tighter range of ±2–5 bpm. Also a third nonlinear equation was produced: HR = 163 + (1.16 × age) − (0.018 × age).\n\nNotwithstanding the research of Tanaka, Monahan, & Seals, the most widely cited formula for HR (which contains no reference to any standard deviation) is still:\n\nAlthough attributed to various sources, it is widely thought to have been devised in 1970 by Dr. William Haskell and Dr. Samuel Fox. Inquiry into the history of this formula reveals that it was not developed from original research, but resulted from observation based on data from approximately 11 references consisting of published research or unpublished scientific compilations. It gained widespread use through being used by Polar Electro in its heart rate monitors, which Dr. Haskell has \"laughed about\", as the formula \"was never supposed to be an absolute guide to rule people's training.\"\n\nWhile it is the most common (and easy to remember and calculate), this particular formula is not considered by reputable health and fitness professionals to be a good predictor of HR. Despite the widespread publication of this formula, research spanning two decades reveals its large inherent error, S = 7–11 bpm. Consequently, the estimation calculated by   HR = 220 − age   has neither the accuracy nor the scientific merit for use in exercise physiology and related fields.\n\nA 2002 study of 43 different formulas for HR (including that of Haskell and Fox – see above) published in the Journal of Exercise Psychology concluded that:\n\n\nResearch conducted at Northwestern University by Martha Gulati, et al., in 2010 suggested a maximum heart rate formula for women:\n\nA 2008 study from Lund, Sweden gives reference values (obtained during bicycle ergometry) for men:\nand for women:\n\n\n\nMaximum heart rates vary significantly between individuals. Even within a single elite sports team, such as Olympic rowers in their 20s, maximum heart rates have been reported as varying from 160 to 220. Such a variation would equate to a 60 or 90 year age gap in the linear equations above, and would seem to indicate the extreme variation about these average figures.\n\nFigures are generally considered averages, and depend greatly on individual physiology and fitness. For example, an endurance runner's rates will typically be lower due to the increased size of the heart required to support the exercise, while a sprinter's rates will be higher due to the improved response time and short duration. While each may have predicted heart rates of 180 (= 220 − age), these two people could have actual HR 20 beats apart (e.g., 170–190).\n\nFurther, note that individuals of the same age, the same training, in the same sport, on the same team, can have actual HR 60 bpm apart (160–220): the range is extremely broad, and some say \"The heart rate is probably the least important variable in comparing athletes.\"\n\n\"Heart rate reserve\" (HR) is the difference between a person's measured or predicted maximum heart rate and resting heart rate. Some methods of measurement of exercise intensity measure percentage of heart rate reserve. Additionally, as a person increases their cardiovascular fitness, their HR will drop, and the heart rate reserve will increase. Percentage of HR is equivalent to percentage of VO reserve.\n\nThis is often used to gauge exercise intensity (first used in 1957 by Karvonen).\n\nKarvonen's study findings have been questioned, due to the following:\n\nFor healthy people, the \"Target Heart Rate\" or \"Training Heart Rate\" (THR) is a desired range of heart rate reached during aerobic exercise which enables one's heart and lungs to receive the most benefit from a workout. This theoretical range varies based mostly on age; however, a person's physical condition, sex, and previous training also are used in the calculation. Below are two ways to calculate one's THR. In each of these methods, there is an element called \"intensity\" which is expressed as a percentage. The THR can be calculated as a range of 65–85% intensity. However, it is crucial to derive an accurate HR to ensure these calculations are meaningful.\n\n\"Example for someone with a HR of 180 (age 40, estimating HR As 220 − age): \"\n\nThe \"Karvonen method\" factors in resting heart rate (HR) to calculate target heart rate (THR), using a range of 50–85% intensity:\n\nEquivalently, \n\n\"Example for someone with a HR of 180 and a HR of 70 (and therefore a HR of 110):\"\n\nAn alternative to the Karvonen method is the \"Zoladz method\", which derives exercise zones by subtracting values from HR:\n\n\"Example for someone with a HR of 180:\"\n\n\"Heart rate recovery\" (HR) is the reduction in heart rate at peak exercise and the rate as measured after a cool-down period of fixed duration. A greater reduction in heart rate after exercise during the reference period is associated with a higher level of cardiac fitness.\n\nHeart rates that do not drop by more than 12 bpm one minute after stopping exercise are associated with an increased risk of death. Investigators of the Lipid Research Clinics Prevalence Study, which included 5,000 subjects, found that patients with an abnormal HR (defined as a decrease of 42 beats per minutes or less at two minutes post-exercise) had a mortality rate 2.5 times greater than patients with a normal recovery. Another study by Nishime et al. and featuring 9,454 patients followed for a median period of 5.2 years found a four-fold increase in mortality in subjects with an abnormal HR (≤12 bpm reduction one minute after the cessation of exercise). Shetler et al. studied 2,193 patients for thirteen years and found that a HR of ≤22 bpm after two minutes \"best identified high-risk patients\". They also found that while HR had significant prognostic value it had no diagnostic value.\n\nThe human heart beats more than 3.5 billion times in an average lifetime.\n\nThe heartbeat of a human embryo begins at approximately 21 days after conception, or five weeks after the last normal menstrual period (LMP), which is the date normally used to date pregnancy in the medical community. The electrical depolarizations that trigger cardiac myocytes to contract arise spontaneously within the myocyte itself. The heartbeat is initiated in the pacemaker regions and spreads to the rest of the heart through a conduction pathway. Pacemaker cells develop in the primitive atrium and the sinus venosus to form the sinoatrial node and the atrioventricular node respectively. Conductive cells develop the bundle of His and carry the depolarization into the lower heart.\n\nThe human heart begins beating at a rate near the mother’s, about 75–80 beats per minute (bpm). The embryonic heart rate then accelerates linearly for the first month of beating, peaking at 165–185 bpm during the early 7th week, (early 9th week after the LMP). This acceleration is approximately 3.3 bpm per day, or about 10 bpm every three days, an increase of 100 bpm in the first month.\n\nAfter peaking at about 9.2 weeks after the LMP, it decelerates to about 150 bpm (+/-25 bpm) during the 15th week after the LMP. After the 15th week the deceleration slows reaching an average rate of about 145 (+/-25 bpm) bpm at term. The regression formula which describes this acceleration before the embryo reaches 25 mm in crown-rump length or 9.2 LMP weeks is:\n\nformula_1\n\nThere is no difference in male and female heart rates before birth.\n\nHeart rate is measured by finding the pulse of the heart. This pulse rate can be found at any point on the body where the artery's pulsation is transmitted to the surface by pressuring it with the index and middle fingers; often it is compressed against an underlying structure like bone. (A good area is on the neck, under the corner of the jaw.) The thumb should not be used for measuring another person's heart rate, as its strong pulse may interfere with the correct perception of the target pulse.\n\nThe radial artery is the easiest to use to check the heart rate. However, in emergency situations the most reliable arteries to measure heart rate are carotid arteries. This is important mainly in patients with atrial fibrillation, in whom heart beats are irregular and stroke volume is largely different from one beat to another. In those beats following a shorter diastolic interval left ventricle doesn't fill properly, stroke volume is lower and pulse wave is not strong enough to be detected by palpation on a distal artery like the radial artery. It can be detected, however, by doppler.\n\nPossible points for measuring the heart rate are:\n\nA more precise method of determining heart rate involves the use of an electrocardiograph, or ECG (also abbreviated EKG). An ECG generates a pattern based on electrical activity of the heart, which closely follows heart function. Continuous ECG monitoring is routinely done in many clinical settings, especially in critical care medicine. On the ECG, instantaneous heart rate is calculated using the R wave-to-R wave (RR) interval and multiplying/dividing in order to derive heart rate in heartbeats/min. Multiple methods exist:\nHeart rate monitors allow measurements to be taken continuously and can be used during exercise when manual measurement would be difficult or impossible (such as when the hands are being used). Various commercial heart rate monitors are also available. Some monitors, used during sport, consist of a chest strap with electrodes. The signal is transmitted to a wrist receiver for display.\n\nAlternative methods of measurement include pulse oximetry and seismocardiography.\n\nTachycardia is a resting heart rate more than 100 beats per minute. This number can vary as smaller people and children have faster heart rates than average adults.\n\nPhysiological conditions where tachycardia occurs:\n\nPathological conditions where tachycardia occurs:\n\nBradycardia was defined as a heart rate less than 60 beats per minute when textbooks asserted that the normal range for heart rates was 60–100 bpm. The normal range has since been revised in textbooks to 50–90 bpm for a human at total rest. Setting a lower threshold for bradycardia prevents misclassification of fit individuals as having a pathologic heart rate. The normal heart rate number can vary as children and adolescents tend to have faster heart rates than average adults. Bradycardia may be associated with medical conditions such as hypothyroidism.\n\nTrained athletes tend to have slow resting heart rates, and resting bradycardia in athletes should not be considered abnormal if the individual has no symptoms associated with it. For example, Miguel Indurain, a Spanish cyclist and five time Tour de France winner, had a resting heart rate of 28 beats per minute, one of the lowest ever recorded in a healthy human. Daniel Green achieved the world record for the slowest heartbeat in a healthy human with a heart rate of just 26 bpm in 2014.\n\nArrhythmias are abnormalities of the heart rate and rhythm (sometimes felt as palpitations). They can be divided into two broad categories: fast and slow heart rates. Some cause few or minimal symptoms. Others produce more serious symptoms of lightheadedness, dizziness and fainting.\n\nA number of investigations indicate that faster resting heart rate has emerged as a new risk factor for mortality in homeothermic mammals, particularly cardiovascular mortality in human beings. Faster heart rate may accompany increased production of inflammation molecules and increased production of reactive oxygen species in cardiovascular system, in addition to increased mechanical stress to the heart. There is a correlation between increased resting rate and cardiovascular risk. This is not seen to be \"using an allotment of heart beats\" but rather an increased risk to the system from the increased rate.\n\nAn Australian-led international study of patients with cardiovascular disease has shown that heart beat rate is a key indicator for the risk of heart attack. The study, published in \"The Lancet\" (September 2008) studied 11,000 people, across 33 countries, who were being treated for heart problems. Those patients whose heart rate was above 70 beats per minute had significantly higher incidence of heart attacks, hospital admissions and the need for surgery. Higher heart rate is thought to be correlated with an increase in heart attack and about a 46 percent increase in hospitalizations for non-fatal or fatal heart attack.\n\nOther studies have shown that a high resting heart rate is associated with an increase in cardiovascular and all-cause mortality in the general population and in patients with chronic disease. A faster resting heart rate is associated with shorter life expectancy and is considered a strong risk factor for heart disease and heart failure, independent of level of physical fitness. Specifically, a resting heart rate above 65 beats per minute has been shown to have a strong independent effect on premature mortality; every 10 beats per minute increase in resting heart rate has been shown to be associated with a 10–20% increase in risk of death. In one study, men with no evidence of heart disease and a resting heart rate of more than 90 beats per minute had a five times higher risk of sudden cardiac death. Similarly, another study found that men with resting heart rates of over 90 beats per minute had an almost two-fold increase in risk for cardiovascular disease mortality; in women it was associated with a three-fold increase.\n\nGiven these data, heart rate should be considered in the assessment of cardiovascular risk, even in apparently healthy individuals. Heart rate has many advantages as a clinical parameter: It is inexpensive and quick to measure and is easily understandable. Although the accepted limits of heart rate are between 60 and 100 beats per minute, this was based for convenience on the scale of the squares on electrocardiogram paper; a better definition of normal sinus heart rate may be between 50 and 90 beats per minute.\n\nStandard textbooks of physiology and medicine mention that heart rate (HR) is readily calculated from the ECG as follows:\n\nLifestyle and pharmacological regimens may be beneficial to those with high resting heart rates. Exercise is one possible measure to take when an individual's heart rate is higher than 80 beats per minute. Diet has also been found to be beneficial in lowering resting heart rate: In studies of resting heart rate and risk of death and cardiac complications on patients with type 2 diabetes, legumes were found to lower resting heart rate. This is thought to occur because in addition to the direct beneficial effects of legumes, they also displace animal proteins in the diet, which are higher in saturated fat and cholesterol.\n\nA very slow heart rate (bradycardia) may be associated with heart block. It may also arise from autonomous nervous system impairment.\n\n\n"}
{"id": "194799", "url": "https://en.wikipedia.org/wiki?curid=194799", "title": "Helge von Koch", "text": "Helge von Koch\n\nNiels Fabian Helge von Koch (25 January 1870 – 11 March 1924) was a Swedish mathematician who gave his name to the famous fractal known as the Koch snowflake, one of the earliest fractal curves to be described.\n\nHe was born into a family of Swedish nobility. His grandfather, Nils Samuel von Koch (1801–1881), was the Attorney-General of Sweden. His father, Richert Vogt von Koch (1838–1913) was a Lieutenant-Colonel in the Royal Horse Guards of Sweden. He was enrolled at the newly created Stockholm University College in 1887 (studying under Gösta Mittag-Leffler), and at Uppsala University in 1888, where he also received his bachelor's degree (\"filosofie kandidat\") since non-governmental college in Stockholm had not yet received the rights to issue degrees. He received his Ph.D. in Uppsala in 1892. He was appointed professor of mathematics at the Royal Institute of Technology in Stockholm in 1905, succeeding Ivar Bendixson, and became professor of pure mathematics at Stockholm University College in 1911.\n\nVon Koch wrote several papers on number theory. One of his results was a 1901 theorem proving that the Riemann hypothesis is equivalent to a stronger form of the prime number theorem.\n\nHe described the Koch curve in a 1904 paper entitled \"On a continuous curve without tangents constructible from elementary geometry\" (\"original French title: \"Sur une courbe continue sans tangente, obtenue par une construction géométrique élémentaire\"\").\n\nHe was an Invited Speaker of the International Congress of Mathematicians in 1900 in Paris with talk \"Sur la distribution des nombres premiers\" and in 1912 in Cambridge, England with talk \"On regular and irregular solutions of some infinite systems of linear equations\".\n\n\n"}
{"id": "10280254", "url": "https://en.wikipedia.org/wiki?curid=10280254", "title": "Isothermal coordinates", "text": "Isothermal coordinates\n\nIn mathematics, specifically in differential geometry, isothermal coordinates on a Riemannian manifold \nare local coordinates where the metric is\nconformal to the Euclidean metric. This means that in isothermal\ncoordinates, the Riemannian metric locally has the form\nwhere formula_2 is a smooth function. (If the Riemannian manifold is oriented, some authors insist that a coordinate system must agree with that orientation to be isothermal.)\n\nIsothermal coordinates on surfaces were first introduced by Gauss. Korn and Lichtenstein proved that isothermal coordinates exist around any point on a two dimensional Riemannian manifold. On higher-dimensional Riemannian manifolds a necessary and sufficient condition for their local existence is the vanishing of the Weyl tensor and of the Cotton tensor.\n\n proved the existence of isothermal coordinates on an arbitrary surface with a real analytic metric, following results of\n\nThe existence of isothermal coordinates can be proved by applying known existence theorems for the Beltrami equation, which rely on L estimates for singular integral operators of Calderón and Zygmund. A simpler approach to the Beltrami equation has been given more recently by the late Adrien Douady.\n\nIf the Riemannian metric is given locally as\n\nthen in the complex coordinate \"z\" = \"x\" + i\"y\", it takes the form\n\nwhere λ and μ are smooth with λ > 0 and |μ| < 1. In fact\n\nIn isothermal coordinates (\"u\", \"v\") the metric should take the form\n\nwith ρ > 0 smooth. The complex coordinate \"w\" = \"u\" + i \"v\" satisfies\n\nso that the coordinates (\"u\", \"v\") will be isothermal if the Beltrami equation\n\nhas a diffeomorphic solution. Such a solution has been proved to exist in any neighbourhood where ||μ|| < 1.\n\nNew coordinates \"u\" and \"v\" are isothermal provided that\n\nwhere formula_10 is the Hodge star operator defined by the metric.\n\nLet formula_11 be the Laplace–Beltrami operator on functions.\n\nThen by standard elliptic theory, \"u\" can be chosen to be harmonic near a given point, i.e. Δ \"u\" = 0, with \"du\" non-vanishing.\n\nBy the Poincaré lemma formula_12 has a local solution \"v\" exactly when formula_13.\n\nSince\n\nthis is equivalent to Δ \"u\" = 0, and hence a local solution exists.\n\nSince \"du\" is non-zero and the square of the Hodge star operator is −1 on 1-forms, \"du\" and \"dv\" are necessarily linearly independent, and therefore \"u\" and \"v\" give local isothermal coordinates.\n\nIn the isothermal coordinates (\"u\", \"v\"), the Gaussian curvature takes the simpler form\n\nwhere formula_16.\n\n\n"}
{"id": "2137251", "url": "https://en.wikipedia.org/wiki?curid=2137251", "title": "Jean van Heijenoort", "text": "Jean van Heijenoort\n\nJean Louis Maxime van Heijenoort (; ; July 23, 1912 – March 29, 1986) was a pioneer historian of mathematical logic. He was also a personal secretary to Leon Trotsky from 1932 to 1939, and from then until 1947, an American Trotskyist activist.\n\nVan Heijenoort was born in Creil, France. His family's financial circumstances were difficult as his Dutch immigrant father died when van Heijenoort was two. He nevertheless acquired a powerful traditional French formal education, to which his French writings attest. (He also published in Spanish.) Although he eventually became a naturalized American citizen, he visited France twice a year from 1958 until his death, and remained very attached to his French extended family and friends.\n\nIn 1932, he joined the Trotskyist movement (recruited by Yvan Craipeau) and the Communist League. Very soon thereafter, the recently exiled Trotsky hired van Heijenoort as a secretary and bodyguard, primarily because of his fluency in French, Russian, German, and English. Thus began seven years in Trotsky's household, during which he served as an all-purpose translator, helping Trotsky write several books and keep up an extensive intellectual and political correspondence in several languages.\n\nIn 1939, van Heijenoort moved to New York City to be with his second wife, Beatrice \"Bunny\" Guyer, where he worked for the Socialist Workers Party (US) (SWP) and wrote a number of articles for the American Trotskyist press and other radical outlets. He was elected to the secretariat of the Fourth International in 1940 but resigned when Felix Morrow and Albert Goldman, with whom he had sided, were expelled from the SWP. Goldman subsequently went on to join the US Workers Party but Morrow joined no other party/grouping. In 1947, he too was expelled from the SWP. In 1948, he published an article, called \"A Century's Balance Sheet\" in which he criticized that part of Marxism which saw the \"proletariat\" as the revolutionary class. He continued to hold other parts of Marxism as true.\n\nVan Heijenoort was spared the ordeal of McCarthyism because everything he published in Trotskyist organs appeared under one or other of more than a dozen pen names. Moreover, Feferman (1993) states that van Heijenoort the logician was quite reticent about his Trotskyist youth, and did not discuss politics. Nevertheless, in the last decade of his life he contributed to the history of the Trotskyist movement by writing the monograph \"With Trotsky in Exile\" (1978), editing a volume of Trotsky's correspondence (1980), and advising and working with the archivists at the Houghton Library in Harvard University, which holds many of Trotsky's papers from his years in exile.\n\nAfter completing a Ph.D. in mathematics at New York University in 1949 under the supervision of J. J. Stoker, he taught mathematics there but evolved into a logician and philosopher of mathematics, in good part because of the influence of Georg Kreisel. He began teaching philosophy, first part-time at Columbia University, then full-time at Brandeis University, 1965-77. He spent much of his last decade at Stanford University, writing and editing eight books, including parts of the \"Collected Works\" of Kurt Gödel.\n\nThe \"Source Book\" (van Heijenoort 1967), perhaps the most important book ever published on the history of logic and of the foundations of mathematics, is an anthology of translations. It begins with the first complete translation of Frege's 1879 \"Begriffsschrift\", which is followed by 45 historically important short pieces on mathematical logic and axiomatic set theory, originally published between 1889 and 1931. The anthology ends with Gödel's landmark paper on the incompletability of Peano arithmetic. For more information on the period covered by this anthology, see Grattan-Guinness (2000).\n\nNearly all the content of the \"Source Book\" was difficult to access in all but the best North American university libraries (e.g., even the Library of Congress did not acquire a copy of the \"Begriffsschrift\" until 1964), and all but four pieces had to be translated from one of six continental European languages. When possible, the author of the original text was asked to review the translation of his work, and suggest corrections and amendments. Each piece included editorial footnotes, all references were combined into one list, and many misprints, inconsistencies, and errors in the originals were corrected. Especially important are the remarkable introductions to each translation, most written by van Heijenoort himself. A few were written by Willard Quine and Burton Dreben.\n\nThe \"Source Book\" did much to advance the view that modern logic begins with, and builds on, the \"Begriffsschrift\". Grattan-Guinness (2000) argues that this perspective on the history of logic is mistaken, because Frege employed an idiosyncratic notation and was far less read than, say, Peano. Ironically, van Heijenoort (1967a) is often cited by those who prefer the alternative model theoretic stance on logic and mathematics. Much of the history of that stance, whose leading lights include George Boole, Charles Sanders Peirce, Ernst Schröder, Leopold Löwenheim, Thoralf Skolem, Alfred Tarski, and Jaakko Hintikka, is covered in Brady (2000). The \"Source Book\" underrated the algebraic logic of De Morgan, Boole, Peirce, and Schröder, but devoted more pages to Skolem than to anyone other than Frege, and included Löwenheim (1915), the founding paper on model theory.\n\nVan Heijenoort had children with two of his four wives. While living with Trotsky in Coyoacán, now a neighborhood of Mexico City, van Heijenoort's first wife left him after clashing with Trotsky's spouse. Van Heijenoort was also one of Frida Kahlo's lovers; in the film \"Frida\", he is played by Felipe Fulop. Having parted company with Trotsky in 1939 for personal reasons, van Heijenoort was innocent of all circumstances leading to Trotsky's 1940 murder. Van Heijenoort himself was also murdered, in Mexico City 46 years later, by his estranged fourth spouse whom he was visiting at the time. She then took her own life.\n\n\nBooks which Van Heijenoort edited alone or with others:\n\n\n"}
{"id": "58729995", "url": "https://en.wikipedia.org/wiki?curid=58729995", "title": "Jeanette McLeod", "text": "Jeanette McLeod\n\nJeanette Claire McLeod is a New Zealand mathematician specializing in combinatorics, including the theories of Latin squares and random graphs. She is a senior lecturer in the School of Mathematics and Statistics\nof the University of Canterbury, a principal investigator for Te Pūnaha Matatini, a Centre of Research Excellence associated with the University of Auckland, an honorary senior lecturer at Australian National University, and the president for the 2018 term of the Combinatorial Mathematics Society of Australasia.\n\nMcLeod earned her Ph.D. in 2007 from Australian National University. Her dissertation, \"Methods in Asymptotic Combinatorics\", was supervised by Brendan McKay.\nShe is one of the cofounders of Maths Craft, a project to popularize mathematics using crafts such as crochet and origami.\n\n"}
{"id": "40579744", "url": "https://en.wikipedia.org/wiki?curid=40579744", "title": "Jesús María Sanz-Serna", "text": "Jesús María Sanz-Serna\n\nJesús María Sanz-Serna (born 12 June 1953 in Valladolid, Spain) is a mathematician who specializes in applied mathematics. Sanz-Serna pioneered the field of geometric integration and wrote the first book on this subject. From 1998 to 2006, he was rector of the University of Valladolid.\n\nHe received the inaugural Dahlquist Prize from the Society for Industrial and Applied Mathematics in 1995 and became one of the inaugural fellows of the American Mathematical Society in 2012. His 60th birthday was celebrated at the 2013 International Conference on Scientific Computation and Differential Equations (SciCADE) in Valladolid.\n"}
{"id": "1521924", "url": "https://en.wikipedia.org/wiki?curid=1521924", "title": "Jet (mathematics)", "text": "Jet (mathematics)\n\nIn mathematics, the jet is an operation that takes a differentiable function \"f\" and produces a polynomial, the truncated Taylor polynomial of \"f\", at each point of its domain. Although this is the definition of a jet, the theory of jets regards these polynomials as being abstract polynomials rather than polynomial functions.\n\nThis article first explores the notion of a jet of a real valued function in one real variable, followed by a discussion of generalizations to several real variables. It then gives a rigorous construction of jets and jet spaces between Euclidean spaces. It concludes with a description of jets between manifolds, and how these jets can be constructed intrinsically. In this more general context, it summarizes some of the applications of jets to differential geometry and the theory of differential equations.\n\nBefore giving a rigorous definition of a jet, it is useful to examine some special cases.\n\nSuppose that formula_1 is a real-valued function having at least \"k\" + 1 derivatives in a neighborhood \"U\" of the point formula_2. Then by Taylor's theorem,\n\nwhere\nThen the \"k\"-jet of \"f\" at the point formula_2 is defined to be the polynomial\n\nJets are normally regarded as abstract polynomials in the variable \"z\", not as actual polynomial functions in that variable. In other words, \"z\" is an indeterminate variable allowing one to perform various algebraic operations among the jets. It is in fact the base-point formula_2 from which jets derive their functional dependency. Thus, by varying the base-point, a jet yields a polynomial of order at most \"k\" at every point. This marks an important conceptual distinction between jets and truncated Taylor series: ordinarily a Taylor series is regarded as depending functionally on its variable, rather than its base-point. Jets, on the other hand, separate the algebraic properties of Taylor series from their functional properties. We shall deal with the reasons and applications of this separation later in the article.\n\nSuppose that formula_8 is a function from one Euclidean space to another having at least (\"k\" + 1) derivatives. In this case, Taylor's theorem asserts that\n\nThe \"k\"-jet of \"f\" is then defined to be the polynomial\n\nin formula_11, where formula_12.\n\nThere are two basic algebraic structures jets can carry. The first is a product structure, although this ultimately turns out to be the least important. The second is the structure of the composition of jets.\n\nIf formula_13 are a pair of real-valued functions, then we can define the product of their jets via\n\nHere we have suppressed the indeterminate \"z\", since it is understood that jets are formal polynomials. This product is just the product of ordinary polynomials in \"z\", modulo formula_15. In other words, it is multiplication in the ring formula_16, where formula_17 is the ideal generated by polynomials homogeneous of order ≥ \"k\" + 1.\n\nWe now move to the composition of jets. To avoid unnecessary technicalities, we consider jets of functions that map the origin to the origin. If formula_18 and formula_19 with \"f\"(0) = 0 and \"g\"(0) = 0, then formula_20. The \"composition of jets\" is defined by\nformula_21\nIt is readily verified, using the chain rule, that this constitutes an associative noncommutative operation on the space of jets at the origin.\n\nIn fact, the composition of \"k\"-jets is nothing more than the composition of polynomials modulo the ideal of polynomials homogeneous of order formula_22.\n\n\"Examples:\"\n\nand\n\nThis subsection focuses on two different rigorous definitions of the jet of a function at a point, followed by a discussion of Taylor's theorem. These definitions shall prove to be useful later on during the intrinsic definition of the jet of a function between two manifolds.\n\nThe following definition uses ideas from mathematical analysis to define jets and jet spaces. It can be generalized to smooth functions between Banach spaces, analytic functions between real or complex domains, to p-adic analysis, and to other areas of analysis.\n\nLet formula_28 be the vector space of smooth functions formula_29. Let \"k\" be a non-negative integer, and let \"p\" be a point of formula_30. We define an equivalence relation formula_31 on this space by declaring that two functions \"f\" and \"g\" are equivalent to order \"k\" if \"f\" and \"g\" have the same value at \"p\", and all of their partial derivatives agree at \"p\" up to (and including) their \"k\"-th-order derivatives. In short,formula_32 iff formula_33 to \"k\"-th order.\n\nThe \"k\"-th-order jet space of formula_28 at \"p\" is defined to be the set of equivalence classes of formula_35, and is denoted by formula_36.\n\nThe \"k\"-th-order jet at \"p\" of a smooth function formula_37 is defined to be the equivalence class of \"f\" in formula_36.\n\nThe following definition uses ideas from algebraic geometry and commutative algebra to establish the notion of a jet and a jet space. Although this definition is not particularly suited for use in algebraic geometry per se, since it is cast in the smooth category, it can easily be tailored to such uses.\n\nLet formula_39 be the vector space of germs of smooth functions formula_29 at a point \"p\" in formula_30. Let formula_42 be the ideal of functions that vanish at \"p\". (This is the maximal ideal for the local ring formula_39.) Then the ideal formula_44 consists of all function germs that vanish to order \"k\" at \"p\". We may now define the jet space at \"p\" by\n\nIf formula_29 is a smooth function, we may define the \"k\"-jet of \"f\" at \"p\" as the element of formula_36 by setting\n\nThis is a more general construction. For an formula_49-space formula_50, let formula_51 be the stalk of the structure sheaf at formula_52 and let formula_42 be the maximal ideal of the local ring formula_51. The kth jet space at formula_52 is defined to be the ring formula_56(formula_44 is the product of ideals).\n\nRegardless of the definition, Taylor's theorem establishes a canonical isomorphism of vector spaces between formula_36 and formula_59. So in the Euclidean context, jets are typically identified with their polynomial representatives under this isomorphism.\n\nWe have defined the space formula_36 of jets at a point formula_61. The subspace of this consisting of jets of functions \"f\" such that \"f\"(\"p\") = \"q\" is denoted by\n\nIf \"M\" and \"N\" are two smooth manifolds, how do we define the jet of a function formula_63? We could perhaps attempt to define such a jet by using local coordinates on \"M\" and \"N\". The disadvantage of this is that jets cannot thus be defined in an equivariant fashion. Jets do not transform as tensors. Instead, jets of functions between two manifolds belong to a jet bundle.\n\nThis section begins by introducing the notion of jets of functions from the real line to a manifold. It proves that such jets form a fibre bundle, analogous to the tangent bundle, which is an associated bundle of a jet group. It proceeds to address the problem of defining the jet of a function between two smooth manifolds. Throughout this section, we adopt an analytic approach to jets. Although an algebro-geometric approach is also suitable for many more applications, it is too subtle to be dealt with systematically here. See jet (algebraic geometry) for more details.\n\nSuppose that \"M\" is a smooth manifold containing a point \"p\". We shall define the jets of curves through \"p\", by which we henceforth mean smooth functions formula_64 such that \"f\"(0) = \"p\". Define an equivalence relation formula_31 as follows. Let \"f\" and \"g\" be a pair of curves through \"p\". We will then say that \"f\" and \"g\" are equivalent to order \"k\" at \"p\" if there is some neighborhood \"U\" of \"p\", such that, for every smooth function formula_66, formula_67. Note that these jets are well-defined since the composite functions formula_68 and formula_69 are just mappings from the real line to itself. This equivalence relation is sometimes called that of \"k\"-th-order contact between curves at \"p\".\n\nWe now define the k\"-jet of a curve \"f\" through \"p\" to be the equivalence class of \"f\" under formula_35, denoted formula_71 or formula_72. The k\"-th-order jet space formula_73 is then the set of \"k\"-jets at \"p\".\nAs \"p\" varies over \"M\", formula_73 forms a fibre bundle over \"M\": the \"k\"-th-order tangent bundle, often denoted in the literature by \"T\"\"M\" (although this notation occasionally can lead to confusion). In the case \"k\"=1, then the first-order tangent bundle is the usual tangent bundle: \"T\"\"M\" = \"TM\".\n\nTo prove that \"T\"\"M\" is in fact a fibre bundle, it is instructive to examine the properties of formula_73 in local coordinates. Let (\"x\")= (\"x\"...,\"x\") be a local coordinate system for \"M\" in a neighborhood \"U\" of \"p\". Abusing notation slightly, we may regard (\"x\") as a local diffeomorphism formula_76.\n\n\"Claim.\" Two curves \"f\" and \"g\" through \"p\" are equivalent modulo formula_31 if and only if formula_78.\n\nHence the ostensible fibre bundle \"T\"\"M\" admits a local trivialization in each coordinate neighborhood. At this point, in order to prove that this ostensible fibre bundle is in fact a fibre bundle, it suffices to establish that it has non-singular transition functions under a change of coordinates. Let formula_86 be a different coordinate system and let formula_87 be the associated change of coordinates diffeomorphism of Euclidean space to itself. By means of an affine transformation of formula_30, we may assume without loss of generality that ρ(0)=0. With this assumption, it suffices to prove that formula_89 is an invertible transformation under jet composition. (See also jet groups.) But since ρ is a diffeomorphism, formula_90 is a smooth mapping as well. Hence,\n\nwhich proves that formula_92 is non-singular. Furthermore, it is smooth, although we do not prove that fact here.\n\nIntuitively, this means that we can express the jet of a curve through \"p\" in terms of its Taylor series in local coordinates on \"M\".\n\n\"Examples in local coordinates:\"\n\n\n\nWe are now prepared to define the jet of a function from a manifold to a manifold.\n\nSuppose that \"M\" and \"N\" are two smooth manifolds. Let \"p\" be a point of \"M\". Consider the space formula_101 consisting of smooth maps formula_63 defined in some neighborhood of \"p\". We define an equivalence relation formula_35 on formula_101 as follows. Two maps \"f\" and \"g\" are said to be \"equivalent\" if, for every curve γ through \"p\" (recall that by our conventions this is a mapping formula_105 such that formula_106), we have formula_107 on some neighborhood of \"0\".\n\nThe jet space formula_108 is then defined to be the set of equivalence classes of formula_101 modulo the equivalence relation formula_35. Note that because the target space \"N\" need not possess any algebraic structure, formula_108 also need not have such a structure. This is, in fact, a sharp contrast with the case of Euclidean spaces.\n\nIf formula_63 is a smooth function defined near \"p\", then we define the \"k\"-jet of \"f\" at \"p\", formula_113, to be the equivalence class of \"f\" modulo formula_35.\n\nJohn Mather introduced the notion of \"multijet\". Loosely speaking, a multijet is a finite list of jets over different base-points. Mather proved the multijet transversality theorem, which he used in his study of stable mappings.\n\nThis subsection deals with the notion of jets of local sections of a vector bundle. Almost everything in this section generalizes mutatis mutandis to the case of local sections of a fibre bundle, a Banach bundle over a Banach manifold, a fibered manifold, or quasi-coherent sheaves over schemes. Furthermore, these examples of possible generalizations are certainly not exhaustive.\n\nSuppose that \"E\" is a finite-dimensional smooth vector bundle over a manifold \"M\", with projection formula_115. Then sections of \"E\" are smooth functions formula_116 such that formula_117 is the identity automorphism of \"M\". The jet of a section \"s\" over a neighborhood of a point \"p\" is just the jet of this smooth function from \"M\" to \"E\" at \"p\".\n\nThe space of jets of sections at \"p\" is denoted by formula_118. Although this notation can lead to confusion with the more general jet spaces of functions between two manifolds, the context typically eliminates any such ambiguity.\n\nUnlike jets of functions from a manifold to another manifold, the space of jets of sections at \"p\" carries the structure of a vector space inherited from the vector space structure on the sections themselves. As \"p\" varies over \"M\", the jet spaces formula_118 form a vector bundle over \"M\", the \"k\"-th-order jet bundle of \"E\", denoted by \"J\"(\"E\").\n\n\nSee the coordinate independent description of a differential operator.\n\n\n"}
{"id": "31104438", "url": "https://en.wikipedia.org/wiki?curid=31104438", "title": "K-tree", "text": "K-tree\n\nIn graph theory, a \"k\"-tree is an undirected graph formed by starting with a (\"k\" + 1)-vertex complete graph and then repeatedly adding vertices in such a way that each added vertex \"v\" has exactly \"k\" neighbors \"U\" such that, together, the \"k\" + 1 vertices formed by \"v\" and \"U\" form a clique.\n\nThe \"k\"-trees are exactly the maximal graphs with a given treewidth, graphs to which no more edges can be added without increasing their treewidth.\nThey are also exactly the \nchordal graphs all of whose maximal cliques are the same size \"k\" + 1 and all of whose minimal clique separators are also all the same size \"k\".\n\n1-trees are the same as unrooted trees. 2-trees are maximal series-parallel graphs, and include also the maximal outerplanar graphs. Planar 3-trees are also known as Apollonian networks.\n\nThe graphs that have treewidth at most \"k\" are exactly the subgraphs of \"k\"-trees, and for this reason they are called partial \"k\"-trees.\n\nThe graphs formed by the edges and vertices of \"k\"-dimensional stacked polytopes, polytopes formed by starting from a simplex and then repeatedly gluing simplices onto the faces of the polytope, are \"k\"-trees when \"k\" ≥ 3. This gluing process mimics the construction of \"k\"-trees by adding vertices to a clique. A \"k\"-tree is the graph of a stacked polytope if and only if no three (\"k\" + 1)-vertex cliques have \"k\" vertices in common.\n"}
{"id": "55737449", "url": "https://en.wikipedia.org/wiki?curid=55737449", "title": "Laura Guggenbühl", "text": "Laura Guggenbühl\n\nLaura Guggenbühl (November 18, 1901 – March 8, 1985) was an American mathematician, one of the earliest women in the U.S. to earn a Ph.D. in mathematics, known for her work in triangle geometry and the history of mathematics.\n\nGuggenbühl was born in New York City, to a family of Swiss immigrants; her father, a butcher and baker, died by 1920. She graduated from Hunter College in 1922 with a bachelor's degree in mathematics, after also taking some classes at Columbia University and New York University. She became an instructor at Hunter College while earning a master's degree and Ph.D. from Bryn Mawr College in 1924 and 1926 respectively. Her dissertation, supervised by Anna Johnson Pell Wheeler, was \"An Integral Equation with an Associated Integral Condition\". She became a regular-rank faculty member at Hunter College in 1932 and retired from there as an associate professor in 1972.\n\nAlthough not active in research mathematics after her doctorate, Guggenbühl represented Hunter College at many offerings of the International Congress of Mathematicians. She\nalso published several works on the history of mathematics, including biographies of Henri Brocard and Karl Wilhelm Feuerbach, on elementary geometry including triangle geometry, and on the Rhind mathematical papyrus.\n\nShe died on a round-the-world cruise, shortly after leaving Hong Kong, after being overcome with grief at the recent death of her brother.\n\n"}
{"id": "1882219", "url": "https://en.wikipedia.org/wiki?curid=1882219", "title": "List of algebraic coding theory topics", "text": "List of algebraic coding theory topics\n\nThis is a list of algebraic coding theory topics.\n"}
{"id": "12524069", "url": "https://en.wikipedia.org/wiki?curid=12524069", "title": "List of disproved mathematical ideas", "text": "List of disproved mathematical ideas\n\nIn mathematics, ideas are supposedly not accepted as fact until they have been rigorously proved. However, there have been some ideas that were fairly accepted in the past but which were subsequently shown to be false. This article is meant to serve as a repository for compiling a list of such ideas.\n\n\n"}
{"id": "53230626", "url": "https://en.wikipedia.org/wiki?curid=53230626", "title": "Mask generation function", "text": "Mask generation function\n\nA mask generation function (MGF) is a cryptographic primitive similar to a cryptographic hash function except that while a hash function's output is a fixed size, a MGF supports output of a variable length. In this respect, a MGF can be viewed as a single-use sponge function: it can absorb any length of input and process it to produce any length of output. Mask generation functions are completely deterministic: for any given input and desired output length the output is always the same.\n\nA mask generation function takes an octet string of variable length and a desired output length as input, and outputs an octet string of the desired length. There may be restrictions on the length of the input and output octet strings, but such bounds are generally very large. Mask generation functions are deterministic; the octet string output is completely determined by the input octet string. The output of a mask generation function should be pseudorandom, that is, if the seed to the function is unknown, it should be infeasible to distinguish the output from a truly random string.\n\nMask generation functions, as generalizations of hash functions, are useful wherever hash functions are. However, use of a MGF is desirable in cases where a fixed-size hash would be inadequate. Examples include generating padding, producing one time pads or keystreams in symmetric key encryption, and yielding outputs for pseudorandom number generators.\n\nMask generation functions were first proposed as part of the specification for padding in the RSA-OAEP algorithm. The OAEP algorithm required a cryptographic hash function that could generate an output equal in size to a \"data block\" whose length was proportional to arbitrarily sized input message.\n\nThe Salsa20 stream cipher may be viewed as a mask generation function as its keystream is produced by hashing the key and nonce with a counter, to yield an arbitrarily long output.\nSalsa20 generates the stream in 64-byte (512-bit) blocks. Each block is an independent hash of the key, the nonce, and a 64-bit block number; there is no chaining from one block to the next. The Salsa20 output stream can therefore be accessed randomly, and any number of blocks can be computed in parallel.\n\nNIST Special Publication 800-90A defines a class of cryptographically secure random number generators, one of which is the \"Hash DRBG\", which uses a hash function with a counter to produce a requested sequence of random bits equal in size to the requested number of random bits.\n\nPerhaps the most common and straightforward mechanism to build a MGF is to iteratively apply a hash function together with an incrementing counter value. The counter may be incremented indefinitely to yield new output blocks until a sufficient amount of output is collected. This is the approach used in MGF1.\n\nMGF1 is a mask generation function defined in the Public Key Cryptography Standard #1 published by RSA Laboratories:\n\nformula_1\n\nformula_3\nformula_4\n\nformula_6\n\n}\\right\\rceil-1</math>, do the following:\n\nBelow is python code implementing MGF1:\n\nExample outputs of MGF1:\n"}
{"id": "47253597", "url": "https://en.wikipedia.org/wiki?curid=47253597", "title": "Nikolai V. Ivanov", "text": "Nikolai V. Ivanov\n\nNikolai V. Ivanov (in Russian: \"Николай В. Иванов\", born in 1954) is a Russian mathematician who works on topology, geometry and group theory (particularly, modular Teichmüller groups). He is a professor at Michigan State University.\n\nHe obtained his Ph.D. under the guidance of Vladimir Abramovich Rokhlin in 1980 at the Steklov Mathematical Institute.\n\nAccording to Google Scholar, on 11 March 2018, Ivanov's works had received 2,234 citations and his h-index was 23.\n\nHe is a fellow of the American Mathematical Society since 2012.\n\nHe is the author of the book \"Subgroups of Teichmüller Modular Groups\".\n\nAmong his contributions to mathematics are his classification of subgroups of surface mapping class groups, and the establishment that surface mapping class groups satisfy the Tits alternative.\n\n\n"}
{"id": "1453977", "url": "https://en.wikipedia.org/wiki?curid=1453977", "title": "Null vector", "text": "Null vector\n\nIn mathematics, given a vector space \"X\" with an associated quadratic form \"q\", written , a null vector or isotropic vector is a non-zero element \"x\" of \"X\" for which .\n\nIn the theory of real bilinear forms, definite quadratic forms and isotropic quadratic forms are distinct. They are distinguished in that only for the latter there exists a nonzero null vector. Where such a vector exists, is called a pseudo-Euclidean space.\n\nA pseudo-Euclidean vector space may be decomposed (non-uniquely) into orthogonal subspaces \"A\" and \"B\", , where \"q\" is positive-definite on \"A\" and negative-definite on \"B\". The null cone, or isotropic cone, of \"X\" consists of the union of balanced spheres:\nThe null cone is also the union of the isotropic lines through the origin.\n\nThe light-like vectors of Minkowski space are null vectors.\n\nThe four linearly independent biquaternions , , , and are null vectors and can serve as a basis for the subspace used to represent spacetime. Null vectors are also used in the Newman–Penrose formalism approach to spacetime manifolds.\n\nA composition algebra \"splits\" when it has a null vector; otherwise it is a division algebra.\n\nIn the Verma module of a Lie algebra there are null vectors.\n\n"}
{"id": "28357868", "url": "https://en.wikipedia.org/wiki?curid=28357868", "title": "Remarks on the Foundations of Mathematics", "text": "Remarks on the Foundations of Mathematics\n\nRemarks on the Foundations of Mathematics () is a book of Ludwig Wittgenstein's notes on the philosophy of mathematics. It has been translated from German to English by G.E.M. Anscombe, edited by G.H. von Wright and Rush Rhees, and published first in 1956. The text has been \nproduced from passages in various sources by selection and editing. The notes have been written during the years 1937-1944 and a few passages are incorporated in the Philosophical Investigations which were composed later. When the book appeared it received many negative reviews mostly from working logicians and mathematicians, among them Michael Dummett, Paul Bernays, and Georg Kreisel. Today \"Remarks on the Foundations of Mathematics\" is read mostly by philosophers sympathetic to Wittgenstein and they tend to adopt a more positive stance.\n\nWittgenstein's philosophy of mathematics is exposed chiefly by simple examples on which further skeptical comments are made. The text offers an extended analysis of the concept of mathematical proof and an exploration of Wittgenstein's contention that philosophical considerations introduce false problems in mathematics. Wittgenstein in the Remarks adopts an attitude of doubt in opposition to much orthodoxy in the philosophy of mathematics.\n\nParticularly controversial in the Remarks was Wittgenstein's \"notorious paragraph\", which contained an unusual commentary on Gödel's incompleteness theorems. Multiple commentators read Wittgenstein as misunderstanding Gödel. In 2000 Juliet Floyd and Hilary Putnam suggested that the majority of commentary misunderstands \nWittgenstein but their interpretation has not been met with approval.\n\nWittgenstein wrote \n\nThe debate has been running around the so-called \"Key Claim\": If one assumes that P is provable in PM, then one should give up the “translation” of P by the English sentence “P is not provable”.\n\nWittgenstein does not mention the name of Kurt Gödel who was a member of the Vienna Circle during the period in which Wittgenstein's early ideal language philosophy and \"Tractatus Logico-Philosophicus\" dominated the circle's thinking; multiple writings of Gödel in his Nachlass contain his own antipathy for Wittgenstein, and belief that Wittgenstein wilfully misread the theorems. Some commentators, such as Rebecca Goldstein, have hypothesized that Gödel developed his logical theorems in opposition to Wittgenstein.\n\n"}
{"id": "43292203", "url": "https://en.wikipedia.org/wiki?curid=43292203", "title": "Retrial queue", "text": "Retrial queue\n\nIn queueing theory, a discipline within the mathematical theory of probability, a retrial queue is a model of a system with finite capacity, where jobs which arrive and find the system busy wait for some time before trying again to enter the system. Examples of such systems include making restaurant reservations and packet switching networks.\n"}
{"id": "9070151", "url": "https://en.wikipedia.org/wiki?curid=9070151", "title": "Robert Connelly", "text": "Robert Connelly\n\nRobert (Bob) Connelly is a mathematician specializing in discrete geometry and rigidity theory. Connelly received his Ph.D. from University of Michigan in 1969.\nHe is currently a professor at Cornell University.\n\nConnelly is best known for discovering embedded flexible polyhedra. One such polyhedron is in the National Museum of American History. His recent interests include tensegrities and the carpenter's rule problem. In 2012 he became a fellow of the American Mathematical Society.\n\nThe asteroid 4816 (Connelly) is named after Robert Connelly.\n\n"}
{"id": "17133396", "url": "https://en.wikipedia.org/wiki?curid=17133396", "title": "Rutherford Aris", "text": "Rutherford Aris\n\nRutherford \"Gus\" Aris (September 15, 1929 – November 2, 2005) was a chemical engineer, control theorist, applied mathematician, and a Regents Professor Emeritus of Chemical Engineering at the University of Minnesota (1958–2005).\n\nAris was born in Bournemouth, England, to Algernon Aris and Janet (Elford). From a young age, Aris was interested in chemistry. Aris's father owned a photo-finishing works, where he would experiment with chemicals and reactions. He attended St Martin's, a small local kindergarten and moved to St Wulfran's, a local preparatory school, now Queen Elizabeth's School. Here, he studied Latin, a skill much used later by him and he was encouraged to continue pursuit of his interest in chemistry. Because of his achievements, he was referred to the Reverend C.B.Canning, Headmaster of Canford School, a well-known public school, close to Wimborne. On the strength of this interview, he was given a place in the newly created house that the school had provided for day-boarders. This was in 1943, when he was 14. His mathematics teacher, H. E. Piggott, had a particular influence on Aris due to \"the liveliness, enthusiasm, and care that he brought to his teaching\", which \"were unparalleled in my experience\". Piggot spent substantial time on pure and applied mathematical papers, an experience that Aris described as \"extraordinary\". Aris dedicated his book \"Discrete Dynamic Programming\" to Piggot 15 years later.\n\nPiggot helped Aris to get a job working for Imperial Chemical Industries (ICI) as a laboratory technician in the Mechanical Engineering Department of the Research Labs, at the age of 17. While working at ICI, Aris attended the University of London part-time to work toward his B.Sc.. Aris described this as \"an excellent way to get a degree, although perhaps not so good a way of getting an education.\" After 2 years Aris made an attempt to earn the B.Sc. Honours Degree. He sat 12 papers (exams) covering a wide range of mathematical topics, and got a degree with first-class honours.\n\nIn 1948, ICI sent him to Edinburgh, Scotland for two years of study at the Mathematical Institute at the University of Edinburgh, which was presided over by Alexander Aitken. Aris, who was accepted for post-graduate studies but not for a Ph.D., did post-graduate work at the University under the supervision of John Cossar. During this break from ICI, Aris also registered for a University of London M.Sc. in the area of mathematical analysis. When he sat the papers, however, he failed to get the degree.\n\nIn 1950, Aris returned to ICI and began working for C. H. Bosanquet in Billingham, England. Working with Bosanquet provided Aris the opportunity to work on a large variety of problems, including catalysis, heat transfer, gas scrubbing, and centrifuge design.\n\nAris was then promoted to Technical Officer, where he began working on chromatography. He utilized results from a paper on dispersion written by Geoffrey Taylor, and extended its results, ultimately writing a paper in 1955 that applied the method of moments to Taylor's approach. He submitted the paper to the \"Proceedings of the Royal Society\", with help from Taylor (who was a Fellow of the Royal Society). Aris communicated with Taylor regarding dispersion and diffusion. In the meantime, however, he was transferred to a different division, where he began working on chemical reactor design. Frustrated with the transfer and with the proprietary nature of his commercial work, which made publishing work very difficult, he decided to move to a university, applying for several lectureship positions during 1954 and 1955 without success. Aris continued to work at ICI, focusing much of his efforts on mathematical modeling of adiabatic multi-bed reactors, a topic that was the central focus of an M.S. student at the University of Minnesota. In 1955, Neal Amundson of the University of Minnesota, who was on sabbatical at Cambridge, visited the ICI Research Department, where Aris was working. Amundson suggested to ICI, during his visit, that Aris be sent to the University of Minnesota in Minneapolis for a year of study. Several months later, Aris later met Amundson at Cambridge and told Amundson of his plans to leave ICI for academia, plans that he had not revealed to his superiors at ICI. Amundson offered Aris a research fellowship at the University of Minnesota, which Aris accepted. After notifying ICI of his intent to leave, he moved to Minneapolis, Minnesota at the end of 1955.\n\nAris began working on chemically reacting laminar flow, applying Kummer's hypergeometric function to the problem, and control of a stirred tank reactor with some unusual properties. Both problems required the use of a computer to perform calculations, and Amundson provided Aris with a computer science graduate student with whom to work. Aris's research fellowship was extended for a second year, but shortly afterward, in October 1956, Aris was informed of a lectureship opening at the University of Edinburgh. He took advantage of the opportunity, and left immediately for Edinburgh.\n\nAris was on the faculty of the University of Edinburgh for two years, 1956–1958. While at Edinburgh, Aris wrote papers on his work at the University of Minnesota and at ICI. Having the lectureship position allowed Aris to gain experience lecturing to students. He also attended the lectures of, and interacted with, the chair of chemical technology at the University of Edinburgh, Kenneth Denbigh, who was a well-known thermodynamicist and an editor of the journal \"Chemical Engineering Science\".\n\nAris returned to Minneapolis in the summer of 1957 to continue his work on the stirred tank reactor problem. In August he became engaged to Claire Holman, and when he informed Amundson, Amundson offered him a faculty position at the University. Aris accepted the job, and began working as an assistant professor at the University of Minnesota in 1958.\n\nAris had not formally received a Ph.D., but had registered three years earlier with the University of London, where he had earned his B.Sc., and which offered Ph.D. degrees by correspondence. A Ph.D. degree could be earned without following a strict preparation process; the individual needed to propose a research program after 3 years, select a committee of examiners, and submit a dissertation, and after an oral examination by and approval from the committee, the degree would be granted. Amundson had suggested Aris look into Richard Bellman's method of dynamic programming for his dissertation. Amundson informally served as Aris's advisor, and Aris completed his dissertation on the topic in 1960.\n\nHis dissertation was published by the Academic Press in a series of which Bellman was the editor, and Bellman took note of the dissertation. Aris and Amundson visited Bellman at the Rand Corporation, where Bellman was working on economic models. The dynamic programming method had originally been developed for economics, but Bellman was attracted by applications in engineering, and the meeting led to a joint collaboration and a publication.\n\nAris's research at the University of Minnesota focused on optimization, dynamic programming, control theory, Taylor diffusion, and computing engines. Aris also taught a graduate fluid mechanics course, and eventually wrote the book \"Vectors, tensors, and the basic equations of fluid mechanics\" in an effort to make the rational mechanics approach of Truesdell, Coleman, and others more accessible to students.\n\nAfter he had been with the department for six years, Aris took a sabbatical at the Shell Department of Chemical Engineering at the University of Cambridge during the 1964–1965 academic year, where he was able to interact with many well-known engineers and mathematicians such as Geoffrey Taylor and John Littlewood. He also lectured in many places in Europe, including Brussels, Copenhagen, and Trondheim.\n\nAris took a second sabbatical after 6 years, again going to the University of Cambridge, during the 1971–1972 academic year. He spent his time writing a monograph on mathematical models for porous catalysts, which he did not finish until 1973. During his sabbatical, he received financial support in the form of a Guggenheim grant. This also provided Aris an opportunity to participate in the board overseeing the formation and development of Los Alamos National Lab's Center for Nonlinear Studies, which allowed Aris the opportunity to travel to Los Alamos during the 1970s and 1980s.\n\nIn 1974, Neal Amundson, who had been the department chairman of the University of Minnesota's Chemical Engineering department for nearly 25 years, resigned from this position. Aris was appointed acting head of the department, while Amundson left Minnesota for the University of Houston. Aris acted as department chair for 4 years, and was relieved of the position in 1978. Coinciding with this was an offer from Princeton University to join the faculty there, as well as the offer to stay at the University of Minnesota and work half-time in the chemical engineering department and half-time in the paleography department. Aris decided to stay at the University of Minnesota.\n\nIn addition to his interest in chemical engineering, Aris was also interested in the humanities. At the University of Minnesota, Aris was able to pursue his interest in paleography when he was granted a professorship in the Classics Department, where he taught classes and published books and research articles. Aris published his book \"Explicatio Formarum Literarum\", or \"The Unfolding of Letterforms\", which covered the history of written letters from the 1st century to the 15th century.\n\nAris had several other sabbaticals over his 40-year career. Through the Fairchild Distinguished Scholar program at the California Institute of Technology, Aris was able to spend a portion of 1977 and a year in 1980–1981 on sabbatical in Pasadena, California. He dedicated a portion of his time to paleography, utilizing the nearby Huntington Library. Additionally, through a personal connection at the University of Leeds, Aris was able to spend several weeks there as Brotherton Professor in 1985. Aris spent his last sabbatical, from 1993–1994, at the Institute for Advanced Study at Princeton.\n\nAris penned many poems and anecdotes, many relating his difficulties with Parkinson's disease, from which Aris eventually died. Aris died on November 2, 2005, in Edina, Minnesota.\n\nOver the course of his long academic career, Aris was a visiting professor at many institutions, including Cambridge University, the California Institute of Technology, and Princeton University; he authored 13 books and more than 300 chemical engineering research articles, and mentored 48 Ph.D. and 20 M.S. graduate students. Aris was well known for his research on mathematical modeling, chemical reactor and chemical process design, and distillation techniques, as well as his paleographic research.\n\nAfter he had been department head for four years, in 1978 he was named Regents Professor. Some of the awards and honors earned by Aris include a Guggenheim Fellowship, election to the National Academy of Engineering in 1975 and the American Academy of Arts and Sciences in 1988. Aris was also a member of the American Chemical Society, the Society for Mathematical Biology, and the Society of Scribes and Illuminators, among others. Aris was awarded the Richard E. Bellman Control Heritage Award in 1992 for his contributions to the field of control theory. He was awarded the Neal R. Amundson Award for Excellence in Chemical Reaction Engineering by the International Symposia on Chemical Reaction Engineering in 1998.\n\nIn 2016, the board of the ISCRE (International Symposia on Chemical Reaction Engineering) established the Rutherford Aris Young Investigator Award for Excellence in Chemical Reaction Engineering. This award honors young researchers under the age of 40 to recognize outstanding contributions in experimental and/or theoretical reaction engineering research.\n\nThe following is a selected bibliography for Rutherford Aris.\n\n\n"}
{"id": "769434", "url": "https://en.wikipedia.org/wiki?curid=769434", "title": "Setoid", "text": "Setoid\n\nIn mathematics, a setoid (\"X\", ~) is a set (or type) \"X\" equipped with an equivalence relation ~. A Setoid may also be called E-set, Bishop set, or extensional set.\n\nSetoids are studied especially in proof theory and in type-theoretic foundations of mathematics. Often in mathematics, when one defines an equivalence relation on a set, one immediately forms the quotient set (turning equivalence into equality). In contrast, setoids may be used when a difference between identity and equivalence must be maintained, often with an interpretation of intensional equality (the equality on the original set) and extensional equality (the equivalence relation, or the equality on the quotient set).\n\nIn proof theory, particularly the proof theory of constructive mathematics based on the Curry–Howard correspondence, one often identifies a mathematical proposition with its set of proofs (if any). A given proposition may have many proofs, of course; according to the principle of proof irrelevance, normally only the truth of the proposition matters, not which proof was used. However, the Curry–Howard correspondence can turn proofs into algorithms, and differences between algorithms are often important. So proof theorists may prefer to identify a proposition with a \"setoid\" of proofs, considering proofs equivalent if they can be converted into one another through beta conversion or the like.\n\nIn type-theoretic foundations of mathematics, setoids may be used in a type theory that lacks quotient types to model general mathematical sets. For example, in Per Martin-Löf's intuitionistic type theory, there is no type of real numbers, only a type of regular Cauchy sequences of rational numbers. To do real analysis in Martin-Löf's framework, therefore, one must work with a \"setoid\" of real numbers, the type of regular Cauchy sequences equipped with the usual notion of equivalence. Predicates and functions of real numbers need to be defined for regular Cauchy sequences and proven to be compatible with the equivalence relation. Typically (although it depends on the type theory used), the axiom of choice will hold for functions between types (intensional functions), but not for functions between setoids (extensional functions). The term \"set\" is variously used either as a synonym of \"type\" or as a synonym of \"setoid\".\n\nIn constructive mathematics, one often takes a setoid with an apartness relation instead of an equivalence relation, called a constructive setoid. One sometimes also considers a partial setoid using a partial equivalence relation or partial apartness. (see e.g. Barthe \"et al.\", section 1)\n\n\n\n"}
{"id": "7490775", "url": "https://en.wikipedia.org/wiki?curid=7490775", "title": "Shaping codes", "text": "Shaping codes\n\nIn digital communications shaping codes are a method of encoding that changes the distribution of signals to improve efficiency.\n\nTypical digital communication systems uses M-Quadrature Amplitude Modulation(QAM) to communicate through an analog channel (specifically a channel with Gaussian noise). For Higher bit rates(M) the minimum Signal to Noise ratio (SNR) required by a QAM system with Error Correcting Codes is about 1.53 dB higher than minimum SNR required by a Gaussian source(>30% more transmitter power) as given in Shannon–Hartley theorem\n\nwhere\n\nThis 1.53 dB difference is called the \"shaping gap\". Typically digital system will encode bits with uniform probability to maximize the entropy. Shaping code act as buffer between digital sources and modulator communication system. They will receive uniformly distributed data and convert it to Gaussian like distribution before presenting to the modulator. Shaping codes are helpful in reducing transmit power and thus reduce the cost of Power amplifier and the interference caused to other users in the vicinity.\n\nSome of the methods used for shaping are described in the trellis shaping paper by Dr. G. D. Forney Jr.\n\nShell mapping is used in V.34 modems to get a shaping gain of .8 dB.\nAll the shaping schemes in the literature try to reduce the transmitted signal power. In future this may have find application in wireless networks where the interference from other nodes are becoming the major issue.\n\n"}
{"id": "12637991", "url": "https://en.wikipedia.org/wiki?curid=12637991", "title": "Sloan Research Fellowship", "text": "Sloan Research Fellowship\n\nThe Sloan Research Fellowships are awarded annually by the Alfred P. Sloan Foundation since 1955 to \"provide support and recognition to early-career scientists and scholars\".\n\nFellowships were initially awarded in physics, chemistry, and mathematics. Awards were later added in neuroscience (1972), economics (1980), computer science (1993), and computational and evolutionary molecular biology (2002). These two-year fellowships are awarded to 126 researchers yearly. \n\nThe foundation has been supportive of scientists who are parents by allowing them extra time after their doctorate during which they remain eligible for the award:\n\n\"Candidates for Sloan Research Fellowships are required to hold the Ph.D. (or equivalent) in chemistry, physics, mathematics, computer science, economics, neuroscience or computational and evolutionary molecular biology, or in a related interdisciplinary field, and must be members of the regular faculty (i.e., tenure track) of a college or university in the United States or Canada. They may be no more than six years from completion of the most recent Ph.D. or equivalent as of the year of their nomination, unless special circumstances such as military service, a change of field, or child rearing are involved or unless they have held a faculty appointment for less than two years. If any of the above circumstances apply, the letter of nomination (see below) should provide a clear explanation. While Fellows are expected to be at an early stage of their research careers, there should be strong evidence of independent research accomplishments. Candidates in all fields are normally below the rank of associate professor and do not hold tenure, but these are not strict requirements. The Alfred P. Sloan Foundation welcomes nominations of all candidates who meet the traditional high standards of the program, and strongly encourages the participation of women and members of underrepresented minority groups.\"\n\nSince the beginning of the program in 1955, 43 fellows have won a Nobel Prize, and 16 have won the Fields Medal in mathematics.\n\n"}
{"id": "454959", "url": "https://en.wikipedia.org/wiki?curid=454959", "title": "State (functional analysis)", "text": "State (functional analysis)\n\nIn functional analysis, a state of an operator system is a positive linear functional of norm 1. States in functional analysis generalize the notion of density matrices in quantum mechanics, which represent quantum states, both . Density matrices in turn generalize state vectors, which only represent pure states. For \"M\" an operator system in a C*-algebra \"A\" with identity, the set of all states of\" \"M, sometimes denoted by S(\"M\"), is convex, weak-* closed in the Banach dual space \"M\". Thus the set of all states of \"M\" with the weak-* topology forms a compact Hausdorff space, known as the state space of \"M\" .\n\nIn the C*-algebraic formulation of quantum mechanics, states in this previous sense correspond to physical states, i.e. mappings from physical observables (self-adjoint elements of the C*-algebra) to their expected measurement outcome (real number).\n\nStates can be viewed as noncommutative generalizations of probability measures. By Gelfand representation, every commutative C*-algebra \"A\" is of the form \"C\"(\"X\") for some locally compact Hausdorff \"X\". In this case, \"S\"(\"A\") consists of positive Radon measures on \"X\", and the are the evaluation functionals on \"X\".\n\nMore generally, the GNS construction shows that every state is, after choosing a suitable representation, a vector state.\n\nA bounded linear functional on a C*-algebra \"A\" is said to be self-adjoint if it is real-valued on the self-adjoint elements of \"A\". Self-adjoint functionals are noncommutative analogues of signed measures.\n\nThe Jordan decomposition in measure theory says that every signed measure can be expressed as the difference of two positive measures supported on disjoint sets. This can be extended to the noncommutative setting.\n\nA proof can be sketched as follows: Let Ω be the weak*-compact set of positive linear functionals on \"A\" with norm ≤ 1, and \"C\"(Ω) be the continuous functions on Ω. \"A\" can be viewed as a closed linear subspace of \"C\"(Ω) (this is \"Kadison's function representation\"). By Hahn–Banach, \"f\" extends to a \"g\" in \"C\"(Ω)* with ||g|| = ||f||.\n\nUsing results from measure theory quoted above, one has\n\nwhere, by the self-adjointness of \"f\", \"μ\" can be taken to be a signed measure. Write\n\na difference of positive measures. The restrictions of the functionals ∫ · d\"μ\" and ∫ · d\"μ\" to \"A\" has the required properties of \"f\" and \"f\". This proves the theorem.\n\nIt follows from the above decomposition that \"A*\" is the linear span of states.\n\nBy the Krein-Milman theorem, the state space of \"M\" has extreme points. The extreme points of the state space are termed pure states and other states are known as mixed states.\n\nFor a Hilbert space \"H\" and a vector \"x\" in \"H\", the equation ω(\"A\") := ⟨\"Ax\",\"x\"⟩ (for \"A\" in \"B(H)\" ), defines a positive linear functional on \"B(H)\". Since ω(\"1\")=||\"x\"||, ω is a state if ||\"x\"||=1. If \"A\" is a C*-subalgebra of \"B(H)\" and \"M\" an operator system in \"A\", then the restriction of ω to \"M\" defines a positive linear functional on \"M\". The states of \"M\" that arise in this manner, from unit vectors in \"H\", are termed vector states of \"M\".\n\nA state formula_3 is called normal, iff for every monotone, increasing net formula_4 of operators with upper bound formula_5, formula_6 converges to formula_7.\n\nA tracial state is a state formula_3 such that\n\nFor any separable C*-algebra, the set of tracial states is a Choquet simplex.\n\nA factorial state of a C*-algebra \"A\" is a state such that the commutant of the corresponding GNS representation of \"A\" is a factor.\n\n"}
{"id": "31075945", "url": "https://en.wikipedia.org/wiki?curid=31075945", "title": "Thomas Parkinson (priest)", "text": "Thomas Parkinson (priest)\n\nThomas Parkinson (1744 or 1745 – 13 November 1830) was an English clergyman.\n\nHe was born in Kirkham, Lancashire.\n\nHe entered Christ's College, Cambridge University in 1764 at age 19 and was senior wrangler and 2nd Smith's prizeman in 1769. He received an M.A. in 1772, a B.D. in 1789, and a D.D. in 1795.\n\nHe was Rector of Kegworth, Leicestershire, from 1789 until his death. He became Archdeacon of Huntingdon from 1794 to 1812 and Archdeacon of Leicester from 1812 until his death in 1830.\n\nHe was the author of \"A System of Mechanics and Hydrostatics\" and was elected a Fellow of the Royal Society in 1786.\n\nHe died in Kegworth in 1830.\n\n"}
{"id": "3054853", "url": "https://en.wikipedia.org/wiki?curid=3054853", "title": "Three-dimensional space", "text": "Three-dimensional space\n\nThree-dimensional space (also: 3-space or, rarely, tri-dimensional space) is a geometric setting in which three values (called parameters) are required to determine the position of an element (i.e., point). This is the informal meaning of the term dimension.\n\nIn physics and mathematics, a sequence of numbers can be understood as a location in -dimensional space. When , the set of all such locations is called three-dimensional Euclidean space. It is commonly represented by the symbol . This serves as a three-parameter model of the physical universe (that is, the spatial part, without considering time) in which all known matter exists. However, this space is only one example of a large variety of spaces in three dimensions called 3-manifolds. In this classical example, when the three values refer to measurements in different directions (coordinates), any three directions can be chosen, provided that vectors in these directions do not all lie in the same 2-space (plane). Furthermore, in this case, these three values can be labeled by any combination of three chosen from the terms \"width\", \"height\", \"depth\", and \"length\".\n\nIn mathematics, analytic geometry (also called Cartesian geometry) describes every point in three-dimensional space by means of three coordinates. Three coordinate axes are given, each perpendicular to the other two at the origin, the point at which they cross. They are usually labeled , and . Relative to these axes, the position of any point in three-dimensional space is given by an ordered triple of real numbers, each number giving the distance of that point from the origin measured along the given axis, which is equal to the distance of that point from the plane determined by the other two axes.\n\nOther popular methods of describing the location of a point in three-dimensional space include cylindrical coordinates and spherical coordinates, though there are an infinite number of possible methods. See Euclidean space.\n\nBelow are images of the above-mentioned systems.\n\nTwo distinct points always determine a (straight) line. Three distinct points are either collinear or determine a unique plane. Four distinct points can either be collinear, coplanar or determine the entire space.\n\nTwo distinct lines can either intersect, be parallel or be skew. Two parallel lines, or two intersecting lines, lie in a unique plane, so skew lines are lines that do not meet and do not lie in a common plane.\n\nTwo distinct planes can either meet in a common line or are parallel (do not meet). Three distinct planes, no pair of which are parallel, can either meet in a common line, meet in a unique common point or have no point in common. In the last case, the three lines of intersection of each pair of planes are mutually parallel.\n\nA line can lie in a given plane, intersect that plane in a unique point or be parallel to the plane. In the last case, there will be lines in the plane that are parallel to the given line.\n\nA hyperplane is a subspace of one dimension less than the dimension of the full space. The hyperplanes of a three-dimensional space are the two-dimensional subspaces, that is, the planes. In terms of cartesian coordinates, the points of a hyperplane satisfy a single linear equation, so planes in this 3-space are described by linear equations. A line can be described by a pair of independent linear equations, each representing a plane having this line as a common intersection.\n\nVarignon's theorem states that the midpoints of any quadrilateral in ℝ form a parallelogram, and so, are coplanar.\n\nA sphere in 3-space (also called a 2-sphere because it is a 2-dimensional object) consists of the set of all points in 3-space at a fixed distance from a central point . The solid enclosed by the sphere is called a ball (or, more precisely a 3-ball). The volume of the ball is given by\n\nAnother type of sphere arises from a 4-ball, whose three-dimensional surface is the 3-sphere: points equidistant to the origin of the euclidean space . If a point has coordinates, , then characterizes those points on the unit 3-sphere centered at the origin.\n\nIn three dimensions, there are nine regular polytopes: the five convex Platonic solids and the four nonconvex Kepler-Poinsot polyhedra.\n\nA surface generated by revolving a plane curve about a fixed line in its plane as an axis is called a surface of revolution. The plane curve is called the \"generatrix\" of the surface. A section of the surface, made by intersecting the surface with a plane that is perpendicular (orthogonal) to the axis, is a circle.\n\nSimple examples occur when the generatrix is a line. If the generatrix line intersects the axis line, the surface of revolution is a right circular cone with vertex (apex) the point of intersection. However, if the generatrix and axis are parallel, the surface of revolution is a circular cylinder.\n\nIn analogy with the conic sections, the set of points whose cartesian coordinates satisfy the general equation of the second degree, namely,\nwhere and are real numbers and not all of and are zero is called a quadric surface.\n\nThere are six types of non-degenerate quadric surfaces:\n\nThe degenerate quadric surfaces are the empty set, a single point, a single line, a single plane, a pair of planes or a quadratic cylinder (a surface consisting of a non-degenerate conic section in a plane and all the lines of through that conic that are normal to ). Elliptic cones are sometimes considered to be degenerate quadric surfaces as well.\n\nBoth the hyperboloid of one sheet and the hyperbolic paraboloid are ruled surfaces, meaning that they can be made up from a family of straight lines. In fact, each has two families of generating lines, the members of each family are disjoint and each member one family intersects, with just one exception, every member of the other family. Each family is called a regulus.\n\nAnother way of viewing three-dimensional space is found in linear algebra, where the idea of independence is crucial. Space has three dimensions because the length of a box is independent of its width or breadth. In the technical language of linear algebra, space is three-dimensional because every point in space can be described by a linear combination of three independent vectors.\n\nA vector can be pictured as an arrow. The vector's magnitude is its length, and its direction is the direction the arrow points. A vector in can be represented by an ordered triple of real numbers. These numbers are called the components of the vector.\n\nThe dot product of two vectors and is defined as:\n\nThe magnitude of a vector is denoted by . The dot product of a vector with itself is\nwhich gives\nthe formula for the Euclidean length of the vector.\n\nWithout reference to the components of the vectors, the dot product of two non-zero Euclidean vectors and is given by\nwhere is the angle between and .\n\nThe cross product or vector product is a binary operation on two vectors in three-dimensional space and is denoted by the symbol ×. The cross product a × b of the vectors a and b is a vector that is perpendicular to both and therefore normal to the plane containing them. It has many applications in mathematics, physics, and engineering.\n\nThe space and product form an algebra over a field, which is neither commutative nor associative, but is a Lie algebra with the cross product being the Lie bracket.\n\nOne can in \"n\" dimensions take the product of vectors to produce a vector perpendicular to all of them. But if the product is limited to non-trivial binary products with vector results, it exists only in three and seven dimensions.\nIn a rectangular coordinate system, the gradient is given by\n\nThe divergence of a continuously differentiable vector field F = \"U\" i + \"V\" j + \"W\" k is equal to the scalar-valued function:\n\nExpanded in Cartesian coordinates (see Del in cylindrical and spherical coordinates for spherical and cylindrical coordinate representations), the curl ∇ × F is, for F composed of [\"F\", \"F\", \"F\"]:\n\nwhere i, j, and k are the unit vectors for the \"x\"-, \"y\"-, and \"z\"-axes, respectively. This expands as follows:\n\nFor some scalar field \"f\" : \"U\" ⊆ R → R, the line integral along a piecewise smooth curve \"C\" ⊂ \"U\" is defined as\nwhere r: [a, b] → \"C\" is an arbitrary bijective parametrization of the curve \"C\" such that r(\"a\") and r(\"b\") give the endpoints of \"C\" and formula_12.\n\nFor a vector field F : \"U\" ⊆ R → R, the line integral along a piecewise smooth curve \"C\" ⊂ \"U\", in the direction of r, is defined as\n\nwhere · is the dot product and r: [a, b] → \"C\" is a bijective parametrization of the curve \"C\" such that r(\"a\") and r(\"b\") give the endpoints of \"C\".\n\nA surface integral is a generalization of multiple integrals to integration over surfaces. It can be thought of as the double integral analog of the line integral. To find an explicit formula for the surface integral, we need to parameterize the surface of interest, \"S\", by considering a system of curvilinear coordinates on \"S\", like the latitude and longitude on a sphere. Let such a parameterization be x(\"s\", \"t\"), where (\"s\", \"t\") varies in some region \"T\" in the plane. Then, the surface integral is given by\n\nwhere the expression between bars on the right-hand side is the magnitude of the cross product of the partial derivatives of x(\"s\", \"t\"), and is known as the surface element. Given a vector field v on \"S\", that is a function that assigns to each x in \"S\" a vector v(x), the surface integral can be defined component-wise according to the definition of the surface integral of a scalar field; the result is a vector.\n\nA volume integral refers to an integral over a 3-dimensional domain.\n\nIt can also mean a triple integral within a region \"D\" in R of a function formula_15 and is usually written as:\n\nThe fundamental theorem of line integrals, says that a line integral through a gradient field can be evaluated by evaluating the original scalar field at the endpoints of the curve.\n\nLet formula_17. Then\n\nStokes' theorem relates the surface integral of the curl of a vector field F over a surface Σ in Euclidean three-space to the line integral of the vector field over its boundary ∂Σ:\n\nSuppose is a subset of formula_20 (in the case of represents a volume in 3D space) which is compact and has a piecewise smooth boundary (also indicated with ). If is a continuously differentiable vector field defined on a neighborhood of , then the divergence theorem says:\n\nThe left side is a volume integral over the volume , the right side is the surface integral over the boundary of the volume . The closed manifold is quite generally the boundary of oriented by outward-pointing normals, and is the outward pointing unit normal field of the boundary . ( may be used as a shorthand for .)\n\nThree-dimensional space has a number of topological properties that distinguish it from spaces of other dimension numbers. For example, at least three dimensions are required to tie a knot in a piece of string.\n\nWith the space formula_21, the topologists locally model all other 3-manifolds.\n\n\n"}
{"id": "159023", "url": "https://en.wikipedia.org/wiki?curid=159023", "title": "Tree decomposition", "text": "Tree decomposition\n\nIn graph theory, a tree decomposition is a mapping of a graph into a tree that can be used to define the treewidth of the graph and speed up solving certain computational problems on the graph.\n\nIn machine learning, tree decompositions are also called junction trees, clique trees, or join trees; they play an important role in problems like probabilistic inference, constraint satisfaction, query optimization, and matrix decomposition.\n\nThe concept of tree decompositions was originally introduced by . Later it was rediscovered by and has since been studied by many other authors.\n\nIntuitively, a tree decomposition represents the vertices of a given graph \"G\" as subtrees of a tree, in such a way that vertices in the given graph are adjacent only when the corresponding subtrees intersect. Thus, \"G\" forms a subgraph of the intersection graph of the subtrees. The full intersection graph is a chordal graph.\n\nEach subtree associates a graph vertex with a set of tree nodes. To define this formally, we represent each tree node as the set of vertices associated with it.\nThus, given a graph \"G\" = (\"V\", \"E\"), a tree decomposition is a pair (\"X\", \"T\"), where \"X\" = {\"X\", ..., \"X\"} is a family of subsets of \"V\", and \"T\" is a tree whose nodes are the subsets \"X\", satisfying the following properties:\n\n\nThe tree decomposition of a graph is far from unique; for example, a trivial tree decomposition contains all vertices of the graph in its single root node.\n\nA tree decomposition in which the underlying tree is a path graph is called a path decomposition, and the width parameter derived from these special types of tree decompositions is known as pathwidth.\n\nA tree decomposition (\"X\", \"T\" = (\"I\", \"F\")) of treewidth \"k\" is \"smooth\", if for all formula_8, and for all formula_9.\n\nThe minimum number of trees in a tree decomposition is the tree number of \"G.\"\n\nThe \"width\" of a tree decomposition is the size of its largest set \"X\" minus one. The treewidth tw(\"G\") of a graph \"G\" is the minimum width among all possible tree decompositions of \"G\". In this definition, the size of the largest set is diminished by one in order to make the treewidth of a tree equal to one. Treewidth may also be defined from other structures than tree decompositions, including chordal graphs, brambles, and havens.\n\nIt is NP-complete to determine whether a given graph \"G\" has treewidth at most a given variable \"k\".\nHowever, when \"k\" is any fixed constant, the graphs with treewidth \"k\" can be recognized, and a width \"k\" tree decomposition constructed for them, in linear time. The time dependence of this algorithm on \"k\" is exponential.\n\nAt the beginning of the 1970s, it was observed that a large class of combinatorial optimization problems defined on graphs could be efficiently solved by non-serial dynamic programming as long as the graph had a bounded \"dimension\", a parameter related to treewidth. Later, several authors independently observed, at the end of the 1980s, that many algorithmic problems that are NP-complete for arbitrary graphs may be solved efficiently by dynamic programming for graphs of bounded treewidth, using the tree-decompositions of these graphs.\n\nAs an example, consider the problem of finding the maximum independent set in a graph of treewidth \"k\". To solve this problem, first choose one of the nodes of the tree decomposition to be the root, arbitrarily. For a node \"X\" of the tree decomposition, let \"D\" be the union of the sets \"X\" descending from \"X\". For an independent set \"S\" ⊂ \"X\", let \"A\"(\"S\",\"i\") denote the size of the largest independent subset \"I\" of \"D\" such that \"I\" ∩ \"X\" = \"S\". Similarly, for an adjacent pair of nodes \"X\" and \"X\", with \"X\" farther from the root of the tree than \"X\", and an independent set \"S\" ⊂ \"X\" ∩ \"X\", let \"B\"(\"S\",\"i\",\"j\") denote the size of the largest independent subset \"I\" of \"D\" such that \"I\" ∩ \"X\" ∩ \"X\" = \"S\". We may calculate these \"A\" and \"B\" values by a bottom-up traversal of the tree:\nwhere the sum in the calculation of formula_12 is over the children of node formula_1.\n\nAt each node or edge, there are at most 2 sets \"S\" for which we need to calculate these values, so if \"k\" is a constant then the whole calculation takes constant time per edge or node. The size of the maximum independent set is the largest value stored at the root node, and the maximum independent set itself can be found (as is standard in dynamic programming algorithms) by backtracking through these stored values starting from this largest value. Thus, in graphs of bounded treewidth, the maximum independent set problem may be solved in linear time. Similar algorithms apply to many other graph problems.\n\nThis dynamic programming approach is used in machine learning via the junction tree algorithm for belief propagation in graphs of bounded treewidth. It also plays a key role in algorithms for computing the treewidth and constructing tree decompositions: typically, such algorithms have a first step that approximates the treewidth, constructing a tree decomposition with this approximate width, and then a second step that performs dynamic programming in the approximate tree decomposition to compute the exact value of the treewidth.\n\n\n"}
{"id": "36886801", "url": "https://en.wikipedia.org/wiki?curid=36886801", "title": "Vámos matroid", "text": "Vámos matroid\n\nIn mathematics, the Vámos matroid or Vámos cube is a matroid over a set of eight elements that cannot be represented as a matrix over any field. It is named after English mathematician Peter Vámos, who first described it in an unpublished manuscript in 1968.\n\nThe Vámos matroid has eight elements, which may be thought of as the eight vertices of a cube or cuboid. The matroid has rank 4: all sets of three or fewer elements are independent, and 65 of the 70 possible sets of four elements are also independent. The five exceptions are four-element circuits in the matroid. Four of these five circuits are formed by faces of the cuboid (omitting two opposite faces). The fifth circuit connects two opposite edges of the cuboid, each of which is shared by two of the chosen four faces.\n\nAnother way of describing the same structure is that it has two elements for each vertex of the diamond graph, and a four-element circuit for each edge of the diamond graph.\n\n"}
{"id": "2853246", "url": "https://en.wikipedia.org/wiki?curid=2853246", "title": "WalkSAT", "text": "WalkSAT\n\nIn computer science, GSAT and WalkSAT are local search algorithms to solve Boolean satisfiability problems.\n\nBoth algorithms work on formulae in Boolean logic that are in, or have been converted into, conjunctive normal form. They start by assigning a random value to each variable in the formula. If the assignment satisfies all clauses, the algorithm terminates, returning the assignment. Otherwise, a variable is flipped and the above is then repeated until all the clauses are satisfied. WalkSAT and GSAT differ in the methods used to select which variable to flip.\n\nGSAT makes the change which minimizes the number of unsatisfied clauses in the new assignment, or with some probability picks a variable at random.\n\nWalkSAT first picks a clause which is unsatisfied by the current assignment, then flips a variable within that clause. The clause is picked at random among unsatisfied clauses. The variable is picked that will result in the fewest previously satisfied clauses becoming unsatisfied, with some probability of picking one of the variables at random. When picking at random, WalkSAT is guaranteed at least a chance of one out of the number of variables in the clause of fixing a currently incorrect assignment. When picking a guessed-to-be-optimal variable, WalkSAT has to do less calculation than GSAT because it is considering fewer possibilities.\n\nThe algorithm may restart with a new random assignment if no solution has been found for too long, as a way of getting out of local minima of numbers of unsatisfied clauses.\n\nMany versions of GSAT and WalkSAT exist. WalkSAT has been proven particularly useful in solving satisfiability problems produced by conversion from automated planning problems. The approach to planning that converts planning problems into Boolean satisfiability problems is called satplan.\n\nMaxWalkSAT is a variant of WalkSAT designed to solve the weighted satisfiability problem, in which each clause has associated with a weight, and the goal is to find an assignment—one which may or may not satisfy the entire formula—that maximizes the total weight of the clauses satisfied by that assignment. \n\n"}
{"id": "24373430", "url": "https://en.wikipedia.org/wiki?curid=24373430", "title": "Weighted network", "text": "Weighted network\n\nA weighted network is a network where the ties among nodes have weights assigned to them. A network is a system whose elements are somehow connected (Wasserman and Faust, 1994). The elements of a system are represented as nodes (also known as actors or vertices) and the connections among interacting elements are known as ties, edges, arcs, or links. The nodes might be neurons, individuals, groups, organisations, airports, or even countries, whereas ties can take the form of friendship, communication, collaboration, alliance, flow, or trade, to name a few.\n\nIn a number of real-world networks, not all ties in a network have the same capacity. In fact, ties are often associated with weights that differentiate them in terms of their strength, intensity, or capacity (Barrat et al., 2004) and Horvath (2011). On the one hand, Mark Granovetter (1973) argued that the strength of social relationships in social networks is a function of their duration, emotional intensity, intimacy, and exchange of services. On the other, for non-social networks, weights often refer to the function performed by ties, e.g., the carbon flow (mg/m²/day) between species in food webs (Luczkowich et al., 2003), the number of synapses and gap junctions in neural networks (Watts and Strogatz, 1998), or the amount of traffic flowing along connections in transportation networks (Opsahl et al., 2008).\n\nBy recording the strength of ties, a weighted network can be created (also known as a valued network). Below is an example of such a network (weights can also be visualized by giving edges different widths):\n\nWeighted networks are also widely used in genomic and systems biologic applications. \n(Horvath, 2011). For example, weighted gene co-expression network analysis (WGCNA) is often used for constructing a weighted network among genes (or gene products) based on gene expression (e.g. microarray) data (Zhang and Horvath 2005). More generally, weighted correlation networks can be defined by soft-thresholding the pairwise correlations among variables (e.g. gene measurements).\n\nAlthough weighted networks are more difficult to analyse than if ties were simply present or absent, a number of network measures has been proposed for weighted networks:\n\nA theoretical advantage of weighted networks is that they allow one to derive relationships among different network measures (also known as network concepts, statistics or indices. \nFor example, Dong and Horvath (2007) show that simple relationships among network measures can be derived in clusters of nodes (modules) in weighted networks. For weighted correlation networks, one can use the angular interpretation of correlations to provide a geometric interpretation of network theoretic concepts and to derive unexpected relationships among them Horvath and Dong (2008) \n\nThere are a number of software packages that can analyse weighted networks, see Social network analysis software. Among these are the proprietary software UCINET and the open-source package tnet.\n\nThe WGCNA R package implements functions for constructing and analyzing weighted networks in particular weighted correlation networks.\n\nDisparity filter algorithm of weighted network\n"}
