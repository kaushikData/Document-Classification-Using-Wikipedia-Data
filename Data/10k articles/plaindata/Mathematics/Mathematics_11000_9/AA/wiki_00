{"id": "421596", "url": "https://en.wikipedia.org/wiki?curid=421596", "title": "130 (number)", "text": "130 (number)\n\n130 (one hundred [and] thirty) is the natural number following 129 and preceding 131.\n\n130 is a sphenic number. It is a noncototient since there is no answer to the equation \"x\" - φ(\"x\") = 130.\n\n130 is the only integer that is the sum of the squares of its first four divisors, including 1: 1 + 2 + 5 + 10 = 130.\n\n130 is the largest number that cannot be written as the sum of four hexagonal numbers.\n\nThe Second Book of Chronicles says that Jehoiada died at the age of 130.\n\nOne hundred [and] thirty is also:\n\n"}
{"id": "6317347", "url": "https://en.wikipedia.org/wiki?curid=6317347", "title": "207 (number)", "text": "207 (number)\n\n207 (two hundred [and] seven) is the natural number following 206 and preceding 208.\n\nIt is a Wedderburn-Etherington number. There are exactly 207 different matchstick graphs with eight edges.\n\n"}
{"id": "426769", "url": "https://en.wikipedia.org/wiki?curid=426769", "title": "Actor (UML)", "text": "Actor (UML)\n\nAn actor in the Unified Modeling Language (UML) \"specifies a role played by a user or any other system that interacts with the subject.\"\n\n\"An Actor models a type of role played by an entity that interacts with the subject (e.g., by exchanging signals and data),\nbut which is external to the subject.\"\n\n\"Actors may represent roles played by human users, external hardware, or other subjects. Actors do not necessarily represent specific physical entities but merely particular facets (i.e., “roles”) of some entities that are relevant to the specification of its associated use cases. A single physical instance may play the role of several different actors and a given actor may be played by multiple different instances.\"\n\nUML 2 does not permit associations between Actors. The use of generalization/specialization relationship between actors is useful in modeling overlapping behaviours between actors and does not violate this constraint since a generalization relation is not a type of association.\n\nActors interact with use cases.\n\n"}
{"id": "4806014", "url": "https://en.wikipedia.org/wiki?curid=4806014", "title": "Arithmetica Universalis", "text": "Arithmetica Universalis\n\nArithmetica Universalis (\"Universal Arithmetic\") is a mathematics text by Isaac Newton. Written in Latin, it was edited and published by William Whiston, Newton's successor as Lucasian Professor of Mathematics at the University of Cambridge. The \"Arithmetica\" was based on Newton's lecture notes.\n\nWhiston's original edition was published in 1707. It was translated into English by Joseph Raphson, who published it in 1720 as the \"Universal Arithmetick\". John Machin published a second Latin edition in 1722.\n\nNone of these editions credits Newton as author; Newton was unhappy with the publication of the \"Arithmetica\", and so refused to have his name appear. In fact, when Whiston's edition was published, Newton was so upset he considered purchasing all of the copies so he could destroy them.\n\nThe \"Arithmetica\" touches on algebraic notation, arithmetic, the relationship between geometry and algebra, and the solution of equations. Newton also applied Descartes' rule of signs to imaginary roots. He also offered, without proof, a rule to determine the number of imaginary roots of polynomial equations. Not for another 150 years would a rigorous proof to Newton's counting formula be found (by James Joseph Sylvester, published in 1865). \n\n"}
{"id": "23003425", "url": "https://en.wikipedia.org/wiki?curid=23003425", "title": "Audrey Terras", "text": "Audrey Terras\n\nAudrey Anne Terras (born September 10, 1942) is an American mathematician who works primarily in number theory. Her research has focused on quantum chaos and on various types of zeta functions.\n\nAudrey Terras was born September 10, 1942 in Washington, D.C.\nShe received a BS degree in mathematics from the University of Maryland, College Park in 1964, and MA and PhD degrees from Yale University in 1966 and 1970 respectively.\nShe stated in a 2008 interview that she chose to study mathematics because \"The U.S. government paid me! And not much! It was the time of Sputnik, so we needed to produce more mathematicians, and when I was deciding between Math and History, they weren’t paying me to do history, they were paying me to do math.\"\n\nTerras joined the University of California San Diego as an assistant professor in 1972, and is now a full professor there.\n\nAs an undergraduate Terras was inspired by her teacher Sigekatu Kuroda to become a number theorist; she was especially interested in the use of analytic techniques to get algebraic results. Today her research interests are in number theory, harmonic analysis on symmetric spaces and finite groups, special functions, algebraic graph theory, zeta functions of graphs, arithmetical quantum chaos, and the Selberg trace formula.\n\nTerras was elected a Fellow of the American Association for the Advancement of Science in 1982. She was the Association for Women in Mathematics-\nMathematical Association of America AWM/MAA Falconer Lecturer in 2000, speaking on \"Finite Quantum Chaos,\"\nand the AWM's Noether Lecturer in 2008, speaking on \"Fun with Zeta Functions of Graphs\". In 2012 she became a fellow of the American Mathematical Society.\nShe is part of the 2019 class of fellows of the Association for Women in Mathematics.\n\n\n\n"}
{"id": "8337647", "url": "https://en.wikipedia.org/wiki?curid=8337647", "title": "Average path length", "text": "Average path length\n\nAverage path length is a concept in network topology that is defined as the average number of steps along the shortest paths for all possible pairs of network nodes. It is a measure of the efficiency of information or mass transport on a network.\n\nAverage path length is one of the three most robust measures of network topology, along with its clustering coefficient and its degree distribution. Some examples are: the average number of clicks which will lead you from one website to another, or the number of people you will have to communicate through, on an average, to contact a complete stranger. It should not be confused with the diameter of the network, which is defined as the longest geodesic, i.e., the longest shortest path between any two nodes in the network (see Distance (graph theory)).\n\nThe average path length distinguishes an easily negotiable network from one, which is complicated and inefficient, with a shorter average path length being more desirable. However, the average path length is simply what the path length will most likely be. The network itself might have some very remotely connected nodes and many nodes, which are neighbors of each other.\n\nConsider an unweighted directed graph formula_1 with the set of vertices formula_2. Let formula_3, where formula_4 denote the shortest distance between formula_5 and formula_6.\nAssume that formula_7 if formula_6 cannot be reached from formula_5. Then, the average path length formula_10 is:\n\nformula_11\n\nwhere formula_12 is the number of vertices in formula_1.\n\nIn a real network like the Internet, a short average path length facilitates the quick transfer of information and reduces costs. The efficiency of mass transfer in a metabolic network can be judged by studying its average path length. A power grid network will have fewer losses if its average path length is minimized. \n\nMost real networks have a very short average path length leading to the concept of a small world where everyone is connected to everyone else through a very short path.\n\nAs a result, most models of real networks are created with this condition in mind. One of the first models which tried to explain real networks was the random network model. It was later followed by the Watts and Strogatz model, and even later there were the scale-free networks starting with the BA model. All these models had one thing in common: they all predicted very short average path length. The average path lengths of some networks are listed in Table.[1]. \nThe average path length depends on the system size but does not change drastically with it. Small world network theory predicts that the average path length changes proportionally to log n, where n is the number of nodes in the network.\n"}
{"id": "13229336", "url": "https://en.wikipedia.org/wiki?curid=13229336", "title": "Berger's inequality for Einstein manifolds", "text": "Berger's inequality for Einstein manifolds\n\nIn mathematics — specifically, in differential topology — Berger's inequality for Einstein manifolds is the statement that any 4-dimensional Einstein manifold (\"M\", \"g\") has non-negative Euler characteristic \"χ\"(\"M\") ≥ 0. The inequality is named after the French mathematician Marcel Berger.\n\n"}
{"id": "3877474", "url": "https://en.wikipedia.org/wiki?curid=3877474", "title": "Binomial transform", "text": "Binomial transform\n\nIn combinatorics, the binomial transform is a sequence transformation (i.e., a transform of a sequence) that computes its forward differences. It is closely related to the Euler transform, which is the result of applying the binomial transform to the sequence associated with its ordinary generating function.\n\nThe binomial transform, \"T\", of a sequence, {\"a\"}, is the sequence {\"s\"} defined by\n\nFormally, one may write \n\nfor the transformation, where \"T\" is an infinite-dimensional operator with matrix elements \"T\".\nThe transform is an involution, that is,\n\nor, using index notation,\n\nwhere formula_5 is the Kronecker delta. The original series can be regained by\n\nThe binomial transform of a sequence is just the \"n\"th forward differences of the sequence, with odd differences carrying a negative sign, namely:\n\nwhere Δ is the forward difference operator.\n\nSome authors define the binomial transform with an extra sign, so that it is not self-inverse:\n\nwhose inverse is\n\nIn this case the former transform is called the \"inverse binomial transform\", and the latter is just \"binomial transform\". This is standard usage for example in On-Line Encyclopedia of Integer Sequences.\n\nBinomial transforms can be seen in difference tables. Consider the following:\n\nThe top line 0, 1, 10, 63, 324, 1485... (a sequence defined by (2\"n\" + \"n\")3) is the (noninvolutive version of the) binomial transform of the diagonal 0, 1, 8, 36, 128, 400... (a sequence defined by \"n\"2).\n\nThe transform connects the generating functions associated with the series. For the ordinary generating function, let\n\nand\n\nthen\n\nThe relationship between the ordinary generating functions is sometimes called the Euler transform. It commonly makes its appearance in one of two different ways. In one form, it is used to accelerate the convergence of an alternating series. That is, one has the identity\n\nwhich is obtained by substituting \"x\"=1/2 into the last formula above. The terms on the right hand side typically become much smaller, much more rapidly, thus allowing rapid numerical summation.\n\nThe Euler transform can be generalized (Borisov B. and Shkodrov V., 2007):\n\nwhere \"p\" = 0, 1, 2...\n\nThe Euler transform is also frequently applied to the Euler hypergeometric integral formula_19. Here, the Euler transform takes the form:\n\nThe binomial transform, and its variation as the Euler transform, is notable for its connection to the continued fraction representation of a number. Let formula_21 have the continued fraction representation\n\nthen\n\nand\n\nFor the exponential generating function, let\n\nand\n\nthen\n\nThe Borel transform will convert the ordinary generating function to the exponential generating function.\n\nWhen the sequence can be interpolated by a complex analytic function, then the binomial transform of the sequence can be represented by means of a Nörlund–Rice integral on the interpolating function.\n\nProdinger gives a related, modular-like transformation: letting\n\ngives\n\nwhere \"U\" and \"B\" are the ordinary generating functions associated with the series formula_30 and formula_31, respectively.\n\nThe rising \"k\"-binomial transform is sometimes defined as\n\nThe falling \"k\"-binomial transform is\n\nBoth are homomorphisms of the kernel of the Hankel transform of a series.\n\nIn the case where the binomial transform is defined as\n\nLet this be equal to the function formula_35\n\nIf a new forward difference table is made and the first elements from each row of this table are taken to form a new sequence formula_31, then the second binomial transform of the original sequence is,\n\nIf the same process is repeated \"k\" times, then it follows that,\n\nIts inverse is,\n\nThis can be generalized as,\n\nwhere formula_41 is the shift operator.\n\nIts inverse is\n\n\n"}
{"id": "37039361", "url": "https://en.wikipedia.org/wiki?curid=37039361", "title": "Björling problem", "text": "Björling problem\n\nIn differential geometry, the Björling problem is the problem of finding a minimal surface passing through a given curve with prescribed normal (or tangent planes). The problem was posed and solved by Swedish mathematician Emanuel Gabriel Björling, with further refinement by Hermann Schwarz.\n\nThe problem can be solved by extending the surface from the curve using complex analytic continuation. If formula_1 is a real analytic curve in ℝ defined over an interval \"I\", with formula_2 and a vector field formula_3 along \"c\" such that formula_4 and formula_5, then the following surface is minimal:\n\nwhere formula_7, formula_8, and formula_9 is a simply connected domain where the interval is included and the power series expansions of formula_1 and formula_3 are convergent.\n\nA classic example is Catalan's minimal surface, which passes through a cycloid curve. Applying the method to a semicubical parabola produces the Henneberg surface, and to a circle (with a suitably twisted normal field) a minimal Möbius strip.\n\nA unique solution always exists. It can be viewed as a Cauchy problem for minimal surfaces, allowing one to find a surface if a geodesic, asymptote or lines of curvature is known. In particular, if the curve is planar and geodesic, then the plane of the curve will be a symmetry plane of the surface.\n\n"}
{"id": "54356", "url": "https://en.wikipedia.org/wiki?curid=54356", "title": "Boolean ring", "text": "Boolean ring\n\nIn mathematics, a Boolean ring \"R\" is a ring for which \"x\" = \"x\" for all \"x\" in \"R\", such as the ring of integers modulo 2. That is, \"R\" consists only of idempotent elements.\n\nEvery Boolean ring gives rise to a Boolean algebra, with ring multiplication corresponding to conjunction or meet ∧, and ring addition to exclusive disjunction or symmetric difference (\"not\" disjunction ∨, which would constitute a semiring). Boolean rings are named after the founder of Boolean algebra, George Boole.\n\nThere are at least four different and incompatible systems of notation for Boolean rings and algebras. \n\nHistorically, the term \"Boolean ring\" has been used to mean a \"Boolean ring possibly without an identity\", and \"Boolean algebra\" has been used to mean a Boolean ring with an identity. The existence of the identity is necessary to consider the ring as an algebra over the field of two elements: otherwise there cannot be a (unital) ring homomorphism of the field of two elements into the Boolean ring. (This is the same as the old use of the terms \"ring\" and \"algebra\" in measure theory.)\n\nOne example of a Boolean ring is the power set of any set \"X\", where the addition in the ring is symmetric difference, and the multiplication is intersection. As another example, we can also consider the set of all finite or cofinite subsets of \"X\", again with symmetric difference and intersection as operations. More generally with these operations any field of sets is a Boolean ring. By Stone's representation theorem every Boolean ring is isomorphic to a field of sets (treated as a ring with these operations).\n\nSince the join operation ∨ in a Boolean algebra is often written additively, it makes sense in this context to denote ring addition by ⊕, a symbol that is often used to denote exclusive or.\n\nGiven a Boolean ring \"R\", for \"x\" and \"y\" in \"R\" we can define\n\nThese operations then satisfy all of the axioms for meets, joins, and complements in a Boolean algebra. Thus every Boolean ring becomes a Boolean algebra. Similarly, every Boolean algebra becomes a Boolean ring thus:\n\nIf a Boolean ring is translated into a Boolean algebra in this way, and then the Boolean algebra is translated into a ring, the result is the original ring. The analogous result holds beginning with a Boolean algebra.\n\nA map between two Boolean rings is a ring homomorphism if and only if it is a homomorphism of the corresponding Boolean algebras. Furthermore, a subset of a Boolean ring is a ring ideal (prime ring ideal, maximal ring ideal) if and only if it is an order ideal (prime order ideal, maximal order ideal) of the Boolean algebra. The quotient ring of a Boolean ring modulo a ring ideal corresponds to the factor algebra of the corresponding Boolean algebra modulo the corresponding order ideal.\n\nEvery Boolean ring \"R\" satisfies \"x\" ⊕ \"x\" = 0 for all \"x\" in \"R\", because we know\n\nand since (\"R\",⊕) is an abelian group, we can subtract \"x\" ⊕ \"x\" from both sides of this equation, which gives \"x\" ⊕ \"x\" = 0. A similar proof shows that every Boolean ring is commutative:\n\nand this yields \"xy\" ⊕ \"yx\" = 0, which means \"xy\" = \"yx\" (using the first property above).\n\nThe property \"x\" ⊕ \"x\" = 0 shows that any Boolean ring is an associative algebra over the field F with two elements, in just one way. In particular, any finite Boolean ring has as cardinality a power of two. Not every associative algebra with one over F is a Boolean ring: consider for instance the polynomial ring F[\"X\"].\n\nThe quotient ring \"R\"/\"I\" of any Boolean ring \"R\" modulo any ideal \"I\" is again a Boolean ring. Likewise, any subring of a Boolean ring is a Boolean ring.\n\nAny localization formula_1 of a Boolean ring \"R\" by a set formula_2 is a Boolean ring, since every element in the localization is idempotent.\n\nThe maximal ring of quotients formula_3 (in the sense of Utumi and Lambek) of a Boolean ring \"R\" is a Boolean ring, since every partial endomorphism is idempotent.\n\nEvery prime ideal \"P\" in a Boolean ring \"R\" is maximal: the quotient ring \"R\"/\"P\" is an integral domain and also a Boolean ring, so it is isomorphic to the field F, which shows the maximality of \"P\". Since maximal ideals are always prime, prime ideals and maximal ideals coincide in Boolean rings.\n\nBoolean rings are von Neumann regular rings.\n\nBoolean rings are absolutely flat: this means that every module over them is flat.\n\nEvery finitely generated ideal of a Boolean ring is principal (indeed, \"(x,y)=(x+y+xy)\").\n\nUnification in Boolean rings is decidable, that is, algorithms exist to solve arbitrary equations over Boolean rings. Both unification and matching in finitely generated free Boolean rings are NP-complete, and NP-hard in finitely presented Boolean rings. (In fact, as any unification problem \"f\"(\"X\") = \"g\"(\"X\") in a Boolean ring can be rewritten as the matching problem \"f\"(\"X\") + \"g\"(\"X\") = 0, the problems are equivalent.)\n\nUnification in Boolean rings is unitary if all the uninterpreted function symbols are nullary and finitary otherwise (i.e. if the function symbols not occurring in the signature of Boolean rings are all constants then there exists a most general unifier, and otherwise the minimal complete set of unifiers is finite).\n\n\n\n"}
{"id": "31211209", "url": "https://en.wikipedia.org/wiki?curid=31211209", "title": "Breath gas analysis", "text": "Breath gas analysis\n\nBreath gas analysis is a method for gaining non-invasive information on the clinical state of an individual by monitoring volatile organic compounds present in the exhaled breath. Breath gas concentration can then be related to blood concentrations via mathematical modeling as for example in blood alcohol testing.\n\nThe area of modern breath testing commenced in 1971, when Nobel Prize winner Linus Pauling demonstrated that human breath is a complex gas, containing more than 200 different volatile organic compounds. However, physicians have used breath analysis since the days of Hippocrates.\n\nEndogenous volatile organic compounds (VOCs) are released within the human organism as a result of normal metabolic activity or due to pathological disorders. They enter the blood stream and are eventually metabolized or excreted via exhalation, skin emission, urine, etc.\n\nBreath sampling is non-invasive and breath samples can be extracted as often as desired.\n\nIdentification and quantification of potential disease biomarkers can be seen as the driving force for the analysis of exhaled breath. Moreover, future applications for medical diagnosis and therapy control with dynamic assessments of normal physiological function or pharmacodynamics are intended.\n\nExogenous VOCs penetrating the body as a result of environmental exposure can be used to quantify body burden. Also breath tests are often based on the ingestion of isotopically labeled precursors, producing isotopically labeled carbon dioxide and potentially many other metabolites.\n\nHowever, breath sampling is far from being a standardized procedure due to the numerous confounding factors biasing the concentrations of volatiles in breath. These factors are related to both the breath sampling protocols as well as the complex physiological mechanisms underlying pulmonary gas exchange. Even under resting conditions exhaled breath concentrations of VOCs can strongly be influenced by specific physiological parameters such as cardiac output and breathing patterns, depending on the physico-chemical properties of the compound under study.\n\nUnderstanding the influence of all this factors and their control is necessary for achieving an accurate standardization of breath sample collection and for the correct deduction of the corresponding blood concentration levels.\n\nThe simplest model relating breath gas concentration to blood concentrations was developed by Farhi\nwhere formula_2 denotes the alveolar concentration which is assumed to be equal to the measured concentration.\nIt expresses the fact that the concentration of an inert gas in the alveolar air depends on the mixed venous concentration formula_3, the substance-specific blood:air partition coefficient formula_4, and the ventilation-perfusion ratio formula_5.\nBut this model fails when two prototypical substances like acetone (partition coefficient formula_6) or isoprene (partition coefficient formula_7 ) are measured.\n\nE.g., multiplying the proposed population mean of approximately formula_8 acetone in end-tidal breath by the partition coefficient formula_6 at body temperature grossly underestimates observed (arterial) blood levels spreading around formula_10. Furthermore, breath profiles of acetone (and other highly soluble volatile compounds such as 2-pentanone or methyl acetate) associated with moderate workload ergometer challenges of normal healthy volunteers drastically depart from the trend suggested by the equation above.\n\nHence some more refined models are necessary. Such models have been developed recently.\n\nBreath gas analysis is used in a number of breath tests.\n\n\nBreath can be collected using a variety of home-made and commercially available devices. The three basic types of breath collector for VOC analysis are:\n\n\nEach of these can be used as a vehicle for direct introduction of a gas sample into an appropriate analytical instrument, or serve as a reservoir of breath gas into which an absorption device such as an SPME fiber is placed to collect specific compounds.\n\nBreath analysis can be done with various forms of mass spectrometry, but there are also simpler methods for specific purposes, such as the Halimeter and the breathalyzer.\n\n\n"}
{"id": "23338010", "url": "https://en.wikipedia.org/wiki?curid=23338010", "title": "Butcher group", "text": "Butcher group\n\nIn mathematics, the Butcher group, named after the New Zealand mathematician John C. Butcher by , is an infinite-dimensional Lie group first introduced in numerical analysis to study solutions of non-linear ordinary differential equations by the Runge–Kutta method. It arose from an algebraic formalism involving rooted trees that provides formal power series solutions of the differential equation modeling the flow of a vector field. It was , prompted by the work of Sylvester on change of variables in differential calculus, who first noted that the derivatives of a composition of functions can be conveniently expressed in terms of rooted trees and their combinatorics.\n\nA rooted tree is a graph with a distinguished node, called the \"root\", in which every other node is connected to the root by a unique path. If the root of a tree t is removed and the nodes connected to the original node by a single bond are taken as new roots, the tree t breaks up into rooted trees t, t, ... Reversing this process a new tree t = [t, t, ...] can be constructed by joining the roots of the trees to a new common root. The number of nodes in a tree is denoted by |t|. A \"heap-ordering\" of a rooted tree t is an allocation of the numbers 1 through |t| to the nodes so that the numbers increase on any path going away from the root. Two heap orderings are \"equivalent\", if there is an automorphism of rooted trees mapping one of them on the other. The number of equivalence classes of heap-orderings on a particular tree is denoted by α(t) and can be computed using the Butcher's formula:\n\nwhere \"S\" denotes the symmetry group of t and the tree factorial is defined recursively by\n\nwith the tree factorial of an isolated root defined to be 1\n\nThe ordinary differential equation for the flow of a vector field on an open subset \"U\" of R can be written\n\nwhere \"x\"(\"s\") takes values in \"U\", \"f\" is a smooth function from \"U\" to R and \"x\" is the starting point of the flow at time \"s\" = 0.\n\nWith this notation\n\nfor \"w\" ≠ 0 in C.\n\nThe loops γ and λ · γ have the same negative part and, for \"t\" real,\n\ndefines a one-parameter subgroup of the complex Butcher group G called the renormalization group flow (RG).\n\nIts infinitesimal generator β is an element of the Lie algebra of G and is defined by\n\nIt is called the beta-function of the model.\n\nIn any given model, there is usually a finite-dimensional space of complex coupling constants. The complex Butcher group acts by diffeomorphims on this space. In particular the renormalization group defines a flow on the space of coupling constants, with the beta function giving the corresponding vector field.\n\nMore general models in quantum field theory require rooted trees to be replaced by Feynman diagrams with vertices decorated by symbols from a finite index set. Connes and Kreimer have also defined Hopf algebras in this setting and have shown how they can be used to systematize standard computations in renormalization theory.\n\n has given a \"toy model\" involving dimensional regularization for H and the algebra \"V\". If \"c\" is a positive integer and \"q\" = \"q\" / μ is a dimensionless constant, Feynman rules can be defined recursively by\n\nwhere \"z\" = 1 – \"D\"/2 is the regularization parameter. These integrals can be computed explicitly in terms of the Gamma function using the formula\n\nIn particular\n\nTaking the renormalization scheme \"R\" of minimal subtraction, the renormalized quantities formula_12 are polynomials in formula_13 when evaluated at \"z\" = 0.\n\n"}
{"id": "901260", "url": "https://en.wikipedia.org/wiki?curid=901260", "title": "Close-packing of equal spheres", "text": "Close-packing of equal spheres\n\nIn geometry, close-packing of equal spheres is a dense arrangement of congruent spheres in an infinite, regular arrangement (or lattice). Carl Friedrich Gauss proved that the highest average density – that is, the greatest fraction of space occupied by spheres – that can be achieved by a lattice packing is\nThe same packing density can also be achieved by alternate stackings of the same close-packed planes of spheres, including structures that are aperiodic in the stacking direction. The Kepler conjecture states that this is the highest density that can be achieved by any arrangement of spheres, either regular or irregular. This conjecture was proven by T. C. Hales. Highest density is known only in case of 1, 2, 3, 8 and 24 dimensions.\n\nMany crystal structures are based on a close-packing of a single kind of atom, or a close-packing of large ions with smaller ions filling the spaces between them. The cubic and hexagonal arrangements are very close to one another in energy, and it may be difficult to predict which form will be preferred from first principles.\n\nThere are two simple regular lattices that achieve this highest average density. They are called face-centered cubic (fcc) (also called cubic close packed) and hexagonal close-packed (hcp), based on their symmetry. Both are based upon sheets of spheres arranged at the vertices of a triangular tiling; they differ in how the sheets are stacked upon one another. The fcc lattice is also known to mathematicians as that generated by the A root system.\n\nThe problem of close-packing of spheres was first mathematically analyzed by Thomas Harriot around 1587, after a question on piling cannonballs on ships was posed to him by Sir Walter Raleigh on their expedition to America. \nCannonballs were usually piled in a rectangular or triangular wooden frame, forming a three-sided or four-sided pyramid. Both arrangements produce a face-centered cubic lattice – with different orientation to the ground. Hexagonal close-packing would result in a six-sided pyramid with a hexagonal base. \nThe cannonball problem asks which flat square arrangements of cannonballs can be stacked into a square pyramid. Édouard Lucas formulated the problem as the Diophantine equation formula_2 or formula_3 and conjectured that the only solutions are formula_4 and formula_5. Here formula_6 is the number of layers in the pyramidal stacking arrangement and formula_7 is the number of cannonballs along an edge in the flat square arrangement.\nIn both the fcc and hcp arrangements each sphere has twelve neighbors. For every sphere there is one gap surrounded by six spheres (octahedral) and two smaller gaps surrounded by four spheres (tetrahedral). The distances to the centers of these gaps from the centers of the surrounding spheres is for the tetrahedral, and for the octahedral, when the sphere radius is 1.\n\nRelative to a reference layer with positioning A, two more positionings B and C are possible. Every sequence of A, B, and C without immediate repetition of the same one is possible and gives an equally dense packing for spheres of a given radius.\n\nThe most regular ones are\n\nThere is an uncountably infinite number of disordered arrangements of planes (e.g. ABCACBABABAC...) that are sometimes collectively referred to as \"Barlow packings\", after crystallographer William Barlow\n\nIn close-packing, the center-to-center spacing of spheres in the \"xy\" plane is a simple honeycomb-like tessellation with a pitch (distance between sphere centers) of one sphere diameter. The distance between sphere centers, projected on the \"z\" (vertical) axis, is:\n\nwhere \"d\" is the diameter of a sphere; this follows from the tetrahedral arrangement of close-packed spheres.\n\nThe coordination number of hcp and fcc is 12 and their atomic packing factors (APFs) are equal to the number mentioned above, 0.74.\n\nWhen forming any sphere-packing lattice, the first fact to notice is that whenever two spheres touch a straight line may be drawn from the center of one sphere to the center of the other intersecting the point of contact. The distance between the centers along the shortest path namely that straight line will therefore be \"r\" + \"r\" where \"r\" is the radius of the first sphere and \"r\" is the radius of the second. In close packing all of the spheres share a common radius, \"r\". Therefore two centers would simply have a distance 2\"r\".\n\nTo form an A-B-A-B-... hexagonal close packing of spheres, the coordinate points of the lattice will be the spheres' centers. Suppose, the goal is to fill a box with spheres according to hcp. The box would be placed on the \"x\"-\"y\"-\"z\" coordinate space.\n\nFirst form a row of spheres. The centers will all lie on a straight line. Their \"x\"-coordinate will vary by 2\"r\" since the distance between each center of the spheres are touching is 2\"r\". The \"y\"-coordinate and z-coordinate will be the same. For simplicity, say that the balls are the first row and that their \"y\"- and \"z\"-coordinates are simply \"r\", so that their surfaces rest on the zero-planes. Coordinates of the centers of the first row will look like (2\"r\", \"r\", \"r\"), (4\"r\", \"r\", \"r\"), (6\"r\" ,\"r\", \"r\"), (8\"r\" ,\"r\", \"r\"), ... .\n\nNow, form the next row of spheres. Again, the centers will all lie on a straight line with \"x\"-coordinate differences of 2\"r\", but there will be a shift of distance \"r\" in the \"x\"-direction so that the center of every sphere in this row aligns with the \"x\"-coordinate of where two spheres touch in the first row. This allows the spheres of the new row to slide in closer to the first row until all spheres in the new row are touching two spheres of the first row. Since the new spheres \"touch\" two spheres, their centers form an equilateral triangle with those two neighbors' centers. The side lengths are all 2\"r\", so the height or \"y\"-coordinate difference between the rows is \"r\". Thus, this row will have coordinates like this:\n\nThe first sphere of this row only touches one sphere in the original row, but its location follows suit with the rest of the row.\n\nThe next row follows this pattern of shifting the \"x\"-coordinate by \"r\" and the \"y\"-coordinate by . Add rows until reaching the \"x\" and \"y\" maximum borders of the box.\n\nIn an A-B-A-B-... stacking pattern, the odd numbered \"planes\" of spheres will have exactly the same coordinates save for a pitch difference in the \"z\"-coordinates and the even numbered \"planes\" of spheres will share the same \"x\"- and \"y\"-coordinates. Both types of planes are formed using the pattern mentioned above, but the starting place for the \"first\" row's first sphere will be different.\n\nUsing the plane described precisely above as plane #1, the A plane, place a sphere on top of this plane so that it lies touching three spheres in the A-plane. The three spheres are all already touching each other, forming an equilateral triangle, and since they all touch the new sphere, the four centers form a regular tetrahedron. All of the sides are equal to 2\"r\" because all of the sides are formed by two spheres touching. The height of which or the \"z\"-coordinate difference between the two \"planes\" is . This, combined with the offsets in the \"x\" and \"y\"-coordinates gives the centers of the first row in the B plane:\n\nThe second row's coordinates follow the pattern first described above and are:\n\nThe difference to the next plane, the A plane, is again in the \"z\"-direction and a shift in the \"x\" and \"y\" to match those \"x\"- and \"y\"-coordinates of the first A plane.\n\nIn general, the coordinates of sphere centers can be written as:\n\nwhere \"i\", \"j\" and \"k\" are indices starting at 0 for the \"x\"-, \"y\"- and \"z\"-coordinates.\n\nCrystallographic features of hcp systems, such as vectors and atomic plane families, can be described using a four-value Miller index notation ( \"hkil\" ) in which the third index \"i\" denotes a convenient but degenerate component which is equal to −\"h\" − \"k\". The \"h\", \"i\" and \"k\" index directions are separated by 120°, and are thus not orthogonal; the \"l\" component is mutually perpendicular to the \"h\", \"i\" and \"k\" index directions.\n\nThe fcc and hcp packings are the densest known packings of equal spheres with the highest symmetry (smallest repeat units).\nDenser sphere packings are known, but they involve unequal sphere packing.\nA packing density of 1, filling space completely, requires non-spherical shapes, such as honeycombs.\n\nReplacing each contact point between two spheres with an edge connecting the centers of the touching spheres produces tetrahedrons and octahedrons of equal edge lengths.\nThe fcc arrangement produces the tetrahedral-octahedral honeycomb.\nThe hcp arrangement produces the gyrated tetrahedral-octahedral honeycomb.\nIf, instead, every sphere is augmented with the points in space that are closer to it than to any other sphere, the duals of these honeycombs are produced: the rhombic dodecahedral honeycomb for fcc, and the trapezo-rhombic dodecahedral honeycomb for hcp.\n\nSpherical bubbles in soapy water in a fcc or hcp arrangement, when the water in the gaps between the bubbles drains out, also approach the rhombic dodecahedral honeycomb or trapezo-rhombic dodecahedral honeycomb. However, such fcc or hcp foams of very small liquid content are unstable, as they do not satisfy Plateau's laws. The Kelvin foam and the Weaire–Phelan foam are more stable, having smaller interfacial energy in the limit of a very small liquid content.\n\n\n"}
{"id": "1550771", "url": "https://en.wikipedia.org/wiki?curid=1550771", "title": "Countable chain condition", "text": "Countable chain condition\n\nIn order theory, a partially ordered set \"X\" is said to satisfy the countable chain condition, or to be ccc, if every strong antichain in \"X\" is countable. \n\nThere are really two conditions: the \"upwards\" and \"downwards\" countable chain conditions. These are not equivalent. The countable chain condition means the downwards countable chain condition, in other words no two elements have a common lower bound.\n\nThis is called the \"countable chain condition\" rather than the more logical term \"countable antichain condition\" for historical reasons related to certain chains of open sets in topological spaces and chains in complete Boolean algebras, where chain conditions sometimes happen to be equivalent to antichain conditions. For example, if κ is a cardinal, then in a complete Boolean algebra every antichain has size less than κ if and only if there is no descending κ-sequence of elements, so chain conditions are equivalent to antichain conditions.\n\nPartial orders and spaces satisfying the ccc are used in the statement of Martin's axiom.\n\nIn the theory of forcing, ccc partial orders are used because forcing with any generic set over such an order preserves cardinals and cofinalities. Furthermore, the ccc property is preserved by finite support iterations (see iterated forcing). For more information on ccc in the context of forcing, see .\n\nMore generally, if κ is a cardinal then a poset is said to satisfy the κ-chain condition if every antichain has size less than κ. The countable chain condition is the ℵ-chain condition.\n\nA topological space is said to satisfy the countable chain condition, or Suslin's Condition, if the partially ordered set of non-empty open subsets of \"X\" satisfies the countable chain condition, \"i.e.\" every pairwise disjoint collection of non-empty open subsets of \"X\" is countable. The name originates from Suslin's Problem.\n\n\nFor example, formula_2 with the product topology is ccc, though \"not\" separable.\n\n"}
{"id": "5980981", "url": "https://en.wikipedia.org/wiki?curid=5980981", "title": "De Bruijn torus", "text": "De Bruijn torus\n\nIn combinatorial mathematics, a De Bruijn torus, named after Nicolaas Govert de Bruijn, is an array of symbols from an alphabet (often just 0 and 1) that contains every \"m\"-by-\"n\" matrix exactly once. It is a torus because the edges are considered wraparound for the purpose of finding matrices. Its name comes from the De Bruijn sequence, which can be considered a special case where \"n\" is 1 (one dimension).\n\nOne of the main open questions regarding De Bruijn tori is whether a De Bruijn torus for a particular alphabet size can be constructed for a given \"m\" and \"n\". It is known that these always exist when \"n\" = 1, since then we simply get the De Bruijn sequences, which always exist. It is also known that \"square\" tori exist whenever \"m\" = \"n\" and even (for the odd case the resulting tori cannot be square).\n\nThe smallest possible binary \"square\" de Bruijn torus, depicted above right, denoted as \"(4,4;2,2)\" de Bruijn torus (or simply as \"B\"), contains all \"2×2\" binary matrices.\n\nApart from \"translation\", \"inversion\" (exchanging \"0\"s and \"1\"s) and \"rotation\" (by 90 degrees), no other \"(4,4;2,2)\" de Bruijn tori are possible - this can be shown by complete inspection of all \"2\" binary matrices (or subset fulfilling constrains such as equal numbers of \"0\"s and \"1\"s) .\nAn example of the next possible binary \"square\" de Bruijn torus, \"(256,256;4,4)\" (abbreviated as \"B\"), has been explicitly constructed.\n\nThe image below shows an example of a \"(256,256;4,4)\" de Bruijn torus / array, where the \"zeroes\" have been encoded as black and the \"ones\" as white pixels respectively.\n\nThe paper in which an example of the \"(256,256;4,4)\" de Bruijn torus was constructed contained over 10 pages of binary, despite its reduced font size, requiring three lines per row of array.\n\nThe subsequent possible binary de Bruijn torus, containing all binary \"6×6\" matrices, would have \"2 = 68,719,476,736\" entries, yielding a square array of dimension \"262,144x262,144\", denoted a \"(262144,262144;6,6)\" de Bruijn torus or simply \"B\". This could easily be stored on a computer &em; if printed with pixels of side 0.1 mm, such a matrix would require an area of approximately 26×26 square metre.\n\nThe object \"B\", containing all binary \"8×8\" matrices and denoted \"(4294967296,4294967296;8,8)\", has a total of \"2 ≈ 18.447×10\" entries: storing such a matrix would require 18.5 exabits, or 2.3 Exabyte of storage, an order of magnitude above even modern data centres.\n\n\n"}
{"id": "1949487", "url": "https://en.wikipedia.org/wiki?curid=1949487", "title": "Dependent type", "text": "Dependent type\n\nIn computer science and logic, a dependent type is a type whose definition depends on a value. A \"pair of integers\" is a type. A \"pair of integers where the second is greater than the first\" is a dependent type because of the dependence on the value. It is an overlapping feature of type theory and type systems. In intuitionistic type theory, dependent types are used to encode logic's quantifiers like \"for all\" and \"there exists\". In functional programming languages like Agda, ATS, Coq, F* , Epigram, Idris, and Shen, dependent types prevent bugs by allowing extremely expressive types.\n\nTwo common examples of dependent types are dependent functions and dependent pairs. A dependent function's return type may depend on the \"value\" (not just type) of an argument. A function that takes a positive integer \"n\" may return an array of length \"n\". (Note that this is different from polymorphism and generic programming, both of which include the type as an argument.) A dependent pair may have a second value that depends on the first. It can be used to encode a pair of integers where the second one is greater than the first.\n\nDependent types add complexity to a type system. Deciding the equality of dependent types in a program may require computations. If arbitrary values are allowed in dependent types, then deciding type equality may involve deciding whether two arbitrary programs produce the same result; hence type checking may become undecidable.\n\nDependent types were created to deepen the connection between programming and logic.\n\nIn 1934, Haskell Curry noticed that the types used in typed lambda calculus, and in its combinatory logic counterpart, followed the same pattern as axioms in propositional logic. Going further, for every proof in the logic, there was a matching function (term) in the programming language. One of Curry's examples was the correspondence between simply typed lambda calculus and intuitionistic logic.\n\nPredicate logic is an extension of propositional logic, adding quantifiers. Howard and de Bruijn extended lambda calculus to match this more powerful logic by creating types for dependent functions, which correspond to \"for all\", and dependent pairs, which correspond to \"there exists\".\n\nLoosely speaking, dependent types are similar to the type of an indexed family of sets. More formally, given a type formula_2 in a universe of types formula_3, one may have a family of types formula_4, which assigns to each term formula_5 a type formula_6. We say that the type \"B(a)\" varies with \"a\".\n\nA function whose type of return value varies with its argument (i.e. there is no fixed codomain) is a dependent function and the type of this function is called dependent product type, pi-type or dependent function type. For this example, the dependent function type is typically written as formula_7 or formula_8\n\nIf formula_4 is a constant function, the corresponding dependent product type is equivalent to an ordinary function type. That is, formula_10 is judgmentally equal to formula_11 when \"B\" does not depend on \"x\".\n\nThe name 'pi-type' comes from the idea that these may be viewed as a Cartesian product of types. Pi-types can also be understood as models of universal quantifiers.\n\nFor example, if we write formula_12 for \"n\"-tuples of real numbers, then formula_13 would be the type of a function which, given a natural number \"n\", returns a tuple of real numbers of size \"n\". The usual function space arises as a special case when the range type does not actually depend on the input. E.g. formula_14 is the type of functions from natural numbers to the real numbers, which is written as formula_15 in typed lambda calculus.\n\nThe dual of the dependent product type is the dependent pair type, dependent sum type, sigma-type, or (confusingly) dependent product type. Sigma-types can also be understood as existential quantifiers. Continuing the above example, if, in the universe of types formula_3, there is a type formula_2 and a family of types formula_4, then there is a dependent pair type formula_20\n\nThe dependent pair type captures the idea of an ordered pair where the type of the second term is dependent on the value of the first. If formula_21 then formula_5 and formula_23. If \"B\" is a constant function, then the dependent pair type becomes (is judgementally equal to) the product type, that is, an ordinary Cartesian product formula_24.\n\nLet formula_2 be some type, and let formula_4. By the Curry–Howard correspondence, \"B\" can be interpreted as a logical predicate on terms of \"A\". For a given formula_5, whether the type \"B(a)\" is inhabited indicates whether \"a\" satisfies this predicate. The correspondence can be extended to existential quantification and dependent pairs: the proposition formula_28 is true if and only if the type formula_29 is inhabited.\n\nFor example, formula_30 is less than or equal to formula_31 if and only if there exists another natural number formula_32 such that \"m\" + \"k\" = \"n\". In logic, this statement is codified by existential quantification: formula_33 This proposition corresponds to the dependent pair type: formula_34 That is, a proof of the statement that \"m\" is less than \"n\" is a pair that contains both a number \"k\", which is the difference between \"m\" and \"n\", and a proof of the equality \"m\" + \"k\" = \"n\".\n\nHenk Barendregt developed the lambda cube as a means of classifying type systems along three axes. The eight corners of the resulting cube-shaped diagram each correspond to a type system, with simply typed lambda calculus in the least expressive corner, and calculus of constructions in the most expressive. The three axes of the cube correspond to three different augmentations of the simply typed lambda calculus: the addition of dependent types, the addition of polymorphism, and the addition of higher kinded type constructors (functions from types to types, for example). The lambda cube is generalized further by pure type systems.\n\nThe system formula_35 of pure first order dependent types, corresponding to the logical framework LF, is obtained by generalising the function space type of the simply typed lambda calculus to the dependent product type.\n\nThe system formula_36 of second order dependent types is obtained from formula_35 by allowing quantification over type constructors. In this theory the dependent product operator subsumes both the formula_38 operator of simply typed lambda calculus and the formula_39 binder of System F.\n\nThe higher order system formula_40 extends formula_36 to all four forms of abstraction from the lambda cube: functions from terms to terms, types to types, terms to types and types to terms. The system corresponds to the calculus of constructions whose derivative, the calculus of inductive constructions is the underlying system of the Coq proof assistant.\n\nThe Curry–Howard correspondence implies that types can be constructed that express arbitrarily complex mathematical properties. If the user can supply a constructive proof that a type is \"inhabited\" (i.e., that a value of that type exists) then a compiler can check the proof and convert it into executable computer code that computes the value by carrying out the construction. The proof checking feature makes dependently typed languages closely related to proof assistants. The code-generation aspect provides a powerful approach to formal program verification and proof-carrying code, since the code is derived directly from a mechanically verified mathematical proof.\n\n\n\n"}
{"id": "56263", "url": "https://en.wikipedia.org/wiki?curid=56263", "title": "Dyadic rational", "text": "Dyadic rational\n\nIn mathematics, a dyadic fraction or dyadic rational is a rational number whose denominator, when the ratio is in minimal (coprime) terms, is a power of two, i.e., a number of the form formula_1 where \"a\" is an integer and \"b\" is a natural number; for example, 1/2 or 3/8, but not 1/3. These are precisely the numbers possessing a finite binary expansion. \n\nThe inch is customarily subdivided in dyadic rather than decimal fractions; similarly, the customary divisions of the gallon into half-gallons, quarts, and pints are dyadic. The ancient Egyptians also used dyadic fractions in measurement, with denominators up to 64.\n\nThe sum, product, or difference of any two dyadic fractions is itself another dyadic fraction:\n\nHowever, the result of dividing one dyadic fraction by another is not necessarily a dyadic fraction.\n\nAddition modulo 1 forms a group; this is the Prüfer 2-group.\n\nBecause they are closed under addition, subtraction, and multiplication, but not division, the dyadic fractions form a subring of the rational numbers Q and an overring of the integers Z. Algebraically, this subring is the localization of the integers Z with respect to the set of powers of two.\n\nThe set of all dyadic fractions is dense in the real line: any real number \"x\" can be arbitrarily closely approximated by dyadic rationals of the form formula_6.\nCompared to other dense subsets of the real line, such as the rational numbers, the dyadic rationals are in some sense a relatively \"small\" dense set, which is why they sometimes occur in proofs. (See for instance Urysohn's lemma.)\n\nWhile it is true that dyadic fractions are precisely those numbers possessing finite binary expansions, their binary expansions are not unique; there is both a finite and an infinite representation of each, with exactly two infinite binary representations for each one other than 0. For example, 0.1000… = 0.0111… = 1/2. Also, 0.11000… = 0.10111… = 3/4. It would be more precise to say these numbers have binary representations which are eventually constant.\n\nConsidering only the addition and subtraction operations of the dyadic rationals gives them the structure of an additive abelian group. The dual group of a group consists of its characters, group homomorphisms to the multiplicative group of the complex numbers, and in the spirit of Pontryagin duality the dual group of the additive dyadic rationals can also be viewed as a topological group. It is called the dyadic solenoid and is an example of a solenoid group and of a protorus.\n\nThe dyadic rationals are the direct limit of infinite cyclic subgroups of the rational numbers,\nand their dual group can be constructed as the inverse limit of the unit circle group under the repeated squaring map\n\nAn element of the dyadic solenoid can be represented as an infinite sequence of complex numbers \"q\", \"q\", \"q\", ..., with the properties that each \"q\" lies on the unit circle and that, for all \"i\" > 0, \"q\" = \"q\". The group operation on these elements multiplies any two sequences componentwise. Each element of the dyadic solenoid corresponds to a character of the dyadic rationals that maps \"a\"/2 to the complex number \"q\". Conversely, every character \"χ\" of the dyadic rationals corresponds to the element of the dyadic solenoid given by \"q\" = \"χ\"(1/2).\n\nAs a topological space the dyadic solenoid is a solenoid, and an indecomposable continuum.\n\nThe surreal numbers are generated by an iterated construction principle which starts by generating all finite dyadic fractions, and then goes on to create new and strange kinds of infinite, infinitesimal and other numbers.\n\nThe binary van der Corput sequence is an equidistributed permutation of the positive dyadic rational numbers.\n\nTime signatures in Western musical notation traditionally consist of dyadic fractions (for example: 2/2, 4/4, 6/8...), although non-dyadic time signatures have been introduced by composers in the twentieth century (for example: 2/, which would literally mean 2/). Non-dyadic time signatures are called \"irrational\" in musical terminology, but this usage does not correspond to the irrational numbers of mathematics, because they still consist of ratios of integers. Irrational time signatures in the mathematical sense are very rare, but one example (/1) appears in Conlon Nancarrow's \"Studies for Player Piano\".\n\nAs a data type used by computers, floating-point numbers are often defined as integers multiplied by positive or negative powers of two, and thus all numbers that can be represented for instance by binary IEEE floating-point datatypes are dyadic rationals. The same is true for the majority of fixed-point datatypes, which also uses powers of two implicitly in the majority of cases.\n\n"}
{"id": "39993538", "url": "https://en.wikipedia.org/wiki?curid=39993538", "title": "Ehrenpreis's fundamental principle", "text": "Ehrenpreis's fundamental principle\n\nIn mathematical analysis, Ehrenpreis's fundamental principle, introduced by Leon Ehrenpreis, states:\n"}
{"id": "3003070", "url": "https://en.wikipedia.org/wiki?curid=3003070", "title": "Empirical modelling", "text": "Empirical modelling\n\nEmpirical modelling refers to any kind of (computer) modelling based on empirical observations rather than on mathematically describable relationships of the system modelled.\n\nEmpirical Modelling as a variety of empirical modelling\n\nEmpirical modelling is a generic term for activities that create models by observation and experiment. Empirical Modelling (with the initial letters capitalised, and often abbreviated to EM) refers to a specific variety of empirical modelling in which models are constructed following particular principles. Though the extent to which these principles can be applied to model-building without computers is an interesting issue (to be revisited below), there are at least two good reasons to consider Empirical Modelling in the first instance as computer-based. Without doubt, new technologies that are based on computers have had a transformative impact where the full exploitation of Empirical Modelling principles is concerned. What is more, the conception of Empirical Modelling has been closely associated with thinking about the role of the computer in model-building.\n\nAn empirical model operates on a simple semantic principle: the maker observes a close correspondence between the behaviour of the model and that of its referent. The crafting of this correspondence can be 'empirical' in a wide variety of senses: it may entail a trial-and-error process, may be based on computational approximation to analytic formulae, it may be derived as a black-box relation that affords no insight into 'why it works'.\n\nEmpirical Modelling is rooted on the key principle of William James's 'radical empiricism', which postulates that all knowing is rooted in connections that are given-in-experience. Empirical Modelling aspires to craft the correspondence between the model and its referent in such a way that its derivation can be traced to connections given-in-experience. Making connections in experience is an essentially individual human activity that requires skill and is highly context-dependent. Examples of such connections include: identifying familiar objects in the stream of thought, associating natural languages words with objects to which they refer, and subliminally interpreting the rows and columns of a spreadsheet as exam results of particular students in particular subjects.\n\nEmpirical Modelling principles\n\nIn Empirical Modelling, the process of construction is an incremental one in which the intermediate products are artefacts that evoke aspects of the intended (and sometimes emerging) referent through live interaction and observation. The connections evoked in this way have distinctive qualities: they are of their essence personal and experiential in character and are provisional in so far as they may be undermined, refined and reinforced as the model builder's experience and understanding of the referent develops. Following a precedent established by David Gooding in his account of the role that artefacts played in Michael Faraday's experimental investigation of electromagnetism, the intermediate products of the Empirical Modelling process are described as 'construals'. Gooding's account is a powerful illustration of how \"making construals\" can support the sense-making activities that lead to conceptual insights (cf. the contribution that Faraday's work made to electromagnetic theory) and to practical products (cf. Faraday's invention of the electric motor).\n\nThe activities associated with making a construal in the Empirical Modelling framework are depicted in Figure 1.\n\nThe eye icon at the centre the figure represents the maker's observation of the current state of development of the construal and its referent. The two arrows emanating from the eye represent the connection given-in-experience between the construal and its referent that is established in the mind of the maker. This connection is crafted through experimental interaction with the construal under construction and its emerging referent. As in genuine experiment, the scope of the interactions that can be entertained by the maker is inconceivably broad. At the maker's discretion, the interactions that characterise the construal are those that respect the connection given in the maker's experience. As the Empirical Modelling process unfolds, the construal, the referent, the maker's understanding and the context for the maker's engagement co-evolve in such a way that:\nEmpirical Modelling concepts\n\nIn Empirical Modelling. making and maintaining the connection given-in-experience between the construal and referent is based on three primary concepts: \"observables\", \"dependencies\" and \"agency\". Within both the construal and its referent, the maker identifies observables as entities that can take on a range of different values, and whose current values determine its current state. All state-changing interactions with the construal and referent are conceived as changes to the values of observables. A change to the value of one observable may be directly attributable to a change in the value of another observable, in which case these values are linked by a dependency. Changes to observable values are attributed to agents, amongst which the most important is the maker of the construal. When changes to observable values are observed to occur simultaneously, this can be construed as concurrent action on the part of different agents, or as concomitant changes to observables derived from a single agent action via dependencies. To craft the connection given-in-experience between the construal and referent, the maker constructs the construal in such a way that its observables, dependencies and agency correspond closely to those that are observed in the referent. To this end, the maker must conceive appropriate ways in which observables and agent actions in the referent can be given suitable experiential counterparts in the construal.\n\nThe semantic framework shown in Figure 1 resembles that adopted in working with spreadsheets, where the state that is currently displayed in the grid is meaningful only when experienced in conjunction with an external referent. In this setting, the cells serve as observables, their definitions specify the dependencies, and agency is enacted by changing the values or the definitions of cells. In making a construal, the maker explores the roles of each relevant agent by projecting agency upon it as if it were a human agent and identifying observables and dependencies from that perspective. By automating agency, construals can then be used to specify behaviours in much the same way that behaviours can be expressed using macros in conjunction with spreadsheets. In this way, animated construals can emulate program-like behaviours in which the intermediate states are meaningful and live to auditing by the maker.\n\nEnvironments to support Empirical Modelling\n\nThe development of computer environments for making construals has been an ongoing subject of research over the last thirty years. The many variants of such environments that have been implemented are based on common principles. The network of dependencies that currently connect observables is recorded as a family of definitions. Semantically such definitions resemble the definitions of spreadsheet cells, whereby changes to the values of observables on the right hand side propagate so as to change the value of the observable on the LHS in a conceptually indivisble manner. The dependencies in these networks are acyclic but are also reconfigurable: redefining an observable may introduce a new definition that alters the dependency structure. Observables built into the environment include scalars, geometric and screen display elements: these can be elaborated using multi-level list structures. A dependency is typically represented by a definition which uses a relatively simple functional expression to relate the value of an observable to the values of other observables. Such functions have typically been expressed in fragments of simple procedural code, but the most recent variants of environments of making construals also enable dependency relations to be expressed by suitably contextualised families of definitions. The maker can interact with a construal through redefining existing observables or introducing new observables in an open-ended unconstrained manner. Such interaction has a crucial role in the experimental activity that informs the incremental development of the construal. Triggered actions can be introduced to automate state-change: these perform redefinitions in response to specified changes in the values of observables.\n\nEmpirical Modelling as a broader view of computing\n\nIn Figure 1, identifying 'the computer' as the medium in which the construal is created is potentially misleading. The term COMPUTER is not merely a reference to a powerful computational device. In making construals, the primary emphasis is on the rich potential scope for interaction and perceptualisation that the computer enables when used in conjunction with other technologies and devices. The primary motivation for developing Empirical Modelling is to give a satisfactory account of computing that integrates these two complementary roles of the computer. The principles by which James and Dewey sought to reconcile perspectives on agency informed by logic and experience play a crucial role in achieving this integration.\n\nThe dual role for the computer implicit in Figure 1 is widely relevant to contemporary computing applications. On this basis, Empirical Modelling can be viewed as providing a foundation for a broader view of computing. This perspective is reflected in numerous Empirical Modelling publications on topics such as educational technology, computer-aided design and software development. Making construals has also been proposed as a suitable technique to support constructionism, as conceived by Seymour Papert, and to meet the guarantees for 'construction' as identified by Bruno Latour.\n\nEmpirical Modelling as generic sense-making?\n\nThe Turing machine provides the theoretical foundation for the role of the computer as a computational device: it can be regarded as modelling 'a mind following rules'. The practical applications of Empirical Modelling to date suggest that making construals is well-suited to supporting the supplementary role the computer can play in orchestrating rich experience. In particular, in keeping with the pragmatic philosophical stance of James and Dewey, making construals can fulfill an explanatory role by offering contingent explanations for human experience in contexts where computational rules cannot be invoked. In this respect, making construals may be regarded as modelling 'a mind making sense of a situation'.\n\nIn the same way that the Turing machine is a conceptual tool for understanding the nature of algorithms whose value is independent of the existence of the computer, Empirical Modelling principles and concepts may have generic relevance as a framework for thinking about sense-making without specific reference to the use of a computer. The contribution that William James's analysis of human experience makes to the concept of Empirical Modelling may be seen as evidence for this. By this token, Empirical Modelling principles may be an appropriate way to analyse varieties of empirical modelling that are not computer-based. For instance, it is plausible that the analysis in terms of observables, dependencies and agency that applies to interaction with electronic spreadsheets would also be appropriate for the manual spreadsheets that predated them.\n\nBackground\n\nEmpirical Modelling has been pioneered since the early 1980s by Meurig Beynon and the Empirical Modelling Research Group in Computer Science at the University of Warwick.\n\nThe term 'Empirical Modelling' (EM) has been adopted for this work since about 1995 to reflect the experiential basis of the modelling process in observation and experiment. Special purpose software supporting the central concepts of observable, dependency and agency has been under continuous development (mainly led by research students) since the late 1980s.\n\nThe principles and tools of EM have been used and developed by many hundreds of students within coursework, project work, and research theses. The undergraduate and MSc module 'Introduction to Empirical Modelling' was taught for many years up to 2013-14 until the retirement of Meurig Beynon and Steve Russ (authors of this article). There is a large website [1] containing research and teaching material with an extensive collection of refereed publications and conference proceedings.\n\nThe term 'construal' has been used since the early 2000s for the artefacts, or models, made with EM tools. The term has been adapted from its use by David Gooding in the book 'Experiment and the Making of Meaning' (1990) to describe the emerging, provisional ideas that formed in Faraday's mind, and were recorded in his notebooks, as he investigated electromagnetism, and made the first electric motors, in the 1800s.\n\nThe main practical activity associated with EM - that of 'making construals' - was the subject of an Erasmus+ Project CONSTRUIT! (2014-2017)[2].\n\n[1] http://www.dcs.warwick.ac.uk/modelling/ Empirical Modelling Research Group\n\n[2] https://warwick.ac.uk/fac/sci/dcs/research/em/welcome/ CONSTRUIT! Project web pages\n"}
{"id": "43958459", "url": "https://en.wikipedia.org/wiki?curid=43958459", "title": "Extended side", "text": "Extended side\n\nIn plane geometry, an extended side or sideline of a polygon is the line that contains one side of the polygon. The extension of a side arises in various contexts.\n\nIn an obtuse triangle, the altitudes from the acute angled vertices intersect the corresponding extended base sides but not the base sides themselves. \n\nThe excircles of a triangle, as well as the triangle's inconics that are not inellipses, are externally tangent to one side and to the other two extended sides.\n\nTrilinear coordinates locate a point in the plane by its relative distances from the extended sides of a reference triangle. If the point is outside the triangle, the perpendicular from the point to the sideline may meet the sideline outside the triangle—that is, not on the actual side of the triangle.\n\nIn a triangle, three intersection points, each of an external angle bisector with the opposite extended side, are collinear.\n\nIn a triangle, three intersection points, two of them between an interior angle bisector and the opposite side, and the third between the other exterior angle bisector and the opposite side extended, are collinear.\n\nAn ex-tangential quadrilateral is a quadrilateral for which there exists a circle that is tangent to all four extended sides. The excenter (center of the tangent circle) lies at the intersection of six angle bisectors. These are the internal angle bisectors at two opposite vertex angles, the external angle bisectors (supplementary angle bisectors) at the other two vertex angles, and the external angle bisectors at the angles formed where the extensions of opposite sides intersect.\n\nPascal's theorem states that if six arbitrary points are chosen on a conic section (i.e., ellipse, parabola or hyperbola) and joined by line segments in any order to form a hexagon, then the three pairs of opposite sides of the hexagon (extended if necessary) meet in three points which lie on a straight line, called the Pascal line of the hexagon.\n"}
{"id": "48253", "url": "https://en.wikipedia.org/wiki?curid=48253", "title": "Fractal art", "text": "Fractal art\n\nFractal art is a form of algorithmic art created by calculating fractal objects and representing the calculation results as still images, animations, and media. Fractal art developed from the mid-1980s onwards. It is a genre of computer art and digital art which are part of new media art. The mathematical beauty of fractals lies at the intersection of generative art and computer art. They combine to produce a type of abstract art.\n\nFractal art (especially in the western world) is rarely drawn or painted by hand. It is usually created indirectly with the assistance of fractal-generating software, iterating through three phases: setting parameters of appropriate fractal software; executing the possibly lengthy calculation; and evaluating the product. In some cases, other graphics programs are used to further modify the images produced. This is called post-processing. Non-fractal imagery may also be integrated into the artwork. The Julia set and Mandelbrot sets can be considered as icons of fractal art.\n\nIt was assumed that fractal art could not have developed without computers because of the calculative capabilities they provide. Fractals are generated by applying iterative methods to solving non-linear equations or polynomial equations. Fractals are any of various extremely irregular curves or shapes for which any suitably chosen part is similar in shape to a given larger or smaller part when magnified or reduced to the same size.\n\nThere are many different kinds of fractal images and can be subdivided into several groups. \n\nFractal Expressionism is a term used to differentiate traditional visual art that incorporates fractal elements such as self-similarity for example. Perhaps the best example of fractal expressionism is found in Jackson Pollock's dripped patterns. They have been analysed and found to contain a fractal dimension which has been attributed to his technique.\n\nFractals of all kinds have been used as the basis for digital art and animation. High resolution color graphics became increasingly available at scientific research labs in the mid-1980s. Scientific forms of art, including fractal art, have developed separately from mainstream culture. Starting with 2-dimensional details of fractals, such as the Mandelbrot Set, fractals have found artistic application in fields as varied as texture generation, plant growth simulation and landscape generation.\n\nFractals are sometimes combined with evolutionary algorithms, either by iteratively choosing good-looking specimens in a set of random variations of a fractal artwork and producing new variations, to avoid dealing with cumbersome or unpredictable parameters, or collectively, as in the Electric Sheep project, where people use fractal flames rendered with distributed computing as their screensaver and \"rate\" the flame they are viewing, influencing the server, which reduces the traits of the undesirables, and increases those of the desirables to produce a computer-generated, community-created piece of art.\n\nMany fractal images are admired because of their perceived harmony. This is typically achieved by the patterns which emerge from the balance of order and chaos. Similar qualities have been described in Chinese painting and miniature trees and rockeries.\n\nThe first fractal image that was intended to be a work of art was probably the famous one on the cover of \"Scientific American\", August 1985. This image showed a landscape formed from the potential function on the domain outside the (usual) Mandelbrot set. However, as the potential function grows fast near the boundary of the Mandelbrot set, it was necessary for the creator to let the landscape grow downwards, so that it looked as if the Mandelbrot set was a plateau atop a mountain with steep sides. The same technique was used a year after in some images in \"The Beauty of Fractals\" by Heinz-Otto Peitgen and Michael M. Richter. They provide a formula to estimate the distance from a point outside the Mandelbrot set to the boundary of the Mandelbrot set (and a similar formula for the Julia sets). Landscapes can, for example, be formed from the distance function for a family of iterations of the form formula_1.\n\nNotable fractal artists include Desmond Paul Henry, Hamid Naderi Yeganeh and musician Bruno Degazio. The British artist William Latham, has used fractal geometry and other computer graphics techniques in his works. Greg Sams has used fractal designs in postcards, T-shirts and textiles. American Vicky Brago-Mitchell has created fractal art which has appeared in exhibitions and on magazine covers. Scott Draves is credited with inventing flame fractals. Carlos Ginzburg has explored fractal art and developed a concept called \"homo fractalus\" which is based around the idea that the human is the ultimate fractal. Merrin Parkers from New Zealand specialises in fractal art. \nKerry Mitchell wrote a \"Fractal Art Manifesto\", claiming that\n\nAccording to Mitchell, fractal art is not computerized art, lacking in rules, unpredictable, nor something that any person with access to a computer can do well. Instead, fractal art is expressive, creative, and requires input, effort, and intelligence. Most importantly, \"fractal art is simply that which is created by Fractal Artists: ART.\"\n\nMore recently, American artist Hal Tenny was hired to design environment in Guardians of the Galaxy Vol. 2.\n\nFractal art has been exhibited at major international art galleries. One of the first exhibitions of fractal art was \"Map Art\", a travelling exhibition of works from researchers at the University of Bremen. Mathematicians Heinz-Otto Peitgen and Michael M. Richter discovered that the public not only found the images aesthetically pleasing but that they also wanted to understand the scientific background to the images.\n\nIn 1989, fractals were part of the subject matter for an art show called \"Strange Attractors: Signs of Chaos\" at the New Museum of Contemporary Art. The show consisted of photographs, installations and sculptures designed to provide greater scientific discourse to the field which had already captured the public's attention through colourful and intricate computer imagery.\n\n\n"}
{"id": "315659", "url": "https://en.wikipedia.org/wiki?curid=315659", "title": "Golden angle", "text": "Golden angle\n\nIn geometry, the golden angle is the smaller of the two angles created by sectioning the circumference of a circle according to the golden ratio; that is, into two arcs such that the ratio of the length of the larger arc to the length of the smaller arc is the same as the ratio of the full circumference to the length of the larger arc.\n\nAlgebraically, let \"a+b\" be the circumference of a circle, divided into a longer arc of length \"a\" and a smaller arc of length \"b\" such that\n\nThe golden angle is then the angle subtended by the smaller arc of length \"b\". It measures approximately 137.5077640500378546463487 ...° or in radians 2.39996322972865332 ... .\n\nThe name comes from the golden angle's connection to the golden ratio \"φ\"; the exact value of the golden angle is\n\nor\n\nwhere the equivalences follow from well-known algebraic properties of the golden ratio.\n\nThe golden ratio is equal to \"φ\" = \"a\"/\"b\" given the conditions above.\n\nLet \"ƒ\" be the fraction of the circumference subtended by the golden angle, or equivalently, the golden angle divided by the angular measurement of the circle.\n\nBut since\n\nit follows that\n\nThis is equivalent to saying that \"φ\" golden angles can fit in a circle.\n\nThe fraction of a circle occupied by the golden angle is therefore\n\nThe golden angle \"g\" can therefore be numerically approximated in degrees as:\n\nor in radians as :\n\nThe golden angle plays a significant role in the theory of phyllotaxis; for example, the golden angle is the angle separating the florets on a sunflower.\n\n\n"}
{"id": "48497362", "url": "https://en.wikipedia.org/wiki?curid=48497362", "title": "Gorenstein scheme", "text": "Gorenstein scheme\n\nIn algebraic geometry, a Gorenstein scheme is a locally Noetherian scheme whose local rings are all Gorenstein. The canonical line bundle is defined for any Gorenstein scheme over a field, and its properties are much the same as in the special case of smooth schemes.\n\nFor a Gorenstein scheme \"X\" of finite type over a field, \"f\": \"X\" → Spec(\"k\"), the dualizing complex \"f\"(\"k\") on \"X\" is a line bundle (called the canonical bundle \"K\"), viewed as a complex in degree −dim(\"X\"). If \"X\" is smooth of dimension \"n\" over \"k\", the canonical bundle \"K\" can be identified with the line bundle Ω of top-degree differential forms.\n\nUsing the canonical bundle, Serre duality takes the same form for Gorenstein schemes as it does for smooth schemes.\n\nLet \"X\" be a normal scheme of finite type over a field \"k\". Then \"X\" is regular outside a closed subset of codimension at least 2. Let \"U\" be the open subset where \"X\" is regular; then the canonical bundle \"K\" is a line bundle. The restriction from the divisor class group Cl(\"X\") to Cl(\"U\") is an isomorphism, and (since \"U\" is smooth) Cl(\"U\") can be identified with the Picard group Pic(\"U\"). As a result, \"K\" defines a linear equivalence class of Weil divisors on \"X\". Any such divisor is called the canonical divisor \"K\". For a normal scheme \"X\", the canonical divisor \"K\" is said to be Q-Cartier if some positive multiple of the Weil divisor \"K\" is Cartier. (This property does not depend on the choice of Weil divisor in its linear equivalence class.) Alternatively, normal schemes \"X\" with \"K\" Q-Cartier are sometimes said to be Q-Gorenstein.\n\nIt is also useful to consider the normal schemes \"X\" for which the canonical divisor \"K\" is Cartier. Such a scheme is sometimes said to be Q-Gorenstein of index 1. (Some authors use \"Gorenstein\" for this property, but that can lead to confusion.) A normal scheme \"X\" is Gorenstein (as defined above) if and only if \"K\" is Cartier and \"X\" is Cohen–Macaulay.\n\n\n"}
{"id": "2514970", "url": "https://en.wikipedia.org/wiki?curid=2514970", "title": "Graded poset", "text": "Graded poset\n\nIn mathematics, in the branch of combinatorics, a graded poset is a partially ordered set (poset) \"P\" equipped with a rank function ρ from \"P\" to N satisfying the following two properties:\nThe value of the rank function for an element of the poset is called its rank. Sometimes a graded poset is called a ranked poset but that phrase has other meanings; see ranked poset. A rank or rank level of a graded poset is the subset of all the elements of the poset that have a given rank value.\n\nGraded posets play an important role in combinatorics and can be visualized by means of a Hasse diagram.\n\nSome examples of graded posets (with the rank function in parentheses) are:\n\nA bounded poset admits a grading if and only if all maximal chains in \"P\" have the same length: setting the rank of the least element to 0 then determines the rank function completely. This covers many finite cases of interest; see picture for a negative example. However, unbounded posets can be more complicated.\n\nA candidate rank function, compatible with the ordering, makes a poset into graded poset if and only if, whenever one has \"x\" < \"z\" with \"z\" of rank \"n\"+1, an element \"y\" of rank \"n\" can be found with \"x\" ≤ \"y\" < \"z\". This condition is sufficient because if \"z\" is taken to be a cover of \"x\", the only possible choice is \"y\" = \"x\" showing that the ranks of \"x\" and \"z\" differ by 1, and it is necessary because in a graded poset one can take for \"y\" any element of maximal rank with \"x\" ≤ \"y\" < \"z\", which always exists and is covered by \"z\".\n\nOften a poset comes with a natural candidate for a rank function; for instance if its elements are finite subsets of some base set \"B\", one can take the number of elements of those subsets. Then the criterion just given can be more practical than the definition because it avoids mention of covers. For instance if \"B\" is itself a poset, and \"P\" consists of its finite lower sets (subsets for which with every one of its elements, all smaller elements are also in the subset), then the criterion is automatically satisfied, since for lower sets \"x\" ⊂ \"z\" there is always a maximal element of \"z\" that is absent from \"x\", and it can be removed from \"z\" to form \"y\".\n\nA graded poset (with positive integer ranks) cannot have any elements \"x\" for which arbitrarily long chains with greatest element \"x\" exist, as otherwise it would have to have elements of arbitrarily small (and eventually negative) rank. For instance, the integers (with the usual order) cannot be a graded poset, nor can any interval (with more than one element) of rational or real numbers. (In particular, graded posets are well-founded, meaning that they satisfy the descending chain condition (DCC): they do not contain any infinite descending chains.) Henceforth we shall therefore only consider posets in which this does not happen. This implies that whenever \"x\" < \"y\" we can get from \"x\" to \"y\" by repeatedly choosing a cover, finitely many times. It also means that (for positive integer rank functions) compatibility of ρ with the ordering follows from the requirement about covers. As a variant of the definition of a graded poset, Birkhoff allows rank functions to have arbitrary (rather than only nonnegative) integer values. In this variant, the integers can be graded (by the identity function) in his setting, and the compatibility of ranks with the ordering is not redundant. As a third variant, Brightwell and West define a rank function to be integer-valued, but don't require its compatibility with the ordering; hence this variant can grade even e.g. the real numbers by any function, as the requirement about covers is vacuous for this example.\n\nNote that graded posets need not satisfy the ascending chain condition (ACC): for instance, the natural numbers contain the infinite ascending chain formula_1.\n\nA poset is graded if and only if every connected component of its comparability graph is graded, so further characterizations will suppose this comparability graph to be connected. On each connected component the rank function is only unique up to a uniform shift (so the rank function can always be chosen so that the elements of minimal rank in their connected component have rank 0).\n\nIf \"P\" has a least element Ô then being graded is equivalent to the condition that for any element \"x\" all maximal chains in the interval [Ô,\"x\"] have the same length. This condition is necessary since every step in a maximal chain is a covering relation, which should change the rank by 1. The condition is also sufficient, since when it holds, one can use the mentioned length to define the rank of \"x\" (the length of a finite chain is its number of \"steps\", so one less than its number of elements), and whenever \"x\" covers \"y\", adjoining \"x\" to a maximal chain in [Ô,\"y\"] gives a maximal chain in [Ô,\"x\"].\n\nIf \"P\" also has a greatest element Î (so that it is a bounded poset), then the previous condition can be simplified to the requirement that all maximal chains in \"P\" have the same (finite) length. This suffices, since any pair of maximal chains in [Ô,\"x\"] can be extended by a maximal chain in [\"x\",Î] to give a pair of maximal chains in \"P\".\n\nMany authors in combinatorics define graded posets in such a way that all minimal elements of \"P\" must have rank 0, and moreover that there is a maximal rank \"r\" which is the rank of any maximal element. Then being graded means that all maximal chains have length \"r\", as is indicated above. In this case one says that \"P\" has rank \"r\".\n\nFurthermore, in this case with the rank levels are associated the rank numbers or Whitney numbers formula_2. These numbers are defined by formula_3 = number of elements of \"P\" having rank \"i\" .\n\nThe Whitney numbers are connected with a lot of important combinatorial theorems. The classic example is Sperner's theorem which can be formulated as follows:\n\nThis means: \n\n\n"}
{"id": "3038013", "url": "https://en.wikipedia.org/wiki?curid=3038013", "title": "Hamiltonian fluid mechanics", "text": "Hamiltonian fluid mechanics\n\nHamiltonian fluid mechanics is the application of Hamiltonian methods to fluid mechanics. Note that this formalism only applies to nondissipative fluids.\n\nTake the simple example of a barotropic, inviscid vorticity-free fluid.\n\nThen, the conjugate fields are the mass density field \"ρ\" and the velocity potential \"φ\". The Poisson bracket is given by\n\nand the Hamiltonian by:\n\nwhere \"e\" is the internal energy density, as a function of \"ρ\". \nFor this barotropic flow, the internal energy is related to the pressure \"p\" by:\n\nwhere an apostrophe ('), denotes differentiation with respect to \"ρ\".\n\nThis Hamiltonian structure gives rise to the following two equations of motion:\n\nwhere formula_5 is the velocity and is vorticity-free. The second equation leads to the Euler equations:\n\nafter exploiting the fact that the vorticity is zero:\n\nAs fluid dynamics is described by non-canonical dynamics, which possess an infinite amount of Casimir invariants, an alternative formulation of Hamiltonian formulation of fluid dynamics can be introduced through the use of Nambu mechanics\n\n\n"}
{"id": "149646", "url": "https://en.wikipedia.org/wiki?curid=149646", "title": "Hamiltonian path problem", "text": "Hamiltonian path problem\n\nIn the mathematical field of graph theory the Hamiltonian path problem and the Hamiltonian cycle problem are problems of determining whether a Hamiltonian path (a path in an undirected or directed graph that visits each vertex exactly once) or a Hamiltonian cycle exists in a given graph (whether directed or undirected). Both problems are NP-complete.\n\nThere is a simple relation between the problems of finding a Hamiltonian path and a Hamiltonian cycle. In one direction, the Hamiltonian path problem for graph G is equivalent to the Hamiltonian cycle problem in a graph H obtained from G by adding a new vertex and connecting it to all vertices of G. Thus, finding a Hamiltonian path cannot be significantly slower (in the worst case, as a function of the number of vertices) than finding a Hamiltonian cycle.\nIn the other direction, the Hamiltonian cycle problem for a graph G is equivalent to the Hamiltonian path problem in the graph H obtained by copying one vertex v of G, v', that is, letting v' have the same neighbourhood as v, and by adding two dummy vertices of degree one, and connecting them with v and v', respectively.\nThe Hamiltonian cycle problem is also a special case of the travelling salesman problem, obtained by setting the distance between two cities to one if they are adjacent and two otherwise, and verifying that the total distance travelled is equal to n (if so, the route is a Hamiltonian circuit; if there is no Hamiltonian circuit then the shortest route will be longer).\n\nThere are \"n\"! different sequences of vertices that \"might\" be Hamiltonian paths in a given \"n\"-vertex graph (and are, in a complete graph), so a brute force search algorithm that tests all possible sequences would be very slow. An early exact algorithm for finding an Hamiltonian cycle on a directed graph was the enumerative algorithm of Martello. A search procedure by Frank Rubin divides the edges of the graph into three classes: those that must be in the path, those that cannot be in the path, and undecided. As the search proceeds, a set of decision rules classifies the undecided edges, and determines whether to halt or continue the search. The algorithm divides the graph into components that can be solved separately. Also, a dynamic programming algorithm of Bellman, Held, and Karp can be used to solve the problem in time O(\"n\" 2). In this method, one determines, for each set \"S\" of vertices and each vertex \"v\" in \"S\", whether there is a path that covers exactly the vertices in \"S\" and ends at \"v\". For each choice of \"S\" and \"v\", a path exists for (\"S\",\"v\") if and only if \"v\" has a neighbor \"w\" such that a path exists for (\"S\" − \"v\",\"w\"), which can be looked up from already-computed information in the dynamic program.\n\nAndreas Björklund provided an alternative approach using the inclusion–exclusion principle to reduce the problem of counting the number of Hamiltonian cycles to a simpler counting problem, of counting cycle covers, which can be solved by computing certain matrix determinants. Using this method, he showed how to solve the Hamiltonian cycle problem in arbitrary \"n\"-vertex graphs by a Monte Carlo algorithm in time O(1.657); for bipartite graphs this algorithm can be further improved to time o(1.415).\n\nFor graphs of maximum degree three, a careful backtracking search can find a Hamiltonian cycle (if one exists) in time O(1.251).\n\nHamiltonian paths and cycles can be found using a SAT solver.\n\nBecause of the difficulty of solving the Hamiltonian path and cycle problems on conventional computers, they have also been studied in unconventional models of computing. For instance, Leonard Adleman showed that the Hamiltonian path problem may be solved using a DNA computer. Exploiting the parallelism inherent in chemical reactions, the problem may be solved using a number of chemical reaction steps linear in the number of vertices of the graph; however, it requires a factorial number of DNA molecules to participate in the reaction. \n\nAn optical solution to the Hamiltonian problem has been proposed in . The idea is to create a graph-like structure made from optical cables and beam splitters which are traversed by light in order to construct a solution for the problem. The weak point of this approach is the required amount of energy which is exponential in the number of nodes.\n\nThe problem of finding a Hamiltonian cycle or path is in FNP; the analogous decision problem is to test whether a Hamiltonian cycle or path exists. The directed and undirected Hamiltonian cycle problems were two of Karp's 21 NP-complete problems. They remain NP-complete even for undirected planar graphs of maximum degree three, for directed planar graphs with indegree and outdegree at most two, for bridgeless undirected planar 3-regular bipartite graphs, for 3-connected 3-regular bipartite graphs, subgraphs of the square grid graph, and cubic subgraphs of the square grid graph.\n\nHowever, 4-connected planar graphs are always Hamiltonian by a result due to Tutte, and the computational task of finding a Hamiltonian cycle in these graphs can be carried out in linear time\nby computing a so-called Tutte path. Tutte proved this result by showing that every 2-connected planar graph contains a Tutte path. Tutte paths in turn can be computed in quadratic time even for 2-connected planar graphs, which may be used to find Hamiltonian cycles and long cycles in generalizations of planar graphs.\n\nPutting all of these conditions together, it remains open whether 3-connected 3-regular bipartite planar graphs must always contain a Hamiltonian cycle, in which case the problem restricted to those graphs could not be NP-complete; see Barnette's conjecture.\n\nIn graphs in which all vertices have odd degree, an argument related to the handshaking lemma shows that the number of Hamiltonian cycles through any fixed edge is always even, so if one Hamiltonian cycle is given, then a second one must also exist. However, finding this second cycle does not seem to be an easy computational task. Papadimitriou defined the complexity class PPA to encapsulate problems such as this one.\n"}
{"id": "44533369", "url": "https://en.wikipedia.org/wiki?curid=44533369", "title": "Higher Topos Theory", "text": "Higher Topos Theory\n\nHigher Topos Theory is a mathematical book by American mathematician Jacob Lurie. The main subject of the book is an ∞-topos. But it also develops the theory of ∞-category as a particular kind of a simplicial set. All together the book provides a categorical foundation for derived algebraic geometry.\n\nThe book is available at https://arxiv.org/abs/math/0608040.\n\n"}
{"id": "25759134", "url": "https://en.wikipedia.org/wiki?curid=25759134", "title": "John Lane Bell", "text": "John Lane Bell\n\nJohn Lane Bell (born March 25, 1945) is Professor of Philosophy at the University of Western Ontario in Canada. He has made contributions to mathematical logic and philosophy, and is the author of a number of books. His research includes such topics as set theory, model theory, lattice theory, modal logic, quantum logic, constructive mathematics, type theory, topos theory, infinitesimal analysis, spacetime theory, and the philosophy of mathematics. He is the author of more than 70 articles and of 11 books. In 2009, he was elected a Fellow of the Royal Society of Canada.\n\nJohn Bell was awarded a scholarship to Oxford University at the age of 15, and graduated with a D.Phil. in Mathematics: his dissertation supervisor was John Crossley. During 1968-89 he was lecturer in Mathematics and Reader in Mathematical Logic at the London School of Economics.\n\nJohn Bell's students include Graham Priest (Ph.D. Mathematics LSE, 1972), Michael Hallett (Ph.D. Philosophy LSE, 1979), David DeVidi (Ph.D. Philosophy UWO, 1994), Elaine Landry (Ph.D. Philosophy UWO, 1997) and Richard Feist (Ph.D. Philosophy UWO, 1999).\n\n\n\n"}
{"id": "38050430", "url": "https://en.wikipedia.org/wiki?curid=38050430", "title": "List of things named after John von Neumann", "text": "List of things named after John von Neumann\n\nThis is a list of things named after John von Neumann. John von Neumann (1903–1957), a mathematician, is the eponym of all of the things (and topics) listed below.\n\n"}
{"id": "3339743", "url": "https://en.wikipedia.org/wiki?curid=3339743", "title": "Medial triangle", "text": "Medial triangle\n\nEach side of the medial triangle is called a \"midsegment\" (or \"midline\"). In general, a midsegment of a triangle is a line segment which joins the midpoints of two sides of the triangle. It is parallel to the third side and has a length equal to half the length of the third side.\n\nThe medial triangle can also be viewed as the image of triangle \"ABC\" transformed by a homothety centered at the centroid with ratio -1/2. Thus, the sides of the medial triangle are half and parallel to the corresponding sides of triangle ABC. Hence, the medial triangle is inversely similar and shares the same centroid and medians with triangle \"ABC\". It also follows from this that the perimeter of the medial triangle equals the semiperimeter of triangle \"ABC\", and that the area is one quarter of the area of triangle \"ABC\". Furthermore, the four triangles that the original triangle is subdivided into by the medial triangle are all mutually congruent by SSS, so their areas are equal and thus the area of each is 1/4 the area of the original triangle.\n\nThe orthocenter of the medial triangle coincides with the circumcenter of triangle \"ABC\". This fact provides a tool for proving collinearity of the circumcenter, centroid and orthocenter. The medial triangle is the pedal triangle of the circumcenter. The nine-point circle circumscribes the medial triangle, and so the nine-point center is the circumcenter of the medial triangle.\n\nThe Nagel point of the medial triangle is the incenter of its reference triangle.\n\nA reference triangle's medial triangle is congruent to the triangle whose vertices are the midpoints between the reference triangle's orthocenter and its vertices.\n\nThe incenter of a triangle lies in its medial triangle.\n\nA point in the interior of a triangle is the center of an inellipse of the triangle if and only if the point lies in the interior of the medial triangle.\n\nThe medial triangle is the only inscribed triangle for which none of the other three interior triangles has smaller area.\n\nLet a = |BC|, b = |CA|, c = |AB| be the sidelengths of triangle ABC. Trilinear coordinates for the vertices of the medial triangle are given by\n\nIf \"XYZ\" is the medial triangle of \"ABC\", then \"ABC\" is the anticomplementary triangle or antimedial triangle of \"XYZ\". The anticomplementary triangle of \"ABC\" is formed by three lines parallel to the sides of \"ABC\": the parallel to \"AB\" through \"C\", the parallel to \"AC\" through \"B\", and the parallel to \"BC\" through \"A\".\n\nTrilinear coordinates for the vertices of the anticomplementary triangle, X'Y'Z', are given by\n\nThe name \"anticomplementary triangle\" corresponds to the fact that its vertices are the anticomplements of the vertices A, B, C of the reference triangle. The vertices of the medial triangle are the complements of A, B, C. \n\n"}
{"id": "19738", "url": "https://en.wikipedia.org/wiki?curid=19738", "title": "Metrization theorem", "text": "Metrization theorem\n\nIn topology and related areas of mathematics, a metrizable space is a topological space that is homeomorphic to a metric space. That is, a topological space formula_1 is said to be metrizable if there is a metric \n\nsuch that the topology induced by \"d\" is formula_3. Metrization theorems are theorems that give sufficient conditions for a topological space to be metrizable.\n\nMetrizable spaces inherit all topological properties from metric spaces. For example, they are Hausdorff paracompact spaces (and hence normal and Tychonoff) and first-countable. However, some properties of the metric, such as completeness, cannot be said to be inherited. This is also true of other structures linked to the metric. A metrizable uniform space, for example, may have a different set of contraction maps than a metric space to which it is homeomorphic.\n\nOne of the first widely recognized metrization theorems was Urysohn's metrization theorem. This states that every Hausdorff second-countable regular space is metrizable. So, for example, every second-countable manifold is metrizable. (Historical note: The form of the theorem shown here was in fact proved by Tychonoff in 1926. What Urysohn had shown, in a paper published posthumously in 1925, was that every second-countable \"normal\" Hausdorff space is metrizable). The converse does not hold: there exist metric spaces that are not second countable, for example, an uncountable set endowed with the discrete metric. The Nagata–Smirnov metrization theorem, described below, provides a more specific theorem where the converse does hold.\n\nSeveral other metrization theorems follow as simple corollaries to Urysohn's Theorem. For example, a compact Hausdorff space is metrizable if and only if it is second-countable.\n\nUrysohn's Theorem can be restated as: A topological space is separable and metrizable if and only if it is regular, Hausdorff and second-countable. The Nagata–Smirnov metrization theorem extends this to the non-separable case. It states that a topological space is metrizable if and only if it is regular, Hausdorff and has a σ-locally finite base. A σ-locally finite base is a base which is a union of countably many locally finite collections of open sets. For a closely related theorem see the Bing metrization theorem.\n\nSeparable metrizable spaces can also be characterized as those spaces which are homeomorphic to a subspace of the Hilbert cube formula_4, i.e. the countably infinite product of the unit interval (with its natural subspace topology from the reals) with itself, endowed with the product topology.\n\nA space is said to be locally metrizable if every point has a metrizable neighbourhood. Smirnov proved that a locally metrizable space is metrizable if and only if it is Hausdorff and paracompact. In particular, a manifold is metrizable if and only if it is paracompact.\n\nThe group of unitary operators formula_5 on a separable Hilbert space formula_6 endowed\nwith the strong operator topology is metrizable (see Proposition II.1 in ).\n\nNon-normal spaces cannot be metrizable; important examples include\n\nThe real line with the lower limit topology is not metrizable. The usual distance function is not a metric on this space because the topology it determines is the usual topology, not the lower limit topology. This space is Hausdorff, paracompact and first countable.\n\nThe long line is locally metrizable but not metrizable; in a sense it is \"too long\".\n\n"}
{"id": "34061412", "url": "https://en.wikipedia.org/wiki?curid=34061412", "title": "Mikhail Kadets", "text": "Mikhail Kadets\n\nMikhail Iosiphovich Kadets (, , sometimes transliterated as Kadec, 30 November 1923 – 7 March 2011) was a Soviet-born Jewish mathematician working in analysis and the theory of Banach spaces.\n\nKadets was born in Kiev. In 1943, he was drafted into the army. After demobilisation in 1946, he studied at Kharkov University, graduating in 1950. After several years in Makeevka he returned to Kharkov in 1957, where he spent the remainder of his life working at various institutes. He defended his PhD in 1955 (under the supervision of Boris Levin), and his doctoral dissertation in 1963. He was awarded the State Prize of Ukraine in 2005.\n\nAfter reading the Ukrainian translation of Banach's monograph \"Théorie des opérations linéaires\", he became interested in the theory of Banach spaces. In 1966, Kadets solved in the affirmative the Banach–Fréchet problem, asking whether every two separable infinite-dimensional Banach spaces are homeomorphic. He developed the method of equivalent norms, which has found numerous applications. For example, he showed that every separable Banach space admits an equivalent Fréchet differentiable norm if and only if the dual space is separable.\n\nTogether with Aleksander Pełczyński, he obtained important results on the topological structure of Lp spaces.\n\nKadets also made several contributions to the theory of finite-dimensional normed spaces. Together with M. G. Snobar (1971), he showed that every \"n\"-dimensional subspace of a Banach space is the image of a projection of norm at most . Together with V. I. Gurarii and V. I. Matsaev, he found the exact order of magnitude of the Banach–Mazur distance between the \"n\"-dimensional spaces and .\n\nIn harmonic analysis, Kadets proved (1964) what is now called the Kadets  theorem, which states that, if |\"λ\" − \"n\"| ≤ \"C\" <  for all integer \"n\", then the sequence (exp(\"iλ\"x)) is a Riesz basis in \"L\"[-, ].\n\nKadets was the founder of the Kharkov school of Banach spaces.\nTogether with his son Vladimir Kadets, he authored two books about series in Banach spaces.\n\n"}
{"id": "964161", "url": "https://en.wikipedia.org/wiki?curid=964161", "title": "Modulus of continuity", "text": "Modulus of continuity\n\nIn mathematical analysis, a modulus of continuity is a function ω : [0, ∞] → [0, ∞] used to measure quantitatively the uniform continuity of functions. So, a function \"f\" : \"I\" → R admits ω as a modulus of continuity if and only if\nfor all \"x\" and \"y\" in the domain of \"f\". Since moduli of continuity are required to be infinitesimal at 0, a function turns out to be uniformly continuous if and only if it admits a modulus of continuity. Moreover, relevance to the notion is given by the fact that sets of functions sharing the same modulus of continuity are exactly equicontinuous families. For instance, the modulus ω(\"t\") := \"kt\" describes the k-Lipschitz functions, the moduli ω(\"t\") := \"kt\" describe the Hölder continuity, the modulus ω(\"t\") := \"kt\"(|log(\"t\")|+1) describes the almost Lipschitz class, and so on. In general, the role of ω is to fix some explicit functional dependence of ε on δ in the (ε, δ) definition of uniform continuity. The same notions generalize naturally to functions between metric spaces. Moreover, a suitable local version of these notions allows to describe quantitatively the continuity at a point in terms of moduli of continuity.\n\nA special role is played by concave moduli of continuity, especially in connection with extension properties, and with approximation of uniformly continuous functions. For a function between metric spaces, it is equivalent to admit a modulus of continuity that is either concave, or subadditive, or uniformly continuous, or sublinear (in the sense of growth). Actually, the existence of such special moduli of continuity for a uniformly continuous function is always ensured whenever the domain is either a compact, or a convex subset of a normed space. However, a uniformly continuous function on a general metric space admits a concave modulus of continuity if and only if the ratios\n\nare uniformly bounded for all pairs (\"x\", \"x\"′) bounded away from the diagonal of \"X x X\". The functions with the latter property constitute a special subclass of the uniformly continuous functions, that in the following we refer to as the \"special uniformly continuous\" functions. Real-valued special uniformly continuous functions on the metric space \"X\" can also be characterized as the set of all functions that are restrictions to \"X\" of uniformly continuous functions over any normed space isometrically containing \"X\". Also, it can be characterized as the uniform closure of the Lipschitz functions on \"X\".\n\nFormally, a modulus of continuity is any real-extended valued function ω : [0, ∞] → [0, ∞], vanishing at 0 and continuous at 0, that is\n\nModuli of continuity are mainly used to give a quantitative account both of the continuity at a point, and of the uniform continuity, for functions between metric spaces, according to the following definitions.\n\nA function \"f\" : (\"X\", \"d\") → (\"Y\", \"d\") admits ω as (local) modulus of continuity at the point \"x\" in \"X\" if and only if,\nAlso, \"f\" admits ω as (global) modulus of continuity if and only if,\nOne equivalently says that ω is a modulus of continuity (resp., at \"x\") for \"f\", or shortly, \"f\" is ω-continuous (resp., at \"x\"). Here, we mainly treat the global notion.\n\n\n\nSpecial moduli of continuity also reflect certain global properties of functions such as extendibility and uniform approximation. In this section we mainly deal with moduli of continuity that are concave, or subadditive, or uniformly continuous, or sublinear. These properties are essentially equivalent in that, for a modulus ω (more precisely, its restriction on [0, ∞]) each of the following implies the next:\n\nThus, for a function \"f\" between metric spaces it is equivalent to admit a modulus of continuity which is either concave, or subadditive, or uniformly continuous, or sublinear. In this case, the function \"f\" is sometimes called a \"special uniformly continuous\" map. This is always true in case of either compact or convex domains. Indeed, a uniformly continuous map \"f\" : \"C\" → \"Y\" defined on a convex set \"C\" of a normed space \"E\" always admits a subadditive modulus of continuity; in particular, real-valued as a function ω : [0, ∞] → [0, ∞]. Indeed, it is immediate to check that the optimal modulus of continuity ω defined above is subadditive if the domain of \"f\" is convex: we have, for all \"s\" and \"t\":\n\nHowever, a uniformly continuous function on a general metric space admits a concave modulus of continuity if and only if the ratios formula_19 are uniformly bounded for all pairs (\"x\", \"x\"′) bounded away from the diagonal of \"X\"; this condition is certainly satisfied by any bounded uniformly continuous function; hence in particular, by any continuous function on a compact metric space.\n\nA sublinear modulus of continuity can easily found for any uniformly function which is a bounded perturbation of a Lipschitz function: if \"f\" is a uniformly continuous function with modulus of continuity ω, and \"g\" is a \"k\" Lipschitz function with uniform distance \"r\" from \"f\", then \"f\" admits the sublinear module of continuity min{ω(\"t\"), 2\"r\"+\"kt\"}. Conversely, at least for real-valued functions, any bounded, uniformly continuous perturbation of a Lipschitz function is a special uniformly continuous function; indeed more is true as shown below. Note that as an immediate consequence, any uniformly continuous function on a convex subset of a normed space has a sublinear growth: there are constants \"a\" and \"b\" such that |\"f\"(\"x\")| ≤ \"a\"|\"x\"|+\"b\" for all \"x\".\n\nThe above property for uniformly continuous function on convex domains admits a sort of converse at least in the case of real-valued functions: that is, every special uniformly continuous real-valued function \"f\" : \"X\" → R defined on a subset \"X\" of a normed space \"E\" admits extensions over \"E\" that preserves any subadditive modulus ω of \"f\". The least and the greatest of such extensions are respectively:\n\nAs remarked, any subadditive modulus of continuity is uniformly continuous: in fact, it admits itself as a modulus of continuity. Therefore, \"f\" and \"f*\" are respectively inferior and superior envelopes of ω-continuous families; hence still ω-continuous. Incidentally, by the Kuratowski embedding any metric space is isometric to a subset of a normed space. Hence, special uniformly continuous real-valued functions are essentially the restrictions of uniformly continuous functions on normed spaces. In particular, this construction provides a quick proof of the Tietze extension theorem on compact metric spaces. However, for mappings with values in more general Banach spaces than R, the situation is quite more complicated; the first non-trivial result in this direction is the Kirszbraun theorem.\n\nEvery special uniformly continuous real-valued function \"f\" : \"X\" → R defined on the metric space \"X\" is uniformly approximable by means of Lipschitz functions. Moreover, the speed of convergence in terms of the Lipschitz constants of the approximations is strictly related to the modulus of continuity of \"f\". Precisely, let ω be the minimal concave modulus of continuity of \"f\", which is\nLet δ(\"s\") be the uniform distance between the function \"f\" and the set Lip of all Lipschitz real-valued functions on \"C\" having Lipschitz constant \"s\" :\nThen the functions ω(\"t\") and δ(\"s\") can be related with each other via a Legendre transformation: more precisely, the functions 2δ(\"s\") and −ω(−\"t\") (suitably extended to +∞ outside their domains of finiteness) are a pair of conjugated convex functions, for\n\nSince ω(\"t\") = o(1) for \"t\" → 0, it follows that δ(\"s\") = o(1) for \"s\" → +∞, that exactly means that \"f\" is uniformly approximable by Lipschitz functions. Correspondingly, an optimal approximation is given by the functions\neach function \"f\" has Lipschitz constant \"s\" and\nin fact, it is the greatest \"s\"-Lipschitz function that realize the distance δ(\"s\"). For example, the α-Hölder real-valued functions on a metric space are characterized as those functions that can be uniformly approximated by \"s\"-Lipschitz functions with speed of convergence formula_27 while the almost Lipschitz functions are characterized by an exponential speed of convergence formula_28\n\n\n\nSteffens (2006, p. 160) attributes the first usage of omega for the modulus of continuity to Lebesgue (1909, p. 309/p. 75) where omega refers to the oscillation of a Fourier transform. De la Vallée Poussin (1919, pp. 7-8) mentions both names (1) \"modulus of continuity\" and (2) \"modulus of oscillation\" and then concludes \"but we choose (1) to draw attention to the usage we will make of it\".\n\nLet 1 ≤ \"p\"; let \"f\" : R → R a function of class \"L\", and let \"h\" ∈ R. The \"h\"-translation of \"f\", the function defined by (τ\"f\")(\"x\") := \"f\"(\"x\"−\"h\"), belongs to the \"L\" class; moreover, if 1 ≤ \"p\" < ∞, then as ǁ\"h\"ǁ → 0 we have:\n\nTherefore, since translations are in fact linear isometries, also\n\nas ǁ\"h\"ǁ → 0, uniformly on \"v\" ∈ R.\n\nIn other words, the map \"h\" → τ defines a strongly continuous group of linear isometries of \"L\". In the case \"p\" = ∞ the above property does not hold in general: actually, it exactly reduces to the uniform continuity, and defines the uniform continuous functions. This leads to the following definition, that generalizes the notion of a modulus of continuity of the uniformly continuous functions: a modulus of continuity \"L\" for a measurable function \"f\" : \"X\" → R is a modulus of continuity ω : [0, ∞] → [0, ∞] such that\n\nThis way, moduli of continuity also give a quantitative account of the continuity property shared by all \"L\" functions.\n\nIt can be seen that formal definition of the modulus uses notion of finite difference of first order:\n\nIf we replace that difference with a difference of order \"n\", we get a modulus of continuity of order \"n\":\n\n\n"}
{"id": "9050760", "url": "https://en.wikipedia.org/wiki?curid=9050760", "title": "Multi-compartment model", "text": "Multi-compartment model\n\nA multi-compartment model is a type of mathematical model used for describing the way materials or energies are transmitted among the \"compartments\" of a system. Each compartment is assumed to be a homogeneous entity within which the entities being modelled are equivalent. For instance, in a pharmacokinetic model, the compartments may represent different sections of a body within which the concentration of a drug is assumed to be uniformly equal.\n\nHence a multi-compartment model is a lumped parameters model.\n\nMulti-compartment models are used in many fields including pharmacokinetics, epidemiology, biomedicine, systems theory, complexity theory, engineering, physics, information science and social science. The circuits systems can be viewed as a multi-compartment model as well.\n\nIn systems theory, it involves the description of a network whose components are compartments that represent a population of elements that are equivalent with respect to the manner in which they process input signals to the compartment.\nMost commonly, the mathematics of multi-compartment models is simplified to provide only a single parameter—such as concentration—within a compartment.\n\nPossibly the simplest application of multi-compartment model is in the single-cell concentration monitoring (see the figure above). If the volume of a cell is \"V\", the mass of solute is \"q\", the input is \"u\"(\"t\") and the secretion of the solution is proportional to the density of it within the cell, then the concentration of the solution \"C\" within the cell over time is given by\n\nwhere \"k\" is the proportionality.\n\nAs the number of compartments increases, the model can be very complex and the solutions usually beyond ordinary calculation.\n\nThe formulae for n-cell multi-compartment models become:\n\nWhere \n\nOr in matrix forms:\n\nWhere\n\nIn the special case of a closed system (see below) i.e. where formula_9 then there is a general solution.\n\nWhere formula_11, formula_12, ... and formula_13 are the eigenvalues of formula_14; formula_15, formula_16, ... and formula_17 are the respective eigenvectors of formula_14; and formula_19, formula_20, ... and formula_21 are constants.\n\nHowever it can be shown that given the above requirement to ensure the 'contents' of a closed system are constant, then for every pair of eigenvalue and eigenvector then either formula_22 or formula_23 and also that one eigenvalue is 0, say formula_11\n\nSo\n\nWhere\n\nThis solution can be rearranged:\n\nThis somewhat inelegant equation demonstrates that all solutions of an \"n-cell\" multi-compartment model with constant or no inputs are of the form:\n\nWhere formula_30 is a \"nxn\" matrix and formula_12, formula_32, ... and formula_13 are constants.\nWhere formula_34\n\nGenerally speaking, as the number of compartments increase, it is challenging both to find the algebraic and numerical solutions of the model. However, there are special cases of models, which rarely exist in nature, when the topologies exhibit certain regularities that the solutions become easier to find. The model can be classified according to the interconnection of cells and input/output characteristics:\n\n\n\n"}
{"id": "2727288", "url": "https://en.wikipedia.org/wiki?curid=2727288", "title": "Multivector", "text": "Multivector\n\nIn multilinear algebra, a multivector, sometimes called Clifford number, is an element of the exterior algebra of a vector space . This algebra is graded, associative and alternating, and consists of linear combinations of simple -vectors (also known as decomposable -vectors or -blades) of the form\nwhere formula_2 are in .\n\n\"Multivector\" may mean either \"homogeneous\" elements (all terms of the linear combination have the same grade or degree , that is are the product of the same number of vectors), which are referred to as -vectors or -vectors, or may allow sums of terms in different degrees.\n\nIn differential geometry, a -vector is the antisymmetric tensor obtained by taking linear combinations of the wedge product of tangent vectors, for some integer . It is the dual concept to a -form.\n\nFor and , these are often called respectively \"scalars\", \"vectors\", \"bivectors\" and \"trivectors\"; they are respectively dual to 0-forms, 1-forms, 2-forms and 3-forms.\n\nThe wedge product operation used to construct multivectors is linear, associative and alternating, which reflect the properties of the determinant. This means for vectors u, v and w in a vector space \"V\" and for scalars \"α\", \"β\", the wedge product has the properties,\n\n\nThe product of \"p\" vectors is called a grade \"p\" multivector, or a \"p\"-vector. The maximum grade of a multivector is the dimension of the vector space \"V\".\n\nThe linearity of the wedge product allows a multivector to be defined as the linear combination of basis multivectors. There are () basis \"p\"-vectors in an \"n\"-dimensional vector space.\n\nThe \"p\"-vector obtained from the wedge product of \"p\" separate vectors in an \"n\"-dimensional space has components that define the projected -volumes of the \"p\"-parallelotope spanned by the vectors. The square root of the sum of the squares of these components defines the volume of the \"p\"-parallelotope.\n\nThe following examples show that a bivector in two dimensions measures the area of a parallelogram, and the magnitude of a bivector in three dimensions also measures the area of a parallelogram. Similarly, a three-vector in three dimensions measures the volume of a parallelepiped.\n\nIt is easy to check that the magnitude of a three-vector in four dimensions measures the volume of the parallelepiped spanned by these vectors.\n\nProperties of multivectors can be seen by considering the two dimensional vector space . Let the basis vectors be e and e, so u and v are given by\n\nand the multivector , also called a bivector, is computed to be\n\nThe vertical bars denote the determinant of the matrix, which is the area of the parallelogram spanned by the vectors u and v. The magnitude of is the area of this parallelogram. Notice that because \"V\" has dimension two the basis bivector is the only multivector in Λ\"V\".\n\nThe relationship between the magnitude of a multivector and the area or volume spanned by the vectors is an important feature in all dimensions. Furthermore, the linear functional version of a multivector that computes this volume is known as a differential form.\n\nMore features of multivectors can be seen by considering the three dimensional vector space . In this case, let the basis vectors be e, e, and e, so u, v and w are given by\n\nand the bivector is computed to be\n\nThe components of this bivector are the same as the components of the cross product. The magnitude of this bivector is the square root of the sum of the squares of its components.\n\nThis shows that the magnitude of the bivector is the area of the parallelogram spanned by the vectors u and v as it lies in the three-dimensional space \"V\". The components of the bivector are the projected areas of the parallelogram on each of the three coordinate planes.\n\nNotice that because \"V\" has dimension three, there is one basis three-vector in Λ\"V\". Compute the three-vector\nThis shows that the magnitude of the three-vector is the volume of the parallelepiped spanned by the three vectors u, v and w.\n\nIn higher-dimensional spaces, the component three-vectors are projections of the volume of a parallelepiped onto the coordinate three-spaces, and the magnitude of the three-vector is the volume of the parallelepiped as it sits in the higher-dimensional space.\n\nIn this section, we consider multivectors on a projective space \"P\", which provide a convenient set of coordinates for lines, planes and hyperplanes that have properties similar to the homogeneous coordinates of points, called Grassmann coordinates.\n\nPoints in a real projective space \"P\" are defined to be lines through the origin of the vector space R. For example, the projective plane \"P\" is the set of lines through the origin of R. Thus, multivectors defined on R can be viewed as multivectors on \"P\".\n\nA convenient way to view a multivector on \"P\" is to examine it in an affine component of \"P\", which is the intersection of the lines through the origin of R with a selected hyperplane, such as . Lines through the origin of R intersect the plane to define an affine version of the projective plane that only lacks the points , called the points at infinity.\n\nPoints in the affine component of the projective plane have coordinates . A linear combination of two points and defines a plane in R that intersects E in the line joining p and q. The multivector defines a parallelogram in R given by\nNotice that substitution of for p multiplies this multivector by a constant. Therefore, the components of are homogeneous coordinates for the plane through the origin of R.\n\nThe set of points on the line through p and q is the intersection of the plane defined by with the plane . These points satisfy , that is,\n\nwhich simplifies to the equation of a line\n\nThis equation is satisfied by points for real values of α and β.\n\nThe three components of that define the line \"λ\" are called the Grassmann coordinates of the line. Because three homogeneous coordinates define both a point and a line, the geometry of points is said to be dual to the geometry of lines in the projective plane. This is called the principle of duality.\n\nThree dimensional projective space, \"P\" consists of all lines through the origin of R. Let the three dimensional hyperplane, , be the affine component of projective space defined by the points . The multivector defines a parallelepiped in R given by\n\nNotice that substitution of for p multiplies this multivector by a constant. Therefore, the components of are homogeneous coordinates for the 3-space through the origin of R.\n\nA plane in the affine component is the set of points in the intersection of H with the 3-space defined by . These points satisfy , that is,\n\nwhich simplifies to the equation of a plane\n\nThis equation is satisfied by points for real values of \"α\", \"β\" and \"γ\".\n\nThe four components of that define the plane \"λ\" are called the Grassmann coordinates of the plane. Because four homogeneous coordinates define both a point and a plane in projective space, the geometry of points is dual to the geometry of planes.\n\nA line as the join of two points: In projective space the line \"λ\" through two points p and q can be viewed as the intersection of the affine space with the plane in R. The multivector provides homogeneous coordinates for the line\n\nThese are known as the Plücker coordinates of the line, though they are also an example of Grassmann coordinates.\n\nA line as the intersection of two planes: A line \"μ\" in projective space can also be defined as the set of points x that form the intersection of two planes \"π\" and \"ρ\" defined by grade three multivectors, so the points x are the solutions to the linear equations\n\nIn order to obtain the Plucker coordinates of the line \"μ\", map the multivectors \"π\" and \"ρ\" to their dual point coordinates using the Hodge star operator,\n\nthen\n\nSo, the Plücker coordinates of the line \"μ\" are given by\n\nBecause the six homogeneous coordinates of a line can be obtained from the join of two points or the intersection of two planes, the line is said to be self dual in projective space.\n\nW. K. Clifford combined multivectors with the inner product defined on the vector space, in order to obtain a general construction for hypercomplex numbers that includes the usual complex numbers and Hamilton's quaternions.\n\nThe Clifford product between two vectors u and v is linear and associative like the wedge product, and has the additional property that the multivector uv is coupled to the inner product by Clifford's relation,\n\nClifford's relation preserves the alternating property for the product of vectors that are perpendicular. This can be seen for the orthogonal unit vectors in R. Clifford's relation yields\n\ntherefore the basis vectors are alternating,\n\nIn contrast to the wedge product, the Clifford product of a vector with itself is no longer zero. To see this compute the product,\n\nwhich yields\n\nThe set of multivectors constructed using Clifford's product yields an associative algebra known as a Clifford algebra. Inner products with different properties can be used to construct different Clifford algebras.\n\nMultivectors play a central role in the mathematical formulation of physics known as geometric algebra. The term \"geometric algebra\" was used by E. Artin for matrix methods in projective geometry. It was D. Hestenes who used \"geometric algebra\" to describe the application of Clifford algebras to classical mechanics, This formulation was expanded to \"geometric calculus\" by D. Hestenes and G. Sobczyk, who provided new terminology for a variety of features in this application of Clifford algebra to physics. C. Doran and A. Lasenby show that Hestene's geometric algebra provides a convenient formulation for modern physics.\n\nIn geometric algebra, a multivector is defined to be the sum of different-grade \"k\"-blades, such as the summation of a scalar, a vector, and a \"2\"-vector. A sum of only \"k\"-grade components is called a \"k\"-vector, or a \"homogeneous\" multivector.\n\nThe highest grade element in a space is called a \"pseudoscalar\".\n\nIf a given element is homogeneous of a grade \"k\", then it is a \"k\"-vector, but not necessarily a \"k\"-blade. Such an element is a \"k\"-blade when it can be expressed as the wedge product of \"k\" vectors. A geometric algebra generated by a 4-dimensional Euclidean vector space illustrates the point with an example: The sum of any two blades with one taken from the XY-plane and the other taken from the ZW-plane will form a 2-vector that is not a 2-blade. In a geometric algebra generated by a Euclidean vector space of dimension 2 or 3, all sums of 2-blades may be written as a single 2-blade.\n\n\nIn the presence of a volume form (such as given an inner product and an orientation), pseudovectors and pseudoscalars can be identified with vectors and scalars, which is routine in vector calculus, but without a volume form this cannot be done without a choice.\n\nIn the algebra of physical space (the geometric algebra of Euclidean 3-space, used as a model of (3+1)-spacetime), a sum of a scalar and a vector is called a paravector, and represents a point in spacetime (the vector the space, the scalar the time).\n\nA bivector is therefore an element of the antisymmetric tensor product of a tangent space with itself.\n\nIn geometric algebra, also, a bivector is a grade 2 element (a 2-vector) resulting from the wedge product of two vectors, and so it is geometrically an \"oriented area\", in the same way a \"vector\" is an oriented line segment. If a and b are two vectors, the bivector has\n\nBivectors are connected to pseudovectors, and are used to represent rotations in geometric algebra.\n\nAs bivectors are elements of a vector space Λ\"V\" (where \"V\" is a finite-dimensional vector space with ), it makes sense to define an inner product on this vector space as follows. First, write any element in terms of a basis as\n\nwhere the Einstein summation convention is being used.\n\nNow define a map by insisting that\n\nwhere formula_31 are a set of numbers.\n\nBivectors play many important roles in physics, for example, in the classification of electromagnetic fields.\n\n"}
{"id": "40384850", "url": "https://en.wikipedia.org/wiki?curid=40384850", "title": "Octahedral cupola", "text": "Octahedral cupola\n\nIn 4-dimensional geometry, the octahedral cupola is a 4-polytope bounded by one octahedron and a parallel rhombicuboctahedron, connected by 20 triangular prisms, and 6 square pyramids.\n\nThe \"octahedral cupola\" can be sliced off from a runcinated 24-cell, on a hyperplane parallel to an octahedral cell. The cupola can be seen in a B and B Coxeter plane orthogonal projection of the runcinated 24-cell:\n\n"}
{"id": "23545", "url": "https://en.wikipedia.org/wiki?curid=23545", "title": "Psychological statistics", "text": "Psychological statistics\n\nPsychological statistics is application of formulas, theorems, numbers and laws to psychology. \nStatistical Methods for psychology include development and application statistical theory and methods for modeling psychological data. \nThese methods include psychometrics, Factor analysis, Experimental Designs, Multivariate Behavioral Research. The article also discusses journals in the same field Wilcox, R. (2012).\n\nPsychometrics deals with measurement of psychological attributes. It involved developing and applying statistical models for mental measurements (Lord and Novik, ; etc.) The measurement theories are divided into two major areas: (1) Classical test theory; (2) Item Response Theory (Nunnally, J. & Bernstein, I. (1994)).\n\nThe classical test theory or true score theory or reliability theory in statistics is a set of statistical procedures useful for development of psychological tests and scales. It is based on fundamental equation \nX = T + E\nwhere, X is total score, T is a true score and E is error of measurement. For each participant, it assumes that there exist a true score and it need to be obtained score (X) has to be as close to it as possible (Lord, F. M. , and Novick, M. R. ( 1 968), Raykov, T. & Marcoulides, G.A. (2010) ). The closeness of X has with T is expressed in terms of ratability of the obtained score. The reliability in terms of classical test procedure is correlation between true score and obtained score. The typical test construction procedures has following steps:\n\n(1) Determine the construct \n(2) Outline the behavioral domain of the construct\n(3) Write 3 to 5 times more items than desired test length\n(4) Get item content analyzed by experts and cull items\n(5) Obtain data on initial version of the test \n(8) After the second cull, make final version\n(9) Use it for research\n\nThe reliability is computed in specific ways. \n(A) Inter-Rater reliability: Inter-Rater reliability is estimate of agreement between independent raters. This is most useful for subjective responses. Cohen’s Kappa, Krippendorff’s Alpha, Intra-Class correlation coefficients, Correlation coefficients, Kendal’s concordance coefficient, etc. are useful statistical tools. \n(B) Test-Retest Reliability: Test-Retest Procedure is estimation of temporal consistency of the test. A test is administered twice to the same sample with a time interval. Correlation between two sets of scores is used as an estimate of reliability. Testing conditions are assumed to be identical. \n(C) Internal Consistency Reliability: Internal consistency reliability estimates consistency of items with each other. Split-half reliability (Spearman- Brown Prophecy) and Cronbach Alpha are popular estimates of this reliability . (Cronbach LJ (1951)). \n(D) Parallel Form Reliability: It is an estimate of consistency between two different instruments of measurement. The inter-correlation between two parallel forms of a test or scale is used as an estimate of parallel form reliability.\n\nValidity of a scale or test is ability of the instrument to measure what it purports to measure (Nunnally, J. & Bernstein, I. (1994)). Construct validity, Content Validity, Criterion Validity are types of validity. \nConstruct validity is estimated by convergent and discriminant validity and factor analysis. Convergent and discriminant validity are ascertained by correlation between similar of different constructs. \nContent Validity: Subject matter experts evaluate content validity. \nCriterion Validity is correlation between the test and a criterion variable (or variables) of the construct. Regression analysis, Multiple regression analysis, Logistic regression is used as an estimate of criterion validity. \nSoftware applications: The R software has ‘psych’ package that is useful for classical test theory analysis.\n\nThe modern test theory is based on latent trait model. Every item estimates the ability of the test taker. The ability parameter is called as theta (θ). The difficulty parameter is called b. the two important assumptions are local independence and unidimensionality. \nThe Item Response Theory has three models. They are one parameter logistic model, two parameter logistic model and three parameter logistic model. In addition, Polychromous IRT Model are also useful (Hambleton & Swaminathan, 1985).\n\nThe R Software has ‘ltm’, packages useful for IRT analysis.\n\nFactor analysis is at the core of psychological statistics. It has two schools: (1) Exploratory Factor analysis (2) Confirmatory Factor analysis\n\nThe exploratory factor analysis begins without a theory or with a very tentative theory. It is a dimension reduction technique. It is useful in psychometrics, multivariate analysis of data and data analytics. \nTypically a k-dimensional correlation matrix or covariance matrix of variables is reduced to k X r factor pattern matrix where r < k. Principal Component analysis and common factor analysis are two ways of extracting data. Principal axis factoring, ML factor analysis, alpha factor analysis and image factor analysis is most useful ways of EFA. \nIt employees various factor rotation methods which can be classified into orthogonal (resulting in uncorrelated factors) and oblique (resulting correlated factors).\n\nThe ‘psych’ package in R is useful for EFA.\n\nConfirmatory Factor Analysis (CFA) is factor analytic technique that begins with theory and test the theory by carrying out factor analysis. \nThe CFA is also called as latent structure analysis, which considers factor as latent variables causing actual observable variables. The basic equation of the CFA is\n\nX = Λξ + δ\n\nwhere, X is observed variables, Λ are structural coefficients, ξ are latent variables (factors) and δ are errors. \nThe parameters are estimated using ML methods however; other methods of estimation are also available. The chi-square test is very sensitive and hence various fit measures are used (Bollen,1989, Loehlin, 1992).\nR package ‘sem’, ‘lavaan’ are useful for the same.\n\nExperimental Methods are very popular in psychology. It has more than 100 years tradition. Experimental psychology has a status of sub-discipline in psychology .\nThe statistical methods are applied for designing and analyzing experimental data. They involve, t-test, ANOVA, ANCOVA, MANOVA, MANCOVA, binomial test, chi-square etc. are used for the analysis of the experimental data.\n\nMultivariate behavioral research is becoming very popular in psychology. These methods include Multiple Regression and Prediction; Moderated and Mediated Regression Analysis; Logistics Regression; Canonical Correlations; Cluster analysis; Multi-level modeling; Survival-Failure analysis; Structural Equations Modeling; hierarchical linear modelling etc. are very useful for psychological statistics (Hayes, 2013; Agresti, 1990; Loehlin, 1992; Menard, 2001; Tabachnick, & Fidell, 2007).\n\nThere are many specialized journals that publish advances in statistical analysis for psychology. Psychometrika is at the forefront. Educational and Psychological Measurement, Assessment, American Journal of Evaluation, Applied Psychological Measurement, Behavior Research Methods, British Journal of Mathematical and Statistical Psychology, Journal of Educational and Behavioral Statistics, Journal of Mathematical Psychology, Multivariate Behavioral Research, Psychological Assessment, Structural Equation Modeling are other useful journals.\n\nVarious software packages are available for statistical methods for psychological research. They can be classified as commercial software (e.g., JMP and SPSS) and Open-Source (e.g., R). Among the free-wares, the R software is most popular one. There are many online references for R and specialised books on R for Psychologist are also being written (e.g., Belhekar, 2016 ). The \"psych\" package of R is very useful for psychologists. Among others, \"lavaan\", \"sem\", \"ltm\", \"ggplot2\" are some of the popular packages. PSPP and KNIME are other free packages. Among the commercial packages include JMP, SPSS and SAS. JMP and SPSS are commonly reported in books.\n\n\n"}
{"id": "12761741", "url": "https://en.wikipedia.org/wiki?curid=12761741", "title": "Robinson's joint consistency theorem", "text": "Robinson's joint consistency theorem\n\nRobinson's joint consistency theorem is an important theorem of mathematical logic. It is related to Craig interpolation and Beth definability.\n\nThe classical formulation of Robinson's joint consistency theorem is as follows:\n\nLet formula_1 and formula_2 be first-order theories. If formula_1 and formula_2 are consistent and the intersection formula_5 is complete (in the common language of formula_1 and formula_2), then the union formula_8 is consistent. Note that a theory is complete if it decides every formula, i.e. either formula_9 or formula_10.\n\nSince the completeness assumption is quite hard to fulfill, there is a variant of the theorem:\n\nLet formula_1 and formula_2 be first-order theories. If formula_1 and formula_2 are consistent and if there is no formula formula_15 in the common language of formula_1 and formula_2 such that formula_18 and formula_19, then the union formula_8 is consistent.\n"}
{"id": "2614482", "url": "https://en.wikipedia.org/wiki?curid=2614482", "title": "S-plane", "text": "S-plane\n\nIn mathematics and engineering, the \"s\"-plane is the complex plane on which Laplace transforms are graphed. It is a mathematical domain where, instead of viewing processes in the time domain modeled with time-based functions, they are viewed as equations in the frequency domain. It is used as a graphical analysis tool in engineering and physics. \n\nA real function formula_1 in time formula_2 is translated into the \"s\"-plane by taking the integral of the function multiplied by formula_3 from formula_4 to formula_5 where \"s\" is a complex number with the form formula_6.\n\nThis transformation from the \"t\"-domain into the \"s\"-domain is known as a Laplace transform and the function formula_8 is called the Laplace transform of formula_1. \nOne way to understand what this equation is doing is to remember how Fourier analysis works. In Fourier analysis, harmonic sine and cosine waves are multiplied into the signal, and the resultant integration provides indication of a signal present at that frequency (i.e. the signal's energy at a point in the frequency domain). The Laplace transform does the same thing, but more generally. The formula_3 not only catches frequencies, but also the real formula_11 effects as well. Laplace transforms therefore cater not only for frequency response, but decay effects as well. For instance, a damped sine wave can be modeled correctly using Laplace transforms. \n\nA function in the s-plane can be translated back into a function of time using the inverse Laplace transform\nwhere the real number formula_13 is chosen so the integration path is within the region of convergence of formula_8. However rather than use this complicated integral, most functions of interest are translated using tables of Laplace transform pairs, and the Cauchy residue theorem.\nAnalysing the complex roots of an \"s\"-plane equation and plotting them on an Argand diagram can reveal information about the frequency response and stability of a real time system. This process is called root locus analysis.\n\n\n"}
{"id": "26221731", "url": "https://en.wikipedia.org/wiki?curid=26221731", "title": "Secondary vector bundle structure", "text": "Secondary vector bundle structure\n\nIn mathematics, particularly differential topology, the secondary vector bundle structure\nrefers to the natural vector bundle structure on the total space \"TE\" of the tangent bundle of a smooth vector bundle , induced by the push-forward of the original projection map .\nThis gives rise to a double vector bundle structure .\n\nIn the special case , where is the double tangent bundle, the secondary vector bundle is isomorphic to the tangent bundle\n\nLet be a smooth vector bundle of rank . Then the preimage of any tangent vector in in the push-forward of the canonical projection is a smooth submanifold of dimension , and it becomes a vector space with the push-forwards\n\nof the original addition and scalar multiplication\n\nas its vector space operations. The triple becomes a smooth vector bundle with these vector space operations on its fibres.\n\nLet be a local coordinate system on the base manifold with and let\n\nbe a coordinate system on formula_4 adapted to it. Then\n\nso the fiber of the secondary vector bundle structure at in is of the form\n\nNow it turns out that\n\ngives a local trivialization for , and the push-forwards of the original vector space operations read in the adapted coordinates as\n\nand\n\nso each fibre is a vector space and the triple is a smooth vector bundle.\n\nThe general Ehresmann connection on a vector bundle can be characterized in terms of the connector map\n\nwhere is the vertical lift, and is the vertical projection. The mapping\n\ninduced by an Ehresmann connection is a covariant derivative on in the sense that\n\nif and only if the connector map is linear with respect to the secondary vector bundle structure on . Then the connection is called \"linear\". Note that the connector map is automatically linear with respect to the tangent bundle structure .\n\n\n"}
{"id": "33273315", "url": "https://en.wikipedia.org/wiki?curid=33273315", "title": "Submodular set function", "text": "Submodular set function\n\nIn mathematics, a submodular set function (also known as a submodular function) is a set function whose value, informally, has the property that the difference in the incremental value of the function that a single element makes when added to an input set decreases as the size of the input set increases. Submodular functions have a natural diminishing returns property which makes them suitable for many applications, including approximation algorithms, game theory (as functions modeling user preferences) and electrical networks. Recently, submodular functions have also found immense utility in several real world problems in machine learning and artificial intelligence, including automatic summarization, multi-document summarization, feature selection, active learning, sensor placement, image collection summarization and many other domains.\n\nIf formula_1 is a finite set, a submodular function is a set function formula_2, where formula_3 denotes the power set of formula_1, which satisfies one of the following equivalent conditions.\n\nA nonnegative submodular function is also a subadditive function, but a subadditive function need not be submodular.\nIf formula_1 is not assumed finite, then the above conditions are not equivalent. In particular a function \nformula_15 defined by formula_16 if formula_17 is finite and formula_18 if formula_17 is infinite \nsatisfies the first condition above, but the second condition fails when formula_17 and formula_21 are infinite sets with finite intersection.\n\nA submodular function formula_15 is \"monotone\" if for every formula_23 we have that formula_24. Examples of monotone submodular functions include:\n\nA submodular function which is not monotone is called \"non-monotone\".\n\nA non-monotone submodular function formula_15 is called \"symmetric\" if for every formula_33 we have that formula_42.\nExamples of symmetric non-monotone submodular functions include:\n\nA non-monotone submodular function which is not symmetric is called asymmetric.\n\nThis extension is named after mathematician László Lovász. Consider any vector formula_59 such that each formula_60. Then the Lovász extension is defined as formula_61 where the expectation is over formula_62 chosen from the uniform distribution on the interval formula_63. The Lovász extension is a convex function.\n\nConsider any vector formula_64 such that each formula_60. Then the multilinear extension is defined as formula_66.\n\nConsider any vector formula_59 such that each formula_60. Then the convex closure is defined as formula_69. It can be shown that formula_70.\n\nConsider any vector formula_59 such that each formula_60. Then the concave closure is defined as formula_73.\n\n\nSubmodular functions have properties which are very similar to convex and concave functions. For this reason, an optimization problem which concerns optimizing a convex or concave function can also be described as the problem of maximizing or minimizing a submodular function subject to some constraints.\n\nThe simplest minimization problem is to find a set formula_33 which minimizes a submodular function subject to no constraints. This problem is computable in (strongly) polynomial time. Computing the minimum cut in a graph is a special case of this general minimization problem. However, even simple constraints like cardinality lower bound constraints make this problem NP hard, with polynomial lower bound approximation factors.\n\nUnlike minimization, maximization of submodular functions is usually NP-hard. Many problems, such as max cut and the maximum coverage problem, can be cast as special cases of this general maximization problem under suitable constraints. Typically, the approximation algorithms for these problems are based on either greedy algorithms or local search algorithms. The problem of maximizing a symmetric non-monotone submodular function subject to no constraints admits a 1/2 approximation algorithm. Computing the maximum cut of a graph is a special case of this problem. The more general problem of maximizing an arbitrary non-monotone submodular function subject to no constraints also admits a 1/2 approximation algorithm. The problem of maximizing a monotone submodular function subject to a cardinality constraint admits a formula_98 approximation algorithm. The maximum coverage problem is a special case of this problem. The more general problem of maximizing a monotone submodular function subject to a matroid constraint also admits a formula_98 approximation algorithm. Many of these algorithms can be unified within a semi-differential based framework of algorithms.\n\nApart from submodular minimization and maximization, another natural problem is Difference of Submodular Optimization. Unfortunately, this problem is not only NP hard, but also inapproximable. A related optimization problem is minimize or maximize a submodular function, subject to a submodular level set constraint (also called submodular optimization subject to submodular cover or submodular knapsack constraint). This problem admits bounded approximation guarantees. Another optimization problem involves partitioning data based on a submodular function, so as to maximize the average welfare. This problem is called the submodular welfare problem.\n\nSubmodular functions naturally occur in several real world applications, in economics, game theory, machine learning and computer vision. Owing the diminishing returns property, submodular functions naturally model costs of items, since there is often a larger discount, with an increase in the items one buys. Submodular functions model notions of complexity, similarity and cooperation when they appear in minimization problems. In maximization problems, on the other hand, they model notions of diversity, information and coverage. For more information on applications of submodularity, particularly in machine learning, see \n\n\n\n"}
{"id": "17218394", "url": "https://en.wikipedia.org/wiki?curid=17218394", "title": "Sulston score", "text": "Sulston score\n\nThe Sulston score is an equation used in DNA mapping to numerically assess the likelihood that a given \"fingerprint\" similarity between two DNA clones is merely a result of chance. Used as such, it is a test of statistical significance. That is, low values imply that similarity is \"significant\", suggesting that two DNA clones overlap one another and that the given similarity is not just a chance event. The name is an eponym that refers to John Sulston by virtue of his being the lead author of the paper that first proposed the equation's use.\n\nEach clone in a DNA mapping project has a \"fingerprint\", \"i.e.\" a set of DNA fragment lengths inferred from (1) enzymatically digesting the clone, (2) separating these fragments on a gel, and (3) estimating their lengths based on gel location. For each pairwise clone comparison, one can establish how many lengths from each set match-up. Cases having at least 1 match indicate that the clones \"might\" overlap because matches \"may\" represent the same DNA. However, the underlying sequences for each match are not known. Consequently, two fragments whose lengths match may still represent different sequences. In other words, matches do not conclusively indicate overlaps. The problem is instead one of using matches to probabilistically classify overlap status.\n\nBiologists have used a variety of means (often in combination) to discern clone overlaps in DNA mapping projects. While many are biological, \"i.e.\" looking for shared markers, others are basically mathematical, usually adopting probabilistic and/or statistical approaches.\n\nThe Sulston score is rooted in the concepts of Bernoulli and binomial processes, as follows. Consider two clones, formula_1 and formula_2, having formula_3 and formula_4 measured fragment lengths, respectively, where formula_5. That is, clone formula_1 has at least as many fragments as clone formula_2, but usually more. The Sulston score is the probability that at least formula_8 fragment lengths on clone formula_2 will be matched by any combination of lengths on formula_1. Intuitively, we see that, at most, there can be formula_4 matches. Thus, for a given comparison between two clones, one can measure the statistical significance of a match of formula_8 fragments, \"i.e.\" how likely it is that this match occurred simply as a result of random chance. Very low values would indicate a significant match that is highly unlikely to have arisen by pure chance, while higher values would suggest that the given match could be just a coincidence.\n\nIn a 2005 paper, Michael Wendl gave an example showing that the assumption of independent trials is not valid. So, although the traditional Sulston score does indeed represent a probability distribution, it is not actually the distribution characteristic of the fingerprint problem. Wendl went on to give the general solution for this problem in terms of the Bell polynomials, showing the traditional score overpredicts P-values by orders of magnitude. (P-values are very small in this problem, so we are talking, for example, about probabilities on the order of 10×10 versus 10×10, the latter Sulston value being 2 orders of magnitude too high.) This solution provides a basis for determining when a problem has sufficient information content to be treated by the probabilistic approach and is also a general solution to the birthday problem of 2 types.\n\nA disadvantage of the exact solution is that its evaluation is computationally intensive and, in fact, is not feasible for comparing large clones. Some fast approximations for this problem have been proposed.\n\n"}
{"id": "25309878", "url": "https://en.wikipedia.org/wiki?curid=25309878", "title": "The Human Use of Human Beings", "text": "The Human Use of Human Beings\n\nThe Human Use of Human Beings is a book by Norbert Wiener, the founding thinker of cybernetics theory and an influential advocate of automation; it was first published in 1950 and revised in 1954. The text argues for the benefits of automation to society; it analyzes the meaning of productive communication and discusses ways for humans and machines to cooperate, with the potential to amplify human power and release people from the repetitive drudgery of manual labor, in favor of more creative pursuits in knowledge work and the arts. The risk that such changes might harm society (through dehumanization or subordination of our species) is explored, and suggestions are offered on how to avoid such risk.\n\nThe word \"cybernetics\" refers to the theory of message transmission among people and machines. The thesis of the book is that: \n\nsociety can only be understood through a study of the messages and the communication facilities which belong to it; and that in the future development of these messages and communication facilities, messages between man and machines, between machines and man, and between machine and machine, are destined to play an ever-increasing part. (p. 16)\n\nCommunication methods have entered a new realm, involving new technologies. Whether a transmission is between people, or between people and machines, the process is similar in that information is sent by one party and received by another, which can send a response. This is a type of feedback. People, animals, and plants all have the ability to take certain actions in response to their environments; in the same way, machines have feedback systems in order for their performances to be altered or evaluated in accordance with results. In the context of human/machine society, Wiener offers a definition of the message as: \n\n\"a sequence of events in time which, though in itself has a certain contingency, strives to hold back nature's tendency toward disorder by adjusting its parts to various purposive ends\" (p. 27).\n\nThe physical world has a \"tendency toward disorder.\" Entropy (although a broad concept used in somewhat different ways across disciplines) roughly describes the way that isolated systems naturally become less and less organized with the passage of time; popularly understood as meaning a gradual decline into a state of chaos, the concept more accurately refers to the diffusion of energy toward a state of equilibrium, following the second law of thermodynamics.\n\nWiener believed that communication of information is essentially negentropic – it resists entropy –, because it relies on organizational structures. There are two kinds of possible disorganizational forces, passive and active:\"Nature offers resistance to decoding, but it does not show ingenuity in finding new and undecipherable methods for jamming our communication with the outer world\" (pp. 35–36).Nature's passive resistance is in contrast to active resistance, like that of a chess opponent. This is similar to Einstein's view, expressed in his famous comment: \"The Lord is subtle but he is not vicious\".\n\nAn increase of information, whether communicated by a living being or a machine, will increase organization. The feedback systems of an organism and those of a machine (informational organization in machines does not necessarily constitute \"vitality\" or a \"soul\") function in a similar way, allowing either to make assessments and act on the actual effectiveness of previous actions; when such feedback modifies not just a discrete action but an entire set of behaviors, Wiener calls this learning.\n\nThe individuality of a being is a certain intricate form, not an enduring substance. In order to understand an organism, it must be thought of as a pattern which maintains itself through homeostasis – life continues by maintaining an internal balance of various factors such as temperature and molecular structure. While the material substances that compose a living being may be constantly replaced by nearly identical ones, an organism continues functioning with the same identity as long as the pattern is kept sufficiently intact. Since patterns can be transmitted, modified, or duplicated, they are therefore a kind of information. Based on this, Wiener suggests it should be theoretically possible to transmit the entirety of a living person as a message (which is practically indistinguishable from the concept of physical teleportation) – although he admits that the obstacles to such a process would be great, because of the enormous amount of information embodied in a person, and the difficulty of reading or writing it.\n\nAccording to Wiener, the \"progress\" of human society as we conceive it today did not exist until four hundred years ago, but now we have entered \"a special period in the history of the world\" (p. 46). The progress of recent centuries has changed our world so dramatically that humans are being forced to adapt to the new environmental order or disorder that we are still creating. Wiener believes the quickness and range of our adaptability has always been the strong point of the human species, which distinguishes us from even the most intelligent of other living creatures. Our advancements in technology have created new opportunities along with new restrictions.\n\nIncreasingly better sensory mechanics will allow machines to react to changes in stimuli, and adapt more efficiently to their surroundings. This type of machine will be most useful in factory assembly lines, giving humans the freedom to supervise and use their creative abilities constructively. \n\nMedicine can benefit from robotic advances in the design of prostheses for the handicapped. Wiener mentions the Vocorder, a device from Bell Telephone Company that creates visual speech. He discusses the possibility of creating an automated prosthesis that inputs speech directly into the brain for processing, effectively giving deaf individuals the ability to \"hear\" speech again. Progress in these areas is ongoing and rapid, exemplified by such devices as the palatometer, a new device created to replace a damaged larynx; it uses a speech synthesizer to recreate words based on its ability to monitor tongue movements. This device effectively rids people with damaged larynxes of the robotic tones associated with artificial speech synthesizers (like the one famously used by disabled physicist Stephen Hawking), enabling people to have more natural social interactions.\n\nMachines, in Wiener's opinion, are meant to interact harmoniously with humanity and provide respite from the industrial trap we have made for ourselves. Wiener describes the automaton as inherently necessary to humanity's societal evolution. People could be free to expand their minds, pursue artistic careers, while automatons take over assembly line production to create necessary commodities. These machines must be \"used for the benefit of man, for increasing his leisure and enriching his spiritual life, rather than merely for profits and the worship of the machine as a new brazen calf\" (p. 162).\n\nThough hopeful that humanity will ultimately prosper by the use of automatons, he mentions a few ways this relationship with technology could be detrimental. Automatons must not be taken for granted, because with advances in technology that allow them to learn, the machines may be able to escape human control if humans do not continue proper supervision of them. We might become entirely dependent on them, or even controlled by them. There is danger in trusting decisions to something which cannot think abstractly, and may therefore be unlikely to identify with intellectual human values which are not purely utilitarian.\n\nNorbert Wiener's book was the forerunner of studies in cybernetics, and has influenced many theorists. It has impacted the fields of computers and technology, engineering, biology, sociology, and a broad range of other sciences. Numerous books have been published in relation to cybernetics theory which explore alternative concepts and models of feedback, human/machine relationships, systems science, and industrial advancement. William Ross Ashby, another founder of cybernetics, wrote the book \"Introduction to Cybernetics\", which presents many new interpretations and definitions. Other theorists have produced writings on systems, communication, and the human experience in cybernetics. N. Katherine Hayles, author of \"How We Became Posthuman: Virtual Bodies in Cybernetics, Literature, and Informatics\", describes the effects of technology in the age of virtual information and what it means for humans to live in an ever-advancing society. The American Society for Cybernetics (ASC) is a research association founded in 1964, the same year Wiener died, and is dedicated to the cooperative understanding and further improvement of cybernetics theory.\n\nThe Human Use of Human Beings has been translated to French in 1950 as \"Cybernétique et société\" (Paris : 10/18).\n\n\n"}
{"id": "2922561", "url": "https://en.wikipedia.org/wiki?curid=2922561", "title": "The Lady Tasting Tea", "text": "The Lady Tasting Tea\n\nThe Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century () is a book by David Salsburg about the history of modern statistics and the role it played in the development of science and industry.\n\nThe title comes from the \"lady tasting tea\", an example from the famous book, \"The Design of Experiments\", by Ronald A. Fisher. Regarding Fisher's example, the statistician Debabrata Basu wrote that \"the famous case of the 'lady tasting tea'\" was \"one of the two supporting pillars [...] of the randomization analysis of experimental data\".\n\n"}
{"id": "4510295", "url": "https://en.wikipedia.org/wiki?curid=4510295", "title": "UML Partners", "text": "UML Partners\n\nUML Partners was a consortium of system integrators and vendors convened in 1996 to specify the Unified Modeling Language (UML). Initially the consortium was led by Grady Booch, Ivar Jacobson, and James Rumbaugh of Rational Software. The UML Partners' UML 1.0 specification draft was proposed to the Object Management Group (OMG) in January 1997. \nDuring the same month the UML Partners formed a Semantics Task Force, chaired by Cris Kobryn, to finalize the semantics of the specification and integrate it with other standardization efforts. The result of this work, UML 1.1, was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.\n\nMembers of the consortium include:\n\n\n\n"}
{"id": "553027", "url": "https://en.wikipedia.org/wiki?curid=553027", "title": "Unitary perfect number", "text": "Unitary perfect number\n\nA unitary perfect number is an integer which is the sum of its positive proper unitary divisors, not including the number itself. (A divisor \"d\" of a number \"n\" is a unitary divisor if \"d\" and \"n\"/\"d\" share no common factors.) Some perfect numbers are not unitary perfect numbers, and some unitary perfect numbers are not regular perfect numbers.\n\n60 is a unitary perfect number, because 1, 3, 4, 5, 12, 15, and 20 are its proper unitary divisors, and 1 + 3 + 4 + 5 + 12 + 15 + 20 = 60. The first five, and only known, unitary perfect numbers are:\n\n6, 60, 90, 87360, 146361946186458562560000 \n\nThe respective sums of proper unitary divisors:\n\nThere are no odd unitary perfect numbers. This follows since one has 2 dividing the sum of the unitary divisors of an odd number (where \"d\"*(\"n\") is the number of distinct prime divisors of n). One gets this because the sum of all the unitary divisors is a multiplicative function and one has the sum of the unitary divisors of a power of a prime \"p\" is \"p\" + 1 which is even for all odd primes \"p\". Therefore, an odd unitary perfect number must have only one distinct prime factor, and it is not hard to show that a power of prime cannot be a unitary perfect number, since there are not enough divisors. \nIt is not known whether or not there are infinitely many unitary perfect numbers, or indeed whether there are any further examples beyond the five already known. A sixth such number would have at least nine odd prime factors.\n\n"}
{"id": "5449464", "url": "https://en.wikipedia.org/wiki?curid=5449464", "title": "Vizing's theorem", "text": "Vizing's theorem\n\nIn graph theory, Vizing's theorem (named for Vadim G. Vizing who published it in 1964) states that every simple undirected graph may be edge colored using a number of colors that is at most one larger than the maximum degree of the graph.\n\nAt least colors are always necessary, so the undirected graphs may be partitioned into two classes: \"class one\" graphs for which colors suffice, and \"class two\" graphs for which colors are necessary.\n\nWhen , the graph must itself be a matching, with no two edges adjacent, and its edge chromatic number is one. That is, all graphs with are of class one.\n\nWhen , the graph must be a disjoint union of paths and cycles. If all cycles are even, they can be 2-edge-colored by alternating the two colors around each cycle. However, if there exists at least one odd cycle, then no 2-edge-coloring is possible. That is, a graph with is of class one if and only if it is bipartite.\n\nMultigraphs do not in general obey Vizing's theorem. For instance, the multigraph formed by doubling each edge of a triangle has maximum degree four but cannot be edge-colored with fewer than six colors.\n\nThis proof is inspired by .\n\nLet be a simple undirected graph. We proceed by induction on , the number of edges. If the graph is empty, the theorem trivially holds. Let and suppose a proper -edge-coloring exists for all where .\n\nWe say that color } is missing in with respect to proper -edge-coloring if for all . Also, let -path from denote the unique maximal path starting in with -colored edge and alternating the colors of edges (the second edge has color , the third edge has color and so on), its length can be . Note that if is a proper -edge-coloring of then every vertex has a missing color with respect to .\n\nSuppose that no proper -edge-coloring of exists. This is equivalent to this statement:\n\nThis is equivalent, because if (1) doesn't hold, then we can interchange the colors and on the -path and set the color of to be , thus creating a proper -edge-coloring of from . The other way around, if a proper -edge-coloring exists, then we can delete an edge, restrict the coloring and (1) won't hold either.\n\nNow, let and be a proper -edge-coloring of and be missing in with respect to . We define to be a maximal sequence of neighbours of such that is missing in with respect to for all .\n\nWe define coloring as\n\nThen is a proper -edge-coloring of due to definition of . Also, note that the missing colors in are the same with respect to for all .\n\nLet be the color missing in with respect to , then is also missing in with respect to for all . Note that cannot be missing in , otherwise we could easily extend , therefore an edge with color is incident to for all . From the maximality of , there exists such that . From the definition of this holds: \n\nLet be the -path from with respect to . From (1), has to end in . But is missing in , so it has to end with an edge of color . Therefore, the last edge of is . Now, let be the -path from with respect to . Since is uniquely determined and the inner edges of are not changed in , the path uses the same edges as in reverse order and visits . The edge leading to clearly has color . But is missing in , so ends in . Which is a contradiction with (1) above.\n\nSeveral authors have provided additional conditions that classify some graphs as being of class one or class two, but do not provide a complete classification. For instance, if the vertices of the maximum degree in a graph form an independent set, or more generally if the induced subgraph for this set of vertices is a forest, then must be of class one.\n\n showed that a planar graph is of class one if its maximum degree is at least eight.\nIn contrast, he observed that for any maximum degree in the range from two to five, there exist\nplanar graphs of class two. For degree two, any odd cycle is such a graph, and for degree three, four, and five, these graphs can be constructed from platonic solids by replacing a single edge by a path of two adjacent edges.\n\nIn Vizing's planar graph conjecture, states that all simple, planar graphs with maximum degree six or seven are of class one, closing the remaining possible cases.\nThus, the only case of the conjecture that remains unsolved is that of maximum degree six. This conjecture has implications for the total coloring conjecture.\n\nThe planar graphs of class two constructed by subdivision of the platonic solids are not regular: they have vertices of degree two as well as vertices of higher degree.\nThe four color theorem (proved by ) on vertex coloring of planar graphs, is equivalent to the statement that every bridgeless 3-regular planar graph is of class one .\n\nIn 1969, Branko Grünbaum conjectured that every 3-regular graph with a polyhedral embedding on any two-dimensional oriented manifold such as a torus must be of class one. In this context, a polyhedral embedding is a graph embedding such that every face of the embedding is topologically a disk and such that the dual graph of the embedding is simple, with no self-loops or multiple adjacencies. If true, this would be a generalization of the four color theorem, which was shown by Tait to be equivalent to the statement that 3-regular graphs with a polyhedral embedding on a sphere are of class one. However, showed the conjecture to be false by finding snarks that have polyhedral embeddings on high-genus orientable surfaces. Based on this construction, he also showed that it is NP-complete to tell whether a polyhedrally embedded graph is of class one.\n\n describe a polynomial time algorithm for coloring the edges of any graph with colors, where is the maximum degree of the graph. That is, the algorithm uses the optimal number of colors for graphs of class two, and uses at most one more color than necessary for all graphs. Their algorithm follows the same strategy as Vizing's original proof of his theorem: it starts with an uncolored graph, and then repeatedly finds a way of recoloring the graph in order to increase the number of colored edges by one.\n\nMore specifically, suppose that is an uncolored edge in a partially colored graph. The algorithm of Misra and Gries may be interpreted as constructing a directed pseudoforest (a graph in which each vertex has at most one outgoing edge) on the neighbors of : for each neighbor of , the algorithm finds a color that is not used by any of the edges incident to , finds the vertex (if it exists) for which edge has color , and adds as an edge to . There are two cases:\nWith some simple data structures to keep track of the colors that are used and available at each vertex, the construction of and the recoloring steps of the algorithm can all be implemented in time , where is the number of vertices in the input graph. Since these steps need to be repeated times, with each repetition increasing the number of colored edges by one, the total time is .\n\nIn an unpublished technical report, claimed a faster formula_1 time bound for the same problem of coloring with colors.\n\nIn both and , Vizing mentions that his work was motivated by a theorem of showing that multigraphs could be colored with at most colors. Although Vizing's theorem is now standard material in many graph theory textbooks, Vizing had trouble publishing the result initially, and his paper on it appears in an obscure journal, \"Diskret. Analiz\".\n\n\n\n"}
{"id": "37702563", "url": "https://en.wikipedia.org/wiki?curid=37702563", "title": "Wedge (symbol)", "text": "Wedge (symbol)\n\nWedge (∧) is a symbol that looks similar to an in-line caret (^). It is used to represent various operations. In Unicode, the symbol is encoded and by codice_1 and codice_2 in TeX. The opposite symbol (∨) is called a vel, or sometimes a (descending) wedge. Some authors who call the descending wedge \"vel\" often call the ascending wedge \"ac\" (the corresponding Latin word for \"and\", also spelled \"atque\"), keeping their usage parallel.\n\nWedge is used to represent various operations:\n\n"}
{"id": "49024", "url": "https://en.wikipedia.org/wiki?curid=49024", "title": "Wolfram Mathematica", "text": "Wolfram Mathematica\n\nWolfram Mathematica (usually termed Mathematica) is a modern technical computing system spanning most areas of technical computing — including neural networks, machine learning, image processing, geometry, data science, visualizations, and others. The system is used in many technical, scientific, engineering, mathematical, and computing fields. It was conceived by Stephen Wolfram and is developed by Wolfram Research of Champaign, Illinois. The Wolfram Language is the programming language used in Mathematica.\n\nFeatures of Wolfram Mathematica include:\n\nWolfram Mathematica is split into two parts, the kernel and the front end. The kernel interprets expressions (Wolfram Language code) and returns result expressions.\n\nThe front end, designed by Theodore Gray in 1988, provides a GUI, which allows the creation and editing of Notebook documents containing program code with prettyprinting, formatted text together with results including typeset mathematics, graphics, GUI components, tables, and sounds. All content and formatting can be generated algorithmically or edited interactively. Standard word processing capabilities are supported, including real-time multi-lingual spell-checking.\n\nDocuments can be structured using a hierarchy of cells, which allow for outlining and sectioning of a document and support automatic numbering index creation. Documents can be presented in a slideshow environment for presentations. Notebooks and their contents are represented as Mathematica expressions that can be created, modified or analyzed by Mathematica programs or converted to other formats.\n\nThe front end includes development tools such as a debugger, input completion, and automatic syntax highlighting.\n\nAmong the alternative front ends is the Wolfram Workbench, an Eclipse based integrated development environment (IDE), introduced in 2006. It provides project-based code development tools for Mathematica, including revision management, debugging, profiling, and testing.\nThere is a plugin for IntelliJ IDEA based IDEs to work with Wolfram Language code which in addition to syntax highlighting can analyse and auto-complete local variables and defined functions.\nThe Mathematica Kernel also includes a command line front end. Other interfaces include JMath, based on GNU readline and MASH which runs self-contained Mathematica programs (with arguments) from the UNIX command line.\n\nIn recent years, the capabilities for high-performance computing have been extended with the introduction of packed arrays (version 4, 1999) and sparse matrices (version 5, 2003), and by adopting the GNU Multi-Precision Library to evaluate high-precision arithmetic.\n\nVersion 5.2 (2005) added automatic multi-threading when computations are performed on multi-core computers. This release included CPU specific optimized libraries. In addition Mathematica is supported by third party specialist acceleration hardware such as ClearSpeed.\n\nIn 2002, gridMathematica was introduced to allow user level parallel programming on heterogeneous clusters and multiprocessor systems and in 2008 parallel computing technology was included in all Mathematica licenses including support for grid technology such as Windows HPC Server 2008, Microsoft Compute Cluster Server and Sun Grid.\n\nSupport for CUDA and OpenCL GPU hardware was added in 2010. Also, since version 8 it can generate C code, which is automatically compiled by a system C compiler, such as GCC or Microsoft Visual Studio.\n\nThere are several ways to deploy applications written in Wolfram Mathematica:\n\nCommunication with other applications occurs through a protocol called Wolfram Symbolic Transfer Protocol (WSTP). It allows communication between the Wolfram Mathematica kernel and front-end, and also provides a general interface between the kernel and other applications. Wolfram Research freely distributes a developer kit for linking applications written in the programming language C to the Mathematica kernel through \"WSTP\". Using \"J/Link\"., a Java program can ask Mathematica to perform computations; likewise, a Mathematica program can load Java classes, manipulate Java objects and perform method calls. Similar functionality is achieved with \".NET /Link\", but with .NET programs instead of Java programs. Other languages that connect to Mathematica include Haskell, AppleScript, Racket, Visual Basic, Python and Clojure.\n\nLinks are available to many mathematical software packages including OpenOffice.org Calc, Microsoft Excel, MATLAB, R, SageMath (which can also pull up Mathematica), Singular, Wolfram SystemModeler, and Origin. Mathematical equations can be exchanged with other computational or typesetting software via MathML.\n\nMathematica includes interfaces to SQL databases (via Java Database Connectivity JDBC)., MongoDB and can read and write to Multichain and Bitcoin Blockchains. Mathematica can also install web services from a Web Services Description Language (WSDL) description. It can access HDFS data via Hadoop.\n\nMathematica can call a variety of cloud services to retrieve or send data including ArXiv, Bing, ChemSpider, CrossRef, Dropbox, Facebook, Factual, Federal Reserve, Fitbit, Flickr, Google (Analytics, Calendar, Contacts, Custom search, Plus, search, translate), Instagram, LinkedIn, MailChimp, Microsoft Translator, Mixpanel, OpenLibrary, OpenPHACTS, PubChem, PubMed, Pushbullet, Reddit, RunKeeper, SeatGeek, SurveyMonkey, TextTranslation, Twilio, Twitter, WebImageSearch, WebSearch, Wikipedia and Yelp.\n\nMathematica can capture real-time data via a link to LabVIEW, from financial data feeds and directly from hardware devices via GPIB (IEEE 488), USB and serial interfaces. It automatically detects and reads from HID devices. It can read directly from a range of Vernier sensors.\n\nWolfram Mathematica includes collections of curated data provided for use in computations. Mathematica is also integrated with Wolfram Alpha, an online computational knowledge answer engine which provides additional data, some of which is kept updated in real time. Some of the data sets include astronomical, chemical, geopolitical, language, biomedical and weather data, in addition to mathematical data (such as knots and polyhedra).\n\n\"BYTE\" in 1989 listed Mathematica as among the \"Distinction\" winners of the BYTE Awards, stating that it \"is another breakthrough Macintosh application ... it could enable you to absorb the algebra and calculus that seemed impossible to comprehend from a textbook\".\n\nWolfram Mathematica built on the ideas in Cole and Wolfram's earlier Symbolic Manipulation Program (SMP). The name of the program \"Mathematica\" was suggested to Stephen Wolfram by Apple cofounder Steve Jobs although Wolfram had thought about it earlier and rejected it.\n\nWolfram Research has released the following versions of Mathematica:\n\n"}
