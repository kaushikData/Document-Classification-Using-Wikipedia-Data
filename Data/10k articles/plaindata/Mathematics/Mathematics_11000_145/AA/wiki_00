{"id": "59146042", "url": "https://en.wikipedia.org/wiki?curid=59146042", "title": "Accuracy paradox", "text": "Accuracy paradox\n\nThe accuracy paradox is the paradoxical finding that accuracy is not a good metric for predictive models when classifying in predictive analytics. This is because a simple model may have a high level of accuracy but be too crude to be useful. For example, if the incidence of category A is dominant, being found in 99% of cases, then predicting that every case is category A will have an accuracy of 99%. Precision and recall are better measures in such cases.\n"}
{"id": "314919", "url": "https://en.wikipedia.org/wiki?curid=314919", "title": "Boolean prime ideal theorem", "text": "Boolean prime ideal theorem\n\nIn mathematics, the Boolean prime ideal theorem states that ideals in a Boolean algebra can be extended to prime ideals. A variation of this statement for filters on sets is known as the ultrafilter lemma. Other theorems are obtained by considering different mathematical structures with appropriate notions of ideals, for example, rings and prime ideals (of ring theory), or distributive lattices and \"maximal\" ideals (of order theory). This article focuses on prime ideal theorems from order theory.\n\nAlthough the various prime ideal theorems may appear simple and intuitive, they cannot be deduced in general from the axioms of Zermelo–Fraenkel set theory without the axiom of choice (abbreviated ZF). Instead, some of the statements turn out to be equivalent to the axiom of choice (AC), while others—the Boolean prime ideal theorem, for instance—represent a property that is strictly weaker than AC. It is due to this intermediate status between ZF and ZF + AC (ZFC) that the Boolean prime ideal theorem is often taken as an axiom of set theory. The abbreviations BPI or PIT (for Boolean algebras) are sometimes used to refer to this additional axiom.\n\nAn order ideal is a (non-empty) directed lower set. If the considered partially ordered set (poset) has binary suprema (a.k.a. joins), as do the posets within this article, then this is equivalently characterized as a non-empty lower set \"I\" that is closed for binary suprema (i.e. \"x\", \"y\" in \"I\" imply \"x\"formula_1\"y\" in \"I\"). An ideal \"I\" is prime if its set-theoretic complement in the poset is a filter. Ideals are proper if they are not equal to the whole poset.\n\nHistorically, the first statement relating to later prime ideal theorems was in fact referring to filters—subsets that are ideals with respect to the dual order. The ultrafilter lemma states that every filter on a set is contained within some maximal (proper) filter—an \"ultrafilter\". Recall that filters on sets are proper filters of the Boolean algebra of its powerset. In this special case, maximal filters (i.e. filters that are not strict subsets of any proper filter) and prime filters (i.e. filters that with each union of subsets \"X\" and \"Y\" contain also \"X\" or \"Y\") coincide. The dual of this statement thus assures that every ideal of a powerset is contained in a prime ideal.\n\nThe above statement led to various generalized prime ideal theorems, each of which exists in a weak and in a strong form. \"Weak prime ideal theorems\" state that every \"non-trivial\" algebra of a certain class has at least one prime ideal. In contrast, \"strong prime ideal theorems\" require that every ideal that is disjoint from a given filter can be extended to a prime ideal that is still disjoint from that filter. In the case of algebras that are not posets, one uses different substructures instead of filters. Many forms of these theorems are actually known to be equivalent, so that the assertion that \"PIT\" holds is usually taken as the assertion that the corresponding statement for Boolean algebras (BPI) is valid.\n\nAnother variation of similar theorems is obtained by replacing each occurrence of \"prime ideal\" by \"maximal ideal\". The corresponding maximal ideal theorems (MIT) are often—though not always—stronger than their PIT equivalents.\n\nThe Boolean prime ideal theorem is the strong prime ideal theorem for Boolean algebras. Thus the formal statement is:\n\nThe weak prime ideal theorem for Boolean algebras simply states:\n\nWe refer to these statements as the weak and strong \"BPI\". The two are equivalent, as the strong BPI clearly implies the weak BPI, and the reverse implication can be achieved by using the weak BPI to find prime ideals in the appropriate quotient algebra.\n\nThe BPI can be expressed in various ways. For this purpose, recall the following theorem:\n\nFor any ideal \"I\" of a Boolean algebra \"B\", the following are equivalent:\nThis theorem is a well-known fact for Boolean algebras. Its dual establishes the equivalence of prime filters and ultrafilters. Note that the last property is in fact self-dual—only the prior assumption that \"I\" is an ideal gives the full characterization. All of the implications within this theorem can be proven in ZF.\n\nThus the following (strong) maximal ideal theorem (MIT) for Boolean algebras is equivalent to BPI:\n\nNote that one requires \"global\" maximality, not just maximality with respect to being disjoint from \"F\". Yet, this variation yields another equivalent characterization of BPI:\n\nThe fact that this statement is equivalent to BPI is easily established by noting the following theorem: For any distributive lattice \"L\", if an ideal \"I\" is maximal among all ideals of \"L\" that are disjoint to a given filter \"F\", then \"I\" is a prime ideal. The proof for this statement (which can again be carried out in ZF set theory) is included in the article on ideals. Since any Boolean algebra is a distributive lattice, this shows the desired implication.\n\nAll of the above statements are now easily seen to be equivalent. Going even further, one can exploit the fact the dual orders of Boolean algebras are exactly the Boolean algebras themselves. Hence, when taking the equivalent duals of all former statements, one ends up with a number of theorems that equally apply to Boolean algebras, but where every occurrence of \"ideal\" is replaced by \"filter\". It is worth noting that for the special case where the Boolean algebra under consideration is a powerset with the subset ordering, the \"maximal filter theorem\" is called the ultrafilter lemma.\n\nSumming up, for Boolean algebras, the weak and strong MIT, the weak and strong PIT, and these statements with filters in place of ideals are all equivalent. It is known that all of these statements are consequences of the Axiom of Choice, \"AC\", (the easy proof makes use of Zorn's lemma), but cannot be proven in ZF (Zermelo-Fraenkel set theory without \"AC\"), if ZF is consistent. Yet, the BPI is strictly weaker than the axiom of choice, though the proof of this statement, due to J. D. Halpern and Azriel Lévy is rather non-trivial.\n\nThe prototypical properties that were discussed for Boolean algebras in the above section can easily be modified to include more general lattices, such as distributive lattices or Heyting algebras. However, in these cases maximal ideals are different from prime ideals, and the relation between PITs and MITs is not obvious.\n\nIndeed, it turns out that the MITs for distributive lattices and even for Heyting algebras are equivalent to the axiom of choice. On the other hand, it is known that the strong PIT for distributive lattices is equivalent to BPI (i.e. to the MIT and PIT for Boolean algebras). Hence this statement is strictly weaker than the axiom of choice. Furthermore, observe that Heyting algebras are not self dual, and thus using filters in place of ideals yields different theorems in this setting. Maybe surprisingly, the MIT for the duals of Heyting algebras is not stronger than BPI, which is in sharp contrast to the abovementioned MIT for Heyting algebras.\n\nFinally, prime ideal theorems do also exist for other (not order-theoretical) abstract algebras. For example, the MIT for rings implies the axiom of choice. This situation requires to replace the order-theoretic term \"filter\" by other concepts—for rings a \"multiplicatively closed subset\" is appropriate.\n\nA filter on a set \"X\" is a nonempty collection of nonempty subsets of \"X\" that is closed under finite intersection and under superset. An ultrafilter is a maximal filter. The ultrafilter lemma states that every filter on a set \"X\" is a subset of some ultrafilter on \"X\". This lemma is most often used in the study of topology. An ultrafilter that does not contain finite sets is called \"non-principal\".The ultrafilter lemma, and in particular the existence of non-principal ultrafilters (consider the filter of all sets with finite complements), follows easily from Zorn's lemma.\n\nThe ultrafilter lemma is equivalent to the Boolean prime ideal theorem, with the equivalence provable in ZF set theory without the axiom of choice. The idea behind the proof is that the subsets of any set form a Boolean algebra partially ordered by inclusion, and any Boolean algebra is representable as an algebra of sets by Stone's representation theorem.\n\nIntuitively, the Boolean prime ideal theorem states that there are \"enough\" prime ideals in a Boolean algebra in the sense that we can extend \"every\" ideal to a maximal one. This is of practical importance for proving Stone's representation theorem for Boolean algebras, a special case of Stone duality, in which one equips the set of all prime ideals with a certain topology and can indeed regain the original Boolean algebra (up to isomorphism) from this data. Furthermore, it turns out that in applications one can freely choose either to work with prime ideals or with prime filters, because every ideal uniquely determines a filter: the set of all Boolean complements of its elements. Both approaches are found in the literature.\n\nMany other theorems of general topology that are often said to rely on the axiom of choice are in fact equivalent to BPI. For example, the theorem that a product of compact Hausdorff spaces is compact is equivalent to it. If we leave out \"Hausdorff\" we get a theorem equivalent to the full axiom of choice.\n\nIn graph theory, the de Bruijn–Erdős theorem is another equivalent to BPI. It states that, if a given infinite graph requires at least some finite number in any graph coloring, then it has a finite subgraph that also requires .\n\nA not too well known application of the Boolean prime ideal theorem is the existence of a non-measurable set (the example usually given is the Vitali set, which requires the axiom of choice). From this and the fact that the BPI is strictly weaker than the axiom of choice, it follows that the existence of non-measurable sets is strictly weaker than the axiom of choice.\n\nIn linear algebra, the Boolean prime ideal theorem can be used to prove that any two bases of a given vector space have the same cardinality.\n\n"}
{"id": "8337948", "url": "https://en.wikipedia.org/wiki?curid=8337948", "title": "Bose–Einstein condensation (network theory)", "text": "Bose–Einstein condensation (network theory)\n\nBose–Einstein condensation in networks is a phase transition observed in complex networks that can be described with the same mathematical model as that explaining Bose–Einstein condensation in physics.\n\nIn physics, a Bose–Einstein condensate is a state of matter that occurs in certain gases at very low temperatures. Any elementary particle, atom, or molecule, can be classified as one of two types: a boson or a fermion. For example, an electron is a fermion, while a photon or a helium atom is a boson. In quantum mechanics, the energy of a (bound) particle is limited to a set of discrete values, called energy levels. An important characteristic of a fermion is that it obeys the Pauli exclusion principle, which states that no two fermions may occupy the same state. Bosons, on the other hand, do not obey the exclusion principle, and any number can exist in the same state. As a result, at very low energies (or temperatures), a great majority of the bosons in a Bose gas can be crowded into the lowest energy state, creating a Bose–Einstein condensate.\n\nA Bose–Einstein condensate is therefore a quantum phenomenon characteristic of boson particles. Nevertheless, a similar type of condensation transition can occur also in off-equilibrium classical systems and in particular, complex networks. In this context, a condensation phenomenon occurs when a distribution of a large number of elements in a large number of element classes becomes degenerate, i.e. instead of having an even distribution of elements in the classes, one class (or a few classes) become occupied by a finite fraction of all the elements of the system.\n\nCondensation transitions occur in traffic jams, where long queues of cars are found, in wealth distribution models where a few people might have a finite fraction of all the wealth, or in Ising spin glass models. However, the condensation transition in these models cannot in general be mapped to a Bose–Einstein condensation.\n\nA network is characterized by a set of nodes or vertices and a set of links between these nodes. In mathematics, graph theory describes networks in general. The theory of random graphs deals in particular with stochastic networks (networks in which each link is present with a given probability \"p\"). A large class of networks that describe real complex systems like the Internet, the world wide web, airport networks or the biological networks of molecular interactions, are described by random networks. Network theory is a recent field of research which investigates methods of characterizing and modeling real complex networks. In particular it has been found that many complex networks have universal features like the small world property and a scale-free degree distribution. The scale-free degree distribution of networks can be caused by the \"preferential attachment\" mechanism.\n\nIn the late 1990s, Ginestra Bianconi was a graduate student, working with Prof. Albert-László Barabási, a noted network theorist. At his request, she began investigating the fitness model, a model in which the network evolves with the \"preferential attachment\" mechanism but in addition, each node has an intrinsic quality or fitness that describe its ability to acquire new links. For example, in the world wide web each web page has a different content, in social networks different people might have different social skills, in airport networks each airport is connected to cities with unevenly distributed economic activity, etc. It was found that under certain conditions, a single node could acquire most, if not all of the links in the network, resulting in the network analog of a Bose–Einstein condensate. In particular, a perfect analogy could be drawn between the mathematics of the network and the mathematics of a Bose gas if each node in the network were thought of as an energy level, and each link as a particle. These results have implications for any real situation involving random graphs, including the world wide web, social networks, and financial markets.\n\nThe result of the efforts of Bose and Einstein is the concept of a Bose gas, governed by the Bose–Einstein statistics, which describes the statistical distribution of identical particles with integer spin, now known as bosons (such as the photon and helium-4). In Bose–Einstein statistics, any number of identical bosons can be in the same state. In particular, given an energy state , the number of non-interacting bosons in thermal equilibrium at temperature is given by the Bose occupation number\n\nwhere the constant is determined by an equation describing the conservation of the number of particles\n\nwith being the density of states of the system.\n\nThis last equation may lack a solution at low enough temperatures when for . In this case a critical temperature is found such that for the system is in a Bose-Einstein condensed phase and a finite fraction of the bosons are in the ground state.\n\nThe density of states depends on the dimensionality of the space. In particular formula_3 therefore for only in dimensions . Therefore, a Bose-Einstein condensation of an ideal Bose gas can only occur for dimensions .\n\nFor a uniform three-dimensional Bose gas consisting of non-interacting particles with no apparent internal degrees of freedom, the critical temperature is given by:\n\nwhere:\n\n\nThe evolution of many complex systems, including the World Wide Web, business, and citation networks, is encoded in the dynamic web describing the interactions between the system’s constituents. Despite their irreversible and nonequilibrium nature these networks follow Bose statistics and can undergo Bose–Einstein condensation. Addressing the dynamical properties of these nonequilibrium systems within the framework of equilibrium quantum gases predicts that the “first-mover-advantage,” “fit-get-rich (FGR),” and “winner-takes-all” phenomena observed in competitive systems are thermodynamically distinct phases of the underlying evolving networks.\n\nStarting from the fitness model, the mapping of a Bose gas to a network can be done by assigning an energy to each node, determined by its fitness through the relation\n\nwhere . In particular when all the nodes have equal fitness, when instead nodes with different \"energy\" have very different fitness. We assume that the network evolves through a modified preferential attachment mechanism. At each time a new node with energy drawn from a probability distribution enters in the network and attach a new link to a node chosen with probability:\n\nIn the mapping to a Bose gas, we assign to every new link linked by preferential attachment to node a particle in the energy state .\n\nThe continuum theory predicts that the rate at which links accumulate on node with \"energy \" is given by\n\nwhere formula_8 indicating the number of links attached to node that was added to the network at the time step formula_9. formula_10 is the partition function, defined as:\n\nThe solution of this differential equation is:\n\nwhere the dynamic exponent formula_13 satisfies formula_14, plays the role of the chemical potential, satisfying the equation\n\nwhere is the probability that a node has \"energy\" and \"fitness\" . In the limit, , the occupation number, giving the number of links linked to nodes with \"energy\" , follows the familiar Bose statistics\n\nThe definition of the constant in the network models is surprisingly similar to the definition of the chemical potential in a Bose gas. In particular for probabilities such that for at high enough value of we have a condensation phase transition in the network model. When this occurs, one node, the one with higher fitness acquires a finite fraction of all the links. The Bose–Einstein condensation in complex networks is therefore a topological phase transition after which the network has a star-like dominant structure.\n\nThe mapping of a Bose gas predicts the existence of two distinct phases as a function of the energy distribution. In the fit-get-rich phase, describing the case of uniform fitness, the fitter nodes acquire edges at a higher rate than older but less fit nodes. In the end the fittest node will have the most edges, but the richest node is not the absolute winner, since its share of the edges (i.e. the ratio of its edges to the total number of edges in the system) reduces to zero in the limit of large system sizes (Fig.2(b)). The unexpected outcome of this mapping is the possibility of Bose–Einstein condensation for , when the fittest node acquires a finite fraction of the edges and maintains this share of edges over time (Fig.2(c)).\n\nA representative fitness distribution that leads to a condensations\n\nwith .\n\nHowever, the existence of the Bose–Einstein condensation or the fit-get-rich phase does not depend on the temperature or of the system but depends only on the functional form of the fitness distribution of the system. In the end, falls out of all topologically important quantities. In fact it can be shown that Bose–Einstein condensation exists in the fitness model even without mapping to a Bose gas. A similar gelation can be seen in models with superlinear preferential attachment, however, it is not clear whether this is an accident or a deeper connection lies between this and the fitness model.\n\nIn evolutionary models each species reproduces proportionally to its fitness. In the infinite alleles model, each mutation generates a new species with a random fitness. This model was studied by the statistician J. F. C. Kingman and is known as the \"house of cards\" models. Depending on the fitness distribution, the model shows a condensation phase transition. Kingman did not realize that this phase transition could be mapped to a Bose–Einstein condensation. Recently the mapping of this model to a Bose–Einstein condensation was made in the context of a stochastic model for non-neutral ecologies. When the condensation phenomenon in an ecological system occurs, one species becomes dominant and strongly reduces the biodiversity of the system. This phase transition describes a basic stylized mechanism which is responsible for the large impact of invasive species in many ecological systems.\n\nHerbert Fröhlich is the source of the idea that quantum coherent waves could be generated in the biological neural network. His studies claimed to show that with an oscillating charge in a thermal bath, large numbers of quanta may condense into a single state known as a Bose condensate. Already in 1970 Pascual-Leone had shown that memory experiments can be modelled by the Bose–Einstein occupancy model. From this and a large body of other empirical findings (based on studies of EEG and psychometrics) Weiss and Weiss draw the generalized conclusion that memory span can be understood as the quantum number of a harmonic oscillator, where memory is to be mapped into an equilibrium Bose gas.\n"}
{"id": "20913204", "url": "https://en.wikipedia.org/wiki?curid=20913204", "title": "Change-making problem", "text": "Change-making problem\n\nThe change-making problem addresses the question of finding the minimum number of coins (of certain denominations) that add up to a given amount of money. It is a special case of the integer knapsack problem, and has applications wider than just currency.\n\nIt is also the most common variation of the \"coin change problem\", a general case of partition in which, given the available denominations of an infinite set of coins, the objective is to find out the number of possible ways of making a change for a specific amount of money, without considering the order of the coins.\n\nIt is weakly NP-hard, but may be solved optimally in pseudo-polynomial time by dynamic programming.\n\nCoin values can be modeled by a set of distinct positive integer values (whole numbers), arranged in increasing order as through . The problem is: given an amount , also a positive integer, to find a set of non-negative (positive or zero) integers }, with each representing how often the coin with value is used, which minimize the total number of coins \n\nsubject to\n\nAn application of change-making problem can be found in computing the ways one can make a nine dart finish in a game of darts.\n\nAnother application is computing the possible atomic (or isotopic) composition of a given mass/charge peak in mass spectrometry.\n\nA classic dynamic programming strategy works upward by finding the combinations of all smaller values that would sum to the current threshold. Thus, at each threshold, all previous thresholds are potentially considered to work upward to the goal amount \"W\". For this reason, this dynamic programming approach may require a number of steps that is at least quadratic in the goal amount \"W\".\n\nSince the problem exhibits optimal substructure, dynamic programming strategy can be applied to reach a solution as follows:\n\nFirstly, given that formula_3 is the optimal solution that contains exactly formula_4 coins, hence formula_5. It may seem as if formula_6, is the optimal solution for the sub-problem that contains exactly formula_7 coins.\n\nHowever, formula_8 does not contain formula_7 coins, and is not optimal, therefore the solution is known as formula_10, hence formula_11 becomes the optimal solution, since it must contain fewer coins than formula_8.\n\nFinally, combining formula_11 with formula_14 achieves the optimal solution that contains exactly formula_4 coins, while contradicting any assumptions that formula_3 is the optimal solution for the original problem.\n\nThe following is a dynamic programming implementation (with Python 3) which uses a matrix to keep track of the optimal solutions to sub-problems, and returns the minimum number of coins. A second matrix may be used to obtain the set of coins for the optimal solution.\n\nThe probabilistic convolution tree can also be used as a more efficient dynamic programming approach. The probabilistic convolution tree merges pairs of coins to produce all amounts which can be created by that pair of coins (with neither coin present, only the first coin present, only the second coin present, and both coins present), and then subsequently merging pairs of these merged outcomes in the same manner. This process is repeated until the final two collections of outcomes are merged into one, leading to a balanced binary tree with \"W log(W)\" such merge operations. Furthermore, by discretizing the coin values, each of these merge operations can be performed via convolution, which can often be performed more efficiently with the fast Fourier transform (FFT). In this manner, the probabilistic convolution tree may be used to achieve a solution in sub-quadratic number of steps: each convolution can be performed in \"n log(n)\", and the initial (more numerous) merge operations use a smaller \"n\", while the later (less numerous) operations require \"n\" on the order of \"W\".\n\nThe probabilistic convolution tree-based dynamic programming method also efficiently solves the probabilistic generalization of the change-making problem, where uncertainty or fuzziness in the goal amount \"W\" makes it a discrete distribution rather than a fixed quantity, where the value of each coin is likewise permitted to be fuzzy (for instance, when an exchange rate is considered), and where different coins may be used with particular frequencies.\n\nFor the so-called canonical coin systems, like those used in the US and many other countries, a greedy algorithm of picking the largest denomination of coin which is not greater than the remaining amount to be made will produce the optimal result. This is not the case for arbitrary coin systems, though: if the coin denominations were 1, 3 and 4, then to make 6, the greedy algorithm would choose three coins (4,1,1) whereas the optimal solution is two coins (3,3).\n\nHowever, there is a modified version of greedy algorithm to solve this question. In our case, we have \nwhere formula_18 and formula_19 \nsince formula_20 and formula_21.\n\nNow let formula_22. We can write\nwhere formula_24.\n\nFor example, given formula_25, we have formula_26.\nHence formula_27.\n\nTherefore \nwhere formula_29 denotes the largest integer less than or equal to formula_30\nand formula_31 denotes the remainder of formula_32 divided by 4.\n\nThe \"optimal denomination problem\" is a problem for people who design entirely new currencies. It asks what denominations should be chosen for the coins in order to minimize the average cost of making change, that is, the average number of coins needed to make change? The version of this problem assumed that the people making change will use the minimum number of coins (from the denominations available). One variation of this problem assumes that the people making change will use the \"greedy algorithm\" for making change, even when that requires more than the minimum number of coins. Most current currencies use a 1-2-5 series, but some other set of denominations would require fewer denominations of coins or a smaller average number of coins to make change or both.\n\n\n"}
{"id": "25413983", "url": "https://en.wikipedia.org/wiki?curid=25413983", "title": "Chen Chung Chang", "text": "Chen Chung Chang\n\nChen Chung Chang (Chinese: 张晨钟) is a mathematician who works in model theory. He obtained his PhD from Berkeley in 1955 on \"Cardinal and Ordinal Factorization of Relation Types\" under Alfred Tarski. He wrote the standard text on model theory. Chang's conjecture and Chang's model are named after him. He also proved the ordinal partition theorem (expressed in the arrow notation for Ramsey theory) ω→(ω,3), originally a problem of Erdős and Hajnal. He also introduced MV-algebras as models for Łukasiewicz logic. Chang is emeritus professor at the mathematics department of the University of California, Los Angeles.\n\n"}
{"id": "596179", "url": "https://en.wikipedia.org/wiki?curid=596179", "title": "Chowla–Selberg formula", "text": "Chowla–Selberg formula\n\nIn mathematics, the Chowla–Selberg formula is the evaluation of a certain product of values of the Gamma function at rational values in terms of values of the Dedekind eta function at imaginary quadratic irrational numbers. The result was essentially found by and rediscovered by .\n\nIn logarithmic form, the Chowla–Selberg formula states that in certain cases the sum\n\ncan be evaluated using the Kronecker limit formula. Here χ is the quadratic residue symbol modulo \"D\", where \"−D\" is the discriminant of an imaginary quadratic field. The sum is taken over 0 < \"r\" < \"D\", with the usual convention χ(\"r\") = 0 if \"r\" and \"D\" have a common factor. The function η is the Dedekind eta function, and \"h\" is the class number, and \"w\" is the number of roots of unity.\n\nThe origin of such formulae is now seen to be in the theory of complex multiplication, and in particular in the theory of periods of an abelian variety of CM-type. This has led to much research and generalization. In particular there is an analog of the Chowla–Selberg formula for p-adic numbers, involving a p-adic gamma function, called the Gross–Koblitz formula.\n\nThe Chowla–Selberg formula gives a formula for a finite product of values of the eta functions. By combining this with the theory of complex multiplication, one can give a formula for the individual absolute values of the eta function as\nfor some algebraic number α.\n\nUsing the reflection formula for the gamma function gives:\n\n\n\n"}
{"id": "41248158", "url": "https://en.wikipedia.org/wiki?curid=41248158", "title": "David A. Cox", "text": "David A. Cox\n\nDavid Archibald Cox (born September 23, 1948 in Washington, D.C.) is an American mathematician, working in algebraic geometry.\n\nCox graduated from Rice University with a bachelor's degree in 1970 and his Ph.D. in 1975 at Princeton University, under the supervision of Eric Friedlander (\"Tubular Neighborhoods in the Etale Topology\"). From 1974 to 1975, he was assistant professor at Haverford College and at Rutgers University from 1975 to 1979. In 1979, he became assistant professor and in 1988 professor at Amherst College.\n\nHe studies, among other things, étale homotopy theory, elliptic surfaces, computer-based algebraic geometry (such as Gröbner basis), Torelli sets and toric varieties, and history of mathematics. He is also known for several textbooks. He is a fellow of the American Mathematical Society.\n\nFrom 1987 to 1988 he was a guest professor at Oklahoma State University. In 2012, he received the Lester Randolph Ford Award for \"Why Eisenstein Proved the Eisenstein Criterion and Why Schönemann Discovered It First\".\n\n\n\n"}
{"id": "2906918", "url": "https://en.wikipedia.org/wiki?curid=2906918", "title": "David E. Rowe", "text": "David E. Rowe\n\nDavid E. Rowe (born August 11, 1950) is an American mathematician and historian. He studied mathematics and the history of science at the University of Oklahoma, and took a second doctorate in history at the Graduate Center of the City University of New York. He served as book review editor, managing editor, and editor of the journal \"Historia Mathematica\". In 1992, Rowe was appointed Professor of History of Mathematics and Natural Sciences at the Johannes Gutenberg University in Mainz where he presently teaches. His research has mainly focused on mathematics in Germany, but in recent years he has been concerned with Einstein's general theory of relativity and the broader cultural and political impact of Einstein's ideas. As part of this effort, he and have co-edited a source book entitled \"Einstein on Politics: His Private Thoughts and Public Stands on Nationalism, Zionism, War, Peace, and the Bomb\", published by Princeton University Press in 2007.\n\n\n"}
{"id": "53761797", "url": "https://en.wikipedia.org/wiki?curid=53761797", "title": "Donatella Danielli", "text": "Donatella Danielli\n\nDonatella Danielli is a professor of mathematics at Purdue University and is known for her contributions to partial differential equations, calculus of variations and geometric measure theory, with specific emphasis on free boundary problems. She received a Laurea cum Laude in Mathematics from the University of Bologna, Italy in 1989. She completed her doctorate in 1999 at Purdue, under the supervision of Carlos Kenig. Before joining the Purdue University faculty in 2001, she held positions at The Johns Hopkins University and at the Institut Mittag-Leffler in Sweden. She was also a visiting fellow at the Isaac Newton Institute for Mathematical Sciences in 2014.\n\nShe is the creator and organizer of the Symposia on Analysis and PDEs and of the Women in Mathematics Days, both at Purdue University. Her mathematics genealogy ID is 52294 .\n\nShe has been the recipient of a National Science Foundation CAREER Award in 2003, a Purdue University Teaching for Tomorrow Award in 2004, a Ruth and Joel Spira Award for Graduate Teaching in 2011, and a Butler Leadership in Action Award in 2013. She was awarded a Simons Fellow in Mathematics in 2014. In 2017, she became a Fellow of the American Mathematical Society \"for contributions to partial differential equations and geometric measure theory, and for service to the mathematical community\".\n\n\n\nShe and her husband, Nicola Garofalo (also a mathematician), have four children.\n\n"}
{"id": "59096855", "url": "https://en.wikipedia.org/wiki?curid=59096855", "title": "Dora Musielak", "text": "Dora Musielak\n\nDora Elia Musielak is a Mexican-American aerospace engineer, historian of mathematics, and book author. She is an expert on high-speed airbreathing jet engines, and an adjunct professor of mechanical and aerospace engineering at the University of Texas at Arlington.\n\nMusielak earned a bachelor's degree in aeronautical engineering from the Instituto Politécnico Nacional in Mexico in 1978, the first woman to earn a degree in this field there. She continued with a master's degree at the University of Tennessee in 1980, and a Ph.D. at the University of Alabama in Huntsville in 1994.\n\nHer employers have included Northrop Grumman, MSE Technology Applications, and ATK Allied Techsystems. She chaired the High Speed Air Breathing Propulsion Technical Committee of the American Institute of Aeronautics and Astronautics from 2014 to 2016.\n\nMusielak's 2004 self-published historical novel \"Sophie's Diary: A Mathematical Novel\", based on the life of mathematician Sophie Germain, was republished in a second edition in 2012 by the Mathematical Association of America.\n\nMusielak also wrote a biography of Germain, \"Prime Mystery: The Life and Mathematics of Sophie Germain\" (2015).\nHer other books include\n\"Kuxan Suum: Path to the Center of the Universe\" (2010) and\n\"Euler Celestial Analysis: Introduction to Spacecraft Orbit Mechanics\" (2018).\nThese remain self-published, through AuthorHouse.\n"}
{"id": "964342", "url": "https://en.wikipedia.org/wiki?curid=964342", "title": "Enterprise value", "text": "Enterprise value\n\nEnterprise value (EV), total enterprise value (TEV), or firm value (FV) is an economic measure reflecting the market value of a business. It is a sum of claims by all claimants: creditors (secured and unsecured) and shareholders (preferred and common). Enterprise value is one of the fundamental metrics used in business valuation, financial modeling, accounting, portfolio analysis, and risk analysis.\n\nEnterprise value is more comprehensive than market capitalization, which only reflects common equity. Importantly, EV reflects the opportunistic nature of business and may change substantially over time because of both external and internal conditions. Therefore, financial analysts often use a comfortable range of EV in their calculations.\n\nFor detailed information on the valuation process see Valuation (finance).\n\nA simplified way to understand the EV concept is to envision purchasing an entire business. If you settle with all the security holders, you pay EV. Counter-intuitively, increases or decreases in enterprise value do not necessarily correspond to \"value creation\" or value destruction\". Any acquisition of assets (whether paid for in cash or through share issues) will increase EV, whether or not those assets are productive. Similarly, reductions in capital intensity (for example by reducing working capital) will reduce EV.\n\nEV can be negative if the company, for example, holds abnormally high amounts of cash that is not reflected in the market value of the stock and total capitalization.\n\nAll the components relevant in liquidation analysis, since using absolute priority in a bankruptcy all securities senior to the equity have par claims. Generally, also, debt is less liquid than equity so that the \"market price\" may be significantly different from the price at which an entire debt issue could be purchased. In valuing equities, this approach is more conservative.\n\nCash is subtracted because it reduces the net cost to a potential purchaser. The effect applies whether the cash is used to issue dividends or to pay down debt.\n\nValue of minority interest is added because it reflects the claim on assets consolidated into the firm in question.\n\nValue of associate companies is subtracted because it reflects the claim on assets consolidated into other firms.\n\nEV should also include such special components as unfunded pension liabilities, employee stock options, environmental provisions, abandonment provisions, and so on, since they also reflect claims on the company.\n\nIt can be demonstrated that enterprise value depends on the probability of default (the rating) and works as a \"negative growth rate\" in the future.\n\nEnterprise value is only a useful measure of success or a useful measure of performance, when, apart from the rating, the earnings risks of the company are accounted for (for example by using the discount interest rate).\n\n\nUnlike market capitalization, where both the market price and the outstanding number of shares in issue are readily available and easy to find, it is virtually impossible to calculate an EV without making a number of adjustments to published data, including often subjective estimations of value:\n\nIn practice, EV calculations rely on reasonable estimates of the market value of these components. For example, in many professional valuations:\n\nWhen using valuation multiples such as EV/EBITDA and EV/EBIT, the numerator should correspond to the denominator. The EV should, therefore, correspond to the market value of the assets that were used to generate the profits in question, excluding assets acquired (and including assets disposed) during a different financial reporting period. This requires restating EV for any mergers and acquisitions (whether paid in cash or equity), significant capital investments or significant changes in working capital occurring after or during the reporting period being examined. Ideally, multiples should be calculated using the market value of the weighted average capital employed of the company during the comparable financial period.\n\nWhen calculating multiples over different time periods (e.g. historic multiples vs forward multiples), EV should be adjusted to reflect the weighted average invested capital of the company in each period.\n\n\n"}
{"id": "3069092", "url": "https://en.wikipedia.org/wiki?curid=3069092", "title": "Enumerative geometry", "text": "Enumerative geometry\n\nIn mathematics, enumerative geometry is the branch of algebraic geometry concerned with counting numbers of solutions to geometric questions, mainly by means of intersection theory.\n\nThe problem of Apollonius is one of the earliest examples of enumerative geometry. This problem asks for the number and construction of circles that are tangent to three given circles, points or lines. In general, the problem for three given circles has eight solutions, which can be seen as 2, each tangency condition imposing a quadratic condition on the space of circles. However, for special arrangements of the given circles, the number of solutions may also be any integer from 0 (no solutions) to six; there is no arrangement for which there are seven solutions to Apollonius' problem.\n\nA number of tools, ranging from the elementary to the more advanced, include:\n\nEnumerative geometry is very closely tied to intersection theory.\n\nEnumerative geometry saw spectacular development towards the end of the nineteenth century, at the hands of Hermann Schubert. He introduced for the purpose the Schubert calculus, which has proved of fundamental geometrical and topological value in broader areas. The specific needs of enumerative geometry were not addressed until some further attention was paid to them in the 1960s and 1970s (as pointed out for example by Steven Kleiman). Intersection numbers had been rigorously defined (by André Weil as part of his foundational programme 1942–6, and again subsequently), but this did not exhaust the proper domain of enumerative questions.\n\nNaïve application of dimension counting and Bézout's theorem yields incorrect results, as the following example shows. In response to these problems, algebraic geometers introduced vague \"fudge factors\", which were only rigorously justified decades later.\n\nAs an example, count the conic sections tangent to five given lines in the projective plane. The conics constitute a projective space of dimension 5, taking their six coefficients as homogeneous coordinates, and five points determine a conic, if the points are in general linear position, as passing through a given point imposes a linear condition. Similarly, tangency to a given line \"L\" (tangency is intersection with multiplicity two) is one quadratic condition, so determined a quadric in \"P\". However the linear system of divisors consisting of all such quadrics is not without a base locus. In fact each such quadric contains the Veronese surface, which parametrizes the conics\n\ncalled 'double lines'. This is because a double line intersects every line in the plane, since lines in the projective plane intersect, with multiplicity two because it is doubled, and thus satisfies the same intersection condition (intersection of multiplicity two) as a nondegenerate conic that is \"tangent\" to the line.\n\nThe general Bézout theorem says 5 general quadrics in 5-space will intersect in 32 = 2 points. But the relevant quadrics here are not in general position. From 32, 31 must be subtracted and attributed to the Veronese, to leave the correct answer (from the point of view of geometry), namely 1. This process of attributing intersections to 'degenerate' cases is a typical geometric introduction of a ''.\n\nHilbert's fifteenth problem was to overcome the apparently arbitrary nature of these interventions; this aspect goes beyond the foundational question of the Schubert calculus itself.\n\nIn 1984 H. Clemens studied the counting of the number of rational curves on a quintic threefold formula_1 and reached the following conjecture.\n\nThis conjecture has been resolved in the case formula_6, but is still open for higher formula_3.\n\nIn 1991 the paper about mirror symmetry on the quintic threefold in formula_8 from the string theoretical viewpoint gives numbers of degree d rational curves on formula_5 for all formula_10. Prior to this, algebraic geometers could calculate these numbers only for formula_11.\n\nSome of the historically important examples of enumerations in algebraic geometry include: \n\n\n"}
{"id": "38404291", "url": "https://en.wikipedia.org/wiki?curid=38404291", "title": "Eugene M. Luks", "text": "Eugene M. Luks\n\nEugene Michael Luks (born circa 1940) is an American mathematician and computer scientist, a professor emeritus of computer and information science at the University of Oregon. He is known for his research on the graph isomorphism problem and on algorithms for computational group theory.\n\nLuks did his undergraduate studies at the City College of New York, earning a bachelor's degree in 1960, and went on to graduate studies at the Massachusetts Institute of Technology, earning a doctorate in mathematics in 1966 under the supervision of Kenkichi Iwasawa. He taught at Tufts University from 1966 to 1968, and at Bucknell University from then until 1983, when he joined the University of Oregon faculty as chair of the computer and information science department. He retired in 2006, but was recalled in 2012–2013 to serve as interim chair.\n\nIn 1985, Luks won the Fulkerson Prize for his work showing that graph isomorphism could be tested in polynomial time for graphs with bounded maximum degree. In 2012 he became a fellow of the American Mathematical Society.\n\n"}
{"id": "41139312", "url": "https://en.wikipedia.org/wiki?curid=41139312", "title": "GIT quotient", "text": "GIT quotient\n\nIn algebraic geometry, an affine GIT quotient, or affine geometric invariant theory quotient, of an affine scheme formula_1 with an action by a group scheme \"G\" is the affine scheme formula_2, the prime spectrum of the ring of invariants of \"A\", and is denoted by formula_3. A GIT quotient is a categorical quotient: any invariant morphism uniquely factors through it.\n\nTaking Proj (of a graded ring) instead of formula_4, one obtains a projective GIT quotient (which is a quotient of the set of semistable points.)\n\nA GIT quotient is a categorical quotient of the locus of semistable points; i.e., \"the\" quotient of the semistable locus. Since the categorical quotient is unique, if there is a geometric quotient, then the two notions coincide: for example, one has formula_5 for an algebraic group \"G\" over a field \"k\" and closed subgroup \"H\".\n\nIf \"X\" is a complex smooth projective variety and if \"G\" is a reductive complex Lie group, then the GIT quotient of \"X\" by \"G\" is homeomorphic to the symplectic quotient of \"X\" by a maximal compact subgroup of \"G\" (Kempf–Ness theorem).\n\nLet \"G\" be a reductive group acting on a quasi-projective scheme \"X\" over a field and \"L\" a linearlized ample line bundle on \"X\". Let formula_6 be the section ring. By definition, the semistable locus formula_7 is the complement of the zero set formula_8 in \"X\"; in other words, it is the union of all open subsets formula_9 for global sections \"s\" of formula_10, \"n\" large. By ampleness, each formula_11 is affine; say formula_12 and so we can form the affine GIT quotient\nNote that formula_14 is of finite type by Hilbert's theorem on the ring of invariants. By universal property of categorical quotients, these affine quotients glue and result in\nwhich is the GIT quotient of \"X\" with respect to \"L\". Note that if \"X\" is projective; i.e., it is the Proj of \"R\", then the quotient formula_16 is given simply as the Proj of the ring of invariants formula_17.\n\nThe most interesting case is when the stable locus formula_18 is nonempty; formula_18 is the open set of semistable points that have finite stabilizers and orbits that are closed in formula_7. In such a case, the GIT quotient restricts to\nwhich has the property: every fiber is an orbit. That is to say, formula_22 is a genuine quotient (i.e., geometric quotient) and one writes formula_23. Because of this, when formula_18 is nonempty, the GIT quotient formula_25 is often referred to as a \"compactification\" of a geometric quotient of an open subset of \"X\".\n\nA difficult and seemingly open question is: which geometric quotient arises in the above GIT fashion? The question is of a great interest since the GIT approach produces an \"explicit\" quotient, as opposed to an abstract quotient, which is hard to compute. One known partial answer to this question is the following: let formula_26 be a locally factorial algebraic variety (for example, a smooth variety) with an action of formula_27. Suppose there are an open subset formula_28 as well as a geometric quotient formula_29 such that (1) formula_25 is an affine morphism and (2) formula_31 is quasi-projective. Then formula_32 for some linearlized line bundle \"L\" on \"X\".\n\nA simple example of a GIT quotient is given by the formula_33-action on formula_34 sending\nNotice that the monomials formula_36 generate the ring formula_37. Hence we can write the ring of invariants as\nScheme theoretically, we get the morphism\n\n\n\n"}
{"id": "669864", "url": "https://en.wikipedia.org/wiki?curid=669864", "title": "Grand Riemann hypothesis", "text": "Grand Riemann hypothesis\n\nIn mathematics, the grand Riemann hypothesis is a generalisation of the Riemann hypothesis and generalized Riemann hypothesis. It states that the nontrivial zeros of all automorphic \"L\"-functions lie on the critical line formula_1 with formula_2 a real number variable and formula_3 the imaginary unit.\n\nThe modified grand Riemann hypothesis is the assertion that the nontrivial zeros of all automorphic \"L\"-functions lie on the critical line or the real line.\n\n"}
{"id": "35261957", "url": "https://en.wikipedia.org/wiki?curid=35261957", "title": "Géza Grünwald", "text": "Géza Grünwald\n\nGéza Grünwald (October 18, 1910, Budapest – September 7, 1943) was a Jewish-Hungarian mathematician who worked on analysis.\n"}
{"id": "244437", "url": "https://en.wikipedia.org/wiki?curid=244437", "title": "Hamiltonian path", "text": "Hamiltonian path\n\nIn the mathematical field of graph theory, a Hamiltonian path (or traceable path) is a path in an undirected or directed graph that visits each vertex exactly once. A Hamiltonian cycle (or Hamiltonian circuit) is a Hamiltonian path that is a cycle. Determining whether such paths and cycles exist in graphs is the Hamiltonian path problem, which is NP-complete.\n\nHamiltonian paths and cycles are named after William Rowan Hamilton who invented the icosian game, now also known as \"Hamilton's puzzle\", which involves finding a Hamiltonian cycle in the edge graph of the dodecahedron. Hamilton solved this problem using the icosian calculus, an algebraic structure based on roots of unity with many similarities to the quaternions (also invented by Hamilton). This solution does not generalize to arbitrary graphs.\n\nDespite being named after Hamilton, Hamiltonian cycles in polyhedra had also been studied a year earlier by Thomas Kirkman, who, in particular, gave an example of a polyhedron without Hamiltonian cycles. Even earlier, Hamiltonian cycles and paths in the knight's graph of the chessboard, the knight's tour, had been studied in the 9th century in Indian mathematics by Rudrata, and around the same time in Islamic mathematics by . In 18th century Europe, knight's tours were published by Abraham de Moivre and Leonhard Euler.\n\nA \"Hamiltonian path\" or \"traceable path\" is a path that visits each vertex of the graph exactly once. A graph that contains a Hamiltonian path is called a traceable graph. A graph is Hamiltonian-connected if for every pair of vertices there is a Hamiltonian path between the two vertices.\n\nA \"Hamiltonian cycle\", \"Hamiltonian circuit\", \"vertex tour\" or \"graph cycle\" is a cycle that visits each vertex exactly once\n(except for the vertex that is both the start and end, which is visited twice). A graph that contains a Hamiltonian cycle is called a Hamiltonian graph.\n\nSimilar notions may be defined for \"directed graphs\", where each edge (arc) of a path or cycle can only be traced in a single direction (i.e., the vertices are connected with arrows and the edges traced \"tail-to-head\").\n\nA Hamiltonian decomposition is an edge decomposition of a graph into Hamiltonian circuits.\n\nA \"Hamilton maze\" is a type of logic puzzle in which the goal is to find the unique Hamiltonian cycle in a given graph.\n\n\nAny Hamiltonian cycle can be converted to a Hamiltonian path by removing one of its edges, but a Hamiltonian path can be extended to Hamiltonian cycle only if its endpoints are adjacent.\n\nAll Hamiltonian graphs are biconnected, but a biconnected graph need not be Hamiltonian (see, for example, the Petersen graph).\n\nAn Eulerian graph \"G\" (a connected graph in which every vertex has even degree) necessarily has an Euler tour, a closed walk passing through each edge of \"G\" exactly once.\nThis tour corresponds to a Hamiltonian cycle in the line graph \"L\"(\"G\"), so the line graph of every Eulerian graph is Hamiltonian. Line graphs may have other Hamiltonian cycles that do not correspond to Euler tours, and in particular the line graph \"L\"(\"G\") of every Hamiltonian graph \"G\" is itself Hamiltonian, regardless of whether the graph \"G\" is Eulerian.\n\nA tournament (with more than two vertices) is Hamiltonian if and only if it is strongly connected.\n\nThe number of different Hamiltonian cycles in a complete undirected graph on \"n\" vertices is and in a complete directed graph on \"n\" vertices is . These counts assume that cycles that are the same apart from their starting point are not counted separately.\n\nThe best vertex degree characterization of Hamiltonian graphs was provided in 1972 by the Bondy–Chvátal theorem, which generalizes earlier results by G. A. Dirac (1952) and Øystein Ore. Both Dirac's and Ore's theorems can also be derived from Pósa's theorem (1962). Hamiltonicity has been widely studied with relation to various parameters such as graph density, toughness, forbidden subgraphs and distance among other parameters. Dirac and Ore's theorems basically state that a graph is Hamiltonian if it has \"enough edges\".\n\nBondy–Chvátal theorem operates on the closure cl(\"G\") of a graph \"G\" with \"n\" vertices, obtained by repeatedly adding a new edge \"uv\" connecting a nonadjacent pair of vertices \"u\" and \"v\" with until no more pairs with this property can be found.\n\nBondy–Chvátal theorem\n\nAs complete graphs are Hamiltonian, all graphs whose closure is complete are Hamiltonian, which is the content of the following earlier theorems by Dirac and Ore.\n\nDirac (1952)\n\nOre (1960)\n\nThe following theorems can be regarded as directed versions:\n\nGhouila-Houiri (1960)\n\nMeyniel (1973)\n\nThe number of vertices must be doubled because each undirected edge corresponds to two directed arcs and thus the degree of a vertex in the directed graph is twice the degree in the undirected graph.\n\nRahman-Kaykobad (2005)\n\nThe above theorem can only recognize the existence of a Hamiltonian path in a graph and not a Hamiltonian Cycle.\n\nMany of these results have analogues for balanced bipartite graphs, in which the vertex degrees are compared to the number of vertices on a single side of the bipartition rather than the number of vertices in the whole graph.\n\n\n\n\n"}
{"id": "20665580", "url": "https://en.wikipedia.org/wiki?curid=20665580", "title": "Hesse normal form", "text": "Hesse normal form\n\nThe Hesse normal form named after Otto Hesse, is an equation used in analytic geometry, and describes a line in formula_1 or a plane in Euclidean space formula_2 or a hyperplane in higher dimensions. It is primarily used for calculating distances (see point-plane distance and point-line distance). \n\nIt is written in vector notation as\n\nThe dot formula_4 indicates the scalar product or dot product.\nThe vector formula_5 represents the unit normal vector of \"E\" or \"g\", that points from the origin of the coordinate system to the plane (or line, in 2D). The distance formula_6 is the distance from the origin to the plane (or line).\n\nThis equation is satisfied by all points \"P\", lying precisely in the plane \"E\" (or in 2D, on the line \"g\"), described by the location vector formula_7 that points from the origin of the coordinate system to \"P\".\n\nNote: For simplicity, the following derivation discusses the 3D case. However, it is also applicable in 2D.\n\nIn the normal form,\n\na plane is given by a normal vector formula_9 as well as an arbitrary position vector formula_10 of a point formula_11. The direction of formula_9 is chosen to satisfy the following inequality \n\nBy dividing the normal vector formula_9 by its magnitude formula_15, we obtain the unit (or normalized) normal vector\n\nand the above equation can be rewritten as\n\nSubstituting\n\nwe obtain the Hesse normal form\n\nIn this diagram, \"d\" is the distance from the origin. Because formula_20 holds for every point in the plane, it is also true at point \"Q\" (the point where the vector from the origin meets the plane E), with formula_21, per the definition of the Scalar product\n\nThe magnitude formula_23 of formula_24 is the shortest distance from the origin to the plane.\n"}
{"id": "13595525", "url": "https://en.wikipedia.org/wiki?curid=13595525", "title": "ICTP Ramanujan Prize", "text": "ICTP Ramanujan Prize\n\nThe ICTP Ramanujan Prize for Young Mathematicians from Developing Countries is a mathematics prize awarded annually by the International Centre for Theoretical Physics and named after the mathematician Srinivasa Ramanujan. It was founded in 2004, and was first awarded in 2005.\n\nThe prize is awarded to a researcher from a developing country less than 45 years of age who has conducted outstanding research in a developing country. The prize is supported by the Ministry of Science and Technology (India) and Norwegian Academy of Science and Letters through the Abel Fund, with the cooperation of the International Mathematical Union.\n\nSource: International Mathematical Union\n\n\n\n"}
{"id": "79378", "url": "https://en.wikipedia.org/wiki?curid=79378", "title": "Impartial game", "text": "Impartial game\n\nIn combinatorial game theory, an impartial game is a game in which the allowable moves depend only on the position and not on which of the two players is currently moving, and where the payoffs are symmetric. In other words, the only difference between player 1 and player 2 is that player 1 goes first. The game is played until a terminal position is reached. A terminal position is one from which no moves are possible. Then one of the players is declared the winner and the other the loser. Furthermore, impartial games are played with perfect information and no chance moves, meaning all information about the game and operations for both players are visible to both players.\n\nImpartial games include Nim, Sprouts, Kayles, Quarto, Cram, Chomp, Subtract a square, Notakto, and poset games. Go and chess are not impartial, as each player can only place or move pieces of their own color. Games such as poker, dice or dominos are not impartial games as they rely on chance.\n\nImpartial games can be analyzed using the Sprague–Grundy theorem, stating that every impartial game under the normal play convention is equivalent to a nimber. The representation of this nimber can change from game to game, but every possible state of any variation of an impartial game board should be able to have some nimber value. For example, several nim heaps in the game nim can be calculated, then summed using nimber addition, to give a nimber value for the game.\n\nA game that is not impartial can be a partisan game though some partisan games can still be evaluated using nimbers such as Domineering. Domineering would not be classified as an impartial game as players use differently acting pieces, one player with vertical dominoes, one with horizontal ones, thereby breaking the rule that each player must be able to act using the same operations.\n\nAll impartial games must meet the following conditions:\n\n"}
{"id": "192123", "url": "https://en.wikipedia.org/wiki?curid=192123", "title": "Index of accounting articles", "text": "Index of accounting articles\n\nThis page is an index of accounting topics.\n\nAccounting ethics - Accounting information system - Accounting research - Activity-Based Costing - Assets\n\nBalance sheet\n- Big Four auditors\n- Bond\n- Bookkeeping\n- Book value\n\nCash-basis accounting\n- Cash-basis versus accrual-basis accounting\n- Cash flow statement\n- Certified General Accountant\n- Certified Management Accountants\n- Certified Public Accountant\n- Chartered accountant\n- Chart of accounts\n- Common stock\n- Comprehensive income\n- Construction accounting\n- Convention of conservatism\n- Convention of disclosure\n- Cost accounting\n- Cost of capital\n- Cost of goods sold\n- Creative accounting\n- Credit\n- Credit note\n- Current asset\n- Current liability\n\nDebitcapital reserve\n- Debit note\n- Debt\n- Deficit (disambiguation)\n- Depreciation\n- Diluted earnings per share\n- Dividend\n- Double-entry bookkeeping system\n- Dual aspect\n\nE-accounting\n- EBIT\n- EBITDA\n- Earnings per share\n- Engagement Letter\n- Entity concept\n- Environmental accounting\n- Expense\n- Equity\n- Equivalent Annual Cost\n\nFinancial Accounting Standards Board\n- Financial accountancy\n- Financial audit\n- Financial reports\n- Financial statements\n- Fixed assets\n- Fixed assets management\n- Forensic accounting\n- Fraud deterrence\n- Free cash flow\n- Fund accounting\n\nGain\n- General ledger\n- Generally Accepted Accounting Principles\n- Going concern\n- Goodwill\n- Governmental Accounting Standards Board\n\nHistorical cost - History of accounting\n\nIncome\n- Income statement\n- Institute of Chartered Accountants in England and Wales\n- Institute of Chartered Accountants of Scotland\n- Institute of Management Accountants\n- Intangible asset\n- Interest\n- Internal audit\n- International Accounting Standards Board\n- International Accounting Standards Committee\n- International Accounting Standards\n- International Federation of Accountants\n- International Financial Reporting Standards\n- Inventory\n- Investment\n- Invoices\n- Indian Accounting Standards\n\nJob costing\n- Journal\n\nLean accounting\n- Ledger\n- Liability\n- Long-term asset\n- Long-term liabilities\n- Loss on sale of residential property\n\nMaker-checker\n- Management accounting\n- Management Assertions\n- Mark-to-market accounting\n- Matching principle\n- Materiality\n- Money measurement concept\n- Mortgage loan\n\nNegative assurance\n- Net income\n- Notes to the Financial Statements\n\nOBERAC\n- Online Accounting\n- Operating expense\n- Ownership equity\n\nPayroll\n- Petty cash\n- Philosophy of Accounting\n- Preferred stock\n- P/E ratio\n- Positive accounting\n- Positive assurance\n- PricewaterhouseCoopers\n- Profit and loss account\n- Pro-forma amount\n- Production accounting\n- Project accounting\n\nRetained earnings\n- Revenue\n- Revenue recognition\n\nSecurity\n- Sales journal\n- Social accounting\n- Spreadsheet\n- Statement of changes in equity\n- Statutory accounting principles\n- Stock option\n- Stock split\n- Stock\n- Shareholder\n- Shareholders' equity\n- South African Institute of Chartered Accountants\n- Sunk cost\n- Society of Accounting Education\n\nThroughput accounting\n- Trade credit\n- Treasury stock\n- Trial balance\n\nUK generally accepted accounting principles\n- Unified Ledger Accounting\n- U.S. Securities and Exchange Commission\n- US generally accepted accounting principles\n- Work sheet\n- Write off\n\n"}
{"id": "9732458", "url": "https://en.wikipedia.org/wiki?curid=9732458", "title": "Informating", "text": "Informating\n\nInformating is a term coined by Shoshana Zuboff in her book \"In the Age of the Smart Machine\" (1988). It is the process that translates descriptions and measurements of activities, events and objects into information. By doing so, these activities become visible to the organization.\n\nInformating has both an empowering and oppressing influence. On the one hand, as information processes become more powerful, the access to information is pushed to ever lower levels of the organization. Conversely, information processes can be used to monitor what Zuboff calls human agency. \n\nFrom \"In the Age of the Smart Machine\", Informating is described as:\n\"What is it, then, that distinguishes information technology from earlier generations of machine technology? As information technology is used to reproduce, extend, and improve upon the process of substituting machines for human agency, it simultaneously accomplishes something quite different. The devices that automate by translating information into action also register data about those automated activities, thus generating new streams of information. For example, computer-based, numerically controlled machine tools or microprocessor-based sensing devices not only apply programmed instructions to equipment but also convert the current state of equipment, product, or process into data. Scanner devices in supermarkets automate the checkout process and simultaneously generate data that can be used for inventory control, warehousing, scheduling of deliveries, and market analysis. The same systems that make it possible to automate office transactions also create a vast overview of an organization's operations, with many levels of data coordinated and accessible for a variety of analytical efforts.\" (Zuboff, 1988; p. 9)\"\nAs per the Zuboff's definition, and examples stated in her book surrounding the concept, confers the basic idea as, any activity, such as two friends who using Facebook communicate to other whats on their mind. Which could also be said they are informating, where using a technological tool such as Facebook, the two friends are converting their activity of thinking or per se something specific to their mutual context, into information. Thereby making their activity visible to friend or friends. \n\nWith advent of parallelism also the concept of informating is achieving further value. Because any activity done is able to produce multiple streams of information, which may be independent, yet held together by the context, in which they are embedded. \n\nRecently a Limited Liability Partnership was successfully registered with Ministry of Corporate Affairs, India with the use of the word \"Informating\" in the Company title.\n\n\"Informating\" as a concept, is being applied to several contexts. \n\nIt is a powerful learning tool because it shifts the power and control to students by giving access to new sources of information and relationships. November states, \"Students have access to content information that was previously only available in the teacher's edition of the textbook.\"\n\nWe consult communities on optimization of their knowledge management and informational activities. We are also involved in designing communication architecture for communities. We analyze Information Flow and Group Dynamics of our adopted communities. Our chief role is in the area of facilitating knowledge exchange and network building activities internationally. \n"}
{"id": "13653437", "url": "https://en.wikipedia.org/wiki?curid=13653437", "title": "Integration by parts operator", "text": "Integration by parts operator\n\nIn mathematics, an integration by parts operator is a linear operator used to formulate integration by parts formulae; the most interesting examples of integration by parts operators occur in infinite-dimensional settings and find uses in stochastic analysis and its applications.\n\nLet \"E\" be a Banach space such that both \"E\" and its continuous dual space \"E\" are separable spaces; let \"μ\" be a Borel measure on \"E\". Let \"S\" be any (fixed) subset of the class of functions defined on \"E\". A linear operator \"A\" : \"S\" → \"L\"(\"E\", \"μ\"; R) is said to be an integration by parts operator for \"μ\" if\n\nfor every \"C\" function \"φ\" : \"E\" → R and all \"h\" ∈ \"S\" for which either side of the above equality makes sense. In the above, D\"φ\"(\"x\") denotes the Fréchet derivative of \"φ\" at \"x\".\n\n\n\n"}
{"id": "191797", "url": "https://en.wikipedia.org/wiki?curid=191797", "title": "Kolmogorov's zero–one law", "text": "Kolmogorov's zero–one law\n\nIn probability theory, Kolmogorov's zero–one law, named in honor of Andrey Nikolaevich Kolmogorov, specifies that a certain type of event, called a \"tail event\", will either almost surely happen or almost surely not happen; that is, the probability of such an event occurring is zero or one.\n\nTail events are defined in terms of infinite sequences of random variables. Suppose\n\nis an infinite sequence of independent random variables (not necessarily identically distributed). Let formula_2 be the σ-algebra generated by the formula_3. Then, a tail event formula_4 is an event which is probabilistically independent of each finite subset of these random variables. (Note: formula_5 belonging to formula_2 implies that membership in formula_5 is uniquely determined by the values of the formula_3 but the latter condition is strictly weaker and does not suffice to prove the zero-one law.) For example, the event that the sequence converges, and the event that its sum converges are both tail events. In an infinite sequence of coin-tosses, a sequence of 100 consecutive heads occurring infinitely many times is a tail event.\n\nIntuitively, tail events are precisely those events whose occurrence can still be determined if an arbitrarily large but finite initial segment of the formula_3 are removed.\n\nIn many situations, it can be easy to apply Kolmogorov's zero–one law to show that some event has probability 0 or 1, but surprisingly hard to determine \"which\" of these two extreme values is the correct one.\n\nA more general statement of Kolmogorov's zero–one law holds for sequences of independent σ-algebras. Let (Ω,\"F\",\"P\") be a probability space and let \"F\" be a sequence of mutually independent σ-algebras contained in \"F\". Let\nbe the smallest σ-algebra containing \"F\", \"F\", …. Then Kolmogorov's zero–one law asserts that for any event\none has either \"P\"(\"F\") = 0 or 1.\n\nThe statement of the law in terms of random variables is obtained from the latter by taking each \"F\" to be the σ-algebra generated by the random variable \"X\". A tail event is then by definition an event which is measurable with respect to the σ-algebra generated by all \"X\", but which is independent of any finite number of \"X\". That is, a tail event is precisely an element of the intersection formula_12.\n\nAn invertible measure-preserving transformation on a standard probability space that obeys the 0-1 law is called a Kolmogorov automorphism. All Bernoulli automorphisms are Kolmogorov automorphisms but not \"vice versa\".\n\n\n\n"}
{"id": "722503", "url": "https://en.wikipedia.org/wiki?curid=722503", "title": "Linear system", "text": "Linear system\n\nA linear system is a mathematical model of a system based on the use of a linear operator.\nLinear systems typically exhibit features and properties that are much simpler than the nonlinear case.\nAs a mathematical abstraction or idealization, linear systems find important applications in automatic control theory, signal processing, and telecommunications. For example, the propagation medium for wireless communication systems can often be\nmodeled by linear systems.\n\nA general deterministic system can be described by an operator, formula_1, that maps an input, formula_2, as a function of formula_3 to an output, formula_4, a type of black box description. Linear systems satisfy the property of superposition. Given two valid inputs \nas well as their respective outputs\nthen a linear system must satisfy\nfor any scalar values formula_10 and formula_11.\nThe system is then defined by the equation formula_12, where formula_4 is some arbitrary function of time, and formula_2 is the system state. Given formula_4 and formula_1, formula_2 can be solved for. For example, a simple harmonic oscillator obeys the differential equation:\n\nIf\nthen formula_1 is a linear operator. Letting formula_21, we can rewrite the differential equation as formula_12, which shows that a simple harmonic oscillator is a linear system.\n\nThe behavior of the resulting system subjected to a complex input can be described as a sum of responses to simpler inputs. In nonlinear systems, there is no such relation. \nThis mathematical property makes the solution of modelling equations simpler than many nonlinear systems.\nFor time-invariant systems this is the basis of the impulse response or the frequency response methods (see LTI system theory), which describe a general input function formula_2 in terms of unit impulses or frequency components. \n\nTypical differential equations of linear time-invariant systems are well adapted to analysis using the Laplace transform in the continuous case, and the Z-transform in the discrete case (especially in computer implementations).\n\nAnother perspective is that solutions to linear systems comprise a system of functions which act like vectors in the geometric sense.\n\nA common use of linear models is to describe a nonlinear system by linearization. This is usually done for mathematical convenience.\n\nThe time-varying impulse response \"h\"(\"t\",\"t\") of a linear system is defined as the response of the system at time \"t\" = \"t\" to a single impulse applied at time \"t\" = \"t\". In other words, if the input \"x\"(\"t\") to a linear system is \n\nwhere δ(\"t\") represents the Dirac delta function, and the corresponding response \"y\"(\"t\") of the system is\n\nthen the function \"h\"(\"t\",\"t\") is the time-varying impulse response of the system. Since the system cannot respond before the input is applied the following causality condition must be satisfied:\n\nThe output of any general continuous-time linear system is related to the input by an integral which may be written over a doubly infinite range because of the causality condition:\n\nIf the properties of the system do not depend on the time at which it is operated then it is said to be time-invariant and h() is a function only of the time difference τ = t-t' which is zero for τ<0 (namely t<t'). By redefinition of h() it is then possible to write the input-output relation equivalently in any of the ways,\n\nLinear time-invariant systems are most commonly characterized by the Laplace transform of the impulse response function called the \"transfer function\" which is:\n\nIn applications this is usually a rational algebraic function of s. Because h(t) is zero for negative t, the integral may equally be written over the doubly infinite range and putting s = iω follows the formula for the \"frequency response function\":\n\nThe output of any discrete time linear system is related to the input by the time-varying convolution sum:\n\nor equivalently for a time-invariant system on redefining h(),\n\nwhere\n\nrepresents the lag time between the stimulus at time \"m\" and the response at time \"n\".\n\n"}
{"id": "4543610", "url": "https://en.wikipedia.org/wiki?curid=4543610", "title": "Loop braid group", "text": "Loop braid group\n\nThe loop braid group is a mathematical group structure that is used in some models of theoretical physics to model the exchange of particles with loop-like topologies within three dimensions of space and time.\n\nThe basic operations which generate a loop braid group for \"n\" loops are exchanges of two adjacent loops, and passing one adjacent loop through another. The topology forces these generators to satisfy some relations, which determine the group.\n\nTo be precise, the loop braid group on \"n\" loops is defined as the motion group of n disjoint circles embedded in a compact three-dimensional \"box\" diffeomorphic to the three-dimensional disk. A motion is a loop in the configuration space, which consists of all possible ways of embedding \"n\" circles into the 3-disk. This becomes a group in the same way as loops in any space can be made into a group; first, we define equivalence classes of loops by letting paths g and h be equivalent iff they are related by a (smooth) homotopy, and then we define a group operation on the equivalence classes by concatenation of paths. In his 1962 Ph.D. thesis, David M. Dahm was able to show that there is an injective homomorphism from this group into the automorphism group of the free group on n generators, so it is natural to identify the group with this subgroup of the automorphism group. One may also show that the loop braid group is isomorphic to the welded braid group, as is done for example in a paper by John C. Baez, Derek Wise, and Alissa Crans, which also gives some presentations of the loop braid group using the work of Xiao-Song Lin.\n\n"}
{"id": "4351071", "url": "https://en.wikipedia.org/wiki?curid=4351071", "title": "Magic graph", "text": "Magic graph\n\nA magic graph is a graph whose edges are labelled by positive integers, so that the sum over the edges incident with any vertex is the same, independent of the choice of vertex; or it is a graph that has such a labelling. If the integers are the first \"q\" positive integers, where \"q\" is the number of edges, the graph and the labelling are called supermagic.\n\nA graph is vertex-magic if its vertices can be labelled so that the sum on any edge is the same. It is total magic if its edges and vertices can be labelled so that the vertex label plus the sum of labels on edges incident with that vertex is a constant.\n\nThere are a great many variations on the concept of magic labelling of a graph. There is much variation in terminology as well. The definitions here are perhaps the most common.\n\nComprehensive references for magic labellings and magic graphs are Gallian (1998), Wallis (2001), and Marr and Wallis (2013).\n\nA semimagic square is an \"n\" × \"n\" square with the numbers 1 to \"n\" in its cells, in which the sum of each row and column is the same. A semimagic square is equivalent to a magic labelling of the complete bipartite graph \"K\". The two vertex sets of \"K\" correspond to the rows and the columns of the square, respectively, and the label on an edge \"rs\" is the value in row \"i\", column \"j\" of the semimagic square. \n\nThe definition of semimagic squares differs from the definition of magic squares in the treatment of the diagonals of the square. Magic squares are required to have diagonals with the same sum as the row and column sums, but for semimagic squares this is not required. Thus, every magic square is semimagic, but not vice versa.\n\n"}
{"id": "26920158", "url": "https://en.wikipedia.org/wiki?curid=26920158", "title": "Martin Hairer", "text": "Martin Hairer\n\nMartin Hairer (born 14 November 1975 in Geneva, Switzerland) is an Austrian mathematician working in the field of stochastic analysis, in particular stochastic partial differential equations. He is Professor of Mathematics at Imperial College London, having previously held appointments at the University of Warwick and the Courant Institute of New York University. In 2014 he was awarded the Fields Medal, one of the highest honours a mathematician can achieve.\n\nHairer attended the College Claparede Geneva where he received his high school diploma in 1994 , followed by the University of Geneva, where he obtained his Bachelor of Science degree in Mathematics in July 1998, Master of Science in Physics in October 1998 and PhD in Physics under the supervision of Jean-Pierre Eckmann in November 2001.\n\nHairer is one of the world's foremost leaders in the field of stochastic partial differential equations in particular, and in stochastic analysis and stochastic dynamics in general. By bringing new ideas to the subject he made fundamental advances in many important directions such as the study of variants of Hörmander's theorem, systematisation of the construction of Lyapunov functions for stochastic systems, development of a general theory of ergodicity for non-Markovian systems, multiscale analysis techniques, theory of homogenisation, theory of path sampling and, most recently, theory of rough paths and the newly introduced theory of regularity structures. \n\nUnder the name HairerSoft, he develops Macintosh software.\n\n\nHairer is an Austrian citizen and speaks French, German and English; he married the mathematician Li Xue-mei in 2003. His father is Ernst Hairer, a mathematician at the University of Geneva.\n"}
{"id": "561585", "url": "https://en.wikipedia.org/wiki?curid=561585", "title": "Master theorem (analysis of algorithms)", "text": "Master theorem (analysis of algorithms)\n\nIn the analysis of algorithms, the master theorem for divide-and-conquer recurrences provides an asymptotic analysis (using Big O notation) for recurrence relations of types that occur in the analysis of many divide and conquer algorithms. The approach was first presented by Jon Bentley, Dorothea Haken, and James B. Saxe in 1980, where it was described as a \"unifying method\" for solving such recurrences. The name \"master theorem\" was popularized by the widely used algorithms textbook \"Introduction to Algorithms\" by Cormen, Leiserson, Rivest, and Stein. \n\nNot all recurrence relations can be solved with the use of this theorem; its generalizations include the Akra–Bazzi method.\n\nConsider a problem that can be solved using a recursive algorithm such as the following:\n\nThe above algorithm divides the problem into a number of subproblems recursively, each subproblem being of size . Its call tree has a node for each recursive call, with the children of that node being the other calls made from that call. The leaves of the tree are the base cases of the recursion, the subproblems (of size less than \"k\") that do not recurse. The above example would have child nodes at each non-leaf node. Each node does an amount of work that corresponds to the size of the sub problem passed to that instance of the recursive call and given by formula_1. The total amount of work done by the entire algorithm is the sum of the work performed by all the nodes in the tree. \n\nThe runtime of an algorithm such as the 'p' above on an input of size 'n', usually denoted formula_2, can be expressed by the recurrence relation\nwhere formula_4 is the time to create the subproblems and combine their results in the above procedure. This equation can be successively substituted into itself and expanded to obtain an expression for the total amount of work done.\nThe master theorem allows many recurrence relations of this form to be converted to Θ-notation directly, without doing an expansion of the recursive relation.\n\nThe master theorem often yields asymptotically tight bounds to some recurrences from divide and conquer algorithms that partition an input into smaller subproblems of equal sizes, solve the subproblems recursively, and then combine the subproblem solutions to give a solution to the original problem. The time for such an algorithm can be expressed by adding the work that they perform at the top level of their recursion (to divide the problems into subproblems and then combine the subproblem solutions) together with the time made in the recursive calls of the algorithm. If formula_2 denotes the total time for the algorithm on an input of size formula_6, and formula_1 denotes the amount of time taken at the top level of the recurrence then the time can be expressed by a recurrence relation that takes the form:\nHere formula_6 is the size of an input problem, formula_10 is the number of subproblems in the recursion, and formula_11 is the factor by which the subproblem size is reduced in each recursive call.\nThe theorem below also assumes that, as a base case for the recurrence, formula_12 when formula_6 is less than some bound formula_14, the smallest input size that will lead to a recursive call.\n\nRecurrences of this form often satisfy one of the three following regimes, based on how the work to split/recombine the problem formula_1 relates to the \"critical exponent\" formula_16.\n\nA useful extension of Case 2 handles all values of formula_18:\n\nAs one can see from the formula above:\n\nNext, we see if we satisfy the case 1 condition:\n\nIt follows from the first case of the master theorem that\n\n(This result is confirmed by the exact solution of the recurrence relation, which is formula_25, assuming formula_26).\n\nformula_27\n\nAs we can see in the formula above the variables get the following values:\n\nNext, we see if we satisfy the case 2 condition:\n\nSo it follows from the second case of the master theorem:\n\nThus the given recurrence relation \"T\"(\"n\") was in Θ(\"n\" log \"n\").\n\nAs we can see in the formula above the variables get the following values:\n\nNext, we see if we satisfy the case 3 condition:\n\nThe regularity condition also holds:\n\nSo it follows from the third case of the master theorem:\n\nThus the given recurrence relation \"T\"(\"n\") was in Θ(\"n\"), that complies with the \"f\" (\"n\") of the original formula.\n\nThe following equations cannot be solved using the master theorem:\n\n\nIn the second inadmissible example above, the difference between formula_1 and formula_49 can be expressed with the ratio formula_56. It is clear that formula_57 for any constant formula_58. Therefore, the difference is not polynomial and the basic form of the Master Theorem does not apply. The extended form (case 2b) does apply, giving the solution formula_59.\n\n\n\n"}
{"id": "4562875", "url": "https://en.wikipedia.org/wiki?curid=4562875", "title": "Motion planning", "text": "Motion planning\n\nMotion planning (also known as the navigation problem or the piano mover's problem) is a term used in robotics for the process of breaking down a desired movement task into discrete motions that satisfy movement constraints and possibly optimize some aspect of the movement.\n\nFor example, consider navigating a mobile robot inside a building to a distant waypoint. It should execute this task while avoiding walls and not falling down stairs. A motion planning algorithm would take a description of these tasks as input, and produce the speed and turning commands sent to the robot's wheels. Motion planning algorithms might address robots with a larger number of joints (e.g., industrial manipulators), more complex tasks (e.g. manipulation of objects), different constraints (e.g., a car that can only drive forward), and uncertainty (e.g. imperfect models of the environment or robot).\n\nMotion planning has several robotics applications, such as autonomy, automation, and robot design in CAD software, as well as applications in other fields, such as animating digital characters, video game, artificial intelligence, architectural design, robotic surgery, and the study of biological molecules.\n\nA basic motion planning problem is to produce a continuous motion that connects a start configuration S and a goal configuration G, while avoiding collision with known obstacles. The robot and obstacle geometry is described in a 2D or 3D \"workspace\", while the motion is represented as a path in (possibly higher-dimensional) configuration space.\n\nA configuration describes the pose of the robot, and the configuration space C is the set of all possible configurations. For example:\n\nThe set of configurations that avoids collision with obstacles is called the free space C. The complement of C in C is called the obstacle or forbidden region.\n\nOften, it is prohibitively difficult to explicitly compute the shape of C. However, testing whether a given configuration is in C is efficient. First, forward kinematics determine the position of the robot's geometry, and collision detection tests if the robot's geometry collides with the environment's geometry.\n\nTarget space is a linear subspace of free space which denotes where we want the robot to move to. In global motion planning, target space is observable by the robot's sensors. However, in local motion planning, the robot cannot observe the target space in some states. To solve this problem, the robot goes through several virtual target spaces, each of which is located within the observable area (around the robot). A virtual target space is called a sub-goal.\n\nLow-dimensional problems can be solved with grid-based algorithms that overlay a grid on top of configuration space, or geometric algorithms that compute the shape and connectivity of C.\n\nExact motion planning for high-dimensional systems under complex constraints is computationally intractable. Potential-field algorithms are efficient, but fall prey to local minima (an exception is the harmonic potential fields). Sampling-based algorithms avoid the problem of local minima, and solve many problems quite quickly.\nThey are unable to determine that no path exists, but they have a probability of failure that decreases to zero as more time is spent.\n\nSampling-based algorithms are currently considered state-of-the-art for motion planning in high-dimensional spaces, and have been applied to problems which have dozens or even hundreds of dimensions (robotic manipulators, biological molecules, animated digital characters, and legged robots).\n\nGrid-based approaches overlay a grid on configuration space, and assume each configuration is identified with a grid point. At each grid point, the robot is allowed to move to adjacent grid points as long as the line between them is completely contained within C (this is tested with collision detection). This discretizes the set of actions, and search algorithms (like A*) are used to find a path from the start to the goal.\n\nThese approaches require setting a grid resolution. Search is faster with coarser grids, but the algorithm will fail to find paths through narrow portions of C. Furthermore, the number of points on the grid grows exponentially in the configuration space dimension, which make them inappropriate for high-dimensional problems.\n\nTraditional grid-based approaches produce paths whose heading changes are constrained to multiples of a given base angle, often resulting in suboptimal paths. Any-angle path planning approaches find shorter paths by propagating information along grid edges (to search fast) without constraining their paths to grid edges (to find short paths).\n\nGrid-based approaches often need to search repeatedly, for example, when the knowledge of the robot about the configuration space changes or the configuration space itself changes during path following. Incremental heuristic search algorithms replan fast by using experience with the previous similar path-planning problems to speed up their search for the current one.\n\nThese approaches are similar to grid-based search approaches except that they generate a paving covering entirely the configuration space instead of a grid. The paving is decomposed into two subpavings X,X made with boxes such that X ⊂ C ⊂ X. Characterizing C amounts to solve a set inversion problem. Interval analysis could thus be used when C cannot be described by linear inequalities in order to have a guaranteed enclosure.\n\nThe robot is thus allowed to move freely in X, and cannot go outside X. To both subpavings, a neighbor graph is built and paths can be found using algorithms such as Dijkstra or A*. When a path is feasible in X, it is also feasible in C. When no path exists in X from one initial configuration to the goal, we have the guarantee that no feasible path exists in C. As for the grid-based approach, the interval approach is inappropriate for high-dimensional problems, due to the fact that the number of boxes to be generated grows exponentially with respect to the dimension of configuration space.\n\nAn illustration is provided by the three figures on the right where a hook with two degrees of freedom has to move from the left to the right, avoiding two horizontal small segments. \nThe decomposition with subpavings using interval analysis also makes it possible to characterize the topology of C such as counting its number of connected components.\n\nPoint robots among polygonal obstacles\n\nTranslating objects among obstacles\n\nReward-based algorithms assume that the robot in each state (position and internal state, including direction) can choose between different actions (motion). However, the result of each action is not definite. In other words, outcomes (displacement) are partly random and partly under the control of the robot. The robot gets positive reward when it reaches the target and gets negative reward if it collides with an obstacle. These algorithms try to find a path which maximizes cumulative future rewards. The Markov decision process (MDP) is a popular mathematical framework that is used in many reward-based algorithms. The advantage of MDPs over other reward-based algorithms is that they generate the optimal path. The disadvantage of MDPs is that they limit the robot to choose from a finite set of actions. Therefore, the path is not smooth (similar to grid-based approaches).\n\nOne approach is to treat the robot's configuration as a point in a potential field that combines attraction to the goal, and repulsion from obstacles. The resulting trajectory is output as the path. This approach has advantages in that the trajectory is produced with little computation. However, they can become trapped in local minima of the potential field and fail to find a path, or can find a non-optimal path. The artificial potential fields can be treated as continuum equations similar to electrostatic potential fields (treating the robot like a point charge), or motion through the field can be discretized using a set of linguistic rules.\n\nSampling-based algorithms represent the configuration space with a roadmap of sampled configurations.\nA basic algorithm samples N configurations in C, and retains those in C to use as \"milestones\". A roadmap is then constructed that connects two milestones P and Q if the line segment PQ is completely in C. Again, collision detection is used to test inclusion in C. To find a path that connects S and G, they are added to the roadmap. If a path in the roadmap links S and G, the planner succeeds, and returns that path. If not, the reason is not definitive: either there is no path in C, or the planner did not sample enough milestones.\n\nThese algorithms work well for high-dimensional configuration spaces, because unlike combinatorial algorithms, their running time is not (explicitly) exponentially dependent on the dimension of C. They are also (generally) substantially easier to implement. They are probabilistically complete, meaning the probability that they will produce a solution approaches 1 as more time is spent. However, they cannot determine if no solution exists.\n\nGiven basic \"visibility\" conditions on C, it has been proven that as the number of configurations N grows higher, the probability that the above algorithm finds a solution approaches 1 exponentially. Visibility is not explicitly dependent on the dimension of C; it is possible to have a high-dimensional space with \"good\" visibility or a low-dimensional space with \"poor\" visibility. The experimental success of sample-based methods suggests that most commonly seen spaces have good visibility.\n\nThere are many variants of this basic scheme:\n\n\nA motion planner is said to be complete if the planner in finite time either produces a solution or correctly reports that there is none. Most complete algorithms are geometry-based. The performance of a complete planner is assessed by its computational complexity.\n\n\"Resolution completeness\" is the property that the planner is guaranteed to find a path if the resolution of an underlying grid is fine enough. Most resolution complete planners are grid-based or interval-based. The computational complexity of resolution complete planners is dependent on the number of points in the underlying grid, which is O(1/h), where h is the resolution (the length of one side of a grid cell) and d is the configuration space dimension.\n\n\"Probabilistic completeness\" is the property that as more “work” is performed, the probability that the planner fails to find a path, if one exists, asymptotically approaches zero. Several sample-based methods are probabilistically complete. The performance of a probabilistically complete planner is measured by the rate of convergence.\n\n\"Incomplete\" planners do not always produce a feasible path when one exists. Sometimes incomplete planners do work well in practice.\n\nMany algorithms have been developed to handle variants of this basic problem.\n\nHolonomic\n\nNonholonomic\n\nHybrid systems are those that mix discrete and continuous behavior. Examples of such systems are:\n\n\n\n\n\n"}
{"id": "1355025", "url": "https://en.wikipedia.org/wiki?curid=1355025", "title": "Numerical model of the Solar System", "text": "Numerical model of the Solar System\n\nA numerical model of the Solar System is a set of mathematical equations, which, when solved, give the approximate positions of the planets as a function of time. Attempts to create such a model established the more general field of celestial mechanics. The results of this simulation can be compared with past measurements to check for accuracy and then be used to predict future positions. Its main use therefore is in preparation of almanacs.\n\nThe simulations can be done in either Cartesian or in spherical coordinates. The former are easier, but extremely calculation intensive, and only practical on an electronic computer. As such only the latter was used in former times. Strictly speaking not much less calculation intensive, but it was possible to start with some simple approximations and then to add perturbations, as much as needed to reach the wanted accuracy.\n\nIn essence this mathematical simulation of the Solar System is a form of the \"N-body problem\". The symbol N represents the number of bodies, which can grow quite large if one includes 1 sun, 8 planets, dozens of moons and countless planetoids, comets and so forth. However the influence of the sun on any other body is so large, and the influence of all the other bodies on each other so small that the problem can be reduced to the analytically solvable 2-body problem. The result for each planet is an orbit, a simple description of its position as function of time. Once this is solved the influences moons and planets have on each other are added as small corrections. These are small compared to a full planetary orbit. Some corrections might be still several degrees large, while measurements can be made to an accuracy of better than 1″.\n\nAlthough this method is no longer used for simulations, it is still useful to find an approximate ephemeris as one can take the relatively simple main solution, perhaps add a few of the largest perturbations, and arrive without too much effort at the wanted planetary position. The disadvantage is that perturbation theory is very advanced mathematics.\n\nThe modern method consists of numerical integration in 3-dimensional space. One starts with a high accuracy value for the position (\"x\", \"y\", \"z\") and the velocity (\"v\", \"v\", \"v\") for each of the bodies involved. When also the mass of each body is known, the acceleration (\"a\", \"a\", \"a\") can be calculated from Newton's Law of Gravitation. Each body attracts each other body, the total acceleration being the sum of all these attractions. Next one chooses a small time-step Δ\"t\" and applies Newton's Second Law of Motion. The acceleration multiplied with Δ\"t\" gives a correction to the velocity. The velocity multiplied with Δ\"t\" gives a correction to the position. This procedure is repeated for all other bodies.\n\nThe result is a new value for position and velocity for all bodies. Then, using these new values one starts over the whole calculation for the next time-step Δ\"t\". Repeating this procedure often enough, and one ends up with a description of the positions of all bodies over time.\n\nThe advantage of this method is that for a computer it is a very easy job to do, and it yields highly accurate results for all bodies at the same time, doing away with the complex and difficult procedures for determining perturbations. The disadvantage is that one must start with highly accurate figures in the first place, or the results will drift away from the reality in time; that one gets \"x\", \"y\", \"z\" positions which are often first to be transformed into more practical ecliptical or equatorial coordinates before they can be used; and that it is an all or nothing approach. If one wants to know the position of one planet on one particular time, then all other planets and all intermediate time-steps are to be calculated too.\n\nIn the previous section it was assumed that acceleration remains constant over a small timestep Δt so that the calculation reduces to simply the addition of V × Δt to R and so forth. In reality this is not the case, except when one takes Δt so small that the number of steps to be taken would be prohibitively high. Because while at any time the position is changed by the acceleration, the value of the acceleration is determined by the instantaneous position. Evidently a full integration is needed.\n\nSeveral methods are available. First notice the needed equations:\n\nformula_1\n\nThis equation describes the acceleration all bodies i running from 1 to N exercise on a particular body j. It is a vector equation, so it is to be split in 3 equations for each of the X, Y, Z components, yielding:\n\nformula_2\n\nwith the additional relationships\n\nformula_3, formula_4\n\nlikewise for Y and Z.\n\nThe former equation (gravitation) may look foreboding, but its calculation is no problem. The latter equations (motion laws) seems simpler, but yet it cannot be calculated. Computers cannot integrate, they cannot work with infinitesimal values, so instead of dt we use Δt and bringing the resulting variable to the left:\n\nformula_5, and: formula_6\n\nRemember that a is still a function of time. The simplest way to solve these is just the Euler algorithm, which in essence is the linear addition described above. Limiting ourselves to 1 dimension only in some general computer language:\n\nAs in essence the acceleration used for the whole duration of the timestep, is the one as it was in the beginning of the timestep, this simple method has no high accuracy. Much better results are achieved by taking a mean acceleration, the average between the beginning value and the expected (unperturbed) end value:\n\nOf course still better results can be expected by taking intermediate values. This is what happens when using the Runge-Kutta method, especially the one of grade 4 or 5 are most useful.\n\nA completely different method is the use of Taylor series. In that case we write: formula_7\n\nbut rather than developing up to some higher derivative in r only, one can develop in r and v (that is r') by writing formula_8and then write out the factors \"f\" and \"g\" in a series.\n\nTo calculate the accelerations the gravitational attraction of each body on each other body is to be taken into account. As a consequence the amount of calculation in the simulation goes up with the square of the number of bodies: Doubling the number of bodies increases the work with a factor four. To increase the accuracy of the simulation not only more decimals are to be taken but also smaller timesteps, again quickly increasing the amount of work. Evidently tricks are to be applied to reduce the amount of work. Some of these tricks are given here.\n\nBy far the most important trick is the use of a proper integration method, as already outlined above.\n\nThe choice of units is important. Rather than to work in SI units, which would make some values extremely small and some extremely large, all units are to be scaled such that they are in the neighbourhood of 1. For example for distances in the Solar System the astronomical unit is most straightforward. If this is not done one is almost certain to see a simulation aborted in the middle of a calculation on a floating point overflow or underflow, and if not that bad, still accuracy is likely to get lost due to truncation errors.\n\nIf N is large (not so much in Solar System simulations, but more in galaxy simulations) it is customary to create dynamic groups of bodies. All bodies in a particular direction and on large distance from the reference body, which is being calculated at that moment, are taken together and their gravitational attraction is averaged over the whole group.\n\nThe total amount of energy and angular momentum of a closed system are conserved quantities. By calculating these amounts after every time step the simulation can be programmed to increase the stepsize Δt if they do not change significantly, and to reduce it if they start to do so. Combining the bodies in groups as in the previous and apply larger and thus less timesteps on the faraway bodies than on the closer ones, is also possible.\n\nTo allow for an excessively rapid change of the acceleration when a particular body is close to the reference body, it is customary to introduce a small softness parameter \"e\" so that\nformula_9\n\nIf the highest possible accuracy is needed, the calculations become much more complex. In the case of comets, nongravitational forces, such as radiation pressure and gas drag, must be taken into account. In the case of Mercury, and other planets for long term calculations, relativistic effects cannot be ignored. Then also the total energy is no longer a constant (because the four vector energy with linear momentum is). The finite speed of light also makes it important to allow for light-time effects, both classical and relativistic. Planets can no longer be considered as particles, but their shape and density must also be considered. For example, the flattening of the Earth causes precession, which causes the axial tilt to change, which affects the long-term movements of all planets.\nLong term models, going beyond a few tens of millions of years, are not possible due to the lack of stability of the Solar System.\n\n"}
{"id": "20893911", "url": "https://en.wikipedia.org/wiki?curid=20893911", "title": "Password Authenticated Key Exchange by Juggling", "text": "Password Authenticated Key Exchange by Juggling\n\nThe Password Authenticated Key Exchange by Juggling (or J-PAKE) is a password-authenticated key agreement protocol, proposed by Feng Hao and Peter Ryan. This protocol allows two parties to establish private and authenticated communication solely based on their shared (low-entropy) password without requiring a Public Key Infrastructure. It provides mutual authentication to the key exchange, a feature that is lacking in the Diffie–Hellman key exchange protocol.\n\nTwo parties, Alice and Bob, agree on a group formula_1 with generator formula_2 of prime order formula_3 in which the discrete log problem is hard. Typically a Schnorr group is used. In general, J-PAKE can use any prime order group that is suitable for public key cryptography, including Elliptic curve cryptography. Let formula_4 be their shared (low-entropy) secret, which can be a password or a hash of a password (formula_5). The protocol executes in two rounds.\n\n\n\nAfter Round 2, Alice computes formula_24. Similarly, Bob computes formula_25. With the same keying material formula_26, Alice and Bob can derive a session key using a Cryptographic hash function: formula_27.\n\nThe two-round J-PAKE protocol is completely symmetric. This helps significantly simplify the security analysis. For example, the proof that one party does not leak any password information in the data exchange must hold true for the other party based on the symmetry. This reduces the number of the needed security proofs by half.\n\nIn practice, it is more likely to implement J-PAKE in three flows since one party shall normally take the initiative. This can be done trivially without loss of security. Suppose Alice initiates the communication by sending to Bob: formula_28 and Zero-knowledge proofs. Then Bob replies with: formula_29 and Zero-knowledge proofs. Finally, Alice sends to Bob: formula_19 and a Zero-knowledge proof. Both parties can now derive the same session key.\n\nDepending on the application requirement, Alice and Bob may perform an optional key confirmation step. There are several ways to do it. A simple method described in SPEKE works as follows: Alice sends to Bob formula_31, and then Bob replies with formula_32. Alternatively, Alice and Bob can realize explicit key confirmation by using the newly constructed session key to encrypt a known value (or a random challenge). EKE, Kerberos and Needham-Schroeder all attempt to provide explicit key confirmation by exactly this method.\n\nGiven that the underlying Schnorr non-interactive zero-knowledge proof is secure, the J-PAKE protocol is proved to satisfy the following properties:\n\n\nIn 2015, Abdalla, Benhamouda and MacKenzie conducted an independent formal analysis of J-PAKE to prove its security in a random oracle model assuming algebraic adversaries.\n\nThe J-PAKE protocol is designed by combining random public keys in such a structured way to achieve a vanishing effect if both parties supplied exactly the same passwords. This is somehow similar to the Anonymous veto network protocol design. The essence of the idea, however, can be traced back to David Chaum's original Dining Cryptographers network protocol, where binary bits are combined in a structured way to achieve a vanishing effect.\n\nJ-PAKE has been implemented in OpenSSL and OpenSSH as an experimental authentication protocol. It was removed from the OpenSSH source code at the end of January 2014. It has also been implemented in NSS and was used by Firefox Sync version 1.1 but discontinued in 1.5 which uses a different key exchange and storage method. Mozilla's J-PAKE server was shut down along with the Sync 1.1 storage servers on 30 September 2015. Pale Moon continues to use J-PAKE as part of its Sync service. Since February 2013, J-PAKE has been added to the lightweight API in Bouncycastle (1.48 and onwards). J-PAKE is also used in the Thread (network protocol)\n\nJ-PAKE has been included in ISO/IEC 11770-4 (2017) as an international standard. It is also published in RFC 8236.\n\n"}
{"id": "43782287", "url": "https://en.wikipedia.org/wiki?curid=43782287", "title": "Permutation model", "text": "Permutation model\n\nIn mathematical set theory, a permutation model is a model of set theory with atoms (ZFA) constructed using a group of permutations of the atoms. A symmetric model is similar except that it is a model of ZF (without atoms) and is constructed using a group of permutations of a forcing poset. One application is to show the independence of the axiom of choice from the other axioms of ZFA or ZF.\nPermutation models were introduced by and developed further by . \nSymmetric models were introduced by Paul Cohen.\n\nSuppose that \"A\" is a set of atoms, and \"G\" is a group of permutations of \"A\". A normal filter of \"G\" is a collection \"F\" of subgroups of \"G\" such that \n\nIf \"V\" is a model of ZFA with \"A\" the set of atoms, then an element of \"V\" is called symmetric if the subgroup fixing it is in \"F\", and is called hereditarily symmetric if it and all elements of its transitive closure are symmetric. The permutation model consists of all hereditarily symmetric elements, and is a model of ZFA.\n\nA filter on a group can be constructed from an invariant ideal on of the Boolean algebra of subsets of \"A\" containing all elements of \"A\". Here an ideal is a collection \"I\" of subsets of \"A\" closed under taking unions and subsets, and is called invariant if it is invariant under the action of the group \"G\". For each element \"S\" of the ideal one can take the subgroup of \"G\" consisting of all elements fixing every element \"S\". These subgroups generate a normal filter of \"G\".\n\n"}
{"id": "20064401", "url": "https://en.wikipedia.org/wiki?curid=20064401", "title": "Pinsker's inequality", "text": "Pinsker's inequality\n\nIn information theory, Pinsker's inequality, named after its inventor Mark Semenovich Pinsker, is an inequality that bounds the total variation distance (or statistical distance) in terms of the Kullback–Leibler divergence.\nThe inequality is tight up to constant factors.\n\nPinsker's inequality states that, if formula_1 and formula_2 are two probability distributions on a measurable space formula_3, then\n\nwhere\n\nis the total variation distance (or statistical distance) between formula_1 and formula_2 and\n\nis the Kullback–Leibler divergence in nats. When the sample space formula_9 is a finite set, the Kullback–Leibler divergence is given by\n\nNote that in terms of the total variation norm formula_11 of the signed measure formula_12, Pinsker's inequality differs from the one given above by a factor of two:\n\nA proof of Pinsker's inequality uses the partition inequality for \"f\"-divergences.\n\nPinsker first proved the inequality with a worse constant. The inequality in the above form was proved independently by Kullback, Csiszár, and Kemperman.\n\nA precise inverse of the inequality cannot hold: for every formula_14, there are distributions formula_15 with formula_16 but formula_17. An easy example is given by the two-point space formula_18 with formula_19 and formula_20. \n\nHowever, an inverse inequality holds on finite spaces formula_9 with a constant depending on formula_2. More specifically, it can be shown that with the definition formula_23 we have for any measure formula_1 which is absolutely continuous to formula_2\n\nAs a consequence, if formula_2 has full support (i.e. formula_28 for all formula_29), then\n\n"}
{"id": "2121791", "url": "https://en.wikipedia.org/wiki?curid=2121791", "title": "Polish Mathematical Society", "text": "Polish Mathematical Society\n\nThe Polish Mathematical Society () began in Kraków, Poland in 1917. It was originally simply called the Mathematical Society. It was officially constituted on April 2, 1919. Hugo Steinhaus, Stefan Banach and Otto Nikodym were among the founders.\n\nEver since its foundation, the Society's main activity was to bring mathematicians together by means of organizing conferences and lectures. The second main activity is the publication of its annals \"Annales Societatis Mathematicae Polonae\", consisting of \n\nThe annals are also known under the Polish name \"Roczniki Polskiego Towarzystwa Matematycznego\" and under the English name \"Polish Mathematical Society Annals\".\n\nThe International Stefan Banach Prize (Polish: \"Międzynarodowa Nagroda im. Stefana Banacha\") is an annual award presented by the Mathematical Society to mathematicians for best doctoral dissertations in the mathematical sciences. Its aim is to \"promote and financially support the most promising young researchers\" in the field of mathematics. It was founded in 2009 and is named in honour of a renowned Polish mathematician Stefan Banach (1892-1945). The laureates of the award also receive a cash prize of PLN 20,000 (c.$5,000).\n\n\n"}
{"id": "249438", "url": "https://en.wikipedia.org/wiki?curid=249438", "title": "Principle of least action", "text": "Principle of least action\n\nThe principle of least action – or, more accurately, the principle of stationary action – is a variational principle that, when applied to the action of a mechanical system, can be used to obtain the equations of motion for that system. In relativity, a different action must be minimized or maximized. The principle can be used to derive Newtonian, Lagrangian and Hamiltonian equations of motion, and even general relativity (see Einstein–Hilbert action). The physicist Paul Dirac, and after him Julian Schwinger and Richard Feynman, demonstrated how this principle can also be used in quantum calculations.\nIt was historically called \"least\" because its solution requires finding the path that has the least value. Its classical mechanics and electromagnetic expressions are a consequence of quantum mechanics, but the stationary action method helped in the development of quantum mechanics.\n\nThe principle remains central in modern physics and mathematics, being applied in thermodynamics, fluid mechanics, the theory of relativity, quantum mechanics, particle physics, and string theory and is a focus of modern mathematical investigation in Morse theory. Maupertuis' principle and Hamilton's principle exemplify the principle of stationary action.\n\nThe action principle is preceded by earlier ideas in optics. In ancient Greece, Euclid wrote in his \"Catoptrica\" that, for the path of light reflecting from a mirror, the angle of incidence equals the angle of reflection. Hero of Alexandria later showed that this path was the shortest length and least time.\n\nScholars often credit Pierre Louis Maupertuis for formulating the principle of least action because he wrote about it in 1744 and 1746. However, Leonhard Euler discussed the principle in 1744, and evidence shows that Gottfried Leibniz preceded both by 39 years.\n\nIn 1933, Paul Dirac discerned the quantum mechanical underpinning of the principle in the quantum interference of amplitudes.\n\nThe starting point is the \"action\", denoted formula_1 (calligraphic S), of a physical system. It is defined as the integral of the Lagrangian \"L\" between two instants of time \"t\" and \"t\" - technically a functional of the \"N\" generalized coordinates q = (\"q\", \"q\", ... , \"q\") which define the configuration of the system:\n\nwhere the dot denotes the time derivative, and \"t\" is time.\n\nMathematically the principle is\n\nwhere \"δ\" (lowercase Greek delta) means a \"small\" change. In words this reads:\n\nIn applications the statement and definition of action are taken together:\n\nThe action and Lagrangian both contain the dynamics of the system for all times. The term \"path\" simply refers to a curve traced out by the system in terms of the coordinates in the configuration space, i.e. the curve q(\"t\"), parameterized by time (see also parametric equation for this concept).\n\nIn the 1600s, Pierre de Fermat postulated that \"\"light travels between two given points along the path of shortest time\",\" which is known as the principle of least time or Fermat's principle.\n\nCredit for the formulation of the principle of least action is commonly given to Pierre Louis Maupertuis, who felt that \"Nature is thrifty in all its actions\", and applied the principle broadly:\n\nThis notion of Maupertuis, although somewhat deterministic today, does capture much of the essence of mechanics.\n\nIn application to physics, Maupertuis suggested that the quantity to be minimized was the product of the duration (time) of movement within a system by the \"vis viva\",\n\nwhich is the integral of twice what we now call the kinetic energy \"T\" of the system.\n\nLeonhard Euler gave a formulation of the action principle in 1744, in very recognizable terms, in the \"Additamentum 2\" to his \"Methodus Inveniendi Lineas Curvas Maximi Minive Proprietate Gaudentes\". Beginning with the second paragraph:\n\nAs Euler states, ∫\"Mv\"d\"s\" is the integral of the momentum over distance travelled, which, in modern notation, equals the abbreviated or reduced action\n\nThus, Euler made an equivalent and (apparently) independent statement of the variational principle in the same year as Maupertuis, albeit slightly later. Curiously, Euler did not claim any priority, as the following episode shows.\n\nMaupertuis' priority was disputed in 1751 by the mathematician Samuel König, who claimed that it had been invented by Gottfried Leibniz in 1707. Although similar to many of Leibniz's arguments, the principle itself has not been documented in Leibniz's works. König himself showed a \"copy\" of a 1707 letter from Leibniz to Jacob Hermann with the principle, but the \"original\" letter has been lost. In contentious proceedings, König was accused of forgery, and even the King of Prussia entered the debate, defending Maupertuis (the head of his Academy), while Voltaire defended König.\n\nEuler, rather than claiming priority, was a staunch defender of Maupertuis, and Euler himself prosecuted König for forgery before the Berlin Academy on 13 April 1752. The claims of forgery were re-examined 150 years later, and archival work by C.I. Gerhardt in 1898 and W. Kabitz in 1913 uncovered other copies of the letter, and three others cited by König, in the Bernoulli archives.\n\nEuler continued to write on the topic; in his \"Reflexions sur quelques loix generales de la nature\" (1748), he called the quantity \"effort\". His expression corresponds to what we would now call potential energy, so that his statement of least action in statics is equivalent to the principle that a system of bodies at rest will adopt a configuration that minimizes total potential energy.\n\nMuch of the calculus of variations was stated by Joseph-Louis Lagrange in 1760 and he proceeded to apply this to problems in dynamics. In \"Méchanique Analytique\" (1788) Lagrange derived the general equations of motion of a mechanical body. William Rowan Hamilton in 1834 and 1835 applied the variational principle to the classical Lagrangian function\n\nto obtain the Euler–Lagrange equations in their present form.\n\nIn 1842, Carl Gustav Jacobi tackled the problem of whether the variational principle always found minima as opposed to other stationary points (maxima or stationary saddle points); most of his work focused on geodesics on two-dimensional surfaces. The first clear general statements were given by Marston Morse in the 1920s and 1930s, leading to what is now known as Morse theory. For example, Morse showed that the number of conjugate points in a trajectory equalled the number of negative eigenvalues in the second variation of the Lagrangian.\n\nOther extremal principles of classical mechanics have been formulated, such as Gauss's principle of least constraint and its corollary, Hertz's principle of least curvature.\n\nThe mathematical equivalence of the differential equations of motion and their integral\ncounterpart has important philosophical implications. The differential equations are statements about quantities localized to a single point in space or single moment of time. For example, Newton's second law\n\nstates that the \"instantaneous\" force F applied to a mass \"m\" produces an acceleration a at the same \"instant\". By contrast, the action principle is not localized to a point; rather, it involves integrals over an interval of time and (for fields) an extended region of space. Moreover, in the usual formulation of classical action principles, the initial and final states of the system are fixed, e.g.,\n\nIn particular, the fixing of the \"final\" state has been interpreted as giving the action principle a teleological character which has been controversial historically. However, according to W. Yourgrau and S. Mandelstam, \"the teleological approach... presupposes that the variational principles themselves have mathematical characteristics which they \"de facto\" do not possess\" In addition, some critics maintain this apparent teleology occurs because of the way in which the question was asked. By specifying some but not all aspects of both the initial and final conditions (the positions but not the velocities) we are making some inferences about the initial conditions from the final conditions, and it is this \"backward\" inference that can be seen as a teleological explanation. Teleology can also be overcome if we consider the classical description as a limiting case of the quantum formalism of path integration, in which stationary paths are obtained as a result of interference of amplitudes along all possible paths.\n\nThe short story \"Story of Your Life\" by the speculative fiction writer Ted Chiang contains visual depictions of Fermat's Principle along with a discussion of its teleological dimension. Keith Devlin's \"The Math Instinct\" contains a chapter, \"Elvis the Welsh Corgi Who Can Do Calculus\" that discusses the calculus \"embedded\" in some animals as they solve the \"least time\" problem in actual situations.\n\n"}
{"id": "12748025", "url": "https://en.wikipedia.org/wiki?curid=12748025", "title": "Quasiidentity", "text": "Quasiidentity\n\nIn universal algebra, a quasi-identity is an implication of the form \n\nwhere \"s, ..., s, s\" and \"t, ..., t,t\" are terms built up from variables using the operation symbols of the specified signature.\n\nQuasi-identities amount to conditional equations for which the conditions themselves are equations. A quasi-identity for which \"n\" = 0 is an ordinary identity or equation, whence quasi-identities are a generalization of identities. Quasi-identities are special type of Horn clauses.\n\nQuasivariety\n\n"}
{"id": "25242779", "url": "https://en.wikipedia.org/wiki?curid=25242779", "title": "Radial function", "text": "Radial function\n\nIn mathematics, a radial function is a function defined on a Euclidean space R whose value at each point depends only on the distance between that point and the origin. For example, a radial function Φ in two dimensions has the form\nwhere φ is a function of a single non-negative real variable. Radial functions are contrasted with spherical functions, and any decent function (e.g., continuous and rapidly decreasing) on Euclidean space can be decomposed into a series consisting of radial and spherical parts: the solid spherical harmonic expansion.\n\nA function is radial if and only if it is invariant under all rotations leaving the origin fixed. That is, \"ƒ\" is radial if and only if\nfor all , the special orthogonal group in \"n\" dimensions. This characterization of radial functions makes it possible also to define radial distributions. These are distributions \"S\" on R such that\nfor every test function φ and rotation ρ.\n\nGiven any (locally integrable) function \"ƒ\", its radial part is given by averaging over spheres centered at the origin. To wit,\nwhere ω is the surface area of the (\"n\"−1)-sphere \"S\", and , . It follows essentially by Fubini's theorem that a locally integrable function has a well-defined radial part at almost every \"r\".\n\nThe Fourier transform of a radial function is also radial, and so radial functions play a vital role in Fourier analysis. Furthermore, the Fourier transform of a radial function typically has stronger decay behavior at infinity than non-radial functions: for radial functions bounded in a neighborhood of the origin, the Fourier transform decays faster than \"R\". The Bessel functions are a special class of radial function that arise naturally in Fourier analysis as the radial eigenfunctions of the Laplacian; as such they appear naturally as the radial portion of the Fourier transform.\n\n"}
{"id": "41323011", "url": "https://en.wikipedia.org/wiki?curid=41323011", "title": "Regularization by spectral filtering", "text": "Regularization by spectral filtering\n\nSpectral regularization is any of a class of regularization techniques used in machine learning to control the impact of noise and prevent overfitting. Spectral regularization can be used in a broad range of applications, from deblurring images to classifying emails into a spam folder and a non-spam folder. For instance, in the email classification example, spectral regularization can be used to reduce the impact of noise and prevent overfitting when a machine learning system is being trained on a labeled set of emails to learn how to tell a spam and a non-spam email apart.\n\nSpectral regularization algorithms rely on methods that were originally defined and studied in the theory of ill-posed inverse problems (for instance, see) focusing on the inversion of a linear operator (or a matrix) that possibly has a bad condition number or an unbounded inverse. In this context, regularization amounts to substituting the original operator by a bounded operator called the \"regularization operator\" that has a condition number controlled by a regularization parameter, a classical example being Tikhonov regularization. To ensure stability, this regularization parameter is tuned based on the level of noise. The main idea behind spectral regularization is that each regularization operator can be described using spectral calculus as an appropriate filter on the eigenvalues of the operator that defines the problem, and the role of the filter is to \"suppress the oscillatory behavior corresponding to small eigenvalues\". Therefore, each algorithm in the class of spectral regularization algorithms is defined by a suitable filter function (which needs to be derived for that particular algorithm). Three of the most commonly used regularization algorithms for which spectral filtering is well-studied are Tikhonov regularization, Landweber iteration, and truncated singular value decomposition (TSVD). As for choosing the regularization parameter, examples of candidate methods to compute this parameter include the discrepancy principle, generalized cross validation, and the L-curve criterion.\n\nIt is of note that the notion of spectral filtering studied in the context of machine learning is closely connected to the literature on function approximation (in signal processing).\n\nThe training set is defined as formula_1, where formula_2 is the formula_3 input matrix and formula_4 is the output vector. Where applicable, the kernel function is denoted by formula_5, and the formula_6 kernel matrix is denoted by formula_7 which has entries formula_8 and formula_9 denotes the Reproducing Kernel Hilbert Space (RKHS) with kernel formula_5. The regularization parameter is denoted by formula_11.\n\n\"(Note: For formula_12 and formula_13, with formula_14 and formula_15 being Hilbert spaces, given a linear, continuous operator formula_16, assume that formula_17 holds. In this setting, the direct problem would be to solve for formula_18 given formula_19 and the inverse problem would be to solve for formula_19 given formula_18. If the solution exists, is unique and stable, the inverse problem (i.e. the problem of solving for formula_19) is well-posed; otherwise, it is ill-posed.) \"\n\nThe connection between the regularized least squares (RLS) estimation problem (Tikhonov regularization setting) and the theory of ill-posed inverse problems is an example of how spectral regularization algorithms are related to the theory of ill-posed inverse problems.\n\nThe RLS estimator solves\n\nand the RKHS allows for expressing this RLS estimator as formula_24 where formula_25 with formula_26. The penalization term is used for controlling smoothness and preventing overfitting. Since the solution of empirical risk minimization formula_27 can be written as formula_28 such that formula_29, adding the penalty function amounts to the following change in the system that needs to be solved:\n\nIn this learning setting, the kernel matrix can be decomposed as formula_31, with\n\nand formula_33 are the corresponding eigenvectors. Therefore, in the initial learning setting, the following holds:\n\nThus, for small eigenvalues, even small perturbations in the data can lead to considerable changes in the solution. Hence, the problem is ill-conditioned, and solving this RLS problem amounts to stabilizing a possibly ill-conditioned matrix inversion problem, which is studied in the theory of ill-posed inverse problems; in both problems, a main concern is to deal with the issue of numerical stability.\n\nEach algorithm in the class of spectral regularization algorithms is defined by a suitable filter function, denoted here by formula_35. If the Kernel matrix is denoted by formula_7, then formula_11 should control the magnitude of the smaller eigenvalues of formula_38. In a filtering setup, the goal is to find estimators formula_39 where formula_40. To do so, a scalar filter function formula_41 is defined using the eigen-decomposition of the kernel matrix:\n\nwhich yields\n\nTypically, an appropriate filter function should have the following properties:\n\n1. As formula_11 goes to zero, formula_45.\n\n2. The magnitude of the (smaller) eigenvalues of formula_46 is controlled by formula_11.\n\nWhile the above items give a rough characterization of the general properties of filter functions for all spectral regularization algorithms, the derivation of the filter function (and hence its exact form) varies depending on the specific regularization method that spectral filtering is applied to.\n\nIn the Tikhonov regularization setting, the filter function for RLS is described below. As shown in, in this setting, formula_48. Thus,\n\nThe undesired components are filtered out using regularization:\nThe filter function for Tikhonov regularization is therefore defined as:\n\nformula_54\n\nThe idea behind the Landweber iteration is gradient descent:\n\nIn this setting, if formula_59 is larger than formula_7's largest eigenvalue, the above iteration converges by choosing formula_61 as the step-size:. The above iteration is equivalent to minimizing formula_62 (i.e. the empirical risk) via gradient descent; using induction, it can be proved that at the formula_63-th iteration, the solution is given by \n\nThus, the appropriate filter function is defined by:\n\nformula_65\n\nIt can be shown that this filter function corresponds to a truncated power expansion of formula_66; to see this, note that the relation formula_67, would still hold if formula_68 is replaced by a matrix; thus, if formula_7 (the kernel matrix), or rather formula_70, is considered, the following holds:\n\nIn this setting, the number of iterations gives the regularization parameter; roughly speaking, formula_72. If formula_63 is large, overfitting may be a concern. If formula_63 is small, oversmoothing may be a concern. Thus, choosing an appropriate time for early stopping of the iterations provides a regularization effect.\n\nIn the TSVD setting, given the eigen-decomposition formula_31 and using a prescribed threshold formula_76, a regularized inverse can be formed for the kernel matrix by discarding all the eigenvalues that are smaller than this threshold.\nThus, the filter function for TSVD can be defined as\n\nIt can be shown that TSVD is equivalent to the (unsupervised) projection of the data using (kernel) Principal Component Analysis (PCA), and that it is also equivalent to minimizing the empirical risk on the projected data (without regularization). Note that the number of components kept for the projection is the only free parameter here.\n"}
{"id": "200305", "url": "https://en.wikipedia.org/wiki?curid=200305", "title": "Rolle's theorem", "text": "Rolle's theorem\n\nIn calculus, Rolle's theorem or Rolle's lemma essentially states that any real-valued differentiable function that attains equal values at two distinct points must have at least one stationary point somewhere between them—that is, a point where the first derivative (the slope of the tangent line to the graph of the function) is zero.\n\nIf a real-valued function is continuous on a proper closed interval , differentiable on the open interval , and , then there exists at least one in the open interval such that\n\nThis version of Rolle's theorem is used to prove the mean value theorem, of which Rolle's theorem is indeed a special case. It is also the basis for the proof of Taylor's theorem.\n\nIndian mathematician Bhāskara II (1114–1185) is credited with knowledge of Rolle's theorem. Although the theorem is named after Michel Rolle, Rolle's 1691 proof covered only the case of polynomial functions. His proof did not use the methods of differential calculus, which at that point in his life he considered to be fallacious. The theorem was first proved by Cauchy in 1823 as a corollary of a proof of the mean value theorem. The name \"Rolle's theorem\" was first used by Moritz Wilhelm Drobisch of Germany in 1834 and by Giusto Bellavitis of Italy in 1846.\n\nFor a radius , consider the function\n\nIts graph is the upper semicircle centered at the origin. This function is continuous on the closed interval and differentiable in the open interval , but not differentiable at the endpoints and . Since , Rolle's theorem applies, and indeed, there is a point where the derivative of is zero. Note that the theorem applies even when the function cannot be differentiated at the endpoints because it only requires the function to be differentiable in the open interval.\n\nIf differentiability fails at an interior point of the interval, the conclusion of Rolle's theorem may not hold. Consider the absolute value function\n\nThen , but there is no between −1 and 1 for which the is zero. This is because that function, although continuous, is not differentiable at . Note that the derivative of changes its sign at , but without attaining the value 0. The theorem cannot be applied to this function because it does not satisfy the condition that the function must be differentiable for every in the open interval. However, when the differentiability requirement is dropped from Rolle's theorem, will still have a critical number in the open interval , but it may not yield a horizontal tangent (as in the case of the absolute value represented in the graph).\n\nThe second example illustrates the following generalization of Rolle's theorem:\n\nConsider a real-valued, continuous function on a closed interval with . If for every in the open interval the right-hand limit\n\nand the left-hand limit\n\nexist in the extended real line , then there is some number in the open interval such that one of the two limits\n\nis ≥ 0 and the other one is ≤ 0 (in the extended real line). If the right- and left-hand limits agree for every , then they agree in particular for , hence the derivative of exists at and is equal to zero.\n\n\nSince the proof for the standard version of Rolle's theorem and the generalization are very similar, we prove the generalization.\n\nThe idea of the proof is to argue that if , then must attain either a maximum or a minimum somewhere between and , say at , and the function must change from increasing to decreasing (or the other way around) at . In particular, if the derivative exists, it must be zero at .\n\nBy assumption, is continuous on , and by the extreme value theorem attains both its maximum and its minimum in . If these are both attained at the endpoints of , then is constant on and so the derivative of is zero at every point in .\n\nSuppose then that the maximum is obtained at an interior point of (the argument for the minimum is very similar, just consider ). We shall examine the above right- and left-hand limits separately.\n\nFor a real such that is in , the value is smaller or equal to because attains its maximum at . Therefore, for every ,\n\nhence\n\nwhere the limit exists by assumption, it may be minus infinity.\nSimilarly, for every , the inequality turns around because the denominator is now negative and we get\n\nhence\n\nwhere the limit might be plus infinity.\n\nFinally, when the above right- and left-hand limits agree (in particular when is differentiable), then the derivative of at must be zero.\n\nWe can also generalize Rolle's theorem by requiring that has more points with equal values and greater regularity. Specifically, suppose that\nThe requirements concerning the th derivative of can be weakened as in the generalization above, giving the corresponding (possibly weaker) assertions for the right- and left-hand limits defined above with in place of .\n\nParticularly, this version of the theorem asserts that if a function differentiable enough times has roots (so they have the same value, that is 0), then there is an internal point where vanishes.\n\nThe proof uses mathematical induction. The case is simply the standard version of Rolle's theorem. As the induction hypothesis, assume the generalization is true for . We want to prove it for . By the standard version of Rolle's theorem, for every integer from 1 to , there exists a in the open interval such that . Hence, the first derivative satisfies the assumptions on the closed intervals . By the induction hypothesis, there is a such that the st derivative of at is zero.\n\nRolle's theorem is a property of differentiable functions over the real numbers, which are an ordered field. As such, it does not generalize to other fields, but the following corollary does: if a real polynomial factors (has all of its roots) over the real numbers, then its derivative does as well. One may call this property of a field Rolle's property. More general fields do not always have differentiable functions, but they do always have polynomials, which can be symbolically differentiated. Similarly, more general fields may not have an order, but one has a notion of a root of a polynomial lying in a field.\n\nThus Rolle's theorem shows that the real numbers have Rolle's property. Any algebraically closed field such as the complex numbers has Rolle's property. However, the rational numbers do not – for example, factors over the rationals, but its derivative,\ndoes not. The question of which fields satisfy Rolle's property was raised in . For finite fields, the answer is that only and have Rolle's property; this was first proven via technical means in , and a simple proof is given in .\n\nFor a complex version, see Voorhoeve index.\n\n\n\n"}
{"id": "463721", "url": "https://en.wikipedia.org/wiki?curid=463721", "title": "Space group", "text": "Space group\n\nIn mathematics, physics and chemistry, a space group is the symmetry group of a configuration in space, usually in three dimensions. In three dimensions, there are 219 distinct types, or 230 if chiral copies are considered distinct. Space groups are also studied in dimensions other than 3 where they are sometimes called Bieberbach groups, and are discrete cocompact groups of isometries of an oriented Euclidean space.\n\nIn crystallography, space groups are also called the crystallographic or Fedorov groups, and represent a description of the symmetry of the crystal. A definitive source regarding 3-dimensional space groups is the \"International Tables for Crystallography\" ().\n\nSpace groups in 2 dimensions are the 17 wallpaper groups which have been known for several centuries, though the proof that the list was complete was only given in 1891, after the much harder case of space groups had been done.\n\nIn 1879 Leonhard Sohncke listed the 65 space groups (called Sohncke groups) whose elements preserve the orientation. More accurately, he listed 66 groups, but Fedorov and Schönflies both noticed that two of them were really the same. The space groups in 3 dimensions were first enumerated by (whose list had 2 omissions (I3d and Fdd2) and one duplication (Fmm2)), and shortly afterwards were independently enumerated by (whose list had 4 omissions (I3d, Pc, Cc, ?) and one duplication (P2m)). The correct list of 230 space groups was found by 1892 during correspondence between Fedorov and Schönflies. later enumerated the groups with a different method, but omitted four groups (Fdd2, I2d, P2d, and P2c) even though he already had the correct list of 230 groups from Fedorov and Schönflies; the common claim that Barlow was unaware of their work is a myth.\n\nThe space groups in three dimensions are made from combinations of the 32 crystallographic point groups with the 14 Bravais lattices, each of the latter belonging to one of 7 lattice systems. This results in a space group being some combination of the translational symmetry of a unit cell including lattice centering, the point group symmetry operations of reflection, rotation and improper rotation (also called rotoinversion), and the screw axis and glide plane symmetry operations. The combination of all these symmetry operations results in a total of 230 different space groups describing all possible crystal symmetries.\n\nThe elements of the space group fixing a point of space are the identity element, reflections, rotations and improper rotations.\n\nThe translations form a normal abelian subgroup of rank 3, called the Bravais lattice. There are 14 possible types of Bravais lattice. The quotient of the space group by the Bravais lattice is a finite group which is one of the 32 possible point groups. Translation is defined as the face moves from one point to another point.\n\nA glide plane is a reflection in a plane, followed by a translation parallel with that plane. This is noted by \"a\", \"b\" or \"c\", depending on which axis the glide is along. There is also the \"n\" glide, which is a glide along the half of a diagonal of a face, and the \"d\" glide, which is a fourth of the way along either a face or space diagonal of the unit cell. The latter is called the diamond glide plane as it features in the diamond structure. In 17 space groups, due to the centering of the cell, the glides occur in two perpendicular directions simultaneously, \"i.e.\" the same glide plane can be called \"b\" or \"c\", \"a\" or \"b\", \"a\" or \"c\". For example, group Abm2 could be also called Acm2, group Ccca could be called Cccb. In 1992, it was suggested to use symbol \"e\" for such planes. The symbols for five space groups have been modified:\nA screw axis is a rotation about an axis, followed by a translation along the direction of the axis. These are noted by a number, \"n\", to describe the degree of rotation, where the number is how many operations must be applied to complete a full rotation (e.g., 3 would mean a rotation one third of the way around the axis each time). The degree of translation is then added as a subscript showing how far along the axis the translation is, as a portion of the parallel lattice vector. So, 2 is a twofold rotation followed by a translation of 1/2 of the lattice vector.\n\nThe general formula for the action of an element of a space group is\n\n\"y\" = \"M\".\"x\" + \"D\"\n\nwhere \"M\" is its matrix, \"D\" is its vector, and where the element transforms point \"x\" into point \"y\". In general, \"D\" = \"D\"(lattice) + \"D\"(\"M\"), where \"D\"(\"M\") is a unique function of \"M\" that is zero for \"M\" being the identity. The matrices \"M\" form a point group that is a basis of the space group; the lattice must be symmetric under that point group.\n\nThe lattice dimension can be less than the overall dimension, resulting in a \"subperiodic\" space group. For (overall dimension, lattice dimension):\n\nThere are at least ten methods of naming space groups. Some of these methods can assign several different names to the same space group, so altogether there are many thousands of different names.\n\n\n\nThere are (at least) 10 different ways to classify space groups into classes. The relations between some of these are described in the following table. Each classification system is a refinement of the ones below it.\n\nIn \"n\" dimensions, an affine space group, or Bieberbach group, is a discrete subgroup of isometries of \"n\"-dimensional Euclidean space with a compact fundamental domain. proved that the subgroup of translations of any such group contains \"n\" linearly independent translations, and is a free abelian subgroup of finite index, and is also the unique maximal normal abelian subgroup. He also showed that in any dimension \"n\" there are only a finite number of possibilities for the isomorphism class of the underlying group of a space group, and moreover the action of the group on Euclidean space is unique up to conjugation by affine transformations. This answers part of Hilbert's eighteenth problem. showed that conversely any group that is the extension of Z by a finite group acting faithfully is an affine space group. Combining these results shows that classifying space groups in \"n\" dimensions up to conjugation by affine transformations is essentially the same as classifying isomorphism classes for groups that are extensions of Z by a finite group acting faithfully.\n\nIt is essential in Bieberbach's theorems to assume that the group acts as isometries; the theorems do not generalize to discrete cocompact groups of affine transformations of Euclidean space. A counter-example is given by the 3-dimensional Heisenberg group of the integers acting by translations on the Heisenberg group of the reals, identified with 3-dimensional Euclidean space. This is a discrete cocompact group of affine transformations of space, but does not contain a subgroup Z.\n\nThis table gives the number of space group types in small dimensions, including the numbers of various classes of space group. The numbers of enantiomorphic pairs are given in parentheses.\na - Trivial group <BR>\nb - One is the group of integers and the other is the infinite dihedral group; see symmetry groups in one dimension <BR>\nc - these 2D space groups are also called wallpaper groups or plane groups. <BR>\nd - In 3D there are 230 crystallographic space group types, which reduces to 219 affine space group types because of some types being different from their mirror image; these are said to differ by \"enantiomorphous character\" (e.g. P312 and P312). Usually \"space group\" refers to 3D. They were enumerated independently by , and . <BR>\ne - The 4895 4-dimensional groups were enumerated by . corrected the number of enantiomorphic groups from 112 to 111, so total number of groups is 4783+111=4894. There are 44 enantiomorphic point groups in 4-dimensional space. If we consider enantiomorphic groups as different, the total number of point groups is 227+44=271. <BR>\nf - enumerated the ones of dimension 5. counted the enantiomorphs. <BR>\ng - enumerated the ones of dimension 6, later the corrected figures were found. Initially published number of 826 Lattice types in was corrected to 841 in . See also . counted the enantiomorphs, but that paper relied on old erroneous CARAT data for dimension 6.\n\nIn addition to crystallographic space groups there are also magnetic space groups (also called two-color (black and white) crystallographic groups or Shubnikov groups). These symmetries contain an element known as time reversal. They treat time as an additional dimension, and the group elements can include time reversal as reflection in it. They are of importance in magnetic structures that contain ordered unpaired spins, i.e. ferro-, ferri- or antiferromagnetic structures as studied by neutron diffraction. The time reversal element flips a magnetic spin while leaving all other structure the same and it can be combined with a number of other symmetry elements. Including time reversal there are 1651 magnetic space groups in 3D . It has also been possible to construct magnetic versions for other overall and lattice dimensions (Daniel Litvin's papers, , ). Frieze groups are magnetic 1D line groups and layer groups are magnetic wallpaper groups, and the axial 3D point groups are magnetic 2D point groups. Number of original and magnetic groups by (overall, lattice) dimension:\n\nTable of the wallpaper groups using the classification of the 3-dimensional space groups:\n\nFor each geometric class, the possible arithmetic classes are\n\nNote. An \"e\" plane is a double glide plane, one having glides in two different directions. They are found in seven orthorhombic, five tetragonal and five cubic space groups, all with centered lattice. The use of the symbol \"e\" became official with .\n\nThe lattice system can be found as follows. If the crystal system is not trigonal then the lattice system is of the same type. If the crystal system is trigonal, then the lattice system is hexagonal unless the space group is one of the seven in the rhombohedral lattice system consisting of the 7 trigonal space groups in the table above whose name begins with R. (The term rhombohedral system is also sometimes used as an alternative name for the whole trigonal system.) The hexagonal lattice system is larger than the hexagonal crystal system, and consists of the hexagonal crystal system together with the 18 groups of the trigonal crystal system other than the seven whose names begin with R.\n\nThe Bravais lattice of the space group is determined by the lattice system together with the initial letter of its name, which for the non-rhombohedral groups is P, I, F, or C, standing for the principal, body centered, face centered, or C-face centered lattices.\n\n\n"}
{"id": "46864821", "url": "https://en.wikipedia.org/wiki?curid=46864821", "title": "Sparse network", "text": "Sparse network\n\nIn the context of network science, a sparse network is a network with fewer links than the maximum possible number of links within the same network. The opposite of the sparse network is dense or complete network. The study of sparse networks is a relatively new area primarily stimulated by the study of real networks, such as social and computer networks.\n\nThe number of links vary from network to network. The number of links in the network can be higher than the number of nodes in network, if each node is linked to every node in the network, such networks are called dense:\nL = links; N = nodes\n\nformula_1\n\nIf each node is linked to all other nodes, except itself, which means that network does not contain loops, than this type of network is referred as complete. If the number of links is smaller than the maximum number of links, then it is a sparse network. Sparse connectivity can be identified in networks in which nodes are difficult to be linked:\n\nformula_2 for formula_3\n\nMost of the real networks are sparse, however they can still be efficiently analyzed. Typically, sparse networks have a scale-free (power-law) node-degree distribution, meaning that there are few extremely linked nodes and many sparsely linked nodes within the same network.\n\nThe node degree distribution changes with the increasing connectivity. Different link densities in the complex networks have different node-degree distribution, as Flickr Network Analysis suggests. The sparsely connected networks have a scale free, power law distribution. With increasing connectivity, the networks show increasing divergence from power law. One of the main factors, influencing on the network connectivity is the node similarity. For instance, in social networks, people are likely to be linked to each other if they share common social background, interests, tastes, beliefs, etc. In context of biological networks, proteins or other molecules are linked if they have exact or complementary fit of their complex surfaces.\n\nIf the nodes in the networks are not weighted, the structural components of the network can be shown through adjacency matrix. If the most elements in the matrix are zero, such matrix is referred as sparse matrix. In contrast, if most of the elements are nonzero, then the matrix is dense. The sparsity or density of the matrix is identified by the fraction of the zero element to the total number of the elements in the matrix. Similarly, in the context of graph theory, if the number of links is close to its maximum, then the graph would be known as dense graph. If the number of links is lower than the maximum number of links, this type of graphs are referred as sparse graph.\n\nSparse Network can be found in social, computer and biological networks, as well as, its applications can be found in transportation, power-line, citation networks, etc. Since most real networks are large and sparse, there were several models developed to understand and analyze them. These networks have inspired sparse network-on-chip design in multiprocessor embedded computer engineering.\n"}
{"id": "5045759", "url": "https://en.wikipedia.org/wiki?curid=5045759", "title": "Stabilizer code", "text": "Stabilizer code\n\nThe theory of quantum error correction plays a prominent role in the practical realization and engineering of\nquantum computing and quantum communication devices. The first quantum\nerror-correcting codes are strikingly similar to classical block codes in their\noperation and performance. Quantum error-correcting codes restore a noisy,\ndecohered quantum state to a pure quantum state. A\nstabilizer quantum error-correcting code appends ancilla qubits\nto qubits that we want to protect. A unitary encoding circuit rotates the\nglobal state into a subspace of a larger Hilbert space. This highly entangled,\nencoded state corrects for local noisy errors. A quantum error-correcting code makes quantum computation\nand quantum communication practical by providing a way for a sender and\nreceiver to simulate a noiseless qubit channel given a noisy qubit channel\nwhose noise conforms to a particular error model.\n\nThe stabilizer theory of quantum error correction allows one to import some\nclassical binary or quaternary codes for use as a quantum code. However, when importing the\nclassical code, it must satisfy the dual-containing (or self-orthogonality)\nconstraint. Researchers have found many examples of classical codes satisfying\nthis constraint, but most classical codes do not. Nevertheless, it is still useful to import classical codes in this way (though, see how the entanglement-assisted stabilizer formalism overcomes this difficulty).\n\nThe Stabilizer formalism exploits elements of\nthe Pauli group formula_1 in formulating quantum error-correcting codes. The set\nformula_2 consists of the Pauli operators:\nThe above operators act on a single qubit---a state represented by a vector in a two-dimensional\nHilbert space. Operators in formula_1 have eigenvalues formula_5 and either commute\nor anti-commute. The set formula_6 consists of formula_7-fold tensor products of\nPauli operators:\nElements of formula_6 act on a quantum register of formula_7 qubits. We\noccasionally omit tensor product symbols in what follows so that\nThe formula_7-fold Pauli group\nformula_6 plays an important role for both the encoding circuit and the\nerror-correction procedure of a quantum stabilizer code over formula_7 qubits.\n\nLet us define an formula_15 stabilizer quantum error-correcting\ncode to encode formula_16 logical qubits into formula_7 physical qubits. The rate of such a\ncode is formula_18. Its stabilizer formula_19 is an abelian subgroup of the\nformula_7-fold Pauli group formula_6: formula_22. formula_19\ndoes not contain the operator formula_24. The simultaneous\nformula_25-eigenspace of the operators constitutes the \"codespace\". The\ncodespace has dimension formula_26 so that we can encode formula_16 qubits into it. The\nstabilizer formula_19 has a minimal representation in terms of formula_29\nindependent generators\n\nThe generators are\nindependent in the sense that none of them is a product of any other two (up\nto a global phase). The operators formula_31 function in the same\nway as a parity check matrix does for a classical linear block code.\n\nOne of the fundamental notions in quantum error correction theory is that it\nsuffices to correct a discrete error set with support in the Pauli group\nformula_6. Suppose that the errors affecting an\nencoded quantum state are a subset formula_33 of the Pauli group formula_6:\n\nBecause formula_33 and formula_19 are both subsets of formula_6, an error formula_39 that affects an\nencoded quantum state either commutes or anticommutes with any particular\nelement formula_40 in formula_19. The error formula_42 is correctable if it\nanticommutes with an element formula_40 in formula_19. An anticommuting error\nformula_42 is detectable by measuring each element formula_40 in formula_19 and\ncomputing a syndrome formula_48 identifying formula_42. The syndrome is a binary\nvector formula_48 with length formula_29 whose elements identify whether the\nerror formula_42 commutes or anticommutes with each formula_53. An error\nformula_42 that commutes with every element formula_40 in formula_19 is correctable if\nand only if it is in formula_19. It corrupts the encoded state if it\ncommutes with every element of formula_19 but does not lie in formula_59. So we compactly summarize the stabilizer error-correcting conditions: a\nstabilizer code can correct any errors formula_60 in formula_33 if\n\nor\n\nwhere formula_64 is the centralizer of formula_19 (i.e., the subgroup of elements that commute with all members of formula_19, also known as the commutant).\n\nA simple but useful mapping exists between elements of formula_1 and the binary\nvector space formula_68. This mapping gives a\nsimplification of quantum error correction theory. It represents quantum codes\nwith binary vectors and binary operations rather than with Pauli operators and\nmatrix operations respectively.\n\nWe first give the mapping for the one-qubit case. Suppose formula_69\nis a set of equivalence classes of an operator formula_70 that have the same phase:\n\nLet formula_72 be the set of phase-free Pauli operators where\nformula_73.\nDefine the map formula_74 as\n\nSuppose formula_76. Let us employ the\nshorthand formula_77 and formula_78 where formula_79, formula_80, formula_81, formula_82. For\nexample, suppose formula_83. Then formula_84. The\nmap formula_85 induces an isomorphism formula_86 because addition of vectors\nin formula_68 is equivalent to multiplication of\nPauli operators up to a global phase:\n\nLet formula_89 denote the symplectic product between two elements formula_90:\nThe symplectic product formula_89 gives the commutation relations of elements of\nformula_1:\n\nThe symplectic product and the mapping formula_85 thus give a useful way to phrase\nPauli relations in terms of binary algebra.\nThe extension of the above definitions and mapping formula_85 to multiple qubits is\nstraightforward. Let formula_97 denote an\narbitrary element of formula_6. We can similarly define the phase-free\nformula_7-qubit Pauli group formula_100 where\n\nThe group operation formula_102 for the above equivalence class is as follows:\nThe equivalence class formula_104 forms a commutative group\nunder operation formula_102. Consider the formula_106-dimensional vector space\nIt forms the commutative group formula_108 with\noperation formula_109 defined as binary vector addition. We employ the notation\nformula_110 to represent any vectors\nformula_111 respectively. Each\nvector formula_112 and formula_113 has elements formula_114 and formula_115 respectively with\nsimilar representations for formula_116 and formula_117.\nThe \"symplectic product\" formula_89 of formula_119 and formula_120 is\nor\nwhere formula_123 and formula_124. Let us define a map formula_125 as follows:\nLet\nso that formula_128 and formula_129 belong to the same\nequivalence class:\nThe map formula_131 is an isomorphism for the same\nreason given as in the previous case:\nwhere formula_111. The symplectic product\ncaptures the commutation relations of any operators formula_134 and formula_135:\nThe above binary representation and symplectic algebra are useful in making\nthe relation between classical linear error correction and quantum error correction more explicit.\n\nBy comparing quantum error correcting codes in this language to symplectic vector spaces, we can see the following. A symplectic subspace corresponds to a direct sum of Pauli algebras (i.e., encoded qubits), while an isotropic subspace corresponds to a set of stabilizers.\n\nAn example of a stabilizer code is the five qubit\nformula_137 stabilizer code. It encodes formula_138 logical qubit\ninto formula_139 physical qubits and protects against an arbitrary single-qubit\nerror. It has code distance formula_140. Its stabilizer consists of formula_141 Pauli operators:\nThe above operators commute. Therefore, the codespace is the simultaneous\n+1-eigenspace of the above operators. Suppose a single-qubit error occurs on\nthe encoded quantum register. A single-qubit error is in the set formula_143 where formula_144 denotes a Pauli error on qubit formula_145.\nIt is straightforward to verify that any arbitrary single-qubit error has a\nunique syndrome. The receiver corrects any single-qubit error by identifying\nthe syndrome and applying a corrective operation.\n\n"}
{"id": "1459010", "url": "https://en.wikipedia.org/wiki?curid=1459010", "title": "Stationary phase approximation", "text": "Stationary phase approximation\n\nIn mathematics, the stationary phase approximation is a basic principle of asymptotic analysis, applying to oscillatory integrals\n\ntaken over \"n\"-dimensional space ℝ where \"i\" is the imaginary unit. Here \"f\" and \"g\" are real-valued smooth functions. The role of \"g\" is to ensure convergence; that is, \"g\" is a test function. The large real parameter \"k\" is considered in the limit as formula_2.\n\nThis method originates from the 19th century, and is due to George Gabriel Stokes and Lord Kelvin.\n\nThe main idea of stationary phase methods relies on the cancellation of sinusoids with rapidly varying phase. If many sinusoids have the same phase and they are added together, they will add constructively. If, however, these same sinusoids have phases which change rapidly as the frequency changes, they will add incoherently, varying between constructive and destructive addition at different times.\n\nLetting formula_3 denote the set of critical points of the function formula_4 (i.e. points where formula_5), under the assumption that formula_6 is either compactly supported or has exponential decay, and that all critical points are nondegenerate (i.e. formula_7 for formula_8) we have the following asymptotic formula, as formula_9:\n\nformula_10\n\nHere formula_11 denotes the Hessian of formula_4.\n\nFor formula_13, this reduces to:\n\nformula_14\n\nIn this case the assumptions on formula_4 reduce to all the critical points being non-degenerate.\n\nThis is just the Wick rotated version of the formula for the method of steepest descent.\n\nConsider a function\n\nThe phase term in this function, , is stationary when\n\nor equivalently,\n\nSolutions to this equation yield dominant frequencies \"ω\" for some \"x\" and \"t\". If we expand \"ϕ\" as a Taylor series about \"ω\" and neglect terms of order higher than ,\n\nwhere \"k\"″ denotes the second derivative of \"k\". When \"x\" is relatively large, even a small difference will generate rapid oscillations within the integral, leading to cancellation. Therefore we can extend the limits of integration beyond the limit for a Taylor expansion. If we double the real contribution from the positive frequencies of the transform to account for the negative frequencies,\n\nThis integrates to\n\nThe first major general statement of the principle involved is that the asymptotic behaviour of \"I\"(\"k\") depends only on the critical points of \"f\". If by choice of \"g\" the integral is localised to a region of space where \"f\" has no critical point, the resulting integral tends to 0 as the frequency of oscillations is taken to infinity. See for example Riemann-Lebesgue lemma.\n\nThe second statement is that when \"f\" is a Morse function, so that the singular points of \"f\" are non-degenerate and isolated, then the question can be reduced to the case \"n\" = 1. In fact, then, a choice of \"g\" can be made to split the integral into cases with just one critical point \"P\" in each. At that point, because the Hessian determinant at \"P\" is by assumption not 0, the Morse lemma applies. By a change of co-ordinates \"f\" may be replaced by\n\nThe value of \"j\" is given by the signature of the Hessian matrix of \"f\" at \"P\". As for \"g\", the essential case is that \"g\" is a product of bump functions of \"x\". Assuming now without loss of generality that \"P\" is the origin, take a smooth bump function \"h\" with value 1 on the interval and quickly tending to 0 outside it. Take\n\nthen Fubini's theorem reduces \"I\"(\"k\") to a product of integrals over the real line like\n\nwith \"f\"(\"x\") = ±\"x\". The case with the minus sign is the complex conjugate of the case with the plus sign, so there is essentially one required asymptotic estimate.\n\nIn this way asymptotics can be found for oscillatory integrals for Morse functions. The degenerate case requires further techniques (see for example Airy function).\n\nThe essential statement is this one:\n\nIn fact by contour integration it can be shown that the main term on the right hand side of the equation is the value of the integral on the left hand side, extended over the range (for a proof see Fresnel integral). Therefore it is the question of estimating away the integral over, say, .\n\nThis is the model for all one-dimensional integrals \"I\"(\"k\") with \"f\" having a single non-degenerate critical point at which \"f\" has second derivative > 0. In fact the model case has second derivative 2 at 0. In order to scale using \"k\", observe that replacing \"k\" by \nwhere \"c\" is constant is the same as scaling \"x\" by √\"c\". It follows that for general values of , the factor becomes\n\nFor one uses the complex conjugate formula, as mentioned before.\n\nAs can be seen from the formula, stationary phase provides the first-order approximation of the asymptotic behavior of the integral. The lower order terms can be understood as a sum of over Feynman diagrams with various weighting factors, for well behaved formula_4.\n\n\n"}
{"id": "52318693", "url": "https://en.wikipedia.org/wiki?curid=52318693", "title": "Tarka-Sangraha", "text": "Tarka-Sangraha\n\nTarka-Sangraha is a treatise in Sanskrit giving a foundational exposition of the ancient Indian system of logic and reasoning. The work is authored by Annambhatta and the author himself has given a detailed commentary, called Tarka-Sangraha Deepika, for the text. Annambhatta composed the text as well as the commentary in the second half of 17th century CE. The text of Tarka-sangraha is a small book with about 15 pages only and it was composed to help boys and girls learn easily the basic principles of Nyaya. Of all the works of Annambhatta, only Tarka-Sangraha and its commentary attained wide acceptance. They have been used as basic text for beginners for several generations. \n\nIn Indian philosophical writings, the traditional structure of presenting a system consisted of three things: \"uddesa\" (listing of items to be discussed), \"laksana\" (defining each item in the list) and \"pariksa\" (critically examining whether the definitions apply properly to the items defined). The Tarka-Sangraha follows this model except for the third item of \"pariksa\". The text presents the ontology, logic and epistemology of the Nyaya-Vaiseshika system. \n\nPractically only very little is known about Annambhatta the author of Tarka-Sangraha. From the scanty references to other works and writers contained in his works, it has been estimated that Annambhatta must be a comparatively modern author and he must have flourished during the seventeenth century CE. His father's name was Advaitavidyacarya Tirumala. He was Tailanga Brahmin of North Arcot District of erstwhile state of Andhra Pradesh who had settled down in Benares. Tirumala was a Rigvedi Smarta Brahmana well versed in Vedanta philosophy. Annambhatta was a learned man in several areas of traditional scholarship, namely, Nyaya, Vyakarana, Vedanta and Purva-Mimamsa. Though not as well-known as Tarka-Sangraha, many of Annambhatta's works on other disciplines have survived. Besides, Tarka-Sangraha and its Commentary Dipika, the following works have been attributed to Annambhatta:\n\nBecause of its wide popularity, several scholars have written commentaries on Tarks-Sangraha. Annambhatta, the author of the treatise, himself has written a commentary named Tarka-Samgraha-Dipika. Researchers have located as many as 90 different commentaries on Tarka-Sangraha including the one by Annambhatta.\n\n\n"}
{"id": "37262638", "url": "https://en.wikipedia.org/wiki?curid=37262638", "title": "Theory of Lie groups", "text": "Theory of Lie groups\n\nIn mathematics, Theory of Lie groups is a series of books on Lie groups by . The first in the series was one of the earliest books on Lie groups to treat them from the global point of view, and for many years was the standard text on Lie groups. The second and third volumes, on algebraic groups and Lie algebras, were written in French, and later reprinted bound together as one volume. Apparently further volumes were planned but not published, though his lectures on the classification of semisimple algebraic groups could be considered as a continuation of the series.\n\n"}
{"id": "29749902", "url": "https://en.wikipedia.org/wiki?curid=29749902", "title": "Uniform field theory", "text": "Uniform field theory\n\nUniform field theory is a formula for determining the effective electrical resistance of a parallel wire system. By calculating the mean square field acting throughout a section of coil, formulae are obtained for the effective resistances of single- and multi-layer solenoidal coils of either solid or stranded wire.\n\n"}
{"id": "3036126", "url": "https://en.wikipedia.org/wiki?curid=3036126", "title": "Vafa–Witten theorem", "text": "Vafa–Witten theorem\n\nIn theoretical physics, the Vafa–Witten theorem, named after Cumrun Vafa and Edward Witten, is a theorem that shows that vector-like global symmetries (those that transform as expected under reflections) such as isospin and baryon number in vector-like gauge theories like QCD cannot be spontanteously broken as long as the theta angle is zero. This theorem can be proved by showing the exponential fall off of the propagator of fermions.\n\n\n"}
{"id": "27479533", "url": "https://en.wikipedia.org/wiki?curid=27479533", "title": "Valentin Afraimovich", "text": "Valentin Afraimovich\n\nValentin Afraimovich (, 2 April 1945, Kirov, Kirov Oblast, USSR – 21 February 2018, Nizhny Novgorod, Russia) was a Soviet, Russian and Mexican mathematician. He made contributions to dynamical systems theory, qualitative theory of ordinary differential equations, bifurcation theory, concept of attractor, strange attractors, space-time chaos, mathematical models of nonequilibrium media and biological systems, traveling waves in lattices, complexity of orbits and dimension-like characteristics in dynamical systems.\n\nHe got his Ph.D. (Kandidat) degree in 1974 at the Nizhny Novgorod State University under the advice of L. P. Shil’nikov. Also in 1990 he received his Doctor of Science degree in Mathematics and Physics, at Saratov State University in Russia.\nAfter then, he held several academic positions, including:\nAfraimovich's students include Mark Shereshevsky, Nizhny Novgorod 1990; Todd Ray Young, Atlanta, Georgia, 1995; Antonio Morante, San Luis Potosí (SLP) México, 2002; Salomé Murgia, SLP México, 2003; Alberto Cordonet, SLP Mexico, 2002; Francisco Ordaz, SLP Mexico, 2004; Leticia Ramirez, SLP Mexico, 2005; Irma Tristan-Lopez, SLP Mexico, 2010; Rosendo Vazquez-Bañuelos, 2013.\n\n\n\n\n"}
