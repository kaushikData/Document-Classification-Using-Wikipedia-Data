{"id": "46762", "url": "https://en.wikipedia.org/wiki?curid=46762", "title": "Alfred Marshall", "text": "Alfred Marshall\n\nAlfred Marshall, FBA (26 July 1842 – 13 July 1924) was one of the most influential economists of his time. His book, \"Principles of Economics\" (1890), was the dominant economic textbook in England for many years. It brings the ideas of supply and demand, marginal utility, and costs of production into a coherent whole. He is known as one of the founders of neoclassical economics. Although Marshall took economics to a more mathematically rigorous level, he did not want mathematics to overshadow economics and thus make economics irrelevant to the layman.\n\nMarshall was born in London. His father was a bank cashier and devout Evangelical. Marshall grew up in Clapham and was educated at the Merchant Taylors' School and St John's College, Cambridge, where he demonstrated an aptitude in mathematics, achieving the rank of Second Wrangler in the 1865 Cambridge Mathematical Tripos. Marshall experienced a mental crisis that led him to abandon physics and switch to philosophy. He began with metaphysics, specifically \"the philosophical foundation of knowledge, especially in relation to theology.\". Metaphysics led Marshall to ethics, specifically a Sidgwickian version of utilitarianism; ethics, in turn, led him to economics, because economics played an essential role in providing the preconditions for the improvement of the working class.\n\nHe saw that the duty of economics was to improve material conditions, but such improvement would occur, Marshall believed, only in connection with social and political forces. His interest in Georgism, liberalism, socialism, trade unions, women's education, poverty and progress reflect the influence of his early social philosophy on his later activities and writings.\n\nMarshall was elected in 1865 to a fellowship at St John's College at Cambridge, and became lecturer in the moral sciences in 1868. In 1885 he became professor of political economy at Cambridge, where he remained until his retirement in 1908. Over the years he interacted with many British thinkers including Henry Sidgwick, W.K. Clifford, Benjamin Jowett, William Stanley Jevons, Francis Ysidro Edgeworth, John Neville Keynes and John Maynard Keynes. Marshall founded the \"Cambridge School\" which paid special attention to increasing returns, the theory of the firm, and welfare economics; after his retirement leaderships passed to Arthur Cecil Pigou and John Maynard Keynes.\n\nMarshall desired to improve the mathematical rigour of economics and transform it into a more scientific profession. In the 1870s he wrote a small number of tracts on international trade and the problems of protectionism. In 1879, many of these works were compiled into a work entitled \"The Theory of Foreign Trade: The Pure Theory of Domestic Values\". In the same year (1879) he published \"The Economics of Industry\" with his wife Mary Paley.\n\nAlthough Marshall took economics to a more mathematically rigorous level, he did not want mathematics to overshadow economics and thus make economics irrelevant to the layman. Accordingly, Marshall tailored the text of his books to laymen and put the mathematical content in the footnotes and appendices for the professionals. In a letter to A. L. Bowley, he laid out the following system: \n\nMarshall had been Mary Paley's professor of political economy at Cambridge and the two were married in 1877, forcing Marshall to leave his position as a Fellow of St John's College, Cambridge to comply with celibacy rules at the university. He became the first principal at University College, Bristol, which was the institution that later became the University of Bristol, again lecturing on political economy and economics. He perfected his \"Economics of Industry\" while at Bristol, and published it more widely in England as an economic curriculum; its simple form stood upon sophisticated theoretical foundations. Marshall achieved a measure of fame from this work, and upon the death of William Jevons in 1882, Marshall became the leading British economist of the scientific school of his time.\n\nMarshall returned to Cambridge, via a brief period at Balliol College, Oxford during 1883–4, to take the seat as Professor of Political Economy in 1884 on the death of Henry Fawcett. At Cambridge he endeavoured to create a new tripos for economics, a goal which he would only achieve in 1903. Until that time, economics was taught under the Historical and Moral Sciences Triposes which failed to provide Marshall the kind of energetic and specialised students he desired.\n\nMarshall began his economic work, the \"Principles of Economics\", in 1881, and spent much of the next decade at work on the treatise. His plan for the work gradually extended to a two-volume compilation on the whole of economic thought. The first volume was published in 1890 to worldwide acclaim, establishing him as one of the leading economists of his time. The second volume, which was to address foreign trade, money, trade fluctuations, taxation, and collectivism, was never published.\n\n\"Principles of Economics\" established his worldwide reputation. It appeared in 8 editions, starting at 750 pages and growing to 870 pages. It decisively shaped the teaching of economics in English-speaking countries. Its main technical contribution was a masterful analysis of the issues of elasticity, consumer surplus, increasing and diminishing returns, short and long terms, and marginal utility. Many of the ideas were original with Marshall; others were improved versions of the ideas by W. S. Jevons and others.\n\nIn a broader sense Marshall hoped to reconcile the classical and modern theories of value. John Stuart Mill had examined the relationship between the value of commodities and their production costs, on the theory that value depends on the effort expended in manufacture. Jevons and the Marginal Utility theorists had elaborated a theory of value based on the idea of maximising utility, holding that value depends on demand. Marshall's work used both these approaches, but he focused more on costs. He noted that, in the short run, supply cannot be changed and market value depends mainly on demand. In an intermediate time period, production can be expanded by existing facilities, such as buildings and machinery, but, since these do not require renewal within this intermediate period, their costs (called fixed, overhead, or supplementary costs) have little influence on the sale price of the product. Marshall pointed out that it is the prime or variable costs, which constantly recur, that influence the sale price most in this period. In a still longer period, machines and buildings wear out and have to be replaced, so that the sale price of the product must be high enough to cover such replacement costs. This classification of costs into fixed and variable and the emphasis given to the element of time probably represent one of Marshall's chief contributions to economic theory. He was committed to partial equilibrium models over general equilibrium on the grounds that the inherently dynamical nature of economics made the former more practically useful.\n\nMuch of the success of Marshall's teaching and \"Principles\" book derived from his effective use of diagrams, which were soon emulated by other teachers worldwide.\n\nAlfred Marshall was the first to develop the standard supply and demand graph demonstrating a number of fundamentals regarding supply and demand including the supply and demand curves, market equilibrium, the relationship between quantity and price in regards to supply and demand, the law of marginal utility, the law of diminishing returns, and the ideas of consumer and producer surpluses. This model is now used by economists in various forms using different variables to demonstrate several other economic principles. Marshall's model allowed a visual representation of complex economic fundamentals where before all the ideas and theories were only capable of being explained through words. These models are now critical throughout the study of economics because they allow a clear and concise representation of the fundamentals or theories being explained.\n\nMarshall is considered to be one of the most influential economists of his time, largely shaping mainstream economic thought for the next fifty years, and being one of the founders of the school of neoclassical economics. Although his economics was advertised as extensions and refinements of the work of Adam Smith, David Ricardo, Thomas Robert Malthus and John Stuart Mill, he extended economics away from its classical focus on the market economy and instead popularised it as a study of human behaviour. He downplayed the contributions of certain other economists to his work, such as Léon Walras, Vilfredo Pareto and Jules Dupuit, and only grudgingly acknowledged the influence of Stanley Jevons himself.\n\nMarshall was one of those who used utility analysis, but not as a theory of value. He used it as a part of the theory to explain demand curves and the principle of substitution. Marshall's scissors analysis – which combined demand and supply, that is, utility and cost of production, as if in the two blades of a pair of scissors – effectively removed the theory of value from the center of analysis and replaced it with the theory of price. While the term \"value\" continued to be used, for most people it was a synonym for \"price\". Prices no longer were thought to gravitate toward some ultimate and absolute basis of price; prices were existential, between the relationship of demand and supply.\n\nMarshall's influence on codifying economic thought is difficult to deny. He popularised the use of supply and demand functions as tools of price determination (previously discovered independently by Cournot); modern economists owe the linkage between price shifts and curve shifts to Marshall. Marshall was an important part of the \"marginalist revolution;\" the idea that consumers attempt to adjust consumption until marginal utility equals the price was another of his contributions. The price elasticity of demand was presented by Marshall as an extension of these ideas. Economic welfare, divided into producer surplus and consumer surplus, was contributed by Marshall, and indeed, the two are sometimes described eponymously as 'Marshallian surplus.' He used this idea of surplus to rigorously analyse the effect of taxes and price shifts on market welfare. Marshall also identified quasi-rents.\n\nMarshall's brief references to the social and cultural relations in the \"industrial districts\" of England were used as a starting point for late twentieth-century work in economic geography and institutional economics on clustering and learning organisations.\n\nGary Becker (1930-2014), the 1992 Nobel prize winner in economics, has mentioned that Milton Friedman and Alfred Marshall were the two greatest influences on his work.\n\nAnother contribution that Marshall made was differentiating concepts of internal and external economies of scale. That is that when costs of input factors of production go down, it is a positive externality for all the firms in the market place, outside the control of any of the firms.\n\nA concept based on a pattern of organisation that was common in late nineteenth century Britain in which firms concentrating on the manufacture of certain products were geographically clustered. Comments made by Marshall in Book 4, Chapter 10 of \"Principles of Economics\" have been used by economists and economic geographers to discuss this phenomenon.\n\nThe two dominant characteristics of a Marshallian industrial district are high degrees of vertical and horizontal specialisation and a very heavy reliance on market mechanism for exchange. Firms tend to be small and to focus on a single function in the production chain. Firms located in industrial districts are highly competitive in the neoclassical sense, and in many cases there is little product differentiation. The major advantages of Marshallian industrial districts arise from simple propinquity of firms, which allows easier recruitment of skilled labour and rapid exchanges of commercial and technical information through informal channels. They illustrate competitive capitalism at its most efficient, with transaction costs reduced to a practical minimum, but they are feasible only when economies of scale are limited.\n\nMarshall served as President of the first day of the 1889 Co-operative Congress.\n\nOver the next two decades he worked to complete the second volume of his \"Principles,\" but his unyielding attention to detail and ambition for completeness prevented him from mastering the work's breadth. The work was never finished and many other, lesser works he had begun work on – a memorandum on trade policy for the Chancellor of the Exchequer in the 1890s, for instance – were left incomplete for the same reasons.\n\nHis health problems had gradually grown worse since the 1880s, and in 1908 he retired from the university. He hoped to continue work on his \"Principles\" but his health continued to deteriorate and the project had continued to grow with each further investigation. The outbreak of the First World War in 1914 prompted him to revise his examinations of the international economy and in 1919 he published \"Industry and Trade\" at the age of 77. This work was a more empirical treatise than the largely theoretical \"Principles\", and for that reason it failed to attract as much acclaim from theoretical economists. In 1923, he published \"Money, Credit, and Commerce,\" a broad amalgam of previous economic ideas, published and unpublished, stretching back a half-century.\n\nFrom 1890 to 1924 he was the respected father of the economic profession and to most economists for the half-century after his death, the venerable grandfather. He had shied away from controversy during his life in a way that previous leaders of the profession had not, although his even-handedness drew great respect and even reverence from fellow economists, and his home at Balliol Croft in Cambridge had no shortage of distinguished guests. His students at Cambridge became leading figures in economics, including John Maynard Keynes and Arthur Cecil Pigou. His most important legacy was creating a respected, academic, scientifically founded profession for economists in the future that set the tone of the field for the remainder of the 20th century.\n\nMarshall died aged 81 at his home in Cambridge and is buried in the Ascension Parish Burial Ground. The library of the Department of Economics at Cambridge University (The Marshall Library of Economics), the Economics society at Cambridge (The Marshall Society) as well as the University of Bristol Economics department are named after him. His archive is available for consultation by appointment at the Marshall Library of Economics.\n\nHis home, Balliol Croft, was renamed Marshall House in 1991 in his honour when it was bought by Lucy Cavendish College, Cambridge.\n\nAlfred Marshall's wife was Mary Paley, co-founder of Newnham College; she continued to live in Balliol Croft until her death in 1944; her ashes were scattered in the garden.\n\n\n\n\n"}
{"id": "57746895", "url": "https://en.wikipedia.org/wiki?curid=57746895", "title": "Ali Reza Ashrafi", "text": "Ali Reza Ashrafi\n\nAli Reza Ashrafi is an Iranian mathematician who works in computational group theory and mathematical chemistry. Ashrafi is a professor of the Department of mathematics of University of Kashan.\n\n"}
{"id": "35409799", "url": "https://en.wikipedia.org/wiki?curid=35409799", "title": "Ampère Prize", "text": "Ampère Prize\n\nThe Prix Ampère de l’Électricité de France is a scientific prize awarded annually by the French Academy of Sciences. \n\nFounded in 1974 in honor of André-Marie Ampère to celebrate his 200th birthday in 1975, the award is granted to one or more French scientists for outstanding research work in mathematics or physics. The monetary award is 30,500 euro, funded by Électricité de France.\n\n"}
{"id": "91111", "url": "https://en.wikipedia.org/wiki?curid=91111", "title": "Angle trisection", "text": "Angle trisection\n\nAngle trisection is a classical problem of compass and straightedge constructions of ancient Greek mathematics. It concerns construction of an angle equal to one third of a given arbitrary angle, using only two tools: an unmarked straightedge and a compass.\n\nThe problem as stated is impossible to solve for arbitrary angles, as proved by Pierre Wantzel in 1837. However, although there is no way to trisect an angle \"in general\" with just a compass and a straightedge, some special angles can be trisected. For example, it is relatively straightforward to trisect a right angle (that is, to construct an angle of measure 30 degrees).\n\nIt is possible to trisect an arbitrary angle by using tools other than straightedge and compass. For example, neusis construction, also known to ancient Greeks, involves simultaneous sliding and rotation of a marked straightedge, which cannot be achieved with the original tools. Other techniques were developed by mathematicians over the centuries.\n\nBecause it is defined in simple terms, but complex to prove unsolvable, the problem of angle trisection is a frequent subject of pseudomathematical attempts at solution by naive enthusiasts. These \"solutions\" often involve mistaken interpretations of the rules, or are simply incorrect.\n\nUsing only an unmarked straightedge and a compass, Greek mathematicians found means to divide a line into an arbitrary set of equal segments, to draw parallel lines, to bisect angles, to construct many polygons, and to construct squares of equal or twice the area of a given polygon.\n\nThree problems proved elusive, specifically, trisecting the angle, doubling the cube, and squaring the circle. The problem of angle trisection reads:\n\nConstruct an angle equal to one-third of a given arbitrary angle (or divide it into three equal angles), using only two tools:\n\nPierre Wantzel published a proof of the impossibility of classically trisecting an arbitrary angle in 1837. Wantzel's proof, restated in modern terminology, uses the abstract algebra of field extensions, a topic now typically combined with Galois theory. However Wantzel published these results earlier than Galois (whose work was published in 1846) and did not use the connection between field extensions and groups that is the subject of Galois theory itself.\n\nThe problem of constructing an angle of a given measure is equivalent to constructing two segments such that the ratio of their length is . From a solution to one of these two problems, one may pass to a solution of the other by a compass and straightedge construction. The triple-angle formula gives an expression relating the cosines of the original angle and its trisection:  = . It follows that, given a segment that is defined to have unit length, the problem of angle trisection is equivalent to constructing a segment whose length is the root of a cubic polynomial. This equivalence reduces the original geometric problem to a purely algebraic problem.\n\nEvery rational number is constructible. Every irrational number that is constructible in a single step from some given numbers is a root of a polynomial of degree 2 with coefficients in the field generated by these numbers. Therefore, any number that is constructible by a sequence of steps is a root of a minimal polynomial whose degree is a power of two. Note also that radians (60 degrees, written 60°) is constructible. The argument below shows that it is impossible to construct a 20° angle. This implies that a 60° angle cannot be trisected, and thus that an arbitrary angle cannot be trisected.\n\nDenote the set of rational numbers by . If 60° could be trisected, the degree of a minimal polynomial of over would be a power of two. Now let . Note that = = . Then by the triple-angle formula, and so . Thus . Define to be the polynomial .\n\nSince is a root of , the minimal polynomial for is a factor of . Because has degree 3, if it is reducible over by then it has a rational root. By the rational root theorem, this root must be or , but none of these is a root. Therefore, is irreducible over by , and the minimal polynomial for is of degree .\n\nSo an angle of measure cannot be trisected.\n\nHowever, some angles can be trisected. For example, for any constructible angle , an angle of measure can be trivially trisected by ignoring the given angle and directly constructing an angle of measure . There are angles that are not constructible but are trisectible (despite the one-third angle itself being non-constructible). For example, is such an angle: five angles of measure combine to make an angle of measure , which is a full circle plus the desired .\n\nFor a positive integer , an angle of measure is \"trisectible\" if and only if does not divide . In contrast, is \"constructible\" if and only if is a power of or the product of a power of with the product of one or more distinct Fermat primes.\n\nAgain, denote the set of rational numbers by .\n\nTheorem: An angle of measure may be trisected if and only if is reducible over the field extension .\n\nThe proof is a relatively straightforward generalization of the proof given above that a angle is not trisectible.\n\nThe general problem of angle trisection is solvable by using additional tools, and thus going outside of the original Greek framework of compass and straightedge.\n\nMany incorrect methods of trisecting the general angle have been proposed. Some of these methods provide reasonable approximations; others (some of which are mentioned below) involve tools not permitted in the classical problem. The mathematician Underwood Dudley has detailed some of these failed attempts in his book \"The Trisectors\".\n\nTrisection can be approximated by repetition of the compass and straightedge method for bisecting an angle. The geometric series 1/3 = 1/4 + 1/16 + 1/64 + 1/256 + ⋯ or 1/3 = 1/2-1/4+1/8-1/16+... can be used as a basis for the bisections. An approximation to any degree of accuracy can be obtained in a finite number of steps.\n\nTrisection, like many constructions impossible by ruler and compass, can easily be accomplished by the more powerful operations of paper folding, or origami. Huzita's axioms (types of folding operations) can construct cubic extensions (cube roots) of given lengths, whereas ruler-and-compass can construct only quadratic extensions (square roots).\n\nThere are a number of simple linkages which can be used to make an instrument to trisect angles including Kempe's Trisector and Sylvester's Link Fan or Isoklinostat.\n\nIn 1932, Ludwig Bieberbach published in \"Journal für die reine und angewandte Mathematik\" his work \"Zur Lehre von den kubischen Konstruktionen\". He states therein (free translation):\n\nThe following description of the adjacent construction (animation) contains their continuation up to the complete angle trisection.\n\nIt begins with the first unit circle around its center formula_1, the first angle limb formula_2, and the second unit circle around formula_3 following it. Now the diameter formula_2 from formula_3 is extended to the circle line of this unit circle, the intersection point formula_6 being created. Following the circle arc around formula_3 with the radius formula_2 and the drawing of the second angle limb from the angle formula_9, the point formula_10 results. Now the so-called \"additional construction mean\" is used, in the illustrated example it is the \"Geodreieck\". This geometry triangle, as it is also called, is now placed on the drawing in the following manner: The vertex of the right angle determines the point formula_11 on the angle leg formula_12, a cathetus of the triangle passes through the point formula_6 and the other affects the unit circle formula_1. \nAfter connecting the point formula_6 to formula_11 and drawing the tangent from formula_11 to the unit circle around formula_1, the above-mentioned \"right angle hook\" respectively \"Rechtwinkelhaken\" is shown. The angle enclosed by the segments formula_19 and formula_20 is thus exactly formula_21. It goes on with the parallel to formula_19 from formula_3, the alternate angle formula_21 and the point formula_25 are being created. A further parallel to formula_19 from formula_1 determines the point of contact formula_28 from the tangent with the unit circle about formula_1.Finally, draw a straight line from formula_3 through formula_28 until it intersects the unit circle in formula_32. Thus the angle formula_9 has exactly three parts.\n\nThere are certain curves called trisectrices which, if drawn on the plane using other methods, can be used to trisect arbitrary angles.\n\nThe known Trisectrix of Colin Maclaurin from the year 1742 is used.\n\nIn Cartesian coordinates this curve is described with the equation\n\nor, in implicit form, \n\nFirst, the diameter formula_36 with its center formula_37 is determined. This is followed by the semicircle formula_38 with the subsequent generation of the trisectrix as the implicit curve. Thus, the basic construction for the angle trisection of angles formula_39 is completed. Now, the second angle limb formula_40 is drawn in such a way that it encloses with the first angle limb formula_41 the angle formula_42 to be divided. The angle limb formula_40 intersects the trisectrix in formula_25. Next, a straight line from formula_1 is drawn through formula_25 to the semicircle, resulting in the intersection formula_28. The angle formula_48 generated by formula_49 and formula_50 is the angle formula_51 sought.\n\nAnother means to trisect an arbitrary angle by a \"small\" step outside the Greek framework is via a ruler with two marks a set distance apart. The next construction is originally due to Archimedes, called a \"Neusis construction\", i.e., that uses tools other than an \"un-marked\" straightedge. The diagrams we use show this construction for an acute angle, but it indeed works for any angle up to 180 degrees.\n\nThis requires three facts from geometry (at right):\n\nLet be the horizontal line in the adjacent diagram. Angle (left of point ) is the subject of trisection. First, a point is drawn at an angle's ray, one unit apart from . A circle of radius is drawn. Then, the markedness of the ruler comes into play: one mark of the ruler is placed at and the other at . While keeping the ruler (but not the mark) touching , the ruler is slid and rotated until one mark is on the circle and the other is on the line . The mark on the circle is labeled and the mark on the line is labeled . This ensures that . A radius is drawn to make it obvious that line segments , , and all have equal length. Now, triangles and are isosceles, thus (by Fact 3 above) each has two equal angles.\n\nHypothesis: Given is a straight line, and , , and all have equal length,\n\nConclusion: angle .\n\nProof:\n\nClearing, , or , and the theorem is proved.\n\nAgain, this construction stepped outside the framework of allowed constructions by using a marked straightedge.\n\nThomas Hutcheson published an article in the Mathematics Teacher that used a string instead of a compass and straight edge. A string can be used as either a straight edge (by stretching it) or a compass (by fixing one point and identifying another), but can also wrap around a cylinder, the key to Hutcheson's solution.\n\nHutcheson constructed a cylinder from the angle to be trisected by drawing an arc across the angle, completing it as a circle, and constructing from that circle a cylinder on which a, say, equilateral triangle was inscribed (a 360-degree angle divided in three). This was then \"mapped\" onto the angle to be trisected, with a simple proof of similar triangles.\n\nA \"tomahawk\" is a geometric shape consisting of a semicircle and two orthogonal line segments, such that the length of the shorter segment is equal to the circle radius. Trisection is executed by leaning the end of the tomahawk's shorter segment on one ray, the circle's edge on the other, so that the \"handle\" (longer segment) crosses the angle's vertex; the trisection line runs between the vertex and the center of the semicircle.\n\nNote that while a tomahawk is constructible with compass and straightedge, it is not generally possible to construct a tomahawk in any desired position. Thus, the above construction does not contradict the nontrisectibility of angles with ruler and compass alone.\n\nThe tomahawk produces the same geometric effect as the paper-folding method: the distance between circle center and the tip of the shorter segment is twice the distance of the radius, which is guaranteed to contact the angle. It is also equivalent to the use of an architects L-Ruler (Carpenter's Square).\n\nAn angle can be trisected with a device that is essentially a four-pronged version of a compass, with linkages between the prongs designed to keep the three angles between adjacent prongs equal.\n\nA cubic equation with real coefficients can be solved geometrically with compass, straightedge, and an angle trisector if and only if it has three real roots.\n\nA regular polygon with \"n\" sides can be constructed with ruler, compass, and angle trisector if and only if formula_63 where \"r, s, k\" ≥ 0 and where the \"p\" are distinct primes greater than 3 of the form formula_64 (i.e. Pierpont primes greater than 3).\n\nFor any nonzero integer , an angle of measure radians can be divided into equal parts with straightedge and compass if and only if is either a power of or is a power of multiplied by the product of one or more distinct Fermat primes, none of which divides . In the case of trisection (, which is a Fermat prime), this condition becomes the above-mentioned requirement that not be divisible by .\n\n\n\n\n"}
{"id": "7580572", "url": "https://en.wikipedia.org/wiki?curid=7580572", "title": "Beckman–Quarles theorem", "text": "Beckman–Quarles theorem\n\nIn geometry, the Beckman–Quarles theorem, named after F. S. Beckman and D. A. Quarles, Jr., states that if a transformation of the Euclidean plane or a higher-dimensional Euclidean space preserves unit distances, then it preserves all distances. Equivalently, every automorphism of the unit distance graph of the plane must be an isometry of the plane.\nBeckman and Quarles published this result in 1953; it was later rediscovered by other authors.\n\nFormally, the result is as follows. Let be a function or multivalued function from a -dimensional Euclidean space to itself, and suppose that, for every pair of points and that are at unit distance from each other, every pair of images and are also at unit distance from each other. Then must be an isometry: it is a one-to-one function that preserves distances between all pairs of points.\n\nBeckman and Quarles observe that the theorem is not true for the real line (one-dimensional Euclidean space). For, the function that returns if is an integer and returns otherwise obeys the preconditions of the theorem (it preserves unit distances) but is not an isometry.\n\nBeckman and Quarles also provide a counterexample for Hilbert space, the space of square-summable sequences of real numbers. This example involves the composition of two discontinuous functions: one that maps every point of the Hilbert space onto a nearby point in a countable dense subspace, and a second that maps this dense set into a countable unit simplex (an infinite set of points all at unit distance from each other). These two transformations map any two points at unit distance from each other to two different points in the dense subspace, and from there map them to two different points of the simplex, which are necessarily at unit distance apart. Therefore, their composition preserves unit distances. However, it is not an isometry, because it maps every pair of points, no matter their original distance, either to the same point or to a unit distance.\n\nFor transformations only of the subset of Euclidean space with Cartesian coordinates that are rational numbers, the situation is more complicated than for the full Euclidean plane. In this case, there exist unit-distance-preserving non-isometries of dimensions up to four, but none for dimensions five and above. Similar results hold also for mappings of the rational points that preserve other distances, such as the square root of two.\n\nOne way of rephrasing the Beckman–Quarles theorem is that, for the unit distance graph whose vertices are all of the points in the plane, with an edge between any two points at unit distance, the only graph automorphisms are the obvious ones coming from isometries of the plane. For pairs of points whose distance is an algebraic number , there is a finite version of this theorem: Maehara showed that there is a finite rigid unit distance graph in which some two vertices and must be at distance from each other, from which it follows that any transformation of the plane that preserves the unit distances in must also preserve the distance between and .\n\nSeveral authors have studied analogous results for other types of geometries. For instance, it is possible to replace Euclidean distance by the value of a quadratic form.\nBeckman–Quarles theorems have been proven for non-Euclidean spaces such as Minkowski space, inversive distance in the Möbius plane, finite Desarguesian planes, and spaces defined over fields with nonzero characteristic.\nAdditionally, theorems of this type have been used to characterize transformations other than the isometries, such as Lorentz transformations.\n"}
{"id": "55828944", "url": "https://en.wikipedia.org/wiki?curid=55828944", "title": "Bellman's lost in a forest problem", "text": "Bellman's lost in a forest problem\n\nBellman's lost-in-a-forest problem is an unsolved minimization problem in geometry, originating in 1955 by the American applied mathematician Richard E. Bellman. The problem is often stated as follows: \"A hiker is lost in a forest whose shape and dimensions are precisely known to him. What is the best path for him to follow to escape from the forest?\" It is usually assumed that the hiker does not know the starting point or direction he is facing. The best path is taken to be the one that minimizes the worst-case distance to travel before reaching the edge of the forest. Other variations of the problem have been studied. \n\nA proven solution is only known for a few shapes or classes of shape. A general solution would be in the form of a geometric algorithm which takes the shape of the forest as input and returns the optimal escape path as the output. Although real world applications are not apparent, the problem falls into a class of geometric optimization problems including search strategies that are of practical importance. A bigger motivation for study has been the connection to Moser's worm problem. It was included in a list of 12 problems described by the mathematician Scott W. Williams as \"million buck problems\" because he believed that the techniques involved in their resolution will be worth at least a million dollars to mathematics.\n"}
{"id": "3876", "url": "https://en.wikipedia.org/wiki?curid=3876", "title": "Binomial distribution", "text": "Binomial distribution\n\n</math>\n\nIn probability theory and statistics, the binomial distribution with parameters \"n\" and \"p\" is the discrete probability distribution of the number of successes in a sequence of \"n\" independent experiments, each asking a yes–no question, and each with its own boolean-valued outcome: a random variable containing a single bit of information: success/yes/true/one (with probability \"p\") or failure/no/false/zero (with probability \"q\" = 1 − \"p\"). \nA single success/failure experiment is also called a Bernoulli trial or Bernoulli experiment and a sequence of outcomes is called a Bernoulli process; for a single trial, i.e., \"n\" = 1, the binomial distribution is a Bernoulli distribution. The binomial distribution is the basis for the popular binomial test of statistical significance.\n\nThe binomial distribution is frequently used to model the number of successes in a sample of size \"n\" drawn with replacement from a population of size \"N.\" If the sampling is carried out without replacement, the draws are not independent and so the resulting distribution is a hypergeometric distribution, not a binomial one. However, for \"N\" much larger than \"n\", the binomial distribution remains a good approximation, and is widely used.\n\nIn general, if the random variable \"X\" follows the binomial distribution with parameters \"n\" ∈ ℕ and \"p\" ∈ [0,1], we write \"X\" ~ B(\"n\", \"p\"). The probability of getting exactly \"k\" successes in \"n\" trials is given by the probability mass function:\n\nfor \"k\" = 0, 1, 2, ..., \"n\", where\n\nis the binomial coefficient, hence the name of the distribution. The formula can be understood as follows. \"k\" successes occur with probability \"p\" and \"n\" − \"k\" failures occur with probability (1 − \"p\"). However, the \"k\" successes can occur anywhere among the \"n\" trials, and there are formula_10 different ways of distributing \"k\" successes in a sequence of \"n\" trials.\n\nIn creating reference tables for binomial distribution probability, usually the table is filled in up to \"n\"/2 values. This is because for \"k\" > \"n\"/2, the probability can be calculated by its complement as\n\nLooking at the expression \"ƒ\"(\"k\", \"n\", \"p\") as a function of \"k\", there is a \"k\" value that maximizes it. This \"k\" value can be found by calculating\nand comparing it to 1. There is always an integer \"M\" that satisfies\n\n\"ƒ\"(\"k\", \"n\", \"p\") is monotone increasing for \"k\" < \"M\" and monotone decreasing for \"k\" > \"M\", with the exception of the case where (\"n\" + 1)\"p\" is an integer. In this case, there are two values for which \"ƒ\" is maximal: (\"n\" + 1)\"p\" and (\"n\" + 1)\"p\" − 1. \"M\" is the \"most probable\" (\"most likely\") outcome of the Bernoulli trials and is called the mode. Note that the probability of it occurring can be fairly small.\n\nThe cumulative distribution function can be expressed as:\n\nwhere formula_15 is the \"floor\" under \"k\", i.e. the greatest integer less than or equal to \"k\".\n\nIt can also be represented in terms of the regularized incomplete beta function, as follows:\n\nSome closed-form bounds for the cumulative distribution function are given below.\n\nSuppose a biased coin comes up heads with probability 0.3 when tossed. What is the probability of achieving 0, 1..., 6 heads after six tosses?\n\nIf \"X\" ~ \"B\"(\"n\", \"p\"), that is, \"X\" is a binomially distributed random variable, n being the total number of experiments and p the probability of each experiment yielding a successful result, then the expected value of \"X\" is:\n\nFor example, if \"n\" = 100, and \"p\" = 1/4, then the average number of successful results will be 25.\n\nProof: We calculate the mean, \"μ\", directly calculated from its definition\n\nand the binomial theorem:\n\nIt is also possible to deduce the mean from the equation formula_27 whereby all formula_28 are Bernoulli distributed random variables with formula_29 (formula_30 if the \"i\"th experiment succeeds and formula_31 otherwise). We get:\nformula_32\nThe variance is:\n\nProof: Let formula_27 where all formula_28 are independently Bernoulli distributed random variables. Since formula_36, we get:\n\nUsually the mode of a binomial \"B\"(\"n\", \"p\") distribution is equal to formula_38, where formula_39 is the floor function. However, when (\"n\" + 1)\"p\" is an integer and \"p\" is neither 0 nor 1, then the distribution has two modes: (\"n\" + 1)\"p\" and (\"n\" + 1)\"p\" − 1. When \"p\" is equal to 0 or 1, the mode will be 0 and \"n\" correspondingly. These cases can be summarized as follows:\n\nProof: Let\n\nFor formula_42 only formula_43 has a nonzero value with formula_44. For formula_45 we find formula_46 and formula_47 for formula_48. This proves that the mode is 0 for formula_42 and formula_7 for formula_45.\n\nLet formula_52. We find\n\nFrom this follows\n\nSo when formula_55 is an integer, then formula_55 and formula_57 is a mode. In the case that formula_58, then only formula_59 is a mode.\n\nIn general, there is no single formula to find the median for a binomial distribution, and it may even be non-unique. However several special results have been established:\n\nIf two binomially distributed random variables \"X\" and \"Y\" are observed together, estimating their covariance can be useful. The covariance is\n\nIn the case \"n\" = 1 (the case of Bernoulli trials) \"XY\" is non-zero only when both \"X\" and \"Y\" are one, and \"μ\" and \"μ\" are equal to the two probabilities. Defining \"p\" as the probability of both happening at the same time, this gives\n\nand for \"n\" independent pairwise trials\n\nIf \"X\" and \"Y\" are the same variable, this reduces to the variance formula given above.\n\nIf \"X\" ~ B(\"n\", \"p\") and \"Y\" ~ B(\"m\", \"p\") are independent binomial variables with the same probability \"p\", then \"X\" + \"Y\" is again a binomial variable; its distribution is \"Z=X+Y\" ~ B(\"n+m\", \"p\"):\n\nHowever, if \"X\" and \"Y\" do not have the same probability \"p\", then the variance of the sum will be smaller than the variance of a binomial variable distributed as formula_64\n\nThis result was first derived by Katz et al. in 1978.\n\nLet p and p be the probabilities of success in the binomial distributions B(X,n) and B(Y,m) respectively. Let T = (X/n)/(Y/m).\n\nThen log(T) is approximately normally distributed with mean log(p/p) and variance ((1/p) - 1)/n + ((1/p) - 1)/m.\n\nIf \"X\" ~ B(\"n\", \"p\") and, conditional on \"X\", \"Y\" ~ B(\"X\", \"q\"), then \"Y\" is a simple binomial variable with distribution \"Y\" ~ B(\"n\", \"pq\").\n\nFor example, imagine throwing \"n\" balls to a basket \"U\" and taking the balls that hit and throwing them to another basket \"U\". If \"p\" is the probability to hit \"U\" then \"X\" ~ B(\"n\", \"p\") is the number of balls that hit \"U\". If \"q\" is the probability to hit \"U\" then the number of balls that hit \"U\" is \"Y\" ~ B(\"X\", \"q\") and therefore \"Y\" ~ B(\"n\", \"pq\").\nSince formula_65 and formula_66, by the law of total probability,\nSince formula_68, the equation above can be expressed as\nFactoring formula_70 and pulling all the terms that don't depend on formula_71 out of the sum now yields\nAfter substituting formula_73 in the expression above, we get\nNotice that the sum (in the parentheses) above equals formula_75 by the binomial theorem. Substituting this in finally yields\nand thus formula_77 as desired.\nThe Bernoulli distribution is a special case of the binomial distribution, where \"n\" = 1. Symbolically, \"X\" ~ B(1, \"p\") has the same meaning as \"X\" ~ B(\"p\"). Conversely, any binomial distribution, B(\"n\", \"p\"), is the distribution of the sum of \"n\" Bernoulli trials, B(\"p\"), each with the same probability \"p\".\n\nThe binomial distribution is a special case of the Poisson binomial distribution, or general binomial distribution, which is the distribution of a sum of \"n\" independent non-identical Bernoulli trials B(\"p\").\n\nIf \"n\" is large enough, then the skew of the distribution is not too great. In this case a reasonable approximation to B(\"n\", \"p\") is given by the normal distribution\n\nand this basic approximation can be improved in a simple way by using a suitable continuity correction.\nThe basic approximation generally improves as \"n\" increases (at least 20) and is better when \"p\" is not near to 0 or 1. Various rules of thumb may be used to decide whether \"n\" is large enough, and \"p\" is far enough from the extremes of zero or one:\n\n\n\nThe rule formula_82 is totally equivalent to request that\nMoving terms around yields:\nSince formula_85 and formula_86, to obtain the desired conditions:\nNotice that these conditions automatically imply that formula_88. On the other hand, apply again the square root and divide by 3,\nSubtracting the second set of inequalities from the first one yields:\nand so, the desired first rule is satisfied,\n\nAssume that both values formula_92 and formula_93 are greater than 9. Since formula_96, we easily have that \nWe only have to divide now by the respective factors formula_98 and formula_99, to deduce the alternative form of the 3-standard-deviation rule:\nThe following is an example of applying a continuity correction. Suppose one wishes to calculate Pr(\"X\" ≤ 8) for a binomial random variable \"X\". If \"Y\" has a distribution given by the normal approximation, then Pr(\"X\" ≤ 8) is approximated by Pr(\"Y\" ≤ 8.5). The addition of 0.5 is the continuity correction; the uncorrected normal approximation gives considerably less accurate results.\n\nThis approximation, known as de Moivre–Laplace theorem, is a huge time-saver when undertaking calculations by hand (exact calculations with large \"n\" are very onerous); historically, it was the first use of the normal distribution, introduced in Abraham de Moivre's book \"The Doctrine of Chances\" in 1738. Nowadays, it can be seen as a consequence of the central limit theorem since B(\"n\", \"p\") is a sum of \"n\" independent, identically distributed Bernoulli variables with parameter \"p\". This fact is the basis of a hypothesis test, a \"proportion z-test\", for the value of \"p\" using \"x/n\", the sample proportion and estimator of \"p\", in a common test statistic.\n\nFor example, suppose one randomly samples \"n\" people out of a large population and ask them whether they agree with a certain statement. The proportion of people who agree will of course depend on the sample. If groups of \"n\" people were sampled repeatedly and truly randomly, the proportions would follow an approximate normal distribution with mean equal to the true proportion \"p\" of agreement in the population and with standard deviation formula_101\n\nThe binomial distribution converges towards the Poisson distribution as the number of trials goes to infinity while the product \"np\" remains fixed or at least \"p\" tends to zero. Therefore, the Poisson distribution with parameter \"λ\" = \"np\" can be used as an approximation to B(\"n\", \"p\") of the binomial distribution if \"n\" is sufficiently large and \"p\" is sufficiently small. According to two rules of thumb, this approximation is good if \"n\" ≥ 20 and \"p\" ≤ 0.05, or if \"n\" ≥ 100 and \"np\" ≤ 10.\n\nConcerning the accuracy of Poisson approximation, see Novak, ch. 4, and references therein.\n\n\nBeta distributions provide a family of prior probability distributions for binomial distributions in Bayesian inference:\n\nEven for quite large values of \"n\", the actual distribution of the mean is significantly nonnormal. Because of this problem several methods to estimate confidence intervals have been proposed.\n\nIn the equations for confidence intervals below, the variables have the following meaning:\n\n\nThe notation in the formula below differs from the previous formulas in two respects:\n\n\nThe exact (Clopper–Pearson) method is the most conservative.\n\nThe Wald method, although commonly recommended in textbooks, is the most biased.\n\nMethods for random number generation where the marginal distribution is a binomial distribution are well-established.\n\nOne way to generate random samples from a binomial distribution is to use an inversion algorithm. To do so, one must calculate the probability that P(X=k) for all values \"k\" from 0 through \"n\". (These probabilities should sum to a value close to one, in order to encompass the entire sample space.) Then by using a pseudorandom number generator to generate samples uniformly between 0 and 1, one can transform the calculated samples U[0,1] into discrete numbers by using the probabilities calculated in step one.\n\nFor \"k\" ≤ \"np\", upper bounds for the lower tail of the distribution function can be derived. Recall that formula_121, the probability that there are at most \"k\" successes.\n\nHoeffding's inequality yields the bound\n\nand Chernoff's inequality can be used to derive the bound\n\nMoreover, these bounds are reasonably tight when \"p\" = 1/2, since the following expression holds for all \"k\" ≥ 3\"n\"/8\n\nHowever, the bounds do not work well for extreme values of \"p\". In particular, as \"p\" formula_125 1, value \"F\"(\"k\";\"n\",\"p\") goes to zero (for fixed \"k\", \"n\" with \"k\" < \"n\")\nwhile the upper bound above goes to a positive constant. In this case a better bound is given by\n\nAsymptotically, this bound is reasonably tight; see\n\nBoth these bounds are derived directly from the Chernoff bound.\nIt can also be shown that,\n\nThis is proved using the method of types (see for example chapter 12 of \"Elements of Information Theory\" by Cover and Thomas ).\n\nWe can also change the formula_129 in the denominator to formula_130, by approximating the binomial coefficient with Stirling's formula.\n\nThis distribution was derived by James Bernoulli. He considered the case where \"p\" = \"r\"/(\"r\" + \"s\") where \"p\" is the probability of success and \"r\" and \"s\" are positive integers. Blaise Pascal had earlier considered the case where \"p\" = 1/2.\n\n\n"}
{"id": "10708102", "url": "https://en.wikipedia.org/wiki?curid=10708102", "title": "Bundle metric", "text": "Bundle metric\n\nIn differential geometry, the notion of a metric tensor can be extended to an arbitrary vector bundle, and to some principal fiber bundles. This metric is often called a bundle metric, or fibre metric.\n\nIf \"M\" is a topological manifold and :\"E\" → \"M\" a vector bundle on \"M\", then a metric on \"E\" is a bundle map \"k\" : \"E\" × \"E\" → \"M\" × R from the fiber product of \"E\" with itself to the trivial bundle with fiber R such that the restriction of \"k\" to each fibre over \"M\" is a nondegenerate bilinear map of vector spaces.\n\nEvery vector bundle with paracompact base space can be equipped with a bundle metric. For a vector bundle of rank \"n\", this follows from the bundle charts formula_1: the bundle metric can be taken as the pullback of the inner product of a metric on formula_2; for example, the orthonormal charts of Euclidean space. The structure group of such a metric is the orthogonal group \"O\"(\"n\").\n\nIf the manifold \"M\" is a Riemannian manifold, and \"E\" is the tangent bundle T\"M\", then the Riemannian metric gives a bundle metric, and vice versa.\n\nIf the bundle :\"P\" → \"M\" is a principal fiber bundle with group \"G\", and \"G\" is a compact Lie group, then there exists an Ad(\"G\")-invariant inner product \"k\" on the fibers, taken from the inner product on the corresponding compact Lie algebra. More precisely, there is a metric tensor \"k\" defined on the vertical bundle E = V\"P\" such that \"k\" is invariant under left-multiplication:\n\nfor vertical vectors \"X\", \"Y\" and \"L\" is left-multiplication by \"g\" along the fiber, and \"L\" is the pushforward. That is, \"E\" is the vector bundle that consists of the vertical subspace of the tangent of the principal bundle.\n\nMore generally, whenever one has a compact group with Haar measure μ, and an arbitrary inner product \"h(X,Y)\" defined at the tangent space of some point in \"G\", one can define an invariant metric simply by averaging over the entire group, i.e. by defining\n\nas the average.\n\nThe above notion can be extended to the associated bundle formula_5 where \"V\" is a vector space transforming covariantly under some representation of \"G\".\n\nIf the base space \"M\" is also a metric space, with metric \"g\", and the principal bundle is endowed with a connection form ω, then g+kω is a metric defined on the entire tangent bundle \"E\" = T\"P\".\n\nMore precisely, one writes g(\"X\",\"Y\") = \"g\"(\"X\", \"Y\") where is the pushforward of the projection , and \"g\" is the metric tensor on the base space \"M\". The expression \"kω\" should be understood as (\"kω\")(\"X\",\"Y\") = \"k\"(\"ω\"(\"X\"),\"ω\"(\"Y\")), with \"k\" the metric tensor on each fiber. Here, \"X\" and \"Y\" are elements of the tangent space T\"P\".\n\nObserve that the lift g vanishes on the vertical subspace T\"V\" (since vanishes on vertical vectors), while kω vanishes on the horizontal subspace T\"H\" (since the horizontal subspace is defined as that part of the tangent space T\"P\" on which the connection ω vanishes). Since the total tangent space of the bundle is a direct sum of the vertical and horizontal subspaces (that is, T\"P\" = T\"V\" ⊕ T\"H\"), this metric is well-defined on the entire bundle.\n\nThis bundle metric underpins the generalized form of Kaluza–Klein theory due to several interesting properties that it possesses. The scalar curvature derived from this metric is constant on each fiber, this follows from the Ad(\"G\") invariance of the fiber metric \"k\". The scalar curvature on the bundle can be decomposed into three distinct pieces:\nwhere \"R\" is the scalar curvature on the bundle as a whole (obtained from the metric g+kω above), and \"R\"(\"g\") is the scalar curvature on the base manifold \"M\" (the Lagrangian density of the Einstein–Hilbert action), and \"L\"(\"g\", ω) is the Lagrangian density for the Yang–Mills action, and \"R\"(\"k\") is the scalar curvature on each fibre (obtained from the fiber metric \"k\", and constant, due to the Ad(\"G\")-invariance of the metric \"k\"). The arguments denote that \"R\"(\"g\") only depends on the metric \"g\" on the base manifold, but not ω or \"k\", and likewise, that \"R\"(\"k\") only depends on \"k\", and not on \"g\" or ω, and so-on.\n"}
{"id": "10276249", "url": "https://en.wikipedia.org/wiki?curid=10276249", "title": "CLEFIA", "text": "CLEFIA\n\nCLEFIA is a proprietary block cipher algorithm, developed by Sony. Its name is derived from the French word \"clef\", meaning \"key\". The block size is 128 bits and the key size can be 128 bit, 192 bit or 256 bit. It is intended to be used in DRM systems. It is among the cryptographic techniques recommended candidate for Japanese government use by CRYPTREC revision in 2013.\n\n\n"}
{"id": "6573013", "url": "https://en.wikipedia.org/wiki?curid=6573013", "title": "Cameron–Martin theorem", "text": "Cameron–Martin theorem\n\nIn mathematics, the Cameron–Martin theorem or Cameron–Martin formula (named after Robert Horton Cameron and W. T. Martin) is a theorem of measure theory that describes how abstract Wiener measure changes under translation by certain elements of the Cameron–Martin Hilbert space.\n\nThe standard Gaussian measure γ on \"n\"-dimensional Euclidean space R is not translation-invariant. (In fact, there is a unique translation invariant Radon measure up to scale by Haar's theorem: the \"n\"-dimensional Lebesgue measure, denoted here \"dx\".) Instead, a measurable subset \"A\" has Gaussian measure\n\nHere formula_2 refers to the standard Euclidean dot product in R. The Gaussian measure of the translation of \"A\" by a vector \"h\" ∈ R is\n\nSo under translation through \"h\", the Gaussian measure scales by the distribution function appearing in the last display:\n\nThe measure that associates to the set \"A\" the number γ(\"A\"−\"h\") is the pushforward measure, denoted (\"T\")(γ). Here \"T\" : R → R refers to the translation map: \"T\"(\"x\") = \"x\" + \"h\".. The above calculation shows that the Radon–Nikodym derivative of the pushforward measure with respect to the original Gaussian measure is given by\n\nAbstract Wiener measure \"γ\" on a separable Banach space \"E\", where \"i\" : \"H\" → \"E\" is an abstract Wiener space, is also a \"Gaussian measure\" in a suitable sense. How does it change under translation? It turns out that a similar formula to the one above holds if we consider only translations by elements of the dense subspace \"i\"(\"H\") ⊆ \"E\".\n\nLet \"i\" : \"H\" → \"E\" be an abstract Wiener space with abstract Wiener measure \"γ\" : Borel(\"E\") → [0, 1]. For \"h\" ∈ \"H\", define \"T\" : \"E\" → \"E\" by \"T\"(\"x\") = \"x\" + \"i\"(\"h\"). Then (\"T\")(γ) is equivalent to \"γ\" with Radon–Nikodym derivative\n\nwhere\n\ndenotes the Paley–Wiener integral.\n\nThe Cameron–Martin formula is valid only for translations by elements of the dense subspace \"i\"(\"H\") ⊆ \"E\", called Cameron–Martin space, and not by arbitrary elements of \"E\". If the Cameron–Martin formula did hold for arbitrary translations, it would contradict the following result:\n\nIn fact, \"γ\" is quasi-invariant under translation by an element \"v\" if and only if \"v\" ∈ \"i\"(\"H\"). Vectors in \"i\"(\"H\") are sometimes known as Cameron–Martin directions.\n\nThe Cameron–Martin formula gives rise to an integration by parts formula on \"E\": if \"F\" : \"E\" → R has bounded Fréchet derivative D\"F\" : \"E\" → Lin(\"E\"; R) = \"E\", integrating the Cameron–Martin formula with respect to Wiener measure on both sides gives\n\nfor any \"t\" ∈ R. Formally differentiating with respect to \"t\" and evaluating at \"t\" = 0 gives the integration by parts formula\n\nComparison with the divergence theorem of vector calculus suggests\n\nwhere \"V\" : \"E\" → \"E\" is the constant \"vector field\" \"V\"(\"x\") = \"i\"(\"h\") for all \"x\" ∈ \"E\". The wish to consider more general vector fields and to think of stochastic integrals as \"divergences\" leads to the study of stochastic processes and the Malliavin calculus, and, in particular, the Clark–Ocone theorem and its associated integration by parts formula.\n\nUsing Cameron–Martin theorem one may establish (See Liptser and Shiryayev 1977, p. 280) that for a \"q\" × \"q\" symmetric non-negative definite matrix, \"H\"(\"t\") whose elements \"H\"(\"t\") are continuous and satisfy the condition\n\nit holds for a \"q\"−dimensional Wiener process \"w\"(\"t\") that\n\nwhere \"G\"(\"t\") is a \"q\" × \"q\" nonpositive definite matrix which is a unique solution of the matrix-valued Riccati differential equation\n\n\n"}
{"id": "27438986", "url": "https://en.wikipedia.org/wiki?curid=27438986", "title": "Chunking (division)", "text": "Chunking (division)\n\nIn mathematics education at primary school level, chunking (sometimes also called the partial quotients method) is an elementary approach for solving simple division questions, by repeated subtraction. It is also known as the hangman method with the addition of a line separating the divisor, dividend, and partial quotients. It has a counterpart in the grid method for multiplication.\n\nTo calculate the result of dividing a large number by a small number, the student repeatedly takes away \"chunks\" of the large number, where each \"chunk\" is an easy multiple (for example 100×, 10×, 5× 2×, etc.) of the small number, until the large number has been reduced to zero or the remainder is less than the divisor. At the same time the student keeps a running total of what multiple of the small number has so far been taken away, which eventually becomes the final result of the sum.\n\nSo, for example, to calculate , one might successively subtract 80, 40 and 8 to leave 4,\n\nto establish that is 16 (10+5+1) with 4 remaining.\n\nIn the UK, this approach for elementary division sums has come into widespread classroom use in primary schools since the late 1990s, when the National Numeracy Strategy in its \"numeracy hour\" brought in a new emphasis on more free-form oral and mental strategies for calculations, rather than the rote learning of standard methods. \n\nCompared to the short division and long division methods that are traditionally taught, chunking may seem strange, unsystematic, and arbitrary. However, it is argued that chunking, rather than moving straight to short division, gives a better introduction to division, in part because the focus is always holistic, focusing throughout on the whole calculation and its meaning, rather than just rules for generating successive digits; and because its more free-form nature requires genuine understanding to be successful, rather than just the ability to follow a ritualised procedure.\n\n"}
{"id": "39847439", "url": "https://en.wikipedia.org/wiki?curid=39847439", "title": "Circle Limit III", "text": "Circle Limit III\n\nCircle Limit III is a woodcut made in 1959 by Dutch artist M. C. Escher, in which \"strings of fish shoot up like rockets from infinitely far away\" and then \"fall back again whence they came\".\n\nIt is one of a series of four woodcuts by Escher depicting ideas from hyperbolic geometry. Dutch physicist and mathematician Bruno Ernst called it \"the best of the four\".\n\nEscher became interested in tessellations of the plane after a 1936 visit to the Alhambra in Granada, Spain,\nand from the time of his 1937 artwork \"Metamorphosis I\" he had begun incorporating tessellated human and animal figures into his artworks.\n\nIn a 1958 letter from Escher to H. S. M. Coxeter, Escher wrote that he was inspired to make his \"Circle Limit\" series by a figure in Coxeter's article \"Crystal \nSymmetry and its Generalizations\". Coxeter's figure depicts a tessellation of the hyperbolic plane by right triangles with angles of 30°, 45°, and 90°; triangles with these angles are possible in hyperbolic geometry but not in Euclidean geometry. This tessellation may be interpreted as depicting the lines of reflection and fundamental domains of the (6,4,2) triangle group. An elementary analysis of Coxeter's figure, as Escher might have understood it, is given by .\n\nEscher seems to have believed that the white curves of his woodcut, which bisect the fish, represent hyperbolic lines in the Poincaré disk model of the hyperbolic plane, in which the whole hyperbolic plane is modeled as a disk in the Euclidean plane, and hyperbolic lines are modeled as circular arcs perpendicular to the disk boundary. Indeed, Escher wrote that the fish move \"perpendicularly to the boundary\". However, as Coxeter demonstrated, there is no hyperbolic arrangement of lines whose faces are alternately squares and equilateral triangles, as the figure depicts. Rather, the white curves are hypercycles that meet the boundary circle at angles of approximately 80°.\n\nThe symmetry axes of the triangles and squares that lie between the white lines are true hyperbolic lines. The squares and triangles of the woodcut closely resemble the tritetragonal tiling of the hyperbolic plane, which also features squares and triangles meeting in the same incidence pattern.\nHowever, the precise geometry of these shapes is not the same. In the tritetragonal tiling, the sides of the squares and triangles are hyperbolically straight line segments, which do not link up in smooth curves; instead they form polygonal chains with corners. In Escher's woodcut, the sides of the squares and triangles are formed by arcs of hypercycles, which are not straight in hyperbolic geometry, but which connect smoothly to each other without corners.\n\nThe points at the centers of the squares, where four fish meet at their fins, form the vertices of an order-8 triangular tiling, while the points where three fish fins meet and the points where three white lines cross together form the vertices of its dual, the octagonal tiling. Similar tessellations by lines of fish may be constructed for other hyperbolic tilings formed by polygons other than triangles and squares, or with more than three white curves at each crossing.\n\nEuclidean coordinates of circles containing the three most prominent white curves in the woodcut may be obtained by calculations in the field of rational numbers extended by the square roots of two and three.\n\nViewed as a pattern, ignoring the colors of the fish, in the hyperbolic plane, the woodcut has three-fold and four-fold rotational symmetry at the centers of its triangles and squares, respectively, and order-three dihedral symmetry (the symmetry of an equilateral triangle) at the points where the white curves cross. In John Conway's orbifold notation, this set of symmetries is denoted 433. Each fish provides a fundamental region for this symmetry group. Contrary to appearances, the fish do not have bilateral symmetry: the white curves of the drawing are not axes of reflection symmetry.\nFor example, the angle at the back of the right fin is 90° (where four fins meet), but at the back of the much smaller left fin it is 120° (where three fins meet).\n\nThe fish in \"Circle Limit III\" are depicted in four colors, allowing each string of fish to have a single color and each two adjacent fish to have different colors. Together with the black ink used to outline the fish, the overall woodcut has five colors. It is printed from five wood blocks, each of which provides one of the colors within a quarter of the disk, for a total of 20 impressions. The diameter of the outer circle, as printed, is .\n\nAs well as being included in the collection of the Escher Museum in The Hague, there is a copy of \"Circle Limit III\" in the collection of the National Gallery of Canada.\n\n"}
{"id": "11441746", "url": "https://en.wikipedia.org/wiki?curid=11441746", "title": "Classifying space for O(n)", "text": "Classifying space for O(n)\n\nIn mathematics, the classifying space for O(\"n\") may be constructed as the Grassmannian of \"n\"-planes in an infinite-dimensional real space formula_1.\nIt is analogous to the classifying space for U(\"n\").\n"}
{"id": "1116842", "url": "https://en.wikipedia.org/wiki?curid=1116842", "title": "Discrepancy theory", "text": "Discrepancy theory\n\nIn mathematics, discrepancy theory describes the deviation of a situation from the state one would like it to be in. It is also called the \"theory of irregularities of distribution\". This refers to the theme of \"classical\" discrepancy theory, namely distributing points in some space such that they are evenly distributed with respect to some (mostly geometrically defined) subsets. The discrepancy (irregularity) measures how far a given distribution deviates from an ideal one.\n\nDiscrepancy theory can be described as the study of inevitable irregularities of distributions, in measure-theoretic and combinatorial settings. Just as Ramsey theory elucidates the impossibility of total disorder, discrepancy theory studies the deviations from total uniformity.\n\nA significant event in the history of discrepancy theory was the 1916 paper of Weyl on the uniform distribution of sequences in the unit interval.\n\nDiscrepancy theory is based on the following classic theorems:\n\nThe unsolved problems relating to discrepancy theory include:\n\nApplications for discrepancy theory include:\n\n\n"}
{"id": "25407293", "url": "https://en.wikipedia.org/wiki?curid=25407293", "title": "Electronic Proceedings in Theoretical Computer Science", "text": "Electronic Proceedings in Theoretical Computer Science\n\nElectronic Proceedings in Theoretical Computer Science is an international, peer-reviewed, open access series reporting research results in theoretical computer science, especially in the form of proceedings and post-proceedings of conferences and workshops, in the field of theoretical computer science. As of December 2009, the editor-in-chief of the series is Rob van Glabbeek. The series is indexed by the Digital Bibliography & Library Project (DBLP).\n\n\n"}
{"id": "25413693", "url": "https://en.wikipedia.org/wiki?curid=25413693", "title": "Equidimensional scheme", "text": "Equidimensional scheme\n\nIn algebraic geometry, a field of mathematics, an equidimensional scheme (or, pure dimensional scheme) is a scheme all of whose irreducible components are of the same dimension. All irreducible schemes are equidimensional.\n\nIn affine space, the union of a line and a point not on the line is \"not\" equidimensional. In general, if two closed subschemes of some scheme, neither containing the other, have unequal dimensions, then their union is not equidimensional.\n\nIf a scheme is smooth (for instance, étale) over Spec \"k\" for some field \"k\", then every \"connected\" component (which is then in fact an irreducible component), is equidimensional.\n\n"}
{"id": "1968551", "url": "https://en.wikipedia.org/wiki?curid=1968551", "title": "Extended affix grammar", "text": "Extended affix grammar\n\nIn computer science, extended affix grammars (EAGs) are a formal grammar formalism for describing the context free and context sensitive syntax of language, both natural language and programming languages.\n\nEAGs are a member of the family of two-level grammars; more specifically, a restriction of Van Wijngaarden grammars with the specific purpose of making parsing feasible.\n\nLike Van Wijngaarden grammars, EAGs have \"hyperrules\" that form a context-free grammar except in that their nonterminals may have arguments, known as \"affixes\", the possible values of which are supplied by another context-free grammar, the \"metarules\".\n\nEAGs were introduced and studied by D.A. Watt in 1974; recognizers were developed at the University of Nijmegen between 1985 and 1995. The EAG compiler developed there will generate either a recogniser, a transducer, a translator, or a syntax directed editor for a language described in the EAG formalism. The formalism is quite similar to Prolog, to the extent that it borrowed its cut operator.\n\nEAGs have been used to write grammars of natural languages such as English, Spanish, and Hungarian. The aim was to verify the grammars by making them parse corpora of text (corpus linguistics); hence, parsing had to be sufficiently practical. However, the parse tree explosion problem that ambiguities in natural language tend to produce in this type of approach is worsened for EAGs because each choice of affix value may produce a separate parse, even when several different values are equivalent. The remedy proposed was to switch to the much simpler Affix Grammar over a Finite Lattice (AGFL) instead, in which metagrammars can only produce simple finite languages.\n\n\n"}
{"id": "22577479", "url": "https://en.wikipedia.org/wiki?curid=22577479", "title": "Flatness (mathematics)", "text": "Flatness (mathematics)\n\nIn mathematics, the flatness (symbol: ⏥) of a surface is the degree to which it approximates a mathematical plane. The term is often generalized for higher-dimensional manifolds to describe the degree to which they approximate the Euclidean space of the same dimensionality. (See \"curvature\".)\n\nFlatness in homological algebra and algebraic geometry means, of an object formula_1 in an abelian category, that formula_2 is an exact functor. See flat module or, for more generality, flat morphism.\n"}
{"id": "35678144", "url": "https://en.wikipedia.org/wiki?curid=35678144", "title": "Folded Reed–Solomon code", "text": "Folded Reed–Solomon code\n\nIn coding theory, folded Reed–Solomon codes are like Reed–Solomon codes, which are obtained by mapping formula_1 Reed–Solomon codewords over a larger alphabet by careful bundling of codeword symbols.\n\nFolded Reed–Solomon codes are also a special case of Parvaresh–Vardy codes.\n\nUsing optimal parameters one can decode with a rate of \"R\", and achieve a decoding radius of 1 − \"R\".\n\nThe term \"folded Reed–Solomon codes\" was coined in a paper by V.Y. Krachkovsky with an algorithm that presented Reed–Solomon codes with many random \"phased burst\" errors . The list-decoding algorithm for folded RS codes corrects beyond the formula_2 bound for Reed–Solomon codes achieved by the Guruswami–Sudan algorithm for such phased burst errors.\n\nOne of the ongoing challenges in Coding Theory is to have error correcting codes achieve an optimal trade-off between (Coding) Rate and Error-Correction Radius. Though this may not be possible to achieve practically (due to Noisy Channel Coding Theory issues), quasi optimal tradeoffs can be achieved theoretically.\n\nPrior to Folded Reed–Solomon codes being devised, the best Error-Correction Radius achieved was formula_3, by Reed–Solomon codes for all rates formula_4.\n\nAn improvement upon this formula_3 bound was achieved by Parvaresh and Vardy for rates formula_6\n\nFor formula_7 the Parvaresh–Vardy algorithm can decode a fraction formula_8 of errors.\n\nFolded Reed–Solomon Codes improve on these previous constructions, and can be list decoded in polynomial time for a fraction formula_9 of errors for any constant formula_10.\n\nConsider a Reed–Solomon formula_12 code of length formula_13 and dimension formula_14 and a folding parameter formula_15. Assume that formula_16 divides formula_17.\n\nMapping for Reed–Solomon codes like this:\n\nwhere formula_19 is a primitive element in\n\nThe formula_21 folded version of Reed Solomon code formula_22, denoted formula_23 is a code of block length formula_24 over formula_25. formula_23 are just formula_27 Reed Solomon codes with formula_28 consecutive symbols from RS codewords grouped together.\n\nThe above definition is made more clear by means of the diagram with formula_29, where formula_28 is the folding parameter.\n\nThe message is denoted by formula_31, which when encoded using Reed–Solomon encoding, consists of values of formula_32 at formula_33, where formula_34.\n\nThen bundling is performed in groups of 3 elements, to give a codeword of length formula_35 over the alphabet formula_36.\n\nSomething to be observed here is that the folding operation demonstrated does not change the rate formula_4 of the original Reed–Solomon code.\n\nTo prove this, consider a linear formula_38 code, of length formula_13, dimension formula_40 and distance formula_41. The formula_1 folding operation will make it a formula_43 code. By this, the rate formula_44 will be the same.\n\nAccording to the asymptotic version of the singleton bound, it is known that the relative distance formula_45, of a code must satisfy formula_46 where formula_47is the rate of the code. As proved earlier, since the rate formula_47 is maintained, the relative distance formula_49 also meets the Singleton bound.\n\nFolded Reed–Solomon codes are basically the same as Reed Solomon codes, just viewed over a larger alphabet. To show how this might help, consider a folded Reed–Solomon code with formula_50. Decoding a Reed–Solomon code and folded Reed–Solomon code from the same fraction of errors formula_51 are tasks of almost of the same computational intensity: one can unfold the received word of the folded Reed–Solomon code, treat it as an received word of the original Reed–Solomon code, and run the Reed–Solomon list decoding algorithm on it. Obviously, this list will contain all the folded Reed–Solomon codewords within distance formula_52 of the received word, along with some extras, which we can expurgate.\n\nAlso, decoding a folded Reed–Solomon code is an easier task. Suppose we want to correct a third of errors. The decoding algorithm chosen must correct an error pattern that corrects every third symbol in the Reed–Solomon encoding. But after folding, this error pattern will corrupt all symbols over formula_53 and will eliminate the need for error correction. This propagation of errors is indicated by the blue color in the graphical description. This proves that the for a fixed fraction of errors formula_54 the folding operation reduces the channel's flexibility to distribute errors, which in turn leads to a reduction in the number of error patterns that need to be corrected.\n\nWe can relate Folded Reed Solomon codes with Parvaresh Vardy codes which encodes a polynomial formula_55 of degree formula_40 with polynomials formula_57 where formula_58 where formula_59 is an irreducible polynomial. While choosing irreducible polynomial formula_60 and parameter formula_61 we should check if every polynomial formula_55 of degree at most formula_63 satisfies formula_64 since formula_65 is just the shifted counterpart of formula_66 where formula_67 is the primitive element in formula_68 Thus folded RS code with bundling together code symbols is PV code of order formula_69 for the set of evaluation points\n\nIf we compare the folded RS code to a PV code of order 2 for the set of evaluation points\n\nwe can see that in PV encoding of formula_55, for every formula_73 and every formula_74 appears at formula_75 and formula_76,\n\nunlike in the folded FRS encoding in which it appears only once. Thus, the PV and folded RS codes have same information but only the rate of FRS is bigger by a factor of formula_77 and hence the list decoding radius trade-off is better for folded RS code by just using the list decodability of the PV codes. The plus point is in choosing FRS code in a way that they are compressed forms of suitable PV code with similar error correction performance with better rate than corresponding PV code. One can use this idea to construct a folded RS codes of rate formula_78 that are list decodable up to radius approximately formula_79 for formula_80. \nA list decoding algorithm which runs in quadratic time to decode FRS code up to radius formula_81 is presented by Guruswami. The algorithm essentially has three steps namely the interpolation step in which welch berlekamp style interpolation is used to interpolate the non-zero polynomial\n\nafter which all the polynomials formula_83 with degree formula_84 satisfying the equation derived in interpolation are found. In the third step the actual list of close-by codewords are known by pruning the solution subspace which takes formula_85time.\n\nGuruswami presents a formula_86 time list decoding algorithm based on linear-algebra, which can decode folded Reed–Solomon code up to radius formula_81 with a list-size of formula_88. There are three steps in this algorithm: Interpolation Step, Root Finding Step and Prune Step. In the Interpolation step it will try to find the candidate message polynomial formula_89 by solving a linear system. In the Root Finding step, it will try to find the solution subspace by solving another linear system. The last step will try to prune the solution subspace gained in the second step. We will introduce each step in details in the following.\n\nIt is a Welch–Berlekamp-style interpolation (because it can be viewed as the higher-dimensional generalization of the Welch–Berlekamp algorithm). Suppose we received a codeword formula_90 of the formula_21-folded Reed–Solomon code as shown below\n\nWe interpolate the nonzero polynomial\n\nby using a carefully chosen degree parameter formula_94.\n\nSo the interpolation requirements will be\n\nThen the number of monomials in formula_97 is\n\nBecause the number of monomials in formula_97 is greater than the number of interpolation conditions. We have below lemma\n\nThis lemma shows us that the interpolation step can be done in near-linear time.\n\nFor now, we have talked about everything we need for the multivariate polynomial formula_97. The remaining task is to focus on the message polynomials formula_66.\n\nHere \"agree\" means that all the formula_21 values in a column should match the corresponding values in codeword formula_90.\n\nThis lemma shows us that any such polynomial formula_97 presents an algebraic condition that must be satisfied for those message polynomials formula_89 that we are interested in list decoding.\n\nCombining Lemma 2 and parameter formula_94, we have\n\nFurther we can get the decoding bound\n\nWe notice that the fractional agreement is\n\nDuring this step, our task focus on how to find all polynomials formula_121 with degree no more than formula_84 and satisfy the equation we get from Step 1, namely\n\nSince the above equation forms a linear system equations over formula_101 in the coefficients formula_125 of the polynomial\n\nthe solutions to the above equation is an affine subspace of formula_127. This fact is the key point that gives rise to an efficient algorithm - we can solve the linear system.\n\nIt is natural to ask how large is the dimension of the solution? Is there any upper bound on the dimension? Having an upper bound is very important in constructing an efficient list decoding algorithm because one can simply output all the codewords for any given decoding problem.\n\nActually it indeed has an upper bound as below lemma argues.\n\nThis lemma shows us the upper bound of the dimension for the solution space.\n\nFinally, based on the above analysis, we have below theorem\n\nWhen formula_144, we notice that this reduces to a unique decoding algorithm with up to a fraction formula_145 of errors. In other words, we can treat unique decoding algorithm as a specialty of list decoding algorithm. The quantity is about formula_146 for the parameter choices that achieve a list decoding radius of formula_81.\n\nTheorem 1 tells us exactly how large the error radius would be.\n\nNow we finally get the solution subspace. However, there is still one problem standing. The list size in the worst case is formula_148. But the actual list of close-by codewords is only a small set within that subspace. So we need some process to prune the subspace to narrow it down. This prune process takes formula_85 time in the worst case. Unfortunately it is not known how to improve the running time because we do not know how to improve the bound of the list size for folded Reed-Solomon code.\n\nThings get better if we change the code by carefully choosing a subset of all possible degree formula_150 polynomials as messages, the list size shows to be much smaller while only losing a little bit in the rate. We will talk about this briefly in next step.\n\nBy converting the problem of decoding a folded Reed–Solomon code into two linear systems, one linear system that is used for the interpolation step and another linear system to find the candidate solution subspace, the complexity of the decoding problem is successfully reduced to quadratic. However, in the worst case, the bound of list size of the output is pretty bad.\n\nIt was mentioned in Step 2 that if one carefully chooses only a subset of all possible degree formula_150 polynomials as messages, the list size can be much reduced. Here we will expand our discussion.\n\nTo achieve this goal, the idea is to limit the coefficient vector formula_152 to a special subset formula_153, which satisfies below two conditions:\n\nThis is to make sure that the rate will be at most reduced by factor of formula_156.\n\nThe bound for the list size at worst case is formula_148, and it can be reduced to a relative small bound formula_163 by using subspace-evasive subsets.\n\nDuring this step, as it has to check each element of the solution subspace that we get from Step 2, it takes formula_85 time in the worst case (formula_159 is the dimension of the solution subspace).\n\nDvir and Lovett improved the result based on the work of Guruswami, which can reduce the list size to a constant.\n\nHere is only presented the idea that is used to prune the solution subspace. For the details of the prune process, please refer to papers by Guruswami, Dvir and Lovett, which are listed in the reference.\n\nIf we don't consider the Step 3, this algorithm can run in quadratic time. A summary for this algorithm is listed below.\n\n\n"}
{"id": "26225243", "url": "https://en.wikipedia.org/wiki?curid=26225243", "title": "Fred S. Roberts", "text": "Fred S. Roberts\n\nFred Stephen Roberts (born June 19, 1943) is an American mathematician, a professor of mathematics at Rutgers University, and a former director of DIMACS.\n\nRoberts did his undergraduate studies at Dartmouth College, and received his Ph.D. from Stanford University in 1968; his doctoral advisor was Dana Scott. After holding positions at the University of Pennsylvania, RAND, and the Institute for Advanced Study, he joined the Rutgers faculty in 1972.\n\nHe has been vice president of the Society for Industrial and Applied Mathematics twice, in 1984 and 1986, and has been director of DIMACS since 1996.\n\nRoberts' research concerns graph theory and combinatorics, and their applications in modeling problems in the social sciences and biology. Among his contributions to pure mathematics, he is known for introducing the concept of boxicity, the minimum dimension needed to represent a given undirected graph as an intersection graph of axis-parallel boxes.\n\nRoberts is the author or co-author of the following books:\n\nHe is also the editor of nearly 20 edited volumes.\n\nRoberts received the ACM SIGACT Distinguished Service Prize in 1999. In 2001, he won the National Science Foundation Science and Technology Centers Pioneer Award for \"pioneering the science and technology center concept\". In 2003, DIMACS held a Conference on Applications of Discrete Mathematics and Theoretical Computer Science, in honor of Roberts' 60th birthday. In 2012 he became a fellow of the American Mathematical Society.\n\n"}
{"id": "38847178", "url": "https://en.wikipedia.org/wiki?curid=38847178", "title": "Free motion equation", "text": "Free motion equation\n\nA free motion equation is a differential equation that describes a mechanical system in the absence of external forces, but in the presence only of an inertial force depending on the choice of a reference frame. \nIn non-autonomous mechanics on a configuration space formula_1, a free motion equation is defined as a second order non-autonomous dynamic equation on formula_1 which is brought into the form \n\nwith respect to some reference frame formula_4 on formula_1. Given an arbitrary reference frame formula_6 on formula_1, a free motion equation reads\n\nwhere formula_9 is a connection on formula_1 associates with the initial reference frame formula_4. The right-hand side of this equation is treated as an inertial force. \n\nA free motion equation need not exist in general. It can be defined if and only if a configuration bundle \nformula_12 of a mechanical system is a toroidal cylinder formula_13.\n\n\n"}
{"id": "51333951", "url": "https://en.wikipedia.org/wiki?curid=51333951", "title": "Garden of Archimedes", "text": "Garden of Archimedes\n\nThe Garden of Archimedes (Italian: \"Il Giardino Di Archimede\") is a museum for mathematics in Florence, Italy. It was founded on March 26, 2004 and opened its doors to the public on April 14 of that year. The mission of the museum is to enhance public understanding and perception of mathematics, to bring mathematics out of the shadows and into the limelight. It has been compared to the National Museum of Mathematics in New York City, the only museum in North America devoted to mathematics.\n\nThe Garden of Archimedes was set up in 2004 by a consortium of government and educational agencies. Current members of the consortium include: the Scuola Normale Superiore di Pisa, the University of Florence, the University of Pisa, the University of Siena, the Italian Mathematical Union, the Istituto Nazionale di Alta Matematica Francesco Severi, the Consortium for the Promotion of Culture of Research and Studies at the University of Avellino. The Consortium is based in Florence, at the Department of Mathematics \"Ulisse Dini\". The President of the consortium is the mathematician Enrico Giusti.\n\nOne initiative of the Garden of Archimedes is to create a history of Mathematics on CD-ROM and distribute it along with related supporting texts.\n\nInspired by museums such at The Exploratorium in San Francisco the Garden of Archimedes is packed with hands-on exhibits and is popular with both adults and children. The museum is divided into different sections or exhibitions, corresponding to different ways of discovering mathematics:\n\nBeyond compasses: the geometry of curves explores the mathematics concealed in everyday objects. Pythagoras and his theorem focuses on puzzles and play inspired by the seminal theorem. A bridge over the Mediterranean is a historical exhibition focusing on Leonardo Fibonacci and his Liber Abaci with emphasis on how mathematics from the Islamic world was reintroduced to Medieval Europe. Helping Nature: from Galileo's Mechanics to everyday life is an interactive exhibit showing how Galileo used mathematics to reveal the working of simple machines. Weapons of mass education features mathematically based games and puzzles. Other historical sections include A short history of calculus, A short history of trigonometry, Ancient mathematics through stamps, and Pink Numbers – Women and Mathematics.\n\n"}
{"id": "47769616", "url": "https://en.wikipedia.org/wiki?curid=47769616", "title": "Gifted (film)", "text": "Gifted (film)\n\nGifted is a 2017 American drama film directed by Marc Webb and written by Tom Flynn. It stars Chris Evans, Mckenna Grace, Lindsay Duncan, Jenny Slate and Octavia Spencer. The plot follows an intellectually gifted 7-year-old who becomes the subject of a custody battle between her uncle and grandmother. The film was released on April 7, 2017, by Fox Searchlight Pictures, and grossed $43 million worldwide.\n\nIn a small town near Tampa, Florida, seven-year-old Mary Adler lives with uncle and \"de facto\" guardian, Frank. Her best friend is her 40-ish neighbor, Roberta. On her first day of first grade, she shows remarkable mathematical talent, which impresses her teacher, Bonnie Stevenson. There, despite her initial disdain for average children her own age and her boredom with their classwork, she begins to bond with them when she brings her one-eyed cat, Fred, for show-and-tell and later defends a classmate from a bully on the bus. Mary is offered a scholarship to a private school for gifted children. However, Frank turns it down. Based on his family's experiences with similar schools, he fears Mary will not have a chance at a \"normal\" childhood.\n\nIt emerges that Mary's mother, Diane, had been a promising mathematician, dedicated to the Navier–Stokes problem (one of the unsolved Millennium Prize Problems) before taking her own life when Mary was six months old. Mary has lived with Frank, a former college professor turned boat repairman, ever since.\n\nFrank's estranged mother and Mary's maternal grandmother, Evelyn, seeks to gain custody of Mary and move her to Massachusetts, believing that Mary is a \"one-in-a-billion\" mathematical prodigy who should be specially tutored in preparation for a life devoted to mathematics, much as Diane was. However, Frank is adamant that his sister would want Mary to be in a normal public school and have the childhood she didn't have. Worried that the judge will rule against him and he will lose Mary completely, Frank accepts a compromise brokered by his lawyer Greg Cullen that sees Mary placed in foster care and attend the private school where Evelyn wants to have her enrolled. The foster parents live just 25 minutes from Frank's home, Frank will be entitled to scheduled visits, and Mary will be able to decide where she wants to live after her 12th birthday.\n\nMary is devastated at being placed in foster care, and her foster father says she refuses to see Frank. When Bonnie sees a picture of Fred up for adoption, she alerts Frank. Frank retrieves the cat from the pound and, learning that Fred was brought in due to allergy issues, realizes that Evelyn—who is allergic to cats—is overseeing Mary's education in the guest house of Mary's foster home. He then reveals to Evelyn—who had been a mathematician herself—that Diane \"had\" solved the Navier–Stokes problem, but stipulated that the solution was to be withheld until Evelyn's death. Knowing that it meant everything to Evelyn to see Diane solve the problem, Frank offers Evelyn the opportunity to publish Diane's work if she drops her objection to his having custody of Mary. Evelyn reluctantly agrees.\n\nThe film ends with Mary back in the custody of Frank, returning to public school while taking college-level courses in the mornings.\n\nIn August 2015, it was announced Chris Evans had been cast in the film, with Marc Webb directing from a screenplay by Tom Flynn. In September 2015, Mckenna Grace, Octavia Spencer, Lindsay Duncan and Jenny Slate joined the cast, and in November 2015, Julie Ann Emery was also added.\n\nFilming began in October 2015 in Savannah, Georgia, as well as in Tybee Island, Georgia. and finished Nov 20, 2015.\n\nThe film was scheduled to be released on April 12, 2017, but was pushed up to April 7, 2017.\n\n\"Gifted\" grossed $24.8 million in the United States and Canada and $17.8 million in other territories for a worldwide total of $42.6 million, against a production budget of $7 million.\n\nThe film went wide on Wednesday, April 12, 2017, and in its opening weekend grossed $3.1 million, finishing 6th at the box office. In its second weekend of wide expansion, it added more screens, and made $4.6 million, an increase of 47.5% from the previous week.\n\nOn review aggregation website Rotten Tomatoes, \"Gifted\" has an approval rating of 73% based on 163 reviews, with an average rating of 6.4/10. The site's critical consensus reads, \"\"Gifted\" isn't quite as bright as its pint-sized protagonist, but a charming cast wrings respectably engaging drama out of a fairly predictable premise.\" On Metacritic, the film has a weighted average score of 60 out of 100, based on 33 critics, indicating \"mixed or average reviews\". Audiences polled by CinemaScore gave the film an average grade of \"A\" on an A+ to F scale.\n\nColin Covert of the \"Star Tribune\" gave the film 3/4 stars, saying, \"Sure, it's a simple, straightforward film, but sometimes that's all you need as long as its heart is true.\" Richard Roeper gave the film 4 out of 4 stars and said, \"\"Gifted\" isn't the best or most sophisticated or most original film of the year so far – but it just might be my favorite.\"\n\n\n"}
{"id": "18548164", "url": "https://en.wikipedia.org/wiki?curid=18548164", "title": "Government Operational Research Service", "text": "Government Operational Research Service\n\nIn the United Kingdom, the Government Operational Research Service (GORS) supports and champions Operational Research across government. GORS currently supports policy-making, strategy and operations in many different departments and agencies across the United Kingdom and employs around 500 analysts, ranging from sandwich students to members of the Senior Civil Service.\n"}
{"id": "39516424", "url": "https://en.wikipedia.org/wiki?curid=39516424", "title": "Grey box model", "text": "Grey box model\n\nIn mathematics, statistics, and computational modelling, a grey box model combines a partial theoretical structure with data to complete the model. The theoretical structure may vary from information on the smoothness of results, to models that need only parameter values from data or existing literature. Thus, almost all models are grey box models as opposed to black box where no model form is assumed or white box models that are purely theoretical. Some models assume a special form such as a linear regression or neural network. These have special analysis methods. In particular linear regression techniques are much more efficient than most non-linear techniques. The model can be deterministic or stochastic (i.e. containing random components) depending on its planned use.\n\nThe general case is a non-linear model with a partial theoretical structure and some unknown parts derived from data. Models with unlike theoretical structures need to be evaluated individually, possibly using simulated annealing or genetic algorithms.\n\nWithin a particular model structure, parameters or variable parameter relations may need to be found. For a particular structure it is arbitrarily assumed that the data consists of sets of feed vectors f, product vectors p, and operating condition vectors c. Typically c will contain values extracted from f, as well as other values. In many cases a model can be converted to a function of the form:\nwhere the vector function m gives the errors between the data p, and the model predictions. The vector q gives some variable parameters that are the model's unknown parts.\n\nThe parameters q vary with the operating conditions c in a manner to be determined. This relation can be specified as q = Ac where A is a matrix of unknown coefficients, and c as in linear regression includes a constant term and possibly transformed values of the original operating conditions to obtain non-linear relations between the original operating conditions and q. It is then a matter of selecting which terms in A are non-zero and assigning their values. The model completion becomes an optimisation problem to determine the non-zero values in A that minimizes the error terms m(f,p,Ac) over the data.\n\nOnce a selection of non-zero values is made, the remaining coefficients in A can be determined by minimizing \"m\"(\"f\",\"p\",\"Ac\") over the data with respect to the nonzero values in A, typically by non-linear least squares. Selection of the nonzero terms can be done by optimization methods such as simulated annealing and evolutionary algorithms. Also the non-linear least squares can provide accuracy estimates for the elements of A that can be used to determine if they are significantly different from zero, thus providing a method of term selection.\n\nIt is sometimes possible to calculate values of q for each data set, directly or by non-linear least squares. Then the more efficient linear regression can be used to predict q using c thus selecting the non-zero values in A and estimating their values. Once the non-zero values are located non-linear least squares can be used on the original model m(f,p,Ac) to refine these values .\n\nA third method is model inversion, which converts the non-linear m(f,p,Ac) into an approximate linear form in the elements of A, that can be examined using efficient term selection and evaluation of the linear regression. For the simple case of a single q value (q = ac) and an estimate q* of q. Putting dq = ac − q* gives\n\nso that a is now in a linear position with all other terms known, and thus can be analyzed by linear regression techniques. For more than one parameter the method extends in a direct manner. After checking that the model has been improved this process can be repeated until convergence. This approach has the advantages that it does not need the parameters q to be able to be determined from an individual data set and the linear regression is on the original error terms\n\nWhere sufficient data is available, division of the data into a separate model construction set and one or two evaluation sets is recommended. This can be repeated using multiple selections of the construction set and the resulting models averaged or used to evaluate prediction differences.\n\nA statistical test such as chi-squared on the residuals is not particularly useful. The chi squared test requires known standard deviations which are seldom available, and failed tests give no indication of how to improve the model\n\nAn attempt to predict the residuals m(, ) with the operating conditions c using linear regression will show if the residuals can be predicted. Residuals that cannot be predicted offer little prospect of improving the model using the current operating conditions. Terms that do predict the residuals are prospective terms to incorporate into the model to improve its performance.\n\nThe model inversion technique above can be used as a method of determining whether a model can be improved. In this case selection of nonzero terms is not so important and linear prediction can be done using the significant eigenvectors of the regression matrix. The values in A determined in this manner need to be substituted into the nonlinear model to assess improvements in the model errors. The absence of a significant improvement indicates the available data is not able to improve the current model form using the defined parameters. Extra parameters can be inserted into the model to make this test more comprehensive.\n"}
{"id": "19749717", "url": "https://en.wikipedia.org/wiki?curid=19749717", "title": "Jeep problem", "text": "Jeep problem\n\nThe jeep problem, desert crossing problem or exploration problem is a mathematics problem in which a jeep must maximise the distance it can travel into a desert with a given quantity of fuel. The jeep can only carry a fixed and limited amount of fuel, but it can leave fuel and collect fuel at fuel dumps anywhere in the desert.\n\nThe problem was solved by N. J. Fine in 1947.\n\nThere are \"n\" units of fuel stored at a fixed base. The jeep can carry at most 1 unit of fuel at any time, and can travel 1 unit of distance on 1 unit of fuel (the jeep's fuel consumption is assumed to be constant). At any point in a trip the jeep may leave any amount of fuel that it is carrying at a fuel dump, or may collect any amount of fuel that was left at a fuel dump on a previous trip, as long as its fuel load never exceeds 1 unit. There are two variants of the problem:\n\n\nIn either case the objective is to maximise the distance travelled by the jeep on its final trip. Alternatively, the objective may be to find the least amount of fuel required to produce a final trip of a given distance.\n\nIn the classic problem the fuel in the jeep and at fuel dumps is treated as a continuous quantity. More complex variations on the problem have been proposed in which the fuel can only be left or collected in discrete amounts.\n\nA strategy that maximises the distance travelled on the final trip for the \"exploring the desert\" variant is as follows:\n\n\nWhen the jeep starts its final trip, there are \"n\" − 1 fuel dumps. The farthest contains 1/2 of a unit of fuel, the next farthest contain 1/3 of a unit of fuel, and so on, and the nearest fuel dump has just 1/\"n\" units of fuel left. Together with 1 unit of fuel with which it starts from base, this means that the jeep can travel a total round trip distance of\n\nunits on its final trip (the maximum distance travelled into the desert is half of this). It collects half of the remaining fuel at each dump on the way out, which fills its tank. After leaving the farthest fuel dump it travels 1/2 a unit further into the desert and then returns to the farthest fuel dump. It collects the remaining fuel from each fuel dump on the way back, which is just enough to reach the next fuel dump or, in the final step, to return to base.\nThe distance travelled on the last trip is the \"n\"th harmonic number, \"H\". As the harmonic numbers are unbounded, it is possible to exceed any given distance on the final trip, as along as sufficient fuel is available at the base. However, the amount of fuel required and the number of fuel dumps both increase exponentially with the distance to be travelled.\n\nThe \"crossing the desert\" variant can be solved with a similar strategy, except that there is now no requirement to collect fuel on the way back on the final trip. So on trip \"k\" the jeep establishes a new \"k\"th fuel dump at a distance of 1/(2\"n\" − 2\"k\" + 1) units from the previous fuel dump and leaves (2\"n\" − 2\"k\" − 1)/(2\"n\" − 2\"k\" + 1) units of fuel there. On each of the next \"n\" − \"k\" − 1 trips it collects 1/(2\"n\" − 2\"k\" + 1) units of fuel from the \"k\"th dump on its way out and another 1/(2\"n\" − 2\"k\" + 1) units of fuel on its way back.\n\nNow when the jeep starts its final trip, there are \"n\" − 1 fuel dumps. The farthest contains 1/3 of a unit of fuel, the next farthest contain 1/5 of a unit of fuel, and so on, and the nearest fuel dump has just 1/(2\"n\" − 1) units of fuel left. Together with 1 unit of fuel with which it starts from base, this means that the jeep can travel a total distance of\n\nunits on its final trip. It collects all of the remaining fuel at each dump on the way out, which fills its tank. After leaving the farthest fuel dump it travels a further distance of 1 unit.\n\nNote that\n\nso it is possible in theory to cross a desert of any size given enough fuel at the base. As before, the amount of fuel required and the number of fuel dumps both increase exponentially with the distance to be travelled.\n\nThe problem can have a practical application in wartime situations, especially with respect to fuel efficiency. In the context of the bombing of Japan in World War II by B-29s, Robert McNamara says in the film \"The Fog of War\" that understanding the fuel efficiency issue caused by having to transport the fuel to forward bases was the main reason why the strategy of launching bombing raids from mainland China was abandoned in favor of the island hopping strategy:\n\nSee also Operation Black Buck for an application of these ideas. In these missions, conducted during the Falklands War, the Royal Air Force used air to air refuelling by staging tankers to enable the Vulcan bombers based on Ascension Island to bomb targets in the Falkland Islands.\n\n"}
{"id": "18557138", "url": "https://en.wikipedia.org/wiki?curid=18557138", "title": "Justesen code", "text": "Justesen code\n\nIn coding theory, Justesen codes form a class of error-correcting codes that have a constant rate, constant relative distance, and a constant alphabet size.\n\nBefore the Justesen error correction code was discovered, no error correction code was known that had all of these three parameters as a constant.\n\nSubsequently, other ECC codes with this property have been discovered, for example expander codes.\nThese codes have important applications in computer science such as in the construction of small-bias sample spaces.\n\nJustesen codes are derived as the code concatenation of a Reed–Solomon code and the Wozencraft ensemble.\n\nThe Reed–Solomon codes used achieve constant rate and constant relative distance at the expense of an alphabet size that is \"linear\" in the message length.\n\nThe Wozencraft ensemble is a family of codes that achieve constant rate and constant alphabet size, but the relative distance is only constant for most of the codes in the family.\n\nThe concatenation of the two codes first encodes the message using the Reed–Solomon code, and then encodes each symbol of the codeword further using a code from the Wozencraft ensemble – using a different code of the ensemble at each position of the codeword.\n\nThis is different from usual code concatenation where the inner codes are the same for each position. The Justesen code can be constructed very efficiently using only logarithmic space.\n\nJustesen code is concatenation code with different linear inner codes, which is composed of an formula_1 outer code formula_2 and different formula_3 inner codes formula_4, formula_5. \n\nMore precisely, the concatenation of these codes, denoted by formula_6, is defined as follows. Given a message formula_7, we compute the codeword produced by an outer code formula_2: formula_9. \n\nThen we apply each code of N linear inner codes to each coordinate of that codeword to produce the final codeword; that is, formula_10. \n\nLook back to the definition of the outer code and linear inner codes, this definition of the Justesen code makes sense because the codeword of the outer code is a vector with formula_11 elements, and we have formula_11 linear inner codes to apply for those formula_11 elements.\n\nHere for the Justesen code, the outer code formula_2 is chosen to be Reed Solomon code over a field formula_15 evaluated over formula_16 of rate formula_17, formula_18 < formula_17 < formula_20. \n\nThe outer code formula_2 have the relative distance formula_22 and block length of formula_23. The set of inner codes is the Wozencraft ensemble formula_24.\n\nAs the linear codes in the Wonzencraft ensemble have the rate formula_25, Justesen code is the concatenated code formula_26 with the rate formula_27. We have the following theorem that estimates the distance of the concatenated code formula_28.\n\nLet formula_29 Then formula_28 has relative distance of at least formula_31\n\nIn order to prove a lower bound for the distance of a code formula_28 we prove that the Hamming distance of an arbitrary but distinct pair of codewords has a lower bound. So let formula_33 be the Hamming distance of two codewords formula_34 and formula_35. For any given\n\nwe want a lower bound for formula_37\n\nNotice that if formula_38, then formula_39. So for the lower bound formula_40, we need to take into account the distance of formula_41\n\nSuppose\n\nRecall that formula_43 is a Wozencraft ensemble. Due to \"Wonzencraft ensemble theorem\", there are at least formula_44 linear codes formula_4 that have distance formula_46 So if for some formula_47 and the code formula_4 has distance formula_49 then\n\nFurther, if we have formula_51 numbers formula_52 such that formula_53 and the code formula_4 has distance formula_55 then\n\nSo now the final task is to find a lower bound for formula_51. Define:\n\nThen formula_51 is the number of linear codes formula_60 having the distance formula_61\n\nNow we want to estimate formula_62 Obviously formula_63.\n\nDue to the Wozencraft Ensemble Theorem, there are at most formula_64 linear codes having distance less than formula_65 so\n\nFinally, we have\n\nThis is true for any arbitrary formula_68. So formula_28 has the relative distance at least formula_70 which completes the proof.\n\nWe want to consider the \"strongly explicit code\". So the question is what the \"strongly explicit code\" is. Loosely speaking, for linear code, the \"explicit\" property is related to the complexity of constructing its generator matrix G. \n\nThat in effect means that we can compute the matrix in logarithmic space without using the brute force algorithm to verify that a code has a given satisfied distance.\n\nFor the other codes that are not linear, we can consider the complexity of the encoding algorithm.\n\nSo by far, we can see that the Wonzencraft ensemble and Reed-Solomon codes are strongly explicit. Therefore, we have the following result:\n\nCorollary: The concatenated code formula_28 is an asymptotically good code(that is, rate formula_17 > 0 and relative distance formula_73 > 0 for small q) and has a strongly explicit construction.\n\nThe following slightly different code is referred to as the Justesen code in MacWilliams/MacWilliams. It is the particular case of the above-considered \nJustesen code for a very particular Wonzencraft ensemble:\n\nLet \"R\" be a Reed-Solomon code of length \"N\" = 2 − 1, rank \"K\" and minimum weight \"N\" − \"K\" + 1. \n\nThe symbols of \"R\" are elements of \"F\" = GF(2) and the codewords are obtained by taking every polynomial ƒ over \"F\" of degree less than \"K\" and listing the values of ƒ on the non-zero elements of \"F\" in some predetermined order. \n\nLet α be a primitive element of \"F\". For a codeword a = (\"a\", ..., \"a\") from \"R\", let b be the vector of length 2\"N\" over \"F\" given by\n\nand let c be the vector of length 2\"N\" \"m\" obtained from \"b\" by expressing each element of \"F\" as a binary vector of length \"m\". The \"Justesen code\" is the linear code containing all such c.\n\nThe parameters of this code are length 2\"m\" \"N\", dimension \"m\" \"K\" and minimum distance at least\n\nwhere formula_76 is the greatest integer satisfying formula_77. (See MacWilliams/MacWilliams for a proof.)\n\n\n"}
{"id": "1068844", "url": "https://en.wikipedia.org/wiki?curid=1068844", "title": "KSD-64", "text": "KSD-64\n\nThe KSD-64[A] Crypto Ignition Key (CIK) is an NSA-developed EEPROM chip packed in a plastic case that looks like a toy key. The model number is due to its storage capacity — 64 kibibits (65,536bits, or 8KiB), enough to store multiple encryption keys. Most frequently it was used in key-splitting applications: either the encryption device or the KSD-64 alone is worthless, but together they can be used to make encrypted connections. It was also used alone as a fill device for transfer of key material, as for the initial seed key loading of an STU-III secure phone.\n\nNewer systems, such as the Secure Terminal Equipment, use the Fortezza PC card as a security token instead of the KSD-64. The KSD-64 was withdrawn from the market in 2014. Over one million were produced in its 30-year life.\n\nThe CIK is a small device which can be loaded with a 128·bit sequence which is different for each user. When the device is removed from the machine, that sequence is automatically added (mod 2) to the unique key in the machine, thus leaving it stored in encrypted form. When it is reattached, the unique key in the machine is decrypted, and it is now ready to operate in the normal way. The analogy with an automobile ignition key is close, thus the name. If the key is lost, the user is still safe unless the finder or thief can match it with the user's machine. In case of loss, the user gets a new CIK, effectively changing the lock in the cipher machine, and gets back in business.\n\nThe ignition key sequence can be provided in several ways. In the first crypto-equipment to use the idea (the KY-70), the CIK is loaded with its sequence at NSA and supplied to each user like any other item of keying material. Follow-on application (as in the STU-II) use an even more clever scheme. The CIK device is simply an empty register which can be supplied with its unique sequence from the randomizer function of the parent machine itself. Not only that, each time the device is removed and re-inserted, it gets a brand new sequence. The effect of this procedure is to provide high protection against the covert compromise of the CIK wherein a thief acquires the device, copies it, and replaces it unknown to its owner. The next morning (say), when the user inserts the device, it will receive a new sequence and the old copied one will be useless thereafter. If the thief has gotten to his machine during the night, he may be able to act into the net; but when the user attempts to start up in the morning the thief's device will no longer work, thus flagging the fact that penetration has occurred.\n\nThis concept appears particularly attractive in office environments where physical structures and guarding arrangements will not be sufficiently rigorous to assure that crypto-equipments cannot be accessed by unauthorized people.\n\n"}
{"id": "11085278", "url": "https://en.wikipedia.org/wiki?curid=11085278", "title": "Langlands decomposition", "text": "Langlands decomposition\n\nIn mathematics, the Langlands decomposition writes a parabolic subgroup \"P\" of a semisimple Lie group as a product formula_1 of a reductive subgroup \"M\", an abelian subgroup \"A\", and a nilpotent subgroup \"N\".\n\nA key application is in parabolic induction, which leads to the Langlands program: if formula_2 is a reductive algebraic group and formula_1 is the Langlands decomposition of a parabolic subgroup \"P\", then parabolic induction consists of taking a representation of formula_4, extending it to formula_5 by letting formula_6 act trivially, and inducing the result from formula_5 to formula_2.\n\n\n"}
{"id": "40708124", "url": "https://en.wikipedia.org/wiki?curid=40708124", "title": "Legendre's three-square theorem", "text": "Legendre's three-square theorem\n\nIn mathematics, Legendre's three-square theorem states that a natural number can be represented as the sum of three squares of integers\n\nif and only if is not of the form formula_2 for integers and .\n\nThe first numbers that cannot be expressed as the sum of three squares (i.e. numbers that can be expressed as formula_2) are\n\nPierre de Fermat gave a criterion for numbers of the form 3\"a\" + 1 to be a sum of three squares essentially equivalent to Legendre's theorem, but did not provide a proof.\nN. Beguelin noticed in 1774 that every positive integer which is neither of the form 8\"n\" + 7, nor of the form 4\"n\", is the sum of three squares, but did not provide a satisfactory proof. In 1796 Gauss proved his Eureka theorem that every positive integer \"n\" is the sum of 3 triangular numbers; this is equivalent to the fact that 8\"n\" + 3 is a sum of three squares. In 1797 or 1798 A.-M. Legendre obtained the first proof of his 3 square theorem. In 1813, A. L. Cauchy noted that Legendre's theorem is equivalent to the statement in the introduction above. Previously, in 1801, C. F. Gauss had obtained a more general result, containing Legendre theorem of 1797–8 as a corollary. In particular, Gauss counted the number of solutions of the expression of an integer as a sum of three squares, and this is a generalisation of yet another result of Legendre, whose proof is incomplete. This last fact appears to be the reason for later incorrect claims according to which Legendre's proof of the three-square theorem was defective and had to be completed by Gauss.\n\nWith Lagrange's four-square theorem and the two-square theorem of Girard, Fermat and Euler, the Waring's problem for \"k\" = 2 is entirely solved.\n\nThe \"only if\" of the theorem is simply because modulo 8, every square is congruent to 0, 1 or 4. There are several proofs of the converse (besides Legendre's proof). One of them is due to J. P. G. L. Dirichlet in 1850, and has become classical. It requires three main lemmas:\n\nThis theorem can be used to prove Lagrange's four-square theorem, which states that all natural numbers can be written as a sum of four squares. Gauss pointed out that the four squares theorem follows easily from the fact that any positive integer that is 1 or 2 mod 4 is a sum of 3 squares, because any positive integer not divisible by 4 can be reduced to this form by subtracting 0 or 1 from it.\nHowever, proving the three-square theorem is considerably more difficult than a direct proof of the four-square theorem that does not use the three-square theorem. Indeed, the four-square theorem was proved earlier, in 1770.\n\n"}
{"id": "47930117", "url": "https://en.wikipedia.org/wiki?curid=47930117", "title": "Louise Duffield Cummings", "text": "Louise Duffield Cummings\n\nLouise Duffield Cummings (21 November 1870 – 9 May 1947) was a Canadian-born American mathematician.\n\nCummings received her B.A. in 1895 from the University of Toronto. She studied mathematics at the graduate level in 1895–1896 at the University of Toronto, in 1896–1897 at the University of Pennsylvania, in 1897–1898 at the University of Chicago, and in 1898–1900 at Bryn Mawr College. During 1900–1901 she taught at the Ontario Normal College and, while completing her A.M. at the University of Toronto, she taught at St. Margaret's College during 1901–1902.\n\nCummings joined the faculty of Vassar in 1902 as an instructor and was promoted to assistant professor in 1915, to associate professor in 1919, and to full professor in 1927 before her retirement in 1936. She was an invited speaker at the International Congress of Mathematicians in 1924 at Toronto and again in 1932 at Zürich.\n\n\n"}
{"id": "55201705", "url": "https://en.wikipedia.org/wiki?curid=55201705", "title": "Mathematical Methods of Classical Mechanics", "text": "Mathematical Methods of Classical Mechanics\n\nMathematical Methods of Classical Mechanics is a classic graduate textbook by Vladimir I. Arnold. It was originally written in Russian, but was translated into English by A. Weinstein and K. Vogtmann.\n\n\nThe Bulletin of the American Mathematical Society said, \"The [book] under review [...] written by a distinguished mathematician [...is one of] the first textbooks [to] successfully to present to students of mathematics and physics, [sic] classical mechanics in a modern setting.\"\n\nA book review in the journal \"Celestial Mechanics\" said, \"In summary, the author has succeeded in producing a mathematical synthesis of the science of dynamics. The book is well presented and beautifully translated [...] Arnold's book is pure poetry; one does not simply read it, one enjoys it.\"\n"}
{"id": "20590", "url": "https://en.wikipedia.org/wiki?curid=20590", "title": "Mathematical model", "text": "Mathematical model\n\nA mathematical model is a description of a system using mathematical concepts and language. The process of developing a mathematical model is termed mathematical modeling. Mathematical models are used in the natural sciences (such as physics, biology, earth science, chemistry) and engineering disciplines (such as computer science, electrical engineering), as well as in the social sciences (such as economics, psychology, sociology, political science). \n\nA model may help to explain a system and to study the effects of different components, and to make predictions about behaviour.\n\nMathematical models can take many forms, including dynamical systems, statistical models, differential equations, or game theoretic models. These and other types of models can overlap, with a given model involving a variety of abstract structures. In general, mathematical models may include logical models. In many cases, the quality of a scientific field depends on how well the mathematical models developed on the theoretical side agree with results of repeatable experiments. Lack of agreement between theoretical mathematical models and experimental measurements often leads to important advances as better theories are developed.\n\nIn the physical sciences, a traditional mathematical model contains most of the following elements:\n\nMathematical models are usually composed of relationships and \"variables\". Relationships can be described by \"operators\", such as algebraic operators, functions, differential operators, etc. Variables are abstractions of system parameters of interest, that can be quantified. Several classification criteria can be used for mathematical models according to their structure:\n\nMathematical models are of great importance in the natural sciences, particularly in physics. Physical theories are almost invariably expressed using mathematical models.\n\nThroughout history, more and more accurate mathematical models have been developed. Newton's laws accurately describe many everyday phenomena, but at certain limits relativity theory and quantum mechanics must be used; even these do not apply to all situations and need further refinement. It is possible to obtain the less accurate models in appropriate limits, for example relativistic mechanics reduces to Newtonian mechanics at speeds much less than the speed of light. Quantum mechanics reduces to classical physics when the quantum numbers are high. For example, the de Broglie wavelength of a tennis ball is insignificantly small, so classical physics is a good approximation to use in this case.\n\nIt is common to use idealized models in physics to simplify things. Massless ropes, point particles, ideal gases and the particle in a box are among the many simplified models used in physics. The laws of physics are represented with simple equations such as Newton's laws, Maxwell's equations and the Schrödinger equation. These laws are such as a basis for making mathematical models of real situations. Many real situations are very complex and thus modeled approximate on a computer, a model that is computationally feasible to compute is made from the basic laws or from approximate models made from the basic laws. For example, molecules can be modeled by molecular orbital models that are approximate solutions to the Schrödinger equation. In engineering, physics models are often made by mathematical methods such as finite element analysis.\n\nDifferent mathematical models use different geometries that are not necessarily accurate descriptions of the geometry of the universe. Euclidean geometry is much used in classical physics, while special relativity and general relativity are examples of theories that use geometries which are not Euclidean.\n\nSince prehistorical times simple models such as maps and diagrams have been used.\n\nOften when engineers analyze a system to be controlled or optimized, they use a mathematical model. In analysis, engineers can build a descriptive model of the system as a hypothesis of how the system could work, or try to estimate how an unforeseeable event could affect the system. Similarly, in control of a system, engineers can try out different control approaches in simulations.\n\nA mathematical model usually describes a system by a set of variables and a set of equations that establish relationships between the variables. Variables may be of many types; real or integer numbers, boolean values or strings, for example. The variables represent some properties of the system, for example, measured system outputs often in the form of signals, timing data, counters, and event occurrence (yes/no). The actual model is the set of functions that describe the relations between the different variables.\n\nIn business and engineering, mathematical models may be used to maximize a certain output. The system under consideration will require certain inputs. The system relating inputs to outputs depends on other variables too: decision variables, state variables, exogenous variables, and random variables.\n\nDecision variables are sometimes known as independent variables. Exogenous variables are sometimes known as parameters or constants.\nThe variables are not independent of each other as the state variables are dependent on the decision, input, random, and exogenous variables. Furthermore, the output variables are dependent on the state of the system (represented by the state variables).\n\nObjectives and constraints of the system and its users can be represented as functions of the output variables or state variables. The objective functions will depend on the perspective of the model's user. Depending on the context, an objective function is also known as an \"index of performance\", as it is some measure of interest to the user. Although there is no limit to the number of objective functions and constraints a model can have, using or optimizing the model becomes more involved (computationally) as the number increases.\n\nFor example, economists often apply linear algebra when using input-output models. Complicated mathematical models that have many variables may be consolidated by use of vectors where one symbol represents several variables.\n\nMathematical modeling problems are often classified into black box or white box models, according to how much a priori information on the system is available. A black-box model is a system of which there is no a priori information available. A white-box model (also called glass box or clear box) is a system where all necessary information is available. Practically all systems are somewhere between the black-box and white-box models, so this concept is useful only as an intuitive guide for deciding which approach to take.\n\nUsually it is preferable to use as much a priori information as possible to make the model more accurate. Therefore, the white-box models are usually considered easier, because if you have used the information correctly, then the model will behave correctly. Often the a priori information comes in forms of knowing the type of functions relating different variables. For example, if we make a model of how a medicine works in a human system, we know that usually the amount of medicine in the blood is an exponentially decaying function. But we are still left with several unknown parameters; how rapidly does the medicine amount decay, and what is the initial amount of medicine in blood? This example is therefore not a completely white-box model. These parameters have to be estimated through some means before one can use the model.\n\nIn black-box models one tries to estimate both the functional form of relations between variables and the numerical parameters in those functions. Using a priori information we could end up, for example, with a set of functions that probably could describe the system adequately. If there is no a priori information we would try to use functions as general as possible to cover all different models. An often used approach for black-box models are neural networks which usually do not make assumptions about incoming data. Alternatively the NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) algorithms which were developed as part of nonlinear system identification can be used to select the model terms, determine the model structure, and estimate the unknown parameters in the presence of correlated and nonlinear noise. The advantage of NARMAX models compared to neural networks is that NARMAX produces models that can be written down and related to the underlying process, whereas neural networks produce an approximation that is opaque.\n\nSometimes it is useful to incorporate subjective information into a mathematical model. This can be done based on intuition, experience, or expert opinion, or based on convenience of mathematical form. Bayesian statistics provides a theoretical framework for incorporating such subjectivity into a rigorous analysis: we specify a prior probability distribution (which can be subjective), and then update this distribution based on empirical data.\n\nAn example of when such approach would be necessary is a situation in which an experimenter bends a coin slightly and tosses it once, recording whether it comes up heads, and is then given the task of predicting the probability that the next flip comes up heads. After bending the coin, the true probability that the coin will come up heads is unknown; so the experimenter would need to make a decision (perhaps by looking at the shape of the coin) about what prior distribution to use. Incorporation of such subjective information might be important to get an accurate estimate of the probability.\n\nIn general, model complexity involves a trade-off between simplicity and accuracy of the model. Occam's razor is a principle particularly relevant to modeling, its essential idea being that among models with roughly equal predictive power, the simplest one is the most desirable. While added complexity usually improves the realism of a model, it can make the model difficult to understand and analyze, and can also pose computational problems, including numerical instability. Thomas Kuhn argues that as science progresses, explanations tend to become more complex before a paradigm shift offers radical simplification .\n\nFor example, when modeling the flight of an aircraft, we could embed each mechanical part of the aircraft into our model and would thus acquire an almost white-box model of the system. However, the computational cost of adding such a huge amount of detail would effectively inhibit the usage of such a model. Additionally, the uncertainty would increase due to an overly complex system, because each separate part induces some amount of variance into the model. It is therefore usually appropriate to make some approximations to reduce the model to a sensible size. Engineers often can accept some approximations in order to get a more robust and simple model. For example, Newton's classical mechanics is an approximated model of the real world. Still, Newton's model is quite sufficient for most ordinary-life situations, that is, as long as particle speeds are well below the speed of light, and we study macro-particles only.\n\nAny model which is not pure white-box contains some parameters that can be used to fit the model to the system it is intended to describe. If the modeling is done by an artificial neural network or other machine learning, the optimization of parameters is called \"training\", while the optimization of model hyperparameters is called \"tuning\" and often uses cross-validation. In more conventional modeling through explicitly given mathematical functions, parameters are often determined by \"curve fitting\".\n\nA crucial part of the modeling process is the evaluation of whether or not a given mathematical model describes a system accurately. This question can be difficult to answer as it involves several different types of evaluation.\n\nUsually the easiest part of model evaluation is checking whether a model fits experimental measurements or other empirical data. In models with parameters, a common approach to test this fit is to split the data into two disjoint subsets: training data and verification data. The training data are used to estimate the model parameters. An accurate model will closely match the verification data even though these data were not used to set the model's parameters. This practice is referred to as cross-validation in statistics.\n\nDefining a metric to measure distances between observed and predicted data is a useful tool of assessing model fit. In statistics, decision theory, and some economic models, a loss function plays a similar role.\n\nWhile it is rather straightforward to test the appropriateness of parameters, it can be more difficult to test the validity of the general mathematical form of a model. In general, more mathematical tools have been developed to test the fit of statistical models than models involving differential equations. Tools from non-parametric statistics can sometimes be used to evaluate how well the data fit a known distribution or to come up with a general model that makes only minimal assumptions about the model's mathematical form.\n\nAssessing the scope of a model, that is, determining what situations the model is applicable to, can be less straightforward. If the model was constructed based on a set of data, one must determine for which systems or situations the known data is a \"typical\" set of data.\n\nThe question of whether the model describes well the properties of the system between data points is called interpolation, and the same question for events or data points outside the observed data is called extrapolation.\n\nAs an example of the typical limitations of the scope of a model, in evaluating Newtonian classical mechanics, we can note that Newton made his measurements without advanced equipment, so he could not measure properties of particles travelling at speeds close to the speed of light. Likewise, he did not measure the movements of molecules and other small particles, but macro particles only. It is then not surprising that his model does not extrapolate well into these domains, even though his model is quite sufficient for ordinary life physics.\n\nMany types of modeling implicitly involve claims about causality. This is usually (but not always) true of models involving differential equations. As the purpose of modeling is to increase our understanding of the world, the validity of a model rests not only on its fit to empirical observations, but also on its ability to extrapolate to situations or data beyond those originally described in the model. One can think of this as the differentiation between qualitative and quantitative predictions. One can also argue that a model is worthless unless it provides some insight which goes beyond what is already known from direct investigation of the phenomenon being studied.\n\nAn example of such criticism is the argument that the mathematical models of optimal foraging theory do not offer insight that goes beyond the common-sense conclusions of evolution and other basic principles of ecology.\n\n\"M\" = (\"Q\", Σ, δ, \"q\", \"F\") where\n\nThe state \"S\" represents that there has been an even number of 0s in the input so far, while \"S\" signifies an odd number. A 1 in the input does not change the state of the automaton. When the input ends, the state will show whether the input contained an even number of 0s or not. If the input did contain an even number of 0s, \"M\" will finish in state \"S\", an accepting state, so the input string will be accepted.\n\nThe language recognized by \"M\" is the regular language given by the regular expression 1*( 0 (1*) 0 (1*) )*, where \"*\" is the Kleene star, e.g., 1* denotes any non-negative number (possibly zero) of symbols \"1\".\n\n\nthat can be written also as:\n\n\n\n\n\n\n\n"}
{"id": "37312015", "url": "https://en.wikipedia.org/wiki?curid=37312015", "title": "Nathan Altshiller Court", "text": "Nathan Altshiller Court\n\nNathan Altshiller Court (1881-1968) was a Polish-American mathematician, a geometer in particular and author of the famous book \"College Geometry - An Introduction to the Modern Geometry of the Triangle and the Circle\".\n\nNathan Court was born on 22 January 1881, in Warsaw, Poland. He attended the \"University of Uege\" and the University of Ghent in Belgium where he received his D.Sc in 1911. Soon after he came to the United States, he studied and taught at Columbia University. In 1912 he married \"Sophie Ravltch\", whom he had known in Warsaw. Dr. Court taught at the University of Washington and the University of Colorado before coming to the University of Oklahoma in 1918. In 1919, he became a U.S. citizen and changed his last name to Court, keeping Altshiller as a middle name. He became a full Professor at the University of Oklahoma in 1935 and retired in 1951. He died in Norman 20 July 1968.\n\n\n"}
{"id": "37711172", "url": "https://en.wikipedia.org/wiki?curid=37711172", "title": "Order of accuracy", "text": "Order of accuracy\n\nIn numerical analysis, order of accuracy quantifies the rate of convergence of a numerical approximation of a differential equation to the exact solution.\nConsider formula_1, the exact solution to a differential equation in an appropriate normed space formula_2. Consider a numerical approximation formula_3, where formula_4 is a parameter characterizing the approximation, such as the step size in a finite difference scheme or the diameter of the cells in a finite element method.\nThe numerical solution formula_3 is said to be formula_6th-order accurate if the error, formula_7 is proportional to the step-size formula_4 to the formula_6th power;\n\nWhere the constant formula_11 is independent of h and usually depends on the solution formula_1.. Using the big O notation an formula_6th-order accurate numerical method is notated as\n\nThis definition is strictly depended on the norm used in the space; the choice of such norm is fundamental to estimate the rate of convergence and, in general, all numerical errors correctly.\n\nThe size of the error of a first-order accurate approximation is directly proportional to formula_4.\nPartial differential equations which vary over both time and space are said to be accurate to order formula_6 in time and to order formula_17 in space.\n"}
{"id": "27972796", "url": "https://en.wikipedia.org/wiki?curid=27972796", "title": "Paul Erdős Prize", "text": "Paul Erdős Prize\n\nThe Paul Erdős Prize (formerly Mathematical Prize) is given to Hungarian mathematicians not older than 40 by the Mathematics Department of the Hungarian Academy of Sciences. It was established and originally funded by Paul Erdős.\n\nThe list on the homepage of the Hungarian academy\n"}
{"id": "639426", "url": "https://en.wikipedia.org/wiki?curid=639426", "title": "Philippa Fawcett", "text": "Philippa Fawcett\n\nPhilippa Garrett Fawcett (4 April 1868 – 10 June 1948) was a British mathematician and educationalist.\n\nPhilippa Garrett Fawcett was the daughter of the suffragist Millicent Fawcett and Henry Fawcett MP, Professor of Political Economy at Cambridge and Postmaster General in Gladstone's government. Her aunt was Elizabeth Garrett Anderson, the first English female doctor. When her father died, she and her mother went to live with Millicent's sister Agnes Garrett, who had set up an interior design business on Gower Street, Bloomsbury.\n\nPhilippa Fawcett was educated at Bedford College, London (now Royal Holloway) and Newnham College, Cambridge which had been co-founded by her mother. In 1890 she became the first woman to obtain the top score in the Cambridge Mathematical Tripos exams. The results were highly publicised, with the top scorers receiving great acclaim. Her score was 13 per cent higher than the second highest, but she did not receive the title of senior wrangler, as only men were then ranked and women were listed separately. Women had been allowed to take the Tripos since 1880, after Charlotte Angas Scott was unofficially ranked as eighth wrangler. When the women's list was announced Fawcett was described as \"above the senior wrangler\". No woman was officially awarded the first position until Ruth Hendry in 1992.\n\nComing amidst the women's suffrage movement, Fawcett's feat gathered worldwide media coverage, spurring much discussion about women's capacities and rights. The lead story in the \"Telegraph\" the following day said:\n\nFollowing Fawcett's achievement in the Tripos, she won the Marion Kennedy scholarship at Cambridge through which she conducted research in fluid dynamics. Her published papers include \"Note on the Motion of Solids in a Liquid\".\nShe was appointed a college lecturer in Mathematics at Newnham College, a position she held for 10 years. In this capacity, her teaching abilities received considerable praise. One student wrote:\n\nFawcett left Cambridge in 1902, when she was appointed as a lecturer to train mathematics teachers at the Normal School in Johannesburg, South Africa, now part of the University of Pretoria. She remained there, setting up schools in South Africa, until 1905, when she returned to Britain to take a position in the administration of education for London County Council. At the LCC, in her work developing secondary schools, she attained a high rank. Denied a Cambridge degree by her sex, she was one of the steamboat ladies who travelled to Ireland between 1904 and 1907 to receive an \"ad eundem\" University of Dublin degree at Trinity College.\n\nPhilippa Fawcett maintained strong links with Newnham College throughout her life. The Fawcett building (1938) was named in recognition of her contribution to the college, and that of her family. She died on 10 June 1948, two months after her 80th birthday, a month after the Grace that allowed women to be awarded the Cambridge BA degree received royal assent.\n\n\n\nPortrait of Phillipa Fawcett at William Morris Gallery \n"}
{"id": "1255458", "url": "https://en.wikipedia.org/wiki?curid=1255458", "title": "Pseudogroup", "text": "Pseudogroup\n\nIn mathematics, a pseudogroup is an extension of the group concept, but one that grew out of the geometric approach of Sophus Lie, rather than out of abstract algebra (such as quasigroup, for example). A theory of pseudogroups was developed by Élie Cartan in the early 1900s.\n\nIt is not an axiomatic algebraic idea; rather it defines a set of closure conditions on sets of homeomorphisms defined on open sets \"U\" of a given Euclidean space \"E\" or more generally of a fixed topological space \"S\". The groupoid condition on those is fulfilled, in that homeomorphisms\n\nand\n\ncompose to a \"homeomorphism\" from \"U\" to \"W\". The further requirement on a pseudogroup is related to the possibility of \"patching\" (in the sense of descent, transition functions, or a gluing axiom).\n\nSpecifically, a pseudogroup on a topological space \"S\" is a collection \"Γ\" of homeomorphisms between open subsets of \"S\" satisfying the following properties.\n\nAn example in space of two dimensions is the pseudogroup of invertible holomorphic functions of a complex variable (invertible in the sense of having an inverse function). The properties of this pseudogroup are what makes it possible to define Riemann surfaces by local data patched together.\n\nIn general, pseudogroups were studied as a possible theory of infinite-dimensional Lie groups. The concept of a local Lie group, namely a pseudogroup of functions defined in neighbourhoods of the origin of \"E\", is actually closer to Lie's original concept of Lie group, in the case where the transformations involved depend on a finite number of parameters, than the contemporary definition via manifolds. One of Cartan's achievements was to clarify the points involved, including the point that a local Lie group always gives rise to a \"global\" group, in the current sense (an analogue of Lie's third theorem, on Lie algebras determining a group). The formal group is yet another approach to the specification of Lie groups, infinitesimally. It is known, however, that \"local topological groups\" do not necessarily have global counterparts.\n\nExamples of infinite-dimensional pseudogroups abound, beginning with the pseudogroup of all diffeomorphisms of \"E\". The interest is mainly in sub-pseudogroups of the diffeomorphisms, and therefore with objects that have a Lie algebra analogue of vector fields. Methods proposed by Lie and by Cartan for studying these objects have become more practical given the progress of computer algebra.\n\nIn the 1950s Cartan's theory was reformulated by Shiing-Shen Chern, and a general deformation theory for pseudogroups was developed by Kunihiko Kodaira and D. C. Spencer. In the 1960s homological algebra was applied to the basic PDE questions involved, of over-determination; this though revealed that the algebra of the theory is potentially very heavy. In the same decade the interest for theoretical physics of infinite-dimensional Lie theory appeared for the first time, in the shape of current algebra.\n"}
{"id": "21687566", "url": "https://en.wikipedia.org/wiki?curid=21687566", "title": "Pseudoideal", "text": "Pseudoideal\n\nIn the theory of partially ordered sets, a pseudoideal is a subset characterized by a bounding operator LU.\nLU(\"A\") is the set of all lower bounds of the set of all upper bounds of the subset \"A\" of a partially ordered set.\n\nA subset \"I\" of a partially ordered set (\"P\",≤) is a Doyle pseudoideal, if the following condition holds:\n\nFor every finite subset \"S\" of \"P\" that has a supremum in \"P\", \"S\"formula_1 \"I\" implies that LU(\"S\") formula_1 \"I\".\n\nA subset \"I\" of a partially ordered set (\"P\",≤) is a pseudoideal, if the following condition holds:\n\nFor every subset \"S\" of \"P\" having at most two elements that has a supremum in \"P\", \"S\"formula_1 \"I\" implies that LU(\"S\") formula_1 \"I\".\n\n\n\n"}
{"id": "4334890", "url": "https://en.wikipedia.org/wiki?curid=4334890", "title": "Rabbi Nehemiah", "text": "Rabbi Nehemiah\n\nRabbi Nehemiah was an Israelite mathematician, circa AD 150 (during the Tannaim era, Fourth Generation).\n\nHe is attributed as the author of the \"Mishnat ha-Middot\" (ca. AD 150), making it the earliest known Hebrew text on geometry, although some historians assign the text to a later period by an unknown author.\n\nThe \"Mishnat ha-Middot\" argues against the common belief that the Bible defines the geometric ratio (pi) as being exactly equal to 3, based on the description in 1 Kings 7:23 (and 2 Chronicles 4:2) of the great bowl situated outside the Temple of Jerusalem as having a diameter of 10 cubits and a circumference of 30 cubits. He maintained that the diameter of the bowl was measured from the \"outside\" brim, while the circumference was measured along the \"inner\" brim, which with a brim that is one handbreadth wide (as described in the subsequent verses 1 Kings 7:24 and 2 Chronicles 4:3) yields a ratio from the circular rim closer to the actual value of .\n\n"}
{"id": "184898", "url": "https://en.wikipedia.org/wiki?curid=184898", "title": "Ramsey's theorem", "text": "Ramsey's theorem\n\nIn combinatorial mathematics, Ramsey's theorem states that one will find monochromatic cliques in any edge labelling (with colours) of a sufficiently large complete graph. To demonstrate the theorem for two colours (say, blue and red), let \"r\" and \"s\" be any two positive integers. Ramsey's theorem states that there exists a least positive integer for which every blue-red edge colouring of the complete graph on vertices contains a blue clique on \"r\" vertices or a red clique on \"s\" vertices. (Here signifies an integer that depends on both \"r\" and \"s\".)\n\nRamsey's theorem is a foundational result in combinatorics. The first version of this result was proved by F. P. Ramsey. This initiated the combinatorial theory now called Ramsey theory, that seeks regularity amid disorder: general conditions for the existence of substructures with regular properties. In this application it is a question of the existence of \"monochromatic subsets\", that is, subsets of connected edges of just one colour.\n\nAn extension of this theorem applies to any finite number of colours, rather than just two. More precisely, the theorem states that for any given number of colours, \"c\", and any given integers \"n\", …, \"n\", there is a number, \"R\"(\"n\", …, \"n\"), such that if the edges of a complete graph of order \"R\"(\"n\", ..., \"n\") are coloured with \"c\" different colours, then for some \"i\" between 1 and \"c\", it must contain a complete subgraph of order \"n\" whose edges are all colour \"i\". The special case above has \"c\" = 2 (and \"n\" = \"r\" and \"n\" = \"s\").\n\nSuppose the edges of a complete graph on 6 vertices are coloured red and blue. Pick a vertex, \"v\". There are 5 edges incident to \"v\" and so (by the pigeonhole principle) at least 3 of them must be the same colour. Without loss of generality we can assume at least 3 of these edges, connecting the vertex, \"v\", to vertices, \"r\", \"s\" and \"t\", are blue. (If not, exchange red and blue in what follows.) If any of the edges, (\"r\", \"s\"), (\"r\", \"t\"), (\"s\", \"t\"), are also blue then we have an entirely blue triangle. If not, then those three edges are all red and we have an entirely red triangle. Since this argument works for any colouring, \"any\" \"K\" contains a monochromatic \"K\", and therefore \"R\"(3, 3) ≤ 6. The popular version of this is called the theorem on friends and strangers.\n\nAn alternative proof works by double counting. It goes as follows: Count the number of ordered triples of vertices, \"x\", \"y\", \"z\", such that the edge, (\"xy\"), is red and the edge, (\"yz\"), is blue. Firstly, any given vertex will be the middle of either 0 × 5 = 0 (all edges from the vertex are the same colour), 1 × 4 = 4 (four are the same colour, one is the other colour), or 2 × 3 = 6 (three are the same colour, two are the other colour) such triples. Therefore, there are at most 6 × 6 = 36 such triples. Secondly, for any non-monochromatic triangle (xyz), there exist precisely two such triples. Therefore, there are at most 18 non-monochromatic triangles. Therefore, at least 2 of the 20 triangles in the \"K\" are monochromatic.\n\nConversely, it is possible to 2-colour a \"K\" without creating any monochromatic \"K\", showing that \"R\"(3, 3) > 5. The unique colouring is shown to the right. Thus \"R\"(3, 3) = 6.\n\nThe task of proving that \"R\"(3, 3) ≤ 6 was one of the problems of William Lowell Putnam Mathematical Competition in 1953, as well as in the Hungarian Math Olympiad in 1947.\n\nFirst we prove the theorem for the 2-colour case, by induction on \"r\" + \"s\". It is clear from the definition that for all \"n\", \"R\"(\"n\", 1) = \"R\"(1, \"n\") = 1. This starts the induction. We prove that \"R\"(\"r\", \"s\") exists by finding an explicit bound for it. By the inductive hypothesis \"R\"(\"r\" − 1, \"s\") and \"R\"(\"r\", \"s\" − 1) exist.\n\nLemma 1. \"R\"(\"r\", \"s\") ≤ \"R\"(\"r\" − 1, \"s\") + \"R\"(\"r\", \"s\" − 1):\n\nProof. Consider a complete graph on \"R\"(\"r\" − 1, \"s\") + \"R\"(\"r\", \"s\" − 1) vertices whose edges are coloured with two colours. Pick a vertex \"v\" from the graph, and partition the remaining vertices into two sets \"M\" and \"N\", such that for every vertex \"w\", \"w\" is in \"M\" if (\"v\", \"w\") is blue, and \"w\" is in \"N\" if (\"v\", \"w\") is red. Because the graph has \"R\"(\"r\" − 1, \"s\") + \"R\"(\"r\", \"s\" − 1) = |\"M\"| + |\"N\"| + 1 vertices, it follows that either |\"M\"| ≥ \"R\"(\"r\" − 1, \"s\") or |\"N\"| ≥ \"R\"(\"r\", \"s\" − 1). In the former case, if \"M\" has a red \"K\" then so does the original graph and we are finished. Otherwise \"M\" has a blue \"K\" and so \"M\" ∪ {\"v\"} has blue \"K\" by definition of \"M\". The latter case is analogous. Thus the claim is true and we have completed the proof for 2 colours.\n\nNote. In the 2-colour case, if \"R\"(\"r\" − 1, \"s\") and \"R\"(\"r\", \"s\" − 1) are both even, the induction inequality can be strengthened to:\nProof. Suppose p=R(r − 1, s) and q=R(r, s − 1) are both even. Let t=p+q-1 and consider a two-coloured graph of t vertices. If formula_1 is degree of formula_2-th vertex in the blue subgraph, then, according to Handshaking lemma, formula_3is even. Given that t is odd, there must be an even formula_1. Assume formula_5is even, \"M\" and \"N\" are the vertices incident to vertex 1 in blue and red subgraphs respectively. Then both formula_6and formula_7are even. According to Pigeonhole principle, either formula_8, or formula_9. Since formula_10 is even, while formula_11 is odd, the first inequality can be strengthen, so either formula_12 or formula_13. Suppose formula_14. Then either M-subgraph has a red formula_15 and the proof is complete, or it has a blue formula_16 which along with vertex 1 makes a blue formula_17 . Case formula_18 is treated similarly.\n\nLemma 2. If c>2, then \"R\"(\"n\", …, \"n\") ≤ \"R\"(\"n\", …, \"n\", \"R\"(\"n\", \"n\")).\n\nProof. Consider a graph of \"R\"(\"n\", …, \"n\", \"R\"(\"n\", \"n\")) vertices and colour its edges with \"c\" colours. Now 'go colour-blind' and pretend that \"c\" − 1 and \"c\" are the same colour. Thus the graph is now (\"c\" − 1)-coloured. Due to the definition of \"R\"(\"n\", …, \"n\", \"R\"(\"n\", \"n\")), such a graph contains either a \"K\" mono-chromatically coloured with colour \"i\" for some 1 ≤ \"i\" ≤ \"c\" − 2 or a \"K\"-coloured in the 'blurred colour'. In the former case we are finished. In the latter case, we recover our sight again and see from the definition of \"R\"(\"n\", \"n\") we must have either a (\"c\" − 1)-monochrome \"K\" or a \"c\"-monochrome \"K\". In either case the proof is complete.\n\nLemma 1 implies that any \"R(r,s)\" is finite. The right hand side of the inequality in Lemma 2 expresses a Ramsey number for \"c\" colours in terms of Ramsey numbers for fewer colours. Therefore any \"R\"(\"n\", …, \"n\") is finite for any number of colours. This proves the theorem.\n\nThe numbers in Ramsey's theorem (and their extensions to more than two colours) are known as Ramsey numbers. The Ramsey number, , gives the solution to the party problem, which asks the minimum number of guests, , that must be invited so that at least will know each other or at least will not know each other. In the language of graph theory, the Ramsey number is the minimum number of vertices, , such that all undirected simple graphs of order , contain a clique of order , or an independent set of order . Ramsey's theorem states that such a number exists for all and .\n\nBy symmetry, it is true that . An upper bound for can be extracted from the proof of the theorem, and other arguments give lower bounds. (The first exponential lower bound was obtained by Paul Erdős using the probabilistic method.) However, there is a vast gap between the tightest lower bounds and the tightest upper bounds. There are also very few numbers and for which we know the exact value of .\n\nComputing a lower bound for usually requires exhibiting a blue/red colouring of the graph with no blue subgraph and no red subgraph. Such a counterexample is called a \"Ramsey graph\". Brendan McKay maintains a list of known Ramsey graphs. Upper bounds are often considerably more difficult to establish: one either has to check all possible colourings to confirm the absence of a counterexample, or to present a mathematical argument for its absence. A sophisticated computer program does not need to look at all colourings individually in order to eliminate all of them; nevertheless it is a very difficult computational task that existing software can only manage on small sizes. Each complete graph has edges, so there would be a total of graphs to search through (for colours) if brute force is used. Therefore, the complexity for searching all possible graphs (via brute force) is for colourings and an upper bound of nodes.\n\nAs described above, . It is easy to prove that , and, more generally, that for all : a graph on nodes with all edges coloured red serves as a counterexample and proves that ; among colourings of a graph on nodes, the colouring with all edges coloured red contains a -node red subgraph, and all other colourings contain a -node blue subgraph (that is, a pair of nodes connected with a blue edge.)\n\nUsing induction inequalities, it can be concluded that , and therefore . There are only two graphs (that is, -colourings of a complete graph on nodes without -node red or blue complete subgraphs) among different -colourings of -node graphs, and only one graph (the Paley graph of order ) among colourings. (This was proven by Evans, Pulham and Sheehan in 1979.) It follows that .\n\nThe fact that was first established by Brendan McKay and Stanisław Radziszowski in 1995.\n\nThe exact value of is unknown, although it is known to lie between (Geoffrey Exoo (1989)) and (Angeltveit and McKay (2017)) (inclusive).\n\nIn 1997, McKay, Radziszowski and Exoo employed computer-assisted graph generation methods to conjecture that . They were able to construct exactly 656 graphs, arriving at the same set of graphs through different routes. None of the 656 graphs can be extended to a graph.\n\nFor with , only weak bounds are available. Lower bounds for and have not been improved since 1965 and 1972, respectively.\n\nWhere not cited otherwise, entries in the table below are taken from this dynamic survey. Note that since , there is a trivial symmetry across the diagonal.\n\nThe inequality may be applied inductively to prove that\n\nIn particular, this result, due to Erdős and Szekeres, implies that when ,\n\nAn exponential lower bound,\n\nwas given by Erdős in 1947 and was instrumental in his introduction of the probabilistic method. There is obviously a huge gap between these two bounds: for example, for , this gives . Nevertheless, exponential growth factors of either bound have not been improved to date and still stand at and respectively. There is no known explicit construction producing an exponential lower bound. The best known lower and upper bounds for diagonal Ramsey numbers currently stand at\n\ndue to Spencer and Conlon respectively.\n\nFor the off-diagonal Ramsey numbers , it is known that they are of order formula_23; this may be stated equivalently as saying that the smallest possible independence number in an -vertex triangle-free graph is\n\nThe upper bound for is given by Ajtai, Komlós, and Szemerédi, the lower bound was obtained originally by Kim, and was improved by Griffiths, Morris, Fiz Pontiveros, and Bohman and Keevash, by analysing the triangle-free process. More generally, for off-diagonal Ramsey numbers, , with fixed and growing, the best known bounds are\n\ndue to Bohman and Keevash and Ajtai, Komlós and Szemerédi respectively.\n\nA multicolour Ramsey number is a Ramsey number using 3 or more colours. There is (up to symmetries) only one non-trivial multicolour Ramsey number for which the exact value is known, namely \"R\"(3, 3, 3) = 17.\n\nSuppose that we have an edge colouring of a complete graph using 3 colours, red, green and blue. Suppose further that the edge colouring has no monochromatic triangles. Select a vertex \"v\". Consider the set of vertices that have a red edge to the vertex \"v\". This is called the red neighbourhood of \"v\". The red neighbourhood of \"v\" cannot contain any red edges, since otherwise there would be a red triangle consisting of the two endpoints of that red edge and the vertex \"v\". Thus, the induced edge colouring on the red neighbourhood of \"v\" has edges coloured with only two colours, namely green and blue. Since \"R\"(3, 3) = 6, the red neighbourhood of \"v\" can contain at most 5 vertices. Similarly, the green and blue neighbourhoods of \"v\" can contain at most 5 vertices each. Since every vertex, except for \"v\" itself, is in one of the red, green or blue neighbourhoods of \"v\", the entire complete graph can have at most 1 + 5 + 5 + 5 = 16 vertices. Thus, we have \"R\"(3, 3, 3) ≤ 17.\n\nTo see that \"R\"(3, 3, 3) ≥ 17, it suffices to draw an edge colouring on the complete graph on 16 vertices with 3 colours that avoids monochromatic triangles. It turns out that there are exactly two such colourings on \"K\", the so-called untwisted and twisted colourings. Both colourings are shown in the figures to the right, with the untwisted colouring on the top, and the twisted colouring on the bottom.\n\nIf we select any colour of either the untwisted or twisted colouring on \"K\", and consider the graph whose edges are precisely those edges that have the specified colour, we will get the Clebsch graph.\n\nIt is known that there are exactly two edge colourings with 3 colours on \"K\" that avoid monochromatic triangles, which can be constructed by deleting any vertex from the untwisted and twisted colourings on \"K\", respectively.\n\nIt is also known that there are exactly 115 edge colourings with 3 colours on \"K\" that avoid monochromatic triangles, provided that we consider edge colourings that differ by a permutation of the colours as being the same.\n\nThe theorem can also be extended to hypergraphs. An \"m\"-hypergraph is a graph whose \"edges\" are sets of \"m\" vertices – in a normal graph an edge is a set of 2 vertices. The full statement of Ramsey's theorem for hypergraphs is that for any integers \"m\" and \"c\", and any integers \"n\", …, \"n\", there is an integer \"R\"(\"n\", …, \"n\";\"c\", \"m\") such that if the hyperedges of a complete \"m\"-hypergraph of order \"R\"(\"n\", …, \"n\";\"c\", \"m\") are coloured with \"c\" different colours, then for some \"i\" between 1 and \"c\", the hypergraph must contain a complete sub-\"m\"-hypergraph of order \"n\" whose hyperedges are all colour \"i\". This theorem is usually proved by induction on \"m\", the 'hyper-ness' of the graph. The base case for the proof is \"m\" = 2, which is exactly the theorem above.\n\nA further result, also commonly called \"Ramsey's theorem\", applies to infinite graphs. In a context where finite graphs are also being discussed it is often called the \"Infinite Ramsey theorem\". As intuition provided by the pictorial representation of a graph is diminished when moving from finite to infinite graphs, theorems in this area are usually phrased in set-theoretic terminology.\n\nTheorem. Let \"X\" be some infinite set and colour the elements of \"X\" (the subsets of \"X\" of size \"n\") in \"c\" different colours. Then there exists some infinite subset \"M\" of \"X\" such that the size \"n\" subsets of \"M\" all have the same colour.\n\nProof: The proof is by induction on \"n\", the size of the subsets. For \"n\" = 1, the statement is equivalent to saying that if you split an infinite set into a finite number of sets, then one of them is infinite. This is evident. Assuming the theorem is true for \"n\" ≤ \"r\", we prove it for \"n\" = \"r\" + 1. Given a \"c\"-colouring of the (\"r\" + 1)-element subsets of \"X\", let \"a\" be an element of \"X\" and let \"Y\" = \"X\" \\ {\"a\"}. We then induce a \"c\"-colouring of the \"r\"-element subsets of \"Y\", by just adding \"a\" to each \"r\"-element subset (to get an (\"r\" + 1)-element subset of \"X\"). By the induction hypothesis, there exists an infinite subset \"Y\" of \"Y\" such that every \"r\"-element subset of \"Y\" is coloured the same colour in the induced colouring. Thus there is an element \"a\" and an infinite subset \"Y\" such that all the (\"r\" + 1)-element subsets of \"X\" consisting of \"a\" and \"r\" elements of \"Y\" have the same colour. By the same argument, there is an element \"a\" in \"Y\" and an infinite subset \"Y\" of \"Y\" with the same properties. Inductively, we obtain a sequence {\"a\", \"a\", \"a\", …} such that the colour of each (\"r\" + 1)-element subset (\"a\", \"a\", …, \"a\") with \"i\"(1) < \"i\"(2) < ... < \"i\"(\"r\" + 1) depends only on the value of \"i\"(1). Further, there are infinitely many values of \"i\"(\"n\") such that this colour will be the same. Take these \"a\"'s to get the desired monochromatic set.\n\nIt is possible to deduce the finite Ramsey theorem from the infinite version by a proof by contradiction. Suppose the finite Ramsey theorem is false. Then there exist integers \"c\", \"n\", \"T\" such that for every integer \"k\", there exists a \"c\"-colouring of formula_26 without a monochromatic set of size \"T\". Let \"C\" denote the \"c\"-colourings of formula_26 without a monochromatic set of size \"T\".\n\nFor any \"k\", the restriction of a colouring in \"C\" to formula_26 (by ignoring the colour of all sets containing \"k\" + 1) is a colouring in \"C\". Define formula_29 to be the colourings in \"C\" which are restrictions of colourings in \"C\". Since \"C\" is not empty, neither is formula_30.\n\nSimilarly, the restriction of any colouring in formula_31 is in formula_29, allowing one to define formula_33 as the set of all such restrictions, a non-empty set. Continuing so, define formula_34 for all integers \"m\", \"k\".\n\nNow, for any integer \"k\", formula_35, and each set is non-empty. Furthermore, \"C\" is finite as formula_36. It follows that the intersection of all of these sets is non-empty, and let formula_37. Then every colouring in \"D\" is the restriction of a colouring in \"D\". Therefore, by unrestricting a colouring in \"D\" to a colouring in \"D\", and continuing doing so, one constructs a colouring of formula_38 without any monochromatic set of size \"T\". This contradicts the infinite Ramsey theorem.\n\nIf a suitable topological viewpoint is taken, this argument becomes a standard compactness argument showing that the infinite version of the theorem implies the finite version.\n\nIt is also possible to define Ramsey numbers for \"directed\" graphs. (These were introduced by P. Erdős & L. Moser.) Let \"R\"(\"n\") be the smallest number \"Q\" such that any complete graph with singly directed arcs (also called a \"tournament\") and with ≥ \"Q\" nodes contains an acyclic (also called \"transitive\") \"n\"-node subtournament.\n\nThis is the directed-graph analogue of what (above) has been called \"R\"(\"n\", \"n\"; 2), the smallest number \"Z\" such that any 2-colouring of the edges of a complete \"un\"directed graph with ≥ \"Z\" nodes, contains a monochromatic complete graph on n nodes. (The directed analogue of the two possible arc \"colours\" is the two \"directions\" of the arcs, the analogue of \"monochromatic\" is \"all arc-arrows point the same way\"; i.e., \"acyclic.\")\n\nWe have \"R\"(0) = 0, \"R\"(1) = 1, \"R\"(2) = 2, \"R\"(3) = 4, \"R\"(4) = 8, \"R\"(5) = 14, \"R\"(6) = 28, 32 ≤ \"R\"(7) ≤ 55, and \"R\"(8) is again a problem, according to Erdős, that one does not want powerful aliens to pose.\n\nRamsey numbers can be determined by some universal quantum computers. The decision question is solved by determining whether the probe qubit exhibits resonance dynamics.\n\n\n\n"}
{"id": "43705185", "url": "https://en.wikipedia.org/wiki?curid=43705185", "title": "Recursive neural network", "text": "Recursive neural network\n\nA recursive neural network (RNN) is a kind of deep neural network created by applying the same set of weights recursively over a structured input, to produce a structured prediction over variable-size input structures, or a scalar prediction on it, by traversing a given structure in topological order. RNNs have been successful, for instance, in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding. RNNs have first been introduced to learn distributed representations of structure, such as logical terms.\nModels and general frameworks have been developed in further works since the 90s.\n\nIn the most simple architecture, nodes are combined into parents using a weight matrix that is shared across the whole network, and a non-linearity such as \"tanh\". If \"c\" and \"c\" are \"n\"-dimensional vector representation of nodes, their parent will also be an \"n\"-dimensional vector, calculated as\n\nformula_1\n\nWhere \"W\" is a learned formula_2 weight matrix.\n\nThis architecture, with a few improvements, has been used for successfully parsing natural scenes and for syntactic parsing of natural language sentences.\n\nRecCC is a constructive neural network approach to deal with tree domains with pioneering applications to chemistry and extension to directed acyclic graphs.\n\nA framework for unsupervised RNN has been introduced in 2004.\n\nRecursive neural tensor networks use one, tensor-based composition function for all nodes in the tree.\n\nTypically, stochastic gradient descent (SGD) is used to train the network. The gradient is computed using backpropagation through structure (BPTS), a variant of backpropagation through time used for recurrent neural networks.\n\nUniversal approximation capability of RNN over trees has been proved in literature.\n\nRecurrent neural networks are recursive artificial neural networks with a certain structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step.\n\nAn efficient approach to implement recursive neural networks is given by the Tree Echo State Network, within the Reservoir Computing paradigm.\n\nExtensions to graphs include Graph Neural Network (GNN), Neural Network for Graphs (NN4G), and more recently convolutional neural networks for graphs.\n"}
{"id": "8646865", "url": "https://en.wikipedia.org/wiki?curid=8646865", "title": "Robert G. Bartle", "text": "Robert G. Bartle\n\nRobert Gardner Bartle (1927 – 2003) was an American mathematician specializing in real analysis. He is known for writing the popular textbooks \"The Elements of Real Analysis\" (1964), \"The Elements of Integration\" (1966), and \"Introduction to Real Analysis\" (2011) published by John Wiley & Sons. \n\nBartle was born in Kansas City, Missouri, and was the son of Glenn G. Bartle and Wanda M. Bartle.\nHe was married to Doris Sponenberg Bartle (born 1927) from 1952 to 1982 and they had two sons, James A. Bartle (born 1955) and John R. Bartle (born 1958). He was on the faculty of the Department of Mathematics at the University of Illinois from 1955 to 1990.\n\nBartle was Executive Editor of \"Mathematical Reviews\" from 1976 to 1978 and from 1986 to 1990. From 1990 to 1999 he taught at Eastern Michigan University. In 1997, he earned a writing award from the Mathematical Association of America for his paper \"Return to the Riemann Integral\". \n\n"}
{"id": "39317783", "url": "https://en.wikipedia.org/wiki?curid=39317783", "title": "Roland Bulirsch", "text": "Roland Bulirsch\n\nRoland Zdeněk Bulirsch (born November 10, 1932, Liberec, Czechoslovakia) is a German mathematician specializing in numerical analysis.\n\nHe earned his Ph.D. in 1966 at the Munich University of Technology and joined the faculty there in 1973.\n\nHe is the author (with Josef Stoer) of \"Introduction to Numerical Analysis\", a standard reference for the theory of numerical methods, and has also authored numerous other books and articles. The book \"From Nano to Space: Applied Mathematics Inspired by Roland Bulirsch\" is a tribute to his work.\n\n"}
{"id": "3897244", "url": "https://en.wikipedia.org/wiki?curid=3897244", "title": "Schur-convex function", "text": "Schur-convex function\n\nIn mathematics, a Schur-convex function, also known as S-convex, isotonic function and order-preserving function is a function formula_1 that for all formula_2 such that formula_3 is majorized by formula_4, one has that formula_5. Named after Issai Schur, Schur-convex functions are used in the study of majorization. Every function that is convex and symmetric is also Schur-convex. The opposite implication is not true, but all Schur-convex functions are symmetric (under permutations of the arguments).\n\nA function \"f\" is 'Schur-concave' if its negative, -\"f\", is Schur-convex.\n\nIf \"f\" is symmetric and all first partial derivatives exist, then \n\"f\" is Schur-convex if and only if\n\nformula_6 for all formula_7\n\nholds for all 1≤\"i\"≠\"j\"≤\"d\".\n\n\n"}
{"id": "43761184", "url": "https://en.wikipedia.org/wiki?curid=43761184", "title": "Sierpiński set", "text": "Sierpiński set\n\nIn mathematics, a Sierpiński set is an uncountable subset of a real vector space whose intersection with every measure-zero set is countable. The existence of Sierpiński sets is independent of the axioms of ZFC. showed that they exist if the continuum hypothesis is true. On the other hand, they do not exist if Martin's axiom for ℵ is true. Sierpiński sets are weakly Luzin sets but are not Luzin sets .\n\nChoose a collection of 2 measure 0 subsets of R such that every measure 0 subset is contained in one of them. By the continuum hypothesis, it is possible to enumerate them as \"S\" for countable ordinals α. For each countable ordinal \"β\" choose a real number \"x\" that is not in any of the sets \"S\" for \"α\" < \"β\", which is possible as the union of these sets has measure 0 so is not the whole of R. Then the uncountable set \"X\" of all these real numbers \"x\" has only a countable number of elements in each set \"S\", so is a Sierpiński set.\n\nIt is possible for a Sierpiński set to be a subgroup under addition. For this one modifies the construction above by choosing a real number \"x\" that is not in any of the countable number of sets of the form (\"S\" + \"X\")/\"n\" for \"α\" < \"β\", where \"n\" is a positive integer and \"X\" is an integral linear combination of the numbers \"x\" for \"α\" < \"β\". Then the group generated by these numbers is a Sierpiński set and a group under addition. More complicated variations of this construction produce examples of Sierpiński sets that are subfields or real-closed subfields of the real numbers.\n\n"}
{"id": "48537102", "url": "https://en.wikipedia.org/wiki?curid=48537102", "title": "Stretch factor", "text": "Stretch factor\n\nIn mathematics, the stretch factor of an embedding measures the factor by which the embedding distorts distances.\nSuppose that one metric space is embedded into another metric space by a metric map, a continuous one-to-one function that preserves or reduces the distance between every pair of points. Then the embedding gives rise to two different notions of distance between pairs of points in . Any pair of points in has both an intrinsic distance, the distance from to in , and a smaller extrinsic distance, the distance from to in . The stretch factor of the pair is the ratio between these two distances, . The stretch factor of the whole mapping is the supremum (if it exists) of the stretch factors of all pairs of points. The stretch factor has also been called the distortion or dilation of the mapping.\n\nThe stretch factor is important in the theory of geometric spanners, weighted graphs that approximate the Euclidean distances between a set of points in the Euclidean plane. In this case, the embedded metric is a finite metric space, whose distances are shortest path lengths in a graph, and the metric into which is embedded is the Euclidean plane. When the graph and its embedding are fixed, but the graph edge weights can vary, the stretch factor is minimized when the weights are exactly the Euclidean distances between the edge endpoints. Research in this area has focused on finding sparse graphs for a given point set that have low stretch factor.\n\nThe Johnson–Lindenstrauss lemma asserts that any finite metric space with points can be embedded into a Euclidean space of dimension with distortion , for any constant , where the constant factor in the -notation depends on the choice of . This result, and related methods of constructing low-distortion metric embeddings, are important in the theory of approximation algorithms.\n\nIn knot theory, the distortion of a knot is a knot invariant, the minimum stretch factor of any embedding of the knot as a space curve in Euclidean space.\nUndergraduate researcher John Pardon won the 2012 Morgan Prize for his research showing that there is no upper bound on the distortion of torus knots, solving a problem originally posed by Mikhail Gromov.\n\nIn the study of the curve-shortening flow, in which each point of a curve in the Euclidean plane moves perpendicularly to the curve, with speed proportional to the local curvature, proved that the stretch factor of any simple closed smooth curve (with intrinsic distances measured by arc length) changes monotonically. More specifically, at each pair that forms a local maximum of the stretch factor, the stretch factor is strictly decreasing, except when the curve is a circle. This property was later used to simplify the proof of the Gage–Hamilton–Grayson theorem, according to which every simple closed smooth curve stays simple and smooth until it collapses to a point, converging in shape to a circle before doing so.\n"}
{"id": "24609727", "url": "https://en.wikipedia.org/wiki?curid=24609727", "title": "Transfinite interpolation", "text": "Transfinite interpolation\n\nIn numerical analysis, transfinite interpolation is a means to construct functions over a planar domain in such a way that they match a given function on the boundary. This method is applied in geometric modelling and in the field of finite element method.\n\nThe transfinite interpolation method, first introduced by William J. Gordon and Charles A. Hall, receives its name due to how a function belonging to this class is able to match the primitive function at a nondenumerable number of points.\nIn the authors' words:\nTransfinite interpolation is similar to the Coons patch, invented in 1967. \n\nWith parametrized curves formula_1, formula_2 describing one pair of opposite sides of a domain, and\nformula_3, formula_4 describing the other pair. the position of point (u,v) in the domain is\n\nformula_5\n\nwhere, e.g., formula_6 is the point where curves formula_7 and formula_8 meet.\n"}
{"id": "247392", "url": "https://en.wikipedia.org/wiki?curid=247392", "title": "Uniform boundedness principle", "text": "Uniform boundedness principle\n\nIn mathematics, the uniform boundedness principle or Banach–Steinhaus theorem is one of the fundamental results in functional analysis. Together with the Hahn–Banach theorem and the open mapping theorem, it is considered one of the cornerstones of the field. In its basic form, it asserts that for a family of continuous linear operators (and thus bounded operators) whose domain is a Banach space, pointwise boundedness is equivalent to uniform boundedness in operator norm.\n\nThe theorem was first published in 1927 by Stefan Banach and Hugo Steinhaus but it was also proven independently by Hans Hahn.\n\nTheorem (Uniform Boundedness Principle). Let \"X\" and \"Y\" be Banach spaces. Suppose that \"F\" is a collection of continuous linear operators from \"X\" to \"Y\". If for all \"x\" in \"X\" one has\n\nthen\n\nThe completeness of \"X\" enables the following short proof, using the Baire category theorem.\n\nProof. Suppose that for every \"x\" in the Banach space \"X\", one has:\n\nFor every integer formula_4, let\n\nThe set formula_6 is a closed set and by the assumption,\n\nBy the Baire category theorem for the non-empty complete metric space \"X\", there exists \"m\" such that \nformula_8 has non-empty interior, \"i.e.\", there exist formula_9 and such that\n\nLet \"u\" ∈ \"X\" with and . One has that:\n\nTaking the supremum over \"u\" in the unit ball of \"X\" and over formula_12, it follows that\n\nThere are also simple proofs not using the Baire theorem .\n\nCorollary. If a sequence of bounded operators (\"T\") converges pointwise, that is, the limit of {\"T\"(\"x\")} exists for all \"x\" in \"X\", then these pointwise limits define a bounded operator \"T\".\n\nNote it is not claimed above that \"T\" converges to \"T\" in operator norm, i.e. uniformly on bounded sets. (However, since {\"T\"} is bounded in operator norm, and the limit operator \"T\" is continuous, a standard \"3-ε\" estimate shows that \"T\" converges to \"T\" uniformly on \"compact\" sets.)\n\nCorollary. Any weakly bounded subset S in a normed space Y is bounded.\"\n\nIndeed, the elements of \"S\" define a pointwise bounded family of continuous linear forms on the Banach space \"X\" = \"Y*\", continuous dual of \"Y\". By the uniform boundedness principle, the norms of elements of \"S\", as functionals on \"X\", that is, norms in the second dual \"Y**\", are bounded. But for every \"s\" in \"S\", the norm in the second dual coincides with the norm in \"Y\", by a consequence of the Hahn–Banach theorem.\n\nLet \"L\"(\"X\", \"Y\") denote the continuous operators from \"X\" to \"Y\", with the operator norm. If the collection \"F\" is unbounded in \"L\"(\"X\", \"Y\"), then by the uniform boundedness principle, we have:\n\nIn fact, \"R\" is dense in \"X\". The complement of \"R\" in \"X\" is the countable union of closed sets ∪\"X\". By the argument used in proving the theorem, each \"X\" is nowhere dense, i.e. the subset ∪\"X\" is \"of first category\". Therefore \"R\" is the complement of a subset of first category in a Baire space. By definition of a Baire space, such sets (called \"residual sets\") are dense. Such reasoning leads to the principle of condensation of singularities, which can be formulated as follows:\n\nTheorem. Let \"X\" be a Banach space, {\"Y\"} a sequence of normed vector spaces, and \"F\" a unbounded family in \"L\"(\"X\", \"Y\"). Then the set\n\nis of second category, and thus dense in \"X\".\n\nProof. The complement of \"R\" is the countable union\n\nof sets of first category. Therefore its residual set \"R\" is dense.\n\nLet formula_17 be the circle, and let formula_18 be the Banach space of continuous functions on formula_17, with the uniform norm. Using the uniform boundedness principle, one can show that there exists an element in formula_18 for which the Fourier series does not converge pointwise.\n\nFor formula_21, its Fourier series is defined by\n\nand the \"N\"-th symmetric partial sum is\n\nwhere \"D\" is the \"N\"-th Dirichlet kernel. Fix formula_24 and consider the convergence of {\"S\"(\"f\")(\"x\")}. The functional φ : formula_25 defined by\n\nis bounded. The norm of φ, in the dual of formula_18, is the norm of the signed measure (2π)\"D\"(\"x\"−\"t\") d\"t\", namely\n\nOne can verify that\n\nSo the collection {φ} is unbounded in formula_30, the dual of formula_18. Therefore by the uniform boundedness principle, for any formula_24, the set of continuous functions whose Fourier series diverges at \"x\" is dense in formula_18.\n\nMore can be concluded by applying the principle of condensation of singularities. Let {\"x\"} be a dense sequence in formula_17. Define φ in the similar way as above. The principle of condensation of singularities then says that the set of continuous functions whose Fourier series diverges at each \"x\" is dense in formula_18 (however, the Fourier series of a continuous function \"f\" converges to \"f\"(\"x\") for almost every formula_24, by Carleson's theorem).\n\nThe least restrictive setting for the uniform boundedness principle is a barrelled space where the following generalised version of the theorem holds :\n\nTheorem. Given a barrelled space \"X\" and a locally convex space \"Y\", then any family of pointwise bounded continuous linear mappings from \"X\" to \"Y\" is equicontinuous (even uniformly equicontinuous).\n\nAlternatively, the statement also holds whenever \"X\" is a Baire space and \"Y\" is a locally convex space .\n\nTheorem. Let \"X\" be a Fréchet space, \"Y\" a normed space, and \"H\" a set of continuous linear mappings of \"X\" into \"Y\". If for every \"x\" in \"X\"\nthen the family \"H\" is equicontinuous.\n\n\n"}
