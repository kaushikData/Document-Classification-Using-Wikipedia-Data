{"id": "4698837", "url": "https://en.wikipedia.org/wiki?curid=4698837", "title": "184 (number)", "text": "184 (number)\n\n184 (one hundred [and] eighty-four) is the natural number following 183 and preceding 185.\n\n\n\n\n\n184 is also:\n\n\n"}
{"id": "9552145", "url": "https://en.wikipedia.org/wiki?curid=9552145", "title": "1 + 2 + 4 + 8 + ⋯", "text": "1 + 2 + 4 + 8 + ⋯\n\nIn mathematics, is the infinite series whose terms are the successive powers of two. As a geometric series, it is characterized by its first term, 1, and its common ratio, 2. As a series of real numbers it diverges to infinity, so in the usual sense it has no sum. In a much broader sense, the series is associated with another value besides ∞, namely −1, which is the limit of the series using the 2-adic metric.\n\nThe partial sums of are since these diverge to infinity, so does the series. \n\nTherefore, any totally regular summation method gives a sum of infinity, including the Cesàro sum and Abel sum. On the other hand, there is at least one generally useful method that sums to the finite value of −1. The associated power series\n\nhas a radius of convergence around 0 of only , so it does not converge at . Nonetheless, the so-defined function \"f\" has a unique analytic continuation to the complex plane with the point deleted, and it is given by the same rule . Since , the original series is said to be summable (E) to −1, and −1 is the (E) sum of the series. (The notation is due to G. H. Hardy in reference to Leonhard Euler's approach to divergent series).\n\nAn almost identical approach (the one taken by Euler himself) is to consider the power series whose coefficients are all 1, i.e.\n\nand plugging in \"y\" = 2. Of course these two series are related by the substitution \"y\" = 2\"x\".\n\nThe fact that (E) summation assigns a finite value to shows that the general method is not totally regular. On the other hand, it possesses some other desirable qualities for a summation method, including stability and linearity. These latter two axioms actually force the sum to be −1, since they make the following manipulation valid:\n\nIn a useful sense, \"s\" = ∞ is a root of the equation (For example, ∞ is one of the two fixed points of the Möbius transformation on the Riemann sphere). If some summation method is known to return an ordinary number for \"s\", \"i.e.\" not ∞, then it is easily determined. In this case \"s\" may be subtracted from both sides of the equation, yielding , so .\n\nThe above manipulation might be called on to produce −1 outside the context of a sufficiently powerful summation procedure. For the most well-known and straightforward sum concepts, including the fundamental convergent one, it is absurd that a series of positive terms could have a negative value. A similar phenomenon occurs with the divergent geometric series 1 − 1 + 1 − 1 + ⋯, where a series of integers appears to have the non-integer sum . These examples illustrate the potential danger in applying similar arguments to the series implied by such recurring decimals as 0.111… and most notably 0.999…. The arguments are ultimately justified for these convergent series, implying that and , but the underlying proofs demand careful thinking about the interpretation of endless sums.\n\nIt is also possible to view this series as convergent in a number system different from the real numbers, namely, the 2-adic numbers. As a series of 2-adic numbers this series converges to the same sum, −1, as was derived above by analytic continuation.\n\n\n\n"}
{"id": "663188", "url": "https://en.wikipedia.org/wiki?curid=663188", "title": "American Invitational Mathematics Examination", "text": "American Invitational Mathematics Examination\n\nThe American Invitational Mathematics Examination (AIME) is a 15-question 3-hour test given since 1983 to those who rank in the top 5% on the AMC 12 high school mathematics contest (formerly known as the AHSME), and starting in 2010, those who rank in the top 2.5% on the AMC 10.\n\nThe AIME is the second of two tests used to determine qualification for the United States of America Mathematical Olympiad (USAMO), the first being the AMC. \n\nThe use of calculators is not allowed on the test.\n\nThe exam consists of 15 questions of increasing difficulty, where each answer is an integer between 0 and 999 inclusive. Thus the test effectively removes the element of chance afforded by a multiple-choice test while preserving the ease of automated grading; answers are entered onto an OMR sheet, similar to the way grid-in math questions are answered on the SAT. Leading zeros must be gridded in; for example, answers of 7 and 43 must be written and gridded in as 007 and 043, respectively.\n\nConcepts typically covered on the exam include topics in elementary algebra, geometry, trigonometry, as well as number theory, probability, and combinatorics. Many of these concepts are not directly covered in typical high school mathematics courses; thus, participants often turn to supplementary resources to prepare for the exam.\n\nOne point is earned for each correct answer, and no points are deducted for incorrect answers. No partial credit is given. Thus AIME scores are integers from 0 to 15 inclusive.\n\nSome recent results are:\n\nA student's score on the AIME is used in combination with his score on the AMC to determine eligibility for the USAMO. A student's score on the AMC is added to 10 times his score on the AIME. In 2006, the cutoff for eligibility in the USAMO was 217 combined points.\n\nDuring the 1990s it was not uncommon for fewer than 2,000 students to qualify for the AIME, although 1994 was a notable exception where 99 students achieved perfect scores on the AHSME and the list of high scorers, which usually was distributed in small pamphlets, had to be distributed several months late in thick newspaper bundles.\n\nThe AIME began in 1983. It was given once per year on a Tuesday or Thursday in late March or early April. Beginning in 2000, the AIME is given twice per year, the second date being an \"alternate\" test given to accommodate those students who are unable to sit for the first test because of Spring Break, illness, or any other reason. However, under no circumstances may a student officially take both exams. The alternate test, commonly called the \"AIME2\" or \"AIME-II,\" is usually given exactly two weeks after the first test, on a Tuesday in early April. However, like the AMC, the AIME recently has been given on a Tuesday in early March, and on the Wednesday 15 days later, e.g. March 7 and 22, 2006.\n\n\nformula_1\n\nwhere formula_2 and formula_3 are positive integers and formula_3 is as large as possible, find formula_5 (\"2003 AIME I #1\")\n\n\n"}
{"id": "54284233", "url": "https://en.wikipedia.org/wiki?curid=54284233", "title": "Andrásfai graph", "text": "Andrásfai graph\n\nIn graph theory, an Andrásfai graph is a triangle-free circulant graph named after .\n\nThe Andrásfai graph And() for any natural number formula_1 is a circulant graph on formula_2 vertices, in which vertex formula_3 is connected by an edge to vertices formula_4, for every formula_5 that is congruent to 1 mod 3. For instance, the Wagner graph is an Andrásfai graph, the graph And(3).\n\nThe graph family is triangle-free, and And() has an independence number of formula_6.\n\n\n"}
{"id": "4843630", "url": "https://en.wikipedia.org/wiki?curid=4843630", "title": "Behavioral operations research", "text": "Behavioral operations research\n\nBehavioral operations research (BOR) examines and takes into consideration human behavior and emotions when facing complex decision problems. BOR is part of Operational Research. BOR relates to the behavioural aspects of the use of operations research in problem solving and decision support. Specifically, it focuses on understanding behaviour in, with and beyond models. The general purpose is to make better use and improve the use of operations research theories and practice, so that the benefits received from the potential improvements to operations research approaches in practice, that arise from recent findings in behavioural sciences, are realised . BOR approaches have heavily influenced supply chain management research, amongst others.\n\nOperations research (or operational research) involves a wide range of problem–solving skills aiming to help individuals or organizations to make more rational decisions as well as improving their efficiency. However, operations research often assumes that agents involved in the process or operating system, such as employees, consumers and suppliers, make fully rational decisions. Their decisions are not affected by their emotions as well as their surroundings and that they are able to react and distinguish between different types of information. In reality, this is not always true; human behavior has an important role in decision making and worker motivation, and therefore should be considered in the study of operations. This has led to the rise of behavioral operations research, which is defined as the study of impacts that human behavior has on operations, design and business interactions in different organizations. Behavioral operations research aims to understand the decision making of managers and tries to make improvements to the supply chain using the insight obtained. Behavioral operations research includes knowledge from a number of fields, such as economics, behavioral science, psychology and other social sciences. Traditional operations research and behavioral operations research have a common intellectual goal, aiming to make differences in operations outcomes, such as flexibility, efficiency and productivity.\n\nHumans have limitations in their ability to collect and react to relevant information. When a decision or conclusion has to be made through complex information, human decision-makers often fail to comply with normative decision theories. Moreover, a person's lifestyle, social interaction and collective behaviors has clear influence on people's decisions.\nThis is a relatively new branch of psychology which focuses on humans' ability to make decisions, solve problems, learning, attention, memory and forgetting, these are only a few of the practical applications of this science, to some extent it is also related to motivation and emotion. Cognitive psychology is interested in what is happening within the mind when new information is received, how people respond to this information, and how this response affects their behavior and emotions. Cognitive psychology is considered to be one of the dominant theoretical force in behavioral science.\n\nPeople often react and behave differently when they are put into different social situations. The aim of social psychology is to understand the nature and causes of individual behaviors. It questions or provides an insight of how human behavior relies on the physical environment. Social psychology theories explain why there are competition between individuals, why it is often the case that individuals or organisations seek to protect and maintain their status, and why it has been observed that individuals and organisations are willing to sacrifice efficiency to achieve their goal of staying at a higher hierarchical position. In addition to status, social psychology also includes people's behavior towards goal setting, feedback and controls, interdependence, and reciprocity.\n\nThe use of psychology in behavioral operations research links to the idea of judging the relationship between people's mental health and wellbeing and their behavior at work. Psychology experts often set up indicators to evaluate how an employee's surroundings, such as working environment and noise, can affect their productivity at work. Organizational behavior is the study of ways people react or behave when they are organized in groups. The purpose of this study is to improve business productivity, trying to create a more efficient business organization. Organizational behavior theories are applied towards human resource trying to maximize the output from individual group members. The study of organization behavior can be broken down into different sections, including Personality, Job Satisfaction and Reward Management, Leadership, Authority, Power and Politics.\n\nThere are four main streams of research that can be considered a part of behavior operations research. These give us an idea of the weakness of the current operations research model and the effectiveness of behavioral operations research in predicting human decisions and reactions when facing different situations.\n\nPhysical technologies can be defined as a socio-technology system, which consists of humans, human activity, spaces, artifacts, tools, and communications media. This theory suggests that social interaction at the workplace is determined by the technology and techniques used at work. Therefore, a change in technology used at work can lead to unexpected consequences. This may include differences in motivation.\n\nHuman factor engineering can be applied to improve workplaces, working systems and products aiming to reduce human errors during operations as well as improving human efficiency, productivity and operational performance. It often involves improvements of large machineries to reduce accidents during work. It is believed that behavior operations research can be related to human factor engineering as systematic errors, which happens very often technology and different machineries, can affect people's decision making and their interaction with the operating system.\nThe bullwhip effect happens along the supply chain of an organisation. This is when agents along the supply chain fail to pass on relevant and accurate information to their superior, leading to a large variation in the demand of goods and services from the customers and the supply from the organisation. Human behavior is a very important aspect to be considered, as decision making and emotions in the market will make a big difference to the final result of the bullwhip effect. This is because managers who can calmly make decisions while others competitors panic and deliver inaccurate messages often perform better within the industry. When the market is in poor condition, 'fear' can be an important factor affecting managers' decisions. Therefore, human behavior is closely related to people's decisions.\n\nSystem dynamics models in operations contexts focuses on the importance of how components in a system interact with each other as well as providing an overview of the processes that takes place within the system. Behavioral operations research often benefits from the development of comprehensive systems models as they are able to analyse and provide an insight of the operational system. This allows the study of behavioral operations to understand how people in these settings or work conditions think about the context in which they operate. In an operations system, it often involves levers for managers to manipulate the components involved in the system. The uses of these levers are determined by human behaviors and these behaviors include feelings of stress and fear.\n\nThe newsvendor model (or newsboy or single-period or perishable) is a mathematical model used in operations management and applied economics to determine optimal level products or services to keep in stock when the demand is unknown. This model is also known as the newsvendor problem by analogy with the situation faced by a newspaper vendor who must make on time stocking decision of how many copies of newspaper to keep in stock when facing uncertain demand. If the newspaper vendor stocks too much, these newspaper may end up worthless by the end of the day as they are no longer up-to-date. However, if the newspaper vendor stocks too little, he misses the opportunity too make more profit and suffer loss of goodwill.\nThe assignment problem is a complex optimization problem. The problem involves number of agents and a number of tasks. The purpose is to assign each agent with a task. All agents are expected or aimed to be allocated in a way that will maximize productivity and efficiency and minimize the total cost of the assignment.\n\nThe secretary problem can also be called the optimal stopping theory. It focuses heavily on applied probability, statistics and decision theory. The idea behind the secretary problem is that to make the best decisions when there is a pool of different options, other options are unknown, details of other options will only be given if the currently existing option is given up. The problem comes down to the idea of optimal strategy, maximizing the probability of selecting the best option.\n"}
{"id": "44558933", "url": "https://en.wikipedia.org/wiki?curid=44558933", "title": "Belt machine", "text": "Belt machine\n\nIn computer engineering and in programming language implementations, a belt machine is a real or emulated computer that uses a \"first in, first out\" (FIFO) queue rather than individual machine processor registers to evaluate each sub-expression in the program. A belt computer is programmed with an instruction set that specifies arguments explicitly but results implicitly.\n\nThe common alternative to belt machines are register machines, in which each instruction explicitly names the specific registers to use for locations of operand arguments and results. Belt machines are related to stack machines, which specify both arguments and results implicitly using a pushdown stack. Other alternatives are accumulator machines, which have only one visible general-purpose temp register, and memory-to-memory machines, which have no visible temp registers.\n\nA belt machine implements temporary storage with a fixed-length FIFO queue, or \"belt\" by analogy to a \"conveyor belt\". The operands of the arithmetic logic units (ALUs) and other functional units may be taken from any position on the belt, and the result from the computation is \"dropped\" (stored) in the front position of the belt, advancing the belt to make room. As the belt is fixed length, drops in the front are matched by older operands falling off the back; pushed-off operands become inaccessible and must be explicitly saved if still needed for later work. Most operations of the instruction set work only with data on the belt, not on data registers or main memory cells.\n\nFor a typical instruction like codice_1, both argument operands come from explicitly named positions on the belt, and the result is dropped on the front, ready for the next instruction. Operations with multiple results simply drop more values at the belt front. Most belt instructions are encoded as just an \"operation code\" (opcode) and two belt positions, with no added fields to specify a result register, memory address, or literal constant. This encoding is easily extended to richer operations with more than two inputs or more than one result. Constant operands are dropped by separate codice_2 instructions. All access of program variables in main \"random-access memory\" (RAM) is segregated into separate codice_3 or codice_4 instructions containing one memory address, or some way to calculate that address from belt operands.\n\nAll belt machines have variants of the load/store opcodes to access local variables and the heap. This can be by offsets, from a pointer on the belt, or from various special-purpose base registers. Similarly, there will be instructions to branch to an address taken from the belt, along with branches relative to the program counter.\n\nBecause each drop of a result moves the prior belt content along to later positions in the queue, a given operand continually changes its position (and hence address) as a result of later execution. In effect, an access to the operand at position zero is a request for the most recent value dropped to the belt, while (for example) a reference to position five is to the sixth most recent drop. Thus the addresses of belt operands reflect the belt history over time. This is \"temporal addressing\". It is hard for human programmers to keep track of belt contents, and hence operand addresses, when writing assembly code for a belt machine. However, it is easy for a compiler to track the changing contents and emit the correct position addresses in generated code.\n\nThe belt is fixed length and may be too short to hold all live transient operands before they are pushed off the end. If an operand is needed for longer than its belt lifetime, it must be saved while still on the belt (spill) and later restored to the belt when needed again (fill). This situation is equivalent to the need to spill registers to memory when a program runs out of registers in a general-register machine. Spilled operands may be written to memory using normal store instructions, and restored using normal load instructions, or spill and fill may use special-purpose storage and associated operations that are faster or offer other advantages over load and store.\n\nThe operands on the belt are read-only. New results do not overwrite prior values. The belt is thus a single-assignment structure, and is immune to the data hazards that must be dealt with by modern out-of-order general-register machines.\n\nDense machine code was very valuable in the 1960s, when main memory was very costly and limited, even on mainframe computers. It became important again on the initially-tiny memories of minicomputers, and then microprocessors. Density remains important today, for applications for smartphone, or downloaded into browsers over slow Internet connections, and in \"read-only memory\" (ROM) for embedded applications. A more general advantage of increased density is improved effectiveness of caches and instruction prefetch.\n\nBelt machines have smaller instructions than register-based machines, due to not needing a destination address for results. This saving can make a significant difference for fixed-length instruction formats, which normally use power-of-two instruction widths. If there are thirty-two addressable elements (registers on a general-register machine, belt positions on a belt machine), then each element address occupies five bits in the instruction, needing 15 bits for the three-address format of a general-register machine, but only 10 bits using the two-address format of a belt machine. Because bits are also needed for opcode and other information in the instruction, the (power-of-two constrained) instruction width often determines the maximum number of addressable elements possible in a design. Typically a belt machine instruction can support the encoding of double the number of addressable elements compared to a general-register machine of the same instruction width. There are similar gains in variable-length instruction encodings.\n\nOverall, belt machine code is less compact than for stack machines, which use no operand addresses, but often must introduce stack-manipulation instructions unneeded on a belt machine. The instructions for accumulator machines are not padded out with multiple register fields, instead, they use the return stack and need no extra memory reference instructions.\n\nWhile a belt machine presents an operand queue as the program model, there is not necessarily a physical queue (shift register) in the implemented hardware. Instead, a belt design may use an implementation analogous to the register renaming common in modern general-register machines. Live data values are kept in conveniently addressable physical resources (individual registers, register files, static random-access memory (SRAM), or operand forwarding from functional units) and generally not moved for the duration of their belt lifetime. Instruction decoder maps logical belt positions to physical locations. The mapping is updated to reflect the changes of logical position arising from newly dropped results.\n\nThe belt-machine architecture was created by startup Mill Computing, Inc., for use in their Mill architecture family of general-purpose CPUs.\n\n"}
{"id": "47843724", "url": "https://en.wikipedia.org/wiki?curid=47843724", "title": "Bounded expansion", "text": "Bounded expansion\n\nIn graph theory, a family of graphs is said to have bounded expansion if all of its shallow minors are sparse graphs. Many natural families of sparse graphs have bounded expansion. A closely related but stronger property, polynomial expansion, is equivalent to the existence of separator theorems for these families. Families with these properties have efficient algorithms for problems including the subgraph isomorphism problem and model checking for the first order theory of graphs.\n\nA \"t\"-shallow minor of a graph \"G\" is defined to be a graph formed from \"G\" by contracting a collection of vertex-disjoint subgraphs of radius \"t\", and deleting the remaining vertices of \"G\". A family of graphs has bounded expansion if there exists a function \"f\" such that, in every \"t\"-shallow minor of a graph in the family, the ratio of edges to vertices is at most \"f\"(\"t\").\n\nEquivalent definitions of classes of bounded expansions are that all shallow minors have chromatic number bounded by a function of \"t\", or that the given family has a bounded value of a \"topological parameter\". Such a parameter is a graph invariant that is monotone under taking subgraphs, such that the parameter value can change only in a controlled way when a graph is subdivided, and such that a bounded parameter value implies that a graph has bounded degeneracy.\n\nA stronger notion is polynomial expansion, meaning that the function \"f\" used to bound the edge density of shallow minors is a polynomial. If a hereditary graph family obeys a separator theorem, stating that any \"n\"-vertex graph in the family can be split into pieces with at most \"n\"/2 vertices by the removal of \"O\"(\"n\") vertices for some constant \"c\" < 1, then that family necessarily has polynomial expansion. Conversely, graphs with polynomial expansion have sublinear separator theorems.\n\nBecause of the connection between separators and expansion, every minor-closed graph family, including the family of planar graphs, has polynomial expansion. The same is true for 1-planar graphs, and more generally the graphs that can be embedded onto surfaces of bounded genus with a bounded number of crossings per edge, as well as the biclique-free string graphs, since these all obey similar separator theorems to the planar graphs. In higher dimensional Euclidean spaces, intersection graphs of systems of balls with the property that any point of space is covered by a bounded number of balls also obey separator theorems that imply polynomial expansion.\n\nAlthough graphs of bounded book thickness do not have sublinear separators, they also have bounded expansion. Other graphs of bounded expansion include graphs of bounded degree, random graphs of bounded average degree in the Erdős–Rényi model, and graphs of bounded queue number.\n\nInstances of the subgraph isomorphism problem in which the goal is to find a target graph of bounded size, as a subgraph of a larger graph whose size is not bounded, may be solved in linear time when the larger graph belongs to a family of graphs of bounded expansion. The same is true for finding cliques of a fixed size, finding dominating sets of a fixed size, or more generally testing properties that can be expressed by a formula of bounded size in the first-order logic of graphs.\n\nFor graphs of polynomial expansion, there exist polynomial-time approximation schemes for the set cover problem, maximum independent set problem, dominating set problem, and several other related graph optimization problems.\n"}
{"id": "838989", "url": "https://en.wikipedia.org/wiki?curid=838989", "title": "Code (cryptography)", "text": "Code (cryptography)\n\nCryptography in simple terms means the use of any alphabet or numerical statement which has a meaning or stores a message.\n\nIn cryptology, a code is a method used to encrypt a message that operates at the level of meaning; that is, words or phrases are converted into something else. A code might transform \"change\" into \"CVGDK\" or \"cocktail lounge\". A \"codebook\" is needed to encrypt, and decrypt the phrases or words.\n\nBy contrast, ciphers encrypt messages at the level of individual letters, or small groups of letters, or even, in modern ciphers, individual bits. Messages can be transformed first by a code, and then by a cipher. Such multiple encryption, or \"superencryption\" aims to make cryptanalysis more difficult.\n\nAnother comparison between codes and ciphers is that a code typically represents a letter or groups of letters directly without the use of mathematics. As such the numbers are configured to represent these three values: 1001 = A, 1002 = B, 1003 = C, ... . The resulting message, then would be 1001 1002 1003 to communicate ABC. Ciphers, however, utilize a mathematical formula to represent letters or groups of letters. For example, A = 1, B = 2, C = 3, ... . Thus the message ABC results by multiplying each letter's value by 13. The message ABC, then would be 13 26 39.\n\nCodes have a variety of drawbacks, including susceptibility to cryptanalysis and the difficulty of managing the cumbersome codebooks, so ciphers are now the dominant technique in modern cryptography.\n\nIn contrast, because codes are representational, they are not susceptible to mathematical analysis of the individual codebook elements. In our the example, the message 13 26 39 can be cracked by dividing each number by 13 and then ranking them alphabetically. However, the focus of codebook cryptanalysis is the comparative frequency of the individual code elements matching the same frequency of letters within the plaintext messages using frequency analysis. In the above example, the code group, 1001, 1002, 1003, might occur more than once and that frequency might match the number of times that ABC occurs in plain text messages.\n\n(In the past, or in non-technical contexts, \"code\" and \"cipher\" are often used to refer to any form of encryption).\n\nCodes are defined by \"codebooks\" (physical or notional), which are dictionaries of codegroups listed with their corresponding plaintext. Codes originally had the codegroups assigned in 'plaintext order' for convenience of the code designed, or the encoder. For example, in a code using numeric code groups, a plaintext word starting with \"a\" would have a low-value group, while one starting with \"z\" would have a high-value group. The same codebook could be used to \"encode\" a plaintext message into a coded message or \"codetext\", and \"decode\" a codetext back into plaintext message.\n\nIn order to make life more difficult for codebreakers, codemakers designed codes with no predictable relationship between the codegroups and the ordering of the matching plaintext. In practice, this meant that two codebooks were now required, one to find codegroups for encoding, the other to look up codegroups to find plaintext for decoding. Such \"two-part\" codes required more effort to develop, and twice as much effort to distribute (and discard safely when replaced), but they were harder to break. The Zimmermann Telegram in January 1917 used the German diplomatic \"0075\" two-part code system which contained upwards of 10,000 phrases and individual words.\n\nA one-time code is a prearranged word, phrase or symbol that is intended to be used only once to convey a simple message, often the signal to execute or abort some plan or confirm that it has succeeded or failed. One-time codes are often designed to be included in what would appear to be an innocent conversation. Done properly they are almost impossible to detect, though a trained analyst monitoring the communications of someone who has already aroused suspicion might be able to recognize a comment like \"Aunt Bertha has gone into labor\" as having an ominous meaning. Famous example of one time codes include:\n\n\nSometimes messages are not prearranged and rely on shared knowledge hopefully known only to the recipients. An example is the telegram sent to U.S. President Harry Truman, then at the Potsdam Conference to meet with Soviet premier Joseph Stalin, informing Truman of the first successful test of an atomic bomb. \n\n\"See also\" one-time pad, an unrelated cypher algorithm\n\nAn \"idiot code\" is a code that is created by the parties using it. This type of communication is akin to the hand signals used by armies in the field.\n\nExample: Any sentence where 'day' and 'night' are used means 'attack'. The location mentioned in the following sentence specifies the location to be attacked.\n\nAn early use of the term appears to be by George Perrault, a character in the science fiction book \"Friday\" by Robert A. Heinlein:\n\nTerrorism expert Magnus Ranstorp said that the men who carried out the September 11, 2001, attacks on the United States used basic e-mail and what he calls \"idiot code\" to discuss their plans.\nWhile solving a monoalphabetic substitution cipher is easy, solving even a simple code is difficult. Decrypting a coded message is a little like trying to translate a document written in a foreign language, with the task basically amounting to building up a \"dictionary\" of the codegroups and the plaintext words they represent.\n\nOne fingerhold on a simple code is the fact that some words are more common than others, such as \"the\" or \"a\" in English. In telegraphic messages, the codegroup for \"STOP\" (i.e., end of sentence or paragraph) is usually very common. This helps define the structure of the message in terms of sentences, if not their meaning, and this is cryptanalytically useful.\n\nFurther progress can be made against a code by collecting many codetexts encrypted with the same code and then using information from other sources \n\nFor example, a particular codegroup found almost exclusively in messages from a particular army and nowhere else might very well indicate the commander of that army. A codegroup that appears in messages preceding an attack on a particular location may very well stand for that location.\n\nCribs can be an immediate giveaway to the definitions of codegroups. As codegroups are determined, they can gradually build up a critical mass, with more and more codegroups revealed from context and educated guesswork. One-part codes are more vulnerable to such educated guesswork than two-part codes, since if the codenumber \"26839\" of a one-part code is determined to stand for \"bulldozer\", then the lower codenumber \"17598\" will likely stand for a plaintext word that starts with \"a\" or \"b\". At least, for simple one part codes.\n\nVarious tricks can be used to \"plant\" or \"sow\" information into a coded message, for example by executing a raid at a particular time and location against an enemy, and then examining code messages sent after the raid. Coding errors are a particularly useful fingerhold into a code; people reliably make errors, sometimes disastrous ones. Planting data and exploiting errors works against ciphers as well.\n\n\nConstructing a new code is like building a new language and writing a dictionary for it; it was an especially big job before computers. If a code is compromised, the entire task must be done all over again, and that means a lot of work for both cryptographers and the code users. In practice, when codes were in widespread use, they were usually changed on a periodic basis to frustrate codebreakers, and to limit the useful life of stolen or copied codebooks.\n\nOnce codes have been created, codebook distribution is logistically clumsy, and increases chances the code will be compromised. There is a saying that \"Three people can keep a secret if two of them are dead,\" () and though it may be something of an exaggeration, a secret becomes harder to keep if it is shared among several people. Codes can be thought reasonably secure if they are only used by a few careful people, but if whole armies use the same codebook, security becomes much more difficult.\n\nIn contrast, the security of ciphers is generally dependent on protecting the cipher keys. Cipher keys can be stolen and people can betray them, but they are much easier to change and distribute.\n\nIt was common to encipher a message after first encoding it, to increase the difficulty of cryptanalysis. With a numerical code, this was commonly done with an \"additive\" - simply a long key number which was digit-by-digit added to the code groups, modulo 10. Unlike the codebooks, additives would be changed frequently. The famous Japanese Navy code, JN-25, was of this design.\n\n\n"}
{"id": "302812", "url": "https://en.wikipedia.org/wiki?curid=302812", "title": "Color vision", "text": "Color vision\n\nColor vision is the ability of an organism or machine to distinguish objects based on the wavelengths (or frequencies) of the light they reflect, emit, or transmit. Colors can be measured and quantified in various ways; indeed, a person's perception of colors is a subjective process whereby the brain responds to the stimuli that are produced when incoming light reacts with the several types of cone cells in the eye. In essence, different people see the same illuminated object or light source in different ways.\n\nIsaac Newton discovered that white light, after being split into its component colours when passed through a dispersive prism, could be recombined to make white light by passing them through a different prism. \n\nThe characteristic colours are, from long to short wavelengths (and, correspondingly, from low to high frequency), red, orange, yellow, green, blue, and violet. Sufficient differences in wavelength cause a difference in the perceived hue; the just-noticeable difference in wavelength varies from about 1 nm in the blue-green and yellow wavelengths, to 10 nm and more in the longer red and shorter blue wavelengths. Although the human eye can distinguish up to a few hundred hues, when those pure spectral colors are mixed together or diluted with white light, the number of distinguishable chromaticities can be quite high.\n\nIn very low light levels, vision is scotopic: light is detected by rod cells of the retina. Rods are maximally sensitive to wavelengths near 500 nm, and play little, if any, role in colour vision. In brighter light, such as daylight, vision is photopic: light is detected by cone cells which are responsible for colour vision. Cones are sensitive to a range of wavelengths, but are most sensitive to wavelengths near 555 nm. Between these regions, mesopic vision comes into play and both rods and cones provide signals to the retinal ganglion cells. The shift in colour perception from dim light to daylight gives rise to differences known as the Purkinje effect\n\nThe perception of \"white\" is formed by the entire spectrum of visible light, or by mixing colours of just a few wavelengths in animals with few types of colour receptors. In humans, white light can be perceived by combining wavelengths such as red, green, and blue, or just a pair of complementary colours such as blue and yellow.\n\nPerception of color begins with specialized retinal cells containing pigments with different spectral sensitivities, known as cone cells. In humans, there are three types of cones sensitive to three different spectra, resulting in trichromatic color vision.\n\nEach individual cone contains pigments composed of opsin apoprotein, which is covalently linked to either 11-cis-hydroretinal or more rarely 11-cis-dehydroretinal.\n\nThe cones are conventionally labeled according to the ordering of the wavelengths of the peaks of their spectral sensitivities: short (S), medium (M), and long (L) cone types. These three types do not correspond well to particular colors as we know them. Rather, the perception of color is achieved by a complex process that starts with the differential output of these cells in the retina and it will be finalized in the visual cortex and associative areas of the brain.\n\nFor example, while the L cones have been referred to simply as red receptors, microspectrophotometry has shown that their peak sensitivity is in the greenish-yellow region of the spectrum. Similarly, the S- and M-cones do not directly correspond to blue and green, although they are often described as such. The RGB color model, therefore, is a convenient means for representing color, but is not directly based on the types of cones in the human eye.\n\nThe peak response of human cone cells varies, even among individuals with so-called normal color vision;\nin some non-human species this polymorphic variation is even greater, and it may well be adaptive.\n\nTwo complementary theories of color vision are the trichromatic theory and the opponent process theory. The trichromatic theory, or Young–Helmholtz theory, proposed in the 19th century by Thomas Young and Hermann von Helmholtz, as mentioned above, states that the retina's three types of cones are preferentially sensitive to blue, green, and red. Ewald Hering proposed the opponent process theory in 1872. It states that the visual system interprets color in an antagonistic way: red vs. green, blue vs. yellow, black vs. white. Both theories are now accepted as valid, describing different stages in visual physiology, visualized in the adjacent diagram. Green←→Magenta and Blue←→Yellow are scales with mutually exclusive boundaries. In the same way that there cannot exist a \"slightly negative\" positive number, a single eye cannot perceive a bluish-yellow or a reddish-green.\n\nA range of wavelengths of light stimulates each of these receptor types to varying degrees. Yellowish-green light, for example, stimulates both L and M cones equally strongly, but only stimulates S-cones weakly. Red light, on the other hand, stimulates L cones much more than M cones, and S cones hardly at all; blue-green light stimulates M cones more than L cones, and S cones a bit more strongly, and is also the peak stimulant for rod cells; and blue light stimulates S cones more strongly than red or green light, but L and M cones more weakly. The brain combines the information from each type of receptor to give rise to different perceptions of different wavelengths of light.\n\nThe opsins (photopigments) present in the L and M cones are encoded on the X chromosome; defective encoding of these leads to the two most common forms of color blindness. The OPN1LW gene, which codes for the opsin present in the L cones, is highly polymorphic (a recent study by Verrelli and Tishkoff found 85 variants in a sample of 236 men). A very small percentage of women may have an extra type of color receptor because they have different alleles for the gene for the L opsin on each X chromosome. X chromosome inactivation means that while only one opsin is expressed in each cone cell, both types occur overall, and some women may therefore show a degree of tetrachromatic color vision. Variations in OPN1MW, which codes the opsin expressed in M cones, appear to be rare, and the observed variants have no effect on spectral sensitivity.\n\nColor processing begins at a very early level in the visual system (even within the retina) through initial color opponent mechanisms. Both Helmholtz's trichromatic theory, and Hering's opponent process theory are therefore correct, but trichromacy arises at the level of the receptors, and opponent processes arise at the level of retinal ganglion cells and beyond. In Hering's theory opponent mechanisms refer to the opposing color effect of red–green, blue–yellow, and light–dark. However, in the visual system, it is the activity of the different receptor types that are opposed. Some midget retinal ganglion cells oppose L and M cone activity, which corresponds loosely to red–green opponency, but actually runs along an axis from blue-green to magenta. Small bistratified retinal ganglion cells oppose input from the S cones to input from the L and M cones. This is often thought to correspond to blue–yellow opponency, but actually runs along a color axis from yellow-green to violet.\n\nVisual information is then sent to the brain from retinal ganglion cells via the optic nerve to the optic chiasma: a point where the two optic nerves meet and information from the temporal (contralateral) visual field crosses to the other side of the brain. After the optic chiasma the visual tracts are referred to as the optic tracts, which enter the thalamus to synapse at the lateral geniculate nucleus (LGN).\n\nThe lateral geniculate nucleus is divided into laminae (zones), of which there are three types: the M-laminae, consisting primarily of M-cells, the P-laminae, consisting primarily of P-cells, and the koniocellular laminae. M- and P-cells receive relatively balanced input from both L- and M-cones throughout most of the retina, although this seems to not be the case at the fovea, with midget cells synapsing in the P-laminae. The koniocellular laminae receive axons from the small bistratified ganglion cells.\n\nAfter synapsing at the LGN, the visual tract continues on back to the primary visual cortex (V1) located at the back of the brain within the occipital lobe. Within V1 there is a distinct band (striation). This is also referred to as \"striate cortex\", with other cortical visual regions referred to collectively as \"extrastriate cortex\". It is at this stage that color processing becomes much more complicated.\n\nIn V1 the simple three-color segregation begins to break down. Many cells in V1 respond to some parts of the spectrum better than others, but this \"color tuning\" is often different depending on the adaptation state of the visual system. A given cell that might respond best to long wavelength light if the light is relatively bright might then become responsive to all wavelengths if the stimulus is relatively dim. Because the color tuning of these cells is not stable, some believe that a different, relatively small, population of neurons in V1 is responsible for color vision. These specialized \"color cells\" often have receptive fields that can compute local cone ratios. Such \"double-opponent\" cells were initially described in the goldfish retina by Nigel Daw; their existence in primates was suggested by David H. Hubel and Torsten Wiesel and subsequently proven by Bevil Conway. As Margaret Livingstone and David Hubel showed, double opponent cells are clustered within localized regions of V1 called blobs, and are thought to come in two flavors, red–green and blue–yellow. Red–green cells compare the relative amounts of red–green in one part of a scene with the amount of red–green in an adjacent part of the scene, responding best to local color contrast (red next to green). Modeling studies have shown that double-opponent cells are ideal candidates for the neural machinery of color constancy explained by Edwin H. Land in his retinex theory.\nFrom the V1 blobs, color information is sent to cells in the second visual area, V2. The cells in V2 that are most strongly color tuned are clustered in the \"thin stripes\" that, like the blobs in V1, stain for the enzyme cytochrome oxidase (separating the thin stripes are interstripes and thick stripes, which seem to be concerned with other visual information like motion and high-resolution form). Neurons in V2 then synapse onto cells in the extended V4. This area includes not only V4, but two other areas in the posterior inferior temporal cortex, anterior to area V3, the dorsal posterior inferior temporal cortex, and posterior TEO. Area V4 was initially suggested by Semir Zeki to be exclusively dedicated to color, but this is now thought to be incorrect. In particular, the presence in V4 of orientation-selective cells led to the view that V4 is involved in processing both color and form associated with color. Color processing in the extended V4 occurs in millimeter-sized color modules called globs. This is the first part of the brain in which color is processed in terms of the full range of hues found in color space.\n\nAnatomical studies have shown that neurons in extended V4 provide input to the inferior temporal lobe . \"IT\" cortex is thought to integrate color information with shape and form, although it has been difficult to define the appropriate criteria for this claim. Despite this murkiness, it has been useful to characterize this pathway (V1 > V2 > V4 > IT) as the ventral stream or the \"what pathway\", distinguished from the dorsal stream (\"where pathway\") that is thought to analyze motion, among many other features.\n\nNothing categorically distinguishes the visible spectrum of electromagnetic radiation from invisible portions of the broader spectrum. In this sense, color is not a property of electromagnetic radiation, but a feature of visual perception by an observer. Furthermore, there is an arbitrary mapping between wavelengths of light in the visual spectrum and human experiences of color. Although most people are assumed to have the same mapping, the philosopher John Locke recognized that alternatives are possible, and described one such hypothetical case with the \"inverted spectrum\" thought experiment. For example, someone with an inverted spectrum might experience green while seeing 'red' (700 nm) light, and experience red while seeing 'green' (530 nm) light. Synesthesia (or ideasthesia) provides some atypical but illuminating examples of subjective color experience triggered by input that is not even light, such as sounds or shapes. The possibility of a clean dissociation between color experience from properties of the world reveals that color is a subjective psychological phenomenon.\n\nThe Himba people have been found to categorize colors differently from most Euro-Americans and are able to easily distinguish close shades of green, barely discernible for most people. The Himba have created a very different color scheme which divides the spectrum to dark shades (\"zuzu\" in Himba), very light (\"vapa\"), vivid blue and green (\"buru\") and dry colors as an adaptation to their specific way of life.\n\nPerception of color depends heavily on the context in which the perceived object is presented. For example, a white page under blue, pink, or purple light will reflect mostly blue, pink, or purple light to the eye, respectively; the brain, however, compensates for the effect of lighting (based on the color shift of surrounding objects) and is more likely to interpret the page as white under all three conditions, a phenomenon known as color constancy.\n\nMany species can see light with frequencies outside the human \"visible spectrum\". Bees and many other insects can detect ultraviolet light, which helps them to find nectar in flowers. Plant species that depend on insect pollination may owe reproductive success to ultraviolet \"colors\" and patterns rather than how colorful they appear to humans. Birds, too, can see into the ultraviolet (300–400 nm), and some have sex-dependent markings on their plumage that are visible only in the ultraviolet range. Many animals that can see into the ultraviolet range, however, cannot see red light or any other reddish wavelengths. For example, bees' visible spectrum ends at about 590 nm, just before the orange wavelengths start. Birds, however, can see some red wavelengths, although not as far into the light spectrum as humans. It is an incorrect popular belief that the common goldfish is the only animal that can see both infrared and ultraviolet light; their color vision extends into the ultraviolet but not the infrared.\n\nThe basis for this variation is the number of cone types that differ between species. Mammals in general have color vision of a limited type, and usually have red-green color blindness, with only two types of cones. Humans, some primates, and some marsupials see an extended range of colors, but only by comparison with other mammals. Most non-mammalian vertebrate species distinguish different colors at least as well as humans, and many species of birds, fish, reptiles and amphibians, and some invertebrates, have more than three cone types and probably superior color vision to humans.\n\nIn most Catarrhini (Old World monkeys and apes—primates closely related to humans) there are three types of color receptors (known as cone cells), resulting in trichromatic color vision. These primates, like humans, are known as trichromats. Many other primates (including New World monkeys) and other mammals are dichromats, which is the general color vision state for mammals that are active during the day (i.e., felines, canines, ungulates). Nocturnal mammals may have little or no color vision. Trichromat non-primate mammals are rare.\n\nMany invertebrates have color vision. Honeybees and bumblebees have trichromatic color vision which is insensitive to red but sensitive to ultraviolet. \"Osmia rufa\", for example, possess a trichromatic color system, which they use in foraging for pollen from flowers. In view of the importance of color vision to bees one might expect these receptor sensitivities to reflect their specific visual ecology; for example the types of flowers that they visit. However, the main groups of hymenopteran insects excluding ants (i.e., bees, wasps and sawflies) mostly have three types of photoreceptor, with spectral sensitivities similar to the honeybee's.\" Papilio\" butterflies possess six types of photoreceptors and may have pentachromatic vision. The most complex color vision system in the animal kingdom has been found in stomatopods (such as the mantis shrimp) with up to 12 spectral receptor types thought to work as multiple dichromatic units.\n\nVertebrate animals such as tropical fish and birds sometimes have more complex color vision systems than humans; thus the many subtle colors they exhibit generally serve as direct signals for other fish or birds, and not to signal mammals. In bird vision, tetrachromacy is achieved through up to four cone types, depending on species. Each single cone contains one of the four main types of vertebrate cone photopigment (LWS/ MWS, RH2, SWS2 and SWS1) and has a colored oil droplet in its inner segment. Brightly colored oil droplets inside the cones shift or narrow the spectral sensitivity of the cell. It has been suggested that it is likely that pigeons are pentachromats.\n\nReptiles and amphibians also have four cone types (occasionally five), and probably see at least the same number of colors that humans do, or perhaps more. In addition, some nocturnal geckos have the capability of seeing color in dim light.\n\nIn the evolution of mammals, segments of color vision were lost, then for a few species of primates, regained by gene duplication. Eutherian mammals other than primates (for example, dogs, mammalian farm animals) generally have less-effective two-receptor (dichromatic) color perception systems, which distinguish blue, green, and yellow—but cannot distinguish oranges and reds. There is some evidence that a few mammals, such as cats, have redeveloped the ability to distinguish longer wavelength colors, in at least a limited way, via one-amino-acid mutations in opsin genes. The adaptation to see reds is particularly important for primate mammals, since it leads to identification of fruits, and also newly sprouting reddish leaves, which are particularly nutritious.\n\nHowever, even among primates, full color vision differs between New World and Old World monkeys. Old World primates, including monkeys and all apes, have vision similar to humans. New World monkeys may or may not have color sensitivity at this level: in most species, males are dichromats, and about 60% of females are trichromats, but the owl monkeys are cone monochromats, and both sexes of howler monkeys are trichromats. Visual sensitivity differences between males and females in a single species is due to the gene for yellow-green sensitive opsin protein (which confers ability to differentiate red from green) residing on the X sex chromosome.\n\nSeveral marsupials such as the fat-tailed dunnart (\"Sminthopsis crassicaudata\") have been shown to have trichromatic color vision.\n\nMarine mammals, adapted for low-light vision, have only a single cone type and are thus monochromats.\n\nColor perception mechanisms are highly dependent on evolutionary factors, of which the most prominent is thought to be satisfactory recognition of food sources. In herbivorous primates, color perception is essential for finding proper (immature) leaves. In hummingbirds, particular flower types are often recognized by color as well. On the other hand, nocturnal mammals have less-developed color vision, since adequate light is needed for cones to function properly. There is evidence that ultraviolet light plays a part in color perception in many branches of the animal kingdom, especially insects. In general, the optical spectrum encompasses the most common electronic transitions in matter and is therefore the most useful for collecting information about the environment.\n\nThe evolution of trichromatic color vision in primates occurred as the ancestors of modern monkeys, apes, and humans switched to diurnal (daytime) activity and began consuming fruits and leaves from flowering plants. Color vision, with UV discrimination, is also present in a number of arthropods—the only terrestrial animals besides the vertebrates to possess this trait.\n\nSome animals can distinguish colors in the ultraviolet spectrum. The UV spectrum falls outside the human visible range, except for some cataract surgery patients. Birds, turtles, lizards, many fish and some rodents have UV receptors in their retinas. These animals can see the UV patterns found on flowers and other wildlife that are otherwise invisible to the human eye.\n\nUltraviolet vision is an especially important adaptation in birds. It allows birds to spot small prey from a distance, navigate, avoid predators, and forage while flying at high speeds. Birds also utilize their broad spectrum vision to recognize other birds, and in sexual selection.\n\nA \"physical color\" is a combination of pure spectral colors (in the visible range). Since there are, in principle, infinitely many distinct spectral colors, the set of all physical colors may be thought of as an infinite-dimensional vector space, in fact a Hilbert space. We call this space \"H\". More technically, the space of physical colors may be considered to be the (mathematical) cone over the simplex whose vertices are the spectral colors, with white at the centroid of the simplex, black at the apex of the cone, and the monochromatic color associated with any given vertex somewhere along the line from that vertex to the apex depending on its brightness.\n\nAn element \"C\" of \"H\" is a function from the range of visible wavelengths—considered as an interval of real numbers [\"W\",\"W\"]—to the real numbers, assigning to each wavelength \"w\" in [\"W\",\"W\"] its intensity \"C\"(\"w\").\n\nA humanly perceived color may be modeled as three numbers: the extents to which each of the 3 types of cones is stimulated. Thus a humanly perceived color may be thought of as a point in 3-dimensional Euclidean space. We call this space R.\n\nSince each wavelength \"w\" stimulates each of the 3 types of cone cells to a known extent, these extents may be represented by 3 functions \"s\"(\"w\"), \"m\"(\"w\"), \"l\"(\"w\") corresponding to the response of the \"S\", \"M\", and \"L\" cone cells, respectively.\n\nFinally, since a beam of light can be composed of many different wavelengths, to determine the extent to which a physical color \"C\" in \"H\" stimulates each cone cell, we must calculate the integral (with respect to \"w\"), over the interval [\"W\",\"W\"], of \"C\"(\"w\")·\"s\"(\"w\"), of \"C\"(\"w\")·\"m\"(\"w\"), and of \"C\"(\"w\")·\"l\"(\"w\"). The triple of resulting numbers associates with each physical color \"C\" (which is an element in \"H\") a particular perceived color (which is a single point in R). This association is easily seen to be linear. It may also easily be seen that many different elements in the \"physical\" space \"H\" can all result in the same single perceived color in R, so a perceived color is not unique to one physical color.\n\nThus human color perception is determined by a specific, non-unique linear mapping from the infinite-dimensional Hilbert space \"H\" to the 3-dimensional Euclidean space R.\n\nTechnically, the image of the (mathematical) cone over the simplex whose vertices are the spectral colors, by this linear mapping, is also a (mathematical) cone in R. Moving directly away from the vertex of this cone represents maintaining the same chromaticity while increasing its intensity. Taking a cross-section of this cone yields a 2D chromaticity space. Both the 3D cone and its projection or cross-section are convex sets; that is, any mixture of spectral colors is also a color.\n\nIn practice, it would be quite difficult to physiologically measure an individual's three cone responses to various physical color stimuli. Instead, a psychophysical approach is taken. Three specific benchmark test lights are typically used; let us call them \"S\", \"M\", and \"L\". To calibrate human perceptual space, scientists allowed human subjects to try to match any physical color by turning dials to create specific combinations of intensities (\"I\", \"I\", \"I\") for the \"S\", \"M\", and \"L\" lights, resp., until a match was found. This needed only to be done for physical colors that are spectral, since a linear combination of spectral colors will be matched by the same linear combination of their (\"I\", \"I\", \"I\") matches. Note that in practice, often at least one of \"S\", \"M\", \"L\" would have to be added with some intensity to the \"physical test color\", and that combination matched by a linear combination of the remaining 2 lights. Across different individuals (without color blindness), the matchings turned out to be nearly identical.\n\nBy considering all the resulting combinations of intensities (\"I\", \"I\", \"I\") as a subset of 3-space, a model for human perceptual color space is formed. (Note that when one of \"S\", \"M\", \"L\" had to be added to the test color, its intensity was counted as negative.) Again, this turns out to be a (mathematical) cone, not a quadric, but rather all rays through the origin in 3-space passing through a certain convex set. Again, this cone has the property that moving directly away from the origin corresponds to increasing the intensity of the \"S\", \"M\", \"L\" lights proportionately. Again, a cross-section of this cone is a planar shape that is (by definition) the space of \"chromaticities\" (informally: distinct colors); one particular such cross section, corresponding to constant \"X\"+\"Y\"+\"Z\" of the CIE 1931 color space, gives the CIE chromaticity diagram.\n\nThis system implies that for any hue or non-spectral color not on the boundary of the chromaticity diagram, there are infinitely many distinct physical spectra that are all perceived as that hue or color. So, in general there is no such thing as \"the\" combination of spectral colors that we perceive as (say) a specific version of tan; instead there are infinitely many possibilities that produce that exact color. The boundary colors that are pure spectral colors can be perceived only in response to light that is purely at the associated wavelength, while the boundary colors on the \"line of purples\" can each only be generated by a specific ratio of the pure violet and the pure red at the ends of the visible spectral colors.\n\nThe CIE chromaticity diagram is horseshoe-shaped, with its curved edge corresponding to all spectral colors (the \"spectral locus\"), and the remaining straight edge corresponding to the most saturated purples, mixtures of red and violet.\n\nIn color science, chromatic adaptation is the estimation of the representation of an object under a different light source from the one in which it was recorded. A common application is to find a \"chromatic adaptation transform\" (CAT) that will make the recording of a neutral object appear neutral (color balance), while keeping other colors also looking realistic. For example, chromatic adaptation transforms are used when converting images between ICC profiles with different white points. Adobe Photoshop, for example, uses the Bradford CAT.\n\nIn color vision, chromatic adaptation refers to color constancy; the ability of the visual system to preserve the appearance of an object under a wide range of light sources.\n\n\n"}
{"id": "11489119", "url": "https://en.wikipedia.org/wiki?curid=11489119", "title": "Conditional change model", "text": "Conditional change model\n\nThe conditional change model in statistics is the analytic procedure in which change scores are regressed on baseline values, together with the explanatory variables of interest (often including indicators of treatment groups). The method has some substantial advantages over the usual two-sample t-test recommended in textbooks.\n\n"}
{"id": "33687518", "url": "https://en.wikipedia.org/wiki?curid=33687518", "title": "Convexity (finance)", "text": "Convexity (finance)\n\nIn mathematical finance, convexity refers to non-linearities in a financial model. In other words, if the price of an underlying variable changes, the price of an output does not change linearly, but depends on the second derivative (or, loosely speaking, higher-order terms) of the modeling function. Geometrically, the model is no longer flat but curved, and the degree of curvature is called the convexity.\n\nStrictly speaking, convexity refers to the second derivative of output price with respect to an input price. In derivative pricing, this is referred to as Gamma (Γ), one of the Greeks. In practice the most significant of these is bond convexity, the second derivative of bond price with respect to interest rates.\n\nAs the second derivative is the first non-linear term, and thus often the most significant, \"convexity\" is also used loosely to refer to non-linearities generally, including higher-order terms. Refining a model to account for non-linearities is referred to as a convexity correction.\n\nFormally, the convexity adjustment arises from the Jensen inequality in probability theory: the expected value of a convex function is greater than or equal to the function of the expected value:\nGeometrically, if the model price curves up on both sides of the present value (the payoff function is convex up, and is \"above\" a tangent line at that point), then if the price of the underlying changes, the price of the output is \"greater\" than is modeled using only the first derivative. Conversely, if the model price curves down (the convexity is \"negative,\" the payoff function is \"below\" the tangent line), the price of the output is \"lower\" than is modeled using only the first derivative.\n\nThe precise convexity adjustment depends on the model of future price movements of the underlying (the probability distribution) and on the model of the price, though it is linear in the convexity (second derivative of the price function).\n\nThe convexity can be used to interpret derivative pricing: mathematically, convexity is optionality – the price of an option (the value of optionality) corresponds to the convexity of the underlying payout.\n\nIn Black–Scholes pricing of options, omitting interest rates and the first derivative, the Black–Scholes equation reduces to formula_2 \"(infinitesimally) the time value is the convexity\". That is, the value of an option is due to the convexity of the ultimate payout: one has the \"option\" to buy an asset or not (in a call; for a put it is an option to sell), and the ultimate payout function (a hockey stick shape) is convex – \"optionality\" corresponds to convexity in the payout. Thus, if one purchases a call option, the expected value of the option is \"higher\" than simply taking the expected future value of the underlying and inputting it into the option payout function: the expected value of a convex function is higher than the function of the expected value (Jensen inequality). The price of the option – the value of the optionality – thus reflects the convexity of the payoff function.\n\nThis value is isolated via a straddle – purchasing an at-the-money straddle (whose value increases if the price of the underlying increases or decreases) has (initially) no delta: one is simply purchasing convexity (optionality), without taking a position on the underlying asset – one benefits from the \"degree\" of movement, not the \"direction\".\n\nFrom the point of view of risk management, being long convexity (having positive Gamma and hence (ignoring interest rates and Delta) negative Theta) means that one benefits from volatility (positive Gamma), but loses money over time (negative Theta) – one net profits if prices move \"more\" than expected, and net lose if prices move \"less\" than expected.\n\nFrom a modeling perspective, convexity adjustments arise every time the underlying financial variables modeled are not martingale under the pricing measure. Applying Girsanov theorem allows expressing the dynamics of the modeled financial variables under the pricing measure and therefore estimating this convexity adjustment. Typical examples of convexity adjustments include:\n\n"}
{"id": "1508434", "url": "https://en.wikipedia.org/wiki?curid=1508434", "title": "Coplanarity", "text": "Coplanarity\n\nIn geometry, a set of points in space are coplanar if there exists a geometric plane that contains them all. For example, three points are always coplanar, and if the points are distinct and non-collinear, the plane they determine is unique. However, a set of four or more distinct points will, in general, not lie in a single plane.\n\nTwo lines in three-dimensional space are coplanar if there is a plane that includes them both. This occurs if the lines are parallel, or if they intersect each other. Two lines that are not coplanar are called skew lines.\n\nDistance geometry provides a solution technique for the problem of determining whether a set of points is coplanar, knowing only the distances between them.\n\nIn three-dimensional space, two linearly independent vectors with the same initial point determine a plane through that point. Their cross product is a normal vector to that plane, and any vector orthogonal to this cross product through the initial point will lie in the plane. This leads to the following coplanarity test using a scalar triple product. Four distinct points, and are coplanar if and only if,\nwhich is equivalent to\nIf three vectors and are coplanar, then if (i.e., and are orthogonal) then\n\nwhere formula_4 denotes the unit vector in the direction of . That is, the vector projections of on and on add to give the original .\n\nSince three or fewer points are always coplanar, the problem of determining when a set of points are coplanar is generally of interest only when there are at least four points involved. In the case that there are exactly four points, several ad hoc methods can be employed, but a general method that works for any number of points uses vector methods and the property that a plane is determined by two linearly independent vectors.\n\nIn an -dimensional space (), a set of points, are coplanar if and only if the matrix of their relative differences, that is, the matrix whose columns (or rows) are the vectors formula_5 is of rank 2 or less. \n\nFor example, given four points, , and , if the matrix\nis of rank 2 or less, the four points are coplanar.\n\nIn the special case of a plane that contains the origin, the property can be simplified in the following way:\nA set of points and the origin are coplanar if and only if the matrix of the coordinates of the points is of rank 2 or less.\n\nA skew polygon is a polygon whose vertices are not coplanar. Such a polygon must have at least four vertices; there are no skew triangles.\n\nA polyhedron that has positive volume has vertices that are not all coplanar.\n\n"}
{"id": "2606696", "url": "https://en.wikipedia.org/wiki?curid=2606696", "title": "Cylindrification", "text": "Cylindrification\n\nIn computability theory a cylindrification is a construction that associates a cylindric numbering to each numbering. The concept was first introduced by Yuri L. Ershov in 1973.\n\nGiven a numbering formula_1 the cylindrification formula_2 is defined as\nwhere formula_5 is the Cantor pairing function.\nThe cylindrification operation takes a relation as input of arity k and outputs a relation of arity k + 1 as follows : Given a relation R of arity K, its cylindrification denoted by c(R), is the following set {(a1...,ak,a)|(a1...,ak)belongs to R and a belongs to A}. Note that the cylindrification operation increases the arity of an input by 1.\n\n\n"}
{"id": "1643924", "url": "https://en.wikipedia.org/wiki?curid=1643924", "title": "Einar Hille", "text": "Einar Hille\n\nCarl Einar Hille (28 June 1894 – 12 February 1980) was an American mathematics professor and scholar. Hille authored or coauthored twelve mathematical books and a number of mathematical papers\nHille was born in New York City. His parents were both immigrants from Sweden who separated before his birth. His father, Carl August Heuman, was a civil engineer. He was brought up by his mother, Edla Eckman, who took the surname Hille. When Einar was two years old, he and his mother returned to Stockholm. Hille spent the next 24 years of his life in Sweden, returning to the United States when he was 26 years old. Hille entered the University of Stockholm in 1911. Hille was awarded his first degree in mathematics in 1913 and the equivalent of a master's degree in the following year. He received a Ph.D. from Stockholm in 1918 for a doctoral dissertation entitled \"Some Problems Concerning Spherical Harmonics\".\n\nIn 1919 Hille was awarded the Mittag-Leffler Prize and was given the right to teach at the University of Stockholm. He subsequently taught at Harvard University, Princeton University, Stanford University and the University of Chicago. In 1933, he became an endowed professor on mathematics in the Graduate School of Yale University.\n\nHille's main work was on integral equations, differential equations, special functions, Dirichlet series and Fourier series. Later in his career his interests turned more towards functional analysis. His name persists among others in the Hille–Yosida theorem. Hille was a member of the London Mathematical Society and the Circolo Matematico di Palermo. Hille served as president of the American Mathematical Society (1937–38) and was the Society's Colloquium lecturer in 1944. He received many honours including election to the United States National Academy of Sciences (1953) and the Swedish Royal Academy of Sciences. He was awarded by Sweden with the Order of the Polar Star.\n\nHille was married to Kirsti Ore Hille (1906–2001), sister of Norwegian mathematician Øystein Ore. They had two sons, Harald and Bertil Hille.\n\n\n"}
{"id": "35338559", "url": "https://en.wikipedia.org/wiki?curid=35338559", "title": "Fatou conjecture", "text": "Fatou conjecture\n\nIn mathematics, the Fatou conjecture, named after Pierre Fatou, states that a quadratic family of maps from the complex plane to itself is hyperbolic for an open dense set of parameters.\n"}
{"id": "26402563", "url": "https://en.wikipedia.org/wiki?curid=26402563", "title": "Foundations of Algebraic Geometry", "text": "Foundations of Algebraic Geometry\n\nFoundations of Algebraic Geometry is a book by that develops algebraic geometry over fields of any characteristic. In particular it gives a careful treatment of intersection theory by defining the local intersection multiplicity of two subvarieties.\n\nWeil was motivated by the need for a rigorous theory of correspondences on algebraic curves in positive characteristic, which he used in his proof of the Riemann hypothesis for curves over a finite field.\n\nWeil introduced abstract rather than projective varieties partly so that he could construct the Jacobian of a curve. (It was not known at the time that Jacobians are always projective varieties.) It was some time before anyone found any examples of complete abstract varieties that are not projective. \nIn the 1950s Weil's work was one of several competing attempts to provide satisfactory foundations for algebraic geometry, all of which were superseded by Grothendieck's development of schemes.\n\n\n"}
{"id": "186969", "url": "https://en.wikipedia.org/wiki?curid=186969", "title": "Hall's marriage theorem", "text": "Hall's marriage theorem\n\nHall's marriage theorem, proved by , is a theorem with two equivalent formulations:\n\nLet formula_1 be a (possibly infinite) family of finite subsets of formula_2, where the members of formula_1 are counted with multiplicity. (That is, formula_1 may contain the same set several times.)\n\nA transversal for formula_1 is the image of an injective function formula_6 from formula_1 to formula_2 such that formula_9 is an element of the set formula_10 for every formula_10 in the family formula_1. In other words, formula_6 selects one representative from each set in formula_1 in such a way that no two sets from formula_1 get the same representative. An alternative term for \"transversal\" is \"system of distinct representatives\".\n\nThe collection \"S\" satisfies the marriage condition when for each subfamily formula_16,\n\nRestated in words, the marriage condition asserts that every subfamily formula_18 of formula_1 covers at least formula_20 different members of formula_2.\n\nIf the marriage condition fails then there cannot be a transversal formula_6 of formula_1.\nSuppose that the marriage condition fails, i.e., that for some subcollection formula_24 of formula_1, formula_26 Suppose, by way of contradiction, that a transversal formula_27 of formula_1 also exists.\n\nThe restriction of formula_6 to the offending subcollection formula_24 would be an injective function from formula_24 into formula_32. This is impossible by the pigeonhole principle since formula_33. Therefore no transversal can exist if the marriage condition fails.\nHall's theorem states that the converse is also true:\n\nExample 1: Consider \"S\" = {\"A\", \"A\", \"A\"} with\n\nA valid transversal would be (1, 4, 5). (Note this is not unique: (2, 1, 3) works equally well, for example.)\n\nExample 2: Consider \"S\" = {\"A\", \"A\", \"A\", \"A\"} with\n\nNo valid transversal exists; the marriage condition is violated as is shown by the subcollection {\"A\", \"A\", \"A\"}.\n\nExample 3: Consider \"S\"= {\"A\", \"A\", \"A\", \"A\"} with\n\nThe only valid transversals are (c, b, a, d) and (c, d, a, b).\n\nThe standard example of an application of the marriage theorem is to imagine two groups; one of \"n\" men, and one of \"n\" women. For each woman, there is a subset of the men, any one of which she would happily marry; and any man would be happy to marry a woman who wants to marry him. Consider whether it is possible to pair up (in marriage) the men and women so that every person is happy.\n\nIf we let \"A\" be the set of men that the \"i\"-th woman would be happy to marry, then the marriage theorem states that each woman can happily marry a man if and only if the collection of sets {\"A\"} meets the marriage condition.\n\nNote that the marriage condition is that, for any subset formula_34 of the women, the number of men whom at least one of the women would be happy to marry, formula_35, be at least as big as the number of women in that subset, formula_36. It is obvious that this condition is \"necessary\", as if it does not hold, there are not enough men to share among the formula_34 women. What is interesting is that it is also a \"sufficient\" condition.\n\nLet \"G\" be a finite bipartite graph with bipartite sets \"X\" and \"Y\" ( \"G\":= (\"X\" + \"Y\", \"E\")). For a set \"W\" of vertices in \"X\", let formula_38 denote the neighborhood of \"W\" in \"G\", i.e. the set of all vertices in \"Y\" adjacent to some element of \"W\". The marriage theorem in this formulation states that there is a matching that entirely covers \"X\" if and only if for every subset \"W\" of \"X\":\nIn other words: every subset \"W\" of \"X\" has sufficiently many adjacent vertices in \"Y\".\n\nGiven a finite bipartite graph \"G\":= (\"X\" + \"Y\", \"E\"), with bipartite sets \"X\" and \"Y\" of equal size, the marriage theorem provides necessary and sufficient conditions for the existence of a perfect matching in the graph.\n\nA generalization to general graphs (that are not necessarily bipartite) is provided by the Tutte theorem.\n\nAn \"X\"-saturating matching is a matching which covers every vertex in \"X\".\n\nWe first prove: If a bipartite graph \"G\" = (\"X\" + \"Y\", \"E\") = \"G\"(\"X\", \"Y\") has an \"X\"-saturating matching, then |N(\"W\")| ≥ |\"W\"| for all \"W\" ⊆ \"X\".\n\nSuppose M is a matching that saturates every vertex of \"X\". \nLet the set of all vertices in \"Y\" matched by M to a given \"W\" be denoted as M(\"W\"). Therefore, |M(\"W\")|=|\"W\"|, by the definition of matching.\nBut M(\"W\") ⊆ N(\"W\"), since all elements of M(\"W\") are neighbours of \"W\".\nSo, |N(\"W\")| ≥ |M(\"W\")| and hence, |N(\"W\")| ≥ |\"W\"|.\n\nNow we prove: If |N(\"W\")| ≥ |\"W\"| for all \"W\" ⊆ X, then \"G\"(\"X\",\"Y\") has a matching that saturates every vertex in \"X\".\n\nAssume for contradiction that \"G\"(\"X\",\"Y\") is a bipartite graph that has no matching that saturates all vertices of \"X\".\nLet \"M\" be a maximum matching, and \"u\" a vertex not saturated by M. Consider all \"alternating paths\" (i.e., paths in \"G\" alternately using edges outside and inside M) starting from \"u\". Let the set of all points in \"Y\" connected to \"u\" by these alternating paths be \"T\", and the set of all points in \"X\" connected to \"u\" by these alternating paths (including \"u\" itself) be \"W\".\nNo maximal alternating path can end in a vertex in \"Y\", lest it would be an \"augmenting path\", so that we could augment M to a strictly larger matching by toggling the status (belongs to \"M\" or not) of all the edges of the path. Thus every vertex in \"T\" is matched by M to a vertex in \"W\" \\ {\"u\"}. Conversely, every vertex \"v\" in \"W\" \\ {\"u\"} is matched by M to a vertex in \"T\" (namely, the vertex preceding \"v\" on an alternating path ending at \"v\"). Thus, M provides a bijection of \"W\" \\ {\"u\"} and \"T\", which implies |\"W\"| = |\"T\"| + 1. On the other hand, N(\"W\") ⊆ \"T\": let \"v\" in \"Y\" be connected to a vertex \"w\" in \"W\". If the edge (\"w\",\"v\") is in M, then \"v\" is in \"T\" by the previous part of the proof, otherwise we can take an alternating path ending in \"w\" and extend it with \"v\", getting an augmenting path and showing that \"v\" is in \"T\". Hence, |N(\"W\")| <= |\"T\"| = |\"W\"| − 1<|\"W\"|, a contradiction.\n\nLet \"S\" = (\"A\", \"A\", ..., \"A\") where the \"A\" are finite sets which need not be distinct. Let the set \"X\" = {\"A\", \"A\", ..., \"A\"} (that is, the set of names of the elements of \"S\") and the set \"Y\" be the union of all the elements in all the \"A\". \n\nWe form a finite bipartite graph \"G\":= (\"X\" + \"Y\", \"E\"), with bipartite sets \"X\" and \"Y\" by joining any element in \"Y\" to each \"A\" which it is a member of. A transversal of \"S\" is an \"X\"-saturating matching (a matching which covers every vertex in \"X\") of the bipartite graph \"G\". Thus a problem in the combinatorial formulation can be easily translated to a problem in the graph-theoretic formulation.\n\nBy examining Philip Hall's original proof carefully, Marshall Hall, Jr. (no relation to Philip Hall) was able to tweak the result in a way that permitted the proof to work for infinite \"S\". This variant refines the marriage theorem and provides a lower bound on the number of transversals that a given \"S\" may have. This variant is:\n\nSuppose that (\"A\", \"A\", ..., \"A\"), where the \"A\" are finite sets that need not be distinct, is a family of sets satisfying the marriage condition, and suppose that |\"A\"| ≥ \"r\" for \"i\" = 1, ..., \"n\". Then the number of different transversals for the family is at least \"r\" ! if \"r\" ≤ \"n\" and \"r\"(\"r\" - 1) ... (\"r\" - \"n\" +1) if \"r\" > \"n\".\n\nRecall that a transversal for a family \"S\" is an ordered sequence, so two different transversals could have exactly the same elements. For instance, the family \"A\" = {1,2,3}, \"A\" = {1,2,5} has both (1,2) and (2,1) as distinct transversals.\n\nThe theorem has many other interesting \"non-marital\" applications. For example, take a standard deck of cards, and deal them out into 13 piles of 4 cards each. Then, using the marriage theorem, we can show that it is always possible to select exactly 1 card from each pile, such that the 13 selected cards contain exactly one card of each rank (Ace, 2, 3, ..., Queen, King).\n\nMore abstractly, let \"G\" be a group, and \"H\" be a finite subgroup of \"G\". Then the marriage theorem can be used to show that there is a set \"T\" such that \"T\" is a transversal for both the set of left cosets and right cosets of \"H\" in \"G\".\n\nThe marriage theorem is used in the usual proofs of the fact that an (r × n) Latin rectangle can always be extended to an ((r+1) × n) Latin rectangle when r < n, and so, ultimately to a Latin square.\n\nThe following example, due to Marshall Hall, Jr., shows that the marriage condition will not guarantee the existence of a transversal in an infinite family in which infinite sets are allowed. \n\nLet \"S\" be the family, \"A\" = {1, 2, 3, ...}, \"A\" = {1}, \"A\" = {2}, ..., \"A\" = {i}, ... \n\nThe marriage condition holds for this infinite family, but no transversal can be constructed.\n\nThe more general problem of selecting a (not necessarily distinct) element from each of a collection of nonempty sets (without restriction as to the number of sets or the size of the sets) is permitted in general only if the axiom of choice is accepted.\n\nThis theorem is part of a collection of remarkably powerful theorems in combinatorics, all of which are related to each other in an informal sense in that it is more straightforward to prove one of these theorems from another of them than from first principles. These include:\n\n\nIn particular, there are simple proofs of the implications Dilworth's theorem ⇔ Hall's theorem ⇔ König–Egerváry theorem ⇔ König's theorem.\n\n\n"}
{"id": "47290509", "url": "https://en.wikipedia.org/wiki?curid=47290509", "title": "Incidence coloring", "text": "Incidence coloring\n\nIn graph theory, coloring generally implies assignment of labels to vertices, edges or faces in a graph. The incidence coloring is a special graph labeling where in each incidence of an edge with a vertex is assigned a color under certain constraints.\n\nLet \"G = (V, E)\" be a simple graph with vertex set (non-empty) \"V(G)\" and edge set \"E(G)\". An incidence is defined as a pair \"(v, e)\" where \"v\" ϵ \"V(G)\" is an end point of \"e\" ϵ \"E(G)\". In simple words, one says that vertex \"v\" is incident to edge \"e\".\n\nConsider a set of incidences, say, \"I(G)\" = {\"(v,e)\" : \"v\" ϵ \"V(G)\" and \"e\" ϵ \"E(G)\" and \"v\" ϵ \"e\"}. The two incidences \"(v,e)\" and \"(u,f)\" are said to be adjacent if one of the given conditions holds:\n\nAn incidence coloring of \"G\" can be defined as a function \"c\": \"I(G)\" → \"N\" such that \"c((v, e))\" ≠ \"c((u,f))\" for any incidences \"(v, e)\" and \"(u, f)\" that are adjacent. This implies that incidence coloring assigns distinct colors to neighborly incidences. [Generally, a simplified notation \"c(v, u)\" is used instead of \"c((v, e))\".]\n\nThe minimum number of colors needed for incidence coloring of a graph is known as incidence chromatic number or incidence coloring number of \"G\", represented by \"formula_1(G)\". This notation was introduced by Jennifer J. Quinn Massey and Richard A. Brualdi in 1993.\n\nLet \"A\" be a finite subset of \"N\", the set of natural numbers. \"A\" is an interval if and only if it contains all the numbers between minimum of \"A\" and maximum of \"A\". Consider \"c\" to be an incidence coloring of graph \"G\". Let \"formula_2(v)\" = {\"c(v,e)\" : \"v\" is an end point of edge \"e\" where \"e\" belongs to edge set \"E(G)\"}. An interval incidence coloring of \"G\" is an incidence coloring \"c\" of graph \"G\" such that for each vertex \"v\" in \"V(G)\", the set \"formula_2(v)\" is an interval.\n\nThe interval incidence coloring number of \"G\" is the minimum number of colors used for the interval incidence coloring of \"G\". It is denoted by \"IIC(G)\". If only \"IIC(G)\" colors are used for the interval incidence coloring, then it is said to be minimal.\n\nThe concept of incidence coloring was introduced by Brualdi and Massey in 1993. They bounded it in terms of \"Δ(G)\", the maximum degree of a graph \"G\". Initially, the incidence chromatic number of trees, complete bipartite graphs and complete graphs was found out. They also conjectured that all graphs can have an incidence coloring using \"Δ(G) + 2\" colors (Incidence coloring conjecture - ICC). This conjecture was disproved by Guiduli, who showed that incidence coloring concept is a directed star arboricity case, introduced by Alon and Algor. His counter example showed that incidence chromatic number is at most \"Δ(G) + O(log Δ(G))\".\n\nChen et al. found the incidence chromatic number of paths, fans, cycles, wheels, complete tripartite graph and adding edge wheels. Few years later, Shiu et al. showed that this conjecture is true for certain cubic graphs such as cubic Hamiltonian graphs. He showed that in case of outerplanar graph of maximum degree 4, the incidence chromatic number is not 5. The bounds for incidence chromatic number of various graph classes is found out now.\n\nConsider a graph \"G\" with maximum degree \"Δ(G)\". The trivial lower bound for \"formula_4(G)\" is given as: \"formula_4(G)\" ≥ \"Δ(G) + 1\".\n\nProof. Let \"v\" be the vertex with maximum degree \"Δ\" in \"G\". Let \"formula_6\" be the edges that are incident with the vertex \"v\". Consider \"formula_7\" = { \"v,w\" }. We can see that every pair of \"Δ + 1\" incidences, that is, ( \"v,formula_7\" ), ( \"v,formula_9\" ), ( \"v,formula_10\" )..., ( \"v,formula_11\" ), ( \"w,formula_7\" ) is neighborly. Therefore, these incidences have to be colored using distinct colors.\n\nThe bound is attained by trees and complete graphs.\n\nThe main results were determined and proved by Brualdi and Massey(1993). Shiu, Sun and Wu have proposed certain necessary conditions for graph to meet the quality \"formula_1(G)\" = \"Δ(G) + 1\".\n\nSeveral algorithms are introduced to provide incidence coloring of meshes like square meshes, honeycomb meshes and hexagonal meshes. These algorithms are optimal. For each mesh, the incidence colors can be made in the linear time with the least number of colors. It is found out that \"Δ(G) + 1\" colors are required for incidence coloring of square meshes, honeycomb meshes and hexagonal meshes.\n\nFor a Halin graph \"G\" with maximum degree greater than 4, the incidence chromatic number is \"Δ(G) + 1\"; which was proved by Chen, Wang and Pang. In case of Halin graphs with \"Δ(G)\" = 3 or 4, Jing-Zhe Qu determined that the incidence chromatic number to be 5 or 6 respectively. If Halin graph \"G\" contains a tree \"T\", then incidence chromatic number of \"formula_22\" is at most \"Δ(formula_23)\" + \"Δ(T)\" + 8. Every cubic Halin graph other than complete graph on 4 vertices satisfies incidence coloring with \"Δ + 2\" colors (Result proved by Shiu and Sun). Su, Meng and Guo extended this result and showed that all pseudo-Halin graphs satisfy incidence coloring conjecture. However, when the incidence coloring number of Halin graphs with low degree is \"Δ(G) + 1\" colors is still an unsolved problem.\n\nD.L. Chen, P.C.B. Lam and W.C. Shiu had conjectured that the incidence chromatic number of a cubic graph \"G\" is at most \"∆(G)+2\". They even proved that it is true in case of certain cubic graphs such as class of Hamiltonian cubic graphs. Based on these results, M. H. Dolama, E. Sopena and X. Zhu (2004) studied the graph classes for which \"formula_4(G)\" is bounded by \"∆(G) + c\" where \"c\" is some fixed constant. A graph is said to be \"k\"-generated if for every subgraph \"H\" of \"G\", the minimum degree of \"H\" is at most \"k\".\n\nConsider an outerplanar graph \"G\". Let vertex \"v\" in \"G\" be a cut vertex such that \"G – v\" is union of graphs \"formula_25\" and \"formula_26\". Let graph \"formula_27\" be the induced subgraph on vertex \"v\" and vertices of \"formula_25\"; and graph \"formula_29\" be the induced subgraph on vertex \"v\" and vertices of \"formula_26\". Then incidence chromatic number of \"G\" is maximum among incidence chromatic number of \"formula_31\" and \"1 + formula_32(v)\" where \"formula_32(v)\" is the degree of vertex \"v\" in \"G\".\n\nThe incidence chromatic number of an outerplanar graph \"G\" is at most \"Δ(G) + 2\" where \"Δ(G)\" is maximum degree of \"G\". In case of outerplanar graphs with \"Δ(G)\" greater than 3, the incidence chromatic number is \"Δ(G) +1\".\n\nSince outerplanar graphs are \"k\"-minor-free graphs, they accept a \"(Δ + 2, 2)\" – incidence coloring. The solution for incidence chromatic number of the outerplanar graph \"G\" having \"Δ(G) = 3\" and 2-connected outerplanar graph is still an open question.\n\nChordal rings are variations of ring networks. The use of chordal rings in communication is very extensive due to its advantages over the interconnection networks with ring topology and other analysed structures such as meshes, hypercubes, Cayley's graphs, etc. Arden and Lee first proposed the chordal ring of degree 3, that is, the ring structured network in which every node has an extra link known as chord, to some other node in the network. Distributed loop networks are chordal rings of degree 4 which is constructed by adding 2 extra chords at every vertex in a ring network.\n\nChordal rings, denoted by \"CR(N,d)\" is a graph with vertex set \"V(G)\" = { \"formula_34\" } and edge set \"E(G)\" = { \" formula_35 = 1 or d \" }, where \"formula_36\" denotes x modulo y, \"n\" is number of nodes and \"d\" is chord length. These graphs are studied due to its application in communication. Kung-Fu Ding, Kung-Jui Pai and Ro-Yu Wu studied the incidence coloring of chordal rings. Several algorithms are formulated to find the incidence chromatic number of chordal rings. The major findings are:\n\nKeaitsuda Nakprasit and Kittikorn Nakprasit studied the incidence coloring of powers of cycles. They have proved that \"formula_40\" except for certain cases, satisfies the \"(Δ + 2)\" conjecture where \"k\" is an integer. It was found out that \"formula_41\" = \"2k + 1\" = \"Δ(formula_42 + 1\" when n is divisible by \"2k + 1\", else \"formula_41\" = \"2k + 2\". If \"n\" is divisible by 5, \"formula_44\" = 5. Otherwise, \"formula_44\" = 6. Apart from the powers of cycles, studies have been done on powers of other graphs too.\n\nConsider a simple connected graph \"G\" with order \"n\", size m and domination number \"formula_46\".\nThen, \"formula_4(G)\" ≥ \"formula_48\".\n\nProof. Form a digraph \"D(G)\" from graph \"G\" by dividing each edge of \"G\" into 2 arcs in opposite directions. We can see that the total number of arcs in \"D(G)\" is \"2m\". According to Guiduli, the incidence coloring of \"G\" is equivalent to proper coloring of the digraph \"D(G)\", where 2 distinct arcs \"formula_49\" and \"formula_50\" are adjacent if one of the following conditions holds: (i) \"u\" = \"x\"; (ii) \"v\" = \"x\" or \"y\" = \"u\". By the definition of adjacency of arcs, an independent set of arcs in \"D(G)\" is a star forest. Therefore, a maximal independent set of arcs is a maximal star forest. This implies that at least \"formula_48\" color classes are required.\n\nThis relation has been widely used in the characterization of \"(r+1)\"-incidence colorable \"r\"-regular graphs. The major result on incidence coloring of \"r\"-regular graphs is: If graph \"G\" is r-regular graph, then \"formula_52\" = \"formula_53\" = \"r + 1\" if and only if \"V (G)\" is a disjoint union of \"r + 1\" dominating sets.\n\nThe interval incidence coloring of graph \"G\" is an incidence coloring of \"G\" such that the set of colors given for the incidences adjoining the same vertex forms an interval. The interval incidence coloring number, denoted by \"formula_54\" is the smallest number of colors needed for an interval incidence coloring of \"G\". It is clear that \"formula_55(G) ≤ formula_54(G)\".\n\nThe concept of interval incidence coloring was introduced by A. Malafiejska, R. Janczewski and M. Malafiejski. They proved that \"formula_54(G) ≤ 2∆(G)\" for any bipartite graph \"G\". In case of regular bipartite graphs, this equality holds. Subcubic bipartite graphs admit an interval incidence coloring using four, five or six colors. They have also proved that in linear time, for every bipartite graph with \"∆(G) = 4\", deciding interval incidence 5-colorability can be performed.\n\nThe fractional version of the incidence coloring was first introduced by Yang in 2007. A \"r\"-tuple incidence \"k\"-coloring of a graph \"G\" is the assignment of \"r\" colors to each incidence of graph \"G\" from a set of \"k\" colors such that the neighboring (adjacent) incidences are given disjoint sets of colors. By definition, it is obvious that 1-tuple incidence \"k\"-coloring is an incidence \"k\"-coloring too.\n\nThe fractional incidence chromatic number of graph \"G\" is the infimum of the fractions \"formula_58\" in such a way that \"G\" admits a \"r\"-tuple incidence \"k\"-coloring. Fractional incidence coloring has great applications in several fields of computer science. Based on incidence coloring results by Guiduli, Yang has proved that the fractional incidence chromatic number of any graph having maximum degree \"∆(G)\" is at most \"∆(G) + 20 log ∆(G) + 84\". He has also proved the existence of graphs with fractional incidence chromatic number at least \"∆(G) + Ω(log ∆(G))\".\n\nThe Nordhaus–Gaddum inequality has been developed for the incidence chromatic number \"formula_4(G)\" of graph \"G\". Consider a graph \"G\" with \"n\" vertices such that \"G\" ≠ \"formula_15\" or \"formula_61\". Let \"formula_62\" represents the complement graph of \"G\".\nThen, \"n +2 ≤ formula_4(G)\" + \"formula_64 ≤ 2n − 1\". These bounds are sharp for all values of \"n\".\n\nIncidence coloring game was first introduced by S. D. Andres. It is the incidence version of the vertex coloring game, in which the incidences of a graph are colored instead of vertices. Incidence game chromatic number is the new parameter defined as a game-theoretic analogous of the incidence chromatic number.\n\nThe game is that two players, Alice and Bob construct a proper incidence coloring. The rules are stated below:\n\nThe incidence game chromatic number of a graph \"G\", denoted by \"formula_65\", is the least number of colors required for Alice to win in incidence coloring game. It unifies the ideas of incidence chromatic number of a graph and game chromatic number in case of an undirected graph. Andres found out that the upper bound for \"formula_65\" in case of \"k\"-degenerate graphs with maximum degree \"Δ\" is \"2Δ + 4k - 2\". This bound was improved to \"2Δ + 3k - 1\" in case of graphs in which \"Δ\" is at least \"5k\". The incidence game chromatic number of stars, cycles, and sufficiently large wheels are also determined. John Y. Kim (2011) has found out the exact incidence game chromatic number of large paths and has given a correct proof of a result stated by Andres concerning the exact incidence game chromatic number of large wheels.\n\n\n"}
{"id": "8704195", "url": "https://en.wikipedia.org/wiki?curid=8704195", "title": "Indra's Pearls (book)", "text": "Indra's Pearls (book)\n\nIndra's Pearls: The Vision of Felix Klein is a geometry book written by David Mumford, Caroline Series and David Wright, and published by Cambridge University Press in 2002 and 2015.\n\nThe book explores the patterns created by iterating conformal maps of the complex plane called Möbius transformations, and their connections with symmetry and self-similarity. These patterns were glimpsed by German mathematician Felix Klein, but modern computer graphics allows them to be fully visualised and explored in detail.\n\nThe book's title refers to Indra's net, a metaphorical object described in the Buddhist text of the \"Flower Garland Sutra\". Indra's net consists of an infinite array of gossamer strands and pearls. The frontispiece to \"Indra's Pearls\" quotes the following description:\n\nThe allusion to Felix Klein's \"vision\" is a reference to Klein's early investigations of Schottky groups and hand-drawn plots of their limit sets. It also refers to Klein's wider vision of the connections between group theory, symmetry and geometry - see Erlangen program.\n\nThe contents of \"Indra's Pearls\" are as follows:\n\n\"Indra's Pearls \" is unusual because it aims to give the reader a sense of the development of a real-life mathematical investigation, rather than just a formal presentation of the final results. It covers a broad span of topics, showing interconnections among geometry, number theory, abstract algebra and computer graphics. It shows how computers are used by contemporary mathematicians. It uses computer graphics, diagrams and cartoons to enhance its written explanations. In the authors' own words:\n\n\n"}
{"id": "36522087", "url": "https://en.wikipedia.org/wiki?curid=36522087", "title": "Interactive Theorem Proving (conference)", "text": "Interactive Theorem Proving (conference)\n\nInteractive Theorem Proving (ITP) is an annual international academic conference on the topic of automated theorem proving, proof assistants and related topics, ranging from theoretical foundations to implementation aspects and applications in program verification, security, and formalization of mathematics.\n\nITP brings together the communities using many systems based on higher-order logic such as ACL2, Coq, Mizar, HOL, Isabelle, NuPRL, PVS, and Twelf. Individual workshops or meetings devoted to individual systems are usually held concurrently with the conference.\n\nTogether with CADE and TABLEAUX, ITP is usually one of the three main conferences of the International Joint Conference on Automated Reasoning (IJCAR) whenever it convenes,\n\nThe inaugural meeting of ITP was held on 11–14 July 2010 in Edinburgh, Scotland, as part of the Federated Logic Conference. It is the extension of the Theorem Proving in Higher Order Logics (TPHOLs) conference series to the broad field of interactive theorem proving. TPHOLs meetings took place every year from 1988 until 2009. \n\nThe first three were informal users' meetings for the HOL system and were the only ones without published papers. Since 1990 TPHOLs has published formal peer-reviewed proceedings, published by Springer's Lecture Notes in Computer Science series. It has also entertained an increasingly wide field of interest.\n\n"}
{"id": "627842", "url": "https://en.wikipedia.org/wiki?curid=627842", "title": "International Congress of Mathematicians", "text": "International Congress of Mathematicians\n\nThe International Congress of Mathematicians (ICM) is the largest conference for the topic of mathematics. It meets once every four years, hosted by the International Mathematical Union (IMU).\n\nThe Fields Medals, the Nevanlinna Prize, the Gauss Prize, and the Chern Medal are awarded during the congress's opening ceremony. Each congress is memorialized by a printed set of Proceedings recording academic papers based on invited talks intended to be relevant to current topics of general interest. Being invited to talk at the ICM has been called \"the equivalent [...] of an induction to a hall of fame.\"\n\nFelix Klein and Georg Cantor are credited with putting forward the idea of an international congress of mathematicians in the 1890s. The first International Congress of Mathematicians was held in Zurich in August 1897. The organizers included such prominent mathematicians as Luigi Cremona, Felix Klein, Gösta Mittag-Leffler, Andrey Markov, and others. The congress was attended by 208 mathematicians from 16 countries, including 12 from Russia and 7 from the U.S.A. Only four were women (none of them speakers): Iginia Massarini, Vera von Schiff, Charlotte Scott, and Charlotte Wedell.\n\nDuring the 1900 congress in Paris, France, David Hilbert announced his famous list of 23 unsolved mathematical problems, now termed Hilbert's problems. Moritz Cantor and Vito Volterra gave the two plenary lectures at the start of the congress.\n\nAt the 1904 ICM Gyula Kőnig delivered a lecture where he claimed that Cantor's famous continuum hypothesis was false. An error in Kőnig's proof was discovered by Ernst Zermelo soon thereafter. Kőnig's announcement at the congress caused considerable uproar, and Klein had to personally explain to the Grand Duke of Baden (who was a financial sponsor of the congress) what could cause such an unrest among mathematicians.\n\nDuring the 1912 congress in Cambridge, England, Edmund Landau listed four basic problems about prime numbers, now called Landau's problems. The 1924 congress in Toronto was organized by John Charles Fields, initiator of the Fields Medal; it included a roundtrip railway excursion to Vancouver and ferry to Victoria. The first two Fields Medals were awarded at the 1936 ICM in Oslo.\n\nIn the aftermath of World War I, at the insistence of the Allied Powers, the 1920 ICM in Strasbourg and the 1924 ICM in Toronto excluded mathematicians from the countries formerly part of the Central Powers. This resulted in a still unresolved controversy as to whether to count the Strasbourg and Toronto congresses as true ICMs. At the opening of the 1932 ICM in Zürich, Hermann Weyl said: \"We attend here to an extraordinary improbable event. For the number of \"n\", corresponding to the just opened International Congress of Mathematicians, we have the inequality 7 ≤ \"n\" ≤ 9; unfortunately our axiomatic foundations are not sufficient to give a more precise statement”. As a consequence of this controversy, from the 1932 Zürich congress onward, the ICMs are not numbered.\n\nFor the 1950 ICM in Cambridge, Massachusetts, Laurent Schwartz, one of the Fields Medalists for that year, and Jacques Hadamard, both of whom were viewed by the U.S. authorities as communist sympathizers, were only able to obtain U.S. visas after the personal intervention of President Harry Truman.\n\nThe first woman to give an ICM plenary lecture, at the 1932 congress in Zürich, was Emmy Noether. The second ICM plenary talk by a woman was delivered 58 years later, at the 1990 ICM in Kyoto, by Karen Uhlenbeck.\n\nThe 1998 congress was attended by 3,346 participants. The American Mathematical Society reported that more than 4,500 participants attended the 2006 conference in Madrid, Spain. The King of Spain presided over the 2006 conference opening ceremony. The 2010 Congress took place in Hyderabad, India, on August 19–27, 2010. The ICM 2014 was held in Seoul, South Korea, on August 13–21, 2014. The 2018 Congress took place in Rio de Janeiro on August 1–9, 2018.\n\nThe organizing committees of the early ICMs were formed in large part on an \"ad hoc\" basis and there was no single body continuously overseeing the ICMs. \nFollowing the end of World War I, the Allied Powers established in 1919 in Brussels the International Research Council (IRC). At the IRC's instructions, in 1920 the \"Union Mathematique Internationale\" (UMI) was created. This was the immediate predecessor of the current International Mathematical Union. Under the IRC's pressure, UMI reassigned the 1920 congress from Stockholm to Strasbourg and insisted on the rule which excluded from the congress mathematicians representing the former Central Powers. The exclusion rule, which also applied to the 1924 ICM, turned out to be quite unpopular among mathematicians from the U.S. and Great Britain. The 1924 ICM was originally scheduled to be held in New York, but had to be moved to Toronto after the American Mathematical Society withdrew its invitation to host the congress, in protest against the exclusion rule. As a result of the exclusion rule and the protests it generated, the 1920 and the 1924 ICMs were considerably smaller than the previous ones. In the run-up to the 1928 ICM in Bologna, IRC and UMI still insisted on applying the exclusion rule. In the face of the protests against the exclusion rule and the possibility of a boycott of the congress by the American Mathematical Society and the London Mathematical Society, the congress's organizers decided to hold the 1928 ICM under the auspices of the University of Bologna rather than of the UMI. The 1928 congress and all the subsequent congresses have been open for participation by mathematicians of all countries. \nThe statutes of the UMI expired in 1931 and at the 1932 ICM in Zurich a decision to dissolve the UMI was made, largely in opposition to IRC's pressure on the UMI.\n\nAt the 1950 ICM the participants voted to reconstitute the International Mathematical Union (IMU), which was formally established in 1951. Starting with the 1954 congress in Amsterdam, the ICMs are held under the auspices of the IMU.\n\nThe Soviet Union sent 27 participants to the 1928 ICM in Bologna and 10 participants to the 1932 ICM in Zurich. No Soviet mathematicians participated in the 1936 ICM, although a number of invitations were extended to them. At the 1950 ICM there were again no participants from the Soviet Union, although quite a few were invited. Similarly, no representatives of other Eastern Bloc countries, except for Yugoslavia, participated in the 1950 congress. Andrey Kolmogorov had been appointed to the Fields Medal selection committee for the 1950 congress, but did not participate in the committee's work. However, in a famous episode, a few days before the end of the 1950 ICM, the congress' organizers received a telegram from Sergei Vavilov, President of the USSR Academy of Sciences. The telegram thanked the organizers for inviting Soviet mathematicians but said that they are unable to attend \"being very much occupied with their regular work\", and wished success to the congress's participants. Vavilov's message was seen as a hopeful sign for the future ICMs and the situation improved further after Joseph Stalin's death in 1953. The Soviet Union was represented by five mathematicians at the 1954 ICM in Amsterdam, and several other Eastern Bloc countries sent their representatives as well. In 1957 the USSR joined the International Mathematical Union and the participation in subsequent ICMs by the Soviet and other Eastern Bloc scientists has been mostly at normal levels.\nHowever, even after 1957, tensions between ICM organizers and the Soviet side persisted. Soviet mathematicians invited to attend the ICMs routinely experienced difficulties with obtaining exit visas from the Soviet Union and were often unable to come. Thus of the 41 invited speakers from the USSR for the 1974 ICM in Vancouver, only 20 actually arrived. Grigory Margulis, who was awarded the Fields Medal at 1978 ICM in Helsinki, was not granted an exit visa and was unable to attend the 1978 congress. Another, related, point of contention was the jurisdiction over Fields Medals for Soviet mathematicians. After 1978 the Soviet Union put forward a demand that the USSR Academy of Sciences approve all Soviet candidates for the Fields Medal, before it was awarded to them. However, the IMU insisted that the decisions regarding invited speakers and Fields medalists be kept under exclusive jurisdiction of the ICM committees appointed for that purpose by the IMU.\n\n\n\n"}
{"id": "1127106", "url": "https://en.wikipedia.org/wiki?curid=1127106", "title": "Ivar Jacobson", "text": "Ivar Jacobson\n\nIvar Hjalmar Jacobson (born 1939) is a Swedish computer scientist and software engineer, known as major contributor to UML, Objectory, Rational Unified Process (RUP), aspect-oriented software development and Essence. \n\nIvar Jacobson was born in Ystad, Sweden on September 2nd, 1939. He received his Master of Electrical Engineering degree at Chalmers Institute of Technology in Gothenburg in 1962. After his work at Ericsson, he formalized the language and method he had been working on in his Ph.D. at the Royal Institute of Technology in Stockholm in 1985 on the thesis Language Constructs for Large Real Time Systems.\n\nAfter his master's degree, Jacobson joined Ericsson and worked in R&D on computerized switching systems AKE and AXE including PLEX. After his PhD thesis in April 1987, he started Objective Systems with Ericsson as a major customer. A majority stake of the company was acquired by Ericsson in 1991, and the company was renamed Objectory AB. Jacobson developed the software method Object-Oriented Software Engineering (OOSE) published 1992, which was a simplified version of the commercial software process Objectory (short for Object Factory).\n\nIn October, 1995, Ericsson divested Objectory to Rational Software and Jacobson started working with Grady Booch and James Rumbaugh, known collectively as the Three Amigos.\n\nWhen IBM bought Rational in 2003, Jacobson decided to leave, after he stayed on until May 2004 as an executive technical consultant.\n\nIn mid-2003 Jacobson formed Ivar Jacobson International (IJI) which operates across three continents with offices in the UK, the US, Sweden, Switzerland, China, and Singapore.\n\nIn 1967 at Ericsson, Jacobson proposed the use of software components in the new generation of software controlled telephone switches Ericsson was developing. In doing this he invented sequence diagrams, and developed collaboration diagrams. He also used state transition diagrams to describe the message flows between components.\n\nJacobson saw a need for \"blueprints\" for software development. He was one of the original developers of the Specification and Design Language (SDL). In 1975, SDL became a standard in the telecoms industry.\n\nAt Objectory he also invented use cases as a way to specify functional software requirements.\n\nAt Rational, Jacobson and his friends, Grady Booch and James Rumbaugh, designed the UML and his Objectory Process evolved to become the Rational Unified Process under the leadership of Philippe Kruchten.\n\nIn November 2005, Jacobson announced the Essential Unified Process or “EssUP” for short. EssUP was a new “Practice” centric software development process that stood on the shoulders of modern but established software development practices. It was a fresh new start, integrating successful practices sourced from the three leading process camps: the unified process camp, the agile software development camp and the process improvement camp. Each one of them contributed different capabilities: structure, agility and process improvement.\n\nIvar has described EssUP as a \"super light and agile\" RUP, and IJI have integrated EssUP into Microsoft Visual Studio Team System and Eclipse.\n\nStanding on the experience of EssUP Ivar and his team, in particular Ian Spence and Pan Wei Ng, developed EssWok starting in 2006. EssWork is a framework for working with methods. It is based on a kernel of universal elements always prevalent in software development endeavors. On top of the kernel some fifteen practices have been defined. A team can create their own method by composing practices.\n\nIn November 2009, Jacobson, Bertrand Meyer and Richard Soley (\"the Troika\") started an initiative called SEMAT (Software Engineering Method and Theory) to seek to develop a rigorous, theoretically sound basis for software engineering practice, and its wide adoption by industry and academia. SEMAT has been inspired by the work at IJI, but it is a fresh new start. It has resulted in Essence, which at the time of this writing has been recommended as an OMG standard.\n\nJacobson has published several books and articles, a selection: \n\n"}
{"id": "59120371", "url": "https://en.wikipedia.org/wiki?curid=59120371", "title": "Jerzy Baksalary", "text": "Jerzy Baksalary\n\nJerzy Kazimierz Baksalary was a Polish mathematician who specialized in mathematical statistics and linear algebra. In 1990 he was appointed professor of mathematical sciences. He authored over 160 academic papers published and won one of the Ministry of National Education awards.\n\nHe was a graduate of the Faculty of Mathematics, Physics and Chemistry at the University of Adam Mickiewicz in Poznan (1969). In the years 1969-1988 he was associated with the Department of Mathematics of the University of Agriculture in Poznań. \nFrom 1996, he was the dean of the Faculty of Mathematics, Physics and Technology at the Military University of WSP, and after the WSP joined the Zielona Góra University of Technology and the emergence of the University of Zielona Góra, he headed the Linear Algebra and Mathematical Statistics group.\n"}
{"id": "31076463", "url": "https://en.wikipedia.org/wiki?curid=31076463", "title": "Joseph Littledale", "text": "Joseph Littledale\n\nSir Joseph Littledale (1767 – 26 June 1842) was a British judge.\n\nHe was eldest son of Henry Littledale of Eton House, Lancashire, who was of a Cumberland family. He entered St John's College, Cambridge University in 1783 and was senior wrangler and 1st Smith's prizeman in 1787. He graduated B.A. in 1787 and M.A. in 1790.\n\nLittledale was admitted to Lincoln's Inn in 1786 but moved to Gray's Inn in 1793. He was called to the bar in 1798, and became a bencher in 1821.\n\nLittledale joined the northern circuit, and attended the Chester sessions. In 1813 he was appointed counsel to the University of Cambridge. He enjoyed a good practice. On 30 April 1824 he was appointed, in succession to Mr. Justice Best, to a judgeship of the King's Bench Division, though beyond being appointed John Hullock's colleague in managing the government prosecutions in Scotland in 1822 he had had little official recognition to that point. He took his seat on the first day of Easter term, 5 May 1824, and was knighted on 9 June. He also became Serjeant-at-law at this point.\n\nLittledale resigned because of failing health on 31 January 1841. He was sworn of the Privy Councillor, but died shortly after at his house in Bedford Square on 26 June 1842. He left £250,000. He edited John Skelton's \"Magnyfycence, an Interlude\" for the Roxburghe Club in 1821.\n\nOn 26 February 1821 Littledale married Hannah Timberlake.\n\nHis only daughter, Elizabeth, married Thomas Coventry, barrister-at-law.\n"}
{"id": "51172817", "url": "https://en.wikipedia.org/wiki?curid=51172817", "title": "List of American mathematicians", "text": "List of American mathematicians\n\nThis is a list of American mathematicians.\n"}
{"id": "3232460", "url": "https://en.wikipedia.org/wiki?curid=3232460", "title": "List of exceptional set concepts", "text": "List of exceptional set concepts\n\nThis is a list of exceptional set concepts. In mathematics, and in particular in mathematical analysis, it is very useful to be able to characterise subsets of a given set \"X\" as 'small', in some definite sense, or 'large' if their complement in \"X\" is small. There are numerous concepts that have been introduced to study 'small' or 'exceptional' subsets. In the case of sets of natural numbers, it is possible to define more than one concept of 'density', for example. See also list of properties of sets of reals.\n\n"}
{"id": "341127", "url": "https://en.wikipedia.org/wiki?curid=341127", "title": "List of functional analysis topics", "text": "List of functional analysis topics\n\nThis is a list of functional analysis topics, by Wikipedia page.\n\n\n\n\n\n\n\n\n\n\"See also list of mathematical topics in quantum theory\"\n\n\n\n\n"}
{"id": "781806", "url": "https://en.wikipedia.org/wiki?curid=781806", "title": "Mac Lane's planarity criterion", "text": "Mac Lane's planarity criterion\n\nIn graph theory, Mac Lane's planarity criterion is a characterisation of planar graphs in terms of their cycle spaces, named after Saunders Mac Lane, who published it in 1937. It states that a finite undirected graph is planar if and only if \nthe cycle space of the graph (taken modulo 2) has a cycle basis in which each edge of the graph participates in at most two basis vectors.\n\nFor any cycle in a graph , one can form an -dimensional 0-1 vector that has a 1 in the coordinate positions corresponding to edges in and a 0 in the remaining coordinate positions. The cycle space of the graph is the vector space formed by all possible linear combinations of vectors formed in this way. In Mac Lane's characterization, is a vector space over the finite field with two elements; that is, in this vector space, vectors are added coordinatewise modulo two. A \"2-basis\" of is a basis of with the property that, for each edge in , at most two basis vectors have nonzero coordinates in the position corresponding to . Then, stated more formally, Mac Lane's characterization is that the planar graphs are exactly the graphs that have a 2-basis.\n\nOne direction of the characterisation states that every planar graph has a 2-basis. Such a basis may be found as the collection of boundaries of the bounded faces of a planar embedding of the given graph .\n\nIf an edge is a bridge of , it appears twice on a single face boundary and therefore has a zero coordinate in the corresponding vector. Thus, the only edges that have nonzero coordinates are the ones that separate two different faces; these edges appear either once (if one of the faces is the unbounded one) or twice in the collection of boundaries of bounded faces. It remains to prove that these cycles form a basis. One way to prove this by induction. As a base case, is a tree, then it has no bounded faces and is zero-dimensional and has an empty basis. Otherwise, removing an edge from the unbounded face of reduces both the dimension of the cycle space and the number of bounded faces by one and the induction follows.\n\nAlternatively, it is possible to use Euler's formula to show that the number of cycles in this collection equals the circuit rank of , which is the dimension of the cycle space. Each nonempty subset of cycles has a vector sum that represents the boundary of the union of the bounded faces in the subset, which cannot be empty (the union includes at least one bounded face and excludes the unbounded face, so there must be some edges separating them). Therefore, there is no subset of cycles whose vectors sum to zero, which means that all the cycles are linearly independent. As a linearly independent set of the same size as the dimension of the space, this collection of cycles must form a basis.\n\n provided the following simple argument for the other direction of the characterization, based on Wagner's theorem characterizing the planar graphs by forbidden minors. As O'Neill observes, the property of having a 2-basis is preserved under graph minors: if one contracts an edge, the same contraction may be performed in the basis vectors, if one removes an edge that has a nonzero coordinate in a single basis vector, then that vector may be removed from the basis, and if one removes an edge that has a nonzero coordinate in two basis vectors, then those two vectors may be replaced by their sum (modulo two). Additionally, if is a cycle basis for any graph, then it must cover some edges exactly once, for otherwise its sum would be zero (impossible for a basis), and so can be augmented by one more cycle consisting of these singly-covered edges while preserving the property that every edge is covered at most twice.\nHowever, the complete graph has no 2-basis: is six-dimensional, each nontrivial vector in has nonzero coordinates for at least three edges, and so any augmented basis would have at least 21 nonzeros, exceeding the 20 nonzeros that would be allowed if each of the ten edges were nonzero in at most two basis vectors. By similar reasoning, the complete bipartite graph has no 2-basis: is four-dimensional, and each nontrivial vector in has nonzero coordinates for at least four edges, so any augmented basis would have at least 20 nonzeros, exceeding the 18 nonzeros that would be allowed if each of the nine edges were nonzero in at most two basis vectors. Since the property of having a 2-basis is minor-closed and is not true of the two minor-minimal nonplanar graphs and , it is also not true of any other nonplanar graph.\n\n used Mac Lane's planarity criterion as part of a parallel algorithm for testing graph planarity and finding planar embeddings. Their algorithm partitions the graph into triconnected components, after which there is a unique planar embedding (up to the choice of the outer face) and the cycles in a 2-basis can be assumed to be all the peripheral cycles of the graph. Ja'Ja' and Simon start with a fundamental cycle basis of the graph (a cycle basis generated from a spanning tree by forming a cycle for each possible combination of a path in the tree and an edge outside the tree) and transform it into a 2-basis of peripheral cycles. These cycles form the faces of a planar embedding of the given graph.\n\nMac Lane's planarity criterion allows the number of bounded face cycles in a planar graph to be counted easily, as the circuit rank of the graph. This property is used in defining the meshedness coefficient of the graph, a normalized variant of the number of bounded face cycles that is computed by dividing the circuit rank by , the maximum possible number of bounded faces of a planar graph with the same vertex set .\n\n"}
{"id": "58689209", "url": "https://en.wikipedia.org/wiki?curid=58689209", "title": "Magnhild Lien", "text": "Magnhild Lien\n\nMagnhild Lien is a Norwegian mathematician specializing in knot theory. She is a professor emeritus of mathematics at California State University, Northridge, and the former executive director of the Association for Women in Mathematics.\n\nLien was born in Arendal, a town on the southern Norwegian island of Tromøya, where she grew up as the youngest of six children and the only girl in her family.\nShe earned a bachelor's degree from McGill University in 1979, and completed her Ph.D. in 1984 from the University of Iowa. Her dissertation, \"Construction of High Dimensional Knot Groups from Classical Knot Groups\", was supervised by Jonathan Kalman Simon.\n\nShe joined the faculty at California State University, Northridge in 1987,\nand served as department chair of mathematics there from 1998 to 2006.\nShe was executive director of the Association for Women in Mathematics from 2012 to 2018.\n\nAs well as publishing her own mathematical research on knot theory, Lien has written about women in mathematics in collaboration with her husband, sociologist Harvey Rich.\n\nLien was included in the 2019 class of fellows of the Association for Women in Mathematics \"for extraordinary leadership and service devoted to advancing and supporting women in the mathematical sciences, as AWM executive director and, for a quarter century, as initiator, director, and fundraiser of programs for women\".\n"}
{"id": "19233726", "url": "https://en.wikipedia.org/wiki?curid=19233726", "title": "Michael Lissack", "text": "Michael Lissack\n\nMichael Lissack (born 1958) is an American business executive, author, business consultant and director of the Institute for the Study of Coherence and Emergence.\n\nLisack was managing director in the municipal bond department at Smith Barney, and came into prominence as the whistleblower, who exposed a yield burning scandal in the 1990s, whereby financial firms made illegal profits from the structuring of U.S. government investment portfolios associated with municipal bonds.\n\nLissack received his BA in American Civilization and Political Economy in 1979 from Williams College, and his MBA in Business from Yale University in 1981. Later in his career in 2000 Lissack received a doctor of business administration degree from Henley Management College in the United Kingdom.\n\nAfter his graduation from Yale, Lissack started at Smith Barney, where he became managing director and served in this position until 1995. Since 1999 he is director of the Institute for the Study of Coherence and Emergence. From 1999 to 2004, Lissack also served as the editor-in-chief of \"Emergence: A Journal of Complexity Issues in Organizations and Management\" now known as E:CO.\n\nLissack was a candidate for county commissioner in Collier County, Florida, in 2002 and in 2006. He briefly taught business and public policy at the Central European University. Lissack is also the president of the American Society for Cybernetics.\n\nIn 1994, Lissack exposed a major yield burning scandal on Wall Street. The issue was eventually settled by a number of firms for over $200 million, to which Lissack was entitled to at least 15% per federal whistleblower laws. Lissack used some of these funds for charitable purposes including endowing a professorship in social responsibility and personal ethics at his alma mater, Williams College.\n\nIn February 1998, Lissack entered into a voluntary agreement with the U.S. Securities and Exchange Commission whereby he was banned from the securities industry for five years and paid a $30,000 fine, as part of an arrangement by Lissack's legal team for Lissack to be on record as taking some responsibility for the scandal. Later that year Lissack was charged by the Manhattan District Attorney's office with making online solicitations for people to harass executives of his former employer, Salmon Smith Barney, by calling them at company headquarters and in some instances their homes. He pleaded guilty to second-degree harassment (a violation and lesser offense than the misdemeanor harassment charge with which he was originally charged ), admitting he sent phony e-mails to Salomon Smith Barney employees. As part of the guilty plea, Lissack was not sentenced to jail and paid no fine.\n\nIn 1999 \"Worth Magazine\" described Lissack as one of \"Wall Street's 25 smartest players\" in 1999 and as one of the 100 Americans who have most influenced \"how we think about money\" in 2001.\n\nIn 1999 Lissack published \"The Next Common Sense\" (1999) This work co-authored with Johan Roos presented the concepts of Identity, Landscape and Simple Guiding Principles. These principles were used to develop the Real-Time Strategy used in the first Lego Serious Play application.\n\nIn 2011 Lissack also authored the book \"Coherence in the Midst of Complexity.\"\n\nLissack has written or co-authored a number of books, a selection: including \n\n"}
{"id": "34514105", "url": "https://en.wikipedia.org/wiki?curid=34514105", "title": "Multiparty communication complexity", "text": "Multiparty communication complexity\n\nIn theoretical computer science, multiparty communication complexity is the study of communication complexity in the setting where there are more than 2 players.\n\nIn the traditional two–party communication game, introduced by , two players, \"P\" and \"P\" attempt to compute a Boolean function \n\nPlayer \"P\" knows the value of \"x\", \"P\" knows the value of \"x\", but \"P\" does not know the value of \"x\", for \"i\" = 1, 2. \n\nIn other words, the players know the other's variables, but not their own. The minimum number of bits that must be communicated by the players to compute \"f\" is the communication complexity of \"f\", denoted by \"κ\"(\"f\").\n\nThe multiparty communication game, defined in 1983, is a powerful generalization of the 2–party case: Here the players know all the others' input, except their own. Because of this property, sometimes this model is called \"numbers on the forehead\" model, since if the players were seated around a round table, each wearing their own input on the forehead, then every player would see all the others' input, except their own.\n\nThe formal definition is as follows: \"k\" players: \"P\",\"P\"...,\"P\" intend to compute a Boolean function \n\nOn set \"S\" = {\"x\",\"x\"...,\"x\"} of variables there is a fixed partition \"A\" of \"k\" classes \"A\",\"A\"...,\"A\", and player \"P\" knows every variable, \"except\" those in \"A\", for \"i\" = 1,2...,\"k\". The players have unlimited computational power, and they communicate with the help of a blackboard, viewed by all players.\n\nThe aim is to compute \"f\"(\"x\",\"x\"...,\"x\"), such that at the end of the computation, every player knows this value. The cost of the computation is the number of bits written onto the blackboard for the given input \"x\" = (\"x\",\"x\"...,\"x\") and partition \"A\" = (\"A\",\"A\"...,\"A\"). The cost of a multiparty protocol is the maximum number of bits communicated for any \"x\" from the set {0,1} and the given partition \"A\". The \"k\"-party communication complexity, \"C\"(\"f\"), of a\nfunction \"f\", with respect to partition \"A\", is the minimum of costs of those \"k\"-party protocols which compute \"f\". The \"k\"-party symmetric communication complexity of \"f\" is defined as \n\nwhere the maximum is taken over all \"k\"-partitions of set \"x\" = (\"x\",\"x\"...,\"x\").\n\nFor a general upper bound both for two and more players, let us suppose that \"A\" is one of the smallest classes of the partition \"A\",\"A\"...,\"A\". Then \"P\" can compute any Boolean function of \"S\" with |\"A\"| + 1 bits of communication: \"P\" writes down the |\"A\"| bits of \"A\" on the blackboard, \"P\" reads it, and computes and announces the value \"f\"(\"x\"). So, we can write:\n\nThe Generalized Inner Product function (GIP) is defined as follows:\nLet \"y\",\"y\"...,\"y\" be \"n\"-bit vectors, and let \"Y\" be the \"n\" times \"k\" matrix, with k columns as the \"y\",\"y\"...,\"y\" vectors. Then GIP(\"y\",\"y\"...,\"y\") is the number of the all-1 rows of matrix \"Y\", taken modulo 2. In other words, if the vectors \"y\",\"y\"...,\"y\" correspond to the characteristic vectors of \"k\" subsets of an \"n\" element base-set, then GIP corresponds to the parity of the intersection of these \"k\" subsets.\n\nIt was shown that\n\nwith a constant \"c\" > 0.\n\nAn upper bound on the multiparty communication complexity of GIP shows that\n\nwith a constant \"c\" > 0.\n\nFor a general Boolean function \"f\", one can bound the multiparty communication complexity of \"f\" by using its \"L\" norm as follows:\n\nA construction of a pseudorandom number generator was based on the BNS lower bound for the GIP function.\n"}
{"id": "361924", "url": "https://en.wikipedia.org/wiki?curid=361924", "title": "Order theory", "text": "Order theory\n\nOrder theory is a branch of mathematics which investigates the intuitive notion of order using binary relations. It provides a formal framework for describing statements such as \"this is less than that\" or \"this precedes that\". This article introduces the field and provides basic definitions. A list of order-theoretic terms can be found in the order theory glossary.\n\nOrders are everywhere in mathematics and related fields like computer science. The first order often discussed in primary school is the standard order on the natural numbers e.g. \"2 is less than 3\", \"10 is greater than 5\", or \"Does Tom have fewer cookies than Sally?\". This intuitive concept can be extended to orders on other sets of numbers, such as the integers and the reals. The idea of being greater than or less than another number is one of the basic intuitions of number systems (compare with numeral systems) in general (although one usually is also interested in the actual difference of two numbers, which is not given by the order). Other familiar examples of orderings are the alphabetical order of words in a dictionary and the genealogical property of lineal descent within a group of people.\n\nThe notion of order is very general, extending beyond contexts that have an immediate, intuitive feel of sequence or relative quantity. In other contexts orders may capture notions of containment or specialization. Abstractly, this type of order amounts to the subset relation, e.g., \"Pediatricians are physicians,\" and \"Circles are merely special-case ellipses.\"\n\nSome orders, like \"less-than\" on the natural numbers and alphabetical order on words, have a special property: each element can be \"compared\" to any other element, i.e. it is smaller (earlier) than, larger (later) than, or identical to. However, many other orders do not. Consider for example the subset order on a collection of sets: though the set of birds and the set of dogs are both subsets of the set of animals, neither the birds nor the dogs constitutes a subset of the other. Those orders like the \"subset-of\" relation for which there exist \"incomparable\" elements are called \"partial orders\"; orders for which every pair of elements is comparable are \"total orders\".\n\nOrder theory captures the intuition of orders that arises from such examples in a general setting. This is achieved by specifying properties that a relation ≤ must have to be a mathematical order. This more abstract approach makes much sense, because one can derive numerous theorems in the general setting, without focusing on the details of any particular order. These insights can then be readily transferred to many less abstract applications.\n\nDriven by the wide practical usage of orders, numerous special kinds of ordered sets have been defined, some of which have grown into mathematical fields of their own. In addition, order theory does not restrict itself to the various classes of ordering relations, but also considers appropriate functions between them. A simple example of an order theoretic property for functions comes from analysis where monotone functions are frequently found.\n\nThis section introduces ordered sets by building upon the concepts of set theory, arithmetic, and binary relations.\n\nOrders are special binary relations. Suppose that \"P\" is a set and that ≤ is a relation on \"P\". Then ≤ is a partial order if it is reflexive, antisymmetric, and transitive, i.e., for all \"a\", \"b\" and \"c\" in \"P\", we have that:\n\nA set with a partial order on it is called a partially ordered set, poset, or just an ordered set if the intended meaning is clear. By checking these properties, one immediately sees that the well-known orders on natural numbers, integers, rational numbers and reals are all orders in the above sense. However, they have the additional property of being total, i.e., for all \"a\" and \"b\" in \"P\", we have that:\n\nThese orders can also be called linear orders or chains. While many classical orders are linear, the subset order on sets provides an example where this is not the case. Another example is given by the divisibility (or \"is-a-factor-of\") relation \"|\". For two natural numbers \"n\" and \"m\", we write \"n\"|\"m\" if \"n\" divides \"m\" without remainder. One easily sees that this yields a partial order.\nThe identity relation = on any set is also a partial order in which every two distinct elements are incomparable. It is also the only relation that is both a partial order and an equivalence relation. Many advanced properties of posets are interesting mainly for non-linear orders.\n\nHasse diagrams can visually represent the elements and relations of a partial ordering. These are graph drawings where the vertices are the elements of the poset and the ordering relation is indicated by both the edges and the relative positioning of the vertices. Orders are drawn bottom-up: if an element \"x\" is smaller than (precedes) \"y\" then there exists a path from \"x\" to \"y\" that is directed upwards. It is often necessary for the edges connecting elements to cross each other, but elements must never be located within an edge. An instructive exercise is to draw the Hasse diagram for the set of natural numbers that are smaller than or equal to 13, ordered by | (the \"divides\" relation).\n\nEven some infinite sets can be diagrammed by superimposing an ellipsis (...) on a finite sub-order. This works well for the natural numbers, but it fails for the reals, where there is no immediate successor above 0; however, quite often one can obtain an intuition related to diagrams of a similar kind.\n\nIn a partially ordered set there may be some elements that play a special role. The most basic example is given by the least element of a poset. For example, 1 is the least element of the positive integers and the empty set is the least set under the subset order. Formally, an element \"m\" is a least element if:\n\nThe notation 0 is frequently found for the least element, even when no numbers are concerned. However, in orders on sets of numbers, this notation might be inappropriate or ambiguous, since the number 0 is not always least. An example is given by the above divisibility order |, where 1 is the least element since it divides all other numbers. In contrast, 0 is the number that is divided by all other numbers. Hence it is the greatest element of the order. Other frequent terms for the least and greatest elements is bottom and top or zero and unit.\n\nLeast and greatest elements may fail to exist, as the example of the real numbers shows. But if they exist, they are always unique. In contrast, consider the divisibility relation | on the set {2,3,4,5,6}. Although this set has neither top nor bottom, the elements 2, 3, and 5 have no elements below them, while 4, 5 and 6 have none above. Such elements are called minimal and maximal, respectively. Formally, an element \"m\" is minimal if:\n\nExchanging ≤ with ≥ yields the definition of maximality. As the example shows, there can be many maximal elements and some elements may be both maximal and minimal (e.g. 5 above). However, if there is a least element, then it is the only minimal element of the order. Again, in infinite posets maximal elements do not always exist - the set of all \"finite\" subsets of a given infinite set, ordered by subset inclusion, provides one of many counterexamples. An important tool to ensure the existence of maximal elements under certain conditions is Zorn's Lemma.\n\nSubsets of partially ordered sets inherit the order. We already applied this by considering the subset {2,3,4,5,6} of the natural numbers with the induced divisibility ordering. Now there are also elements of a poset that are special with respect to some subset of the order. This leads to the definition of upper bounds. Given a subset \"S\" of some poset \"P\", an upper bound of \"S\" is an element \"b\" of \"P\" that is above all elements of \"S\". Formally, this means that\n\nLower bounds again are defined by inverting the order. For example, -5 is a lower bound of the natural numbers as a subset of the integers. Given a set of sets, an upper bound for these sets under the subset ordering is given by their union. In fact, this upper bound is quite special: it is the smallest set that contains all of the sets. Hence, we have found the least upper bound of a set of sets. This concept is also called supremum or join, and for a set \"S\" one writes sup(\"S\") or formula_1 for its least upper bound. Conversely, the greatest lower bound is known as infimum or meet and denoted inf(\"S\") or formula_2. These concepts play an important role in many applications of order theory. For two elements \"x\" and \"y\", one also writes formula_3 and formula_4 for sup({\"x\",\"y\"}) and inf({\"x\",\"y\"}), respectively.\n\nFor example, 1 is the infimum of the positive integers as a subset of integers.\n\nFor another example, consider again the relation | on natural numbers. The least upper bound of two numbers is the smallest number that is divided by both of them, i.e. the least common multiple of the numbers. Greatest lower bounds in turn are given by the greatest common divisor.\n\nIn the previous definitions, we often noted that a concept can be defined by just inverting the ordering in a former definition. This is the case for \"least\" and \"greatest\", for \"minimal\" and \"maximal\", for \"upper bound\" and \"lower bound\", and so on. This is a general situation in order theory: A given order can be inverted by just exchanging its direction, pictorially flipping the Hasse diagram top-down. This yields the so-called dual, inverse, or opposite order.\n\nEvery order theoretic definition has its dual: it is the notion one obtains by applying the definition to the inverse order. Since all concepts are symmetric, this operation preserves the theorems of partial orders. For a given mathematical result, one can just invert the order and replace all definitions by their duals and one obtains another valid theorem. This is important and useful, since one obtains two theorems for the price of one. Some more details and examples can be found in the article on duality in order theory.\n\nThere are many ways to construct orders out of given orders. The dual order is one example. Another important construction is the cartesian product of two partially ordered sets, taken together with the product order on pairs of elements. The ordering is defined by (\"a\", \"x\") ≤ (\"b\", \"y\") if (and only if) \"a\" ≤ \"b\" and \"x\" ≤ \"y\". (Notice carefully that there are three distinct meanings for the relation symbol ≤ in this definition.) The disjoint union of two posets is another typical example of order construction, where the order is just the (disjoint) union of the original orders.\n\nEvery partial order ≤ gives rise to a so-called strict order <, by defining \"a\" < \"b\" if \"a\" ≤ \"b\" and not \"b\" ≤ \"a\". This transformation can be inverted by setting \"a\" ≤ \"b\" if \"a\" < \"b\" or \"a\" = \"b\". The two concepts are equivalent although in some circumstances one can be more convenient to work with than the other.\n\nIt is reasonable to consider functions between partially ordered sets having certain additional properties that are related to the ordering relations of the two sets. The most fundamental condition that occurs in this context is monotonicity. A function \"f\" from a poset \"P\" to a poset \"Q\" is monotone, or order-preserving, if \"a\" ≤ \"b\" in \"P\" implies \"f\"(\"a\") ≤ \"f\"(\"b\") in \"Q\" (Noting that, strictly, the two relations here are different since they apply to different sets.). The converse of this implication leads to functions that are order-reflecting, i.e. functions \"f\" as above for which \"f\"(\"a\") ≤ \"f\"(\"b\") implies \"a\" ≤ \"b\". On the other hand, a function may also be order-reversing or antitone, if \"a\" ≤ \"b\" implies \"f\"(\"b\") ≤ \"f\"(\"a\").\n\nAn order-embedding is a function \"f\" between orders that is both order-preserving and order-reflecting. Examples for these definitions are found easily. For instance, the function that maps a natural number to its successor is clearly monotone with respect to the natural order. Any function from a discrete order, i.e. from a set ordered by the identity order \"=\", is also monotone. Mapping each natural number to the corresponding real number gives an example for an order embedding. The set complement on a powerset is an example of an antitone function.\n\nAn important question is when two orders are \"essentially equal\", i.e. when they are the same up to renaming of elements. Order isomorphisms are functions that define such a renaming. An order-isomorphism is a monotone bijective function that has a monotone inverse. This is equivalent to being a surjective order-embedding. Hence, the image \"f\"(\"P\") of an order-embedding is always isomorphic to \"P\", which justifies the term \"embedding\".\n\nA more elaborate type of functions is given by so-called Galois connections. Monotone Galois connections can be viewed as a generalization of order-isomorphisms, since they constitute of a pair of two functions in converse directions, which are \"not quite\" inverse to each other, but that still have close relationships.\n\nAnother special type of self-maps on a poset are closure operators, which are not only monotonic, but also idempotent, i.e. \"f\"(\"x\") = \"f\"(\"f\"(\"x\")), and extensive (or \"inflationary\"), i.e. \"x\" ≤ \"f\"(\"x\"). These have many applications in all kinds of \"closures\" that appear in mathematics.\n\nBesides being compatible with the mere order relations, functions between posets may also behave well with respect to special elements and constructions. For example, when talking about posets with least element, it may seem reasonable to consider only monotonic functions that preserve this element, i.e. which map least elements to least elements. If binary infima ∧ exist, then a reasonable property might be to require that \"f\"(\"x\" ∧ \"y\") = \"f\"(\"x\") ∧ \"f\"(\"y\"), for all \"x\" and \"y\". All of these properties, and indeed many more, may be compiled under the label of limit-preserving functions.\n\nFinally, one can invert the view, switching from \"functions of orders\" to \"orders of functions\". Indeed, the functions between two posets \"P\" and \"Q\" can be ordered via the pointwise order. For two functions \"f\" and \"g\", we have \"f\" ≤ \"g\" if \"f\"(\"x\") ≤ \"g\"(\"x\") for all elements \"x\" of \"P\". This occurs for example in domain theory, where function spaces play an important role.\n\nMany of the structures that are studied in order theory employ order relations with further properties. In fact, even some relations that are not partial orders are of special interest. Mainly the concept of a preorder has to be mentioned. A preorder is a relation that is reflexive and transitive, but not necessarily antisymmetric. Each preorder induces an equivalence relation between elements, where \"a\" is equivalent to \"b\", if \"a\" ≤ \"b\" and \"b\" ≤ \"a\". Preorders can be turned into orders by identifying all elements that are equivalent with respect to this relation.\n\nSeveral types of orders can be defined from numerical data on the items of the order: a total order results from attaching distinct real numbers to each item and using the numerical comparisons to order the items; instead, if distinct items are allowed to have equal numerical scores, one obtains a strict weak ordering. Requiring two scores to be separated by a fixed threshold before they may be compared leads to the concept of a semiorder, while allowing the threshold to vary on a per-item basis produces an interval order.\n\nAn additional simple but useful property leads to so-called well-founded, for which all non-empty subsets have a minimal element. Generalizing well-orders from linear to partial orders, a set is well partially ordered if all its non-empty subsets have a finite number of minimal elements.\n\nMany other types of orders arise when the existence of infima and suprema of certain sets is guaranteed. Focusing on this aspect, usually referred to as completeness of orders, one obtains:\n\n\nHowever, one can go even further: if all finite non-empty infima exist, then ∧ can be viewed as a total binary operation in the sense of universal algebra. Hence, in a lattice, two operations ∧ and ∨ are available, and one can define new properties by giving identities, such as\n\nThis condition is called distributivity and gives rise to distributive lattices. There are some other important distributivity laws which are discussed in the article on distributivity in order theory. Some additional order structures that are often specified via algebraic operations and defining identities are\n\n\nwhich both introduce a new operation ~ called negation. Both structures play a role in mathematical logic and especially Boolean algebras have major applications in computer science.\nFinally, various structures in mathematics combine orders with even more algebraic operations, as in the case of quantales, that allow for the definition of an addition operation.\n\nMany other important properties of posets exist. For example, a poset is locally finite if every closed interval [\"a\", \"b\"] in it is finite. Locally finite posets give rise to incidence algebras which in turn can be used to define the Euler characteristic of finite bounded posets.\n\nIn an ordered set, one can define many types of special subsets based on the given order. A simple example are upper sets; i.e. sets that contain all elements that are above them in the order. Formally, the upper closure of a set \"S\" in a poset \"P\" is given by the set {\"x\" in \"P\" | there is some \"y\" in \"S\" with \"y\" ≤ \"x\"}. A set that is equal to its upper closure is called an upper set. Lower sets are defined dually.\n\nMore complicated lower subsets are ideals, which have the additional property that each two of their elements have an upper bound within the ideal. Their duals are given by filters. A related concept is that of a directed subset, which like an ideal contains upper bounds of finite subsets, but does not have to be a lower set. Furthermore, it is often generalized to preordered sets.\n\nA subset which is - as a sub-poset - linearly ordered, is called a chain. The opposite notion, the antichain, is a subset that contains no two comparable elements; i.e. that is a discrete order.\n\nAlthough most mathematical areas \"use\" orders in one or the other way, there are also a few theories that have relationships which go far beyond mere application. Together with their major points of contact with order theory, some of these are to be presented below.\n\nAs already mentioned, the methods and formalisms of universal algebra are an important tool for many order theoretic considerations. Beside formalizing orders in terms of algebraic structures that satisfy certain identities, one can also establish other connections to algebra. An example is given by the correspondence between Boolean algebras and Boolean rings. Other issues are concerned with the existence of free constructions, such as \"free lattices\" based on a given set of generators. Furthermore, closure operators are important in the study of universal algebra.\n\nIn topology, orders play a very prominent role. In fact, the set of open sets provides a classical example of a complete lattice, more precisely a complete Heyting algebra (or \"frame\" or \"locale\"). Filters and nets are notions closely related to order theory and the closure operator of sets can be used to define topology. Beyond these relations, topology can be looked at solely in terms of the open set lattices, which leads to the study of pointless topology. Furthermore, a natural preorder of elements of the underlying set of a topology is given by the so-called specialization order, that is actually a partial order if the topology is T.\n\nConversely, in order theory, one often makes use of topological results. There are various ways to define subsets of an order which can be considered as open sets of a topology. Considering topologies on a poset (\"X\", ≤) that in turn induce ≤ as their specialization order, the \"finest\" such topology is the Alexandrov topology, given by taking all upper sets as opens. Conversely, the \"coarsest\" topology that induces the specialization order is the upper topology, having the complements of principal ideals (i.e. sets of the form {\"y\" in \"X\" | \"y\" ≤ \"x\"} for some \"x\") as a subbase. Additionally, a topology with specialization order ≤ may be order consistent, meaning that their open sets are \"inaccessible by directed suprema\" (with respect to ≤). The finest order consistent topology is the Scott topology, which is coarser than the Alexandrov topology. A third important topology in this spirit is the Lawson topology. There are close connections between these topologies and the concepts of order theory. For example, a function preserves directed suprema iff it is continuous with respect to the Scott topology (for this reason this order theoretic property is also called Scott-continuity).\n\nThe visualization of orders with Hasse diagrams has a straightforward generalization: instead of displaying lesser elements \"below\" greater ones, the direction of the order can also be depicted by giving directions to the edges of a graph. In this way, each order is seen to be equivalent to a directed acyclic graph, where the nodes are the elements of the poset and there is a directed path from \"a\" to \"b\" if and only if \"a\" ≤ \"b\". Dropping the requirement of being acyclic, one can also obtain all preorders.\n\nWhen equipped with all transitive edges, these graphs in turn are just special categories, where elements are objects and each set of morphisms between two elements is at most singleton. Functions between orders become functors between categories. Many ideas of order theory are just concepts of category theory in small. For example, an infimum is just a categorical product. More generally, one can capture infima and suprema under the abstract notion of a categorical limit (or \"colimit\", respectively). Another place where categorical ideas occur is the concept of a (monotone) Galois connection, which is just the same as a pair of adjoint functors.\n\nBut category theory also has its impact on order theory on a larger scale. Classes of posets with appropriate functions as discussed above form interesting categories. Often one can also state constructions of orders, like the product order, in terms of categories. Further insights result when categories of orders are found categorically equivalent to other categories, for example of topological spaces. This line of research leads to various \"representation theorems\", often collected under the label of Stone duality.\n\nAs explained before, orders are ubiquitous in mathematics. However, earliest explicit mentionings of partial orders are probably to be found not before the 19th century. In this context the works of George Boole are of great importance. Moreover, works of Charles Sanders Peirce, Richard Dedekind, and Ernst Schröder also consider concepts of order theory. Certainly, there are others to be named in this context and surely there exists more detailed material on the history of order theory. \n\nThe term \"poset\" as an abbreviation for partially ordered set was coined by Garrett Birkhoff in the second edition of his influential book \"Lattice Theory\".\n\n\n\n"}
{"id": "11396416", "url": "https://en.wikipedia.org/wiki?curid=11396416", "title": "Paradox of the plankton", "text": "Paradox of the plankton\n\nIn aquatic biology, the paradox of the plankton describes the situation in which a limited range of resources supports an unexpectedly wide range of plankton species, apparently flouting the competitive exclusion principle which holds that when two species compete for the same resource, one will be driven to extinction.\n\nThe paradox of the plankton results from the clash between the observed diversity of plankton and the competitive exclusion principle, also known as Gause's law, which states that, when two species compete for the same resource, ultimately only one will persist and the other will be driven to extinction. Phytoplankton life is diverse at all phylogenetic levels despite the limited range of resources (e.g. light, nitrate, phosphate, silicic acid, iron) for which they compete amongst themselves.\n\nThe paradox of the plankton was originally described in 1961 by G. Evelyn Hutchinson, who proposed that the paradox could be resolved by factors such as vertical gradients of light or turbulence, symbiosis or commensalism, differential predation, or constantly changing environmental conditions. More recent work has proposed that the paradox can be resolved by factors such as: chaotic fluid motion; size-selective grazing; spatio-temporal heterogeneity; and environmental fluctuations. More generally, some researchers suggest that ecological and environmental factors continually interact such that the planktonic habitat never reaches an equilibrium for which a single species is favoured. In Mitchell et al. (2008), researchers found that small-scale analysis of plankton distribution exhibited patches of aggregation, on the order of 10 cm, that had sufficient lifetimes (> 10 minutes) to enable plankton grazing, competition, and infection.\n\n\n"}
{"id": "27901050", "url": "https://en.wikipedia.org/wiki?curid=27901050", "title": "Peter Koellner", "text": "Peter Koellner\n\nPeter Koellner is Professor of Philosophy at Harvard University. He received his Ph.D from MIT in 2003. His main areas of research are mathematical logic, specifically set theory, and philosophy of mathematics, philosophy of physics, analytic philosophy, and philosophy of language.\n\nIn 2008 Koellner was awarded a Kurt Gödel Centenary Research Prize Fellowship. Currently, Koellner serves on the American Philosophical Association's Advisory Committee to the Eastern Division Program Committee in the area of Logic.\n\nAccording to a review by Pierre Matet on Zentralblatt MATH, his joint paper with Hugh Woodin \"Incompatible Ω-Complete Theories\" contains an illuminating discussion of the issues involved, which makes it recommended reading for anyone interested in modern set theory.\n\n\n"}
{"id": "48136809", "url": "https://en.wikipedia.org/wiki?curid=48136809", "title": "Petersen–Morley theorem", "text": "Petersen–Morley theorem\n\nIn geometry, the Petersen–Morley theorem states that, if\n,\n\nare three general skew lines in space, if\n,\nrespectively for the pairs , and ,\nand if , and \nare the lines of shortest distance respectively\nfor the pairs , and , then there\nis a single line meeting at right angles all of\n,\nand\n\nThe theorem is named after Julius Petersen and Frank Morley.\n\n"}
{"id": "53375344", "url": "https://en.wikipedia.org/wiki?curid=53375344", "title": "Plant arithmetic", "text": "Plant arithmetic\n\nPlant arithmetic is a form of plant cognition whereby plants appear to perform arithmetic operations – a form of number sense in plants.\n\nThe Venus flytrap can count to two and five in order to trap and then digest its prey.\n\nThe Venus flytrap is a carnivorous plant that catches its prey with a trapping structure formed by the terminal portion of each of the plant's leaves, which is triggered by tiny hairs on their inner surfaces. When an insect or spider crawling along the leaves contacts a hair, the trap prepares to close, snapping shut only if a second contact occurs within approximately twenty seconds of the first strike. The requirement of redundant triggering in this mechanism serves as a safeguard against wasting energy by trapping objects with no nutritional value, and the plant will only begin digestion after five more stimuli to ensure it has caught a live bug worthy of consumption.\n\nThe mechanism is so highly specialized that it can distinguish between living prey and non-prey stimuli, such as falling raindrops; two trigger hairs must be touched in succession within 20 seconds of each other or one hair touched twice in rapid succession, whereupon the lobes of the trap will snap shut, typically in about one-tenth of a second.\n\n\"Arabidopsis thaliana\" performs division to control starch use at night.\n\nMost plants accumulate starch by day, then metabolize it at a fixed rate during night time. However, if the onset of darkness is unusually early, \"Arabidopsis thaliana\" reduces its use of starch by an amount that effectively requires division. However, there are alternative explanations, such as feedback control by sensing the amount of soluble sugars left is possible, but as of 2015, open questions remain.\n\n"}
{"id": "652660", "url": "https://en.wikipedia.org/wiki?curid=652660", "title": "Poncelet–Steiner theorem", "text": "Poncelet–Steiner theorem\n\nIn Euclidean geometry, the Poncelet–Steiner theorem is one of several results concerning compass and straightedge constructions with additional restrictions. This result states that whatever can be constructed by straightedge and compass together can be constructed by straightedge alone, provided that a single circle and its centre are given. This should be contrasted with the Mohr–Mascheroni theorem, which states that any compass and straightedge construction can be performed with only a compass.\n\nIt is not possible to construct everything that can be constructed with straightedge and compass with straightedge alone. and furthermore, if the centre of the circle is not given, it cannot be obtained by a straightedge alone. Also, the entire circle is not required. In 1904, Francesco Severi proved that any small arc together with the centre will suffice.\n\nIn the tenth century, the Persian mathematician Abu al-Wafa' Buzjani (940−998) considered geometric constructions using a straightedge and a compass with a fixed opening, a so-called \"rusty compass\". Constructions of this type appeared to have some practical significance as they were used by artists Leonardo da Vinci and Albrecht Dürer in Europe in the late fifteenth century. A new viewpoint developed in the mid sixteenth century when the size of the opening was considered fixed but arbitrary and the question of how many of Euclid's constructions could be obtained was paramount.\n\nRenaissance mathematician Lodovico Ferrari, a student of Gerolamo Cardano in a \"mathematical challenge\" against Niccolò Fontana Tartaglia was able to show that \"all of Euclid\" (that is, the straightedge and compass constructions in the first six books of Euclid's Elements) could be accomplished with a straightedge and rusty compass. Within ten years additional sets of solutions were obtained by Cardano, Tartaglia and Tartaglia's student Benedetti. During the next century these solutions were generally forgotten until, in 1673, Georg Mohr published (anonymously and in Dutch) \"Euclidis Curiosi\" containing his own solutions. Mohr had only heard about the existence of the earlier results and this led him to work on the problem.\n\nShowing that \"all of Euclid\" could be performed with straightedge and rusty compass is not the same as proving that \"all\" straightedge and compass constructions could be done with a straightedge and just a rusty compass. Such a proof would require the formalization of what a straightedge and compass could construct. This groundwork was provided by Jean Victor Poncelet in 1822. He also conjectured and suggested a possible proof that a straightedge and rusty compass would be equivalent to a straightedge and compass, and moreover, the rusty compass need only be used once! The result that a straightedge and single circle with given centre is equivalent to a straightedge and compass was proved by Jakob Steiner in 1833.\n\n\n"}
{"id": "18045517", "url": "https://en.wikipedia.org/wiki?curid=18045517", "title": "Proof without words", "text": "Proof without words\n\nIn mathematics, a proof without words is a proof of an identity or mathematical statement which can be demonstrated as self-evident by a diagram without any accompanying explanatory text. Such proofs can be considered more elegant than more formal and mathematically rigorous proofs due to their self-evident nature. When the diagram demonstrates a particular case of a general statement, to be a proof, it must be generalisable.\n\nThe statement that the sum of all positive odd numbers up to 2\"n\" − 1 is a perfect square—more specifically, the perfect square \"n\"—can be demonstrated by a proof without words, as shown on the right. The first square is formed by 1 block; 1 is the first square. The next strip, made of white squares, shows how adding 3 more blocks makes another square: four. The next strip, made of black squares, shows how adding 5 more blocks makes the next square. This process can be continued indefinitely.\n\nThe Pythagorean theorem can be proven without words as shown in the second diagram on left. The two different methods for determining the area of the large square give the relation \nbetween the sides. This proof is more subtle than the above, but still can be considered a proof without words.\n\nJensen's inequality can also be proven graphically, as illustrated on the third diagram. The dashed curve along the \"X\" axis is the hypothetical distribution of \"X\", while the dashed curve along the \"Y\" axis is the corresponding distribution of \"Y\" values. Note that the convex mapping \"Y\"(\"X\") increasingly \"stretches\" the distribution for increasing values of \"X\".\n\n\"Mathematics Magazine\" and the \"College Mathematics Journal\" run a regular feature titled \"Proof without words\" containing, as the title suggests, proofs without words. The Art of Problem Solving and USAMTS websites run Java applets illustrating proofs without words.\n\n\n"}
{"id": "9894468", "url": "https://en.wikipedia.org/wiki?curid=9894468", "title": "RCOS (computer sciences)", "text": "RCOS (computer sciences)\n\nrCOS stands for refinement of object and component systems. It is a formal method providing component-based model-driven software development.\n\nrCOS was originally developed by He Jifeng, Zhiming Liu and Xiaoshan Li at UNU-IIST in Macau, and consists of a unified multi-view modeling notation with a theory of relational semantic and graph-based operational semantics, a refinement calculus and tool support for model construction, model analysis and verification, and model transformations. Model transformations automate refinement rules and design patterns and generate conditions as proof obligations. rCOS support multiple dimensional modeling: models at different levels of abstraction related by refinement relations, hierarchy of compositions of components, and models of different views of the system (interaction protocols of components, reactive behaviors of components, data functionality, and class structures and data types). Components are composed and integrated based on their models of interfaces to support third party composition.\n\n\n\n"}
{"id": "31986082", "url": "https://en.wikipedia.org/wiki?curid=31986082", "title": "Rigorous coupled-wave analysis", "text": "Rigorous coupled-wave analysis\n\nRigorous coupled-wave analysis (RCWA) is a semi-analytical method in computational electromagnetics that is most typically applied to solve scattering from periodic dielectric structures. It is a Fourier-space method so devices and fields are represented as a sum of spatial harmonics. \n\nThe method is based on Floquet's theorem that the solutions of periodic differential equations can be expanded with Floquet functions (or sometimes referred as Bloch wave, especially in solid-state physics community). A device is divided into layers that are each uniform in the z direction. A staircase approximation is needed for curved devices with properties such as dielectric permittivity graded along the z-direction. The electromagnetic modes in each layer are calculated and analytically propagated through the layers. The overall problem is solved by matching boundary conditions at each of the interfaces between the layers using a technique like scattering matrices. To solve for the electromagnetic modes, which are decided by the wave vector of the incident plane wave, in periodic dielectric medium, the Maxwell's equations (in partial differential form) as well as the boundary conditions are expanded by the Floquet functions and turned into infinitely large algebraic equations. With the cutting off of higher order Floquet functions, depending on the accuracy and convergence speed one needs, the infinitely large algebraic equations become finite and thus solvable by computers.\n\nBeing a Fourier-space method it suffers several drawbacks. Gibbs phenomenon is particularly severe for devices with high dielectric contrast. Truncating the number of spatial harmonics can also slow convergence and techniques like fast Fourier factorization (FFF) should be used. FFF is straightforward to implement for 1D gratings, but the community is still working on a straightforward approach for crossed grating devices. The difficulty with FFF in crossed grating devices is that the field must be decomposed into parallel and perpendicular components at all of the interfaces. This is not a straightforward calculation for arbitrarily shaped devices.\n\nBoundary conditions must be enforced at the interfaces between all the layers. When many layers are used, this becomes too large to solve simultaneously. Instead, we borrow from network theory and calculate scattering matrices. This lets us solve the boundary conditions one layer at a time. Almost without exception, however, the scattering matrices implemented for RCWA are inefficient and do not follow long standing conventions in terms of how S11, S12, S21, and S22 are defined. Other methods exist like the enhanced transmittance matrices (ETM), R matrices, H matrices, and probably more. ETM, for example, is considerably faster but less memory efficient.\n\nRCWA analysis applied to a polarized broadband reflectometry measurement is used within the semiconductor power device industry as a measurement technique to obtain detailed profile information of periodic trench structures. This technique has been used to provide trench depth and critical dimension (CD) results comparable to cross-section SEM, while having the added benefit of being both high-throughput and non-destructive.\n\nIn order to extract critical dimensions of a trench structure (depth, CD, and sidewall angle), the measured polarized reflectance data must have a sufficiently large wavelength range and analyzed with a physically valid model (for example: RCWA in combination with the ). Studies have shown that the limited wavelength range of a standard reflectometer (375 - 750 nm) does not provide the sensitivity to accurately measure trench structures with small CD values (less than 200 nm). However, by using a reflectometer with the wavelength range extended from 190 - 1000 nm, it is possible to accurately measure these smaller structures.\n\nRCWA is also used to improve diffractive structures for high efficiency solar cells. For the simulation of the whole solar cell or solar module, RCWA can be efficiently combined with the OPTOS formalism.\n\n\n"}
{"id": "18589091", "url": "https://en.wikipedia.org/wiki?curid=18589091", "title": "Ron Eglash", "text": "Ron Eglash\n\nRon Eglash (born December 25, 1958 in Chestertown, Maryland) is an American who works in cybernetics, professor of science and technology studies at the Rensselaer Polytechnic Institute, and author widely known for his work in the field of ethnomathematics, which aims to study the diverse relationships between mathematics and culture.\n\nHis research includes the use of fractal patterns in African architecture, art, and religion, and the relationships between indigenous cultures and modern technology, such as that between Native American cultural and spiritual practices and cybernetics. He holds a bachelor's degree in cybernetics and a master's in systems engineering both at the University of California, Los Angeles, and a Ph.D. in history of consciousness at the University of California, Santa Cruz. A Fulbright fellowship enabled his postdoctoral field research on African ethnomathematics, which was later published in the book \"African Fractals: Modern Computing and Indigenous Design\".\n\nDr. Eglash has also conducted studies in teaching children math and computing through simulations of indigenous and vernacular cultural practices. He explains that the simulations do not impose math externally, but rather translate the mathematical ideas already present in the cultural practices to their equivalent form in school-taught math. Examples include transformational geometry in cornrow braiding, spiral arcs in graffiti, least common multiples in percussion rhythms, and analytic geometry in Native American beadwork. His approach is one of many attempts to draw the inspiration to learn out of students' own cultural backgrounds.\n\nHe also studies social justice issues as they manifest in the practice of science and technology, ranging from the ethnic identity of “nerds” to the so-called appropriation of science and technology by groups disempowered on the basis of race, class, gender. Another branch of this research explores how the “bottom-up” egalitarian principles found in many indigenous cultures could be applied to modern society in fields from economics to political science.\n\nHe has served as a senior lecturer in comparative studies at Ohio State University, and is currently a professor at the University of Michigan, Ann Arbor.\n\n\n\n"}
{"id": "30666823", "url": "https://en.wikipedia.org/wiki?curid=30666823", "title": "SIAM Journal on Discrete Mathematics", "text": "SIAM Journal on Discrete Mathematics\n\nAlthough its official ISO abbreviation is \"SIAM J. Discrete Math.\", its publisher and contributors frequently use the shorter abbreviation \"SIDMA\".\n"}
{"id": "11989433", "url": "https://en.wikipedia.org/wiki?curid=11989433", "title": "Scott information system", "text": "Scott information system\n\nIn domain theory, a branch of mathematics and computer science, a Scott information system is a primitive kind of logical deductive system often used as an alternative way of presenting Scott domains.\n\nA Scott information system, \"A\", is an ordered triple formula_1\nsatisfying\n\nHere formula_10 means formula_11\n\nThe return value of a partial recursive function, which either returns a natural number or goes into an infinite recursion, can be expressed as a simple Scott information system as follows:\n\nThat is, the result can either be a natural number, represented by the singleton set formula_15, or \"infinite recursion,\" represented by formula_16.\n\nOf course, the same construction can be carried out with any other set instead of formula_17.\n\nThe propositional calculus gives us a very simple Scott information system as follows:\n\n\nLet \"D\" be a Scott domain. Then we may define an information system as follows\n\n\nLet formula_24 be the mapping that takes us from a Scott domain, \"D\", to the information system defined above.\n\nGiven an information system, formula_25, we can build a Scott domain as follows. \n\n\nLet formula_29 denote the set of points of A with the subset ordering. formula_29 will be a countably based Scott domain when T is countable. In general, for any Scott domain D and information system A\nwhere the second congruence is given by approximable mappings.\n\n\n"}
{"id": "1986011", "url": "https://en.wikipedia.org/wiki?curid=1986011", "title": "Simply typed lambda calculus", "text": "Simply typed lambda calculus\n\nThe simply typed lambda calculus (formula_1), a form\nof type theory, is a typed interpretation of the lambda calculus with only one type constructor: formula_2 that builds function types. It is the canonical and simplest example of a typed lambda calculus. The simply typed lambda calculus was originally introduced by Alonzo Church in 1940 as an attempt to avoid paradoxical uses of the untyped lambda calculus, and it exhibits many desirable and interesting properties.\n\nThe term \"simple type\" is also used to refer to extensions of the simply typed lambda calculus such as products, coproducts or natural numbers (System T) or even full recursion (like PCF). In contrast, systems which introduce polymorphic types (like System F) or dependent types (like the Logical Framework) are not considered \"simply typed\". The former, except full recursion, are still considered \"simple\" because the Church encodings of such structures can be done using only formula_2 and suitable type variables, while polymorphism and dependency cannot.\n\nIn this article, we use formula_4 and formula_5 to range over types. Informally, the \"function type\" formula_6 refers to the type of functions that, given an input of type formula_4, produce an output of type formula_5.\nBy convention, formula_2 associates to the right: we read formula_10 as formula_11.\n\nTo define the types, we begin by fixing a set of \"base types\", formula_12. These are sometimes called \"atomic types\" or \"type constants\". With this fixed, the syntax of types is:\n\nFor example, formula_14, generates an infinite set of types starting with formula_15formula_16formula_17formula_18formula_19formula_20\n\nWe also fix a set of \"term constants\" for the base types. For example, we might assume a base type nat, and the term constants could be the natural numbers. In the original presentation, Church used only two base types: formula_21 for \"the type of propositions\" and formula_22 for \"the type of individuals\". The type formula_21 has no term constants, whereas formula_22 has one term constant. Frequently the calculus with only one base type, usually formula_21, is considered.\n\nThe syntax of the simply typed lambda calculus is essentially that of the lambda calculus itself. We write formula_26 to denote that the variable formula_27 is of type formula_5. The term syntax, in BNF, is then:\n\nwhere formula_30 is a term constant.\n\nThat is, \"variable reference\", \"abstractions\", \"application\", and \"constant\". A variable reference formula_27 is \"bound\" if it is inside of an abstraction binding formula_27. A term is \"closed\" if there are no unbound variables.\n\nCompare this to the syntax of untyped lambda calculus:\n\nWe see that in typed lambda calculus every function (\"abstraction\") must specify the type of its argument.\n\nTo define the set of well typed lambda terms of a given type, we will define a typing relation between terms and types. First, we introduce \"typing contexts\" or \"typing environments\" formula_34, which are sets of typing assumptions. A \"typing assumption\" has the form formula_35, meaning formula_27 has type formula_4.\n\nThe \"typing relation\" formula_38 indicates that formula_39 is a term of type formula_4 in context formula_41. In this case formula_39 is said to be \"well-typed\" (having type formula_4). Instances of the typing relation are called \"typing judgements\". The validity of a typing judgement is shown by providing a \"typing derivation\", constructed using typing rules (wherein the premises above the line allow us to derive the conclusion below the line). Simply-typed lambda calculus uses these rules:\n\nIn words,\n\nExamples of closed terms, \"i.e.\" terms typable in the empty context, are:\nThese are the typed lambda calculus representations of the basic combinators of combinatory logic.\n\nEach type formula_5 is assigned an order, a number formula_68. For base types, formula_69; for function types, formula_70. That is, the order of a type measures the depth of the most left-nested arrow. Hence:\n\nBroadly speaking, there are two different ways of assigning meaning to the simply typed lambda calculus, as to typed languages more generally, sometimes called \"intrinsic\" vs. \"extrinsic\", or \"Church-style\" vs. \"Curry-style\".\nAn intrinsic/Church-style semantics only assigns meaning to well-typed terms, or more precisely, assigns meaning directly to typing derivations. This has the effect that terms differing only by type annotations can nonetheless be assigned different meanings. For example, the identity term formula_73 on integers and the identity term formula_74 on booleans may mean different things. (The classic intended interpretations\nare the identity function on integers and the identity function on boolean values.)\nIn contrast, an extrinsic/Curry-style semantics assigns meaning to terms regardless of typing, as they would be interpreted in an untyped language. In this view, formula_73 and formula_74 mean the same thing (\"i.e.\", the same thing as formula_77).\n\nThe distinction between intrinsic and extrinsic semantics is sometimes associated with the presence or absence of annotations on lambda abstractions, but strictly speaking this usage is imprecise. It is possible to define a Curry-style semantics on annotated terms simply by ignoring the types (\"i.e.\", through type erasure), as it is possible to give a Church-style semantics on unannotated terms when the types can be deduced from context (\"i.e.\", through type inference). The essential difference between intrinsic and extrinsic approaches is just whether the typing rules are viewed as defining the language, or as a formalism for verifying properties of a more primitive underlying language. Most of the different semantic interpretations discussed below can be seen through either a Church or Curry perspective.\n\nThe simply typed lambda calculus has the same equational theory of βη-equivalence as untyped lambda calculus, but subject to type restrictions. The equation for beta reduction\nholds in context formula_41 whenever formula_80 and formula_81, while the equation for eta reduction\nholds whenever formula_83 and formula_27 does not appear free in formula_85.\n\nLikewise, the operational semantics of simply typed lambda calculus can be fixed as for the untyped lambda calculus, using call by name, call by value, or other evaluation strategies. As for any typed language, type safety is a fundamental property of all of these evaluation strategies. Additionally, the strong normalization property described below implies that any evaluation strategy will terminate on all simply typed terms.\n\nThe simply typed lambda calculus (with formula_86-equivalence) is the internal language of Cartesian closed categories (CCCs), as was first observed by Lambek. Given any specific CCC, the basic types of the corresponding lambda calculus are just the objects, and the terms are the morphisms. Conversely, every simply typed lambda calculus gives a CCC whose objects are the types, and morphisms are equivalence classes of terms.\n\nTo make the correspondence clear, a type constructor for the Cartesian product is typically added to the above. To preserve the categoricity of the Cartesian product, one adds type rules for \"pairing\", \"projection\", and a \"unit term\". Given two terms formula_87 and formula_88, the term formula_89 has type formula_90. Likewise, if one has a term formula_91, then there are terms formula_92 and formula_93 where the formula_94 correspond to the projections of the Cartesian product. The \"unit term\", of type 1, is written as formula_95 and vocalized as 'nil', is the final object. The equational theory is extended likewise, so that one has\nThis last is read as \"if t has type 1, then it reduces to nil\".\n\nThe above can then be turned into a category by taking the types as the objects. The morphisms formula_100 are equivalence classes of pairs formula_101 where \"x\" is a variable (of type formula_4) and \"t\" is a term (of type formula_5), having no free variables in it, except for (optionally) \"x\". Closure is obtained from currying and application, as usual.\n\nMore precisely, there exist functors between the category of Cartesian closed categories, and the category of simply-typed lambda theories.\n\nIt is common to extend this case to closed symmetric monoidal categories by using a linear type system. The reason for this is that the CCC is a special case of the closed symmetric monoidal category, which is typically taken to be the category of sets. This is fine for laying the foundations of set theory, but the more general topos seems to provide a superior foundation.\n\nThe simply typed lambda calculus is closely related to the implicational fragment of propositional intuitionistic logic, i.e., minimal logic, via the Curry–Howard isomorphism: terms correspond precisely to proofs in natural deduction, and inhabited types are exactly the tautologies of minimal logic.\n\nThe presentation given above is not the only way of defining the syntax of the simply typed lambda calculus. One alternative is to remove type annotations entirely (so that the syntax is identical to the untyped lambda calculus), while ensuring that terms are well-typed via Hindley–Milner type inference. The inference algorithm is terminating, sound, and complete: whenever a term is typable, the algorithm computes its type. More precisely, it computes the term's principal type, since often an unannotated term (such as formula_77) may have more than one type (formula_105, formula_106, etc., which are all instances of the principal type formula_107).\n\nAnother alternative presentation of simply typed lambda calculus is based on bidirectional type checking, which requires more type annotations than Hindley–Milner inference but is easier to describe. The type system is divided into two judgments, representing both \"checking\" and \"synthesis\", written formula_108 and formula_109 respectively. Operationally, the three components formula_41, formula_39, and formula_5 are all \"inputs\" to the checking judgment formula_108, whereas the synthesis judgment formula_109 only takes formula_41 and formula_39 as inputs, producing the type formula_5 as output. These judgments are derived via the following rules:\n\nObserve that rules [1]–[4] are nearly identical to rules (1)–(4) above, except for the careful choice of checking or synthesis judgments. These choices can be explained like so:\nObserve that the rules for synthesis are read top-to-bottom, whereas the rules for checking are read bottom-to-top. Note in particular that we do not need any annotation on the lambda abstraction in rule [3], because the type of the bound variable can be deduced from the type at which we check the function. Finally, we explain rules [5] and [6] as follows:\n\nBecause of these last two rules coercing between synthesis and checking, it is easy to see that any well-typed but unannotated term can be checked in the bidirectional system, so long as we insert \"enough\" type annotations. And in fact, annotations are needed only at β-redexes.\n\nGiven the standard semantics, the simply typed lambda calculus is strongly normalizing: that is, well-typed terms always reduce to a value, i.e., a formula_132 abstraction. This is because recursion is not allowed by the typing rules: it is impossible to find types for fixed-point combinators and the looping term formula_133. Recursion can be added to the language by either having a special operator formula_134of type formula_135 or adding general recursive types, though both eliminate strong normalization.\n\nSince it is strongly normalising, it is decidable whether or not a simply typed lambda calculus program halts: in fact, it \"always\" halts. We can therefore conclude that the language is \"not\" Turing complete.\n\n\n\n"}
{"id": "1454130", "url": "https://en.wikipedia.org/wiki?curid=1454130", "title": "Slashed zero", "text": "Slashed zero\n\nThe slashed zero is a representation of the number '0' (zero), with a slash through it. The slashed zero glyph is often used to distinguish the digit \"zero\" (\"0\") from the Latin script letter \"O\" anywhere that the distinction needs emphasis, particularly in encoding systems, scientific and engineering applications, computer programming (such as software development), and telecommunications. It thus helps to differentiate characters that would otherwise be homoglyphs. It was commonly used during the punched card era, when programs were typically written out by hand, to avoid ambiguity when the character was later typed on a card punch.\n\nUnlike in the Scandinavian vowel 'Ø' and the \"empty set\" symbol '∅', the slash of a slashed zero usually does not extend past the ellipse in most typographic designs. However, the slashed zero is sometimes approximated by overlaying zero and slash characters, producing the character \"0̸\". Some fonts make slashed zero look like \"0/\"\n\nIn character encoding terms, it has no explicit code point, but it is an alternate glyph (in addition to the open zero glyph) for the zero character.\n\nThe slashed zero long predates computers, and is known to have been used in the twelfth and thirteenth centuries. It is used in many Baudot teleprinter applications, specifically the keytop and typepallet that combines \"P\" and slashed zero. Additionally, the slashed zero is used in many ASCII graphic sets descended from the default typewheel on the Teletype Model 33.\n\nThe slashed zero is used in a number of fields in order to avoid confusion with the letter 'O'. It is used by computer programmers, in recording amateur radio call signs and in military radio, as logs of such contacts tend to contain both letters and numerals.\n\nThe slashed zero, sometimes called communications zero, was used on teleprinter circuits for weather applications.\n\nThe slashed zero can be used in stoichiometry to avoid confusion with the symbol for oxygen (capital O).\n\nAlong with the Westminster, MICR, and OCR-A fonts, the slashed zero became one of the things associated with hacker culture in the 1980s. Some cartoons depicted computer users talking in binary code with 1s and 0s using a \"slashed zero\" for the 0.\n\nTo generate a slashed zero on typewriters, typists would type a normal \"O\" or zero, backspace, and then hit the slash key to mark the zero.\n\nThe use of the slashed zero by many computer systems of the 1970s and 1980s inspired the 1980s space rock band Underground Zerø to use a heavy metal umlaut Scandinavian vowel \"ø\" in the band's name and as the band logo on all their album covers (see link below).\n\nSlashed zeroes have been used in the Flash-based artwork of Young-Hae Chang Heavy Industries, notably in their 2003 work, Operation Nukorea. The reason for their use is unknown, but has been conjectured to be related to themes of 'negation, erasure, and absence'.\n\nSlashed zeroes can also be used on cheques in order to prevent fraud, for example: changing a 0 to an 8.\n\nThe treatment of slashed zero as a glyph is supported by any font whose designer chose the option. Successful display on any local system depends on having the font available there, either via the system's font files or via font embedding.\n\nUnicode supports explicit slashed zero, but only via a pair of combining characters, not as a distinct single character (or \"code point\", in Unicode parlance). It is treated literally as \"a zero that is slashed\", and it is coded as two characters: the commonplace zero and then the \"combining long solidus overlay\" codice_1. These combining characters overlay the preceding character, creating a composite glyph.\n\nWhen used in HTML, use of such combining characters is valid but not yet supported by all current web browsers. They may be coded as codice_2 giving 0̸.\n\nUnicode 9.0 introduced another method to create a short diagonal stroked form of zero by utilizing the Variation Selector 1 (VS1) in the sequence U+0030 U+FE00.\n\nThe \"slashed zero\" has the disadvantage that it can be confused with several other symbols:\n\n\nThe zero with a dot in the center seems to have originated as an option on IBM 3270 display controllers. The dotted zero may appear similar to the Greek letter theta (particularly capital theta, Θ), but the two have different glyphs. In raster fonts, the theta usually has a horizontal line connecting, or nearly touching, the sides of an O; while the dotted zero simply has a dot in the middle. However, on a low-definition display, such a form can be confused with a numeral 8. In some fonts the IPA letter for a bilabial click (ʘ) looks similar to the dotted zero.\n\nAlternatively, the dot can become a vertical trace, for example by adding a \"combining short vertical line overlay\" codice_3. It may be coded as codice_4 giving 0⃓.\n\nIBM (and a few other early mainframe makers) used a convention in which the letter O had a slash and the digit 0 did not. This is even more problematic for Danes, Faroese, and Norwegians because it means two of their letters—the O and slashed O (Ø)—are visually similar.\n\nThis was later flipped and most mainframe chain or band printers used the opposite convention (letter O printed as is, and digit zero printed with a slash Ø). This was the de facto standard from 70's to 90's. However current use of network laser printers that use PC style fonts caused the demise of the slashed zero in most companies - only a few configured laser printers to use Ø.\n\nUse of the \"combining short solidus overlay\" codice_5 produces a result where the slash is contained within the zero. This may be coded as codice_6 to yield 0̷.\n\nSome Burroughs/Unisys equipment displays a zero with a \"reversed\" slash, similar to the no symbol,  ⃠.\n\nYet another convention common on early line printers left zero unornamented but added a tail or hook to the letter-O so that it resembled an inverted Q (like U+213A ℺) or cursive capital letter-O (formula_1).\n\nIn the Fixedsys typeface, the numeral 0 has two internal barbs along the lines of the slash. This appears much like a white \"S\" within the black borders of the zero.\n\nIn the FE-Schrift typeface, used on German car license plates because it is harder to alter, the zero is rectangular and has an \"insinuated\" slash: a diagonal crack just beneath the top right curve.\n\nTypefaces commonly found on personal computers that use the slashed zero include:\n\n\nDotted zero typefaces:\n\n\n\n"}
{"id": "6512886", "url": "https://en.wikipedia.org/wiki?curid=6512886", "title": "Stephen Schanuel", "text": "Stephen Schanuel\n\nStephen H. Schanuel (1934—2014) was an American mathematician working in the fields of abstract algebra and category theory, number theory, and measure theory.\n\nWhile he was a graduate student at University of Chicago, he discovered Schanuel's lemma, an essential lemma in homological algebra. Schanuel received his Ph.D. in mathematics from Columbia University in 1963, under the supervision of Serge Lang.\n\nShortly thereafter he stated a conjecture in the field of transcendental number theory, which remains an important open problem to this day. Schanuel was a professor emeritus of mathematics at University at Buffalo.\n\n"}
{"id": "1221919", "url": "https://en.wikipedia.org/wiki?curid=1221919", "title": "Sturm's theorem", "text": "Sturm's theorem\n\nIn mathematics, the Sturm sequence of a univariate polynomial is a sequence of polynomials associated with and its derivative by a variant of Euclid's algorithm for polynomials. Sturm's theorem expresses the number of distinct real roots of located in an interval in terms of the number of changes of signs of the values of the Sturm sequence at the bounds of the interval. Applied to the interval of all the real numbers, it gives the total number of real roots of .\n\nWhereas the fundamental theorem of algebra readily yields the overall number of complex roots, counted with multiplicity, it does not provide a procedure for calculating them. Sturm's theorem counts the number of distinct real roots and locates them in intervals. By subdividing the intervals containing some roots, it can isolate the roots into arbitrary small intervals, each containing exactly one root. This yields an arbitrary-precision numeric root-finding algorithm for univariate polynomials.\n\nFor computing over the reals, Sturm's theorem is less efficient than other methods based on Descartes' rule of signs. However, it works on every real closed field, and, therefore, remains fundamental for the theoretical study of the computational complexity of decidability and quantifier elimination in the first order theory of real numbers.\n\nThe Sturm sequence and Sturm's theorem are named after Jacques Charles François Sturm, who discovered the theorem in 1829.\n\nThe Sturm chain or Sturm sequence of a univariate polynomial with real coefficients is the sequence of polynomials formula_1 such that \nfor , where is the derivative of , and formula_3 is the remainder of the Euclidean division of formula_4 by formula_5 The length of the Sturm sequence is at most the degree of .\n\nThe number of sign variations at of the Sturm sequence of is the number of sign changes–ignoring zeros—in the sequence of real numbers\nThis number of sign variations is denoted here . \n\nSturm's theorem states that, if is a square-free polynomial, the number of distinct real roots of in the half-open interval is (here, and are real numbers such that ).\n\nThe theorem extends to unbounded intervals by defining the sign at of a polynomial as the sign of its leading coefficient (that is, the coefficient of the term of highest degree). At the sign of a polynomial is the sign of its leading coefficient for a polynomial of even degree, and the opposite sign for a polynomial of odd degree.\n\nIn the case of a non-square-free polynomial, if neither nor is a multiple root of , then is the number of \"distinct\" real roots of .\n\nThe proof of the theorem is as follows: when the value of increases from to , it may pass through a zero of some formula_7 (); when this occurs, the number of sign variations of formula_8 does not changes. When passes through a root of formula_9 the number of sign variations of formula_10 decreases from 1 to 0. These are the only values of where some sign may change.\n\nSuppose we wish to find the number of roots in some range for the polynomial formula_11. So\n\nThe remainder of the Euclidean division of by is formula_13 multiplying it by we obtain \nNext dividing by and multiplying the remainder by , we obtain \nNow dividing by and multiplying the remainder by , we obtain \nAs this is a constant, this finishes the computation of the Sturm sequence.\n\nTo find the number of real roots between of formula_17 one has to evaluate the sequences of the signs of these polynomials at and , which are respectively and . Thus\nwhich shows that has two real roots.\n\nThis can be verified by noting that can be factored as , where the first factor has the roots and , and second factor has no real roots. This last assertion results from the quadratic formula, and also from Sturm's theorem, which gives the sign sequences at and at .\n\nSturm sequences have been generalized in two directions. To define each polynomial in the sequence, Sturm used the negative of the remainder of the Euclidean division of the two preceding ones. The theorem remains true if one replaces the negative of the remainder by its product or quotient by a positive constant or the square of a polynomial. It is also useful (see below) to consider sequences where the second polynomial is not the derivative of the first one.\n\nA \"generalized Sturm sequence\" is a finite sequence of polynomials with real coefficients\nsuch that\n\nThe last condition implies that two consecutive polynomials do not have any common real root. In particular\nthe original Sturm sequence is a generalized Sturm sequence, if (and only if) the polynomial has no multiple real root (if a polynomial has a multiple root, the two first polynomials of its Sturm sequence have a common toot). \n\nWhen computing the original Sturm sequence by Euclidean division, it may happen that one encounters a polynomial that has a factor that is never negative, such a formula_22 or formula_23. In this case, if one continues the computation with the polynomial replaced by its quotient by the nonnegative factor, one gets a generalized Sturm sequence, which may also be used for computing the number of real roots, since the proof of Sturm's theorem still applies (because of the third condition). This may sometimes simplify the computation, although it is generally difficult to find such nonnegative factors, except for even powers of .\n\nIn computer algebra, the polynomials that are considered have integer coefficients or may be transformed to have integer coefficients. The Sturm sequence of a polynomial with integer coefficients generally contains polynomials whose coefficients are not integers (see above example). \n\nTo avoid computation with rational numbers, a common method is to replace Euclidean division by pseudo-division for computing polynomial greatest common divisors. This amounts to replacing the remainder sequence of the Euclidean algorithm by a pseudo-remainder sequence, a pseudo remainder sequence being a sequence formula_24 of polynomials such that there are constants formula_25 and formula_26 such that formula_27 is the remainder of the Euclidean division of formula_28 by formula_29 For example, the remainder sequence of the Euclidean algorithm is a pseudo-remainder sequence with formula_30 for every , and the Sturm sequence of a polynomial is a pseudo-remainder sequence with formula_31 and formula_32 for every .\n\nVarious pseudo-remainder sequences have been designed for computing greatest common divisors of polynomials with integer coefficients without introducing denominators (see Pseudo-remainder sequence). They can all be made generalized Sturm sequences by choosing the sign of the formula_26 to be the opposite of the sign of the formula_34 This allows the use of Sturm's theorem with pseudo-remainder sequences.\n\nFor a polynomial with real coefficients, \"root isolation\" consists of finding, for each real root, an interval that contains this root, and no other roots. \n\nThis is useful for root finding, allowing the selection of the root to be found and providing a good starting point for fast numerical algorithms such as Newton's method; it is also useful for certifying the result, as if Newton's method converge outside the interval one may immediately deduce that it converges to the wrong root.\n\nRoot isolation is also useful for computing with algebraic numbers. For computing with algebraic numbers, a common method is to represent them as a pair of a polynomial to which the algebraic number is a root, and an isolation interval. For example formula_35 may be unambiguously represented by formula_36\n\nSturm's theorem provides a way for isolating real roots that is less efficient (for polynomials with integer coefficients) than other methods involving Descartes' rule of signs. However, it remains useful in some circumstances, mainly for theoretical purposes, for example for algorithms of real algebraic geometry that involve infinitesimals.\n\nFor isolating the real roots, one starts from an interval formula_37 containing all the real roots, or the roots of interest (often, typically in physical problems, only positive roots are interesting), and one computes formula_38 and formula_39 For defining this starting interval, one may use bounds on the size of the roots (see ). Then, one divides this interval in two, by choosing in the middle of formula_40 The computation of formula_41 provides the number of real roots in formula_42 and formula_43 and one may repeat the same operation on each subinterval. When one encounters, during this process an interval that does not contain any root, it may be suppressed from the list of intervals to consider. When one encounters an interval containing exactly one root, one may stop dividing it, as it is an isolation interval. The process stops eventually, when only isolating intervals remain. \n\nThis isolating process may be used with any method for computing the number of real roots in an interval. Theoretical complexity analysis and practical experiences show that methods based on Descartes' rule of signs are more efficient. It follows that, nowadays, Sturm sequences are rarely used for root isolation.\n\nGeneralized Sturm sequences allow counting the roots of a polynomial where another polynomial is positive (or negative), without computing these root explicitly. If one knows an isolating interval for a root of the first polynomial, this allows also finding the sign of the second polynomial at this particular root of the first polynomial, without computing a better approximation of the root.\n\nLet and be two polynomials with real coefficients such that and have no common root and has no multiple roots. In other words, and are coprime polynomials. This restriction does not really affect the generality of what follows as GCD computations allows reducing the general case to this case, and the cost of the computation of a Sturm sequence is the same as that of a GCD. \n\nLet denote the number of sign variations at of a generalized Sturm sequence starting from and . If are two real numbers, then is the number of roots of in the interval formula_37 such that minus the number of roots in the same interval such that . Combined with the total number of roots of in the same interval given by Sturm's theorem, this gives the number of roots of such that and the number of roots of such that .\n\n"}
{"id": "31968302", "url": "https://en.wikipedia.org/wiki?curid=31968302", "title": "Synopsis of Pure Mathematics", "text": "Synopsis of Pure Mathematics\n\nSynopsis of Pure Mathematics is a book by G. S. Carr, written in 1886. The book attempted to summarize the state of most of the basic mathematics known at the time.\n\nThe book is noteworthy because it was a major source of information for the legendary and self-taught mathematician Srinivasa Ramanujan who managed to obtain a library loaned copy from a friend in 1903. Ramanujan reportedly studied the contents of the book in detail. The book is generally acknowledged as a key element in awakening the genius of Ramanujan.\n\nCarr acknowledged the main sources of his book in its preface:\n\n"}
{"id": "54139796", "url": "https://en.wikipedia.org/wiki?curid=54139796", "title": "Van der Corput inequality", "text": "Van der Corput inequality\n\nIn mathematics, the van der Corput inequality is a corollary of the Cauchy–Schwarz inequality that is useful in the study of correlations among vectors, and hence random variables. It is also useful in the study of equidistributed sequences, for example in the Weyl equidistribution estimate. Loosely stated, the van der Corput inequality asserts that if a unit vector formula_1 in an inner product space formula_2 is strongly correlated with many unit vectors formula_3, then many of the pairs formula_4 must be strongly correlated with each other. Here, the notion of correlation is made precise by the inner product of the space formula_2: when the absolute value of formula_6 is close to formula_7, then formula_8 and formula_1 are considered to be strongly correlated. (More generally, if the vectors involved are not unit vectors, then strong correlation means that formula_10.)\n\nLet formula_2 be a real or complex inner product space with inner product formula_12 and induced norm formula_13. Suppose that formula_14 and that formula_15. Then\n\nIn terms of the correlation heuristic mentioned above, if formula_1 is strongly correlated with many unit vectors formula_18, then the left-hand side of the inequality will be large, which then forces a significant proportion of the vectors formula_19 to be strongly correlated with one another.\n\n"}
{"id": "51441", "url": "https://en.wikipedia.org/wiki?curid=51441", "title": "Zero divisor", "text": "Zero divisor\n\nIn abstract algebra, an element of a ring is called a left zero divisor if there exists a nonzero such that , or equivalently if the map from to that sends to is not injective. Similarly, an element of a ring is called a right zero divisor if there exists a nonzero such that . This is a partial case of divisibility in rings. An element that is a left or a right zero divisor is simply called a zero divisor. An element  that is both a left and a right zero divisor is called a two-sided zero divisor (the nonzero such that may be different from the nonzero such that ). If the ring is commutative, then the left and right zero divisors are the same.\n\nAn element of a ring that is not a zero divisor is called regular, or a non-zero-divisor. A zero divisor that is nonzero is called a nonzero zero divisor or a nontrivial zero divisor. If there are no nontrivial zero divisors in , then is a domain.\n\n\n\n\n\nThere is no need for a separate convention regarding the case , because the definition applies also in this case: \n\nSuch properties are needed in order to make the following general statements true:\n\nSome references choose to exclude as a zero divisor by convention, but then they must introduce exceptions in the two general statements just made.\n\nLet be a commutative ring, let be an -module, and let be an element of . One says that is -regular if the multiplication by map formula_48 is injective, and that is a zero divisor on otherwise. The set of -regular elements is a multiplicative set in .\n\nSpecializing the definitions of \"-regular\" and \"zero divisor on \" to the case recovers the definitions of \"regular\" and \"zero divisor\" given earlier in this article.\n\n\n"}
