{"id": "1549929", "url": "https://en.wikipedia.org/wiki?curid=1549929", "title": "173 (number)", "text": "173 (number)\n\n173 (one hundred [and] seventy-three) is the natural number following 172 and preceding 174.\n\n173 is:\n\n\n\n\n173 is also:\n\n\n"}
{"id": "391881", "url": "https://en.wikipedia.org/wiki?curid=391881", "title": "89 (number)", "text": "89 (number)\n\n89 (eighty-nine) is the natural number following 88 and preceding 90.\n\n89 is:\n\n\n\"M\" is the 10th Mersenne prime.\n\nAlthough 89 is not a Lychrel number in base 10, it is unusual that it takes 24 iterations of the reverse and add process to reach a palindrome. Among the known non-Lychrel numbers in the first 10000 integers, no other number requires that many or more iterations. The palindrome reached is also unusually large.\n\nEighty-nine is:\n\n\n\n\nEighty-nine is also:\n\n\n"}
{"id": "8288587", "url": "https://en.wikipedia.org/wiki?curid=8288587", "title": "Action (UML)", "text": "Action (UML)\n\nIn the Unified Modeling Language, an action is a named element that is the fundamental unit of executable functionality. The execution of an action represents some transformation or processing in the modeled system. An action execution represents the run-time behavior of executing an action within a specific behavior execution. All action executions will be executions of specific kinds of actions because action is an abstract class. When the action executes, and what its actual inputs are, is determined by the concrete action and the behaviors in which it is used.\n\nAn action is the specification of an executable statement and is the fundamental unit of processing or behavior in an activity node that represents some transformation in the modeled system.\n\nAn action forms an abstraction of a computational procedure which is an atomic execution and therefore completes without interruption. An action is considered to take zero time and cannot be interrupted. In contrast, an activity is a more complex collection of behavior that may run for a long duration. An activity may be interrupted by events, in which case it does not run to completion.\n\nAn action is a result of a system state change and is realized by sending a message to an object or modifying a link or a value of an attribute.\n\nAn action may receive inputs in the form of control flows and object flows (the latter via input pins) and passes the results of its processing or transformations to one or more outgoing control flows or object flows (the latter via output pins) and onto downstream nodes.\n\nExecution of the action cannot begin until all its prerequisites are satisfied. All incoming control flows have control tokens and all input pins have object tokens.\n\nAn action refers to the suite of rules and policies associated with a state machine state, and is represented as an object method.\n\nActions are contained within and are provided context by activities.\n\nAn action behavior accompanies a transition event.\n\n"}
{"id": "7844336", "url": "https://en.wikipedia.org/wiki?curid=7844336", "title": "Apartness relation", "text": "Apartness relation\n\nIn constructive mathematics, an apartness relation is a constructive form of inequality, and is often taken to be more basic than equality. It is often written as # to distinguish from the negation of equality (the \"denial inequality\") ≠, which is weaker. \n\nAn apartness relation is a symmetric irreflexive binary relation with the additional condition that if two elements are apart, then any other element is apart from at least one of them (this last property is often called \"co-transitivity\" or \"comparison\").\n\nThat is, a binary relation # is an apartness relation if it satisfies:\nThe negation of an apartness relation is an equivalence relation, as the above three conditions become reflexivity, symmetry, and transitivity. If this equivalence relation is in fact equality, then the apartness relation is called \"tight\". That is, # is a tight apartness relation if it additionally satisfies:\nIn classical mathematics, it also follows that every apartness relation is the negation of an equivalence relation, and the only tight apartness relation on a given set is the negation of equality. So in that domain, the concept is not useful. In constructive mathematics, however, this is not the case.\n\nThe prototypical apartness relation is that of the real numbers: two real numbers are said to be apart if there exists (one can construct) a rational number between them. In other words, real numbers \"x\" and \"y\" are apart if there exists a rational number \"z\" such that \"x\" < \"z\" < \"y\" or \"y\" < \"z\" < \"x\". The natural apartness relation of the real numbers is then the disjunction of its natural pseudo-order. The complex numbers, real vector spaces, and indeed any metric space then naturally inherit the apartness relation of the real numbers, even though they do not come equipped with any natural ordering.\n\nIf there is no rational number between two real numbers, then the two real numbers are equal. Classically, then, if two real numbers are not equal, one would conclude that there exists a rational number between them. However it does not follow that one can actually construct such a number. Thus to say two real numbers are apart is a stronger statement, constructively, than to say that they are not equal, and while equality of real numbers is definable in terms of their apartness, the apartness of real numbers cannot be defined in terms of their equality. For this reason, in constructive topology especially, the apartness relation over a set is often taken as primitive, and equality is a defined relation.\n\nA set endowed with an apartness relation is known as a constructive setoid. A function formula_5 where \"A\" and \"B\" are constructive setoids is called a \"morphism\" for # and # if formula_6.\n"}
{"id": "24783293", "url": "https://en.wikipedia.org/wiki?curid=24783293", "title": "Assumed mean", "text": "Assumed mean\n\nIn statistics the assumed mean is a method for calculating the arithmetic mean and standard deviation of a data set. It simplifies calculating accurate values by hand. Its interest today is chiefly historical but it can be used to quickly estimate these statistics. There are other rapid calculation methods which are more suited for computers which also ensure more accurate results than the obvious methods.\n\nFirst:\nThe mean of the following numbers is sought:\n\nSuppose we start with a plausible initial guess that the mean is about 240. Then the deviations from this \"assumed\" mean are the following:\n\nIn adding these up, one finds that:\n\nand so on. We are left with a sum of −30. The \"average\" of these 15 deviations from the assumed mean is therefore −30/15 = −2. Therefore, that is what we need to add to the assumed mean to get the correct mean:\n\nThe method depends on estimating the mean and rounding to an easy value to calculate with. This value is then subtracted from all the sample values. When the samples are classed into equal size ranges a central class is chosen and the count of ranges from that is used in the calculations. For example, for people's heights a value of 1.75m might be used as the assumed mean.\n\nFor a data set with assumed mean \"x\" suppose:\n\nThen\n\nor for a sample standard deviation using Bessel's correction:\n\nWhere there are a large number of samples a quick reasonable estimate of the mean and standard deviation can be got by grouping the samples into classes using equal size ranges. This introduces a quantization error but is normally accurate enough for most purposes if 10 or more classes are used.\n\nFor instance with the sample:\n\nThe minimum and maximum are 159.6 and 187.6 we can group them as follows rounding the numbers down. The class size (CS) is 3. The assumed mean is the centre of the range from 174 to 177 which is 175.5. The differences are counted in classes.\n\nThe mean is then estimated to be\n\nwhich is very close to the actual mean of 173.846.\n\nThe standard deviation is estimated as\n"}
{"id": "863579", "url": "https://en.wikipedia.org/wiki?curid=863579", "title": "Call-by-push-value", "text": "Call-by-push-value\n\nIn programming language theory, the call-by-push-value (CBPV) paradigm, inspired by monads, allows writing semantics for lambda-calculus without writing two variants to deal with the difference between call-by-name and call-by-value. To do so, CBPV introduces a term language that distinguishes computations and values, according to the slogan \"a value is, a computation does\"; this term language has a single evaluation order. However, to evaluate a lambda-calculus term according to either the call-by-name (CBN) or call-by-value (CBV) reduction strategy, one can translate the term to CBPV using a call-by-name or call-by-value translation strategy, which give rise to different terms. Evaluating the result of the call-by-value translation corresponds to evaluating the original term with the call-by-value strategy; evaluating the result of the call-by-name translation corresponds instead to evaluating the original term with the call-by-name strategy.\n\nThis is especially useful when dealing with the semantics of different side effects, such as nontermination, mutable state or nondeterminism. Instead of giving two variants of the semantics, one for the call-by-name evaluation order and one for the call-by-value one, one can simply give a semantics for the CBPV term language; one gets two semantics for lambda-calculus by composing this CBPV semantics with the same CBV and CBN translations from lambda-calculus.\n"}
{"id": "39530563", "url": "https://en.wikipedia.org/wiki?curid=39530563", "title": "Centered pentachoric number", "text": "Centered pentachoric number\n\nA centered pentachoric number is a figurate number that represents a pentachoron.\n\nThe first such numbers are: 1, 6, 21, 56, 126, 251, 456, 771, 1231, 1876, 2751, 3906, 5396, 7281, … .\n\nCentered pentachoric numbers can also be represented as the sum of the first n pentatope numbers.\n"}
{"id": "502831", "url": "https://en.wikipedia.org/wiki?curid=502831", "title": "Charles Fefferman", "text": "Charles Fefferman\n\nCharles Louis Fefferman (born April 18, 1949) is an American mathematician at Princeton University. His primary field of research is mathematical analysis.\n\nA child prodigy, Fefferman entered the University of Maryland at age 14, and had written his first scientific paper by the age of 15. He graduated with degrees in math and physics at 17, and earned his PhD in mathematics three years later from Princeton University, under Elias Stein. Fefferman achieved a full professorship at the University of Chicago at the age of 22, making him the youngest full professor ever appointed in the United States. At 24, he returned to Princeton as a full professor—a position he still holds. He won the Alan T. Waterman Award in 1976 (the first person to get the award) and the Fields Medal in 1978 for his work in mathematical analysis, specifically convergence and divergence. He was elected to the National Academy of Sciences in 1979. He was appointed the Herbert Jones Professor at Princeton in 1984.\n\nIn addition to the above, his honors include the Salem Prize in 1971, the Bôcher Memorial Prize in 2008, the Bergman Prize in 1992, and the Wolf Prize in Mathematics for 2017, as well as election to the American Academy of Arts and Sciences.\n\nFefferman contributed several innovations that revised the study of multidimensional complex analysis by finding fruitful generalisations of classical low-dimensional results. Fefferman's work on partial differential equations, Fourier analysis, in particular convergence, multipliers, divergence, singular integrals and Hardy spaces earned him a Fields Medal at the International Congress of Mathematicians at Helsinki in 1978. He was a Plenary Speaker of the ICM in 1974 in Vancouver.\n\nHis early work included a study of the asymptotics of the Bergman kernel off the boundaries of pseudoconvex domains in formula_1. He has studied mathematical physics, harmonic analysis, fluid dynamics, neural networks, geometry, mathematical finance and spectral analysis, amongst others.\n\nCharles Fefferman and his wife Julie have two daughters, Nina and Lainie. Lainie Fefferman is a composer, taught math at Saint Ann's School (New York City) and holds a degree in music from Yale University as well as a Ph.D. in music composition from Princeton. She has an interest in Middle Eastern music. Nina is a computational biologist whose research is concerned with the application of mathematical models to complex biological systems. Charles Fefferman's brother, Robert Fefferman, is also a mathematician and former Dean of the Physical Sciences Division at the University of Chicago.\n\nFefferman's most cited papers, in the order of citations, include the following.\n\n"}
{"id": "36246682", "url": "https://en.wikipedia.org/wiki?curid=36246682", "title": "Connective constant", "text": "Connective constant\n\nIn mathematics, the connective constant is a numerical quantity associated with self-avoiding walks on a lattice. It is studied in connection with the notion of universality in two-dimensional statistical physics models. While the connective constant depends on the choice of lattice so itself is not universal (similarly to other lattice-dependent quantities such as the critical probability threshold for percolation), it is nonetheless an important quantity that appears in conjectures for universal laws. Furthermore, the mathematical techniques used to understand the connective constant, for example in the recent rigorous proof by Duminil-Copin and Smirnov that the connective constant of the hexagonal lattice has the precise value formula_1, may provide clues to a possible approach for attacking other important open problems in the study of self-avoiding walks, notably the conjecture that self-avoiding walks converge in the scaling limit to the Schramm–Loewner evolution.\n\nThe connective constant is defined as follows. Let formula_2 denote the number of \"n\"-step self-avoiding walks starting from a fixed origin point in the lattice. Since every \"n\" + \"m\" step self avoiding walk can be decomposed into an \"n\"-step self-avoiding walk and an m-step self-avoiding walk, it follows that formula_3. Then by applying Fekete's lemma to the logarithm of the above relation, the limit formula_4 can be shown to exist. This number formula_5 is called the connective constant, and clearly depends on the particular lattice chosen for the walk since formula_2 does. The value of formula_5 is precisely known only for two lattices, see below. For other lattices, formula_5 has only been approximated numerically. It is conjectured that formula_9 as n goes to infinity, where formula_5 depends on the lattice, but the critical exponent formula_11 is universal (it depends on dimension, but not the specific lattice). In 2-dimensions it is conjectured that formula_12 \n\nThese values are taken from the 1998 Jensen–Guttmann paper. The connective constant of the formula_13 lattice, since each step on the hexagonal lattice corresponds to either two or three steps in it, can be expressed exactly as the largest real root of the polynomial\n\ngiven the exact expression for the hexagonal lattice connective constant. More information about these lattices can be found in the percolation threshold article.\n\nIn 2010, Hugo Duminil-Copin and Stanislav Smirnov published the first rigorous proof of the fact that formula_15 for the hexagonal lattice.\nThis had been conjectured by Nienhuis in 1982 as part of a larger study of O(\"n\") models using renormalization techniques.\nThe rigorous proof of this fact came from a program of applying tools from complex analysis to discrete probabilistic models that has also produced impressive results about the Ising model among others.\nThe argument relies on the existence of a parafermionic observable that satisfies half of the discrete Cauchy–Riemann equations for the hexagonal lattice. We modify slightly the definition of a self-avoiding walk by having it start and end on mid-edges between vertices. Let H be the set of all mid-edges of the hexagonal lattice. For a self-avoiding walk formula_11 between two mid-edges formula_17 and formula_18, we define formula_19 to be the number of vertices visited and its winding formula_20 as the total rotation of the direction in radians when formula_11 is traversed from formula_17 to formula_18. The aim of the proof is to show that the partition function\n\nconverges for formula_25 and diverges for formula_26 where the critical parameter is given by formula_27. This immediately implies that formula_28.\n\nGiven a domain formula_29 in the hexagonal lattice, a starting mid-edge formula_17, and two parameters formula_31 and formula_32, we define the parafermionic observable\n\nformula_33\n\nIf formula_34 and formula_35, then for any vertex formula_36 in formula_29, we have\n\nwhere formula_39 are the mid-edges emanating from formula_36. This lemma establishes that the parafermionic observable is divergence-free. It has not been shown to be curl-free, but this would solve several open problems (see conjectures). The proof of this lemma is a clever computation that relies heavily on the geometry of the hexagonal lattice.\n\nNext, we focus on a finite trapezoidal domain formula_41 with 2L cells forming the left hand side, T cells across, and upper and lower sides at an angle of formula_42. (Picture needed.) We embed the hexagonal lattice in the complex plane so that the edge lengths are 1 and the mid-edge in the center of the left hand side is positioned at −1/2. Then the vertices in formula_41 are given by\n\nWe now define partition functions for self-avoiding walks starting at formula_17 and ending on different parts of the boundary. Let formula_46 denote the left hand boundary, formula_47 the right hand boundary, formula_48 the upper boundary, and formula_49 the lower boundary. Let\n\nBy summing the identity\n\nover all vertices in formula_52 and noting that the winding is fixed depending on which part of the boundary the path terminates at, we can arrive at the relation\n\nafter another clever computation. Letting formula_54, we get a strip domain formula_55 and partition functions\n\nIt was later shown that formula_57, but we do not need this for the proof.\nWe are left with the relation\n\nFrom here, we can derive the inequality\n\nAnd arrive by induction at a strictly positive lower bound for formula_60. Since formula_61, we have established that formula_62.\n\nFor the reverse inequality, for an arbitrary self avoiding walk on the honeycomb lattice, we perform a canonical decomposition due to Hammersley and Welsh of the walk into bridges of widths formula_63 and formula_64. Note that we can bound\n\nwhich implies formula_66. \nFinally, it is possible to bound the partition function by the bridge partition functions\n\nAnd so, we have that formula_68 as desired.\n\nNienhuis argued in favor of Flory's prediction that the mean squared displacement of the self-avoiding random walk formula_69 satisfies the scaling relation\nformula_70,\nwith formula_71.\nThe scaling exponent formula_72 and the universal constant formula_73 could be computed if the self-avoiding walk possesses a conformally invariant scaling limit, conjectured to be a Schramm–Loewner evolution with formula_74.\n\n"}
{"id": "26661047", "url": "https://en.wikipedia.org/wiki?curid=26661047", "title": "Counting rods", "text": "Counting rods\n\nCounting rods () are small bars, typically 3–14 cm long, that were used by mathematicians for calculation in ancient East Asia. They are placed either horizontally or vertically to represent any integer or rational number.\n\nThe written forms based on them are called rod numerals. They are a true positional numeral system with digits for 1–9 and a blank for 0, from the Warring states period (circa 475 BCE) to the 16th century.\n\nChinese arithmeticians used counting rods well over two thousand years ago. In 1954 forty-odd counting rods of the Warring States period (5th century BCE to 221 BCE) were found in Zuǒjiāgōngshān (左家公山) Chu Grave No.15 in Changsha, Hunan.\n\nIn 1973 archeologists unearthed a number of wood scripts from a tomb in Hubei dating from the period of the Han dynasty (206 BCE to 220 CE). On one of the wooden scripts was written: \"当利二月定算\". This is one of the earliest examples of using counting-rod numerals in writing.\n\nIn 1976 a bundle of Western Han-era (202 BCE to 9 CE) counting rods made of bones was unearthed from Qianyang County in Shaanxi. The use of counting rods must predate it; Sunzi ( 544 to 496 BCE), a military strategist at the end of Spring and Autumn period of 771 BCE to 5th century BCE, mentions their use to make calculations to win wars before going into the battle; Laozi (died 531 BCE), writing in the Warring States period, said \"a good calculator doesn't use counting rods\". The \"Book of Han\" (finished 111 CE) recorded: \"they calculate with bamboo, diameter one fen, length six cun, arranged into a hexagonal bundle of two hundred seventy one pieces\".\n\nAt first, calculating rods were round in cross-section, but by the time of the Sui dynasty (581 to 618 CE) mathematicians used triangular rods to represent positive numbers and rectangular rods for negative numbers.\n\nAfter the abacus flourished, counting rods were abandoned except in Japan, where rod numerals developed into a symbolic notation for algebra.\n\nCounting rods represent digits by the number of rods, and the perpendicular rod represents five. To avoid confusion, vertical and horizontal forms are alternately used. Generally, vertical rod numbers are used for the position for the units, hundreds, ten thousands, etc., while horizontal rod numbers are used for the tens, thousands, hundred thousands etc. It is written in \"Sunzi Suanjing\" that \"one is vertical, ten is horizontal\".\n\nRed rods represent positive numbers and black rods represent negative numbers. Ancient Chinese clearly understood negative numbers and zero (leaving a blank space for it), though they had no symbol for the latter. The Nine Chapters on the Mathematical Art, which was mainly composed in the first century CE, stated \"(when using subtraction) subtract same signed numbers, add different signed numbers, subtract a positive number from zero to make a negative number, and subtract a negative number from zero to make a positive number\". Later, a go stone was sometimes used to represent zero.\n\nThis alternation of vertical and horizontal rod numeral form is very important to understanding written transcription of rod numerals on manuscripts correctly. For instance, in Licheng suanjin, 81 was transcribed as , and 108 was transcribed as ; it is clear that the latter clearly had a blank zero on the \"counting board\" (i.e., floor or mat), even though on the written transcription, there was no blank. In the same manuscript, 405 was transcribed as , with a blank space in between for obvious reasons, and could in no way be interpreted as \"45\". In other words, transcribed rod numerals may not be positional, but on the counting board, they are positional. is an exact image of the counting rod number 405 on a table top or floor.\n\nThe value of a number depends on its physical position on the counting board. A 9 at the rightmost position on the board stands for 9. Moving the batch of rods representing 9 to the left one position (i.e., to the tens place) gives 9[] or 90. Shifting left again to the third position (to the hundreds place) gives 9[][] or 900. Each time one shifts a number one position to the left, it is multiplied by 10. Each time one shifts a number one position to the right, it is divided by 10. This applies to single-digit numbers or multiple-digit numbers.\n\nSong dynasty mathematician Jia Xian used hand-written Chinese decimal orders 步十百千萬 as rod numeral place value, as evident from a facsimile from a page of Yongle Encyclopedia. He arranged 七萬一千八百二十四 as\n\nHe treated the Chinese order numbers as place value markers, and 七一八二四 became place value decimal number. He then wrote the rod numerals according to their place value:\n\nIn Japan, mathematicians put counting rods on a counting board, a sheet of cloth with grids, and used only vertical forms relying on the grids. An 18th-century Japanese mathematics book has a checker counting board diagram, with the order of magnitude symbols \"千百十一分厘毛“(thousand, hundred, ten, unit, tenth, hundredth, thousandth).\n\nExamples:\nRod numerals are a positional numeral system made from shapes of counting rods. Positive numbers are written as they are and the negative numbers are written with a slant bar at the last digit. The vertical bar in the horizontal forms 6–9 are drawn shorter to have the same character height.\n\nA circle (〇) is used for 0. Many historians think it was imported from Indian numerals by Gautama Siddha in 718, but some think it was created from the Chinese text space filler \"□\", and others think that the Indians acquired it from China, because it resembles a Confucian philosophical symbol for \"nothing\".\n\nIn the 13th century, Southern Song mathematicians changed digits for 4, 5, and 9 to reduce strokes. The new horizontal forms eventually transformed into Suzhou numerals. Japanese continued to use the traditional forms.\n\nExamples:\nIn Japan, Seki Takakazu developed the rod numerals into symbolic notation for algebra and drastically improved Japanese mathematics. After his period, the positional numeral system using Chinese numeral characters was developed, and the rod numerals were used only for the plus and minus signs. \n\nA fraction was expressed with rod numerals as two rod numerals one on top of another (without any other symbol, like the modern horizontal bar).\n\nThe method for using counting rods for mathematical calculation was called \"rod calculation\" or rod calculus (筹算). Rod calculus can be used for a wide range of calculations, including finding the value of , finding square roots, cube roots, or higher order roots, and solving a system of linear equations.\n\nBefore the introduction of written zero, there was no way to distinguish 10007 and 107 in written forms except by inserting a bigger space between 1 and 7, and so rod numerals were used only for doing calculations with counting rods. Once written zero came into play, the rod numerals had become independent, and their use indeed outlives the counting rods, after its replacement by abacus. One variation of horizontal rod numerals, the Suzhou numerals is still in use for book-keeping and in herbal medicine prescription in Chinatowns in some parts of the world.\n\nUnicode 5.0 includes counting rod numerals in their own block in the Supplementary Multilingual Plane (SMP) from U+1D360 to U+1D37F. The code points for the horizontal digits 1–9 are U+1D360 to U+1D368 and those for the vertical digits 1–9 are U+1D369 to U+1D371. The former are called \"unit digits\" and the latter are called \"tens digits\", which is opposite of the convention described above. Zero should be represented by U+3007 (〇, ideographic number zero) and the negative sign should be represented by U+20E5 (combining reverse solidus overlay). As these were recently added to the character set and since they are included in the SMP, font support may still be limited.\n\n\nFor a look of the ancient counting rods, and further explanation, you can visit the sites\n"}
{"id": "35810608", "url": "https://en.wikipedia.org/wiki?curid=35810608", "title": "Courcelle's theorem", "text": "Courcelle's theorem\n\nIn the study of graph algorithms, Courcelle's theorem is the statement that every graph property definable in the monadic second-order logic of graphs can be decided in linear time on graphs of bounded treewidth. The result was first proved by Bruno Courcelle in 1990 and independently rediscovered by .\nIt is considered the archetype of algorithmic meta-theorems.\n\nIn one variation of monadic second-order graph logic known as MSO, the graph is described by a set of vertices \"V\" and a binary adjacency relation adj(..), and the restriction to monadic logic means that the graph property in question may be defined in terms of sets of vertices of the given graph, but not in terms of sets of edges or of tuples of vertices.\n\nAs an example, the property of a graph being colorable with three colors (represented by three sets of vertices \"R\", \"G\", and \"B\") may be defined by the monadic second-order formula\n\nThe first part of this formula ensures that the three color classes cover all the vertices of the graph, and the second ensures that they each form an independent set. (It would also be possible to add clauses to the formula to ensure that the three color classes are disjoint, but this makes no difference to the result.) Thus, by Courcelle's theorem, 3-colorability of graphs of bounded treewidth may be tested in linear time.\n\nFor this variation of graph logic, Courcelle's theorem can be extended from treewidth to clique-width: for every fixed MSO property \"P\", and every fixed bound \"b\" on the clique-width of a graph, there is a linear-time algorithm for testing whether a graph of clique-width at most \"b\" has property \"P\". The original formulation of this result required the input graph to be given together with a construction proving that it has bounded clique-width, but later approximation algorithms for clique-width removed this requirement.\n\nCourcelle's theorem may also be used with a stronger variation of monadic second-order logic known as MSO. In this formulation, a graph is represented by a set \"V\" of vertices, a set \n\"E\" of edges, and an incidence relation between vertices and edges. This variation allows quantification over sets of vertices or edges, but not over more complex relations on tuples of vertices or edges.\n\nFor instance, the property of having a Hamiltonian cycle may be expressed in MSO by describing the cycle as a set of edges that includes exactly two edges incident to each vertex, such that every nonempty proper subset of vertices has an edge in the cycle with exactly one endpoint in the subset. However, Hamiltonicity cannot be expressed in MSO.\n\nIt is possible to apply the same results to graphs in which the vertices or edges have labels from a fixed finite set, either by augmenting the graph logic to incorporate predicates describing the labels, or by representing the labels by unquantified vertex set or edge set variables.\n\nAnother direction for extending Courcelle's theorem concerns logical formulas that include predicates for counting the size of the test.\nIn this context, it is not possible to perform arbitrary arithmetic operations on set sizes, nor even to test whether two sets have the same size.\nHowever, MSO and MSO can be extended to logics called CMSO and CMSO, that include for every two constants \"q\" and \"r\" a predicate formula_1 which tests whether the cardinality of set \"S\" is congruent to \"r\" modulo \"q\". Courcelle's theorem can be extended to these logics.\n\nAs stated above, Courcelle's theorem applies primarily to decision problems: does a graph have a property or not. However, the same methods also allow the solution to optimization problems in which the vertices or edges of a graph have integer weights, and one seeks the minimum or maximum weight vertex set that satisfies a given property, expressed in second order logic. These optimization problems can be solved in linear time on graphs of bounded clique-width.\n\nRather than bounding the time complexity of an algorithm that recognizes an MSO property on bounded-treewidth graphs, it is also possible to analyze the space complexity of such an algorithm; that is, the amount of memory needed above and beyond the size of the input itself (which is assumed to be represented in a read-only way so that its space requirements cannot be put to other purposes).\nIn particular, it is possible to recognize the graphs of bounded treewidth, and any MSO property on these graphs, by a deterministic Turing machine that uses only logarithmic space.\n\nThe typical approach to proving Courcelle's theorem involves the construction of a finite bottom-up tree automaton that acts on the tree decompositions of the given graph.\n\nIn more detail, two graphs \"G\" and \"G\", each with a specified subset \"T\" of vertices called terminals, may be defined to be equivalent with respect to an MSO formula \"F\" if, for all other graphs \"H\" whose intersection with \"G\" and \"G\" consists only of vertices in \"T\", the two graphs\n\"G\" ∪ \"H\" and \"G\" ∪ \"H\" behave the same with respect to \"F\": either they both model \"F\" or they both do not model \"F\". This is an equivalence relation, and it can be shown by induction on the length of \"F\" that (when the sizes of \"T\" and \"F\" are both bounded) it has finitely many equivalence classes.\n\nA tree decomposition of a given graph \"G\" consists of a tree and, for each tree node, a subset of the vertices of \"G\" called a bag. It must satisfy two properties: for each vertex \"v\" of \"G\", the bags containing \"v\" must be associated with a contiguous subtree of the tree, and for each edge \"uv\" of \"G\", there must be a bag containing both \"u\" and \"v\".\nThe vertices in a bag can be thought of as the terminals of a subgraph of \"G\", represented by the subtree of the tree decomposition descending from that bag. When \"G\" has bounded treewidth, it has a tree decomposition in which all bags have bounded size, and such a decomposition can be found in fixed-parameter tractable time. Moreover, it is possible to choose this tree decomposition so that it forms a binary tree, with only two child subtrees per bag. Therefore, it is possible to perform a bottom-up computation on this tree decomposition, computing an identifier for the equivalence class of the subtree rooted at each bag by combining the edges represented within the bag with the two identifiers for the equivalence classes of its two children.\n\nThe size of the automaton constructed in this way is not an elementary function of the size of the input MSO formula. This non-elementary complexity is necessary, in the sense that (unless P = NP) it is not possible to test MSO properties on trees in a time that is fixed-parameter tractable with an elementary dependence on the parameter.\n\nThe proofs of Courcelle's theorem show a stronger result: not only can every (counting) monadic second-order property be recognized in linear time for graphs of bounded treewidth, but it can be recognized by a finite-state tree automaton. Courcelle conjectured a converse to this: if a property of graphs of bounded treewidth is recognized by a tree automaton, then it can be defined in counting monadic second-order logic. In 1998 , claimed a resolution of the conjecture. However, the proof is widely regarded as unsatisfactory. Until 2016, only a few special cases were resolved: in particular, the conjecture has been proved for graphs of treewidth at most three, for k-connected graphs of treewidth k, for graphs of constant treewidth and chordality, and for k-outerplanar graphs.\nThe general version of the conjecture has been finally proved by Mikołaj Bojańczyk and Michał Pilipczuk\n\nMoreover, for Halin graphs (a special case of treewidth three graphs) counting is not needed: for these graphs, every property that can be recognized by a tree automaton can also be defined in monadic second-order logic. The same is true more generally for certain classes of graphs in which a tree decomposition can itself be described in MSOL. However, it cannot be true for all graphs of bounded treewidth, because in general counting adds extra power over monadic second-order logic without counting. For instance, the graphs with an even number of vertices can be recognized using counting, but not without.\n\nThe satisfiability problem for a formula of monadic second-order logic is the problem of determining whether there exists at least one graph (possibly within a restricted family of graphs) for which the formula is true. For arbitrary graph families, and arbitrary formulas, this problem is undecidable. However, satisfiability of MSO formulas is decidable for the graphs of bounded treewidth, and satisfiability of MSO formulas is decidable for graphs of bounded clique-width. The proof involves building a tree automaton for the formula and then testing whether the automaton has an accepting path.\n\nAs a partial converse, proved that, whenever a family of graphs has a decidable MSO satisfiability problem, the family must have bounded treewidth. The proof is based on a theorem of Robertson and Seymour that the families of graphs with unbounded treewidth have arbitrarily large grid minors. Seese also conjectured that every family of graphs with a decidable MSO satisfiability problem must have bounded clique-width; this has not been proven, but a weakening of the conjecture that replaces MSO by CMSO is true.\n\n used Courcelle's theorem to show that computing the crossing number of a graph \"G\" is fixed-parameter tractable with a quadratic dependence on the size of \"G\", improving a cubic-time algorithm based on the Robertson–Seymour theorem. An additional later improvement to linear time by follows the same approach. If the given graph \"G\" has small treewidth, Courcelle's theorem can be applied directly to this problem. On the other hand, if \"G\" has large treewidth, then it contains a large grid minor, within which the graph can be simplified while leaving the crossing number unchanged. Grohe's algorithm performs these simplifications until the remaining graph has a small treewidth, and then applies Courcelle's theorem to solve the reduced subproblem.\n\nIn computational topology, extend Courcelle's theorem from MSO to a form of monadic second-order logic on simplicial complexes of bounded dimension that allows quantification over simplices of any fixed dimension. As a consequence, they show how to compute certain quantum invariants of 3-manifolds as well as how to solve certain problems in discrete Morse theory efficiently, when the manifold has a triangulation (avoiding degenerate simplices) whose dual graph has small treewidth.\n\nMethods based on Courcelle's theorem have also been applied to database theory, knowledge representation and reasoning, automata theory, and model checking.\n"}
{"id": "157397", "url": "https://en.wikipedia.org/wiki?curid=157397", "title": "Daniel Sleator", "text": "Daniel Sleator\n\nDaniel Dominic Kaplan Sleator (born 10 December 1953 in St. Louis) is a Professor of Computer Science at Carnegie Mellon University, Pittsburgh, United States. In 1999, he won the ACM Paris Kanellakis Award (jointly with Robert Tarjan) for the splay tree data structure.\n\nHe was one of the pioneers in amortized analysis of algorithms, early examples of which were the analyses of the move-to-front heuristic, and splay trees. He invented many data structures with Robert Tarjan, such as splay trees, link/cut trees, and skew heaps.\n\nThe Sleator and Tarjan paper on the move-to-front heuristic first suggested the idea of comparing an online algorithm to an optimal offline algorithm, for which the term competitive analysis was later coined in a paper of Karlin, Manasse, Rudolph, and Sleator. Sleator also developed the theory of link grammars, and the Serioso music analyzer for analyzing meter and harmony in written music.\n\nSleator commercialized the volunteer-based Internet Chess Server into the Internet Chess Club despite outcry from fellow volunteers. The ICS has since become one of the most successful internet-based commercial chess servers.\n\nHe is the brother of William Sleator, who wrote science fiction for young adults.\n\nFrom 2003 to 2008, Sleator co-hosted the progressive talk show \"Left Out\" on WRCT-FM with Carnegie Mellon University School of Computer Science faculty member Bob Harper.\n\n"}
{"id": "33865195", "url": "https://en.wikipedia.org/wiki?curid=33865195", "title": "Effective dose (pharmacology)", "text": "Effective dose (pharmacology)\n\nIn pharmacology, an effective dose (ED) or effective concentration (EC) is a dose or concentration of a drug that produces a biological response. The term effective dose is used when measurements are taken \"in vivo,\" while the term effective concentration is used when the measurements are taken \"in vitro\".\n\nIt has been stated that any substance can be toxic at a high enough dose. This concept was exemplified in 2007 when a California woman died of water intoxication in a contest sanctioned by a radio station. The line between efficacy and toxicity is dependent upon the particular patient, although the dose administered by a physician should fall into the predetermined therapeutic window of the drug.\n\nThe importance of determining the therapeutic range of a drug cannot be overstated. This is generally defined by the range between the minimum effective dose (MED) and the maximum tolerated dose (MTD). The MED is defined as the lowest dose level of a pharmaceutical product that provides a clinically significant response in average efficacy, which is also statistically significantly superior to the response provided by the placebo. Similarly, the MTD is the highest possible but still tolerable dose level with respect to a pre-specified clinical limiting toxicity. In general, these limits refer to the average patient population. For instances in which there is a large discrepancy between the MED and MTD, it is stated that the drug has a large therapeutic window. Conversely, if the range is relatively small, or if the MTD is less than the MED, then the pharmaceutical product will have little to no practical value.\n\nThe \"median effective dose\" is the dose that produces a quantal effect (all or nothing) in 50% of the population that takes it (median referring to the 50% population base). It is also sometimes abbreviated as the ED, meaning \"effective dose for 50% of the population\". The ED50 is commonly used as a measure of the reasonable expectancy of a drug effect, but does not necessarily represent the dose that a clinician might use. This depends on the need for the effect, and also the toxicity. The toxicity and even the lethality of a drug can be quantified by the TD and LD respectively. Ideally, the effective dose would be substantially less than either the toxic or lethal dose for a drug to be therapeutically relevant.\nThe ED95 is the dose required to achieve the desired effect in 95% of the population.\n\nIn anaesthesia, the term ED is also used when referring to the pharmacology of neuromuscular blocking drugs. In this context, it is the dose which will cause 95% depression of the height of a single muscle twitch, in half of the population. Put another way, it is the ED for 95% reduction in twitch height. The single twitch response occurs when a nerve stimulator is used to stimulate the ulnar nerve, and the degree of twitch of the adductor pollicus muscle is measured. A more accurate nomenclature when used in this way would be \"ED95\".\n\n"}
{"id": "41144410", "url": "https://en.wikipedia.org/wiki?curid=41144410", "title": "Equivariant topology", "text": "Equivariant topology\n\nIn mathematics, equivariant topology is the study of topological spaces that possess certain symmetries. In studying topological spaces, one often considers continuous maps formula_1, and while equivariant topology also considers such maps, there is the additional constraint that each map \"respects symmetry\" in both its domain and target space.\n\nThe notion of symmetry is usually captured by considering a Group action of formula_2 on formula_3 and formula_4 and demanding that formula_5 is equivariant under this action, so that formula_6 for all formula_7, a property usually denoted by formula_8 . Heuristically speaking, standard topology views two spaces as equivalent \"up to deformation,\" but equivariant topology considers spaces equivalent up to deformation so long as it pays attention to any symmetry possessed by both spaces. A famous theorem of equivariant topology is the Borsuk–Ulam theorem , which asserts that every formula_9-equivariant map formula_10 necessarily vanishes.\n\nAn important construction used in Equivariant cohomology and other applications includes a naturally occurring group bundle ( see Principal bundle for details.)\n\nLet us first consider the case where formula_2 acts freely on formula_3. Then, given a formula_2-equivariant map formula_15, we obtain sections formula_16 given by formula_17,\n\nwhere formula_18 gets the diagonal action, formula_19 and the bundle is formula_20, with fiber formula_4 and projection given by formula_22. Often, the total space is written formula_23\n\nMore generally, the assignment formula_24 actually does not map to formula_25 generally. Since formula_5 is equivariant, if formula_27(the isotropy subgroup), then by equivariance, we have that formula_28, so in fact formula_5 will map to the collection of formula_30. In this case, one can replace the bundle by a homotopy quotient where formula_2 acts freely and is bundle homotopic to the induced bundle on formula_3 by formula_5.\n\nIn the same way that one can deduce the Ham sandwich theorem from the Borsuk-Ulam Theorem, one can find many applications of equivariant topology to problems of discrete geometry. This is accomplished by using the Configuration-Space Test-Map paradigm:\n\nGiven a geometric problem formula_34, we define the \"configuration space\", formula_3, which parametrizes all associated solutions to the problem (such as points, lines, or arcs.) Additionally, we consider a \"test space\" formula_36 and a map formula_37 where formula_38 is a solution to a problem if and only if formula_39. Finally, it is usual to consider natural symmetries in a discrete problem by some group formula_2 that acts on formula_3 and formula_42 so that formula_5 is equivariant under these actions. The problem is solved if we can show the nonexistence of an equivariant map formula_44.\n\nObstructions to the existence of such maps are often formulated algebraically from the topological data of formula_3 and formula_46. An archetypal example of such an obstruction can be derived having formula_42 a vector space and formula_48. In this case, a nonvanishing map would also induce a nonvanishing section formula_49 from the discussion above, so formula_50, the top Stiefel–Whitney class would need to vanish.\n\n\n"}
{"id": "23169584", "url": "https://en.wikipedia.org/wiki?curid=23169584", "title": "Ernesto Estrada", "text": "Ernesto Estrada\n\nErnesto Estrada (born 2 May 1966) is a Cuban-Spanish scientist. He is the Chair in Complexity Science, Full Professor at the Department of Mathematics and Statistics, and a member of the Institute of Complex Systems of the University of Strathclyde, Glasgow, United Kingdom. He is known by his contributions in different disciplines including chemistry, and the mathematics and physics of Complex Systems.\n\nEstrada was born in the city of Sancti Spiritus, in the central region of Cuba. Since the age of 11 he studied in a school which specialized in exact sciences. He later studied for a technical degree as an analytical chemist in a technological institute in Havana. At the age of 28 he presented his first scientific paper in an international congress together with his mentor, Dr. Jose F. Fernandez-Bertran. The paper was about the detection of polyatomic anions in matrices of NaCl using infrared spectroscopy. Between 1985 and 1990, he studied chemical sciences at the Central University of Las Villas in Santa Clara, Cuba, where he obtained his degree in only 4 of the 5 years established for the program. In the first years after graduation, Estrada investigated the synthesis and spectroscopic characterization of new organic molecules with biological activity. This research introduced him to the world of computational chemistry due to the requirement of using efficient methods to design biologically active molecules. In 1997, he obtained his Ph.D in Chemistry under the direction of Prof. Luis A. Montero Cabrera on the topic of \"Graph Theory Applied to Molecular Design\". By this time, Estrada had published 8 scientific papers in major chemistry journals.\n\nAfter completing his Ph.D., Estrada spent some time as post-doctoral research at the University of Valencia, Spain working with Prof. Jorge Galvez and at the Lisa-Meitner Institute for Computational Quantum Chemistry, Hebrew University of Jerusalem with Prof. David Avnir. In 2000, he officially emigrated to Spain where he obtained a fellowship at the University of Santiago de Compostela. Between 2002 and 2003, Estrada worked as a scientist at the Safety and Environmental Assurance Centre, Unilever in Colworth, U.K. He then obtained a position as \"Ramon y Cajal\" researcher at the University of Santiago de Compostela, Spain. Since 2008, Estrada occupies the newly created Chair in Complexity Science at the University of Strathclyde and has published more than 160 papers, 10 book Chapters and 2 books. He is a Cuban and Spanish citizen, resident in Scotland, U.K.\n\nEstrada has been a major contributor in the area of study of complex network, where he has developed several approaches to investigate the structure and dynamics of such systems. An index introduced by him in 1999 to characterise the degree of folding of proteins, and then generalised to the study of complex networks in 2005, has eventually became the Estrada index of a graph or network, and it is the subject of intensive research in mathematics and other fields. Estrada is also known in the field of spectral graph theory where he has introduced several approaches to characterise the organizational architecture of complex networks, such as the \"subgraph centrality\", \"communicability\", \"spectral scaling\", \"golden spectral graphs\", etc.\n\nEstrada is also known in the area of Mathematical Chemistry, in particular to the development and use of molecular descriptors based on the use of Graph Theory. He is known for the introduction of several approaches in this field, such as the Topological Sub-Structural Molecular Design (TOPS-MODE) approach, and the generalization of topological indices.\n\nIn 2007, Estrada received the prize as Outstanding Scientist from the International Academy of Mathematical Chemistry. The prize recognizes his valuable contributions in developing and applying graph theory to solve problems in many interdisciplinary areas, such as chemistry, physics, biology, ecology and technology. In 2014 Estrada received the prestigious Royal Society Wolfson Research Merit Award of the Royal Society of London, which recognises \"scientists of outstanding achievement and potential\".\n\nEstrada has been visiting Professor at the Statistical and Applied Mathematical Sciences Institute in North Carolina, USA, the Centre of Mathematical Research in Guanajuato, Mexico, the Quantititative Methods and Theory Institute and the Department of Mathematics and Computer Science at Emory University in Atlanta, USA.\n\nSince 2013 Estrada is the Editor-in-Chief of the Journal of Complex Networks published by Oxford University Press.\n\n\n"}
{"id": "25275445", "url": "https://en.wikipedia.org/wiki?curid=25275445", "title": "Evolution in Variable Environment", "text": "Evolution in Variable Environment\n\nEvolution in Variable Environment (EVE) is a computer program designed to simulate microbial cellular behavior in various environments. The prediction of cellular responses is a rapidly evolving topic in systems biology and computational biology. The goal is to predict the behavior a particular organism in response to a set of environmental stimuli \"in silico\". Such predictions can have a significant impact on preventive medicine, biotechnology, and microbe re-engineering. Computational prediction of behavior has two major components: the integration and simulation of vast biological networks and the creation of external stimuli. Current limitations of the method are: lack of comprehensive experimental data on the various cellular subsystems and inadequate computational algorithms.\n\nAn organism that learns to modulate its behavior and gene expression based on temporal interrelationships between environmental factors possesses a competitive advantage of over other organisms that are unable to make such predictions. For example, learning when nutrients are going to be present in the environment allows the organism to selectively express genes that will take up the food source, thus allowing the organism to harvest energy.\n\nModeling these type of behaviors of even simple bacteria poses certain challenges. Given the diversity of biological systems, it would appear that the number of behavior responses to an environmental change would be nearly infinite. However, recent studies have shown that biological systems are optimized for a certain environment and will thus respond relatively specific ways to stimuli. This specificity simplifies the computations considerably.\n\nThe second challenge is the seemingly random environmental events. Ruling out circadian or temporal cycles, such as daytime versus nighttime or the different seasons, many events in the environment are unpredictable, such as weather patterns, water salinity, and oxygen levels. However, it turns out that certain environmental factors are coupled temporally. For example, an increase in water temperature is frequently correlated with an increase in water salinity. These relationships allow organisms to respond to specific environmental factors in a timely manner and thus increase their biological fitness.\n\nThe prediction of cellular responses bears considerable interest to scientists, physicians, and bio-engineers alike. For example, studying how a particular organism responds to external and internal stimuli can yield insights into the mechanisms of evolution. At the same time, such knowledge can also help physicians and health officials understand the infectious cycles of disease-causing bacteria and protists, allowing to them to establish preventive measures. Finally, knowing how bacteria behave under different stimuli may facilitate the development of engineered bacteria that perform certain functions, such as clearing oil spills. These examples are only some of the many applications of predicting behaviors.\n\nWith the rapid expansion of human understanding of cell, molecular, and chemical biology, a vast set of data has been generated on the metabolic pathways, signal-transductional pathways, and gene regulatory networks. Cellular modeling attempts to analyze and visualize these pathways with the help of computers. A substantial portion of EVE is devoted to writing algorithms, data structures, and visualization tools for these biological systems.\n\nThe frequency of occurrence of environmental factors exists between two extremes: the completely periodic events and completely random events. Certain events, when viewed in isolation, appear completely random. However, then taken in conjunction with another event, these events can appear highly “predictable.” Such relationships can exist at multiple time scales, which reflect the highly structural habitats of free-living organisms. EVE attempts to model these intermediate events.\n\nMost cellular models have been based on unicellular microbes. Since these simple organisms lack a complex neural network, computational modeling focuses on the various biochemical pathways of the cells, such as transcription, translation, post-translational modification, and protein-protein interactions. A variety of algorithms and programs exist that attempt to model these type of interactions.\n\nEVE is a simulation framework that is able to model predictive internal models around of complex environments. EVE operates under the “central dogma,” the assumption that all biochemical pathways proceed through the following steps: DNA => RNA => protein. Furthermore, the biochemical networks evolve in an asynchronous and stochastic manner. These two assumptions allow for the simulation of temporal dynamics of cascades of biochemical interactions/transformations.\n\nBuilding upon previous attempts to simulate cellular behavior, such as circadian rhythms, EVE, according to its makers, “integrates many features that improve the biochemical, evolutionary, and ecological realism of our simulations, features that are crucial for simulating microbial regulatory networks in the context of interactions with the environment.” The program takes into account all the molecular species and their interactions, including but limited to RNA, mRNA, and proteins. Each component is represented by a so-called node, which contains simulates biological parameters, such as basal expression, degradation, and regulatory strength. The program links these network of nodes together and simulates the interactions between the individual nodes.\n\nEach response pathway is modeled to have a high energetic cost. The artificial organism takes in energy in form of “food” from the surroundings, while each interaction pathway expends high levels of energy. This setup generates a selection pressure that favors energy minimization.\n\nCells \"in silico\" are placed into this computational ecology and allowed to compete with each other for resources. The distribution of resources is set in a temporal dependent manner. During each round, random mutations and perturbations are introduced to the biochemical pathways. At the end of each round, the cells with the lowest energy count are eliminated. This selects for cells that are able to maximize energy uptake by optimizing the expression of its pathways in a particular time period.\n\nA fixed size population receives a pre-defined “energy packet.” At a given point during the simulation, the pathways of the cell undergo mutations and the properties of each node are updated. After the end of one round, the cells are selected based on a probability that is directly proportional to their acquired energy.\n\nSimilar to the generation-based simulator, cells receive a predefined energy packet at the beginning of the simulation. At any given point of the experiment, however, the cells can mutate or die.\n\nBased on the selection pressure, the different simulations were categorized into the following groups:\n- Delayed Gates: Signals and resource are related by OR, AND, XOR, NAND, NOR dynamic logic functions.\n- Multi-gates: Signals and resource are interchangeably related by combinations of OR, AND, XOR, NAND, NOR dynamic logic functions.\n-Oscillators: Selection pressure to evolve oscillatory expression of RP1 with or without a periodic guiding signal.\n- Bi-stable switches: Selection pressure to evolve bi-stability in environments where two environmental signals operate as ON/OFF pulse switches.\n-Duration/variance locking: Selection pressure to evolve networks that predict the duration of an Environmental resource that has fluctuating duration or phase variance.\n\nAfter a few thousand generations, the simulation produced organisms that were able to predict their “mealtimes” based on temporally linked environmental cues. This pattern of evolution repeated itself for every type of the aforementioned simulations performed. The results from this study prompted scientists to experimentally reprogram \"E. coli\" cells \"in vivo\". Normally, \"E. coli\" switches to anaerobic respiration when encountered with a significant temperature change. However, following the principles of the simulation, scientists were able to make the bacteria turn on aerobic respiration when exposed to higher temperatures. These experiments show how such simulations can yield important insights into a bacterium’s cellular response pathways.\n\nSimulations take a large amount of computing power and time. The EVE framework used multi-node supercomputer clusters (BlueGene/L and Beowulf) that ran for an average of 500 node workload for over 2 years in simulation of \"E. coli\".\nPossessing the correct amount of data is essential for the success of the program. Since the program integrates information on known pathways and interactions, these types of simulations are only useful for organism whose essential biochemical pathways have largely been elucidated.\n\n"}
{"id": "21281625", "url": "https://en.wikipedia.org/wiki?curid=21281625", "title": "Fourier division", "text": "Fourier division\n\nFourier division or cross division is a pencil-and-paper method of division which helps to simplify the process when the divisor has more than two digits. It was invented by Joseph Fourier.\n\nThe following exposition assumes that the numbers are broken into two-digit pieces, separated by commas: e.g. 3456 becomes 34,56. In general \"x,y\" denotes \"x\"·100 + \"y\" and \"x,y,z\" denotes \"x\"·10000 + \"y\"·100 + \"z\", etc.\n\nSuppose that we wish to divide \"c\" by \"a\", to obtain the result \"b\". (So \"a\" × \"b\" = \"c\".)\n\nNote that \"a\" may not have a leading zero; it should stand alone as a two-digit number.\n\nWe can find the successive terms \"b\", \"b\", etc., using the following formulae:\n\nEach time we add a term to the numerator until it has as many terms as \"a\". From then on, the number of terms remains constant, so there is no increase in difficulty. Once we have as much precision as we need, we use an estimate to place the decimal point.\n\nIt will often be the case that one of the \"b\" terms will be negative. For example, 93,−12 denotes 9288, while −16,32 denotes −1600 + 32 or −1568. (Note: 45,−16,32 denotes 448432.) Care must be taken with the signs of the remainders also.\n\nThe general term is\n\nIn cases where one or more of the \"b\" terms has more than two digits, the final quotient value \"b\" cannot be constructed simply by concatenating the digit pairs. Instead, each term, starting with formula_7 should be multiplied by 100, and the next term added (or, if negative, subtracted). This result should be multiplied by 100, and the next term added or subtracted, etc., until all terms are exhausted. In other words, we construct partial sums of the \"b\" terms:\n\nThe last partial sum is the value for \"b\".\n\nFind the reciprocal of π ≈ 3.14159.\n\nThe result is 32,-17,10 or 31,83,10 yielding 0.318310.\n\n\n"}
{"id": "3050954", "url": "https://en.wikipedia.org/wiki?curid=3050954", "title": "Fredholm alternative", "text": "Fredholm alternative\n\nIn mathematics, the Fredholm alternative, named after Ivar Fredholm, is one of Fredholm's theorems and is a result in Fredholm theory. It may be expressed in several ways, as a theorem of linear algebra, a theorem of integral equations, or as a theorem on Fredholm operators. Part of the result states that a non-zero complex number in the spectrum of a compact operator is an eigenvalue.\n\nIf \"V\" is an \"n\"-dimensional vector space and formula_1 is a linear transformation, then exactly one of the following holds:\n\n\nA more elementary formulation, in terms of matrices, is as follows. Given an \"m\"×\"n\" matrix \"A\" and a \"m\"×1 column vector b, exactly one of the following must hold:\n\n\nIn other words, \"A\" x = b has a solution formula_4 if and only if for any y s.t. \"A\" y = 0, yb = 0 formula_5.\nLet formula_6 be an integral kernel, and consider the homogeneous equation, the Fredholm integral equation,\n\nand the inhomogeneous equation\n\nThe Fredholm alternative is the statement that, for every non-zero fixed complex number formula_9, either the first equation has a non-trivial solution, or the second equation has a solution for all formula_10.\n\nA sufficient condition for this statement to be true is for formula_6 to be square integrable on the rectangle formula_12 (where \"a\" and/or \"b\" may be minus or plus infinity). The integral operator defined by such a \"K\" is called a Hilbert–Schmidt integral operator.\n\nResults on the Fredholm operator generalize these results to vector spaces of infinite dimensions, Banach spaces.\n\nThe integral equation can be reformulated in terms of operator notation as follows. Write (somewhat informally)\n\nto mean\n\nwith formula_15 the Dirac delta function, considered as a distribution, or generalized function, in two variables. Then by convolution, \"T\" induces a linear operator acting on a Banach space \"V\" of functions formula_16, which we also call \"T\", so that\n\nis given by\n\nwith formula_19 given by\n\nIn this language, the Fredholm alternative for integral equations is seen to be analogous to the Fredholm alternative for finite-dimensional linear algebra.\n\nThe operator \"K\" given by convolution with an \"L\" kernel, as above, is known as a Hilbert–Schmidt integral operator.\nSuch operators are always compact. More generally, the Fredholm alternative is valid when \"K\" is any compact operator. The Fredholm alternative may be restated in the following form: a nonzero formula_21 either is an eigenvalue of \"K\", or lies in the domain of the resolvent\n\nThe Fredholm alternative can be applied to solving linear elliptic boundary value problems. The basic result is: if the equation and the appropriate Banach spaces have been set up correctly, then either\n\nThe argument goes as follows. A typical simple-to-understand elliptic operator \"L\" would be the Laplacian plus some lower order terms. Combined with suitable boundary conditions and expressed on a suitable Banach space \"X\" (which encodes both the boundary conditions and the desired regularity of the solution), \"L\" becomes an unbounded operator from \"X\" to itself, and one attempts to solve\n\nwhere \"f\" ∈ \"X\" is some function serving as data for which we want a solution. The Fredholm alternative, together with the theory of elliptic equations, will enable us to organize the solutions of this equation.\n\nA concrete example would be an elliptic boundary-value problem like\n\nsupplemented with the boundary condition\n\nwhere Ω ⊆ R is a bounded open set with smooth boundary and \"h\"(\"x\") is a fixed coefficient function (a potential, in the case of a Schrödinger operator). The function \"f\" ∈ \"X\" is the variable data for which we wish to solve the equation. Here one would take \"X\" to be the space \"L\"(Ω) of all square-integrable functions on Ω, and \"dom\"(\"L\") is then the Sobolev space \"W\" (Ω) ∩ \"W\"(Ω), which amounts to the set of all square-integrable functions on Ω whose weak first and second derivatives exist and are square-integrable, and which satisfy a zero boundary condition on ∂Ω.\n\nIf \"X\" has been selected correctly (as it has in this example), then for \"μ\" » 0 the operator \"L\" + \"μ\" is positive, and then employing elliptic estimates, one can prove that \"L\"+\"μ\" : \"dom\"(\"L\") → \"X\" is a bijection, and its inverse is a compact, everywhere-defined operator \"K\" from \"X\" to \"X\", with image equal to \"dom\"(\"L\"). We fix one such \"μ\", but its value is not important as it is only a tool.\n\nWe may then transform the Fredholm alternative, stated above for compact operators, into a statement about the solvability of the boundary-value problem (*)-(**). The Fredholm alternative, as stated above, asserts:\n\n\nLet us explore the two alternatives as they play out for the boundary-value problem. Suppose \"λ\" ≠ 0. Then either\n\n(A) \"λ\" is an eigenvalue of K ⇔ there is a solution \"h\" ∈ \"dom\"(\"L\") of (\"L\" + \"μ\") \"h\" = \"λ\"\"h\" ⇔\n-\"μ\"+\"λ\" is an eigenvalue of \"L\".\n\n(B) The operator \"K\" - \"λ\" : \"X\" → \"X\" is a bijection ⇔ (\"K\" - \"λ\") (\"L\" + \"μ\") = \"Id\" - \"λ\" (\"L\" + \"μ\") : \"dom\"(\"L\") → \"X\" is a bijection ⇔ \"L\" + \"μ\" - \"λ\" : \"dom\"(\"L\") → \"X\" is a bijection.\n\nReplacing -\"μ\"+\"λ\" by \"λ\", and treating the case \"λ\" = -\"μ\" separately, this yields the following Fredholm alternative for an elliptic boundary-value problem:\n\n\nThe latter function \"u\" solves the boundary-value problem (*)-(**) introduced above. This is the dichotomy that was claimed in (1)-(2) above. By the spectral theorem for compact operators, one also obtains that the set of \"λ\" for which the solvability fails is a discrete subset of R (the eigenvalues of \"L\"). The eigenvalues’ associated eigenfunctions can be thought of as \"resonances\" that block the solvability of the equation.\n\n\n"}
{"id": "1714764", "url": "https://en.wikipedia.org/wiki?curid=1714764", "title": "Harmonic (mathematics)", "text": "Harmonic (mathematics)\n\nIn mathematics, a number of concepts employ the word harmonic. The similarity of this terminology to that of music is not accidental: the equations of motion of vibrating strings, drums and columns of air are given by formulas involving Laplacians; the solutions to which are given by eigenvalues corresponding to their modes of vibration. Thus, the term \"harmonic\" is applied when one is considering functions with sinusoidal variations, or solutions of Laplace's equation and related concepts.\n\n"}
{"id": "1518742", "url": "https://en.wikipedia.org/wiki?curid=1518742", "title": "Helly–Bray theorem", "text": "Helly–Bray theorem\n\nIn probability theory, the Helly–Bray theorem relates the weak convergence of cumulative distribution functions to the convergence of expectations of certain measurable functions. It is named after Eduard Helly and Hubert Evelyn Bray.\n\nLet \"F\" and \"F\", \"F\", ... be cumulative distribution functions on the real line. The Helly–Bray theorem states that if \"F\" converges weakly to \"F\", then\n\nfor each bounded, continuous function \"g\": R → R, where the integrals involved are Riemann–Stieltjes integrals.\n\nNote that if \"X\" and \"X\", \"X\", ... are random variables corresponding to these distribution functions, then the Helly–Bray theorem does not imply that E(\"X\") → E(\"X\"), since \"g\"(\"x\") = \"x\" is not a bounded function.\n\nIn fact, a stronger and more general theorem holds. Let \"P\" and \"P\", \"P\", ... be probability measures on some set \"S\". Then \"P\" converges weakly to \"P\" if and only if\n\nfor all bounded, continuous and real-valued functions on \"S\". (The integrals in this version of the theorem are Lebesgue–Stieltjes integrals.)\n\nThe more general theorem above is sometimes taken as \"defining\" weak convergence of measures (see Billingsley, 1999, p. 3).\n"}
{"id": "3948734", "url": "https://en.wikipedia.org/wiki?curid=3948734", "title": "Homoclinic orbit", "text": "Homoclinic orbit\n\nIn mathematics, a homoclinic orbit is a trajectory of a flow of a dynamical system which joins a saddle equilibrium point to itself. More precisely, a homoclinic orbit lies in the intersection of the stable manifold and the unstable manifold of an equilibrium.\n\nConsider the continuous dynamical system described by the ODE\n\nSuppose there is an equilibrium at formula_2, then a solution formula_3 is a homoclinic orbit if\n\nIf the phase space has three or more dimensions, then it is important to consider the topology of the unstable manifold of the saddle point. The figures show two cases. First, when the stable manifold is topologically a cylinder, and secondly, when the unstable manifold is topologically a Möbius strip; in this case the homoclinic orbit is called \"twisted\".\n\nHomoclinic orbits and homoclinic points are defined in the same way for iterated functions, as the intersection of the stable set and unstable set of some fixed point or periodic point of the system. \n\nWe also have the notion of homoclinic orbit when considering discrete dynamical systems. In such a case, if formula_5 is a diffeomorphism of a manifold formula_6, we say that formula_7 is a homoclinic point if it has the same past and future - more specifically, if there exists a fixed (or periodic) point \nformula_8 such that\n\nThe existence of one homoclinic point implies the existence of an infinite number of them.\nThis comes from its definition: the intersection of a stable and unstable set. Both sets are invariant by definition, which means that the forward iteration of the homoclinic point is both on the stable and unstable set. By iterating N times, the map approaches the equilibrium point by the stable set, but in every iteration it is on the unstable manifold too, which shows this property.\n\nThis property suggests that complicated dynamics arise by the existence of a homoclinic point. Indeed, Smale (1967) showed that these points leads to horseshoe map like dynamics, which is associated with chaos.\n\nBy using the Markov partition, the long-time behaviour of hyperbolic system can be studied using the techniques of symbolic dynamics. In this case, a homoclinic orbit has a particularly simple and clear representation. Suppose that formula_10 is a finite set of \"M\" symbols. The dynamics of a point \"x\" is then represented by a bi-infinite string of symbols \n\nA periodic point of the system is simply a recurring sequence of letters. A heteroclinic orbit is then the joining of two distinct periodic orbits. It may be written as\n\nwhere formula_13 is a sequence of symbols of length \"k\", (of course, formula_14), and formula_15 is another sequence of symbols, of length \"m\" (likewise, formula_16). The notation formula_17 simply denotes the repetition of \"p\" an infinite number of times. Thus, a heteroclinic orbit can be understood as the transition from one periodic orbit to another. By contrast, a homoclinic orbit can be written as \n\nwith the intermediate sequence formula_19 being non-empty, and, of course, not being \"p\", as otherwise, the orbit would simply be formula_17.\n\n\n\n"}
{"id": "910043", "url": "https://en.wikipedia.org/wiki?curid=910043", "title": "Inferential role semantics", "text": "Inferential role semantics\n\nInferential role semantics (also conceptual role semantics, functional role semantics, procedural semantics, semantic inferentialism) is an approach to the theory of meaning that identifies the meaning of an expression with its relationship to other expressions (typically its inferential relations with other expressions), in contradistinction to denotationalism, according to which denotations are the primary sort of meaning.\n\nGeorg Wilhelm Friedrich Hegel is considered an early proponent of what is now called inferentialism. He believed that the ground for the axioms and the foundation for the validity of the inferences are the right consequences and that the axioms do not explain the consequence.\n\nContemporary proponents of semantic inferentialism include Robert Brandom, Gilbert Harman, Paul Horwich, and Ned Block. Inferential role semantics originated in the work of late Ludwig Wittgenstein.\n\nJerry Fodor coined the term \"inferential role semantics\" in order to criticise it as a holistic (i.e. essentially non-compositional) approach to the theory of meaning. Inferential role semantics is sometimes contrasted to truth-conditional semantics. \n\nThe approach is related to accounts of proof-theoretic semantics in the semantics of logic which associate meaning with the reasoning process.\n\nSemantic inferentialism is related to logical expressivism and semantic anti-realism.\n"}
{"id": "981655", "url": "https://en.wikipedia.org/wiki?curid=981655", "title": "Integer square root", "text": "Integer square root\n\nIn number theory, the integer square root (isqrt) of a positive integer \"n\" is the positive integer \"m\" which is the greatest integer less than or equal to the square root of \"n\",\n\nFor example, formula_2 because formula_3 and formula_4.\n\nOne way of calculating formula_5 and formula_6 is to use Newton's method to find a solution for the equation formula_7, giving the iterative formula\n\nThe sequence formula_9 converges quadratically to formula_5 as formula_11. It can be proven that if formula_12 is chosen as the initial guess, one can stop as soon as\nto ensure that formula_14\n\nFor computing formula_15 for very large integers \"n\", one can use the quotient of Euclidean division for both of the division operations. This has the advantage of only using integers for each intermediate value, thus making the use of floating point representations of large numbers unnecessary. It is equivalent to using the iterative formula\n\nBy using the fact that\n\none can show that this will reach formula_15 within a finite number of iterations.\n\nHowever, formula_15 is not necessarily a fixed point of the above iterative formula. Indeed, it can be shown that formula_15 is a fixed point if and only if formula_21 is not a perfect square. If formula_21 is a perfect square, the sequence ends up in a period-two cycle between formula_15 and formula_24 instead of converging. For termination, it suffices to check that either the number has converged or it has increased by exactly one from the previous step, in which case the new result is discarded.\n\nAlthough formula_5 is irrational for many formula_26, the sequence formula_9 contains only rational terms when formula_28 is rational. Thus, with this method it is unnecessary to exit the field of rational numbers in order to calculate formula_6, a fact which has some theoretical advantages.\n\nOne can prove that formula_30 is the largest possible number for which the stopping criterion \nensures formula_32\nin the algorithm above.\n\nIn implementations which use number formats that cannot represent all rational numbers exactly (for example, floating point), a stopping constant less than one should be used to protect against roundoff errors.\n\nThe traditional pen-and-paper algorithm for computing the square root formula_5 is based on working from higher digit places to lower, and as each new digit pick the largest that will still yield a square formula_34. If stopping after the one's place, the result computed will be the integer square root.\n\nIf working in base 2, the choice of digit is simplified to that between 0 (the \"small candidate\") and 1 (the \"large candidate\"), and digit manipulations can be expressed in terms of binary shift operations. With codice_1 being multiplication, codice_2 being left shift, and codice_3 being logical right shift, a recursive algorithm to find the integer square root of any natural number is:\n\nOr, iteratively instead of recursively:\n\nTraditional pen-and-paper presentations of the digit-by-digit algorithm include various optimisations not present in the code above, in particular the trick of presubtracting the square of the previous digits which makes a general multiplication step unnecessary.\n\n\n"}
{"id": "4282992", "url": "https://en.wikipedia.org/wiki?curid=4282992", "title": "Introduction to Arithmetic", "text": "Introduction to Arithmetic\n\nThe book Introduction to Arithmetic (, \"Arithmetike eisagoge\") is the only extant work on mathematics by Nicomachus (60–120 AD). \n\nThe work contains both philosophical prose and basic mathematical ideas. Nicomachus refers to Plato quite often, and writes that philosophy can only be possible if one knows enough about mathematics. Nicomachus also describes how natural numbers and basic mathematical ideas are eternal and unchanging, and in an abstract realm. It consists of two books, twenty-three and twenty-nine chapters, respectively.\n\nAlthough he was preceded by the Babylonians and the Chinese, Nicomachus provided one of the earliest Greco-Roman multiplication tables, whereas the oldest extant Greek multiplication table is found on a wax tablet dated to the 1st century AD (now found in the British Museum).\n\n\n\n"}
{"id": "188725", "url": "https://en.wikipedia.org/wiki?curid=188725", "title": "Irreducible polynomial", "text": "Irreducible polynomial\n\nIn mathematics, an irreducible polynomial (or prime polynomial) is, roughly speaking, a non-constant polynomial that cannot be factored into the product of two non-constant polynomials. The property of irreducibility depends on the nature of the coefficients that are accepted for the possible factors, that is, the field or ring to which the coefficients of the polynomial and its possible factors are supposed to belong. For example, the polynomial is a polynomial with integer coefficients, but, as every integer is also a real number, it is also a polynomial with real coefficients. It is irreducible if it is considered as a polynomial with integer coefficients, but it factors as formula_1 if it is considered as a polynomial with real coefficients. One says that the polynomial is irreducible over the integers but not over the reals.\n\nA polynomial that is irreducible over any field containing the coefficients is absolutely irreducible. By the fundamental theorem of algebra, a univariate polynomial is absolutely irreducible if and only if its degree is one. On the other hand, with several indeterminates, there are absolutely irreducible polynomials of any degree, such as formula_2 for any positive integer .\n\nA polynomial that is not irreducible is sometimes said to be reducible. However, this term must be used with care, as it may refer to other notions of reduction.\n\nIrreducible polynomials appear naturally in the study of polynomial factorization and algebraic field extensions.\n\nIt is helpful to compare irreducible polynomials to prime numbers: prime numbers (together with the corresponding negative numbers of equal magnitude) are the irreducible integers. They exhibit many of the general properties of the concept of \"irreducibility\" that equally apply to irreducible polynomials, such as the essentially unique factorization into prime or irreducible factors.\n\nIf \"F\" is a field, a non-constant polynomial is irreducible over \"F\" if its coefficients belong to \"F\" and it cannot be factored into the product of two non-constant polynomials with coefficients in \"F\". \n\nA polynomial with integer coefficients, or, more generally, with coefficients in a unique factorization domain \"R\", is sometimes said to be \"irreducible\" (or \"irreducible over R\") if it is an irreducible element of the polynomial ring, that is, it is not invertible, not zero, and cannot be factored into the product of two non-invertible polynomials with coefficients in \"R\". Another definition is frequently used, saying that a polynomial is \"irreducible over R\" if it is irreducible over the field of fractions of \"R\" (the field of rational numbers, if \"R\" is the integers).\nBoth definitions generalize the definition given for the case of coefficients in a field, because, in this case, the non-constant polynomials are exactly the polynomials that are non-invertible and non-zero.\n\nThe absence of an explicit algebraic expression for a factor does not by itself imply that a polynomial is irreducible. When a polynomial is reducible into factors, these factors may be explicit algebraic expressions or implicit expressions. For example, formula_3 can be factored explicitly over the complex numbers as formula_4 however, the Abel–Ruffini theorem states that there are polynomials of any degree greater than 4 for which complex factors exist that have no explicit algebraic expression. Such a factor can be written simply as, say, formula_5 where formula_6 is defined implicitly as a particular solution of the equation that sets the polynomial equal to 0. Further, factors of either type can also be expressed as numerical approximations obtainable by root-finding algorithms, for example as formula_7\n\nThe following six polynomials demonstrate some elementary properties of reducible and irreducible polynomials:\n\nOver the integers, the first three polynomials are reducible (the third one is reducible because the factor 3 is not invertible in the integers); the last two are irreducible. (The fourth, of course, is not a polynomial over the integers.)\n\nOver the rational numbers, the first two and the fourth polynomials are reducible, but the other three polynomials are irreducible (as a polynomial over the rationals, 3 is a unit, and, therefore, does not count as a factor).\n\nOver the real numbers, the first five polynomials are reducible, but formula_14 is irreducible.\n\nOver the complex numbers, all six polynomials are reducible.\n\nOver the complex field, and, more generally, over an algebraically closed field, a univariate polynomial is irreducible if and only if its degree is one. This fact is known as the fundamental theorem of algebra in the case of the complex numbers and, in general, as the condition of being algebraically closed.\n\nIt follows that every nonconstant univariate polynomial can be factored as\n\nwhere formula_16 is the degree, formula_17 is the leading coefficient and formula_18 are the zeros of the polynomial (not necessarily distinct, and not necessarily having explicit algebraic expressions).\n\nThere are irreducible multivariate polynomials of every degree over the complex numbers. For example, the polynomial \nwhich defines a Fermat curve, is irreducible for every positive \"n\".\n\nOver the field of reals, the degree of an irreducible univariate polynomial is either one or two. More precisely, the irreducible polynomials are the polynomials of degree one and the quadratic polynomials formula_20 that have a negative discriminant formula_21 \nIt follows that every non-constant univariate polynomial can be factored as a product of polynomials of degree at most two. For example,\nformula_22 factors over the real numbers as formula_23 and it cannot be factored further, as both factors have a negative discriminant: formula_24\n\nEvery polynomial over a field may be factored into a product of a non-zero constant and a finite number of irreducible (over ) polynomials. This decomposition is unique up to the order of the factors and the multiplication of the factors by non-zero constants whose product is 1.\n\nOver a unique factorization domain the same theorem is true, but is more accurately formulated by using the notion of primitive polynomial. A primitive polynomial is a polynomial over a unique factorization domain, such that 1 is a greatest common divisor of its coefficients. \n\nLet be a unique factorization domain. A non-constant irreducible polynomial over is primitive. A primitive polynomial over is irreducible over if and only if it is irreducible over the field of fractions of . Every polynomial over may be decomposed into the product of a non-zero constant and a finite number of non-constant irreducible primitive polynomials. The non-zero constant may itself be decomposed into the product of a unit of and a finite number of irreducible elements of .\nBoth factorizations are unique up to the order of the factors and the multiplication of the factors by a unit of .\n\nThis is this theorem which motivates that the definition of \"irreducible polynomial over a unique factorization domain\" often supposes that the polynomial is non-constant. \n\nAll algorithms which are presently implemented for factoring polynomials over the integers and over the rational numbers use this result (see Factorization of polynomials).\n\nThe irreducibility of a polynomial over the integers formula_25 is related to that over the field formula_26 of formula_27 elements (for a prime formula_27). In particular, if a univariate polynomial \"f\" over formula_25 is irreducible over formula_26 for some prime formula_27 that does not divide the leading coefficient of \"f\" (the coefficient of the higher power of the variable), then \"f\" is irreducible over formula_25. Eisenstein's criterion is a variant of this property where irreducibility over formula_33 is also involved.\n\nThe converse, however, is not true: there are polynomials of arbitrarily large degree that are irreducible over the integers and reducible over every finite field. A simple example of such a polynomial is formula_34\n\nThe relationship between irreducibility over the integers and irreducibility modulo \"p\" is deeper than the previous result: to date, all implemented algorithms for factorization and irreducibility over the integers and over the rational numbers use the factorization over finite fields as a subroutine.\n\nThe number of irreducible monic polynomials over a field formula_35 for prime \"p\" is given by the necklace counting function. For \"p\"=2, such polynomials are commonly used to generate pseudorandom binary sequences.\n\nThe unique factorization property of polynomials does not mean that the factorization of a given polynomial may always be computed. Even the irreducibility of a polynomial may not always be proved by a computation: there are fields over which no algorithm can exist for deciding the irreducibility of arbitrary polynomials. \n\nAlgorithms for factoring polynomials and deciding irreducibility are known and implemented in computer algebra systems for polynomials over the integers, the rational numbers, finite fields and finitely generated field extension of these fields. All these algorithms use the algorithms for factorization of polynomials over finite fields.\n\nThe notions of irreducible polynomial and of algebraic field extension are strongly related, in the following way.\n\nLet \"x\" be an element of an extension \"L\" of a field \"K\". This element is said to be \"algebraic\" if it is a root of a polynomial with coefficients in \"K\". Among the polynomials of which \"x\" is a root, there is exactly one which is monic and of minimal degree, called the minimal polynomial of \"x\". The minimal polynomial of an algebraic element \"x\" of \"L\" is irreducible, and is the unique monic irreducible polynomial of which \"x\" is a root. The minimal polynomial of \"x\" divides every polynomial which has \"x\" as a root (this is Abel's irreducibility theorem). \n\nConversely, if formula_36 is a univariate polynomial over a field \"K\", let formula_37 be the quotient ring of the polynomial ring formula_38 by the ideal generated by . Then is a field if and only if is irreducible over . In this case, if is the image of in , the minimal polynomial of is the quotient of by its leading coefficient.\n\nAn example of the above is the standard definition of the complex numbers as formula_39\n\nIf a polynomial has an irreducible factor over , which has a degree greater than one, one may apply to the preceding construction of an algebraic extension, to get an extension in which has at least one more root than in . Iterating this construction, one gets eventually a field over which factors into linear factors. This field, unique up to a field isomorphism, is called the splitting field of .\n\nIf \"R\" is an integral domain, an element \"f\" of \"R\" that is neither zero nor a unit is called irreducible if there are no non-units \"g\" and \"h\" with \"f\" = \"gh\". One can show that every prime element is irreducible; the converse is not true in general but holds in unique factorization domains. The polynomial ring \"F\"[\"x\"] over a field \"F\" (or any unique-factorization domain) is again a unique factorization domain. Inductively, this means that the polynomial ring in \"n\" indeterminants (over a ring \"R\") is a unique factorization domain if the same is true for \"R\".\n\n\n\n"}
{"id": "11506732", "url": "https://en.wikipedia.org/wiki?curid=11506732", "title": "Journal of Symbolic Logic", "text": "Journal of Symbolic Logic\n\nThe Journal of Symbolic Logic is a peer-reviewed mathematics journal published quarterly by Association for Symbolic Logic. It was established in 1936 and covers mathematical logic. The journal is indexed by \"Mathematical Reviews\", Zentralblatt MATH, and Scopus. Its 2009 MCQ was 0.28, and its 2009 impact factor was 0.631.\n"}
{"id": "42823419", "url": "https://en.wikipedia.org/wiki?curid=42823419", "title": "K-cell (mathematics)", "text": "K-cell (mathematics)\n\nA \"k\"-cell is a higher-dimensional version of a rectangle or rectangular solid. It is the Cartesian product of \"k\" closed intervals on the real line. This means that a \"k\"-dimensional rectangular solid has each of its edges equal to one of the closed intervals used in the definition. The \"k\" intervals need not be identical. For example, a 2-cell is a rectangle in such that the sides of the rectangles are parallel to the coordinate axes.\n\nLet \"a\" ∈ and \"b\" ∈ . If \"a\" < \"b\" for all \"i\" = 1...,\"k\", the set of all points x = (\"x\"...,\"x\") in whose coordinates satisfy the inequalities \"a\" ≤ \"x\" ≤ \"b\" is a \"k\"-cell.\nEvery \"k\"-cell is compact.\n\nA \"k\"-cell of dimension \"k\" ≤ 3 is especially simple. For example, a 1-cell is simply the interval [\"a\",\"b\"] with \"a\" < \"b\". A 2-cell is the rectangle formed by the Cartesian product of two closed intervals, and a 3-cell is a rectangular solid. \n\nThe sides and edges of a \"k\"-cell need not be equal in (Euclidean) length; although the unit cube (which has boundaries of equal Euclidean length) is a 3-cell, the set of all 3-cells with equal-length edges is a strict subset of the set of all 3-cells.\n"}
{"id": "1471798", "url": "https://en.wikipedia.org/wiki?curid=1471798", "title": "Kraft–McMillan inequality", "text": "Kraft–McMillan inequality\n\nIn coding theory, the Kraft–McMillan inequality gives a necessary and sufficient condition for the existence of a prefix code (in Leon G. Kraft's version) or a uniquely decodable code (in Brockway McMillan's version) for a given set of codeword lengths. Its applications to prefix codes and trees often find use in computer science and information theory.\n\nKraft's inequality was published in . However, Kraft's paper discusses only prefix codes, and attributes the analysis leading to the inequality to Raymond Redheffer. The result was independently discovered in . McMillan proves the result for the general case of uniquely decodable codes, and attributes the version for prefix codes to a spoken observation in 1955 by Joseph Leo Doob.\n\nKraft's inequality limits the lengths of codewords in a prefix code: if one takes an exponential of the length of each valid codeword, the resulting set of values must look like a probability mass function, that is, it must have total measure less than or equal to one. Kraft's inequality can be thought of in terms of a constrained budget to be spent on codewords, with shorter codewords being more expensive. Among the useful properties following from the inequality are the following statements:\n\n\nLet each source symbol from the alphabet\n\nbe encoded into a uniquely decodable code over an alphabet of size formula_2 with codeword lengths\n\nThen\n\nConversely, for a given set of natural numbers formula_5 satisfying the above inequality, there exists a uniquely decodable code over an alphabet of size formula_2 with those codeword lengths.\n\nAny binary tree can be viewed as defining a prefix code for the leaves of the tree. Kraft's inequality states that\n\nHere the sum is taken over the leaves of the tree, i.e. the nodes without any children. The depth is the distance to the root node. In the tree to the right, this sum is\n\nFirst, let us show that the Kraft inequality holds whenever formula_9 is a prefix code.\n\nSuppose that formula_10. Let formula_11 be the full formula_2-ary tree of depth formula_13 (thus, every node of formula_11 at level formula_15 has formula_2 children, while the nodes at level formula_13 are leaves). Every word of length formula_18 over an formula_2-ary alphabet corresponds to a node in this tree at depth formula_20. The formula_21th word in the prefix code corresponds to a node formula_22; let formula_23 be the set of all leaf nodes (i.e. of nodes at depth formula_13) in the subtree of formula_11 rooted at formula_22. That subtree being of height formula_27, we have\n\nSince the code is a prefix code, those subtrees cannot share any leaves, which means that\n\nThus, given that the total number of nodes at depth formula_13 is formula_31, we have\n\nfrom which the result follows.\n\nConversely, given any ordered sequence of formula_33 natural numbers,\n\nsatisfying the Kraft inequality, one can construct a prefix code with codeword lengths equal to each formula_35 by choosing a word of length formula_35 arbitrarily, then ruling out all words of greater length that have it as a prefix. There again, we shall interpret this in terms of leaf nodes of an formula_2-ary tree of depth formula_13. First choose any node from the full tree at depth formula_39; it corresponds to the first word of our new code. Since we are building a prefix code, all the descendants of this node (i.e., all words that have this first word as a prefix) become unsuitable for inclusion in the code. We consider the descendants at depth formula_13 (i.e., the leaf nodes among the descendants); there are formula_41 such descendant nodes that are removed from consideration. The next iteration picks a (surviving) node at depth formula_42 and removes formula_43 further leaf nodes, and so on. After formula_33 iterations, we have removed a total of\n\nnodes. The question is whether we need to remove more leaf nodes than we actually have available — formula_46 in all — in the process of building the code. Since the Kraft inequality holds, we have indeed\n\nand thus a prefix code can be built. Note that as the choice of nodes at each step is largely arbitrary, many different suitable prefix codes can be built, in general.\n\nNow, we will prove that the Kraft inequality holds whenever formula_9 is a uniquely decodable code. (The converse needs not be proven, since we have already proven it for prefix codes, which is a stronger claim.)\n\nConsider the generating function in inverse of \"x\" for the code \"S\"\n\nin which formula_50—the coefficient in front of formula_51—is the number of distinct codewords of length formula_20. Here min is the length of the shortest codeword in \"S\", and max is the length of the longest codeword in \"S\".\n\nConsider all \"m\"-powers \"S\", in the form of words formula_53, where formula_54 are indices between 1 and \"n\". Note that, since \"S\" was assumed to uniquely decodable,\nformula_55 implies formula_56. Because of this property, one can compute the generating function formula_57 for formula_58 from the generating function formula_59 as\n\nHere, similarly as before, formula_61 — the coefficient in front of formula_51 in formula_57 — is the number of words of length formula_20 in formula_58. Clearly, formula_61 cannot exceed formula_67.\nHence for any positive \"x\",\n\nSubstituting the value \"x\" = \"r\" we have\nfor any positive integer formula_70. The left side of the inequality grows exponentially in formula_70 \nand the right side only linearly. The only possibility for the inequality to be valid for all formula_70\nis that formula_73.\nLooking back on the definition of formula_59 we finally get the inequality.\n\nGiven a sequence of formula_33 natural numbers,\n\nsatisfying the Kraft inequality, we can construct a prefix code as follows. Define the \"i\" codeword, \"C\", to be the first formula_35 digits after the radix point (e.g. decimal point) in the base \"r\" representation of\n\nNote that by Kraft's inequality, this sum is never more than 1. Hence the codewords capture the entire value of the sum. Therefore, for \"j\" > \"i\", the first formula_35 digits of \"C\" form a larger number than \"C\", so the code is prefix free.\n\nChaitin's constant, Canonical Huffman code.\n"}
{"id": "34871920", "url": "https://en.wikipedia.org/wiki?curid=34871920", "title": "Labelled enumeration theorem", "text": "Labelled enumeration theorem\n\nIn combinatorial mathematics, the labelled enumeration theorem is the counterpart of the Pólya enumeration theorem for the labelled case, where we have a set of labelled objects given by an exponential generating function (EGF) \"g\"(\"z\") which are being distributed into \"n\" slots and a permutation group \"G\" which permutes the slots, thus creating equivalence classes of configurations. There is a special re-labelling operation that re-labels the objects in the slots, assigning labels from 1 to \"k\", where \"k\" is the total number of nodes, i.e. the sum of the number of nodes of the individual objects. The EGF formula_1 of the number of different configurations under this re-labelling process is given by\n\nIn particular, if \"G\" is the symmetric group of order \"n\" (hence, |\"G\"| = \"n\"!), the functions \"f\"_\"n\"(\"z\") can be further combined into a single generating function:\n\nwhich is exponential w.r.t. the variable \"z\" and ordinary w.r.t. the variable \"t\".\n\nWe assume that an object formula_4 of size formula_5 represented by formula_6 contains formula_7 labelled internal nodes, with the labels going from 1 to \"m\". The action of \"G\" on the slots is greatly simplified compared to the unlabelled case, because the labels distinguish the objects in the slots, and the orbits under \"G\" all have the same size formula_8. (The EGF \"g\"(\"z\") may not include objects of size zero. This is because they are not distinguished by labels and therefore the presence of two or more of such objects creates orbits whose size is less than formula_8.) As mentioned, the nodes of the objects are re-labelled when they are distributed into the slots. Say an object of size formula_10 goes into the first slot, an object of size formula_11 into the second slot, and so on, and the total size of the configuration is \"k\", so that\n\nThe re-labelling process works as follows: choose one of\n\npartitions of the set of \"k\" labels into subsets of size formula_14\nNow re-label the internal nodes of each object using the labels from the respective subset, preserving the order of the labels. E.g. if the first object contains four nodes labelled from 1 to 4 and the set of labels chosen for this object is {2, 5, 6, 10}, then node 1 receives the label 2, node 2, the label 5, node 3, the label 6 and node 4, the label 10. In this way the labels on the objects induce a unique labelling using the labels from the subset of formula_15 chosen for the object.\n\nIt follows from the re-labelling construction that there are\n\nor\n\ndifferent configurations of total size \"k\". The formula evaluates to an integer because formula_18 is zero for \"k\" < \"n\" (remember that \"g\" does not include objects of size zero) and when formula_19 we have formula_20 and the order formula_8 of \"G\" divides the order of formula_22, which is formula_23, by Lagrange's theorem. The conclusion is that the EGF of the labelled configurations is given by\n\nThis formula could also be obtained by enumerating sequences, i.e. the case when the slots are not being permuted, and by using the above argument without the formula_25-factor to show that their generating function under re-labelling is given by formula_26. Finally note that every sequence belongs to an orbit of size formula_8, hence the generating function of the orbits is given by formula_28\n\n"}
{"id": "23371726", "url": "https://en.wikipedia.org/wiki?curid=23371726", "title": "Lagrangian mechanics", "text": "Lagrangian mechanics\n\nLagrangian mechanics is a reformulation of classical mechanics, introduced by the Italian-French mathematician and astronomer Joseph-Louis Lagrange in 1788.\n\nIn Lagrangian mechanics, the trajectory of a system of particles is derived by solving the Lagrange equations in one of two forms, either the \"Lagrange equations of the first kind\", which treat constraints explicitly as extra equations, often using Lagrange multipliers; or the \"Lagrange equations of the second kind\", which incorporate the constraints directly by judicious choice of generalized coordinates. In each case, a mathematical function called the Lagrangian is a function of the generalized coordinates, their time derivatives, and time, and contains the information about the dynamics of the system.\n\nNo new physics are necessarily introduced in applying Lagrangian mechanics compared to Newtonian mechanics. It is, however, more mathematically sophisticated and systematic. Newton's laws can include non-conservative forces like friction; however, they must include constraint forces explicitly and are best suited to Cartesian coordinates. Lagrangian mechanics is ideal for systems with conservative forces and for bypassing constraint forces in any coordinate system. Dissipative and driven forces can be accounted for by splitting the external forces into a sum of potential and non-potential forces, leading to a set of modified Euler–Lagrange (EL) equations. Generalized coordinates can be chosen by convenience, to exploit symmetries in the system or the geometry of the constraints, which may simplify solving for the motion of the system. Lagrangian mechanics also reveals conserved quantities and their symmetries in a direct way, as a special case of Noether's theorem.\n\nLagrangian mechanics is important not just for its broad applications, but also for its role in advancing deep understanding of physics. Although Lagrange only sought to describe classical mechanics in his treatise \"Mécanique analytique\", William Rowan Hamilton later developed \"Hamilton's principle\" that can be used to derive the Lagrange equation and was later recognized to be applicable to much of fundamental theoretical physics as well, particularly quantum mechanics and the theory of relativity. It can also be applied to other systems \"by analogy\", for instance to coupled electric circuits with inductances and capacitances.\n\nLagrangian mechanics is widely used to solve mechanical problems in physics and when Newton's formulation of classical mechanics is not convenient. Lagrangian mechanics applies to the dynamics of particles, while fields are described using a Lagrangian density. Lagrange's equations are also used in optimisation problems of dynamic systems. In mechanics, Lagrange's equations of the second kind are used much more than those of the first kind.\n\nSuppose we have a bead sliding around on a wire, or a swinging simple pendulum, etc. If one tracks each of the massive objects (bead, pendulum bob, etc.) as a particle, calculation of the motion of the particle using Newtonian mechanics would require solving for the time-varying constraint force required to keep the particle in the constrained motion (reaction force exerted by the wire on the bead, or tension in the pendulum rod). For the same problem using Lagrangian mechanics, one looks at the path the particle can take and chooses a convenient set of \"independent\" generalized coordinates that completely characterize the possible motion of the particle. This choice eliminates the need for the constraint force to enter into the resultant system of equations. There are fewer equations since one is not directly calculating the influence of the constraint on the particle at a given moment.\n\nFor a wide variety of physical systems, if the size and shape of a massive object are negligible, it is a useful simplification to treat it as a point particle. For a system of \"N\" point particles with masses \"m\", \"m\", ..., \"m\", each particle has a position vector, denoted r, r, ..., r. Cartesian coordinates are often sufficient, so r = (\"x\", \"y\", \"z\"), r = (\"x\", \"y\", \"z\") and so on. In three dimensional space, each position vector requires three coordinates to uniquely define the location of a point, so there are 3\"N\" coordinates to uniquely define the configuration of the system. These are all specific points in space to locate the particles, a general point in space is written r = (\"x\", \"y\", \"z\"). The velocity of each particle is how fast the particle moves along its path of motion, and is the time derivative of its position, thusformula_1In Newtonian mechanics, the equations of motion are given by Newton's laws. The second law \"net force equals mass times acceleration\",formula_2applies to each particle. For an \"N\" particle system in 3 dimensions, there are 3\"N\" second order ordinary differential equations in the positions of the particles to solve for.\n\nInstead of forces, Lagrangian mechanics uses the energies in the system. The central quantity of Lagrangian mechanics is the Lagrangian, a function which summarizes the dynamics of the entire system. Overall, the Lagrangian has units of energy, but no single expression for all physical systems. Any function which generates the correct equations of motion, in agreement with physical laws, can be taken as a Lagrangian. It is nevertheless possible to construct general expressions for large classes of applications. The \"non-relativistic\" Lagrangian for a system of particles can be defined by\n\nwhere\n\nis the total kinetic energy of the system, equalling the sum Σ of the kinetic energies of the particles, and \"V\" is the potential energy of the system.\n\nKinetic energy is the energy of the system's motion, and \"v\" = v · v is the magnitude squared of velocity, equivalent to the dot product of the velocity with itself. The kinetic energy is a function only of the velocities v, not the positions r nor time \"t\", so \"T\" = \"T\"(v, v, ...).\n\nThe potential energy of the system reflects the energy of interaction between the particles, i.e. how much energy any one particle will have due to all the others and other external influences. For conservative forces (e.g. Newtonian gravity), it is a function of the position vectors of the particles only, so \"V\" = \"V\"(r, r, ...). For those non-conservative forces which can be derived from an appropriate potential (e.g. electromagnetic potential), the velocities will appear also, \"V\" = \"V\"(r, r, ..., v, v, ...). If there is some external field or external driving force changing with time, the potential will change with time, so most generally \"V\" = \"V\"(r, r, ..., v, v, ..., \"t\").\n\nThe above form of \"L\" does not hold in relativistic Lagrangian mechanics, and must be replaced by a function consistent with special or general relativity. Also, for dissipative forces another function must be introduced alongside \"L\".\n\nOne or more of the particles may each be subject to one or more holonomic constraints, such a constraint is described by an equation of the form \"f\"(r, \"t\") = 0. If the number of constraints in the system is \"C\", then each constraint has an equation, \"f\"(r, \"t\") = 0, \"f\"(r, \"t\") = 0, ... \"f\"(r, \"t\") = 0, each could apply to any of the particles. If particle \"k\" is subject to constraint \"i\", then \"f\"(r, \"t\") = 0. At any instant of time, the coordinates of a constrained particle are linked together and not independent. The constraint equations determine the allowed paths the particles can move along, but not where they are or how fast they go at every instant of time. Nonholonomic constraints depend on the particle velocities, accelerations, or higher derivatives of position. Lagrangian mechanics \"can only be applied to systems whose constraints, if any, are all holonomic\". Three examples of nonholonomic constraints are: when the constraint equations are nonintegrable, when the constraints have inequalities, or with complicated non-conservative forces like friction. Nonholonomic constraints require special treatment, and one may have to revert to Newtonian mechanics, or use other methods.\n\nIf \"T\" or \"V\" or both depend explicitly on time due to time-varying constraints or external influences, the Lagrangian \"L\"(r, r, ... v, v, ... \"t\") is \"explicitly time-dependent\". If neither the potential nor the kinetic energy depend on time, then the Lagrangian \"L\"(r, r, ... v, v, ...) is \"explicitly independent of time\". In either case, the Lagrangian will always have implicit time-dependence through the generalized coordinates.\n\nWith these definitions Lagrange's equations of the first kind are\n\nwhere \"k\" = 1, 2, ..., \"N\" labels the particles, there is a Lagrange multiplier \"λ\" for each constraint equation \"f\", and\n\nare each shorthands for a vector of partial derivatives with respect to the indicated variables (not a derivative with respect to the entire vector). Each overdot is a shorthand for a time derivative. This procedure does increase the number of equations to solve compared to Newton's laws, from 3\"N\" to 3\"N\" + \"C\", because there are 3\"N\" coupled second order differential equations in the position coordinates and multipliers, plus \"C\" constraint equations. However, when solved alongside the position coordinates of the particles, the multipliers can yield information about the constraint forces. The coordinates do not need to be eliminated by solving the constraint equations.\n\nIn the Lagrangian, the position coordinates and velocity components are all independent variables, and derivatives of the Lagrangian are taken with respect to these separately according to the usual differentiation rules (e.g. the derivative of \"L\" with respect to the \"z\"-velocity component of particle 2, \"v\" = d\"z\"/d\"t\", is just that; no awkward chain rules or total derivatives need to be used to relate the velocity component to the corresponding coordinate \"z\").\n\nIn each constraint equation, one coordinate is redundant because it is determined from the other two. The number of \"independent\" coordinates is therefore \"n\" = 3\"N\" − \"C\". We can transform each position vector to a common set of \"n\" generalized coordinates, conveniently written as an \"n\"-tuple q = (\"q\", \"q\", ... \"q\"), by expressing each position vector, and hence the position coordinates, as functions of the generalized coordinates and time,\n\nThe vector q is a point in the configuration space of the system. The time derivatives of the generalized coordinates are called the generalized velocities, and for each particle the transformation of its velocity vector, the total derivative of its position with respect to time, is\n\nGiven this v, the kinetic energy \"in generalized coordinates\" depends on the generalized velocities, generalized coordinates, and time if the position vectors depend explicitly on time due to time-varying constraints, so \"T\" = \"T\"(q, dq/d\"t\", \"t\").\n\nWith these definitions we have the Euler–Lagrange equations, or Lagrange's equations of the second kind\n\nare mathematical results from the calculus of variations, which can also be used in mechanics. Substituting in the Lagrangian \"L\"(q, dq/d\"t\", \"t\"), gives the equations of motion of the system. The number of equations has decreased compared to Newtonian mechanics, from 3\"N\" to \"n\" = 3\"N\" − \"C\" coupled second order differential equations in the generalized coordinates. These equations do not include constraint forces at all, only non-constraint forces need to be accounted for.\n\nAlthough the equations of motion include partial derivatives, the results of the partial derivatives are still ordinary differential equations in the position coordinates of the particles. The total time derivative denoted d/d\"t\" often involves implicit differentiation. Both equations are linear in the Lagrangian, but will generally be nonlinear coupled equations in the coordinates.\n\nFor simplicity, Newton's laws can be illustrated for one particle without much loss of generality (for a system of \"N\" particles, all of these equations apply to each particle in the system). The equation of motion for particle of mass \"m\" is Newton's second law of 1687, in modern vector notation\n\nwhere a is its acceleration and F the resultant force acting \"on\" it. In three spatial dimensions, this is a system of three coupled second order ordinary differential equations to solve, since there are three components in this vector equation. The solutions are the position vectors r of the particles at time \"t\", subject to the initial conditions of r and v when \"t\" = 0.\n\nNewton's laws are easy to use in Cartesian coordinates, but Cartesian coordinates are not always convenient, and for other coordinate systems the equations of motion can become complicated. In a set of curvilinear coordinates ξ = (\"ξ\", \"ξ\", \"ξ\"), the law in tensor index notation is the \"Lagrangian form\"\n\nwhere \"F\" is the \"a\"th contravariant components of the resultant force acting on the particle, Γ\"\" are the Christoffel symbols of the second kind,\n\nis the kinetic energy of the particle, and \"g\" the covariant components of the \"metric tensor\" of the curvilinear coordinate system. All the indices \"a\", \"b\", \"c\", each take the values 1, 2, 3. Curvilinear coordinates are not the same as generalized coordinates.\n\nIt may seem like an overcomplication to cast Newton's law in this form, but there are advantages. The acceleration components in terms of the Christoffel symbols can be avoided by evaluating derivatives of the kinetic energy instead. If there is no resultant force acting on the particle, F = 0, it does not accelerate, but moves with constant velocity in a straight line. Mathematically, the solutions of the differential equation are \"geodesics\", the curves of extremal length between two points in space (These may end up being minimal so the shortest paths, but that is not necessary). In flat 3d real space the geodesics are simply straight lines. So for a free particle, Newton's second law coincides with the geodesic equation, and states free particles follow geodesics, the extremal trajectories it can move along. If the particle is subject to forces, F ≠ 0, the particle accelerates due to forces acting on it, and deviates away from the geodesics it would follow if free. With appropriate extensions of the quantities given here in flat 3d space to 4d curved spacetime, the above form of Newton's law also carries over to Einstein's general relativity, in which case free particles follow geodesics in curved spacetime that are no longer \"straight lines\" in the ordinary sense.\n\nHowever, we still need to know the total resultant force F acting on the particle, which in turn requires the resultant non-constraint force N plus the resultant constraint force C,\n\nThe constraint forces can be complicated, since they will generally depend on time. Also, if there are constraints, the curvilinear coordinates are not independent but related by one or more constraint equations.\n\nThe constraint forces can either be eliminated from the equations of motion so only the non-constraint forces remain, or included by including the constraint equations in the equations of motion.\n\nA fundamental result in analytical mechanics is D'Alembert's principle, introduced in 1708 by Jacques Bernoulli to understand static equilibrium, and developed by D'Alembert in 1743 to solve dynamical problems. The principle asserts for \"N\" particles the virtual work, i.e. the work along a virtual displacement, δr, is zero\n\nThe \"virtual displacements\", δr, are by definition infinitesimal changes in the configuration of the system consistent with the constraint forces acting on the system \"at an instant of time\", i.e. in such a way that the constraint forces maintain the constrained motion. They are not the same as the actual displacements in the system, which are caused by the resultant constraint and non-constraint forces acting on the particle to accelerate and move it. Virtual work is the work done along a virtual displacement for any force (constraint or non-constraint).\n\nSince the constraint forces act perpendicular to the motion of each particle in the system to maintain the constraints, the total virtual work by the constraint forces acting on the system is zero;\n\nso that\n\nThus D'Alembert's principle allows us to concentrate on only the applied non-constraint forces, and exclude the constraint forces in the equations of motion. The form shown is also independent of the choice of coordinates. However, it cannot be readily used to set up the equations of motion in an arbitrary coordinate system since the displacements δr might be connected by a constraint equation, which prevents us from setting the \"N\" individual summands to 0. We will therefore seek a system of mutually independent coordinates for which the total sum will be 0 if and only if the individual summands are 0. Setting each of the summands to 0 will eventually give us our separated equations of motion.\n\nIf there are constraints on particle \"k\", then since the coordinates of the position r = (\"x\", \"y\", \"z\") are linked together by a constraint equation, so are those of the virtual displacements \"δr = (\"δx\", \"δy\", \"δz\"). Since the generalized coordinates are independent, we can avoid the complications with the \"δr by converting to virtual displacements in the generalized coordinates. These are related in the same form as a total differential,\n\nThere is no partial time derivative with respect to time multiplied by a time increment, since this is a virtual displacement, one along the constraints in an \"instant\" of time.\n\nThe first term in D'Alembert's principle above is the virtual work done by the non-constraint forces N along the virtual displacements \"δ\"r, and can without loss of generality be converted into the generalized analogues by the definition of generalized forces\n\nso that\n\nThis is half of the conversion to generalized coordinates. It remains to convert the acceleration term into generalized coordinates, which is not immediately obvious. Recalling the Lagrange form of Newton's second law, the partial derivatives of the kinetic energy with respect to the generalized coordinates and velocities can be found to give the desired result;\n\nNow D'Alembert's principle is in the generalized coordinates as required,\n\nand since these virtual displacements \"δq\" are independent and nonzero, the coefficients can be equated to zero, resulting in Lagrange's equations or the generalized equations of motion,\n\nThese equations are equivalent to Newton's laws \"for the non-constraint forces\". The generalized forces in this equation are derived from the non-constraint forces only – the constraint forces have been excluded from D'Alembert's principle and do not need to be found. The generalized forces may be non-conservative, provided they satisfy D'Alembert's principle.\n\nFor a non-conservative force which depends on velocity, it \"may\" be possible to find a potential energy function \"V\" that depends on positions and velocities. If the generalized forces \"Q\" can be derived from a potential \"V\" such that\n\nequating to Lagrange's equations and defining the Lagrangian as \"L\" = \"T\" − \"V\" obtains Lagrange's equations of the second kind or the Euler–Lagrange equations of motion\n\nHowever, the Euler–Lagrange equations can only account for non-conservative forces \"if\" a potential can be found as shown. This may not always be possible for non-conservative forces, and Lagrange's equations do not involve any potential, only generalized forces; therefore they are more general than the Euler–Lagrange equations.\n\nThe Euler–Lagrange equations also follow from the calculus of variations. The \"variation\" of the Lagrangian is\n\nwhich has a similar form to the total differential of \"L\", but the virtual displacements and their time derivatives replace differentials, and there is no time increment in accordance with the definition of the virtual displacements. An integration by parts with respect to time can transfer the time derivative of \"δq\" to the ∂\"L\"/∂(d\"q\"/d\"t\"), in the process exchanging d(\"δq\")/d\"t\" for \"δq\", allowing the independent virtual displacements to be factorized from the derivatives of the Lagrangian,\n\nNow, if the condition \"δq\"(\"t\") = \"δq\"(\"t\") = 0 holds for all \"j\", the terms not integrated are zero. If in addition the entire time integral of \"δL\" is zero, then because the \"δq\" are independent, and the only way for a definite integral to be zero is if the integrand equals zero, each of the coefficients of \"δq\" must also be zero. Then we obtain the equations of motion. This can be summarized by Hamilton's principle;\n\nThe time integral of the Lagrangian is another quantity called the action, defined as\n\nwhich is a \"functional\"; it takes in the Lagrangian function for all times between \"t\" and \"t\" and returns a scalar value. Its dimensions are the same as [ angular momentum ], [energy]·[time], or [length]·[momentum]. With this definition Hamilton's principle is\n\nThus, instead of thinking about particles accelerating in response to applied forces, one might think of them picking out the path with a stationary action, with the end points of the path in configuration space held fixed at the initial and final times. Hamilton's principle is sometimes referred to as the \"principle of least action\", however the action functional need only be \"stationary\", not necessarily a maximum or a minimum value. Any variation of the functional gives an increase in the functional integral of the action.\n\nHistorically, the idea of finding the shortest path a particle can follow subject to a force motivated the first applications of the calculus of variations to mechanical problems, such as the Brachistochrone problem solved by Jean Bernoulli in 1696, as well as Leibniz, Daniel Bernoulli, L'Hôpital around the same time, and Newton the following year. Newton himself was thinking along the lines of the variational calculus, but did not publish. These ideas in turn lead to the variational principles of mechanics, of Fermat, Maupertuis, Euler, Hamilton, and others.\n\nHamilton's principle can be applied to nonholonomic constraints if the constraint equations can be put into a certain form, a linear combination of first order differentials in the coordinates. The resulting constraint equation can be rearranged into first order differential equation. This will not be given here.\n\nThe Lagrangian \"L\" can be varied in the Cartesian r coordinates, for \"N\" particles,\n\nHamilton's principle is still valid even if the coordinates \"L\" is expressed in are not independent, here r, but the constraints are still assumed to be holonomic. As always the end points are fixed \"δr(\"t\") = \"δr(\"t\") = 0 for all \"k\". What cannot be done is to simply equate the coefficients of δr to zero because the δr are not independent. Instead, the method of Lagrange multipliers can be used to include the constraints. Multiplying each constraint equation \"f\"(r, \"t\") = 0 by a Lagrange multiplier \"λ\" for \"i\" = 1, 2, ..., \"C\", and adding the results to the original Lagrangian, gives the new Lagrangian\n\nThe Lagrange multipliers are arbitrary functions of time \"t\", but not functions of the coordinates r, so the multipliers are on equal footing with the position coordinates. Varying this new Lagrangian and integrating with respect to time gives\n\nThe introduced multipliers can be found so that the coefficients of \"δ\"r are zero, even though the r are not independent. The equations of motion follow. From the preceding analysis, obtaining the solution to this integral is equivalent to the statement\n\nwhich are Lagrange's equations of the first kind. Also, the \"λ\" Euler-Lagrange equations for the new Lagrangian return the constraint equations\n\nFor the case of a conservative force given by the gradient of some potential energy \"V\", a function of the r coordinates only, substituting the Lagrangian \"L\" = \"T\" − \"V\" gives\n\nand identifying the derivatives of kinetic energy as the (negative of the) resultant force, and the derivatives of the potential equaling the non-constraint force, it follows the constraint forces are\n\nthus giving the constraint forces explicitly in terms of the constraint equations and the Lagrange multipliers.\n\nIn some cases, the Lagrangian has properties which can provide information about the system without solving the equations of motion. These follow from Lagrange's equations of the second kind.\n\nThe Lagrangian of a given system is not unique. A Lagrangian \"L\" can be multiplied by a nonzero constant \"a\", an arbitrary constant \"b\" can be added, and the new Lagrangian \"aL\" + \"b\" will describe exactly the same motion as \"L\". A less obvious result is that two Lagrangians describing the same system can differ by the total derivative (not partial) of some function \"f\"(q, \"t\") with respect to time;\n\nEach Lagrangian will obtain exactly the same equations of motion.\n\nGiven a set of generalized coordinates q, if we change these variables to a new set of generalized coordinates s according to a point transformation q = q(s, \"t\"), the new Lagrangian \"L\"′ is a function of the new coordinates\n\nand by the chain rule for partial differentiation, Lagrange's equations are invariant under this transformation;\n\nThis may simplify the equations of motion.\n\nAn important property of the Lagrangian is that conserved quantities can easily be read off from it. The \"generalized momentum\" \"canonically conjugate to\" the coordinate \"q\" is defined by\n\nIf the Lagrangian \"L\" does \"not\" depend on some coordinate \"q\", it follows immediately from the Euler–Lagrange equations that\n\nand integrating shows the corresponding generalized momentum equals a constant, a conserved quantity. This is a special case of Noether's theorem. Such coordinates are called \"cyclic\" or \"ignorable\".\n\nFor example, a system may have a Lagrangian\n\nwhere \"r\" and \"z\" are lengths along straight lines, \"s\" is an arc length along some curve, and \"θ\" and \"φ\" are angles. Notice \"z\", \"s\", and \"φ\" are all absent in the Lagrangian even though their velocities are not. Then the momenta\n\nare all conserved quantities. The units and nature of each generalized momentum will depend on the corresponding coordinate; in this case \"p\" is a translational momentum in the \"z\" direction, \"p\" is also a translational momentum along the curve \"s\" is measured, and \"p\" is an angular momentum in the plane the angle \"φ\" is measured in. However complicated the motion of the system is, all the coordinates and velocities will vary in such a way that these momenta are conserved.\n\nTaking the total derivative of the Lagrangian \"L\" = \"T\" − \"V\" with respect to time leads to the general result\n\nIf the entire Lagrangian is explicitly independent of time, it follows the partial time derivative of the Lagrangian is zero, , so the quantity under the total time derivative in brackets\n\nmust be a constant for all times during the motion of the system, and it also follows the kinetic energy is a homogenous function of degree 2 in the generalized velocities. If in addition the potential \"V\" is only a function of coordinates and independent of velocities, it follows by direct calculation, or use of Euler's theorem for homogenous functions, that\n\nUnder all these circumstances, the constant\n\nis the \"total conserved energy\" of the system. The kinetic and potential energies still change as the system evolves, but the motion of the system will be such that their sum, the total energy, is constant. This is a valuable simplification, since the energy \"E\" is a constant of integration that counts as an arbitrary constant for the problem, and it may be possible to integrate the velocities from this energy relation to solve for the coordinates. In the case the velocity or kinetic energy or both depends on time, then the energy is \"not\" conserved.\n\nIf the potential energy is a homogeneous function of the coordinates and independent of time, and all position vectors are scaled by the same nonzero constant \"α\", r′ = \"α\"r, so that\n\nand time is scaled by a factor \"β\", \"t\"′ = \"βt\", then the velocities v are scaled by a factor of \"α\"/\"β\" and the kinetic energy \"T\" by (\"α\"/\"β\"). The entire Lagrangian has been scaled by the same factor if\n\nSince the lengths and times have been scaled, the trajectories of the particles in the system follow geometrically similar paths differing in size. The length \"l\" traversed in time \"t\" in the original trajectory corresponds to a new length \"l′\" traversed in time \"t′\" in the new trajectory, given by the ratios\n\nFor a given system, if two subsystems \"A\" and \"B\" are non-interacting, the Lagrangian \"L\" of the overall system is the sum of the Lagrangians \"L\" and \"L\" for the subsystems:\n\nIf they do interact this is not possible. In some situations, it may be possible to separate the Lagrangian of the system \"L\" into the sum of non-interacting Lagrangians, plus another Lagrangian \"L\" containing information about the interaction,\n\nThis may be physically motivated by taking the non-interacting Lagrangians to be kinetic energies only, while the interaction Lagrangian is the system's total potential energy. Also, in the limiting case of negligible interaction, \"L\" tends to zero reducing to the non-interacting case above.\n\nThe extension to more than two non-interacting subsystems is straightforward – the overall Lagrangian is the sum of the separate Lagrangians for each subsystem. If there are interactions, then interaction Lagrangians may be added.\n\nThe following examples apply Lagrange's equations of the second kind to mechanical problems.\n\nA particle of mass \"m\" moves under the influence of a conservative force derived from the gradient ∇ of a scalar potential,\n\nIf there are more particles, in accordance with the above results, the total kinetic energy is a sum over all the particle kinetic energies, and the potential is a function of all the coordinates.\n\nThe Lagrangian of the particle can be written\n\nThe equations of motion for the particle are found by applying the Euler–Lagrange equation, for the \"x\" coordinate\n\nwith derivatives\n\nhence\n\nand similarly for the \"y\" and \"z\" coordinates. Collecting the equations in vector form we find\n\nwhich is Newton's second law of motion for a particle subject to a conservative force.\n\nThe Lagrangian for the above problem in spherical coordinates, with a central potential, is\n\nso the Euler–Lagrange equations are\n\nThe \"φ\" coordinate is cyclic since it does not appear in the Lagrangian, so the conserved momentum in the system is the angular momentum\n\nin which \"r\", \"θ\" and \"dφ/dt\" can all vary with time, but only in such a way that \"p\" is constant.\n\nConsider a pendulum of mass \"m\" and length \"ℓ\", which is attached to a support with mass \"M\", which can move along a line in the \"x\"-direction. Let \"x\" be the coordinate along the line of the support, and let us denote the position of the pendulum by the angle \"θ\" from the vertical. The coordinates and velocity components of the pendulum bob are\n\nThe generalized coordinates can be taken to be \"x\" and \"θ\". The kinetic energy of the system is then\n\nand the potential energy is\n\ngiving the Lagrangian\n\nSince \"x\" is absent from the Lagrangian, it is a cyclic coordinate. The conserved momentum is\n\nand the Lagrange equation for the support coordinate \"x\" is\n\nThe Lagrange equation for the angle \"θ\" is\n\nand simplifying\n\nThese equations may look quite complicated, but finding them with Newton's laws would have required carefully identifying all forces, which would have been much more laborious and prone to errors. By considering limit cases, the correctness of this system can be verified: For example, formula_70 should give the equations of motion for a simple pendulum that is at rest in some inertial frame, while formula_71 should give the equations for a pendulum in a constantly accelerating system, etc. Furthermore, it is trivial to obtain the results numerically, given suitable starting conditions and a chosen time step, by stepping through the results iteratively.\n\nTwo bodies of masses \"m\" and \"m\" with position vectors r and r are in orbit about each other due to an attractive central potential \"V\". We may write down the Lagrangian in terms of the position coordinates as they are, but it is an established procedure to convert the two-body problem into a one-body problem as follows. Introduce the Jacobi coordinates; the separation of the bodies r = r − r and the location of the center of mass R = (\"m\"r + \"m\"r)/(\"m\" + \"m\"). The Lagrangian is then\n\nwhere \"M\" = \"m\" + \"m\" is the total mass, \"μ\" = \"m\"\"m\"/(\"m\" + \"m\") is the reduced mass, and \"V\" the potential of the radial force, which depends only on the magnitude of the separation |r| = |r − r|. The Lagrangian splits into a \"center-of-mass\" term \"L\" and a \"relative motion\" term \"L\".\n\nThe Euler–Lagrange equation for R is simply\n\nwhich states the center of mass moves in a straight line at constant velocity.\n\nSince the relative motion only depends on the magnitude of the separation, it is ideal to use polar coordinates (\"r\", \"θ\") and take \"r\" = |r|,\n\nso \"θ\" is an ignorable coordinate with the corresponding conserved (angular) momentum\n\nThe radial coordinate \"r\" and angular velocity d\"θ\"/d\"t\" can vary with time, but only in such a way that \"ℓ\" is constant. The Lagrange equation for \"r\" is\n\nThis equation is identical to the radial equation obtained using Newton's laws in a \"co-rotating\" reference frame, that is, a frame rotating with the reduced mass so it appears stationary. Eliminating the angular velocity d\"θ\"/d\"t\" from this radial equation,\n\nwhich is the equation of motion for a one-dimensional problem in which a particle of mass \"μ\" is subjected to the inward central force − d\"V\"/d\"r\" and a second outward force, called in this context the centrifugal force\n\nOf course, if one remains entirely within the one-dimensional formulation, \"ℓ\" enters only as some imposed parameter of the external outward force, and its interpretation as angular momentum depends upon the more general two-dimensional problem from which the one-dimensional problem originated.\n\nIf one arrives at this equation using Newtonian mechanics in a co-rotating frame, the interpretation is evident as the centrifugal force in that frame due to the rotation of the frame itself. If one arrives at this equation directly by using the generalized coordinates (\"r\", \"θ\") and simply following the Lagrangian formulation without thinking about frames at all, the interpretation is that the centrifugal force is an outgrowth of \"using polar coordinates\". As Hildebrand says:\n\n\"Since such quantities are not true physical forces, they are often called \"inertia forces\". Their presence or absence depends, not upon the particular problem at hand, but \"upon the coordinate system chosen\".\" In particular, if Cartesian coordinates are chosen, the centrifugal force disappears, and the formulation involves only the central force itself, which provides the centripetal force for a curved motion.\n\nThis viewpoint, that fictitious forces originate in the choice of coordinates, often is expressed by users of the Lagrangian method. This view arises naturally in the Lagrangian approach, because the frame of reference is (possibly unconsciously) selected by the choice of coordinates. For example, see for a comparison of Lagrangians in an inertial and in a noninertial frame of reference. See also the discussion of \"total\" and \"updated\" Lagrangian formulations in. Unfortunately, this usage of \"inertial force\" conflicts with the Newtonian idea of an inertial force. In the Newtonian view, an inertial force originates in the acceleration of the frame of observation (the fact that it is not an inertial frame of reference), not in the choice of coordinate system. To keep matters clear, it is safest to refer to the Lagrangian inertial forces as \"generalized\" inertial forces, to distinguish them from the Newtonian vector inertial forces. That is, one should avoid following Hildebrand when he says (p. 155) \"we deal \"always\" with \"generalized\" forces, velocities accelerations, and momenta. For brevity, the adjective \"generalized\" will be omitted frequently.\"\n\nIt is known that the Lagrangian of a system is not unique. Within the Lagrangian formalism the Newtonian fictitious forces can be identified by the existence of alternative Lagrangians in which the fictitious forces disappear, sometimes found by exploiting the symmetry of the system.\n\nA test particle is a particle whose mass and charge are assumed to be so small that its effect on external system is insignificant. It is often a hypothetical simplified point particle with no properties other than mass and charge. Real particles like electrons and up quarks are more complex and have additional terms in their Lagrangians.\n\nThe Lagrangian for a charged particle with electrical charge \"q\", interacting with an electromagnetic field, is the prototypical example of a velocity-dependent potential. The electric scalar potential \"ϕ\" = \"ϕ\"(r, \"t\") and magnetic vector potential A = A(r, \"t\") are defined from the electric field E = E(r, \"t\") and magnetic field B = B(r, \"t\") as follows;\n\nThe Lagrangian of a massive charged test particle in an electromagnetic field is\n\nwhich produces the Lorentz force law\n\nAn interesting detail in this example is the generalized momentum conjugate to r is the ordinary momentum plus a contribution from the A field,\n\nIf r is cyclic, which happens if the \"ϕ\" and A fields are uniform (independent of position), then this expression for p given here is the conserved momentum, while the usual quantity \"m\"v is not. This relation is also used in the minimal coupling prescription in quantum mechanics and quantum field theory.\n\nDissipation (i.e. non-conservative systems) can also be treated with an effective Lagrangian formulated by a certain doubling of the degrees of freedom.\n\nIn a more general formulation, the forces could be both conservative and viscous. If an appropriate transformation can be found from the F, Rayleigh suggests using a dissipation function, \"D\", of the following form:\n\nwhere \"C\" are constants that are related to the damping coefficients in the physical system, though not necessarily equal to them. If \"D\" is defined this way, then\n\nand\n\nThe ideas in Lagrangian mechanics have numerous applications in other areas of physics, and can adopt generalized results from the calculus of variations.\n\nA closely related formulation of classical mechanics is Hamiltonian mechanics. The Hamiltonian is defined by\n\nand can be obtained by performing a Legendre transformation on the Lagrangian, which introduces new variables canonically conjugate to the original variables. For example, given a set of generalized coordinates, the variables canonically conjugate are the generalized momenta. This doubles the number of variables, but makes differential equations first order. The Hamiltonian is a particularly ubiquitous quantity in quantum mechanics (see Hamiltonian (quantum mechanics)).\n\nRouthian mechanics is a hybrid formulation of Lagrangian and Hamiltonian mechanics, which is not often used in practice but an efficient formulation for cyclic coordinates.\n\nThe Euler–Lagrange equations can also be formulated in terms of the generalized momenta rather than generalized coordinates. Performing a Legendre transformation on the generalized coordinate Lagrangian \"L\"(q, dq/d\"t\", \"t\") obtains the generalized momenta Lagrangian \"L\"′(p, dp/d\"t\", \"t\") in terms of the original Lagrangian, as well the EL equations in terms of the generalized momenta. Both Lagrangians contain the same information, and either can be used to solve for the motion of the system. In practice generalized coordinates are more convenient to use and interpret than generalized momenta.\n\nThere is no reason to restrict the derivatives of generalized coordinates to first order only. It is possible to derive modified EL equations for a Lagrangian containing higher order derivatives, see Euler–Lagrange equation for details.\n\nLagrangian mechanics can be applied to geometrical optics, by applying variational principles to rays of light in a medium, and solving the EL equations gives the equations of the paths the light rays follow.\n\nLagrangian mechanics can be formulated in special relativity and general relativity. Some features of Lagrangian mechanics are retained in the relativistic theories but difficulties quickly appear in other respects. In particular, the EL equations take the same form, and the connection between cyclic coordinates and conserved momenta still applies, however the Lagrangian must be modified and is not simply the kinetic minus the potential energy of a particle. Also, it is not straightforward to handle multiparticle systems in a manifestly covariant way, it may be possible if a particular frame of reference is singled out.\n\nIn quantum mechanics, action and quantum-mechanical phase are related via Planck's constant, and the principle of stationary action can be understood in terms of constructive interference of wave functions.\n\nIn 1948, Feynman discovered the path integral formulation extending the principle of least action to quantum mechanics for electrons and photons. In this formulation, particles travel every possible path between the initial and final states; the probability of a specific final state is obtained by summing over all possible trajectories leading to it. In the classical regime, the path integral formulation cleanly reproduces Hamilton's principle, and Fermat's principle in optics.\n\nIn Lagrangian mechanics, the generalized coordinates form a discrete set of variables that define the configuration of a system. In classical field theory, the physical system is not a set of discrete particles, but rather a continuous field \"ϕ\"(r, \"t\") defined over a region of 3d space. Associated with the field is a Lagrangian density\n\ndefined in terms of the field and its space and time derivatives at a location r and time \"t\". Analogous to the particle case, for non-relativistic applications the Lagrangian density is also the kinetic energy density of the field, minus its potential energy density (this is not true in general, and the Lagrangian density has to be \"reverse engineered\"). The Lagrangian is then the volume integral of the Lagrangian density over 3d space\n\nwhere dr is a 3d differential volume element. The Lagrangian is a function of time since the Lagrangian density has implicit space dependence via the fields, and may have explicit spatial dependence, but these are removed in the integral, leaving only time in as the variable for the Lagrangian.\n\nThe action principle, and the Lagrangian formalism, are tied closely to Noether's theorem, which connects physical conserved quantities to continuous symmetries of a physical system.\n\nIf the Lagrangian is invariant under a symmetry, then the resulting equations of motion are also invariant under that symmetry. This characteristic is very helpful in showing that theories are consistent with either special relativity or general relativity.\n\n\n\n\n"}
{"id": "23280456", "url": "https://en.wikipedia.org/wiki?curid=23280456", "title": "Lamination (topology)", "text": "Lamination (topology)\n\nIn topology, a branch of mathematics, a lamination is a :\n\nA lamination of a surface is a partition of a closed subset of the surface into smooth curves.\n\nIt may or may not be possible to fill the gaps in a lamination to make a foliation.\n\n\n\n"}
{"id": "195982", "url": "https://en.wikipedia.org/wiki?curid=195982", "title": "Levi-Civita symbol", "text": "Levi-Civita symbol\n\nIn mathematics, particularly in linear algebra, tensor analysis, and differential geometry, the Levi-Civita symbol represents a collection of numbers; defined from the sign of a permutation of the natural numbers , for some positive integer . It is named after the Italian mathematician and physicist Tullio Levi-Civita. Other names include the permutation symbol, antisymmetric symbol, or alternating symbol, which refer to its antisymmetric property and definition in terms of permutations.\n\nThe standard letters to denote the Levi-Civita symbol are the Greek lower case epsilon or , or less commonly the Latin lower case . Index notation allows one to display permutations in a way compatible with tensor analysis:\n\nwhere \"each\" index takes values . There are indexed values of , which can be arranged into an -dimensional array. The key defining property of the symbol is \"total antisymmetry\" in all the indices. When any two indices are interchanged, equal or not, the symbol is negated:\n\nIf any two indices are equal, the symbol is zero. When all indices are unequal, we have:\n\nwhere (called the parity of the permutation) is the number of pairwise interchanges of indices necessary to unscramble into the order , and the factor is called the sign or signature of the permutation. The value must be defined, else the particular values of the symbol for all permutations are indeterminate. Most authors choose , which means the Levi-Civita symbol equals the sign of a permutation when the indices are all unequal. This choice is used throughout this article.\n\nThe term \"-dimensional Levi-Civita symbol\" refers to the fact that the number of indices on the symbol matches the dimensionality of the vector space in question, which may be Euclidean or non-Euclidean, for example, or Minkowski space. The values of the Levi-Civita symbol are independent of any metric tensor and coordinate system. Also, the specific term \"symbol\" emphasizes that it is not a tensor because of how it transforms between coordinate systems; however it can be interpreted as a tensor density.\n\nThe Levi-Civita symbol allows the determinant of a square matrix, and the cross product of two vectors in three-dimensional Euclidean space, to be expressed in index notation.\n\nThe Levi-Civita symbol is most often used in three and four dimensions, and to some extent in two dimensions, so these are given here before defining the general case.\n\nIn two dimensions, Levi-Civita symbol is defined by:\n\nThe values can be arranged into a 2 × 2 antisymmetric matrix:\n\nUse of the two-dimensional symbol is relatively uncommon, although in certain specialized topics like supersymmetry and twistor theory it appears in the context of 2-spinors. The three- and higher-dimensional Levi-Civita symbols are used more commonly.\n\nIn three dimensions, the Levi-Civita symbol is defined by:\n\nThat is, is if is an even permutation of , if it is an odd permutation, and 0 if any index is repeated. In three dimensions only, the cyclic permutations of are all even permutations, similarly the anticyclic permutations are all odd permutations. This means in 3d it is sufficient to take cyclic or anticyclic permutations of and easily obtain all the even or odd permutations.\n\nAnalogous to 2-dimensional matrices, the values of the 3-dimensional Levi-Civita symbol can be arranged into a array:\n\nwhere is the depth (: ; : ; : ), is the row and is the column.\n\nSome examples:\n\nIn four dimensions, the Levi-Civita symbol is defined by:\n\nThese values can be arranged into a array, although in 4 dimensions and higher this is difficult to draw.\n\nSome examples:\n\nMore generally, in dimensions, the Levi-Civita symbol is defined by:\n\nThus, it is the sign of the permutation in the case of a permutation, and zero otherwise.\n\nUsing the capital pi notation for ordinary multiplication of numbers, an explicit expression for the symbol is:\n\nwhere the signum function (denoted ) returns the sign of its argument while discarding the absolute value if nonzero. The formula is valid for all index values, and for any (when or , this is the empty product). However, computing the formula above naively has a time complexity of , whereas the sign can be computed from the parity of the permutation from its disjoint cycles in only cost.\n\nA tensor whose components in an orthonormal basis are given by the Levi-Civita symbol (a tensor of covariant rank ) is sometimes called a permutation tensor.\n\nUnder the ordinary transformation rules for tensors the Levi-Civita symbol is unchanged under pure rotations, consistent with that it is (by definition) the same in all coordinate systems related by orthogonal transformations. However, the Levi-Civita symbol is a pseudotensor because under an orthogonal transformation of Jacobian determinant −1, for example, a reflection in an odd number of dimensions, it \"should\" acquire a minus sign if it were a tensor. As it does not change at all, the Levi-Civita symbol is, by definition, a pseudotensor. \n\nAs the Levi-Civita symbol is a pseudotensor, the result of taking a cross product is a pseudovector, not a vector.\n\nUnder a general coordinate change, the components of the permutation tensor are multiplied by the Jacobian of the transformation matrix. This implies that in coordinate frames different from the one in which the tensor was defined, its components can differ from those of the Levi-Civita symbol by an overall factor. If the frame is orthonormal, the factor will be ±1 depending on whether the orientation of the frame is the same or not.\n\nIn index-free tensor notation, the Levi-Civita symbol is replaced by the concept of the Hodge dual.\n\nIn a context where tensor index notation is used to manipulate tensor components, the Levi-Civita symbol may be written with its indices as either subscripts or superscripts with no change in meaning, as might be convenient. Thus, one could write\nIn these examples, superscripts should be considered equivalent with subscripts.\n\nSummation symbols can be eliminated by using Einstein notation, where an index repeated between two or more terms indicates summation over that index. For example,\nIn the following examples, Einstein notation is used.\n\nIn two dimensions, when all each take the values 1 and 2,\n\nIn three dimensions, when all each take values 1, 2, and 3:\n\nThe Levi-Civita symbol is related to the Kronecker delta. In three dimensions, the relationship is given by the following equations (vertical lines denote the determinant):\n\nA special case of this result is ():\n\nsometimes called the \"contracted epsilon identity\".\n\nIn Einstein notation, the duplication of the index implies the sum on . The previous is then denoted .\n\nIn dimensions, when all take values :\n\n^{j_{k+1}} \\dots \\delta_{i_n ]}^{j_n} = k!~\\delta^{j_{k+1} \\dots j_n}_{i_{k+1} \\dots i_n} </math>\nwhere the exclamation mark () denotes the factorial, and is the generalized Kronecker delta. For any , the property\n\nfollows from the facts that \n\nIn general, for dimensions, one can write the product of two Levi-Civita symbols as:\n\nFor (), both sides are antisymmetric with respect of and . We therefore only need to consider the case and . By substitution, we see that the equation holds for , that is, for and . (Both sides are then one). Since the equation is antisymmetric in and , any set of values for these can be reduced to the above case (which holds). The equation thus holds for all values of and .\n\nUsing (), we have for ()\n\nHere we used the Einstein summation convention with going from 1 to 2. Next, () follows similarly from ().\n\nTo establish (), notice that both sides vanish when . Indeed, if , then one can not choose and such that both permutation symbols on the left are nonzero. Then, with fixed, there are only two ways to choose and from the remaining two indices. For any such indices, we have\n\n(no summation), and the result follows.\n\nThen () follows since and for any distinct indices taking values , we have \n\nIn linear algebra, the determinant of a square matrix can be written\n\nSimilarly the determinant of an matrix can be written as\n\nwhere each should be summed over , or equivalently:\n\nwhere now each and each should be summed over . More generally, we have the identity\n\nIf and are vectors in (represented in some right-handed coordinate system using an orthonormal basis), their cross product can be written as a determinant:\n\nhence also using the Levi-Civita symbol, and more simply:\n\nIn Einstein notation, the summation symbols may be omitted, and the th component of their cross product equals\n\nThe first component is\n\nthen by cyclic permutations of the others can be derived immediately, without explicitly calculating them from the above formulae:\n\nFrom the above expression for the cross product, we have:\n\nIf is a third vector, then the triple scalar product equals\n\nFrom this expression, it can be seen that the triple scalar product is antisymmetric when exchanging any pair of arguments. For example,\n\nIf is a vector field defined on some open set of as a function of position (using Cartesian coordinates). Then the th component of the curl of equals\n\nwhich follows from the cross product expression above, substituting components of the gradient vector operator (nabla).\n\nIn any arbitrary curvilinear coordinate system and even in the absence of a metric on the manifold, the Levi-Civita symbol as defined above may be considered to be a tensor density field in two different ways. It may be regarded as a contravariant tensor density of weight +1 or as a covariant tensor density of weight −1. In \"n\" dimensions using the generalized Kronecker delta,\n\nNotice that these are numerically identical. In particular, the sign is the same.\n\nOn a pseudo-Riemannian manifold, one may define a coordinate-invariant covariant tensor field whose coordinate representation agrees with the Levi-Civita symbol wherever the coordinate system is such that the basis of the tangent space is orthonormal with respect to the metric and matches a selected orientation. This tensor should not be confused with the tensor density field mentioned above. The presentation in this section closely follows .\n\nThe covariant Levi-Civita tensor (also known as the Riemannian volume form) in any coordinate system that matches the selected orientation is\n\nwhere is the representation of the metric in that coordinate system. We can similarly consider a contravariant Levi-Civita tensor by raising the indices with the metric as usual,\n\nbut notice that if the metric signature contains an odd number of negatives , then the sign of the components of this tensor differ from the standard Levi-Civita symbol:\n\nwhere , and formula_39 is the usual Levi-Civita symbol discussed in the rest of this article. More explicitly, when the tensor and basis orientation are chosen such that formula_40, we have that formula_41.\n\nFrom this we can infer the identity,\nwhere\nis the generalized Kronecker delta.\n\nIn Minkowski space (the four-dimensional spacetime of special relativity), the covariant Levi-Civita tensor is\n\nwhere the sign depends on the orientation of the basis. The contravariant Levi-Civita tensor is\n\nThe following are examples of the general identity above specialized to Minkowski space (with the negative sign arising from the odd number of negatives in the signature of the metric tensor in either sign convention):\n\n\n"}
{"id": "30315403", "url": "https://en.wikipedia.org/wiki?curid=30315403", "title": "List of things named after Karl Weierstrass", "text": "List of things named after Karl Weierstrass\n\nThis is a list of things named after the German mathematician Karl Weierstrass.\n\n\n\n\n\n"}
{"id": "31742143", "url": "https://en.wikipedia.org/wiki?curid=31742143", "title": "Lorentz surface", "text": "Lorentz surface\n\nIn mathematics, a Lorentz surface is a two-dimensional oriented smooth manifold with a conformal equivalence class of Lorentzian metrics. It is the analogue of a Riemann surface in indefinite signature.\n\n"}
{"id": "1536976", "url": "https://en.wikipedia.org/wiki?curid=1536976", "title": "MINQUE", "text": "MINQUE\n\nIn statistics, the theory of minimum norm quadratic unbiased estimation (MINQUE) was developed by C.R. Rao. Its application was originally to the problem of heteroscedasticity and the estimation of variance components in random effects models. \n\nThe theory involves three stages:\n"}
{"id": "1819983", "url": "https://en.wikipedia.org/wiki?curid=1819983", "title": "Natural density", "text": "Natural density\n\nIn number theory, natural density (also referred to as asymptotic density or arithmetic density) is one of the possibilities to measure how large a subset of the set of natural numbers is.\n\nIntuitively, it is thought that there are more positive integers than perfect squares, since every perfect square is already positive, and many other positive integers exist besides. However, the set of positive integers is not in fact larger than the set of perfect squares: both sets are infinite and countable and can therefore be put in one-to-one correspondence. Nevertheless if one goes through the natural numbers, the squares become increasingly scarce. The notion of natural density makes this intuition precise.\n\nIf an integer is randomly selected from the interval [1, \"n\"], then the probability that it belongs to \"A\" is the ratio of the number of elements of \"A\" in [1, \"n\"] to the total number of elements in [1, \"n\"]. If this probability tends to some limit as \"n\" tends to infinity, then this limit is referred to as the asymptotic density of \"A\". This notion can be understood as a kind of probability of choosing a number from the set \"A\". Indeed, the asymptotic density (as well as some other types of densities) is studied in probabilistic number theory.\n\nAsymptotic density contrasts, for example, with the Schnirelmann density. One drawback of asymptotic density is that it is not defined for all subsets of formula_1.\n\nA subset \"A\" of positive integers has natural density \"α\" if the proportion of elements of \"A\" among all natural numbers from 1 to \"n\" converges to \"α\" as \"n\" tends to infinity.\n\nMore explicitly, if one defines for any natural number \"n\" the counting function \"a\"(\"n\") as the number of elements of \"A\" less than or equal to \"n\", then the natural density of A being α exactly means that\n\nIt follows from the definition that if a set \"A\" has natural density \"α\" then 0 ≤ \"α\" ≤ 1.\n\nLet formula_2 be a subset of the set of natural numbers formula_3 For any formula_4 put formula_5 and formula_6.\n\nDefine the \"upper asymptotic density\" formula_7 of formula_2 by\n\nwhere lim sup is the limit superior. formula_7 is also known simply as the upper density of formula_11\n\nSimilarly, formula_12, the \"lower asymptotic density\" of formula_2, is defined by\n\nOne may say formula_2 has asymptotic density formula_16 if formula_17, in which case formula_16 is equal to this common value.\n\nThis definition can be restated in the following way:\n\nif the limit exists.\n\nIt can be proven that the definitions imply that the following also holds. If one were to write a subset of formula_1 as an increasing sequence\nthen\nand\nformula_24\nif the limit exists.\n\nA somewhat weaker notion of density is the \"upper Banach density\"; given a set formula_25, define formula_26 as\n\n\n\nOther density functions on subsets of the natural numbers may be defined analogously. For example, the \"logarithmic density\" of a set \"A\" is defined as the limit (if it exists)\n\nUpper and lower logarithmic densities are defined analogously as well.\n\n\n"}
{"id": "1063946", "url": "https://en.wikipedia.org/wiki?curid=1063946", "title": "Occurs check", "text": "Occurs check\n\nIn computer science, the occurs check is a part of algorithms for syntactic unification. It causes unification of a variable \"V\" and a structure \"S\" to fail if \"S\" contains \"V\".\n\nIn theorem proving, unification without the occurs check can lead to unsound inference. For example, the Prolog goal\nformula_1\nwill succeed, binding \"X\" to a cyclic structure which has no counterpart in the Herbrand universe.\nAs another example, \n\nwithout occurs-check, a resolution proof can be found for the non-theorem \n\nformula_2: the negation of that formula has the conjunctive normal form formula_3, with formula_4 and formula_5 denoting the Skolem function for the first and second existential quantifier, respectively; the literals formula_6 and formula_7 are unifiable without occurs check, producing the refuting empty clause.\n\nBy default, Prolog implementations usually omit the occurs check for reasons of efficiency, which can lead to circular data structures and looping.\nBy not performing the occurs check, the worst case complexity of unifying a term formula_8 with term formula_9 is reduced in many cases from\nformula_10\nto\nformula_11;\nin the particular, frequent case of variable-term unifications, runtime shrinks to formula_12.\n\nA naive omission of the occurs check leads to the creation of cyclic structures\nand may cause unification to loop forever.\nModern implementations, based on Colmerauer's Prolog II,\n\nuse rational tree unification to avoid looping.\nSee image for an example run of the unification algorithm given in Unification (computer science)#A unification algorithm, trying to solve the goal formula_13, however without the \"occurs check rule\" (named \"check\" there); applying rule \"eliminate\" instead leads to a cyclic graph (i.e. an infinite term) in the last step.\n\nISO Prolog implementations have the built-in predicate unify_with_occurs_check/2\nfor sound unification but are free to use unsound or even looping algorithms\nwhen unification is invoked otherwise, provided the algorithm works correctly for all cases that are \"not subject to occurs-check\" (NSTO).\nImplementations offering sound unification\nfor all unifications are\nQu-Prolog and Strawberry Prolog and (optionally, via a runtime flag): XSB, SWI-Prolog and Tau Prolog.\n"}
{"id": "7812706", "url": "https://en.wikipedia.org/wiki?curid=7812706", "title": "Paradoxical set", "text": "Paradoxical set\n\nIn set theory, a paradoxical set is a set that has a paradoxical decomposition. A paradoxical decomposition of a set is two families of disjoint subsets, along with appropriate group actions that act on some universe (of which the set in question is a subset), such that each partition can be mapped back onto the entire set using only finitely many distinct functions (or compositions thereof) to accomplish the mapping. A set that admits such a paradoxical decomposition where the actions belong to a group formula_1 is called formula_1-paradoxical or paradoxical with respect to formula_1.\n\nParadoxical sets exist as a consequence of the Axiom of Infinity. Admitting infinite classes as sets is sufficient to allow paradoxical sets.\n\nGiven a group formula_1 that acts on a set formula_5. Then formula_5 is formula_1-paradoxical if there exists some disjoint subsets formula_8 and some group actions formula_9 such that:\n\nformula_10 and formula_11\n\nThe Free group \"F\" on two generators \"a,b\" has the decomposition formula_12 where \"e\" is the identity word and formula_13 is the collection of all (reduced) words that start with the letter \"i\". This is a paradoxical decomposition because formula_14\n\nThe most famous, and indeed motivational, example of paradoxical sets is the Banach–Tarski paradox, which divides the sphere into paradoxical sets for the special orthogonal group. This result depends on the axiom of choice.\n"}
{"id": "18326967", "url": "https://en.wikipedia.org/wiki?curid=18326967", "title": "Paul Cohn", "text": "Paul Cohn\n\nPaul Moritz Cohn FRS (8 January 1924 – 20 April 2006) was Astor Professor of Mathematics at University College London, 1986-9, and author of many textbooks on algebra. His work was mostly in the area of algebra, especially non-commutative rings.\n\nHe was the only child of Jewish parents, James (or Jakob) Cohn, owner of an import business, and Julia (\"née\" Cohen), a schoolteacher.\n\nBoth of his parents were born in Hamburg, as were three of his grandparents. His ancestors came from various parts of Germany. His father fought in the German army in World War I; he was wounded several times and awarded the Iron Cross. A street in Hamburg is named in memory of his mother.\n\nWhen he was born, his parents were living with his mother's mother in Isestraße. After her death in October 1925, the family moved to a rented flat in a new building in Lattenkamp, in the Winterhude quarter. He attended a kindergarten then, in April 1930, moved to Alsterdorfer Straße School. After a while, he had a new teacher, a National Socialist, who picked on him and punished him without cause. Thus in 1931, he moved to the Meerweinstraße School where his mother taught.\n\nFollowing the rise of the Nazis in 1933, his father's business was confiscated and his mother dismissed. He moved to the Talmud-Tora-Schule, a Jewish school. In mid-1937, the family moved to Klosterallee. This was nearer the school, the synagogue and other pupils, being in the Jewish area. His German teacher was Dr. Ernst Loewenberg, the son of the poet Jakob Loewenberg.\n\nOn the night of 9/10 November 1938 (Kristallnacht), his father was arrested and sent to Sachsenhausen concentration camp. He was released after four months but told to emigrate. Cohn went to Britain in May 1939 on the Kindertransport to work on a chicken farm, and never saw his parents again. He corresponded regularly with them until late 1941. At the end of the War, he learned that they were deported to Riga on 6 December 1941 and never returned. At the end of 1941, the farm closed. He trained as a precision engineer, acquired a work permit and worked in a factory for 4½ years. He passed the Cambridge Scholarship Examination, and won an exhibition to Trinity College, Cambridge.\n\nHe received a B.A in Mathematics from Cambridge University in 1948 and a Ph.D. (supervised by Philip Hall) in 1951. He then spent a year as a Chargé de Recherches at the University of Nancy. On his return, he became a lecturer in mathematics at Manchester University. He was a visiting professor at Yale University in 1961–62, and for part of 1962 was at the University of California at Berkeley. On his return, he became Reader at Queen Mary College. He was a visiting professor at the University of Chicago in 1964 and at the State University of New York at Stony Brook in 1967. By then, he was regarded as one of the world's leading algebraists.\n\nAlso in 1967, he became head of the Department of Mathematics at Bedford College. He held several visiting professorships, in America, Paris, Delhi, Canada, Haifa and Bielefeld. He was awarded the Lester R Ford Award from the Mathematical Association of America in 1972 and the Senior Berwick Prize of the London Mathematical Society in 1974.\n\nIn the early 1980s, funding cuts caused the closure of the small colleges of the University of London. Cohn moved to University College in 1984, together with the two other experts at Bedford on ring theory, Bill Stephenson and Warren Dicks. He became Astor Professor of Mathematics there in 1986. He continued to be a visiting professor, for example to the University of Alberta in 1986 and to Bar Ilan University in 1987. He retired in 1989, but remained active as Professor Emeritus and Honorary Research Fellow until his death.\n\nHe was President of the London Mathematical Society, 1982-4, having been its secretary, 1965–67 and a Council member in 1968–71, 1972–75 and 1979–82. He was editor of the Society's Monographs in 1968–77 and 1980–93. He was elected a Fellow of the Royal Society in 1980 and was on its council, 1985–87. He was a member of the Mathematical Committee of the Science Research Council, 1977–1980. He chaired the National Committee for Mathematics, 1988-9.\n\nIn all, Cohn wrote nearly 200 mathematical papers. He worked in many areas of algebra, mainly in non-commutative ring theory. His first papers, covering many topics, were published in 1952. He generalised a theorem due to Wilhelm Magnus, and worked on the structure of tensor spaces. In 1953 he published a joint paper with Kurt Mahler on pseudo-valuations and in 1954 he published a work on Lie algebras.\n\nPapers over the next few years covered areas such as group theory, field theory, Lie rings, semigroups, Abelian groups and ring theory. He published his first book, \"Lie groups\", in 1957. After that, he moved into the areas of Jordan algebras, Lie division rings, skew fields, free ideal rings and non-commutative unique factorisation domains. He published his second book, \"Linear equations\", in 1958 and his third, \"Solid geometry\", in 1961. \"Universal algebra\" appeared in 1965 (second edition 1981). After that, he concentrated on non-commutative ring theory and the theory of algebras.\n\nHis monograph \"Free rings and their relations\" appeared in 1971. It covered the work of Cohn and others on free associative algebras and related classes of rings, especially free ideal rings. He included all of his own published results on the embedding of rings into skew fields. The second, enlarged edition appeared in 1985.\n\nCohn also wrote undergraduate textbooks. \"Algebra\" volume I appeared in 1974 and volume II in 1977. The second edition, in three volumes, was published by Wiley between 1982 and 1991. These volumes were in line with the British (rather than American) curricula at the time and include both linear algebra and abstract algebra. Cohn wrote a subsequent revised iteration the first volume as \"Classical Algebra\" (Wiley, 2000) as a more \"user friendly\" version for undergraduates (according to its preface); this book also includes a few selected topics from volumes II and III of \"Algebra\". The final incarnation of Cohn's algebra textbooks appeared in 2003 as two Springer volumes \"Basic Algebra\" and \"Further Algebra and Applications\"; the material in \"Basic Algebra\" is (according to its preface) rather more concise and while corresponding roughly with volume I of \"Algebra\" assumes knowledge of linear algebra; the material on basic theories (groups, rings, fields) is pursued in more depth in \"Basic Algebra\" compared to volume I of \"Algebra\". \"Further Algebra and Applications\" roughly corresponds to volumes II and III of \"Algebra\", but reflects the shift of some material from these volumes to \"Basic Algebra\".\n\nHis recreation was etymology and language in all its forms. He married Deirdre Sharon in 1958, and they had two daughters.\n\n\n"}
{"id": "3619133", "url": "https://en.wikipedia.org/wiki?curid=3619133", "title": "Paul Matthieu Hermann Laurent", "text": "Paul Matthieu Hermann Laurent\n\nPaul Matthieu Hermann Laurent (2 September 1841 Luxembourg City – 19 February 1908 Paris, France) was a French mathematician. Despite his large body of works, Laurent series expansions for complex functions were \"not\" named after him, but after Pierre Alphonse Laurent.\n\n\n"}
{"id": "11790568", "url": "https://en.wikipedia.org/wiki?curid=11790568", "title": "Percolation threshold", "text": "Percolation threshold\n\nPercolation threshold is a mathematical concept related to percolation theory, which is the formation of long-range connectivity in random systems. Below the threshold a giant connected component does not exist; while above it, there exists a giant component of the order of system size. In engineering and coffee making, percolation represents the flow of fluids through porous media, but in the mathematics and physics worlds it generally refers to simplified lattice models of random systems or networks (graphs), and the nature of the connectivity in them. The percolation threshold is the critical value of the occupation probability \"p\", or more generally a critical surface for a group of parameters \"p\", \"p\", ..., such that infinite connectivity (\"percolation\") first occurs.\n\nThe most common percolation model is to take a regular lattice, like a square lattice, and make it into a random network by randomly \"occupying\" sites (vertices) or bonds (edges) with a statistically independent probability \"p\". At a critical threshold \"p\", large clusters and long-range connectivity first appears, and this is called the percolation threshold. Depending on the method for obtaining the random network, one distinguishes between the site percolation threshold and the bond percolation threshold. More general systems have several probabilities \"p\", \"p\", etc., and the transition is characterized by a \"critical surface\" or \"manifold\". One can also consider continuum systems, such as overlapping disks and spheres placed randomly, or the negative space (\"Swiss-cheese\" models).\n\nIn the systems described so far, it has been assumed that the occupation of a site or bond is completely random—this is the so-called \"Bernoulli percolation.\" For a continuum system, random occupancy corresponds to the points being placed by a Poisson process. Further variations involve correlated percolation, such as percolation clusters related to Ising and Potts models of ferromagnets, in which the bonds are put down by the Fortuin-Kasteleyn method. In \"bootstrap\" or \"k-sat\" percolation, sites and/or bonds are first occupied and then successively culled from a system if a site does not have at least \"k\" neighbors. Another important model of percolation, in a different universality class altogether, is directed percolation, where connectivity along a bond depends upon the direction of the flow.\n\nOver the last several decades, a tremendous amount of work has gone into finding exact and approximate values of the percolation thresholds for a variety of these systems. Exact thresholds are only known for certain two-dimensional lattices that can be broken up into a self-dual array, such that under a triangle-triangle transformation, the system remains the same. Studies using numerical methods have led to numerous improvements in algorithms and several theoretical discoveries.\n\nSimply duality in two dimensions implies that all fully triangulated lattices (e.g., the triangular, union jack, cross dual, martini dual and asanoha or 3-12 dual, and the Delaunay triangulation) all have site thresholds of 1/2, and self-dual lattices (square, martini-B) have bond thresholds of 1/2.\n\nThe notation such as (4,8) comes from Grünbaum and Shephard, and indicates that around a given vertex, going in the clockwise direction, one encounters first a square and then two octagons. Besides the eleven Archimedean lattices composed of regular polygons with every site equivalent, many other more complicated lattices with sites of different classes have been studied.\n\nError bars in the last digit or digits are shown by numbers in parentheses. Thus, 0.729724(3) signifies 0.729724 ± 0.000003, and 0.74042195(80) signifies 0.74042195 ± 0.00000080. The error bars variously represent one or two standard deviations in net error (including statistical and expected systematic error), or an empirical confidence interval.\n\nThis is a picture of the 11 Archimedean Lattices or uniform tilings, in which all polygons are regular and each vertex is surrounded by the same sequence of polygons. The notation (3</VAR >, 6) for example means that every vertex is surrounded by four triangles and one hexagon. Drawings from . See also Uniform tilings.\n\nNote: sometimes \"hexagonal\" is used in place of honeycomb, although in some fields, a triangular lattice is also called a hexagonal lattice. \"z\" = bulk coordination number.\n\n2N = nearest neighbors, 3N = next-nearest neighbors, 4N = next-next-nearest neighbors, etc. These are called NN, 2NN, 3NN respectively in the 3D versions below.\n\nFor more neighbors, see\n\nSite bond percolation (both thresholds apply simultaneously to one system).\n<nowiki>*</nowiki> For more values, see An Investigation of site-bond percolation\n\nApproximate formula for a honeycomb lattice\n\nLaves lattices are the duals to the Archimedean lattices. Drawings from. See also Uniform tilings.\n\nTop 3 lattices: #13 #12 #36\n\nBottom 3 lattices: #34 #37 #11\n\nTop 2 lattices: #35 #30\n\nBottom 2 lattices: #41 #42\n\nTop 4 lattices: #22 #23 #21 #20\n\nBottom 3 lattices: #16 #17 #15\n\nTop 2 lattices: #31 #32\n\nBottom lattice: #33\n\nThis figure shows something similar to the 2-uniform lattice #37, except the polygons are not all regular—there is a rectangle in the place of the two squares—and the size of the polygons is changed. This lattice is in the isoradial representation in which each polygon is inscribed in a circle of unit radius.\nThe two squares in the 2-uniform lattice must now be represented as a single rectangle in order to satisfy the isoradial condition.\nThe lattice is shown by black edges, and the dual lattice by red dashed lines. The green circles show the isoradial constraint on both the original and dual lattices. The yellow polygons highlight the three types of polygons on the lattice, and the pink polygons highlight the two types of polygons on the dual lattice. The lattice has vertex types (1/2)(3,4) + (1/2)(3,4,6,4), while\nthe dual lattice has vertex types (1/15)(4)+(6/15)(4,5)+(2/15)(5)+(6/15)(5,4). The critical point is where the longer\nbonds (on both the lattice and dual lattice) have occupation probability p = 2 sin (π/18) = 0.347296... which is the bond percolation threshold on a triangular lattice, and the shorter bonds have \noccupation probability 1 − 2 sin(π/18) = 0.652703..., which is the bond percolation on a hexagonal lattice. These results follow from the isoradial condition but also follow from applying the star-triangle transformation to certain stars on the honeycomb lattice. Finally, it can be generalized to having three different probabilities in the three different directions, p, p and p for the long bonds, and 1 - p, 1 - p, and 1 - p for the short bonds, where p, p and p satisfy the critical surface for the inhomogeneous triangular lattice.\n\nTo the left, center, and right are: the martini lattice, the martini-A lattice, the martini-B lattice. Below: the martini covering/medial lattice, same as the 2x2, 1x1 subnet for kagome-type lattices (removed).\n\nSome other examples of generalized bow-tie lattices (a-d) and the duals of the lattices (e-h):\n\n(4, 6, 12) covering/medial lattice\n(4, 8) covering/medial lattice\n(3,12) covering/medial lattice (in light grey), equivalent to the kagome (2 x 2) subnet, and in black, the dual of these lattices.\n\n(left) (3,4,6,4) covering/medial lattice, (right) (3,4,6,4) medial dual, shown in red, with medial lattice in light gray behind it. The pattern on the left appears in 7 Iranian tilework.\n\nThe 2 x 2, 3 x 3, and 4 x 4 subnet kagome lattices. The 2 × 2 subnet is also known as the \"triangular kagome\" lattice.\n\nThe threshold gives the fraction of sites occupied by the objects when site percolation first takes place (not at full jamming). For longer dimers see Ref.\n\nHere, we are dealing with networks that are obtained by covering a lattice with dimers, and then consider bond percolation on the remaining bonds. In discrete mathematics, this problem is known as the 'perfect matching' or the 'dimer covering' problem.\n\nSystem is composed of ordinary (non-avoiding) random walks of length l on the square lattice.\n\nformula_1 equals critical total area for disks, where N is the number of objects and L is the system size.\n\nformula_2 gives the number of disk centers within the circle of influence (radius 2 r).\n\nformula_3 is the critical disk radius.\n\nformula_4 for ellipses of semi-major and semi-minor axes of a and b, respectively. Aspect ratio formula_5 with formula_6.\n\nformula_7 for rectangles of dimensions formula_8 and formula_9. Aspect ratio formula_10 with formula_11.\n\nformula_12 for power-law distributed disks with formula_13, formula_14.\n\nformula_15 equals critical area fraction.\n\nformula_16 equals number of objects of maximum length formula_17 per unit area.\n\nFor ellipses, formula_18\n\nFor void percolation, formula_19 is the critical void fraction.\n\nFor more ellipse values, see\n\nFor more rectangle values, see\n\nFor binary dispersions of disks, see\n\n<nowiki>*</nowiki>Theoretical estimate\n\nAssuming power-law correlations formula_20\n\nMore for SC open b.c. in Ref.\n\nh is the thickness of the slab, h x ∞ x ∞.\n\nFilling factor = fraction of space filled by touching spheres at every lattice site (for systems with uniform bond length only). Also called Atomic Packing Factor.\n\nFilling fraction (or Critical Filling Fraction) = filling factor * p(site).\n\nNN = nearest neighbor, 2NN = next-nearest neighbor, 3NN = next-next-nearest neighbor, etc.\n\nQuestion: the bond thresholds for the hcp and fcc lattice\nagree within the small statistical error. Are they identical,\nand if not, how far apart are they? Which threshold is expected to be bigger? Similarly for the ice and diamond lattices. See\n\nAll overlapping except for jammed spheres and polymer matrix.\nformula_21 is the total volume (for spheres), where N is the number of objects and L is the system size.\n\nformula_15 is the critical volume fraction.\n\nFor disks and plates, these are effective volumes and volume fractions.\n\nFor void (\"Swiss-Cheese\" model), formula_19 is the critical void fraction.\n\nFor more results on void percolation around ellipsoids and elliptical plates, see.\n\nFor more ellipsoid percolation values see.\n\nFor spherocylinders, H/D is the ratio of the height to the diameter of the cylinder, which is then capped by hemispheres. Additional values are given in.\n\n\nformula_24.\n\nIn 4d, formula_25.\n\nIn 5d, formula_26.\n\nIn 6d, formula_27.\n\nformula_15 is the critical volume fraction.\n\nFor void models, formula_19 is the critical void fraction, and formula_30 is the total volume of the overlapping objects\n\nFor thresholds on high dimensional hypercubic lattices, we have the asymptotic series expansions \n\nformula_31\n\nformula_32\n\nwhere formula_33.\n\nIn a one-dimensional chain we establish bonds between distinct sites formula_34 and formula_35 with probability formula_36\n\nInhomogeneous honeycomb lattice bond percolation = kagome lattice site percolation\n\nformula_37\n\nInhomogeneous (3,12^2) lattice, site percolation\nformula_38\nor formula_39\n\nInhomogeneous union-jack lattice, site percolation with probabilities formula_40\n\nformula_41\n\nInhomogeneous martini lattice, bond percolation\n\nformula_42\nformula_43\n\nInhomogeneous martini lattice, site percolation. \"r\" = site in the star\n\nformula_44\n\nInhomogeneous martini-A (3–7) lattice, bond percolation. Left side (top of \"A\" to bottom): formula_45. Right side: formula_46. Cross bond: formula_47.\n\nformula_48\n\nInhomogeneous martini-B (3–5) lattice, bond percolation\n\nInhomogeneous martini lattice with outside enclosing triangle of bonds, probabilities formula_49 from inside to outside, bond percolation\n\nformula_50\n\nInhomogeneous checkerboard lattice, bond percolation\nformula_51\n\nInhomogeneous bow-tie lattice, bond percolation\n\nformula_52\nformula_53\n\nwhere formula_54 are the four bonds around the square and formula_55 is the diagonal bond connecting the vertex between bonds formula_56 and formula_57.\n\nFor random graphs not embedded in space the percolation threshold can be calculated exactly. For example, for random regular graphs where all nodes have the same degree k, p=1/k. For Erdős–Rényi (ER) graphs with Poissonian degree distribution, p=1/<k>. The critical threshold was calculated exactly also for a network of interdependent ER networks.\n\n"}
{"id": "7650282", "url": "https://en.wikipedia.org/wiki?curid=7650282", "title": "Picture language", "text": "Picture language\n\nIn formal language theory, a picture language is a set of \"pictures\", where a picture is a 2D array of characters over some alphabet.\n\nFor example, the language formula_1 defines the language of rectangles composed of the character formula_2. This language formula_3 contains pictures such as:\n\nformula_4\n\nThe study of picture languages was initially motivated by the problems of pattern recognition and image processing, but two-dimensional patterns also appear in the study of cellular automata and other parallel computing models. Some formal systems have been created to define picture languages, such as array grammars and tiling systems.\n\n"}
{"id": "16275025", "url": "https://en.wikipedia.org/wiki?curid=16275025", "title": "Polynomial arithmetic", "text": "Polynomial arithmetic\n\nPolynomial arithmetic is a branch of algebra dealing with some properties of polynomials which share strong analogies with properties of number theory relative to integers.\nIt includes basic mathematical operations such as addition, subtraction, and multiplication, as well as more elaborate operations like Euclidean division, and properties related to roots of polynomials. The latter are essentially connected to the fact that the set \"K\"[\"X\"] of univariate polynomials with coefficients in a field \"K\" is a commutative ring, such as the ring of integers formula_1.\n\nAddition and subtraction of two polynomials are performed by adding or subtracting corresponding coefficients. If\n\nthen addition is defined as\n\nMultiplication is performed much the same way as addition and subtraction, but instead by multiplying the corresponding coefficients. If formula_2 then multiplication is defined as formula_5 where formula_6. Note that we treat formula_7 as zero for formula_8 and that the degree of the product is equal to the sum of the degrees of the two polynomials.\n\nMany fascinating properties of polynomials can be found when, thanks to the basic operations that can be performed on two polynomials and the underlying commutative ring structure of the set they live in, one tries to apply reasonings similar to those known from number theory.\n\nTo see this, one first needs to introduce two concepts: the notion of root of a polynomial and that of divisibility for pairs of polynomials. \n\nIf one considers a polynomial formula_9 of a single variable \"X\" in a field \"K\" (typically formula_10 or formula_11), and with coefficients in that field, a root formula_12 of formula_9 is an element of \"K\" such that\n\nThe second concept, divisibility of polynomials, allows to see a first analogy with number theory: a polynomial formula_15 is said to divide another polynomial formula_16 when the latter can be written as\n\nwith C being ALSO a polynomial. This definition is similar to divisibility for integers, and the fact that formula_15 divides formula_16 is also denoted formula_20.\n\nThe relation between both concepts above arises when noticing the following property: formula_12 is a root of formula_9 if and only if formula_23. Whereas one logical inclusion (\"if\") is obvious, the other (\"only if\") relies on a more elaborate concept, the Euclidean division of polynomials, here again strongly reminding of the Euclidean division of integers.\n\nFrom this it follows that one can define prime polynomials, as polynomials that cannot be divided by any other polynomials but 1 and themselves (up to an overall constant factor) - here again the analogously with prime integers is manifest, and allows that some of the main definitions and theorems related to prime numbers and number theory have their counterpart in polynomial algebra. The most important result is the fundamental theorem of algebra, allowing for factorization of any polynomial as a product of prime ones. Worth mentioning is also the Bézout's identity in the context of polynomials. It states that two given polynomials P and Q have as greatest common divisor (GCD) a third polynomial D (D is then unique as GCD of P and Q up to a finite constant factor), if and only if there exists polynomials U and V such that\n\n\n"}
{"id": "27882423", "url": "https://en.wikipedia.org/wiki?curid=27882423", "title": "Proprism", "text": "Proprism\n\nIn geometry of 4 dimensions or higher, a proprism is a polytope resulting from the Cartesian product of two or more polytopes, each of two dimensions or higher. The term was coined by John Horton Conway for \"product prism\". The dimension of the space of a proprism equals the sum of the dimensions of all its product elements. Proprisms are often seen as \"k\"-face elements of uniform polytopes.\n\nThe number of vertices in a proprism is equal to the product of the number of vertices in all the polytopes in the product.\n\nThe minimum symmetry order of a proprism is the product of the symmetry orders of all the polytopes. A higher symmetry order is possible if polytopes in the product are identical.\n\nA proprism is convex if all its product polytopes are convex.\n\nIn geometry of 4 dimensions or higher, duoprism is a polytope resulting from the Cartesian product of two polytopes, each of two dimensions or higher. The Cartesian product of an \"a\"-polytope, a \"b\"-polytope is an \"(a+b)\"-polytope, where \"a\" and \"b\" are 2-polytopes (polygon) or higher.\n\nMost commonly this refers to the product of two polygons in 4-dimensions. In the context of a product of polygons, Henry P. Manning's 1910 work explaining the fourth dimension called these double prisms.\n\nThe Cartesian product of two polygons is the set of points:\nwhere \"P\" and \"P\" are the sets of the points contained in the respective polygons.\n\nThe smallest is a 3-3 duoprism, made as the product of 2 triangles. If the triangles are regular it can be written as a product of Schläfli symbols, {3} × {3}, and is composed of 9 vertices.\n\nThe tesseract, can be constructed as the duoprism {4} × {4}, the product of two equal-size orthogonal squares, composed of 16 vertices. The 6-cube can be constructed as a duoprism {4} × {4,3}, the product of a square and cube, while the 8-cube can be constructed as the product of two cubes, {4,3} × {4,3}.\n\nIn geometry of 6 dimensions or higher, a triple product is a polytope resulting from the Cartesian product of three polytopes, each of two dimensions or higher. The Cartesian product of an \"a\"-polytope, a \"b\"-polytope, and a \"c\"-polytope is an (\"a\" + \"b\" + \"c\")-polytope, where \"a\", \"b\" and \"c\" are 2-polytopes (polygon) or higher.\n\nThe lowest-dimensional forms are 6-polytopes being the Cartesian product of three polygons. The smallest can be written as {3} × {3} × {3} in Schläfli symbols if they are regular, and contains 27 vertices. This is the product of three equilateral triangles and is a uniform polytope.\n\nThe 6-cube, can be constructed as a triple product {4} × {4} × {4}.\n"}
{"id": "7203361", "url": "https://en.wikipedia.org/wiki?curid=7203361", "title": "Reduction criterion", "text": "Reduction criterion\n\nIn quantum information theory, the reduction criterion is a necessary condition a mixed state must satisfy in order for it to be separable. In other words, the reduction criterion is a \"separability criterion\". It was first proved \nand independently formulated in 1999. Violation of the reduction criterion is closely related to the distillability of the state in question.\n\nLet \"H\" and \"H\" be Hilbert spaces of finite dimensions \"n\" and \"m\" respectively. \"L\"(\"H\") will denote the space of linear operators acting on \"H\". Consider a bipartite quantum system whose state space is the tensor product\n\nAn (un-normalized) mixed state \"ρ\" is a positive linear operator (density matrix) acting on \"H\".\n\nA linear map Φ: \"L\"(\"H\") → \"L\"(\"H\") is said to be positive if it preserves the cone of positive elements, i.e. \"A\" is positive implied \"Φ\"(\"A\") is also.\n\nFrom the one-to-one correspondence between positive maps and entanglement witnesses, we have that a state \"ρ\" is entangled if and only if there exists a positive map \"Φ\" such that\n\nis not positive. Therefore, if \"ρ\" is separable, then for all positive map Φ,\n\nThus every positive, but not completely positive, map Φ gives rise to a necessary condition for separability in this way. The reduction criterion is a particular example of this.\n\nSuppose \"H\" = \"H\". Define the positive map Φ: \"L\"(\"H\") → \"L\"(\"H\") by\n\nIt is known that Φ is positive but not completely positive. So a mixed state \"ρ\" being separable implies\n\nDirect calculation shows that the above expression is the same as\n\nwhere \"ρ\" is the partial trace of \"ρ\" with respect to the second system. The dual relation\n\nis obtained in the analogous fashion. The reduction criterion consists of the above two inequalities.\n\nThe above last two inequalities together with lower bounds for \"ρ\" can be seen as quantum Fréchet inequalities, that is as the quantum analogous of the classical Fréchet probabilistic bounds, that hold for separable quantum states. The upper bounds are the previous ones formula_8, formula_9, and the lower bounds are the obvious constraint formula_10 together with formula_11, where formula_12 are identity matrices of suitable dimensions. The lower bounds have been obtained in. These bounds are satisfied by separable density matrices, while entangled states can violate them. Entangled states exhibit a form of \"stochastic dependence stronger than the strongest classical dependence\" and in fact they violate Fréchet like bounds.\nIt is also worth mentioning that is possible to give a Bayesian interpretation of these bounds.\n"}
{"id": "13415343", "url": "https://en.wikipedia.org/wiki?curid=13415343", "title": "Regular matroid", "text": "Regular matroid\n\nIn mathematics, a regular matroid is a matroid that can be represented over all fields.\n\nA matroid is defined to be a family of subsets of a finite set, satisfying certain axioms. The sets in the family are called \"independent sets\". One of the ways of constructing a matroid is to select a finite set of vectors in a vector space, and to define a subset of the vectors to be independent in the matroid when it is linearly independent in the vector space. Every family of sets constructed in this way is a matroid, but not every matroid can be constructed in this way, and the vector spaces over different fields lead to different sets of matroids that can be constructed from them.\n\nA matroid formula_1 is regular when, for every field formula_2, formula_1 can be represented by a system of vectors over formula_2.\n\nIf a matroid is regular, so is its dual matroid, and so is every one of its minors. Every direct sum of regular matroids remains regular.\n\nEvery graphic matroid (and every co-graphic matroid) is regular. Conversely, every regular matroid may be constructed by combining graphic matroids, co-graphic matroids, and a certain ten-element matroid that is neither graphic nor co-graphic, using an operation for combining matroids that generalizes the clique-sum operation on graphs.\n\nThe number of bases in a regular matroid may be computed as the determinant of an associated matrix, generalizing Kirchhoff's matrix-tree theorem for graphic matroids.\n\nThe uniform matroid formula_5 (the four-point line) is not regular: it cannot be realized over the two-element finite field GF(2), so it is not a binary matroid, although it can be realized over all other fields. The matroid of the Fano plane (a rank-three matroid in which seven of the triples of points are dependent) and its dual are also not regular: they can be realized over GF(2), and over all fields of characteristic two, but not over any other fields than those. As showed, these three examples are fundamental to the theory of regular matroids: every non-regular matroid has at least one of these three as a minor. Thus, the regular matroids are exactly the matroids that do not have one of the three forbidden minors formula_5, the Fano plane, or its dual.\n\nIf a matroid is regular, it must clearly be realizable over the two fields GF(2) and GF(3). The converse is true: every matroid that is realizable over both of these two fields is regular. The result follows from a forbidden minor characterization of the matroids realizable over these fields, part of a family of results codified by Rota's conjecture.\n\nThe regular matroids are the matroids that can be defined from a totally unimodular matrix, a matrix in which every square submatrix has determinant 0, 1, or −1. The vectors realizing the matroid may be taken as the rows of the matrix. For this reason, regular matroids are sometimes also called unimodular matroids. The equivalence of regular matroids and unimodular matrices, and their characterization by forbidden minors, are deep results of W. T. Tutte, originally proved by him using the Tutte homotopy theorem. later published an alternative and simpler proof of the characterization of unimodular matrices by forbidden minors.\n\nThere is a polynomial time algorithm for testing whether a matroid is regular, given access to the matroid through an independence oracle.\n"}
{"id": "28510", "url": "https://en.wikipedia.org/wiki?curid=28510", "title": "Second-order predicate", "text": "Second-order predicate\n\nIn mathematical logic, a second-order predicate is a predicate that takes a first-order predicate as an argument. Compare higher-order predicate.\n\nThe idea of second order predication was introduced by the German mathematician and philosopher Frege. It is based on his idea that a predicate such as \"is a philosopher\" designates a concept, rather than an object. Sometimes a concept can itself be the subject of a proposition, such as in \"There are no Bosnian philosophers\". In this case, we are not saying anything of any Bosnian philosophers, but of the concept \"is a Bosnian philosopher\" that it is not satisfied. Thus the predicate \"is not satisfied\" attributes something to the concept \"is a Bosnian philosopher\", and is thus a second-level predicate.\n\nThis idea is the basis of Frege's theory of number.\n"}
{"id": "7221237", "url": "https://en.wikipedia.org/wiki?curid=7221237", "title": "Tannakian formalism", "text": "Tannakian formalism\n\nIn mathematics, a Tannakian category is a particular kind of monoidal category \"C\", equipped with some extra structure relative to a given field \"K\". The role of such categories \"C\" is to approximate, in some sense, the category of linear representations of an algebraic group \"G\" defined over \"K\". A number of major applications of the theory have been made, or might be made in pursuit of some of the central conjectures of contemporary algebraic geometry and number theory.\n\nThe name is taken from Tannaka–Krein duality, a theory about compact groups \"G\" and their representation theory. The theory was developed first in the school of Alexander Grothendieck. It was later reconsidered by Pierre Deligne, and some simplifications made. The pattern of the theory is that of Grothendieck's Galois theory, which is a theory about finite permutation representations of groups \"G\" which are profinite groups.\n\nThe gist of the theory, which is rather elaborate in detail in the exposition of Saavedra Rivano, is that the fiber functor Φ of the Galois theory is replaced by a tensor functor \"T\" from \"C\" to K-Vect. The group of natural transformations of Φ to itself, which turns out to be a profinite group in the Galois theory, is replaced by the group (\"a priori\" only a monoid) of natural transformations of \"T\" into itself, that respect the tensor structure. This is by nature not an algebraic group, but an inverse limit of algebraic groups (pro-algebraic group).\n\nA neutral Tannakian category is a rigid abelian tensor category, such that there exists a \"K\"-tensor functor to the category of finite dimensional K-vector spaces that is exact and faithful.\n\nThe construction is used in cases where a Hodge structure or l-adic representation is to be considered in the light of group representation theory. For example, the Mumford–Tate group and motivic Galois group are potentially to be recovered from one cohomology group or Galois module, by means of a mediating Tannakian category it generates.\n\nThose areas of application are closely connected to the theory of motives. Another place in which Tannakian categories have been used is in connection with the Grothendieck–Katz p-curvature conjecture; in other words, in bounding monodromy groups.\n\nThe Geometric Satake equivalence establishes an equivalence between representations of the Langlands dual group formula_1 of a reductive group \"G\" and certain equivariant perverse sheaves on the affine Grassmannian associated to \"G\". This equivalence provides a non-combinatorial construction of the Langlands dual group. It is proved by showing that the mentioned category of perverse sheaves is a Tannakian category and identifying its Tannaka dual group with formula_1.\n\n has established partial Tannaka duality results in the situation where the category is \"R\"-linear, where \"R\" is no longer a field (as in classical Tannakian duality), but certain valuation rings. showed a Tannaka duality result if \"R\" is a Dedekind ring.\n\n\n"}
{"id": "50691950", "url": "https://en.wikipedia.org/wiki?curid=50691950", "title": "Triangulation (surveying)", "text": "Triangulation (surveying)\n\nIn surveying, triangulation is the process of determining the location of a point by measuring only angles to it from known points at either end of a fixed baseline, rather than measuring distances to the point directly as in trilateration. The point can then be fixed as the third point of a triangle with one known side and two known angles.\n\nTriangulation can also refer to the accurate surveying of systems of very large triangles, called triangulation networks. This followed from the work of Willebrord Snell in 1615–17, who showed how a point could be located from the angles subtended from \"three\" known points, but measured at the new unknown point rather than the previously fixed points, a problem called resectioning. Surveying error is minimized if a mesh of triangles at the largest appropriate scale is established first. Points inside the triangles can all then be accurately located with reference to it. Such triangulation methods were used for accurate large-scale land surveying until the rise of global navigation satellite systems in the 1980s.\n\nTriangulation may be used to find the position of the ship when the positions of A and B are known. An observer at \"A\" measures the angle \"α\", while the observer at \"B\" measures \"β\" .\n\nThe position of any vertex of a triangle can be calculated if the position of one side, and two angles, are known. The following formulae are strictly correct only for a flat surface. If the curvature of the Earth must be allowed for, then spherical trigonometry must be used.\n\nWith \"l\" being the distance between \"A\" and \"B\" we have:\n\nUsing the trigonometric identities tan α = sin α / cos α and sin(α + β) = sin α cos β + cos α sin β, this is equivalent to:\n\ntherefore:\n\nFrom this, it is easy to determine the distance of the unknown point from either observation point, its north/south and east/west offsets from the observation point, and finally its full coordinates.\n\n \nTriangulation today is used for many purposes, including surveying, navigation, metrology, astrometry, binocular vision, model rocketry and gun direction of weapons.\n\nIn the field, triangulation methods were apparently not used by the Roman specialist land surveyors, the \"agromensores\"; but were introduced into medieval Spain through Arabic treatises on the astrolabe, such as that by Ibn al-Saffar (d. 1035). Abu Rayhan Biruni (d. 1048) also introduced triangulation techniques to measure the size of the Earth and the distances between various places. Simplified Roman techniques then seem to have co-existed with more sophisticated techniques used by professional surveyors. But it was rare for such methods to be translated into Latin (a manual on geometry, the eleventh century \"Geomatria incerti auctoris\" is a rare exception), and such techniques appear to have percolated only slowly into the rest of Europe. Increased awareness and use of such techniques in Spain may be attested by the medieval Jacob's staff, used specifically for measuring angles, which dates from about 1300; and the appearance of accurately surveyed coastlines in the Portolan charts, the earliest of which that survives is dated 1296.\n\nOn land, the cartographer Gemma Frisius proposed using triangulation to accurately position far-away places for map-making in his 1533 pamphlet \"Libellus de Locorum describendorum ratione\" (\"Booklet concerning a way of describing places\"), which he bound in as an appendix in a new edition of Peter Apian's best-selling 1524 \"Cosmographica\". This became very influential, and the technique spread across Germany, Austria and the Netherlands. The astronomer Tycho Brahe applied the method in Scandinavia, completing a detailed triangulation in 1579 of the island of Hven, where his observatory was based, with reference to key landmarks on both sides of the Øresund, producing an estate plan of the island in 1584. In England Frisius's method was included in the growing number of books on surveying which appeared from the middle of the century onwards, including William Cuningham's \"Cosmographical Glasse\" (1559), Valentine Leigh's \"Treatise of Measuring All Kinds of Lands\" (1562), William Bourne's \"Rules of Navigation\" (1571), Thomas Digges's \"Geometrical Practise named Pantometria\" (1571), and John Norden's \"Surveyor's Dialogue\" (1607). It has been suggested that Christopher Saxton may have used rough-and-ready triangulation to place features in his county maps of the 1570s; but others suppose that, having obtained rough bearings to features from key vantage points, he may have estimated the distances to them simply by guesswork.\n\nThe modern systematic use of triangulation networks stems from the work of the Dutch mathematician Willebrord Snell, who in 1615 surveyed the distance from Alkmaar to Breda, approximately 72 miles (116 kilometres), using a chain of quadrangles containing 33 triangles in all. Snell underestimated the distance by 3.5%. The two towns were separated by one degree on the meridian, so from his measurement he was able to calculate a value for the circumference of the earth – a feat celebrated in the title of his book \"Eratosthenes Batavus\" (\"The Dutch Eratosthenes\"), published in 1617. Snell calculated how the planar formulae could be corrected to allow for the curvature of the earth. He also showed how to resection, or calculate, the position of a point inside a triangle using the angles cast between the vertices at the unknown point. These could be measured much more accurately than bearings of the vertices, which depended on a compass. This established the key idea of surveying a large-scale primary network of control points first, and then locating secondary subsidiary points later, within that primary network.\n\nSnell's methods were taken up by Jean Picard who in 1669–70 surveyed one degree of latitude along the Paris Meridian using a chain of thirteen triangles stretching north from Paris to the clocktower of Sourdon, near Amiens. Thanks to improvements in instruments and accuracy, Picard's is rated as the first reasonably accurate measurement of the radius of the earth. Over the next century this work was extended most notably by the Cassini family: between 1683 and 1718 Jean-Dominique Cassini and his son Jacques Cassini surveyed the whole of the Paris meridian from Dunkirk to Perpignan; and between 1733 and 1740 Jacques and his son César Cassini undertook the first triangulation of the whole country, including a re-surveying of the meridian arc, leading to the publication in 1745 of the first map of France constructed on rigorous principles.\n\nTriangulation methods were by now well established for local mapmaking, but it was only towards the end of the 18th century that other countries began to establish detailed triangulation network surveys to map whole countries. The Principal Triangulation of Great Britain was begun by the Ordnance Survey in 1783, though not completed until 1853; and the Great Trigonometric Survey of India, which ultimately named and mapped Mount Everest and the other Himalayan peaks, was begun in 1801. For the Napoleonic French state, the French triangulation was extended by Jean Joseph Tranchot into the German Rhineland from 1801, subsequently completed after 1815 by the Prussian general Karl von Müffling. Meanwhile, the famous mathematician Carl Friedrich Gauss was entrusted from 1821 to 1825 with the triangulation of the kingdom of Hanover, for which he developed the method of least squares to find the best fit solution for problems of large systems of simultaneous equations given more real-world measurements than unknowns.\n\nToday, large-scale triangulation networks for positioning have largely been superseded by the global navigation satellite systems established since the 1980s, but many of the control points for the earlier surveys still survive as valued historical features in the landscape, such as the concrete triangulation pillars set up for retriangulation of Great Britain (1936–1962), or the triangulation points set up for the Struve Geodetic Arc (1816–1855), now scheduled as a UNESCO World Heritage Site.\n\n\n"}
