{"id": "65425", "url": "https://en.wikipedia.org/wiki?curid=65425", "title": "Aleksandr Lyapunov", "text": "Aleksandr Lyapunov\n\nAleksandr Mikhailovich Lyapunov (, ; – November 3, 1918) was a Russian mathematician, mechanician and physicist. His surname is sometimes romanized as Ljapunov, Liapunov, Liapounoff or Ljapunow. He was the son of astronomer Mikhail Lyapunov and the brother of pianist and composer Sergei Lyapunov.\n\nLyapunov is known for his development of the stability theory of a dynamical system, as well as for his many contributions to mathematical physics and probability theory.\n\nLyapunov was born in Yaroslavl, Russian Empire. His father Mikhail Vasilyevich Lyapunov (1820–1868) was an astronomer employed by the Demidov Lyceum. His brother, Sergei Lyapunov, was a gifted composer and pianist. In 1863, M. V. Lyapunov retired from his scientific career and relocated his family to his wife's estate at Bolobonov, in the Simbirsk province (now Ulyanovsk Oblast). After the death of his father in 1868, Aleksandr Lyapunov was educated by his uncle R. M. Sechenov, brother of the physiologist Ivan Mikhailovich Sechenov. At his uncle's family, Lyapunov studied with his distant cousin Natalia Rafailovna, who became his wife in 1886. In 1870, his mother moved with her sons to Nizhny Novgorod, where he started the third class of the gymnasium. He graduated from the gymnasium with distinction in 1876.\n\nIn 1876, Lyapunov entered the Physico-Mathematical department at the University of Saint Petersburg, but after one month he transferred to the Mathematics department of the university.\n\nAmong the Saint Petersburg professors of mathematics were Chebyshev and his students Aleksandr Nikolaevich Korkin and Yegor Ivanovich Zolotarev. Lyapunov wrote his first independent scientific works under the guidance of the professor of mechanics, D. K. Bobylev. In 1880 Lyapunov received a gold medal for a work on hydrostatics. This was the basis for his first published scientific works \"On the equilibrium of a heavy body in a heavy fluid contained in a vessel of a fixed form\" and \"On the potential of hydrostatic pressure\". Lyapunov completed his university course in 1880, two years after Andrey Markov who had also graduated at Saint Petersburg University. Lyapunov would maintain a scientific contact with Markov during all his life.\n\nA major theme in Lyapunov's research was the stability of a rotating fluid mass with possible astronomical application. This subject was proposed to Lyapunov by Chebyshev as a topic for his masters thesis which he submitted in 1884 with the title \"On the stability of ellipsoidal forms of rotating fluids\". \nIn 1885, Lyapunov became privatdozent and was proposed to accept the chair of mechanics at Kharkov University, where he went the same year. About the initial stay at Kharkov, Smirnov writes in his biography of Lyapunov:\n\nHis student and collaborator, Vladimir Steklov, recalled his first lecture in the following way: \"A handsome young man, almost of the age of the other students, came before the audience, where there was also the old Dean, professor Levakovsky, who was respected by all students. After the Dean had left, the young man with a trembled voice started to lecture a course on the dynamics of material points, instead of a course on dynamical systems. This subject was already known to the students from the lectures of professor Delarue. But what Lyapunov taught us was new to me and I had never seen this material in any textbook. All antipathy to the course was immediately blown to dust. From that day students would show Lyapunov a special respect.\"\n\nThe main contribution was published in the celebrated monograph 'A.M. Lyapunov, The general problem of the stability of motion. 1892. Kharkov Mathematical Society, Kharkov, 251p. (in Russian)'. \nThis led on to his 1892 doctoral thesis \"The general problem of the stability of motion\". The thesis was defended in Moscow University on September 12, 1892, with Nikolai Zhukovsky and V. B. Mlodzeevski as opponents. In 1908, the Kharkov edition was translated to French.\nRepublished by the University of Toulouse: 'Probleme General de la Stabilite du Mouvement, Par M.A. Liapounoff. Traduit du russe par M.Edouard Davaux'. \n\nLyapunov returned to Saint Petersburg in 1902, after being elected acting member of the Academy of Science as well as ordinary professor in the Faculty of Applied Mathematics of the university. The position had been left vacant by the death of his former teacher, Chebyshev. Not having any teaching obligations, this allowed Lyapunov to focus on his studies and in particular he was able to bring to a conclusion the work on the problem of Chebyshev with which he started his scientific career.\n\nIn 1908, he took part to the Fourth International Mathematical Congress in Rome. He also participated in the publication of Euler's selected works: he was an editor of the volumes 18 and 19.\n\nBy the end of June 1917, Lyapunov traveled with his wife to his brother's place in Odessa. Lyapunov's wife was suffering from tuberculosis so they moved following her doctor's orders. She died on October 31, 1918. The same day, Lyapunov shot himself in the head, and three days later he died. By that time, he was going blind from cataracts.\n\nLyapunov contributed to several fields, including differential equations, potential theory, dynamical systems and probability theory. His main preoccupations were the stability of equilibria and the motion of mechanical systems, and the study of particles under the influence of gravity. His work in the field of mathematical physics regarded the boundary value problem of the equation of Laplace. In the theory of potential, his work from 1897 \"On some questions connected with Dirichlet's problem\" clarified several important aspects of the theory. His work in this field is in close connection with the work of Steklov. Lyapunov developed many important approximation methods. His methods, which he developed in 1899, make it possible to define the stability of sets of ordinary differential equations. He created the modern theory of the stability of a dynamic system. In the theory of probability, he generalised the works of Chebyshev and Markov, and proved the Central Limit Theorem under more general conditions than his predecessors. The method of characteristic functions he used for the proof later found widespread use in probability theory.\n\nLike many mathematicians, Lyapunov preferred to work alone and communicated mainly with few colleagues and close relatives. He usually worked late, four to five hours at night, sometimes the whole night. Once or twice a year he visited the theatre, or went to some concert. He had many students. He was an honorary member of many universities, an honorary member of the Academy in Rome and a corresponding member of the Academy of Sciences in Paris.\n\nLyapunov's impact was significant, and a number of different mathematical concepts therefore bear his name: \n\n\n\n\n"}
{"id": "21121170", "url": "https://en.wikipedia.org/wiki?curid=21121170", "title": "André plane", "text": "André plane\n\nIn mathematics, André planes are non-Desarguesian planes with transitive automorphism groups found by .\n\n"}
{"id": "160556", "url": "https://en.wikipedia.org/wiki?curid=160556", "title": "Ball (mathematics)", "text": "Ball (mathematics)\n\nIn mathematics, a ball is the space bounded by a sphere. It may be a closed ball (including the boundary points that constitute the sphere) or an open ball (excluding them).\n\nThese concepts are defined not only in three-dimensional Euclidean space but also for lower and higher dimensions, and for metric spaces in general. A \"ball\" or hyperball in dimensions is called an -ball and is bounded by an ()-sphere. Thus, for example, a ball in the Euclidean plane is the same thing as a disk, the area bounded by a circle. In Euclidean 3-space, a ball is taken to be the volume bounded by a 2-dimensional sphere. In a one-dimensional space, a ball is a line segment.\n\nIn other contexts, such as in Euclidean geometry and informal use, \"sphere\" is sometimes used to mean \"ball\".\n\nIn Euclidean -space, an (open) -ball of radius and center is the set of all points of distance less than from . A closed -ball of radius is the set of all points of distance less than or equal to away from .\n\nIn Euclidean -space, every ball is bounded by a hypersphere. The ball is a bounded interval when , is a disk bounded by a circle when , and is bounded by a sphere when .\n\nThe -dimensional volume of a Euclidean ball of radius in -dimensional Euclidean space is:\nwhere  is Leonhard Euler's gamma function (which can be thought of as an extension of the factorial function to fractional arguments). Using explicit formulas for particular values of the gamma function at the integers and half integers gives formulas for the volume of a Euclidean ball that do not require an evaluation of the gamma function. These are:\nIn the formula for odd-dimensional volumes, the double factorial is defined for odd integers as .\n\nLet be a metric space, namely a set with a metric (distance function) . The open (metric) ball of radius centered at a point in , usually denoted by or , is defined by\n\nThe closed (metric) ball, which may be denoted by or , is defined by\n\nNote in particular that a ball (open or closed) always includes itself, since the definition requires .\n\nThe closure of the open ball is usually denoted . While it is always the case that , it is always the case that . For example, in a metric space with the discrete metric, one has and , for any .\n\nA unit ball (open or closed) is a ball of radius 1.\n\nA subset of a metric space is bounded if it is contained in some ball. A set is totally bounded if, given any positive radius, it is covered by finitely many balls of that radius.\n\nThe open balls of a metric space can serve as a base, giving this space a topology, the open sets of which are all possible unions of open balls. This topology on a metric space is called the topology induced by the metric .\n\nAny normed vector space with norm formula_5 is also a metric space with the metric formula_6 In such spaces, an arbitrary ball formula_7 of points formula_8 around a point formula_9 with a distance of less than formula_10 may be viewed as a scaled (by formula_10) and translated (by formula_9) copy of a \"unit ball\" formula_13 Such \"centered\" balls with formula_14 are denoted with formula_15\n\nThe Euclidean balls discussed earlier are an example of balls in a normed vector space.\n\nIn a Cartesian space with the -norm , that is\n\nan open ball around the origin with radius formula_10 is given by the set\n\nFor , in a 2-dimensional plane formula_19, \"balls\" according to the -norm (often called the \"taxicab\" or \"Manhattan\" metric) are bounded by squares with their \"diagonals\" parallel to the coordinate axes; those according to the -norm, also called the Chebyshev metric, have squares with their \"sides\" parallel to the coordinate axes as their boundaries. The -norm, known as the Euclidean metric, generates the well known discs within circles, and for other values of , the corresponding balls are areas bounded by Lamé curves (hypoellipses or hyperellipses).\n\nFor , the - balls are within octahedra with axes-aligned \"body diagonals\", the -balls are within cubes with axes-aligned \"edges\", and the boundaries of balls for with are superellipsoids. Obviously, generates the inner of usual spheres.\n\nMore generally, given any centrally symmetric, bounded, open, and convex subset of , one can define a norm on where the balls are all translated and uniformly scaled copies of . Note this theorem does not hold if \"open\" subset is replaced by \"closed\" subset, because the origin point qualifies but does not define a norm on .\n\nOne may talk about balls in any topological space , not necessarily induced by a metric. An (open or closed) -dimensional topological ball of is any subset of which is homeomorphic to an (open or closed) Euclidean -ball. Topological -balls are important in combinatorial topology, as the building blocks of cell complexes.\n\nAny open topological -ball is homeomorphic to the Cartesian space and to the open unit -cube (hypercube) . Any closed topological -ball is homeomorphic to the closed -cube .\n\nAn -ball is homeomorphic to an -ball if and only if . The homeomorphisms between an open -ball and can be classified in two classes, that can be identified with the two possible topological orientations of .\n\nA topological -ball need not be smooth; if it is smooth, it need not be diffeomorphic to a Euclidean -ball.\n\n"}
{"id": "36033877", "url": "https://en.wikipedia.org/wiki?curid=36033877", "title": "Bisection (software engineering)", "text": "Bisection (software engineering)\n\nBisection is a method used in software development to identify change sets that result in a specific behavior change. It is mostly employed for finding the patch that introduced a bug. Another application area is finding the patch that indirectly fixed a bug.\n\nThe process of locating the changeset that introduced a specific regression was described as \"source change isolation\" in 1997 by Brian Ness and Viet Ngo of Cray Research. Regression testing was performed on Cray's compilers in editions comprising one or more changesets. Editions with known regressions could not be validated until developers addressed the problem. Source change isolation narrowed the cause to a single changeset that could then be excluded from editions, unblocking them with respect to this problem, while the author of the change worked on a fix. Ness and Ngo outlined linear search and binary search methods of performing this isolation.\n\nCode bisection has the goal of minimizing the effort to find a specific change set.\nIt employs a divide and conquer algorithm that\ndepends on having access to the code history which is usually preserved by\nrevision control in a code repository.\n\nCode history has the structure of a directed acyclic graph which can be topologically sorted. This makes it possible to use a divide and conquer search algorithm which:\n\nBisection is in LSPACE having an algorithmic complexity of formula_1 with formula_2 denoting the number of revisions in the search space, and is similar to a binary search.\n\nFor code bisection it is desirable that each revision in the search space can be built and tested independently.\n\nAlthough the bisection method can be completed manually, one of its main advantages is that it can be easily automated. It can thus fit into existing test automation processes: failures in exhaustive automated regression tests can trigger automated bisection to localize faults. Ness and Ngo focused on its potential in Cray's continuous delivery-style environment in which the automatically-isolated bad changeset could be automatically excluded from builds.\n\nThe revision control systems Git and Mercurial have built-in functionality for code bisection. The user can start a bisection session with a specified range of revisions from which the revision control system proposes a revision to test, the user tells the system whether the revision tested as \"good\" or \"bad\", and the process repeats until the specific \"bad\" revision has been identified. Other revision control systems, such as Bazaar or Subversion, support bisection through plugins or external scripts.\n\nPhoronix Test Suite can do bisection automatically to find performance regressions.\n\n"}
{"id": "34929672", "url": "https://en.wikipedia.org/wiki?curid=34929672", "title": "Campbell's theorem (probability)", "text": "Campbell's theorem (probability)\n\nIn probability theory and statistics, Campbell's theorem or the Campbell–Hardy theorem is either a particular equation or set of results relating to the expectation of a function summed over a point process to an integral involving the mean measure of the point process, which allows for the calculation of expected value and variance of the random sum. One version of the theorem, also known as Campbell's formula, entails an integral equation for the aforementioned sum over a general point process, and not necessarily a Poisson point process. There also exist equations involving moment measures and factorial moment measures that are considered versions of Campbell's formula. All these results are employed in probability and statistics with a particular importance in the theory of point processes and queueing theory as well as the related fields stochastic geometry, continuum percolation theory, and spatial statistics.\n\nAnother result by the name of Campbell's theorem is specifically for the Poisson point process and gives a method for calculating moments as well as the Laplace functional of a Poisson point process.\n\nThe name of both theorems stems from the work by Norman R. Campbell on thermionic noise, also known as shot noise, in vacuum tubes, which was partly inspired by the work of Ernest Rutherford and Hans Geiger on alpha particle detection, where the Poisson point process arose as a solution to a family of differential equations by Harry Bateman. In Campbell's work, he presents the moments and generating functions of the random sum of a Poisson process on the real line, but remarks that the main mathematical argument was due to G. H. Hardy, which has inspired the result to be sometimes called the Campbell–Hardy theorem.\n\nFor a point process formula_1 defined on \"d\"-dimensional Euclidean space formula_2, Campbell's theorem offers a way to calculate expectations of a real-valued function formula_3 defined also on formula_2 and summed over formula_1, namely:\n\nwhere formula_7 denotes the expectation and set notation is used such that formula_1 is considered as a random set (see Point process notation). For a point process formula_1, Campbell's theorem relates the above expectation with the intensity measure Λ. In relation to a Borel set \"B\" the intensity measure of formula_1 is defined as:\n\nwhere the measure notation is used such that formula_1 is considered a random counting measure. The quantity Λ(\"B\") can be interpreted as the average number of points of formula_1 located in the set \"B\".\n\nOne version of Campbell's theorem is for a general (not necessarily simple) point process formula_1 with intensity measure:\n\nis known as Campbell's formula or Campbell's theorem, which gives a method for calculating expectations of sums of measurable functions formula_16 with ranges on the real line. More specifically, for a point process formula_1 and a measurable function formula_18, the sum of formula_16 over the point process is given by the equation:\n\nwhere if one side of the equation is finite, then so is the other side. This equation is essentially an application of Fubini's theorem and it holds for a wide class of point processes, simple or not. Depending on the integral notation, this integral may also be written as:\n\nIf the intensity measure formula_22 of a point process formula_1 has a density formula_24, then Campbell's formula becomes:\n\nFor a stationary point process formula_1 with constant density formula_27, Campbell's theorem or formula reduces to a volume integral:\n\nThis equation naturally holds for the homogeneous Poisson point processes, which is an example of a stationary stochastic process.\n\nCampbell's theorem for general point processes gives a method for calculating the expectation of a function of a point (of a point process) summed over all the points in the point process. These random sums over point processes have applications in many areas where they are used as mathematical models.\n\nCampbell originally studied a problem of random sums motivated by understanding thermionic noise in valves, which is also known as shot-noise. Consequently, the study of random sums of functions over point processes is known as shot noise in probability and, particularly, point process theory.\n\nIn wireless network communication, when a transmitter is trying to send a signal to a receiver, all the other transmitters in the network can be considered as interference, which poses a similar problem as noise does in traditional wired telecommunication networks in terms of the ability to send data based on information theory. If the positioning of the interfering transmitters are assumed to form some point process, then shot noise can be used to model the sum of their interfering signals, which has led to stochastic geometry models of wireless networks.\n\nFor general point processes, other more general versions of Campbell's theorem exist depending on the nature of the random sum and in particular the function being summed over the point process.\n\nIf the function is a function of more than one point of the point process, the moment measures or factorial moment measures of the point process are needed, which can be compared to moments and factorial of random variables. The type of measure needed depends on whether the points of the point process in the random sum are need to be distinct or may repeat.\n\nMoment measures are used when points are allowed to repeat.\n\nFactorial moment measures are used when points are not allowed to repeat, hence points are distinct.\n\nFor general point processes, Campbell's theorem is only for sums of functions of a single point of the point process. To calculate the sum of a function of a single point as well as the entire point process, then generalized Campbell's theorems are required using the Palm distribution of the point process, which is based on the branch of probability known as Palm theory or Palm calculus.\n\nAnother version of Campbell's theorem says that for a Poisson point process formula_1 with intensity measure formula_22 and a measurable function formula_31, the random sum\n\nis absolutely convergent with probability one if and only if the integral\n\nProvided that this integral is finite, then the theorem further asserts that for any complex value formula_34 the equation\n\nholds if the integral on the right-hand side converges, which is the case for purely imaginary formula_34. Moreover,\n\nand if this integral converges, then\n\nwhere formula_39 denotes the variance of the random sum formula_40.\n\nFrom this theorem some expectation results for the Poisson point process follow, including its Laplace functional. \n\nFor a Poisson point process formula_41 with intensity measure formula_22, the Laplace functional is a consequence of the above version of Campbell's theorem and is given by:\n\nwhich for the homogeneous case is:\n"}
{"id": "336568", "url": "https://en.wikipedia.org/wiki?curid=336568", "title": "Classical orthogonal polynomials", "text": "Classical orthogonal polynomials\n\nIn mathematics, the classical orthogonal polynomials are the most widely used orthogonal polynomials: the Hermite polynomials, Laguerre polynomials, Jacobi polynomials (including as a special case the Gegenbauer polynomials, Chebyshev polynomials, and Legendre polynomials).\n\nThey have many important applications in such areas as mathematical physics (in particular, the theory of random matrices), approximation theory, numerical analysis, and many others.\n\nClassical orthogonal polynomials appeared in the early 19th century in the works of Adrien-Marie Legendre, who introduced the Legendre polynomials. In the late 19th century, the study of continued fractions to solve the moment problem by P. L. Chebyshev and then A.A. Markov and T.J. Stieltjes led to the general notion of orthogonal polynomials.\n\nFor given polynomials formula_1 and formula_2 the classical orthogonal polynomials formula_3 are characterized by being solutions of the differential equation \nwith to be determined constants formula_5.\n\nThere are several more general definitions of orthogonal classical polynomials; for example, use the term for all polynomials in the Askey scheme.\n\nIn general, the orthogonal polynomials formula_6 with respect to a weight formula_7\n\nThe relations above define formula_6 up to multiplication by a number. Various normalisations are used to fix the constant, e.g.\n\nThe classical orthogonal polynomials correspond to the three families of weights:\n\nThe standard normalisation (also called \"standardization\") is detailed below.\n\nFor formula_12 the Jacobi polynomials are given by the formula\n\nThey are normalised (standardized) by\n\nand satisfy the orthogonality condition\n\nThe Jacobi polynomials are solutions to the differential equation\n\nThe Jacobi polynomials with formula_17 are called the Gegenbauer polynomials (with parameter formula_18)\n\nFor formula_19, these are called the Legendre polynomials (for which the interval of orthogonality is [−1, 1] and the weight function is simply 1):\n\nFor formula_21, one obtains the Chebyshev polynomials (of the second and first kind, respectively).\n\nThe Hermite polynomials are defined by\n\nThey satisfy the orthogonality condition\n\nand the differential equation\n\nThe generalised Laguerre polynomials are defined by\n\nThey satisfy the orthogonality relation\n\nand the differential equation\n\nThe classical orthogonal polynomials arise from a differential equation of the form\n\nwhere \"Q\" is a given quadratic (at most) polynomial, and \"L\" is a given linear polynomial. The function \"f\", and the constant \"λ\", are to be found.\n\nThis is a Sturm–Liouville type of equation. Such equations generally have singularities in their solution functions f except for particular values of \"λ\". They can be thought of an eigenvector/eigenvalue problems: Letting \"D\" be the differential operator, formula_30, and changing the sign of \"λ\", the problem is to find the eigenvectors (eigenfunctions) f, and the\ncorresponding eigenvalues \"λ\", such that f does not have singularities and \"D\"(\"f\") = \"λf\".\n\nThe solutions of this differential equation have singularities unless \"λ\" takes on\nspecific values. There is a series of numbers \"λ\", \"λ\", \"λ\", ... that led to a series of polynomial solutions \"P\", \"P\", \"P\", ... if one of the following sets of conditions are met:\n\n\nThese three cases lead to the Jacobi-like, Laguerre-like, and Hermite-like polynomials, respectively.\n\nIn each of these three cases, we have the following:\n\n\nBecause of the constant of integration, the quantity \"R\"(\"x\") is determined only up to an arbitrary positive multiplicative constant. It will be used only in homogeneous differential equations\n(where this doesn't matter) and in the definition of the weight function (which can also be\nindeterminate.) The tables below will give the \"official\" values of \"R\"(\"x\") and \"W\"(\"x\").\n\nUnder the assumptions of the preceding section,\n\"P\"(\"x\") is proportional to formula_33\n\nThis is known as Rodrigues' formula, after Olinde Rodrigues. It is often written\n\n\nThere are also some mixed recurrences. In each of these, the numbers \"a\", \"b\", and \"c\" depend on \"n\"\nand \"r\", and are unrelated in the various formulas.\n\n\nThere are an enormous number of other formulas involving orthogonal polynomials\nin various ways. Here is a tiny sample of them, relating to the Chebyshev,\nassociated Laguerre, and Hermite polynomials:\n\n\nThe differential equation for a particular \"λ\" may be written (omitting explicit dependence on x)\n\nmultiplying by formula_45 yields\n\nand reversing the subscripts yields\n\nsubtracting and integrating:\n\nbut it can be seen that\n\nso that:\n\nIf the polynomials \"f\" are such that the term on the left is zero, and formula_51 for formula_52, then the orthogonality relationship will hold:\n\nfor formula_52.\n\nAll of the polynomial sequences arising from the differential equation above are equivalent, under scaling and/or shifting of the domain, and standardizing of the polynomials, to more restricted classes. Those restricted classes are exactly \"classical orthogonal polynomials\".\n\n\nBecause all polynomial sequences arising from a differential equation in the manner\ndescribed above are trivially equivalent to the classical polynomials, the actual classical\npolynomials are always used.\n\nThe Jacobi-like polynomials, once they have had their domain shifted and scaled so that\nthe interval of orthogonality is [−1, 1], still have two parameters to be determined.\nThey are formula_61 and formula_62 in the Jacobi polynomials,\nwritten formula_55. We have formula_64 and\nformula_65.\nBoth formula_61 and formula_62 are required to be greater than −1.\n\nWhen formula_61 and formula_62 are not equal, these polynomials\nare not symmetrical about \"x\" = 0.\n\nThe differential equation\n\nis Jacobi's equation.\n\nFor further details, see Jacobi polynomials.\n\nWhen one sets the parameters formula_61 and formula_62 in the Jacobi polynomials equal to each other, one obtains the Gegenbauer or ultraspherical polynomials. They are written formula_73, and defined as\n\nWe have formula_64 and\nformula_76.\nThe parameter formula_61 is required to be greater than −1/2.\n\nIgnoring the above considerations, the parameter formula_61 is closely related to the derivatives of formula_73:\n\nor, more generally:\n\nAll the other classical Jacobi-like polynomials (Legendre, etc.) are special cases of the Gegenbauer polynomials, obtained by choosing a value of formula_61 and choosing a standardization.\n\nFor further details, see Gegenbauer polynomials.\n\nThe differential equation is\n\nThis is Legendre's equation.\n\nThe second form of the differential equation is:\n\nThe recurrence relation is\n\nA mixed recurrence is\n\nRodrigues' formula is\n\nFor further details, see Legendre polynomials.\n\nThe Associated Legendre polynomials, denoted\nformula_89 where formula_90 and formula_91 are integers with formula_92, are defined as\n\nThe \"m\" in parentheses (to avoid confusion with an exponent) is a parameter. The \"m\" in brackets denotes the \"m\"-th derivative of the Legendre polynomial.\n\nThese \"polynomials\" are misnamed—they are not polynomials when \"m\" is odd.\n\nThey have a recurrence relation:\n\nFor fixed \"m\", the sequence formula_95 are orthogonal over [−1, 1], with weight 1.\n\nFor given \"m\", formula_89 are the solutions of\n\nThe differential equation is\n\nThis is Chebyshev's equation.\n\nThe recurrence relation is\n\nRodrigues' formula is\n\nThese polynomials have the property that, in the interval of orthogonality,\n\nThis means that all their local minima and maxima have values of −1 and +1, that is, the polynomials are \"level\". Because of this, expansion of functions in terms of Chebyshev polynomials is sometimes used for polynomial approximations in computer math libraries.\n\nSome authors use versions of these polynomials that have been shifted so that the interval of orthogonality is [0, 1] or [−2, 2].\n\nThere are also Chebyshev polynomials of the second kind, denoted formula_102\n\nWe have:\n\nFor further details, including the expressions for the first few\npolynomials, see Chebyshev polynomials.\n\nThe most general Laguerre-like polynomials, after the domain has been shifted and scaled, are the Associated Laguerre polynomials (also called generalized Laguerre polynomials), denoted formula_57. There is a parameter formula_61, which can be any real number strictly greater than −1. The parameter is put in parentheses to avoid confusion with an exponent. The plain Laguerre polynomials are simply the formula_106 version of these:\n\nThe differential equation is\n\nThis is Laguerre's equation.\n\nThe second form of the differential equation is\n\nThe recurrence relation is\n\nRodrigues' formula is\n\nThe parameter formula_61 is closely related to the derivatives of formula_57:\n\nor, more generally:\n\nLaguerre's equation can be manipulated into a form that is more useful in applications:\n\nis a solution of\n\nThis can be further manipulated. When formula_118 is an integer, and formula_119:\n\nis a solution of\n\nThe solution is often expressed in terms of derivatives instead of associated Laguerre polynomials:\n\nThis equation arises in quantum mechanics, in the radial part of the solution of the Schrödinger equation for a one-electron atom.\n\nPhysicists often use a definition for the Laguerre polynomials that is larger, by a factor of formula_123, than the definition used here.\n\nFor further details, including the expressions for the first few polynomials, see Laguerre polynomials.\n\nThe differential equation is\n\nThis is Hermite's equation.\n\nThe second form of the differential equation is\n\nThe third form is\n\nThe recurrence relation is\n\nRodrigues' formula is\n\nThe first few Hermite polynomials are\n\nOne can define the associated Hermite functions\n\nBecause the multiplier is proportional to the square root of the weight function, these functions\nare orthogonal over formula_59 with no weight function.\n\nThe third form of the differential equation above, for the associated Hermite functions, is\n\nThe associated Hermite functions arise in many areas of mathematics and physics.\nIn quantum mechanics, they are the solutions of Schrödinger's equation for the harmonic oscillator.\nThey are also eigenfunctions (with eigenvalue (−\"i\")) of the continuous Fourier transform.\n\nMany authors, particularly probabilists, use an alternate definition of the Hermite polynomials, with a weight function of formula_137 instead of formula_138. If the notation \"He\" is used for these Hermite polynomials, and \"H\" for those above, then these may be characterized by\n\nFor further details, see Hermite polynomials.\n\nThere are several conditions that single out the classical orthogonal polynomials from the others.\n\nThe first condition was found by Sonine (and later by Hahn), who showed that (up to linear changes of variable) the classical orthogonal polynomials are the only ones such that their derivatives are also orthogonal polynomials.\n\nBochner characterized classical orthogonal polynomials in terms of their recurrence relations.\n\nTricomi characterized classical orthogonal polynomials as those that have a certain analogue of the Rodrigues formula.\n\nThe following table summarises the properties of the classical orthogonal polynomials.\n\n\n"}
{"id": "9903220", "url": "https://en.wikipedia.org/wiki?curid=9903220", "title": "Complete set of invariants", "text": "Complete set of invariants\n\nIn mathematics, a complete set of invariants for a classification problem is a collection of maps\n(where \"X\" is the collection of objects being classified, up to some equivalence relation, and the formula_2 are some sets), such that formula_3 if and only if formula_4 for all \"i\". In words, such that two objects are equivalent if and only if all invariants are equal.\n\nSymbolically, a complete set of invariants is a collection of maps such that\nis injective.\n\nAs invariants are, by definition, equal on equivalent objects, equality of invariants is a \"necessary\" condition for equivalence; a \"complete\" set of invariants is a set such that equality of these is \"sufficient\" for equivalence. In the context of a group action, this may be stated as: invariants are functions of coinvariants (equivalence classes, orbits), and a complete set of invariants characterizes the coinvariants (is a set of defining equations for the coinvariants).\n\n\nA complete set of invariants does not immediately yield a classification theorem: not all combinations of invariants may be realized. Symbolically, one must also determine the image of\n"}
{"id": "23558705", "url": "https://en.wikipedia.org/wiki?curid=23558705", "title": "Decimal128 floating-point format", "text": "Decimal128 floating-point format\n\nIn computing, decimal128 is a decimal floating-point computer numbering format that occupies 16 bytes (128 bits) in computer memory.\nIt is intended for applications where it is necessary to emulate decimal rounding exactly, such as financial and tax computations.\n\nDecimal128 supports 34 decimal digits of significand and an exponent range of −6143 to +6144, i.e. to . (Equivalently, to .) Therefore, decimal128 has the greatest range of values compared with other IEEE basic floating point formats. Because the significand is not normalized, most values with less than 34 significant digits have multiple possible representations; , etc. Zero has possible representations ( if you include both signed zeros).\n\nDecimal128 floating point is a relatively new decimal floating-point format, formally introduced in the 2008 version of IEEE 754 as well as with .\n\nIEEE 754 allows two alternative representation methods for decimal128 values.\nThe standard does not specify how to signify which representation is used,\nfor instance in a situation where decimal128 values are communicated between systems.\n\nIn one representation method, based on binary integer decimal (BID),\nthe significand is represented as binary coded positive integer.\n\nThe other, alternative, representation method is based on\ndensely packed decimal (DPD) for most of the\nsignificand (except the most significant digit).\n\nBoth alternatives provide exactly the same range of representable numbers: 34 digits of significand and 3×2 = possible exponent values.\n\nIn both cases, the most significant 4 bits of the significand (which actually only have 10 possible values) are combined with the most significant 2 bits of the exponent (3 possible values) to use 30 of the 32 possible values of 5 bits in the combination field. The remaining combinations encode infinities and NaNs.\n\nIn the case of Infinity and NaN, all other bits of the encoding are ignored. Thus, it is possible to initialize an array to Infinities or NaNs by filling it with a single byte value.\n\nThis format uses a binary significand from 0 to 10−1 = = 1ED09BEAD87C0378D8E63FFFFFFFF = \nThe encoding can represent binary significands up to 10×2−1 = but values larger than 10−1 are illegal (and the standard requires implementations to treat them as 0, if encountered on input).\n\nAs described above, the encoding varies depending on whether the most significant 4 bits of the significand are in the range 0 to 7 (0000 to 0111), or higher (1000 or 1001).\n\nIf the 2 bits after the sign bit are \"00\", \"01\", or \"10\", then the \nexponent field consists of the 14 bits following the sign bit, and the\nsignificand is the remaining 113 bits, with an implicit leading 0 bit:\n\nThis includes subnormal numbers where the leading significand digit is 0.\n\nIf the 2 bits after the sign bit are \"11\", then the 14-bit exponent field is shifted 2 bits to the right (after both the sign bit and the \"11\" bits thereafter), and the represented significand is in the remaining 111 bits. In this case there is an implicit (that is, not stored) leading 3-bit sequence \"100\" in the true significand.\n\nThe \"11\" 2-bit sequence after the sign bit indicates that there is an \"implicit\" \"100\" 3-bit\nprefix to the significand. Compare having an implicit 1 in the significand of normal\nvalues for the binary formats. Note also that the \"00\", \"01\", or \"10\" bits are part of the exponent field.\n\nFor the decimal128 format, all of these significands are out of the valid range (they begin with 2^113 > 1.038×10), and are thus decoded as zero, but the pattern is same as decimal32 and decimal64.\n\nIn the above cases, the value represented is\n\nIf the four bits after the sign bit are \"1111\" then the value is an infinity or a NaN, as described above:\n\nIn this version, the significand is stored as a series of decimal digits. The leading digit is between 0 and 9 (3 or 4 binary bits), and the rest of the significand uses the densely packed decimal (DPD) encoding.\n\nUnlike the binary integer significand version, where the exponent changed position and came before the significand, this encoding combines the leading 2 bits of the exponent and the leading digit (3 or 4 bits) of the significand into the five bits that follow the sign bit.\n\nThis twelve bits after that are the exponent continuation field, providing the less-significant bits of the exponent.\n\nThe last 110 bits are the significand continuation field, consisting of eleven 10-bit \"declets\". Each declet encodes three decimal digits using the DPD encoding.\n\nIf the first two bits after the sign bit are \"00\", \"01\", or \"10\", then those are\nthe leading bits of the exponent, and the three bits after that are interpreted as\nthe leading decimal digit (0 to 7):\n\nIf the first two bits after the sign bit are \"11\", then the \nsecond two bits are the leading bits of the exponent, and the last bit is\nprefixed with \"100\" to form the leading decimal digit (8 or 9):\n\nThe remaining two combinations (11110 and 11111) of the 5-bit field\nare used to represent ±infinity and NaNs, respectively.\n\nThe DPD/3BCD transcoding for the declets is given by the following table.\nb9...b0 are the bits of the DPD, and d2...d0 are the three BCD digits.\n\nThe 8 decimal values whose digits are all 8s or 9s have four codings each.\nThe bits marked x in the table above are ignored on input, but will always be 0 in computed results.\n\nIn the above cases, with the \"true significand\" as the sequence of decimal digits decoded, the value represented is\n\n"}
{"id": "39645680", "url": "https://en.wikipedia.org/wiki?curid=39645680", "title": "Decomposition method (queueing theory)", "text": "Decomposition method (queueing theory)\n\nIn queueing theory, a discipline within the mathematical theory of probability, the decomposition method is an approximate method for the analysis of queueing networks where the network is broken into subsystems which are independently analyzed.\n\nThe individual queueing nodes are considered to be independent G/G/1 queues where arrivals are governed by a renewal process and both service time and arrival distributions are parametrised to match the first two moments of data.\n"}
{"id": "42726919", "url": "https://en.wikipedia.org/wiki?curid=42726919", "title": "Deferred Measurement Principle", "text": "Deferred Measurement Principle\n\nThe Deferred Measurement Principle is a result in quantum computing which states that delaying measurements until the end of a quantum computation doesn't affect the probability distribution of outcomes.\n\nA consequence of the deferred measurement principle is that measuring commutes with conditioning.\nThe choice of whether to measure a qubit before, after, or during an operation conditioned on that qubit will have no observable effect on a circuit's final expected results.\n\nThanks to the deferred measurement principle, measurements in a quantum circuit can often be shifted around so they happen at better times.\nFor example, measuring qubits as early as possible can reduce the maximum number of simultaneously stored qubits; potentially enabling an algorithm to be run on a smaller quantum computer or to be simulated more efficiently.\nAlternatively, deferring all measurements until the end of circuits allows them to be analyzed using only pure states.\n"}
{"id": "229528", "url": "https://en.wikipedia.org/wiki?curid=229528", "title": "Delta operator", "text": "Delta operator\n\nIn mathematics, a delta operator is a shift-equivariant linear operator \"formula_1\" on the vector space of polynomials in a variable formula_2 over a field formula_3 that reduces degrees by one.\n\nTo say that formula_4 is shift-equivariant means that if formula_5, then\n\nIn other words, if \"formula_7\" is a \"shift\" of \"formula_8\", then \"formula_9\" is also a shift of \"formula_10\", and has the same \"shifting vector\" \"formula_11\".\n\nTo say that \"an operator reduces degree by one\" means that if \"formula_7\" is a polynomial of degree \"formula_13\", then \"formula_9\" is either a polynomial of degree formula_15, or, in case formula_16, \"formula_9\" is 0.\n\nSometimes a \"delta operator\" is defined to be a shift-equivariant linear transformation on polynomials in \"formula_2\" that maps \"formula_2\" to a nonzero constant. Seemingly weaker than the definition given above, this latter characterization can be shown to be equivalent to the stated definition, since shift-equivariance is a fairly strong condition.\n\n\n\n\nEvery delta operator \"formula_4\" has a unique sequence of \"basic polynomials\", a polynomial sequence defined by three conditions:\n\n\nSuch a sequence of basic polynomials is always of binomial type, and it can be shown that no other sequences of binomial type exist. If the first two conditions above are dropped, then the third condition says this polynomial sequence is a Sheffer sequence—a more general concept.\n\n"}
{"id": "1020021", "url": "https://en.wikipedia.org/wiki?curid=1020021", "title": "Distance (graph theory)", "text": "Distance (graph theory)\n\nIn the mathematical field of graph theory, the distance between two vertices in a graph is the number of edges in a shortest path (also called a graph geodesic) connecting them. This is also known as the geodesic distance. Notice that there may be more than one shortest path between two vertices. If there is no path connecting the two vertices, i.e., if they belong to different connected components, then conventionally the distance is defined as infinite.\n\nIn the case of a directed graph the distance formula_1 between two vertices formula_2 and formula_3 is defined as the length of a shortest directed path from formula_2 to formula_3 consisting of arcs, provided at least one such path exists. Notice that, in contrast with the case of undirected graphs, formula_1 does not necessarily coincide with formula_7, and it might be the case that one is defined while the other is not.\n\nA metric space defined over a set of points in terms of distances in a graph defined over the set is called a graph metric.\nThe vertex set (of an undirected graph) and the distance function form a metric space, if and only if the graph is connected.\n\nThe eccentricity formula_8 of a vertex formula_3 is the greatest geodesic distance between formula_3 and any other vertex. It can be thought of as how far a node is from the node most distant from it in the graph.\n\nThe radius formula_11 of a graph is the minimum eccentricity of any vertex or, in symbols, formula_12.\n\nThe diameter formula_13 of a graph is the maximum eccentricity of any vertex in the graph. That is, formula_13 is the greatest distance between any pair of vertices or, alternatively, formula_15. To find the diameter of a graph, first find the shortest path between each pair of vertices. The greatest length of any of these paths is the diameter of the graph. \n\nA central vertex in a graph of radius formula_11 is one whose eccentricity is formula_11—that is, a vertex that achieves the radius or, equivalently, a vertex formula_3 such that formula_19.\n\nA peripheral vertex in a graph of diameter formula_13 is one that is distance formula_13 from some other vertex—that is, a vertex that achieves the diameter. Formally, formula_3 is peripheral if formula_23.\n\nA pseudo-peripheral vertex formula_3 has the property that for any vertex formula_2, if formula_3 is as far away from formula_2 as possible, then formula_2 is as far away from formula_3 as possible. Formally, a vertex \"u\" is pseudo-peripheral, \nif for each vertex \"v\" with formula_30 holds formula_31.\n\nThe partition of a graph's vertices into subsets by their distances from a given starting vertex is called the level structure of the graph.\n\nA graph such that for every pair of vertices there is a unique shortest path connecting them is called a geodetic graph. For example, all trees are geodetic.\n\nOften peripheral sparse matrix algorithms need a starting vertex with a high eccentricity. A peripheral vertex would be perfect, but is often hard to calculate. In most circumstances a pseudo-peripheral vertex can be used. A pseudo-peripheral vertex can easily be found with the following algorithm:\n\n\n"}
{"id": "217680", "url": "https://en.wikipedia.org/wiki?curid=217680", "title": "Domain relational calculus", "text": "Domain relational calculus\n\nIn computer science, domain relational calculus (DRC) is a calculus that was introduced by Michel Lacroix and Alain Pirotte as a declarative database query language for the relational data model.\n\nIn DRC, \"queries\" have the form:\n\nwhere each X is either a domain variable or constant, and formula_2 denotes a DRC \"formula\". The result of the query is the set of tuples X to X that make the DRC formula true.\n\nThis language uses the same operators as tuple calculus,\nthe logical connectives ∧ (and), ∨ (or) and ¬ (not). The existential quantifier (∃) and the universal quantifier (∀) can be used to bind the variables.\n\nIts computational expressiveness is equivalent to that of Relational algebra.\n\nLet (A, B, C) mean (Rank, Name, ID) in the Enterprise relation\n\nand let (D, E, F) mean (Name, DeptName, ID) in the Department relation\n\nFind all captains of the starship USS Enterprise:\n\nformula_3\n\nIn this example, A, B, C denotes both the result set and a set in the table Enterprise.\n\nFind names of Enterprise crew members who are in Stellar Cartography:\n\nformula_4\n\nIn this example, we're only looking for the name, and that's B. The condition F = C is a requirement because we need to find Enterprise crew members AND they are in the Stellar Cartography Department.\n\nAn alternate representation of the previous example would be:\n\nformula_5\n\nIn this example, the value of the requested F domain is directly placed in the formula and the C domain variable is re-used in the query for the existence of a department, since it already holds a crew member's ID.\n\n\n"}
{"id": "3434148", "url": "https://en.wikipedia.org/wiki?curid=3434148", "title": "Dynamic Data Driven Applications Systems", "text": "Dynamic Data Driven Applications Systems\n\nDynamic Data Driven Applications Systems (DDDAS) is a new paradigm whereby the computation and instrumentation aspects of an application system are dynamically integrated in a feed-back control loop, in the sense that instrumentation data can be dynamically incorporated into the executing model of the application, and in reverse the executing model can control the instrumentation. Such approaches have been shown that can enable more accurate and faster modeling and analysis of the characteristics and behaviors of a system and can exploit data in intelligent ways to convert them to new capabilities, including decision support systems with the accuracy of full scale modeling, efficient data collection, management, and data mining. The DDDAS concept - and the term - was proposed by Frederica Darema for the National Science Foundation (NSF) workshop in March 2000.\n\nThere are several affiliated annual meetings and conferences, including:\n\n\n"}
{"id": "294995", "url": "https://en.wikipedia.org/wiki?curid=294995", "title": "Euler–Lagrange equation", "text": "Euler–Lagrange equation\n\nIn the calculus of variations, the Euler–Lagrange equation, Euler's equation, or Lagrange's equation (although the latter name is ambiguous—see disambiguation page), is a second-order partial differential equation whose solutions are the functions for which a given functional is stationary. It was developed by Swiss mathematician Leonhard Euler and Italian-French mathematician Joseph-Louis Lagrange in the 1750s.\n\nBecause a differentiable functional is stationary at its local maxima and minima, the Euler–Lagrange equation is useful for solving optimization problems in which, given some functional, one seeks the function minimizing or maximizing it. This is analogous to Fermat's theorem in calculus, stating that at any point where a differentiable function attains a local extremum its derivative is zero.\n\nIn Lagrangian mechanics, because of Hamilton's principle of stationary action, the evolution of a physical system is described by the solutions to the Euler–Lagrange equation for the action of the system. In classical mechanics, it is equivalent to Newton's laws of motion, but it has the advantage that it takes the same form in any system of generalized coordinates, and it is better suited to generalizations. In classical field theory there is an analogous equation to calculate the dynamics of a field.\n\nThe Euler–Lagrange equation was developed in the 1750s by Euler and Lagrange in connection with their studies of the tautochrone problem. This is the problem of determining a curve on which a weighted particle will fall to a fixed point in a fixed amount of time, independent of the starting point.\n\nLagrange solved this problem in 1755 and sent the solution to Euler. Both further developed Lagrange's method and applied it to mechanics, which led to the formulation of Lagrangian mechanics. Their correspondence ultimately led to the calculus of variations, a term coined by Euler himself in 1766.\n\nThe Euler–Lagrange equation is an equation satisfied by a function q\nof a real argument \"t\", which is a stationary point of the functional\n\nwhere:\n\nThe Euler–Lagrange equation, then, is given by\nwhere formula_18 and formula_19 denote the partial derivatives of formula_13 with respect to the second and third arguments, respectively.\n\nIf the dimension of the space formula_16 is greater than 1, this is a system of differential equations, one for each component:\n\nA standard example is finding the real-valued function \"f\" on the interval [\"a\", \"b\"], such that \"f\"(\"a\") = \"c\" and \"f\"(\"b\") = \"d\", for which the path length along the curve traced by \"f\" is as short as possible. \nthe integrand function being .\n\nThe partial derivatives of \"L\" are:\nBy substituting these into the Euler–Lagrange equation, we obtain\nthat is, the function must have constant first derivative, and thus its graph is a straight line.\n\nThe stationary values of the functional\ncan be obtained from the Euler–Lagrange equation \nunder fixed boundary conditions for the function itself as well as for the first formula_28 derivatives (i.e. for all formula_29). The endpoint values of the highest derivative formula_30 remain flexible.\n\nIf the problem involves finding several functions (formula_31) of a single independent variable (formula_32) that define an extremum of the functional\nthen the corresponding Euler–Lagrange equations are\n\nA multi-dimensional generalization comes from considering a function on n variables. If formula_35 is some surface, then\n\nis extremized only if \"f\" satisfies the partial differential equation\n\nWhen \"n\" = 2 and functional formula_38 is the energy functional, this leads to the soap-film minimal surface problem.\n\nIf there are several unknown functions to be determined and several variables such that\nthe system of Euler–Lagrange equations is\n\nIf there is a single unknown function \"f\" to be determined that is dependent on two variables \"x\" and \"x\" and if the functional depends on higher derivatives of \"f\" up to \"n\"-th order such that\nthen the Euler–Lagrange equation is\nwhich can be represented shortly as:\nwherein formula_44 are indices that span the number of variables, that is, here they go from 1 to 2. Here summation over the formula_44 indices is only over formula_46 in order to avoid counting the same partial derivative multiple times, for example formula_47 appears only once in the previous equation.\n\nIf there are \"p\" unknown functions \"f\" to be determined that are dependent on \"m\" variables \"x\" ... \"x\" and if the functional depends on higher derivatives of the \"f\" up to \"n\"-th order such that\n\nwhere formula_44 are indices that span the number of variables, that is they go from 1 to m. Then the Euler–Lagrange equation is\n\nwhere the summation over the formula_44 is avoiding counting the same derivative formula_52 several times, just as in the previous subsection. This can be expressed more compactly as\n\nLet formula_54 be a smooth manifold, and let formula_55 denote the space of smooth functions formula_56. Then, for functionals formula_57 of the form\nwhere formula_59 is the Lagrangian, the statement formula_60 is equivalent to the statement that, for all formula_61, each coordinate frame trivialization formula_62 of a neighborhood of formula_63 yields the following formula_64 equations:\n\n\n"}
{"id": "36727478", "url": "https://en.wikipedia.org/wiki?curid=36727478", "title": "Foundations of Differential Geometry", "text": "Foundations of Differential Geometry\n\nFoundations of Differential Geometry is an influential 2-volume mathematics book on differential geometry written by Shoshichi Kobayashi and Katsumi Nomizu. The first volume was published in 1963 and the second in 1969, by Interscience Publishers. Both were published again in 1996 as Wiley Classics Library.\n\nThe first volume considers manifolds, fiber bundles, tensor analysis, connections in bundles, and the role of Lie groups. It also covers holonomy, the de Rham decomposition theorem and the Hopf–Rinow theorem. According to the review of James Eells, it has a \"fine expositional style\" and consists of a \"special blend of algebraic, analytic, and geometric concepts\". Eells says it is \"essentially a textbook (even though there are no exercises)\". An advanced text, it has a \"pace geared to a [one] term graduate course\".\n\nThe second volume considers submanifolds of Riemannian manifolds, the Gauss map, and the second fundamental form. It continues with geodesics on Riemannian manifolds, Jacobi fields, the Morse index, the Rauch comparison theorems, and the Cartan–Hadamard theorem. Then it ascends to complex manifolds, Kähler manifolds, homogeneous spaces, and symmetric spaces. In a discussion of curvature representation of characteristic classes of principal bundles (Chern–Weil theory), it covers Euler classes, Chern classes, and Pontryagin classes. The second volume also received a favorable review by J. Eells in \"Mathematical Reviews\".\n\n"}
{"id": "23574973", "url": "https://en.wikipedia.org/wiki?curid=23574973", "title": "Geometry of roots of real polynomials", "text": "Geometry of roots of real polynomials\n\nGraphical methods provide a means of determining or approximating the roots of a polynomial—the values that make the polynomial equal to zero. Practical tools for performing these include graph paper, graphical calculators and computer graphics.\n\nThe fundamental theorem of algebra states that a \"n\"th-degree polynomial with complex coefficients (including real coefficients) has \"n\" complex roots (not necessarily real even if the coefficients are real), although its roots may not all be different from each other. If the polynomial has real coefficients, its roots are either real, or else occur as complex conjugates. Suppose a polynomial \"P\"(\"x\") is graphed as \"y\" = \"P\"(\"x\"). At a real root, the graph of the polynomial crosses the \"x\"-axis. Thus, the real roots of a polynomial can be demonstrated graphically.\n\nFor some kinds of polynomials, all the roots, including the complex roots, can be found graphically. Polynomial equations up to the fifth degree may be solved graphically.\n\nThe geometrical methods of ruler and compass may be used to solve any linear or quadratic equation. Descartes showed that the constructions of Euclid were equivalent to the algebraic solution of quadratics.\n\nCubic equations may be solved by solid geometry. Archimedes' work \"On the Sphere and the Cylinder\" provided solutions of some cubics and Omar Khayyam systematised this to provide geometrical solutions of all quadratics and cubics.\n\nFor polynomials with real coefficients, a local minimum point above the \"x\"-axis or a local maximum point below the \"x\"-axis indicates the existence of two non-real complex roots, which are each other's complex conjugates. The converse, however, is not true; for example, the cubic polynomial \"x\" + \"x\" has two complex roots, but its graph has no local minima or maxima.\n\nThe simplest such case involves parabolas.\n\nIf a parabola has a global minimum point above the \"x\"-axis, or a global maximum point below the \"x\"-axis, then its \"x\" intercepts are not real. For\n\nif \"a\" and \"k\" are positive, then the roots are non-real complex numbers. The number \"k\" is then the height of the vertex above the \"x\"-axis. In the example in the illustration, we have \"k\" = 9. Suppose one goes \"k\" units in the opposite direction from the vertex, i.e. away from the \"x\"-axis, then horizontally as far as it takes to reach the curve (in the example, that distance is 3. The horizontal distance from that point to the curve is the absolute value of the imaginary part of the root. The \"x\"-coordinate of the vertex is the real part. Thus, in the example, the roots are\n\nThis method is specific to quadratics and does not generalise to higher-degree polynomial equations.\n\nCubic function#Geometric interpretation of the roots\n\n"}
{"id": "35132005", "url": "https://en.wikipedia.org/wiki?curid=35132005", "title": "Gibbons–Hawking ansatz", "text": "Gibbons–Hawking ansatz\n\nIn mathematics, the Gibbons–Hawking ansatz is a method of constructing gravitational instantons introduced by . It gives examples of hyperkähler manifolds in dimension 4 that are invariant under a circle action.\n\n\n"}
{"id": "27145862", "url": "https://en.wikipedia.org/wiki?curid=27145862", "title": "Graph algebra (social sciences)", "text": "Graph algebra (social sciences)\n\nGraph algebra is systems-centric modeling tool for the social sciences. It was first developed by Sprague, Pzeworski, and Cortes as a hybridized version of engineering plots to describe social phenomena.\n"}
{"id": "9938244", "url": "https://en.wikipedia.org/wiki?curid=9938244", "title": "Graph operations", "text": "Graph operations\n\nGraph operations produce new graphs from initial ones. They may be separated into the following major categories. \n\nUnary operations create a new graph from one initial one.\n\nElementary operations or editing operations create a new graph from one initial one by a simple local change, such as addition or deletion of a vertex or of an edge, merging and splitting of vertices, edge contraction, etc.\nThe graph edit distance between a pair of graphs is the minimum number of elementary operations required to transform one graph into the other.\n\nAdvanced operations create a new graph from one initial one by a complex changes, such as:\n\nBinary operations create a new graph from two initial ones and , such as:\n\n"}
{"id": "6763077", "url": "https://en.wikipedia.org/wiki?curid=6763077", "title": "Group key", "text": "Group key\n\nIn cryptography, a group key is a cryptographic key that is shared between a group of users. Typically, group keys are distributed by sending them to individual users, either physically, or encrypted individually for each user using either that user's pre-distributed private key.\n\nA common use of group keys is to allow a group of users to decrypt a broadcast message that is intended for that entire group of users, and no-one else. \n\nFor example, in the Second World War, group keys (known as \"iodoforms\", a term invented by a classically educated non-chemist, and nothing to do with the chemical of the same name) were sent to groups of agents by the Special Operations Executive. These group keys allowed all the agents in a particular group to receive a single coded message.\n\nIn present-day applications, group keys are commonly used in conditional access systems, where the key is the common key used to decrypt the broadcast signal, and the group in question is the group of all paying subscribers. In this case, the group key is typically distributed to the subscribers' receivers using a combination of a physically distributed secure cryptoprocessor in the form of a smartcard and encrypted over-the-air messages.\n"}
{"id": "12450", "url": "https://en.wikipedia.org/wiki?curid=12450", "title": "Gödel's completeness theorem", "text": "Gödel's completeness theorem\n\nGödel's completeness theorem is a fundamental theorem in mathematical logic that establishes a correspondence between semantic truth and syntactic provability in first-order logic. It makes a close link between model theory that deals with what is true in different models, and proof theory that studies what can be formally proven in particular formal systems.\n\nIt was first proved by Kurt Gödel in 1929. It was then simplified in 1947, when Leon Henkin observed in his Ph.D. thesis that the hard part of the proof can be presented as the Model Existence Theorem (published in 1949). Henkin's proof was simplified by Gisbert Hasenjaeger in 1953.\n\nThere are numerous deductive systems for first-order logic, including systems of natural deduction and Hilbert-style systems. Common to all deductive systems is the notion of a formal deduction. This is a sequence (or, in some cases, a finite tree) of formulas with a specially-designated conclusion. The definition of a deduction is such that it is finite and that it is possible to verify algorithmically (by a computer, for example, or by hand) that a given sequence (or tree) of formulas is indeed a deduction.\n\nA first-order formula is called logically valid if it is true in every structure for the language of the formula (i.e. for any assignment of values to the variables of the formula). To formally state, and then prove, the completeness theorem, it is necessary to also define a deductive system. A deductive system is called complete if every logically valid formula is the conclusion of some formal deduction, and the completeness theorem for a particular deductive system is the theorem that it is complete in this sense. Thus, in a sense, there is a different completeness theorem for each deductive system. A converse to completeness is soundness, the fact that only logically valid formulas are provable in the deductive system.\n\nIf some specific deductive system of first-order logic is sound and complete, then it is \"perfect\" (a formula is provable if and only if it is logically valid), thus equivalent to any other deductive system with the same quality (any proof in one system can be converted into the other).\n\nWe first fix a deductive system of first-order predicate calculus, choosing any of the well-known equivalent systems. Gödel's original proof assumed the Hilbert-Ackermann proof system.\n\nThe completeness theorem says that if a formula is logically valid then there is a finite deduction (a formal proof) of the formula.\n\nThus, the deductive system is \"complete\" in the sense that no additional inference rules are required to prove all the logically valid formulas. A converse to completeness is soundness, the fact that only logically valid formulas are provable in the deductive system. Together with soundness (whose verification is easy), this theorem implies that a formula is logically valid if and only if it is the conclusion of a formal deduction.\n\nThe theorem can be expressed more generally in terms of logical consequence. We say that a sentence \"s\" is a syntactic consequence of a theory \"T\", denoted formula_1, if \"s\" is provable from \"T\" in our deductive system. We say that \"s\" is a semantic consequence of \"T\", denoted formula_2, if \"s\" holds in every model of \"T\". The completeness theorem then says that for any first-order theory \"T\" with a well-orderable language, and any sentence \"s\" in the language of \"T\",\n\nSince the converse (soundness) also holds, it follows that formula_2 iff formula_1, and thus that syntactic and semantic consequence are equivalent for first-order logic.\n\nThis more general theorem is used implicitly, for example, when a sentence is shown to be provable from the axioms of group theory by considering an arbitrary group and showing that the sentence is satisfied by that group.\n\nGödel's original formulation is deduced by taking the particular case of a theory without any axiom.\n\nThe completeness theorem can also be understood in terms of consistency, as a consequence of Henkin's model existence theorem. We say that a theory \"T\" is syntactically consistent if there is no sentence \"s\" such that both \"s\" and its negation ¬\"s\" are provable from \"T\" in our deductive system. The model existence theorem says that for any first-order theory \"T\" with a well-orderable language,\n\nAnother version, with connections to the Löwenheim–Skolem theorem, says:\n\nGiven Henkin's theorem, the completeness theorem can be proved as follows: If formula_9, then formula_10 does not have models. By the contrapositive of Henkin's, then formula_10 is syntactically inconsistent. So a contradiction (formula_12) is provable from formula_10 in the deductive system. Hence formula_14, and then by the properties of the deductive system, formula_1.\n\nThe Model Existence Theorem and its proof can be formalized in the framework of Peano arithmetic. Precisely, we can systematically define a model of any consistent effective first-order theory \"T\" in Peano arithmetic by interpreting each symbol of \"T\" by an arithmetical formula whose free variables are the arguments of the symbol. However, the definition expressed by this formula is not recursive.\n\nAn important consequence of the completeness theorem is that it is possible to recursively enumerate the semantic consequences of any effective first-order theory, by enumerating all the possible formal deductions from the axioms of the theory, and use this to produce an enumeration of their conclusions. \n\nThis comes in contrast with the direct meaning of the notion of semantic consequence, that quantifies over all structures in a particular language, which is clearly not a recursive definition.\n\nAlso, it makes the concept of \"provability,\" and thus of \"theorem,\" a clear concept that only depends on the chosen system of axioms of the theory, and not on the choice of a proof system.\n\nGödel's incompleteness theorem, another celebrated result, shows that there are inherent limitations in what can be achieved with formal proofs in mathematics. The name for the incompleteness theorem refers to another meaning of \"complete\" (see model theory – Using the compactness and completeness theorems).\n\nIt shows that in any consistent effective theory \"T\" containing Peano arithmetic (PA), the formula \"C\" expressing the consistency of \"T\" cannot be proven within \"T\".\n\nApplying the completeness theorem to this result, gives the existence of a model of \"T\" where the formula \"C\" is false. Such a model (precisely, the set of \"natural numbers\" it contains) is necessarily non-standard, as it contains the code number of a proof of a contradiction of \"T\".\nBut \"T\" is consistent when viewed from the outside. Thus this code number of a proof of contradiction of \"T\" must be a non-standard number.\n\nIn fact, the model of \"any\" theory containing PA obtained by the systematic construction of the arithmetical model existence theorem, is \"always\" non-standard with a non-equivalent provability predicate and a non-equivalent way to interpret its own construction, so that this construction is non-recursive (as recursive definitions would be unambiguous).\n\nAlso, there is no recursive non-standard model of PA.\n\nThe completeness theorem and the compactness theorem are two cornerstones of first-order logic. While neither of these theorems can be proven in a completely effective manner, each one can be effectively obtained from the other.\n\nThe compactness theorem says that if a formula φ is a logical consequence of a (possibly infinite) set of formulas Γ then it is a logical consequence of a finite subset of Γ. This is an immediate consequence of the completeness theorem, because only a finite number of axioms from Γ can be mentioned in a formal deduction of φ, and the soundness of the deductive system then implies φ is a logical consequence of this finite set. This proof of the compactness theorem is originally due to Gödel.\n\nConversely, for many deductive systems, it is possible to prove the completeness theorem as an effective consequence of the compactness theorem.\n\nThe ineffectiveness of the completeness theorem can be measured along the lines of reverse mathematics. When considered over a countable language, the completeness and compactness theorems are equivalent to each other and equivalent to a weak form of choice known as weak König's lemma, with the equivalence provable in RCA (a second-order variant of Peano arithmetic restricted to induction over Σ formulas). Weak König's lemma is provable in ZF, the system of Zermelo–Fraenkel set theory without axiom of choice, and thus the completeness and compactness theorems for countable languages are provable in ZF. However the situation is different when the language is of arbitrary large cardinality since then, though the completeness and compactness theorems remain provably equivalent to each other in ZF, they are also provably equivalent to a weak form of the axiom of choice known as the ultrafilter lemma. In particular, no theory extending ZF can prove either the completeness or compactness theorems over arbitrary (possibly uncountable) languages without also proving the ultrafilter lemma on a set of same cardinality, knowing that on countable sets, the ultrafilter lemma becomes equivalent to weak König's lemma.\n\nThe completeness theorem is a central property of first-order logic that does not hold for all logics. Second-order logic, for example, does not have a completeness theorem for its standard semantics (but does have the completeness property for Henkin semantics), and the set of logically-valid formulas in second-order logic is not recursively enumerable. The same is true of all higher-order logics. It is possible to produce sound deductive systems for higher-order logics, but no such system can be complete.\n\nLindström's theorem states that first-order logic is the strongest (subject to certain constraints) logic satisfying both compactness and completeness.\n\nA completeness theorem can be proved for modal logic or intuitionistic logic with respect to Kripke semantics.\n\nGödel's original proof of the theorem proceeded by reducing the problem to a special case for formulas in a certain syntactic form, and then handling this form with an \"ad hoc\" argument.\n\nIn modern logic texts, Gödel's completeness theorem is usually proved with Henkin's proof, rather than with Gödel's original proof. Henkin's proof directly constructs a term model for any consistent first-order theory. James Margetson (2004) developed a computerized formal proof using the Isabelle theorem prover. Other proofs are also known.\n\n\n"}
{"id": "47650504", "url": "https://en.wikipedia.org/wiki?curid=47650504", "title": "Hamiltonian coloring", "text": "Hamiltonian coloring\n\nHamiltonian coloring is a type of graph coloring. Hamiltonian coloring uses a concept called detour distance between two vertices of the graph. It has many applications in different areas of science and technology.\n\nThe distance between two vertices in a graph is defined as the minimum of lengths of paths connecting those vertices. The detour distance between two vertices, say, u and v is defined as the length of the longest u-v path in the graph. In the case of a tree the detour distance between any two vertices is same as the distance between the two vertices.\n"}
{"id": "1268560", "url": "https://en.wikipedia.org/wiki?curid=1268560", "title": "Hankel contour", "text": "Hankel contour\n\nIn mathematics, a Hankel contour is a path in the complex plane which extends from \n[∞,δ], around the origin counter clockwise and back to\n[∞,−δ], where δ is an arbitrarily small positive number. The contour thus remains arbitrarily close to the real axis but without crossing the real axis except for negative values of \"x\".\n\nUse of Hankel contours is one of the methods of contour integration. This type of path for contour integrals was first used by Hermann Hankel in his investigations of the Gamma function.\n\nThe mirror image extending from −∞, circling the origin clockwise, and returning\nto −∞ is also called a Hankel contour.\n"}
{"id": "22616744", "url": "https://en.wikipedia.org/wiki?curid=22616744", "title": "Herz–Schur multiplier", "text": "Herz–Schur multiplier\n\nIn the mathematical field of representation theory, a Herz–Schur multiplier (named after Carl S. Herz and Issai Schur) is a special kind of mapping from a group to the field of complex numbers.\n\nLet Ψ be a mapping of a group \"G\" to the complex numbers. It is a Herz–Schur multiplier if the induced map Ψ: \"N\"(\"G\") → \"N\"(\"G\") is a completely positive map, where \"N\"(\"G\") is the closure of the span \"M\" of the image of λ in \"B\"(\"ℓ\"(\"G\")) with respect to the weak topology, λ is the left regular representation of \"G\" and Ψ is on \"M\" defined as\n\n\n"}
{"id": "2974863", "url": "https://en.wikipedia.org/wiki?curid=2974863", "title": "Idempotent matrix", "text": "Idempotent matrix\n\nIn linear algebra, an idempotent matrix is a matrix which, when multiplied by itself, yields itself. That is, the matrix \"M\" is idempotent if and only if \"MM\" = \"M\". For this product \"MM\" to be defined, \"M\" must necessarily be a square matrix. Viewed this way, idempotent matrices are idempotent elements of matrix rings.\n\nExamples of a formula_1 and a formula_2 idempotent matrix are formula_3 and formula_4, respectively.\n\nIf a matrix formula_5 is idempotent, then\n\nThus a necessary condition for a 2 × 2 matrix to be idempotent is that either it is diagonal or its trace equals 1.\nNotice that, for idempotent diagonal matrices, formula_16 and formula_17 must be either 1 or 0.\n\nIf \"b\" = \"c\", the matrix formula_18 will be idempotent provided formula_19 so \"a\" satisfies the quadratic equation\n\nwhich is a circle with center (1/2, 0) and radius 1/2. In terms of an angle θ,\n\nHowever, \"b\" = \"c\" is not a necessary condition: any matrix\n\nWith the exception of the identity matrix, an idempotent matrix is singular; that is, its number of independent rows (and columns) is less than its number of rows (and columns). This can be seen from writing formula_25, assuming that has full rank (is non-singular), and pre-multiplying by formula_26 to obtain formula_27.\n\nWhen an idempotent matrix is subtracted from the identity matrix, the result is also idempotent. This holds since \n\nA matrix is idempotent if and only if for all positive integers n, formula_28. The 'if' direction trivially follows by taking formula_29. The 'only if' part can be shown using proof by induction. Clearly we have the result for formula_30, as formula_31. Suppose that formula_32. Then, formula_33, as required. Hence by the principle of induction, the result follows.\n\nAn idempotent matrix is always diagonalizable and its eigenvalues are either 0 or 1. The trace of an idempotent matrix — the sum of the elements on its main diagonal — equals the rank of the matrix and thus is always an integer. This provides an easy way of computing the rank, or alternatively an easy way of determining the trace of a matrix whose elements are not specifically known (which is helpful in statistics, for example, in establishing the degree of bias in using a sample variance as an estimate of a population variance).\n\nIdempotent matrices arise frequently in regression analysis and econometrics. For example, in ordinary least squares, the regression problem is to choose a vector of coefficient estimates so as to minimize the sum of squared residuals (mispredictions) \"e\": in matrix form,\n\nwhere \"y\" is a vector of dependent variable observations, and \"X\" is a matrix each of whose columns is a column of observations on one of the independent variables. The resulting estimator is\n\nwhere superscript \"T\" indicates a transpose, and the vector of residuals is\n\nHere both \"M\" and formula_37(the latter being known as the hat matrix) are idempotent and symmetric matrices, a fact which allows simplification when the sum of squared residuals is computed:\n\nThe idempotency of \"M\" plays a role in other calculations as well, such as in determining the variance of the estimator formula_39.\n\nAn idempotent linear operator \"P\" is a projection operator on the range space along its null space . \"P\" is an orthogonal projection operator if and only if it is idempotent and symmetric.\n\n"}
{"id": "20787202", "url": "https://en.wikipedia.org/wiki?curid=20787202", "title": "International Conference on Software Engineering and Formal Methods", "text": "International Conference on Software Engineering and Formal Methods\n\nThe International Conference on Software Engineering and Formal Methods (SEFM) is an international academic conference in the field of software engineering.\n\nUntil 2002, SEFM was a workshop; it then became a full international conference. It is sponsored by the IEEE Computer Society. The \"1st IEEE International Conferences on Software Engineering and Formal Methods\" (SEFM 2003) was held at Brisbane, Australia in September 2003. Submissions originated from 22 different countries. As well as IEEE-CS, supporters for SEFM 2003 included the Australian Computer Society (ACS), Boeing Australia, and the Italian Embassy in Canberra.\n\nThe proceedings for the conference are published by the Springer Science+Business Media in LNCS since 2011. Previously, the proceedings were published by IEEE.\n\nSEFM aims to bring together practitioners and researchers from academia, industry, and government, to advance the state of the art in formal methods, to help in their large-scale application in the software industry, and to encourage their integration with other practical software engineering methods.\n\nThe conferences are often held in the Asia and Pacific regions and specifically in developing countries. An important aim of the SEFM conferences is to encourage research cooperation between developing countries and industrialized countries. SEFM 2010 was in Pisa, Italy. SEFM 2013 was in Madrid, Spain. SEFM 2014 takes place in Grenoble, France \n\nThe SEFM conference series is included on the DBLP online publications database. Revised selected papers sometimes appear as special journal issues.\n\n"}
{"id": "31640999", "url": "https://en.wikipedia.org/wiki?curid=31640999", "title": "International Institute for Advanced Studies in Systems Research and Cybernetics", "text": "International Institute for Advanced Studies in Systems Research and Cybernetics\n\nThe International Institute for Advanced Studies is a non-profit educational organization committed to the development and promotion of Cybernetics and Systems Research and the advancement of interdisciplinary studies in the sciences, engineering, arts and humanities.\n\nIIAS hosts an annual symposium in Baden-Baden, Germany, where researchers from around the world submit and share their papers on topics ranging from artificial intelligence and nanotechnology to risk analysis.\n\n"}
{"id": "341810", "url": "https://en.wikipedia.org/wiki?curid=341810", "title": "Joseph Liouville", "text": "Joseph Liouville\n\nJoseph Liouville FRS FRSE FAS (; ; 24 March 1809 – 8 September 1882) was a French mathematician.\n\nHe was born in Saint-Omer in France on 24 March 1809.\n\nLiouville graduated from the École Polytechnique in 1827. After some years as an assistant at various institutions including the École Centrale Paris, he was appointed as professor at the École Polytechnique in 1838. He obtained a chair in mathematics at the Collège de France in 1850 and a chair in mechanics at the Faculté des Sciences in 1857.\n\nBesides his academic achievements, he was very talented in organisational matters. Liouville founded the \"Journal de Mathématiques Pures et Appliquées\" which retains its high reputation up to today, in order to promote other mathematicians' work. He was the first to read, and to recognize the importance of, the unpublished work of Évariste Galois which appeared in his journal in 1846. Liouville was also involved in politics for some time, and he became a member of the Constituting Assembly in 1848. However, after his defeat in the legislative elections in 1849, he turned away from politics.\n\nLiouville worked in a number of different fields in mathematics, including number theory, complex analysis, differential geometry and topology, but also mathematical physics and even astronomy. He is remembered particularly for Liouville's theorem. In number theory, he was the first to prove the existence of transcendental numbers by a construction using continued fractions (Liouville numbers). In mathematical physics, Liouville made two fundamental contributions: the Sturm–Liouville theory, which was joint work with Charles François Sturm, and is now a standard procedure to solve certain types of integral equations by developing into eigenfunctions, and the fact (also known as Liouville's theorem) that time evolution is measure preserving for a Hamiltonian system. In Hamiltonian dynamics, Liouville also introduced the notion of action-angle variables as a description of completely integrable systems. The modern formulation of this is sometimes called the Liouville–Arnold theorem, and the underlying concept of integrability is referred to as Liouville integrability.\n\nIn 1851, he was elected a foreign member of the Royal Swedish Academy of Sciences.\n\nThe crater Liouville on the Moon is named after him. So is the Liouville function, an important function in number theory.\n\n\n\n"}
{"id": "929502", "url": "https://en.wikipedia.org/wiki?curid=929502", "title": "Kantorovich inequality", "text": "Kantorovich inequality\n\nIn mathematics, the Kantorovich inequality is a particular case of the Cauchy–Schwarz inequality, which is itself a generalization of the triangle inequality.\n\nThe triangle inequality states that the length of two sides of any triangle, added together, will be equal to or greater than the length of the third side. In simplest terms, the Kantorovich inequality translates the basic idea of the triangle inequality into the terms and notational conventions of linear programming. (See vector space, inner product, and normed vector space for other examples of how the basic ideas inherent in the triangle inequality—line segment and distance—can be generalized into a broader context.)\n\nMore formally, the Kantorovich inequality can be expressed this way:\n\nThe Kantorovich inequality is used in convergence analysis; it bounds the convergence rate of Cauchy's steepest descent.\n\nEquivalents of the Kantorovich inequality have arisen in a number of different fields. For instance, the Cauchy–Schwarz–Bunyakovsky inequality and the Wielandt inequality are equivalent to the Kantorovich inequality and all of these are, in turn, special cases of the Hölder inequality.\n\nThe Kantorovich inequality is named after Soviet economist, mathematician, and Nobel Prize winner Leonid Kantorovich, a pioneer in the field of linear programming.\n\nThere is also Matrix version of the Kantrovich inequality due to Marshall and Olkin.\n\n\n"}
{"id": "2993692", "url": "https://en.wikipedia.org/wiki?curid=2993692", "title": "Kuiper's theorem", "text": "Kuiper's theorem\n\nIn mathematics, Kuiper's theorem (after Nicolaas Kuiper) is a result on the topology of operators on an infinite-dimensional, complex Hilbert space \"H\". It states that the space GL(\"H\") of invertible bounded endomorphisms of \"H\" is such that all maps from any finite complex \"Y\" to GL(\"H\") are homotopic to a constant, for the norm topology on operators.\n\nA significant corollary, also referred to as \"Kuiper's theorem\", is that this group is weakly contractible, \"ie.\" all its homotopy groups are trivial. This result has important uses in topological K-theory.\n\nFor finite dimensional \"H\", this group would be a complex general linear group and not at all contractible. In fact it is homotopy equivalent to its maximal compact subgroup, the unitary group \"U\" of \"H\". The proof that the complex general linear group and unitary group have the same homotopy type is by the Gram-Schmidt process, or through the matrix polar decomposition, and carries over to the infinite-dimensional case of separable Hilbert space, basically because the space of upper triangular matrices is contractible as can be seen quite explicitly. The underlying phenomenon is that passing to infinitely many dimensions causes much of the topological complexity of the unitary groups to vanish; but see the section on Bott's unitary group, where the passage to infinity is more constrained, and the resulting group has non-trivial homotopy groups.\n\nIt is a surprising fact that the unit sphere, sometimes denoted \"S\", in infinite-dimensional Hilbert space \"H\" is a contractible space, while no finite-dimensional spheres are contractible. This result, certainly known decades before Kuiper's, may have the status of mathematical folklore, but it is quite often cited. In fact more is true: \"S\" is diffeomorphic to \"H\", which is certainly contractible by its convexity. One consequence is that there are smooth counterexamples to an extension of the Brouwer fixed-point theorem to the unit ball in \"H\". The existence of such counter-examples that are homeomorphisms was shown in 1943 by Shizuo Kakutani, who may have first written down a proof of the contractibility of the unit sphere. But the result was anyway essentially known (in 1935 Andrey Nikolayevich Tychonoff showed that the unit sphere was a retract of the unit ball).\n\nThe result on the group of bounded operators was proved by the Dutch mathematician Nicolaas Kuiper, for the case of a separable Hilbert space; the restriction of separability was later lifted. The same result, but for the strong operator topology rather than the norm topology, was published in 1963 by Jacques Dixmier and Adrien Douady. The geometric relationship of the sphere and group of operators is that the unit sphere is a homogeneous space for the unitary group \"U\". The stabiliser of a single vector \"v\" of the unit sphere is the unitary group of the orthogonal complement of \"v\"; therefore the homotopy long exact sequence predicts that all the homotopy groups of the unit sphere will be trivial. This shows the close topological relationship, but is not in itself quite enough, since the inclusion of a point will be a weak homotopy equivalence only, and that implies contractibility directly only for a CW complex. In a paper published two years after Kuiper's, Richard Palais provided technical results on infinite-dimensional manifolds sufficient to resolve this issue.\n\nThere is another infinite-dimensional unitary group, of major significance in homotopy theory, that to which the Bott periodicity theorem applies. It is certainly not contractible. The difference from Kuiper's group can be explained: Bott's group is the subgroup in which a given operator acts non-trivially only on a subspace spanned by the first \"N\" of a fixed orthonormal basis {\"e\"}, for some \"N\", being the identity on the remaining basis vectors.\n\nAn immediate consequence, given the general theory of fibre bundles, is that every Hilbert bundle is a trivial bundle.\n\nThe result on the contractibility of \"S\" gives a geometric construction of classifying spaces for certain groups that act freely it, such as the cyclic group with two elements and the circle group. The unitary group \"U\" in Bott's sense has a classifying space \"BU\" for complex vector bundles (see Classifying space for U(n)). A deeper application coming from Kuiper's theorem is the proof of the Atiyah–Jänich theorem (after Klaus Jänich and Michael Atiyah), stating that the space of Fredholm operators on \"H\", with the norm topology, represents the functor \"K\"(.) of topological (complex) K-theory, in the sense of homotopy theory. This is given by Atiyah.\n\nThe same question may be posed about invertible operators on any Banach space of infinite dimension. Here there are only partial results. Some classical sequence spaces have the same property, namely that the group of invertible operators is contractible. On the other hand, there are examples known where it fails to be a connected space. Where all homotopy groups are known to be trivial, the contractibility in some cases may remain unknown.\n"}
{"id": "33188525", "url": "https://en.wikipedia.org/wiki?curid=33188525", "title": "Maass–Selberg relations", "text": "Maass–Selberg relations\n\nIn mathematics, the Maass–Selberg relations are some relations describing the inner products of truncated real analytic Eisenstein series, that in some sense say that distinct Eisenstein series are orthogonal. introduced the Maass–Selberg relations for the case of real analytic Eisenstein series on the upper half plane. extended the relations to symmetric spaces of rank 1. generalized the Maass–Selberg relations to Eisenstein series of higher rank semisimple group (and named the relations after Maass and Selberg). found some analogous relations between Eisenstein integrals, that he also called Maass–Selberg relations.\n\nInformally, the Maass–Selberg relations say that the inner product of two distinct Eisenstein series is zero. However the integral defining the inner product does not converge, so the Eisenstein series first have to be truncated. The Maass–Selberg relations then say that the inner product of two truncated Eisenstein series is given by a finite sum of elementary factors that depend on the truncation chosen, whose finite part tends to zero as the truncation is removed.\n\n"}
{"id": "33253916", "url": "https://en.wikipedia.org/wiki?curid=33253916", "title": "Mikhail Atallah", "text": "Mikhail Atallah\n\nMikhail Jibrayil (Mike) Atallah is a Lebanese American computer scientist, a distinguished professor of computer science at Purdue University.\n\nAtallah received his bachelor's degree from the American University of Beirut in 1975. He then moved to Johns Hopkins University for his graduate studies, earning a master's degree in 1980 and a Ph.D. in 1982 under the supervision of S. Rao Kosaraju. Since that time he has been a member of the Purdue University faculty.\n\nIn 2001, Atallah co-founded Arxan Technologies, Inc., a provider of internet anti-piracy and anti-tampering software, and in 2007, he became its chief technology officer.\n\nAtallah has published over 200 papers on topics in algorithms and computer security.\n\nAtallah's thesis work was on the subject of parallel algorithms, and he continued working in that area as a faculty member.\nAlgorithmic research by Atallah includes papers on parallel and dynamic computational geometry, finding the symmetries of geometric figures, divide and conquer algorithms, and efficient parallel computations of the Levenshtein distance between pairs of strings. With his student Marina Blanton, Atallah is the editor of the \"Algorithms and Theory of Computation Handbook\" (CRC Press, 2nd ed., 2009, ).\n\nAtallah's more recent research has been in the area of computer security. His work in this area has included techniques for text-based digital watermarking. and the addition of multiple guard points within software as an anti-piracy measure.\n\nIn 2006, Atallah was elected as a fellow of the Association for Computing Machinery for his \"contributions to parallel and distributed computation\". He has also been a fellow of the IEEE since 1997.\n"}
{"id": "31394970", "url": "https://en.wikipedia.org/wiki?curid=31394970", "title": "Miklós Simonovits", "text": "Miklós Simonovits\n\nMiklós Simonovits (4 September 1943 in Budapest) is a Hungarian mathematician who currently works at the Rényi Institute of Mathematics in Budapest and is a member of the Hungarian Academy of Sciences. He is on the advisory board of the journal Combinatorica. He is best known for his work in extremal graph theory and was awarded Széchenyi Prize in 2014. Among other things, he discovered the method of progressive induction which he used to describe graphs which do not contain a predetermined graph and the number of edges is close to maximal. With Lovász, he gave a randomized algorithm using \"O\"(\"n\" log \"n\")\nseparation calls to approximate the volume of a convex body within a fixed relative error.\n\nSimonovits was also one of the most frequent collaborators with Paul Erdős, co-authoring 21 papers with him.\n\nHe began his university studies at the Mathematics department of Eötvös Loránd University in 1962, after winning a silver and bronze medal at the International Mathematics Olympiad in 1961 and 1962 respectively. He got his diploma in mathematics from the university in 1967 and defended his PhD under Vera T. Sós in 1971. He taught as an assistant professor and then associate professor at Eotvos Lorand, from 1971 to 1979, mainly combinatorics and analysis. He joined Alfréd Rényi Institute of Mathematics in 1979. In the coming years, he was appointed as the professor in Discrete mathematics. He was also a visiting professor at a number of foreign institutions in US and Canada. He was also a visiting researcher at Moscow State University, Charles University, Prague, Warsaw University, Denmark and various institutions in India. He was elected as a corresponding member at the Hungarian Academy of Sciences in 2001 and full membership was awarded in 2008.\n\nHis main research interests are Combinatorics, Extremal Graph Theory, Theoretical Computer Science and Random Graphs.\n\nHe discovered the method of progressive induction which he used to describe graphs which do not contain a predetermined graph and the number of edges is close to maximal. With Laszlo Lovász, he gave a randomized algorithm using \"O\"(\"n\" log \"n\")\nseparation calls to approximate the volume of a convex body within a fixed relative error.\n\nHe is a long-time collaborator of Endre Szemeredi and worked with him closely.\n\nSimonovits was also one of the most frequent collaborators with Paul Erdős, co-authoring 21 papers with him.\n\nHis father Simonovits István (1907–1985) was a doctor and a hematologist. He was a member of the Hungarian Academy of Sciences. Beke Anna, his mother, was a mathematics and physics teacher, who also worked in a book publishing company.\n\n\n\n"}
{"id": "48725243", "url": "https://en.wikipedia.org/wiki?curid=48725243", "title": "Mlecchita vikalpa", "text": "Mlecchita vikalpa\n\nMlecchita Vikalpa is one of the 64 arts listed in Vatsyayana's Kamasutra. The list appears in Chapter 3 of Part I of Kamasutra and Mlecchita Vikalpa appears as the 44th item in the list. The term Mlecchita Vikalapa has been translated into English as \"the art of understanding writing in cypher, and the writing of words in a peculiar way\".\n\nMlecchita Vikalpa is the art of secret writing and secret communications. In \"The Codebreakers\", a 1967 book by David Kahn about the history of cryptography, the reference to Mlecchita Vikalpa in Kamasutra is cited as proof of the prevalence of cryptographic methods in ancient India. Though Kamasutra does not have details of the methods by which people of that time practiced this particular form of art, later commentators of Kamasutra have described several methods. For example, Yasodhara in his Jayamangala commentary on Kamasutra gives descriptions of methods known by the names \"Kautilya\" and \"Muladeviya\". The ciphers described in the Jayamangala commentary are substitution ciphers: in Kautiliyam the letter substitutions are based on phonetic relations, and Muladeviya is a simplified version of Kautiliyam. There are also references to other methods for secret communications like Gudhayojya, Gudhapada and Gudhavarna. Some modern writers on cryptography have christened the ciphers alluded to in the Kamasutra as \"Kamasutra cipher\" or \"Vatsyayana cipher\".\n\nThe exact date of the composition of Kamasutra has not been fixed. It is supposed that Vatsyayana must have lived between the sixth and first century BC. However, the date of the Jayamangla commentary has been fixed as between the tenth and thirteenth centuries CE.\n\nThis is a Mlecchita named after Kautilya, the author of the ancient Indian political treatise, the Arthashastra. In this system, the short and long vowels, the anusvara and the spirants are interchanged for the consonants and the conjuncts. The following table shows the substitutions used in the Kautiliyam cipher. The characters not listed in the table are left unchanged.\n\nThere is a simplified form of this scheme known by the name \"Durbodha\".\n\nAnother form of secret writing mentioned in Yasodhara's commentary on Kamasutra is known by the name \"Muladeviya\". This existed both in the spoken form and in the written form. In the written form it is called \"Gudhalekhya\". This form of secret communications were used by kings' spies as well as traders in various geographical locations in India. Also this form of secret communications has been popular among thieves and robbers. However, there were variations in the actual scheme across the various geographical areas. For example, in the erstwhile Travancore Kingdom, spread over a part of present-day Kerala State in India, it was practiced under the name Mulabhadra with some changes from the schemes described by Yashodhara.\n\nThe cipher alphabet of Muladeviya consists of the reciprocal one specified in the table below.\n\nThe great Indian epic Mahabharata contains an incident involving the use of this type of secret talking. Duryodhana was planning to burn Pandavas alive and had made arrangements to send Pandavas to Varanavata. Vidura resorted to secret talk to warn Yudhishthira about the dangers in front of everybody present. Only Yudhishthira could understand the secret message. None others even suspected that it was a warning.\n\nThis is an elementary and trivial method for obscuring the true content of spoken messages and it is popular as a game among children. The idea is to add some unnecessary letters chosen randomly to the beginning or to the end of every word in a sentence. For example, to obscure the sentence \"will visit you tonight\" one may add the letters \"dis\" at the beginning of every word and convey the message as \"diswill disvisit disyou distonight\" the real content of which may not be intelligible to the uninitiated if pronounced rapidly.\n\n"}
{"id": "43679237", "url": "https://en.wikipedia.org/wiki?curid=43679237", "title": "Node influence metric", "text": "Node influence metric\n\nIn graph theory and network analysis, node influence metrics are measures that rank or quantify the influence of every node (also called vertex) within a graph. They are related to centrality indices. Applications include measuring the influence of each person in a social network, understanding the role of infrastructure nodes in transportation networks, the Internet, or urban networks, and the participation of a given node in disease dynamics.\n\nThe traditional approach to understanding node importance is via centrality indicators. Centrality indices are designed to produce a ranking which accurately identifies the most influential nodes. Since the mid 2000s, however, social scientists and network physicists have begun to question the suitability of centrality indices for understanding node influence. Centralities may indicate the most influential nodes, but they are rather less informative for the vast majority of nodes which are not highly influential.\n\nBorgatti and Everett's 2006 review article\nshowed that the accuracy of centrality indices is highly dependent on network topology.\nThis finding has been repeatedly observed since then. (e.g.).\nIn 2012, Bauer and colleagues reminded us that centrality indices only rank nodes but do not quantify the difference between them.\nIn 2013, Sikic and colleagues presented strong evidence that centrality indices considerably underestimate the power of non-hub nodes.\nThe reason is quite clear. The accuracy of a centrality measure depends on network topology, but complex networks have heterogenous topology. Hence a centrality measure which is appropriate for identifying highly influential nodes will most likely be inappropriate for the remainder of the network.\n\nThis has inspired the development of novel methods designed to measure the influence of all network nodes. The most general of these are\nthe accessibility, which uses the diversity of random walks to measure how accessible the rest of the network is from a given start node,\nand the expected force, derived from the expected value of the force of infection generated by a node.\nBoth of these measures can be meaningfully computed from the structure of the network alone.\n\nThe Accessibility is derived from the theory of random walks. It measures the diversity of self-avoiding walks which start from a given node. A walk on a network is a sequence of adjacent vertices; a self-avoiding walk lists visits each vertex at most once. \nThe original work used simulated walks of length 60 to characterize the network of urban streets in a Brazilian city.\nIt was later formalized as a modified form of hierarchical degree which controls for both transmission probabilities and the diversity of walks of a given fixed length.\n\nThe hierarchical degree measures the number of nodes reachable from a start node by performing walks of length formula_1. For a fixed formula_1 and walk type, each of these neighbors is reached with a (potentially different) probability formula_3. \nGiven a vector of such probabilities, the accessibility of node formula_4 at scale formula_1 is defined\n\nThe probabilities can be based on uniform-probability random walks, or additionally modulated by edge weights and/or explicit (per edge) transmission probabilities.\n\nThe accessibility has been shown to reveal community structure in urban networks, corresponds to the number of nodes which can be visited in a defined time period, and is predictive of the outcome of epidemiological SIR model spreading processes on networks with large diameter and low density.\n\nThe expected force measures node influence from an epidemiological perspective. It is the expected value of the force of infection generated by the node after two transmissions.\n\nThe expected force of a node formula_4 is given by\n\nwhere the sum is taken over the set formula_9 of all possible transmission clusters resulting from two transmissions starting from formula_4, and formula_11 is the normalized cluster degree of cluster formula_12.\n\nThe definition naturally extends to directed networks by limiting the enumeration formula_9 by edge direction.\nLikewise, extension to weighted networks, or networks with heterogeneous transmission probabilities, is a matter of adjusting the normalization of formula_11 to include the probability that that cluster forms. \nIt is also possible to use more than two transmissions to define the set formula_9.\n\nThe expected force has been shown to strongly correlate with SI, SIS, and SIR epidemic outcomes over a broad range of network topologies, both simulated and empirical.\nIt has also been used to measure the pandemic potential of world airports, and mentioned in the context of \ndigital payments,\necology, \nfitness,\nand project management.\n\nOthers suggest metrics which explicitly encode the dynamics of a specified process unfolding on the network.\nThe dynamic influence is the proportion of infinite walks starting from each node, where walk steps are scaled such that the linear dynamics of the system are expected to converge to a non-null steady state. The Impact sums, over increasing walk lengths, the probability of transmission to the end node of the walk and that the end node has not been previously visited by a shorter walk.\nWhile both measures well predict the outcome of the dynamical systems they encode, in each case the authors admit that results from one dynamic do not translate to other dynamics.\n\n"}
{"id": "40158142", "url": "https://en.wikipedia.org/wiki?curid=40158142", "title": "Nonlinear system identification", "text": "Nonlinear system identification\n\nSystem identification is a method of identifying or measuring the mathematical model of a system from measurements of the system inputs and outputs. The applications of system identification include any system where the inputs and outputs can be measured and include industrial processes, control systems, economic data, biology and the life sciences, medicine, social systems and many more.\n\nA nonlinear system is defined as any system that is not linear, that is any system that does not satisfy the superposition principle. This negative definition tends to obscure that there are very many different types of nonlinear systems. Historically, system identification for nonlinear systems has developed by focusing on specific classes of system and can be broadly categorised into five basic approaches, each defined by a model class:\n\nThere are four steps to be followed for system identification: data gathering, model postulate, parameter identification and model validation. Data gathering is considered as the first and essential part in identification terminology, used as the input for the model which is prepared later. It consists of selecting an appropriate data set, pre-processing and processing. It involves the implementation of the known algorithms together with the transcription of flight tapes, data storage and data management, calibration, processing, analysis and presentation. Moreover, model validation is necessary to gain confidence in, or reject, a particular model. In particular, the parameter estimation and the model validation are integral parts of the system identification. Validation refers to the process of confirming the conceptual model and demonstrating an adequate correspondence between the computational results of the model and the actual data.\n\nThe early work was dominated by methods based on the Volterra series, which in the discrete time case can be expressed as\n\nwhere \"u\"(\"k\"), \"y\"(\"k\"); \"k\" = 1, 2, 3, … are the measured input and output respectively and formula_2 is the \"l\"th-order Volterra kernel, or \"l\"th-order nonlinear impulse response. The Volterra series is an extension of the linear convolution integral. Most of the earlier identification algorithms assumed that just the first two, linear and quadratic, Volterra kernels are present and used special inputs such as Gaussian white noise and correlation methods to identify the two Volterra kernels. In most of these methods the input has to be Gaussian and white which is a severe restriction for many real processes. These results were later extended to include the first three Volterra kernels, to allow different inputs, and other related developments including the Wiener series. A very important body of work was developed by Wiener, Lee, Bose and colleagues at MIT from the 1940s to the 1960s including the famous Lee and Schetzen method. While these methods are still actively studied today there are several basic restrictions. These include the necessity of knowing the number of Volterra series terms a priori, the use of special inputs, and the large number of estimates that have to be identified. For example for a system where the first order Volterra kernel is described by say 30 samples, 30x30 points will be required for the second order kernel, 30x30x30 for the third order and so on and hence the amount of data required to provide good estimates becomes excessively large. These numbers can be reduced by exploiting certain symmetries but the requirements are still excessive irrespective of what algorithm is used for the identification.\n\nBecause of the problems of identifying Volterra models other model forms were investigated as a basis for system identification for nonlinear systems. Various forms of block structured nonlinear models have been introduced or re-introduced. The Hammerstein model consists of a static single valued nonlinear element followed by a linear dynamic element. The Wiener model is the reverse of this combination so that the linear element occurs before the static nonlinear characteristic. The Wiener-Hammerstein model consists of a static nonlinear element sandwiched between two dynamic linear elements, and several other model forms are available. All these models can be represented by a Volterra series but in this case the Volterra kernels take on a special form in each case. Identification consists of correlation based and parameter estimation methods. The correlation methods exploit certain properties of these systems, which means that if specific inputs are used, often white Gaussian noise, the individual elements can be identified one at a time. This results in manageable data requirements and the individual blocks can sometimes be related to components in the system under study.\n\nMore recent results are based on parameter estimation and neural network based solutions. Many results have been introduced and these systems continue to be studied in depth. One problem is that these methods are only applicable to a very special form of model in each case and usually this model form has to be known prior to identification.\n\nArtificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced.\nNeural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications.\nThere are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models. \nNeural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties. \nNeural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.\n\nThe nonlinear autoregressive moving average model with exogenous inputs (NARMAX model) can represent a wide class of nonlinear systems, and is defined as\n\nwhere \"y\"(\"k\"), \"u\"(\"k\") and \"e\"(\"k\") are the system output, input, and noise sequences respectively; formula_4, formula_5, and formula_6 are the maximum lags for the system output, input and noise; F[•] is some nonlinear function, d is a time delay typically set to \"d\" = 1.The model is essentially an expansion of past inputs, outputs and noise terms. Because the noise is modelled explicitly, unbiased estimates of the system model can be obtained in the presence of unobserved highly correlated and nonlinear noise.\nThe Volterra, the block structured models and many neural network architectures can all be considered as subsets of the NARMAX model. Since NARMAX was introduced, by proving what class of nonlinear systems can be represented by this model, many results and algorithms have been derived based around this description. Most of the early work was based on polynomial expansions of the NARMAX model. These are still the most popular methods today but other more complex forms based on wavelets and other expansions have been introduced to represent severely nonlinear and highly complex nonlinear systems. A significant proportion of nonlinear systems can be represented by a NARMAX model including systems with exotic behaviours such as chaos, bifurcations, and subharmonics.\nWhile NARMAX started as the name of a model it has now developed into a philosophy of nonlinear system identification. The NARMAX approach consists of several steps:\n\n\nStructure detection forms the most fundamental part of NARMAX. For example a NARMAX model which consists of one lagged input and one lagged output term, three lagged noise terms, expanded as a cubic polynomial would consist of fifty six possible candidate terms. This number of candidate terms arises because the expansion by definition includes all possible combinations within the cubic expansion. Naively proceeding to estimate a model which includes all these terms and then pruning will cause numerical and computational problems and should always be avoided. However, only a few terms are often important in the model. Structure detection, which aims to select terms one at a time, is therefore critically important. These objectives can easily be achieved by using the Orthogonal Least Squares algorithm and its derivatives to select the NARMAX model terms one at a time. These ideas can also be adapted for pattern recognition and feature selection and provide an alternative to principal component analysis but with the advantage that the features are revealed as basis functions that are easily related back to the original problem. \nNARMAX methods are designed to do far more than to just find the best approximating model. System identification can be divided into two aims. The first involves approximation where the key aim is to develop a model that approximates the data set such that good predictions can be made. There are many applications where this approach is appropriate, for example in time series prediction of the weather, stock prices, speech, target tracking, pattern classification etc. In such applications the form of the model is not that important. The objective is to find an approximation scheme which produces the minimum prediction errors. A second objective of system identification, which includes the first objective as a subset, involves much more than just finding a model to achieve the best mean squared errors. This second aim is why the NARMAX philosophy was developed and is linked to the idea of finding the simplest model structure. The aim here is to develop models that reproduce the dynamic characteristics of the underlying system, to find the simplest possible model, and if possible to relate this to components and behaviours of the system under study. The core aim of this second approach to identification is therefore to identify and reveal the rule that represents the system. These objectives are relevant to model simulation and control systems design, but increasingly to applications in medicine, neuro science, and the life sciences. Here the aim is to identify models, often nonlinear, that can be used to understand the basic mechanisms of how these systems operate and behave so that we can manipulate and utilise these. NARMAX methods have also been developed in the frequency and spatio-temporal domains.\n\nIn a general situation, it might be the case that some exogenous uncertain disturbance passes through the nonlinear dynamics and influence the outputs. A model class that is general enough to capture this situation is the class of stochastic nonlinear state-space models. A state-space model is usually obtained using first principle laws, such as mechanical, electrical, or thermodynamic physical laws, and the parameters to be identified usually have some physical meaning or significance.\n\nA discrete-time state-space model may be defined by the difference equations:\n\nin which formula_8 is a positive integer referring to time. The functions formula_9 and formula_10 are general nonlinear functions. The first equation is known as the state equation and the second is known as the output equation. All the signals are modeled uing stochastic processes. The process formula_11 is known as the state process, formula_12 and formula_13 are usually assumed independent and mutually independent such that formula_14. The parameter formula_15 is usually a finite-dimensional (real) parameter to be estimated (using experimental data). Observe that the state process does not have to be a physical signal, and it is normally unobserved (not measured). The data set is given as a set of input-output pairs formula_16 for formula_17 for some finite positive integer value formula_18.\n\nUnfortunately, due to the nonlinear transformation of unobserved random variables, the likelihood function of the outputs is analytically intractable; it is given in terms of a multidimensional marginalization integral. Consequently, commonly used parameter estimation methods such as the Maximum Likelihood Method or the Prediction Error Method based on the optimal one-step ahead predictor are analytically intractable. Recently, algorithms based on sequential Monte Carlo methods have been used to approximate the conditional mean of the outputs or, in conjunction with the Expectation-Maximization algorithm, to approximate the maximum likelihood estimator. These methods, albeit asymptotically optimal, are computationally demanding and their use is limited to specific cases where the fundamental limitations of the employed particle filters can be avoided. An alternative solution is to apply the prediction error method using a sub-optimal predictor. The resulting estimator can be shown to be strongly consistent and asymptotically normal and can be evaluated using relatively simple algorithms.\n\n\n"}
{"id": "21506", "url": "https://en.wikipedia.org/wiki?curid=21506", "title": "Numerical analysis", "text": "Numerical analysis\n\nNumerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics). Numerical analysis naturally finds application in all fields of engineering and the physical sciences, but in the 21st century also the life sciences, social sciences, medicine, business and even the arts have adopted elements of scientific computations. As an aspect of mathematics and computer science that generates, analyzes, and implements algorithms, the growth in power and the revolution in computing has raised the use of realistic mathematical models in science and engineering, and complex numerical analysis is required to provide solutions to these more involved models of the world. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.\n\nBefore the advent of modern computers, numerical methods often depended on hand interpolation in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software algorithms for solving differential equations.\n\nOne of the earliest mathematical writings is a Babylonian tablet from the Yale Babylonian Collection (YBC 7289), which gives a sexagesimal numerical approximation of the square root of 2, the length of the diagonal in a unit square. Being able to compute the sides of a triangle (and hence, being able to compute square roots) is extremely important, for instance, in astronomy, carpentry and construction.\n\nNumerical analysis continues this long tradition of practical mathematical calculations. Much like the Babylonian approximation of the square root of 2, modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.\n\nThe overall goal of the field of numerical analysis is the design and analysis of techniques to give approximate but accurate solutions to hard problems, the variety of which is suggested by the following:\n\n\nThe rest of this section outlines several important themes of numerical analysis.\n\nThe field of numerical analysis predates the invention of modern computers by many centuries. Linear interpolation was already in use more than 2000 years ago. Many great mathematicians of the past were preoccupied by numerical analysis, as is obvious from the names of important algorithms like Newton's method, Lagrange interpolation polynomial, Gaussian elimination, or Euler's method.\n\nTo facilitate computations by hand, large books were produced with formulas and tables of data such as interpolation points and function coefficients. Using these tables, often calculated out to 16 decimal places or more for some functions, one could look up values to plug into the formulas given and achieve very good numerical estimates of some functions. The canonical work in the field is the NIST publication edited by Abramowitz and Stegun, a 1000-plus page book of a very large number of commonly used formulas and functions and their values at many points. The function values are no longer very useful when a computer is available, but the large listing of formulas can still be very handy.\n\nThe mechanical calculator was also developed as a tool for hand computation. These calculators evolved into electronic computers in the 1940s, and it was then found that these computers were also useful for administrative purposes. But the invention of the computer also influenced the field of numerical analysis, since now longer and more complicated calculations could be done.\n\nDirect methods compute the solution to a problem in a finite number of steps. These methods would give the precise answer if they were performed in infinite precision arithmetic. Examples include Gaussian elimination, the QR factorization method for solving systems of linear equations, and the simplex method of linear programming. In practice, finite precision is used and the result is an approximation of the true solution (assuming stability).\n\nIn contrast to direct methods, iterative methods are not expected to terminate in a finite number of steps. Starting from an initial guess, iterative methods form successive approximations that converge to the exact solution only in the limit. A convergence test, often involving the residual, is specified in order to decide when a sufficiently accurate solution has (hopefully) been found. Even using infinite precision arithmetic these methods would not reach the solution within a finite number of steps (in general). Examples include Newton's method, the bisection method, and Jacobi iteration. In computational matrix algebra, iterative methods are generally needed for large problems.\n\nIterative methods are more common than direct methods in numerical analysis. Some methods are direct in principle but are usually used as though they were not, e.g. GMRES and the conjugate gradient method. For these methods the number of steps needed to obtain the exact solution is so large that an approximation is accepted in the same manner as for an iterative method.\n\nFurthermore, continuous problems must sometimes be replaced by a discrete problem whose solution is known to approximate that of the continuous problem; this process is called \"discretization\". For example, the solution of a differential equation is a function. This function must be represented by a finite amount of data, for instance by its value at a finite number of points at its domain, even though this domain is a continuum.\n\nThe study of errors forms an important part of numerical analysis. There are several ways in which error can be introduced in the solution of the problem.\n\nRound-off errors arise because it is impossible to represent all real numbers exactly on a machine with finite memory (which is what all practical digital computers are).\n\nTruncation errors are committed when an iterative method is terminated or a mathematical procedure is approximated, and the approximate solution differs from the exact solution. Similarly, discretization induces a discretization error because the solution of the discrete problem does not coincide with the solution of the continuous problem. For instance, in the iteration in the sidebar to compute the solution of formula_1, after 10 or so iterations, we conclude that the root is roughly 1.99 (for example). We therefore have a truncation error of 0.01.\n\nOnce an error is generated, it will generally propagate through the calculation. For instance, we have already noted that the operation + on a calculator (or a computer) is inexact. It follows that a calculation of the type is even more inexact.\n\nWhat does it mean when we say that the truncation error is created when we approximate a mathematical procedure? We know that to integrate a function exactly requires one to find the sum of infinite trapezoids. But numerically one can find the sum of only finite trapezoids, and hence the approximation of the mathematical procedure. Similarly, to differentiate a function, the differential element approaches zero but numerically we can only choose a finite value of the differential element.\n\nNumerical stability is an important notion in numerical analysis. An algorithm is called \"numerically stable\" if an error, whatever its cause, does not grow to be much larger during the calculation. This happens if the problem is \"well-conditioned\", meaning that the solution changes by only a small amount if the problem data are changed by a small amount. To the contrary, if a problem is \"ill-conditioned\", then any small error in the data will grow to be a large error.\n\nBoth the original problem and the algorithm used to solve that problem can be \"well-conditioned\" and/or \"ill-conditioned\", and any combination is possible.\n\nSo an algorithm that solves a well-conditioned problem may be either numerically stable or numerically unstable. An art of numerical analysis is to find a stable algorithm for solving a well-posed mathematical problem. For instance, computing the square root of 2 (which is roughly 1.41421) is a well-posed problem. Many algorithms solve this problem by starting with an initial approximation \"x\" to formula_2, for instance \"x\" = 1.4, and then computing improved guesses \"x\", \"x\", etc. One such method is the famous Babylonian method, which is given by \"x\" = \"x\"/2 + 1/\"x\". Another method, which we will call Method X, is given by \"x\" = (\"x\" − 2) + \"x\". We have calculated a few iterations of each scheme in table form below, with initial guesses \"x\" = 1.4 and \"x\" = 1.42.\n\nObserve that the Babylonian method converges quickly regardless of the initial guess, whereas Method X converges extremely slowly with initial guess \"x\" = 1.4 and diverges for initial guess \"x\" = 1.42. Hence, the Babylonian method is numerically stable, while Method X is numerically unstable.\n\n\nThe field of numerical analysis includes many sub-disciplines. Some of the major ones are:\n\nOne of the simplest problems is the evaluation of a function at a given point. The most straightforward approach, of just plugging in the number in the formula is sometimes not very efficient. For polynomials, a better approach is using the Horner scheme, since it reduces the necessary number of multiplications and additions. Generally, it is important to estimate and control round-off errors arising from the use of floating point arithmetic.\n\nInterpolation solves the following problem: given the value of some unknown function at a number of points, what value does that function have at some other point between the given points?\n\nExtrapolation is very similar to interpolation, except that now we want to find the value of the unknown function at a point which is outside the given points.\n\nRegression is also similar, but it takes into account that the data is imprecise. Given some points, and a measurement of the value of some function at these points (with an error), we want to determine the unknown function. The least squares-method is one popular way to achieve this.\n\nAnother fundamental problem is computing the solution of some given equation. Two cases are commonly distinguished, depending on whether the equation is linear or not. For instance, the equation formula_7 is linear while formula_8 is not.\n\nMuch effort has been put in the development of methods for solving systems of linear equations. Standard direct methods, i.e., methods that use some matrix decomposition are Gaussian elimination, LU decomposition, Cholesky decomposition for symmetric (or hermitian) and positive-definite matrix, and QR decomposition for non-square matrices. Iterative methods such as the Jacobi method, Gauss–Seidel method, successive over-relaxation and conjugate gradient method are usually preferred for large systems. General iterative methods can be developed using a matrix splitting.\n\nRoot-finding algorithms are used to solve nonlinear equations (they are so named since a root of a function is an argument for which the function yields zero). If the function is differentiable and the derivative is known, then Newton's method is a popular choice. Linearization is another technique for solving nonlinear equations.\n\nSeveral important problems can be phrased in terms of eigenvalue decompositions or singular value decompositions. For instance, the spectral image compression algorithm is based on the singular value decomposition. The corresponding tool in statistics is called principal component analysis.\n\nOptimization problems ask for the point at which a given function is maximized (or minimized). Often, the point also has to satisfy some constraints.\n\nThe field of optimization is further split in several subfields, depending on the form of the objective function and the constraint. For instance, linear programming deals with the case that both the objective function and the constraints are linear. A famous method in linear programming is the simplex method.\n\nThe method of Lagrange multipliers can be used to reduce optimization problems with constraints to unconstrained optimization problems.\n\nNumerical integration, in some instances also known as numerical quadrature, asks for the value of a definite integral. Popular methods use one of the Newton–Cotes formulas (like the midpoint rule or Simpson's rule) or Gaussian quadrature. These methods rely on a \"divide and conquer\" strategy, whereby an integral on a relatively large set is broken down into integrals on smaller sets. In higher dimensions, where these methods become prohibitively expensive in terms of computational effort, one may use Monte Carlo or quasi-Monte Carlo methods (see Monte Carlo integration), or, in modestly large dimensions, the method of sparse grids.\n\nNumerical analysis is also concerned with computing (in an approximate way) the solution of differential equations, both ordinary differential equations and partial differential equations.\n\nPartial differential equations are solved by first discretizing the equation, bringing it into a finite-dimensional subspace. This can be done by a finite element method, a finite difference method, or (particularly in engineering) a finite volume method. The theoretical justification of these methods often involves theorems from functional analysis. This reduces the problem to the solution of an algebraic equation.\n\nSince the late twentieth century, most algorithms are implemented in a variety of programming languages. The Netlib repository contains various collections of software routines for numerical problems, mostly in Fortran and C. Commercial products implementing many different numerical algorithms include the IMSL and NAG libraries; a free-software alternative is the GNU Scientific Library.\n\nThere are several popular numerical computing applications such as MATLAB, TK Solver, S-PLUS, and IDL as well as free and open source alternatives such as FreeMat, Scilab, GNU Octave (similar to Matlab), and IT++ (a C++ library). There are also programming languages such as R (similar to S-PLUS) and Python with libraries such as NumPy, SciPy and SymPy. Performance varies widely: while vector and matrix operations are usually fast, scalar loops may vary in speed by more than an order of magnitude.\n\nMany computer algebra systems such as Mathematica also benefit from the availability of arbitrary precision arithmetic which can provide more accurate results.\n\nAlso, any spreadsheet software can be used to solve simple problems relating to numerical analysis.\n\n\n\nJournals\n\nOnline texts\n\nOnline course material\n"}
{"id": "22330", "url": "https://en.wikipedia.org/wiki?curid=22330", "title": "Octal", "text": "Octal\n\nThe octal numeral system, or oct for short, is the base-8 number system, and uses the digits 0 to 7. Octal numerals can be made from binary numerals by grouping consecutive binary digits into groups of three (starting from the right). For example, the binary representation for decimal 74 is 1001010. Two zeroes can be added at the left: , corresponding the octal digits , yielding the octal representation 112.\n\nIn the decimal system each decimal place is a power of ten. For example:\nIn the octal system each place is a power of eight. For example:\nBy performing the calculation above in the familiar decimal system we see why 112 in octal is equal to 64+8+2 = 74 in decimal.\n\nThe Yuki language in California and the Pamean languages in Mexico have octal systems because the speakers count using the spaces between their fingers rather than the fingers themselves.\n\n\nOctal became widely used in computing when systems such as the PDP-8, ICL 1900 and IBM mainframes employed 12-bit, 24-bit or 36-bit words. Octal was an ideal abbreviation of binary for these machines because their word size is divisible by three (each octal digit represents three binary digits). So four, eight or twelve digits could concisely display an entire machine word. It also cut costs by allowing Nixie tubes, seven-segment displays, and calculators to be used for the operator consoles, where binary displays were too complex to use, decimal displays needed complex hardware to convert radices, and hexadecimal displays needed to display more numerals.\n\nAll modern computing platforms, however, use 16-, 32-, or 64-bit words, further divided into eight-bit bytes. On such systems three octal digits per byte would be required, with the most significant octal digit representing two binary digits (plus one bit of the next significant byte, if any). Octal representation of a 16-bit word requires 6 digits, but the most significant octal digit represents (quite inelegantly) only one bit (0 or 1). This representation offers no way to easily read the most significant byte, because it's smeared over four octal digits. Therefore, hexadecimal is more commonly used in programming languages today, since two hexadecimal digits exactly specify one byte. Some platforms with a power-of-two word size still have instruction subwords that are more easily understood if displayed in octal; this includes the PDP-11 and Motorola 68000 family. The modern-day ubiquitous x86 architecture belongs to this category as well, but octal is rarely used on this platform, although certain properties of the binary encoding of opcodes become more readily apparent when displayed in octal, e.g. the ModRM byte, which is divided into fields of 2, 3, and 3 bits, so octal can be useful in describing these encodings.\n\nOctal is sometimes used in computing instead of hexadecimal, perhaps most often in modern times in conjunction with file permissions under Unix systems (see chmod). It has the advantage of not requiring any extra symbols as digits (the hexadecimal system is base-16 and therefore needs six additional symbols beyond 0–9). It is also used for digital displays.\n\nIn programming languages, octal literals are typically identified with a variety of prefixes, including the digit 0, the letters o or q, the digit–letter combination 0o, or the symbol & or $. In \"Motorola convention\", octal numbers are prefixed with @, whereas a small letter o is added as a postfix following the \"Intel convention\". In DR-DOS and Multiuser DOS various environment variables like $CLS, $ON, $OFF, $HEADER or $FOOTER support an \\nnn octal number notation, and DR-DOS DEBUG utilizes \\ to prefix octal numbers as well.\n\nFor example, the literal 73 (base 8) might be represented as 073, o73, q73, 0o73, \\73, @73, &73, $73 or 73o in various languages.\n\nNewer languages have been abandoning the prefix 0, as decimal numbers are often represented with leading zeroes. The prefix q was introduced to avoid the prefix o being mistaken for a zero, while the prefix 0o was introduced to avoid starting a numerical literal with an alphabetic character (like o or q), since these might cause the literal to be confused with a variable name. The prefix 0o also follows the model set by the prefix 0x used for hexadecimal literals in the C language; it is supported by Haskell, OCaml, Perl 6, Python as of version 3.0, Ruby, Tcl as of version 9, and it is intended to be supported by ECMAScript 6 (the prefix 0 has been discouraged in ECMAScript 3 and dropped in ECMAScript 5).\n\nOctal numbers that are used in some programming languages (C, Perl, PostScript…) for textual/graphical representations of byte strings when some byte values (unrepresented in a code page, non-graphical, having special meaning in current context or otherwise undesired) have to be to escaped as \\nnn. Octal representation may be particularly handy with non-ASCII bytes of UTF-8, which encodes groups of 6 bits, and where any start byte has octal value \\3nn and any continuation byte has octal value \\2nn.\n\nTransponders in aircraft transmit a code, expressed as a four-octal-digit number, when interrogated by ground radar. This code is used to distinguish different aircraft on the radar screen.\n\nTo convert integer decimals to octal, divide the original number by the largest possible power of 8 and divide the remainders by successively smaller powers of 8 until the power is 1. The octal representation is formed by the quotients, written in the order generated by the algorithm.\nFor example, to convert 125 to octal:\nTherefore, 125 = 175.\n\nAnother example:\nTherefore, 900 = 1604.\n\nTo convert a decimal fraction to octal, multiply by 8; the integer part of the result is the first digit of the octal fraction. Repeat the process with the fractional part of the result, until it is null or within acceptable error bounds.\n\nExample: Convert 0.1640625 to octal:\nTherefore, 0.1640625 = 0.124.\n\nThese two methods can be combined to handle decimal numbers with both integer and fractional parts, using the first on the integer part and the second on the fractional part.\n\nTo convert integer decimals to octal, prefix the number with \"0.\". Perform the following steps for as long as digits remain on the right side of the radix:\nDouble the value to the left side of the radix, using \"octal\" rules, move the radix point one digit rightward, and then place the doubled value underneath the current value so that the radix points align. If the moved radix point crosses over a digit that is 8 or 9, convert it to 0 or 1 and add the carry to the next leftward digit of the current value. \"Add\" \"octally\" those digits to the left of the radix and simply drop down those digits to the right, without modification.\n\nExample:\nTo convert a number to decimal, use the formula that defines its base-8 representation:\n\nIn this formula, is an individual octal digit being converted, where is the position of the digit (counting from 0 for the right-most digit).\n\nExample: Convert 764 to decimal:\n\nFor double-digit octal numbers this method amounts to multiplying the lead digit by 8 and adding the second digit to get the total.\n\nExample: 65 = 6 × 8 + 5 = 53\n\nTo convert octals to decimals, prefix the number with \"0.\". Perform the following steps for as long as digits remain on the right side of the radix: Double the value to the left side of the radix, using \"decimal\" rules, move the radix point one digit rightward, and then place the doubled value underneath the current value so that the radix points align. \"Subtract\" \"decimally\" those digits to the left of the radix and simply drop down those digits to the right, without modification.\n\nExample:\nTo convert octal to binary, replace each octal digit by its binary representation.\nExample: Convert 51 to binary:\nTherefore, 51 = 101 001.\n\nThe process is the reverse of the previous algorithm. The binary digits are grouped by threes, starting from the least significant bit and proceeding to the left and to the right. Add leading zeroes (or trailing zeroes to the right of decimal point) to fill out the last group of three if necessary. Then replace each trio with the equivalent octal digit.\n\nFor instance, convert binary 1010111100 to octal:\n\nTherefore, 1010111100 = 1274.\n\nConvert binary 11100.01001 to octal:\n\nTherefore, 11100.01001 = 34.22.\n\nThe conversion is made in two steps using binary as an intermediate base. Octal is converted to binary and then binary to hexadecimal, grouping digits by fours, which correspond each to a hexadecimal digit.\n\nFor instance, convert octal 1057 to hexadecimal:\n\nTherefore, 1057 = 22F.\n\nHexadecimal to octal conversion proceeds by first converting the hexadecimal digits to 4-bit binary values, then regrouping the binary bits into 3-bit octal digits.\n\nFor example, to convert 3FA5:\n\nTherefore, 3FA5 = 37645.\n\nDue to having only factors of two, many octal fractions have repeating digits, although these tend to be fairly simple:\n\nThe table below gives the expansions of some common irrational numbers in decimal and octal.\n\n"}
{"id": "31366779", "url": "https://en.wikipedia.org/wiki?curid=31366779", "title": "Opposite group", "text": "Opposite group\n\nIn group theory, a branch of mathematics, an opposite group is a way to construct a group from another group that allows one to define right action as a special case of left action.\n\nMonoids, groups, rings, and algebras can be viewed as categories with a single object. The construction of the opposite category generalizes the opposite group, opposite ring, etc.\n\nLet formula_1 be a group under the operation formula_2. The opposite group of formula_1, denoted formula_4, has the same underlying set as formula_1, and its group operation formula_6 is defined by formula_7.\n\nIf formula_1 is abelian, then it is equal to its opposite group. Also, every group formula_1 (not necessarily abelian) is naturally isomorphic to its opposite group: An isomorphism formula_10 is given by formula_11. More generally, any antiautomorphism formula_12 gives rise to a corresponding isomorphism formula_13 via formula_14, since\n\nLet formula_16 be an object in some category, and formula_17 be a right action. Then formula_18 is a left action defined by formula_19, or formula_20.\n\n\n"}
{"id": "26297589", "url": "https://en.wikipedia.org/wiki?curid=26297589", "title": "Owen's T function", "text": "Owen's T function\n\nIn mathematics, Owen's T function \"T\"(\"h\", \"a\"), named after statistician Donald Bruce Owen, is defined by\n\nThe function was first introduced by Owen in 1956.\n\nThe function \"T\"(\"h\", \"a\") gives the probability of the event (\"X>h\" and 0\"<YSowden, R R and Ashford, J R (1969). \"Computation of the bivariate normal integral\". \"Applied Statististics\", 18, 169–180.</ref> and, from there, in the calculation of multivariate normal distribution probabilities.\nIt also frequently appears in various integrals involving Gaussian functions.\n\nComputer algorithms for the accurate calculation of this function are available; quadrature having been employed since the 1970s. \n\nHere \"Φ(x)\" is the standard normal cumulative density function\nMore properties can be found in the literature.\n\n\n\n"}
{"id": "56625736", "url": "https://en.wikipedia.org/wiki?curid=56625736", "title": "Paden–Kahan subproblems", "text": "Paden–Kahan subproblems\n\nPaden–Kahan subproblems are a set of solved geometric problems which occur frequently in inverse kinematics of common robotic manipulators. Although the set of problems is not exhaustive, it may be used to simplify inverse kinematic analysis for many industrial robots.\n\nFor a structure equation defined by the product of exponentials method, Paden–Kahan subproblems may be used to simplify and solve the inverse kinematics problem. Notably, the matrix exponentials are non-commutative.\n\nGenerally, subproblems are applied to solve for particular points in the inverse kinematics problem (e.g., the intersection of joint axes) in order to solve for joint angles.\n\nSimplification is accomplished by the principle that a rotation has no effect on a point lying on its axis. For example, if the point formula_1 is on the axis of a revolute twist formula_2, its position is unaffected by the actuation of the twist. To wit:formula_3\n\nThus, for a structure equationformula_4where formula_5, formula_6 and formula_7 are all zero-pitch twists, applying both sides of the equation to a point formula_1 which is on the axis of formula_7 (but not on the axes of formula_5 or formula_6) yieldsformula_12By the cancellation of formula_7, this yieldsformula_14which, if formula_5 and formula_6 intersect, may be solved by Subproblem 2.\n\nIn some cases, the problem may also be simplified by subtracting a point from both sides of the equation and taking the norm of the result.\n\nFor example, to solveformula_4for formula_7, where formula_5 and formula_6 intersect at the point formula_21, both sides of the equation may be applied to a point formula_1 that is not on the axis of formula_7. Subtracting formula_21 and taking the norm of both sides yieldsformula_25 \nThis may be solved using Subproblem 3.\n\nEach subproblem is presented as an algorithm based on a geometric proof. Code to solve a given subproblem, which should be written to account for cases with multiple solutions or no solution, may be integrated into inverse kinematics algorithms for a wide range of robots.\n\nIn this subproblem, a point formula_1 is rotated around a given axis formula_2 such that it coincides with a second point formula_21.\n\nLet formula_33 be a point on the axis of formula_2. Define the vectors formula_35 and formula_36. Since formula_33 is on the axis of formula_2, formula_39 Therefore, formula_40\n\nNext, the vectors formula_41 and formula_42 are defined to be the projections of formula_43 and formula_44 onto the plane perpendicular to the axis of formula_2. For a vector formula_46 in the direction of the axis of formula_2,formula_48andformula_49In the event that formula_50, formula_51 and both points lie on the axis of rotation. The subproblem therefore yields an infinite number of possible solutions in that case.\n\nIn order for the problem to have a solution, it is necessary that the projections of formula_43 and formula_44 onto the formula_54 axis and onto the plane perpendicular to formula_54 have equal lengths. It is necessary to check, to wit, that:formula_56and thatformula_57\n\nIf these equations are satisfied, the value of the joint angle formula_28 may be found using the atan2 function:formula_59Provided that formula_60, this subproblem should yield one solution for formula_28.\n\nThis problem corresponds to rotating formula_1 around the axis of formula_6 by formula_66, then rotating it around the axis of formula_5 by formula_65, so that the final location of formula_1 is coincident with formula_21. (If the axes of formula_5 and formula_6 are coincident, then this problem reduces to Subproblem 1, admitting all solutions such that formula_77.)\n\nProvided that the two axes are not parallel (i.e., formula_78), let formula_79 be a point such that formula_80 In other words, formula_79 represents the point to which formula_1 is rotated around one axis before it is rotated around the other axis to be coincident with formula_21. Each individual rotation is equivalent to Subproblem 1, but it’s necessary to identify one or more valid solutions for formula_79 in order to solve for the rotations.\n\nLet formula_33 be the point of intersection of the two axes:formula_86Define the vectors formula_35, formula_36 and formula_89. Therefore,formula_90\n\nThis implies that formula_91, formula_92, and formula_93. Since formula_94, formula_95 and formula_96 are linearly independent, formula_97 can be written asformula_98\n\nThe values of the coefficients may be solved thus:formula_99formula_100, andformula_101The subproblem yields two solutions in the event that the circles intersect at two points; one solution if the circles are tangential; and no solution if the circles fail to intersect.\n\nIn this problem, a point formula_1 is rotated about an axis formula_2 until the point is a distance formula_104 from a point formula_21. In order for a solution to exist, the circle defined by rotating formula_1 around formula_2 must have a sphere of radius formula_104 centered at formula_21.\n\nLet formula_33 be a point on the axis of formula_2. The vectors formula_35 and formula_36 are defined so thatformula_119\n\nThe projections of formula_43 and formula_44 are formula_122 and formula_123 The “projection” of the line segment defined by formula_104 is found by subtracting the component of formula_125 in the formula_54 direction:formula_127The angle formula_128 between the vectors formula_41 and formula_42 is found using the atan2 function:formula_131The joint angle formula_28 is found by the formulaformula_133This subproblem may yield zero, one, or two solutions, depending on the number of points at which the circle of radius formula_134 intersects the circle of radius formula_135.\n\nThis problem is analogous to Subproblem 2, except that the final point is constrained by distances to two known points.\n"}
{"id": "25958537", "url": "https://en.wikipedia.org/wiki?curid=25958537", "title": "Phyloscan", "text": "Phyloscan\n\nPhyloscan is a web service for DNA sequence analysis that is free and open to all users (without login requirement). For locating matches to a user-specified sequence motif for a regulatory binding site, Phyloscan provides a statistically sensitive scan of user-supplied mixed aligned and unaligned DNA sequence data. Phyloscan's strength is that it brings together \n\n"}
{"id": "30684123", "url": "https://en.wikipedia.org/wiki?curid=30684123", "title": "Ptolemy's table of chords", "text": "Ptolemy's table of chords\n\nThe table of chords, created by the astronomer, geometer, and geographer Ptolemy in Egypt during the 2nd century AD, is a trigonometric table in Book I, chapter 11 of Ptolemy's \"Almagest\", a treatise on mathematical astronomy. It is essentially equivalent to a table of values of the sine function. It was the earliest trigonometric table extensive enough for many practical purposes, including those of astronomy (an earlier table of chords by Hipparchus gave chords only for arcs that were multiples of ). Centuries passed before more extensive trigonometric tables were created. One such table is the \"Canon Sinuum\" created at the end of the 16th century.\n\nA chord of a circle is a line segment whose endpoints are on the circle. Ptolemy used a circle whose diameter is 120. He tabulated the length of a chord whose endpoints are separated by an arc of \"n\" degrees, for \"n\" ranging from to 180 by increments of . In modern notation, the length of the chord corresponding to an arc of \"θ\" degrees is\n\nAs \"θ\" goes from 0 to 180, the chord of a \"θ\"° arc goes from 0 to 120. For tiny arcs, the chord is to the arc angle in degrees as is to 3, or more precisely, the ratio can be made as close as desired to  ≈  by making \"θ\" small enough. Thus, for the arc of °, the chord length is slightly more than the arc angle in degrees. As the arc increases, the ratio of the chord to the arc decreases. When the arc reaches 60°, the chord length is exactly equal to the number of degrees in the arc, i.e. chord 60° = 60. For arcs of more than 60°, the chord is less than the arc, until an arc of 180° is reached, when the chord is only 120.\n\nThe fractional parts of chord lengths were expressed in sexagesimal (base 60) numerals. For example, where the length of a chord subtended by a 112° arc is reported to be 99 29 5, it has a length of\n\nrounded to the nearest .\n\nAfter the columns for the arc and the chord, a third column is labeled \"sixtieths\". For an arc of \"θ\"°, the entry in the \"sixtieths\" column is\n\nThis is the average number of sixtieths of a unit that must be added to chord(\"θ\"°) each time the angle increases by one minute of arc, between the entry for \"θ\"° and that for (\"θ\" + )°. Thus, it is used for linear interpolation. Glowatzki and Göttsche showed that Ptolemy must have calculated chords to five sexigesimal places in order to achieve the degree of accuracy found in the \"sixtieths\" column.\n\nChapter 10 of Book I of the \"Almagest\" presents geometric theorems used for computing chords. Ptolemy used geometric reasoning based on Proposition 10 of Book XIII of Euclid's \"Elements\" to find the chords of 72° and 36°. That Proposition states that if an equilateral pentagon is inscribed in a circle, then the area of the square on the side of the pentagon equals the sum of the areas of the squares on the sides of the hexagon and the decagon inscribed in the same circle.\n\nHe used Ptolemy's theorem on quadrilaterals inscribed in a circle to derive formulas for the chord of a half-arc, the chord of the sum of two arcs, and the chord of a difference of two arcs. The theorem states that for a quadrilateral inscribed in a circle, the product of the lengths of the diagonals equals the sum of the products of the two pairs of lengths of opposite sides. The derivations of trigonometric identities rely on a cyclic quadrilateral in which one side is a diameter of the circle.\n\nTo find the chords of arcs of 1° and ° he used approximations based on Aristarchus's inequality. The inequality states that for arcs \"α\" and \"β\", if 0 < \"β\" < \"α\" < 90°, then\n\nPtolemy showed that for arcs of 1° and °, the approximations correctly give the first two sexigesimal places after the integer part.\n\nLengths of arcs of the circle, in degrees, and the integer parts of chord lengths, were expressed in a base 10 numeral system that used 21 of the letters of the Greek alphabet with the meanings given in the following table, and a symbol, \"∠′ \", that means and a raised circle \"○\" that fills a blank space (effectively representing zero). Two of the letters, labeled \"archaic\" in the table below, had not been in use in the Greek language for some centuries before the \"Almagest\" was written, but were still in use as numerals and musical notes.\nThus, for example, an arc of ° is expressed as \"ρμγ\"∠′. (As the table only reaches 180°, the Greek numerals for 200 and above are not used.)\n\nThe fractional parts of chord lengths required great accuracy, and were given in two columns in the table: The first column gives an integer multiple of , in the range 0–59, the second an integer multiple of  = , also in the range 0–59.\n\nThus in Heiberg's edition of the \"Almagest\" with the table of chords on pages 48–63, the beginning of the table, corresponding to arcs from ° to °, looks like this:\n\nLater in the table, one can see the base-10 nature of the numerals expressing the integer parts of the arc and the chord length. Thus an arc of 85° is written as \"πε\" (\"π\" for 80 and \"ε\" for 5) and not broken down into 60 + 25. The corresponding chord length is 81 plus a fractional part. The integer part begins with \"πα\", likewise not broken into 60 + 21. But the fractional part,  + , is written as \"δ\", for 4, in the column, followed by \"ιε\", for 15, in the column.\nThe table has 45 lines on each of eight pages, for a total of 360 lines.\n\n\n\n"}
{"id": "58209457", "url": "https://en.wikipedia.org/wiki?curid=58209457", "title": "QAMA Calculator", "text": "QAMA Calculator\n\nThe QAMA Calculator is a calculator that requires users to provide a reasonable estimate of the answer before the precise answer is delivered. QAMA stands for Quick Approximate Mental Arithmetic.\n\nInvented by Ilan Samson, it aims to get users to think first by estimating before they get the correct answer. Estimation is seen by many as an essential part of mathematics, and some believe that the presence and popularity of calculators could inhibit the use of estimation skills.\n\nA physical version of the calculator was released for sale in 2014, with apps for smartphones and tablets developed in 2016.\n"}
{"id": "39611193", "url": "https://en.wikipedia.org/wiki?curid=39611193", "title": "Simplicial presheaf", "text": "Simplicial presheaf\n\nIn mathematics, more specifically in homotopy theory, a simplicial presheaf is a presheaf on a site (e.g., the category of topological spaces) taking values in simplicial sets (i.e., a contravariant functor from the site to the category of simplicial sets). Equivalently, a simplicial presheaf is a simplicial object in the category of presheaves on a site. The notion was introduced by A. Joyal in the 1970s. Similarly, a simplicial sheaf on a site is a simplicial object in the category of sheaves on the site.\n\nExample: Let us consider, say, the étale site of a scheme \"S\". Each \"U\" in the site represents the presheaf formula_1. Thus, a simplicial scheme, a simplicial object in the site, represents a simplicial presheaf (in fact, often a simplicial sheaf).\n\nExample: Let \"G\" be a presheaf of groupoids. Then taking nerves section-wise, one obtains a simplicial presheaf formula_2. For example, one might set formula_3. These types of examples appear in K-theory.\n\nIf formula_4 is a local weak equivalence of simplicial presheaves, then the induced map formula_5 is also a local weak equivalence.\n\nLet \"F\" be a simplicial presheaf on a site. The homotopy sheaves formula_6 of \"F\" is defined as follows. For any formula_7 in the site and a 0-simplex \"s\" in \"F\"(\"X\"), set formula_8 and formula_9. We then set formula_10 to be the sheaf associated with the pre-sheaf formula_11.\n\nThe category of simplicial presheaves on a site admits many different model structures.\n\nSome of them are obtained by viewing simplicial presheaves as functors \nThe category of such functors is endowed with (at least) three model structures, namely the projective, the Reedy, and the injective model structure. The weak equivalences / fibrations in the first are maps\nsuch that \nis a weak equivalence / fibration of simplicial sets, for all \"U\" in the site \"S\". The injective model structure is similar, but with weak equivalences and cofibrations instead.\n\nA simplicial presheaf \"F\" on a site is called a stack if, for any \"X\" and any hypercovering \"H\" →\"X\", the canonical map\nis a weak equivalence as simplicial sets, where the right is the homotopy limit of\n\nAny sheaf \"F\" on the site can be considered as a stack by viewing formula_17 as a constant simplicial set; this way, the category of sheaves on the site is included as a subcategory to the homotopy category of simplicial presheaves on the site. The inclusion functor has a left adjoint and that is exactly formula_18.\n\nIf \"A\" is a sheaf of abelian group (on the same site), then we define formula_19 by doing classifying space construction levelwise (the notion comes from the obstruction theory) and set formula_20. One can show (by induction): for any \"X\" in the site,\nwhere the left denotes a sheaf cohomology and the right the homotopy class of maps.\n\n\n\n\n"}
{"id": "26507256", "url": "https://en.wikipedia.org/wiki?curid=26507256", "title": "Svetlana Gannushkina", "text": "Svetlana Gannushkina\n\nSvetlana Alekseevna Gannushkina (, born 6 March 1942) is a mathematician and human rights activist in Russia who was reported to have been a serious contender for the 2010 Nobel Peace Prize.\n\nGannushkina worked for many years as a professor of mathematics at a Moscow university. In 1990, she helped to found the group Citizen’s Assistance (\"Grazhdanskoe Sodeistvie\"), an NGO which campaigns for human rights, particularly with regard to immigrants and refugees in Russian society. Since 2015 the organization is labelled a \"foreign agent\" by the Russian government.\n\nGannushkina is a member of the Presidential Council for Civil Society and Human Rights. She is also on the council of Memorial, a society dedicated to the remembrance of victims of Soviet repression.\n\nIn 2006, she was awarded the Homo Homini Award for human rights activism by the Czech group People in Need.\n\nIn 2016, Gannushkina received the Right Livelihood Award, often referred to as \"Alternative Nobel Prize\", Stockholm, Sweden, \"for her decades-long commitment to promoting human rights and justice for refugees and forced migrants, and tolerance among different ethnic groups\"\n\n"}
{"id": "98748", "url": "https://en.wikipedia.org/wiki?curid=98748", "title": "Tree automaton", "text": "Tree automaton\n\nA tree automaton is a type of state machine. Tree automata deal with tree structures, rather than the strings of more conventional state machines.\n\nThe following article deals with branching tree automata, which correspond to regular languages of trees.\n\nAs with classical automata, finite tree automata (FTA) can be either a deterministic automaton or not. According to how the automaton processes the input tree, finite tree automata can be of two types: (a) bottom up, (b) top down. This is an important issue, as although non-deterministic (ND) top-down and ND bottom-up tree automata are equivalent in expressive power, deterministic top-down automata are strictly less powerful than their deterministic bottom-up counterparts, because tree properties specified by deterministic top-down tree automata can only depend on path properties. (Deterministic bottom-up tree automata are as powerful as ND tree automata.)\n\nA bottom-up finite tree automaton over \"F\" is defined as a tuple\n(\"Q\", \"F\", \"Q\", Δ),\nwhere \"Q\" is a set of states, \"F\" is a ranked alphabet (i.e., an alphabet whose symbols have an associated arity), \"Q\" ⊆ \"Q\" is a set of final states, and Δ is a set of transition rules of the form \"f\"(\"q\"(\"x\")...,\"q\"(\"x\")) → \"q\"(\"f\"(\"x\"...,\"x\")), for an \"n\"-ary \"f\" ∈ \"F\", \"q\", \"q\" ∈ \"Q\", and \"x\" variables denoting subtrees. That is, members of Δ are rewrite rules from nodes whose childs' roots are states, to nodes whose roots are states. Thus the state of a node is deduced from the states of its children.\n\nFor \"n\"=0, that is, for a constant symbol \"f\", the above transition rule definition reads \"f\"() → \"q\"(\"f\"()); often the empty parentheses are omitted for convenience: \"f\" → \"q\"(\"f\").\nSince these transition rules for constant symbols (leaves) do not require a state, no explicitly defined initial states are needed.\nA bottom-up tree automaton is run on a ground term over \"F\", starting at all its leaves simultaneously and moving upwards, associating a run state from \"Q\" with each subterm.\nThe term is accepted if its root is associated to an accepting state from \"Q\".\n\nA top-down finite tree automaton over \"F\" is defined as a tuple\n(\"Q\", \"F\", \"Q\", Δ),\nwith two differences with bottom-up tree automata. First, \"Q\" ⊆ \"Q\", the set of its initial states, replaces \"Q\"; second, its transition rules are oriented conversely:\n\"q\"(\"f\"(\"x\"...,\"x\")) → \"f\"(\"q\"(\"x\")...,\"q\"(\"x\")), for an \"n\"-ary \"f\" ∈ \"F\", \"q\", \"q\" ∈ \"Q\", and \"x\" variables denoting subtrees.\nThat is, members of Δ are here rewrite rules from nodes whose roots are states to nodes whose childs' roots are states.\nA top-down automaton starts in some of its initial states at the root and moves downward along branches of the tree, associating along a run a state with each subterm inductively.\nA tree is accepted if every branch can be gone through this way.\n\nA tree automaton is called deterministic (abbreviated DFTA) if no two rules from Δ have the same left hand side; otherwise it is called nondeterministic (NFTA). Non-deterministic top-down tree automata have the same expressive power as non-deterministic bottom-up ones; the transition rules are simply reversed, and the final states become the initial states.\n\nIn contrast, deterministic top-down tree automata are less powerful than their bottom-up counterparts, because in a deterministic tree automaton no two transition rules have the same left-hand side. For tree automata, transition rules are rewrite rules; and for top-down ones, the left-hand side will be parent nodes. Consequently, a deterministic top-down tree automaton will only be able to test for tree properties that are true in all branches, because the choice of the state to write into each child branch is determined at the parent node, without knowing the child branches contents.\n\nEmploying coloring to distinguish members of \"F\" and \"Q\", and using the ranked alphabet \"F\"={ ,,(..) }, with having arity 2 and all other symbols having arity 0, a bottom-up tree automaton accepting the set of all finite lists of boolean values can be defined as (\"Q\", \"F\", \"Q\", Δ) with , \"Q\"={ }, and Δ consisting of the rules\n\nIn this example, the rules can be understood intuitively as assigning to each term its type in a bottom-up manner; e.g. rule (4) can be read as \"A term (\"x\",\"x\") has type , provided \"x\" and \"x\" has type and , respectively\".\nAn accepting example run is\n\nCf. the derivation of the same term from a regular tree grammar corresponding to the automaton, shown at Regular tree grammar#Examples.\n\nAn rejecting example run is\n\nIntuivitvely, this corresponds to the term (,) not being well-typed.\n\nUsing the same colorization as above, this example shows how tree automata generalize ordinary string automata.\nThe finite deterministic string automaton shown in the picture accepts all strings of binary digits that denote a multiple of 3.\nUsing the notions from Deterministic finite automaton#Formal definition, it is defined by:\n\nIn the tree automaton setting, the input alphabet is changed such that the symbols and are both unary, and a nullary symbol, say is used for tree leaves.\nFor example, the binary string \"\" in the string automaton setting corresponds to the term \"((()))\" in the tree automaton setting; this way, strings can be generalized to trees, or terms.\nThe top-down finite tree automaton accepting the set of all terms corresponding to multiples of 3 in binary string notation is then defined by:\nFor example, the tree \"((()))\" is accepted by the following tree automaton run:\n\nIn contrast, the term \"(())\" leads to following non-accepting automaton run:\n\nSince there are no other initial states than to start an automaton run with, the term \"(())\" is not accepted by the tree automaton.\n\nFor comparison purposes, the table gives in column (A) and (D) a (right) regular (string) grammar, and a regular tree grammar, respectively, each accepting the same language as its automaton counterpart.\n\nFor a bottom-up automaton, a ground term \"t\" (that is, a tree) is accepted if there exists a reduction that starts from \"t\" and ends with \"q\"(\"t\"), where \"q\" is a final state. For a top-down automaton, a ground term \"t\" is accepted if there exists a reduction that starts from \"q\"(\"t\") and ends with \"t\", where \"q\" is an initial state.\n\nThe tree language \"L\"(\"A\") accepted, or recognized, by a tree automaton \"A\" is the set of all ground terms accepted by \"A\". A set of ground terms is recognizable if there exists a tree automaton that accepts it.\n\nA linear (that is, arity-preserving) tree homomorphism preserves recognizability.\n\nA non-deterministic finite tree automaton is complete if there is at least one transition rule available for every possible symbol-states combination.\nA state \"q\" is accessible if there exists a ground term \"t\" such that there exists a reduction from \"t\" to \"q\"(\"t\").\nAn NFTA is reduced if all its states are accessible.\n\nEvery sufficiently large ground term \"t\" in a recognizable tree language \"L\" can be vertically tripartited such that arbitrary repetition (\"pumping\") of the middle part keeps the resulting term in \"L\".\n\nFor the language of all finite lists of boolean values from the above example, all terms beyond the height limit \"k\"=2 can be pumped, since they need to contain an occurrence of . For example,\n\nall belong to that language.\n\nThe class of recognizable tree languages is closed under union, under complementation, and under intersection.\n\nA congruence on the set of all trees over a ranked alphabet \"F\" is an equivalence relation such that \"u\" ≡ \"v\" and ... and \"u\" ≡ \"v\" implies \"f\"(\"u\"...,\"u\") ≡ \"f\"(\"v\"...,\"v\"), for every \"f\" ∈ \"F\".\nIt is of finite index if its number of equivalence-classes is finite.\n\nFor a given tree-language \"L\", a congruence can be defined by \"u\" ≡ \"v\" if \"C\"[\"u\"] ∈ \"L\" ⇔ \"C\"[\"v\"] ∈ \"L\" for each context \"C\".\n\nThe Myhill–Nerode theorem for tree automaton states that the following three statements are equivalent:\n\n\n\n\n"}
{"id": "27033090", "url": "https://en.wikipedia.org/wiki?curid=27033090", "title": "Unit sphere", "text": "Unit sphere\n\nIn mathematics, a unit sphere is the set of points of distance 1 from a fixed central point, where a generalized concept of distance may be used; a closed unit ball is the set of points of distance less than or equal to 1 from a fixed central point. Usually a specific point has been distinguished as the origin of the space under study and it is understood that a unit sphere or unit ball is centered at that point. Therefore one speaks of \"the\" unit ball or \"the\" unit sphere.\n\nFor example, a one-dimensional sphere is the surface of what is commonly called a \"circle\", while such a circle's interior and surface together are the two-dimensional ball. Similarly, a two-dimensional sphere is the surface of the Euclidean solid known colloquially as a \"sphere\", while the interior and surface together are the three-dimensional ball.\n\nA unit sphere is simply a sphere of radius one. The importance of the unit sphere is that any sphere can be transformed to a unit sphere by a combination of translation and scaling. In this way the properties of spheres in general can be reduced to the study of the unit sphere.\n\nIn Euclidean space of \"n\" dimensions, the -dimensional unit sphere is the set of all points formula_1 which satisfy the equation\n\nThe \"n\"-dimensional open unit ball is the set of all points satisfying the inequality\n\nand the \"n\"-dimensional closed unit ball is the set of all points satisfying the inequality\n\nThe classical equation of a unit sphere is that of the ellipsoid with a radius of 1 and no alterations to the \"x\"-, \"y\"-, or \"z\"- axes:\n\nThe volume of the unit ball in \"n\"-dimensional Euclidean space, and the surface area of the unit sphere, appear in many important formulas of analysis. The volume of the unit ball in \"n\" dimensions, which we denote \"V\", can be expressed by making use of the gamma function. It is\n\nwhere \"n\"<nowiki>!!</nowiki> is the double factorial.\n\nThe hypervolume of the (\"n\"−1)-dimensional unit sphere (\"i.e.\", the \"area\" of the boundary of the \"n\"-dimensional unit ball), which we denote \"A\", can be expressed as\n\nwhere the last equality holds only for .\n\nThe surface areas and the volumes for some values of formula_8 are as follows:\nwhere the decimal expanded values for \"n\" ≥ 2 are rounded to the displayed precision.\n\nThe \"A\" values satisfy the recursion:\n\nThe \"V\" values satisfy the recursion:\n\nThe formulae for \"A\" and \"V\" can be computed for any real number \"n\" ≥ 0, and there are circumstances under which it is appropriate to seek the sphere area or ball volume when \"n\" is not a non-negative integer.\n\nThe surface area of an (\"n\"–1)-dimensional sphere with radius \"r\" is \"A\" \"r\" and the volume of an \"n\"-dimensional ball with radius \"r\" is \"V\" \"r\". For instance, the area is for the surface of the three-dimensional ball of radius \"r\". The volume is for the three-dimensional ball of radius \"r\".\n\nMore precisely, the open unit ball in a normed vector space formula_18, with the norm formula_19, is\n\nIt is the interior of the closed unit ball of (\"V\",||·||),\n\nThe latter is the disjoint union of the former and their common border, the unit sphere of (\"V\",||·||),\n\nThe 'shape' of the \"unit ball\" is entirely dependent on the chosen norm; it may well have 'corners', and for example may look like [−1,1], in the case of the norm \"l\" in \"R\". The \"round ball\" is understood as the usual Hilbert space norm, based in the finite-dimensional case on the Euclidean distance; its boundary is what is usually meant by the \"unit sphere\". Here are some images of the unit ball for the two-dimensional formula_23 space for various values of \"p\" (the unit ball being concave for \"p\" < 1 and convex for \"p\" ≥ 1):\n\nThese illustrate why the condition \"p\" ≥ 1 is necessary in the definition of the formula_23 norm, as the unit ball in any normed space must be convex as a consequence of the triangle inequality.\n\nNote that for the circumferences formula_25 of the two-dimensional unit balls we have:\n\nAll three of the above definitions can be straightforwardly generalized to a metric space, with respect to a chosen origin. However, topological considerations (interior, closure, border) need not apply in the same way (e.g., in ultrametric spaces, all of the three are simultaneously open and closed sets), and the unit sphere may even be empty in some metric spaces.\n\nIf \"V\" is a linear space with a real quadratic form \"F\":\"V\" → R, then { p ∈ \"V\" : \"F\"(p) = 1 } may be called the unit sphere or unit quasi-sphere of \"V\". For example, the quadratic form formula_29, when set equal to one, produces the unit hyperbola which plays the role of the \"unit circle\" in the plane of split-complex numbers. Similarly, quadratic form x yields a pair of lines for the unit sphere in the dual number plane.\n\n\n"}
{"id": "24320887", "url": "https://en.wikipedia.org/wiki?curid=24320887", "title": "Yupana", "text": "Yupana\n\nA \"yupana\" (from Quechua yupay: count) is an abacus used to perform arithmetic operations dating back to the time of the Incas.\n\nThe term \"yupana\" refers to two distinct classes of objects:\n\n\nAlthough very different from each other, most of the scholars who have dealt with table-yupana, have then extended its reasoning and theories to the yupana of Poma de Ayala and vice versa, perhaps in an attempt to find a unifying thread or a common method. It should also be noted that the Nueva Coronica was discovered only in 1916 in the library of Copenhagen and that part of the studies on it were based on previous studies and theories regarding table-yupanas.\n\nSeveral chroniclers of the Indies described, unfortunately approximately, the Incan abacus and its operation.\n\nThe first was Guaman Poma de Ayala that in 1615 approximately, wrote:\n\nIn addition to providing this brief description, Poma de Ayala draws a picture of the yupana: a board of five rows and four columns in which are designed a series of white and black circles.\n\nThe father Jesuit \"José de Acosta\" wrote:\n\nFather \"Juan de Velasco\" wrote:\n\nThe first table-yupana which we know was found in 1869 in Chordeleg in the department of Cuenca (Ecuador). It is a rectangular table (33x27 cm) of wood which contains 17 compartments, of which 14 square, 2 rectangular and one octagonal. On two edges of the table there are other square compartments (12x12 cm) raised and symmetrically arranged one another, to which two square platforms (7x7 cm), are overlapped. These structures are called towers. The table presents a symmetry of the compartments with respect to the diagonal of the rectangle. The four sides of the board are also engraved with figures of human heads and a crocodile. As a result of this discovery, Charles Wiener began in 1877 a systematic study of these objects. Wiener came to the conclusion that the table-yupanas served to calculate the taxes that farmers paid to the Incan empire.\n\nFound at Caraz in 1878 - 1879, this table-yupana is different from that of Chordeleg as the material of construction is the stone and the central compartment of octagonal shape is replaced with a rectangular one; towers also have three shelves instead of two.\n\nA series of table-yupanas much different from the first, was described by Erland Nordenskiöld in 1931. These yupana, made of stone, present a series of rectangular and square compartments. The tower is composed of two rectangular compartments. The compartments are arranged symmetrically with respect to the axis of the smaller side of the table.\n\nThese yupana, made of stone, have 18 compartments of triangular shape, arranged around the table. On one side there is a rectangular tower with only one floor and three triangular compartments. In the central part there are four square compartments, coupled between them.\n\nIdentical to the yupana of Chordeleg, both for the material and the arrangement of the compartments, this table-yupana was found in the archaeological complex of Chan Chan in Peru in 1967.\n\nDiscovered in the province of Pisco (Peru), these table-yupanas are two tables in clay and bone. The first is rectangular (47x32 cm), has 22 square (5x5 cm) and three rectangular (16x18 cm) compartments, and has no towers. The second is rectangular (32x23 cm) containing 22 square compartments, two L-shaped and three rectangular in the center. The compartments are arranged symmetrically with respect to the axis of the longer side.\n\nDiscovered in the upper Ecuador by Max Uhle in 1922, this yupana is made of stone and its bins are drawn. It has the shape of a scale consisting of 10 overlapping rectangles: four on the first floor, three on the second, two in the third and one in the fourth. This yupana is the one that is closest to the picture by Poma de Ayala in Nueva Coronica, while having a line less and being half drawn.\n\nC. Florio presents a study \nwhich does not identify a yupana in these archaeological findings, but an object whose name is unknown and which has been forgotten. Instead, this object is to connect to the tocapu (an ideogram already used by pre-Incas civilizations) called “llave inca” (i.e. Inca key) and to the yanantin-masintin philosophy. The scholar reaches this conclusion starting from the lack of objective evidences which recognize a yupana in this object, a belief that consolidated over years only for the repeat of this hypothesis never demonstrated, and by crossing data from the Miccinelli Documents and the tocapu(s) catalogued by Victoria de la Jara. \nSupposing to colour the different compartments of the table-yupana (fig. A), C. Florio identifies a drawing (fig. B) very similar to a really existing tocapu (fig. C) and catalogued by Victoria de la Jara. In addition, in the tocapu reported in figure D, also catalogued by V. de la Jara, Florio identifies a stylization of the tocapu C and the departure point for creating the tocapu “llave inca” (Inca key). She finds the relation between the table-yupana and the Inca key also in their connection with the concept of duality: the table-yupana structure is clearly dual and Blas Valera in “Exul Immeritus Blas Valera populo suo” (one of the two Miccinelli Documents) describes the tocapu we call Inca key as representing the concept of the “opposite forces” and the “number 2”, both strictly linked to the concept of duality.\n\nAccording to C. Florio, the real yupana used by the Incas is that of Guáman Poma, but with more columns and rows. Guáman Poma would have represented just the part of the yupana useful for carrying out a specific calculation, which Florio identifies to be a multiplication (see below).\n\nIn 1931, Henry Wassen studied the yupana of Poma de Ayala, proposing for the first time a possible representation of the numbers on the board and the operations of addition and multiplication. He interpreted the white circles as gaps, carved into yupana in which to insert the seeds described by chroniclers: so the white circles correspond to empty gaps, while the blacks circles correspond to the same gaps filled with a black seed.\n\nThe numbering system at the base of the abacus was positional notation in base 10 (in line with the writings of the chroniclers of the Indies).\n\nThe representation of the numbers, then followed a vertical progression such that the units were positioned in the first row from the bottom, in the second the tens, hundreds in the third, and so on.\n\nWassen proposed a progression of values of the seeds that depends on their position in the table: 1, 5, 15, 30, respectively, depending on who occupy a gap in the first, second, third and fourth columns (see the table below). Only a maximum of five seeds could be included in a box belonging to the first column, so that the maximum value of said box was 5, multiplied by the power of the corresponding line. These seeds could be replaced with one seed of the next column, useful during arithmetic operations. According to the theory of Wassen, therefore, the operations of sum and product were carried out horizontally.\n\nThis theory received a lot of criticism due to the high complexity of the calculations and was therefore considered inadequate and soon abandoned.\n\nBy way of example, the following table shows the number 13457.\n\nThis first interpretation of the yupana of Poma de Ayala was the starting point for the theories developed by subsequent authors, up to the present day. In particular, no one ever moved away from the positional numbering system until 2008.\n\nEmilio Mendizabal was the first to propose in 1976 that the Inca were using, as well as the decimal representation, also a representation based on the progression 1,2,3,5. Mendizabal in the same publication pointed out that the series of numbers 1,2,3 and 5, in the drawing of Poma de Ayala, are part of the Fibonacci sequence, and stressed the importance of \"magic\" that had the number 5 for civilization the north of Peru, and the number 8 for the civilizations of the south of Peru.\n\nIn 1979, Carlos Radicati di Primeglio emphasized the difference of table-yupana from that of Poma de Ayala, describing the state of the art of the research and theories advanced so far. He also proposed the algorithms for calculating the four basic arithmetic operations for yupana of Poma de Ayala, according to a new interpretation for which it was possible to have up to nine seeds in each box with vertical progression for powers of ten. The choice of Radicati was to associate to each gap a value of 1.\n\nIn the following table is represented the number 13457\n\nIn 1981, the English textile engineer William Burns Glynn proposed a positional base 10 solution for the yupana of Poma de Ayala.\n\nGlynn, as Radicati, adopted the same Wassen's idea of full and empty gaps, as well as a vertical progression of the powers of ten, but proposed an architecture that allowed to greatly simplify the arithmetic operations.\n\nThe horizontal progression of the values of the seeds in its representation is 1, 1, 1 for the first three columns, so that in each row is possible to deposit a maximum of ten seeds (5 + 3 + 2 seeds). Ten seeds of any row is correspond to a single seed of the upper line.\n\nThe last column is dedicated to the \"memory\", which is a place where you can drop momentarily ten seeds, waiting to move them to the upper line. According to the author, this is very useful during arithmetic operations in order to reduce the possibility of error.\n\nThe solution of Glynn has been adopted in various teaching projects all over the world, and even today some of its variants are used in some schools of South America.\n\nIn the following table is represented the number 13457\n\nThe Italian engineer Nicolino de Pasquale in 2001 proposed a positional solution in base 40 of the yupana of Poma de Ayala, taking the representation theory of Fibonacci already proposed by Emilio Mendizabal and developing it for the four operations.\n\nDe Pasquale also adopts a vertical progression to represent numbers by powers of 40. The representation of the numbers is based on the fact that the sum of the values of the circles in each row gives as total 39, if each circle takes the value 5 in the first column, 3 in the second column, 2 in the third and 1 in the fourth one; it is thus possible to represent 39 numbers, united to neutral element ( zero or no seeds in the table); this forms the basis of 40 symbols necessary for the numbering system.\n\nOne of the possible representations of the number 13457 in the yupana by De Pasquale is shown in the following table:\n\nThe theory of De Pasquale opened, in the years after his birth, great controversy among researchers who divided mainly into two groups: one supporting the base 10 theory and another supporting the base 40 one. It should be noted in this regard that the Spanish chronicles of the time of the conquest of the Americas indicated that the Incas used a decimal system and that since 2003 the base 10 has been proposed as the basis for calculating both with the abacus and the quipu\n\nDe Pasquale has recently proposed the use of yupana as astronomical calendar running in mixed base 36/40 and provided its own interpretation of the Quechua word \"huno\", translating it as 0.1. This interpretation diverges from all the chroniclers of the Indies, starting from Domingo de Santo Tomas which in 1560 translated \"huno\" with \"chunga guaranga\" (ten thousand).\n\nIn 2008 Cinzia Florio proposes an alternative and revolutionary approach in respect to all the theories proposed so far. For the first time we deviate from the positional numbering system and we adopt the additive, or sign-value notation.\n\nRelying exclusively on the design of Poma de Ayala, the author explains the arrangement of white and black circles and interprets the use of the abacus as a board for making multiplications, in which the multiplicand is represented in the right column, the multiplier in the two central columns and the result (product) is shown in the left column. See the following table.\n\nThe theory differs from all the previous by several aspects: first, the white and black circles would not be any gaps that may be filled with a seed, but different colors of the seeds, representatives respectively tens and units (this according to the chronicler Juan de Velasco ).\n\nSecondly, the multiplicand is entered in the first column respecting the sign-value notation: so, the seeds can be entered in any order and the number is given by the sum of the values of these seeds.\n\nThe multiplier is represented as the sum of two factors, since the procedure for obtaining the product is based on the distributive property of multiplication over addition.\n\nThe table multiplier drawn by Poma de Ayala with that provision of the seeds, represent according to the author, the calculation: 32 x 5, where the multiplier 5 is decomposed into 3 + 2. The sequence of numbers 1,2,3,5 would be casual, contingent to the calculation done and not related to the Fibonacci series.\n\nKey: o = 10; • = 1; The operation represented is: 32 x 5 = 32 x (2 + 3) = (32 x 2) + (32 x 3) = 64 + 96 = 160\n\nThe numbers represented in the columns are, from left to right: 32 (the multiplicand), 64 = 32 x 2 and 32 x 3 = 96 (which together constitute the multiplicand, multiplied by the two factors in which the multiplier has been broken down) and finally 151. In this issue (error) are based all possible criticisms of this interpretation, since 151 is obviously not the sum of 96 and 64. Florio, however, notes that a mistake of Poma de Ayala, in designing a black circle instead of a white one, would have been possible. In this case, changing just a black circle with a white one in the last column, we obtain the number 160, which is exactly the product sought as the sum of the quantities present in the central columns.\n\nWith a yupana as the one designed by Poma de Ayala can not be represented every multiplicands, but it is necessary to extend the yupana vertically (adding rows) to represent numbers whose sum of digits exceeds 5. The same thing goes for the multipliers: to represent all the numbers is necessary to extend the number of columns. It should be emphasized that this interpretation, apart the supposed error calculation (or representation by the designer), is the only one that identifies in the yupana of Poma de Ayala a mathematical and consistent message (multiplication) and not a series of random numbers as in other interpretations.\n\n\n\n\n\n\n\n"}
