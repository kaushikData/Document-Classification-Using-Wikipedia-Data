{"id": "2294289", "url": "https://en.wikipedia.org/wiki?curid=2294289", "title": "211 (number)", "text": "211 (number)\n\n211 (two hundred [and] eleven) is the natural number between 210 and 212. It is also a prime number.\n\n211 is an odd number.\n\n211 is a primorial prime, sum of three consecutive primes (67 + 71 + 73), Chen prime, centered decagonal prime, and self prime.\n\n211 is a repdigit in base 14 (111).\n\nMultiplying its digits, it is still a prime (2), and adding its digits, it is square (4). Rearranging its digits, 211 becomes 121, which also is a square. Adding any two of its digits will be prime (2 or 3).\n\n2-1-1 is special abbreviated telephone number reserved in Canada and the United States as an easy-to-remember three-digit telephone number meant to provide quick information and referrals to health and human service organizations for both services from charities and from governmental agencies.\n\n211 is also associated with E211, the preservative sodium benzoate\n\n211 is also the California Penal Code section defining robbery. Sometimes it is paired with 187, California PC section for murder.\n\n211 is also an EDI (Electronic Data Interchange) document known as an Electronic Bill of Lading.\n\n211 is also a nickname for Steel Reserve, a malt liquor alcoholic beverage.\n\n211 is also SMTP status code for system status.\n\n+211 is the code for international direct-dial phone calls to South Sudan.\n\n"}
{"id": "244463", "url": "https://en.wikipedia.org/wiki?curid=244463", "title": "Adjacency matrix", "text": "Adjacency matrix\n\nIn graph theory and computer science, an adjacency matrix is a square matrix used to represent a finite graph. The elements of the matrix indicate whether pairs of vertices are adjacent or not in the graph.\n\nIn the special case of a finite simple graph, the adjacency matrix is a (0,1)-matrix with zeros on its diagonal. If the graph is undirected, the adjacency matrix is symmetric. \nThe relationship between a graph and the eigenvalues and eigenvectors of its adjacency matrix is studied in spectral graph theory.\n\nThe adjacency matrix should be distinguished from the incidence matrix for a graph, a different matrix representation whose elements indicate whether vertex–edge pairs are incident or not, and degree matrix which contains information about the degree of each vertex.\n\nFor a simple graph with vertex set \"V\", the adjacency matrix is a square  ×  matrix \"A\" such that its element \"A\" is one when there is an edge from vertex \"i\" to vertex \"j\", and zero when there is no edge. The diagonal elements of the matrix are all zero, since edges from a vertex to itself (loops) are not allowed in simple graphs. It is also sometimes useful in algebraic graph theory to replace the nonzero elements with algebraic variables.\n\nThe same concept can be extended to multigraphs and graphs with loops by storing the number of edges between each two vertices in the corresponding matrix element, and by allowing nonzero diagonal elements. Loops may be counted either once (as a single edge) or twice (as two vertex-edge incidences), as long as a consistent convention is followed. Undirected graphs often use the latter convention of counting loops twice, whereas directed graphs typically use the former convention.\n\nThe adjacency matrix \"A\" of a bipartite graph whose two parts have \"r\" and \"s\" vertices can be written in the form \nwhere \"B\" is an matrix, and 0 and 0 represent the and zero matrices. In this case, the smaller matrix \"B\" uniquely represents the graph, and the remaining parts of \"A\" can be discarded as redundant. \"B\" is sometimes called the biadjacency matrix.\n\nFormally, let be a bipartite graph with parts } and }. The \"biadjacency matrix\" is the 0–1 matrix \"B\" in which if and only if ∈ \"E\". \n\nIf \"G\" is a bipartite multigraph or weighted graph then the elements \"b\" are taken to be the number of edges between the vertices or the weight of the edge , respectively.\n\nAn -\"adjacency matrix\" \"A\" of a simple graph has \"A\" = \"a\" if (\"i\", \"j\") is an edge, \"b\" if it is not, and \"c\" on the diagonal. The Seidel adjacency matrix is a -\"adjacency matrix\". This matrix is used in studying strongly regular graphs and two-graphs.\n\nThe distance matrix has in position (\"i\", \"j\") the distance between vertices \"v\" and \"v\". The distance is the length of a shortest path connecting the vertices. Unless lengths of edges are explicitly provided, the length of a path is the number of edges in it. The distance matrix resembles a high power of the adjacency matrix, but instead of telling only whether or not two vertices are connected (i.e., the connection matrix, which contains boolean values), it gives the exact distance between them.\n\nThe convention followed here (for undirected graphs) is that each edge adds 1 to the appropriate cell in the matrix, and each loop adds 2. This allows the degree of a vertex to be easily found by taking the sum of the values in either its respective row or column in the adjacency matrix.\n\nIn directed graphs, the in-degree of a vertex can be computed by summing the entries of the corresponding column, and the out-degree can be computed by summing the entries of the corresponding row.\nThe adjacency matrix of a complete graph contains all ones except along the diagonal where there are only zeros. The adjacency matrix of an empty graph is a zero matrix.\n\nThe adjacency matrix of an undirected simple graph is symmetric, and therefore has a complete set of real eigenvalues and an orthogonal eigenvector basis. The set of eigenvalues of a graph is the spectrum of the graph. It is common to denote the eigenvalues by formula_2\n\nThe greatest eigenvalue formula_3 is bounded above by the maximum degree. This can be seen as result of the Perron–Frobenius theorem, but it can be proved easily. Let \"v\" be one eigenvector associated to formula_3 and \"x\" the component in which \"v\" has maximum absolute value. Without loss of generality assume \"v\" is positive since otherwise you simply take the eigenvector formula_5, also associated to formula_3. Then\n\nFor \"d\"-regular graphs, \"d\" is the first eigenvalue of \"A\" for the vector (it is easy to check that it is an eigenvalue and it is the maximum because of the above bound). The multiplicity of this eigenvalue is the number of connected components of \"G\", in particular formula_8 for connected graphs. It can be shown that for each eigenvalue formula_9, its opposite formula_10 is also an eigenvalue of \"A\" if \"G\" is a bipartite graph. In particular −\"d\" is an eigenvalue of bipartite graphs.\n\nThe difference formula_11 is called the spectral gap and it is related to the expansion of \"G\". It is also useful to introduce the spectral radius of formula_12 denoted by formula_13. This number is bounded by formula_14. This bound is tight in the Ramanujan graphs, which have applications in many areas.\n\nSuppose two directed or undirected graphs \"G\" and \"G\" with adjacency matrices \"A\" and \"A\" are given. \"G\" and \"G\" are isomorphic if and only if there exists a permutation matrix \"P\" such that\nIn particular, \"A\" and \"A\" are similar and therefore have the same minimal polynomial, characteristic polynomial, eigenvalues, determinant and trace. These can therefore serve as isomorphism invariants of graphs. However, two graphs may possess the same set of eigenvalues but not be isomorphic. Such linear operators are said to be isospectral.\n\nIf \"A\" is the adjacency matrix of the directed or undirected graph \"G\", then the matrix \"A\" (i.e., the matrix product of \"n\" copies of \"A\") has an interesting interpretation: the element gives the number of (directed or undirected) walks of length \"n\" from vertex \"i\" to vertex \"j\". If \"n\" is the smallest nonnegative integer, such that for some \"i\", \"j\", the element of \"A\" is positive, then \"n\" is the distance between vertex \"i\" and vertex \"j\". This implies, for example, that the number of triangles in an undirected graph \"G\" is exactly the trace of \"A\" divided by 6. Note that the adjacency matrix can be used to determine whether or not the graph is connected.\n\nThe adjacency matrix may be used as a data structure for the representation of graphs in computer programs for manipulating graphs. The main alternative data structure, also in use for this application, is the adjacency list.\n\nBecause each entry in the adjacency matrix requires only one bit, it can be represented in a very compact way, occupying only /8 bytes to represent a directed graph, or (by using a packed triangular format and only storing the lower triangular part of the matrix) approximately /16 bytes to represent an undirected graph. Although slightly more succinct representations are possible, this method gets close to the information-theoretic lower bound for the minimum number of bits needed to represent all -vertex graphs. For storing graphs in text files, fewer bits per byte can be used to ensure that all bytes are text characters, for instance by using a Base64 representation.\nBesides avoiding wasted space, this compactness encourages locality of reference.\nHowever, for a large sparse graph, adjacency lists require less storage space, because they do not waste any space to represent edges that are \"not\" present.\n\nAn alternative form of adjacency matrix (which, however, requires a larger amount of space) replaces the numbers in each element of the matrix with pointers to edge objects (when edges are present) or null pointers (when there is no edge).\nIt is also possible to store edge weights directly in the elements of an adjacency matrix.\n\nBesides the space tradeoff, the different data structures also facilitate different operations. Finding all vertices adjacent to a given vertex in an adjacency list is as simple as reading the list, and takes time proportional to the number of neighbors. With an adjacency matrix, an entire row must instead be scanned, which takes a larger amount of time, proportional to the number of vertices in the whole graph. On the other hand, testing whether there is an edge between two given vertices can be determined at once with an adjacency matrix, while requiring time proportional to the minimum degree of the two vertices with the adjacency list.\n\n\n"}
{"id": "26950215", "url": "https://en.wikipedia.org/wiki?curid=26950215", "title": "Andris Ambainis", "text": "Andris Ambainis\n\nAndris Ambainis (born 18 January 1975) is a Latvian computer scientist active in the fields of quantum information theory and quantum computing. He has held past positions at the Institute for Advanced Study at Princeton, New Jersey and the Institute for Quantum Computing at the University of Waterloo. He is currently a professor in the Faculty of Computing at the University of Latvia. He received a Bachelors (1996), Masters (1997), and Doctorate (1997) in Computer Science from the University of Latvia, as well as a Ph.D. (2001) from the University of California, Berkeley. Ambainis has contributed extensively to quantum information processing and foundations of quantum mechanics, mostly through his work on quantum walks and lower bounds for quantum query complexity. In 1991 he received a perfect score and gold medal at the International Mathematical Olympiad. He won an Alfred P. Sloan Fellowship in 2008. Ambainis was an invited speaker at the 2018 International Congress of Mathematicians, speaking on mathematical aspects of computer science.\n\n"}
{"id": "7605515", "url": "https://en.wikipedia.org/wiki?curid=7605515", "title": "Archibald Read Richardson", "text": "Archibald Read Richardson\n\nArchibald Read Richardson FRS (21 August 1881 – 4 November 1954) was a British mathematician known for his work in algebra.\n\nRichardson collaborated with Dudley E. Littlewood on invariants and group representation theory. They introduced the immanant of a matrix, studied Schur functions and developed the Littlewood–Richardson rule for their multiplication.\n\nRichardson was elected a Fellow of the Royal Society on 21 March 1946.\n"}
{"id": "1252834", "url": "https://en.wikipedia.org/wiki?curid=1252834", "title": "Bicyclic molecule", "text": "Bicyclic molecule\n\nA bicyclic molecule (\"bi\" = two, \"cycle\" = ring) is a molecule that features two joined rings. Bicyclic structures occur widely, for example in many biologically important molecules like α-thujene and camphor. A bicyclic compound can be carbocyclic (all of the ring atoms are carbons), or heterocyclic (the rings atoms consist of at least two different elements), like DABCO. Moreover, the two rings can both be aliphatic (\"e.g.\" decalin and norbornane), or can be aromatic (\"e.g.\" naphthalene), or a combination of aliphatic and aromatic (\"e.g.\" tetralin).\n\nThere are three possible modes of ring junction for a bicyclic compound:\n\nBicyclic molecules have a strict nomenclature. The root of the compound name depends on the total number of atoms in all rings together, possibly followed by a suffix denoting the functional group with the highest priority. Numbering of the carbon chain always begins at one bridgehead atom (where the rings meet) and follows the carbon chain along the longest path, to the next bridgehead atom. Then numbering is continued along the second longest path and so on. Fused and bridged bicyclic compounds get the prefix \"bicyclo\", whereas spirocyclic compounds get the prefix \"spiro\". In between the prefix and the suffix, a pair of brackets with numerals denotes the number of carbon atoms between each of the bridgehead atoms. These numbers are arranged in descending order and are separated by periods.\nFor example, the carbon frame of norbornane contains a total of 7 atoms, hence the root name heptane. This molecule has two paths of 2 carbon atoms and a third path of 1 carbon atom between the two bridgehead carbons, so the brackets are filled in descending order: [2.2.1]. Addition of the prefix \"bicyclo\" gives the total name bicyclo[2.2.1]heptane.\n\nThe carbon frame of camphor also counts 7 atoms, but is substituted with a carbonyl in this case, hence the suffix heptanone. We start with numbering the carbon frame at the bridgehead atom with the highest priority (methyl goes before proton), hence the bridgehead carbon in front gets number 1, the carbonyl gets number 2 and numbering continues along the carbon chain\nfollowing the longest path, until the doubly substituted top carbon (number 7). Equal to norbornane, this molecule also has two paths of 2 carbon atoms and one path of 1 carbon atom between the two bridgehead carbons, so the numbers within the brackets stay [2.2.1]. Combining the brackets and suffix (now filling in the position of the carbonyl as well) gives us [2.2.1]heptan-2-one. Besides \"bicyclo\", the prefix should also specify the positions of all methyl substituents so the complete, official name becomes 1,7,7-trimethylbicyclo[2.2.1]heptan-2-one.\n\nWhen naming simple fused bicyclic compounds, the same method as for bridged bicyclic compounds is applied, except the third path between the two bridgehead atoms now consists of zero atoms. Therefore, fused bicyclic compounds have a \"0\" included in the brackets. For example, decalin is named bicyclo[4.4.0]decane. The numbers are sometimes omitted when no ambiguity is created as there is no alternative possible – for example, bicyclo[1.1.0]butane is typically called simply bicyclobutane.\n\nThe heterocyclic molecule DABCO has a total of 8 atoms in its bridged structure, hence the root name octane. Note that here the two bridgehead atoms are nitrogen instead of carbon atoms. Therefore, the official name gets the additional prefix \"1,4-diaza\" and the total name becomes 1,4-diazabicyclo[2.2.2]octane.\n\n"}
{"id": "32770184", "url": "https://en.wikipedia.org/wiki?curid=32770184", "title": "Boas–Buck polynomials", "text": "Boas–Buck polynomials\n\nIn mathematics, Boas–Buck polynomials are sequences of polynomials Φ(\"x\") given by generating functions of the form\n\nThe case \"r\"=1, sometimes called generalized Appell polynomials, was studied by .\n"}
{"id": "28046776", "url": "https://en.wikipedia.org/wiki?curid=28046776", "title": "Cantor's intersection theorem", "text": "Cantor's intersection theorem\n\nCantor's intersection theorem refers to two closely related theorems in general topology and real analysis, named after Georg Cantor, about intersections of decreasing nested sequences of non-empty compact sets.\n\nLet formula_1 be a topological space. A decreasing nested sequence of non-empty compact subsets of formula_1 has a non-empty intersection. In other words, supposing (\"C\") is a sequence of non-empty, compact subsets of formula_1 satisfying\n\nit follows that\n\nAssume, by way of contradiction, that formula_6. For each n, let formula_7. Since formula_8 and formula_6, we have formula_10.\n\nSince formula_11⊂formula_1 is compact and formula_13 is an open cover (on formula_11) of formula_11, we can extract a finite cover formula_16. Let formula_17 be the largest set of this cover, which exists by the ordering hypothesis on the collection formula_18 Then formula_11⊂formula_17. But then formula_21, a contradiction.\n\nThe theorem in real analysis draws the same conclusion for closed and bounded subsets of the set of real numbers R. It states that a decreasing nested sequence (\"C\") of non-empty, closed and bounded subsets of R has a non-empty intersection.\n\nThis version follows from the general topological statement in light of the Heine–Borel theorem, which states that sets of real numbers are compact if and only if they are closed and bounded. However, it is typically used as a lemma in proving said theorem, and therefore warrants a separate proof.\n\nAs an example, if \"C\" = [0, 1/\"k\"], the intersection over {\"C\"} is {0}. On the other hand, both the sequence of open bounded sets \"C\" = (0, 1/\"k\") and the sequence of unbounded closed sets \"C\" = [\"k\", ∞) have empty intersection. All these sequences are properly nested.\n\nThis version of the theorem generalizes to R, the set of \"n\"-element vectors of real numbers, but does not generalize to arbitrary metric spaces. For example, in the space of rational numbers, the sets\n\nare closed and bounded, but their intersection is empty.\n\nNote that this contradicts neither the topological statement, as the sets \"C\" are not compact, nor the variant below, as the rational numbers are not complete with respect to the usual metric.\n\nA simple corollary of the theorem is that the Cantor set is nonempty, since it is defined as the intersection of a decreasing nested sequence of sets, each of which is defined as the union of a finite number of closed intervals; hence each of these sets is non-empty, closed, and bounded. In fact, the Cantor set contains uncountably many points.\n\nEach closed bounded non-empty subset \"C\" of R admits a minimal element \"x\". Since for each \"k\", we have \nit follows that\nso (\"x\") is an increasing sequence contained in the bounded set \"C\". The Monotone convergence theorem now guarantees the existence of a limit point\nFor fixed \"k\", we have that \"x\"∈\"C\" for all \"j\"≥\"k\" and since \"C\" was closed, it follows that \"x\"∈\"C\".\nOur choice of \"k\" was arbitrary, hence \"x\" belongs to the intersection of all \"C\" and the proof is complete.\n\nIn a complete metric space, the following variant of Cantor's intersection theorem holds. Suppose that \"X\" is a non-empty complete metric space, and \"C\" is a sequence of closed nested subsets of \"X\" whose diameters tend to zero:\nwhere diam(\"C\") is defined by\nThen the intersection of the \"C\" contains exactly one point:\nfor some \"x\" in \"X\".\n\nA proof goes as follows. Since the diameters tend to zero, the diameter of the intersection of the \"C\" is zero, so it is either empty or consists of a single point. So it is sufficient to show that it is not empty. Pick an element \"x\" of \"C\" for each \"n\". Since the diameter of \"C\" tends to zero and the \"C\" are nested, the \"x\" form a Cauchy sequence. Since the metric space is complete this Cauchy sequence converges to some point \"x\". Since each \"C\" is closed, and \"x\" is a limit of a sequence in \"C\", \"x\" must lie in \"C\". This is true for every \"n\", and therefore the intersection of the \"C\" must contain \"x\".\n\nA converse to this theorem is also true: if \"X\" is a metric space with the property that the intersection of any nested family of closed subsets whose diameters tend to zero is non-empty, then \"X\" is a complete metric space. (To prove this, let \"x\" be a Cauchy sequence in \"X\", and let \"C\" be the closure of the tail of this sequence.)\n\n"}
{"id": "6988390", "url": "https://en.wikipedia.org/wiki?curid=6988390", "title": "Cryptographic nonce", "text": "Cryptographic nonce\n\nIn cryptography, a nonce is an arbitrary number that can be used just once in a cryptographic communication. It is similar in spirit to a nonce word, hence the name. It is often a random or pseudo-random number issued in an authentication protocol to ensure that old communications cannot be reused in replay attacks. They can also be useful as initialization vectors and in cryptographic hash functions.\n\nA nonce is an arbitrary number used only once in a cryptographic communication, in the spirit of a nonce word. They are often random or pseudo-random numbers. Many nonces also include a timestamp to ensure exact timeliness, though this requires clock synchronization between organizations. The addition of a client nonce (\"cnonce\") helps to improve the security in some ways as implemented in digest access authentication. To ensure that a nonce is used only once, it should be time-variant (including a suitably fine-grained timestamp in its value), or generated with enough random bits to ensure a probabilistically insignificant chance of repeating a previously generated value. Some authors define pseudo-randomness (or unpredictability) as a requirement for a nonce.\n\nAuthentication protocols may use nonces to ensure that old communications cannot be reused in replay attacks. For instance, nonces are used in HTTP digest access authentication to calculate an MD5 digest of the password. The nonces are different each time the 401 authentication challenge response code is presented, thus making replay attacks virtually impossible. The scenario of ordering products over the Internet can provide an example of the usefulness of nonces in replay attacks. An attacker could take the encrypted information and—without needing to decrypt—could continue to send a particular order to the supplier, thereby ordering products over and over again under the same name and purchase information. The nonce is used to give 'originality' to a given message so that if the company receives any other orders from the same person with the same nonce, it will discard those as invalid orders.\n\nA nonce may be used to ensure security for a stream cipher. Where the same key is used for more than one message and then a different nonce is used to ensure that the keystream is different for different messages encrypted with that key; often the message number is used.\n\nSecret nonce values are used by the Lamport signature scheme as a signer-side secret which can be selectively revealed for comparison to public hashes for signature creation and verification.\n\nInitialization vectors may be referred to as nonces, as they are typically random or pseudo-random.\n\nNonces are used in proof-of-work systems to vary the input to a cryptographic hash function so as to obtain a hash for a certain input that fulfills certain arbitrary conditions. In doing so, it becomes far more difficult to create a \"desirable\" hash than to verify it, shifting the burden of work onto one side of a transaction or system. For example, proof of work, using hash functions, was considered as a means to combat email spam by forcing email senders to find a hash value for the email (which included a timestamp to prevent pre-computation of useful hashes for later use) that had an arbitrary number of leading zeroes, by hashing the same input with a large number of nonce values until a \"desirable\" hash was obtained.\n\nSimilarly, the bitcoin blockchain hashing algorithm can be tuned to an arbitrary difficulty by changing the required minimum/maximum value of the hash so that the number of bitcoins awarded for new blocks does not increase linearly with increased network computation power as new users join. This is likewise achieved by forcing bitcoin miners to add nonce values to the value being hashed to change the hash algorithm output. Because cryptographic hash algorithms cannot easily be predicted based on their inputs, this makes the act of blockchain hashing and the possibility of being awarded bitcoins something of a lottery, where the first \"miner\" to find a nonce that delivers a desirable hash is awarded bitcoins.\n\n\n"}
{"id": "2272644", "url": "https://en.wikipedia.org/wiki?curid=2272644", "title": "Daina Taimina", "text": "Daina Taimina\n\nDaina Taimina (; born August 19, 1954) is a Latvian mathematician, currently Adjunct Associate Professor at Cornell University, known for crocheting objects to illustrate hyperbolic space.\n\nTaimina received all her formal education in Riga, Latvia, where in 1977 she graduated summa cum laude from the University of Latvia and completed her graduate work in theoretical computer science (supervised by Prof. Rūsiņš Mārtiņš Freivalds) in 1990. At that time, a doctoral thesis had to be defended outside of Latvia, so she defended hers in Minsk. This explains the fact that formally Taimina's doctorate was issued by the Institute of Mathematics of the National Academy of Sciences of Belarus. After Latvia regained independence in 1991, Taimina received her doctorate in mathematics from the University of Latvia, where she taught for 20 years.\n\nDaina Taimina joined the Cornell Math Department in December 1996.\n\nWhile attending a geometry workshop in 1997, she saw fragile paper models of hyperbolic planes, designed by geometer William Thurston. She decided to make more durable models, and did so by crocheting them. Due to her success in this she was invited, together with her husband David Henderson, a math professor also at Cornell, to give a presentation at a Cornell workshop.\nCrocheted mathematical models later appeared in three geometry textbooks they wrote together, of which the most popular is \"Experiencing Geometry: Euclidean and non-Euclidean with History\".\n\nAn article about Taimina's innovation in \"New Scientist\" was spotted by the Institute For Figuring, a small non-profit organisation based in Los Angeles, and she was invited to speak about hyperbolic space and its connections with nature to a general audience which included artists and movie producers. Taimina's initial lecture and following other public presentations sparked great interest in this new tactile way of exploring concepts of hyperbolic geometry, making this advanced topic accessible to wide audiences. Originally creating purely mathematical models, Taimina soon became popular as a fiber artist and public presenter for general audiences of ages five and up. In June 2005, her work was first shown as art in an exhibition \"Not The Knitting You Know\" at Eleven Eleven Sculpture Space, an art gallery in Washington, D.C. Since then she has participated regularly in various shows in galleries in US, UK, Latvia, Italy, Belgium, Ireland. Her artwork is in the collections of several private collectors, colleges and universities, and has been included in the American Mathematical Model Collection of the Smithsonian Museum, Cooper–Hewitt, National Design Museum, and Institut Henri Poincaré.\n\nHer work has received wide interest in media. It has been written about in 'Knit Theory' in Discover magazine. The Times (Alex Bellos “How Crochet Solved age-old Math Problem”, The Times, July 1, 2008 ) Margaret Wertheim interviewed Daina Taimina and David Henderson for Cabinet Magazine \nLater, the Institute For Figuring published a brochure \"A Field Guide to Hyperbolic Space\". In 2005 the IFF decided that to incorporate Taimina's ideas and approach of explaining hyperbolic space in their mission of popularizing mathematics, and curated an exhibition at Machine Project gallery, which was the subject of a piece in the Los Angeles Times.\nTaimina's way of exploring hyperbolic space via crochet and connections with nature, combatting math phobia, was adapted by Margaret Wertheim in her talks and became highly successful in the IFF-curated Hyperbolic Crochet Coral Reef project.\n\nTaimina's book \"Crocheting Adventures with Hyperbolic Planes\" (A K Peters, Ltd., 2009, ) won the 2009 Bookseller/Diagram Prize for Oddest Title of the Year.\nIt also won the 2012 Euler Book Prize of the Mathematical Association of America.\n\nTaimina also contributed to David W. Henderson's book \"Differential Geometry: A Geometric Introduction\" (Prentice Hall, 1998) and, with Henderson, wrote \"Experiencing Geometry: Euclidean and Non-Euclidean with History\" (Prentice Hall, 2005).\n\n\n\n\n"}
{"id": "33993923", "url": "https://en.wikipedia.org/wiki?curid=33993923", "title": "Data at rest", "text": "Data at rest\n\nData at rest in information technology means inactive data that is stored physically in any digital form (e.g. databases, data warehouses, spreadsheets, archives, tapes, off-site backups, mobile devices etc.). \n\"Data at rest\" is used as a complement to the terms \"data in use\" and \"data in transit\" which together define the three states of digital data (\"see Figure 1\"). \n\nThere is some disagreement as to the boundary between data at rest and data in use. Data at rest generally refers to data stored in persistent storage (disk, tape) while data in use generally refers to data being processed by a computer central processing unit (CPU) or in random access memory (RAM, also referred to as main memory or simply memory). \nDefinitions include: \n\"...all data in computer storage while excluding data that is traversing a network or temporarily residing in computer memory to be read or updated.\" \n\"...all data in storage but excludes any data that frequently traverses the network or that which resides in temporary memory. Data at rest includes but is not limited to archived data, data which is not accessed or changed frequently, files stored on hard drives, USB thumb drives, files stored on backup tape and disks, and also files stored off-site or on a storage area network (SAN).\"\n\nData in use has also been taken to mean “active data” in the context of being in a database or being manipulated by an application. For example, some enterprise encryption gateway solutions for the cloud claim to encrypt data at rest, data in transit and data in use.\n\nWhile it is generally accepted that archive data (i.e. which never changes), regardless of its storage medium, is data at rest and active data subject to constant or frequent change is data in use. “Inactive data” could be taken to mean data which may change, but infrequently. The imprecise nature of terms such as “constant” and “frequent” means that some stored data cannot be comprehensively defined as either data at rest or in use. These definitions could be taken to assume that Data at Rest is a superset of data in use; however, data in use, subject to frequent change, has distinct processing requirements from data at rest, whether completely static or subject to occasional change.\n\nThe division of data at rest into the sub-categories \"static\" and \"inconstant\" addresses this distinction (\"see Figure 2\")..\n\nBecause of its nature data at rest is of increasing concern to businesses, government agencies and other institutions. Mobile devices are often subject to specific security protocols to protect data at rest from unauthorised access when lost or stolen and there is an increasing recognition that database management systems and file servers should also be considered as at risk; the longer data is left unused in storage, the more likely it might be retrieved by unauthorized individuals outside the network.\n\nData encryption, which prevents data visibility in the event of its unauthorized access or theft, is commonly used to protect data in motion and increasingly promoted for protecting data at rest.\n\nThe encryption of data at rest should only include strong encryption methods such as AES or RSA. Encrypted data should remain encrypted when access controls such as usernames and password fail. Increasing encryption on multiple levels is recommended. Cryptography can be implemented on the database housing the data and on the physical storage where the databases are stored. Data encryption keys should be updated on a regular basis. Encryption keys should be stored separately from the data. Encryption also enables crypto-shredding at the end of the data or hardware lifecycle. Periodic auditing of sensitive data should be part of policy and should occur on scheduled occurrences. Finally, only store the minimum possible amount of sensitive data.\n\nTokenization is a non-mathematical approach to protecting data at rest that replaces sensitive data with non-sensitive substitutes, referred to as tokens, which have no extrinsic or exploitable meaning or value. This process does not alter the type or length of data, which means it can be processed by legacy systems such as databases that may be sensitive to data length and type. \n\nTokens require significantly less computational resources to process and less storage space in databases than traditionally encrypted data. This is achieved by keeping specific data fully or partially visible for processing and analytics while sensitive information is kept hidden. Lower processing and storage requirements makes tokenization an ideal method of securing data at rest in systems that manage large volumes of data. \n\nA further method of preventing unwanted access to data at rest is the use of data federation especially when data is distributed globally (e.g. in off-shore archives). An example of this would be a European organisation which stores its archived data off-site in the USA. Under the terms of the USA PATRIOT Act the American authorities can demand access to all data physically stored within its boundaries, even if it includes personal information on European citizens with no connections to the USA. Data encryption alone cannot be used to prevent this as the authorities have the right to demand decrypted information. A data federation policy which retained personal citizen information with no foreign connections within its country of origin (separate from information which is either not personal or is relevant to off-shore authorities) is one option to address this concern.\n"}
{"id": "47322965", "url": "https://en.wikipedia.org/wiki?curid=47322965", "title": "Dilution ratio", "text": "Dilution ratio\n\nIn chemistry and biology, the dilution ratio is the ratio of solute to solvent. It is often used for \"simple dilutions,\" one in which a unit volume of a liquid material of interest is combined with an appropriate volume of a solvent liquid to achieve the desired concentration. The diluted material must be thoroughly mixed to achieve the true dilution. For example, in a 1:5 dilution, with a 1:5 dilution ratio, entails combining 1 unit volume of solute (the material to be diluted) with 5 unit volumes of the solvent to give 6 total units of total volume.\n\nThis is often confused with \"dilution factor\" which is an expression which describes the ratio of the aliquot volume to the final volume. Dilution factor is a notation often used in commercial assays. For example, in a 1:5 dilution, with a 1:5 dilution factor, (verbalize as \"1 to 5\" dilution) entails combining 1 unit volume of solute (the material to be diluted) with (approximately) 4 unit volumes of the solvent to give 5 units of total volume. Note that some solutions and mixtures take up slightly less volume than their components.\n\nThe dilution factor can be expressed using exponents: 1:5 would be 5e−1 (5 i.e. one-fifth:one); 1:100 would be 10e−2 (10 i.e. one hundredth:one), and so on.\n\nThere is often confusion between dilution ratio (1:n meaning 1 part solute to n parts solvent) and dilution factor (1:n+1) where the second number (n+1) represents the total volume of solute + solvent. In scientific and serial dilution assays, the given dilution factor often means the ratio to the final volume, not to just the solvent. The factors then can easily be multiplied to give an overall dilution factor. Some have suggested that dilution factors should more clearly be written as a/total or a þ b, as the use of the colon symbol \":\" is widely used to represent ratios in fields like mathematics, chemistry, or organic chemistry. Leave ratios for actual ratios 1:100 = 101. However, at this time, both dilution conventions are widely used at this time -which is why it is important for laboratory personnel to always clarify whether a \"dilution ratio\" or \"dilution factor\" ought to be used in performing dilutions.\nIn analytical chemistry, dilution factor is always greater than 1 using equation ,\n<br>\nformula_1 \n\nor sometimes the inverse for other fields: formula_2\nIn other areas of science such as pharmacy, and in non-scientific usage, a dilution is normally given as a plain ratio of solvent to solute. For large factors, this confusion makes only a minor difference, but in precise work it can be important to make clear which ratio is intended.\n\n"}
{"id": "50335944", "url": "https://en.wikipedia.org/wiki?curid=50335944", "title": "Discrete Weibull distribution", "text": "Discrete Weibull distribution\n\nIn probability theory and statistics, the discrete Weibull distribution is the discrete variant of the Weibull distribution. It was first described by Nakagawa and Osaki in 1975.\n\nIn the original paper by Nakagawa and Osaki they used the parametrization formula_1 making the cmf formula_2 with formula_3. Setting formula_4 makes the relationship with the geometric distribution apparent.\n\nThe continuous Weibull distribution has a close relationship with the Gumbel distribution which is easy to see when log-transforming the variable. A similar transformation can be made on the discrete-weibull.\n\nDefine formula_5 where (unconventionally) formula_6 and define parameters formula_7 and formula_8. By replacing formula_9 in the cmf:\n\nWe see that we get a location-scale parametrization:\n\nwhich in estimation-settings makes a lot of sense. This opens up the possibility of regression with frameworks developed for weibull-regression and extreme-value-theory. \n"}
{"id": "6612667", "url": "https://en.wikipedia.org/wiki?curid=6612667", "title": "Equivalence (measure theory)", "text": "Equivalence (measure theory)\n\nIn mathematics, and specifically in measure theory, equivalence is a notion of two measures being qualitatively similar. Specifically, the two measures agree on which events have measure zero.\n\nLet formula_1 and formula_2 be two measures on the measurable space formula_3, and let \nbe the set of all formula_5-null sets; formula_6 is similarly defined. Then the measure formula_2 is said to be absolutely continuous in reference to formula_1 iff formula_9. This is denoted as formula_10.\n\nThe two measures are called equivalent iff formula_11 and formula_10, which is denoted as formula_13. An equivalent definition is that two measures are equivalent if they satisfy formula_14.\n\nDefine the two measures on the real line as\nfor all Borel sets formula_17. Then formula_1 and formula_2 are equivalent, since all sets outside of formula_20 have formula_21 measure zero, and a set inside formula_20 is a formula_21 null set in respect to the Lebesgue measure.\n\nLook at some measurable space formula_3 and let formula_1 be the counting measure, so\n\nwhere formula_27 is the cardinality of the set a. So the counting measure has only one null set, which is the empty set. Therefore,\n\nSo by the second definition, any other measure formula_2 is equivalent to the counting measure iff it also has just the empty set as the only null set.\n\nA measure formula_1 is called a supporting measure of a measure formula_2 if formula_1 is formula_33-finite and formula_2 is equivalent to formula_1.\n"}
{"id": "7167202", "url": "https://en.wikipedia.org/wiki?curid=7167202", "title": "Exponential-Golomb coding", "text": "Exponential-Golomb coding\n\nAn exponential-Golomb code (or just Exp-Golomb code) is a type of universal code. To encode any nonnegative integer \"x\" using the exp-Golomb code:\n\nThe first few values of the code are:\n\nThis is identical to the Elias gamma code of \"x\"+1, allowing it to encode 0.\n\nExp-Golomb coding for \"k\" = 0 is used in the H.264/MPEG-4 AVC and H.265 High Efficiency Video Coding video compression standards, in which there is also a variation for the coding of signed numbers by assigning the value 0 to the binary codeword '0' and assigning subsequent codewords to input values of increasing magnitude (and alternating sign, if the field can contain a negative number):\n\nIn other words, a non-positive integer \"x\"≤0 is mapped to an even integer −2\"x\", while a positive integer \"x\">0 is mapped to an odd integer 2\"x\"−1.\n\nExp-Golomb coding is also used in the Dirac video codec.\n\nTo encode larger numbers in fewer bits (at the expense of using more bits to encode smaller numbers), this can be generalized using a nonnegative integer parameter  \"k\". To encode a nonnegative integer \"x\" in an order-\"k\" exp-Golomb code:\nAn equivalent way of expressing this is:\n\n"}
{"id": "37479627", "url": "https://en.wikipedia.org/wiki?curid=37479627", "title": "Good filtration", "text": "Good filtration\n\nIn mathematical representation theory, a good filtration is a filtration of a representation of a reductive algebraic group \"G\" such that the subquotients are isomorphic to the spaces of sections \"F\"(λ) of line bundles λ over \"G\"/\"B\" for a Borel subgroup \"B\". In characteristic 0 this is automatically true as the irreducible modules are all of the form \"F\"(λ), but this is not usually true in positive characteristic. showed that the tensor product of two modules \"F\"(λ)⊗\"F\"(μ) has a good filtration, completing the results of who proved it in most cases and who proved it in large characteristic. showed that the existence of good filtrations for these tensor products also follows from standard monomial theory.\n\n"}
{"id": "150062", "url": "https://en.wikipedia.org/wiki?curid=150062", "title": "Goodstein's theorem", "text": "Goodstein's theorem\n\nIn mathematical logic, Goodstein's theorem is a statement about the natural numbers, proved by Reuben Goodstein in 1944, which states that every \"Goodstein sequence\" eventually terminates at 0. Kirby and Paris showed that it is unprovable in Peano arithmetic (but it can be proven in stronger systems, such as second-order arithmetic). This was the third example of a true statement that is unprovable in Peano arithmetic, after Gödel's incompleteness theorem and Gerhard Gentzen's 1943 direct proof of the unprovability of ε-induction in Peano arithmetic. The Paris–Harrington theorem was a later example.\n\nLaurence Kirby and Jeff Paris introduced a graph-theoretic hydra game with behavior similar to that of Goodstein sequences: the \"Hydra\" is a rooted tree, and a move consists of cutting off one of its \"heads\" (a branch of the tree), to which the hydra responds by growing a finite number of new heads according to certain rules. Kirby and Paris proved that the Hydra will eventually be killed, regardless of the strategy that Hercules uses to chop off its heads, though this may take a very long time. Just like for Goodstein sequences, Kirby and Paris showed that it cannot be proven in Peano arithmetic alone.\n\nGoodstein sequences are defined in terms of a concept called \"hereditary base-\"n\" notation\". This notation is very similar to usual base-\"n\" positional notation, but the usual notation does not suffice for the purposes of Goodstein's theorem.\n\nIn ordinary base-\"n\" notation, where \"n\" is a natural number greater than 1, an arbitrary natural number \"m\" is written as a sum of multiples of powers of \"n\":\nwhere each coefficient \"a\" satisfies , and . For example, in base 2,\nThus the base-2 representation of 35 is 100011, which means . Similarly, 100 represented in base 3 is 10201:\nNote that the exponents themselves are not written in base-\"n\" notation. For example, the expressions above include 2 and 3.\n\nTo convert a base-\"n\" representation to hereditary base-\"n\" notation, first rewrite all of the exponents in base-\"n\" notation. Then rewrite any exponents inside the exponents, and continue in this way until every number appearing in the expression has been converted to base-\"n\" notation.\n\nFor example, while 35 in ordinary base-2 notation is , it is written in hereditary base-2 notation as\nusing the fact that Similarly, 100 in hereditary base-3 notation is\n\nThe Goodstein sequence \"G\"(\"m\") of a number \"m\" is a sequence of natural numbers. The first element in the sequence \"G\"(\"m\") is \"m\" itself. To get the second, \"G\"(\"m\")(2), write \"m\" in hereditary base-2 notation, change all the 2s to 3s, and then subtract 1 from the result. In general, the term of the Goodstein sequence of \"m\" is as follows:\n\nEarly Goodstein sequences terminate quickly. For example, \"G\"(3) terminates at the 6th step:\n\nLater Goodstein sequences increase for a very large number of steps. For example, \"G\"(4) starts as follows:\n\nElements of \"G\"(4) continue to increase for a while, but at base formula_6,\nthey reach the maximum of formula_7, stay there for the next formula_6 steps, and then begin their first and final descent.\n\nThe value 0 is reached at base formula_9. (Curiously, this is a Woodall number: formula_10. This is also the case with all other final bases for starting values greater than 4.)\n\nHowever, even \"G\"(4) doesn't give a good idea of just \"how\" quickly the elements of a Goodstein sequence can increase.\n\"G\"(19) increases much more rapidly and starts as follows:\n\nIn spite of this rapid growth, Goodstein's theorem states that every Goodstein sequence eventually terminates at 0, no matter what the starting value is.\n\nGoodstein's theorem can be proved (using techniques outside Peano arithmetic, see below) as follows: Given a Goodstein sequence \"G\"(\"m\"), we construct a parallel sequence \"P\"(\"m\") of ordinal numbers which is strictly decreasing and terminates. Then \"G\"(\"m\") must terminate too, and it can terminate only when it goes to 0. A common misunderstanding of this proof is to believe that \"G\"(\"m\") goes to 0 \"because\" it is dominated by \"P\"(\"m\"). Actually, the fact that \"P\"(\"m\") dominates \"G\"(\"m\") plays no role at all. The important point is: \"G\"(\"m\")(\"k\") exists if and only if \"P\"(\"m\")(\"k\") exists (parallelism). Then if \"P\"(\"m\") terminates, so does \"G\"(\"m\"). And \"G\"(\"m\") can terminate only when it comes to 0.\n\nMore precisely, each term \"P\"(\"m\")(\"n\") of the sequence \"P\"(\"m\") is obtained by applying a function \"f\" on the term \"G\"(\"m\")(\"n\") of the Goodstein sequence of \"m\" as follows: take the hereditary base representation of \"G\"(\"m\")(\"n\"), and replace each occurrence of the base with the first infinite ordinal number ω. For example, and . Addition, multiplication and exponentiation of ordinal numbers are well defined.\n\n\nIf the sequence \"G\"(\"m\") did not go to 0, it would not terminate and would be infinite (since would always exist). Consequently, \"P\"(\"m\") also would be infinite (since in its turn would always exist too). But \"P\"(\"m\") is strictly decreasing and the standard order < on ordinals is well-founded, therefore an infinite strictly decreasing sequence cannot exist, or equivalently, every strictly decreasing sequence of ordinals does terminate (and cannot be infinite). This contradiction shows that \"G\"(\"m\") terminates, and since it terminates, goes to 0 (by the way, since there exists a natural number \"k\" such that , by construction of \"P\"(\"m\") we have that ).\n\nWhile this proof of Goodstein's theorem is fairly easy, the \"Kirby–Paris theorem\", which shows that Goodstein's theorem is not a theorem of Peano arithmetic, is technical and considerably more difficult. It makes use of countable nonstandard models of Peano arithmetic.\n\nSuppose the definition Goodstein sequence is changed so that instead of\nreplacing each occurrence of the base \"b\" with \nit was replaces it with . Would the sequence still terminate?\nMore generally, let \"b\", \"b\", \"b\", … be any sequences of integers.\nThen let the \nterm of the extended Goodstein sequence of \"m\" be as\nfollows: take the hereditary base \"b\" representation of\n\"G\"(\"m\")(\"n\"), and replace each occurrence of the base \"b\"\nwith and then subtract one.\nThe claim is that this sequence still terminates.\nThe extended proof defines as\nfollows: take the hereditary base \"b\" representation of\n\"G\"(\"m\")(\"n\"), and replace each occurrence of the base\n\"b\" with the first infinite ordinal number ω.\nThe \"base-changing\" operation of the Goodstein sequence when going\nfrom \"G\"(\"m\")(\"n\") to still does not change the value of \"f\".\nFor example, if and if ,\nthen\nformula_14, hence the ordinal formula_15 is strictly greater than the ordinal formula_16\n\nThe Goodstein function, formula_17, is defined such that formula_18 is the length of the Goodstein sequence that starts with \"n\". (This is a total function since every Goodstein sequence terminates.) The extreme growth-rate of formula_19 can be calibrated by relating it to various standard ordinal-indexed hierarchies of functions, such as the functions formula_20 in the Hardy hierarchy, and the functions formula_21 in the fast-growing hierarchy of Löb and Wainer:\n\n\n\n\nSome examples:\n\nGoodstein's theorem can be used to construct a total computable function that Peano arithmetic cannot prove to be total. The Goodstein sequence of a number can be effectively enumerated by a Turing machine; thus the function which maps \"n\" to the number of steps required for the Goodstein sequence of \"n\" to terminate is computable by a particular Turing machine. This machine merely enumerates the Goodstein sequence of \"n\" and, when the sequence reaches \"0\", returns the length of the sequence. Because every Goodstein sequence eventually terminates, this function is total. But because Peano arithmetic does not prove that every Goodstein sequence terminates, Peano arithmetic does not prove that this Turing machine computes a total function.\n\n\n\n"}
{"id": "3821625", "url": "https://en.wikipedia.org/wiki?curid=3821625", "title": "GraphML", "text": "GraphML\n\nGraphML is an XML-based file format for graphs. The GraphML file format results from the joint effort of the graph drawing community to define a common format for exchanging graph structure data. It uses an XML-based syntax and supports the entire range of possible graph structure constellations including directed, undirected, mixed graphs, hypergraphs, and application-specific attributes.\n\nA GraphML file consists of an XML file containing a codice_1 element, within which is an unordered sequence of codice_2 and codice_3 elements. Each codice_2 element should have a distinct codice_5 attribute, and each codice_3 element has codice_7 and codice_8 attributes that identify the endpoints of an edge by having the same value as the codice_5 attributes of those endpoints.\nHere is what a simple undirected graph with two nodes and one edge between them looks like:\nAdditional features of the GraphML language allow its users to specify whether edges are directed or undirected, and to associate additional data with vertices or edges.\n\n\n"}
{"id": "56012962", "url": "https://en.wikipedia.org/wiki?curid=56012962", "title": "Graph polynomial", "text": "Graph polynomial\n\nIn mathematics, a graph polynomial is a graph invariant whose values are polynomials. Invariants of this type are studied in algebraic graph theory.\nImportant graph polynomials include:\n\n"}
{"id": "35262599", "url": "https://en.wikipedia.org/wiki?curid=35262599", "title": "Grigorii Fichtenholz", "text": "Grigorii Fichtenholz\n\nGrigorii Mikhailovich Fichtenholz (or Fikhtengolts) () (June 5, 1888 in Odessa – June 26, 1959 in Leningrad) was a Russian mathematician working on real analysis and functional analysis. Fichtenholz was one of the founders of the Leningrad school of real analysis.\n\nHe also authored a three-volume textbook 'Differential and Integral Calculus'. The books cover mathematical analysis of function of one real variable, functions of many real variables and of complex functions. Due to depth and precision of presentation of material, these books are defined as classical position in mathematical analysis. Book was translated, among others, into German, Chinese, and Persian however translation to English language has not been done still.\n\nFichtenholz's books about analysis are widely used in Middle and Eastern European as well as Chinese universities due to its exceptionality of detailed and well-ordered presentation of material about mathematical analysis. Due to unknown reasons, these books do not have the same fame in universities in other areas of the world.\n\nHe was an Invited Speaker of the ICM in 1924 in Toronto.\n\nLeonid Kantorovich and Isidor Natanson were among his students.\n\n\n"}
{"id": "43269014", "url": "https://en.wikipedia.org/wiki?curid=43269014", "title": "Gödel logic", "text": "Gödel logic\n\nIn mathematical logic, a first-order Gödel logic is a member of a family of finite- or infinite-valued logics in which the sets of truth values \"V\" are closed subsets of the interval [0,1] containing both 0 and 1. Different such sets \"V\" in general determine different Gödel logics. The concept is named after Kurt Gödel.\n"}
{"id": "1264842", "url": "https://en.wikipedia.org/wiki?curid=1264842", "title": "Harmonices Mundi", "text": "Harmonices Mundi\n\nHarmonices Mundi (Latin: \"The Harmony of the World\", 1619) is a book by Johannes Kepler. In the work Kepler discusses harmony and congruence in geometrical forms and physical phenomena. The final section of the work relates his discovery of the so-called \"third law of planetary motion\".\n\nIt is estimated that Kepler had begun working on \"Harmonices Mundi\" sometime near 1599, which was the year Kepler sent a letter to Maestlin detailing the mathematical data and proofs that he intended to use for his upcoming text, which he originally planned to name \"De harmonia mundi\". Kepler was aware that the content of \"Harmonices Mundi\" closely resembled that of the subject matter for Ptolemy’s \"Harmonica\", but was not concerned because the new astronomy Kepler would use, most notably the adoption of elliptic orbits in the Copernican system, allowed him to explore new theorems. Another important development that allowed Kepler to establish his celestial-harmonic relationships, was the abandonment of the Pythagorean tuning as the basis for musical consonance and the adoption of geometrically supported musical ratios; this would eventually be what allowed Kepler to relate musical consonance and the angular velocities of the planets. Thus Kepler could reason that his relationships gave evidence for God acting as a grand geometer, rather than a Pythagorean numerologist.\n\nThe concept of musical harmonies intrinsically existing within the spacing of the planets existed in medieval philosophy prior to Kepler. Musica Universalis was a traditional philosophical metaphor that was taught in the quadrivium, and was often referred to as the “music of the spheres.” Kepler was intrigued by this idea while he sought explanation for a rational arrangement of the heavenly bodies. It should be noted that when Kepler uses the term “harmony” it is not strictly referring to the musical definition, but rather, a broader definition encompassing congruence in Nature and the workings of both the celestial and terrestrial bodies. He notes musical harmony as being a product of man, derived from angles, in contrast to a harmony that he refers to as being a phenomenon that interacts with the human soul. In turn, this allowed Kepler to claim the Earth has a soul because it is subjected to astrological harmony.\n\nKepler divides \"The Harmony of the World\" into five long chapters: the first is on regular polygons; the second is on the congruence of figures; the third is on the origin of harmonic proportions in music; the fourth is on harmonic configurations in astrology; and the fifth on the harmony of the motions of the planets.\n\nChapters 1 and 2 of \"The Harmony of the World\" contain most of Kepler’s contributions concerning polyhedra. He is primarily interested with how polygons, which he defines as either regular or semiregular, can come to be fixed together around a central point on a plane to form congruence. His primary objective was to be able to rank polygons based on a measure of sociability, or rather, their ability to form partial congruence when combined with other polyhedra. He returns to this concept later in \"Harmonices Mundi\" with relation to astronomical explanations. In the second chapter is the earliest mathematical understanding of two types of regular star polyhedra, the small and great stellated dodecahedron; they would later be referred to as Kepler's solids or Kepler Polyhedra and, together with two regular polyhedra discovered by Louis Poinsot, as the Kepler–Poinsot polyhedra. He describes polyhedra in terms of their faces, which is similar to the model used in Plato’s \"Timaeus\" to describe the formation of Platonic solids in terms of basic triangles.\n\nWhile medieval philosophers spoke metaphorically of the \"music of the spheres\", Kepler discovered physical harmonies in planetary motion. He found that the difference between the maximum and minimum angular speeds of a planet in its orbit approximates a harmonic proportion. For instance, the maximum angular speed of the Earth as measured from the Sun varies by a semitone (a ratio of 16:15), from \"mi\" to \"fa\", between aphelion and perihelion. Venus only varies by a tiny 25:24 interval (called a diesis in musical terms). Kepler explains the reason for the Earth's small harmonic range:\n\nThe celestial choir Kepler formed was made up of a tenor (Mars), two bass (Saturn and Jupiter), a soprano (Mercury), and two altos (Venus and Earth). Mercury, with its large elliptical orbit, was determined to be able to produce the greatest number of notes, while Venus was found to be capable of only a single note because its orbit is nearly a circle.\n\nAt very rare intervals all of the planets would sing together in \"perfect concord\": Kepler proposed that this may have happened only once in history, perhaps at the time of creation. Kepler reminds us that harmonic order is only mimicked by man, but has origin in the alignment of the heavenly bodies:\n\nKepler discovers that all but one of the ratios of the maximum and minimum speeds of planets on neighboring orbits approximate musical harmonies within a margin of error of less than a diesis (a 25:24 interval). The orbits of Mars and Jupiter produce the one exception to this rule, creating the inharmonic ratio of 18:19. The cause of this dissonance might be explained by the fact that the asteroid belt separates those two planetary orbits, as discovered in 1801, 150 years after Kepler's death.\n\nKepler's previous book \"Astronomia nova\" related the discovery of the first two of the principles that we know today as Kepler's laws. The third law, which shows a constant proportionality between the cube of the semi-major axis of a planet's orbit and the square of the time of its orbital period, is set out in Chapter 5 of this book, immediately after a long digression on astrology.\n\nA copy of the 1619 edition was stolen from the National Library of Sweden in the 1990s.\n\nA small number of recent compositions either make reference to or are based on the concepts of Harmonices Mundi or Harmony of the Spheres. The most notable of these are:\n\n\n\n"}
{"id": "58479429", "url": "https://en.wikipedia.org/wiki?curid=58479429", "title": "Ilona Palásti", "text": "Ilona Palásti\n\nIlona Palásti (1924–1991) was a Hungarian mathematician who worked at the Alfréd Rényi Institute of Mathematics. She is known for her research in discrete geometry, geometric probability, and the theory of random graphs.\nWith Alfréd Rényi and others, she was considered to be one of the members of the Hungarian School of Probability.\n\nIn connection to the Erdős distinct distances problem, Palásti studied the existence of point sets for which the formula_1th least frequent distance occurs formula_1 times. That is, in such points there is one distance that occurs only once, another distance that occurs exactly two times, a third distance that occurs exactly three times, etc. For instance, three points with this structure must form an isosceles triangle. Any formula_3 evenly-spaced points on a line or circular arc also have the same property, but Paul Erdős asked whether this is possible for points in general position (no three on a line, and no four on a circle). Palásti found an eight-point set with this property, and showed that for any number of points between three and eight (inclusive) there is a subset of the hexagonal lattice with this property. Palásti's eight-point example remains the largest known.\n\nAnother of Palásti's results in discrete geometry concerns the number of triangular faces in an arrangement of lines. When no three lines may cross at a single point, she and Zoltán Füredi found sets of formula_3 lines, subsets of the diagonals of a regular formula_5-gon, having formula_6 triangles. This remains the best lower bound known for this problem, and differs from the upper bound by only formula_7 triangles.\n\nIn geometric probability, Palásti is known for her conjecture on random sequential adsorption, also known in the one-dimensional case as \"the parking problem\". In this problem, one places non-overlapping balls within a given region, one at a time with random locations, until no more can be placed. Palásti conjectured that the average packing density in formula_8-dimensional space could be computed as the formula_8th power of the one-dimensional density. Although her conjecture led to subsequent research in the same area, it has been shown to be inconsistent with the actual average packing density in dimensions two through four.\n\nPalásti's results in the theory of random graphs include bounds on the probability that a random graph has a Hamiltonian circuit, and on the probability that a random directed graph is strongly connected.\n"}
{"id": "380191", "url": "https://en.wikipedia.org/wiki?curid=380191", "title": "Janko group", "text": "Janko group\n\nIn the area of modern algebra known as group theory, the Janko groups are the four sporadic simple groups \"J\", \"J\", \"J\" and \"J\" introduced by Zvonimir Janko. Unlike the Mathieu groups, Conway groups, or Fischer groups, the Janko groups do not form a series, and the relation among the four groups is mainly historical rather than mathematical.\n\nJanko constructed the first of these groups, \"J\", in 1965 and predicted the existence of \"J\" and \"J\". In 1976, he suggested the existence of \"J\". Later, \"J\", \"J\" and \"J\" were all shown to exist.\n\n\"J\" was the first sporadic simple group discovered in nearly a century: until then only the Mathieu groups were known, \"M\" and \"M\" having been found in 1861, and \"M\", \"M\" and \"M\" in 1873. The discovery of \"J\" caused a great \"sensation\" and \"surprise\" among group theory specialists. This began the modern theory of sporadic groups. \n\nAnd in a sense, \"J\" ended it. It would be the last sporadic group (and, since the non-sporadic families had already been found, the last finite simple group) predicted and discovered, though this could only be said in hindsight when the Classification theorem was completed. \n"}
{"id": "5753816", "url": "https://en.wikipedia.org/wiki?curid=5753816", "title": "Jean Cavaillès", "text": "Jean Cavaillès\n\nJean Cavaillès (; ; May 15, 1903 – February 17, 1944) was a French philosopher and logician who specialized in philosophy of mathematics and philosophy of science. He took part in the French Resistance within the \"Libération\" movement and was shot by the Gestapo on February 17, 1944.\n\nCavaillès was born in Saint-Maixent, Deux-Sèvres. After passing his first baccalauréat in 1919 and baccalauréats in mathematics and philosophy the following year, he studied at the Lycée Louis-le-Grand, including two years of \"\", before entering the École Normale Supérieure in 1923, reading philosophy. In 1927 he successfully passed the \"agrégation\" competitive exam. He began graduate studies in Philosophy in 1928 under the supervision of Léon Brunschvicg. Cavaillès won a Rockefeller Foundation scholarship in 1929-1930. In 1931 he travelled extensively in Germany; in Göttingen he conceived, jointly with Emmy Noether, the project of publishing the Cantor--Dedekind correpsondence. He was a teaching assistant at the École Normale Supérieure between 1929 and 1935, then teacher at the Lycée d'Amiens (now ) in 1936. In 1937, he successfully defended his doctoral theses at the University of Paris and became a Doctor of Letters in Philosophy. He was then appointed in Logic and in General Philosophy at the University of Strasbourg.\n\nAfter the outbreaks of World War II, he was mobilized in 1939 as an infantry lieutenant with the 43rd Regiment, and was later attached to the Staff of the 4th Colonial Division. He was honoured for bravery twice, and was captured on June 11, 1940. At the end of July 1940 he escaped from Belgium and fled to Clermont-Ferrand, where the university of Strasbourg was re-organized.\n\nAt the end of December 1940, he met Emmanuel d'Astier de la Vigerie, with whom he created a small group of resistance fighters, known as \"the Last Column\". To reach a broader audience, it was decided to create a newspaper, which was to become \"Libération\", the mouthpiece of both \"Libération-Sud\" and \"Libération-Nord\". Cavaillès took an active part in editing the paper. The first edition appeared in July 1941.\n\nHe was appointed professor at the Sorbonne in 1941, and left Clermont-Ferrand for Paris, where he helped form the Libération-Nord resistance group, becoming part of its management committee.\n\nIn April 1942, at the instigation of Christian Pineau, the central Office of Information and Action (BCRA) of London charged him with the task of forming an intelligence network in the Northern Zone, known as \"Cohors\". He was ordered by Christian Pineau to pass into the Southern Zone, and Cavaillès headed the network and formed similar groups in Belgium and the north of France.\n\nIn Narbonne he was arrested with Pineau by the French police in September 1942. After a failed attempt at escaping to London, he was interned in Montpellier at the Saint-Paul d' Eyjeaux prison camp from where he escaped at the end of December 1942. The book Cavaillès wrote in prison in Montpellier in 1942 was published posthumously in 1946, edited by the epistemologist Georges Canguilhem and the mathematician Charles Ehresmann under the title \"Sur la logique et la theorie de la science\".\n\nDenounced as a public enemy by the Vichy regime, and sought by the police, he fled clandestinely to London in February 1943. There he met General Charles de Gaulle on several occasions.\n\nBack in France on April 15 he resigned from the management Committee of the \"Libération\" movement in order to dedicate himself entirely to direct action. He was in charge of the sabotage of the stores of the \"Kriegsmarine\" in Brittany and German radio installations on the coast.\n\nBetrayed by one of his liaison officers, he was arrested on August 28, 1943 in Paris with his sister and her brother-in-law. Tortured, imprisoned in Fresnes then in Compiègne, he was transferred to the Citadel from Arras and was shot on February 17, 1944. Buried in Arras under a wooden cross marked \"unknown n°5\", his body was exhumed in 1946 to be buried in the Crypt in the Sorbonne, in Paris.\n\nThe Centre Cavaillès de l'École Normale Supérieure was established in Paris in 1969, at 3e étage au 29 rue d'Ulm, as Centre for the Study of the History and Philosophy of Science. At the formal opening, philosopher Georges Canguilhem said, \"A philosopher-mathematician loaded with explosives, lucid and reckless, resolute without optimism. If that's not a hero, what is a hero?\" (Translated from the original French language: \"Un philosophe mathématicien bourré d'explosifs, un lucide téméraire, un résolu sans optimisme. Si ce n'est pas un héros, qu'est-ce qu'un héros?)\n\nCavaillès is honored in the Heroes of the Resistance postage stamp set.\n\nIn \"L'Armée des ombres\", a 1969 film directed by Jean-Pierre Melville, the character of \"Luc Jardie\" (the Chief) was in part inspired by Cavaillès. Jardie's chief operative, recuperating from injuries in a hideaway, has only five books; the title of each is a publication of Cavaillès, though the author is shown as \"Luc Jardie.\"\n\n\n\n\n"}
{"id": "113222", "url": "https://en.wikipedia.org/wiki?curid=113222", "title": "Law of sines", "text": "Law of sines\n\nIn trigonometry, the law of sines, sine law, sine formula, or sine rule is an equation relating the lengths of the sides of a triangle (any shape) to the sines of its angles. According to the law,\n\nwhere , and are the lengths of the sides of a triangle, and , and are the opposite angles (see the figure to the right), while is the diameter of the triangle's circumcircle. When the last part of the equation is not used, the law is sometimes stated using the reciprocals;\n\nThe law of sines can be used to compute the remaining sides of a triangle when two angles and a side are known—a technique known as triangulation. It can also be used when two sides and one of the non-enclosed angles are known. In some such cases, the triangle is not uniquely determined by this data (called the \"ambiguous case\") and the technique gives two possible values for the enclosed angle.\n\nThe law of sines is one of two trigonometric equations commonly applied to find lengths and angles in scalene triangles, with the other being the law of cosines.\n\nThe law of sines can be generalized to higher dimensions on surfaces with constant curvature.\n\nThe area of any triangle can be written as one half of its base times its height. Selecting one side of the triangle as the base, the height of the triangle relative to that base is computed as the length of another side times the sine of the angle between the chosen side and the base. Thus, depending on the selection of the base the area of the triangle can be written as any of:\nMultiplying these by gives\n\nWhen using the law of sines to find a side of a triangle, an ambiguous case occurs when two separate triangles can be constructed from the data provided (i.e., there are two different possible solutions to the triangle). In the case shown below they are triangles and .\n\nGiven a general triangle, the following conditions would need to be fulfilled for the case to be ambiguous:\n\nIf all the above conditions are true, then both angles or produce a valid triangle, meaning that both of the following are true:\n\nFrom there we can find the corresponding and or and if required, where is the side bounded by angles and and bounded by and .\n\nWithout further information it is impossible to decide which is the triangle being asked for.\n\nThe following are examples of how to solve a problem using the law of sines.\n\nGiven: side , side , and angle . Angle A is desired.\n\nUsing the law of sines, we conclude that\n\nNote that the potential solution is excluded because that would necessarily give .\n\nIf the lengths of two sides of the triangle and are equal to , the third side has length , and the angles opposite the sides of lengths , , and are , , and respectively then\n\nIn the identity\n\nthe common value of the three fractions is actually the diameter of the triangle's circumcircle which dates back to Ptolemy.\nAs shown in the figure, let's have a circle with inscribedformula_10 and another inscribed formula_11 that passes through the circle's center O. The formula_12 has a central angle of \"formula_13\" thus formula_14. Since formula_15 is a right triangle,\n\nAngles formula_17 and formula_18 has the same central angle thus they are the same formula_19.\n\nRearranging,\n\nRepeating the process of creating formula_11 with other points respectively, gives\n\nwhere formula_23 is the radius of the circumscribing circle of the triangle.\n\nThe area of a triangle can be solved by formula_24. Substituting the sine law to the equation,\n\nTaking formula_26 as the circumscribing radius, \n\nIt can also be shown that this quantity is equal to\n\nwhere is the area of the triangle and is the semiperimeter\n\nThe second equality above readily simplifies to Heron's formula for the area. Neatly, it can be written as\n\nwhere \"R\" is the radius of the triangle's circumcircle.\n\nThe law of sines takes on a similar form in the presence of curvature.\n\nIn the spherical case, the formula is:\n\nHere, , , and are the angles at the center of the sphere subtended by the three arcs of the spherical surface triangle , , and , respectively. , , and are the surface angles opposite their respective arcs.\nHere is a simple derivation. Consider a unit sphere with three unit vectors , and drawn from the origin to the vertices of the triangle. Thus the angles , , and are the angles , , and , respectively. The arc subtends an angle of magnitude at the centre. Introduce a Cartesian basis with along the -axis and in the -plane making an angle with the -axis. The vector projects to in the -plane and the angle between and the -axis is . Therefore, the three vectors have components:\n\nThe scalar triple product, is the volume of the parallelepiped formed by the position vectors of the vertices of the spherical triangle , and . This volume is invariant to the specific coordinate system used to represent , and . The value of the scalar triple product is the 3 × 3 determinant with , and as its rows. With the -axis along the square of this determinant is\nRepeating this calculation with the -axis along gives , while with the -axis along it is . Equating these expressions and dividing throughout by gives \nwhere is the volume of the parallelepiped formed by the position vector of the vertices of the spherical triangle.\n\nIt is easy to see how for small spherical triangles, when the radius of the sphere is much greater than the sides of the triangle, this formula becomes the planar formula at the limit, since\n\nand the same for and .\n\nIn hyperbolic geometry when the curvature is −1, the law of sines becomes\n\nIn the special case when is a right angle, one gets\n\nwhich is the analog of the formula in Euclidean geometry expressing the sine of an angle as the opposite side divided by the hypotenuse.\n\nDefine a generalized sine function, depending also on a real parameter :\n\nThe law of sines in constant curvature reads as\n\nBy substituting , , and , one obtains respectively the Euclidean, spherical, and hyperbolic cases of the law of sines described above.\n\nLet indicate the circumference of a circle of radius in a space of constant curvature . Then . Therefore the law of sines can also be expressed as:\n\nThis formulation was discovered by János Bolyai.\n\nFor an -dimensional simplex (i.e., triangle (), tetrahedron (), pentatope (), etc.) in -dimensional Euclidean space, the absolute value of the polar sine of the normal vectors of the faces that meet at a vertex, divided by the hyperarea of the face opposite the vertex is independent of the choice of the vertex. Writing for the hypervolume of the -dimensional simplex and for the product of the hyperareas of its -dimensional faces, the common ratio is\n\nFor example, a tetrahedron has four triangular faces. The absolute value of the polar sine of the normal vectors to the three faces that share a vertex, divided by the area of the fourth face will not depend upon the choice of the vertex:\n\nAccording to Ubiratàn D'Ambrosio and Helaine Selin, the spherical law of sines was discovered in the 10th century. It is variously attributed to Abu-Mahmud Khojandi, Abu al-Wafa' Buzjani, Nasir al-Din al-Tusi and Abu Nasr Mansur.\n\nIbn Muʿādh al-Jayyānī's \"The book of unknown arcs of a sphere\" in the 11th century contains the general law of sines. The plane law of sines was later stated in the 13th century by Nasīr al-Dīn al-Tūsī. In his \"On the Sector Figure\", he stated the law of sines for plane and spherical triangles, and provided proofs for this law.\n\nAccording to Glen Van Brummelen, \"The Law of Sines is really Regiomontanus's foundation for his solutions of right-angled triangles in Book IV, and these solutions are in turn the bases for his solutions of general triangles.\" Regiomontanus was a 15th-century German mathematician.\n\n\n"}
{"id": "6563584", "url": "https://en.wikipedia.org/wiki?curid=6563584", "title": "MIME Object Security Services", "text": "MIME Object Security Services\n\nMIME Object Security Services (MOSS) is a protocol that uses the multipart/signed and multipart/encrypted framework to apply digital signature and encryption services to MIME objects.\n\nThe services are offered through the use of end-to-end cryptography between an originator and a recipient at the application layer. Asymmetric (public key) cryptography is used in support of the digital signature service and encryption key management. Symmetric (secret key) cryptography is used in support of the encryption service. The procedures are intended to be compatible with a wide range of public key management approaches, including both ad hoc and certificate-based schemes. Mechanisms are provided to support many public key management approaches.\n\nMOSS was never widely deployed and is now abandoned, largely due to the popularity of PGP.\n\n"}
{"id": "55677839", "url": "https://en.wikipedia.org/wiki?curid=55677839", "title": "Matrix variate Dirichlet distribution", "text": "Matrix variate Dirichlet distribution\n\nIn statistics, the matrix variate Dirichlet distribution is a generalization of the matrix variate beta distribution.\n\nSuppose formula_1 are formula_2 positive definite matrices with formula_3, where formula_4 is the formula_2 identity matrix. Then we say that the formula_6 have a matrix variate Dirichlet distribution, formula_7, if their joint probability density function is\n\nwhere formula_9 and formula_10 is the multivariate beta function.\n\nIf we write formula_11 then the PDF takes the simpler form\n\non the understanding that formula_13.\n\nSuppose formula_14 are independently distributed Wishart formula_2 positive definite matrices. Then, defining formula_16 (where formula_17 is the sum of the matrices and formula_18 is any reasonable factorization of formula_19), we have\n\nIf formula_21, and if formula_22, then:\n\nAlso, with the same notation as above, the density of formula_24 is given by\n\nwhere we write formula_26.\n\nSuppose formula_21 and suppose that formula_28 is a partition of formula_29 (that is, formula_30 and formula_31 if formula_32). Then, writing formula_33 and formula_34 (with formula_35), we have:\n\nSuppose formula_21. Define\n\nwhere formula_39 is formula_40 and formula_41 is formula_42. Writing the Schur complement formula_43 we have\n\nand\n\n\nA. K. Gupta and D. K. Nagar 1999. \"Matrix variate distributions\". Chapman and Hall.\n"}
{"id": "25312886", "url": "https://en.wikipedia.org/wiki?curid=25312886", "title": "Modal depth", "text": "Modal depth\n\nIn modal logic, the modal depth of a formula is the deepest nesting of modal operators (commonly formula_1 and formula_2). Modal formulas without modal operators have a modal depth of zero.\n\nModal depth can be defined as follows. Let formula_3 be a function that computes the modal depth for a modal formula formula_4:\n\nThe following computation gives the modal depth of formula_15:\n\nThe modal depth of a formula indicates 'how far' one needs to look in a Kripke model when checking the validity of the formula. For each modal operator, one needs to transition from a world in the model to a world that is accessible through the accessibility relation. The modal depth indicates the longest 'chain' of transitions from a world to the next that is needed to verify the validity of a formula.\n\nFor example, to check whether formula_22, one needs to check whether there exists an accessible world formula_23 for which formula_24. If that is the case, one needs to check whether there is also a world formula_25 such that formula_26 and formula_25 is accessible from formula_23. We have made two steps from the world formula_29 (from formula_29 to formula_23 and from formula_23 to formula_25) in the model to determine whether the formula holds; this is, by definition, the modal depth of that formula.\n\nThe modal depth is an upper bound (inclusive) on the number of transitions as for boxes, a modal formula is also true whenever a world has no accessible worlds (i.e., formula_34 holds for all formula_35 in a world formula_29 when formula_37, where formula_38 is the set of worlds and formula_39 is the accessibility relation). To check whether formula_40, it may be needed to take two steps in the model but it could be less, depending on the structure of the model. Suppose no worlds are accessible in formula_29; the formula now trivially holds by the previous observation about the validity of formulas with a box as outer operator.\n"}
{"id": "918609", "url": "https://en.wikipedia.org/wiki?curid=918609", "title": "Natural number object", "text": "Natural number object\n\nIn category theory, a natural number object (NNO) is an object endowed with a recursive structure similar to natural numbers. More precisely, in a category E with a terminal object 1, an NNO \"N\" is given by:\n\n\nsuch that for any object \"A\" of E, global element \"q\" : 1 → \"A\", and arrow \"f\" : \"A\" → \"A\", there exists a unique arrow \"u\" : \"N\" → \"A\" such that:\n\n\nIn other words, the triangle and square in the following diagram commute.\n\nThe pair (\"q\", \"f\") is sometimes called the \"recursion data\" for \"u\", given in the form of a recursive definition:\n\n\nThe above definition is the universal property of NNOs, meaning they are defined up to canonical isomorphism. If the arrow \"u\" as defined above merely has to exist, \"i.e.\" uniqueness is not required, then \"N\" is called a \"weak\" NNO.\n\nNNOs in cartesian closed categories (CCCs) or topoi are sometimes defined in the following equivalent way (due to Lawvere): for every pair of arrows \"g\" : \"A\" → \"B\" and \"f\" : \"B\" → \"B\", there is a unique \"h\" : \"N\" × \"A\" → \"B\" such that the squares in the following diagram commute.\n\nThis same construction defines weak NNOs in cartesian categories that are not cartesian closed.\n\nIn a category with a terminal object 1 and binary coproducts (denoted by +), an NNO can be defined as the initial algebra of the endofunctor that acts on objects by and on arrows by .\n\n\n\n\n\n\n"}
{"id": "25235416", "url": "https://en.wikipedia.org/wiki?curid=25235416", "title": "Nehari manifold", "text": "Nehari manifold\n\nIn the calculus of variations, a branch of mathematics, a Nehari manifold is a manifold of functions, whose definition is motivated by the work of . It is a differential manifold associated to the Dirichlet problem for the semilinear elliptic partial differential equation\n\nHere Δ is the Laplacian on a bounded domain Ω in R.\n\nThere are infinitely many solutions to this problem. Solutions are precisely the critical points for the energy functional\n\non the Sobolev space . The Nehari manifold is defined to be the set of such that\nSolutions to the original variational problem that lie in the Nehari manifold are (constrained) minimizers of the energy, and so direct methods in the calculus of variations can be brought to bear.\n\nMore generally, given a suitable functional \"J\", the associated Nehari manifold is defined as the set of functions \"u\" in an appropriate function space for which\n\nHere \"J\"′ is the functional derivative of \"J\".\n\n"}
{"id": "2633325", "url": "https://en.wikipedia.org/wiki?curid=2633325", "title": "Norman Macleod Ferrers", "text": "Norman Macleod Ferrers\n\nNorman Macleod Ferrers (11 August 1829 – 31 January 1903) was a British mathematician and university administrator and editor of a mathematical journal.\n\nFerrers was educated at Eton College before studying at Gonville and Caius College, Cambridge, where he was Senior Wrangler in 1851. He was appointed to a Fellowship at the college in 1852, was called to the bar in 1855 and was ordained deacon in 1859 and priest in 1860. In 1880, he was appointed Master of the college, and served as vice-chancellor of Cambridge University from 1884 to 1885.\n\nFerrers made many contributions to mathematical literature. From 1855 to 1891 he worked with J. J. Sylvester as editors, with others, in publishing The Quarterly Journal of Pure and Applied Mathematics. Ferrers assembled the papers of George Green for publication in 1871.\n\nIn 1861 he published \"An Elementary Treatise on Trilinear Co-ordinates\". \nOne of his early contributions was on Sylvester's development of Poinsot's representation of the motion of a rigid body about a fixed point.\nHe pointed out a conjugacy in integer partition diagrams, which are accordingly called Ferrers diagrams and are closely related to Young diagrams.\nIn 1871 he first suggested to extend the equations of motion with nonholonomic constraints.\nHis another treatise on \"Spherical Harmonics,\" published in 1877, presented many original features. \nIn 1881 he studied Kelvin's investigation of the law of distribution of electricity in equilibrium on an uninfluenced spherical bowl \nand made the addition of finding the potential at any point of space in zonal harmonics.\n\nHe died at the College Lodge on 31 January 1903.\n\nFerrers was cited by Jacques Riguet in Comptes Rendus for 1952 in connection with binary relations. Ferrer relations satisfy the inclusion formula_1 using the notions and symbols of algebraic logic.\n\nOn 3 April 1866, he married Emily, daughter of John Lamb, dean of Bristol cathedral. \nThey had four sons and one daughter.\n\n\n"}
{"id": "14597235", "url": "https://en.wikipedia.org/wiki?curid=14597235", "title": "Ordered geometry", "text": "Ordered geometry\n\nOrdered geometry is a form of geometry featuring the concept of intermediacy (or \"betweenness\") but, like projective geometry, omitting the basic notion of measurement. Ordered geometry is a fundamental geometry forming a common framework for affine, Euclidean, absolute, and hyperbolic geometry (but not for projective geometry).\n\nMoritz Pasch first defined a geometry without reference to measurement in 1882. His axioms were improved upon by Peano (1889), Hilbert (1899), and Veblen (1904). Euclid anticipated Pasch's approach in definition 4 of \"The Elements\": \"a straight line is a line which lies evenly with the points on itself\".\n\nThe only primitive notions in ordered geometry are points A, B, C, ... and the relation of intermediacy [ABC] which can be read as \"B is between A and C\".\n\nThe \"segment\" AB is the set of points P such that [APB].\n\nThe \"interval\" AB is the segment AB and its end points A and B.\n\nThe \"ray\" A/B (read as \"the ray from A away from B\") is the set of points P such that [PAB].\n\nThe \"line\" AB is the interval AB and the two rays A/B and B/A. Points on the line AB are said to be \"collinear\".\n\nAn \"angle\" consists of a point O (the \"vertex\") and two non-collinear rays out from O (the \"sides\").\n\nA \"triangle\" is given by three non-collinear points (called \"vertices\") and their three \"segments\" AB, BC, and CA.\n\nIf three points A, B, and C are non-collinear, then a \"plane\" ABC is the set of all points collinear with pairs of points on one or two of the sides of triangle ABC.\n\nIf four points A, B, C, and D are non-coplanar, then a \"space\" (3-space) ABCD is the set of all points collinear with pairs of points selected from any of the four \"faces\" (planar regions) of the tetrahedron ABCD.\n\n\nThese axioms are closely related to Hilbert's axioms of order. For a comprehensive survey of axiomatizations of ordered geometry see.\n\nThe Sylvester–Gallai theorem can be proven within ordered geometry.\n\nGauss, Bolyai, and Lobachevsky developed a notion of parallelism which can be expressed in ordered geometry.\n\nTheorem (existence of parallelism): Given a point A and a line r, not through A, there exist exactly two limiting rays from A in the plane Ar which do not meet r. So there is a \"parallel\" line through A which does not meet r.\n\nTheorem (transmissibility of parallelism): The parallelism of a ray and a line is preserved by adding or subtracting a segment from the beginning of a ray.\n\nThe symmetry of parallelism cannot be proven in ordered geometry. Therefore, the \"ordered\" concept of parallelism does not form an equivalence relation on lines.\n\n"}
{"id": "40964822", "url": "https://en.wikipedia.org/wiki?curid=40964822", "title": "Peter McMullen", "text": "Peter McMullen\n\nPeter McMullen (born 11 May 1942) is a British mathematician, a professor emeritus of mathematics at University College London.\n\nMcMullen earned bachelor's and master's degrees from Trinity College, Cambridge, and taught at Western Washington University from 1968 to 1969.\n\nHe is known for his work in polyhedral combinatorics and discrete geometry, and in particular for proving what was then called the upper bound conjecture and now is the upper bound theorem. This result states that cyclic polytopes have the maximum possible number of faces among all polytopes with a given dimension and number of vertices. McMullen also formulated the g-conjecture, later the g-theorem of Louis Billera, Carl W. Lee, and Richard P. Stanley, characterizing the \"f\"-vectors of simplicial spheres.\n\nMcMullen was invited to speak at the 1974 International Congress of Mathematicians in Vancouver; his contribution there had the title \"Metrical and combinatorial properties of convex polytopes\".\n\nHe was elected as a foreign member of the Austrian Academy of Sciences in 2006. In 2012 he became an inaugural fellow of the American Mathematical Society.\n\n\n\n"}
{"id": "214303", "url": "https://en.wikipedia.org/wiki?curid=214303", "title": "Piero della Francesca", "text": "Piero della Francesca\n\nPiero della Francesca ( ; c. 1415 – 12 October 1492) was an Italian painter of the Early Renaissance. As testified by Giorgio Vasari in his \"Lives of the Most Excellent Painters, Sculptors, and Architects\", to contemporaries he was also known as a mathematician and geometer. Nowadays Piero della Francesca is chiefly appreciated for his art. His painting is characterized by its serene humanism, its use of geometric forms and perspective. His most famous work is the cycle of frescoes \"The History of the True Cross\" in the church of San Francesco in the Tuscan town of Arezzo.\n\nPiero was born and died in the town of Borgo Santo Sepolcro, modern-day Tuscany, to Benedetto de' Franceschi, a tradesman, and Romana di Perino da Monterchi, members of the Florentine and Tuscan Franceschi noble family.\n\nHe was most probably apprenticed to the local painter Antonio di Giovanni d'Anghiari, because in documents about payments it is noted that he was working with Antonio in 1432 and May 1438. He certainly took notice of the work of some of the Sienese artists active in San Sepolcro during his youth; e.g. Sassetta. In 1439 Piero received, together with Domenico Veneziano, payments for his work on frescoes for the church of Sant'Egidio in Florence, now lost. In Florence he must have met leading masters like Fra Angelico, Luca della Robbia, Donatello, and Brunelleschi. The classicism of Masaccio's frescoes and his majestic figures in the Santa Maria del Carmine were for him an important source of inspiration. Dating of Piero's undocumented work is difficult because his style does not seem to have developed over the years.\n\nIn 1442, Piero was listed as eligible for the City Council of Sansepolcro. Three years later, he received a commission to paint the \"Madonna della Misericordia\" altarpiece for the church of the Misericordia in Sansepolcro, which was completed in the early 1460s. In 1449 he executed several frescoes in the Castello Estense and the church of Sant'Andrea of Ferrara, now also lost. His influence was particularly strong in the later Ferrarese allegorical works of Cosimo Tura.\n\nTwo years later he was in Rimini, working for the condottiero Sigismondo Pandolfo Malatesta. In 1451, during that sojourn, he executed the famous fresco of \"St. Sigismund and Sigismondo Pandolfo Malatesta\" in the Tempio Malatestiano, as well as a portrait of Sigismondo. In Rimini, Piero may have met the famous Renaissance mathematician and architect Leon Battista Alberti, who had redesigned the Tempio Malatestiano, although it is known that Alberti directed the execution of his designs for the church by correspondence with his building supervisor. Thereafter Piero was active in Ancona, Pesaro and Bologna.\n\nIn 1454, he signed a contract for the \"Polyptych of Saint Augustine\" in the church of Sant'Agostino in Sansepolcro. The central panel of this polyptych is lost, and the four panels of the wings, with representations of saints, are now scattered around the world. A few years later, summoned by Pope Nicholas V, he moved to Rome, where he executed frescoes in the Basilica di Santa Maria Maggiore, of which only fragments remain. Two years later he was again in the Papal capital, painting frescoes in the Vatican Palace, which have since been destroyed.\n\nThe \"Baptism of Christ\", now in the National Gallery in London, was executed in about 1460 for the high altar of the church of the Priory of S. Giovanni Battista at Sansepolcro. Other notable mature works of Piero della Francesca are the frescoes of the \"Resurrection of Christ\" in Sansepolcro, and the \"Madonna del parto\" in Monterchi, near Sansepolcro.\n\nIn 1452, Piero della Francesca was called to Arezzo to replace Bicci di Lorenzo in painting the frescoes of the basilica of San Francesco. The work was finished before 1466, probably between 1452 and 1456.\nThe cycle of frescoes, depicting the Legend of the True Cross, is generally considered among his masterworks and those of Renaissance painting in general. The story in these frescoes derives from legendary medieval sources as to how timber relics of the True Cross came to be found. These stories were collected in the \"Golden Legend\" of Jacopo da Varazze (Jacopo da Varagine) of the mid-13th century.\nBetween 1469 and 1486 Piero della Francesca worked repeatedly in the service of Count Federico III da Montefeltro (Duke in 1474). According to Giorgio Vasari, Piero would have worked for Federico's father Guidantonio, who died in February 1443. However, this is unlikely because this statement is not confirmed by documents or paintings. Vasari may have confused Guidantonio with Federico. \"The Flagellation\" is generally considered Piero's oldest work in Urbino (dating c. 1455–1470). It is one of the most famous and controversial pictures of the early Renaissance. As discussed in its own entry, it is marked by an air of geometric sobriety, in addition to presenting a perplexing enigma as to the nature of the three men standing at the foreground.\n\nAnother famous work painted in Urbino is the \"Double Portrait\" of Federico and his wife Battista Sforza, in the Uffizi. The portraits in profile take their inspiration from large bronze medals and stucco roundels with the official portraits of Fedederico and his wife. Other paintings made in Urbino are the monumental \"Montefeltro Altarpiece\" in the Brera Gallery in Milan and probably also the \"Madonna of Senigallia\".\n\nIn Urbino Piero met the painters Melozzo da Forlì, Fra Carnevale and the Flemish Justus van Gent (or Joos van Wassenhove or Giusto di Gant), the mathematician Fra Luca Pacioli, the architect Francesco di Giorgio Martini and probably also Leon Battista Alberti.\n\nIn his later years, painters such as Perugino and Luca Signorelli frequently visited his workshop. It is documented that Piero rented a house in Rimini in 1482. Although he may have given up painting in his later years, Vasari's remarks that he went blind at old age and at the age of sixty, have to be doubted, since in 1485 he completed his treatise on regular solids, dedicated to Guidobaldo da Montefeltro, son and heir of Duke Federico, in his own fine handwriting. Piero made his will in 1487 and he died five years later, on 12 October 1492, in his own house in San Sepolcro. He left his possessions to his family and the church. He died the same day that Christopher Columbus arrived in the Americas.\n\nRecently, the Frick Collection in New York until 19 May 2013, collected seven of the eight extant paintings of Piero known to exist in the United States for exhibition. Of the seven pieces in the exhibit, critic Jerry Saltz writing in \"New York\" magazine singled out Piero's \"Virgin and Child Enthroned With Four Angels\" for its exemplary qualities. Saltz states that, \"The Virgin and child are elevated two steps. They are in a world itself apart from this world apart. Mary isn't looking at her child and looks instead at the rose he reaches for. You begin to glean the revelation she is having. The flower represents love, devotion, and beauty. It also symbolizes blood and the crown of thorns Christ will wear. This child who will suffer a horrendous death reaches for his acceptance of fate. Mary does not pull the flower back. You sense an inner agony, noticing her deep-blue robe open to reveal scarlet beneath, symbol of outward passion and pain to come. In the dead-center vertical line of the painting is Christ's right palm that will be nailed to the cross.\" Saltz accepted this oil painting as the most exceptional work of Piero on display in the exhibit.\n\nPiero's deep interest in the theoretical study of perspective and his contemplative approach to his paintings are apparent in all his work.\nIn his youth, Piero was trained in mathematics, which most likely was for mercantilism. Three treatises written by Piero are known to modern mathematicians: \"Abacus Treatise (Trattato d'Abaco)\", \"Short Book on the Five Regular Solids (Libellus de Quinque Corporibus Regularibus)\" and \"On Perspective for Painting\" (\"De Prospectiva Pingendi\"). The subjects covered in these writings include arithmetic, algebra, geometry and innovative work in both solid geometry and perspective. Much of Piero’s work was later absorbed into the writing of others, notably Luca Pacioli. Piero’s work on solid geometry appears in Pacioli’s \"De divina proportione\", a work illustrated by Leonardo da Vinci. Biographers of his patron, Federico da Montefeltro of Urbino, record that he was encouraged to pursue the interest in perspective which was shared by the Duke.\n\nIn the late 1450s, Piero copied and illustrated the following works of Archimedes: \"On the Sphere and the Cylinder; On the Measurement of the Circle; On Conoids and Spheroids; On Spirals; On the Equilibrium of Planes; On the Quadrature of the Parabola; The Sand Reckoner\". The manuscript consists of 82 folio leaves. It's held in the collection of the Biblioteca Riccardiana and it is a copy of the translation of the Archimedean corpus made by the Italian humanist Iacopo da San Cassiano (also known as Iacobus Cremonensis).\n\nBohuslav Martinů wrote a three movement work for orchestra entitled \"Les Fresques de Piero della Francesca\". Dedicated to Rafael Kubelik, it was premiered by Kubelik and the Vienna Philharmonic at the 1956 Salzburg Festival. Piero's geometrical perfection and the almost magic atmosphere of the light in his painting inspired modern painters like Giorgio de Chirico, Massimo Campigli, Felice Casorati and Balthus.\n\n\n"}
{"id": "44403744", "url": "https://en.wikipedia.org/wiki?curid=44403744", "title": "Pollination network", "text": "Pollination network\n\nA pollination network is a bipartite mutualistic network in which plants and pollinators are the nodes, and the pollination interactions form the links between these nodes. The pollination network is bipartite as interactions only exist between two distinct, non-overlapping sets of species, but not within the set: a pollinator can never be pollinated, unlike in a predator-prey network where a predator can be depredated. A pollination network is two-modal, i.e., it includes only links connecting plant and animal communities.\n\nA key feature of pollination networks is their nested design. A study of 52 mutualist networks (including plant-pollinator interactions and plant-seed disperser interactions) found that most of the networks were nested. This means that the core of the network is made up of highly connected generalists (a pollinator that visits many different species of plant), while specialized species interact with a subset of the species that the generalists interact with (a pollinator that visits few species of plant, which are also visited by generalist pollinators). As the number of interactions in a network increases, the degree of nestedness increases as well. One property that results from nested structure of pollination networks is an asymmetry in specialization, where specialist species are often interacting with some of the most generalized species. This is in contrast to the idea of reciprocal specialization, where specialist pollinators interact with specialist plants. Similar to the relationship between network complexity and network nestedness, the amount of asymmetry in specialization increases as the number of interactions increases.\n\nAnother feature that is common in pollination networks is modularity. Modularity occurs when certain groups of species within a network are much more highly connected to each other than they are with the rest of the network, with weak interactions connecting different modules. \nWithin modules it has been shown that individual species play certain roles. Highly specialized species often only interact with individuals within their own module and are known as ‘peripheral species’; more generalized species can be thought of as ‘hubs’ within their own module, with interactions between many different species; there are also species which are very generalized which can act as ‘connectors’ between their own module and other modules. A study of three separate networks, all of which showed modularity, revealed that hub species were always plants and not the insect pollinators. Previous work has found that networks will become nested at a smaller size (number of species) than that where networks frequently become modular.\n\nThere is substantial interest into the robustness of pollination networks to species loss and collapse, especially due to anthropogenic factors such as habitat destruction. The structure of a network is thought to affect how long it is able to persist after species decline begins. In particular, the nested structure of networks has been shown to protect against complete destruction of the network, because the core group of generalists are the most robust to extinction by habitat loss. Models specifically focused on the effects of habitat loss have shown that specialist species tend to go extinct first, while the last species to go extinct are the most generalized of the network. Other studies focusing specifically on the removal of different types of species showed that species decline is the fastest when removing the most generalized species. However, there have been contrasting results on how rapidly decline occurs with removal of these species. One study showed that even at the fastest rate, the decline was still linear. Another study revealed that with the removal of the most common pollinator species, the network showed a drastic collapse. In addition to focusing on the removal of species themselves, other work has emphasized the importance of studying the loss of interactions, as this will often precede species loss and may well accelerate the rate at which extinction occurs.\n\n\n"}
{"id": "32848692", "url": "https://en.wikipedia.org/wiki?curid=32848692", "title": "Q-Bessel polynomials", "text": "Q-Bessel polynomials\n\nIn mathematics, the \"q\"-Bessel polynomials are a family of basic hypergeometric orthogonal polynomials in the basic Askey scheme. give a detailed list of their properties.\n\nThe polynomials are given in terms of basic hypergeometric functions and the Pochhammer symbol by ：\n\nformula_2\n\n"}
{"id": "9902256", "url": "https://en.wikipedia.org/wiki?curid=9902256", "title": "Reform mathematics", "text": "Reform mathematics\n\nReform mathematics is an approach to mathematics education, particularly in North America. It is based on principles explained in 1989 by the National Council of Teachers of Mathematics (NCTM). The NCTM document, Curriculum and Evaluation Standards for School Mathematics, attempted to set forth a vision for K-12 (ages 5-18) mathematics education in the United States and Canada. Their recommendations were adopted by many education agencies, from local to federal levels through the 1990s. In 2000, NCTM revised its standards with the publication of Principles and Standards for School Mathematics (PSSM). Like the first publication, these updated standards have continued to serve as the basis for many states' mathematics standards, and for many federally funded textbook projects. The first standards gave a strong call for a de-emphasis on manual arithmetic in favor of students' discovering their own knowledge and conceptual thinking. The PSSM has taken a more balanced view, but still emphasizes conceptual thinking and problem solving.\n\nMathematics instruction in this style has been called \"standards-based mathematics\" or \"reform mathematics\". \n\nThe momentum for reform in mathematics education began in the early 1980s, as educators reacted to the \"new math\" of the 1960s and 1970s. The work of Piaget and other developmental psychologists was shifting the focus of mathematics educators from mathematics content to how children best learn mathematics. The National Council of Teachers of Mathematics summarized the state of current research with the publication of Curriculum and Evaluation Standards in 1989 and Principles and Standards for School Mathematics in 2000, bringing definition to the reform movement in North America.\n\nReform mathematics curricula challenge students to make sense of new mathematical ideas through explorations and projects, often in real contexts. Reform texts emphasize written and verbal communication, working in cooperative groups, making connections between concepts, and connections between representations. By contrast, \"traditional\" textbooks emphasize procedural mathematics and provide step-by-step examples with skill exercises.\n\nTraditional mathematics focuses on teaching algorithms that will lead to the correct answer. Because of this focus on application of algorithms, the traditional math student must always use the specific method that is being taught. This kind of algorithmic dependence is de-emphasized in reform mathematics. Reformers do not oppose correct answers, but prefer to focus students' attention on the process leading to the answer, rather than the answer itself. The presence of occasional errors is deemed less important than the overall thought process. Research has shown that children make fewer mistakes with calculations and remember algorithms longer when they understand the concepts underlying the methods they use. In general, children in reform classes perform at least as well as children in traditional classes on tests of calculation skill, and considerably better on tests of problem solving.\n\n\"Principles and Standards for School Mathematics\" was championed by educators, administrators and some mathematicians as raising standards for all students, but it was criticized by some for valuing understanding processes more than learning standard procedures. Parents, educators and some mathematicians opposing reform mathematics complained about students becoming confused and frustrated, claiming that it was an inefficient style of instruction characterized by frequent false starts. Proponents of reform mathematics countered that research showed that, when done correctly, students in reform math curricula learned basic math skills at least as well as those in traditional programs, and additionally understood the underlying concepts much better. Communities that adopted reform curricula generally saw increased math scores by their students. However, one study has found that first-grade students with a below-average aptitude in math responded best to teacher-directed instruction.\n\nDuring the 1990s, the development and large-scale adoption of curricula such as Mathland was criticized for partially or entirely abandoning teaching of standard arithmetic methods such as regrouping or common denominators. Protests from groups such as Mathematically Correct led to many districts and states abandoning such textbooks. Some states such as California revised their mathematics standards to partially or largely repudiate the basic beliefs of reform mathematics, and re-emphasize mastery of standard mathematics facts and methods.\n\nThe American Institutes for Research (AIR) reported in 2005 that the NCTM proposals \"risk exposing students to unrealistically advanced mathematics content in the early grades.\" This is in reference to NCTM's recommendation that algebraic concepts, such as understanding patterns and properties like commutativity (2+3=3+2), should be taught as early as first grade.\n\nSome, such as the 2008 National Mathematics Advisory Panel, called for a balance between reform and traditional mathematics teaching styles rather than a \"war\" between the two styles. In 2006 NCTM published its Curriculum Focal Points, which made clear that standard algorithms were to be included in all elementary school curricula, as well as activities aiming at conceptual understanding. \n\nA common misconception was that reform educators did not want children to learn the standard methods of arithmetic. As the NCTM Focal Points made clear, such methods were still the ultimate goal, but reformers believed that conceptual understanding should come first. Reform educators believed that such understanding is best pursued by allowing children at first to solve problems using their own understanding and methods. Under guidance from the teacher, students eventually arrive at an understanding of standard methods. Even the controversial NCTM Standards of 1989 did not call for abandoning standard algorithms, but instead recommended a decreased emphasis on complex paper-and-pencil computation drills and greater attention to mental computation, estimation skills, thinking strategies for mastering basic facts and conceptual understanding of arithmetic operations.\n\nDuring the peak of the controversy in the 1990s, unfavorable terminology for reform mathematics appeared in press and web articles, including \"Where's the math?\", \"anti-math\", \"math for dummies\", \"rainforest algebra\", \"math for women and minorities\", and \"new new math\". Most of these critical terms refer to the 1989 standards rather than the PSSM.\n\nBeginning in 2011, most states adopted the Common Core Standards, which attempted to incorporate reform ideas, rigor (introducing ideas at a younger age) and a leaner math curriculum.\n\n\n"}
{"id": "1825296", "url": "https://en.wikipedia.org/wiki?curid=1825296", "title": "Silverman–Toeplitz theorem", "text": "Silverman–Toeplitz theorem\n\nIn mathematics, the Silverman–Toeplitz theorem, first proved by Otto Toeplitz, is a result in summability theory characterizing matrix summability methods that are regular. A regular matrix summability method is a matrix transformation of a convergent sequence which preserves the limit.\n\nAn infinite matrix formula_1 with complex-valued entries defines a regular summability method if and only if it satisfies all of the following properties:\n\nAn example is Cesaro summation, a matrix summability method with\n\n"}
{"id": "20847164", "url": "https://en.wikipedia.org/wiki?curid=20847164", "title": "Stone functor", "text": "Stone functor\n\nIn mathematics, the Stone functor is a functor \"S\": Top → Bool, where Top is the category of topological spaces and Bool is the category of Boolean algebras and Boolean homomorphisms. It assigns to each topological space \"X\" the Boolean algebra \"S\"(\"X\") of its clopen subsets, and to each morphism \"f\": \"X\" → \"Y\" in Top (i.e., a continuous map \"f\": \"Y\" → \"X\") the homomorphism \"S\"(\"f\"): \"S\"(\"X\") → \"S\"(\"Y\") given by \"S\"(\"f\")(\"Z\") = \"f\"[\"Z\"].\n\n\n"}
{"id": "450062", "url": "https://en.wikipedia.org/wiki?curid=450062", "title": "Subgraph isomorphism problem", "text": "Subgraph isomorphism problem\n\nIn theoretical computer science, the subgraph isomorphism problem is a computational task in which two graphs \"G\" and \"H\" are given as input, and one must determine whether \"G\" contains a subgraph that is isomorphic to \"H\".\nSubgraph isomorphism is a generalization of both the maximum clique problem and the problem of testing whether a graph contains a Hamiltonian cycle, and is therefore NP-complete. However certain other cases of subgraph isomorphism may be solved in polynomial time.\n\nSometimes the name subgraph matching is also used for the same problem. This name puts emphasis on finding such a subgraph as opposed to the bare decision problem.\n\nTo prove subgraph isomorphism is NP-complete, it must be formulated as a decision problem. The input to the decision problem is a pair of graphs \"G\" and \"H\". The answer to the problem is positive if \"H\" is isomorphic to a subgraph of \"G\", and negative otherwise.\n\nFormal question:\n\nLet formula_1, formula_2 be graphs. Is there a subgraph formula_3 such that formula_4? I.e., does there exist a bijection formula_5 such that formula_6?\n\nThe proof of subgraph isomorphism being NP-complete is simple and based on reduction of the clique problem, an NP-complete decision problem in which the input is a single graph \"G\" and a number \"k\", and the question is whether \"G\" contains a complete subgraph with \"k\" vertices. To translate this to a subgraph isomorphism problem, simply let \"H\" be the complete graph \"K\"; then the answer to the subgraph isomorphism problem for \"G\" and \"H\" is equal to the answer to the clique problem for \"G\" and \"k\". Since the clique problem is NP-complete, this polynomial-time many-one reduction shows that subgraph isomorphism is also NP-complete.\n\nAn alternative reduction from the Hamiltonian cycle problem translates a graph \"G\" which is to be tested for Hamiltonicity into the pair of graphs \"G\" and \"H\", where \"H\" is a cycle having the same number of vertices as \"G\". Because the Hamiltonian cycle problem is NP-complete even for planar graphs, this shows that subgraph isomorphism remains NP-complete even in the planar case.\n\nSubgraph isomorphism is a generalization of the graph isomorphism problem, which asks whether \"G\" is isomorphic to \"H\": the answer to the graph isomorphism problem is true if and only if \"G\" and \"H\" both have the same numbers of vertices and edges and the subgraph isomorphism problem for \"G\" and \"H\" is true. However the complexity-theoretic status of graph isomorphism remains an open question.\n\nIn the context of the Aanderaa–Karp–Rosenberg conjecture on the query complexity of monotone graph properties, showed that any subgraph isomorphism problem has query complexity Ω(\"n\"); that is, solving the subgraph isomorphism requires an algorithm to check the presence or absence in the input of Ω(\"n\") different edges in the graph.\n\n describes a recursive backtracking procedure for solving the subgraph isomorphism problem. Although its running time is, in general, exponential, it takes polynomial time for any fixed choice of \"H\" (with a polynomial that depends on the choice of \"H\"). When \"G\" is a planar graph (or more generally a graph of bounded expansion) and \"H\" is fixed, the running time of subgraph isomorphism can be reduced to linear time.\n\nAs subgraph isomorphism has been applied in the area of cheminformatics to find similarities between chemical compounds from their structural formula; often in this area the term substructure search is used. A query structure is often defined graphically using a structure editor program; SMILES based database systems typically define queries using SMARTS, a SMILES extension.\n\nThe closely related problem of counting the number of isomorphic copies of a graph \"H\" in a larger graph \"G\" has been applied to pattern discovery in databases, the bioinformatics of protein-protein interaction networks, and in exponential random graph methods for mathematically modeling social networks.\n\nThe problem is also of interest in artificial intelligence, where it is considered part of an array of pattern matching in graphs problems; an extension of subgraph isomorphism known as graph mining is also of interest in that area.\n\n\n"}
{"id": "151013", "url": "https://en.wikipedia.org/wiki?curid=151013", "title": "T-symmetry", "text": "T-symmetry\n\nT-symmetry or time reversal symmetry is the theoretical symmetry of physical laws under the transformation of time reversal:\nT-symmetry can be shown to be equivalent to the conservation of entropy, by Noether's Theorem. But as the second law of thermodynamics does not permit entropy to be conserved in general, it follows that the observable universe does not in general show symmetry under time reversal. In other words, time is said to be non-symmetric, or asymmetric, except for special equilibrium states when the second law of thermodynamics predicts the time symmetry to hold.\nHowever, quantum noninvasive measurements are predicted to violate time symmetry even in equilibrium, contrary to their classical counterparts, although this has not yet been experimentally confirmed.\n\nTime \"asymmetries\" are generally distinguished as among those...\n\nPhysicists also discuss the time-reversal invariance of local and/or macroscopic descriptions of physical systems, independent of the invariance of the underlying microscopic physical laws. \nFor example, Maxwell's equations with material absorption or Newtonian mechanics with friction are not time-reversal invariant at the macroscopic level where they are normally applied, even if they are invariant at the microscopic level; when one includes the atomic motions, the \"lost\" energy is translated into heat.\n\nOur daily experience shows that T-symmetry does not hold for the behavior of bulk materials. Of these macroscopic laws, most notable is the second law of thermodynamics. Many other phenomena, such as the relative motion of bodies with friction, or viscous motion of fluids, reduce to this, because the underlying mechanism is the dissipation of usable energy (for example, kinetic energy) into heat.\n\nThe question of whether this time-asymmetric dissipation is really inevitable has been considered by many physicists, often in the context of Maxwell's demon. The name comes from a thought experiment described by James Clerk Maxwell in which a microscopic demon guards a gate between two halves of a room. It only lets slow molecules into one half, only fast ones into the other. By eventually making one side of the room cooler than before and the other hotter, it seems to reduce the entropy of the room, and reverse the arrow of time. Many analyses have been made of this; all show that when the entropy of room and demon are taken together, this total entropy does increase. Modern analyses of this problem have taken into account Claude E. Shannon's relation between entropy and information. Many interesting results in modern computing are closely related to this problem — reversible computing, quantum computing and physical limits to computing, are examples. These seemingly metaphysical questions are today, in these ways, slowly being converted into hypotheses of the physical sciences.\n\nThe current consensus hinges upon the Boltzmann-Shannon identification of the logarithm of phase space volume with the negative of Shannon information, and hence to entropy. In this notion, a fixed initial state of a macroscopic system corresponds to relatively low entropy because the coordinates of the molecules of the body are constrained. As the system evolves in the presence of dissipation, the molecular coordinates can move into larger volumes of phase space, becoming more uncertain, and thus leading to increase in entropy.\n\nOne can, however, equally well imagine a state of the universe in which the motions of all of the particles at one instant were the reverse (strictly, the CPT reverse). Such a state would then evolve in reverse, so presumably entropy would decrease (Loschmidt's paradox). Why is 'our' state preferred over the other?\n\nOne position is to say that the constant increase of entropy we observe happens \"only\" because of the initial state of our universe. Other possible states of the universe (for example, a universe at heat death equilibrium) would actually result in no increase of entropy. In this view, the apparent T-asymmetry of our universe is a problem in cosmology: why did the universe start with a low entropy? This view, if it remains viable in the light of future cosmological observation, would connect this problem to one of the big open questions beyond the reach of today's physics — the question of \"initial conditions\" of the universe.\n\nAn object can cross through the event horizon of a black hole from the outside, and then fall rapidly to the central region where our understanding of physics breaks down. Since within a black hole the forward light-cone is directed towards the center and the backward light-cone is directed outward, it is not even possible to define time-reversal in the usual manner. The only way anything can escape from a black hole is as Hawking radiation.\n\nThe time reversal of a black hole would be a hypothetical object known as a white hole. From the outside they appear similar. While a black hole has a beginning and is inescapable, a white hole has an ending and cannot be entered. The forward light-cones of a white hole are directed outward; and its backward light-cones are directed towards the center.\n\nThe event horizon of a black hole may be thought of as a surface moving outward at the local speed of light and is just on the edge between escaping and falling back. The event horizon of a white hole is a surface moving inward at the local speed of light and is just on the edge between being swept outward and succeeding in reaching the center. They are two different kinds of horizons—the horizon of a white hole is like the horizon of a black hole turned inside-out.\n\nThe modern view of black hole irreversibility is to relate it to the second law of thermodynamics, since black holes are viewed as thermodynamic objects. Indeed, according to the Gauge–gravity duality conjecture, all microscopic processes in a black hole are reversible, and only the collective behavior is irreversible, as in any other macroscopic, thermal system.\n\nIn physical and chemical kinetics, T-symmetry of the mechanical microscopic equations implies two important laws: the principle of detailed balance and the Onsager reciprocal relations. T-symmetry of the microscopic description together with its kinetic consequences are called microscopic reversibility.\n\nClassical variables that do not change upon time reversal include:\n\nClassical variables that time reversal negates include:\n\nMost systems are asymmetric under time reversal, but there may be phenomena with symmetry. In classical mechanics, a velocity \"v\" reverses under the operation of \"T\", but an acceleration does not. Therefore, one models dissipative phenomena through terms that are odd in \"v\". However, delicate experiments in which known sources of dissipation are removed reveal that the laws of mechanics are time reversal invariant. Dissipation itself is originated in the second law of thermodynamics.\n\nThe motion of a charged body in a magnetic field, \"B\" involves the velocity through the Lorentz force term \"v\"×\"B\", and might seem at first to be asymmetric under \"T\". A closer look assures us that \"B\" also changes sign under time reversal. This happens because a magnetic field is produced by an electric current, \"J\", which reverses sign under \"T\". Thus, the motion of classical charged particles in electromagnetic fields is also time reversal invariant. (Despite this, it is still useful to consider the time-reversal non-invariance in a \"local\" sense when the external field is held fixed, as when the magneto-optic effect is analyzed. This allows one to analyze the conditions under which optical phenomena that locally break time-reversal, such as Faraday isolators and directional dichroism, can occur.) The laws of gravity also seem to be time reversal invariant in classical mechanics.\n\nIn physics one separates the laws of motion, called kinematics, from the laws of force, called dynamics. Following the classical kinematics of Newton's laws of motion, the kinematics of quantum mechanics is built in such a way that it presupposes nothing about the time reversal symmetry of the dynamics. In other words, if the dynamics are invariant, then the kinematics will allow it to remain invariant; if the dynamics is not, then the kinematics will also show this. The structure of the quantum laws of motion are richer, and we examine these next.\n\nThis section contains a discussion of the three most important properties of time reversal in quantum mechanics; chiefly,\nThe strangeness of this result is clear if one compares it with parity. If parity transforms a pair of quantum states into each other, then the sum and difference of these two basis states are states of good parity. Time reversal does not behave like this. It seems to violate the theorem that all abelian groups be represented by one-dimensional irreducible representations. The reason it does this is that it is represented by an anti-unitary operator. It thus opens the way to spinors in quantum mechanics.\n\nEugene Wigner showed that a symmetry operation \"S\" of a Hamiltonian is represented, in quantum mechanics either by a unitary operator, \"S\" = \"U\", or an antiunitary one, \"S\" = \"UK\" where \"U\" is unitary, and \"K\" denotes complex conjugation. These are the only operations that act on Hilbert space so as to preserve the \"length\" of the projection of any one state-vector onto another state-vector.\n\nConsider the parity operator. Acting on the position, it reverses the directions of space, so that \"PxP\" = −\"x\". Similarly, it reverses the direction of \"momentum\", so that \"PpP\" = −\"p\", where \"x\" and \"p\" are the position and momentum operators. This preserves the canonical commutator [\"x\", \"p\"] = \"iħ\", where \"ħ\" is the reduced Planck constant, only if \"P\" is chosen to be unitary, \"PiP\" = \"i\".\n\nOn the other hand, the \"time reversal\" operator \"T\", it does nothing to the x-operator, \"TxT\" = \"x\", but it reverses the direction of p, so that \"TpT\" = −\"p\". The canonical commutator is invariant only if \"T\" is chosen to be anti-unitary, i.e., \"TiT\" = −\"i\".\n\nAnother argument involves energy, the time-component of the momentum. If time reversal were implemented as a unitary operator, it would reverse the sign of the energy just as space-reversal reverses the sign of the momentum. This is not possible, because, unlike momentum, energy is always positive. Since energy in quantum mechanics is defined as the phase factor exp(-iEt) that one gets when one moves forward in time, the way to reverse time while preserving the sign of the energy is to also reverse the sense of \"i\", so that the sense of phases is reversed.\n\nSimilarly, any operation that reverses the sense of phase, which changes the sign of i, will turn positive energies into negative energies unless it also changes the direction of time. So every antiunitary symmetry in a theory with positive energy must reverse the direction of time. Every antiunitary operator can be written as the product of the time reversal operator and a unitary operator that does not reverse time.\n\nFor a particle with spin \"J\", one can use the representation\n\nwhere \"J\" is the \"y\"-component of the spin, and use of \"TJT\" = −J has been made.\n\nThis has an interesting consequence on the electric dipole moment (EDM) of any particle. The EDM is defined through the shift in the energy of a state when it is put in an external electric field: Δ\"e\" = d·\"E\" + \"E\"·δ·\"E\", where \"d\" is called the EDM and δ, the induced dipole moment. One important property of an EDM is that the energy shift due to it changes sign under a parity transformation. However, since d is a vector, its expectation value in a state |ψ> must be proportional to <ψ| \"J\" |ψ>, that is the expected spin. Thus, under time reversal, an invariant state must have vanishing EDM. In other words, a non-vanishing EDM signals both \"P\" and \"T\" symmetry-breaking.\n\nSome molecules, such as water, must have EDM irrespective of whether T is a symmetry. This is correct; if a quantum system has degenerate ground states that transform into each other under parity, then time reversal need not be broken to give EDM.\n\nExperimentally observed bounds on the electric dipole moment of the nucleon currently set stringent limits on the violation of time reversal symmetry in the strong interactions, and their modern theory: quantum chromodynamics. Then, using the CPT invariance of a relativistic quantum field theory, this puts strong bounds on strong CP violation.\n\nExperimental bounds on the electron electric dipole moment also place limits on theories of particle physics and their parameters.\n\nFor \"T\", which is an anti-unitary \"Z\" symmetry generator\n\nwhere Φ is a diagonal matrix of phases. As a result, \"U\" = Φ\"U\" and \"U\" = \"U\"Φ, showing that\n\nThis means that the entries in Φ are ±1, as a result of which one may have either \"T\" = ±1. This is specific to the anti-unitarity of \"T\". For a unitary operator, such as the parity, any phase is allowed.\n\nNext, take a Hamiltonian invariant under \"T\". Let |\"a\"> and \"T\"|\"a\"> be two quantum states of the same energy. Now, if \"T\" = −1, then one finds that the states are orthogonal: a result called Kramers' theorem. This implies that if \"T\" = −1, then there is a twofold degeneracy in the state. This result in non-relativistic quantum mechanics presages the spin statistics theorem of quantum field theory.\n\nQuantum states that give unitary representations of time reversal, i.e., have T=1, are characterized by a multiplicative quantum number, sometimes called the T-parity.\n\nTime reversal transformation for fermions in quantum field theories can be represented by an 8-component spinor in which the above-mentioned T-parity can be a complex number with unit radius. The CPT invariance is not a theorem but a better-to-have property in these class of theories.\n\nParticle physics codified the basic laws of dynamics into the standard model. This is formulated as a quantum field theory that has CPT symmetry, i.e., the laws are invariant under simultaneous operation of time reversal, parity and charge conjugation. However, time reversal itself is seen not to be a symmetry (this is usually called CP violation). There are two possible origins of this asymmetry, one through the mixing of different flavours of quarks in their weak decays, the second through a direct CP violation in strong interactions. The first is seen in experiments, the second is strongly constrained by the non-observation of the EDM of a neutron.\n\nTime reversal violation is unrelated to the second law of thermodynamics, because due to the conservation of the CPT symmetry, the effect of time reversal is to rename particles as antiparticles and \"vice versa\". Thus the second law of thermodynamics is thought to originate in the initial conditions in the universe.\n\nStrong measurements (both classical and quantum) are certainly disturbing, causing asymmetry due to second law of thermodynamics. However,\nnoninvasive measurements should not disturb the evolution so they are expected to be time-symmetric. Surprisingly, it is true only in classical physics but not quantum, even in a thermodynamically invariant equilibrium state.\n\n\n"}
{"id": "1387689", "url": "https://en.wikipedia.org/wiki?curid=1387689", "title": "Table of Lie groups", "text": "Table of Lie groups\n\nThis article gives a table of some common Lie groups and their associated Lie algebras.\n\nThe following are noted: the topological properties of the group (dimension; connectedness; compactness; the nature of the fundamental group; and whether or not they are simply connected) as well as on their algebraic properties (abelian; simple; semisimple).\n\nFor more examples of Lie groups and other related topics see the list of simple Lie groups; the Bianchi classification of groups of up to three dimensions; and the list of Lie group topics.\n\nColumn legend\n\nTable legend:\n\nThe dimensions given are dimensions over C. Note that every complex Lie group/algebra can also be viewed as a real Lie group/algebra of twice the dimension.\n\nThe dimensions given are dimensions over C. Note that every complex Lie algebra can also be viewed as a real Lie algebra of twice the dimension.\n"}
{"id": "25716941", "url": "https://en.wikipedia.org/wiki?curid=25716941", "title": "Telescoping Markov chain", "text": "Telescoping Markov chain\n\nIn probability theory, a telescoping Markov chain (TMC) is a vector-valued stochastic process that satisfies a Markov property and admits a hierarchical format through a network of transition matrices with cascading dependence.\n\nFor any formula_1 consider the set of spaces formula_2. The hierarchical process formula_3 defined in the product-space\n\nis said to be a TMC if there is a set of transition probability kernels formula_5 such that\n\n\nwhere formula_14 and formula_15\n"}
{"id": "1982652", "url": "https://en.wikipedia.org/wiki?curid=1982652", "title": "Typographical conventions in mathematical formulae", "text": "Typographical conventions in mathematical formulae\n\nTypographical conventions in mathematical formulae provide uniformity across mathematical texts and help the readers of those texts to grasp new concepts quickly.\n\nMathematical notation includes letters from various alphabets, as well as special mathematical symbols. Letters in various fonts often have specific, fixed meanings in particular areas of mathematics. A mathematical article or a theorem typically starts from the definitions of the introduced symbols, such as: \"Let \"G\" = (\"V\", \"E\") be a graph with the vertex set \"V\" and edge set \"E\"...\". Theoretically it is admissible to write \"Let \"X\" = (\"a\", \"q\") be a graph with the vertex set \"a\" and edge set \"q\"...\"; however, this would decrease readability, since the reader has to consciously memorize these unusual notations in a limited context.\n\nUsage of subscripts and superscripts is also an important convention. In the early days of computers with limited graphical capabilities for text, subscripts and superscripts were represented with the help of additional notation. In particular, \"n\" could be written as n^2 or n**2 (the latter borrowed from FORTRAN) and \"n\" could be written as n_2.\n\nVarious international authorities, including IUPAC, NIST and ISO have produced similar recommendations with regard to typesetting variables and other mathematical symbols (whether in equations or otherwise).\n\nIn general, anything that represents a variable (for example, \"h\" for a patient's height) should be set in italic, and everything else should be set in roman type. This applies equally to characters from the Latin/English alphabet (a, b, c, ...; A, B, C, ...) as to letters from any other alphabet, most notably Greek (α, β, γ, ...; Α, Β, Γ, ...). Any operator, such as cos (representing cosine) or ∑ (representing summation), should therefore be set roman. Note that each element must be set depending upon its own merits, including subscripts and superscripts. Thus, \"h\" would be suitable for the initial height, while \"h\" would represent one instance from a set of heights (\"h\", \"h\", \"h\", ...). Notice that numbers (1, 2, 3, \"etc.\") are not variables, and so are always set roman. Likewise, in some special cases symbols are used to represent general constants, such as π used to represent the ratio of a circle's circumference to its diameter, and such general constants can be set in roman. (This does not apply to parameters which are merely chosen to not vary.) \nFor vectors, matrices and tensors, it is recommended to set the variable itself in boldface (excluding any associated subscripts or superscripts). Hence, u would be suitable for the initial velocity, while u\" would represent one instance from a set of velocities (u, u, u\", ...). Italic is still used for variables, both for lowercase and for uppercase symbols (Latin, Greek, or otherwise). The only general situation where italic is not used for bolded symbols is for vector operators, such as ∇ (nabla), set bold and roman.\n\nThe rules of mathematical typography differ from country to country; thus, American mathematical journals and books will tend to use slightly different conventions from those of European journals.\n\nOne advantage of mathematical notation is its modularity—it is possible to write extremely complicated formulae involving multiple levels of super- or subscripting, and multiple levels of fraction bars. However, it is considered poor style to set up a formula in such a way as to leave more than a certain number of levels; for example, in non-math publications\nmight be rewritten as\n\nIncidentally, the above formula demonstrates the American rule that italic type is used for all letters representing variables and parameters except uppercase Greek letters, which are in upright type. Upright type is also standard for digits and punctuation; currently, the ISO-mandated style of using upright for constants (such as e, i) is not widespread. Bold Latin capital letters usually represent matrices, and bold lowercase letters are often used for vectors. The names of well-known functions, such as sin \"x\" (the trigonometric function sine) and exp \"x\" (the constant \"e\" raised to the power of \"x\") are written in lowercase upright letters (and often, as shown here, without parentheses around the argument).\n\nCertain important constructs are sometimes referred to by blackboard bold letters. For example, some authors denote the set of natural numbers by formula_3. Other authors prefer to use bold Latin for these symbols. (In context of math, font variations such as bold/non-bold may encode an arbitrary relation between symbols; using specialized symbols for formula_3 etc. allows the author more freedom of expressing such relations.)\n\nDonald Knuth's TeX typesetting engine incorporates a large amount of additional knowledge about American-style mathematical typography.\n\n"}
{"id": "1227628", "url": "https://en.wikipedia.org/wiki?curid=1227628", "title": "Vaughan Jones", "text": "Vaughan Jones\n\nSir Vaughan Frederick Randal Jones (born 31 December 1952) is a New Zealand mathematician, known for his work on von Neumann algebras and knot polynomials. He was awarded a Fields Medal in 1990, and famously wore a New Zealand rugby jersey when he gave his acceptance speech in Kyoto.\n\nVaughan Jones was born in Gisborne, New Zealand and brought up in Cambridge, New Zealand, completing secondary school at Auckland Grammar School. His undergraduate studies were at the University of Auckland, from where he obtained a BSc in 1972 and an MSc in 1973. For his graduate studies, he went to Switzerland, where he completed his PhD at the University of Geneva in 1979. His thesis, titled \"Actions of finite groups on the hyperfinite II factor\", was written under the supervision of André Haefliger. In 1980, he moved to the United States, where he taught at the University of California, Los Angeles (1980–1981) and the University of Pennsylvania (1981–1985), before being appointed as Professor of Mathematics at the University of California, Berkeley.\n\nHis work on knot polynomials, with the discovery of what is now called the Jones polynomial, was from an unexpected direction with origins in the theory of von Neumann algebras, an area of analysis already much developed by Alain Connes. It led to the solution of a number of classical problems of knot theory, and to increased interest in low-dimensional topology.\n\nJones has since 2011 been at Vanderbilt University as Stevenson Distinguished Professor of mathematics. He remains Professor Emeritus at University of California, Berkeley where he has been on the faculty since 1985 and is a Distinguished Alumni Professor at the University of Auckland.\n\nHe was made an honorary vice-president for life of the International Guild of Knot Tyers in 1992.\n\n\n\n\n"}
{"id": "65798", "url": "https://en.wikipedia.org/wiki?curid=65798", "title": "Wang tile", "text": "Wang tile\n\nWang tiles (or Wang dominoes), first proposed by mathematician, logician, and philosopher Hao Wang in 1961, are a class of formal systems. They are modelled visually by square tiles with a color on each side. A set of such tiles is selected, and copies of the tiles are arranged side by side with matching colors, \"without\" rotating or reflecting them.\n\nThe basic question about a set of Wang tiles is whether it can tile the plane or not, i.e., whether an entire infinite plane can be filled this way. The next question is whether this can be done in a periodic pattern.\n\nIn 1961, Wang conjectured that if a finite set of Wang tiles can tile the plane, then there exists also a \"periodic\" tiling, i.e., a tiling that is invariant under translations by vectors in a 2-dimensional lattice, like a wallpaper pattern. He also observed that this conjecture would imply the existence of an algorithm to decide whether a given finite set of Wang tiles can tile the plane. The idea of constraining adjacent tiles to match each other occurs in the game of dominoes, so Wang tiles are also known as Wang dominoes. The algorithmic problem of determining whether a tile set can tile the plane became known as the domino problem.\n\nAccording to Wang's student, Robert Berger,\n\nThe Domino Problem deals with the class of all domino sets. It consists of deciding, for each domino set, whether or not it is solvable. We say that the Domino Problem is \"decidable\" or \"undecidable\" according to whether there exists or does not exist an algorithm which, given the specifications of an arbitrary domino set, will decide whether or not the set is solvable.\n\nIn other words, the domino problem asks whether there is an effective procedure that correctly settles the problem for all given domino sets.\n\nIn 1966, Wang's student Robert Berger solved the domino problem in the negative. He proved that no algorithm for the problem can exist, by showing how to translate any Turing machine into a set of Wang tiles that tiles the plane if and only if the Turing machine does not halt. The undecidability of the halting problem (the problem of testing whether a Turing machine eventually halts) then implies the undecidability of Wang's tiling problem.\n\nCombining Berger's undecidability result with Wang's observation shows that there must exist a finite set of Wang tiles that tiles the plane, but only \"aperiodically\". This is similar to a Penrose tiling, or the arrangement of atoms in a quasicrystal. Although Berger's original set contained 20,426 tiles, he conjectured that smaller sets would work, including subsets of his set, and in his unpublished Ph.D. thesis, he reduced the number of tiles to 104. In later years, increasingly smaller sets were found.\nFor example, the set of 13 tiles given in the image above is an aperiodic set published by Karel Culik II in 1996. It can tile the plane, but not periodically. A smaller set of 11 tiles using 4 colors was discovered by Emmanuel Jeandel and Michael Rao in 2015, using an exhaustive search to prove that 10 tiles or 3 colors are insufficient to force aperiodicity.\n\nWang tiles can be generalized in various ways, all of which are also undecidable in the above sense. For example, \"Wang cubes\" are equal-sized cubes with colored faces and side colors can be matched on any polygonal tessellation.\nCulik and Kari have demonstrated aperiodic sets of Wang cubes. Winfree et al. have demonstrated the feasibility of creating molecular \"tiles\" made from DNA (deoxyribonucleic acid) that can act as Wang tiles. Mittal et al. have shown that these tiles can also be composed of peptide nucleic acid (PNA), a stable artificial mimic of DNA.\n\nWang tiles have recently become a popular tool for procedural synthesis of textures, heightfields, and other large and nonrepeating bidimensional data sets; a small set of precomputed or hand-made source tiles can be assembled very cheaply without too obvious repetitions and without periodicity.\nIn this case, traditional aperiodic tilings would show their very regular structure; much less constrained sets that guarantee at least two tile choices for any two given side colors are common because tileability is easily ensured and each tile can be selected pseudorandomly.\n\nWang tiles have also been used in cellular automata theory decidability proofs.\n\nThe short story \"Wang's Carpets\", later expanded to the novel \"Diaspora\", by Greg Egan, postulates a universe, complete with resident organisms and intelligent beings, embodied as Wang tiles implemented by patterns of complex molecules.\n\n\n"}
{"id": "8688682", "url": "https://en.wikipedia.org/wiki?curid=8688682", "title": "Yuri Burago", "text": "Yuri Burago\n\nYuri Dmitrievich Burago () (born 1936) is a Russian mathematician. He works in differential and convex geometry.\n\nBurago studied at Leningrad University, where he obtained his Ph.D. and Habilitation degrees. His advisors were Victor Zalgaller and Aleksandr Aleksandrov.\n\nBurago is the head of the Laboratory of Geometry and Topology that is part of the St. Petersburg Department of Steklov Institute of Mathematics. He took part in a report for the United States Civilian Research and Development Foundation for the Independent States of the former Soviet Union.\n\n\nHis other books and papers include:\n\nHe has advised Grigori Perelman, who solved the Poincaré conjecture, one of the seven Millennium Prize Problems. Burago was an advisor to Perelman during the latter's post-graduate research at St. Petersburg Department of Steklov Institute of Mathematics.\n\n"}
{"id": "852688", "url": "https://en.wikipedia.org/wiki?curid=852688", "title": "Zerah Colburn (mental calculator)", "text": "Zerah Colburn (mental calculator)\n\nZerah Colburn (September 1, 1804 – March 2, 1839) was a child prodigy of the 19th century who gained fame as a mental calculator.\n\nColburn was born in Cabot, Vermont, in 1804. He was thought to be intellectually disabled until the age of six. However, after six weeks of schooling his father overheard him repeating his multiplication tables. His father was not sure whether or not he learned the tables from his older brothers and sisters but he decided to test him further on his mathematical abilities and discovered that there was something special about his son when Zerah correctly multiplied 13 and 97.\n\nColburn's abilities developed rapidly and he was soon able to solve such problems as the number of seconds in 2,000 years, the product of 12,225 and 1,223, or the square root of 1,449. When he was seven years old he took six seconds to give the numbers of hours in thirty-eight years, two months, and seven days.\n\nZerah is reported to have been able to solve fairly complex problems. For example, the sixth Fermat number is 2+1 (or 2+1). The question is whether this number, 4,294,967,297, is prime or not. Zerah calculated in his head that it was not and has divisor 641. The other divisor is 6,700,417 and can easily be found using a calculator.\n\nHis father capitalized on his boy's talents by taking Zerah around the country and eventually abroad, demonstrating the boy's exceptional abilities. The two left Vermont in the winter of 1810-11. Passing through Hanover, New Hampshire, John Wheelock, then president of Dartmouth College, offered to take upon himself the whole care and expense of his education, but his father rejected the offer. At Boston, the boy's performances attracted much attention. He was visited by Harvard College professors and eminent people from all professions, and the newspapers ran numerous articles concerning his powers of computation.\n\nAfter leaving Boston, his father exhibited Zerah for money throughout the middle and part of the southern states and, in January 1812, sailed with him for England. In September 1813 Colburn was being exhibited in Dublin. Colburn was pitted against the 8-year-old William Rowan Hamilton in a mental arithmetic contest, with Colburn emerging the clear victor. In reaction to his defeat, Hamilton dedicated less time to studying languages and more time to studying mathematics. After traveling over England, Scotland, and Ireland, they spent 18 months in Paris. Here Zerah was placed in the Lycée Napoléon, but was soon removed by his father, who, at length, in 1816, returned to England in deep poverty.\n\nThe Earl of Bristol soon became interested in the boy, and placed him in Westminster School, where he remained until 1819. In consequence of his father's refusal to comply with certain arrangements proposed by the earl, Zerah was removed from Westminster, and his father now proposed to Zerah that he should study to become an actor. Accordingly, he studied for this profession, and was for a few months under the tuition of Charles Kemble. His first appearance, however, dissatisfied both his instructor and himself so much, that he was not accepted for the stage, so he accepted a position as an assistant in a school, and soon afterward commenced a school of his own. To this he added the performing of some astronomical calculations for Thomas Young, then secretary of the Board of Longitude.\n\nIn 1824 when his father died, he was enabled by the Earl of Bristol and other friends to return to the United States. Though Zerah's schooling was rather irregular, he showed talent in languages. He went to Fairfield, New York, as assistant teacher of an academy; but not being pleased with his situation, he moved in March following to Burlington, Vermont, where he taught French, pursuing his studies at the same time in the University of Vermont. Toward the end of 1825 he connected himself with the Methodist Church and, after nine years of service as an itinerant preacher, he settled in Norwich, Vermont, in 1835, where he was soon after appointed professor of languages at Dartmouth College in Hanover, New Hampshire.\n\nIn 1833 he published his autobiography. From this it appears that his faculty of computation left him about the time he reached adulthood. He died of tuberculosis at the age of 34 and was buried in Norwich’s Old Meeting House Cemetery.\n\nHis nephew, also named Zerah Colburn, was a noted locomotive engineer and technical journalist. He has known descendants living in Vermont.\n\n\n"}
