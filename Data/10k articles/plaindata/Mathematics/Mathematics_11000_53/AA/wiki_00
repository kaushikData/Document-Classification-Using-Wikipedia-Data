{"id": "9929142", "url": "https://en.wikipedia.org/wiki?curid=9929142", "title": "1 + 1 + 1 + 1 + ⋯", "text": "1 + 1 + 1 + 1 + ⋯\n\nIn mathematics, , also written formula_1, formula_2, or simply formula_3, is a divergent series, meaning that its sequence of partial sums does not converge to a limit in the real numbers. The sequence 1 can be thought of as a geometric series with the common ratio 1. Unlike other geometric series with rational ratio (except −1), it converges in neither the real numbers nor in the -adic numbers for some . In the context of the extended real number line\nsince its sequence of partial sums increases monotonically without bound.\n\nWhere the sum of occurs in physical applications, it may sometimes be interpreted by zeta function regularization, as the value at of the Riemann zeta function\n\nThe two formulas given above are not valid at zero however, so one might try the analytic continuation of the Riemann zeta function,\n\nUsing this one gets (given that ),\n\nwhere the power series expansion for about follows because has a simple pole of residue one there. In this sense .\n\nEmilio Elizalde presents an anecdote on attitudes toward the series:\n"}
{"id": "12216711", "url": "https://en.wikipedia.org/wiki?curid=12216711", "title": "Abel's binomial theorem", "text": "Abel's binomial theorem\n\nAbel's binomial theorem, named after Niels Henrik Abel, is a mathematical identity involving sums of binomial coefficients. It states the following:\n\n"}
{"id": "359377", "url": "https://en.wikipedia.org/wiki?curid=359377", "title": "Abstract structure", "text": "Abstract structure\n\nAn abstract structure is a formal object that is defined by a set of laws, properties and relationships in a way that is logically if not always historically independent of the structure of contingent experiences, for example, those involving physical objects. Abstract structures are studied not only in logic and mathematics but in the fields that apply them, as computer science, and in the studies that reflect on them, such as philosophy (especially the philosophy of mathematics). Indeed, modern mathematics has been defined in a very general sense as the study of abstract structures (by the Bourbaki group: see discussion there, at algebraic structure and also structure).\n\nAn abstract structure may be represented (perhaps with some degree of approximation) by one or more physical objects this is called an implementation or instantiation of the abstract structure. But the abstract structure itself is defined in a way that is not dependent on the properties of any particular implementation.\n\nAn abstract structure has a richer structure than a concept or an idea. An abstract structure must include precise rules of behaviour which can be used to determine whether a candidate implementation actually matches the abstract structure in question. Thus we may debate how well a particular government fits the concept of democracy, but there is no room for debate over whether a given sequence of moves is or is not a valid game of chess. \n\n\n"}
{"id": "38731422", "url": "https://en.wikipedia.org/wiki?curid=38731422", "title": "Alternated order-4 hexagonal tiling", "text": "Alternated order-4 hexagonal tiling\n\nIn geometry, the alternated order-4 hexagonal tiling or ditetragonal tritetratrigonal tiling is a uniform tiling of the hyperbolic plane. It has Schläfli symbol of (3,4,4), h{6,4}, and hr{6,6}.\n\nThere are four uniform constructions, with some of lower ones which can be seen with two colors of triangles:\n\n\n\n"}
{"id": "44849824", "url": "https://en.wikipedia.org/wiki?curid=44849824", "title": "Atiyah–Bott formula", "text": "Atiyah–Bott formula\n\nIn algebraic geometry, the Atiyah–Bott formula says the cohomology ring\nof the moduli stack of principal bundles is a free graded-commutative algebra on certain homogeneous generators. Michael Atiyah and Raoul Bott's original work concerned integral cohomology ring of Bun(\"X\").\n\n\n"}
{"id": "44260845", "url": "https://en.wikipedia.org/wiki?curid=44260845", "title": "Beam and Warming scheme", "text": "Beam and Warming scheme\n\nIn numerical mathematics, Beam and Warming scheme or Beam–Warming implicit scheme introduced in 1978 by Richard M. Beam and R. F. Warming, is a second order accurate implicit scheme, mainly used for solving non-linear hyperbolic equation. It is not used much nowadays.\n\nThis scheme is a spatially factored, non iterative, ADI scheme and uses implicit Euler to perform the time Integration. The algorithm is in delta-form, linearized through implementation of a Taylor-series. Hence observed as increments of the conserved variables. In this an efficient factored algorithm is obtained by evaluating the spatial cross derivatives explicitly. This allows for direct derivation of scheme and efficient solution using this computational algorithm. The efficiency is because although it is three-time-level scheme, but requires only two time levels of data storage. This results in unconditional stability. It is centered and needs the artificial dissipation operator to guarantee numerical stability.\n\nThe delta form of the equation produced has the advantageous property of stability (if existing) independent of the size of the time step.\n\nConsider the inviscid Burgers' equation in one dimension\n\nBurgers' equation in conservation form,\n\nwhere :formula_3\n\nThe expansion of :formula_4\n\nThis is also known as the trapezoidal formula.\n\nResulting tri-diagonal system:\nThis resulted system of linear equations can be solved using the modified tridiagonal matrix algorithm, also known as the Thomas algorithm.\n\nUnder the condition of shock wave, dissipation term is required for nonlinear hyperbolic equations such as this. This is done to keep the solution under control and maintain convergence of the solution.\n\nThis term is added explicitly at level formula_10 to the right hand side. This is always used for successful computation where high-frequent oscillations are observed and must be suppressed.\n\nIf only the stable solution is required, then in the equation to the right hand side a second-order smoothing term is added on the implicit layer.\nThe other term in the same equation can be second-order because it has no influence on the stable solution if\n\nThe addition of smoothing term increases the number of steps required by three.\n\nThis scheme is produced by combining the trapezoidal formula, linearization, factoring, Padt spatial differencing, the homogeneous property of the flux vectors (where applicable), and hybrid spatial differencing and is most suitable for nonlinear systems in conservation-law form. ADI algorithm retains the order of accuracy and the steady-state property while reducing the bandwidth of the system of equations.\nStability of the equation is\nThe order of Truncation error is\nThe result is smooth with considerable overshoot (that does not grow much with time).\n"}
{"id": "49468975", "url": "https://en.wikipedia.org/wiki?curid=49468975", "title": "Birkhoff orthogonality", "text": "Birkhoff orthogonality\n\nTwo vectors \"x\" and \"y\" in a normed linear space are said to be Birkhoff orthogonal if and only if || \"x\" + λ\"y\" || ≥ || \"x\" || for all scalar λ.\n"}
{"id": "17553460", "url": "https://en.wikipedia.org/wiki?curid=17553460", "title": "Bit-string physics", "text": "Bit-string physics\n\nBit-string physics is a body of theory which supposes that reality can be represented by a process of operations on finite strings of dichotomous symbols, or bits (1's and 0's). Bit-string physics has developed from Frederick Parker-Rhodes' 1964 discovery of the combinatorial hierarchy: four numbers produced from a purely mathematical recursive algorithm that correspond to the relative strengths of the four forces. These strengths are characterized by the strong, weak, electromagnetic (fine-structure constant), and gravitational coupling constants. Other leading contributors in the field include H. Pierre Noyes, Ted Bastin, Clive W. Kilmister, John Amson, Mike Manthey, and David McGoveran.\n\nIn a 2001 paper by Noyes, evidence was presented for predictions made by the theory that were later confirmed.\n\n"}
{"id": "14709171", "url": "https://en.wikipedia.org/wiki?curid=14709171", "title": "C. P. Ramanujam", "text": "C. P. Ramanujam\n\nChakravarthi Padmanabhan Ramanujam (9 January 1938 – 27 October 1974) was an Indian mathematician who worked in the fields of number theory and algebraic geometry. He was elected a fellow of the Indian Academy of Sciences in 1973.\n\nLike his namesake Srinivasa Ramanujan, Ramanujam also had a very short life.\n\nAs David Mumford put it, Ramanujam felt that the spirit of mathematics demanded of him not merely routine developments but the right theorem on any given topic. \"He wanted mathematics to be beautiful and to be clear and simple. He was sometimes tormented by the difficulty of these high standards, but in retrospect, it is clear to us how often he succeeded in adding to our knowledge, results both new, beautiful and with a genuinely original stamp\".\n\nRamanujam was born to a Tamil family on 9 January 1938 in Madras (now Chennai), India, as the eldest of seven, to Chakravarthi Srinivasa Padmanabhan. He finished his schooling in Town Hr.Sec. School , Kumbakonam . and joined Loyola College in Madras in 1952. He wanted to specialise in mathematics and he set out to master it with vigour and passion. He also enjoyed music and his favourite musician was Dr. M. D. Ramanathan, a maverick concert musician. His teacher and friend at this time was Father Racine, a missionary who had obtained his doctorate under the supervision of Élie Cartan. With Father Racine's encouragement and recommendation, Ramanujam applied and was admitted to the graduate school at the Tata Institute of Fundamental Research in Bombay. His father had wanted him to join the Indian Statistical Institute in Calcutta as he had passed the entrance exam meritoriously.\n\nRamanujam set out for Mumbai at the age of eighteen to pursue his interest in mathematics. He and his friend and schoolmate Raghavan Narasimhan, and S. Ramanan joined TIFR together in 1957. At the Tata Institute there was a stream of first-rate visiting mathematicians from all over the world. It was a tradition for some graduate student to write up the notes of each course of lectures. Accordingly, Ramanujam wrote up in his first year, the notes of Max Deuring's lectures on \"Algebraic functions of one variable\". It was a nontrivial effort and the notes were written clearly and were well received. The analytical mind was much in evidence in this effort as he could simplify and extend the notes within a short time period. \"He could reduce difficult solutions to be simple and elegant due to his deep knowledge of the subject matter\" states Ramanan. \"Max Deuring's lectures gave him a taste for algebraic number theory. He studied not only algebraic geometry and analytic number theory of which he displayed a deep knowledge but he became an expert in several other allied subjects as well\".\n\nOn the suggestion of his doctoral advisor, K. G. Ramanathan, he began working on a problem relating to the work of the German number theorist Carl Ludwig Siegel. In the course of proving the main result to the effect that every cubic form in 54 variables over any algebraic number field \"K\" had a non-trivial zero over that field, he had also simplified the earlier method of Siegel. He took up Waring's problem in algebraic number fields and got interesting results. In recognition of his work and his contribution to Number Theory, the Institute promoted him to associate professor. He protested against this promotion as 'undeserved', and had to be persuaded to accept the position. He proceeded to write his thesis in 1966 and took his doctoral examination in 1967. Dr. Siegel, who was one of the examiners, was highly impressed with the young man's depth of knowledge and his great mathematical abilities.\n\nRamanujam was a scribe for Igor Shafarevich's course of lectures in 1965 on minimal models and birational transformation of two-dimensional schemes. Professor Shafarevich subsequently wrote to say that Ramanujam not only corrected his mistakes but complemented the proofs of many results. The same was the case with Mumford's lectures on abelian varieties, which were delivered at TIFR around 1967. Mumford wrote in the preface to his book that the notes improved upon his work and that his current work on abelian varieties was a joint effort between him and Ramanujam. A little-known fact is that during this time he started teaching himself German, Italian, Russian and French so that he could study mathematical works in their original form. His personal library contained quite a few non-English mathematical works.\n\nBetween 1964 and 1968, he was making great strides in number theory and his contacts with Shafarevich and Mumford led him on to algebraic geometry. According to Ramanathan and other colleagues, his progress and deep understanding of algebraic geometry was phenomenal. In 1964, based on his participation in the International Colloquium on Differential Analysis, he earned the respect of Alexander Grothendieck and of David Mumford, who invited him to Paris and Harvard. He accepted the invitation and was in Paris, but for a brief period. He was diagnosed in 1964 with schizophrenia with severe depression and left Paris for Chennai. He later decided to quit his position at TIFR.\n\nHe quit his post at Mumbai in 1965 after a bout of illness and secured a tenured position as a professor in Chandigarh, Punjab. There he met the young student Chitikila Musili, who later went on to prove interesting results in the geometry connected with the theory of Lie groups and wrote good expository books. Ramanujam stayed in Chandigarh only eight months and he had to return to Chennai again for treatment. TIFR was his real home and he was back there again in June 1965. Around this time he accepted an invitation from Institut des Hautes Études Scientifiques, near Paris. He was barely there before he was flown back to Chennai. Unfortunately schizophrenia, a highly treatable condition today, was not properly diagnosed and treated at that time. Thus he continued until the end of his life to be highly creative for short periods before the recurrent illness overtook him. Again, in 1970, he sent his resignation letter to TIFR but the institute would not take it seriously. Around this time, Mumford invited him to Warwick as a visiting professor during the algebraic geometry year. Mumford writes that he spent many delightful evenings with Ramanujam and that his presence contributed importantly to the success of the algebraic geometry year. A famous paper written during this time, by Michael Artin and David Mumford acknowledges Ramanujam's suggestions and help. He also had a short tenure at Turin where he was widely appreciated and accepted. Just after his death a commemorative hall was named after him in the former Istituto di Matematica (Institute of Mathematics) of the university of Genoa.\n\nBack in India after his year at the University of Warwick, Ramanujam requested for a professorship at the Tata Institute but to be made tenable in their Bangalore campus. The Tata Institute had an applied mathematics wing in Bangalore. Although Ramanjuam had nothing to do with this area, the Institute, wishing him to continue his research, made a special arrangement by which he could stay and work there. By this time, he was deeply affected and depressed by his illness. He was put in charge of a new branch dealing with applied mathematics. He settled down in Bangalore, but again in the depths of depression caused by his illness, he tried to leave the Institute and obtain a university teaching post. During one of the attacks, he tried to take his life, but was rescued in time. However, late one evening on 27 October 1974, after a lively discussion with a visiting foreign professor he took his life with an overdose of barbiturates. He was barely thirty-seven.\n\n"}
{"id": "882427", "url": "https://en.wikipedia.org/wiki?curid=882427", "title": "Categorial grammar", "text": "Categorial grammar\n\nCategorial grammar is a term used for a family of formalisms in natural language syntax motivated by the principle of compositionality and organized according to the view that syntactic constituents should generally combine as functions or according to a function-argument relationship. Most versions of categorial grammar analyze sentence structure in terms of constituencies (as opposed to dependencies) and are therefore phrase structure grammars (as opposed to dependency grammars).\n\nA categorial grammar consists of two parts: a lexicon, which assigns a set of types (also called categories) to each basic symbol, and some type inference rules, which determine how the type of a string of symbols follows from the types of the constituent symbols. It has the advantage that the type inference rules can be fixed once and for all, so that the specification of a particular language grammar is entirely determined by the lexicon.\n\nA categorial grammar shares some features with the simply typed lambda calculus.\nWhereas the lambda calculus has only one function type formula_1,\na categorial grammar typically has two function types, one type which is applied on the left,\nand one on the right. For example, a simple categorial grammar might have two function types formula_2 and formula_3.\nThe first, formula_2, is the type of a phrase that results in a phrase of type\nformula_5 when followed (on the right) by a phrase of type formula_6.\nThe second, formula_7, is the type of a phrase that results\nin a phrase of type formula_5 when preceded (on the left) by a phrase of type \nformula_6. \n\nThe notation is based upon algebra. A fraction when multiplied by (i.e. concatenated with) its denominator yields its numerator. As concatenation is not commutative, it makes a difference whether the denominator occurs to the left or right. The concatenation must be on the same side as the denominator for it to cancel out.\n\nThe first and simplest kind of categorial grammar is called a basic categorial grammar, or sometimes an AB-grammar (after Ajdukiewicz and Bar-Hillel).\nGiven a set of primitive types formula_10, let \nformula_11 be the set of types constructed from primitive types. In the basic case, this is the least set such that formula_12\nand if formula_13\nthen formula_14.\nThink of these as purely formal expressions freely generated from the primitive types; any semantics will be added later. Some authors assume a fixed infinite set of primitive types used by all grammars, but by making the primitive types part of the grammar, the whole construction is kept finite.\n\nA basic categorial grammar is a tuple formula_15\nwhere formula_16 is a finite set of symbols,\nformula_10 is a finite set of primitive types, and formula_18.\n\nThe relation formula_19 is the lexicon, which relates types to symbols formula_20.\nSince the lexicon is finite, it can be specified by listing a set of pairs like formula_21.\n\nSuch a grammar for English might have three basic types formula_22, assigning count nouns the type formula_23, complete noun phrases the type\nformula_24, and sentences the type formula_25.\nThen an adjective could have the type formula_26, because if it is followed by a noun then the whole phrase is a noun. \nSimilarly, a determiner has the type formula_27,\nbecause it forms a complete noun phrase when followed by a noun.\nIntransitive verbs have the type formula_28, and transitive verbs the type formula_29.\nThen a string of words is a sentence if it has overall type formula_25.\n\nFor example, take the string \"the bad boy made that mess\". Now \"the\" and \"that\" are determiners, \"boy\" and \"mess\" are nouns, \"bad\" is an adjective, and \"made\" is a transitive verb, so the lexicon is\nformula_32,\nformula_33,\nformula_34,\nformula_35,\nformula_36}.\n\nand the sequence of types in the string is\n\nformula_37\n\nnow find functions and appropriate arguments and reduce them according to the two inference rules\nformula_38 and\nformula_39:\n\nformula_40<br>\nformula_41<br>\nformula_42<br>\nformula_43<br>\nformula_44<br>\nformula_45\n\nThe fact that the result is formula_25 means that the string is a sentence, while the sequence of reductions shows that it must be parsed as ((the (bad boy)) (made (that mess))).\n\nCategorial grammars of this form (having only function application rules) are equivalent in generative capacity to context-free grammars and are thus often considered inadequate for theories of natural language syntax. Unlike CFGs, categorial grammars are lexicalized, meaning that only a small number of (mostly language-independent) rules are employed, and all other syntactic phenomena derive from the lexical entries of specific words.\n\nAnother appealing aspect of categorial grammars is that it is often easy to assign them a compositional semantics, by first assigning interpretation types to all the basic categories, and then associating all the derived categories with appropriate function types. The interpretation of any constituent is then simply the value of a function at an argument. With some modifications to handle intensionality and quantification, this approach can be used to cover a wide variety of semantic phenomena.\n\nA Lambek grammar is an elaboration of this idea that has a\nconcatenation operator for types, and several other inference rules.\nPentus has shown that these still have the generative capacity of\ncontext-free grammars.\n\nFor the Lambek calculus, there is a type concatenation\noperator formula_47, so\nthat formula_12\nand if formula_13\nthen formula_50.\n\nThe Lambek calculus consists of several deduction rules, which specify\nhow type inclusion assertions can be derived. In the following\nrules, upper case roman letters stand for types, upper case Greek\nletters stand for sequences of types. A sequent of the form\nformula_51\ncan be read: a string is of type formula_52 if it consists of the concatenation\nof strings of each of the types in formula_53. If a type is\ninterpreted as a set of strings, then the\nformula_54 may be interpreted as formula_55,\nthat is, \"includes as a subset\". \nA horizontal line means that the inclusion above the line\nimplies the one below the line.\n\nThe process is begun by the Axiom rule, which has no antecedents and\njust says that any type includes itself.\n\n<math>\n(Axiom)\\quad\n"}
{"id": "26569682", "url": "https://en.wikipedia.org/wiki?curid=26569682", "title": "Clockwise", "text": "Clockwise\n\nTwo-dimensional rotation can occur in two possible directions. A clockwise (typically abbreviated as CW) motion is one that proceeds in the same direction as a clock's hands: from the top to the right, then down and then to the left, and back up to the top. The opposite sense of rotation or revolution is (in North American English) counterclockwise (CCW) or (in Commonwealth English) anticlockwise (ACW).\n\nBefore clocks were commonplace, the terms \"sunwise\" and \"deasil\", \"deiseil\" and even \"deocil\" from the Scottish Gaelic language and from the same root as the Latin \"dexter\" (\"right\") were used for clockwise. \"Widdershins\" or \"withershins\" (from Middle Low German \"weddersinnes\", \"opposite course\") was used for counterclockwise.\nThe terms clockwise and counterclockwise can only be applied to a rotational motion once a side of the rotational plane is specified, from which the rotation is observed. For example, the daily rotation of the Earth is clockwise when viewed from above the South Pole, and counterclockwise when viewed from above the North Pole (considering \"above a point\" to be defined as \"farther away from the center of earth and on the same ray\").\n\nClocks traditionally follow this sense of rotation because of the clock's predecessor: the sundial. Clocks with hands were first built in the Northern Hemisphere (see \"Clock\"), and they were made to work like horizontal sundials. In order for such a sundial to work north of the equator during spring and summer, and north of the Tropic of Cancer the whole year, the noon-mark of the dial must be placed northward of the pole casting the shadow. Then, when the Sun moves in the sky (from east to south to west), the shadow, which is cast on the sundial in the opposite direction, moves with the same sense of rotation (from west to north to east). This is why hours must be drawn in horizontal sundials in that manner, and why modern clocks have their numbers set in the same way, and their hands moving accordingly. For a vertical sundial (such as those placed on the walls of buildings, the dial being \"below\" the post), the movement of the sun is from right to top to left, and, accordingly, the shadow moves from left to down to right, i.e., counterclockwise. This effect is caused by the plane of the dial having been rotated through the plane of the motion of the sun and thus the shadow is observed from the other side of the dial's plane and is observed as moving in the opposite direction. Some clocks were constructed to mimic this. The best-known surviving example is the astronomical clock in the Münster Cathedral, whose hands move counterclockwise.\n\nOccasionally, clocks whose hands revolve counterclockwise are nowadays sold as a novelty. Historically, some Jewish clocks were built that way, for example in some synagogue towers in Europe such as the Jewish Town Hall in Prague, to accord with right-to-left reading in the Hebrew language. In 2014 under Bolivian president Evo Morales, the clock outside the Legislative Assembly in Plaza Murillo, La Paz, was shifted to counterclockwise motion to promote indigenous values.\n\nTypical nuts, screws, bolts, bottle caps, and jar lids are tightened (moved away from the observer) clockwise and loosened (moved towards the observer) counterclockwise in accordance with the right-hand rule.\n\nTo apply the right-hand rule, place one's loosely clenched right hand above the object with the thumb pointing in the direction one wants the screw, nut, bolt, or cap ultimately to move, and the curl of the fingers, from the palm to the tips, will indicate in which way one needs to turn the screw, nut, bolt or cap to achieve the desired result. Almost all threaded objects obey this rule except for a few left-handed exceptions described below.\n\nThe reason for the clockwise standard for most screws and bolts is that supination of the arm, which is used by a right-handed person to tighten a screw clockwise, is generally stronger than pronation used to loosen. \n\nSometimes the opposite (left-handed, counterclockwise, reverse) sense of threading is used for a special reason. A thread might need to be left-handed to prevent operational stresses from loosening it. For example, some older cars and trucks had right-handed lug nuts on the right wheels and left-handed lug nuts on the left wheels, so that, as the vehicle moved forward, the lug nuts tended to tighten rather than loosen. For bicycle pedals, the one on the left must be reverse-threaded to prevent it unscrewing during use. Similarly, the flyer whorl of a spinning wheel uses a left-hand thread to keep it from loosening. A turnbuckle has right-handed threads on one end and left-handed threads on the other. Some gas fittings are left-handed to prevent disastrous misconnections: oxygen fittings are right-handed, but acetylene, propane, and other flammable gases are unmistakably distinguished by left-handed fittings.\n\nIn trigonometry and mathematics in general, plane angles are conventionally measured counterclockwise, starting with 0° or 0 radians pointing directly to the right (or east), and 90° pointing straight up (or north). However, in navigation, compass headings increase clockwise around the compass face, starting with 0° at the top of the compass (the northerly direction), with 90° to the right (east).\n\nA circle defined parametrically in a positive Cartesian plane by the equations and is traced counterclockwise as the angle \"t\" increases in value, from the right-most point at . An alternative formulation with sin and cos swapped gives a clockwise trace from the upper-most point.\n\nIn general, most card games, board games, parlor games and multiple team sports play in a clock-wise turn rotation in Western Countries and Latin America with a notable resistance to playing in the opposite direction (counter-clockwise). Traditionally (and still continued for the most part) turns pass counter-clockwise in many Asian countries. In Western countries when speaking and discussion activities take part in a circle, turns tend to naturally pass in a clockwise motion even though there is no obligation to do so. Curiously, unlike with games, there is usually no objection when the activity uncharacteristically begins in a counter-clockwise motion.\n\nNotably, the game of baseball is played counter-clockwise.\n\nMost left-handed people prefer to draw circles and circulate in buildings clockwise, while most right-handed people prefer to draw circles and circulate in buildings counterclockwise. This is believed to result from dominant brain hemispheres, though some attribute it to muscle mechanics.\n\n"}
{"id": "1765838", "url": "https://en.wikipedia.org/wiki?curid=1765838", "title": "Comprehensive School Mathematics Program", "text": "Comprehensive School Mathematics Program\n\nComprehensive School Mathematics Program (CSMP) stands for both the name of a curriculum and the name of the project that was responsible for developing curriculum materials. \n\nTwo major curricula were developed under CSMP project, Comprehensive School Mathematics Program(CSMP), a K-6 mathematics program for regular classroom instruction, and the Elements of Mathematics (EM) program, a grades 7-12 mathematics program for gifted students. EM treats traditional topics rigorously and in depth and was the only curriculum that strictly adhere to the Goals for School Mathematics: The Report of the Cambridge Conference on School Mathematics (1963). As a result, it includes much of the content generally required for an undergraduate mathematics major. These two curricula are unrelated to one another but certain members of the CSMP staff contributed to the development of both projects. (There was also some interaction with the Secondary School Mathematics Curriculum Improvement Study program being developed around the same time.) The Elements of Mathematics is widely used at the IMACS institute listed below. What follows is a description of the K-6 program that was designed for a general heterogeneous audience.\n\nThe CSMP Project was established in 1966, under the direction of Burt Kaufman, who remained director until 1979 when Clare Heidema became director until 2003. It was originally affiliated with Southern Illinois University in Carbondale, Illinois. After a year of planning, CSMP was incorporated into the Central Midwest Regional Educational Laboratory (later CEMREL, Inc.), one of the national educational laboratories funded at that time by the U.S. Office of Education. (see Final Evaluation Report by Martin Herbert referenced below for more detail) . In 1984, the project moved to Mid-continental Research for Learning (McREL) Institute's Comprehensive School Reform program, who supported the program until 2003. Clare Heidema remained director to its conclusion. In 1984, it was implemented in 150 school districts in 42 states and about 55,000 students.\n\nThe most influential figure on this project was Frederque’ Papy. This project employs four non-verbal languages for the purpose of posing problems and representing mathematical concepts. They are: the Papy Minicomputer(mental computation), Arrows(relations), Strings(classification) and Calculators(patterns). It was designed to teach mathematics as a problem solving activity rather than just teaching arithmetic skills. The program was highly structured using the spiral scheme of program development. It introduced many basic concepts such as fractions earlier than normal but was criticized for lack of emphasis given to calculation. New content in probability and geometry was introduced. There was a range of supporting material including story books with mathematical problems. Lessons were often posed in a story. One character in these books was Eli the Elephant, a pachyderm with a bag of magic peanuts — some representing positive integers, some negative.\n\nOne device used throughout the program was a \"mini-computer\". This was a 2 by 2 grid of squares, the squares represented the numbers 1, 2, 4, and 8. Checkers could be placed on the grid to represent different numbers in a similar fashion to the way the binary numeral system is used to represent numbers in a computer.\n\nThe mini-computer is laid out as follows: a white square in the lower right corner with a value of 1, a red square in the lower left with a value of 2, a purple square in the upper right with a value of 4, and a brown square in the upper left with a value of 8. Each mini-computer is designed to represent a single decimal digit, and multiple mini-computers can be used together to represent multiple-digit numbers. Each successive board's values are increased by a power of ten. For example, a second mini-computer's squares will represent 10, 20, 40, and 80; a third, 100, 200, 400, and 800, and so on.\n\nStudents are instructed to represent values on the mini-computers by adding checkers to the proper squares. To do this only requires a memorization of representations for the digits zero through nine, although non-standard representations are possible since squares can hold more than one checker. Each checker is worth the value of the square it is in, and the sum of the checkers on the board(s) determine the overall value represented. Most checkers used by students are a solid color- any color is fine. The only exception is checkers marked with a caret (^), which are negative.\n\nAn example of representing a number: 9067 requires four boards. The leftmost board has two checkers in the 8 and 1 squares (8000 + 1000). The second board has none, as the value has zero hundreds. The third board has checkers in the 4 and 2 squares (40 + 20), and the rightmost board has checkers in the 4, 2, and 1 squares (4 + 2 + 1). Together, these 7 values (8000 + 1000 + 40 + 20 + 4 + 2 + 1) total up to 9067.\n\nThis would be considered a standard way to represent the number as it involves the fewest checkers possible without involving negatives. It would be simpler to replace the last board with a positive checker in the 8 and a negative checker in the 1, but this is not taught as the standard.\n\nArithmetic can be performed on the mini-computer by combining two numbers' representations into a single board and performing simplification techniques. One such technique is to replace checkers from the 8 and 2 squares of one board with a checker on the 1 square of the adjacent board to the left. Another technique is to replace a pair of checkers in the same square with one checker in the next higher square, such as two 4's with an 8.\n\nThe program received extensive evaluation, with over 50 studies. These studies showed broadly similar results for non CSMP students in computation, concepts and applications. However there was a marked improvement when assessed according to The Mathematics Applied to Novel Situations (MANS) tests which were introduced to measure students ability for problem solving in novel situations.\n\nBurt Kaufman, a mathematics curriculum specialist, headed the team at SIU writing CSMP. He eventually started the Institute for Mathematics & Computer Science (IMACS). IMACS appears to use elements of the program in their \"Mathematics Enrichment\" program. For instance, mini-computers and \"Eli the Elephant\" are present in the IMACS material. IMACS is a private education business focusing on the instruction of students from first grade through high school.https://www.imacs.org/about/news/burt-kaufman.html\n"}
{"id": "5021705", "url": "https://en.wikipedia.org/wiki?curid=5021705", "title": "Conway polyhedron notation", "text": "Conway polyhedron notation\n\nIn geometry, Conway polyhedron notation, invented by John Horton Conway and promoted by George W. Hart, is used to describe polyhedra based on a seed polyhedron modified by various prefix operations.\n\nConway and Hart extended the idea of using operators, like truncation as defined by Kepler, to build related polyhedra of the same symmetry. For example, \"tC\" represents a truncated cube, and \"taC\", parsed as , is a truncated cuboctahedron. The simplest operator dual swaps vertex and face elements; e.g., a dual cube is an octahedron: \"dC\"=\"O\". Applied in a series, these operators allow many higher order polyhedra to be generated. Conway defined the operators \"abdegjkmost\", while Hart added \"r\" and \"p\". Conway's basic operations are sufficient to generate the Archimedean and Catalan solids from the Platonic solids. Some basic operations can be made as composites of others. Later implementations named further operators, sometimes referred to as \"extended\" operators.\n\nIn general, it is difficult to predict the resulting appearance of the composite of two or more operations from a given seed polyhedron. For instance, ambo applied twice is the expand operation: \"aa\" = \"e\", while a truncation after ambo produces bevel: \"ta\" = \"b\". Many basic questions about Conway operators remain open, for instance, how many operators of a given \"size\" exist.\n\nIn Conway's notation, operations on polyhedra are applied like functions, from right to left. For example, a cuboctahedron is an \"ambo cube\", i.e. , and a truncated cuboctahedron is . Repeated application of an operator can be denoted with an exponent: \"j\" = \"o\". In general, Conway operators are not commutative. The resulting polyhedron has a fixed topology (vertices, edges, faces), while exact geometry is not specified: it can be thought of as one of many embeddings of a polyhedral graph on the sphere. Often the polyhedron is put into canonical form.\n\nIndividual operators can be visualized in terms of \"chambers\", as below. Each white chamber is a rotated version of the others. For achiral operators, the red chambers are a reflection of the white chambers. Achiral and chiral operators are also called local symmetry-preserving operations (LSP) and local operations that preserve orientation-preserving symmetries (LOPSP), respectively, although the exact definition is a little more restrictive.\n\nThe relationship between the number of vertices, edges, and faces of the seed and the polyhedron created by the operations listed in this article can be expressed as a matrix formula_1. When \"x\" is the operator, formula_2 are the vertices, edges, and faces of the seed (respectively), and formula_3 are the vertices, edges, and faces of the result, then \n\nThe matrix for the composition of two operators is just the product of the matrixes for the two operators. Distinct operators may have the same matrix, for example, \"p\" and \"l\". The edge count of the result is an integer multiple \"d\" of that of the seed: this is called the inflation rate, or the edge factor.\n\nThe simplest operators, the identity operator \"S\" and the dual operator \"d\", have simple matrix forms:\nTwo dual operators cancel out; \"dd\" = \"S\", and the square of formula_7 is the identity matrix. When applied to other operators, the dual operator corresponds to horizontal and vertical reflections of the matrix. Operators can be grouped into groups of four (or fewer if some forms are the same) by identifying the operators \"x\", \"xd\" (operator of dual), \"dx\" (dual of operator), and \"dxd\" (conjugate of operator). In this article, only the matrix for \"x\" is given, since the others are simple reflections.\n\nHart introduced the reflection operator \"r\", that gives the mirror image of the polyhedron. This is not strictly a LOPSP, since it does not preserve orientation (it reverses it). \"r\" has no effect on achiral seeds, and \"rr\" returns the original seed. An overline can be used to indicate the other chiral form of an operator, like = \"rsr\". \"r\" does not affect the matrix.\n\nAn operation is irreducible if it cannot be expressed as a composition of operators aside from \"d\" and \"r\". The majority of Conway's original operators are irreducible: the exceptions are \"e\", \"b\", \"o\", and \"m\".\n\nSome open questions about Conway operators include:\n\nStrictly, seed (\"S\"), needle (\"n\"), and zip (\"z\") were not included by Conway, but they are related to original Conway operations by duality so are included here.\n\nFrom here on, operations are visualized on cube seeds, drawn on the surface of that cube. Blue faces cross edges of the seed, and pink faces lie over vertices of the seed. There is some flexibility in the exact placement of vertices, especially with chiral operators. \n\nAny polyhedron can serve as a seed, as long as the operations can be executed on it. Common seeds have been assigned a letter.\nThe Platonic solids are represented by the first letter of their name (Tetrahedron, Octahedron, Cube, Icosahedron, Dodecahedron); the prisms (P) for \"n\"-gonal forms; antiprisms (A); cupolae (U); anticupolae (V); and pyramids (Y). Any Johnson solid can be referenced as J, for \"n\"=1..92.\n\nAll of the five regular polyhedra can be generated from prismatic generators with zero to two operators:\n\nThe regular Euclidean tilings can also be used as seeds:\n\nThese are operations created after Conway's original set. Note that many more operations exist than have been named; just because an operation is not here does not mean it does not exist (or is not an LSP or LOPSP). To simplify, only irreducible operators are \nincluded in this list: others can be created by composing operators together.\n\nA number of operators can be grouped together by some criteria, or have their behavior modified by an index. These are written as an operator with a subscript: \"x\".\n\nAugmentation operations retain original edges. They may be applied to any independent subset of faces, or may be converted into a \"join\"-form by removing the original edges. Conway notation supports an optional index to these operators: 0 for the join-form, or 3 or higher for how many sides affected faces have. For example, \"k\"\"Y\"=O: taking a square-based pyramid and gluing another pyramid to the square base gives an octahedron.\nThe truncate operator \"t\" also has an index form \"t\", indicating that only vertices of a certain degree are truncated. It is equivalent to \"dkd\".\n\nSome of the extended operators can be created in special cases with \"k\" and \"t\" operators. For example, a chamfered cube, \"cC\", can be constructed as \"t\"\"daC\", as a rhombic dodecahedron, \"daC\" or \"jC\", with its degree-4 vertices truncated. A lofted cube, \"lC\" is the same as \"t\"\"kC\". A quinto-dodecahedron, \"qD\" can be constructed as \"t\"\"daaD\" or \"t\"\"deD\" or \"t\"\"oD\", a deltoidal hexecontahedron, \"deD\" or \"oD\", with its degree-5 vertices truncated.\n\nMeta adds vertices at the center and along the edges, while bevel adds faces at the center, seed vertices, and along the edges. The index is how many vertices or faces are added along the edges. Meta (in its non-indexed form) is also called cantitruncation or omnitruncation. Note that 0 here does not mean the same as for augmentation operations: it means zero vertices (or faces) are added along the edges.\nMedial is like meta, except it does not add edges from the center to each seed vertex. The index 1 form is identical to Conway's ortho and expand operators: expand is also called cantellation and expansion. Note that \"o\" and \"e\" have their own indexed forms, described below. Also note that some implementations start indexing at 0 instead of 1.\nThe Goldberg-Coxeter (GC) Conway operators are two infinite families of operators that are an extension of the Goldberg-Coxeter construction. The GC construction can be thought of as taking a triangular section of a triangular lattice, or a square section of a square lattice, and laying that over each face of the polyhedron. This construction can be extended to any face by identifying the chambers of the triangle or square (the \"master polygon\"). Operators in the triangular family can be used to produce the Goldberg polyhedra and geodesic polyhedra: see List of geodesic polyhedra and Goldberg polyhedra for formulas.\n\nThe two families are the triangular GC family, \"c\" and \"u\", and the quadrilateral GC family, \"e\" and \"o\". Both the GC families are indexed by two integers formula_8 and formula_9. They possess many nice qualities:\n\nThe operators are divided into three classes (examples are written in terms of \"c\" but apply to all 4 operators):\n\nOf the original Conway operations, the only ones that do not fall into the GC family are \"g\" and \"s\" (gyro and snub). Meta and bevel (\"m\" and \"b\") can be expressed in terms of one operator from the triangular family and one from the quadrilateral family.\n\nBy basic number theory, for any values of \"a\" and \"b\", formula_10.\n\nSee also List of geodesic polyhedra and Goldberg polyhedra.\n\nConway's original set of operators can create all of the Archimedean solids and Catalan solids, using the Platonic solids as seeds. (Note that the \"r\" operator is not necessary to create both chiral forms.)\n\nThe truncated icosahedron, \"tI = zD\", can be used as a seed to create some more visually-pleasing polyhedra, although these are neither vertex nor face-transitive.\n\nEach of the convex uniform tilings can be created by applying Conway operators to the regular tilings Q, H, and Δ.\n\nConway operators can also be applied to toroidal polyhedra and polyhedra with multiple holes.\n\n\n"}
{"id": "54727095", "url": "https://en.wikipedia.org/wiki?curid=54727095", "title": "Cumulative accuracy profile", "text": "Cumulative accuracy profile\n\nThe cumulative accuracy profile (CAP) is used in data science to visualize the discriminative power of a model. The CAP of a model represents the cumulative number of positive outcomes along the \"y\"-axis versus the corresponding cumulative number of a classifying parameter along the \"x\"-axis. The CAP is distinct from the receiver operating characteristic (ROC), which plots the true-positive rate against the false-positive rate.\n\nAn example is a model that predicts whether a product is bought (positive outcome) by each individual from a group of people (classifying parameter) based on factors such as their gender, age, income etc. If group members would be contacted at random, the cumulative number of products sold would rise linearly toward a maximum value corresponding to the total number of buyers within the group. This distribution is called the \"random\" CAP. A perfect prediction, on the other hand, determines exactly which group members will buy the product, such that the maximum number of products sold will be reached with a minimum number of calls. This produces a steep line on the CAP curve that stays flat once the maximum is reached (contacting all other group members will not lead to more products sold), which is the \"perfect\" CAP.\n\nA successful model predicts the likelihood of individuals purchasing the product and ranks these probabilities to produce a list of potential customers to be contacted first. The resulting cumulative number of sold products will increase rapidly and eventually flatten out to the given maximum as more group members are contacted. This results in a distribution that lies between the random and the perfect CAP curves.\n\nThe CAP can be used to evaluate a model by comparing the curve to the perfect CAP in which the maximum number of positive outcomes is achieved directly and to the random CAP in which the positive outcomes are distributed equally. A good model will have a CAP between the perfect CAP and the random CAP with a better model tending to the perfect CAP.\n\nThe accuracy ratio (AR) is defined as the ratio of the area between the model CAP and the random CAP and the area between the perfect CAP and the random CAP. For a successful model the AR has values between zero and one, with a higher value for a stronger model.\n\nAnother indication of the model strength is given by the cumulative number of positive outcomes at 50% of the classifying parameter. For a successful model this value should lie between 50% and 100% of the maximum, with a higher percentage for stronger models.\n\nThe CAP and the ROC are both commonly used by banks and regulators to analyze the discriminatory ability of rating systems that evaluate the credit risks \n"}
{"id": "56665655", "url": "https://en.wikipedia.org/wiki?curid=56665655", "title": "Curve448", "text": "Curve448\n\nIn cryptography, Curve448 or Curve448-Goldilocks is an elliptic curve potentially offering 224 bits of security and designed for use with the elliptic curve Diffie–Hellman (ECDH) key agreement scheme. Developed by Mike Hamburg of Rambus Cryptography Research, Curve448 allows fast performance compared with other proposed curves with comparable security. The reference implementation is available under an MIT license. The curve is favored by the Internet Research Task Force Crypto Forum Research Group (IRTF CFRG) for inclusion in future TLS standards along with Curve25519. In 2017, NIST announced that Curve25519 and Curve448 would be added to Special Publication 800-186, which specifies approved elliptic curves for use by the US Federal Government. Both are described in RFC 7748.\n\nHamburg chose the Solinas trinomial prime base \"p\" = 2 − 2 − 1, calling it a “Goldilocks” prime “because its form defines the golden ratio φ ≡ 2.” The main advantage of a golden-ratio prime is fast Karatsuba multiplication.\n\nThe curve Hamburg used is an untwisted Edwards curve\nE: . The constant \"d\" = −39081 was chosen as the smallest absolute value that had the required mathematical properties, thus a nothing up my sleeve number.\n\nCurve448 is constructed such that it avoids many potential implementation pitfalls.\n"}
{"id": "1090930", "url": "https://en.wikipedia.org/wiki?curid=1090930", "title": "DFT matrix", "text": "DFT matrix\n\nIn applied mathematics, a DFT matrix is an expression of a discrete Fourier transform (DFT) as a transformation matrix, which can be applied to a signal through matrix multiplication.\n\nAn \"N\"-point DFT is expressed as the multiplication formula_1, where formula_2 is the original input signal, formula_3 is the \"N\"-by-\"N\" square DFT matrix, and formula_4 is the DFT of the signal.\n\nThe transformation matrix formula_3 can be defined as formula_6, or equivalently:\n\nwhere formula_8 is a primitive \"N\"th root of unity in which formula_9.\nThis is the Vandermonde matrix for the roots of unity, up to the normalization factor. Note that the normalization factor in front of the sum( formula_10 ) and the sign of the exponent in ω are merely conventions, and differ in some treatments. All of the following discussion applies regardless of the convention, with at most minor adjustments. The only important thing is that the forward and inverse transforms have opposite-sign exponents, and that the product of their normalization factors be 1/\"N\". However, the formula_10 choice here makes the resulting DFT matrix unitary, which is convenient in many circumstances.\n\nFast Fourier transform algorithms utilize the symmetries of the matrix to reduce the time of multiplying a vector by this matrix, from the usual formula_12. Similar techniques can be applied for multiplications by matrices such as Hadamard matrix and the Walsh matrix.\n\nThe two-point DFT is a simple case, in which the first entry is the DC (sum) and the second entry is the AC (difference).\n\nThe first row performs the sum, and the second row performs the difference.\n\nThe factor of formula_14 is to make the transform unitary (see below).\n\nThe four-point DFT matrix is as follows:\n\nwhere formula_16.\n\nThe first non-trivial integer power of two case is for eight points:\n\nwhere\n\nThe following image depicts the DFT as a matrix multiplication, with elements of the matrix depicted by samples of complex exponentials:\n\nThe real part (cosine wave) is denoted by a solid line, and the imaginary part (sine wave) by a dashed line.\n\nThe top row is all ones (scaled by formula_20 for unitarity), so it \"measures\" the DC component in the input signal. The next row is eight samples of negative one cycle of a complex exponential, i.e., a signal with a fractional frequency of −1/8, so it \"measures\" how much \"strength\" there is at fractional frequency +1/8 in the signal. Recall that a matched filter compares the signal with a time reversed version of whatever we're looking for, so when we're looking for fracfreq. 1/8 we compare with fracfreq. −1/8 so that is why this row is a negative frequency. The next row is negative two cycles of a complex exponential, sampled in eight places, so it has a fractional frequency of −1/4, and thus \"measures\" the extent to which the signal has a fractional frequency of +1/4.\n\nThe following summarizes how the 8-point DFT works, row by row, in terms of fractional frequency:\n\n\nEquivalently the last row can be said to have a fractional frequency of +1/8 and thus measure how much of the signal has a fractional frequency of −1/8. In this way, it could be said that the top rows of the matrix \"measure\" positive frequency content in the signal and the bottom rows measure negative frequency component in the signal.\n\nThe DFT is (or can be, through appropriate selection of scaling) a unitary transform, i.e., one that preserves energy. The appropriate choice of scaling to achieve unitarity is formula_10, so that the energy in the physical domain will be the same as the energy in the Fourier domain, i.e., to satisfy Parseval's theorem. (Other, non-unitary, scalings, are also commonly used for computational convenience; e.g., the convolution theorem takes on a slightly simpler form with the scaling shown in the discrete Fourier transform article.)\n\nFor other properties of the DFT matrix, including its eigenvalues, connection to convolutions, applications, and so on, see the discrete Fourier transform article.\n\nIf we make a very large matrix with complex exponentials in the rows (i.e., cosine real parts and sine imaginary parts), and increase the resolution without bound, we approach the kernel of the Fredholm integral equation of the 2nd kind, namely the Fourier operator that defines the continuous Fourier transform. A rectangular portion of this continuous Fourier operator can be displayed as an image, analogous to the DFT matrix, as shown at right, where greyscale pixel value denotes numerical quantity.\n\n\n\n"}
{"id": "1022286", "url": "https://en.wikipedia.org/wiki?curid=1022286", "title": "Derivative algebra (abstract algebra)", "text": "Derivative algebra (abstract algebra)\n\nIn abstract algebra, a derivative algebra is an algebraic structure of the signature \nwhere \n\nis a Boolean algebra and is a unary operator, the derivative operator, satisfying the identities: \n\nx is called the derivative of x. Derivative algebras provide an algebraic abstraction of the derived set operator in topology. They also play the same role for the modal logic \"wK4\" = \"K\" + \"p\"∧?\"p\" → ??\"p\" that Boolean algebras play for ordinary propositional logic. \n\n"}
{"id": "3371483", "url": "https://en.wikipedia.org/wiki?curid=3371483", "title": "Diamond-square algorithm", "text": "Diamond-square algorithm\n\nThe diamond-square algorithm is a method for generating heightmaps for computer graphics. It is a slightly better algorithm than the three-dimensional implementation of the midpoint displacement algorithm which produces two-dimensional landscapes. It is also known as the random midpoint displacement fractal, the cloud fractal or the plasma fractal, because of the plasma effect produced when applied.\n\nThe idea was first introduced by Fournier, Fussell and Carpenter at SIGGRAPH 1982.\n\nThe diamond-square algorithm starts with a 2D grid then randomly generates terrain height from four seed values arranged in a grid of points so that the entire plane is covered in squares.\n\nThe diamond-square algorithm begins with a 2D square array of width and height 2 + 1. The four corner points of the array must first be set to initial values.\nThe diamond and square steps are then performed alternately until all array values have been set.\n\nThe diamond step: For each square in the array, set the midpoint of that square to be the average of the four corner points plus a random value.\n\nThe square step: For each diamond in the array, set the midpoint of that diamond to be the average of the four corner points plus a random value.\n\nAt each iteration, the magnitude of the random value should be reduced.\n\nDuring the square steps, points located on the edges of the array will have only three adjacent values set rather than four. There are a number of ways to handle this complication - the simplest being to take the average of just the three adjacent values. Another option is to 'wrap around', taking the fourth value from the other side of the array. When used with consistent initial corner values this method also allows generated fractals to be stitched together without discontinuities.\n\nThe image below shows the steps involved in running the diamond-square algorithm on a 5 × 5 array.\n\nThis algorithm can be used to generate realistic-looking landscapes, and different implementations are used in computer graphics software such as Terragen. It is also applicable as a common component in procedural textures.\n\nThe diamond-square algorithm was analyzed by Gavin S. P. Miller in SIGGRAPH 1986 who described it as flawed because the algorithm produces noticeable vertical and horizontal \"creases\" due to the most significant perturbation taking place in a rectangular grid. The grid artifacts were addressed in a generalized algorithm introduced by J.P. Lewis. In this variant the weights on the neighboring points are obtained by solving\na small linear system motivated by estimation theory, rather than being fixed. The Lewis algorithm also allows the synthesis of non-fractal heightmaps such as rolling hills or ocean waves.\nSimilar results can be efficiently obtained with Fourier synthesis, although the possibility of adaptive refinement is lost. The diamond-square algorithm and its refinements are reviewed in the book.\n\n"}
{"id": "7735427", "url": "https://en.wikipedia.org/wiki?curid=7735427", "title": "E-function", "text": "E-function\n\nIn mathematics, E-functions are a type of power series that satisfy particular arithmetic conditions on the coefficients. They are of interest in transcendental number theory, and are more special than G-functions.\n\nA function \"f\"(\"x\") is called of type \"E, or an E\"-function, if the power series\n\nsatisfies the following three conditions:\n\nwhere the left hand side represents the maximum of the absolute values of all the algebraic conjugates of \"c\";\n\nThe second condition implies that \"f\" is an entire function of \"x\".\n\n\"E\"-functions were first studied by Siegel in 1929. He found a method to show that the values taken by certain \"E\"-functions were algebraically independent.This was a result which established the algebraic independence of classes of numbers rather than just linear independence. Since then these functions have proved somewhat useful in number theory and in particular they have application in transcendence proofs and differential equations.\n\nPerhaps the main result connected to \"E\"-functions is the Siegel–Shidlovsky theorem (also known as the Shidlovsky and Shidlovskii theorem), named after Carl Ludwig Siegel and Andrei Borisovich Shidlovskii.\n\nSuppose that we are given \"n\" \"E\"-functions, \"E\"(\"x\")...,\"E\"(\"x\"), that satisfy a system of homogeneous linear differential equations\nwhere the \"f\" are rational functions of \"x\", and the coefficients of each \"E\" and \"f\" are elements of an algebraic number field \"K\". Then the theorem states that if \"E\"(\"x\")...,\"E\"(\"x\") are algebraically independent over \"K\"(\"x\"), then for any non-zero algebraic number α that is not a pole of any of the \"f\" the numbers \"E\"(α)...,\"E\"(α) are algebraically independent.\n\n\n"}
{"id": "47824717", "url": "https://en.wikipedia.org/wiki?curid=47824717", "title": "Echo removal", "text": "Echo removal\n\nEcho removal is the process of removing echo and reverberation artifacts from audio signals. The reverberation is typically modeled as the convolution of a (sometimes time-varying) impulse response with a hypothetical clean input signal, where both the clean input signal (which is to be recovered) and the impulse response are unknown. This is an example of an inverse problem. In almost all cases, there is insufficient information in the input signal to uniquely determine a plausible original image, making it an ill-posed problem. This is generally solved by the use of a regularization term to attempt to eliminate implausible solutions.\n\nThis problem is analogous to deblurring in the image processing domain.\n\n"}
{"id": "12379776", "url": "https://en.wikipedia.org/wiki?curid=12379776", "title": "Edward Hall Alderson", "text": "Edward Hall Alderson\n\nSir Edward Hall Alderson (baptised 11 September 1787 – 27 January 1857) was an English lawyer and judge whose many judgments on commercial law helped to shape the emerging British capitalism of the Victorian era.\n\nHe was a Baron of the Exchequer and so held the honorary title Baron Alderson, in print Alderson, B.\n\nBorn in Great Yarmouth, Alderson was the eldest son of Robert (died 1833), a barrister and recorder, and Elizabeth \"née\" Hurry who died in 1791. Alderson suffered an unstable childhood, variously living with relatives, unhappily attending Charterhouse School but, more positively, being tutored by Edward Maltby. He was an able student of mathematics and classics at Gonville and Caius College, Cambridge, about to take exams he heard of the sad death of his sister Isabella. A year later in 1809 he graduated as senior wrangler, First Smith's prize, was First Medallist, and Chancellor's Gold Medallist. During free time he became an ardent debater and avid reader; winning Middle Bachelors, and the Latin Prize for \"Comparison of Ancient Dialogues with Modern\". In his finals year he also won the Members Prize, and Senior Bachelors Prize. He was consequently elected fellow.\n\nA pupil of Joseph Chitty, Alderson was called to the bar in 1811 at the Inner Temple and began work on the northern circuit where he established a substantial practice. He joined with Richard Barnewall as a law reporter from 1817 to 1822. On 26 October 1823 he married Georgina Drewe (died 1871) and the couple had many children.\n\nAn early indication of his abilities came in 1825 when he was instructed by opponents of the proposed Liverpool and Manchester Railway, principally the directors of the Bridgewater and Leeds and Liverpool Canals, as their counsel in the committee stage of the private bill needed to establish the railway. Alderson was to cross-examine George Stephenson on his designs for the railway and the surveys on which they were based. Alderson proved an able advocate and Stephenson a poor witness. Stephenson later confessed, \"I was not long in the witness box before I began to wish for a hole to creep out at.\" Largely owing to Alderson's devastating closing speech, the bill was lost, the railway was delayed for several years and Stephenson's early reputation badly damaged.\n\nAlderson was appointed to the Common Law Commission in 1828 and a judge of the Court of Common Pleas in 1830, with the attendant knighthood. He became a Baron of the Exchequer in the Exchequer of Pleas in 1834, and transferred to the Court of Chancery in 1841. He was an advocate of the plasticity of the common law in adapting to the changing times. According to Hedley, he was popular and jocular, a \"clever, analytical, and forthright judge, with little patience for those of lesser abilities\". He never sought to be a Queen's Counsel or Member of Parliament.\n\nThough, as a criminal judge at the assizes, he was instrumental in suppressing the Luddites and Chartists, he believed that rehabilitation was the principal goal of sentencing. He was dubious of the effects of deterrence and argued for the limitation of capital punishment, himself seeking to disapply it, by whatever technical means he could creatively devise.\n\nAn active member of the Church of England and a close friend of Bishop of London Charles James Blomfield, Alderson supported the Gorham judgment which held that the Church was subject to secular law. He was a noted advocate of affirmation as an alternative to the oath for witnesses but opposed the growing contemporary campaign for secular education. Hedley describes Alderson as a \"Conservative... suspicious of the 'tyranny' he saw in democracy\".\n\nAlderson established homes in London and Lowestoft where he wrote poetry, in English and Latin, and corresponded with his cousin, novelist Amelia Opie. He was also an enthusiastic and knowledgeable follower of horse racing.\n\nWhile sitting at Liverpool assizes in December 1856, he heard of a serious injury to one of his sons and collapsed. He died the following January at his London home from a brain disease. He was buried at St Mary Magdalen's Church, Risby, near Bury St Edmunds.\n\nAlderson's daughter Georgina Charlotte married British statesman, Robert Gascoyne-Cecil, 3rd Marquess of Salisbury in 1857. Salisbury's father, James Gascoyne-Cecil, 2nd Marquess of Salisbury, opposed the marriage owing to Georgina's lack of wealth and social standing.\n\n\n\n"}
{"id": "598971", "url": "https://en.wikipedia.org/wiki?curid=598971", "title": "Fisher information", "text": "Fisher information\n\nIn mathematical statistics, the Fisher information (sometimes simply called information) is a way of measuring the amount of information that an observable random variable \"X\" carries about an unknown parameter \"θ\" of a distribution that models \"X\". \nFormally, it is the variance of the score, or the expected value of the observed information. In Bayesian statistics, the Asymptotic distribution of the posterior mode depends on the Fisher information and not on the prior (according to the Bernstein–von Mises theorem, which was anticipated by Laplace for exponential families). The role of the Fisher information in the asymptotic theory of maximum-likelihood estimation was emphasized by the statistician Ronald Fisher (following some initial results by Francis Ysidro Edgeworth). The Fisher information is also used in the calculation of the Jeffreys prior, which is used in Bayesian statistics.\n\nThe Fisher-information matrix is used to calculate the covariance matrices associated with maximum-likelihood estimates. It can also be used in the formulation of test statistics, such as the Wald test.\n\nStatistical systems of a scientific nature (physical, biological, etc.) whose likelihood functions obey shift invariance have been shown to obey maximum Fisher information. The level of the maximum depends upon the nature of the system constraints.\n\nThe Fisher information is a way of measuring the amount of information that an observable random variable \"X\" carries about an unknown parameter \"θ\" upon which the probability of \"X\" depends. Let \"f\"(\"X\"; \"θ\") be the probability density function (or probability mass function) for \"X\" conditional on the value of \"θ\". This is also the likelihood function for \"θ\". It describes the probability that we observe a given sample \"X\", \"given\" a known value of \"θ\". If \"f\" is sharply peaked with respect to changes in \"θ\", it is easy to indicate the “correct” value of \"θ\" from the data, or equivalently, that the data \"X\" provides a lot of information about the parameter \"θ\". If the likelihood \"f\" is flat and spread-out, then it would take many samples like of \"X\" to estimate the actual “true” value of \"θ\" that \"would\" be obtained using the entire population being sampled. This suggests studying some kind of variance with respect to \"θ\".\n\nFormally, the partial derivative with respect to \"θ\" of the natural logarithm of the likelihood function is called the “\"score\"”. Under certain regularity conditions, if \"θ\" is the true parameter (i.e. \"X\" is actually distributed as \"f\"(\"X\"; \"θ\")), it can be shown that the expected value (the first moment) of the score is 0:\nThe variance (which equals the second central moment) is defined to be the Fisher information:\nNote that formula_3. A random variable carrying high Fisher information implies that the absolute value of the score is often high. The Fisher information is not a function of a particular observation, as the random variable \"X\" has been averaged out.\n\nIf is twice differentiable with respect to \"θ\", and under certain regularity conditions, then the Fisher information may also be written as\nsince\nand\nThus, the Fisher information may be seen as the curvature of the support curve (the graph of the log-likelihood). Near the maximum likelihood estimate, low Fisher information therefore indicates that the maximum appears \"blunt\", that is, the maximum is shallow and there are many nearby values with a similar log-likelihood. Conversely, high Fisher information indicates that the maximum is sharp.\n\nThe Cramér–Rao bound states that the inverse of the Fisher information is a lower bound on the variance of any unbiased estimator of \"θ\". H.L. Van Trees (1968) and B. Roy Frieden (2004) provide the following method of deriving the Cramér–Rao bound, a result which describes use of the Fisher information.\n\nInformally, we begin by considering an unbiased estimator formula_7. Mathematically, \"unbiased\" means that\n\nThis expression is zero independent of \"θ\", so its partial derivative with respect to \"θ\" must also be zero. By the product rule, this partial derivative is also equal to\n\nFor each \"θ\", the likelihood function is a probability density function, and therefore formula_10. A basic computation implies that\n\nUsing these two facts in the above, we get\n\nFactoring the integrand gives\n\nSquaring the expression in the integral, the Cauchy–Schwarz inequality yields\n\nThe second bracketed factor is defined to be the Fisher Information, while the first bracketed factor is the expected mean-squared error of the estimator formula_15. By rearranging, the inequality tells us that\n\nIn other words, the precision to which we can estimate \"θ\" is fundamentally limited by the Fisher information of the likelihood function.\n\nA Bernoulli trial is a random variable with two possible outcomes, \"success\" and \"failure\", with success having a probability of \"θ\". The outcome can be thought of as determined by a coin toss, with the probability of heads being \"θ\" and the probability of tails being .\n\nLet \"X\" be a Bernoulli trial. The Fisher information contained in \"X\" may be calculated to be\n\nBecause Fisher information is additive, the Fisher information contained in \"n\" independent Bernoulli trials is therefore\nThis is the reciprocal of the variance of the mean number of successes in \"n\" Bernoulli trials, so in this case, the Cramér–Rao bound is an equality.\n\nWhen there are \"N\" parameters, so that \"θ\" is a vector formula_19 then the Fisher information takes the form of an matrix. This matrix is called the Fisher information matrix (FIM) and has typical element\n\nThe FIM is a positive semidefinite matrix. If it is positive definite, then it defines a Riemannian metric on the \"N\"-dimensional parameter space. The topic information geometry uses this to connect Fisher information to differential geometry, and in that context, this metric is known as the Fisher information metric.\n\nUnder certain regularity conditions, the Fisher information matrix may also be written as\n\nThe result is interesting in several ways:\n\nWe say that two parameters \"θ\" and \"θ\" are orthogonal if the element of the \"i\"th row and \"j\"th column of the Fisher information matrix is zero. Orthogonal parameters are easy to deal with in the sense that their maximum likelihood estimates are independent and can be calculated separately. When dealing with research problems, it is very common for the researcher to invest some time searching for an orthogonal parametrization of the densities involved in the problem.\n\nIf the Fisher information matrix is positive definite for all , then the corresponding statistical model is said to be \"regular\"; otherwise, the statistical model is said to be \"singular\". Examples of singular statistical models include the following: normal mixtures, binomial mixtures, multinomial mixtures, Bayesian networks, neural networks, radial basis functions, hidden Markov models, stochastic context-free grammars, reduced rank regressions, Boltzmann machines.\n\nIn machine learning, if a statistical model is devised so that it extracts hidden structure from a random phenomenon, then it naturally becomes singular.\n\nThe FIM for a \"N\"-variate multivariate normal distribution, formula_22 has a special form. Let the \"K\"-dimensional vector of parameters be formula_23 and the vector of random normal variables be formula_24. Assume that the mean values of these random variables are formula_25, and let formula_26 be the covariance matrix. Then, for formula_27, the (\"m\", \"n\") entry of the FIM is:\n\nwhere formula_29 denotes the transpose of a vector, denotes the trace of a square matrix, and:\n\n\n\nNote that a special, but very common, case is the one where\nformula_32, a constant. Then\n\nIn this case the Fisher information matrix may be identified with the coefficient matrix of the normal equations of least squares estimation theory.\n\nAnother special case occurs when the mean and covariance depend on two different vector parameters, say, \"β\" and \"θ\". This is especially popular in the analysis of spatial data, which often uses a linear model with correlated residuals. In this case,\n\nwhere\n\nSimilar to the entropy or mutual information, the Fisher information also possesses a chain rule decomposition. In particular, if \"X\" and \"Y\" are jointly distributed random variables, it follows that: \nwhere formula_37 is the Fisher information of \"Y\" relative to formula_38 calculated with respect to the conditional density of \"Y\" given a specific value \"X\" = \"x\".\n\nAs a special case, if the two random variables are independent, the information yielded by the two random variables is the sum of the information from each random variable separately:\nConsequently, the information in a random sample of \"n\" independent and identically distributed observations is \"n\" times the information in a sample of size 1.\n\nThe information provided by a sufficient statistic is the same as that of the sample \"X\". This may be seen by using Neyman's factorization criterion for a sufficient statistic. If \"T\"(\"X\") is sufficient for \"θ\", then\nfor some functions \"g\" and \"h\". The independence of \"h\"(\"X\") from \"θ\" implies\nand the equality of information then follows from the definition of Fisher information. More generally, if is a statistic, then\n\nwith equality if and only if \"T\" is a sufficient statistic.\n\nThe Fisher information depends on the parametrization of the problem. If \"θ\" and \"η\" are two scalar parametrizations of an estimation problem, and \"θ\" is a continuously differentiable function of \"η\", then\nwhere formula_44 and formula_45 are the Fisher information measures of \"η\" and \"θ\", respectively.\n\nIn the vector case, suppose formula_46 and formula_47 are \"k\"-vectors which parametrize an estimation problem, and suppose that formula_46 is a continuously differentiable function of formula_47, then,\nwhere the (\"i\", \"j\")th element of the \"k\" × \"k\" Jacobian matrix formula_51 is defined by\nand where formula_53 is the matrix transpose of formula_54\n\nIn information geometry, this is seen as a change of coordinates on a Riemannian manifold, and the intrinsic properties of curvature are unchanged under different parametrization. In general, the Fisher information matrix provides a Riemannian metric (more precisely, the Fisher–Rao metric) for the manifold of thermodynamic states, and can be used as an information-geometric complexity measure for a classification of phase transitions, e.g., the scalar curvature of the thermodynamic metric tensor diverges at (and only at) a phase transition point.\n\nIn the thermodynamic context, the Fisher information matrix is directly related to the rate of change in the corresponding order parameters. In particular, such relations identify second-order phase transitions via divergences of individual elements of the Fisher information matrix.\n\nFisher information is widely used in optimal experimental design. Because of the reciprocity of estimator-variance and Fisher information, \"minimizing\" the \"variance\" corresponds to \"maximizing\" the \"information\".\n\nWhen the linear (or linearized) statistical model has several parameters, the mean of the parameter estimator is a vector and its variance is a matrix. The inverse of the variance matrix is called the \"information matrix\". Because the variance of the estimator of a parameter vector is a matrix, the problem of \"minimizing the variance\" is complicated. Using statistical theory, statisticians compress the information-matrix using real-valued summary statistics; being real-valued functions, these \"information criteria\" can be maximized.\n\nTraditionally, statisticians have evaluated estimators and designs by considering some summary statistic of the covariance matrix (of an unbiased estimator), usually with positive real values (like the determinant or matrix trace). Working with positive real numbers brings several advantages: If the estimator of a single parameter has a positive variance, then the variance and the Fisher information are both positive real numbers; hence they are members of the convex cone of nonnegative real numbers (whose nonzero members have reciprocals in this same cone).\nFor several parameters, the covariance matrices and information matrices are elements of the convex cone of nonnegative-definite symmetric matrices in a partially ordered vector space, under the Loewner (Löwner) order. This cone is closed under matrix addition and inversion, as well as under the multiplication of positive real numbers and matrices. An exposition of matrix theory and Loewner order appears in Pukelsheim.\n\nThe traditional optimality criteria are the information matrix's invariants, in the sense of invariant theory; algebraically, the traditional optimality criteria are functionals of the eigenvalues of the (Fisher) information matrix (see optimal design).\n\nIn Bayesian statistics, the Fisher information is used to calculate the Jeffreys prior, which is a standard, non-informative prior for continuous distribution parameters.\n\nThe Fisher information has been used to find bounds on the accuracy of neural codes. In that case, \"X\" is typically the joint responses of many neurons representing a low dimensional variable \"θ\" (such as a stimulus parameter). In particular the role of correlations in the noise of the neural responses has been studied.\n\nFisher information plays a central role in a controversial principle put forward by Frieden as the basis of physical laws, a claim that has been disputed.\n\nThe Fisher information is used in machine learning techniques such as elastic weight consolidation, which reduces catastrophic forgetting in artificial neural networks.\n\nFisher information is related to relative entropy. Consider a family of probability distributions formula_55 where formula_38 is a parameter which lies in a range of values. Then the relative entropy, or Kullback–Leibler divergence, between two distributions in the family can be written as\nwhile the Fisher information matrix is:\n\nIf formula_38 is fixed, then the relative entropy between two distributions of the same family is minimized at formula_60. For formula_61 close to formula_38, one may expand the previous expression in a series up to second order:\n\nThus the Fisher information represents the curvature of the relative entropy.\n\nSchervish (1995: §2.3) says the following.\nThe Fisher information was discussed by several early statisticians, notably F. Y. Edgeworth. For example, Savage says: \"In it [Fisher information], he [Fisher] was to some extent anticipated (Edgeworth 1908–9 esp. 502, 507–8, 662, 677–8, 82–5 and references he [Edgeworth] cites including Pearson and Filon 1898 [. . .]).\"\nThere are a number of early historical sources\nand a number of reviews of this early work.\n\n\nOther measures employed in information theory:\n\n"}
{"id": "20962073", "url": "https://en.wikipedia.org/wiki?curid=20962073", "title": "Fisher–Tippett–Gnedenko theorem", "text": "Fisher–Tippett–Gnedenko theorem\n\nIn statistics, the Fisher–Tippett–Gnedenko theorem (also the Fisher–Tippett theorem or the extreme value theorem) is a general result in extreme value theory regarding asymptotic distribution of extreme order statistics. The maximum of a sample of iid random variables after proper renormalization can only converge in distribution to one of 3 possible distributions, the Gumbel distribution, the Fréchet distribution, or the Weibull distribution. Credit for the extreme value theorem (or convergence to types theorem) is given to Gnedenko (1948), previous versions were stated by Ronald Fisher and Leonard Henry Caleb Tippett in 1928 and Fréchet in 1927.\n\nThe role of the extremal types theorem for maxima is similar to that of central limit theorem for averages, except that the central limit theorem applies to the average of a sample from any distribution with finite variance, while the Fisher-Tippet-Gnedenko theorem only states that \"if\" the distribution of a normalized maximum converges, \"then\" the limit has to be one of a particular class of distributions. It does not state that the distribution of the normalized maximum does converge.\n\nLet formula_1 be a sequence of independent and identically-distributed random variables, and formula_2. If a sequence of pairs of real numbers formula_3 exists such that each formula_4 and\nformula_5,\nwhere formula_6 is a non degenerate distribution function, then the limit distribution formula_6 belongs to either the Gumbel, the Fréchet or the Weibull family. These can be grouped into the generalized extreme value distribution.\n\nIf \"G\" is the distribution function of \"X\", then \"M\" can be rescaled to converge in distribution to\n\nConvergence conditions for the Gumbel distribution are more involved.\n\n\n"}
{"id": "501532", "url": "https://en.wikipedia.org/wiki?curid=501532", "title": "Fractional part", "text": "Fractional part\n\nThe fractional part or decimal part of a non‐negative real number formula_1 is the excess beyond that number's integer part. If the latter is defined as the largest integer not greater than \"x\", called floor of \"x\" or formula_2, its fractional part can be written as: \n\nFor a positive number written in a conventional positional numeral system (such as binary or decimal), its fractional part hence corresponds to the digits appearing after the radix point.\n\nHowever, in case of negative numbers, there are various conflicting ways to extend the fractional part function to them: It is either defined in the same way as for positive numbers, i.e. by formula_4 , or as the part of the number to the right of the radix point, formula_5 , finally, by the odd function \n\nwith formula_7 as the smallest integer not less than \"x\", also called the ceiling of \"x\". By consequence, we may get, for example, three different values for the fractional part of just one x: let it be −1.3, its fractional part will be 0.7 according to the first definition, 0.3 according to the second definition, and −0.3 according to the third definition, whose result can also be obtained in a straightforward way by\n\nUnder the first definition all real numbers can be written in the form formula_9, where formula_10 is the number to the left of the radix point, and the remaining fractional part formula_11 is a nonnegative real number less than one. If formula_1 is a positive rational number, then the fractional part of formula_1 can be expressed in the form formula_14, where formula_15 and formula_16 are integers and formula_17. For example, if \"x\" = 1.05, then the fractional part of \"x\" is 0.05 and can be expressed as 5 / 100 = 1 / 20.\n\nEvery real number can be essentially uniquely represented as a continued fraction, namely as the sum of its integer part and the reciprocal of its fractional part which is written as the sum of \"its\" integer part and the reciprocal of \"its\" fractional part, and so on.\n\n"}
{"id": "1362593", "url": "https://en.wikipedia.org/wiki?curid=1362593", "title": "Further Mathematics", "text": "Further Mathematics\n\nFurther Mathematics is the title given to a number of advanced secondary mathematics courses. Higher and Further Mathematics may also refer to any of several advanced mathematics courses at many institutions. It thought to be a course of high-calibre as compared to the standard AS-Level mathematics course.\n\nIn the United Kingdom, Further Mathematics describes a course studied in addition to the standard mathematics AS-Level and A-Level courses. In Victoria, Australia it describes a course delivered as part of the Victorian Certificate of Education. See the section on Victoria for a more detailed explanation. Globally, it describes a course studied in addition to GCE AS-Level and A-Level Mathematics, or one delivered as part of the International Baccalaureate Diploma.\n\nA qualification in Further Mathematics involves studying both pure and applied modules. Whilst the pure modules - formerly known as Pure 4-6 (or Core 4-6), now known as Further Pure 1-3 (4 exists for the AQA board) - build on knowledge from the core mathematics modules, the applied modules may start from first principles. \n\nTo achieve an A level in Further Maths, candidates must study six modules which have not already been used for their Maths A level. These six modules must consist of FP1, at least one of FP2 or FP3, and 4 other modules.\nWith regard to Mathematics degrees, most universities do not require Further Mathematics, and may incorporate foundation math modules or offer \"catch-up\" classes covering any additional content. Exceptions are the University of Warwick, the University of Cambridge which requires Further Mathematics to at least AS level; University College London requires or recommends an A2 in Further Maths for its maths courses; Imperial College requires an A in A level Further Maths while other universities may recommend it or may promise lower offers in return. Some schools and colleges may not offer Further mathematics but online resources are available \nAlthough the subject has about 60% of its cohort obtaining \"A\" grades, students choosing the subject are assumed to be more able in mathematics. \n\nSome medicine courses do not count maths and further maths as separate subjects for the purposes of making offers due to the perceived overlap in content and the potentially narrow education a candidate with maths, further maths and just one other subject may have.\n\nIn contrast with other Further Mathematics courses, Further Maths as part of the VCE is the easiest level of mathematics. Any student wishing to undertake tertiary studies in areas such as Science, Engineering, Commerce, Economics, and some Information Technology courses, must undertake one or both of the other two VCE maths subjects- Mathematical Methods or Specialist Mathematics. The Further Mathematics syllabus in VCE consists of three core modules, which all students undertake, plus two modules chosen by the student (or usually by the school or teacher) from a list of four. The core modules are Univariate Data, Bivariate Data, Time Series, Number Patterns and Business-Related Mathematics. The optional modules are Geometry and Trigonometry, Graphs and Relations, Networks and Decision Mathematics, or Matrices.\nSee also: Mickey Mouse\n\nFurther Mathematics is available as a second and higher mathematics course at A Level (now H2), in addition to the Mathematics course at A Level. Students can offer this subject if they have A2 and better in 'O' Level Mathematics and Additional Mathematics, depending on the school. \n\nFurther Mathematics, as studied within the International Baccalaureate Diploma Programme, is a Higher Level (HL) course that can be taken in conjunction with Mathematics HL or on its own. It consists of studying all four of the options in Mathematics HL, plus two additional topics.\n\nTopics studied in Further Mathematics include:\n\n\n"}
{"id": "13028195", "url": "https://en.wikipedia.org/wiki?curid=13028195", "title": "Ideal tasks", "text": "Ideal tasks\n\nIdeal tasks arise during task analysis. Ideal tasks are different from real tasks. They are ideals in the Platonic sense of a circle being an ideal whereas a drawn circle is flawed and real. The study of theoretically best or “mathematically ideal” tasks (Green & Swets, 1966), has been the basis of the branch of stimulus control in psychology called Psychophysics as well as being part of Artificial Intelligence (e. g. Goel & Chandrasekaran, 1992). Such studies include the instantiation of such ideal tasks in the real world. The notion of the ideal task has also played an important role in information theory. Tasks are defined as sequences of contingencies, each presenting stimuli and requiring an action or a sequence of actions to occur in some non-arbitrary fashion. These contingencies may not only provide stimuli that require the discrimination of relations among actions and events but among task actions themselves. Again, Task actions, E, are actions that are required to complete tasks. Properties of tasks (usually the stimuli, or the relationship among stimuli and actions) are varied, and responses to them can be measured and analyzed.\n\n"}
{"id": "27430123", "url": "https://en.wikipedia.org/wiki?curid=27430123", "title": "Integer triangle", "text": "Integer triangle\n\nAn integer triangle or integral triangle is a triangle all of whose sides have lengths that are integers. A rational triangle can be defined as one having all sides with rational length; any such rational triangle can be integrally rescaled (can have all sides multiplied by the same integer, namely a common multiple of their denominators) to obtain an integer triangle, so there is no substantive difference between integer triangles and rational triangles in this sense. Note however, that other definitions of the term \"rational triangle\" also exist: In 1914 Carmichael used the term in the sense that we today use the term Heronian triangle; Somos uses it to refer to triangles whose ratios of sides are rational; Conway and Guy define a rational triangle as one with rational sides and rational angles measured in degrees—in which case the only rational triangle is the rational-sided equilateral triangle.\n\nThere are various general properties for an integer triangle, given in the first section below. All other sections refer to classes of integer triangles with specific properties.\n\nAny triple of positive integers can serve as the side lengths of an integer triangle as long as it satisfies the triangle inequality: the longest side is shorter than the sum of the other two sides. Each such triple defines an integer triangle that is unique up to congruence. So the number of integer triangles (up to congruence) with perimeter \"p\" is the number of partitions of \"p\" into three positive parts that satisfy the triangle inequality. This is the integer closest to when \"p\" is even and to when \"p\" is odd. It also means that the number of integer triangles with even numbered perimeters \"p\" = 2\"n\" is the same as the number of integer triangles with odd numbered perimeters \"p\" = 2\"n\" − 3. Thus there is no integer triangle with perimeter 1, 2 or 4, one with perimeter 3, 5, 6 or 8, and two with perimeter 7 or 10. The sequence of the number of integer triangles with perimeter \"p\", starting at \"p\" = 1, is:\n\nThe number of integer triangles (up to congruence) with given largest side \"c\" and integer triple (\"a\", \"b\", \"c\") is the number of integer triples such that \"a\" + \"b\" > \"c\" and \"a\" ≤ \"b\" ≤ \"c\". This is the integer value Ceiling[] * Floor[]. Alternatively, for \"c\" even it is the double triangular number ( + 1) and for \"c\" odd it is the square . It also means that the number of integer triangles with greatest side \"c\" exceeds the number of integer triangles with greatest side \"c\"−2 by \"c\". The sequence of the number of non-congruent integer triangles with largest side \"c\", starting at \"c\" = 1, is:\n\nThe number of integer triangles (up to congruence) with given largest side \"c\" and integer triple (\"a\", \"b\", \"c\") that lie on or within a semicircle of diameter \"c\" is the number of integer triples such that \"a\" + \"b\" > \"c\" , \"a\" + \"b\" ≤ \"c\" and \"a\" ≤ \"b\" ≤ \"c\". This is also the number of integer sided obtuse or right (non-acute) triangles with largest side \"c\". The sequence starting at \"c\" = 1, is:\n\nConsequently, the difference between the two above sequences gives the number of acute integer sided triangles (up to congruence) with given largest side \"c\". The sequence starting at \"c\" = 1, is:\n\nBy Heron's formula, if \"T\" is the area of a triangle whose sides have lengths \"a\", \"b\", and \"c\" then\n\nSince all the terms under the radical on the right side of the formula are integers it follows that all integer triangles must have an integer value of \"16T\" and \"T\" will be rational.\n\nBy the law of cosines, every angle of an integer triangle has a rational cosine.\n\nIf the angles of any triangle form an arithmetic progression then one of its angles must be 60°. For integer triangles the remaining angles must also have rational cosines and a method of generating such triangles is given below. However, apart from the trivial case of an equilateral triangle there are no integer triangles whose angles form either a geometric or harmonic progression. This is because such angles have to be rational angles of the form with rational  0 < < 1. But all the angles of integer triangles must have rational cosines and this will occur only when  =   i.e. the integer triangle is equilateral.\n\nThe square of each internal angle bisector of an integer triangle is rational, because the general triangle formula for the internal angle bisector of angle \"A\" is formula_2 where \"s\" is the semiperimeter (and likewise for the other angles' bisectors).\n\nAny altitude dropped from a vertex onto an opposite side or its extension will split that side or its extension into rational lengths.\n\nThe square of twice any median of an integer triangle is an integer, because the general formula for the squared median \"m\" to side \"a\" is formula_3, giving (2\"m\") = 2\"b\" + 2\"c\" − \"a\" (and likewise for the medians to the other sides).\n\nBecause the square of the area of an integer triangle is rational, the square of its circumradius is also rational, as is the square of the inradius.\n\nThe ratio of the inradius to the circumradius of an integer triangle is rational, equaling formula_4 for semiperimeter \"s\" and area \"T\".\n\nThe product of the inradius and the circumradius of an integer triangle is rational, equaling formula_5\n\nThus the squared distance between the incenter and the circumcenter of an integer triangle, given by Euler's theorem as \"R−2Rr\", is rational.\n\nAll Heronian triangles can be placed on a lattice with each vertex at a lattice point.\n\nA Heronian triangle, also known as a Heron triangle or a Hero triangle, is a triangle with integer sides and integer area. Every Heronian triangle has sides proportional to\n\nfor integers \"m\", \"n\" and \"k\" subject to the constraints:\n\nThe proportionality factor is generally a rational  formula_14  where  formula_15  reduces the generated Heronian triangle to its primitive and  formula_16  scales up this primitive to the required size.\n\nA Pythagorean triangle is right angled and Heronian. Its three integer sides are known as a Pythagorean triple or Pythagorean triplet or Pythagorean triad. All Pythagorean triples formula_17 with hypotenuse formula_18 which are primitive (the sides having no common factor) can be generated by\n\nwhere \"m\" and \"n\" are coprime integers and one of them is even with \"m\" > \"n\".\n\nEvery even number greater than 2 can be the leg of a Pythagorean triangle (not necessarily primitive) because if the leg is given by formula_24 and we choose formula_25 as the other leg then the hypotenuse is formula_26. This is essentially the generation formula above with formula_27 set to 1 and allowing formula_28 to range from 2 to infinity.\n\nThere are no primitive Pythagorean triangles with integer altitude from the hypotenuse. This is because twice the area equals any base times the corresponding height: 2 times the area thus equals both \"ab\" and \"cd\" where \"d\" is the height from the hypotenuse \"c\". The three side lengths of a primitive triangle are coprime, so \"d\" =  is in fully reduced form; since \"c\" cannot equal 1 for any primitive Pythagorean triangle, \"d\" cannot be an integer.\n\nHowever, any Pythagorean triangle with legs \"x\", \"y\" and hypotenuse \"z\" can generate a Pythagorean triangle with an integer altitude, by scaling up the sides by the length of the hypotenuse \"z\". If \"d\" is the altitude, then the generated Pythagorean triangle with integer altitude is given by\n\nConsequently, all Pythagorean triangles with legs \"a\" and \"b\", hypotenuse \"c\", and integer altitude \"d\" from the hypotenuse, with gcd (\"a, b, c, d\") = 1, which necessarily have both \"a\" + \"b\" = c and formula_30, are generated by\n\nfor coprime integers \"m\", \"n\" with \"m\" > \"n\".\n\nA triangle with integer sides and integer area has sides in arithmetic progression if and only if the sides are (\"b\" – \"d\", \"b\", \"b\" + \"d\"), where\n\nand where \"g\" is the greatest common divisor of formula_39 formula_40, and formula_41\n\nAll Heronian triangles with B=2A are generated by either\n\nwith integers \"k\", \"s\", \"r\" such that \"s\" > 3\"r\", or\n\nwith integers \"q\", \"u\", \"v\" such that \"v\" > \"u\" and \"v\" < (7+4)\"u\".\n\nNo Heronian triangles with \"B\" = 2\"A\" are isosceles or right triangles because all resulting angle combinations generate angles with non-rational sines, giving a non-rational area or side.\n\nAll isosceles Heronian triangles are decomposable. They are formed by joining two congruent Pythagorean triangles along either of their common legs such that the equal sides of the isosceles triangle are the hypotenuses of the Pythagorean triangles, and the base of the isosceles triangle is twice the other Pythagorean leg. Consequently, every Pythagorean triangle is the building block for two isosceles Heronian triangles since the join can be along either leg.\nAll pairs of isosceles Heronian triangles are given by rational multiples of\n\nand\n\nfor coprime integers \"u\" and \"v\" with \"u\" > \"v\" and \"u\" + \"v\" odd.\n\nIt has been shown that a Heronian triangle whose perimeter is four times a prime is uniquely associated with the prime and that the prime is of the form formula_56. It is well known that such a prime formula_16 can be uniquely partitioned into integers formula_28 and formula_27 such that formula_60 (see Euler's idoneal numbers). Furthermore, it has been shown that such Heronian triangles are primitive since the smallest side of the triangle has to be equal to the prime that is one quarter of its perimeter.\n\nConsequently, all primitive Heronian triangles whose perimeter is four times a prime can be generated by\n\nfor integers formula_28 and formula_27 such that formula_68 is a prime.\n\nFurthermore, the factorization of the area is formula_69 where formula_60 is prime. However the area of a Heronian triangle is always divisible by formula_71. This gives the result that apart from when formula_72 and formula_73 which gives formula_74 all other parings of formula_28 and formula_27 must have formula_28 odd with only one of them divisible by formula_78.\n\nThere are infinitely many decomposable, and infinitely many indecomposable, primitive Heronian (non-Pythagorean) triangles with integer radii for the incircle and each excircle. A family of decomposible ones is given by\nand a family of indecomposable ones is given by\n\nThere exist tetrahedra having integer-valued volume and Heron triangles as faces. One example has one edge of 896, the opposite edge of 190, and the other four edges of 1073; two faces have areas of 436800 and the other two have areas of 47120, while the volume is 62092800.\n\nA 2D lattice is a regular array of isolated points where if any one point is chosen as the Cartesian origin (0, 0), then all the other points are at (\"x, y\") where \"x\" and \"y\" range over all positive and negative integers. A lattice triangle is any triangle drawn within a 2D lattice such that all vertices lie on lattice points. By Pick's theorem a lattice triangle has a rational area that either is an integer or has a denominator of 2. If the lattice triangle has integer sides then it is Heronian with integer area.\n\nFurthermore, it has been proved that all Heronian triangles can be drawn as lattice triangles. Consequently, an integer triangle is Heronian if and only if it can be drawn as a lattice triangle.\n\nThere are infinitely many primitive Heronian (non-Pythagorean) triangles which can be placed on an integer lattice with all vertices, the incenter, and all three excenters at lattice points. Two families of such triangles are the ones with parametrizations given above at #Heronian triangles with integer inradius and exradii.\n\nAn automedian triangle is one whose medians are in the same proportions (in the opposite order) as the sides. If \"x\", \"y\", and \"z\" are the three sides of a right triangle, sorted in increasing order by size, and if 2\"x\" < \"z\", then \"z\", \"x\" + \"y\", and \"y\" − \"x\" are the three sides of an automedian triangle. For instance, the right triangle with side lengths 5, 12, and 13 can be used in this way to form the smallest non-trivial (i.e., non-equilateral) integer automedian triangle, with side lengths 13, 17, and 7.\n\nConsequently, using Euclid's formula, which generates primitive Pythagorean triangles, it is possible to generate primitive integer automedian triangles as\nwith formula_28 and formula_27 coprime and formula_88 odd, and formula_89  (if the quantity inside the absolute value signs is negative) or  formula_90 (if that quantity is positive) to satisfy the triangle inequality.\n\nAn important characteristic of the automedian triangle is that the squares of its sides form an arithmetic progression. Specifically, formula_91 so formula_92.\n\nA triangle family with integer sides formula_93 and with rational bisector formula_94 of angle A is given by\n\nwith integers formula_99.\n\nThere exist infinitely many non-similar triangles in which the three sides and the bisectors of each of the three angles are integers.\n\nThere exist infinitely many non-similar triangles in which the three sides and the two trisectors of each of the three angles are integers.\n\nHowever, for \"n\" > 3 there exist no triangles in which the three sides and the (\"n\"–1) \"n\"-sectors of each of the three angles are integers.\n\nSome integer triangles with one angle at vertex \"A\" having given rational cosine \"h/k\" (\"h\"<0 or >0; \"k\">0) are given by\n\nwhere \"p\" and \"q\" are any coprime positive integers such that \"p>qk\".\n\nAll integer triangles with a 60° angle have their angles in an arithmetic progression. All such triangles are proportional to:\n\nwith coprime integers \"m\", \"n\" and 1 ≤ \"n\" ≤ \"m\" or 3\"m\" ≤ \"n\". From here, all primitive solutions can be obtained by dividing \"a\", \"b\", and \"c\" by their greatest common divisor.\n\nInteger triangles with a 60° angle can also be generated by\n\nwith coprime integers \"m\", \"n\" with 0 < \"n\" < \"m\" (the angle of 60° is opposite to the side of length \"a\"). From here, all primitive solutions can be obtained by dividing \"a\", \"b\", and \"c\" by their greatest common divisor (e.g. an equilateral triangle solution is obtained by taking \"m\" = 2 and \"n\" = 1, but this produces \"a\" = \"b\" = \"c\" = 3, which is not a primitive solution). See also \n\nMore precisely, If formula_109, then formula_110, otherwise formula_111. Two different pairs formula_112 and formula_113 generate the same triple. Unfortunately the two pairs can both be of gcd=3, so we can't avoid duplicates by simply skipping that case. Instead, duplicates can be avoided by formula_27 going only till formula_115. Note that we still need to divide by 3 if gcd=3. The only solution for formula_116 under the above constraints is formula_117 for formula_118. With this additional formula_119 constraint all triples can be generated uniquely.\n\nAn Eisenstein triple is a set of integers which are the lengths of the sides of a triangle where one of the angles is 60 degrees.\n\nInteger triangles with a 120° angle can be generated by\n\nwith coprime integers \"m\", \"n\" with 0 < \"n\" < \"m\" (the angle of 120° is opposite to the side of length \"a\"). From here, all primitive solutions can be obtained by dividing \"a\", \"b\", and \"c\" by their greatest common divisor (e.g. by taking \"m\" = 4 and \"n\" = 1, one obtains \"a\" = 21, \"b\" = 9 and \"c\" = 15, which is not a primitive solution, but leads to the primitive solution \"a\" = 7, \"b\" = 3, and \"c\" = 5 which, up to order, can be obtained with the values \"m\" = 2 and \"n\" = 1). See also.\n\nMore precisely, If formula_123, then formula_110, otherwise formula_111. Since the biggest side \"a\" can only be generated with a single formula_112 pair, each primitive triple can be generated in precisely two ways: once directly with gcd=1, and once indirectly with gcd=3. Therefore, in order to generate all primitive triples uniquely, one can just add additional formula_127 condition.\n\nFor positive relatively prime integers \"h\" and \"k\", the triangle with the following sides has angles formula_128, formula_129, and formula_130 and hence two angles in the ratio \"h : k\", and its sides are integers:\n\nwhere formula_134 and \"p\" and \"q\" are any relatively prime integers such that formula_135.\n\nWith angle A opposite side formula_136 and angle B opposite side formula_137, some triangles with B=2A are generated by\n\nwith integers \"m\", \"n\" such that 0 < \"n\" < \"m\" < 2\"n\".\n\nNote that all triangles with \"B\" = 2\"A\" (whether integer or not) have formula_141.\n\nThe equivalence class of similar triangles with formula_142 are generated by\n\nwith integers formula_146 such that formula_147, where formula_148 is the golden ratio formula_149.\n\nNote that all triangles with formula_142 (whether with integer sides or not) satisfy formula_151.\n\nWe can generate the full equivalence class of similar triangles that satisfy B=3A by using the formulas \n\nwhere formula_28 and formula_27 are integers such that formula_157.\n\nNote that all triangles with B = 3A (whether with integer sides or not) satisfy formula_158.\n\nThe only integer triangle with three rational angles (rational numbers of degrees, or equivalently rational fractions of a full turn) is the equilateral triangle. This is because integer sides imply three rational cosines by the law of cosines, and by Niven's theorem a rational cosine coincides with a rational angle if and only if the cosine equals 0, ±1/2, or ±1. The only ones of these giving an angle strictly between 0° and 180° are the cosine value 1/2 with the angle 60°, the cosine value –1/2 with the angle 120°, and the cosine value 0 with the angle 90°. The only combination of three of these, allowing multiple use of any of them and summing to 180°, is three 60° angles.\n\nConditions are known in terms of elliptic curves for an integer triangle to have an integer ratio \"N\" of the circumradius to the inradius. The smallest case, that of the equilateral triangle, has \"N\"=2. In every known case, \"N\" ≡ 2 (mod 8)—that is, \"N\"–2 is divisible by 8.\n\nA 5-Con triangle pair is a pair of triangles that are similar but not congruent and that share three angles and two sidelengths. Primitive integer 5-Con triangles, in which the four distinct integer sides (two sides each appearing in both triangles, and one other side in each triangle) share no prime factor, have triples of sides \n\nfor positive coprime integers \"x\" and \"y\". The smallest example is the pair (8, 12, 18), (12, 18, 27), generated by \"x\" = 2, \"y\" = 3.\n\n\n"}
{"id": "1562189", "url": "https://en.wikipedia.org/wiki?curid=1562189", "title": "Internal validity", "text": "Internal validity\n\nInternal validity is the extent to which a piece of evidence supports a claim about cause and effect, within the context of a particular study. It is one of the most important properties of scientific studies, and is an important concept in reasoning about evidence more generally. Internal validity is determined by how well a study can rule out alternative explanations for its findings (usually, sources of systematic error or 'bias'). It contrasts with external validity, the extent to which results can justify conclusions about other contexts (that is, the extent to which results can be generalized).\n\nInferences are said to possess internal validity if a causal relationship between two variables is properly demonstrated. \nA valid causal inference may be made when three criteria are satisfied: \n\nIn scientific experimental settings, researchers often change the state of one variable (the independent variable) to see what effect it has on a second variable (the dependent variable). For example, a researcher might manipulate the dosage of a particular drug between different groups of people to see what effect it has on health. In this example, the researcher wants to make a causal inference, namely, that different doses of the drug may be \"held responsible\" for observed changes or differences. When the researcher may confidently attribute the observed changes or differences in the dependent variable to the independent variable (that is, when the researcher observes an association between these variables and can rule out other explanations or \"rival hypotheses\"), then the causal inference is said to be internally valid.\n\nIn many cases, however, the size of effects found in the dependent variable may not just depend on \n\nRather, a number of variables or circumstances uncontrolled for (or uncontrollable) may lead to additional or alternative explanations (a) for the effects found and/or (b) for the magnitude of the effects found. Internal validity, therefore, is more a matter of degree than of either-or, and that is exactly why research designs other than true experiments may also yield results with a high degree of internal validity.\n\nIn order to allow for inferences with a high degree of internal validity, precautions may be taken during the design of the study. As a rule of thumb, conclusions based on direct manipulation of the independent variable allow for greater internal validity than conclusions based on an association observed without manipulation. \n\nWhen considering only Internal Validity, highly controlled true experimental designs (i.e. with random selection, random assignment to either the control or experimental groups, reliable instruments, reliable manipulation processes, and safeguards against confounding factors) may be the \"gold standard\" of scientific research. However, the very methods used to increase internal validity may also limit the generalizability or external validity of the findings. For example, studying the behavior of animals in a zoo may make it easier to draw valid causal inferences within that context, but these inferences may not generalize to the behavior of animals in the wild. In general, a typical experiment in a laboratory, studying a particular process, may leave out many variables that normally strongly affect that process in nature.\n\nWhen it is not known which variable changed first, it can be difficult to determine which variable is the cause and which is the effect.\n\nA major threat to the validity of causal inferences is confounding: Changes in the dependent variable may rather be attributed to variations in a third variable which is related to the manipulated variable. Where spurious relationships cannot be ruled out, rival hypotheses to the original causal inference may be developed.\n\nSelection bias refers to the problem that, at pre-test, differences between groups exist that may interact with the independent variable and thus be 'responsible' for the observed outcome. Researchers and participants bring to the experiment a myriad of characteristics, some learned and others inherent. For example, sex, weight, hair, eye, and skin color, personality, mental capabilities, and physical abilities, but also attitudes like motivation or willingness to participate.\n\nDuring the selection step of the research study, if an unequal number of test subjects have similar subject-related variables there is a threat to the internal validity. For example, a researcher created two test groups, the experimental and the control groups. The subjects in both groups are not alike with regard to the independent variable but similar in one or more of the subject-related variables.\n\nSelf-selection also has a negative effect on the interpretive power of the dependent variable. This occurs often in online surveys where individuals of specific demographics opt into the test at higher rates than other demographics.\n\nEvents outside of the study/experiment or between repeated measures of the dependent variable may affect participants' responses to experimental procedures. Often, these are large-scale events (natural disaster, political change, etc.) that affect participants' attitudes and behaviors such that it becomes impossible to determine whether any change on the dependent measures is due to the independent variable, or the historical event.\n\nSubjects change during the course of the experiment or even between measurements. For example, young children might mature and their ability to concentrate may change as they grow up. Both permanent changes, such as physical growth and temporary ones like fatigue, provide \"natural\" alternative explanations; thus, they may change the way a subject would react to the independent variable. So upon completion of the study, the researcher may not be able to determine if the cause of the discrepancy is due to time or the independent variable.\n\nRepeatedly measuring the participants may lead to bias. Participants may remember the correct answers or may be conditioned to know that they are being tested. Repeatedly taking (the same or similar) intelligence tests usually leads to score gains, but instead of concluding that the underlying skills have changed for good, this threat to Internal Validity provides a good rival hypotheses.\n\nThe instrument used during the testing process can change the experiment. This also refers to observers being more concentrated or primed, or having unconsciously changed the criteria they use to make judgments. This can also be an issue with self-report measures given at different times. In this case the impact may be mitigated through the use of retrospective pretesting. If any instrumentation changes occur, the internal validity of the main conclusion is affected, as alternative explanations are readily available.\n\nThis type of error occurs when subjects are selected on the basis of extreme scores (one far away from the mean) during a test. For example, when children with the worst reading scores are selected to participate in a reading course, improvements at the end of the course might be due to regression toward the mean and not the course's effectiveness. If the children had been tested again before the course started, they would likely have obtained better scores anyway.\nLikewise, extreme outliers on individual scores are more likely to be captured in one instance of testing but will likely evolve into a more normal distribution with repeated testing.\n\nThis error occurs if inferences are made on the basis of only those participants that have participated from the start to the end. However, participants may have dropped out of the study before completion, and maybe even due to the study or programme or experiment itself. For example, the percentage of group members having quit smoking at post-test was found much higher in a group having received a quit-smoking training program than in the control group. However, in the experimental group only 60% have completed the program. \nIf this attrition is systematically related to any feature of the study, the administration of the independent variable, the instrumentation, or if dropping out leads to relevant bias between groups, a whole class of alternative explanations is possible that account for the observed differences.\n\nThis occurs when the subject-related variables, color of hair, skin color, etc., and the time-related variables, age, physical size, etc., interact. If a discrepancy between the two groups occurs between the testing, the discrepancy may be due to the age differences in the age categories.\n\nIf treatment effects spread from treatment groups to control groups, a lack of differences between experimental and control groups may be observed. This does not mean, however, that the independent variable has no effect or that there is no relationship between dependent and independent variable.\n\nBehavior in the control groups may alter as a result of the study. For example, control group members may work extra hard to see that expected superiority of the experimental group is not demonstrated. Again, this does not mean that the independent variable produced no effect or that there is no relationship between dependent and independent variable. Vice versa, changes in the dependent variable may only be affected due to a demoralized control group, working less hard or motivated, not due to the independent variable.\n\nExperimenter bias occurs when the individuals who are conducting an experiment inadvertently affect the outcome by non-consciously behaving in different ways to members of control and experimental groups. It is possible to eliminate the possibility of experimenter bias through the use of double blind study designs, in which the experimenter is not aware of the condition to which a participant belongs.\n\nFor eight of these threats there exists the first letter mnemonic \"THIS MESS\", which refers to the first letters of \"T\"esting (repeated testing), \"H\"istory, \"I\"nstrument change, \"S\"tatistical Regression toward the mean, \"M\"aturation, \"E\"xperimental mortality, \"S\"election and \"S\"election Interaction.\n\n\n"}
{"id": "364643", "url": "https://en.wikipedia.org/wiki?curid=364643", "title": "Israel Gelfand", "text": "Israel Gelfand\n\nIsrael Moiseevich Gelfand, also written Israïl Moyseyovich Gel'fand, or Izrail M. Gelfand (, ; – 5 October 2009) was a prominent Soviet mathematician. He made significant contributions to many branches of mathematics, including group theory, representation theory and functional analysis. The recipient of many awards, including the Order of Lenin and the Wolf Prize, he was a Fellow of the Royal Society and professor at Moscow State University and, after immigrating to the United States shortly before his 76th birthday, at Rutgers University.\n\nHis legacy continues through his students, who include Endre Szemerédi, Alexandre Kirillov, Edward Frenkel, Joseph Bernstein, as well as his own son, Sergei Gelfand.\n\nA native of Kherson Governorate of the Russian Empire, Gelfand was born into a Jewish family in the small southern Ukrainian town of Okny. According to his own account, Gelfand was expelled from high school because his father had been a mill owner. Bypassing both high school and college, he proceeded to postgraduate study at the age of 19 at Moscow State University, where his advisor was the preeminent mathematician Andrei Kolmogorov. \n\nGelfand is known for many developments including:\n\nThe Gelfand–Tsetlin (also spelled Zetlin) basis is a widely used tool in theoretical physics and the result of Gelfand's work on the representation theory of the unitary group and Lie groups in general.\n\nGelfand also published works on biology and medicine. For a long time he took an interest in cell biology and organized a research seminar on the subject.\n\nHe worked extensively in mathematics education, particularly with correspondence education. In 1994, he was awarded a MacArthur Fellowship for this work.\n\nGelfand was married to Zorya Shapiro, and their two sons, Sergei and Vladimir both live in the United States. A third son, Aleksandr, died of leukemia. Following the divorce from his first wife, Gelfand married his second wife, Tatiana; together they had a daughter, Tatiana. The family also includes four grandchildren and three great-grandchildren.\nThe memories about I.Gelfand are collected at the special site handled by his family.\n\nGelfand held several honorary degrees and was awarded the Order of Lenin three times for his research. In 1977 he was elected a Foreign Member of the Royal Society. He won the Wolf Prize in 1978, Kyoto Prize in 1989 and MacArthur Foundation Fellowship in 1994. He held the presidency of the Moscow Mathematical Society between 1968 and 1970, and was elected a foreign member of the U.S. National Academy of Science, the American Academy of Arts and Sciences, the Royal Irish Academy, the American Mathematical Society and the London Mathematical Society.\n\nIn an October 2003 article in \"The New York Times\", written on the occasion of his 90th birthday, Gelfand is described as a scholar who is considered \"among the greatest mathematicians of the 20th century\", having exerted a tremendous influence on the field both through his own works and those of his students.\n\nIsrael Gelfand died at the Robert Wood Johnson University Hospital near his home in Highland Park, New Jersey. He was less than five weeks past his 96th birthday. His death was first reported on the blog of his former collaborator Andrei Zelevinsky and confirmed a few hours later by an obituary in the Russian online newspaper \"Polit.ru\".\n\n\n\n"}
{"id": "5717942", "url": "https://en.wikipedia.org/wiki?curid=5717942", "title": "Jack Copeland", "text": "Jack Copeland\n\nBrian Jack Copeland (born 1950) is Professor of Philosophy at the University of Canterbury, Christchurch, New Zealand, and author of books on the computing pioneer Alan Turing.\n\nJack Copeland's education includes a BPhil and a DPhil from the University of Oxford in philosophy, where he undertook research on modal and non-classical logic under the supervision of Dana Scott.\n\nJack Copeland is the Director of the Turing Archive for the History of Computing, an extensive online archive on the computing pioneer Alan Turing. He has also written and edited books on Turing. He is one of the people responsible for identifying the concept of hypercomputation and machines more capable than Turing machines.\n\nCopeland has held visiting professorships at the University of Sydney, Australia (1997, 2002), the University of Aarhus, Denmark (1999), the University of Melbourne, Australia (2002, 2003), and the University of Portsmouth, United Kingdom (1997–2005). In 2000, he was a Senior Fellow in the Dibner Institute for the History of Science and Technology at the Massachusetts Institute of Technology, United States.\n\nCopeland is also President of the US Society for Machines and Mentality and a member of the UK Bletchley Park Trust Heritage Advisory Panel. He is the founding editor of \"The Rutherford Journal\", established in 2005.\n\nCopeland was awarded Lecturer of the Year 2010 by the University of Canterbury's student union.\n\nJack Copeland is editor-in-chief of The Rutherford Journal, a peer-reviewed online academic journal published in New Zealand that covers the history and philosophy of science and technology. The journal is published as needed and was established in December 2005 by Copeland. The full text of articles is freely available online in HTML format.\n\n\"The Rutherford Journal\" is named after the chemist and physicist Ernest Rutherford (1871–1937), a New Zealander, who studied for a BA at the Canterbury College (Christchurch) in 1890.\n\nThe journal is indexed as an open access scholarly resource and journal and in various index lists. It was listed in an article on electronic journals in the Journal for the Association of History and Computing and included in the \"Isis Current Bibliography of the History of Science and Its Cultural Influences\". The journal features technology as diverse as totalisators and the CSIRAC computer.\n\n\n"}
{"id": "3625459", "url": "https://en.wikipedia.org/wiki?curid=3625459", "title": "Jack Edmonds", "text": "Jack Edmonds\n\nJack R. Edmonds (born April 5, 1934) is an American computer scientist, regarded as one of the most important contributors to the field of combinatorial optimization.\n\nA breakthrough contribution of Edmonds is the Cobham–Edmonds thesis, defining the concept of polynomial time characterising the difference between a practical and an impractical algorithm (in modern terms, a tractable problem or intractable problem). Today, problems solvable in polynomial time are called the complexity class PTIME, or simply P. Another of Edmonds' earliest and most notable contributions is the blossom algorithm for constructing maximum matchings on graphs, discovered in 1961 and published in 1965. This was the first polynomial-time algorithm for maximum matching in graphs. Its generalization to weighted graphs was a conceptual breakthrough in the use of linear programming ideas in combinatorial optimization. It sealed in the importance of there being proofs, or \"witnesses\", that the answer for an instance is yes and there being proofs, or \"witnesses\", that the answer for an instance is no. In this blossom algorithm paper, Edmonds also characterizes feasible problems as those solvable in polynomial time; this is one of the origins of the Cobham–Edmonds thesis.\n\nAdditional landmark work of Edmonds is in the area of matroids. He found a polyhedral description for all spanning trees of a graph, and more generally for all independent sets of a matroid. Building on this, as a novel application of linear programming to discrete mathematics, he proved the matroid intersection theorem, a very general combinatorial min-max theorem which, in modern terms, showed that the matroid intersection problem lay in both NP and co-NP.\n\nEdmonds is well known for his theorems on max-weight branching algorithms and packing edge-disjoint branchings and his work with Richard Karp on faster flow algorithms. The Edmonds–Gallai decomposition theorem describes finite graphs from the point of view of matchings. He introduced polymatroids, submodular flows with Richard Giles, and the terms clutter and blocker in the study of hypergraphs. A recurring theme in his work is to seek algorithms whose time complexity is polynomially bounded by their input size and bit-complexity.\n\nEdmonds graduated with a baccalaureate degree from George Washington University in 1958, and obtained a master's degree from the University of Maryland in 1959, with a thesis on the problem of embedding graphs\ninto surfaces.\n\nFrom 1959 to 1969 he worked at the National Institute of Standards and Technology (then the National Bureau of Standards), and was a founding member of Alan Goldman’s newly created Operations Research Section in 1961.\n\nFrom 1969 on, with the exception of 1991-1993, he held a faculty position at the Department of Combinatorics and Optimization at the University of Waterloo's Faculty of Mathematics. He supervised the doctoral work of a dozen students in this time.\n\nFrom 1991 to 1993, he was involved in a dispute (\"the Edmonds affair\") with the University of Waterloo, wherein the university claimed that a letter submitted constituted a letter of resignation, which Edmonds denied. The conflict was resolved in 1993, and he returned to the university.\n\nEdmonds retired in 1999. The fifth Aussois Workshop on Combinatorial Optimization in 2001 was dedicated to him.\n\nJack's son Jeff Edmonds is a professor of computer science at York University, and his wife Kathie Cameron is a professor of mathematics at Laurier University.\n\n\n"}
{"id": "337876", "url": "https://en.wikipedia.org/wiki?curid=337876", "title": "List of calculus topics", "text": "List of calculus topics\n\nThis is a list of calculus topics.\n\n\nDeriviablity & continuty\n\n\n\n\n\n\"See also [[list of numerical analysis topics]]\"\n\n\n\n\n\n\nFor further developments: see [[list of real analysis topics]], [[list of complex analysis topics]], [[list of multivariable calculus topics]].\n\n[[Category:Mathematics-related lists|Calculus]]\n[[Category:Calculus| ]]\n[[Category:Lists of topics|Calculus]]"}
{"id": "39664285", "url": "https://en.wikipedia.org/wiki?curid=39664285", "title": "List of things named after Évariste Galois", "text": "List of things named after Évariste Galois\n\nThese are things named after Évariste Galois (1811–1832), a French mathematician.\n"}
{"id": "5989279", "url": "https://en.wikipedia.org/wiki?curid=5989279", "title": "Logical machine", "text": "Logical machine\n\nA logical machine is a tool containing a set of parts that uses energy to perform formal logic operations. Early logical machines were mechanical devices that performed basic operations in Boolean logic. Contemporary logical machines are computer-based electronic programs that perform proof assistance with theorems in mathematical logic. In the 21st century, these proof assistant programs have given birth to a new field of study called mathematical knowledge management.\n\nThe earliest logical machines were mechanical constructs built in the late 19th century. William Stanley Jevons invented the first logical machine in 1869, the logic piano. In 1883, Allan Marquand invented a new logical machine that performed the same operations as Jevons' logic piano but with improvements in design simplification, portability, and input-output controls.\n\n\n"}
{"id": "42628154", "url": "https://en.wikipedia.org/wiki?curid=42628154", "title": "Master stability function", "text": "Master stability function\n\nIn mathematics, the master stability function is a tool used to analyse the stability of the synchronous state in a dynamical system consisting of many identical oscillators which are coupled together, such as the Kuramoto model. \n\nThe setting is as follows. Consider a system with formula_1 identical oscillators. Without the coupling, they evolve according to the same differential equation, say formula_2 where formula_3 denotes the state of oscillator formula_4. A synchronous state of the system of oscillators is where all the oscillators are in the same state.\n\nThe coupling is defined by a coupling strength formula_5, a matrix formula_6 which describes how the oscillators are coupled together, and a function formula_7 of the state of a single oscillator. Including the coupling leads to the following equation:\nIt is assumed that the row sums formula_9 vanish so that the manifold of synchronous states is neutrally stable.\n\nThe master stability function is now defined as the function which maps the complex number formula_10 to the greatest Lyapunov exponent of the equation\nThe synchronous state of the system of coupled oscillators is stable if the master stability function is negative at formula_12 where formula_13 ranges over the eigenvalues of the coupling matrix formula_14.\n\n"}
{"id": "18376786", "url": "https://en.wikipedia.org/wiki?curid=18376786", "title": "Mathematical Society of Japan", "text": "Mathematical Society of Japan\n\nThe Mathematical Society of Japan (MSJ, ) was the first academic society in Japan.\n\nIn 1877, the organization was established as the \"Tokyo Sugaku Kaisha.\" It was re-organized and re-established in its present form in 1946.\n\nToday, the MSJ has more than 5,000 members. They have the opportunity to participate in programs at MSJ meetings which take place in spring and autumn each year. They also have the opportunity to announce their own research at these meetings.\n\nIn the context of its 50th anniversary celebrations, the Mathematical Society of Japan established the Takebe Prizes for the encouragement of those who show promise as mathematicians. The award is named after Edo period mathematician who is also known as Takebe Kenkō.\n\n\n\n"}
{"id": "3351916", "url": "https://en.wikipedia.org/wiki?curid=3351916", "title": "Maximum satisfiability problem", "text": "Maximum satisfiability problem\n\nIn computational complexity theory, the maximum satisfiability problem (MAX-SAT) is the problem of determining the maximum number of clauses, of a given Boolean formula in conjunctive normal form, that can be made true by an assignment of truth values to the variables of the formula. It is a generalization of the Boolean satisfiability problem, which asks whether there exists a truth assignment that makes all clauses true.\n\nThe conjunctive normal form formula\nis not satisfiable: no matter which truth values are assigned to its two variables, at least one of its four clauses will be false.\nHowever, it is possible to assign truth values in such a way as to make three out of four clauses true; indeed, every truth assignment will do this.\nTherefore, if this formula is given as an instance of the MAX-SAT problem, the solution to the problem is the number three.\n\nThe MAX-SAT problem is NP-hard, since its solution easily leads to the solution of the boolean satisfiability problem, which is NP-complete.\n\nIt is also difficult to find an approximate solution of the problem, that satisfies a number of clauses within a guaranteed approximation ratio of the optimal solution. More precisely, the problem is APX-complete, and thus does not admit a polynomial-time approximation scheme unless P = NP.\n\nMore generally, one can define a weighted version of MAX-SAT as follows: given a conjunctive normal form formula with non-negative weights assigned to each clause, find truth values for its variables that maximize the combined weight of the satisfied clauses. The MAX-SAT problem is an instance of weighted MAX-SAT where all weights are 1.\n\nRandomly assigning each variable to be true with probability 1/2 gives an expected 2-approximation. More precisely, if each clause has at least variables, then this yields a (1 − 2)-approximation. This algorithm can be derandomized using the method of conditional probabilities.\n\nMAX-SAT can also be expressed using an integer linear program (ILP). Fix a conjunctive normal form formula with variables , , ..., , and let denote the clauses of . For each clause in , let and denote the sets of variables which are not negated in , and those that are negated in , respectively. The variables of the ILP will correspond to the variables of the formula , whereas the variables will correspond to the clauses. The ILP is as follows: \nThe above program can be relaxed to the following linear program :\n\nThe following algorithm using that relaxation is an expected (1-1/e)-approximation:\n\nThis algorithm can also be derandomized using the method of conditional probabilities.\n\nThe 1/2-approximation algorithm does better when clauses are large whereas the (1-1/)-approximation does better when clauses are small. They can be combined as follows:\n\nThis is a deterministic factor (3/4)-approximation.\n\nOn the formula\n\nwhere formula_3, the (1-1/)-approximation will set each variable to True with probability 1/2, and so will behave identically to the 1/2-approximation. Assuming that the assignment of is chosen first during derandomization, the derandomized algorithms will pick a solution with total weight formula_4, whereas the optimal solution has weight formula_5.\n\nMany exact solvers for MAX-SAT have been developed during recent years, and many of them were presented in the well-known conference on the boolean satisfiability problem and related problems, the SAT Conference. In 2006 the SAT Conference hosted the first MAX-SAT evaluation comparing performance of practical solvers for MAX-SAT, as it has done in the past for the pseudo-boolean satisfiability problem and the quantified boolean formula problem.\nBecause of its NP-hardness, large-size MAX-SAT instances cannot in general be solved exactly, and one must often resort to approximation algorithms\nand heuristics \n\nThere are several solvers submitted to the last Max-SAT Evaluations:\n\nMAX-SAT is one of the optimization extensions of the boolean satisfiability problem, which is the problem of determining whether the variables of a given Boolean formula can be assigned in such a way as to make the formula evaluate to TRUE. If the clauses are restricted to have at most 2 literals, as in 2-satisfiability, we get the MAX-2SAT problem. If they are restricted to at most 3 literals per clause, as in 3-satisfiability, we get the MAX-3SAT problem.\n\nThere are many problems related to the satisfiability of conjunctive normal form Boolean formulas.\n\n\n\n"}
{"id": "41024203", "url": "https://en.wikipedia.org/wiki?curid=41024203", "title": "Near polygon", "text": "Near polygon\n\nIn mathematics, a near polygon is an incidence geometry introduced by Ernest E. Shult and Arthur Yanushka in 1980. Shult and Yanushka showed the connection between the so-called tetrahedrally closed line-systems in Euclidean spaces and a class of point-line geometries which they called near polygons. These structures generalise the notion of generalized polygon as every generalized 2\"n\"-gon is a near 2\"n\"-gon of a particular kind. Near polygons were extensively studied and connection between them and dual polar spaces was shown in 1980s and early 1990s. Some sporadic simple groups, for example the Hall-Janko group and the Mathieu groups, act as automorphism groups of near polygons.\n\nA near 2\"d\"-gon is an incidence structure (formula_1), where formula_2 is the set of points, formula_3 is the set of lines and formula_4 is the incidence relation, such that:\n\nNote that the distance are measure in the collinearity graph of points, i.e., the graph formed by taking points as vertices and joining a pair of vertices if they are incident with a common line. \nWe can also give an alternate graph theoretic definition, a near 2\"d\"-gon is a connected graph of finite diameter \"d\" with the property that for every vertex \"x\" and every maximal clique \"M\" there exists a unique vertex \"x\"' in \"M\" nearest to \"x\". \nThe maximal cliques of such a graph correspond to the lines in the incidence structure definition. \nA near 0-gon (\"d\" = 0) is a single point while a near 2-gon (\"d\" = 1) is just a single line, i.e., a complete graph. A near quadrangle (\"d\" = 2) is same as a (possibly degenerate) generalized quadrangle. In fact, it can be shown that every generalized 2\"d\"-gon is a near 2\"d\"-gon that satisfies the following two additional conditions: \n\nA near polygon is called dense if every line is incident with at least three points and if every two points at distance two have at least two common neighbours. It is said to have order (\"s\", \"t\") if every line is incident with precisely \"s\" + 1 points and every point is incident with precisely \"t\" + 1 lines. Dense near polygons have a rich theory and several classes of them (like the slim dense near polygons) have been completely classified.\n\n\nA finite near formula_9-gon S is called regular if it has an order formula_10 and if there exist constants formula_11, such that for every two points formula_5 and formula_13 at distance formula_14, there are precisely formula_15 lines through formula_13 containing a (necessarily unique) point at distance formula_17 from formula_5. It turns out that regular near formula_9-gons are precisely those near formula_9-gons whose point graph (also known as a collinearity graph) is a distance-regular graph. A generalized formula_9-gon of order formula_22 is a regular near formula_9-gon with parameters formula_24\n\n\n"}
{"id": "15015787", "url": "https://en.wikipedia.org/wiki?curid=15015787", "title": "Optimality criterion", "text": "Optimality criterion\n\nIn statistics, an optimality criterion provides a measure of the fit of the data to a given hypothesis, to aid in model selection. A model is designated as the \"best\" of the candidate models if it gives the best value of an objective function measuring the degree of satisfaction of the criterion used to evaluate the alternative hypotheses.\n\nThe term has been used to identify the different criteria that are used to evaluate a phylogenetic tree. For example, in order to determine the best topology between two phylogenetic trees using the maximum likelihood optimality criterion, one would calculate the maximum likelihood score of each tree and choose the one that had the better score. However, different optimality criteria can select different hypotheses. In such circumstances caution should be exercised when making strong conclusions.\n\nMany other disciplines use similar criteria or have specific measures geared toward the objectives of the field. Optimality criteria include maximum likelihood, Bayesian, maximum parsimony, sum of squared residuals, least absolute deviations, and many others.\n"}
{"id": "58006825", "url": "https://en.wikipedia.org/wiki?curid=58006825", "title": "P-recursive equation", "text": "P-recursive equation\n\nIn mathematics a P-recursive equation is a linear equation of sequences where the coefficient sequences can be represented as polynomials. P-recursive equations are linear recurrence equations (or linear recurrence relations or linear difference equations) with polynomial coefficients. These equations play an important role in different areas of mathematics, specifically in combinatorics. The sequences which are solutions of these equations are called holonomic, P-recursive or D-finite.\n\nFrom the late 1980s on the first algorithms were developed to find solutions for these equations. Sergei A. Abramov, Marko Petkovšek and Mark van Hoeij described algorithms to find polynomial, rational, hypergeometric and d'Alembertian solutions.\n\nLet formula_1 be a field of characteristic zero (for example formula_2), formula_3 polynomials for formula_4,formula_5 a sequence and formula_6 an unknown sequence. The equationformula_7is called a linear recurrence equation with polynomial coefficients (all recurrence equations in this article are of this form). If formula_8 and formula_9 are both nonzero, then formula_10 is called the order of the equation. If formula_11 is zero the equation is called homogeneous, otherwise it is called inhomogeneous.\n\nThis can also be written as formula_12 where formula_13 is a linear recurrence operator with polynomial coefficients and formula_14 is the shift operator, i.e. formula_15.\n\nLet formula_16 or equivalently formula_17 be a recurrence equation with polynomial coefficients. There exist several algorithms which compute solutions of this equation. These algorithms can compute polynomial, rational, hypergeometric and d'Alembertian solutions. The solution of a homogeneous equation is given by the kernel of the linear recurrence operator: formula_18. As a subspace of the space of sequences this kernel has a basis. Let formula_19 be a basis of formula_20, then the formal sum formula_21 for arbitrary constants formula_22 is called the general solution of the homogeneous problem formula_23. If formula_24 is a particular solution of formula_17, i.e. formula_26, then formula_27 is also a solution of the inhomogeneous problem and it is called the general solution of the inhomogeneous problem.\n\nIn the late 1980s Sergei A. Abramov described an algorithm which finds the general polynomial solution of a recurrence equation, i.e. formula_28, with a polynomial right-hand sideformula_29. He (and a few years later Marko Petkovšek) gave a degree bound for polynomial solutions. This way the problem can simply be solved by considering a system of linear equations. In 1995 Abramov, Bronstein and Petkovšek showed that the polynomial case can be solved more efficiently by considering power series solution of the recurrence equation in a specific power basis (i.e. not the ordinary basis formula_30).\n\nThe other algorithms for finding more general solutions (e.g. rational or hypergeometric solutions) also rely on algorithms which compute polynomial solutions.\n\nIn 1989 Sergei A. Abramov showed that a general rational solution, i.e. formula_31, with polynomial right-hand side formula_32, can be found by using the notion of a universal denominator. A universal denominator is a polynomial formula_33 such that the denominator of every rational solution divides formula_33. Abramov showed how this universal denominator can be computed by only using the first and the last coefficient polynomial formula_8 and formula_9. Substituting this universal denominator for the unknown denominator of formula_37 all rational solutions can be found by computing all polynomial solutions of a transformed equation.\n\nA sequence formula_38 is called hypergeometric if the ratio of two consecutive terms is a rational function in formula_39, i.e. formula_40. This is the case if and only if the sequence is the solution of a first-order recurrence equation with polynomial coefficients. The set of hypergeometric sequences is not a subspace of the space of sequences as it is not closed under addition.\n\nIn 1992 Marko Petkovšek gave an algorithm to get the general hypergeometric solution of a recurrence equation where the right-hand side formula_41 is the sum of hypergeometric sequences. The algorithm makes use of the Gosper-Petkovšek normal-form of a rational function. With this specific representation it is again sufficient to consider polynomial solutions of a transformed equation.\n\nA different and more efficient approach is due to Mark van Hoeij. Considering the roots of the first and the last coefficient polynomial formula_8 and formula_9 – called singularities – one can build a solution step by step making use of the fact that every hypergeometric sequence formula_44 has a representation of the formformula_45for some formula_46 with formula_47 for formula_48 and formula_49. Here formula_50 denotes the Gamma function and formula_51 the algebraic closure of the field formula_1. Then the formula_53 have to be singularities of the equation (i.e. roots of formula_8 or formula_9). Furthermore one can compute bounds for the exponents formula_56. For fixed values formula_57 it is possible to make an ansatz which gives candidates for formula_58. For a specific formula_58 one can again make an ansatz to get the rational function formula_60 by Abramov's algorithm. Considering all possibilities one gets the general solution of the recurrence equation.\n\nA sequence formula_37 is called d'Alembertian if formula_62 for some hypergeometric sequences formula_63 and formula_64 means that formula_65 where formula_66 denotes the difference operator, i.e. formula_67. This is the case if and only if there are first-order linear recurrence operators formula_68 with rational coefficients such that formula_69.\n\n1994 Abramov and Petkovšek described an algorithm which computes the general d'Alembertian solution of a recurrence equation. This algorithm computes hypergeometric solutions and reduces the order of the recurrence equation recursively.\n\nThe number of signed permutation matrices of size formula_70 can be described by the sequence formula_71. A signed permutation matrix is a square matrix which has exactly one nonzero entry in every row and in every column. The nonzero entries can be formula_72. The sequence is determined by the linear recurrence equation with polynomial coefficientsformula_73and the initial values formula_74. Applying an algorithm to find hypergeometric solutions one can find the general hypergeometric solutionformula_75for some constant formula_76. Also considering the initial values, the sequence formula_77 describes the number of signed permutation matrices.\n\nThe number of involutions formula_38 of a set with formula_79 elements is given by the recurrence equationformula_80Applying for example Petkovšek's algorithm it is possible to see that there is no polynomial, rational or hypergeometric solution for this recurrence equation.\n\nA function formula_81 is called hypergeometric if formula_82 where formula_83 denotes the rational functions in formula_79 and formula_85. A hypergeometric sum is a finite sum of the form formula_86 where formula_81 is hypergeometric. Zeilberger's creative telescoping algorithm can transform such a hypergeometric sum into a recurrence equation with polynomial coefficients. This equation can then be solved to get for example a linear combination of hypergeometric solutions which is called a closed form solution of formula_11.\n"}
{"id": "502897", "url": "https://en.wikipedia.org/wiki?curid=502897", "title": "Remainder", "text": "Remainder\n\nIn mathematics, the remainder is the amount \"left over\" after performing some computation. In arithmetic, the remainder is the integer \"left over\" after dividing one integer by another to produce an integer quotient (integer division). In algebra, the remainder is the polynomial \"left over\" after dividing one polynomial by another. The \"modulo operation\" is the operation that produces such a remainder when given a dividend and divisor.\n\nFormally it is also true that a remainder is what is left after subtracting one number from another, although this is more precisely called the \"difference\". This usage can be found in some elementary textbooks; colloquially it is replaced by the expression \"the rest\" as in \"Give me two dollars back and keep the rest.\" However, the term \"remainder\" is still used in this sense when a function is approximated by a series expansion and the error expression (\"the rest\") is referred to as the remainder term.\n\nIf \"a\" and \"d\" are integers, with \"d\" non-zero, it can be proven that there exist unique integers \"q\" and \"r\", such that \"a\" = \"qd\" + \"r\" and 0 ≤ \"r\" < \"|d|\". The number \"q\" is called the \"quotient\", while \"r\" is called the \"remainder\".\n\nSee Euclidean division for a proof of this result and division algorithm for algorithms describing how to calculate the remainder.\n\nThe remainder, as defined above, is called the \"least positive remainder\" or simply the \"remainder\". The integer \"a\" is either a multiple of \"d\" or lies in the interval between consecutive multiples of \"d\", namely, \"q⋅d\" and (\"q\" + 1)\"d\" (for positive \"q\").\n\nAt times it is convenient to carry out the division so that \"a\" is as close as possible to an integral multiple of \"d\", that is, we can write \nIn this case, \"s\" is called the \"least absolute remainder\". As with the quotient and remainder, \"k\" and \"s\" are uniquely determined except in the case where \"d\" = 2\"n\" and \"s\" = ± \"n\". For this exception we have,\nA unique remainder can be obtained in this case by some convention such as always taking the positive value of \"s\".\n\nIn the division of 43 by 5 we have:\nso 3 is the least positive remainder. We also have,\nand −2 is the least absolute remainder.\n\nThese definitions are also valid if \"d\" is negative, for example, in the division of 43 by −5,\n\nand 3 is the least positive remainder, while,\n\nand −2 is the least absolute remainder.\n\nIn the division of 42 by 5 we have:\nand since 2 < 5/2, 2 is both the least positive remainder and the least absolute remainder.\n\nIn these examples, the (negative) least absolute remainder is obtained from the least positive remainder by subtracting 5, which is \"d\". This holds in general. When dividing by \"d\", either both remainders are positive and therefore equal, or they have opposite signs. If the positive remainder is \"r\", and the negative one is \"r\", then\n\nWhen \"a\" and \"d\" are floating-point numbers, with \"d\" non-zero, \"a\" can be divided by \"d\" without remainder, with the quotient being another floating-point number. If the quotient is constrained to being an integer, however, the concept of remainder is still necessary. It can be proved that there exists a unique integer quotient \"q\" and a unique floating-point remainder \"r\" such that \"a\" = \"qd\" + \"r\" with 0 ≤ \"r\" < |\"d\"|.\n\nExtending the definition of remainder for floating-point numbers as described above is not of theoretical importance in mathematics; however, many programming languages implement this definition, see modulo operation.\n\nWhile there are no difficulties inherent in the definitions, there are implementation issues that arise when negative numbers are involved in calculating remainders. Different programming languages have adopted different conventions:\n\n\n\n\n\nEuclidean division of polynomials is very similar to Euclidean division of integers and leads to polynomial remainders. Its existence is based on the following theorem: Given two univariate polynomials \"a\"(\"x\") and \"b\"(\"x\") (with \"b\"(\"x\") not the zero polynomial) defined over a field (in particular, the reals or complex numbers), there exist two polynomials \"q\"(\"x\") (the \"quotient\") and \"r\"(\"x\") (the \"remainder\") which satisfy:\nwhere\nwhere \"deg(...)\" denotes the degree of the polynomial (the degree of the constant polynomial whose value is always 0 is defined to be negative, so that this degree condition will always be valid when this is the remainder.) Moreover, \"q\"(\"x\") and \"r\"(\"x\") are uniquely determined by these relations.\n\nThis differs from the Euclidean division of integers in that, for the integers, the degree condition is replaced by the bounds on the remainder \"r\" (non-negative and less than the divisor, which insures that \"r\" is unique.) The similarity of Euclidean division for integers and also for polynomials leads one to ask for the most general algebraic setting in which Euclidean division is valid. The rings for which such a theorem exists are called Euclidean domains, but in this generality uniqueness of the quotient and remainder are not guaranteed.\n\nPolynomial division leads to a result known as the Remainder theorem: If a polynomial \"f\"(\"x\") is divided by \"x\" − \"k\", the remainder is the constant \"r\" = \"f\"(\"k\").\n\n\n"}
{"id": "1631010", "url": "https://en.wikipedia.org/wiki?curid=1631010", "title": "Risk-adjusted return on capital", "text": "Risk-adjusted return on capital\n\nRisk-adjusted return on capital (RAROC) is a risk-based profitability measurement framework for analysing risk-adjusted financial performance and providing a consistent view of profitability across businesses. The concept was developed by Bankers Trust and principal designer Dan Borge in the late 1970s. Note, however, that increasingly return on risk adjusted capital (RORAC) is used as a measure, whereby the risk adjustment of Capital is based on the capital adequacy guidelines as outlined by the Basel Committee, currently Basel III.\n\nBroadly speaking, in business enterprises, risk is traded off against benefit. RAROC is defined as the ratio of risk adjusted return to economic capital. The economic capital is the amount of money which is needed to secure the survival in a worst-case scenario, it is a buffer against unexpected shocks in market values. Economic capital is a function of market risk, credit risk, and operational risk, and is often calculated by VaR. This use of capital based on risk improves the capital allocation across different functional areas of banks, insurance companies, or any business in which capital is placed at risk for an expected return above the risk-free rate.\n\nRAROC system allocates capital for two basic reasons:\n\nFor risk management purposes, the main goal of allocating capital to individual business units is to determine the bank's optimal capital structure—that is economic capital allocation is closely correlated with individual business risk. As a performance evaluation tool, it allows banks to assign capital to business units based on the economic value added of each unit.\n\nWith the financial crisis of 2007, and the introduction of Dodd–Frank Act, and Basel III, minimum required regulatory capital requirements have become onerous. An implication of stringent regulatory capital requirements spurred debates on the validity of required economic capital in managing an organization’s portfolio composition, highlighting that constraining requirements should have organizations focus entirely on the return on regulatory capital in measuring profitability and in guiding portfolio composition. The counterargument highlights that concentration and diversification effects should play a prominent role in portfolio selection – dynamics recognized in economic capital, but not regulatory capital. \n\nIt did not take long for the industry to recognize the relevance and importance of both regulatory and economic measures, and eschewed focusing exclusively on one or the other. Relatively simple rules were devised to have both regulatory and economic capital enter into the process. In 2012, researchers at Moody’s Analytics designed a formal extension to the RORAC model that accounts for regulatory capital requirements as well as economic risks. In the framework, capital allocation can be represented as a composite capital measure (CCM) that is a weighted combination of economic and regulatory capital – with the weight on regulatory capital determined by the degree to which an organization is capital constrained.\n\n\n"}
{"id": "2058995", "url": "https://en.wikipedia.org/wiki?curid=2058995", "title": "Scientific community metaphor", "text": "Scientific community metaphor\n\nIn computer science, the scientific community metaphor is a metaphor used to aid understanding scientific communities. The first publications on the scientific community metaphor in 1981 and 1982 involved the development of a programming language named Ether that invoked procedural plans to process goals and assertions concurrently by dynamically creating new rules during program execution. Ether also addressed issues of conflict and contradiction with multiple sources of knowledge and multiple viewpoints.\n\nThe scientific community metaphor builds on the philosophy, history and sociology of science. It was originally developed building on work in the philosophy of science by Karl Popper and Imre Lakatos. In particular, it initially made use of Lakatos' work on proofs and refutations. Subsequently, development has been influenced by the work of Geof Bowker, Michel Callon, Paul Feyerabend, Elihu M. Gerson, Bruno Latour, John Law, Karl Popper, Susan Leigh Star, Anselm Strauss, and Lucy Suchman.\n\nIn particular Latour's \"Science in Action\" had great influence. In the book, Janus figures make paradoxical statements about scientific development. An important challenge for the scientific community metaphor is to reconcile these paradoxical statements.\n\nScientific research depends critically on monotonicity, concurrency, commutativity, and pluralism to propose, modify, support, and oppose scientific methods, practices, and theories. \nQuoting from Carl Hewitt, scientific community metaphor systems have characteristics of \"monotonicity\", \"concurrency\", \"commutativity\", \"pluralism\", \"skepticism\" and \"provenance\".\n\nThe above characteristics are limited in real scientific communities. Publications are sometimes lost or difficult to retrieve. Concurrency is limited by resources including personnel and funding. Sometimes it is easier to rederive a result than to look it up. Scientists only have so much time and energy to read and try to understand the literature. Scientific fads sometimes sweep up almost everyone in a field. The order in which information is received can influence how it is processed. Sponsors can try to control scientific activities. In Ether the semantics of the kinds of activity described in this paragraph are governed by the actor model.\n\nScientific research includes generating theories and processes for modifying, supporting, and opposing these theories. Karl Popper called the process \"conjectures and refutations\", which although expressing a core insight, has been shown to be too restrictive a characterization by the work of Michel Callon, Paul Feyerabend, Elihu M. Gerson, Mark Johnson, Thomas Kuhn, George Lakoff, Imre Lakatos, Bruno Latour, John Law, Susan Leigh Star, Anselm Strauss, Lucy Suchman, Ludwig Wittgenstein, \"etc.\". Three basic kinds of participation in Ether are proposing, supporting, and opposing. Scientific communities are structured to support competition as well as cooperation.\n\nThese activities affect the adherence to approaches, theories, methods, \"etc.\" in scientific communities. Current adherence does not imply adherence for all future time. Later developments will modify and extend current understandings. Adherence is a local rather than a global phenomenon. No one speaks for the scientific community as a whole.\n\nOpposing ideas may coexist in communities for centuries. On rare occasions a community reaches a \"breakthrough\" that clearly decides an issue previously muddled.\n\nEther used \"viewpoints\" to relativist information in publications. However a great deal of information is shared across viewpoints. So Ether made use of \"inheritance\" so that information in a viewpoint could be readily used in other viewpoints. Sometimes this inheritance is not exact as when the laws of physics in Newtonian mechanics are derived from those of Special Relativity. In such cases Ether used \"translation\" instead of inheritance. Bruno Latour has analyzed translation in scientific communities in the context of actor network theory. Imre Lakatos studied very sophisticated kinds of translations of mathematical (\"e.g.\", the Euler formula for polyhedra) and scientific theories.\n\nViewpoints were used to implement natural deduction (Fitch [1952]) in Ether. In order to prove a goal of the form (P \"implies\" Q) in a viewpoint V, it is sufficient to create a new viewpoint V' that inherits from V, assert P in V', and then prove Q in V'. An idea like this was originally introduced into programming language proving by Rulifson, Derksen, and Waldinger [1973] except since Ether is concurrent rather than being sequential it does not rely on being in a single viewpoint that can be sequentially pushed and popped to move to other viewpoints.\n\nUltimately resolving issues among these viewpoints are matters for negotiation (as studied in the sociology and philosophy of science by Geof Bowker, Michel Callon, Paul Feyerabend, Elihu M. Gerson, Bruno Latour, John Law, Karl Popper, Susan Leigh Star, Anselm Strauss, Lucy Suchman, etc.).\n\nAlan Turing was one of the first to attempt to more precisely characterize \"individual\" intelligence through the notion of his famous Turing Test. This paradigm was developed and deepened in the field of Artificial Intelligence. Allen Newell and Herbert A. Simon did pioneer work in analyzing the protocols of individual human problem solving behavior on puzzles. More recently Marvin Minsky has developed the idea that the mind of an individual human is composed of a society of agents in Society of Mind (see the analysis by Push Singh).\n\nThe above research on individual human problem solving is \"complementary\" to the scientific community metaphor.\n\nSome developments in hardware and software technology for the Internet are being applied in light of the scientific community metaphor.\n\nLegal concerns (\"e.g.\", HIPAA, Sarbanes-Oxley, \"The Books and Records Rules\" in SEC Rule 17a-3/4 and \"Design Criteria Standard for Electronic Records Management Software Applications\" in DOD 5015.2 in the U.S.) are leading organizations to store information monotonically forever. It has just now become less costly in many cases to store information on magnetic disk than on tape. With increasing storage capacity, sites can monotonically record what they read from the Internet as well as monotonically recording their own operations.\n\nSearch engines currently provide rudimentary access to all this information. Future systems will provide interactive question answering broadly conceived that will make all this information much more useful.\n\nMassive concurrency (\"i.e.,\" Web services and multi-core computer architectures) lies in the future posing enormous challenges and opportunities for the scientific community metaphor. In particular, the scientific community metaphor is being used in client cloud computing.\n\n\n"}
{"id": "32457161", "url": "https://en.wikipedia.org/wiki?curid=32457161", "title": "Shelling (topology)", "text": "Shelling (topology)\n\nIn mathematics, a shelling of a simplicial complex is a way of gluing it together from its maximal simplices (simplices that are not a face of another simplex) in a well-behaved way. A complex admitting a shelling is called shellable.\n\nA \"d\"-dimensional simplicial complex is called pure if its maximal simplices all have dimension \"d\". Let formula_1 be a finite or countably infinite simplicial complex. An ordering formula_2 of the maximal simplices of formula_1 is a shelling if the complex formula_4 is pure and formula_5-dimensional for all formula_6. That is, the \"new\" simplex formula_7 meets the previous simplices along some union formula_8 of top-dimensional simplices of the boundary of formula_7. If formula_8 is the entire boundary of formula_7 then formula_7 is called spanning.\n\nFor formula_1 not necessarily countable, one can define a shelling as a well-ordering of the maximal simplices of formula_1 having analogous properties.\n\n\n\n"}
{"id": "2481457", "url": "https://en.wikipedia.org/wiki?curid=2481457", "title": "Singly and doubly even", "text": "Singly and doubly even\n\nIn mathematics an even integer, that is, a number that is divisible by 2, is called evenly even or doubly even if it is a multiple of 4, and oddly even or singly even if it is not. (The former names are traditional ones, derived from the ancient Greek; the latter have become common in recent decades.)\n\nThese names reflect a basic concept in number theory, the 2-order of an integer: how many times the integer can be divided by 2. This is equivalent to the multiplicity of 2 in the prime factorization.\nA singly even number can be divided by 2 only once; it is even but its quotient by 2 is odd.\nA doubly even number is an integer that is divisible more than once by 2; it is even and its quotient by 2 is also even.\n\nThe separate consideration of oddly and evenly even numbers is useful in many parts of mathematics, especially in number theory, combinatorics, coding theory (see even codes), among others.\n\nThe ancient Greek terms \"even-times-even\" and \"even-times-odd\" were given various inequivalent definitions by Euclid and later writers such as Nicomachus. Today, there is a standard development of the concepts. The 2-order or 2-adic order is simply a special case of the \"p\"-adic order at a general prime number \"p\"; see \"p\"-adic number for more on this broad area of mathematics. Many of the following definitions generalize directly to other primes.\n\nFor an integer \"n\", the 2-order of \"n\" (also called \"valuation\") is the largest natural number ν such that 2 divides \"n\". This definition applies to positive and negative numbers \"n\", although some authors restrict it to positive \"n\"; and one may define the 2-order of 0 to be infinity (see also parity of zero). The 2-order of \"n\" is written ν(\"n\") or ord(\"n\"). It is not to be confused with the multiplicative order modulo 2.\n\nThe 2-order provides a unified description of various classes of integers defined by evenness:\n\nOne can also extend the 2-order to the rational numbers by defining ν(\"q\") to be the unique integer ν where\nand \"a\" and \"b\" are both odd. For example, half-integers have a negative 2-order, namely −1. Finally, by defining the 2-adic norm,\none is well on the way to constructing the 2-adic numbers.\n\nThe object of the game of darts is to reach a score of 0, so the player with the smaller score is in a better position to win. At the beginning of a leg, \"smaller\" has the usual meaning of absolute value, and the basic strategy is to aim at high-value areas on the dartboard and score as many points as possible. At the end of a leg, since one needs to double out to win, the 2-adic norm becomes the relevant measure. With any odd score no matter how small in absolute value, it takes at least two darts to win. Any even score between 2 and 40 can be satisfied with a single dart, and 40 is a much more desirable score than 2, due to the effects of missing.\n\nA common miss when aiming at the double ring is to hit a single instead and accidentally halve one's score. Given a score of 22 — a singly even number — one has a game shot for double 11. If one hits single 11, the new score is 11, which is odd, and it will take at least two further darts to recover. By contrast, when shooting for double 12, one may make the same mistake but still have 3 game shots in a row: D12, D6, and D3. Generally, with a score of , one has such game shots. This is why is such a desirable score: it splits 5 times.\n\nThe classic proof that the square root of 2 is irrational operates by infinite descent. Usually, the descent part of the proof is abstracted away by assuming (or proving) the existence of irreducible representations of rational numbers. An alternate approach is to exploit the existence of the ν operator.\n\nAssume by contradiction that\n\nwhere \"a\" and \"b\" are non-zero natural numbers. Square both sides of the equality and apply\nthe 2-order valuation operator ν to :\n\nSince 2-order valuations are integers, the difference cannot be equal to the rational formula_8. By contradiction, therefore, is not a rational. \n\nMore concretely, since the valuation of 2\"b\" is odd, while valuation of \"a\" is even, they must be distinct integers, so that formula_9. An easy calculation then yields a lower bound of formula_10 for the difference formula_11, yielding a direct proof of irrationality not relying on the law of excluded middle.\n\nIn geometric topology, many properties of manifolds depend only on their dimension mod 4 or mod 8; thus one often studies manifolds of singly even and doubly even dimension (4\"k\"+2 and 4\"k\") as classes. For example, doubly even-dimensional manifolds have a \"symmetric\" nondegenerate bilinear form on their middle-dimension cohomology group, which thus has an integer-valued signature. Conversely, singly even-dimensional manifolds have a \"skew\"-symmetric nondegenerate bilinear form on their middle dimension; if one defines a quadratic refinement of this to a quadratic form (as on a framed manifold), one obtains the Arf invariant as a mod 2 invariant. Odd-dimensional manifolds, by contrast, do not have these invariants, though in algebraic surgery theory one may define more complicated invariants. This 4-fold and 8-fold periodicity in the structure of manifolds is related to the 4-fold periodicity of L-theory and the 8-fold periodicity of real topological K-theory, which is known as Bott periodicity – note further that real K-theory is 4-fold periodic away from 2.\n\nIf a compact oriented smooth spin manifold has dimension , or exactly, then its signature is an integer multiple of 16.\n\nA singly even number cannot be a powerful number. It cannot be represented as a difference of two squares. However, a singly even number can be represented as the difference of two pronic numbers or of two powerful numbers.\n\nIn group theory, it is relatively simple to show that the order of a nonabelian finite simple group cannot be a singly even number. In fact, by the Feit–Thompson theorem, it cannot be odd either, so every such group has doubly even order.\n\nLambert's continued fraction for the tangent function gives the following continued fraction involving the positive singly even numbers:\n\nThis expression leads to similar representations of.\n\nIn organic chemistry, Hückel's rule, also known as the 4n + 2 rule, predicts that a cyclic π-bond system containing a singly even number of p electrons will be aromatic.\n\nAlthough the 2-order can detect when an integer is congruent to 0 (mod 4) or 2 (mod 4), it cannot tell the difference between 1 (mod 4) or 3 (mod 4). This distinction has some interesting consequences, such as Fermat's theorem on sums of two squares.\n\n\n"}
{"id": "10639143", "url": "https://en.wikipedia.org/wiki?curid=10639143", "title": "Symbol of a differential operator", "text": "Symbol of a differential operator\n\nIn mathematics, the symbol of a linear differential operator is obtained from a differential operator of a polynomial by, roughly speaking, replacing each partial derivative by a new variable. The symbol of a differential operator has broad applications to Fourier analysis. In particular, in this connection it leads to the notion of a pseudo-differential operator. The highest-order terms of the symbol, known as the principal symbol, almost completely controls the qualitative behavior of solutions of a partial differential equation. Linear elliptic partial differential equations can be characterized as those whose principal symbol is nowhere zero. In the study of hyperbolic and parabolic partial differential equations, zeros of the principal symbol correspond to the characteristics of the partial differential equation. Consequently, the symbol is often fundamental for the solution of such equations, and is one of the main computational devices used to study their singularities.\n\nLet \"P\" be a linear differential operator of order \"k\" on the Euclidean space R. Then \"P\" is a polynomial in the derivative \"D\", which in multi-index notation can be written\nThe total symbol of \"P\" is the polynomial \"p\":\n\nThe leading symbol, also known as the principal symbol, is the highest-degree component of \"p\" :\n\nand is of importance later because it is the only part of the symbol that transforms as a tensor under changes to the coordinate system.\n\nThe symbol of \"P\" appears naturally in connection with the Fourier transform as follows. Let ƒ be a Schwartz function. Then by the inverse Fourier transform,\n\nThis exhibits \"P\" as a Fourier multiplier. A more general class of functions \"p\"(\"x\",ξ) which satisfy at most polynomial growth conditions in ξ under which this integral is well-behaved comprises the pseudo-differential operators.\n\nLet \"E\" and \"F\" be vector bundles over a closed manifold \"X\", and suppose\n\nis a differential operator of order formula_6. In local coordinates on \"X\", we have \n\nwhere, for each multi-index α, formula_8 is a bundle map, symmetric on the indices α.\n\nThe \"k\" order coefficients of \"P\" transform as a symmetric tensor\n\nfrom the tensor product of the \"k\" symmetric power of the cotangent bundle of \"X\" with \"E\" to \"F\". This symmetric tensor is known as the principal symbol (or just the symbol) of \"P\".\n\nThe coordinate system \"x\" permits a local trivialization of the cotangent bundle by the coordinate differentials d\"x\", which determine fiber coordinates ξ. In terms of a basis of frames \"e\", \"f\" of \"E\" and \"F\", respectively, the differential operator \"P\" decomposes into components\n\non each section \"u\" of \"E\". Here \"P\" is the scalar differential operator defined by\n\nWith this trivialization, the principal symbol can now be written\n\nIn the cotangent space over a fixed point \"x\" of \"X\", the symbol formula_13 defines a homogeneous polynomial of degree \"k\" in formula_14 with values in formula_15. \n\nThe differential operator formula_16 is elliptic if its symbol is invertible; that is for each nonzero formula_17 the bundle map formula_18 is invertible. On a compact manifold, it follows from the elliptic theory that \"P\" is a Fredholm operator: it has finite-dimensional kernel and cokernel.\n\n\n"}
{"id": "683561", "url": "https://en.wikipedia.org/wiki?curid=683561", "title": "Total variation", "text": "Total variation\n\nIn mathematics, the total variation identifies several slightly different concepts, related to the (local or global) structure of the codomain of a function or a measure. For a real-valued continuous function \"f\", defined on an interval [\"a\", \"b\"] ⊂ ℝ, its total variation on the interval of definition is a measure of the one-dimensional arclength of the curve with parametric equation \"x\" ↦ \"f\"(\"x\"), for \"x\" ∈ [\"a\", \"b\"].\n\nThe concept of total variation for functions of one real variable was first introduced by Camille Jordan in the paper . He used the new concept in order to prove a convergence theorem for Fourier series of discontinuous periodic functions whose variation is bounded. The extension of the concept to functions of more than one variable however is not simple for various reasons.\n\n The total variation of a real-valued (or more generally complex-valued) function \"formula_1\", defined on an interval formula_2 is the quantity\n\nwhere the supremum runs over the set of all partitions formula_4 of the given interval.\n\n Let Ω be an open subset of ℝ. Given a function \"f\" belonging to \"L\"(Ω), the total variation of \"f\" in Ω is defined as\n\nwhere formula_6 is the set of continuously differentiable vector functions of compact support contained in formula_7, and formula_8 is the essential supremum norm. Note that this definition \"does not require\" that the domain formula_9 of the given function be a bounded set.\n\nFollowing , consider a signed measure \"formula_10\" on a measurable space formula_11: then it is possible to define two set functions formula_12 and formula_13, respectively called upper variation and lower variation, as follows\n\nclearly\n\nand its total variation is defined as the value of this measure on the whole space of definition, i.e.\n\n uses upper and lower variations to prove the Hahn–Jordan decomposition: according to his version of this theorem, the upper and lower variation are respectively a non-negative and a non-positive measure. Using a more modern notation, define\n\nThen \"formula_22\" and \"formula_23\" are two non-negative measures such that\n\nThe last measure is sometimes called, by abuse of notation, total variation measure.\n\nIf the measure \"formula_10\" is complex-valued i.e. is a complex measure, its upper and lower variation cannot be defined and the Hahn–Jordan decomposition theorem can only be applied to its real and imaginary parts. However, it is possible to follow and define the total variation of the complex-valued measure \"formula_10\" as follows\n\nwhere the supremum is taken over all partitions \"formula_30\" of a measurable set \"formula_31\" into a countable number of disjoint measurable subsets.\n\nThis definition coincides with the above definition \"formula_25\" for the case of real-valued signed measures.\n\nThe variation so defined is a positive measure (see ) and coincides with the one defined by when \"formula_10\" is a signed measure: its total variation is defined as above. This definition works also if \"formula_10\" is a vector measure: the variation is then defined by the following formula\n\nwhere the supremum is as above. Note also that this definition is slightly more general than the one given by since it requires only to consider \"finite partitions\" of the space \"formula_36\": this implies that it can be used also to define the total variation on finite-additive measures.\n\nThe total variation of any probability measure is exactly one, therefore it is not interesting as a means of investigating the properties of such measures. However, when μ and ν are probability measures, the total variation distance of probability measures can be defined as \"formula_37\" where the norm is the total variation norm of signed measures. Using the property that \"formula_38\", we eventually arrive at the equivalent definition\n\nand its values are non-trivial. The factor formula_40 above is usually dropped (as is the convention in the article total variation distance of probability measures). Informally, this is the largest possible difference between the probabilities that the two probability distributions can assign to the same event. For a categorical distribution it is possible to write the total variation distance as follows\n\nIt may also be normalized to values in formula_42 by halving the previous definition as follows\n\nThe total variation of a formula_44 function formula_1 can be expressed as an integral involving the given function instead of as the supremum of the functionals of definitions and .\n\n The total variation of a differentiable function \"formula_1\", defined on an interval formula_2, has the following expression if \"formula_48\" is Riemann integrable\n\n Given a formula_44 function formula_1 defined on a bounded open set formula_9, with formula_53 of class formula_54, the total variation of formula_1 has the following expression\n\nHere formula_57 denotes the formula_58-norm.\n\nThe first step in the proof is to first prove an equality which follows from the Gauss–Ostrogradsky theorem.\n\nUnder the conditions of the theorem, the following equality holds:\n\nFrom the Gauss–Ostrogradsky theorem:\nby substituting formula_61, we have:\n\nwhere formula_63 is zero on the border of formula_7 by definition:\n\nUnder the conditions of the theorem, from the lemma we have:\nin the last part formula_71 could be omitted, because by definition its essential supremum is at most one.\n\nOn the other hand we consider formula_72 and formula_73 which is the up to formula_74 approximation of formula_75 in formula_76 with the same integral. We can do this since formula_76 is dense in formula_78. Now again substituting into the lemma:\n\nThis means we have a convergent sequence of formula_80 that tends to formula_81 as well as we know that formula_82. q.e.d.\n\nIt can be seen from the proof that the supremum is attained when\n\nThe function \"formula_1\" is said to be of bounded variation precisely if its total variation is finite.\n\nThe total variation is a norm defined on the space of measures of bounded variation. The space of measures on a σ-algebra of sets is a Banach space, called the ca space, relative to this norm. It is contained in the larger Banach space, called the ba space, consisting of \"finitely additive\" (as opposed to countably additive) measures, also with the same norm. The distance function associated to the norm gives rise to the total variation distance between two measures \"μ\" and \"ν\".\n\nFor finite measures on ℝ, the link between the total variation of a measure \"μ\" and the total variation of a function, as described above, goes as follows. Given \"μ\", define a function formula_85 by\nThen, the total variation of the signed measure \"μ\" is equal to the total variation, in the above sense, of the function \"φ\". In general, the total variation of a signed measure can be defined using Jordan's decomposition theorem by\nfor any signed measure \"μ\" on a measurable space formula_11.\n\nTotal variation can be seen as a non-negative real-valued functional defined on the space of real-valued functions (for the case of functions of one variable) or on the space of integrable functions (for the case of functions of several variables). As a functional, total variation finds applications in several branches of mathematics and engineering, like optimal control, numerical analysis, and calculus of variations, where the solution to a certain problem has to minimize its value. As an example, use of the total variation functional is common in the following two kind of problems\n\n\n\n\n\nOne variable\n\nOne and more variables\n\nMeasure theory\n\n\n"}
{"id": "14223173", "url": "https://en.wikipedia.org/wiki?curid=14223173", "title": "Wiener index", "text": "Wiener index\n\nIn chemical graph theory, the Wiener index (also Wiener number) introduced by Harry Wiener, is a topological index of a molecule, defined as the sum of the lengths of the shortest paths between all pairs of vertices in the chemical graph representing the non-hydrogen atoms in the molecule.\n\nThe Wiener index is named after Harry Wiener, who introduced it in 1947; at the time, Wiener called it the \"path number\". It is the oldest topological index related to molecular branching. Based on its success, many other topological indexes of chemical graphs, based on information in the distance matrix of the graph, have been developed subsequently to Wiener's work.\nThe same quantity has also been studied in pure mathematics, under various names including the gross status, the distance of a graph, and the transmission. The Wiener index is also closely related to the closeness centrality of a vertex in a graph, a quantity inversely proportional to the sum of all distances between the given vertex and all other vertices that has been frequently used in sociometry and the theory of social networks.\n\nButane (CH) has two different structural isomers: \"n\"-butane, with a linear structure of four carbon atoms, and isobutane, with a branched structure. The chemical graph for \"n\"-butane is a four-vertex path graph, and the chemical graph for isobutane is a tree with one central vertex connected to three leaves.\n\nThe \"n\"-butane molecule has three pairs of vertices at distance one from each other, two pairs at distance two, and one pair at distance three, so its Wiener index is\nThe isobutane molecule has three pairs of vertices at distances one from each other (the three leaf-center pairs), and three pairs at distance two (the leaf-leaf pairs). Therefore, its Wiener index is\nThese numbers are instances of formulas for special cases of the Wiener index: it is formula_3 for any formula_4-vertex path graph such as the graph of \"n\"-butane, and formula_5 for any formula_4-vertex star such as the graph of isobutane.\n\nThus, even though these two molecules have the same chemical formula, and the same numbers of carbon-carbon and carbon-hydrogen bonds, their different structures give rise to different Wiener indices.\n\nWiener showed that the Wiener index number is closely correlated with the boiling points of alkane molecules. Later work on quantitative structure–activity relationships showed that it is also correlated with other quantities including the parameters of its critical point, the density, surface tension, and viscosity of its liquid phase, and the van der Waals surface area of the molecule.\n\nThe Wiener index may be calculated directly using an algorithm for computing all pairwise distances in the graph. When the graph is unweighted (so the length of a path is just its number of edges), these distances may be calculated by repeating a breadth-first search algorithm, once for each starting vertex. The total time for this approach is O(\"nm\"), where \"n\" is the number of vertices in the graph and \"m\" is its number of edges.\n\nFor weighted graphs, one may instead use the Floyd–Warshall algorithm or Johnson's algorithm, with running time O(\"n\") or O(\"nm\" + \"n\" log \"n\") respectively. Alternative but less efficient algorithms based on repeated matrix multiplication have also been developed within the chemical informatics literature.\n\nWhen the underlying graph is a tree (as is true for instance for the alkanes originally studied by Wiener), the Wiener index may be calculated more efficiently. If the graph is partitioned into two subtrees by removing a single edge \"e\", then its Wiener index is the sum of the Wiener indices of the two subtrees, together with a third term representing the paths that pass through \"e\". This third term may be calculated in linear time by computing the sum of distances of all vertices from \"e\" within each subtree and multiplying the two sums. This divide and conquer algorithm can be generalized from trees to graphs of bounded treewidth, and leads to near-linear-time algorithms for such graphs.\nAn alternative method for calculating the Wiener index of a tree, by Bojan Mohar and Tomaž Pisanski, works by generalizing the problem to graphs with weighted vertices, where the weight of a path is the product of its length with the weights of its two endpoints. If \"v\" is a leaf vertex of the tree then the Wiener index of the tree may be calculated by merging \"v\" with its parent (adding their weights together), computing the index of the resulting smaller tree, and adding a simple correction term for the paths that pass through the edge from \"v\" to its parent. By repeatedly removing leaves in this way, the Wiener index may be calculated in linear time.\n\nFor graphs that are constructed as products of simpler graphs, the Wiener index of the product graph can often be computed by a simple formula that combines the indices of its factors. Benzenoids (graphs formed by gluing regular hexagons edge-to-edge) can be embedded isometrically into the Cartesian product of three trees, allowing their Wiener indices to be computed in linear time by using the product formula together with the linear time tree algorithm.\n\n considered the problem of determining which numbers can be represented as the Wiener index of a graph. They showed that all but two positive integers have such a representation; the two exceptions are the numbers 2 and 5, which are not the Wiener index of any graph. For graphs that must be bipartite, they found that again almost all integers can be represented, with a larger set of exceptions: none of the numbers in the set\ncan be represented as the Wiener index of a bipartite graph.\n\nGutman and Yeh conjectured, but were unable to prove, a similar description of the numbers that can be represented as Wiener indices of trees, with a set of 49 exceptional values:\nThe conjecture was later proven by Wagner, Wang, and Yu.\n"}
{"id": "8910528", "url": "https://en.wikipedia.org/wiki?curid=8910528", "title": "Zonal polynomial", "text": "Zonal polynomial\n\nIn mathematics, a zonal polynomial is a multivariate symmetric homogeneous polynomial. The zonal polynomials form a basis of the space of symmetric polynomials.\n\nThey appear as zonal spherical functions of the Gelfand pairs\nformula_1 (here, formula_2 is the hyperoctahedral group) and formula_3, which means that they describe canonical basis of the double class\nalgebras formula_4 and formula_5.\n\nThey are applied in multivariate statistics.\n\nThe zonal polynomials are the formula_6 case of the C normalization of the Jack function.\n\n"}
