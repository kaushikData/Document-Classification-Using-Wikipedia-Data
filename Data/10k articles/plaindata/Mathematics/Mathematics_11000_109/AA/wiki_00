{"id": "208155", "url": "https://en.wikipedia.org/wiki?curid=208155", "title": "12 (number)", "text": "12 (number)\n\n12 (twelve) is the natural number following 11 and preceding 13. The product of the first three factorials, twelve is a superior highly composite number, divisible by 2, 3, 4, and 6. It is central to many systems of counting, including the Western calendar and units of time, and frequently appears in the Abrahamic religions.\n\nTwelve is the largest number with a single-syllable name in English. Early Germanic numbers have been theorized to have been non-decimal: evidence includes the unusual phrasing of eleven and twelve, the former use of \"hundred\" to refer to groups of 120, and the presence of glosses such as \"tentywise\" or \"ten-count\" in medieval texts showing that writers could not presume their readers would normally understand them that way. Such uses gradually disappeared with the introduction of Arabic numerals during the 12th-century Renaissance.\n\nIt derives from the Old English ' and ', first attested in the 10th-century Lindisfarne Gospels' Book of John. It has cognates in every Germanic language (e.g. German \"zwölf\"), whose Proto-Germanic ancestor has been reconstructed as ', from ' (\"two\") and suffix ' or ' of uncertain meaning. It is sometimes compared with the Lithuanian ', although ' is used as the suffix for all numbers from 11 to 19 (analogous to \"-teen\"). Every other Indo-European language instead uses a form of \"two\"+\"ten\", such as the Latin \"\". The usual ordinal form is \"twelfth\" but \"dozenth\" or \"duodecimal\" (from the Latin word) is also used in some contexts, particularly base-12 numeration. Similarly, a group of twelve things is usually a \"dozen\" but may also be referred to as a \"duodecad\". The adjective referring to a group of twelve is \"duodecuple\".\n\nAs with eleven, the earliest forms of twelve are often considered to be connected with Proto-Germanic ' or ' (\"to leave\"), with the implicit meaning that \"two is left\" after having already counted to ten. The Lithuanian suffix is also considered to share a similar development. The suffix \" has also been connected with reconstructions of the Proto-Germanic for ten.\n\nWhile, as mentioned above, 12 has its own name in Germanic languages such as English, German, and Swedish. It is a compound number in many other languages, e.g. Italian \"dodici\" (but in Spanish and Portuguese, 16, \nand in French, 17 is the first compound number), Japanese 十二 \"jūni\".\n\nIn prose writing, \"twelve\", being the last single-syllable numeral, is sometimes taken as the last number to be written as a word, and 13 the first to be written using digits.\nThis is not a binding rule, and in English language tradition, it is sometimes recommended to spell out numbers up to and including either \"nine\", \"ten\" or \"twelve\", or even \"ninety-nine\" or \"one hundred\". Another system spells out all numbers written in one or two words (\"sixteen\", \"twenty-seven\", \"fifteen thousand\", but \"372\" or \"15,001\").\nIn German orthography, there used to be the widely followed (but unofficial) rule of spelling out numbers up to twelve (\"zwölf\"). The Duden (the German standard dictionary) mentions this rule as outdated.\n\nTwelve is a composite number, the smallest number with exactly six divisors, its divisors being 1, 2, 3, 4, 6 and 12. Twelve is also a highly composite number, the next one being twenty-four.\n\nTwelve is the smallest abundant number, since it is the smallest integer for which the sum of its proper divisors (1 + 2 + 3 + 4 + 6 = 16) is greater than itself. Twelve is a sublime number, a number that has a perfect number of divisors, and the sum of its divisors is also a perfect number. Since there is a subset of 12's proper divisors that add up to 12 (all of them but with 4 excluded), 12 is a semiperfect number.\n\nIf an odd perfect number is of the form 12\"k\" + 1, it has at least twelve distinct prime factors.\n\nA twelve-sided polygon is a dodecagon. A twelve-faced polyhedron is a dodecahedron. Regular cubes and octahedrons both have 12 edges, while regular icosahedrons have 12 vertices. Twelve is a pentagonal number. The densest three-dimensional lattice sphere packing has each sphere touching 12 others, and this is almost certainly true for \"any\" arrangement of spheres (the Kepler conjecture). Twelve is also the kissing number in three dimensions.\n\nTwelve is the smallest weight for which a cusp form exists. This cusp form is the discriminant Δ(\"q\") whose Fourier coefficients are given by the Ramanujan -function and which is (up to a constant multiplier) the 24th power of the Dedekind eta function. This fact is related to a constellation of interesting appearances of the number twelve in mathematics ranging from the value of the Riemann zeta function at −1 i.e. ζ(−1) = −, the fact that the abelianization of SL(2,Z) has twelve elements, and even the properties of lattice polygons.\n\nThere are twelve Jacobian elliptic functions and twelve cubic distance-transitive graphs.\n\nThere are 12 Latin squares of size 3 × 3.\n\nThe duodecimal system (12 [twelve] = 10), which is the use of 12 as a division factor for many ancient and medieval weights and measures, including hours, probably originates from Mesopotamia.\n\nIn base thirteen and higher bases (such as hexadecimal), twelve is represented as C. In base 10, the number 12 is a Harshad number.\n\nThe number twelve carries religious, mythological and magical symbolism, generally representing perfection, entirety, or cosmic order in traditions since antiquity. Notably, twelve is the number of full lunations in a solar year, and hence the number of months in a solar calendar, as well as the number of signs in the Western and the Chinese zodiac. \n\n\nIshmael - the first-born son of Abraham - has 12 sons/princes (Genesis 25:16), and Jacob also has 12 sons, who are the progenitors of the Twelve Tribes of Israel. This is reflected in Christian tradition, notably in the twelve Apostles. When Judas Iscariot is disgraced, a meeting is held (Acts) to add Saint Matthias to complete the number twelve once more.\nThe Book of Revelation contains much numerical symbolism, and many of the numbers mentioned have 12 as a divisor. mentions a woman—interpreted as the people of Israel, the Church and the Virgin Mary—wearing a crown of twelve stars (representing each of the twelve tribes of Israel). Furthermore, there are 12,000 people sealed from each of the twelve tribes of Israel, making a total of 144,000 (which is the square of 12 multiplied by a thousand).\n\n\nIn English Common Law, the tradition of twelve jurors harks back to the 10th-century law code introduced by Aethelred the Unready.\n\n\nIn the King Arthur Legend, Arthur is said to subdue 12 rebel princes and to win 12 great battles against Saxon invaders\n\nIn Twelver Shi'a Islam, there are twelve Imams, legitimate successors of the Islamic prophet, Muhammad. These twelve early leaders of Islam are—Ali, Hasan, Husayn, and nine of Husayn's descendants.\nSura 12 in the Quran is sura Yusuf, narrating the story of the sons of Jacob.\n\n\n\n\n\nMovies with the number twelve or its variations in their titles include \n\n\n\n\n\n\n\n\n\n\n"}
{"id": "779196", "url": "https://en.wikipedia.org/wiki?curid=779196", "title": "Alternating finite automaton", "text": "Alternating finite automaton\n\nIn automata theory, an alternating finite automaton (AFA) is a nondeterministic finite automaton whose transitions are divided into \"existential\" and \"universal\" transitions. For example, let \"A\" be an alternating automaton.\n\nNote that due to the universal quantification a run is represented by a run \"tree\". \"A\" accepts a word \"w\", if there \"exists\" a run tree on \"w\" such that \"every\" path ends in an accepting state.\n\nA basic theorem states that any AFA is equivalent to a deterministic finite automaton (DFA), hence AFAs accept exactly the regular languages.\n\nAn alternative model which is frequently used is the one where Boolean combinations are represented as \"clauses\". For instance, one could assume the combinations to be in disjunctive normal form so that formula_7 would represent formula_8. The state tt (\"true\") is represented by formula_9 in this case and ff (\"false\") by formula_10.\nThis clause representation is usually more efficient.\n\nAn alternating finite automaton (AFA) is a 6-tuple,\nformula_11, where\n\n\nThe model was introduced by Chandra, Kozen and Stockmeyer.\n\nEven though AFA can accept exactly the regular languages, they are different from other types of finite automata in the succinctness of description, measured by the number of their states.\n\nChandra et al. proved that converting an formula_23-state AFA to an equivalent DFA\nrequires formula_24 states in the worst case. Another construction by Fellah, Jürgensen and Yu. converts an AFA with formula_23 states to a nondeterministic finite automaton (NFA) with up to formula_26 states by performing a similar kind of powerset construction as used for the transformation of an NFA to a DFA.\n\nThe membership problem asks, given an AFA formula_27 and a word formula_28, whether formula_27 accepts formula_28. This problem is P-complete. This is true even on a singleton alphabet, i.e., when the automaton accepts a unary language.\n\nThe non-emptiness problem (is the language of an input AFA non-empty?), the universality problem (is the complement of the language of an input AFA empty?), and the equivalence problem (do two input AFAs recognize the same language) are PSPACE-complete for AFAs.\n"}
{"id": "39130672", "url": "https://en.wikipedia.org/wiki?curid=39130672", "title": "Artificial precision", "text": "Artificial precision\n\nIn numerical mathematics, artificial precision is a source of error that occurs when a numerical value or semantic is expressed with more precision than was initially provided from measurement or user input.\nFor example, a person enters their birthday as the date 1984-01-01 but it is stored in a database as 1984-01-01T00:00:00Z which introduces the artificial precision of the hour, minute, and second they were born, and may even affect the date, depending on the user's actual place of birth. This is also an example of false precision, which is artificial precision specifically of numerical quantities or measures.\n\n"}
{"id": "23683488", "url": "https://en.wikipedia.org/wiki?curid=23683488", "title": "Aurifeuillean factorization", "text": "Aurifeuillean factorization\n\nIn number theory, an aurifeuillean factorization, or aurifeuillian factorization, named after Léon-François-Antoine Aurifeuille, is a special type of algebraic factorization that comes from non-trivial factorizations of cyclotomic polynomials over the integers. Although cyclotomic polynomials themselves are irreducible over the integers, when restricted to particular integer values they may have an algebraic factorization, as in the examples below.\n\n\n\n\n\nIn 1871, Aurifeuille discovered the factorization of formula_1 for \"k\" = 14 as the following:\n\nThe second factor is prime, and the factorization of the first factor is formula_29. The general form of the factorization was later discovered by Lucas.\n\n"}
{"id": "23396410", "url": "https://en.wikipedia.org/wiki?curid=23396410", "title": "Bull graph", "text": "Bull graph\n\nIn the mathematical field of graph theory, the bull graph is a planar undirected graph with 5 vertices and 5 edges, in the form of a triangle with two disjoint pendant edges.\n\nIt has chromatic number 3, chromatic index 3, radius 2, diameter 3 and girth 3. It is also a block graph, a split graph, an interval graph, a claw-free graph, a 1-vertex-connected graph and a 1-edge-connected graph.\n\nA graph is bull-free if it has no bull as an induced subgraph. The triangle-free graphs are bull-free graphs, since every bull contains a triangle. The strong perfect graph theorem was proven for bull-free graphs long before its proof for general graphs, and a polynomial time recognition algorithm for Bull-free perfect graphs is known.\n\nMaria Chudnovsky and Shmuel Safra have studied bull-free graphs more generally, showing that any such graph must have either a large clique or a large independent set (that is, the Erdős–Hajnal conjecture holds for the bull graph), and developing a general structure theory for these graphs.\n\nThe chromatic polynomial of the bull graph is formula_1. Two other graphs are chromatically equivalent to the bull graph.\n\nIts characteristic polynomial is formula_2.\n\nIts Tutte polynomial is formula_3.\n"}
{"id": "4695115", "url": "https://en.wikipedia.org/wiki?curid=4695115", "title": "Cameron Gordon (mathematician)", "text": "Cameron Gordon (mathematician)\n\nCameron Gordon (born 1945) is a Professor and Sid W. Richardson Foundation Regents Chair in the Department of mathematics at the University of Texas at Austin, known for his work in knot theory. Among his notable results is his work with Marc Culler, John Luecke, and Peter Shalen on the cyclic surgery theorem. This was an important ingredient in his work with Luecke showing that knots were determined by their complement. Gordon was also involved in the resolution of the Smith conjecture.\n\nAndrew Casson and Gordon defined and proved basic theorems regarding strongly irreducible Heegaard splittings, an important concept in the modernization of Heegaard splitting theory. They also worked on the slice-ribbon conjecture, inventing the Casson-Gordon invariants in the process.\n\nGordon was a 1999 Guggenheim Fellow.\nIn 2005 Gordon was elected a Corresponding Fellow of the Royal Society of Edinburgh.\n\n"}
{"id": "706295", "url": "https://en.wikipedia.org/wiki?curid=706295", "title": "Canonical commutation relation", "text": "Canonical commutation relation\n\nIn quantum mechanics (physics), the canonical commutation relation is the fundamental relation between canonical conjugate quantities (quantities which are related by definition such that one is the Fourier transform of another). For example,\n\nbetween the position operator and momentum operator in the direction of a point particle in one dimension, where is the commutator of and , is the imaginary unit, and is the reduced Planck's constant . In general, position and momentum are vectors of operators and their commutation relation between different components of position and momentum can be expressed as\n\nwhere formula_3 is the Kronecker delta.\n\nThis relation is attributed to Max Born (1925), who called it a \"quantum condition\" serving as a postulate of the theory; it was noted by E. Kennard (1927) to imply the Heisenberg uncertainty principle. The Stone–von Neumann theorem gives a uniqueness result for operators satisfying (an exponentiated form of) the canonical commutation relation.\n\nBy contrast, in classical physics, all observables commute and the commutator would be zero. However, an analogous relation exists, which is obtained by replacing the commutator with the Poisson bracket multiplied by :\n\nThis observation led Dirac to propose that the quantum counterparts , of classical observables , satisfy\n\nIn 1946, Hip Groenewold demonstrated that a \"general systematic correspondence\" between quantum commutators and Poisson brackets could not hold consistently. However, he did appreciate that such a systematic correspondence does, in fact, exist between the quantum commutator and a \"deformation\" of the Poisson bracket, the Moyal bracket, and, in general, quantum operators and classical observables and distributions in phase space. He thus finally elucidated the correspondence mechanism, the Wigner–Weyl transform, that underlies an alternate equivalent mathematical representation of quantum mechanics known as deformation quantization.\n\nThe group formula_6 generated by exponentiation of the 3-dimensional Lie algebra determined by the commutation relation formula_7 is called the Heisenberg group. This group can be realized as the group of formula_8 upper triangular matrices with ones on the diagonal.\n\nAccording to the standard mathematical formulation of quantum mechanics, quantum observables such as formula_9 and formula_10 should be represented as self-adjoint operators on some Hilbert space. It is relatively easy to see that two operators satisfying the above canonical commutation relations cannot both be bounded. Certainly, if formula_9 and formula_10 were trace class operators, the relation formula_13 gives a nonzero number on the right and zero on the left.\n\nAlternately, if formula_9 and formula_10 were bounded operators, note that formula_16, hence the operator norms would satisfy \nHowever, can be arbitrarily large, so at least one operator cannot be bounded, and the dimension of the underlying Hilbert space cannot be finite. If the operators satisfy the Weyl relations (an exponentiated version of the canonical commutation relations, described below) then as a consequence of the Stone–von Neumann theorem, \"both\" operators must be unbounded.\n\nStill, these canonical commutation relations can be rendered somewhat \"tamer\" by writing them in terms of the (bounded) unitary operators formula_19 and formula_20. The resulting braiding relations for these operators are the so-called Weyl relations \nThese relations may be thought of as an exponentiated version of the canonical commutation relations; they reflect that translations in position and translations in momentum do not commute. One can easily reformulate the Weyl relations in terms of the representations of the Heisenberg group.\n\nThe uniqueness of the canonical commutation relations—in the form of the Weyl relations—is then guaranteed by the Stone–von Neumann theorem.\n\nIt is important to note that for technical reasons, the Weyl relations are not strictly equivalent to the canonical commutation relation formula_7. If formula_9 and formula_10 were bounded operators, then a special case of the Baker–Campbell–Hausdorff formula would allow one to \"exponentiate\" the canonical commutation relations to the Weyl relations. Since, as we have noted, any operators satisfying the canonical commutation relations must be unbounded, the Baker–Campbell–Hausdorff formula does not apply without additional domain assumptions. Indeed, counterexamples exist satisfying the canonical commutation relations but not the Weyl relations. (These same operators give a counterexample to the naive form of the uncertainty principle.) These technical issues are the reason that the Stone–von Neumann theorem is formulated in terms of the Weyl relations.\n\nA discrete version of the Weyl relations, in which the parameters \"s\" and \"t\" range over formula_25, can be realized on a finite-dimensional Hilbert space by means of the .\n\nThe simple formula\n\nvalid for the quantization of the simplest classical system, can be generalized to the case of an arbitrary Lagrangian formula_27. We identify canonical coordinates (such as in the example above, or a field in the case of quantum field theory) and canonical momenta (in the example above it is , or more generally, some functions involving the derivatives of the canonical coordinates with respect to time):\n\nThis definition of the canonical momentum ensures that one of the Euler–Lagrange equations has the form\n\nThe canonical commutation relations then amount to\n\nwhere is the Kronecker delta.\n\nFurther, it can be easily shown that\n\nUsing formula_32, it can be easily shown that by mathematical induction\" \"\n\nCanonical quantization is applied, by definition, on canonical coordinates. However, in the presence of an electromagnetic field, the canonical momentum is not gauge invariant. The correct gauge-invariant momentum (or \"kinetic momentum\") is\n\nwhere is the particle's electric charge, is the vector potential, and is the speed of light. Although the quantity is the \"physical momentum\", in that it is the quantity to be identified with momentum in laboratory experiments, it \"does not\" satisfy the canonical commutation relations; only the canonical momentum does that. This can be seen as follows.\n\nThe non-relativistic Hamiltonian for a quantized charged particle of mass in a classical electromagnetic field is (in cgs units)\n\nwhere is the three-vector potential and is the scalar potential. This form of the Hamiltonian, as well as the Schrödinger equation , the Maxwell equations and the Lorentz force law are invariant under the gauge transformation\nwhere\n\nand \"Λ=Λ(x,t)\" is the gauge function.\n\nThe angular momentum operator is\nand obeys the canonical quantization relations \ndefining the Lie algebra for so(3), where formula_44 is the Levi-Civita symbol. Under gauge transformations, the angular momentum transforms as\n\nThe gauge-invariant angular momentum (or \"kinetic angular momentum\") is given by\n\nwhich has the commutation relations\nwhere\nis the magnetic field. The inequivalence of these two formulations shows up in the Zeeman effect and the Aharonov–Bohm effect.\n\nAll such nontrivial commutation relations for pairs of operators lead to corresponding uncertainty relations, involving positive semi-definite expectation contributions by their respective commutators and anticommutators. In general, for two Hermitian operators and , consider expectation values in a system in the state , the variances around the corresponding expectation values being , etc.\n\nThen \nwhere is the commutator of and , and is the anticommutator.\n\nThis follows through use of the Cauchy–Schwarz inequality, since\n, and ; and similarly for the shifted operators and . (Cf. uncertainty principle derivations.)\n\nSubstituting for and (and taking care with the analysis) yield Heisenberg's familiar uncertainty relation for and , as usual.\n\nFor the angular momentum operators , etc., one has that \nwhere formula_51 is the Levi-Civita symbol and simply reverses the sign of the answer under pairwise interchange of the indices. An analogous relation holds for the spin operators.\n\nHere, for and , in angular momentum multiplets , one has, for the transverse components of the Casimir invariant , the -symmetric relations \nas well as .\n\nConsequently, the above inequality applied to this commutation relation specifies\nhence \nand therefore\nso, then, it yields useful constraints such as a lower bound on the Casimir invariant: , and hence , among others.\n\n\n"}
{"id": "2072741", "url": "https://en.wikipedia.org/wiki?curid=2072741", "title": "Category of small categories", "text": "Category of small categories\n\nIn mathematics, specifically in category theory, the category of small categories, denoted by Cat, is the category whose objects are all small categories and whose morphisms are functors between categories. Cat may actually be regarded as a 2-category with natural transformations serving as 2-morphisms.\n\nThe initial object of Cat is the \"empty category\" 0, which is the category of no objects and no morphisms. The terminal object is the \"terminal category\" or \"trivial category\" 1 with a single object and morphism.\n\nThe category Cat is itself a large category, and therefore not an object of itself. In order to avoid problems analogous to Russell's paradox one cannot form the “category of all categories”. But it is possible to form a quasicategory (meaning objects and morphisms merely form a conglomerate) of all categories.\n\nThe category Cat has a forgetful functor \"U\" into the quiver category Quiv:\n\nThis functor forgets the identity morphisms of a given category, and it forgets morphism compositions. The left adjoint of this functor is a functor \"F\" taking Quiv to the corresponding free categories:\n\n\n"}
{"id": "53796787", "url": "https://en.wikipedia.org/wiki?curid=53796787", "title": "Center (ring theory)", "text": "Center (ring theory)\n\nIn algebra, the center of a ring \"R\" is the subring consisting of the elements \"x\" such that \"xy = yx\" for all elements \"y\" in \"R\". It is a commutative ring and is denoted as formula_1; \"Z\" stands for the German word \"Zentrum\", meaning \"center\".\n\nIf \"R\" is a ring, then \"R\" is an associative algebra over its center. Conversely, if \"R\" is an associative algebra over a commutative subring \"S\", then \"S\" is a subring of the center of \"R\", and if \"S\" happens to be the center of \"R\", then the algebra \"R\" is called a central algebra.\n\n\n\n"}
{"id": "2634917", "url": "https://en.wikipedia.org/wiki?curid=2634917", "title": "Circumscription (logic)", "text": "Circumscription (logic)\n\nCircumscription is a non-monotonic logic created by John McCarthy to formalize the common sense assumption that things are as expected unless otherwise specified. Circumscription was later used by McCarthy in an attempt to solve the frame problem. To implement circumscription in its initial formulation, McCarthy augmented first-order logic to allow the minimization of the extension of some predicates, where the extension of a predicate is the set of tuples of values the predicate is true on. This minimization is similar to the closed-world assumption that what is not known to be true is false.\n\nThe original problem considered by McCarthy was that of missionaries and cannibals: there are three missionaries and three cannibals on one bank of a river; they have to cross the river using a boat that can only take two, with the additional constraint that cannibals must never outnumber the missionaries on either bank (as otherwise the missionaries would be killed and, presumably, eaten). The problem considered by McCarthy was not that of finding a sequence of steps to reach the goal (the article on the missionaries and cannibals problem contains one such solution), but rather that of excluding conditions that are not explicitly stated. For example, the solution “go half a mile south and cross the river on the bridge” is intuitively not valid because the statement of the problem does not mention such a bridge. On the other hand, the existence of this bridge is not excluded by the statement of the problem either. That the bridge does not exist is\na consequence of the implicit assumption that the statement of the problem contains everything that is relevant to its solution. Explicitly stating that a bridge does not exist is not a solution to this problem, as there are many other exceptional conditions that should be excluded (such as the presence of a rope for fastening the cannibals, the presence of a larger boat nearby, etc.)\n\nCircumscription was later used by McCarthy to formalize the implicit assumption of inertia: things do not change unless otherwise specified. Circumscription seemed to be useful to avoid specifying that conditions are not changed by all actions except those explicitly known to change them; this is known as the frame problem. However, the solution proposed by McCarthy was later shown leading to wrong results in some cases, like in the Yale shooting problem scenario. Other solutions to the frame problem that correctly formalize the Yale shooting problem exist; some use circumscription but in a different way.\n\nWhile circumscription was initially defined in the first-order logic case, the particularization to the propositional case is easier to define. Given a propositional formula formula_1, its circumscription is the formula having only the models of formula_1 that do not assign a variable to true unless necessary.\n\nFormally, propositional models can be represented by sets of propositional variables; namely, each model is represented by the set of propositional variables it assigns to true. For example, the model assigning true to formula_3, false to formula_4, and true to formula_5 is represented by the set formula_6, because formula_3 and formula_5 are exactly the variables that are assigned to true by this model.\n\nGiven two models formula_9 and formula_10 represented this way, the condition formula_11 is equivalent to formula_9 setting to true every variable that formula_10 sets to true. In other words, formula_14 models the relation of “setting to true less variables”. formula_15 means that formula_11 but these two models do not coincide.\n\nThis lets us define models that do not assign variables to true unless necessary.\nA model formula_9 of a theory formula_1 is called minimal, if and only if there is no model\nformula_10 of formula_1 for which formula_15.\n\nCircumscription is expressed by selecting only the minimal models. It is defined as follows:\n\nAlternatively, one can define formula_23 as a formula having exactly the above set of models; furthermore, one can also avoid giving a definition of formula_24 and only define minimal inference as formula_25 if and only if every minimal model of formula_1 is also a model of formula_27.\n\nAs an example, the formula formula_28 has three models:\n\n\nThe first model is not minimal in the set of variables it assigns to true. Indeed, the second model makes the same assignments except for formula_5, which is assigned to false and not to true. Therefore, the first model is not minimal. The second and third models are incomparable: while the second assigns true to formula_4, the third assigns true to formula_5 instead. Therefore, the models circumscribing formula_1 are the second and third models of the list. A propositional formula having exactly these two models is the following one:\n\nIntuitively, in circumscription a variable is assigned to true only if this is necessary. Dually, if a variable can be false, it must be false. For example, at least one of formula_4 and formula_5 must be assigned to true according to formula_1; in the circumscription exactly one of the two variables must be true. The variable formula_3 cannot be false in any model of formula_1 and neither of the circumscription.\n\nThe extension of circumscription with fixed and varying predicates is due to Vladimir Lifschitz. The idea is that some conditions are not to be minimized. In propositional logic terms, some variables are not to be falsified if possible. In particular, two kind of variables can be considered:\n\n\n\nThe difference is that the value of the varying conditions are simply assumed not to matter. The fixed conditions instead characterize a possible situation, so that comparing two situations where these conditions have different value makes no sense.\n\nFormally, the extension of circumscription that incorporate varying and fixed variables is as follows, where formula_51 is the set of variables to minimize, formula_52 the fixed variables, and the varying variables are those not in formula_53:\n\nIn words, minimization of the variables assigned to true is only done for the variables in formula_51; moreover, models are only compared if they assign the same values to the variables of formula_52. All other variables are not taken into account while comparing models.\n\nThe solution to the frame problem proposed by McCarthy is based on circumscription with no fixed conditions. In the propositional case, this solution can be described as follows: in addition to the formulae directly encoding what is known, one also define new variables representing changes in the values of the conditions; these new variables are then minimized.\n\nFor example, of the domain in which there is a door that is closed at time 0 and in which the action of opening the door is executed at time 2, what is explicitly known is represented by the two formulae:\n\nThe frame problem shows in this example as the problem that formula_59 is not a consequence of the above formulae, while the door is supposed to stay closed until the action of opening it is performed. Circumscription can be used to this aim by defining new variables formula_60 to model changes and then minimizing them:\n\nAs shown by the Yale shooting problem, this kind of solution does not work. For example, formula_63 is not yet entailed by the circumscription of the formulae above: the model in which formula_64 is true and formula_65 is false is incomparable with the model with the opposite values. Therefore, the situation in which the door becomes open at time 1 and then remains open as a consequence of the action is not excluded by circumscription.\n\nSeveral other formalizations of dynamical domains not suffering from such problems have been developed (see frame problem for an overview). Many use circumscription but in a different way.\n\nThe original definition of circumscription proposed by McCarthy is about first-order logic. The role of variables in propositional logic (something that can be true or false) is played in first-order logic by predicates. Namely, a propositional formula can be expressed in first-order logic by replacing each propositional variable with a predicate of zero arity (i.e., a predicate with no arguments). Therefore, minimization is done on predicates in the first-order logic version of circumscription: the circumscription of a formula is obtained forcing predicates to be false whenever possible.\n\nGiven a first-order logic formula formula_1 containing a predicate formula_51, circumscribing this predicate amounts to selecting only the models of formula_1 in which formula_51 is assigned to true on a minimal set of tuples of values.\n\nFormally, the \"extension\" of a predicate in a first-order model is the set of tuples of values this predicate assign to true in the model. First-order models indeed includes the evaluation of each predicate symbol; such an evaluation tells whether the predicate is true or false for any possible value of its arguments. Since each argument of a predicate must be a term, and each term evaluates to a value, the models tells whether formula_70 is true for any possible tuple of values formula_71. The extension of formula_51 in a model is the set of tuples of terms such that formula_70 is true in the model.\n\nThe circumscription of a predicate formula_51 in a formula formula_1 is obtained by selecting only the models of formula_1 with a minimal extension of formula_51. For example, if a formula has only two models, differing only because formula_70 is true in one and false in the second, then only the second model is selected. This is because formula_71 is in the extension of formula_51 in the first model but not in the second.\n\nThe original definition by McCarthy was syntactical rather than semantical. Given a formula formula_1 and a predicate formula_51, circumscribing formula_51 in formula_1 is the following second-order formula:\n\nIn this formula, formula_88 is a n-tuple of terms, where n is the arity of formula_51. This formula states that extension minimization has to be done: in order for a truth evaluation on formula_51 of a model being considered, it must be the case that no other predicate formula_91 can assign to false every tuple that formula_51 assigns to false and yet being different from formula_51.\n\nThis definition only allows circumscribing a single predicate. While the extension to more than one predicate is trivial, minimizing the extension of a single predicate has an important application: capturing the idea that things are usually as expected. This idea can be formalized by minimizing a single predicate expressing the abnormality of situations. In particular, every known fact is expressed in logic with the addition of a literal formula_94 stating that the fact holds only in normal situations. Minimizing the extension of this predicate allows for reasoning under the implicit assumption that things are as expected (that is, they are not abnormal), and that this assumption is made only if possible (abnormality can be assumed false only if this is consistent with the facts.)\n\nPointwise circumscription is a variant of first-order circumscription that has been introduced by Vladimir Lifschitz. In the propositional case, pointwise and predicate circumscription coincide. The rationale of pointwise circumscription is that it minimizes the value of a predicate for each tuple of values separately, rather than minimizing the extension of the predicate. For example, there are two models of formula_95 with domain formula_36, one setting formula_97 and the other setting formula_98. Since the extension of formula_51 in the first model is formula_100 while the extension for the second is formula_36, circumscription only selects the first model.\n\nIn pointwise circumscription, each tuple of values is considered separately. For example, in the formula formula_95 one would consider the value of formula_103 separately from formula_104. A model is minimal only if it is not possible to turn any such value from true to false while still satisfying the formula. As a result, the model in which formula_98 is selected by pointwise circumscription because turning only formula_103 into false does not satisfy the formula, and the same happens for formula_104.\n\nAn earlier formulation of circumscription by McCarthy is based on minimizing the domain of first-order models, rather than the extension of predicates. Namely, a model is considered less than another if it has a smaller domain and the two models coincide on the evaluation of the common tuples of values. This version of circumscription can be reduced to predicate circumscription.\n\nFormula circumscription was a later formalism introduced by McCarthy. This is a generalization of circumscription in which the extension of a formula is minimized, rather than the extension of a predicate. In other words, a formula can be specified so that the set of tuples of values of the domain that satisfy the formula is made as small as possible.\n\nCircumscription does not always correctly handle disjunctive information. Ray Reiter provided the following example: a coin is tossed over a checkboard, and the result is that the coin is either on a black area, or on a white area, or both. However, there are a large number of other possible places where the coin is not supposed to be on; for example, it is implicit that the coin is not on the floor, or on the refrigerator, or on the surface of the moon. Circumscription can therefore be used to minimize the extension of formula_108 predicate, so that formula_109 is false even if this is not explicitly stated.\n\nOn the other hand, the minimization of the formula_108 predicate leads\nto the wrong result that the coin is either on a black area or on a white area, \"but not both\". This is because the models in which formula_108 is true only on formula_112 and only on formula_113 have a minimal extension of formula_108, while the model in which the extension of formula_108 is composed of both pairs is not minimal.\n\nTheory curbing is a solution proposed by Thomas Eiter, Georg Gottlob, and Yuri Gurevich. The idea is that the model that circumscription fails to select, the one in which both formula_116 and formula_117 are true, is a model of the formula that is greater (w.r.t. the extension of formula_108) than both the two models that are selected. More specifically, among the models of the formula, the excluded model is the least upper bound of the two selected models. Theory curbing selects such least upper bounds models in addition to the ones selected by circumscription. This inclusion is done until the set of models is closed, in the sense that it includes all least upper bounds of all sets of models it contains.\n\n\n"}
{"id": "6592539", "url": "https://en.wikipedia.org/wiki?curid=6592539", "title": "Computer Aided Verification", "text": "Computer Aided Verification\n\nComputer Aided Verification (CAV) is an annual academic conference on the theory and practice of computer aided formal analysis of software and hardware systems. It is one of the very best conferences in computer science. For example, several important model checking techniques were published in CAV, such as counterexample-guided abstraction refinement and partial order reduction methods.\n\nThe first CAV was held in 1989 in Grenoble, France.\n\n\n"}
{"id": "308815", "url": "https://en.wikipedia.org/wiki?curid=308815", "title": "Connection (mathematics)", "text": "Connection (mathematics)\n\nIn geometry, the notion of a connection makes precise the idea of transporting data along a curve or family of curves in a \"parallel\" and consistent manner. There are various kinds of connections in modern geometry, depending on what sort of data one wants to transport. For instance, an affine connection, the most elementary type of connection, gives a means for transporting tangent vectors to a manifold from one point to another along a curve. An affine connection is typically given in the form of a covariant derivative, which gives a means for taking directional derivatives of vector fields: the infinitesimal transport of a vector field in a given direction.\n\nConnections are of central importance in modern geometry in large part because they allow a comparison between the local geometry at one point and the local geometry at another point. Differential geometry embraces several variations on the connection theme, which fall into two major groups: the infinitesimal and the local theory. The local theory concerns itself primarily with notions of parallel transport and holonomy. The infinitesimal theory concerns itself with the differentiation of geometric data. Thus a covariant derivative is a way of specifying a derivative of a vector field along another vector field on a manifold. A Cartan connection is a way of formulating some aspects of connection theory using differential forms and Lie groups. An Ehresmann connection is a connection in a fibre bundle or a principal bundle by specifying the allowed directions of motion of the field. A Koszul connection is a connection generalizing the derivative in a vector bundle.\n\nConnections also lead to convenient formulations of \"geometric invariants\", such as the curvature (see also curvature tensor and curvature form), and torsion tensor.\n\nConsider the following problem. Suppose that a tangent vector to the sphere \"S\" is given at the north pole, and we are to define a manner of consistently moving this vector to other points of the sphere: a means for \"parallel transport\". Naïvely, this could be done using a particular coordinate system. However, unless proper care is applied, the parallel transport defined in one system of coordinates will not agree with that of another coordinate system. A more appropriate parallel transportation system exploits the symmetry of the sphere under rotation. Given a vector at the north pole, one can transport this vector along a curve by rotating the sphere in such a way that the north pole moves along the curve without axial rolling. This latter means of parallel transport is the Levi-Civita connection on the sphere. If two different curves are given with the same initial and terminal point, and a vector \"v\" is rigidly moved along the first curve by a rotation, the resulting vector at the terminal point will be \"different from\" the vector resulting from rigidly moving \"v\" along the second curve. This phenomenon reflects the curvature of the sphere. A simple mechanical device that can be used to visualize parallel transport is the south-pointing chariot.\n\nFor instance, suppose that \"S\" is given coordinates by the stereographic projection. Regard \"S\" as consisting of unit vectors in R. Then \"S\" carries a pair of coordinate patches: one covering a neighborhood of the north pole, and the other of the south pole. The mappings\ncover a neighborhood \"U\" of the north pole and \"U\" of the south pole, respectively. Let \"X\", \"Y\", \"Z\" be the ambient coordinates in R. Then φ and φ have inverses\nso that the coordinate transition function is inversion in the circle:\n\nLet us now represent a vector field in terms of its components relative to the coordinate derivatives. If \"P\" is a point of \"U\" ⊂ \"S\", then a vector field may be represented by the pushforward\n\nwhere formula_5 denotes the Jacobian matrix of φ, and v = v(\"x\", \"y\") is a vector field on R uniquely determined by \"v\". Furthermore, on the overlap between the coordinate charts \"U\" ∩ \"U\", it is possible to represent the same vector field with respect to the φ coordinates:\n\nTo relate the components v and v, apply the chain rule to the identity φ = φ o φ:\n\nApplying both sides of this matrix equation to the component vector v(φ(\"P\")) and invoking (1) and (2) yields\n\nWe come now to the main question of defining how to transport a vector field parallelly along a curve. Suppose that \"P\"(\"t\") is a curve in \"S\". Naïvely, one may consider a vector field parallel if the coordinate components of the vector field are constant along the curve. However, an immediate ambiguity arises: in \"which\" coordinate system should these components be constant?\n\nFor instance, suppose that \"v\"(\"P\"(\"t\")) has constant components in the \"U\" coordinate system. That is, the functions v(\"φ\"(\"P\"(\"t\"))) are constant. However, applying the product rule to (3) and using the fact that \"d\"v/\"dt\" = 0 gives\n\nBut formula_10 is always a non-singular matrix (provided that the curve \"P\"(\"t\") is not stationary), so v and v \"cannot ever be\" simultaneously constant along the curve.\n\nThe problem observed above is that the usual directional derivative of vector calculus does not behave well under changes in the coordinate system when applied to the components of vector fields. This makes it quite difficult to describe how to translate vector fields in a parallel manner, if indeed such a notion makes any sense at all. There are two fundamentally different ways of resolving this problem.\n\nThe first approach is to examine what is required for a generalization of the directional derivative to \"behave well\" under coordinate transitions. This is the tactic taken by the covariant derivative approach to connections: good behavior is equated with covariance. Here one considers a modification of the directional derivative by a certain linear operator, whose components are called the Christoffel symbols, which involves no derivatives on the vector field itself. The directional derivative \"D\"v of the components of a vector v in a coordinate system φ in the direction u are replaced by a \"covariant derivative\":\n\n"}
{"id": "6759", "url": "https://en.wikipedia.org/wiki?curid=6759", "title": "Context-free grammar", "text": "Context-free grammar\n\nIn formal language theory, a context-free grammar (CFG) is a certain type of formal grammar: a set of production rules that describe all possible strings in a given formal language. Production rules are simple replacements. For example, the rule\n\nformula_1\n\nreplaces formula_2 with formula_3. There can be multiple replacement rules for any given value. For example,\n\nformula_1\n\nformula_5\n\nmeans that formula_2 can be replaced with either formula_3 or formula_8.\n\nIn context-free grammars, all rules are one-to-one, one-to-many, or one-to-none. These rules can be applied regardless of context. The left-hand side of the production rule is always a nonterminal symbol. This means that the symbol does not appear in the resulting formal language. So in our case, our language contains the letters formula_3 and formula_8 but not formula_11\n\nRules can also be applied in reverse to check if a string is grammatically correct according to the grammar.\n\nHere is an example context-free grammar that describes all two-letter strings containing the letters formula_3 or formula_8.\n\nformula_14\n\nformula_1\n\nformula_5\n\nIf we start with the nonterminal symbol formula_17 then we can use the rule\nformula_14 to turn formula_17 into formula_20. We can then apply one of the two later rules. For example, if we apply formula_5 to the first formula_2 we get formula_23. If we then apply formula_1 to the second formula_2 we get formula_26. Since both formula_3 and formula_8 are terminal symbols, and in context-free grammars terminal symbols never appear on the left hand side of a production rule, there are no more rules that can be applied. This same process can be used, applying the last two rules in different orders in order to get all possible strings within our simple context-free grammar.\n\nLanguages generated by context-free grammars are known as context-free languages (CFL). Different context-free grammars can generate the same context-free language. It is important to distinguish the properties of the language (intrinsic properties) from the properties of a particular grammar (extrinsic properties). The language equality question (do two given context-free grammars generate the same language?) is undecidable.\n\nContext-free grammars arise in linguistics where they are used to describe the structure of sentences and words in a natural language, and they were in fact invented by the linguist Noam Chomsky for this purpose, but have not really lived up to their original expectation. By contrast, in computer science, as the use of recursively-defined concepts increased, they were used more and more. In an early application, grammars are used to describe the structure of programming languages. In a newer application, they are used in an essential part of the Extensible Markup Language (XML) called the \"Document Type Definition\".\n\nIn linguistics, some authors use the term phrase structure grammar to refer to context-free grammars, whereby phrase-structure grammars are distinct from dependency grammars. In computer science, a popular notation for context-free grammars is Backus–Naur form, or \"BNF\".\n\nSince the time of Pāṇini, at least, linguists have described the grammars of languages in terms of their block structure, and described how sentences are recursively built up from smaller phrases, and eventually individual words or word elements. An essential property of these block structures is that logical units never overlap. For example, the sentence:\ncan be logically parenthesized as follows:\nA context-free grammar provides a simple and mathematically precise mechanism for describing the methods by which phrases in some natural language are built from smaller blocks, capturing the \"block structure\" of sentences in a natural way. Its simplicity makes the formalism amenable to rigorous mathematical study. Important features of natural language syntax such as agreement and reference are not part of the context-free grammar, but the basic recursive structure of sentences, the way in which clauses nest inside other clauses, and the way in which lists of adjectives and adverbs are swallowed by nouns and verbs, is described exactly.\n\nContext-free grammars are a special form of Semi-Thue systems that in their general form date back to the work of Axel Thue.\n\nThe formalism of context-free grammars was developed in the mid-1950s by Noam Chomsky, and also their classification as a special type of formal grammar (which he called phrase-structure grammars). What Chomsky called a phrase structure grammar is also known now as a constituency grammar, whereby constituency grammars stand in contrast to dependency grammars. In Chomsky's generative grammar framework, the syntax of natural language was described by context-free rules combined with transformation rules.\n\nBlock structure was introduced into computer programming languages by the Algol project (1957–1960), which, as a consequence, also featured a context-free grammar to describe the resulting Algol syntax. This became a standard feature of computer languages, and the notation for grammars used in concrete descriptions of computer languages came to be known as Backus–Naur form, after two members of the Algol language design committee. The \"block structure\" aspect that context-free grammars capture is so fundamental to grammar that the terms syntax and grammar are often identified with context-free grammar rules, especially in computer science. Formal constraints not captured by the grammar are then considered to be part of the \"semantics\" of the language.\n\nContext-free grammars are simple enough to allow the construction of efficient parsing algorithms that, for a given string, determine whether and how it can be generated from the grammar. An Earley parser is an example of such an algorithm, while the widely used LR and LL parsers are simpler algorithms that deal only with more restrictive subsets of context-free grammars.\n\nA context-free grammar is defined by the 4-tuple:\n\nformula_29\nwhere\n\nA production rule in is formalized mathematically as a pair formula_32, where formula_33 is a nonterminal and formula_34 is a string of variables and/or terminals; rather than using ordered pair notation, production rules are usually written using an arrow operator with as its left hand side and as its right hand side:\nformula_35.\n\nIt is allowed for to be the empty string, and in this case it is customary to denote it by ε. The form formula_36 is called an -production.\n\nIt is common to list all right-hand sides for the same left-hand side on the same line, using | (the pipe symbol) to separate them. Rules formula_37 and formula_38 can hence be written as formula_39. In this case, formula_40 and formula_41 is called the first and second alternative, respectively.\n\nFor any strings formula_42, we say directly yields , written as formula_43, if formula_44 with formula_33 and formula_46 such that formula_47 and formula_48. Thus, is a result of applying the rule formula_49 to .\n\nFor any strings formula_50 we say yields , written as formula_51 (or formula_52 in some textbooks), if formula_53 such that formula_54. In this case, if formula_55 (i.e., formula_56), the relation formula_57 holds. In other words, formula_58 and formula_59 are the reflexive transitive closure (allowing a word to yield itself) and the transitive closure (requiring at least one step) of formula_60, respectively.\n\nThe language of a grammar formula_29 is the set\n\nA language is said to be a context-free language (CFL), if there exists a CFG , such that formula_63.\n\nNon-deterministic pushdown automata recognize exactly the context-free languages.\n\nA context-free grammar is said to be \"proper\", if it has\n\nEvery context-free grammar can be effectively transformed into a weakly equivalent one without unreachable symbols, a weakly equivalent one without unproductive symbols, and a weakly equivalent one without cycles.\nEvery context-free grammar not producing ε can be effectively transformed into a weakly equivalent one without ε-productions; altogether, every such grammar can be effectively transformed into a weakly equivalent proper CFG.\n\nThe grammar formula_68, with productions\n\nis context-free. It is not proper since it includes an ε-production. A typical derivation in this grammar is\nThis makes it clear that \nformula_69. \nThe language is context-free, however, it can be proved that it is not regular.\n\nThe canonical example of a context-free grammar is parenthesis matching, which is representative of the general case. There are two terminal symbols \"(\" and \")\" and one nonterminal symbol S. The production rules are\n\nThe first rule allows the S symbol to multiply; the second rule allows the S symbol to become enclosed by matching parentheses; and the third rule terminates the recursion.\n\nA second canonical example is two different kinds of matching nested parentheses, described by the productions:\n\nwith terminal symbols [ ] ( ) and nonterminal S.\n\nThe following sequence can be derived in that grammar:\n\nIn a context-free grammar, we can pair up characters the way we do with brackets. The simplest example:\n\nThis grammar generates the language formula_70, which is not regular (according to the pumping lemma for regular languages).\n\nThe special character ε stands for the empty string. By changing the above grammar to\nwe obtain a grammar generating the language formula_71 instead. This differs only in that it contains the empty string while the original grammar did not.\n\nA context-free grammar for the language consisting of all strings over {a,b} containing an unequal number of a's and b's:\nHere, the nonterminal T can generate all strings with the same number of a's as b's, the nonterminal U generates all strings with more a's than b's and the nonterminal V generates all strings with fewer a's than b's. Omitting the third alternative in the rule for U and V doesn't restrict the grammar's language.\n\nAnother example of a non-regular language is formula_72. It is context-free as it can be generated by the following context-free grammar:\n\nThe formation rules for the terms and formulas of formal logic fit the definition of context-free grammar, except that the set of symbols may be infinite and there may be more than one start symbol.\n\nIn contrast to well-formed nested parentheses and square brackets in the previous section, there is no context-free grammar for generating all sequences of two different types of parentheses, each separately balanced \"disregarding the other\", where the two types need not nest inside one another, for example:\n\nor\n\nThe fact that this language is not context free can be proven using Pumping lemma for context-free languages and a proof by contradiction, observing that all words of the form \nformula_73\nshould belong to the language. This language belongs instead to a more general class and can be described by a conjunctive grammar, which in turn also includes other non-context-free languages, such as the language of all words of the form\nformula_74.\n\nEvery regular grammar is context-free, but not all context-free grammars are regular. The following context-free grammar, however, is also regular.\n\nThe terminals here are \"a\" and \"b\", while the only nonterminal is S.\nThe language described is all nonempty strings of formula_75s and formula_76s that end in formula_75.\n\nThis grammar is regular: no rule has more than one nonterminal in its right-hand side, and each of these nonterminals is at the same end of the right-hand side.\n\nEvery regular grammar corresponds directly to a nondeterministic finite automaton, so we know that this is a regular language.\n\nUsing pipe symbols, the grammar above can be described more tersely as follows:\n\nA \"derivation\" of a string for a grammar is a sequence of grammar rule applications that transform the start symbol into the string.\nA derivation proves that the string belongs to the grammar's language.\n\nA derivation is fully determined by giving, for each step:\nFor clarity, the intermediate string is usually given as well.\n\nFor instance, with the grammar:\n\nthe string\n\ncan be derived with the derivation:\n\nOften, a strategy is followed that deterministically determines the next nonterminal to rewrite:\nGiven such a strategy, a derivation is completely determined by the sequence of rules applied. For instance, the leftmost derivation\n\ncan be summarized as\n\nThe distinction between leftmost derivation and rightmost derivation is important because in most parsers the transformation of the input is defined by giving a piece of code for every grammar rule that is executed whenever the rule is applied. Therefore, it is important to know whether the parser determines a leftmost or a rightmost derivation because this determines the order in which the pieces of code will be executed. See for an example LL parsers and LR parsers.\n\nA derivation also imposes in some sense a hierarchical structure on the string that is derived. For example, if the string \"1 + 1 + a\" is derived according to the leftmost derivation:\n\nthe structure of the string would be:\n\nwhere { ... } indicates a substring recognized as belonging to S. This hierarchy can also be seen as a tree:\n\nThis tree is called a \"parse tree\" or \"concrete syntax tree\" of the string, by contrast with the abstract syntax tree. In this case the presented leftmost and the rightmost derivations define the same parse tree; however, there is another (rightmost) derivation of the same string\n\nand this defines the following parse tree:\n\nNote however that both parse trees can be obtained by both leftmost and rightmost derivations. For example, the last tree can be obtained with the leftmost derivation as follows:\n\nIf a string in the language of the grammar has more than one parsing tree, then the grammar is said to be an \"ambiguous grammar\". Such grammars are usually hard to parse because the parser cannot always decide which grammar rule it has to apply. Usually, ambiguity is a feature of the grammar, not the language, and an unambiguous grammar can be found that generates the same context-free language. However, there are certain languages that can only be generated by ambiguous grammars; such languages are called \"inherently ambiguous languages\".\n\nHere is a context-free grammar for syntactically correct infix algebraic expressions in the variables x, y and z:\n\nThis grammar can, for example, generate the string\n\nas follows:\n\nNote that many choices were made underway as to which rewrite was going to be performed next.\nThese choices look quite arbitrary. As a matter of fact, they are, in the sense that the string finally generated is always the same. For example, the second and third rewrites\n\ncould be done in the opposite order:\n\nAlso, many choices were made on which rule to apply to each selected codice_1.\nChanging the choices made and not only the order they were made in usually affects which terminal string comes out at the end.\n\nLet's look at this in more detail. Consider the parse tree of this derivation:\n\nStarting at the top, step by step, an S in the tree is expanded, until no more unexpanded codice_1es (nonterminals) remain.\nPicking a different order of expansion will produce a different derivation, but the same parse tree.\nThe parse tree will only change if we pick a different rule to apply at some position in the tree.\n\nBut can a different parse tree still produce the same terminal string,\nwhich is codice_3 in this case?\nYes, for this particular grammar, this is possible.\nGrammars with this property are called ambiguous.\n\nFor example, codice_4 can be produced with these two different parse trees:\n\nHowever, the \"language\" described by this grammar is not inherently ambiguous:\nan alternative, unambiguous grammar can be given for the language, for example:\n\n(once again picking codice_1 as the start symbol). This alternative grammar will produce codice_4 with a parse tree similar to the left one above, i.e. implicitly assuming the association codice_7, which is not according to standard operator precedence. More elaborate, unambiguous and context-free grammars can be constructed that produce parse trees that obey all desired operator precedence and associativity rules.\nEvery context-free grammar that does not generate the empty string can be transformed into one in which there is no ε-production (that is, a rule that has the empty string as a product). If a grammar does generate the empty string, it will be necessary to include the rule formula_78, but there need be no other ε-rule. Every context-free grammar with no ε-production has an equivalent grammar in Chomsky normal form, and a grammar in Greibach normal form. \"Equivalent\" here means that the two grammars generate the same language.\n\nThe especially simple form of production rules in Chomsky normal form grammars has both theoretical and practical implications. For instance, given a context-free grammar, one can use the Chomsky normal form to construct a polynomial-time algorithm that decides whether a given string is in the language represented by that grammar or not (the CYK algorithm).\n\nContext-free languages are closed under the various operations, that is, if the languages \"K\" and \"L\" are \ncontext-free, so is the result of the following operations:\n\n\nThey are not closed under general intersection (hence neither under complementation) and set difference.\n\nThe following are some decidable problems about context-free grammars.\n\nThe parsing problem, checking whether a given word belongs to the language given by a context-free grammar, is decidable, using one of the general-purpose parsing algorithms:\n\nIt is decidable to check whether a given non-terminal of a context-free grammar is reachable, whether it is productive, and whether it is nullable (that is, it can derive an empty string).\n\nIt is decidable whether a given grammar is a regular grammar, as well as whether it is an LL(1) grammar (see LL parser).\n\nThere are algorithms to decide whether a language of a given context-free language is empty, as well as whether it is finite..\n\nSome questions that are undecidable for wider classes of grammars become decidable for context-free grammars; e.g. the emptiness problem (whether the grammar generates any terminal strings at all), is undecidable for context-sensitive grammars, but decidable for context-free grammars.\n\nHowever, many problems are undecidable even for context-free grammars. Examples are:\n\nGiven a CFG, does it generate the language of all strings over the alphabet of terminal symbols used in its rules?\n\nA reduction can be demonstrated to this problem from the well-known undecidable problem of determining whether a Turing machine accepts a particular input (the halting problem). The reduction uses the concept of a \"computation history\", a string describing an entire computation of a Turing machine. A CFG can be constructed that generates all strings that are not accepting computation histories for a particular Turing machine on a particular input, and thus it will accept all strings only if the machine doesn't accept that input.\n\nGiven two CFGs, do they generate the same language?\n\nThe undecidability of this problem is a direct consequence of the previous: it is impossible to even decide whether a CFG is equivalent to the trivial CFG defining the language of all strings.\n\nGiven two CFGs, can the first one generate all strings that the second one can generate?\n\nIf this problem was decidable, then language equality could be decided too: two CFGs G1 and G2 generate the same language if L(G1) is a subset of L(G2) and L(G2) is a subset of L(G1).\n\nUsing Greibach's theorem, it can be shown that the two following problems are undecidable:\n\n\nGiven a CFG, is it ambiguous?\n\nThe undecidability of this problem follows from the fact that if an algorithm to determine ambiguity existed, the Post correspondence problem could be decided, which is known to be undecidable.\n\nGiven two CFGs, is there any string derivable from both grammars?\n\nIf this problem was decidable, the undecidable Post correspondence problem could be decided, too: given strings formula_79 over some alphabet formula_80, let the grammar consist of the rule\nwhere formula_82 denotes the reversed string formula_83 and formula_76 doesn't occur among the formula_85; and let grammar consist of the rule\nThen the Post problem given by formula_79 has a solution if and only if and share a derivable string.\n\nAn obvious way to extend the context-free grammar formalism is to allow nonterminals to have arguments, the values of which are passed along within the rules. This allows natural language features such as agreement and reference, and programming language analogs such as the correct use and definition of identifiers, to be expressed in a natural way. E.g. we can now easily express that in English sentences, the subject and verb must agree in number. In computer science, examples of this approach include affix grammars, attribute grammars, indexed grammars, and Van Wijngaarden two-level grammars. Similar extensions exist in linguistics.\n\nAn extended context-free grammar (or regular right part grammar) is one in which the right-hand side of the production rules is allowed to be a regular expression over the grammar's terminals and nonterminals. Extended context-free grammars describe exactly the context-free languages.\n\nAnother extension is to allow additional terminal symbols to appear at the left-hand side of rules, constraining their application. This produces the formalism of context-sensitive grammars.\n\nThere are a number of important subclasses of the context-free grammars:\n\n\nLR parsing extends LL parsing to support a larger range of grammars; in turn, generalized LR parsing extends LR parsing to support arbitrary context-free grammars. On LL grammars and LR grammars, it essentially performs LL parsing and LR parsing, respectively, while on nondeterministic grammars, it is as efficient as can be expected. Although GLR parsing was developed in the 1980s, many new language definitions and parser generators continue to be based on LL, LALR or LR parsing up to the present day.\n\nChomsky initially hoped to overcome the limitations of context-free grammars by adding transformation rules.\n\nSuch rules are another standard device in traditional linguistics; e.g. passivization in English. Much of generative grammar has been devoted to finding ways of refining the descriptive mechanisms of phrase-structure grammar and transformation rules such that exactly the kinds of things can be expressed that natural language actually allows. Allowing arbitrary transformations does not meet that goal: they are much too powerful, being Turing complete unless significant restrictions are added (e.g. no transformations that introduce and then rewrite symbols in a context-free fashion).\n\nChomsky's general position regarding the non-context-freeness of natural language has held up since then, although his specific examples regarding the inadequacy of context-free grammars in terms of their weak generative capacity were later disproved.\nGerald Gazdar and Geoffrey Pullum have argued that despite a few non-context-free constructions in natural language (such as cross-serial dependencies in Swiss German and reduplication in Bambara), the vast majority of forms in natural language are indeed context-free.\n\n\n\n\n"}
{"id": "6235098", "url": "https://en.wikipedia.org/wiki?curid=6235098", "title": "Convergent validity", "text": "Convergent validity\n\nConvergent validity, a parameter often used in sociology, psychology, and other behavioral sciences, refers to the degree to which two measures of constructs that theoretically should be related, are in fact related. Convergent validity, along with discriminant validity, is a subtype of construct validity. Convergent validity can be established if two similar constructs correspond with one another, while discriminant validity applies to two dissimilar constructs that are easily differentiated.\n\nCampbell and Fiske (1959) developed the Multitrait-Multimethod Matrix to assess the construct validity of a set of measures in a study. The approach stresses the importance of using both discriminant and convergent validation techniques when assessing new tests. In other words, in order to establish construct validity, you have to demonstrate both convergence and discrimination.\n\nConvergent validity can be estimated using correlation coefficients. A successful evaluation of convergent validity shows that a test of a concept is highly correlated with other tests designed to measure theoretically similar concepts. For instance, to show the convergent validity of a test of mathematics skills, the scores on the test can be correlated with scores on other tests that are also designed to measure basic mathematics ability. High correlations between the test scores would be evidence of convergent validity.\n\nConvergent evidence is best interpreted relative to discriminant evidence. That is, patterns of intercorrelations between two dissimilar measures should be low while correlations with similar measures should be substantially greater. This evidence can be organized as a multitrait-multimethod matrix. For example, in order to test the convergent validity of a measure of self-esteem, a researcher may want to show that measures of similar constructs, such as self-worth, confidence, social skills, and self-appraisal are also related to self-esteem, whereas non-overlapping factors, such as intelligence, should not relate.\n\n"}
{"id": "19948728", "url": "https://en.wikipedia.org/wiki?curid=19948728", "title": "Covering number", "text": "Covering number\n\nIn mathematics, a covering number is the number of spherical balls of a given size needed to completely cover a given space, with possible overlaps. Two related concepts are the \"packing number\", the number of disjoint balls that fit in a space, and the \"metric entropy\", the number of points that fit in a space when constrained to lie at some fixed minimum distance apart.\n\nLet (\"M\", \"d\") be a metric space, let \"K\" be a subset of \"M\", and let \"r\" be a positive real number. Let \"B\"(\"x\") denote the ball of radius \"r\" centered at \"x\". A subset \"C\" of \"M\" is an \"r-external covering\" of \"K\" if:\nIn other words, for every formula_2 there exists formula_3 such that formula_4.\n\nIf furthermore \"C\" is a subset of \"K\", then it is an \"r-internal covering\".\n\nThe external covering number of \"K\", denoted formula_5, is the minimum cardinality of any external covering of \"K\". The internal covering number, denoted formula_6, is the minimum cardinality of any internal covering.\n\nA subset \"P\" of \"K\" is a \"packing\" if formula_7 and the set formula_8 is pairwise disjoint. The packing number of \"K\", denoted formula_9, is the maximum cardinality of any packing of \"K\".\n\nA subset \"S\" of \"K\" is \"r\"-\"separated\" if each pair of points \"x\" and \"y\" in \"S\" satisfies \"d\"(\"x\", \"y\") ≥ \"r\". The metric entropy of \"K\", denoted formula_10, is the maximum cardinality of any \"r\"-separated subset of \"K\".\n\n1. The metric space is the real line formula_11. formula_12 is a set of real numbers whose absolute value is at most formula_13. Then, there is an external covering of formula_14 intervals of length formula_15, covering the interval formula_16. Hence:\n\n2. The metric space is the Euclidean space formula_18 with the Euclidean metric.\nformula_19 is a set of vectors whose length (norm) is at most formula_13.\nIf formula_21 lies in a \"d\"-dimensional subspace of formula_18, then:\n\n3. The metric space is the space of real-valued functions, with the l-infinity metric.\nThe covering number formula_6\nis the smallest number formula_13 \nsuch that, there exist formula_26\nsuch that, for all formula_27 there exists formula_28\nsuch that the supremum distance between formula_29 and formula_30\nis at most formula_15.\nThe above bound is not relevant since the space is formula_32-dimensional.\nHowever, when formula_21 is a compact set, every covering of it has a finite sub-covering, \nso formula_6 is finite.\n\n1. The internal and external covering numbers, the packing number, and the metric entropy are all closely related. The following chain of inequalities holds for any subset \"K\" of a metric space and any positive real number \"r\".\n\n2. Each function except the internal covering number is non-increasing in \"r\" and non-decreasing in \"K\". The internal covering number is monotone in \"r\" but not necessarily in \"K\".\n\nThe following properties relate to covering numbers in the standard Euclidean space formula_18:\n\n3. If all vectors in formula_21 are translated by a constant vector formula_38, then the covering number does not change.\n\n4. If all vectors in formula_21 are multiplied by a scalar formula_40, then:\n\n5. If all vectors in formula_21 are operated by a Lipschitz function formula_44 with Lipschitz constant formula_13, then:\n\nLet formula_21 be a space of real-valued functions, with the l-infinity metric (see example 3 above).\nSuppose all functions in formula_21 are bounded by a real constant formula_50.\nThen, the covering number can be used to bound the generalization error\nof learning functions from formula_21,\nrelative to the squared loss:\n\n"}
{"id": "49089", "url": "https://en.wikipedia.org/wiki?curid=49089", "title": "Cox's theorem", "text": "Cox's theorem\n\nCox's theorem, named after the physicist Richard Threlkeld Cox, is a derivation of the laws of probability theory from a certain set of postulates. This derivation justifies the so-called \"logical\" interpretation of probability, as the laws of probability derived by Cox's theorem are applicable to any proposition. Logical (a.k.a. objective Bayesian) probability is a type of Bayesian probability. Other forms of Bayesianism, such as the subjective interpretation, are given other justifications.\n\nCox wanted his system to satisfy the following conditions:\n\n\nThe postulates as stated here are taken from Arnborg and Sjödin.\n\"Common sense\" includes consistency with Aristotelian logic in the sense that logically equivalent propositions shall have the same plausibility.\n\nThe postulates as originally stated by Cox were not mathematically\nrigorous (although better than the informal description above), e.g.,\nas noted by Halpern. However it appears to be possible\nto augment them with various mathematical assumptions made either\nimplicitly or explicitly by Cox to produce a valid proof.\n\nCox's notation:\n\nCox's postulates and functional equations are:\n\n\nThe laws of probability derivable from these postulates are the following. Let formula_34 be the plausibility of the proposition formula_1 given formula_6 satisfying Cox's postulates. Then there is a function formula_15 mapping plausibilities to interval [0,1] and a positive number formula_38 such that\n\n\nIt is important to note that the postulates imply only these general properties. We may recover the usual laws of probability by setting a new function, conventionally denoted formula_42 or formula_43, equal to formula_44. Then we obtain the laws of probability in a more familiar form:\n\n\nRule 2 is a rule for negation, and rule 3 is a rule for conjunction. Given that any proposition containing conjunction, disjunction, and negation can be equivalently rephrased using conjunction and negation alone (the conjunctive normal form), we can now handle any compound proposition.\n\nThe laws thus derived yield finite additivity of probability, but not countable additivity. The measure-theoretic formulation of Kolmogorov assumes that a probability measure is countably additive. This slightly stronger condition is necessary for the proof of certain theorems.\n\nCox's theorem has come to be used as one of the justifications for the\nuse of Bayesian probability theory. For example, in Jaynes it is\ndiscussed in detail in chapters 1 and 2 and is a cornerstone for the\nrest of the book. Probability is interpreted as a formal system of\nlogic, the natural extension of Aristotelian logic (in which every\nstatement is either true or false) into the realm of reasoning in the\npresence of uncertainty.\n\nIt has been debated to what degree the theorem excludes alternative models for reasoning about uncertainty. For example, if certain \"unintuitive\" mathematical assumptions were dropped then alternatives could be devised, e.g., an example provided by Halpern. However Arnborg and Sjödin suggest additional\n\"common sense\" postulates, which would allow the assumptions to be relaxed in some cases while still ruling out the Halpern example. Other approaches were devised by Hardy or Dupré and Tipler.\n\nThe original formulation of Cox's theorem is in which is extended with additional results and more discussion in . Jaynes cites Abel for the first known use of the associativity functional equation. Aczél provides a long proof of the \"associativity equation\" (pages 256-267). Jaynes reproduces the shorter proof by Cox in which differentiability is assumed. A guide to Cox's theorem by Van Horn aims at comprehensively introducing the reader to all these references.\n\n\n"}
{"id": "40625991", "url": "https://en.wikipedia.org/wiki?curid=40625991", "title": "Cybernetics: Or Control and Communication in the Animal and the Machine", "text": "Cybernetics: Or Control and Communication in the Animal and the Machine\n\nCybernetics: Or Control and Communication in the Animal and the Machine is a book written by Norbert Wiener and published in 1948. It is the first public usage of the term \"cybernetics\" to refer to self-regulating mechanisms. The book laid the theoretical foundation for servomechanisms (whether electrical, mechanical or hydraulic), automatic navigation, analog computing, artificial intelligence, neuroscience, and reliable communications.\n\nA second edition with minor changes and two additional chapters was published in 1961.\n\nThe book aroused a considerable amount of public discussion and comment at the time of publication, unusual for a predominantly technical subject.\n\nThe public interest aroused by this book inspired Wiener to address the sociological and political issues raised in a book targeted at the non-technical reader, resulting in the publication in 1950 of \"The Human Use of Human Beings\".\n\nIntroduction\n\n1. Newtonian and Bergsonian Time\n\n2. Groups and Statistical Mechanics\n\n3. Time Series, Information, and Communication\n\n4. Feedback and Oscillation\n\n5. Computing Machines and the Nervous System\n\n6. Gestalt and Universals\n\n7. Cybernetics and Psychopathology\n\n8. Information, Language, and Society\n\n9. On Learning and Self-Reproducing Machines\n\n10. Brain Waves and Self-Organising Systems\n\nWiener recounts that the origin of the ideas in this book is a ten-year-long series of meetings at the Harvard Medical School where medical scientists and physicians discussed scientific method with mathematicians, physicists and engineers. He details the interdisciplinary nature of his approach and refers to his work with Vannevar Bush and his differential analyzer (a primitive analog computer), as well as his early thoughts on the features and design principles of future digital calculating machines. He traces the origins of cybernetic analysis to the philosophy of Leibniz, citing his work on universal symbolism and a calculus of reasoning.\n\nThe theme of this chapter is an exploration of the contrast between time-reversible processes governed by Newtonian mechanics and time-irreversible processes in accordance with the Second Law of Thermodynamics. In the opening section he distinguishes the predictable nature of astronomy from the challenges posed in meteorology, anticipating future developments in Chaos theory. He points out that in fact, even in the case of astronomy, tidal forces between the planets introduce a degree of decay over cosmological time spans, and so strictly speaking Newtonian mechanics does not precisely apply.\n\nThis chapter opens with a review of the – entirely independent and apparently unrelated – work of two scientists in the early 20th century: Willard Gibbs and Henri Lebesgue. Gibbs was a physicist working on a statistical approach to Newtonian dynamics and thermodynamics, and Lebesgue was a pure mathematician working on the theory of trigonometric series. Wiener suggests that the questions asked by Gibbs find their answer in the work of Lebesque. Weiner claims that the Lebesgue integral had unexpected but important implications in establishing the validity of Gibbs' work on the foundations of statistical mechanics. The notions of \"average\" and \"measure\" in the sense established by Lebesgue were urgently needed to provide a rigorous proof of Gibbs' ergodic hypothesis.\nThe concept of entropy in statistical mechanics is developed, and its relationship to the way the concept is used in thermodynamics. By an analysis of the thought experiment Maxwell's demon, he relates the concept of entropy to that of information.\n\nThis is one of the more mathematically intensive chapters in the book. It deals with the transmission or recording of a varying analog signal as a sequence of numerical samples, and lays much of the groundwork for the development of digital audio and telemetry over the past six decades. It also examines the relationship between bandwidth, noise, and information capacity, as developed by Wiener in collaboration with Claude Shannon. This chapter and the next one form the core of the foundational principles for the development of automation systems and digital communications and data processing which has taken place over the decades since the book was published.\n\nThis chapter lays down the foundations for the mathematical treatment of negative feedback in automated control systems. The opening passage illustrates the effect of faulty feedback mechanisms by the example of patients suffering from various forms of ataxia. He then discusses railway signalling, the operation of a thermostat, and a steam engine centrifugal governor. The rest of the chapter is mostly taken up with the development of a mathematical formulation of the operation of the principles underlying all of these processes. More complex systems are then discussed such as automated navigation, and the control of non-linear situations such as steering on an icy road. He concludes with a reference to the homeostatic processes in living organisms.\n\nThis chapter opens with a discussion of the relative merits of analog computers and digital computers (which Wiener referred to as \"analogy machines\" and \"numerical machines\"), and maintains that digital machines will be more accurate, electronic implementations will be superior to mechanical or electro-mechanical ones, and that the binary system is preferable to other numerical scales. After discussing the need to store both the data to be processed and the algorithms which are employed for processing that data, and the challenges involved in implementing a suitable memory system, he goes on to draw the parallels between binary digital computers and the nerve structures in organisms.\n\nAmong the mechanisms that he speculated for implementing a computer memory system was \"a large array of small condensers [ie capacitors in today's terminology] which could be rapidly charged or discharged\", thus prefiguring the essential technology of modern dynamic random-access memory chips.\n\nVirtually all of the principles which Wiener enumerated as being desirable characteristics of calculating and data processing machines have been adopted in the design of digital computers, from the early mainframes of the 1950s to the latest microchips.\n\nThis brief chapter is a philosophical enquiry into the relationship between the physical events in the central nervous system and the subjective experiences of the individual. It concentrates principally on the processes whereby nervous signals from the retina are transformed into a representation of the visual field. It also explores the various feedback loops involved in the operation of the eyes: the homeostatic operation of the retina to control light levels, the adjustment of the lens to bring objects into focus, and the complex set of reflex movements to bring an object of attention into the detailed vision area of the fovea.\nThe chapter concludes with an outline of the challenges presented by attempts to implement a reading machine for the blind.\n\nWiener opens this chapter with the disclaimers that he is neither a psychopathologist nor a psychiatrist, and that he is not asserting that mental problems are failings of the brain to operate as a computing machine. However, he suggests that there might be fruitful lines of enquiry opened by considering the parallels between the brain and a computer. (He employed the archaic-sounding phrase \"computing machine\", because at the time of writing the word \"computer\" referred to a person who is employed to perform routine calculations). He then discussed the concept of 'redundancy' in the sense of having two or three computing mechanisms operating simultaneously on the same problem, so that errors may be recognised and corrected.\n\nStarting with an outline of the hierarchical nature of living organisms, and a discussion of the structure and organisation of colonies of symbiotic organisms, such as the Portuguese Man o' War, this chapter explores the parallels with the structure of human societies, and the challenges faced as the scale and complexity of society increases.\n\nThe Chapter closes with speculation about the possibility of constructing a chess-playing machine, and concludes that it would be conceivable to build a machine capable of a standard of play better than most human players but not at expert level. Such a possibility seemed entirely fanciful to most commentators in the 1940s, bearing in mind the state of computing technology at the time, although events have turned out to vindicate the prediction – and even to exceed it.\n\nStarting with an examination of the learning process in organisms, Wiener expands the discussion to John von Neumann's theory of games, and the application to military situations. He then speculates about the manner in which a chess-playing computer could be programmed to analyse its past performances and improve its performance. This proceeds to a discussion of the evolution of conflict, as in the examples of matador and bull, or mongoose and cobra, or between opponents in a tennis game. He discusses various stories such as The Sorcerer's Apprentice, which illustrate the literal-minded nature of \"magical\" processes, the context being the drawing of attention to the need for caution in delegating to machines the responsibility for warfare strategy in an age of Nuclear weapons. The chapter concludes with a discussion of the possibility of self-replicating machines and the work of Professor Dennis Gabor in this area.\n\nThis chapter opens with a discussion of the mechanism of evolution by Natural Selection, which he refers to as \"\"phylogenetic learning\", since it is driven by a feedback mechanism caused by the success or otherwise in surviving and reproducing; and modifications of behaviour over a lifetime in response to experience, which he calls \"ontogenetic learning\". He suggests that both processes involve non-linear feedback, and speculates that the learning process is correlated with changes in patterns of the rhythms of the waves of electrical activity that can be observed on an electroencephalograph. After a discussion of the technical limitations of earlier designs of such equipment, he suggests that the field will become more fruitful as more sensitive interfaces and higher performance amplifiers are developed and the readings are stored in digital form for numerical analysis, rather than recorded by pen galvanometers in real time - which was the only available technique at the time of writing. He then develops suggestions for a mathematical treatment of the waveforms by Fourier analysis, and draws a parallel with the processing of the results of the Michelson-Morley experiment which confirmed the constancy of the velocity of light, which in turn led Einstein to develop the Theory of Special Relativity. As with much of the other material in this book, these pointers have been both prophetic of future developments and suggestive of fruitful lines of research and enquiry.\n\nThe book provided a foundation for research into electronic engineering, computing (both analog and digital), servomechanisms, automation, telecommunications and neuroscience. It also created widespread public debates on the technical, philosophical and sociological issues it discussed. And it inspired a wide range of books on various subjects peripherally related to its content.\n\nThe book introduced the word 'cybernetics' itself into public discourse.\n\nMaxwell Maltz titled his pioneering self-development work \"Psycho-Cybernetics\"\" in reference to the process of steering oneself towards a pre-defined goal by making corrections to behaviour. Much of the personal development industry and the Human potential movement is said to be derived from Maltz's work.\n\n\"Cybernetics\" became a surprise bestseller and was widely read beyond the technical audience that Wiener had expected. In response he wrote \"The Human Use of Human Beings\" in which he further explored the social and psychological implications in a format more suited to the non-technical reader.\n\nIn 1954, Marie Neurath produced a children's book \"Machines which seem to Think\" , which introduced the concepts of \"Cybernetics\", control systems and negative feedback in an accessible format.\n"}
{"id": "37647499", "url": "https://en.wikipedia.org/wiki?curid=37647499", "title": "Diamond operator", "text": "Diamond operator\n\nIn number theory, the diamond operators 〈\"d\"〉 are operators acting on the space of modular forms for the group Γ(\"N\"), given by the action of a matrix () in Γ(\"N\"). The diamond operators form an abelian group and commute with the Hecke operators.\n\nIn Unicode, the diamond operator is represented by the character .\n"}
{"id": "1864484", "url": "https://en.wikipedia.org/wiki?curid=1864484", "title": "Enumerator polynomial", "text": "Enumerator polynomial\n\nIn coding theory, the weight enumerator polynomial of a binary linear code specifies the number of words of each possible Hamming weight.\n\nLet formula_1 be a binary linear code length formula_2. The weight distribution is the sequence of numbers\n\ngiving the number of codewords \"c\" in \"C\" having weight \"t\" as \"t\" ranges from 0 to \"n\". The weight enumerator is the bivariate polynomial\n\n\nDenote the dual code of formula_1 by\n\n(where formula_11 denotes the vector dot product and which is taken over formula_12).\n\nThe MacWilliams identity states that\n\nThe identity is named after Jessie MacWilliams.\n\nThe distance distribution or inner distribution of a code \"C\" of size \"M\" and length \"n\" is the sequence of numbers\n\nwhere \"i\" ranges from 0 to \"n\". The distance enumerator polynomial is\n\nand when \"C\" is linear this is equal to the weight enumerator.\n\nThe outer distribution of \"C\" is the 2-by-\"n\"+1 matrix \"B\" with rows indexed by elements of GF(2) and columns indexed by integers 0...\"n\", and entries\n\nThe sum of the rows of \"B\" is \"M\" times the inner distribution vector (\"A\"...,\"A\").\n\nA code \"C\" is regular if the rows of \"B\" corresponding to the codewords of \"C\" are all equal.\n\n"}
{"id": "48706788", "url": "https://en.wikipedia.org/wiki?curid=48706788", "title": "Erica Klarreich", "text": "Erica Klarreich\n\nErica Gail Klarreich is an American mathematician, journalist and science popularizer.\n\nAs a mathematician, Klarreich is known for her theorem in geometric topology that states that the boundary of the curve complex is homeomorphic to the space of ending laminations.\n\nAs a science writer, her work appears in publications such as \"Nature\", \"Scientific American\", \"New Scientist\", and \"Quanta Magazine\".\n\nKlarreich's father was a professor of mathematics, and her mother was a mathematics teacher.\n\nKlarreich obtained her Ph.D. in mathematics under the guidance of Yair Nathan Minsky at Stony Brook University in 1997.\n\n\n"}
{"id": "43806809", "url": "https://en.wikipedia.org/wiki?curid=43806809", "title": "Esenin-Volpin's theorem", "text": "Esenin-Volpin's theorem\n\nIn mathematics, Esenin-Volpin's theorem states that weight of an infinite compact dyadic space is the supremum of the weights of its points.\nIt was introduced by . It was generalized by and .\n\n"}
{"id": "16703428", "url": "https://en.wikipedia.org/wiki?curid=16703428", "title": "Euler–Poisson–Darboux equation", "text": "Euler–Poisson–Darboux equation\n\nIn mathematics, the Euler–Poisson–Darboux equation is the partial differential equation\n\nThis equation is named for Siméon Poisson, Leonhard Euler, and Gaston Darboux. It plays an important role in solving the classical wave equation.\n\nThis equation is related to\n\nby formula_3, formula_4, where formula_5 and some sources quote this equation when referring to the Euler–Poisson–Darboux equation.\n"}
{"id": "53270578", "url": "https://en.wikipedia.org/wiki?curid=53270578", "title": "European Study Groups with Industry", "text": "European Study Groups with Industry\n\nA European Study Group with Industry (ESGI) is usually a week-long meeting where applied mathematicians work on problems presented by industry and research centres. The aim of the meeting is to solve or at least make progress on the problems.\n\nThe study group concept originated in Oxford, in 1968 (initiated by Leslie Fox and Alan Tayler). Subsequently, the format was adopted in other European countries to form ESGIs. Currently, with a variety of names, they appear in the same or a similar format throughout the world. More specific topics have also formed the subject of focussed meetings, such as the environment, medicine and agriculture.\n\nProblems successfully tackled at study groups are discussed in a number of textbooks as well as a collection of case studies, European Success Stories in Industrial Mathematics. A guide for organising and running study groups is provided by the .\n\nA European Study Group with Industry or ESGI is a type of workshop where mathematicians work on problems presented by industry representatives. The meetings typically last five days, from Monday to Friday. On the Monday morning the industry representatives present problems of current interest to an audience of applied mathematicians. Subsequently, the mathematicians split into working groups to investigate the suggested topics. On the Friday solutions and results are presented back to the industry representative. After the meeting a report is prepared for the company, detailing the progress made and usually with suggestions for further work or experiments.\n\nThe original Study Groups with Industry started in Oxford in 1968. The format provided a method for initiating interaction between universities and private industry which often led to further collaboration, student projects and new fields of research (many advances in the field of free or moving boundary problems are attributed to the industrial case studies of the 1970s.). Study groups were later adopted in other countries, starting in Europe and then spreading throughout the world. The subject areas have also diversified, for example the Mathematics in Medicine Study Groups, Mathematics in the Plant Sciences Study Groups, the environment, uncertainty quantification and agriculture.\n\nThe academics work on the problems for free. The following have been given as motivation for this work:\n\n\nA number of reasons have also been quoted for companies to attend ESGIs:\n\n\nESGIs are currently an activity of the European Consortium for Mathematics in Industry. Their ESGI webpage contains details of European meetings and contact details for prospective industry or academics participants. The current co-ordinator of the ESGIs is Prof. Tim Myers of the Centre de Recerca Matemática, Barcelona. Between 2015 and 2019 ESGIs are eligible for funding through the COST network MI-Net (Maths for Industry Network).\n\nPast European meetings are listed on the European Consortium for Mathematics in Industry website. International meetings are covered by the Mathematics in Industry Information Service.\n\nRecent ESGIs include:\n\n\nAs well as being held throughout Europe, annual study groups take place in Australia, Brazil, Canada, India, New Zealand, US, Russia and South Africa. A site dedicated solely to Dutch study groups may be found here Dutch ESGI. Information on past and upcoming meetings throughout the world may be found on the Mathematics in Industry Information Service website.\n\nThere are many books on mathematical modelling, a number of them containing problems arising from ESGIs or other study groups from around the world, examples include:\n\nThe book \"European Success Stories in Industrial Mathematics\" contains brief descriptions of a wide variety of industrial mathematics case studies. The Mathematics in Industry Information Service contains a large repository of past reports from study groups throughout the world.\n\nA guide for organising and running study groups, the ESGI Handbook, has been developed by the Mathematics for Industry Network.\n"}
{"id": "27633333", "url": "https://en.wikipedia.org/wiki?curid=27633333", "title": "Fekete–Szegő inequality", "text": "Fekete–Szegő inequality\n\nIn mathematics, the Fekete–Szegő inequality is an inequality for the coefficients of univalent analytic functions found by , related to the Bieberbach conjecture. Finding similar estimates for other classes of functions is called the Fekete–Szegő problem.\n\nThe Fekete–Szegő inequality states that if\n\nis a univalent analytic function on the unit disk and 0 ≤ λ < 1, then\n"}
{"id": "23265859", "url": "https://en.wikipedia.org/wiki?curid=23265859", "title": "Hannan Medal", "text": "Hannan Medal\n\nThe Hannan Medal in the Mathematical Sciences is awarded every two years by the Australian Academy of Science to recognize achievements by Australians in the fields of pure mathematics, applied and computational mathematics, and statistical science.\n\nThis medal commemorates the work of the late Edward J. Hannan, FAA, for his achievements in time series analysis.\n\n\"Source:\"\n"}
{"id": "1109126", "url": "https://en.wikipedia.org/wiki?curid=1109126", "title": "Hasse–Minkowski theorem", "text": "Hasse–Minkowski theorem\n\nThe Hasse–Minkowski theorem is a fundamental result in number theory which states that two quadratic forms over a number field are equivalent if and only if they are equivalent \"locally at all places\", i.e. equivalent over every completion of the field (which may be real, complex, or p-adic). A related result is that a quadratic space over a number field is isotropic if and only if it is isotropic locally everywhere, or equivalently, that a quadratic form over a number field nontrivially represents zero if and only if this holds for all completions of the field. The theorem was proved in the case of the field of rational numbers by Hermann Minkowski and generalized to number fields by Helmut Hasse. The same statement holds even more generally for all global fields.\n\nThe importance of the Hasse–Minkowski theorem lies in the novel paradigm it presented for answering arithmetical questions: in order to determine whether an equation of a certain type has a solution in rational numbers, it is sufficient to test whether it has solutions over complete fields of real and \"p\"-adic numbers, where analytic considerations, such as Newton's method and its \"p\"-adic analogue, Hensel's lemma, apply. This is encapsulated in the idea of a local-global principle, which is one of the most fundamental techniques in arithmetic geometry.\n\nThe Hasse–Minkowski theorem reduces the problem of classifying quadratic forms over a number field \"K\" up to equivalence to the set of analogous but much simpler questions over local fields. Basic invariants of a nonsingular quadratic form are its dimension, which is a positive integer, and its discriminant modulo the squares in K, which is an element of the multiplicative group K/K. In addition, for every place \"v\" of \"K\", there is an invariant coming from the completion K. Depending on the choice of \"v\", this completion may be the real numbers R, the complex numbers C, or a p-adic number field, each of which has different kinds of invariants:\n\nThese invariants must satisfy some compatibility conditions: a parity relation (the sign of the discriminant must match the negative index of inertia) and a product formula (a local–global relation). Conversely, for every set of invariants satisfying these relations, there is a quadratic form over K with these invariants.\n\n"}
{"id": "670453", "url": "https://en.wikipedia.org/wiki?curid=670453", "title": "Helly family", "text": "Helly family\n\nIn combinatorics, a Helly family of order \"k\" is a family of sets such that any minimal subfamily with an empty intersection has \"k\" or fewer sets in it. Equivalently, every finite subfamily such that every formula_1-fold intersection is non-empty has non-empty total intersection.\n\nThe \"k\"-Helly property is the property of being a Helly family of order \"k\". These concepts are named after Eduard Helly (1884 - 1943); Helly's theorem on convex sets, which gave rise to this notion, states that convex sets in Euclidean space of dimension \"n\" are a Helly family of order \"n\" + 1. The number \"k\" is frequently omitted from these names in the case that \"k\" = 2.\n\n\nMore formally, a Helly family of order \"k\" is a set system (\"F\", \"E\"), with \"F\" a collection of subsets of \"E\", such that, for every finite \"G\" ⊆ \"F\" with \n\nwe can find \"H\" ⊆ \"G\" such that \n\nand \n\nIn some cases, the same definition holds for every subcollection \"G\", regardless of finiteness. However, this is a more restrictive condition. For instance, the open intervals of the real line satisfy the Helly property for finite subcollections, but not for infinite subcollections: the intervals (0,1/\"i\") (for \"i\" = 0, 1, 2, ...) have pairwise nonempty intersections, but have an empty overall intersection.\n\nIf a family of sets is a Helly family of order \"k\", that family is said to have Helly number \"k\". The Helly dimension of a metric space is one less than the Helly number of the family of metric balls in that space; Helly's theorem implies that the Helly dimension of a Euclidean space equals its dimension as a real vector space.\n\nThe Helly dimension of a subset S of a Euclidean space, such as a polyhedron, is one less than the Helly number of the family of translates of S. For instance, the Helly dimension of any hypercube is 1, even though such a shape may belong to a Euclidean space of much higher dimension.\n\nHelly dimension has also been applied to other mathematical objects. For instance defines the Helly dimension of a group (an algebraic structure formed by an invertible and associative binary operation) to be one less than the Helly number of the family of left cosets of the group.\n\nIf a family of nonempty sets has an empty intersection, its Helly number must be at least two, so the smallest \"k\" for which the \"k\"-Helly property is nontrivial is \"k\" = 2.\nThe 2-Helly property is also known as the Helly property. A 2-Helly family is also known as a Helly family.\n\nA convex metric space in which the closed balls have the 2-Helly property (that is, a space with Helly dimension 1, in the stronger variant of Helly dimension for infinite subcollections) is called injective or hyperconvex. The existence of the tight span allows any metric space to be embedded isometrically into a space with Helly dimension 1.\n"}
{"id": "23664973", "url": "https://en.wikipedia.org/wiki?curid=23664973", "title": "Higraph", "text": "Higraph\n\nA higraph is a diagramming object that formalizes relations into a visual structure, it was developed by David Harel in 1988. Higraphs extend mathematical graphs by including notions of depth and orthogonality. In particular, nodes in a higraph can contain other nodes inside them, creating a hierarchy. The idea was initially developed for applications to databases, knowledge representation, and the behavioral specification of complex concurrent systems using the higraph-based language of statecharts.\n\nHigraphs are widely used in industrial applications like UML. Recently they have been used by philosophers to formally study the use of diagrams in mathematical proofs and reasoning.\n\n"}
{"id": "11953", "url": "https://en.wikipedia.org/wiki?curid=11953", "title": "History of geometry", "text": "History of geometry\n\nGeometry (from the ; \"geo-\" \"earth\", \"-metron\" \"measurement\") arose as the field of knowledge dealing with spatial relationships. Geometry was one of the two fields of pre-modern mathematics, the other being the study of numbers (arithmetic).\n\nClassic geometry was focused in compass and straightedge constructions. Geometry was revolutionized by Euclid, who introduced mathematical rigor and the axiomatic method still in use today. His book, \"The Elements\" is widely considered the most influential textbook of all time, and was known to all educated people in the West until the middle of the 20th century.\n\nIn modern times, geometric concepts have been generalized to a high level of abstraction and complexity, and have been subjected to the methods of calculus and abstract algebra, so that many modern branches of the field are barely recognizable as the descendants of early geometry. (See Areas of mathematics and Algebraic geometry.)\n\nThe earliest recorded beginnings of geometry can be traced to early peoples, who discovered obtuse triangles in the ancient Indus Valley (see Harappan Mathematics), and ancient Babylonia (see Babylonian mathematics) from around 3000 BC. Early geometry was a collection of empirically discovered principles concerning lengths, angles, areas, and volumes, which were developed to meet some practical need in surveying, construction, astronomy, and various crafts. Among these were some surprisingly sophisticated principles, and a modern mathematician might be hard put to derive some of them without the use of calculus. For example, both the Egyptians and the Babylonians were aware of versions of the Pythagorean theorem about 1500 years before Pythagoras and the Indian Sulba Sutras around 800 B.C. contained the first statements of the theorem; the Egyptians had a correct formula for the volume of a frustum of a square pyramid;\n\nThe ancient Egyptians knew that they could approximate the area of a circle as follows:\n\nProblem 30 of the Ahmes papyrus uses these methods to calculate the area of a circle, according to a rule that the area is equal to the square of 8/9 of the circle's diameter. This assumes that is 4×(8/9)² (or 3.160493...), with an error of slightly over 0.63 percent. This value was slightly less accurate than the calculations of the Babylonians (25/8 = 3.125, within 0.53 percent), but was not otherwise surpassed until Archimedes' approximation of 211875/67441 = 3.14163, which had an error of just over 1 in 10,000.\n\nAhmes knew of the modern 22/7 as an approximation for , and used it to split a hekat, hekat x 22/x x 7/22 = hekat; however, Ahmes continued to use the traditional 256/81 value for for computing his hekat volume found in a cylinder.\n\nProblem 48 involved using a square with side 9 units. This square was cut into a 3x3 grid. The diagonal of the corner squares were used to make an irregular octagon with an area of 63 units. This gave a second value for of 3.111...\n\nThe two problems together indicate a range of values for between 3.11 and 3.16.\n\nProblem 14 in the Moscow Mathematical Papyrus gives the only ancient example finding the volume of a frustum of a pyramid, describing the correct formula:\n\nThe Babylonians may have known the general rules for measuring areas and volumes. They measured the circumference of a circle as three times the diameter and the area as one-twelfth the square of the circumference, which would be correct if \"π\" is estimated as 3. The volume of a cylinder was taken as the product of the base and the height, however, the volume of the frustum of a cone or a square pyramid was incorrectly taken as the product of the height and half the sum of the bases. The Pythagorean theorem was also known to the Babylonians. Also, there was a recent discovery in which a tablet used \"π\" as 3 and 1/8. The Babylonians are also known for the Babylonian mile, which was a measure of distance equal to about seven miles today. This measurement for distances eventually was converted to a time-mile used for measuring the travel of the Sun, therefore, representing time. There have been recent discoveries showing that ancient Babylonians may have discovered astronomical geometry nearly 1400 years before Europeans did.\n\nThe Indian Vedic period had a tradition of geometry, mostly expressed in the construction of elaborate altars.\nEarly Indian texts (1st millennium BC) on this topic include the \"Satapatha Brahmana\" and the \"Śulba Sūtras\".\nAccording to , the \"Śulba Sūtras\" contain \"the earliest extant verbal expression of the Pythagorean Theorem in the world, although it had already been known to the Old Babylonians.\" The diagonal rope (') of an oblong (rectangle) produces both which the flank (\"pārśvamāni\") and the horizontal (') <ropes> produce separately.\" \nThey contain lists of Pythagorean triples, which are particular cases of Diophantine equations.\nThey also contain statements (that with hindsight we know to be approximate) about squaring the circle and \"circling the square.\"\n\nThe \"Baudhayana Sulba Sutra\", the best-known and oldest of the \"Sulba Sutras\" (dated to the 8th or 7th century BC) contains examples of simple Pythagorean triples, such as: formula_2, formula_3, formula_4, formula_5, and formula_6 as well as a statement of the Pythagorean theorem for the sides of a square: \"The rope which is stretched across the diagonal of a square produces an area double the size of the original square.\" It also contains the general statement of the Pythagorean theorem (for the sides of a rectangle): \"The rope stretched along the length of the diagonal of a rectangle makes an area which the vertical and horizontal sides make together.\"\n\nAccording to mathematician S. G. Dani, the Babylonian cuneiform tablet Plimpton 322 written c. 1850 BC \"contains fifteen Pythagorean triples with quite large entries, including (13500, 12709, 18541) which is a primitive triple, indicating, in particular, that there was sophisticated understanding on the topic\" in Mesopotamia in 1850 BC. \"Since these tablets predate the Sulbasutras period by several centuries, taking into account the contextual appearance of some of the triples, it is reasonable to expect that similar understanding would have been there in India.\" Dani goes on to say:\n\nwould not correspond directly to the overall knowledge on the topic at that time. Since, unfortunately, no other contemporaneous sources have been found it may never be possible to settle this issue satisfactorily.\"\n\nIn all, three \"Sulba Sutras\" were composed. The remaining two, the \"Manava Sulba Sutra\" composed by Manava (fl. 750-650 BC) and the \"Apastamba Sulba Sutra\", composed by Apastamba (c. 600 BC), contained results similar to the \"Baudhayana Sulba Sutra\".\n\nFor the ancient Greek mathematicians, geometry was the crown jewel of their sciences, reaching a completeness and perfection of methodology that no other branch of their knowledge had attained. They expanded the range of geometry to many new kinds of figures, curves, surfaces, and solids; they changed its methodology from trial-and-error to logical deduction; they recognized that geometry studies \"eternal forms\", or abstractions, of which physical objects are only approximations; and they developed the idea of the \"axiomatic method\", still in use today.\n\nThales (635-543 BC) of Miletus (now in southwestern Turkey), was the first to whom deduction in mathematics is attributed. There are five geometric propositions for which he wrote deductive proofs, though his proofs have not survived. Pythagoras (582-496 BC) of Ionia, and later, Italy, then colonized by Greeks, may have been a student of Thales, and traveled to Babylon and Egypt. The theorem that bears his name may not have been his discovery, but he was probably one of the first to give a deductive proof of it. He gathered a group of students around him to study mathematics, music, and philosophy, and together they discovered most of what high school students learn today in their geometry courses. In addition, they made the profound discovery of incommensurable lengths and irrational numbers.\n\nPlato (427-347 BC) is a philosopher that is highly esteemed by the Greeks. There is a story that he had inscribed above the entrance to his famous school, \"Let none ignorant of geometry enter here.\" However, the story is considered to be untrue. Though he was not a mathematician himself, his views on mathematics had great influence. Mathematicians thus accepted his belief that geometry should use no tools but compass and straightedge – never measuring instruments such as a marked ruler or a protractor, because these were a workman’s tools, not worthy of a scholar. This dictum led to a deep study of possible compass and straightedge constructions, and three classic construction problems: how to use these tools to trisect an angle, to construct a cube twice the volume of a given cube, and to construct a square equal in area to a given circle. The proofs of the impossibility of these constructions, finally achieved in the 19th century, led to important principles regarding the deep structure of the real number system. Aristotle (384-322 BC), Plato’s greatest pupil, wrote a treatise on methods of reasoning used in deductive proofs (see Logic) which was not substantially improved upon until the 19th century.\n\nEuclid (c. 325-265 BC), of Alexandria, probably a student at the Academy founded by Plato, wrote a treatise in 13 books (chapters), titled \"The Elements of Geometry\", in which he presented geometry in an ideal axiomatic form, which came to be known as Euclidean geometry. The treatise is not a compendium of all that the Hellenistic mathematicians knew at the time about geometry; Euclid himself wrote eight more advanced books on geometry. We know from other references that Euclid’s was not the first elementary geometry textbook, but it was so much superior that the others fell into disuse and were lost. He was brought to the university at Alexandria by Ptolemy I, King of Egypt.\n\n\"The Elements\" began with definitions of terms, fundamental geometric principles (called \"axioms\" or \"postulates\"), and general quantitative principles (called \"common notions\") from which all the rest of geometry could be logically deduced. Following are his five axioms, somewhat paraphrased to make the English easier to read.\n\n\nConcepts, that are now understood as algebra, were expressed geometrically by Euclid, a method referred to as Greek geometric algebra.\n\nArchimedes (287-212 BC), of Syracuse, Sicily, when it was a Greek city-state, is often considered to be the greatest of the Greek mathematicians, and occasionally even named as one of the three greatest of all time (along with Isaac Newton and Carl Friedrich Gauss). Had he not been a mathematician, he would still be remembered as a great physicist, engineer, and inventor. In his mathematics, he developed methods very similar to the coordinate systems of analytic geometry, and the limiting process of integral calculus. The only element lacking for the creation of these fields was an efficient algebraic notation in which to express his concepts.\n\nAfter Archimedes, Hellenistic mathematics began to decline. There were a few minor stars yet to come, but the golden age of geometry was over. Proclus (410-485), author of \"Commentary on the First Book of Euclid\", was one of the last important players in Hellenistic geometry. He was a competent geometer, but more importantly, he was a superb commentator on the works that preceded him. Much of that work did not survive to modern times, and is known to us only through his commentary. The Roman Republic and Empire that succeeded and absorbed the Greek city-states produced excellent engineers, but no mathematicians of note.\n\nThe great Library of Alexandria was later burned. There is a growing consensus among historians that the Library of Alexandria likely suffered from several destructive events, but that the destruction of Alexandria's pagan temples in the late 4th century was probably the most severe and final one. The evidence for that destruction is the most definitive and secure. Caesar's invasion may well have led to the loss of some 40,000-70,000 scrolls in a warehouse adjacent to the port (as Luciano Canfora argues, they were likely copies produced by the Library intended for export), but it is unlikely to have affected the Library or Museum, given that there is ample evidence that both existed later.\n\nCivil wars, decreasing investments in maintenance and acquisition of new scrolls and generally declining interest in non-religious pursuits likely contributed to a reduction in the body of material available in the Library, especially in the 4th century. The Serapeum was certainly destroyed by Theophilus in 391, and the Museum and Library may have fallen victim to the same campaign.\n\nIn the Bakhshali manuscript, there is a handful of geometric problems (including problems about volumes of irregular solids). The Bakhshali manuscript also \"employs a decimal place value system with a dot for zero.\" Aryabhata's \"Aryabhatiya\" (499) includes the computation of areas and volumes.\n\nBrahmagupta wrote his astronomical work \"\" in 628. Chapter 12, containing 66 Sanskrit verses, was divided into two sections: \"basic operations\" (including cube roots, fractions, ratio and proportion, and barter) and \"practical mathematics\" (including mixture, mathematical series, plane figures, stacking bricks, sawing of timber, and piling of grain). In the latter section, he stated his famous theorem on the diagonals of a cyclic quadrilateral:\n\nBrahmagupta's theorem: If a cyclic quadrilateral has diagonals that are perpendicular to each other, then the perpendicular line drawn from the point of intersection of the diagonals to any side of the quadrilateral always bisects the opposite side.\n\nChapter 12 also included a formula for the area of a cyclic quadrilateral (a generalization of Heron's formula), as well as a complete description of rational triangles (\"i.e.\" triangles with rational sides and rational areas).\n\nBrahmagupta's formula: The area, \"A\", of a cyclic quadrilateral with sides of lengths \"a\", \"b\", \"c\", \"d\", respectively, is given by\n\nwhere \"s\", the semiperimeter, given by: formula_8\n\nBrahmagupta's Theorem on rational triangles: A triangle with rational sides formula_9 and rational area is of the form:\n\nfor some rational numbers formula_11 and formula_12.\n\nThe first definitive work (or at least oldest existent) on geometry in China was the \"Mo Jing\", the Mohist canon of the early philosopher Mozi (470-390 BC). It was compiled years after his death by his followers around the year 330 BC. Although the \"Mo Jing\" is the oldest existent book on geometry in China, there is the possibility that even older written material existed. However, due to the infamous Burning of the Books in a political maneuver by the Qin Dynasty ruler Qin Shihuang (r. 221-210 BC), multitudes of written literature created before his time were purged. In addition, the \"Mo Jing\" presents geometrical concepts in mathematics that are perhaps too advanced not to have had a previous geometrical base or mathematic background to work upon.\n\nThe \"Mo Jing\" described various aspects of many fields associated with physical science, and provided a small wealth of information on mathematics as well. It provided an 'atomic' definition of the geometric point, stating that a line is separated into parts, and the part which has no remaining parts (i.e. cannot be divided into smaller parts) and thus forms the extreme end of a line is a point. Much like Euclid's first and third definitions and Plato's 'beginning of a line', the \"Mo Jing\" stated that \"a point may stand at the end (of a line) or at its beginning like a head-presentation in childbirth. (As to its invisibility) there is nothing similar to it.\" Similar to the atomists of Democritus, the \"Mo Jing\" stated that a point is the smallest unit, and cannot be cut in half, since 'nothing' cannot be halved. It stated that two lines of equal length will always finish at the same place, while providing definitions for the \"comparison of lengths\" and for \"parallels\", along with principles of space and bounded space. It also described the fact that planes without the quality of thickness cannot be piled up since they cannot mutually touch. The book provided definitions for circumference, diameter, and radius, along with the definition of volume.\n\nThe Han Dynasty (202 BC-220 AD) period of China witnessed a new flourishing of mathematics. One of the oldest Chinese mathematical texts to present geometric progressions was the \"Suàn shù shū\" of 186 BC, during the Western Han era. The mathematician, inventor, and astronomer Zhang Heng (78-139 AD) used geometrical formulas to solve mathematical problems. Although rough estimates for pi (π) were given in the \"Zhou Li\" (compiled in the 2nd century BC), it was Zhang Heng who was the first to make a concerted effort at creating a more accurate formula for pi. Zhang Heng approximated pi as 730/232 (or approx 3.1466), although he used another formula of pi in finding a spherical volume, using the square root of 10 (or approx 3.162) instead. Zu Chongzhi (429-500 AD) improved the accuracy of the approximation of pi to between 3.1415926 and 3.1415927, with ⁄ (密率, Milü, detailed approximation) and ⁄ (约率, Yuelü, rough approximation) being the other notable approximation. In comparison to later works, the formula for pi given by the French mathematician Franciscus Vieta (1540-1603) fell halfway between Zu's approximations.\n\n\"The Nine Chapters on the Mathematical Art\", the title of which first appeared by 179 AD on a bronze inscription, was edited and commented on by the 3rd century mathematician Liu Hui from the Kingdom of Cao Wei. This book included many problems where geometry was applied, such as finding surface areas for squares and circles, the volumes of solids in various three-dimensional shapes, and included the use of the Pythagorean theorem. The book provided illustrated proof for the Pythagorean theorem, contained a written dialogue between of the earlier Duke of Zhou and Shang Gao on the properties of the right angle triangle and the Pythagorean theorem, while also referring to the astronomical gnomon, the circle and square, as well as measurements of heights and distances. The editor Liu Hui listed pi as 3.141014 by using a 192 sided polygon, and then calculated pi as 3.14159 using a 3072 sided polygon. This was more accurate than Liu Hui's contemporary Wang Fan, a mathematician and astronomer from Eastern Wu, would render pi as 3.1555 by using ⁄. Liu Hui also wrote of mathematical surveying to calculate distance measurements of depth, height, width, and surface area. In terms of solid geometry, he figured out that a wedge with rectangular base and both sides sloping could be broken down into a pyramid and a tetrahedral wedge. He also figured out that a wedge with trapezoid base and both sides sloping could be made to give two tetrahedral wedges separated by a pyramid. Furthermore, Liu Hui described Cavalieri's principle on volume, as well as Gaussian elimination. From the \"Nine Chapters\", it listed the following geometrical formulas that were known by the time of the Former Han Dynasty (202 BCE–9 CE).\n\nAreas for the\n\n\nVolumes for the\n\n\nContinuing the geometrical legacy of ancient China, there were many later figures to come, including the famed astronomer and mathematician Shen Kuo (1031-1095 CE), Yang Hui (1238-1298) who discovered Pascal's Triangle, Xu Guangqi (1562-1633), and many others.\n\nBy the beginning of the 9th century, the \"Islamic Golden Age\" flourished, the establishment of the House of Wisdom in Baghdad marking a separate tradition of science in the medieval Islamic world, building not only Hellenistic but also on Indian sources.\n\nAlthough the Islamic mathematicians are most famed for their work on algebra, number theory and number systems, they also made considerable contributions to geometry, trigonometry and mathematical astronomy, and were responsible for the development of algebraic geometry.\n\nAl-Mahani (born 820) conceived the idea of reducing geometrical problems such as duplicating the cube to problems in algebra. Al-Karaji (born 953) completely freed algebra from geometrical operations and replaced them with the arithmetical type of operations which are at the core of algebra today.\n\nThābit ibn Qurra (known as Thebit in Latin) (born 836) contributed to a number of areas in mathematics, where he played an important role in preparing the way for such important mathematical discoveries as the extension of the concept of number to (positive) real numbers, integral calculus, theorems in spherical trigonometry, analytic geometry, and non-Euclidean geometry. In astronomy Thabit was one of the first reformers of the Ptolemaic system, and in mechanics he was a founder of statics. An important geometrical aspect of Thabit's work was his book on the composition of ratios. In this book, Thabit deals with arithmetical operations applied to ratios of geometrical quantities. The Greeks had dealt with geometric quantities but had not thought of them in the same way as numbers to which the usual rules of arithmetic could be applied. By introducing arithmetical operations on quantities previously regarded as geometric and non-numerical, Thabit started a trend which led eventually to the generalisation of the number concept.\n\nIn some respects, Thabit is critical of the ideas of Plato and Aristotle, particularly regarding motion. It would seem that here his ideas are based on an acceptance of using arguments concerning motion in his geometrical arguments. Another important contribution Thabit made to geometry was his generalization of the Pythagorean theorem, which he extended from special right triangles to all triangles in general, along with a general proof.\n\nIbrahim ibn Sinan ibn Thabit (born 908), who introduced a method of integration more general than that of Archimedes, and al-Quhi (born 940) were leading figures in a revival and continuation of Greek higher geometry in the Islamic world. These mathematicians, and in particular Ibn al-Haytham, studied optics and investigated the optical properties of mirrors made from conic sections.\n\nAstronomy, time-keeping and geography provided other motivations for geometrical and trigonometrical research. For example, Ibrahim ibn Sinan and his grandfather Thabit ibn Qurra both studied curves required in the construction of sundials. Abu'l-Wafa and Abu Nasr Mansur both applied spherical geometry to astronomy.\n\nA 2007 paper in the journal \"Science\" suggested that girih tiles possessed properties consistent with self-similar fractal quasicrystalline tilings such as the Penrose tilings.\n\nThe transmission of the Greek Classics to medieval Europe via the Arabic literature of the 9th to 10th century \"Islamic Golden Age\" began in the 10th century and culminated in the Latin translations of the 12th century.\nA copy of Ptolemy's \"Almagest\" was brought back to Sicily by Henry Aristippus (d. 1162), as a gift from the Emperor to King William I (r. 1154–1166). An anonymous student at Salerno travelled to Sicily and translated the \"Almagest\" as well as several works by Euclid from Greek to Latin. Although the Sicilians generally translated directly from the Greek, when Greek texts were not available, they would translate from Arabic. Eugenius of Palermo (d. 1202) translated Ptolemy's \"Optics\" into Latin, drawing on his knowledge of all three languages in the task.\nThe rigorous deductive methods of geometry found in Euclid's \"Elements of Geometry\" were relearned, and further development of geometry in the styles of both Euclid (Euclidean geometry) and Khayyam (algebraic geometry) continued, resulting in an abundance of new theorems and concepts, many of them very profound and elegant.\n\nAdvances in the treatment of perspective were made in Renaissance art of the 14th to 15th century which went beyond what had been achieved in antiquity. \nIn Renaissance architecture of the \"Quattrocento\", concepts of architectural order were explored and rules were formulated. A prime example of is the Basilica di San Lorenzo in Florence by Filippo Brunelleschi (1377–1446).\n\nIn c. 1413 Filippo Brunelleschi demonstrated the geometrical method of perspective, used today by artists, by painting the outlines of various Florentine buildings onto a mirror. \nSoon after, nearly every artist in Florence and in Italy used geometrical perspective in their paintings, notably Masolino da Panicale and Donatello. Melozzo da Forlì first used the technique of upward foreshortening (in Rome, Loreto, Forlì and others), and was celebrated for that. Not only was perspective a way of showing depth, it was also a new method of composing a painting. Paintings began to show a single, unified scene, rather than a combination of several.\n\nAs shown by the quick proliferation of accurate perspective paintings in Florence, Brunelleschi likely understood (with help from his friend the mathematician Toscanelli), but did not publish, the mathematics behind perspective. Decades later, his friend Leon Battista Alberti wrote \"De pictura\" (1435/1436), a treatise on proper methods of showing distance in painting based on Euclidean geometry. Alberti was also trained in the science of optics through the school of Padua and under the influence of Biagio Pelacani da Parma who studied Alhazen's \"Optics'.\n\nPiero della Francesca elaborated on Della Pittura in his \"De Prospectiva Pingendi\" in the 1470s. Alberti had limited himself to figures on the ground plane and giving an overall basis for perspective. Della Francesca fleshed it out, explicitly covering solids in any area of the picture plane. Della Francesca also started the now common practice of using illustrated figures to explain the mathematical concepts, making his treatise easier to understand than Alberti's. Della Francesca was also the first to accurately draw the Platonic solids as they would appear in perspective.\n\nPerspective remained, for a while, the domain of Florence. Jan van Eyck, among others, was unable to create a consistent structure for the converging lines in paintings, as in London's The Arnolfini Portrait, because he was unaware of the theoretical breakthrough just then occurring in Italy. However he achieved very subtle effects by manipulations of scale in his interiors. Gradually, and partly through the movement of academies of the arts, the Italian techniques became part of the training of artists across Europe, and later other parts of the world.\nThe culmination of these Renaissance traditions finds its ultimate synthesis in the research of the architect, geometer, and optician Girard Desargues on perspective, optics and projective geometry.\n\nThe \"Vitruvian Man\" by Leonardo da Vinci(c. 1490) depicts a man in two superimposed positions with his arms and legs apart and inscribed in a circle and square. The drawing is based on the correlations of ideal human proportions with geometry described by the ancient Roman architect Vitruvius in Book III of his treatise \"De Architectura\".\n\nIn the early 17th century, there were two important developments in geometry. The first and most important was the creation of analytic geometry, or geometry with coordinates and equations, by René Descartes (1596–1650) and Pierre de Fermat (1601–1665). This was a necessary precursor to the development of calculus and a precise quantitative science of physics. The second geometric development of this period was the systematic study of projective geometry by Girard Desargues (1591–1661). Projective geometry is the study of geometry without measurement, just the study of how points align with each other. There had been some early work in this area by Hellenistic geometers, notably Pappus (c. 340). The greatest flowering of the field occurred with Jean-Victor Poncelet (1788–1867).\n\nIn the late 17th century, calculus was developed independently and almost simultaneously by Isaac Newton (1642–1727) and Gottfried Wilhelm Leibniz (1646–1716). This was the beginning of a new field of mathematics now called analysis. Though not itself a branch of geometry, it is applicable to geometry, and it solved two families of problems that had long been almost intractable: finding tangent lines to odd curves, and finding areas enclosed by those curves. The methods of calculus reduced these problems mostly to straightforward matters of computation.\n\nThe very old problem of proving Euclid’s Fifth Postulate, the \"Parallel Postulate\", from his first four postulates had never been forgotten. Beginning not long after Euclid, many attempted demonstrations were given, but all were later found to be faulty, through allowing into the reasoning some principle which itself had not been proved from the first four postulates. Though Omar Khayyám was also unsuccessful in proving the parallel postulate, his criticisms of Euclid's theories of parallels and his proof of properties of figures in non-Euclidean geometries contributed to the eventual development of non-Euclidean geometry. By 1700 a great deal had been discovered about what can be proved from the first four, and what the pitfalls were in attempting to prove the fifth. Saccheri, Lambert, and Legendre each did excellent work on the problem in the 18th century, but still fell short of success. In the early 19th century, Gauss, Johann Bolyai, and Lobatchewsky, each independently, took a different approach. Beginning to suspect that it was impossible to prove the Parallel Postulate, they set out to develop a self-consistent geometry in which that postulate was false. In this they were successful, thus creating the first non-Euclidean geometry. By 1854, Bernhard Riemann, a student of Gauss, had applied methods of calculus in a ground-breaking study of the intrinsic (self-contained) geometry of all smooth surfaces, and thereby found a different non-Euclidean geometry. This work of Riemann later became fundamental for Einstein's theory of relativity.\n\nIt remained to be proved mathematically that the non-Euclidean geometry was just as self-consistent as Euclidean geometry, and this was first accomplished by Beltrami in 1868. With this, non-Euclidean geometry was established on an equal mathematical footing with Euclidean geometry.\n\nWhile it was now known that different geometric theories were mathematically possible, the question remained, \"Which one of these theories is correct for our physical space?\" The mathematical work revealed that this question must be answered by physical experimentation, not mathematical reasoning, and uncovered the reason why the experimentation must involve immense (interstellar, not earth-bound) distances. With the development of relativity theory in physics, this question became vastly more complicated.\n\nAll the work related to the Parallel Postulate revealed that it was quite difficult for a geometer to separate his logical reasoning from his intuitive understanding of physical space, and, moreover, revealed the critical importance of doing so. Careful examination had uncovered some logical inadequacies in Euclid's reasoning, and some unstated geometric principles to which Euclid sometimes appealed. This critique paralleled the crisis occurring in calculus and analysis regarding the meaning of infinite processes such as convergence and continuity. In geometry, there was a clear need for a new set of axioms, which would be complete, and which in no way relied on pictures we draw or on our intuition of space. Such axioms, now known as Hilbert's axioms, were given by David Hilbert in 1894 in his dissertation \"Grundlagen der Geometrie\" (\"Foundations of Geometry\"). Some other complete sets of axioms had been given a few years earlier, but did not match Hilbert's in economy, elegance, and similarity to Euclid's axioms.\n\nIn the mid-18th century, it became apparent that certain progressions of mathematical reasoning recurred when similar ideas were studied on the number line, in two dimensions, and in three dimensions. Thus the general concept of a metric space was created so that the reasoning could be done in more generality, and then applied to special cases. This method of studying calculus- and analysis-related concepts came to be known as analysis situs, and later as topology. The important topics in this field were properties of more general figures, such as connectedness and boundaries, rather than properties like straightness, and precise equality of length and angle measurements, which had been the focus of Euclidean and non-Euclidean geometry. Topology soon became a separate field of major importance, rather than a sub-field of geometry or analysis.\n\nDevelopments in algebraic geometry included the study of curves and surfaces over finite fields as demonstrated by the works of among others André Weil, Alexander Grothendieck, and Jean-Pierre Serre as well as over the real or complex numbers. Finite geometry itself, the study of spaces with only finitely many points, found applications in coding theory and cryptography. With the advent of the computer, new disciplines such as computational geometry or digital geometry deal with geometric algorithms, discrete representations of geometric data, and so forth.\n\n\n\n\n"}
{"id": "589307", "url": "https://en.wikipedia.org/wiki?curid=589307", "title": "Jean-Christophe Yoccoz", "text": "Jean-Christophe Yoccoz\n\nJean-Christophe Yoccoz (May 29, 1957 – September 3, 2016) was a French mathematician. He was awarded a Fields Medal in 1994, for his work on dynamical systems.\n\nYoccoz attended the Lycée Louis-le-Grand, during which time he was a silver medalist at the 1973 International Mathematical Olympiad and a gold medalist in 1974. He entered the École Normale Supérieure in 1975, and completed an agrégation in mathematics in 1977. After completing military service in Brazil, he completed his Ph.D. under Michael Herman in 1985 at , which is a research unit jointly operated by the French National Center for Scientific Research (CNRS) and Ecole polytechnique. He took up a position at the University of Paris-Sud in 1987, and became a professor at the Collège de France in 1997, where he remained until his death. He was a member of Bourbaki.\n\nYoccoz won the Salem Prize in 1988.\nHe was an invited speaker at the International Congress of Mathematicians in 1990 at Kyoto, \nand was awarded the Fields Medal at the International Congress of Mathematicians in 1994 in Zürich. He joined the French Academy of Sciences and Brazilian Academy of Sciences in 1994, became a chevalier in the French Legion of Honor in 1995, and was awarded the Grand Cross of the Brazilian National Order of Scientific Merit in 1998.\n\nYoccoz's worked on the theory of dynamical systems, his contributions include advances to KAM theory, and the introduction of the method of Yoccoz puzzles, a combinatorial technique which proved useful to the study of Julia sets.\n"}
{"id": "15185443", "url": "https://en.wikipedia.org/wiki?curid=15185443", "title": "Korn's inequality", "text": "Korn's inequality\n\nIn mathematical analysis, Korn's inequality is an inequality concerning the gradient of a vector field that generalizes the following classical theorem: if the gradient of a vector field is skew-symmetric at every point, then the gradient must be equal to a constant skew-symmetric matrix. Korn's theorem is a quantitative version of this statement, which intuitively says that if the gradient of a vector field is on average not far from the space of skew-symmetric matrices, then the gradient must not be far from a \"particular\" skew-symmetric matrix. The statement that Korn's inequality generalizes thus arises as a special case of rigidity.\n\nIn (linear) elasticity theory, the symmetric part of the gradient is a measure of the strain that an elastic body experiences when it is deformed by a given vector-valued function. The inequality is therefore an important tool as an a priori estimate in linear elasticity theory.\n\nLet be an open, connected domain in -dimensional Euclidean space , . Let be the Sobolev space of all vector fields on that, along with their (first) weak derivatives, lie in the Lebesgue space . Denoting the partial derivative with respect to the \"i\" component by , the norm in is given by\n\nThen there is a constant , known as the Korn constant of , such that, for all ,\n\nwhere denotes the symmetrized gradient given by\n\nInequality is known as Korn's inequality.\n\n\n"}
{"id": "3772909", "url": "https://en.wikipedia.org/wiki?curid=3772909", "title": "Large deviations theory", "text": "Large deviations theory\n\nIn probability theory, the theory of large deviations concerns the asymptotic behaviour of remote tails of sequences of probability distributions. While some basic ideas of the theory can be traced to Laplace, the formalization started with insurance mathematics, namely ruin theory with Cramér and Lundberg. A unified formalization of large deviation theory was developed in 1966, in a paper by Varadhan. Large deviations theory formalizes the heuristic ideas of \"concentration of measures\" and widely generalizes the notion of convergence of probability measures.\n\nRoughly speaking, large deviations theory concerns itself with the exponential decline of the probability measures of certain kinds of extreme or \"tail\" events.\n\nConsider a sequence of independent tosses of a fair\ncoin. The possible outcomes could be heads or tails. Let us denote the possible outcome of the i-th trial by\nformula_1, where we encode head as 1 and tail as 0. Now let formula_2 denote the mean value after formula_3 trials, namely\nThen formula_2 lies between 0 and 1. From the law of large numbers (and also from our experience) we know that as N grows, the distribution of formula_2 converges to formula_7 (the expectation value of a single coin toss) almost surely.\n\nMoreover, by the central limit theorem, we know that formula_2 is approximately normally distributed for large formula_9. The central limit theorem can provide more detailed information about the behavior of formula_2 than the law of large numbers. For example, we can approximately find a tail probability of formula_2, formula_12, that\nformula_2 is greater than formula_14, for a fixed value of formula_3. However, the approximation by the CLT may not be accurate if formula_16 is far from formula_17 unless formula_9 is sufficiently large. Also, it does not provide information about the convergence of the tail probabilities as formula_19. However, the large deviation theory can provide answers for such problems.\n\nLet us make this statement more precise. For a given value formula_20, let us compute the tail probability formula_12. Define\nNote that the function formula_23 is a convex, nonnegative function that is zero at \"x=1/2\" and increases as you move to \"x=1\". It is the negative of the Bernoulli entropy with \"p=1/2\"; that it's appropriate for coin tosses follows from the asymptotic equipartition property applied to a Bernoulli trial. Then by Chernoff's inequality, it can be shown that formula_24. This bound is rather sharp, in the sense that formula_23 cannot be replaced with a larger number which would yield a strict inequality for all positive formula_3 (However, the exponential bound can still be reduced by a subexponential factor on the order of formula_27; this follows from the Stirling approximation applied to the binomial coefficient appearing in the Bernoulli distribution.) Hence, we obtain the following result:\nThe probability formula_12 decays exponentially as formula_3 grows to infinity, at a rate depending on x. This formula approximates any tail probability of the sample mean of i.i.d. variables and gives its convergence as the number of samples increases.\n\nIn the above example of coin-tossing we explicitly assumed that each toss is an\nindependent trial, and the probability of getting head or tail is always the same.\n\nLet formula_31 be independent and identically distributed (i.i.d.) random variables whose common distribution satisfies a certain growth condition. Then the following limit exists:\n\nFunction formula_34 is called the \"rate function\" or \"Cramér function\" or sometimes the \"entropy function\".\n\nThe above-mentioned limit means that for large formula_9,\n\nwhich is the basic result of large deviations theory.\n\nIf we know the probability distribution of formula_37, an explicit expression for the rate function can be obtained. This is given by a Legendre–Fenchel transformation,\n\nwhere\n\nis called the cumulant generating function (CGF) and formula_40 denotes the mathematical expectation.\n\nIf formula_37 follows a normal distribution, the rate function becomes a parabola with its apex at the mean of the normal distribution.\n\nIf formula_42 is a Markov chain, the variant of the basic large deviations result stated above may hold.\n\nGiven a Polish space formula_43 let formula_44 be a sequence of Borel probability measures on formula_43, let formula_46 be a sequence of positive real numbers such that formula_47, and finally let formula_48 be a lower semicontinuous functional on formula_43. The sequence formula_44 is said to satisfy a large deviation principle with \"speed\" formula_51 and \"rate\" formula_52 if, and only if, for each Borel measurable set formula_53,\n\nwhere formula_55 and formula_56 denote respectively the closure and interior of formula_57.\n\nThe first rigorous results concerning large deviations are due to the Swedish mathematician\nHarald Cramér, who applied them to model the insurance business. From the point\nof view of an insurance company, the earning is at a constant rate per month\n(the monthly premium) but the claims come randomly. For the company to be successful\nover a certain period of time (preferably many months), the total earning should\nexceed the total claim. Thus to estimate the premium you have to ask the following\nquestion : \"What should we choose as the premium formula_58 such that over\nformula_3 months the total claim formula_60 should\nbe less than formula_61 ? \" This is clearly the same question asked by\nthe large deviations theory. Cramér gave a solution to this question for i.i.d. random variables, where the rate function is expressed as a power series.\n\nA very incomplete list of mathematicians who have made important advances would\ninclude Petrov, Sanov, \nS.R.S. Varadhan (who has won the Abel prize for his contribution to the theory), D. Ruelle, O.E. Lanford, Amir Dembo, and Ofer Zeitouni.\n\nPrinciples of large deviations may be effectively applied to gather information out of a probabilistic model. Thus, theory of large deviations finds its applications in information theory and risk management. In physics, the best known application of large deviations theory arise in thermodynamics and statistical mechanics (in connection with relating entropy with rate function).\n\nThe rate function is related to the entropy in statistical mechanics. This can be heuristically seen in the following way. In statistical mechanics the entropy of a particular macro-state is related to the number of micro-states which corresponds to this macro-state. In our coin tossing example the mean value formula_2 could designate a particular macro-state. And the particular sequence of heads and tails which gives rise to a particular value of formula_2 constitutes a particular micro-state. Loosely speaking a macro-state having a higher number of micro-states giving rise to it, has higher entropy. And a state with higher entropy has a higher chance of being realised in actual experiments. The macro-state with mean value of 1/2 (as many heads as tails) has the highest number of micro-states giving rise to it and it is indeed the state with the highest entropy. And in most practical situations\nwe shall indeed obtain this macro-state for large numbers of trials. The \"rate function\" on the other hand measures the probability of appearance of a particular macro-state. The smaller the rate function the higher is the chance of a macro-state appearing. In our coin-tossing the value of the \"rate function\" for mean value equal to 1/2 is zero. In this way one can see the \"rate function\" as the negative of the \"entropy\".\n\nThere is a relation between the \"rate function\" in large deviations theory and the Kullback–Leibler divergence (see Sanov and \nNovak, ch. 14.5).\n\nIn a special case, large deviations are closely related to the concept of Gromov–Hausdorff limits.\n\n\n\n"}
{"id": "44398234", "url": "https://en.wikipedia.org/wiki?curid=44398234", "title": "Link-centric preferential attachment", "text": "Link-centric preferential attachment\n\nIn mathematical modeling of social networks, link-centric preferential attachment\nis a node's propensity to re-establish links to nodes it has previously been in contact with in time-varying networks. This preferential attachment model relies on nodes keeping memory of previous neighbors up to the current time.\n\nIn real social networks individuals exhibit a tendency to re-connect with past contacts (ex. family, friends, co-workers, etc.) rather than strangers. In 1970, Mark Granovetter examined this behaviour in the social networks of a group of workers and identified tie strength, a characteristic of social ties describing the frequency of contact between two individuals. From this comes the idea of strong and weak ties, where an individual's strong ties are those she has come into frequent contact with. Link-centric preferential attachment aims to explain the mechanism behind strong and weak ties as a stochastic reinforcement process for old ties in agent-based modeling where nodes have long-term memory.\n\nIn a simple model for this mechanism, a node's propensity to establish a new link can be characterized solely by formula_1, the number of contacts it has had in the past. The probability for a node with n social ties to establish a new social tie could then be simply given by\n\nwhere \"c\" is an offset constant. The probability for a node to re-connect with old ties is then \nFigure 1. shows an example of this process: in the first step nodes A and C connect to node B, giving B a total of two social ties. With \"c\" = 1, in the next step B has a probability \"P\"(2) = 1/(2 + 1) = 1/3 to create a new tie with D, whereas the probability to reconnect with A or C is twice that at 2/3.\n\nMore complex models may take into account other variables, such as frequency of contact, contact and intercontact duration, as well as short term memory effects.\n\nEffects on the spreading of contagions / weakness of strong ties\n\nUnderstanding the evolution of a network's structure and how it can influence dynamical processes has become an important part of modeling the spreading of contagions. In models of social and biological contagion spreading on time-varying networks link-centric preferential attachment can alter the spread of the contagion to the entire population. Compared to the classic rumour spreading process where nodes are memory-less, link-centric preferential attachment can cause not only a slower spread of the contagion but also one less diffuse. In these models an infected node's chances of connecting to new contacts diminishes as their size of their social circle formula_1 grows leading to a limiting effect on the growth of n. The result is strong ties with a node's early contacts and consequently the weakening of the diffusion of the contagion.\n\n"}
{"id": "31528482", "url": "https://en.wikipedia.org/wiki?curid=31528482", "title": "Lobachevsky (song)", "text": "Lobachevsky (song)\n\n\"Lobachevsky\" is a humorous song by Tom Lehrer, referring to the mathematician Nikolai Lobachevsky.\nAccording to Lehrer, the song is \"not intended as a slur on [Lobachevsky's] character\" and the name was chosen \"solely for prosodic reasons\".\n\nIn the introduction, Lehrer describes the song as an adaptation of a routine that Danny Kaye did to honor the Russian actor Constantin Stanislavski. (The Danny Kaye routine is sung from the perspective of a famous Russian actor who learns and applies Stanislavski's secret to method acting: \"Suffer.\") Lehrer sings the song from the point of a view of a preeminent Russian mathematician who learns, from Lobachevsky, that plagiarism is the secret of success in mathematics (though adding \"only be sure always to call it please 'research'\"). The narrator later uses this strategy to get a paper published ahead of a rival, then to write a book and earn a fortune selling the movie rights.\n\nLehrer wrote that he did not know Russian. In the song he quotes two book reviews in Russian; the first is a long sentence that he then translates succinctly as \"It stinks\". The second, a different but equally long sentence, is also translated as \"It stinks.\" The actual text of these sentences bear no relation to academics: the first phrase quotes Mussorgsky's \"Song of the Flea\": \"Once there was a king who had a pet flea.\" The second references a Russian joke: \"Now I must go where even the Tsar goes on foot\" [the bathroom].\n\nThe song was first performed as part of \"The Physical Revue,\" a 1951–1952 musical revue by Lehrer and a few other professors. It is track 6 on \"Songs by Tom Lehrer\", which was re-released as part of \"Songs & More Songs by Tom Lehrer\" and \"The Remains of Tom Lehrer\". In this early version, Ingrid Bergman is named to star in the role of \"Hypotenuse\" in \"The Eternal Triangle\", a film purportedly based on the narrator's book. It was recorded again for \"Revisited (Tom Lehrer album)\", with Brigitte Bardot as Hypotenuse. A third recording is included in \"Tom Lehrer Discovers Australia (And Vice Versa)\", a live album recorded in Australia, featuring Marilyn Monroe as Hypotenuse. A fourth recording was made in 1966 when \"Songs by Tom Lehrer\" was reissued in stereo, with Doris Day playing Hypotenuse.\n\nThe song is frequently quoted, especially in works about plagiarism. Writing about it in \"Billboard\", Jim Bessman calls the song \"dazzlingly inventive in its shameless promotion of plagiarism\", calling out in particular a sequence in which Lehrer strings together rhymes from the names of ten Russian cities.\nMathematician Jordan Ellenberg has called it \"surely the greatest comic musical number of all time about mathematical publishing\".\n\n"}
{"id": "13244280", "url": "https://en.wikipedia.org/wiki?curid=13244280", "title": "Multiple discriminant analysis", "text": "Multiple discriminant analysis\n\nMultiple Discriminant Analysis (MDA) is a multivariate dimensionality reduction technique. It has been used to predict signals as diverse as neural memory traces and corporate failure.\n\nMDA is not directly used to perform classification. It merely supports classification by yielding a compressed signal amenable to classification. The method described in Duda et al. (2001) §3.8.3 projects the multivariate signal down to an \"M\"−1 dimensional space where \"M\" is the number of categories.\n\nMDA is useful because most classifiers are strongly affected by the curse of dimensionality. In other words, when signals are represented in very-high-dimensional spaces, the classifier's performance is catastrophically impaired by the overfitting problem. This problem is reduced by compressing the signal down to a lower-dimensional space as MDA does.\n\nMDA has been used to reveal neural codes.\n\n"}
{"id": "14566906", "url": "https://en.wikipedia.org/wiki?curid=14566906", "title": "Ordinal definable set", "text": "Ordinal definable set\n\nIn mathematical set theory, a set \"S\" is said to be ordinal definable if, informally, it can be defined in terms of a finite number of ordinals by a first order formula. Ordinal definable sets were introduced by .\n\nA drawback to this informal definition is that requires quantification over all first order formulas, which cannot be formalized in the language of set theory. However there is a different way of stating the definition that can be so formalized. In this approach, a set \"S\" is formally defined to be ordinal definable if there is some collection of ordinals \"α\"...\"α\" such that formula_1 and formula_2 can be defined as an element of formula_3 by a first-order formula φ taking α...α as parameters. Here formula_3 denotes the set indexed by the ordinal \"α\" in the von Neumann hierarchy of sets. In other words, \"S\" is the unique object such that φ(\"S\", α...α) holds with its quantifiers ranging over formula_3.\n\nThe class of all ordinal definable sets is denoted OD; it is not necessarily transitive, and need not be a model of ZFC because it might not satisfy the axiom of extensionality. A set is hereditarily ordinal definable if it is ordinal definable and all elements of its transitive closure are ordinal definable. The class of hereditarily ordinal definable sets is denoted by HOD, and is a transitive model of ZFC, with a definable well ordering. It is consistent with the axioms of set theory that all sets are ordinal definable, and so hereditarily ordinal definable. The assertion that this situation holds is referred to as V = OD or V = HOD. It follows from V = L, and is equivalent to the existence of a (definable) well-ordering of the universe. Note however that the formula expressing V = HOD need not hold true within HOD, as it is not absolute for models of set theory: within HOD, the interpretation of the formula for HOD may yield an even smaller inner model.\n\nHOD has been found to be useful in that it is an inner model that can accommodate essentially all known large cardinals. This is in contrast with the situation for core models, as core models have not yet been constructed that can accommodate supercompact cardinals, for example.\n\n"}
{"id": "44844703", "url": "https://en.wikipedia.org/wiki?curid=44844703", "title": "Orientation of a vector bundle", "text": "Orientation of a vector bundle\n\nIn mathematics, an orientation of a real vector bundle is a generalization of an orientation of a vector space; thus, given a real vector bundle π: \"E\" →\"B\", an orientation of \"E\" means: for each fiber \"E\", there is an orientation of the vector space \"E\" and one demands that each trivialization map (which is a bundle map)\nis fiberwise orientation-preserving, where R is given the standard orientation. In more concise terms, this says that the structure group of the frame bundle of \"E\", which is the real general linear group \"GL\"(R), can be reduced to the subgroup consisting of those with positive determinant.\n\nIf \"E\" is a real vector bundle of rank \"n\", then a choice of metric on \"E\" amounts to a reduction of the structure group to the orthogonal group \"O\"(\"n\"). In that situation, an orientation of \"E\" amounts to a reduction from \"O\"(\"n\") to the special orthogonal group \"SO\"(\"n\").\n\nA vector bundle together with an orientation is called an oriented bundle. A vector bundle that can be given an orientation is called an orientable vector bundle.\n\nThe basic invariant of an oriented bundle is the Euler class. The multiplication (that is, cup product) by the Euler class of an oriented bundle gives rise to a Gysin sequence.\n\nA complex vector bundle is oriented in a canonical way.\n\nThe notion of an orientation of a vector bundle generalizes an orientation of a manifold: an orientation of a manifold is an orientation of the tangent bundle of the manifold. In particular, a manifold is orientable if and only if its tangent bundle is orientable as a vector bundle. (note: as a manifold, a tangent bundle is always orientable.)\n\nTo give an orientation to a real vector bundle \"E\" of rank \"n\" is to give an orientation to the (real) determinant bundle formula_2 of \"E\". Similarly, to give an orientation to \"E\" is to give an orientation to the unit sphere bundle of \"E\".\n\nJust as a real vector bundle is classified by the real infinite Grassmannian, oriented bundles are classified by the infinite Grassmannian of oriented real vector spaces.\n\nFrom the cohomological point of view, for any ring Λ, a Λ-orientation of a real vector bundle \"E\" of rank \"n\" means a choice (and existence) of a class\nin the cohomology ring of the Thom space \"T\"(\"E\") such that \"u\" generates formula_4 as a free formula_5-module globally and locally: i.e.,\nis an isomorphism (called the Thom isomorphism), where \"tilde\" means reduced cohomology, that restricts to each isomorphism\ninduced by the trivialization formula_8. One can show, with some work, that the usual notion of an orientation coincides with a Z-orientation.\n\n\n"}
{"id": "34251134", "url": "https://en.wikipedia.org/wiki?curid=34251134", "title": "Pi (art project)", "text": "Pi (art project)\n\nPi is the name of a multimedia installation in the vicinity of the Viennese Karlsplatz. \"Pi\" is located in the Opernpassage between the entrance to the subway and the subway stop in Secession near the Naschmarkt. The individual behind the project was the Canadian artist Ken Lum from Vancouver.\n\n\"Pi\", under construction from January 2005 to November 2006 and opened in December 2006, consists of statistical information and a representation of π to 478 decimal places. A more recent project is the calculation of the decimal places of π, indicating the importance of the eponymous media for installation of their number and infinity.\n\nThe exhibit is 130 meters long. In addition to the number pi, there is a total of 16 \"factoids\" of reflective display cases that convey a variety of statistical data in real time. Apart from the World population there are also topics such as the worldwide number of malnourished children and the growth of Sahara since the beginning of the year. Even less serious issues such as the number of eaten Wiener Schnitzels in Vienna of the given year and the current number of lovers in Vienna are represented.\n\nIn the middle of the passage standing there is a glass case with images, texts and books on the subjects of population and migration.\n\nThe scientific data were developed jointly by Ken Lum and the . \"Pi\" is to show that contemporary art is in a position to connect art to science, architecture and sociology. The aim of this project was to transform the Karlsplatz into a \"vibrant place to meet, with communicative artistic brilliance.\"\n"}
{"id": "72634", "url": "https://en.wikipedia.org/wiki?curid=72634", "title": "Polyomino", "text": "Polyomino\n\nA polyomino is a plane geometric figure formed by joining one or more equal squares edge to edge. It is a polyform whose cells are squares. It may be regarded as a finite subset of the regular square tiling with a connected interior.\n\nPolyominoes are classified according to how many cells they have:\n\nPolyominoes have been used in popular puzzles since at least 1907, and the enumeration of pentominoes is dated to antiquity. Many results with the pieces of 1 to 6 squares were first published in Fairy Chess Review between the years 1937 to 1957, under the name of \"dissection problems.\" The name \"polyomino\" was invented by Solomon W. Golomb in 1953, and it was popularized by Martin Gardner in a November 1960 \"Mathematical Games\" column in \"Scientific American\".\n\nRelated to polyominoes are polyiamonds, formed from equilateral triangles; polyhexes, formed from regular hexagons; and other plane polyforms. Polyominoes have been generalized to higher dimensions by joining cubes to form polycubes, or hypercubes to form polyhypercubes.\n\nIn statistical physics, the study of polyominoes and their higher-dimensional analogs (which are often referred to as lattice animals in this literature) is applied to problems in physics and chemistry. Polyominoes have been used as models of branched polymers and of percolation clusters.\n\nLike many puzzles in recreational mathematics, polyominoes raise many combinatorial problems. The most basic is enumerating polyominoes of a given size. No formula has been found except for special classes of polyominoes. A number of estimates are known, and there are algorithms for calculating them.\n\nPolyominoes with holes are inconvenient for some purposes, such as tiling problems. In some contexts polyominoes with holes are excluded, allowing only simply connected polyominoes.\n\nThere are three common ways of distinguishing polyominoes for enumeration:\n\nThe following table shows the numbers of polyominoes of various types with \"n\" cells. \n, Iwan Jensen has enumerated the fixed polyominoes up to \"n\" = 56; the number of fixed polyominoes with 56 cells is approximately 6.915. Free polyominoes have been enumerated up to \"n\" = 28 by Tomás Oliveira e Silva.\n\nThe dihedral group \"D\" is the group of symmetries (symmetry group) of a square. This group contains four rotations and four reflections. It is generated by alternating reflections about the \"x\"-axis and about a diagonal. One free polyomino corresponds to at most 8 fixed polyominoes, which are its images under the symmetries of \"D\". However, those images are not necessarily distinct: the more symmetry a free polyomino has, the fewer distinct fixed counterparts it has. Therefore, a free polyomino that is invariant under some or all non-trivial symmetries of \"D\" may correspond to only 4, 2 or 1 fixed polyominoes. Mathematically, free polyominoes are equivalence classes of fixed polyominoes under the group \"D\".\n\nPolyominoes have the following possible symmetries; the least number of squares needed in a polyomino with that symmetry is given in each case:\n\nIn the same way, the number of one-sided polyominoes depends on polyomino symmetry as follows:\n\nThe following table shows the numbers of polyominoes with \"n\" squares, sorted by symmetry groups. \n\nEach polyomino of order \"n\"+1 can be obtained by adding a square to a polyomino of order \"n\". This leads to algorithms for generating polyominoes inductively.\n\nMost simply, given a list of polyominoes of order \"n\", squares may be added next to each polyomino in each possible position, and the resulting polyomino of order \"n\"+1 added to the list if not a duplicate of one already found; refinements in ordering the enumeration and marking adjacent squares that should not be considered reduce the number of cases that need to be checked for duplicates. This method may be used to enumerate either free or fixed polyominoes.\n\nA more sophisticated method, described by Redelmeier, has been used by many authors as a way of not only counting polyominoes (without requiring that all polyominoes of order \"n\" be stored in order to enumerate those of order \"n\"+1), but also proving upper bounds on their number. The basic idea is that we begin with a single square, and from there, recursively add squares. Depending on the details, it may count each \"n\"-omino \"n\" times, once from starting from each of its \"n\" squares, or may be arranged to count each once only.\n\nThe simplest implementation involves adding one square at a time. Beginning with an initial square, number the adjacent squares, clockwise from the top, 1, 2, 3, and 4. Now pick a number between 1 and 4, and add a square at that location. Number the unnumbered adjacent squares, starting with 5. Then, pick a number larger than the previously picked number, and add that square. Continue picking a number larger than the number of the current square, adding that square, and then numbering the new adjacent squares. When \"n\" squares have been created, an \"n\"-omino has been created.\n\nThis method ensures that each fixed polyomino is counted exactly \"n\" times, once for each starting square. It can be optimized so that it counts each polyomino only once, rather than \"n\" times. Starting with the initial square, declare it to be the lower-left square of the polyomino. Simply do not number any square that is on a lower row, or left of the square on the same row. This is the version described by Redelmeier.\n\nIf one wishes to count free polyominoes instead, then one may check for symmetries after creating each \"n\"-omino. However, it is faster to generate symmetric polyominoes separately (by a variation of this method) and so determine the number of free polyominoes by Burnside's lemma.\n\nThe most modern algorithm for enumerating the fixed polyominoes was discovered by Iwan Jensen. An improvement on Andrew Conway's method, it is exponentially faster than the previous methods (however, its running time is still exponential in \"n\").\n\nBoth Conway's and Jensen's versions of the transfer-matrix method involve counting the number of polyominoes that have a certain width. Computing the number for all widths gives the total number of polyominoes. The basic idea behind the method is that possible beginning rows are considered, and then to determine the minimum number of squares needed to complete the polyomino of the given width. Combined with the use of generating functions, this technique is able to count many polyominoes at once, thus enabling it to run many times faster than methods that have to generate every polyomino.\n\nAlthough it has excellent running time, the tradeoff is that this algorithm uses exponential amounts of memory (many gigabytes of memory are needed for \"n\" above 50), is much harder to program than the other methods, and can't currently be used to count free polyominoes.\n\nTheoretical arguments and numerical calculations support the estimate for the number of fixed polyominoes of size n\n\nwhere \"λ\" = 4.0626 and \"c\" = 0.3169. However, this result is not proven and the values of \"λ\" and \"c\" are only estimates.\n\nThe known theoretical results are not nearly as specific as this estimate. It has been proven that\n\nexists. In other words, \"A\" grows exponentially. The best known lower bound for \"λ\" is 4.00253. The best known upper bound, not improved since the 1970s, is .\n\nTo establish a lower bound, a simple but highly effective method is concatenation of polyominoes. Define the upper-right square to be the rightmost square in the uppermost row of the polyomino. Define the bottom-left square similarly. Then, the upper-right square of any polyomino of size \"n\" can be attached to the bottom-left square of any polyomino of size \"m\" to produce a unique (\"n\"+\"m\")-omino. This proves . Using this equation, one can show for all \"n\". Refinements of this procedure combined with data for \"A\" produce the lower bound given above.\n\nThe upper bound is attained by generalizing the inductive method of enumerating polyominoes. Instead of adding one square at a time, one adds a cluster of squares at a time. This is often described as adding \"twigs\". By proving that every \"n\"-omino is a sequence of twigs, and by proving limits on the combinations of possible twigs, one obtains an upper bound on the number of \"n\"-ominoes. For example, in the algorithm outlined above, at each step we must choose a larger number, and at most three new numbers are added (since at most three unnumbered squares are adjacent to any numbered square). This can be used to obtain an upper bound of 6.75. Using 2.8 million twigs, Klarner and Rivest obtained an upper bound of 4.65.\n\nApproximations for the number of fixed polyominoes and free polyominoes are related in a simple way. A free polyomino with no symmetries (rotation or reflection) corresponds to 8 distinct fixed polyominoes, and for large \"n\", most \"n\"-ominoes have no symmetries. Therefore, the number of fixed \"n\"-ominoes is approximately 8 times the number of free \"n\"-ominoes. Moreover, this approximation is exponentially more accurate as \"n\" increases.\n\nExact formulas are known for enumerating polyominoes of special classes, such as the class of \"convex\" polyominoes and the class of \"directed\" polyominoes.\n\nThe definition of a \"convex\" polyomino is different from the usual definition of convexity, but is similar to the definition used for the orthogonal convex hull. A polyomino is said to be \"vertically\" or \"column convex\" if its intersection with any vertical line is convex (in other words, each column has no holes). Similarly, a polyomino is said to be \"horizontally\" or \"row convex\" if its intersection with any horizontal line is convex. A polyomino is said to be \"convex\" if it is row and column convex.\n\nA polyomino is said to be \"directed\" if it contains a square, known as the \"root\", such that every other square can be reached by movements of up or right one square, without leaving the polyomino.\n\nDirected polyominoes, column (or row) convex polyominoes, and convex polyominoes have been effectively enumerated by area \"n\", as well as by some other parameters such as perimeter, using generating functions.\n\nA polyomino is equable if its area equals its perimeter. An equable polyomino must be made from an even number of squares; every even number greater than 15 is possible. For instance, the 16-omino in the form of a 4 × 4 square and the 18-omino in the form of a 3 × 6 rectangle are both equable. For polyominoes with fewer than 15 squares, the perimeter always exceeds the area.\n\nIn recreational mathematics, challenges are often posed for tiling a prescribed region, or the entire plane, with polyominoes, and related problems are investigated in mathematics and computer science.\n\nPuzzles commonly ask for tiling a given region with a given set of polyominoes, such as the 12 pentominoes. Golomb's and Gardner's books have many examples. A typical puzzle is to tile a 6×10 rectangle with the twelve pentominoes; the 2339 solutions to this were found in 1960. Where multiple copies of the polyominoes in the set are allowed, Golomb defines a hierarchy of different regions that a set may be able to tile, such as rectangles, strips, and the whole plane, and shows that whether polyominoes from a given set can tile the plane is undecidable, by mapping sets of Wang tiles to sets of polyominoes.\n\nAnother class of problems asks whether copies of a given polyomino can tile a rectangle, and if so, what rectangles they can tile. These problems have been extensively studied for particular polyominoes, and tables of results for individual polyominoes are available. Klarner and Göbel showed that for any polyomino there is a finite set of \"prime\" rectangles it tiles, such that all other rectangles it tiles can be tiled by those prime rectangles. Kamenetsky and Cooke showed how various disjoint (called \"holey\") polyominoes can tile rectangles.\n\nBeyond rectangles, Golomb gave his hierarchy for single polyominoes: a polyomino may tile a rectangle, a half strip, a bent strip, an enlarged copy of itself, a quadrant, a strip, a half plane, the whole plane, certain combinations, or none of these. There are certain implications among these, both obvious (for example, if a polyomino tiles the half plane then it tiles the whole plane) and less so (for example, if a polyomino tiles an enlarged copy of itself, then it tiles the quadrant). Polyominoes of orders up to 6 are characterized in this hierarchy (with the status of one hexomino, later found to tile a rectangle, unresolved at that time).\n\nIn 2001 Cristopher Moore and John Michael Robson showed that the problem of tiling one polyomino with copies of another is NP-complete.\n\nTiling the plane with copies of a single polyomino has also been much discussed. It was noted in 1965 that all polyominoes up to hexominoes and all but four heptominoes tile the plane. It was then established by David Bird that all but 26 octominoes tile the plane. Rawsthorne found that all but 235 polyominoes of order 9 tile, and such results have been extended to higher orders by Rhoads (to order 14) and others. Polyominoes tiling the plane have been classified by the symmetries of their tilings and by the number of aspects (orientations) in which the tiles appear in them.\nThe study of which polyominoes can tile the plane has been facilitated using the Conway criterion: except for two nonominoes, all tiling polyominoes up to order 9 form a patch of at least one tile satisfying it, with higher-order exceptions more frequent.\n\nSeveral polyominoes can tile larger copies of themselves, and repeating this process recursively gives a rep-tile tiling of the plane. For instance, for every positive integer , it is possible to combine copies of the L-tromino, L-tetromino, or P-pentomino into a single larger shape similar to the smaller polyomino from which it was formed.\n\nThe \"compatibility problem\" is to take two or more polyominoes and find a figure that can be tiled with each. Polyomino compatibility has been widely studied since the 1990s. Jorge Luis Mireles and Giovanni Resta have published websites of systematic results, and Livio Zucca shows results for some complicated cases like three different pentominoes. The general problem can be hard. The first compatibility figure for the L and X pentominoes was published in 2005 and had 80 tiles of each kind. Many pairs of polyominoes have been proved incompatible by systematic exhaustion. No algorithm is known for deciding whether two arbitrary polyominoes are compatible.\n\nIn addition to the tiling problems described above, there are recreational mathematics puzzles that require folding a polyomino to create other shapes. Gardner proposed several simple games with a set of free pentominoes and a chessboard. Some variants of the Sudoku puzzle use polyomino-shaped regions on the grid. The video game Tetris is based on the seven one-sided tetrominoes (spelled \"Tetriminos\" in the game), and the board game Blokus uses all of the free polyominoes up to pentominoes.\n\nThe word \"polyomino\" and the names of the various orders of polyomino are all back-formations from the word \"domino\", a common game piece consisting of two squares, with the first letter \"d-\" fancifully interpreted as a version of the prefix \"di-\" meaning \"two.\" The name \"domino\" for the game piece is believed to come from the spotted masquerade garment \"domino\", from Latin \"dominus\".\n\nMost of the numerical prefixes are Greek. Polyominoes of order 9 and 11 more often take the Latin prefixes \"nona-\" (nonomino) and \"undeca-\" (undecomino) than the Greek prefixes \"ennea-\" (enneomino) and \"hendeca-\" (hendecomino).\n\n\n\n\n"}
{"id": "2592262", "url": "https://en.wikipedia.org/wiki?curid=2592262", "title": "Protein subcellular localization prediction", "text": "Protein subcellular localization prediction\n\nProtein subcellular localization prediction (or just protein localization prediction) involves the prediction of where a protein resides in a cell, its subcellular localization.\n\nIn general, prediction tools take as input information about a protein, such as a protein sequence of amino acids, and produce a predicted location within the cell as output, such as the nucleus, Endoplasmic reticulum, Golgi apparatus, extracellular space, or other organelles. The aim is to build tools that can accurately predict the outcome of protein targeting in cells.\n\nPrediction of protein subcellular localization is an important component of bioinformatics based prediction of protein function and genome annotation, and it can aid the identification of drug targets. \n\nExperimentally determining the subcellular localization of a protein can be a laborious and time consuming task. Immunolabeling or tagging (such as with a green fluorescent protein) to view localization using fluorescence microscope are often used. A high throughput alternative is to use prediction. \n\nThrough the development of new approaches in computer science, coupled with an increased dataset of proteins of known localization, computational tools can now provide fast and accurate localization predictions for many organisms. This has resulted in subcellular localization prediction becoming one of the challenges being successfully aided by bioinformatics, and machine learning.\n\nMany prediction methods now exceed the accuracy of some high-throughput laboratory methods for the identification of protein subcellular localization. Particularly, some predictors have been developed that can be used to deal with proteins that may simultaneously exist, or move between, two or more different subcellular locations. Experimental validation is typically required to confirm the predicted localizations.\n\nIn 1999 PSORT was the first published program to predict subcellular localization. Subsequent tools and websites have been released using techniques such as artificial neural networks, support vector machine and protein motifs. Predictors can be specialized for proteins in different organisms. Some are specialized for eukaryotic proteins, some for human proteins, and some for plant proteins. Methods for the prediction of bacterial localization predictors, and their accuracy, have been reviewed.\n\nThe development of protein subcellular location prediction has been summarized in two comprehensive review articles. Recent tools and an experience report can be found in a recent paper by Meinken and Min (2012).\n\nKnowledge of the subcellular localization of a protein can significantly improve target identification during the drug discovery process. For example, secreted proteins and plasma membrane proteins are easily accessible by drug molecules due to their localization in the extracellular space or on the cell surface.\n\nBacterial cell surface and secreted proteins are also of interest for their potential as vaccine candidates or as diagnostic targets. Aberrant subcellular localization of proteins has been observed in the cells of several diseases, such as cancer and Alzheimer's disease. Secreted proteins from some archaea that can survive in unusual environments have industrially important applications.\n\nBy using prediction a high number of proteins can be assessed in order to find candidates that are trafficked to the desired location.\n\nThe results of subcellular localization prediction can be stored in databases. Examples include the multi-species database Compartments, FunSecKB2, a fungal database; PlantSecKB, a plant database; MetazSecKB, an animal and human database; and ProtSecKB, a protist database.\n\n"}
{"id": "68503", "url": "https://en.wikipedia.org/wiki?curid=68503", "title": "Pseudometric space", "text": "Pseudometric space\n\nIn mathematics, a pseudometric space is a generalization of a metric space in which the distance between two distinct points can be zero. In the same way as every normed space is a metric space, every seminormed space is a pseudometric space. Because of this analogy the term semimetric space (which has a different meaning in topology) is sometimes used as a synonym, especially in functional analysis.\n\nWhen a topology is generated using a family of pseudometrics, the space is called a gauge space.\n\nA pseudometric space formula_1 is a set formula_2 together with a non-negative real-valued function formula_3 (called a pseudometric) such that, for every formula_4,\n\nUnlike a metric space, points in a pseudometric space need not be distinguishable; that is, one may have formula_8 for distinct values formula_9.\n\n\n\n\nThe pseudometric topology is the topology induced by the open balls\n\nwhich form a basis for the topology. A topological space is said to be a pseudometrizable topological space if the space can be given a pseudometric such that the pseudometric topology coincides with the given topology on the space.\n\nThe difference between pseudometrics and metrics is entirely topological. That is, a pseudometric is a metric if and only if the topology it generates is T (i.e. distinct points are topologically distinguishable).\n\nThe vanishing of the pseudometric induces an equivalence relation, called the metric identification, that converts the pseudometric space into a full-fledged metric space. This is done by defining formula_25 if formula_8. Let formula_27 be the quotient space of by this equivalence relation and define\nThen formula_29 is a metric on formula_30 and formula_31 is a well-defined metric space, called the metric space induced by the pseudometric space formula_32.\n\nThe metric identification preserves the induced topologies. That is, a subset formula_33 is open (or closed) in formula_1 if and only if formula_35 is open (or closed) in formula_31 and A is saturated. The topological identification is the Kolmogorov quotient.\n\nAn example of this construction is the completion of a metric space by its Cauchy sequences.\n\n"}
{"id": "13342698", "url": "https://en.wikipedia.org/wiki?curid=13342698", "title": "Robbins lemma", "text": "Robbins lemma\n\nIn statistics, the Robbins lemma, named after Herbert Robbins, states that if \"X\" is a random variable having a Poisson distribution with parameter \"λ\", and \"f\" is any function for which the expected value E(\"f\"(\"X\")) exists, then\n\nRobbins introduced this proposition while developing empirical Bayes methods.\n"}
{"id": "37629940", "url": "https://en.wikipedia.org/wiki?curid=37629940", "title": "Shimura subgroup", "text": "Shimura subgroup\n\nIn mathematics, the Shimura subgroup Σ(\"N\") is a subgroup of the Jacobian of the modular curve \"X\"(\"N\") of level \"N\", given by the kernel of the natural map to the Jacobian of \"X\"(\"N\"). It is named after Goro Shimura. There is a similar subgroup Σ(\"N\",\"D\") associated to Shimura curves of quaternion algebras.\n\n"}
{"id": "21014777", "url": "https://en.wikipedia.org/wiki?curid=21014777", "title": "Smart market", "text": "Smart market\n\nA smart market is a periodic auction which is cleared by the operations research technique of mathematical optimization, such as linear programming. The smart market is operated by a market manager. Trades are not bilateral, between pairs of people, but rather to or from a pool. A smart market can assist market operation when trades would otherwise have significant transaction costs or externalities.\n\nMost other types of auctions can be cleared by a simple process of sorting bids from lowest to highest. Goods may be divisible, as with milk or flour, or indivisible, as with paintings or houses. Finding a market-clearing allocation corresponds to solution of a simple knapsack problem, and does not require much computation. By contrast, a smart market allows market clearing with arbitrary constraints. During market design, constraints are selected to match the relevant physics and economics of the allocation problem. A good overview is given in McCabe et al. (1991).\n\nCombinatorial auctions are smart markets in which goods are indivisible, but some smart markets allocate divisible goods such as electricity and natural gas.\n\nCompared to traditional market structures, a smart market substantially reduces transaction costs, allows competition which would not be possible otherwise, and can eliminate externalities. Despite complex constraints, a smart market allows the benefits of a modern financial exchange system. Fulfilment of the contract is backed by the exchange; parties are generally anonymous; the market manager enforces regulation to ensure fairness and transparency; and markets are orderly, especially during stressful conditions.\n\nA smart market may be a one-sided auction in which participants buy from the market manager, a one-sided procurement (reverse auction) in which participants sell to the market manager, or two-sided, in which the market manager balances supplying participants with demanding participants. In a two-sided smart market, the market manager may be a net seller, a net buyer, or simply a revenue-neutral broker.\n\nSmart markets are achievable due to an enabling confluence of technologies: the internet to transmit users’ bids and the resulting prices and quantities, increased computation power to run the simulation and linear program, and real time monitoring.\n\nThe term appears to have been first used by Rassenti, Smith, and Bulfin in 1982. That article proposed a combinatorial auction for airplane take-off and landing slots. The U.S. government is now seeking to implement such an auction.\n\nThe modern electricity market is an important example of a two-sided smart market., Electricity markets clear every few minutes, and require coordination to ensure that power generation matches demand, and that power flows do not exceed network line capacities. Generators offer to supply tranches of power at a range of prices. Wholesale power distributors bid to buy tranches of power at a range of prices. To clear the market, the market manager solves a linear program in which the decision variables are how much power to accept from each generator, the flow of power on each line, and how much power to provide to each distributor. \n\nAfter solution, the primal variables prescribe the dispatch (that is, how much power each generator should produce). The dual variables provide the market clearing prices. By clearing the market based on the dual prices, participants are charged on marginal values, rather than as bid. Thus, every seller is guaranteed to receive at least as much as was bid and possibly more. Every buyer is guaranteed to pay no more than was offered, and possibly less. Without the smart market, the line operator, all generators, and all distributors would have to be part of a monopoly in order to guarantee system coordination.\n\nNatural gas markets are sometimes cleared by smart markets, as in Australia . The system operator serves as the market manager. Operation of the gas pipeline network require coordination to ensure that gas supply matches demand, and that flows do not exceed pipe capacities. Gas suppliers offer a range of quantities at a range of prices. Distributors bid to buy a range of quantities at a range of prices. To clear the market, the market manager solves a linear program in which the decision variables are the gas to accept from each supplier, the flow of gas on each pipe segment, and how much gas to provide to each distributor. As with electricity markets, after solution, the primal variables prescribe the optimal flows, and the dual variables provide the market clearing prices. The objective minimizes the cost of supplying power.\n\nThe spectrum auction is a one-sided smart market which is cleared by an integer program. Participants purchase radio spectrum from government. These combinatorial auctions are cleared as bid, rather than at prices based on dual variables. Only recently have researchers found robust means to obtain dual variables from integer programs.\n\nCompanies and governments sometimes use smart markets in procurement, as for transportation services. The Chilean government, for example, uses a smart market to choose caterers for school meal programs. The University of Chicago Booth School of Business uses a smart market for course registration. The system ensures that the class seats go to those students who most want them, while ensuring that the number of students in each class stays within the room capacity.\n\nSmart markets are now being proposed for environmental services, including water. The more sophisticated of these designs rely on hydrological optimization and hydrological run-off models.\n\nA smart market formulation may be written as a net pool, in which the decision variables explicitly calculate buys and sells, and the market model clears only those quantities. The net pool market can be mathematically infeasible if participants are unwilling to trade sufficient quantities to allow feasibility. Alternatively, the formulation may be a gross pool, in which the decision variables determine total quantities that each participant receives; the market manager calculates net sales after the model's solution, based on participants' initial holdings. The gross pool market will tend to be mathematically feasible, but could have an unacceptably high cost in the optimal objective value, should (buy) bids be too low compared to (sell) offers. The difference between these two formulations is only technical, as the market designs are economically equivalent by the Coase theorem.\n\nSee also mechanism design.\n"}
{"id": "22821969", "url": "https://en.wikipedia.org/wiki?curid=22821969", "title": "Spherical code", "text": "Spherical code\n\nIn geometry and coding theory, a spherical code with parameters (\"n\",\"N\",\"t\") is a set of \"N\" points on the unit hypersphere in \"n\" dimensions for which the dot product of unit vectors from the origin to any two points is less than or equal to \"t\". The kissing number problem may be stated as the problem of finding the maximal \"N\" for a given \"n\" for which a spherical code with parameters (\"n\",\"N\",1/2) exists. The Tammes problem may be stated as the problem of finding a spherical code with minimal \"t\" for given \"n\" and \"N\".\n\n"}
{"id": "29525", "url": "https://en.wikipedia.org/wiki?curid=29525", "title": "Square-free integer", "text": "Square-free integer\n\nIn mathematics, a square-free integer (or squarefree integer) is an integer which is divisible by no perfect square other than 1. That is, its prime factorization has exactly one factor for each prime that appears in it. For example, is square-free, but is not, because 18 is divisible by . The smallest positive square-free numbers are\n\nThe radical of an integer is its largest square-free factor. An integer is square-free if and only if it is equal to its radical.\n\nAny arbitrary positive integer can be represented in a unique way as the product of a powerful number (that is a integer such that is divisible by the square of every prime factor) and a square-free integer, which are coprime. The square-free factor, called the \"square-free part\" of the number, is the largest square-free divisor of that is coprime with . The square-free part of an integer may be smaller than the largest square-free divisor.\n\nAny arbitrary positive integer can be represented in a unique way as the product of a square and a square-free integer :\nIn this factorization, is the largest divisor of \" such that is a divisor of . \n\nIn summary, there are three square-free factors that are naturally associated to every integer: the square-free part, the above factor , and the largest square-free factor. Each is a factor of the next one. All are easily deduced from a prime factorization: if\nis the prime factorization of , where formula_3 are distinct prime numbers, then the square-free part is\nThe square-free factor such the quotient is a square is \nand the largest square-free factor is \n\nFor example, if formula_7 the square-free part is , the square-free factor such that the quotient is a square is , and the largest square-free factor is .\n\nNo algorithm is known for computing any of these square-free factors, which is faster than computing the complete prime factorization. In particular, there is no known polynomial-time algorithm for computing the square-free part of an integer, and no known polynomial-time algorithm for determining whether a number is square-free. On the opposite, polynomial-time algorithms are known for primality testing. This is a major difference between the arithmetic of the integers, and the arithmetic of the univariate polynomials, as polynomial-time algorithms are known for square-free factorization of polynomials (in short, the largest square-free factor of a polynomial is its quotient by the greatest common divisor of the polynomial and its formal derivative).\n\nA positive integer \"n\" is square-free if and only if in the prime factorization of \"n\", no prime factor occurs with an exponent larger than one. Another way of stating the same is that for every prime factor \"p\" of \"n\", the prime \"p\" does not evenly divide \"n\" / \"p\". Also \"n\" is square-free if and only if in every factorization \"n\" = \"ab\", the factors \"a\" and \"b\" are coprime. An immediate result of this definition is that all prime numbers are square-free.\n\nA positive integer \"n\" is square-free if and only if all abelian groups of order \"n\" are isomorphic, which is the case if and only if any such group is cyclic. This follows from the classification of finitely generated abelian groups.\n\nA integer \"n\" is square-free if and only if the factor ring Z / \"nZ (see modular arithmetic) is a product of fields. This follows from the Chinese remainder theorem and the fact that a ring of the form Z / \"kZ is a field if and only if \"k\" is a prime.\n\nFor every positive integer \"n\", the set of all positive divisors of \"n\" becomes a partially ordered set if we use divisibility as the order relation. This partially ordered set is always a distributive lattice. It is a Boolean algebra if and only if \"n\" is square-free.\n\nA positive integer \"n\" is square-free if and only if μ(\"n\") ≠ 0, where μ denotes the Möbius function.\n\nThe Dirichlet generating function for the square-free numbers is\n\nThis is easily seen from the Euler product\n\nLet \"Q\"(\"x\") denote the number of square-free integers between 1 and \"x\" ( shifting index by 1). For large \"n\", 3/4 of the positive integers less than \"n\" are not divisible by 4, 8/9 of these numbers are not divisible by 9, and so on. Because these events are independent, we obtain the approximation:\n\nThis argument can be made rigorous, and a very elementary estimate yields\n\n(see pi and big O notation) since we use the above characterization to obtain\nand, observing that the last summand is zero for formula_14, we have\nBy exploiting the largest known zero-free region of the Riemann zeta function, due to Ivan Matveyevich Vinogradov, and Hans-Egon Richert, the maximal size of the error term has been reduced\nby Arnold Walfisz and we have\nfor some positive constant \"c\". Under the Riemann hypothesis, the error term can be further reduced to yield\n\nThe asymptotic/natural density of square-free numbers is therefore\n\nwhere ζ is the Riemann zeta function and 1/ζ(2) is approximately 0.6079. Therefore over 3/5 of the integers are square-free.\n\nLikewise, if \"Q\"(\"x\",\"n\") denotes the number of \"n\"-free integers (e.g. 3-free integers being cube-free integers) between 1 and \"x\", one can show\n\nSince a multiple of 4 must have a square factor 4=2, it cannot occur that four consecutive integers are all square-free. On the other hand, there exist infinitely many integers \"n\" for which 4\"n\" +1, 4\"n\" +2, 4\"n\" +3 are all square-free. Otherwise, observing that 4\"n\" and at least one of 4\"n\" +1, 4\"n\" +2, 4\"n\" +3 among four could be non-square-free for sufficiently large \"n\", half of all positive integers minus finitely many must be non-square-free and therefore\ncontrary to the above asymptotic estimate for formula_21.\n\nThere exist sequences of consecutive non-square-free integers of arbitrary length. Indeed, if \"n\" satisfies a simultaneous congruence\nfor distinct primes formula_23, then each formula_24 is divisible by \"p\". On the other hand, the above-mentioned estimate formula_25 implies that, for some constant \"c\", there always exists a square-free integer between \"x\" and formula_26 for positive \"x\". Moreover, an elementary argument allows us to replace formula_26 by formula_28. The ABC conjecture would allow formula_29.\n\nIf we represent a square-free number as the infinite product\n\nthen we may take those formula_31 and use them as bits in a binary number with the encoding\n\nThe square-free number 42 has factorization 2 × 3 × 7, or as an infinite product 2 · 3  · 5 · 7 · 11 · 13 ··· Thus the number 42 may be encoded as the binary sequence ...001011 or 11 decimal. (Note that the binary digits are reversed from the ordering in the infinite product.)\n\nSince the prime factorization of every number is unique, so also is every binary encoding of the square-free integers.\n\nThe converse is also true. Since every positive integer has a unique binary representation it is possible to reverse this encoding so that they may be decoded into a unique square-free integer.\n\nAgain, for example, if we begin with the number 42, this time as simply a positive integer, we have its binary representation 101010. This decodes to 2 · 3 · 5 · 7 · 11 · 13 = 3 × 7 × 13 = 273.\n\nThus binary encoding of squarefree numbers describes a bijection between the nonnegative integers and the set of positive squarefree integers.\n\nThe central binomial coefficient\n\nis never squarefree for \"n\" > 4. This was proven in 1985 for all sufficiently large integers by András Sárközy, and for all integers > 4 in 1996 by Olivier Ramaré and Andrew Granville.\n\nThe multiplicative function formula_34 is defined\nto map positive integers \"n\" to \"t\"-free numbers by reducing the\nexponents in the prime power representation modulo \"t\":\nThe value set of formula_36, in particular, are the\nsquare-free integers. Their Dirichlet generating functions are\n\nOEIS representatives are (\"t\"=2), (\"t\"=3) and (\"t\"=4).\n\n"}
{"id": "19821042", "url": "https://en.wikipedia.org/wiki?curid=19821042", "title": "Technical lettering", "text": "Technical lettering\n\nTechnical lettering is the process of forming letters, numerals, and other characters in technical drawing. It is used to describe, or provide detailed specifications for, an object. With the goals of legibility and uniformity, styles are standardized and lettering ability has little relationship to normal writing ability. Engineering drawings use a Gothic sans-serif script, formed by a series of short strokes. Lower case letters are rare in most drawings of machines.\n\n\nThe letters to be drawn, though freehanded, should be stable and graceful. In some cases stability is impossible; for example, \"P\" and \"F\" are unavoidably top-heavy. In other cases the stability and grace of the letters may be maintained either by drawing the lower parts of the letters like \"B\",\"E\" etc. wider than the upper parts, or by drawing the horizontal line at the center of these letters just above their geometric axis. (Exception: In case of the letter \"A\", the horizontal member is drawn below the geometric center, to maintain \"equality of areas below and above the center line\". If a horizontal line is drawn exactly at the center, then the difference in the areas of the triangle above the line and the trapezium below the line is much larger. This creates an unusual effect to our eyes.)\n\nEmphasis should be on the overall beauty of a word, rather than individual letters.\n\nMost freehand lettering is done in a \"gothic\" style, i.e., with a constant line thickness; either \"straight gothic\", with vertical strokes perpendicular to the baseline, or \"inclined gothic\", with vertical strokes at about 75°.\n\nMechanical lettering is sometimes done using a pantograph, a device consisting of four bars (\"links\") which are pinned to each other to form a parallelogram. The links can pivot about these pins. The lowermost link of the parallelogram is fixed to two rigid supports. One vertical link at one end is connected to a profile tracer, which traces the profile of the letter to be drawn, and the second vertical link and the other horizontal link are jointly connected to a pencil that draws the exact shape of the profile traced.\n\n\n"}
{"id": "3995195", "url": "https://en.wikipedia.org/wiki?curid=3995195", "title": "Vopěnka's principle", "text": "Vopěnka's principle\n\nIn mathematics, Vopěnka's principle is a large cardinal axiom. \nThe intuition behind the axiom is that the set-theoretical universe is so large that in every proper class, some members are similar to others, with this similarity formalized through elementary embeddings.\n\nVopěnka's principle was first introduced by Petr Vopěnka and independently considered by H. Jerome Keisler, and was written up by .\nAccording to , Vopěnka's principle was originally intended as a joke: Vopěnka was apparently unenthusiastic about large cardinals and introduced his principle as a bogus large cardinal property, planning to show later that it was not consistent. However, before publishing his inconsistency proof he found a flaw in it.\n\nVopěnka's principle asserts that for every proper class of binary relations (each with set-sized domain), there is one elementarily embeddable into another. This cannot be stated as a single sentence of ZFC as it involves a quantification over classes. A cardinal κ is called a Vopěnka cardinal if it is inaccessible and Vopěnka's principle holds in the rank \"V\" (allowing arbitrary \"S\" ⊂ \"V\" as \"classes\").\nMany equivalent formulations are possible.\nFor example, Vopěnka's principle is equivalent to each of the following statements. \n\nEven when restricted to predicates and proper classes definable in first order set theory, the principle implies existence of Σ correct extendible cardinals for every \"n\".\n\nIf κ is an almost huge cardinal, then a strong form of Vopěnka's principle holds in \"V\":\n\n\n gives a number of equivalent definitions of Vopěnka's principle.\n"}
{"id": "27175770", "url": "https://en.wikipedia.org/wiki?curid=27175770", "title": "Watson's lemma", "text": "Watson's lemma\n\nIn mathematics, Watson's lemma, proved by G. N. Watson (1918, p. 133), has significant application within the theory on the asymptotic behavior of integrals.\n\nLet formula_1 be fixed. Assume formula_2, where formula_3 has an infinite number of derivatives in the neighborhood of formula_4, with formula_5, and formula_6. \n\nSuppose, in addition, either that \n\nwhere formula_8 are independent of formula_9, or that\n\nThen, it is true that for all positive formula_11 that\n\nand that the following asymptotic equivalence holds:\n\nSee, for instance, for the original proof or for a more recent development.\n\nWe will prove the version of Watson's lemma which assumes that formula_14 has at most exponential growth as formula_15. The basic idea behind the proof is that we will approximate formula_3 by finitely many terms of its Taylor series. Since the derivatives of formula_17 are only assumed to exist in a neighborhood of the origin, we will essentially proceed by removing the tail of the integral, applying Taylor's theorem with remainder in the remaining small interval, then adding the tail back on in the end. At each step we will carefully estimate how much we are throwing away or adding on. This proof is a modification of the one found in .\n\nLet formula_1 and suppose that formula_19 is a measurable function of the form formula_20, where formula_6 and formula_17 has an infinite number of continuous derivatives in the interval formula_23 for some formula_24, and that formula_25 for all formula_26, where the constants formula_27 and formula_28 are independent of formula_9.\n\nWe can show that the integral is finite for formula_11 large enough by writing\nand estimating each term.\n\nFor the first term we have\nfor formula_33, where the last integral is finite by the assumptions that formula_17 is continuous on the interval formula_23 and that formula_6. For the second term we use the assumption that formula_19 is exponentially bounded to see that, for formula_38,\nThe finiteness of the original integral then follows from applying the triangle inequality to formula_40.\n\nWe can deduce from the above calculation that\nas formula_42.\n\nBy appealing to Taylor's theorem with remainder we know that, for each integer formula_43,\nfor formula_45, where formula_46. Plugging this in to the first term in formula_47 we get\nTo bound the term involving the remainder we use the assumption that formula_49 is continuous on the interval formula_23, and in particular it is bounded there. As such we see that\nHere we have used the fact that\nif formula_53 and formula_54, where formula_55 is the gamma function.\n\nFrom the above calculation we see from formula_56 that\nas formula_42.\n\nWe will now add the tails on to each integral in formula_59. For each formula_60 we have\nand we will show that the remaining integrals are exponentially small. Indeed, if we make the change of variables formula_62 we get\nfor formula_64, so that\n\nIf we substitute this last result into formula_59 we find that\nas formula_42. Finally, substituting this into formula_47 we conclude that\nas formula_42.\n\nSince this last expression is true for each integer formula_43 we have thus shown that\nas formula_42, where the infinite series is interpreted as an asymptotic expansion of the integral in question.\n\nWhen formula_75, the confluent hypergeometric function of the first kind has the integral representation\nwhere formula_55 is the gamma function. The change of variables formula_78 puts this into the form\nwhich is now amenable to the use of Watson's lemma. Taking formula_80 and formula_81, Watson's lemma tells us that\nwhich allows us to conclude that\n\n"}
