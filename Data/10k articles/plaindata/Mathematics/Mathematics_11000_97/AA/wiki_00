{"id": "39524578", "url": "https://en.wikipedia.org/wiki?curid=39524578", "title": "Accumulator (cryptography)", "text": "Accumulator (cryptography)\n\nA cryptographic accumulator is a one way membership function. It answers a query as to whether a potential candidate is a member of a set without revealing the individual members of the set. One example is how large composite numbers accumulate their prime factors, as it's currently impractical to factor a composite number, but relatively easy to divide a specific prime into another number to see if it is one of the factors and/or to factor it out. New members may be added or subtracted to the set of factors simply by multiplying or factoring out the number respectively. More practical accumulators use a quasi-commutative hash function where the size (number of bits) of the accumulator does not grow with the number of members.\n\nThe concept was introduced by J. Benaloh and M. de Mare in 1993\n\nThe concept has received renewed interest recently due to the proposed Zerocoin add on to bitcoin, which employs cryptographic accumulators to eliminate trackable linkage in the bitcoin blockchain, which would make bitcoin anonymous and untraceable, increasing privacy of transactions.\n\n\n"}
{"id": "46587298", "url": "https://en.wikipedia.org/wiki?curid=46587298", "title": "Almgren–Pitts min-max theory", "text": "Almgren–Pitts min-max theory\n\nIn mathematics, the Almgren–Pitts min-max theory (named after Frederick J. Almgren, Jr. and his student Jon T. Pitts) is an analogue of Morse theory for hypersurfaces.\n\nThe theory started with the efforts for generalizing George David Birkhoff's method for the construction of simple closed geodesics on the sphere, to allow the construction of embedded minimal surfaces in arbitrary 3-manifolds.\n\nIt has played roles in the solutions to a number of conjectures in geometry and topology found by Almgren and Pitts themselves and also by other mathematicians, such as Mikhail Gromov, Richard Schoen, Shing-Tung Yau, Fernando Codá Marques, André Neves, Ian Agol, among others.\n\nThe theory allows the construction of embedded minimal hypersurfaces though variational methods.\n\n\n"}
{"id": "7010617", "url": "https://en.wikipedia.org/wiki?curid=7010617", "title": "Bayesian average", "text": "Bayesian average\n\nA Bayesian average is a method of estimating the mean of a population using outside information, especially a pre-existing belief, that is factored into the calculation. This is a central feature of Bayesian interpretation. This is relevant when the available data set is small.\n\nCalculating the Bayesian average uses the prior mean \"m\" and a constant \"C\". \"C\" is assigned a value that is proportional to the typical data set size. The value is larger when the expected variation between data sets (within the larger population) is small. It is smaller when the data sets are expected to vary substantially from one another.\n\nThis is equivalent to adding \"C\" data points of value \"m\" to the data set. It is a weighted average of a prior average \"m\" and the sample average.\n\n"}
{"id": "31569035", "url": "https://en.wikipedia.org/wiki?curid=31569035", "title": "Branching random walk", "text": "Branching random walk\n\nIn probability theory, a branching random walk is a stochastic process that generalizes both the concept of a random walk and of a branching process. At every generation (a point of discrete time), a branching random walk's value is a set of elements that are located in some linear space, such as the real line. Each element of a given generation can have several descendants in the next generation. The location of any descendant is the sum of its parent's location and a random variable.\n\nAn example of branching random walk can be constructed where the branching process generates exactly two descendants for each element, a \"binary \"branching random walk. Given the initial condition that \"X\" = 0, we suppose that \"X\" and \"X\" are the two children of \"X\". Further, we suppose that they are independent (0, 1) random variables. Consequently, in generation 2, the random variables \"X\" and \"X\" are each the sum of \"X\" and a (0, 1) random variable. In the next generation, the random variables \"X\" and \"X\" are each the sum of \"X\" and a (0, 1) random variable. The same construction produces the values at successive times.\n\nEach lineage in the infinite \"genealogical tree\" produced by this process, such as the sequence \"X\", \"X\", \"X\", \"X\", ..., forms a conventional random walk.\n\n"}
{"id": "1884234", "url": "https://en.wikipedia.org/wiki?curid=1884234", "title": "Brāhmasphuṭasiddhānta", "text": "Brāhmasphuṭasiddhānta\n\nThe Brāhmasphuṭasiddhānta (\"Correctly Established Doctrine of Brahma\", abbreviated BSS)\nis the main work of Brahmagupta, written c. 628. Τhis text of mathematical astronomy contains significant mathematical content, including a good understanding of the role of zero, rules for manipulating both negative and positive numbers, a method for computing square roots, methods of solving linear and quadratic equations, and rules for summing series, Brahmagupta's identity, and Brahmagupta’s theorem. \n\nThe book was written completely in verse and does not contain any kind of mathematical notation. Nevertheless, it contained the first clear description of the quadratic formula (the solution of the quadratic equation).\n\nBrāhmasphuṭasiddhānta is one of the first books to provide concrete ideas on positive numbers, negative numbers, and zero. He wrote the following rules:\n\n\nThe last two of these rules are notable as the earliest attempt to define division by zero, even though they are not compatible with modern number theory (division by zero is undefined for a field).\n\n"}
{"id": "42478196", "url": "https://en.wikipedia.org/wiki?curid=42478196", "title": "Chiral Lie algebra", "text": "Chiral Lie algebra\n\nIn algebra, a chiral Lie algebra is a D-module on a curve with a certain structure of Lie algebra. It is related to an formula_1-algebra via the Riemann–Hilbert correspondence.\n\n\n"}
{"id": "8503698", "url": "https://en.wikipedia.org/wiki?curid=8503698", "title": "Complexity economics", "text": "Complexity economics\n\nComplexity economics is the application of complexity science to the problems of economics. It sees the economy not as a system in equilibrium, but as one in motion, perpetually constructing itself anew. It uses computational rather than mathematical analysis to explore how economic structure is formed and reformed, in continuous interaction with the adaptive behavior of the 'agents' in the economy \n\nThe \"nearly archetypal example\" is an artificial stock market model created by the Santa Fe Institute in 1989. The model shows two different outcomes, one where \"agents do not search much for predictors and there is convergence on a homogeneous rational expectations outcome\" and another where \"all kinds of technical trading strategies appearing and remaining and periods of bubbles and crashes occurring\".\n\nAnother area has studied the prisoner's dilemma, such as in a network where agents play amongst their nearest neighbors or a network where the agents can make mistakes from time to time and \"evolve strategies\". In these models, the results show a system which displays \"a pattern of constantly changing distributions of the strategies\".\n\nMore generally, complexity economics models are often used to study how non-intuitive results at the macro-level of a system can emerge from simple interactions at the micro level. This avoids assumptions of the representative agent method, which attributes outcomes in collective systems as the simple sum of the rational actions of the individuals.\n\nMIT physicist César Hidalgo and Harvard economist Ricardo Hausmann introduced a spectral method to measure the complexity of a country's economy by inferring it from the structure of the network connecting countries to the products that they export. The measure combines information of a country's diversity, which is positively correlated with a country's productive knowledge, with measures of a product ubiquity (number of countries that produce or export the product). This concept, known as the \"Product Space\", has been further developed by MIT's Observatory of Economic Complexity, and in The Atlas of Economic Complexity in 2011.\n\nThe economic complexity index (ECI) introduced by Hidalgo and Hausmann is highly predictive of future GDP per capita growth. In Hausmann, Hidalgo et al., the authors show that the List of countries by future GDP (based on ECI) estimates ability of the ECI to predict future GDP per capita growth is between 5 times and 20 times larger than the World Bank's measure of governance, the World Economic Forum's (WEF) Global Competitiveness Index (GCI) and standard measures of human capital, such as years of schooling and cognitive ability.\n\nPietronero and collaborators have recently proposed a different approach. These metrics are defined as the fixed point of non-linear iterative map. Differently from the linear algorithm giving rise to the ECI, this non-linearity is a key point to properly deal with the nested structure of the data. The authors of this alternative formula claim it has several advantages:\n\n\nThe metrics for country fitness and product complexity have been used in a report of the Boston Consulting Group on Sweden growth and development perspectives.\n\nBrian Arthur, Steven N. Durlauf, and David A. Lane describe several features of complex systems that deserve greater attention in economics.\n\n\nComplexity economics has a complex relation to previous work in economics and other sciences, and to contemporary economics. Complexity-theoretic thinking to understand economic problems has been present since their inception as academic disciplines. Research has shown that no two separate micro-events are completely isolated, and there is a relationship that forms a macroeconomic structure. However, the relationship is not always in one direction; there is a reciprocal influence when feedback is in operation.\n\nComplexity economics has been applied to many fields.\n\nComplexity economics draws inspiration from behavioral economics, Marxian economics, institutional economics/evolutionary economics, Austrian economics and the work of Adam Smith. It also draws inspiration from other fields, such as statistical mechanics in physics, and evolutionary biology. Some of the 20th century intellectual background of complexity theory in economics is examined in Alan Marshall (2002) The Unity of Nature, Imperial College Press: London. See Douma & Schreuder (2017) for a non-technical introduction to Complexity Economics and a comparison with other economic theories (as applied to markets and organizations).\n\nThe theory of complex dynamic systems has been applied in diverse fields in economics and other decision sciences. These applications include capital theory, game theory, the dynamics of opinions among agents composed of multiple selves, and macroeconomics. In voting theory, the methods of symbolic dynamics have been applied by Donald G. Saari. Complexity economics has attracted the attention of historians of economics. Ben Ramalingam's Aid on the Edge of Chaos includes numerous applications of complexity economics that are relevant to foreign aid.\n\nAccording to , , and contemporary mainstream economics is evolving to be more \"eclectic\", diverse, and pluralistic. state that contemporary mainstream economics is \"moving away from a strict adherence to the holy trinity – rationality, selfishness, and equilibrium\", citing complexity economics along with recursive economics and dynamical systems as contributions to these trends. They classify complexity economics as now mainstream but non-orthodox.\n\nIn 1995-1997 publications, \"Scientific American\" journalist John Horgan \"ridiculed\" the movement as being the fourth \"C\" among the \"failed fads\" of \"complexity, chaos, catastrophe, and cybernetics\". In 1997, Horgan wrote that the approach had \"created some potent metaphors: the butterfly effect, fractals, artificial life, the edge of chaos, self organized criticality. But they have not told us anything about the world that is both concrete and truly surprising, either in a negative or in a positive sense.\"\n\nRosser \"granted\" Horgan \"that it is hard to identify a concrete and surprising discovery (rather than \"mere metaphor\") that has arisen due to the emergence of complexity analysis\" in the discussion journal of the American Economic Association, the \"Journal of Economic Perspectives\". Surveying economic studies based on complexity science, Rosser wrote that the findings, rather than being surprising, confirmed \"already-observed facts.\" Rosser wrote that there has been \"little work on empirical techniques for testing dispersed agent complexity models.\" Nonetheless, Rosser wrote that \"there is a strain of common perspective that has been accumulating as the four C's of cybernetics, catastrophe, chaos, and complexity emerged, which may now be reaching a critical mass in terms of influencing the thinking of economists more broadly.\"\n\n\n\n"}
{"id": "3168110", "url": "https://en.wikipedia.org/wiki?curid=3168110", "title": "Cryptographic Service Provider", "text": "Cryptographic Service Provider\n\nIn Microsoft Windows, a Cryptographic Service Provider (CSP) is a software library that implements the Microsoft CryptoAPI (CAPI). CSPs implement encoding and decoding functions, which computer application programs may use, for example, to implement strong user authentication or for secure email. \n\nCSPs are independent modules that can be used by different applications. A user program calls CryptoAPI functions and these are redirected to CSPs functions. Since CSPs are responsible for implementing cryptographic algorithms and standards, applications do not need to be concerned about security details. Furthermore, one application can define which CSP it is going to use on its calls to CryptoAPI. In fact, all cryptographic activity is implemented in CSPs. CryptoAPI only works as a bridge between the application and the CSP.\n\nCSPs are implemented basically as a special type of DLL with special restrictions on loading and use. Every CSP must be digitally signed by Microsoft and the signature is verified when Windows loads the CSP. In addition, after being loaded, Windows periodically re-scans the CSP to detect tampering, either by malicious software such as computer viruses or by the user him/herself trying to circumvent restrictions (for example on cryptographic key length) that might be built into the CSP's code.\n\nTo obtain a signature, non-Microsoft CSP developers must supply paperwork to Microsoft promising to obey various legal restrictions and giving valid contact information. As of circa 2000, Microsoft did not charge any fees to supply these signatures. For development and testing purposes, a CSP developer can configure Windows to recognize the developer's own signatures instead of Microsoft's, but this is a somewhat complex and obscure operation unsuitable for nontechnical end users.\n\nThe CAPI/CSP architecture had its origins in the era of restrictive US government controls on the export of cryptography. Microsoft's default or \"base\" CSP then included with Windows was limited to 512-bit RSA public-key cryptography and 40-bit symmetric cryptography, the maximum key lengths permitted in exportable mass market software at the time. CSPs implementing stronger cryptography were available only to U.S. residents, unless the CSPs themselves had received U.S. government export approval. The system of requiring CSPs to be signed only on presentation of completed paperwork was intended to prevent the easy spread of unauthorized CSPs implemented by anonymous or foreign developers. As such, it was presented as a concession made by Microsoft to the government, in order to get export approval for the CAPI itself.\n\nAfter the \"Bernstein v. United States\" court decision establishing computer source code as protected free speech and the transfer of cryptographic regulatory authority from the U.S. State Department to the more pro-export Commerce Department, the restrictions on key lengths were dropped, and the CSPs shipped with Windows now include full-strength cryptography. The main use of third-party CSPs is to interface with external cryptography hardware such as hardware security modules (HSM) or smart cards.\n\nThese cryptographic functions can be realised by a smart card, thus the Smart Card CSP is the Microsoft way of a PKCS#11. Microsoft Windows is identifying the correct Smart Card CSP, which have to be used, analysing the answer to reset (ATR) of the smart card, which is registered in the Windows Registry. Installing a new CSP, all ATRs of the supported smart cards are enlisted in the registry.\n\nCryptographic service providers can be used for encryption of Word, Excel, and PowerPoint documents starting from Microsoft Office XP. A standard encryption algorithm with a 40-bit key is used by default, but enabling a CSP enhances key length and thus makes decryption process more continuous. This only applies to passwords that are required to \"open\" document because this password type is the only one that encrypts a password-protected document.\n\n\n"}
{"id": "35205462", "url": "https://en.wikipedia.org/wiki?curid=35205462", "title": "Deformation ring", "text": "Deformation ring\n\nIn mathematics, a deformation ring is a ring that controls liftings of a representation of a Galois group from a finite field to a local field. In particular for any such lifting problem there is often a universal deformation ring that classifies all such liftings, and whose spectrum is the universal deformation space.\n\nA key step in Wiles's proof of the modularity theorem was to study the relation between universal deformation rings and Hecke algebra. \n"}
{"id": "10574794", "url": "https://en.wikipedia.org/wiki?curid=10574794", "title": "Deligne–Lusztig theory", "text": "Deligne–Lusztig theory\n\nIn mathematics, Deligne–Lusztig theory is a way of constructing linear representations of finite groups of Lie type using ℓ-adic cohomology with compact support, introduced by .\n\nSuppose that \"G\" is a reductive group defined over a finite field, with Frobenius map \"F\".\n\nIan G. Macdonald conjectured that there should be a map from \"general position\" characters of \"F\"-stable maximal tori to irreducible representations of \"G\" (the fixed points of \"F\"). For general linear groups this was already known by the work of . This was the main result proved by Pierre Deligne and George Lusztig; they found a virtual representation for all characters of an \"F\"-stable maximal torus, which is irreducible (up to sign) when the character is in general position.\n\nWhen the maximal torus is split, these representations were well known and are given by parabolic induction of characters of the torus (extend the character to a Borel subgroup, then induce it up to \"G\"). The representations of parabolic induction can be constructed using functions on a space, which can be thought of as elements of a suitable zeroth cohomology group. Deligne and Lusztig's construction is a generalization of parabolic induction to non-split tori using higher cohomology groups. (Parabolic induction can also be done with tori of \"G\" replaced by Levi subgroups of \"G\", and there is a generalization of Deligne–Lusztig theory to this case too.)\n\nVladimir Drinfeld proved that the discrete series representations of SL(F) can be found in the ℓ-adic cohomology groups\n\nof the affine curve \"X\" defined by\n\nThe polynomial \"xy−yx\" is a determinant used in the construction of the Dickson invariant of the general linear group, and is an invariant of the special linear group.\n\nThe construction of Deligne and Lusztig is a generalization of this fundamental example to other groups. The affine curve \"X\" is generalized to a \"T\" bundle over a \"Deligne–Lusztig variety\" where \"T\" is a maximal torus of \"G\", and instead of using just the first cohomology group they use an alternating sum of ℓ-adic cohomology groups with compact support to construct virtual representations.\n\nThe Deligne-Lusztig construction is formally similar to Hermann Weyl's construction of the representations of a compact group from the characters of a maximal torus. The case of compact groups is easier partly because there is only one conjugacy class of maximal tori. The Borel–Weil–Bott construction of representations of algebraic groups using coherent sheaf cohomology is also similar.\n\nFor real semisimple groups there is an analogue of the construction of Deligne and Lusztig, using Zuckerman functors to construct representations.\n\nThe construction of Deligne-Lusztig characters uses a family of auxiliary algebraic varieties \"X\" called Deligne–Lusztig varieties, constructed from a reductive linear algebraic group \"G\" defined over a finite field F.\n\nIf \"B\" is a Borel subgroup of \"G\" and \"T\" a maximal torus of \"B\" then we write\n\nfor the Weyl group (normalizer mod centralizer)\n\nof \"G\" with respect to \"T\", together with the simple roots corresponding to \"B\". If \"B\" is another Borel subgroup with maximal torus \"T\" then there is a canonical isomorphism from \"T\" to \"T\" that identifies the two Weyl groups. So we can identify all these Weyl groups, and call it 'the' Weyl group \"W\" of \"G\". Similarly there is a canonical isomorphism between any two maximal tori with given choice of positive roots, so we can identify all these and call it 'the' maximal torus \"T\" of \"G\".\n\nBy the Bruhat decomposition\n\nthe subgroup \"B\" can be written as the conjugate of \"B\" by \"bw\" for some \"b\"∈\"B\" and \"w\"∈\"W\" (identified with \"W\") where \"w\" is uniquely determined. In this case we say that \"B\" and \"B\" are in relative position \"w\".\n\nSuppose that \"w\" is in the Weyl group of \"G\", and write \"X\" for the smooth projective variety of all Borel subgroups of \"G\". \nThe Deligne-Lusztig variety \"X\"(\"w\") consists of all Borel subgroups \"B\" of \"G\" such that \"B\" and \"F\"(\"B\") are in relative position \"w\" [recall that \"F\" is the Frobenius map]. In other words, it is the inverse image of the \"G\"-homogeneous space of pairs of Borel subgroups in relative position \"w\", under the Lang isogeny with formula\n\nFor example, if \"w\"=1 then \"X\"(\"w\") is 0-dimensional and its points are the rational Borel subgroups of \"G\".\n\nWe let \"T\"(\"w\") be the torus \"T\", with the rational structure for which the Frobenius is \"wF\".\nThe \"G\" conjugacy classes of \"F\"-stable maximal tori of \"G\" can be identified with the \"F\"-conjugacy classes of \"W\", where we say \"w\"∈\"W\" is \"F\"-conjugate to elements of the form \"vwF\"(\"v\") for \"v\"∈\"W\". If the group \"G\" is split, so that \"F\" acts trivially on \"W\", this is the same as ordinary conjugacy, but in general for non-split groups \"G\", \"F\" may act on \"W\" via a non-trivial diagram automorphism. The \"F\"-stable conjugacy classes can be identified with elements of the non-abelian galois cohomology group of torsors\n\nFix a maximal torus \"T\" of \"G\" and a Borel subgroup \"B\" containing it, both invariant under the Frobenius map \"F\", and write \"U\" for the unipotent radical of \"B\".\nIf we choose a representative \"w\"′ of the normalizer \"N\"(\"T\") representing \"w\", then we define \"X\"′(\"w\"′) to be the elements of \"G\"/\"U\" with \"F\"(\"u\")=\"uw\"′.\nThis is acted on freely by \"T\"(\"F\"), and the quotient is isomorphic to \"X\"(\"T\"). So\nfor each character θ of \"T\"(\"w\") we get a corresponding local system \"F\" on \"X\"(\"w\"). The\nDeligne-Lusztig virtual representation\n\nof \"G\" is defined by the alternating sum\n\nof \"l\"-adic compactly supported cohomology groups of \"X\"(\"w\") with coefficients in the \"l\"-adic local system \"F\".\n\nIf \"T\" is a maximal \"F\"-invariant torus of \"G\" contained in a Borel subgroup \"B\" such that\n\"B\" and \"FB\" are in relative position \"w\" then \"R\"(\"w\") is also \ndenoted by \"R\", or by \"R\" as up to isomorphism it does not depend on the choice of \"B\".\n\n\n\n\nLusztig classified all the irreducible characters of \"G\" by decomposing such a character into a semisimple character and a unipotent character (of another group) and separately classifying the semisimple and unipotent characters.\n\nThe representations of \"G\" are classified using conjugacy classes of the dual group of \"G\". \nA reductive group over a finite field determines a root datum (with choice of Weyl chamber) together with an action of the Frobenius element on it. \nThe dual group \"G\" of a reductive algebraic group \"G\" defined over a finite field is the one with dual root datum (and adjoint Frobenius action). \nThis is similar to the Langlands dual group (or L-group), except here the dual group is defined over a finite field rather than over the complex numbers. The dual group has the same root system, except that root systems of type B and C get exchanged.\n\nThe local Langlands conjectures state (very roughly) that representations of an algebraic group over a local field should be closely related to conjugacy classes in the Langlands dual group. Lusztig's classification of representations of reductive groups over finite fields can be thought of as a verification of an analogue of this conjecture for finite fields (though Langlands never stated his conjecture for this case).\n\nIn this section \"G\" will be a reductive group with connected center.\n\nAn irreducible character is called unipotent if it occurs in some \"R\", and is called semisimple if its average value on regular unipotent elements is non-zero (in which case the average value is 1 or −1). If \"p\" is a good prime for \"G\" (meaning that it does not divide any of the coefficients of roots expressed as linear combinations of simple roots) then an irreducible character is semisimple if and only if its order is not divisible by \"p\".\n\nAn arbitrary irreducible character has a \"Jordan decomposition\": to it one can associate a semisimple character (corresponding to some semisimple element \"s\" of the dual group), and a unipotent representation of the centralizer of \"s\". The dimension of the irreducible character is the product of the dimensions of its semisimple and unipotent components.\n\nThis (more or less) reduces the classification of irreducible characters to the problem of finding the semisimple and the unipotent characters.\n\nTwo pairs (\"T\",θ), (\"T\"′,θ′) of a maximal torus \"T\" of \"G\" fixed by \"F\" and a character θ of \"T\" are called geometrically conjugate if they are conjugate under some element of \"G\"(\"k\"), where \"k\" is the algebraic closure of F. If an irreducible representation occurs in both \"R\" and \"R\" then (\"T\",θ), (\"T\"′,θ′) need not be conjugate under \"G\", but are always geometrically conjugate. For example, if θ = θ′ = 1 and \"T\" and \"T\"′ are not conjugate, then the identity representation occurs in both Deligne–Lusztig characters, and the corresponding pairs (\"T\",1), (\"T\"′,1) are geometrically conjugate but not conjugate.\n\nThe geometric conjugacy classes of pairs (\"T\",θ) are parameterized by geometric conjugacy classes of semisimple elements \"s\" of the group \"G\" of elements of the dual group \"G\" fixed by \"F\". Two elements of \"G\" are called geometrically conjugate if they are conjugate over the algebraic closure of the finite field; if the center of \"G\" is connected this is equivalent to conjugacy in \"G\". The number of geometric conjugacy classes of pairs (\"T\",θ) is |\"Z\"|\"q\" where \"Z\" is the identity component of the center \"Z\" of \"G\" and \"l\" is the semisimple rank of \"G\".\n\nIn this subsection \"G\" will be a reductive group with connected center \"Z\". (The case when the center is not connected has some extra complications.)\n\nThe semisimple characters of \"G\" correspond to geometric conjugacy classes of pairs (\"T\",θ) (where \"T\" is a maximal torus invariant under \"F\" and θ is a character of \"T\"); in fact among the irreducible characters occurring in the Deligne–Lusztig characters of a geometric conjugacy class there is exactly one semisimple character. If\nthe center of \"G\" is connected there are |\"Z\"|\"q\" semisimple characters. If κ is a geometric conjugacy class of pairs (\"T\",θ) then the character of the corresponding semisimple representation is given up to sign by\nand its dimension is the \"p\"′ part of the index of the centralizer of the element \"s\" of the dual group corresponding to it.\n\nThe semisimple characters are (up to sign) exactly the duals of the regular characters, under Alvis–Curtis duality, a duality operation on generalized characters. \nAn irreducible character is called regular if it occurs in the Gelfand–Graev representation \n\"G\", which is the representation induced from a certain \"non-degenerate\" 1-dimensional character of the Sylow \"p\"-subgroup. It is reducible, and any irreducible character of \"G\" occurs at most once in it. If κ is a geometric conjugacy class of pairs (\"T\",θ) then the character of the corresponding regular representation is given by\nand its dimension is the \"p\"′ part of the index of the centralizer of the element \"s\" of the dual group corresponding to it times the \"p\"-part of the order of the centralizer.\n\nThese can be found from the cuspidal unipotent characters: those that cannot be obtained from decomposition of parabolically induced characters of smaller rank groups. The unipotent cuspidal characters were listed by Lusztig using rather complicated arguments. The number of them depends only on the type of the group and not on the underlying field; and is given as follows:\nThe unipotent characters can be found by decomposing the characters induced from the cuspidal ones, using results of Howlett and Lehrer. The number of unipotent characters depends only on the root system of the group and not on the field (or the center). The dimension of the unipotent characters can be given by universal polynomials in the order of the ground field depending only on the root system; for example the Steinberg representation has dimension \"q\", where \"r\" is the number of positive roots of the root system.\n\nLusztig discovered that the unipotent characters of a group \"G\" (with irreducible Weyl group) fall into families of size 4 (\"n\" ≥ 0), 8, 21, or 39. The characters of each family are indexed by conjugacy classes of pairs (\"x\",σ) where \"x\" is in one of the groups Z/2Z, \"S\", \"S\", \"S\" respectively, and σ is a representation of its centralizer. (The family of size 39 only occurs for groups of type \"E\", and the family of size 21 only occurs for groups of type \"F\".) The families are in turn indexed by special representations of the Weyl group, or equivalently by 2-sided cells of the Weyl group. \nFor example, the group \"E\"(F) has 46 families of unipotent characters corresponding to the 46 special representations of the Weyl group of \"E\". There are 23 families with 1 character, 18 families with 4 characters, 4 families with 8 characters, and one family with 39 characters (which includes the 13 cuspidal unipotent characters).\n\nSuppose that \"q\" is an odd prime power, and \"G\" is the algebraic group \"SL\". \nWe describe the Deligne–Lusztig representations of the group \"SL\"(F). (The representation theory of these groups was well known long before Deligne–Lusztig theory.)\n\nThe irreducible representations are:\n\nThere are two classes of tori associated to the two elements (or conjugacy classes) of the\nWeyl group, denoted by \"T\"(1) (cyclic of order \"q\"−1) and \"T\"(\"w\") (cyclic of order \"q\" + 1). The non-trivial element of the Weyl group acts on the characters of these tori by changing each character to its inverse. So the Weyl group fixes a character if and only if it has order 1 or 2. By the orthogonality formula, \n\"R\"(\"w\") is (up to sign) irreducible if θ does not have order 1 or 2, and a sum of two irreducible representations if it has order 1 or 2.\n\nThe Deligne-Lusztig variety \"X\"(1) for the split torus is 0-dimensional with \"q\"+1 points, and can be identified with the points of 1-dimensional projective space defined over F. \nThe representations \"R\"(1) are given as follows:\n\nThe Deligne-Lusztig variety \"X\"(\"w\") for the non-split torus is 1-dimensional, and can be identified with the complenent of \"X\"(1) in 1-dimensional projective space. So it is the set of points (\"x\":\"y\") of projective space not fixed by the Frobenius map (\"x\":\"y\")→ (\"x\":\"y\"), in other words the points with\nDrinfeld's variety of points (\"x\",\"y\") of affine space with\nmaps to \"X\"(\"w\") in the obvious way, and is acted on freely by the group of \"q\"+1th roots\nλ of 1 (which can be identified with the elements of the non-split torus that are defined over F), with λ taking (\"x\",\"y\") to (λ\"x\",λ\"y\"). The Deligne Lusztig variety is the quotient of Drinfeld's variety by this group action. \nThe representations −\"R\"(\"w\") are given as follows:\n\nThe unipotent representations are the trivial representation and the Steinberg representation, and the semisimple representations are all the representations other than the Steinberg representation. (In this case the semisimple representations do not correspond exactly to geometric conjugacy classes of the dual group, as the center of \"G\" is not connected.)\n\n replaced the ℓ-adic cohomology used to define the Deligne-Lusztig representations with intersection ℓ-adic cohomology, and introduced certain perverse sheaves called character sheaves. The representations defined using intersection cohomology are related to those defined using ordinary cohomology by Kazhdan–Lusztig polynomials. The \"F\"-invariant irreducible character sheaves are closely related to the irreducible characters of the group \"G\".\n\n"}
{"id": "17375166", "url": "https://en.wikipedia.org/wiki?curid=17375166", "title": "Double (manifold)", "text": "Double (manifold)\n\nIn the subject of manifold theory in mathematics, if formula_1 is a manifold with boundary, its double is obtained by gluing two copies of formula_1 together along their common boundary. Precisely, the double is formula_3 where formula_4 for all formula_5. \n\nAlthough the concept makes sense for any manifold, and even for some non-manifold sets such as the Alexander horned sphere, the notion of double tends to be used primarily in the context that formula_6 is non-empty and formula_1 is compact.\n\nGiven a manifold formula_1, the double of formula_1 is the boundary of formula_10. This gives doubles a special role in cobordism.\n\nThe 'n'-sphere is the double of the \"n\"-ball. In this context, the two balls would be the upper and lower hemi-sphere respectively. More generally, if formula_1 is closed, the double of formula_12 is formula_13. Even more generally, the double of a disc bundle over a manifold is a sphere bundle over the same manifold. More concretely, the double of the Möbius strip is the Klein bottle.\n\nIf formula_1 is a closed, oriented manifold and if formula_15 is obtained from formula_1 by removing an open ball, then the connected sum formula_17 is the double of formula_15. \n\nThe double of a Mazur manifold is a homotopy 4-sphere.\n"}
{"id": "641132", "url": "https://en.wikipedia.org/wiki?curid=641132", "title": "Edgeworth series", "text": "Edgeworth series\n\nThe Gram–Charlier A series (named in honor of Jørgen Pedersen Gram and Carl Charlier), and the Edgeworth series (named in honor of Francis Ysidro Edgeworth) are series that approximate a probability distribution in terms of its cumulants. The series are the same; but, the arrangement of terms (and thus the accuracy of truncating the series) differ. The key idea of these expansions is to write the characteristic function of the distribution whose probability density function is to be approximated in terms of the characteristic function of a distribution with known and suitable properties, and to recover through the inverse Fourier transform.\n\nWe examine a continuous random variable. Let formula_1 be the characteristic function of its distribution whose density function is , and formula_2 its cumulants. We expand in terms of a known distribution with probability density function , characteristic function formula_3, and cumulants formula_4. The density is generally chosen to be that of the normal distribution, but other choices are possible as well. By the definition of the cumulants, we have (see Wallace, 1958)\nwhich gives the following formal identity:\n\nBy the properties of the Fourier transform, formula_8 is the Fourier transform of formula_9, where is the differential operator with respect to . Thus, after changing formula_10 with formula_11 on both sides of the equation, we find for the formal expansion\n\nIf is chosen as the normal density \n\nwith mean and variance as given by , that is, mean formula_14 and variance formula_15, then the expansion becomes\n\nsince formula_17 for all > 2, as higher cumulants of the normal distribution are 0. By expanding the exponential and collecting terms according to the order of the derivatives, we arrive at the Gram–Charlier A series. Such an expansion can be written compactly in terms of Bell polynomials as\n\nSince the n-th derivative of the Gaussian function formula_19 is given in terms of Hermite polynomial as\n\nthis gives us the final expression of the Gram-Charlier A series as\n\nIntegrating the series gives us the cumulative distribution function \n\nwhere formula_23 is the CDF of the normal distribution. \n\nIf we include only the first two correction terms to the normal distribution, we obtain\n\nwith formula_25 and formula_26. \n\nNote that this expression is not guaranteed to be positive, and is therefore not a valid probability distribution. The Gram–Charlier A series diverges in many cases of interest—it converges only if formula_27 falls off faster than formula_28 at infinity (Cramér 1957). When it does not converge, the series is also not a true asymptotic expansion, because it is not possible to estimate the error of the expansion. For this reason, the Edgeworth series (see next section) is generally preferred over the Gram–Charlier A series.\n\nEdgeworth developed a similar expansion as an improvement to the central limit theorem. The advantage of the Edgeworth series is that the error is controlled, so that it is a true asymptotic expansion.\n\nLet formula_29 be a sequence of independent and identically distributed random variables with mean formula_30 and variance formula_31, and let formula_32 be their standardized sums:\n\nLet formula_34 denote the cumulative distribution functions of the variables formula_32. Then by the central limit theorem,\n\nfor every formula_10, as long as the mean and variance are finite.\n\nNow assume that, in addition to having mean formula_30 and variance formula_31, the i.i.d. random variables formula_40 have higher cumulants formula_41. From the additivity and homogeneity properties of cumulants, the cumulants of formula_32 in terms of the cumulants of formula_40 are for formula_44, \n\nIf we expand in terms of the standard normal distribution, that is, if we set\n\nthen the cumulant differences in the formal expression of the characteristic function formula_47 of formula_34 are\n\nThe Gram-Charlier A series for the density function of formula_32 is now\n\nThe Edgeworth series is developed similarly to the Gram–Charlier A series, only that now terms are collected according to powers of formula_54. The coefficients of \"n\" term can be obtained by collecting the monomials of the Bell polynomials corresponding to the integer partitions of \"m\". Thus, we have the characteristic function as\n\nwhere formula_56 is a polynomial of degree formula_57. Again, after inverse Fourier transform, the density function formula_58 follows as\n\nLikewise, integrating the series, we obtain the distribution function \n\nWe can explicitly write the polynomial formula_61 as \n\nwhere the summation is over all the integer partitions of \"m\" such that formula_63 and formula_64 and formula_65 \n\nFor example, if \"m\" = 3, then there are three ways to partition this number: 1 + 1 + 1 = 2 + 1 = 3. As such we need to examine three cases:\n\n\nThus, the required polynomial is\n\nThe first five terms of the expansion are\n\nHere, is the \"j\"-th derivative of at point \"x\". Remembering that the derivatives of the density of the normal distribution are related to the normal density by formula_68, (where formula_69 is the Hermite polynomial of order \"n\"), this explains the alternative representations in terms of the density function. Blinnikov and Moessner (1998) have given a simple algorithm to calculate higher-order terms of the expansion.\n\nNote that in case of a lattice distributions (which have discrete values), the Edgeworth expansion must be adjusted to account for the discontinuous jumps between lattice points.\n\nTake formula_71 and the sample mean formula_72.\n\nWe can use several distributions for formula_73:\n\nEdgeworth expansions can suffer from a few issues:\n\n\n"}
{"id": "37312651", "url": "https://en.wikipedia.org/wiki?curid=37312651", "title": "Fundamenta nova theoriae functionum ellipticarum", "text": "Fundamenta nova theoriae functionum ellipticarum\n\nFundamenta nova theoriae functionum ellipticarum (New Foundations of the Theory of Elliptic Functions) is a book on Jacobi elliptic functions by Carl Gustav Jacob Jacobi. The book was first published in 1829, and has been reprinted in volume 1 of his collected works and on several later occasions. The book introduces Jacobi elliptic functions and the Jacobi triple product identity.\n\n"}
{"id": "400711", "url": "https://en.wikipedia.org/wiki?curid=400711", "title": "Grünwald–Letnikov derivative", "text": "Grünwald–Letnikov derivative\n\nIn mathematics, the Grünwald–Letnikov derivative is a basic extension of the derivative in fractional calculus that allows one to take the derivative a non-integer number of times. It was introduced by Anton Karl Grünwald (1838–1920) from Prague, in 1867, and by Aleksey Vasilievich Letnikov (1837–1888) in Moscow in 1868.\n\nThe formula \nfor the derivative can be applied recursively to get higher-order derivatives. For example, the second-order derivative would be:\n\nAssuming that the \"h\" 's converge synchronously, this simplifies to:\n\nwhich can be justified rigorously by the mean value theorem. In general, we have (see binomial coefficient):\n\nRemoving the restriction that \"n\" be a positive integer, it is reasonable to define:\n\nThis defines the Grünwald–Letnikov derivative.\n\nTo simplify notation, we set:\n\nSo the Grünwald–Letnikov derivative may be succinctly written as:\n\nIn the preceding section, the general first principles equation for integer order derivatives was derived. It can be shown that the equation may also be written as\n\nor removing the restriction that \"n\" must be a positive integer:\n\nThis equation is called the reverse Grünwald–Letnikov derivative. If the substitution \"h\" → −\"h\" is made, the resulting equation is called the direct Grünwald–Letnikov derivative:\n\n"}
{"id": "1409506", "url": "https://en.wikipedia.org/wiki?curid=1409506", "title": "Helly's theorem", "text": "Helly's theorem\n\nHelly's theorem is a basic result in discrete geometry on the intersection of convex sets. It was discovered by Eduard Helly in 1913, but not published by him until 1923, by which time alternative proofs by and had already appeared. Helly's theorem gave rise to the notion of a Helly family.\n\nLet be a finite collection of convex subsets of , with . If the intersection of every of these sets is nonempty, then the whole collection has a nonempty intersection; that is,\n\nFor infinite collections one has to assume compactness: \n\nLet be a collection of compact convex subsets of , such that every subcollection of cardinality at most has nonempty intersection. Then the whole collection has nonempty intersection.\n\nWe prove the finite version, using Radon's theorem as in the proof by . The infinite version then follows by the finite intersection property characterization of compactness: a collection of closed subsets of a compact space has a non-empty intersection if and only if every finite subcollection has a non-empty intersection (once you fix a single set, the intersection of all others with it are closed subsets of a fixed compact space).\n\nThe proof is by induction:\n\n<ins>Base case:</ins> Let . By our assumptions, for every there is a point that is in the common intersection of all with the possible exception of . Now we apply Radon's theorem to the set which furnishes us with disjoint subsets of such that the convex hull of intersects the convex hull of . Suppose that is a point in the intersection of these two convex hulls. We claim that \n\nIndeed, consider any We shall prove that Note that the only element of that may not be in is . If , then , and therefore . Since is convex, it then also contains the convex hull of and therefore also . Likewise, if , then , and by the same reasoning . Since is in every , it must also be in the intersection.\n\nAbove, we have assumed that the points are all distinct. If this is not the case, say for some , then is in every one of the sets , and again we conclude that the intersection is nonempty. This completes the proof in the case .\n\n<ins>Inductive Step:</ins> Suppose and that the statement is true for . The argument above shows that any subcollection of sets will have nonempty intersection. We may then consider the collection where we replace the two sets and with the single set . In this new collection, every subcollection of sets will have nonempty intersection. The inductive hypothesis therefore applies, and shows that this new collection has nonempty intersection. This implies the same for the original collection, and completes the proof.\n\n\n"}
{"id": "4075543", "url": "https://en.wikipedia.org/wiki?curid=4075543", "title": "Indiscernibles", "text": "Indiscernibles\n\nIn mathematical logic, indiscernibles are objects which cannot be distinguished by any property or relation defined by a formula. Usually only first-order formulas are considered. \nIf \"a\", \"b\", and \"c\" are distinct and {\"a\", \"b\", \"c\"} is a set of indiscernibles, then, for example, for each binary formula formula_1, we must have\n\nHistorically, the identity of indiscernibles was one of the laws of thought of Gottfried Leibniz.\n\nIn some contexts one considers the more general notion of order-indiscernibles, and the term sequence of indiscernibles often refers implicitly to this weaker notion. In our example of binary formulas, to say that the triple (\"a\", \"b\", \"c\") of distinct elements is a sequence of indiscernibles implies\n\nOrder-indiscernibles feature prominently in the theory of Ramsey cardinals, Erdős cardinals, and Zero sharp.\n\n"}
{"id": "462730", "url": "https://en.wikipedia.org/wiki?curid=462730", "title": "Inscribed angle", "text": "Inscribed angle\n\nIn geometry, an inscribed angle is the angle formed in the interior of a circle when two secant lines (or, in a degenerate case, when one secant line and one tangent line of that circle) intersect on the circle. It can also be defined as the angle subtended at a point on the circle by two given points on the circle.\n\nEquivalently, an inscribed angle is defined by two chords of the circle sharing an endpoint.\n\nThe inscribed angle theorem relates the measure of an inscribed angle to that of the central angle subtending the same arc.\n\nThe inscribed angle theorem states that an angle \"θ\" inscribed in a circle is half of the central angle 2\"θ\" that subtends the same arc on the circle. Therefore, the angle does not change as its vertex is moved to different positions on the circle.\n\nLet \"O\" be the center of a circle, as in the diagram at right. Choose two points on the circle, and call them \"V\" and \"A\". Draw line \"VO\" and extended past \"O\" so that it intersects the circle at point \"B\" which is diametrically opposite the point \"V\". Draw an angle whose vertex is point \"V\" and whose sides pass through points \"A\" and \"B\".\n\nDraw line \"OA\". Angle \"BOA\" is a central angle; call it \"θ\". Lines \"OV\" and \"OA\" are both radii of the circle, so they have equal lengths. Therefore, triangle \"VOA\" is isosceles, so angle \"BVA\" (the inscribed angle) and angle \"VAO\" are equal; let each of them be denoted as \"ψ\".\n\nAngles \"BOA\" and \"AOV\" are supplementary. They add up to 180°, since line \"VB\" passing through \"O\" is a straight line. Therefore, angle \"AOV\" measures 180° − \"θ\".\n\nIt is known that the three angles of a triangle add up to 180°, and the three angles of triangle \"VOA\" are:\n\nTherefore,\n\nSubtract 180° from both sides,\n\nwhere \"θ\" is the central angle subtending arc \"AB\" and \"ψ\" is the inscribed angle subtending arc \"AB\".\n\nGiven a circle whose center is point \"O\", choose three points \"V\", \"C\", and \"D\" on the circle. Draw lines \"VC\" and \"VD\": angle \"DVC\" is an inscribed angle. Now draw line \"VO\" and extend it past point \"O\" so that it intersects the circle at point \"E\". Angle \"DVC\" subtends arc \"DC\" on the circle.\n\nSuppose this arc includes point \"E\" within it. Point \"E\" is diametrically opposite to point \"V\". Angles \"DVE\" and \"EVC\" are also inscribed angles, but both of these angles have one side which passes through the center of the circle, therefore the theorem from the above Part 1 can be applied to them.\n\nTherefore,\n\nthen let\n\nso that\n\nDraw lines \"OC\" and \"OD\". Angle \"DOC\" is a central angle, but so are angles \"DOE\" and \"EOC\", and\n\nLet\n\nso that\n\nFrom Part One we know that formula_13 and that formula_14. Combining these results with equation (2) yields\n\ntherefore, by equation (1),\n\nThe previous case can be extended to cover the case where the measure of the inscribed angle is the \"difference\" between two inscribed angles as discussed in the first part of this proof.\n\nGiven a circle whose center is point \"O\", choose three points \"V\", \"C\", and \"D\" on the circle. Draw lines \"VC\" and \"VD\": angle \"DVC\" is an inscribed angle. Now draw line \"VO\" and extend it past point \"O\" so that it intersects the circle at point \"E\". Angle \"DVC\" subtends arc \"DC\" on the circle.\n\nSuppose this arc does not include point \"E\" within it. Point \"E\" is diametrically opposite to point \"V\". Angles \"DVE\" and \"EVC\" are also inscribed angles, but both of these angles have one side which passes through the center of the circle, therefore the theorem from the above Part 1 can be applied to them.\n\nTherefore,\nthen let\nso that\n\nDraw lines \"OC\" and \"OD\". Angle \"DOC\" is a central angle, but so are angles \"DOE\" and \"EOC\", and\nLet\nso that\n\nFrom Part One we know that formula_13 and that formula_14. Combining these results with equation (4) yields\ntherefore, by equation (3),\n\nBy a similar argument, the angle between a chord and the tangent line at one of its intersection points equals half of the central angle subtended by the chord. See also Tangent lines to circles.\n\nThe inscribed angle theorem is used in many proofs of elementary Euclidean geometry of the plane. A special case of the theorem is Thales' theorem, which states that the angle subtended by a diameter is always 90°, i.e., a right angle. As a consequence of the theorem, opposite angles of cyclic quadrilaterals sum to 180°; conversely, any quadrilateral for which this is true can be inscribed in a circle. As another example, the inscribed angle theorem is the basis for several theorems related to the power of a point with respect to a circle. Further, it allows one to prove that when two chords intersect in a circle, the products of the lengths of their pieces are equal.\n\nInscribed angle theorems exist for ellipses, hyperbolas and parabolas, too. The essential differences are the measurements of an angle. (An angle is considered as a pair of intersecting lines.)\n\n\n"}
{"id": "23476429", "url": "https://en.wikipedia.org/wiki?curid=23476429", "title": "Intersection (set theory)", "text": "Intersection (set theory)\n\nIn mathematics, the intersection \"A\" ∩ \"B\" of two sets \"A\" and \"B\" is the set that contains all elements of \"A\" that also belong to \"B\" (or equivalently, all elements of \"B\" that also belong to \"A\"), but no other elements.\n\nFor explanation of the symbols used in this article, refer to the table of mathematical symbols.\n\nThe intersection of two sets \"A\" and \"B\", denoted by , is the set of all objects that are members of both the sets and .\nIn symbols,\n\nThat is, \"x\" is an element of the intersection \"A\" ∩ \"B\" if and only if \"x\" is both an element of \"A\" and an element of \"B\".\n\nFor example:\n\nIntersection is an associative operation; that is, for any sets \"A\", \"B\", and \"C\", one has \"A\" ∩ (\"B\" ∩ \"C\") = (\"A\" ∩ \"B\") ∩ \"C\". Intersection is also commutative; for any \"A\" and \"B\", one has \"A\" ∩ \"B\" = \"B\" ∩ \"A.\" It thus makes sense to talk about intersections of multiple sets. The intersection of \"A\", \"B\", \"C\", and \"D\", for example, is unambiguously written \"A\" ∩ \"B\" ∩ \"C\" ∩ \"D\".\n\nInside a universe \"U\" one may define the complement \"A\" of \"A\" to be the set of all elements of \"U\" not in \"A\". Now the intersection of \"A\" and \"B\" may be written as the complement of the union of their complements, derived easily from De Morgan's laws:<br>\n\"A\" ∩ \"B\" = (\"A\" ∪ \"B\")\n\nWe say that \"A intersects (meets) B at an element x\" if \"x\" belongs to \"A\" and \"B\". We say that \"A intersects (meets) B\" if \"A\" intersects B at some element. \"A\" intersects \"B\" if their intersection is inhabited.\n\nWe say that \"A and B are disjoint\" if \"A\" does not intersect \"B\". In plain language, they have no elements in common. \"A\" and \"B\" are disjoint if their intersection is empty, denoted formula_2.\n\nFor example, the sets {1, 2} and {3, 4} are disjoint, while the set of even numbers intersects the set of multiples of 3 at the multiples of 6.\n\nThe most general notion is the intersection of an arbitrary \"nonempty\" collection of sets.\nIf \"M\" is a nonempty set whose elements are themselves sets, then \"x\" is an element of the \"intersection\" of \"M\" if and only if for every element \"A\" of \"M\", \"x\" is an element of \"A\".\nIn symbols:\n\nThe notation for this last concept can vary considerably. Set theorists will sometimes write \"⋂\"M\"\", while others will instead write \"⋂\"A\"\".\nThe latter notation can be generalized to \"⋂ \"A\"\", which refers to the intersection of the collection {\"A\" : \"i\" ∈ \"I\"}.\nHere \"I\" is a nonempty set, and \"A\" is a set for every \"i\" in \"I\".\n\nIn the case that the index set \"I\" is the set of natural numbers, notation analogous to that of an infinite product may be seen:\n\nWhen formatting is difficult, this can also be written \"\"A\" ∩ \"A\" ∩ \"A\" ∩ ...\". This last example, an intersection of countably many sets, is actually very common; for an example see the article on σ-algebras.\n\nNote that in the previous section we excluded the case where \"M\" was the empty set (∅). The reason is as follows: The intersection of the collection \"M\" is defined as the set (see set-builder notation)\nIf \"M\" is empty there are no sets \"A\" in \"M\", so the question becomes \"which \"x\"<nowiki>'</nowiki>s satisfy the stated condition?\" The answer seems to be \"every possible x\". When \"M\" is empty the condition given above is an example of a vacuous truth. So the intersection of the empty family should be the universal set (the identity element for the operation of intersection) \n\nUnfortunately, according to standard (ZFC) set theory, the universal set does not exist. A fix for this problem can be found if we note that the intersection over a set of sets is always a subset of the union over that set of sets. This can symbolically be written as\nTherefore, we can modify the definition slightly to\nNow if \"M\" is empty there is no problem. The intersection is the empty set, because the union over the empty set is the empty set. In fact, this is the operation that we would have defined in the first place if we were defining the set in ZFC, as except for the operations defined by the axioms (the power set of a set, for instance), every set must be defined as the subset of some other set or by replacement.\n\n\n"}
{"id": "31872403", "url": "https://en.wikipedia.org/wiki?curid=31872403", "title": "Kai Behrend", "text": "Kai Behrend\n\nKai Behrend is a German mathematician. He is a professor at the University of British Columbia in Vancouver, British Columbia, Canada.\n\nHis work is in algebraic geometry and he has made important contributions in the theory of algebraic stacks, Gromov–Witten invariants and Donaldson–Thomas theory (cf. Behrend function.) He is also known for Behrend's formula, the generalization of the Grothendieck–Lefschetz trace formula to algebraic stacks.\n\nHe is the recipient of the 2001 Coxeter–James Prize, the 2011 Jeffery–Williams Prize, and the 2015 CRM-Fields-PIMS Prize. He was elected to the 2018 class of fellows of the American Mathematical Society.\n\n\n"}
{"id": "29673674", "url": "https://en.wikipedia.org/wiki?curid=29673674", "title": "Kervaire manifold", "text": "Kervaire manifold\n\nIn mathematics, specifically in differential topology, a Kervaire manifold \"K\" is a piecewise-linear manifold of dimension 4\"n\"+2 constructed by by plumbing together the tangent bundles of two 2\"n\"+1-spheres, and then gluing a ball to the result. In 10 dimensions this gives a piecewise-linear manifold with no smooth structure.\n\n\n"}
{"id": "10228498", "url": "https://en.wikipedia.org/wiki?curid=10228498", "title": "Lissajous orbit", "text": "Lissajous orbit\n\nIn orbital mechanics, a Lissajous orbit (), named after Jules Antoine Lissajous, is a quasi-periodic orbital trajectory that an object can follow around a Lagrangian point of a three-body system without requiring any propulsion. Lyapunov orbits around a Lagrangian point are curved paths that lie entirely in the plane of the two primary bodies. In contrast, Lissajous orbits include components in this plane and perpendicular to it, and follow a Lissajous curve. Halo orbits also include components perpendicular to the plane, but they are periodic, while Lissajous orbits are not.\n\nIn practice, any orbits around Lagrangian points , , or are dynamically unstable, meaning small departures from equilibrium grow over time. As a result, spacecraft in these Lagrangian point orbits must use their propulsion systems to perform orbital station-keeping. Although they are not perfectly stable, a modest effort of station keeping keeps a spacecraft in a desired Lissajous orbit for a long time.\n\nIn the absence of other influences, orbits about Lagrangian points and are dynamically stable so long as the ratio of the masses of the two main objects is greater than about 25 , meaning the natural dynamics keep the spacecraft (or natural celestial body) in the vicinity of the Lagrangian point, without energy input such as use of a propulsion system, even when slightly perturbed from equilibrium. These orbits can however be destabilized by other nearby massive objects. It has been found for example that the and points in the Earth–Moon system would be stable for billions of years, even with perturbations from the sun, but because of smaller perturbations by the planets, orbits around these points can last only a few million years.\n\nSeveral missions have used Lissajous orbits: ACE at Sun–Earth , SOHO at Sun-Earth , DSCOVR at Sun–Earth , WMAP at Sun–Earth , and also the Genesis mission collecting solar particles at .\nOn 14 May 2009, the European Space Agency (ESA) launched into space the Herschel and Planck observatories, both of which use Lissajous orbits at Sun–Earth .\nESA's current Gaia mission also uses a Lissajous orbit at Sun–Earth .\nIn 2011, NASA transferred two of its THEMIS spacecraft from Earth orbit to Lunar orbit by way of Earth-Moon and Lissajous orbits.\nChina's lunar orbiter Chang'e 2 left lunar orbit on June 8, 2011 and was placed in a Lissajous orbit at Earth-Sun until mid-2012 prior to leaving Earth orbit entirely to fly by the asteroid 4179 Toutatis.\n\nIn the 2005 science fiction novel \"Sunstorm\" by Arthur C. Clarke and Stephen Baxter, a huge shield is constructed in space to protect the Earth from a deadly solar storm. The shield is described to have been in a Lissajous orbit at . In the story a group of wealthy and powerful people shelter opposite the shield at so as to be protected from the solar storm by the shield, the Earth and the Moon.\n"}
{"id": "50250840", "url": "https://en.wikipedia.org/wiki?curid=50250840", "title": "List of Fields Medal winners by university affiliation", "text": "List of Fields Medal winners by university affiliation\n\nThe following list comprehensively shows Fields Medal winners by university affiliations since 1936 (as of 2018, 60 winners in total). This list considers Fields medalists as equal individuals, regardless of the total number of winners who received the medal each time at the quadrennial International Congress of Mathematicians (ICM). It does not include affiliations with research institutes such as IAS and MSRI in the United States, as well as IHES and CNRS in France. In this list, universities are presented in descending order starting from those affiliated with most Fields medal winners.\n\nThe university affiliations in this list are all official academic affiliations such as degree programs and official academic employment. Non-academic affiliations such as advisory committee and administrative staff are generally excluded. The official academic affiliations fall into three categories: 1) \"Alumni (graduate & attendee)\", 2) \"Long-term academic Staff\", and 3) \"Short-term academic staff\". Graduates are defined as those who hold Bachelor's, Master's, Doctorate or equivalent degrees from a university, while attendees are those who formally enrolled in degree programs at a university but did not complete the programs; thus, honorary degrees, summer attendees, exchange students and auditing students are excluded. The category of \"Long-term academic staff\" consists of tenure/tenure-track and equivalent academic positions, while that of \"Short-term academic staff\" consists of lecturers (without tenure), postdoctoral researchers, visiting professors/scholars (visitors), and equivalent academic positions. At any university, the specific academic title solely determines the type of affiliation, regardless of the actual time the position was held by a medalist.\n\nFurther explanations on \"visitors\" under \"Short-term academic staff\" are now presented. 1) All informal/personal visits are excluded from the list; 2) all employment-based visiting positions, which carry teaching/research duties, are included as affiliations in the list; 3) as for award/honor-based visiting positions, this list takes a conservative view and includes the positions as affiliations only if the medalists were required to assume employment-level duty (teaching/research) or the medalists specifically classified the visiting positions as \"appointment\" or similar in reliable sources such as their curriculum vitae. To be specific, some award/honor-based visiting positions such as the \"Shiing-Shen Chern Visiting Professorship\" in UC Berkeley are awards/honors/recognition without employment-level duty; attending meetings and giving public lectures, talks or non-curricular seminars are not employment-level duties. Finally, summer visitors are generally excluded from the list unless summer work yielded significant end products such as research publications and components of Fields-winning work, since summer terms are not part of formal academic years; the same rule applies to extension schools of universities.\n\nThe number following a person's name is the year he/she received the Fields Medal; in particular, a number with asterisk (*) means the person received the award while he/she was working at the institution (including emeritus staff) containing that asterisk. A name underlined implies that this person has been listed for a same institution previously (i.e., multiple affiliations). If a person had multiple positions under one category, only the position with highest rank is considered.\n\nThis list, together with List of Nobel laureates by university affiliation and List of Turing Award laureates by university affiliation, presents the university affiliations of people who have won highest honors in fundamental academic disciplines.\n\nAccording to Wikipedia policies on and , it is impossible in Wikipedia to assign various weights to different types of affiliations. Hence, all types of affiliations count equally in the following table and throughout the whole page.\nAccording to Wikipedia policies on and , it is impossible in Wikipedia to assign various weights to different types of affiliations. Hence, all types of affiliations count equally in the following table and throughout the whole page.\n\n\n"}
{"id": "33434715", "url": "https://en.wikipedia.org/wiki?curid=33434715", "title": "List of Laplace transforms", "text": "List of Laplace transforms\n\nThe following is a list of Laplace transforms for many common functions of a single variable. The Laplace transform is an integral transform that takes a function of a positive real variable (often time) to a function of a complex variable (frequency).\n\nThe Laplace transform of a function formula_1 can be obtained using the formal definition of the Laplace transform. However, some properties of the Laplace transform can be used to obtain the Laplace transform of some functions more easily.\n\nBecause the Laplace transform is a linear operator, the Laplace transform of a sum is the sum of Laplace transforms of each term.\n\nAlso, this implies that the Laplace transform of a multiple of a function is that multiple times the Laplace transformation of that function.\n\nThe Laplace transform of formula_4 is formula_5.\n\nformula_6 is the Laplace transform of formula_7.\n\nThe unilateral Laplace transform takes as input a function whose time domain is the non-negative reals, which is why all of the time domain functions in the table below are multiples of the Heaviside step function, .\n\nThe entries of the table that involve a time delay are required to be causal (meaning that ). A causal system is a system where the impulse response is zero for all time prior to . In general, the region of convergence for causal systems is not the same as that of anticausal systems.\n\nThe following functions and variables are used in the table below:\n\n\n </math> \n\n"}
{"id": "14528017", "url": "https://en.wikipedia.org/wiki?curid=14528017", "title": "List of incomplete proofs", "text": "List of incomplete proofs\n\nThis page lists notable examples of incomplete published mathematical proofs. Most of these were accepted as correct for several years but later discovered to contain gaps. There are both examples where a complete proof was later found and where the alleged result turned out to be false.\n\nThis section lists examples of proofs that were published and accepted as complete before a gap or error was found in them. It does not include any of the many incomplete attempted solutions by amateurs of famous problems such as Fermat's last theorem or the squaring of the circle. It also does not include unpublished preprints that were withdrawn because an error was found before publication.\n\nThe examples are arranged roughly in order of the publication date of the incomplete proof. Several of the examples on the list were taken from answers to questions on the MathOverflow site, listed in the external links below. The examples use the following symbols:\n<br>\n\n\n\n\n\n"}
{"id": "38188845", "url": "https://en.wikipedia.org/wiki?curid=38188845", "title": "List of things named after Issai Schur", "text": "List of things named after Issai Schur\n\nThis is a list of things named after Issai Schur.\n\n"}
{"id": "12457518", "url": "https://en.wikipedia.org/wiki?curid=12457518", "title": "Management cybernetics", "text": "Management cybernetics\n\nManagement cybernetics is the application of cybernetics to management and organizations. \"Management cybernetics\" was first introduced by Stafford Beer in the late 1950s. Beer developed the theory through a combination of practical applications and a series of influential books. The practical applications involved steel production, publishing and operations research in a large variety of different industries.\n\nAs practiced by Beer, research into operations involved multidisciplinary teams seeking practical assistance for difficult managerial issues. It often involved the development of models borrowed from basic sciences and put into an isomorphic relationships with an organizational situation. Beer initially called this \"operations research\" (OR) but, along with Russell Ackoff, became increasingly disenchanted with that term as the field transitioned into one in which a predefined set of mathematical tools was applied to well-formulated problems. Beer's critique of traditional OR, in part, was that it became a matter of experts in mathematics looking for situations that could be conformed to their methods. Beer insisted that what was needed for effective research into operations was to first understand the key dynamics within the situation and only then to select the theory or methods that would allow one to understand that situation in detail. Beer's \"Decision and Control\", especially chapter six, discusses the methodology in some detail.\n\nViable means capable of independent existence and implies both maintaining internal stability and adaptation to a changing environment. \"Internal stability\" and \"adaptation\" can be in conflict, particularly if the relevant environment is changing rapidly, so the viable system model (VSM) is about maintaining a balance between the two such that the system is able to survive.\n\nThe VSM is a model of the structures and functions that are both necessary and sufficient for the long term survival of a system in a changing environment. Allenna Leonard, Beer's longtime partner, suggested that the most useful way to think about the VSM is as a language. The VSM is a language of viability. The VSM is a language for diagnosing organizations and managerial teams in terms of their viability or lack thereof. The VSM is also a language for designing organizations and managerial teams that will be viable.\n\nOne of the great difficulties in managing the modern large organization is that many of the issues are far too complex for even small groups. The critical knowledge is often dispersed among a substantial number of people. Organizations are often faced with choosing between 1) very costly and time-consuming meetings of large groups or 2) making bad decisions based on an inadequate grasp of the relevant factors. Syntegration is a group method designed to solve this conundrum.\n\nSyntegration melds a number of cybernetic principles with Bucky Fuller's ideas on tensegrity. The initial \"team syntegrity\" format involved 30 people divided into 12 overlapping teams to deal with some broad and initially ill-defined issue. The teams and roles within the teams are arranged to achieve the mathematically optimum degree of resonance of information throughout the entire group. In practice, syntegration achieves a remarkable degree of shared understanding of the initial issue. In syntegrations intended to develop a plan of action, the implementation phase is usually very quick and effective, probably because of the shared understanding developed among the participants.\n\nThe literature on management cybernetics is extensive. Beer wrote hundreds of papers and about ten books. Others have contributed perhaps an equal amount. Barry Clemson, at Beer's urging, wrote an introduction to management cybernetics. Patrick Hoverstadt wrote an introduction using real-life examples.\n\nOrganizational cybernetics (OC) is distinguished from management cybernetics. Both use many of the same terms but interpret them according to another philosophy of systems thinking. The full flowering of management cybernetics is represented in Beer's books\n\nOrganizational cybernetics studies organizational design, and the regulation and self-regulation of organizations from a systems theory perspective that also takes the social dimension into consideration. Extending the principles of autonomous agency theory (AAT), cultural agency theory (CAT) has been formulated for the generation of higher cybernetic orders.\nResearchers in economics, public administration and political science focus on the changes in institutions, organisation and mechanisms of social steering at various levels (sub-national, national, European, international) and in different sectors (including the private, semi-private and public sectors; the latter sector is emphasised).\n\n\n\n"}
{"id": "27291631", "url": "https://en.wikipedia.org/wiki?curid=27291631", "title": "Maximilien Winter", "text": "Maximilien Winter\n\nMaximilien Winter (1871–1935) was a French philosopher of mathematics.\n\nIn 1893 Winter helped Xavier Léon to found the \"Revue de métaphysique et de morale\". After the First World War Winter ran the \"Supplément\" of the \"Revue\" until his death in 1935.\n\n"}
{"id": "48589694", "url": "https://en.wikipedia.org/wiki?curid=48589694", "title": "Maximum Downside Exposure (MDE)", "text": "Maximum Downside Exposure (MDE)\n\nMaximum downside exposure (MDE) values the maximum downside to the portfolio. In other words, it tells you the most your portfolio could lose in the event of a catastrophe. As such, MDE obviates the need to worry about the market's unpredictable swings as it virtually eliminates downside surprises.\n\nThe formula is simple and easy to calculate: MDE = unhedged exposure/total portfolio value. For example, if you have half of your funds in inflation-protected cash, and the other half in stocks, you can’t possibly lose more than 50% of your money – that’s your portfolio’s MDE.\n\nThe main benefit of MDE is that – unlike probabilistic risk models (such as VaR) – it appropriately factors in all risks to the portfolio without looking at historical (and often erroneous) data and relying on simplistic statistical assumptions that don’t correspond to the real world. This makes MDE a very robust risk management tool.\n\n"}
{"id": "474118", "url": "https://en.wikipedia.org/wiki?curid=474118", "title": "Module homomorphism", "text": "Module homomorphism\n\nIn algebra, a module homomorphism is a function between modules that preserves module structures. Explicitly, if \"M\" and \"N\" are left modules over a ring \"R\", then a function formula_1 is called a module homomorphism or an \"R\"-linear map if for any \"x\", \"y\" in \"M\" and \"r\" in \"R\",\nIf \"M\", \"N\" are right modules, then the second condition is replaced with\n\nThe pre-image of the zero element under \"f\" is called the kernel of \"f\". The set of all module homomorphisms from \"M\" to \"N\" is denoted by Hom(\"M\", \"N\"). It is an abelian group (under pointwise addition) but is not necessarily a module unless \"R\" is commutative.\n\nThe composition of module homomorphisms is again a module homomorphism. Thus, all the (say left) modules together with all the module homomorphisms between them form the category of modules.\n\nA module homomorphism is called an isomorphism if it admits an inverse homomorphism; in particular, it is a bijection. One can show a bijective module homomorphism is an isomorphism; i.e., the inverse is a module homomorphism. In particular, a module homomorphism is an isomorphism if and only if it is an isomorphism between the underlying abelian groups.\n\nThe isomorphism theorems hold for module homomorphisms.\n\nA module homomorphism from a module \"M\" to itself is called an endomorphism and an isomorphism from \"M\" to itself an automorphism. One writes formula_5 for the set of all endomorphisms between a module \"M\". It is not only an abelian group but is also a ring with multiplication given by function composition, called the endomorphism ring of \"M\".\n\nSchur's lemma says that a homomorphism between simple modules (a module having only two submodules) must be either zero or an isomorphism. In particular, the endomorphism ring of a simple module is a division ring.\n\nIn the language of the category theory, an injective homomorphism is also called a monomorphism and a surjective homomorphism an epimorphism.\n\n\nIn short, Hom inherits a ring action that was not \"used up\" to form Hom. More precise, let \"M\", \"N\" be left \"R\"-modules. Suppose \"M\" has a right action of a ring \"S\" that commutes with the \"R\"-action; i.e., \"M\" is an (\"R\", \"S\")-module. Then\nhas the structure of a left \"S\"-module defined by: for \"s\" in \"S\" and \"x\" in \"M\",\nIt is well-defined (i.e., formula_21 is \"R\"-linear) since\nSimilarly, formula_21 is a ring action since\n\nNote: the above verification would \"fail\" if one used the left \"R\"-action in place of the right \"S\"-action. In this sense, Hom is often said to \"use up\" the \"R\"-action.\n\nSimilarly, if \"M\" is a left \"R\"-module and \"N\" is an (\"R\", \"S\")-module, then formula_19 is a right \"S\"-module by formula_26.\n\nThe relationship between matrices and linear transformations in linear algebra generalizes in a natural way to module homomorphisms. Precisely, given a right \"R\"-module \"U\", there is the canonical isomorphism of the abelian groups\nobtained by viewing formula_28 consisting of column vectors and then writing \"f\" as an \"m\" × \"n\" matrix. In particular, viewing \"R\" as a right \"R\"-module, one has\nwhich turns out to be a ring isomorphism.\n\nNote the above isomorphism is canonical; no choice is involved. On the other hand, if one is given a module homomorphism between finite-rank free modules, then a choice of an ordered basis corresponds to a choice of an isomorphism formula_30. The above procedure then gives the matrix representation with respect to such choices of the bases. For more general modules, matrix representations may either lack uniqueness or not exist.\n\nIn practice, one often defines a module homomorphism by specifying its values on a generating set of a module. More precise, let \"M\" and \"N\" be left \"R\"-modules. Suppose a subset \"S\" generates \"M\"; i.e., there is a surjection formula_31 with a free module \"F\" with a basis indexed by \"S\" and kernel \"K\" (i.e., the free presentation). Then to give a module homomorphism formula_32 is to give a module homomorphism formula_33 that kills \"K\" (i.e., maps \"K\" to zero).\n\nIf formula_1 and formula_35 are module homomorphisms, then their direct sum is\nand their tensor product is\n\nLet formula_1 be a module homomorphism between left modules. The graph Γ of \"f\" is the submodule of \"M\" ⊕ \"N\" given by\nwhich is the image of the graph morphism \n\nThe transpose of \"f\" is\nIf \"f\" is an isomorphism, then the transpose of the inverse of \"f\" is called the contragredient of \"f\".\n\nA short sequence of modules over a commutative ring\nconsists of modules \"A\", \"B\", \"C\" and homomorphisms \"f\", \"g\". It is exact if \"f\" is injective, the kernel of \"g\" is the image of \"f\" and \"g\" is surjective. A longer exact sequence is defined in the similar way. A sequence of modules is exact if and only if it is exact as a sequence of abelian groups. Also the sequence is exact if and only if it is exact at all the maximal ideals:\nwhere the subscript formula_43 means the localization of a module at formula_43.\n\nAny module homomorphism \"f\" fits into\nwhere \"K\" is the kernel of \"f\" and \"C\" is the cokernel, the quotien of \"N\" by the image of \"f\".\n\nIf formula_46 are module homomorphisms, then they are said to form a fiber square (or pullback square), denoted by \"M\" × \"N\", if it fits into\nwhere formula_48.\n\nExample: Let formula_49 be commutative rings, and let \"I\" be the annihilator of the quotient \"B\"-module \"A\"/\"B\" (which is an ideal of \"A\"). Then canonical maps formula_50 form a fiber square with formula_51\n\nLet formula_52 be an endomorphism between finitely generated \"R\"-modules for a commutative ring \"R\". Then\n\nSee also: Herbrand quotient (which can be defined for any endomorphism with some finiteness conditions.)\n\nAn additive relation formula_32 from a module \"M\" to a module \"N\" is a submodule of formula_56 In other words, it is a \"many-valued\" homomorphism defined on some submodule of \"M\". The inverse formula_57 of \"f\" is the submodule formula_58. Any additive relation \"f\" determines a homomorphism from a submodule of \"M\" to a quotient of \"N\"\nwhere formula_60 consists of all elements \"x\" in \"M\" such that (\"x\", \"y\") belongs to \"f\" for some \"y\" in \"N\".\n\nA transgression that arises from a spectral sequence is an example of an additive relation.\n\n\n"}
{"id": "13233639", "url": "https://en.wikipedia.org/wiki?curid=13233639", "title": "Nasik magic hypercube", "text": "Nasik magic hypercube\n\nA Nasik magic hypercube is a magic hypercube with the added restriction that all possible lines through each cell sum correctly to formula_1 where \"S\" = the magic constant, \"m\" = the order and \"n\" = the dimension, of the hypercube.\n\nOr, to put it more concisely, all pan-\"r\"-agonals sum correctly for \"r\" = 1...\"n\".\n\nThe above definition is the same as the Hendricks definition of perfect, but different from the Boyer/Trump definition. See Perfect magic cube\n\nA Nasik magic cube is a magic cube with the added restriction that all 13\"m\" possible lines sum correctly to the magic constant. This class of magic cube is commonly called perfect (John Hendricks definition.). See Magic cube classes.\nHowever, the term \"perfect\" is ambiguous because it is also used for other types of magic cubes. Perfect magic cube demonstrates just one example of this.<br>\nThe term \"nasik\" would apply to all dimensions of magic hypercubes in which the number of correctly summing paths (lines) through any cell of the hypercube is \"P\" = (3- 1)/2\n\nA pandiagonal magic square then would be a \"nasik\" square because 4 magic line pass through each of the \"m\"cells. This was A.H. Frost’s original definition of nasik.<br>\nA \"nasik\" magic cube would have 13 magic lines passing through each of its \"m\" cells. (This cube also contains 9\"m\" pandiagonal magic squares of order \"m\".)<br>\nA \"nasik\" magic tesseract would have 40 lines passing through each of its \"m\" cells.<br> And so on.\n\nIn 1866 and 1878, Rev. A. H. Frost coined the term \"Nasik\" for the type of magic square we commonly call \"pandiagonal\" and often call \"perfect\". He then demonstrated the concept with an order-7 cube we now class as \"pandiagonal\", and an order-8 cube we class as \"pantriagonal\".<br>\nIn another 1878 paper he showed another \"pandiagonal\" magic cube and a cube where all 13\"m\" lines sum correctly i.e. Hendricks \"perfect\".\nHe referred to all of these cubes as nasik as a respect to the great Indian Mathematician D R Kaprekar who hails from Deolali in Nasik District in Maharashtra, India.\nIn 1905 Dr. Planck expanded on the nasik idea in his Theory of Paths Nasik. In the introductory to his paper, he wrote;\nIn 1917, Dr. Planck wrote again on this subject.\nIn 1939, B. Rosser and R. J. Walker published a series of papers on diabolic (perfect) magic squares and cubes. They specifically mentioned that these cubes contained 13\"m\" correctly summing lines. They also had 3\"m\" pandiagonal magic squares parallel to the faces of the cube, and 6\"m\" pandiagonal magic squares parallel to the triagonal planes.\n\n\n"}
{"id": "52600654", "url": "https://en.wikipedia.org/wiki?curid=52600654", "title": "Olle Häggström", "text": "Olle Häggström\n\nOlle Häggström (born 4 October 1967) is a professor of mathematical statistics at Chalmers University. He is an elected member of the Royal Swedish Academy of Sciences.\n\nIn 2016, Häggström published (via Oxford University Press) \"Here Be Dragons: Science, Technology and the Future of Humanity\", an attempt to draft a road map of potential dangers that could be associated with various emerging technologies: \"There is no denying that advances in science and technology have brought us prosperity and improved our lives tremendously... but there is a flip side: some of the advances that may lie ahead of us can actually make us worse off.\"\n\nOn human enhancement, Häggström argues that any enhancement, from growth hormones to cognitive enhancement, can encourage an \"arms race\" in which everyone is compelled to participate for fear of falling behind: \"It is hard to imagine the US silently sitting still and watching a cognitive enhancement development that can turn China into the world’s military overlords.\" On geoengineering, Häggström discusses a proposed form of geoengineering that involves continuously pumping sulphur dioxide into the atmosphere to counteract global warming, and warns this could create a catastrophic risk of a massive temperature spike if a future generation were unable to continue pumping. On nanotechnology, Häggström discusses concerns about self-replicating nanobots, as well as the potential for emerging manufacturing technologies to undo existing gun control measures and to radically upscale the quantities of existing weaponry, and to create destabilizing new classes of weapons. Other topics include existential risks from high-energy physics experiments, as well as from advanced artificial intelligence: Häggström poses a scenario in which a superintelligent computer, aiming to maximise happiness in the universe, calculates that sentient beings are happy less than half the time, and proceeds to exterminate all sentient life, in order to increase the existing sum of happiness a negative number to zero. Häggström also discusses SETI, criticizing \"inexcusably reckless\" attempts to communicate with aliens.\n\nIn a positive review in \"New Scientist\", a reviewer raises the question: \"What if extraterrestrial advice could have saved us from some other danger, and we doomed civilisation by not asking?\" and cautions there are \"no easy answers\" to these questions.\n\n"}
{"id": "14893994", "url": "https://en.wikipedia.org/wiki?curid=14893994", "title": "Ordered weighted averaging aggregation operator", "text": "Ordered weighted averaging aggregation operator\n\nIn applied mathematics – specifically in fuzzy logic – the ordered weighted averaging (OWA) operators provide a parameterized class of mean type aggregation operators. They were introduced by Ronald R. Yager. Many notable mean operators such as the max, arithmetic average, median and min, are members of this class. They have been widely used in computational intelligence because of their ability to model linguistically expressed aggregation instructions.\n\nFormally an OWA operator of dimension formula_1 is a mapping formula_2 that has an associated collection of weights formula_3 lying in the unit interval and summing to one and with \n\nwhere formula_5 is the \"j\" largest of the formula_6.\n\nBy choosing different \"W\" one can implement different aggregation operators. The OWA operator is a non-linear operator as a result of the process of determining the \"b\".\n\nThe OWA operator is a mean operator. It is bounded, monotonic, symmetric, and idempotent, as defined below.\n\nTwo features have been used to characterize the OWA operators. The first is the attitudinal character(orness).\n\nThis is defined as\n\nIt is known that formula_19.\n\nIn addition \"A\" − \"C\"(max) = 1, A − C(ave) = A − C(med) = 0.5 and A − C(min) = 0. Thus the A − C goes from 1 to 0 as we go from Max to Min aggregation. The attitudinal character characterizes the similarity of aggregation to OR operation(OR is defined as the Max).\n\nThe second feature is the dispersion. This defined as\n\nAn alternative definition is formula_21 The dispersion characterizes how uniformly the arguments are being used\nÀĚ\n\nThe historical reconstruction of scientific development of the OWA field, the identification of the dominant direction of knowledge accumulation that emerged since the publication of the first OWA paper, and to discover the most active lines of research has recently been published, (see: http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full). The results suggest, as expected, that Yager's paper[1] (IEEE Trans. Systems Man Cybernet, 18(1), 183–190, 1988) is the most influential paper and the starting point of all other research using OWA. Starting from his contribution, other lines of research developed and we describe them. Full list of papers published in OWA is also available at http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full)\n\nThe above Yager's OWA operators are used to aggregate the crisp values. Can we aggregate fuzzy sets in the OWA mechanism ? The\nType-1 OWA operators have been proposed for this purpose. So the type-1 OWA operators provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.\n\nThe type-1 OWA operator is defined according to the alpha-cuts of fuzzy sets as follows:\n\nGiven the \"n\" linguistic weights formula_22 in the form of fuzzy sets defined on the domain of discourse formula_23, then for each formula_24, an formula_25-level type-1 OWA operator with formula_25-level sets formula_27 to aggregate the formula_25-cuts of fuzzy sets formula_29 is given as\n\nwhere formula_31, and formula_32 is a permutation function such that formula_33, i.e., formula_34 is the formula_35th largest\nelement in the set formula_36.\n\nThe computation of the type-1 OWA output is implemented by computing the left end-points and right end-points of the intervals formula_37:\nformula_38 and formula_39\nwhere formula_40. Then membership function of resulting aggregation fuzzy set is:\n\nFor the left end-points, we need to solve the following programming problem:\n\nwhile for the right end-points, we need to solve the following programming problem:\n\nThis paper has presented a fast method to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently.\n\n"}
{"id": "1283725", "url": "https://en.wikipedia.org/wiki?curid=1283725", "title": "Orthant", "text": "Orthant\n\nIn geometry, an orthant or hyperoctant is the analogue in \"n\"-dimensional Euclidean space of a quadrant in the plane or an octant in three dimensions.\n\nIn general an orthant in \"n\"-dimensions can be considered the intersection of \"n\" mutually orthogonal half-spaces. By independent selections of half-space signs, there are 2 orthants in \"n\"-dimensional space.\n\nMore specifically, a closed orthant in R is a subset defined by constraining each Cartesian coordinate to be nonnegative or nonpositive. Such a subset is defined by a system of inequalities:\nwhere each ε is +1 or −1.\n\nSimilarly, an open orthant in R is a subset defined by a system of strict inequalities\nwhere each ε is +1 or −1.\n\nBy dimension:\n\nJohn Conway defined the term \"n\"-orthoplex from orthant complex as a regular polytope in n-dimensions with 2 simplex facets, one per orthant.\n\nThe nonnegative orthant is the generalization of the first quadrant to n dimensions and is important in many constrained optimization problems.\n\n\n\n[[Mohammad Amin Ahmadi]"}
{"id": "21278306", "url": "https://en.wikipedia.org/wiki?curid=21278306", "title": "Package (UML)", "text": "Package (UML)\n\nA package in the Unified Modeling Language is used \"to group elements, and to provide a namespace for the grouped elements\". A package may contain other packages, thus providing for a hierarchical organization of packages.\n\nPretty much all UML elements can be grouped into packages. Thus, classes, objects, use cases, components, nodes, node instances etc. can all be organized as packages, thus enabling a manageable organization of the myriad elements that a real-world UML model entails.\n\nWhen organizing functional models (use case models, workflow models etc.), use packages to model the real-world modular structure of the system being modeled. When organizing source code, use packages to represent the different layers of the source code. For instance:\n\nWhen organizing component models, use packages to group the components according to ownership and/or reuse possibilities. For instance:\n\nWhen organizing deployment models, use packages to represent the different types of deployment environments that you will be modeling. For instance:\n"}
{"id": "23666", "url": "https://en.wikipedia.org/wiki?curid=23666", "title": "Prime number", "text": "Prime number\n\nA prime number (or a prime) is a natural number greater than 1 that cannot be formed by multiplying two smaller natural numbers. A natural number greater than 1 that is not prime is called a composite number. For example, 5 is prime because the only ways of writing it as a product, or , involve 5 itself.\nHowever, 6 is composite because it is the product of two numbers () that are both smaller than 6. Primes are central in number theory because of the fundamental theorem of arithmetic: every natural number greater than 1 is either a prime itself or can be factorized as a product of primes that is unique up to their order.\n\nThe property of being prime is called primality. A simple but slow method of checking the primality of a given number formula_1, called trial division, tests whether formula_1 is a multiple of any integer between 2 and formula_3. Faster algorithms include the Miller–Rabin primality test, which is fast but has a small chance of error, and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical. Particularly fast methods are available for numbers of special forms, such as Mersenne numbers. the largest known prime number has 23,249,425 decimal digits.\n\nThere are infinitely many primes, as demonstrated by Euclid around 300 BC. No known simple formula separates prime numbers from composite numbers. However, the distribution of primes within the natural numbers in the large can be statistically modelled. The first result in that direction is the prime number theorem, proven at the end of the 19th century, which says that the probability of a randomly chosen number being prime is inversely proportional to its number of digits, that is, to its logarithm.\n\nSeveral historical questions regarding prime numbers are still unsolved. These include Goldbach's conjecture, that every even integer greater than 2 can be expressed as the sum of two primes, and the twin prime conjecture, that there are infinitely many pairs of primes having just one even number between them. Such questions spurred the development of various branches of number theory, focusing on analytic or algebraic aspects of numbers. Primes are used in several routines in information technology, such as public-key cryptography, which relies on the difficulty of factoring large numbers into their prime factors. In abstract algebra, objects that behave in a generalized way like prime numbers include prime elements and prime ideals.\n\nA natural number (1, 2, 3, 4, 5, 6, etc.) is called a prime number (or a prime) if it is greater than 1 and cannot be written as a product of two natural numbers that are both smaller than it. The numbers greater than 1 that are not prime are called composite numbers. In other words, formula_1 is prime if formula_1 items cannot be divided up into smaller equal-size groups of more than one item, or if it is not possible to arrange formula_1 dots into a rectangular grid that is more than one dot wide and more than one dot high.\nFor example, among the numbers 1 through 6, the numbers 2, 3, and 5 are the prime numbers, as there are no other numbers that divide them evenly (without a remainder).\n1 is not prime, as it is specifically excluded in the definition. and are both composite.\nThe divisors of a natural number formula_1 are the numbers that divide formula_1 evenly.\nEvery natural number has both 1 and itself as a divisor. If it has any other divisor, it cannot be prime. This idea leads to a different but equivalent definition of the primes: they are the numbers with exactly two positive divisors, 1 and the number itself.\nYet another way to say the same thing is that a number formula_1 is prime if it is greater than one and if none of the numbers formula_10 divides formula_1 evenly.\n\nThe first 25 prime numbers (all the prime numbers less than 100) are:\n\nNo even number formula_1 greater than 2 is prime because any such number can be expressed as the product formula_13. Therefore, every prime number other than 2 is an odd number, and is called an \"odd prime\". Similarly, when written in the usual decimal system, all prime numbers larger than 5 end in 1, 3, 7, or 9. The numbers that end with other digits are all composite:\ndecimal numbers that end in 0, 2, 4, 6, or 8 are even, and decimal numbers that end in 0 or 5 are divisible by 5.\n\nThe set of all primes is sometimes denoted by formula_14 (a boldface capital P) or by formula_15 (a blackboard bold capital P).\n\nThe Rhind Mathematical Papyrus, from around 1550 BC, has Egyptian fraction expansions of different forms for prime and composite numbers. However, the earliest surviving records of the explicit study of prime numbers come from Ancient Greek mathematics. Euclid's \"Elements\" (circa 300 BC) proves the infinitude of primes and the fundamental theorem of arithmetic, and shows how to construct a perfect number from a Mersenne prime. Another Greek invention, the Sieve of Eratosthenes, is still used to construct lists of primes.\nAround 1000 AD, the Islamic mathematician Alhazen found Wilson's theorem, characterizing the prime numbers as the numbers formula_1 that evenly divide formula_17. Alhazen also conjectured that all even perfect numbers come from Euclid's construction using Mersenne primes, but was unable to prove it. Another Islamic mathematician, Ibn al-Banna' al-Marrakushi, observed that the sieve of Eratosthenes can be sped up by testing only the divisors up to the square root of the largest number to be tested. Fibonacci brought the innovations from Islamic mathematics back to Europe. His book \"Liber Abaci\" (1202) was the first to describe trial division for testing primality, again using divisors only up to the square root.\n\nIn 1640 Pierre de Fermat stated (without proof) Fermat's little theorem (later proved by Leibniz and Euler). Fermat also investigated the primality of the Fermat numbers \nformula_18, and Marin Mersenne studied the Mersenne primes, prime numbers of the form formula_19 with formula_20 itself a prime. Christian Goldbach formulated Goldbach's conjecture, that every even number is the sum of two primes, in a 1742 letter to Euler. Euler proved Alhazen's conjecture (now the Euclid–Euler theorem) that all even perfect numbers can be constructed from Mersenne primes. He introduced methods from mathematical analysis to this area in his proofs of the infinitude of the primes and the divergence of the sum of the reciprocals of the primes formula_21.\nAt the start of the 19th century, Legendre and Gauss conjectured that as formula_22 tends to infinity, the number of primes up to formula_22 is asymptotic to formula_24, where formula_25 is the natural logarithm of formula_22. Ideas of Riemann in his 1859 paper on the zeta-function sketched an outline for proving this. Although the closely related Riemann hypothesis remains unproven, Riemann's outline was completed in 1896 by Hadamard and de la Vallée Poussin, and the result is now known as the prime number theorem. Another important 19th-century result was Dirichlet's theorem on arithmetic progressions, that certain arithmetic progressions contain infinitely many primes.\n\nMany mathematicians have worked on primality tests for numbers larger than those where trial division is practicably applicable. Methods that are restricted to specific number forms include Pépin's test for Fermat numbers (1877), Proth's theorem (around 1878), the Lucas–Lehmer primality test (originated 1856), and the generalized Lucas primality test.\nSince 1951 all the largest known primes have been found using these tests on computers. The search for ever larger primes has generated interest outside mathematical circles, through the Great Internet Mersenne Prime Search and other distributed computing projects. The idea that prime numbers had few applications outside of pure mathematics was shattered in the 1970s when public-key cryptography and the RSA cryptosystem were invented, using prime numbers as their basis.\nThe increased practical importance of computerized primality testing and factorization led to the development of improved methods capable of handling large numbers of unrestricted form. The mathematical theory of prime numbers also moved forward with the Green–Tao theorem (2004) on long arithmetic progressions of prime numbers, and Yitang Zhang's 2013 proof that there exist infinitely many prime gaps of bounded size.\n\nMost early Greeks did not even consider 1 to be a number, so they could not consider its primality. A few mathematicians from this time also considered the prime numbers to be a subdivision of the odd numbers, so they also did not consider 2 to be prime. However, Euclid and a majority of the other Greek mathematicians considered 2 as prime. The medieval Islamic mathematicians largely followed the Greeks in viewing 1 as not being a number.\nBy the Middle Ages and Renaissance mathematicians began treating 1 as a number, and some of them included it as the first prime number. In the mid-18th century Christian Goldbach listed 1 as prime in his correspondence with Leonhard Euler; however, Euler himself did not consider 1 to be prime. In the 19th century many mathematicians still considered 1 to be prime, and lists of primes that included 1 continued to be published as recently as 1956.\n\nIf the definition of a prime number were changed to call 1 a prime, many statements involving prime numbers would need to be reworded in a more awkward way. For example, the fundamental theorem of arithmetic would need to be rephrased in terms of factorizations into primes greater than 1, because every number would have multiple factorizations with different numbers of copies of 1. Similarly, the sieve of Eratosthenes would not work correctly if it handled 1 as a prime, because it would eliminate all multiples of 1 (that is, all other numbers) and output only the single number 1. Some other more technical properties of prime numbers also do not hold for the number 1: for instance, the formulas for Euler's totient function or for the sum of divisors function are different for prime numbers than they are for 1. By the early 20th century, mathematicians began to agree that 1 should not be listed as prime, but rather in its own special category as a \"unit\".\n\nWriting a number as a product of prime numbers is called a \"prime factorization\" of the number. For example:\nThe terms in the product are called prime factors. The same prime factor may occur more than once; this example has two copies of the prime factor formula_28.\nWhen a prime occurs multiple times, exponentiation can be used to group together multiple copies of the same prime number: for instance, in the second way of writing the product above, formula_29 denotes the square or second power of formula_28.\n\nThe central importance of prime numbers to number theory and mathematics in general stems from the \"fundamental theorem of arithmetic\". This theorem states that every integer larger than 1 can be written as a product of one or more primes. More strongly, \nthis product is unique in the sense that any two prime factorizations of the same number will have the same numbers of copies of the same primes,\nalthough their ordering may differ. So, although there are many different ways of finding a factorization using an integer factorization algorithm, they all must produce the same result. Primes can thus be considered the \"basic building blocks\" of the natural numbers.\n\nSome proofs of the uniqueness of prime factorizations are based on Euclid's lemma: If formula_20 is a prime number and formula_20 divides a product formula_33 of integers formula_34 and formula_35, then formula_20 divides formula_34 or formula_20 divides formula_35 (or both). Conversely, if a number formula_20 has the property that when it divides a product it always divides at least one factor of the product, then formula_20 must be prime.\n\nThere are infinitely many prime numbers. Another way of saying this is that the sequence\nof prime numbers never ends. This statement is referred to as \"Euclid's theorem\" in honor of the ancient Greek mathematician Euclid, since the first known proof for this statement is attributed to him. Many more proofs of the infinitude of primes are known, including an analytical proof by Euler, Goldbach's proof based on Fermat numbers, Furstenberg's proof using general topology, and Kummer's elegant proof.\n\nEuclid's proof shows that every finite list of primes is incomplete. The key idea is to multiply together the primes in any given list and add formula_42. If the list consists of the primes\nformula_43, this gives the number\nBy the fundamental theorem, formula_45 has a prime factorization\nwith one or more prime factors. formula_45 is evenly divisible by each of these factors, but formula_45 has a remainder of one when divided by any of the prime numbers in the given list, so none of the prime factors of formula_45 can be in the given list. Because there is no finite list of all the primes, there must be infinitely many primes.\n\nThe numbers formed by adding one to the products of the smallest primes are called Euclid numbers. The first five of them are prime, but the sixth,\nis a composite number.\n\nThere is no known efficient formula for primes. For example, there is no non-constant polynomial, even in several variables, that takes \"only\" prime values. However, there are numerous expressions that do encode all primes, or only primes. One possible formula is based on Wilson's theorem and generates the number 2 many times and all other primes exactly once. There is also a set of Diophantine equations in nine variables and one parameter with the following property: the parameter is prime if and only if the resulting system of equations has a solution over the natural numbers. This can be used to obtain a single formula with the property that all its \"positive\" values are prime.\n\nOther examples of prime-generating formulas come from Mills' theorem and a theorem of Wright. These assert that there are real constants formula_51 and formula_52 such that\nare prime for any natural number formula_1 in the first formula, and any number of exponents in the second formula. Here formula_55 represents the floor function, the largest integer less than or equal to the number in question. However, these are not useful for generating primes, as the primes must be generated first in order to compute the values of formula_56 or formula_52.\n\nMany conjectures revolving about primes have been posed. Often having an elementary formulation, many of these conjectures have withstood proof for decades: all four of Landau's problems from 1912 are still unsolved. One of them is Goldbach's conjecture, which asserts that every even integer formula_1 greater than 2 can be written as a sum of two primes. , this conjecture has been verified for all numbers up to formula_59. Weaker statements than this have been proven, for example, Vinogradov's theorem says that every sufficiently large odd integer can be written as a sum of three primes. Chen's theorem says that every sufficiently large even number can be expressed as the sum of a prime and a semiprime, the product of two primes. Also, any even integer can be written as the sum of six primes. The branch of number theory studying such questions is called additive number theory.\n\nAnother type of problem concerns prime gaps, the differences between consecutive primes.\nThe existence of arbitrarily large prime gaps can be seen by noting that the sequence formula_60 consists of formula_61 composite numbers, for any natural number formula_1. However, large prime gaps occur much earlier than this argument shows. For example, the first prime gap of length 8 is between the primes 89 and 97, much smaller than formula_63. It is conjectured that there are infinitely many twin primes, pairs of primes with difference 2; this is the twin prime conjecture. Polignac's conjecture states more generally that for every positive integer formula_64, there are infinitely many pairs of consecutive primes that differ by formula_65.\nAndrica's conjecture, Brocard's conjecture, Legendre's conjecture, and Oppermann's conjecture all suggest that the largest gaps between primes from formula_42 to formula_1 should be at most approximately formula_3, a result that is known to follow from the Riemann hypothesis,\nwhile the much stronger Cramér conjecture sets the largest gap size at formula_69.\nPrime gaps can be generalized to prime formula_64-tuples, patterns in the differences between more than two prime numbers. Their infinitude and density are the subject of the first Hardy–Littlewood conjecture, which can be motivated by the heuristic that the prime numbers behave similarly to a random sequence of numbers with density given by the prime number theorem.\n\nAnalytic number theory studies number theory through the lens of continuous functions, limits, infinite series, and the related mathematics of the infinite and infinitesimal.\n\nThis area of study began with Leonhard Euler and his first major result, the solution to the Basel problem.\nThe problem asked for the value of the infinite sum formula_71\nwhich today can be recognized as the value formula_72 of the Riemann zeta function. This function is closely connected to the prime numbers and to one of the most significant unsolved problems in mathematics, the Riemann hypothesis. Euler showed that formula_73.\nThe reciprocal of this number, formula_74, is the limiting probability that two random numbers selected uniformly from a large range are relatively prime (have no factors in common).\n\nThe distribution of primes in the large, such as the question how many primes are smaller than a given, large threshold, is described by the prime number theorem, but no efficient formula for the formula_1-th prime is known.\nDirichlet's theorem on arithmetic progressions, in its basic form, asserts that linear polynomials\nwith relatively prime integers formula_34 and formula_35 take infinitely many prime values. Stronger forms of the theorem state that the sum of the reciprocals of these prime values diverges, and that different linear polynomials with the same formula_35 have approximately the same proportions of primes.\nAlthough conjectures have been formulated about the proportions of primes in higher-degree polynomials, they remain unproven, and it is unknown whether there exists a quadratic polynomial that (for integer arguments) is prime infinitely often.\n\nEuler's proof that there are infinitely many primes considers the sums of reciprocals of primes,\n\nEuler showed that, for any arbitrary real number formula_22, there exists a prime formula_20 for which this sum is bigger than formula_22. This shows that there are infinitely many primes, because if there were finitely many primes the sum would reach its maximum value at the biggest prime rather than growing past every formula_22. \nThe growth rate of this sum is described more precisely by Mertens' second theorem. For comparison, the sum\n\ndoes not grow to infinity as formula_1 goes to infinity (see the Basel problem). In this sense, prime numbers occur more often than squares of natural numbers,\nalthough both sets are infinite. Brun's theorem states that the sum of the reciprocals of twin primes,\n\nis finite. Because of Brun's theorem, it is not possible to use Euler's method to solve the twin prime conjecture, that there exist infinitely many twin primes.\n\nThe prime counting function formula_88 is defined as the number of primes not greater than formula_1. For example, formula_90, since there are five primes less than or equal to 11. Methods such as the Meissel–Lehmer algorithm can compute exact values of formula_88 faster than it would be possible to list each prime up to formula_1. The prime number theorem states that formula_88 is asymptotic to formula_94, which is denoted as\nand means that the ratio of formula_88 to the right-hand fraction approaches 1 as formula_1 grows to infinity. This implies that the likelihood that a randomly chosen number less than formula_1 is prime is (approximately) inversely proportional to the number of digits in formula_1.\nIt also implies that the formula_1th prime number is proportional to formula_101\nand therefore that the average size of a prime gap is proportional to formula_102.\nA more accurate estimate for formula_88 is given by the offset logarithmic integral\n\nAn arithmetic progression is a finite or infinite sequence of numbers such that consecutive numbers in the sequence all have the same difference. This difference is called the modulus of the progression. For example,\nis an infinite arithmetic progression with modulus 9. In an arithmetic progression, all the numbers have the same remainder when divided by the modulus; in this example, the remainder is 3. Because both the modulus 9 and the remainder 3 are multiples of 3, so is every element in the sequence. Therefore, this progression contains only one prime number, 3 itself. In general, the infinite progression\ncan have more than one prime only when its remainder formula_34 and modulus formula_107 are relatively prime. If they are relatively prime, Dirichlet's theorem on arithmetic progressions asserts that the progression contains infinitely many primes.\n\nThe Green–Tao theorem shows that there are arbitrarily long finite arithmetic progressions consisting only of primes.\n\nEuler noted that the function\nyields prime numbers for formula_109, although composite numbers appear among its later values. The search for an explanation for this phenomenon led to the deep algebraic number theory of Heegner numbers and the class number problem. The Hardy-Littlewood conjecture F predicts the density of primes among the values of quadratic polynomials with integer coefficients\nin terms of the logarithmic integral and the polynomial coefficients. No quadratic polynomial has been proven to take infinitely many prime values.\n\nThe Ulam spiral arranges the natural numbers in a two-dimensional grid, spiraling in concentric squares surrounding the origin with the prime numbers highlighted. Visually, the primes appear to cluster on certain diagonals and not others, suggesting that some quadratic polynomials take prime values more often than others.\n\nOne of the most famous unsolved questions in mathematics, dating from 1859, and one of the Millennium Prize Problems, is the Riemann hypothesis, which asks where the zeros of the Riemann zeta function formula_110 are located.\nThis function is an analytic function on the complex numbers. For complex numbers formula_111 with real part greater than one it equals both an infinite sum over all integers, and an infinite product over the prime numbers,\nThis equality between a sum and a product, discovered by Euler, is called an Euler product. The Euler product can be derived from the fundamental theorem of arithmetic, and shows the close connection between the zeta function and the prime numbers.\nIt leads to another proof that there are infinitely many primes: if there were only finitely many,\nthen the sum-product equality would also be valid at formula_113, but the sum would diverge (it is the harmonic series formula_114) while the product would be finite, a contradiction.\n\nThe Riemann hypothesis states that the zeros of the zeta-function are all either negative even numbers, or complex numbers with real part equal to 1/2. The original proof of the prime number theorem was based on a weak form of this hypothesis, that there are no zeros with real part equal to 1, although other more elementary proofs have been found.\nThe prime-counting function can be expressed by Riemann's explicit formula as a sum in which each term comes from one of the zeros of the zeta function; the main term of this sum is the logarithmic integral, and the remaining terms cause the sum to fluctuate above and below the main term.\nIn this sense, the zeros control how regularly the prime numbers are distributed. If the Riemann hypothesis is true, these fluctuations will be small, and the\nasymptotic distribution of primes given by the prime number theorem will also hold over much shorter intervals (of length about the square root of formula_22 for intervals near a number formula_22).\n\nModular arithmetic modifies usual arithmetic by only using the numbers formula_117, for a natural number formula_1 called the modulus.\nAny other natural number can be mapped into this system by replacing it by its remainder after division by formula_1.\nModular sums, differences and products are calculated by performing the same replacement by the remainder\non the result of the usual sum, difference, or product of integers. Equality of integers corresponds to \"congruence\" in modular arithmetic:\nformula_22 and formula_121 are congruent (written formula_122 mod formula_1) when they have the same remainder after division by formula_1. However, in this system of numbers, division by all nonzero numbers is possible if and only if the modulus is prime. For instance, with the prime number formula_125 as modulus, division by formula_28 is possible: formula_127, because clearing denominators by multiplying both sides by formula_28 gives the valid formula formula_129. However, with the composite modulus formula_130, division by formula_28 is impossible. There is no valid solution to formula_132: clearing denominators by multiplying by formula_28 causes the left-hand side to become formula_134 while the right-hand side becomes either formula_135 or formula_28.\nIn the terminology of abstract algebra, the ability to perform division means that modular arithmetic modulo a prime number forms a field or, more specifically, a finite field, while other moduli only give a ring but not a field.\n\nSeveral theorems about primes can be formulated using modular arithmetic. For instance, Fermat's little theorem states that if\nformula_137 (mod formula_20), then formula_139 (mod formula_20).\nSumming this over all choices of formula_34 gives the equation\nvalid whenever formula_20 is prime.\nGiuga's conjecture says that this equation is also a sufficient condition for formula_20 to be prime.\nWilson's theorem says that an integer formula_145 is prime if and only if the factorial formula_146 is congruent to formula_147 mod formula_20. For a composite this cannot hold, since one of its factors divides both and formula_149, and so formula_150 is impossible.\n\nThe formula_20-adic order formula_152 of an integer formula_1 is the number of copies of formula_20 in the prime factorization of formula_1. The same concept can be extended from integers to rational numbers by defining the formula_20-adic order of a fraction formula_157 to be formula_158. The formula_20-adic absolute value formula_160 of any rational number formula_107 is then defined as\nformula_162. Multiplying an integer by its formula_20-adic absolute value cancels out the factors of formula_20 in its factorization, leaving only the other primes. Just as the distance between two real numbers can be measured by the absolute value of their distance, the distance between two rational numbers can be measured by their formula_20-adic distance, the formula_20-adic absolute value of their difference. For this definition of distance, two numbers are close together (they have a small distance) when their difference is divisible by a high power of formula_20. In the same way that the real numbers can be formed from the rational numbers and their distances, by adding extra limiting values to form a complete field, the rational numbers with the formula_20-adic distance can be extended to a different complete field, the formula_20-adic numbers.\n\nThis picture of an order, absolute value, and complete field derived from them can be generalized to algebraic number fields and their valuations (certain mappings from the multiplicative group of the field to a totally ordered additive group, also called orders), absolute values (certain multiplicative mappings from the field to the real numbers, also called norms), and places (extensions to complete fields in which the given field is a dense set, also called completions). The extension from the rational numbers to the real numbers, for instance, is a place in which the distance between numbers is the usual absolute value of their difference. The corresponding mapping to an additive group would be the logarithm of the absolute value, although this does not meet all the requirements of a valuation. According to Ostrowski's theorem, up to a natural notion of equivalence, the real numbers and formula_20-adic numbers, with their orders and absolute values, are the only valuations, absolute values, and places on the rational numbers. The local-global principle allows certain problems over the rational numbers to be solved by piecing together solutions from each of their places, again underlining the importance of primes to number theory.\n\nA commutative ring is an algebraic structure where addition, subtraction and multiplication are defined. The integers are a ring, and the prime numbers in the integers have been generalized to rings in two different ways, \"prime elements\" and \"irreducible elements\". An element formula_20 of a ring formula_172 is called prime if it is nonzero, has no multiplicative inverse (that is, it is not a unit), and satisfies the following requirement: whenever formula_20 divides the product formula_174 of two elements of formula_172, it also divides at least one of formula_22 or formula_121. An element is irreducible if it is neither a unit nor the product of two other non-unit elements. In the ring of integers, the prime and irreducible elements form the same set,\nIn an arbitrary ring, all prime elements are irreducible. The converse does not hold in general, but does hold for unique factorization domains.\n\nThe fundamental theorem of arithmetic continues to hold (by definition) in unique factorization domains. An example of such a domain is the Gaussian integers formula_179, the ring of complex numbers of the form formula_180 where formula_181 denotes the imaginary unit and formula_34 and formula_35 are arbitrary integers. Its prime elements are known as Gaussian primes. Not every number that is prime among the integers remains prime in the Gaussian integers; for instance, the number 2 can be written as a product of the two Gaussian primes formula_184 and formula_185. Rational primes (the prime elements in the integers) congruent to 3 mod 4 are Gaussian primes, but rational primes congruent to 1 mod 4 are not. This is a consequence of Fermat's theorem on sums of two squares,\nwhich states that an odd prime formula_20 is expressible as the sum of two squares, formula_187, and therefore factorizable as formula_188, exactly when formula_20 is 1 mod 4.\n\nNot every ring is a unique factorization domain. For instance, in the ring of numbers formula_190 (for integers formula_34 and formula_35) the number formula_193 has two factorizations formula_194, where neither of the four factors can be reduced any further, so it does not have a unique factorization. In order to extend unique factorization to a larger class of rings, the notion of a number can be replaced with that of an ideal, a subset of the elements of a ring that contains all sums of pairs of its elements, and all products of its elements with ring elements.\n\"Prime ideals\", which generalize prime elements in the sense that the principal ideal generated by a prime element is a prime ideal, are an important tool and object of study in commutative algebra, algebraic number theory and algebraic geometry. The prime ideals of the ring of integers are the ideals (0), (2), (3), (5), (7), (11), … The fundamental theorem of arithmetic generalizes to the Lasker–Noether theorem, which expresses every ideal in a Noetherian commutative ring as an intersection of primary ideals, which are the appropriate generalizations of prime powers.\n\nThe spectrum of a ring is a geometric space whose points are the prime ideals of the ring. Arithmetic geometry also benefits from this notion, and many concepts exist in both geometry and number theory. For example, factorization or ramification of prime ideals when lifted to an extension field, a basic problem of algebraic number theory, bears some resemblance with ramification in geometry. These concepts can even assist with in number-theoretic questions solely concerned with integers. For example, prime ideals in the ring of integers of quadratic number fields can be used in proving quadratic reciprocity, a statement that concerns the existence of square roots modulo integer prime numbers.\nEarly attempts to prove Fermat's Last Theorem led to Kummer's introduction of regular primes, integer prime numbers connected with the failure of unique factorization in the cyclotomic integers.\nThe question of how many integer prime numbers factor into a product of multiple prime ideals in an algebraic number field is addressed by Chebotarev's density theorem, which (when applied to the cyclotomic integers) has Dirichlet's theorem on primes in arithmetic progressions as a special case.\n\nIn the theory of finite groups the Sylow theorems imply that, if a power of a prime number formula_195 divides the order of a group, then it has a subgroup of order formula_195. By Lagrange's theorem, any group of prime order is a cyclic group,\nand by the Burnside theorem any group whose order is divisible by only two primes is solvable.\n\nFor a long time, number theory in general, and the study of prime numbers in particular, was seen as the canonical example of pure mathematics, with no applications outside of mathematics with the exception of use of prime numbered gear teeth to distribute wear evenly. In particular, number theorists such as British mathematician G. H. Hardy prided themselves on doing work that had absolutely no military significance.\n\nThis vision of the purity of number theory was shattered in the 1970s, when it was publicly announced that prime numbers could be used as the basis for the creation of public key cryptography algorithms.\nThese applications have led to significant study of algorithms for computing with prime numbers, and in particular of primality testing, methods for determining whether a given number is prime.\nThe most basic primality testing routine, trial division, is too slow to be useful for large numbers. One group of modern primality tests is applicable to arbitrary numbers, while more efficient tests are available for numbers of special types. Most primality tests only tell whether their argument is prime or not. Routines that also provide a prime factor of composite arguments (or all of its prime factors) are called factorization algorithms.\nPrime numbers are also used in computing for checksums, hash tables, and pseudorandom number generators.\n\nThe most basic method of checking the primality of a given integer formula_1 is called \"trial division\". This method divides formula_1 by each integer from 2 up to the square root of formula_1. Any such integer dividing formula_1 evenly establishes formula_1 as composite; otherwise it is prime.\nIntegers larger than the square root do not need to be checked because, whenever formula_202, one of the two factors formula_34 and formula_35 is less than or equal to the square root of formula_1. Another optimization is to check only primes as factors in this range.\nFor instance, to check whether 37 is prime, this method divides it by the primes in the range from 2 to , which are 2, 3, and 5. Each division produces a nonzero remainder, so 37 is indeed prime.\n\nAlthough this method is simple to describe, it is impractical for testing the primality of large integers, because the number of tests that it performs grows exponentially as a function of the number of digits of these integers. However, trial division is still used, with a smaller limit than the square root on the divisor size, to quickly discover composite numbers with small factors, before using more complicated methods on the numbers that pass this filter.\n\nBefore computers, mathematical tables listing all of the primes or prime factorizations up to a given limit were commonly printed. The oldest method for generating a list of primes is called the sieve of Eratosthenes. The animation shows an optimized variant of this method.\nAnother more efficient sieving method for the same problem is the sieve of Atkin. In advanced mathematics, sieve theory applies similar methods to other problems.\n\nSome of the fastest modern tests for whether an arbitrary given number formula_1 is prime are probabilistic (or Monte Carlo) algorithms, meaning that they have a small random chance of producing an incorrect answer.\nFor instance the Solovay–Strassen primality test on a given number formula_20 chooses a number formula_34 randomly from formula_42 to formula_210 and uses modular exponentiation to check\nwhether formula_211 is divisible by formula_20. If so, it answers yes and otherwise it answers no. If formula_20 really is prime, it will always answer yes, but if formula_20 is composite then it answers yes with probability at most 1/2 and no with probability at least 1/2.\nIf this test is repeated formula_1 times on the same number,\nthe probability that a composite number could pass the test every time is at most formula_216. Because this decreases exponentially with the number of tests, it provides high confidence (although not certainty) that a number that passes the repeated test is prime. On the other hand, if the test ever fails, then the number is certainly composite.\nA composite number that passes such a test is called a pseudoprime.\n\nIn contrast, some other algorithms guarantee that their answer will always be correct: primes will always be determined to be prime and composites will always be determined to be composite.\nFor instance, this is true of trial division.\nThe algorithms with guaranteed-correct output include both deterministic (non-random) algorithms, such as the AKS primality test,\nand randomized Las Vegas algorithms where the random choices made by the algorithm do not affect its final answer, such as some variations of elliptic curve primality proving.\nThe elliptic curve primality test is the fastest in practice of the guaranteed-correct primality tests, but its runtime analysis is based on heuristic arguments rather than rigorous proofs. The AKS primality test has mathematically proven time complexity, but is slower than elliptic curve primality proving in practice. These methods can be used to generate large random prime numbers, by generating and testing random numbers until finding one that is prime;\nwhen doing this, a faster probabilistic test can quickly eliminate most composite numbers before a guaranteed-correct algorithm is used to verify that the remaining numbers are prime.\n\nThe following table lists some of these tests. Their running time is given in terms of formula_1, the number to be tested and, for probabilistic algorithms, the number formula_64 of tests performed. Moreover, formula_219 is an arbitrarily small positive number, and log is the logarithm to an unspecified base. The big O notation means that each time bound should be multiplied by a constant factor to convert it from dimensionless units to units of time; this factor depends on implementation details such as the type of computer used to run the algorithm, but not on the input parameters formula_1 and formula_64.\n\nIn addition to the aforementioned tests that apply to any natural number, some numbers of a special form can be tested for primality more quickly.\nFor example, the Lucas–Lehmer primality test can determine whether a Mersenne number (one less than a power of two) is prime, deterministically,\nin the same time as a single iteration of the Miller–Rabin test. This is why since 1992 () the largest \"known\" prime has always been a Mersenne prime.\nIt is conjectured that there are infinitely many Mersenne primes.\n\nThe following table gives the largest known primes of various types. Some of these primes have been found using distributed computing. In 2009, the Great Internet Mersenne Prime Search project was awarded a US$100,000 prize for first discovering a prime with at least 10 million digits. The Electronic Frontier Foundation also offers $150,000 and $250,000 for primes with at least 100 million digits and 1 billion digits, respectively.\n\nGiven a composite integer formula_1, the task of providing one (or all) prime factors is referred to as \"factorization\" of formula_1. It is significantly more difficult than primality testing, and although many factorization algorithms are known, they are slower than the fastest primality testing methods. Trial division and Pollard's rho algorithm can be used to find very small factors of formula_1, and elliptic curve factorization can be effective when formula_1 has factors of moderate size. Methods suitable for arbitrary large numbers that do not depend on the size of its factors include the quadratic sieve and general number field sieve. As with primality testing, there are also factorization algorithms that require their input to have a special form, including the special number field sieve. the largest number known to have been factored by a general-purpose algorithm is RSA-768, which has 232 decimal digits (768 bits) and is the product of two large primes.\n\nShor's algorithm can factor any integer in a polynomial number of steps on a quantum computer. However, current technology can only run this algorithm for very small numbers. the largest number that has been factored by a quantum computer running Shor's algorithm is 21.\n\nSeveral public-key cryptography algorithms, such as RSA and the Diffie–Hellman key exchange, are based on large prime numbers (2048-bit primes are common). RSA relies on the assumption that it is much easier (that is, more efficient) to perform the multiplication of two (large) numbers formula_22 and formula_121 than to calculate formula_22 and formula_121 (assumed coprime) if only the product formula_174 is known. The Diffie–Hellman key exchange relies on the fact that there are efficient algorithms for modular exponentiation (computing formula_231), while the reverse operation (the discrete logarithm) is thought to be a hard problem.\n\nPrime numbers are frequently used for hash tables. For instance the original method of Carter and Wegman for universal hashing was based on computing hash functions by choosing random linear functions modulo large prime numbers. Carter and Wegman generalized this method to formula_64-independent hashing by using higher-degree polynomials, again modulo large primes. As well as in the hash function, prime numbers are used for the hash table size in quadratic probing based hash tables to ensure that the probe sequence covers the whole table.\n\nSome checksum methods are based on the mathematics of prime numbers. For instance the checksums used in International Standard Book Numbers are defined by taking the rest of the number modulo 11, a prime number. Because 11 is prime this method can detect both single-digit errors and transpositions of adjacent digits. Another checksum method, Adler-32, uses arithmetic modulo 65521, the largest prime number less than formula_233.\nPrime numbers are also used in pseudorandom number generators including linear congruential generators and the Mersenne Twister.\n\nPrime numbers are of central importance to number theory but also have many applications to other areas within mathematics, including abstract algebra and elementary geometry. For example, it is possible to place prime numbers of points in a two-dimensional grid so that no three are in a line, or so that every triangle formed by three of the points has large area. Another example is Eisenstein's criterion, a test for whether a polynomial is irreducible based on divisibility of its coefficients by a prime number and its square.\nThe concept of prime number is so important that it has been generalized in different ways in various branches of mathematics. Generally, \"prime\" indicates minimality or indecomposability, in an appropriate sense. For example, the prime field of a given field is its smallest subfield that contains both 0 and 1. It is either the field of rational numbers or a finite field with a prime number of elements, whence the name. Often a second, additional meaning is intended by using the word prime, namely that any object can be, essentially uniquely, decomposed into its prime components. For example, in knot theory, a prime knot is a knot that is indecomposable in the sense that it cannot be written as the connected sum of two nontrivial knots. Any knot can be uniquely expressed as a connected sum of prime knots. The prime decomposition of 3-manifolds is another example of this type.\n\nBeyond mathematics and computing, prime numbers have potential connections to quantum mechanics, and have been used metaphorically in the arts and literature. They have also been used in evolutionary biology to explain the life cycles of cicadas.\n\nFermat primes are primes of the form\nwith formula_64 a natural number. They are named after Pierre de Fermat, who conjectured that all such numbers are prime. The first five of these numbers – 3, 5, 17, 257, and 65,537 – are prime, but formula_236 is composite and so are all other Fermat numbers that have been verified as of 2017. A regular formula_1-gon is constructible using straightedge and compass if and only if the odd prime factors of formula_1 (if any) are distinct Fermat primes. Likewise, a regular formula_1-gon may be constructed using straightedge, compass, and an angle trisector if and only if the prime factors of formula_1 are any number of copies of 2 or 3 together with a (possibly empty) set of distinct Pierpont primes, primes of the form formula_241.\n\nIt is possible to partition any convex polygon into formula_1 smaller convex polygons of equal area and equal perimeter, when formula_1 is a power of a prime number, but this is not known for other values of formula_1.\n\nBeginning with the work of Hugh Montgomery and Freeman Dyson in the 1970s, mathematicians and physicists have speculated that the zeros of the Riemann zeta function are connected to the energy levels of quantum systems. Prime numbers are also significant in quantum information science, thanks to mathematical structures such as mutually unbiased bases and symmetric informationally complete positive-operator-valued measures.\n\nThe evolutionary strategy used by cicadas of the genus \"Magicicada\" makes use of prime numbers. These insects spend most of their lives as grubs underground. They only pupate and then emerge from their burrows after 7, 13 or 17 years, at which point they fly about, breed, and then die after a few weeks at most. Biologists theorize that these prime-numbered breeding cycle lengths have evolved in order to prevent predators from synchronizing with these cycles.\nIn contrast, the multi-year periods between flowering in bamboo plants are hypothesized to be smooth numbers, having only small prime numbers in their factorizations.\n\nPrime numbers have influenced many artists and writers.\nThe French composer Olivier Messiaen used prime numbers to create ametrical music through \"natural phenomena\". In works such as \"La Nativité du Seigneur\" (1935) and \"Quatre études de rythme\" (1949–50), he simultaneously employs motifs with lengths given by different prime numbers to create unpredictable rhythms: the primes 41, 43, 47 and 53 appear in the third étude, \"Neumes rythmiques\". According to Messiaen this way of composing was \"inspired by the movements of nature, movements of free and unequal durations\".\n\nIn his science fiction novel \"Contact\", scientist Carl Sagan suggested that prime factorization could be used as a means of establishing two-dimensional image planes in communications with aliens, an idea that he had first developed informally with American astronomer Frank Drake in 1975. In the novel \"The Curious Incident of the Dog in the Night-Time\" by Mark Haddon, the narrator arranges the sections of the story by consecutive prime numbers as a way to convey the mental state of its main character, a mathematically gifted teen with Asperger syndrome. Prime numbers are used as a metaphor for loneliness and isolation in the Paolo Giordano novel \"The Solitude of Prime Numbers\", in which they are portrayed as \"outsiders\" among integers.\n\n\n"}
{"id": "1561666", "url": "https://en.wikipedia.org/wiki?curid=1561666", "title": "Principles and Standards for School Mathematics", "text": "Principles and Standards for School Mathematics\n\nPrinciples and Standards for School Mathematics (PSSM) are guidelines produced by the National Council of Teachers of Mathematics (NCTM) in 2000, setting forth recommendations for mathematics educators. They form a national vision for preschool through twelfth grade mathematics education in the US and Canada. It is the primary model for standards-based mathematics.\n\nThe NCTM employed a consensus process that involved classroom teachers, mathematicians, and educational researchers. The resulting document sets forth a set of six principles (Equity, Curriculum, Teaching, Learning, Assessment, and Technology) that describe NCTM's recommended framework for mathematics programs, and ten general strands or standards that cut across the school mathematics curriculum. These strands are divided into mathematics content (Number and Operations, Algebra, Geometry, Measurement, and Data Analysis and Probability) and processes (Problem Solving, Reasoning and Proof, Communication, Connections, and Representation). Specific expectations for student learning are described for ranges of grades (preschool to 2, 3 to 5, 6 to 8, and 9 to 12).\n\nThe \"Principles and Standards for School Mathematics\" was developed by the NCTM. The NCTM's stated intent was to improve mathematics education. The contents were based on surveys of existing curriculum materials, curricula and policies from many countries, educational research publications, and government agencies such as the U.S. National Science Foundation. The original draft was widely reviewed at the end of 1998 and revised in response to hundreds of suggestions from teachers.\n\nThe \"PSSM\" is intended to be \"a single resource that can be used to improve mathematics curricula, teaching, and assessment.\" The latest update was published in 2000. The \"PSSM\" is available as a book, and in hypertext format on the NCTM web site.\n\nThe \"PSSM\" replaces three prior publications by NCTM: \n\n\nTen general strands or standards of mathematics content and processes were defined that cut across the school mathematics curriculum. Specific expectations for student learning, derived from the philosophy of outcome-based education, are described for ranges of grades (preschool to 2, 3 to 5, 6 to 8, and 9 to 12). These standards were made an integral part of nearly all outcome-based education and later standards-based education reform programs that were widely adopted across the United States.\n\n\n\nIn 2006, NCTM issued a document called \"Curriculum Focal Points\" that presented the most critical mathematical topics for each grade in elementary and middle schools. American mathematics instruction tends to be diffuse and is criticized for including too many topics each year. In part, this publication is intended to assist teachers in identifying the most critical content for targeted attention. More such publications are planned.\n\nNCTM stated that \"Focal Points\" was a step in the implementation of the Standards, not a reversal of its position on teaching students to learn foundational topics with conceptual understanding. Contrary to the expectation of many textbook publishers and educational progressives, the 2006 Curriculum Focal Points strongly emphasized the importance of basic arithmetic skills in lower and middle grades. Because of this, the \"Curriculum Focal Points\" was perceived by the media as an admission that the PSSM had originally recommended, or at least had been interpreted as recommending, reduced instruction in basic arithmetic facts.\n\nThe 2006 Curriculum Focal Points identifies three critical areas at each grade level for pre-kindergarten through Grade 8. Samples of the specific focal points for three grades are below. (Note that the Simple Examples below are not quotes from the Focal Points, but are based on the descriptions of activities found in the Focal Points.)\n\nThe Focal Points define not only the recommended curriculum emphases, but also the ways in which students should learn them, as in the PSSM. An example of a complete description of one focal point is the following for fourth grade:\n\n\"Number and Operations\" and \"Algebra\": Developing quick recall of multiplication facts and related division facts and fluency with whole number multiplicationStudents use understandings of multiplication to develop quick recall of the basic multiplication facts and related division facts. They apply their understanding of models for multiplication (i.e., equal-sized groups, arrays, area models, equal intervals on the number line), place value, and properties of operations (in particular, the distributive property) as they develop, discuss, and use efficient, accurate, and generalizable methods to multiply multidigit whole numbers. They select appropriate methods and apply them accurately to estimate products or calculate them mentally, depending on the context and numbers involved. They develop fluency with efficient procedures, including the standard algorithm, for multiplying whole numbers, understand why the procedures work (on the basis of place value and properties of operations), and use them to solve problems.\n\nBecause most education agencies in the United States have adopted the NCTM recommendations to varying degrees, many textbook publishers promote their products as being compliant with the publishers' interpretations of the PSSM. However, the NCTM does not endorse, approve, or recommend any textbooks or other products and has never agreed that any textbook accurately represents their goals.\n\n\n"}
{"id": "4965178", "url": "https://en.wikipedia.org/wiki?curid=4965178", "title": "Quasi-triangular quasi-Hopf algebra", "text": "Quasi-triangular quasi-Hopf algebra\n\nA quasi-triangular quasi-Hopf algebra is a specialized form of a quasi-Hopf algebra defined by the Ukrainian mathematician Vladimir Drinfeld in 1989. It is also a generalized form of a quasi-triangular Hopf algebra.\n\nA quasi-triangular quasi-Hopf algebra is a set formula_1 where formula_2 is a quasi-Hopf algebra and formula_3 known as the R-matrix, is an invertible element such that \n\nso that formula_7 is the switch map and\n\nwhere formula_10 and formula_11.\n\nThe quasi-Hopf algebra becomes \"triangular\" if in addition, formula_12.\n\nThe twisting of formula_13 by formula_14 is the same as for a quasi-Hopf algebra, with the additional definition of the twisted \"R\"-matrix\n\nA quasi-triangular (resp. triangular) quasi-Hopf algebra with formula_15 is a quasi-triangular (resp. triangular) Hopf algebra as the latter two conditions in the definition reduce the conditions of quasi-triangularity of a Hopf algebra.\n\nSimilarly to the twisting properties of the quasi-Hopf algebra, the property of being quasi-triangular or triangular quasi-Hopf algebra is preserved by twisting.\n\n\n"}
{"id": "402673", "url": "https://en.wikipedia.org/wiki?curid=402673", "title": "Ray Solomonoff", "text": "Ray Solomonoff\n\nRay Solomonoff (July 25, 1926 – December 7, 2009) was the inventor of algorithmic probability, his General Theory of Inductive Inference (also known as Universal Inductive Inference), and was a founder of algorithmic information theory. He was an originator of the branch of artificial intelligence based on machine learning, prediction and probability. He circulated the first report on non-semantic machine learning in 1956.\n\nSolomonoff first described algorithmic probability in 1960, publishing the theorem that launched Kolmogorov complexity and algorithmic information theory. He first described these results at a Conference at Caltech in 1960, and in a report, Feb. 1960, \"A Preliminary Report on a General Theory of Inductive Inference.\" He clarified these ideas more fully in his 1964 publications, \"A Formal Theory of Inductive Inference,\" Part I and Part II.\n\nAlgorithmic probability is a mathematically formalized combination of Occam's razor, and the Principle of Multiple Explanations.\nIt is a machine independent method of assigning a probability value to each hypothesis (algorithm/program) that explains a given observation, with the simplest hypothesis (the shortest program) having the highest probability and the increasingly complex hypotheses receiving increasingly small probabilities.\n\nSolomonoff founded the theory of universal inductive inference, which is based on solid philosophical foundations and has its root in Kolmogorov complexity and algorithmic information theory. The theory uses algorithmic probability in a Bayesian framework. The universal prior is taken over the class of all computable measures; no hypothesis will have a zero probability. This enables Bayes' rule (of causation) to be used to predict the most likely next event in a series of events, and how likely it will be.\n\nAlthough he is best known for algorithmic probability and his general theory of inductive inference, he made many other important discoveries throughout his life, most of them directed toward his goal in artificial intelligence: to develop a machine that could solve hard problems using probabilistic methods.\n\nRay Solomonoff was born on July 25, 1926, in Cleveland, Ohio, son of Jewish Russian immigrants Phillip Julius and Sarah Mashman Solomonoff. He attended Glenville High School, graduating in 1944. In 1944 he joined the United States Navy as Instructor in Electronics. From 1947–1951 he attended the University of Chicago, studying under Professors such as Rudolf Carnap and Enrico Fermi, and graduated with an M.S. in Physics in 1951.\n\nFrom his earliest years he was motivated by the pure joy of mathematical discovery and by the desire to explore where no one had gone before. At age of 16, in 1942, he began to search for a general method to solve mathematical problems.\n\nIn 1952 he met Marvin Minsky, John McCarthy and others interested in machine intelligence. In 1956 Minsky and McCarthy and others organized the Dartmouth Summer Research Conference on Artificial Intelligence, where Ray was one of the original 10 invitees—he, McCarthy, and Minsky were the only ones to stay all summer. It was for this group that Artificial Intelligence was first named as a science. Computers at the time could solve very specific mathematical problems, but not much else. Ray wanted to pursue a bigger question, how to make machines more generally intelligent, and how computers could use probability for this purpose.\n\nHe wrote three papers, two with Anatol Rapoport, in 1950–52, that are regarded as the earliest statistical analysis of networks.\n\nHe was one of the 10 attendees at the 1956 Dartmouth Summer Research Project on Artificial Intelligence. He wrote and circulated a report among the attendees: \"An Inductive Inference Machine\". It viewed machine learning as probabilistic, with an emphasis on the importance of training sequences, and on the use of parts of previous solutions to problems in constructing trial solutions for new problems. He published a version of his findings in 1957. These were the first papers to be written on probabilistic Machine Learning.\n\nIn the late 1950s, he invented probabilistic languages and their associated grammars. A probabilistic language assigns a probability value to every possible string. \nGeneralizing the concept of probabilistic grammars led him to his discovery in 1960 of Algorithmic Probability and General Theory of Inductive Inference.\n\nPrior to the 1960s, the usual method of calculating probability was based on frequency: taking the ratio of favorable results to the total number of trials. In his 1960 publication, and, more completely, in his 1964 publications, Solomonoff seriously revised this definition of probability. He called this new form of probability \"Algorithmic Probability\" and showed how to use it for prediction in his theory of inductive inference. As part of this work, he produced the philosophical foundation for the use of Bayes rule of causation for prediction.\n\nThe basic theorem of what was later called Kolmogorov Complexity was part of his General Theory. Writing in 1960, he begins: \"Consider a very long sequence of symbols ...We shall consider such a sequence of symbols to be 'simple' and have a high a priori probability, if there exists a very brief description of this sequence – using, of course, some sort of stipulated description method. More exactly, if we use only the symbols 0 and 1 to express our description, we will assign the probability 2 to a sequence of symbols if its shortest possible binary description contains \"N\" digits.\"\n\nThe probability is with reference to a particular Universal Turing machine. Solomonoff showed and in 1964 proved that the choice of machine, while it could add a constant factor would not change the probability ratios very much. These probabilities are machine independent.\n\nIn 1965, the Russian mathematician Kolmogorov independently published similar ideas. When he became aware of Solomonoff's work, he acknowledged Solomonoff, and for several years, Solomonoff's work was better known in the Soviet Union than in the Western World. The general consensus in the scientific community, however, was to associate this type of complexity with Kolmogorov, who was more concerned with randomness of a sequence. Algorithmic Probability and Universal (Solomonoff) Induction became associated with Solomonoff, who was focused on prediction—the extrapolation of a sequence.\n\nLater in the same 1960 publication Solomonoff describes his extension of the single-shortest-code theory. This is Algorithmic\nProbability. He states: \"It would seem that if there are several different methods of describing a sequence, each of these methods should be given \"some\" weight in determining the probability of that sequence.\" He then shows how this idea can be used to generate the universal a priori probability distribution and how it enables the use of Bayes rule in inductive inference. Inductive inference, by adding up the predictions of all models describing a particular sequence, using suitable weights based on the lengths of those models, gets the probability distribution for the extension of that sequence. This method of prediction has since become known as Solomonoff induction.\n\nHe enlarged his theory, publishing a number of reports leading up to the publications in 1964. The 1964 papers give a more detailed description of Algorithmic Probability, and Solomonoff Induction, presenting five different models, including the model popularly called the Universal Distribution.\n\nOther scientists who had been at the 1956 Dartmouth Summer Conference (such as Newell and Simon) were developing the branch of Artificial Intelligence which used machines governed by if-then rules, fact based. Solomonoff was developing the branch of Artificial Intelligence that focussed on probability and prediction; his specific view of A.I. described machines that were governed by the Algorithmic Probability distribution. The machine generates theories together with their associated probabilities, to solve problems, and as new problems and theories develop, updates the probability distribution on the theories.\n\nIn 1968 he found a proof for the\nefficacy of Algorithmic Probability, but mainly because of lack of general interest at that time, did not publish it until 10 years later. In his report, he published the proof for the convergence theorem.\n\nIn the years following his discovery of Algorithmic Probability he focused on how to use this probability and Solomonoff Induction in actual prediction and problem solving for A.I. He also wanted to understand the deeper implications of this probability system.\n\nOne important aspect of Algorithmic Probability is that it is complete and incomputable.\n\nIn the 1968 report he shows that Algorithmic Probability is \"complete\"; that is, if there is any describable regularity in a body of data, Algorithmic Probability will eventually discover that regularity, requiring a relatively small sample of that data. Algorithmic Probability is the only probability system known to be complete in this way. As a necessary consequence of its completeness it is \"incomputable\". The incomputability is because some algorithms—a subset of those that are partially recursive—can never be evaluated fully because it would take too long. But these programs will at least be recognized as possible solutions. On the other hand, any \"computable\" system is \"incomplete\". There will always be descriptions outside that system's search space which will never be acknowledged or considered, even in an infinite amount of time. Computable prediction models hide this fact by ignoring such algorithms.\n\nIn many of his papers he described how to search for solutions to problems and in the 1970s and early 1980s developed what he felt was the best way to update the machine.\n\nThe use of probability in A.I., however, did not have a completely smooth path. In the early years of A.I., the relevance of probability was problematic. Many in the A.I. community felt probability was not usable in their work. The area of pattern recognition did use a form of probability, but because there was no broadly based theory of how to incorporate probability in any A.I. field, most fields did not use it at all.\n\nThere were, however, researchers such as Pearl and Peter Cheeseman who argued that probability could be used in artificial intelligence.\n\nAbout 1984, at an annual meeting of the American Association for Artificial Intelligence (AAAI), it was decided that probability was in no way relevant to A.I.\n\nA protest group formed, and the next year there was a workshop at the AAAI meeting devoted to \"Probability and Uncertainty in AI.\" This yearly workshop has continued to the present day.\n\nAs part of the protest at the first workshop, Solomonoff gave a paper on how to apply the universal distribution to problems in A.I. This was an early version of the system he has been developing since that time.\n\nIn that report, he described the search technique he had developed. In search problems, the best order of search, is time formula_1, where formula_2 is the time needed to test the trial and formula_3 is the probability of success of that trial. He called this the \"Conceptual Jump Size\" of the problem. Levin's search technique approximates this order, and so Solomonoff, who had studied Levin's work, called this search technique Lsearch.\n\nIn other papers he explored how to limit the time needed to search for solutions, writing on resource bounded search. The search space is limited by available time or computation cost rather than by cutting out search space as is done in some other prediction methods, such as Minimum Description Length.\n\nThroughout his career Solomonoff was concerned with the potential benefits and dangers of A.I., discussing it in many of his published reports. In 1985 he analyzed a likely evolution of A.I., giving a formula predicting when it would reach the \"Infinity Point\". This work is part of the history of thought about a possible technological singularity.\n\nOriginally algorithmic induction methods extrapolated ordered sequences of strings. Methods were needed for dealing with other kinds of data.\n\nA 1999 report, generalizes the Universal Distribution and associated convergence theorems to unordered sets of strings and a 2008 report, to unordered pairs of strings.\n\nIn 1997, 2003 and 2006 he showed that incomputability and subjectivity are both necessary and desirable characteristics of any high performance induction system.\n\nIn 1970 he formed his own one man company, Oxbridge Research, and continued his research there except for periods at other\ninstitutions such as MIT, University of Saarland in Germany and the Dalle Molle Institute for Artificial Intelligence in Lugano, Switzerland. In 2003 he was the first recipient of the Kolmogorov Award by The Computer Learning Research Center at the Royal Holloway, University of London, where he gave the inaugural Kolmogorov Lecture. Solomonoff was most recently a visiting Professor at the CLRC.\n\nIn 2006 he spoke at AI@50, \"Dartmouth Artificial Intelligence Conference: the Next Fifty Years\" commemorating the fiftieth anniversary\nof the original Dartmouth summer study group. Solomonoff was one of five original participants to attend.\n\nIn Feb. 2008, he gave the keynote address at the Conference \"Current Trends in the Theory and Application of Computer Science\" (CTTACS), held at Notre Dame University in Lebanon. He followed this with a short series of lectures, and began research on new applications of Algorithmic Probability.\n\nAlgorithmic Probability and Solomonoff Induction have many advantages for Artificial Intelligence. Algorithmic Probability gives extremely accurate probability estimates. These estimates can be revised by a reliable method so that they continue to be acceptable. It utilizes search time in a very efficient way. In addition to probability estimates, Algorithmic Probability \"has for AI another important value: its multiplicity of models gives us many different ways to understand our data;\n\nA description of Solomonoff's life and work prior to 1997 is in \"The Discovery of Algorithmic Probability\", Journal of Computer and System Sciences, Vol 55, No. 1, pp 73–88, August 1997. The paper, as well as most of the others mentioned here, are available on his website at the publications page.\n\nIn an article published the year of his death, a journal article said of Solomonoff: \"A very conventional scientist understands his science using a single 'current paradigm'—the way of understanding that is most in vogue at the present time. A more creative scientist understands his science in very many ways, and can more easily create new theories, new ways of understanding, when the 'current paradigm' no longer fits the current data\".\n\n\n"}
{"id": "26400591", "url": "https://en.wikipedia.org/wiki?curid=26400591", "title": "Regular constraint", "text": "Regular constraint\n\nIn artificial intelligence and operations research, a regular constraint is a kind of global constraint. It can be used to solve a particular type of puzzle called a nonogram or logigrams.\n\n"}
{"id": "252355", "url": "https://en.wikipedia.org/wiki?curid=252355", "title": "Sequent", "text": "Sequent\n\nIn mathematical logic, a sequent is a very general kind of conditional assertion.\n\nA sequent may have any number \"m\" of condition formulas \"A\" (called \"antecedents\") and any number \"n\" of asserted formulas \"B\" (called \"succedents\" or \"consequents\"). A sequent is understood to mean that if all of the antecedent conditions are true, then at least one of the consequent formulas is true. This style of conditional assertion is almost always associated with the conceptual framework of sequent calculus.\n\nSequents are best understood in the context of the following three kinds of logical judgments:\n\nThus sequents are a generalization of simple conditional assertions, which are a generalization of unconditional assertions.\n\nThe word \"OR\" here is the inclusive OR. The motivation for disjunctive semantics on the right side of a sequent comes from three main benefits.\nAll three of these benefits were identified in the founding paper by .\n\nNot all authors have adhered to Gentzen's original meaning for the word \"sequent\". For example, used the word \"sequent\" strictly for simple conditional assertions with one and only one consequent formula. The same single-consequent definition for a sequent is given by .\n\nIn a general sequent of the form\nboth Γ and Σ are sequences of logical formulas, not sets. Therefore both the number and order of occurrences of formulas are significant. In particular, the same formula may appear twice in the same sequence. The full set of sequent calculus inference rules contains rules to swap adjacent formulas on the left and on the right of the assertion symbol (and thereby arbitrarily permute the left and right sequences), and also to insert arbitrary formulas and remove duplicate copies within the left and the right sequences. (However, , uses \"sets\" of formulas in sequents instead of sequences of formulas. Consequently the three pairs of \"structural rules\" called \"thinning\", \"contraction\" and \"interchange\" are not required.)\n\nThe symbol ' formula_3 ' is often referred to as the \"turnstile\", \"right tack\", \"tee\", \"assertion sign\" or \"assertion symbol\". It is often read, suggestively, as \"yields\", \"proves\" or \"entails\".\n\nSince every formula in the antecedent (the left side) must be true to conclude the truth of at least one formula in the succedent (the right side), adding formulas to either side results in a weaker sequent, while removing them from either side gives a stronger one. This is one of the symmetry advantages which follows from the use of disjunctive semantics on the right hand side of the assertion symbol, whereas conjunctive semantics is adhered to on the left hand side.\n\nIn the extreme case where the list of \"antecedent\" formulas of a sequent is empty, the consequent is unconditional. This differs from the simple unconditional assertion because the number of consequents is arbitrary, not necessarily a single consequent. Thus for example, ' ⊢ \"B\", \"B\" ' means that either \"B\", or \"B\", or both must be true. An empty antecedent formula list is equivalent to the \"always true\" proposition, called the \"verum\", denoted \"⊤\". (See Tee (symbol).)\n\nIn the extreme case where the list of \"consequent\" formulas of a sequent is empty, the rule is still that at least one term on the right be true, which is clearly impossible. This is signified by the 'always false' proposition, called the \"falsum\", denoted \"⊥\". Since the consequence is false, at least one of the antecedents must be false. Thus for example, ' \"A\", \"A\" ⊢ ' means that at least one of the antecedents \"A\" and \"A\" must be false.\n\nOne sees here again a symmetry because of the disjunctive semantics on the right hand side. If the left side is empty, then one or more right-side propositions must be true. If the right side is empty, then one or more of the left-side propositions must be false.\n\nThe doubly extreme case ' ⊢ ', where both the antecedent and consequent lists of formulas are empty is \"not satisfiable\". In this case, the meaning of the sequent is effectively ' ⊤ ⊢ ⊥ '. This is equivalent to the sequent ' ⊢ ⊥ ', which clearly cannot be valid.\n\nA sequent of the form ' ⊢ α, β ', for logical formulas α and β, means that either α is true or β is true. But it does not mean that either α is a tautology or β is a tautology. To clarify this, consider the example ' ⊢ B ∨ A, C ∨ ¬A '. This is a valid sequent because either B ∨ A is true or C ∨ ¬A is true. But neither of these expressions is a tautology in isolation. It is the \"disjunction\" of these two expressions which is a tautology.\n\nSimilarly, a sequent of the form ' α, β ⊢ ', for logical formulas α and β, means that either α is false or β is false. But it does not mean that either α is a contradiction or β is a contradiction. To clarify this, consider the example ' B ∧ A, C ∧ ¬A ⊢ '. This is a valid sequent because either B ∧ A is false or C ∧ ¬A is false. But neither of these expressions is a contradiction in isolation. It is the \"conjunction\" of these two expressions which is a contradiction.\n\nMost proof systems provide ways to deduce one sequent from another. These inference rules are written with a list of sequents above and below a line. This rule indicates that if everything above the line is true, so is everything under the line.\n\nA typical rule is:\n\nThis indicates that if we can deduce that formula_5 yields formula_6, and that formula_7 yields formula_8, then we can also deduce that formula_7 yields formula_6. (See also the full set of sequent calculus inference rules.)\n\nThe assertion symbol in sequents originally meant exactly the same as the implication operator. But over time, its meaning has changed to signify provability within a theory rather than semantic truth in all models.\n\nIn 1934, Gentzen did not define the assertion symbol ' ⊢ ' in a sequent to signify provability. He defined it to mean exactly the same as the implication operator ' ⇒ '. Using ' → ' instead of ' ⊢ ' and ' ⊃ ' instead of ' ⇒ ', he wrote: \"The sequent A, ..., A → B, ..., B signifies, as regards content, exactly the same as the formula (A & ... & A) ⊃ (B ∨ ... ∨ B)\". (Gentzen employed the right-arrow symbol between the antecedents and consequents of sequents. He employed the symbol ' ⊃ ' for the logical implication operator.)\n\nIn 1939, Hilbert and Bernays stated likewise that a sequent has the same meaning as the corresponding implication formula.\n\nIn 1944, Alonzo Church emphasized that Gentzen's sequent assertions did not signify provability.\n\nNumerous publications after this time have stated that the assertion symbol in sequents does signify provability within the theory where the sequents are formulated. Curry in 1963, Lemmon in 1965, and Huth and Ryan in 2004 all state that the sequent assertion symbol signifies provability. However, states that the assertion symbol in Gentzen-system sequents, which he denotes as ' ⇒ ', is part of the object language, not the metalanguage.\n\nAccording to Prawitz (1965): \"The calculi of sequents can be understood as meta-calculi for the deducibility relation in the corresponding systems of natural deduction.\" And furthermore: \"A proof in a calculus of sequents can be looked upon as an instruction on how to construct a corresponding natural deduction.\" In other words, the assertion symbol is part of the object language for the sequent calculus, which is a kind of meta-calculus, but simultaneously signifies deducibility in an underlying natural deduction system.\n\nA sequent is a formalized statement of provability that is frequently used when specifying calculi for deduction. In the sequent calculus, the name \"sequent\" is used for the construct, which can be regarded as a specific kind of judgment, characteristic to this deduction system.\n\nThe intuitive meaning of the sequent formula_2 is that under the assumption of Γ the conclusion of Σ is provable. Classically, the formulae on the left of the turnstile can be interpreted conjunctively while the formulae on the right can be considered as a disjunction. This means that, when all formulae in Γ hold, then at least one formula in Σ also has to be true. If the succedent is empty, this is interpreted as falsity, i.e. formula_12 means that Γ proves falsity and is thus inconsistent. On the other hand an empty antecedent is assumed to be true, i.e., formula_13 means that Σ follows without any assumptions, i.e., it is always true (as a disjunction). A sequent of this form, with Γ empty, is known as a logical assertion.\n\nOf course, other intuitive explanations are possible, which are classically equivalent. For example, formula_2 can be read as asserting that it cannot be the case that every formula in Γ is true and every formula in Σ is false (this is related to the double-negation interpretations of classical intuitionistic logic, such as Glivenko's theorem).\n\nIn any case, these intuitive readings are only pedagogical. Since formal proofs in proof theory are purely syntactic, the meaning of (the derivation of) a sequent is only given by the properties of the calculus that provides the actual rules of inference.\n\nBarring any contradictions in the technically precise definition above we can describe sequents in their introductory logical form. formula_7 represents a set of assumptions that we begin our logical process with, for example \"Socrates is a man\" and \"All men are mortal\". The formula_6 represents a logical conclusion that follows under these premises. For example \"Socrates is mortal\" follows from a reasonable formalization of the above points and we could expect to see it on the formula_6 side of the \"turnstile\". In this sense, formula_3 means the process of reasoning, or \"therefore\" in English.\n\nThe general notion of sequent introduced here can be specialized in various ways. A sequent is said to be an intuitionistic sequent if there is at most one formula in the succedent (although multi-succedent calculi for intuitionistic logic are also possible). More precisely, the restriction of the general sequent calculus to single-succedent-formula sequents, \"with the same inference rules\" as for general sequents, constitutes an intuitionistic sequent calculus. (This restricted sequent calculus is denoted LJ.)\n\nSimilarly, one can obtain calculi for dual-intuitionistic logic (a type of paraconsistent logic) by requiring that sequents be singular in the antecedent.\n\nIn many cases, sequents are also assumed to consist of multisets or sets instead of sequences. Thus one disregards the order or even the numbers of occurrences of the formulae. For classical propositional logic this does not yield a problem, since the conclusions that one can draw from a collection of premises do not depend on these data. In substructural logic, however, this may become quite important.\n\nNatural deduction systems use single-consequence conditional assertions, but they typically do not use the same sets of inference rules as Gentzen introduced in 1934. In particular, tabular natural deduction systems, which are very convenient for practical theorem-proving in propositional calculus and predicate calculus, were applied by and for teaching introductory logic in textbooks.\n\nHistorically, sequents have been introduced by Gerhard Gentzen in order to specify his famous sequent calculus. In his German publication he used the word \"Sequenz\". However, in English, the word \"sequence\" is already used as a translation to the German \"Folge\" and appears quite frequently in mathematics. The term \"sequent\" then has been created in search for an alternative translation of the German expression.\n\nKleene makes the following comment on the translation into English: \"Gentzen says 'Sequenz', which we translate as 'sequent', because we have already used 'sequence' for any succession of objects, where the German is 'Folge'.\"\n\n\n"}
{"id": "317062", "url": "https://en.wikipedia.org/wiki?curid=317062", "title": "Significant figures", "text": "Significant figures\n\nThe significant figures (also known as the significant digits) of a number are digits that carry meaning contributing to its measurement resolution. This includes all digits \"except\":\n\nSignificance arithmetic are approximate rules for roughly maintaining significance throughout a computation. The more sophisticated scientific rules are known as propagation of uncertainty.\n\nNumbers are often rounded to avoid reporting insignificant figures. For example, it would create false precision to express a measurement as 12.34500 kg (which has seven significant figures) if the scales only measured to the nearest gram and gave a reading of 12.345 kg (which has five significant figures). Numbers can also be rounded merely for simplicity rather than to indicate a given precision of measurement, for example, to make them faster to pronounce in news broadcasts.\n\n\nSpecifically, the rules for identifying significant figures when writing or interpreting numbers are as follows:\n\nIn most cases, the same rules apply to numbers expressed in scientific notation. However, in the normalized form of that notation, placeholder leading and trailing digits do not occur, so all digits are significant. For example, (two significant figures) becomes , and (six significant figures) becomes . In particular, the potential ambiguity about the significance of trailing zeros is eliminated. For example, to four significant figures is written as , while to two significant figures is written as .\n\nThe part of the representation that contains the significant figures (as opposed to the base or the exponent) is known as the significand or mantissa.\n\nThe basic concept of significant figures is often used in connection with rounding. Rounding to significant figures is a more general-purpose technique than rounding to \"n\" decimal places, since it handles numbers of different scales in a uniform way. For example, the population of a city might only be known to the nearest thousand and be stated as 52,000, while the population of a country might only be known to the nearest million and be stated as 52,000,000. The former might be in error by hundreds, and the latter might be in error by hundreds of thousands, but both have two significant figures (5 and 2). This reflects the fact that the significance of the error (its likely size relative to the size of the quantity being measured) is the same in both cases.\n\nTo round to \"n\" significant figures:\n\n\nIn financial calculations, a number is often rounded to a given number of places (for example, to two places after the decimal separator for many world currencies). Rounding to a fixed number of decimal places in this way is an orthographic convention that does not maintain significance, and may either lose information or create false precision.\n\nIn UK personal tax returns payments received are always rounded down to the nearest pound, whilst tax paid is rounded up although tax deducted at source is calculated to the nearest penny. This creates an interesting situation where anyone with tax accurately deducted at source has a significant likelihood of a small rebate if they complete a tax return.\n\nAs an illustration, the decimal quantity 12.345 can be expressed with various numbers of significant digits or decimal places. If insufficient precision is available then the number is rounded in some manner to fit the available precision. The following table shows the results for various total precisions and decimal places.\nAnother example for 0.012345:\n\nThe representation of a positive number \"x\" to a precision of \"p\" significant digits has a numerical value that is given by the formula:\nFor negative numbers, the formula can be used on the absolute value; for zero, no transformation is necessary. Note that the result may need to be written with one of the above conventions explained in the section \"Identifying significant figures\" to indicate the actual number of significant digits if the result includes for example trailing significant zeros.\n\nAs there are rules for determining the number of significant figures in directly \"measured\" quantities, there are rules for determining the number of significant figures in quantities \"calculated\" from these \"measured\" quantities.\n\nOnly \"measured\" quantities figure into the determination of the number of significant figures in \"calculated quantities\". Exact mathematical quantities like the in the formula for the area of a circle with radius , has no effect on the number of significant figures in the final calculated area. Similarly the in the formula for the kinetic energy of a mass with velocity , , has no bearing on the number of significant figures in the final calculated kinetic energy. The constants and are considered to have an \"infinite\" number of significant figures.\n\nFor quantities created from measured quantities by multiplication and division, the calculated result should have as many significant figures as the \"measured\" number with the \"least\" number of significant figures. For example, \nwith only \"two\" significant figures. The first factor has four significant figures and the second has two significant figures. The factor with the least number of significant figures is the second one with only two, so the final calculated result should also have a total of two significant figures.\n\nFor quantities created from measured quantities by addition and subtraction, the last significant \"decimal place\" (hundreds, tens, ones, tenths, and so forth) in the calculated result should be the same as the \"leftmost\" or largest \"decimal place\" of the last significant figure out of all the \"measured\" quantities in the terms of the sum. For example, \nwith the last significant figure in the \"tenths\" place. The first term has its last significant figure in the tenths place and the second term has its last significant figure in the thousandths place. The leftmost of the decimal places of the last significant figure out of all the terms of the sum is the tenths place from the first term, so the calculated result should also have its last significant figure in the tenths place.\n\nThe rules for calculating significant figures for multiplication and division are opposite to the rules for addition and subtraction. For multiplication and division, only the total number of significant figures in each of the factors matter; the decimal place of the last significant figure in each factor is irrelevant. For addition and subtraction, only the decimal place of the last significant figure in each of the terms matters; the total number of significant figures in each term is irrelevant.\nIn a base 10 logarithm of a normalized number, the result should be rounded to the number of significant figures in the normalized number. For example, log(3.000×10) = log(10) + log(3.000) ≈ 4 + 0.47712125472, should be rounded to 4.4771.\n\nWhen taking antilogarithms, the resulting number should have as many significant figures as the mantissa in the logarithm.\n\nWhen performing a calculation, do not follow these guidelines for intermediate results; keep as many digits as is practical (at least 1 more than implied by the precision of the final result) until the end of calculation to avoid cumulative rounding errors.\n\nWhen using a ruler, initially use the smallest mark as the first estimated digit. For example, if a ruler's smallest mark is cm, and 4.5 cm is read, it is 4.5 (±0.1 cm) or 4.4 – 4.6 cm.\n\nIt is possible that the overall length of a ruler may not be accurate to the degree of the smallest mark and the marks may be imperfectly spaced within each unit. However assuming a normal good quality ruler, it should be possible to estimate tenths between the nearest two marks to achieve an extra decimal place of accuracy. Failing to do this adds the error in reading the ruler to any error in the calibration of the ruler.\n\nWhen estimating the proportion of individuals carrying some particular characteristic in a population, from a random sample of that population, the number of significant figures should not exceed the maximum precision allowed by that sample size. The correct number of significant figures is given by the order of magnitude of sample size. This can be found by taking the base 10 logarithm of sample size and rounding to the nearest integer.\n\nFor example, in a poll of 120 randomly chosen viewers of a regularly visited web page we find that 10 people disagree with a proposition on that web page. The order of magnitude of our sample size is Log(120) = 2.0791812460..., which rounds to 2. Our estimated proportion of people who disagree with the proposition is therefore 0.083, or 8.3%, with 2 significant figures. This is because in different samples of 120 people from this population, our estimate would vary in units of 1/120, and any additional figures would misrepresent the size of our sample by giving spurious precision. To interpret our estimate of the number of viewers who disagree with the proposition we should then calculate some measure of our confidence in this estimate.\n\nTraditionally, in various technical fields, \"accuracy\" refers to the closeness of a given measurement to its true value; \"precision\" refers to the stability of that measurement when repeated many times. Hoping to reflect the way the term \"accuracy\" is actually used in the scientific community, there is a more recent standard, ISO 5725, which keeps the same definition of precision but defines the term \"trueness\" as the closeness of a given measurement to its true value and uses the term \"accuracy\" as the combination of trueness and precision. (See the Accuracy and precision article for a fuller discussion.) In either case, the number of significant figures roughly corresponds to \"precision\", not to either use of the word accuracy or to the newer concept of trueness. \n\nComputer representations of floating point numbers typically use a form of rounding to significant figures, but with binary numbers. The number of correct significant figures is closely related to the notion of relative error (which has the advantage of being a more accurate measure of precision, and is independent of the radix of the number system used).\n\n\n"}
{"id": "22359636", "url": "https://en.wikipedia.org/wiki?curid=22359636", "title": "Simplicial sphere", "text": "Simplicial sphere\n\nIn geometry and combinatorics, a simplicial (or combinatorial) \"d\"-sphere is a simplicial complex homeomorphic to the \"d\"-dimensional sphere. Some simplicial spheres arise as the boundaries of convex polytopes, however, in higher dimensions most simplicial spheres cannot be obtained in this way.\n\nThe most important open problem in the field is the g-conjecture, formulated by Peter McMullen, which asks about possible numbers of faces of different dimensions of a simplicial sphere.\n\n\nIt follows from Euler's formula that any simplicial 2-sphere with \"n\" vertices has 3\"n\" − 6 edges and 2\"n\" − 4 faces. The case of \"n\" = 4 is realized by the tetrahedron. By repeatedly performing the barycentric subdivision, it is easy to construct a simplicial sphere for any \"n\" ≥ 4. Moreover, Ernst Steinitz gave a characterization of 1-skeleta (or edge graphs) of convex polytopes in R implying that any simplicial 2-sphere is a boundary of a convex polytope.\n\nBranko Grünbaum constructed an example of a non-polytopal simplicial sphere. Gil Kalai proved that, in fact, \"most\" simplicial spheres are non-polytopal. The smallest example is of dimension \"d\" = 4 and has \"f\" = 8 vertices.\n\nThe upper bound theorem gives upper bounds for the numbers \"f\" of \"i\"-faces of any simplicial \"d\"-sphere with \"f\" = \"n\" vertices. This conjecture was proved for polytopal spheres by Peter McMullen in 1970 and by Richard Stanley for general simplicial spheres in 1975.\n\nThe g\"-conjecture, formulated by McMullen in 1970, asks for a complete characterization of \"f\"-vectors of simplicial \"d\"-spheres. In other words, what are the possible sequences of numbers of faces of each dimension for a simplicial \"d\"-sphere? In the case of polytopal spheres, the answer is given by the g\"-theorem, proved in 1979 by Billera and Lee (existence) and Stanley (necessity). It has been conjectured that the same conditions are necessary for general simplicial spheres. The conjecture is open for \"d\" at least 5 (as of 2018).\n\n\n"}
{"id": "2660978", "url": "https://en.wikipedia.org/wiki?curid=2660978", "title": "Sine and cosine transforms", "text": "Sine and cosine transforms\n\nIn mathematics, the Fourier sine and cosine transforms are forms of the Fourier integral transform that do not use complex numbers. They are the forms originally used by Joseph Fourier and are still preferred in some applications, such as signal processing or statistics.\n\nThe Fourier sine transform of , sometimes denoted by either formula_1 or formula_2, is\n\nIf means time, then is frequency in cycles per unit time, but in the abstract, they can be any pair of variables which are dual to each other.\n\nThis transform is necessarily an odd function of frequency, i.e. for all :\n\nThe numerical factors in the Fourier transforms are defined uniquely only by their product. Here, in order that the Fourier inversion formula not have any numerical factor, the factor of 2 appears because the sine function has norm of formula_5\n\nThe Fourier cosine transform of , sometimes denoted by either formula_6 or formula_7, is\n\nIt is necessarily an even function of frequency, i.e. for all :\n\nSome authors only define the cosine transform for even functions of , in which case its sine transform is zero. Since cosine is also even, a simpler formula can be used, \n\nSimilarly, if is an odd function, then the cosine transform is zero and the sine transform can be simplified to \n\nOther authors also define the cosine transform as\n\nformula_12\n\nand sine as\n\nformula_13\n\nThe original function can be recovered from its transform under the usual hypotheses, that and both of its transforms should be absolutely integrable. For more details on the different hypotheses, see Fourier inversion theorem.\n\nThe inversion formula is\n\nwhich has the advantage that all frequencies are positive and all quantities are real. If the numerical factor 2 is left out of the definitions of the transforms, then the inversion formula is usually written as an integral over both negative and positive frequencies.\n\nUsing the addition formula for cosine, this is sometimes rewritten as\n\nwhere denotes the one-sided limit of as approaches zero from above, and denotes the one-sided limit of as approaches zero from below.\n\nIf the original function is an even function, then the sine transform is zero; if is an odd function, then the cosine transform is zero. In either case, the inversion formula simplifies.\n\nThe form of the Fourier transform used more often today is\n\n\n"}
{"id": "48485384", "url": "https://en.wikipedia.org/wiki?curid=48485384", "title": "Sophie Germain Prize", "text": "Sophie Germain Prize\n\nThe Sophie Germain Prize (in French: \"Prix Sophie Germain\") is an annual mathematics prize from the French Academy of Sciences conferred since the year 2003. It is named after the French mathematician Sophie Germain, and comes with a prize of €8000.\n\n"}
{"id": "42690608", "url": "https://en.wikipedia.org/wiki?curid=42690608", "title": "Strange nonchaotic attractor", "text": "Strange nonchaotic attractor\n\nIn mathematics, a strange nonchaotic attractor (SNA) is a form of attractor which, while converging to a limit, is strange, because it is not piecewise differentiable, and also non-chaotic, in that its Lyapunov exponents are non-positive. SNAs were introduced as a topic of study by Grebogi et al. in 1984. SNAs can be distinguished from periodic, quasiperiodic and chaotic attractors using the 0-1 test for chaos.\n\nPeriodically driven damped nonlinear systems can exhibit complex dynamics characterized by strange chaotic attractors, where strange refers to the fractal geometry of the attractor and chaotic refers to the exponential sensitivity of orbits on the attractor. Quasiperiodically driven systems forced by incommensurate frequencies are natural extensions of periodically driven ones and are phenomenologically richer. In addition to periodic or quasiperiodic motion, they can exhibit chaotic or nonchaotic motion on strange attractors. Although quasiperiodic forcing is not necessary for strange nonchaotic dynamics (e.g., the period doubling accumulation point of a period doubling cascade), if quasiperiodic driving is not present, strange nonchaotic attractors are typically not robust and not expected to occur naturally because they exist only when the system is carefully tuned to a precise critical parameter value. On the other hand, it was shown in the paper of Grebogi et al. that SNA's can be robust when the system is quasiperiodically driven. The first experiment to demonstrate a robust strange nonchaotic attractor involved the buckling of a magnetoelastic ribbon driven quasiperiodically by two incommensurate frequencies in the golden ratio. Strange nonchaotic attractors have been robustly observed in laboratory experiments involving magnetoelastic ribbons, electrochemical cells, electronic circuits, a neon glow discharge and most recently detected in the dynamics of the pulsating RR Lyrae variables KIC 5520878 (as obtained from the Kepler Space Telescope) which may be the first strange nonchaotic dynamical system observed in the wild.\n"}
{"id": "36791755", "url": "https://en.wikipedia.org/wiki?curid=36791755", "title": "Truchet tiles", "text": "Truchet tiles\n\nIn information visualization and graphic design, Truchet tiles are square tiles decorated with patterns that are not rotationally symmetric. When placed within a square tiling of the plane, they can form varied patterns, and the orientation of each tile can be used to visualize information associated with the tile's position within the tiling.\n\nTruchet tiles were first described in a 1704 memoir by Sébastien Truchet entitled \"Mémoire sur les combinaisons\", and were popularized in 1987 by Cyril Stanley Smith.\n\nThe tile originally studied by Truchet is split along the diagonal into two triangles of contrasting colors. The tile has four possible orientations.\n\nSome examples of surface filling made tiling such a pattern.\n\nWith a scheme:\nWith random placement:\nA second common form of the Truchet tiles, due to , decorates each tile with two quarter-circles connecting the midpoints of adjacent sides. Each such tile has two possible orientations.\n\nWe have such a tiling:\n\nThis type of tile has also been used in abstract strategy games Trax and the Black Path Game, prior to Smith's work.\n\nA labyrinth can be generated by tiles in the form of a white square with a black diagonal. As with the quarter-circle tiles, each such tile has two orientations.\nThe connectivity of the resulting labyrinth can be analyzed mathematically using percolation theory as bond percolation at the critical point of a diagonally-oriented grid.\nNick Montfort considers the single line of Commodore 64 BASIC required to generate such patterns - codice_1 - to be \"a concrete poem, a found poem\".\n\n\n"}
{"id": "3021425", "url": "https://en.wikipedia.org/wiki?curid=3021425", "title": "Van Aubel's theorem", "text": "Van Aubel's theorem\n\nIn plane geometry, Van Aubel's theorem describes a relationship between squares constructed on the sides of a quadrilateral. Starting with a given quadrilateral (a polygon having four sides), construct a square on each side. Van Aubel's theorem states that the two line segments between the centers of opposite squares are of equal lengths and are at right angles to one another. Another way of saying the same thing is that the center points of the four squares form the vertices of an equidiagonal orthodiagonal quadrilateral. The theorem is named after H. H. van Aubel, who published it in 1878.\n\n\n"}
{"id": "15394298", "url": "https://en.wikipedia.org/wiki?curid=15394298", "title": "Venkatesan Guruswami", "text": "Venkatesan Guruswami\n\nVenkatesan Guruswami (born 1976) is a Professor of computer science at Carnegie Mellon University in Pittsburgh, United States. He did his high schooling at Padma Seshadri Bala Bhavan in Chennai, India. He completed his undergraduate in Computer Science from IIT Madras and his doctorate from Massachusetts Institute of Technology under the supervision of Madhu Sudan in 2001 . After receiving his PhD, he spent a year at UC Berkeley as a Miller Fellow, and then was a member of the faculty at the University of Washington from 2002 to 2009. His primary area of research is computer science, and in particular on error-correcting codes. During 2007-2008, he visited the Institute for Advanced Study as a Member of School of Mathematics. He also visited SCS at Carnegie Mellon University during 2008-09 as a Visiting Faculty. In July 2009, he joined the School of Computer Science at Carnegie Mellon University as Associate Professor in the Computer Science Department.\n\nGuruswami was awarded the 2002 ACM Doctoral Dissertation Award for his dissertation \"List Decoding of Error-Correcting Codes\". , which introduced an algorithm that allowed for the correction of errors beyond half the minimum distance of the code. It applies to Reed–Solomon codes and more generally to algebraic geometric codes. This algorithm produces a list of codewords (it is a list-decoding algorithm) and is based on interpolation and factorization of polynomials over formula_1 and its extensions.\n\nHe was an invited speaker in International Congress of Mathematicians 2010, Hyderabad on the topic of \"Mathematical Aspects of Computer Science.\"\n\nGuraswami was one of two winners of the 2012 Presburger Award, given by the European Association for Theoretical Computer Science for outstanding contributions by a young theoretical computer scientist.\nHe was elected as an ACM Fellow in 2017.\n\n\n\n"}
{"id": "44185220", "url": "https://en.wikipedia.org/wiki?curid=44185220", "title": "Wilf equivalence", "text": "Wilf equivalence\n\nIn the study of permutations and permutation patterns, Wilf equivalence is an equivalence relation on permutation classes.\nTwo permutation classes are Wilf equivalent when they have the same numbers of permutations of each possible length, or equivalently if they have the same generating functions. The equivalence classes for Wilf equivalence are called Wilf classes; they are the combinatorial classes of permutation classes. The counting functions and Wilf equivalences among many specific permutation classes are known.\n\nWilf equivalence may also be described for individual permutations rather than permutation classes. In this context, two permutations are said to be Wilf equivalent if the principal permutation classes formed by forbidding them are Wilf equivalent.\n"}
{"id": "33065815", "url": "https://en.wikipedia.org/wiki?curid=33065815", "title": "ΔP", "text": "ΔP\n\nΔ (Delta P) is a mathematical term used to illustrate a change (Δ) in pressure ().\n\n\nAs used in the Darcy–Weisbach equation — Given that the head loss \"h\" expresses the pressure loss \"Δp\" as the height of a column of fluid,\n\nwhere ρ is the density of the fluid, the Darcy–Weisbach equation can also be written in terms of pressure loss:\n\nIn general, compliance is defined by the change in volume (ΔV) versus the associated change in pressure (ΔP), or ΔV/ΔP. During mechanical ventilation, compliance can be influenced by 3 key physiologic factors:\n\nLung compliance is influenced by a variety of primary abnormalities of lung parenchyma, both chronic and acute. Airway resistance is typically increased by bronchospasm and airway secretions. Chest wall compliance can be decreased by fixed abnormalities (e.g. kyphoscoliosis, morbid obesity) or more variable problems driven by patient agitation while intubated.\n\nCalculating Compliance on minute volume (V: ΔV is always defined by tidal volume (V), but ΔP is different for the measurement of dynamic vs. static compliance.\n\nwhere PIP = peak inspiratory pressure (the maximum pressure during inspiration), and PEEP = positive end expiratory pressure. Alterations in airway resistance, lung compliance and chest wall compliance influence C.\n\nwhere P = plateau pressure. P is measured at the end of inhalation and prior to exhalation using an inspiratory hold maneuver. During this maneuver, airflow is transiently (~0.5 sec) discontinued, which eliminates the effects of airway resistance. P is never > PIP and is typically < 3-5 cmHO lower than PIP when airway resistance is normal.\n\n"}
