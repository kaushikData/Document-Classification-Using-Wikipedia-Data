{"id": "1032155", "url": "https://en.wikipedia.org/wiki?curid=1032155", "title": "All one polynomial", "text": "All one polynomial\n\nAn all one polynomial (AOP) is a polynomial in which all coefficients are one. Over the finite field of order two, conditions for the AOP to be irreducible are known, which allow this polynomial to be used to define efficient algorithms and circuits for multiplication in finite fields of characteristic two. The AOP is a 1-equally spaced polynomial.\n\nAn AOP of degree \"m\" has all terms from \"x\" to \"x\" with coefficients of 1, and can be written as\n\nor\n\nor\n\nThus the roots of the all one polynomial of degree \"m\" are all (\"m\"+1)th roots of unity other than unity itself.\n\nOver GF(2) the AOP has many interesting properties, including:\n\n\nDespite the fact that the Hamming weight is large, because of the ease of representation and other improvements there are efficient implementations in areas such as coding theory and cryptography.\n\nOver formula_4, the AOP is irreducible whenever \"m + 1\" is prime p, and therefore in these cases, the \"p\"th cyclotomic polynomial.\n"}
{"id": "5536595", "url": "https://en.wikipedia.org/wiki?curid=5536595", "title": "Analytic and enumerative statistical studies", "text": "Analytic and enumerative statistical studies\n\nAnalytic and enumerative statistical studies are two types of scientific studies:\n\nIn any statistical study the ultimate aim is to provide a rational basis for action. Enumerative and analytic studies differ by where the action is taken. Deming first published on this topic in 1942. Deming summarized the distinction between enumerative and analytic studies as follows:\nEnumerative study: A statistical study in which action will be taken on the material in the frame being studied.\n\nAnalytic study: A statistical study in which action will be taken on the process or cause-system that produced the frame being studied. The aim being to improve practice in the future.\n\nThese terms were introduced in \"Some Theory of Sampling\" (1950, Chapter 7) by W. Edwards Deming.\n\nIn other words, an enumerative study is a statistical study in which the focus is on judgment of results, and an analytic study is one in which the focus is on improvement of the process or system which created the results being evaluated and which will continue creating results in the future. A statistical study can be enumerative or analytic, but it cannot be both.\n\nStatistical theory in enumerative studies is used to describe the precision of estimates and the validity of hypotheses for the population studied. In analytical studies, the standard error of a statistic does not address the most important source of uncertainty, namely, the change in study conditions in the future. Although analytical studies need to take into account the uncertainty due to sampling, as in enumerative studies, the attributes of the study design and analysis of the data primarily deal with the uncertainty resulting from extrapolation to the future (generalisation to the conditions in future time periods). The methods used in analytical studies encourage the exploration of mechanisms through multifactor designs, contextual variables introduced through blocking and replication over time.\n\nThis distinction between enumerative and analytic studies is the theory behind the Fourteen Points for Management. Dr. Deming's philosophy is that management should be analytic instead of enumerative. In other words, management should focus on improvement of processes for the future instead of on judgment of current results.\n\n\"Use of data requires knowledge about the different sources of uncertainty.\nMeasurement is a process. Is the system of measurement stable or unstable? Use\nof data requires also understanding of the distinction between enumerative studies and analytic problems.\"\n\n\"The interpretation of results of a test or experiment is something else. It is\nprediction that a specific change in a process or procedure will be a wise choice, or that no change would be better. Either way the choice is prediction. This is known as an analytic problem, or a problem of inference, prediction.\"\n\nStatistician Dr. Mike Tveite has pointed out the dangers of attempting to use an enumerative study for prediction.\n\nCo-presenter and author Henry Neave discusses the issues associated with enumerative and analytic studies along with the many contributions made by Deming in his 1990 book, \"The Deming Dimension {reference added} and the 12 Days to Deming Course made accessible by the British Deming Association at: http://www.franbo.uk/deming/the-course/\n\nProvost wrote about the distinction of analytic studies in health care.\n\nNeave HR. The deming dimension. Knoxville, Tenn: SPC Press; 1990:440.\n\nNeave HR. The deming dimension. Knoxville, Tenn: SPC Press; 1990:440.\n"}
{"id": "4052453", "url": "https://en.wikipedia.org/wiki?curid=4052453", "title": "Behavioral modeling", "text": "Behavioral modeling\n\nThe behavioral approach to systems theory and control theory was initiated in the late-1970s by J. C. Willems as a result of resolving inconsistencies present in classical approaches based on state-space, transfer function, and convolution representations. This approach is also motivated by the aim of obtaining a general framework for system analysis and control that respects the underlying physics.\n\nThe main object in the behavioral setting is the behavior – the set of all signals compatible with the system. An important feature of the behavioral approach is that it does not distinguish a priority between input and output variables. Apart from putting system theory and control on a rigorous basis, the behavioral approach unified the existing approaches and brought new results on controllability for nD systems, control via interconnection, and system identification.\n\nIn the behavioral setting, a dynamical system is a triple \nwhere\nformula_8 means that formula_9 is a trajectory of the system, while formula_10 means that the laws of the system forbid the trajectory formula_9 to happen. Before the phenomenon is modeled, every signal in formula_5 is deemed possible, while after modeling, only the outcomes in formula_13 remain as possibilities.\n\nSpecial cases:\n\nSystem properties are defined in terms of the behavior. The system formula_1 is said to be \nwhere formula_24 denotes the formula_25-shift, defined by \nIn these definitions linearity articulates the superposition law, while time-invariance articulates that the time-shift of a legal trajectory is in its turn a legal trajectory.\n\nA \"linear time-invariant differential system\" is a dynamical system formula_27 whose behavior formula_13 is the solution set of a system of constant coefficient linear ordinary differential equations formula_29, where formula_30 is a matrix of polynomials with real coefficients. The coefficients of formula_30 are the parameters of the model. In order to define the corresponding behavior, we need to specify when we consider a signal formula_32 to be a solution of formula_29. For ease of exposition, often infinite differentiable solutions are considered. There are other possibilities, as taking distributional solutions, or solutions in formula_34, and with the ordinary differential equations interpreted in the sense of distributions. The behavior defined is\nThis particular way of representing the system is called \"kernel representation\" of the corresponding dynamical system. There are many other useful representations of the same behavior, including transfer function, state space, and convolution.\n\nFor accessible sources regarding the behavioral approach, see \n\nA key question of the behavioral approach is whether a quantity w1 can be deduced given an observed quantity w2 and a model. If w1 can be deduced given w2 and the model, w2 is said to be observable. In terms of mathematical modeling, the to-be-deduced quantity or variable is often referred to as the latent variable and the observed variable is the manifest variable. Such a system is then called an observable (latent variable) system.\n\n"}
{"id": "33605690", "url": "https://en.wikipedia.org/wiki?curid=33605690", "title": "Borel isomorphism", "text": "Borel isomorphism\n\nIn mathematics, a Borel isomorphism is a measurable bijective function between two measurable standard Borel spaces. By Souslin's theorem in standard Borel spaces (a set that is both analytic and coanalytic is necessarily Borel), the inverse of any such measurable bijective function is also measurable. Borel isomorphisms are closed under composition and under taking of inverses. The set of Borel isomorphisms from a space to itself clearly forms a group under composition. Borel isomorphisms on standard Borel spaces are analogous to homeomorphisms on topological spaces: both are bijective and closed under composition, and a homeomorphism and its inverse are both continuous, instead of both being only Borel measurable.\n\nA measurable space that is Borel isomorphic to a measurable subset of the real numbers is called a Borel space.\n\n\n"}
{"id": "1178398", "url": "https://en.wikipedia.org/wiki?curid=1178398", "title": "Classical mathematics", "text": "Classical mathematics\n\nIn the foundations of mathematics, classical mathematics refers generally to the mainstream approach to mathematics, which is based on classical logic and ZFC set theory. It stands in contrast to other types of mathematics such as constructive mathematics or predicative mathematics. In practice, the most common non-classical systems are used in constructive mathematics.\n\nClassical mathematics is sometimes attacked on philosophical grounds, due to constructivist and other objections to the logic, set theory, etc., chosen as its foundations, such as have been expressed by L. E. J. Brouwer. Almost all mathematics, however, is done in the classical tradition, or in ways compatible with it.\n\nDefenders of classical mathematics, such as David Hilbert, have argued that it is easier to work in, and is most fruitful; although they acknowledge non-classical mathematics has at times led to fruitful results that classical mathematics could not (or could not so easily) attain, they argue that on the whole, it is the other way round. \n\n"}
{"id": "1640805", "url": "https://en.wikipedia.org/wiki?curid=1640805", "title": "Digital Signature Standard", "text": "Digital Signature Standard\n\nThe Digital Signature Standard (DSS) is a Federal Information Processing Standard specifying a suite of algorithms that can be used to generate digital signatures established by the U.S. National Institute of Standards and Technology (NIST) in 2013. Four revisions to the initial specification have been released: FIPS 186-1 in 1996, FIPS 186-2 in 2000, FIPS 186-3 in 2009, and FIPS 186-4 in 2013.\n\nIt defines the Digital Signature Algorithm, contains a definition of RSA signatures based on the definitions contained within PKCS #1 version 2.1 and in American National Standard X9.31 with some additional requirements, and contains a definition of the Elliptic Curve Digital Signature Algorithm based on the definition provided by American National Standard X9.62 with some additional requirements and some recommended elliptic curves. It also approves the use of all three algorithms.\n"}
{"id": "30313771", "url": "https://en.wikipedia.org/wiki?curid=30313771", "title": "Dykstra's projection algorithm", "text": "Dykstra's projection algorithm\n\nDykstra's algorithm is a method that computes a point in the intersection of convex sets, and is a variant of the alternating projection method (also called the projections onto convex sets method). In its simplest form, the method finds a point in the intersection of two convex sets by iteratively projecting onto each of the convex set; it differs from the alternating projection method in that there are intermediate steps. A parallel version of the algorithm was developed by Gaffke and Mathar.\n\nThe method is named after R. L. Dykstra who proposed it in the 1980s.\n\nA key difference between Dykstra's algorithm and the standard alternating projection method occurs when there is more than one point in the intersection of the two sets. In this case, the alternating projection method gives some arbitrary point in this intersection, whereas Dykstra's algorithm gives a specific point: the projection of \"r\" onto the intersection, where \"r\" is the initial point used in the algorithm,\n\nDykstra's algorithm finds for each formula_1 the only formula_2 such that:\n\nwhere formula_4 are convex sets. This problem is equivalent to finding the projection of formula_1 onto the set formula_6, which we denote by formula_7.\n\nTo use Dykstra's algorithm, one must know how to project onto the sets formula_8 and formula_9 separately.\n\nFirst, consider the basic alternating projection (aka POCS) method (first studied, in the case when the sets formula_4 were linear subspaces, by John von Neumann), which initializes formula_11 and then generates the sequence\n\nDykstra's algorithm is of a similar form, but uses additional auxiliary variables. Start with formula_13 and update by\n\nThen the sequence formula_18 converges to the solution of the original problem. For convergence results and a modern perspective on the literature, see \n\n"}
{"id": "2973610", "url": "https://en.wikipedia.org/wiki?curid=2973610", "title": "Face validity", "text": "Face validity\n\nFace validity is the extent to which a test is subjectively viewed as covering the concept it purports to measure. It refers to the transparency or relevance of a test as it appears to test participants. In other words, a test can be said to have face validity if it \"looks like\" it is going to measure what it is supposed to measure. For instance, if a test is prepared to measure whether students can perform multiplication, and the people to whom it is shown all agree that it looks like a good test of multiplication ability, this demonstrates face validity of the test. Face validity is often contrasted with content validity and construct validity.\n\nSome people use the term face validity to refer only to the validity of a test to observers who are not expert in testing methodologies. For instance, if a test is designed to measure whether children are good spellers, and parents are asked whether the test is a good test, this measures the face validity of the test. If an expert is asked instead, some people would argue that this does not measure face validity. This distinction seems too careful for most applications. Generally, face validity means that the test \"looks like\" it will work, as opposed to \"has been shown to work\".\n\nIn simulation, the first goal of the system designer is to construct a system which can support a task to be accomplished, and to record the learner's task performance for any particular trial. The task(s)—and therefore, the task performance—on the simulator should be representative of the real world that they model. Face validity is a subjective measure of the extent to which this selection appears reasonable \"on the face of it\"—that is, subjectively to an expert after only a superficial examination of the content. Some assume that it is representative of the realism of the system, according to users and others who are knowledgeable about the real system being simulated. Those would say that if these experts feel the model is adequate, then it has face validity. However, in fact \"face validity\" refers to the test, not the system.\n\n\n"}
{"id": "9315192", "url": "https://en.wikipedia.org/wiki?curid=9315192", "title": "Fractal sequence", "text": "Fractal sequence\n\nIn mathematics, a fractal sequence is one that contains itself as a proper subsequence. An example is\n\nIf the first occurrence of each n is deleted, the remaining sequence is identical to the original. The process can be repeated indefinitely, so that actually, the original sequence contains not only one copy of itself, but rather, infinitely many.\n\nThe precise definition of fractal sequence depends on a preliminary definition: a sequence \"x = (x)\" is an infinitive sequence if for every \"i\",\n\nLet \"a(i,j)\" be the \"jth\" index \"n\" for which \"x = i\". An infinitive sequence \"x\" is a fractal sequence if two additional conditions hold:\n\nAccording to (F2), the first occurrence of each \"i > 1\" in \"x\" must be preceded at least once by each of the numbers 1, 2, ..., i-1, and according to (F3), between consecutive occurrences of \"i\" in \"x\", each \"h\" less than \"i\" occurs exactly once.\n\nSuppose θ is a positive irrational number. Let\n\nand let\n\nbe the sequence obtained by arranging the numbers in S(θ) in increasing order. The sequence c(θ) is the \"signature of θ\", and it is a fractal sequence.\n\nFor example, the signature of the golden ratio (i.e., θ = (1 + sqrt(5))/2) begins with\n\nand the signature of 1/θ = θ - 1 begins with\n\nThese are sequences and in the On-Line Encyclopedia of Integer Sequences, where further examples from a variety of number-theoretic and combinatorial settings are given.\n\n\n"}
{"id": "156411", "url": "https://en.wikipedia.org/wiki?curid=156411", "title": "Galois connection", "text": "Galois connection\n\nIn mathematics, especially in order theory, a Galois connection is a particular correspondence (typically) between two partially ordered sets (posets). The same notion can also be defined on preordered sets or classes; this article presents the common case of posets. Galois connections generalize the correspondence between subgroups and subfields investigated in Galois theory (named after the French mathematician Évariste Galois). They find applications in various mathematical theories.\n\nA Galois connection is rather weak compared to an order isomorphism between the involved posets, but every Galois connection gives rise to an isomorphism of certain sub-posets, as will be explained below.\n\nThe literature contains two closely related notions of \"Galois connection\". In this article, we will distinguish between the two by referring to the first as (monotone) Galois connection and to the second as antitone Galois connection.\n\nThe term Galois correspondence is sometimes used to mean bijective \"Galois connection\"; this is simply an order isomorphism (or dual order isomorphism, depending on whether we take monotone or antitone Galois connections).\n\nLet and be two partially ordered sets. A \"monotone Galois connection\" between these posets consists of two monotone functions: and , such that for all in and in , we have\n\nIn this situation, is called the lower adjoint of and is called the upper adjoint of \"F\". Mnemonically, the upper/lower terminology refers to where the function application appears relative to ≤. The term \"adjoint\" refers to the fact that monotone Galois connections are special cases of pairs of adjoint functors in category theory as discussed further below. Other terminology encountered here is left adjoint (respectively right adjoint) for the lower (respectively upper) adjoint.\n\nAn essential property of a Galois connection is that an upper/lower adjoint of a Galois connection \"uniquely\" determines the other:\n\nA consequence of this is that if or is invertible, then each is the inverse of the other, i.e. .\n\nGiven a Galois connection with lower adjoint and upper adjoint , we can consider the compositions , known as the associated closure operator, and , known as the associated kernel operator. Both are monotone and idempotent, and we have for all in and for all in .\n\nA Galois insertion of into is a Galois connection in which the closure operator is the identity on .\n\nThe above definition is common in many applications today, and prominent in lattice and domain theory. However the original notion in Galois theory is slightly different. In this alternative definition, a Galois connection is a pair of \"antitone\", i.e. order-reversing, functions and between two posets and , such that\n\nThe symmetry of and in this version erases the distinction between upper and lower, and the two functions are then called polarities rather than adjoints. Each polarity uniquely determines the other, since\n\nThe compositions and are the associated closure operators; they are monotone idempotent maps with the property for all in and for all in .\n\nThe implications of the two definitions of Galois connections are very similar, since an antitone Galois connection between and is just a monotone Galois connection between and the order dual of . All of the below statements on Galois connections can thus easily be converted into statements about antitone Galois connections.\n\nFor an order theoretic example, let be some set, and let and both be the power set of , ordered by inclusion. Pick a fixed subset of . Then the maps and , where , and , form a monotone Galois connection, with being the lower adjoint. A similar Galois connection whose lower adjoint is given by the meet (infimum) operation can be found in any Heyting algebra. Especially, it is present in any Boolean algebra, where the two mappings can be described by and . In logical terms: \"implication from \" is the upper adjoint of \"conjunction with \".\n\nFurther interesting examples for Galois connections are described in the article on completeness properties. Roughly speaking, it turns out that the usual functions ∨ and ∧ are lower and upper adjoints to the diagonal map . The least and greatest elements of a partial order are given by lower and upper adjoints to the unique function Going further, even complete lattices can be characterized by the existence of suitable adjoints. These considerations give some impression of the ubiquity of Galois connections in order theory.\n\nLet act transitively on and pick some point in . Consider\n\nthe set of blocks containing . Further, let formula_2 consist of the subgroups of containing the stabilizer of .\n\nThen, the correspondence formula_3:\nis a monotone, one-to-one Galois connection. As a corollary, one can establish that doubly transitive actions have no blocks other than the trivial ones (singletons or the whole of ): this follows from the stabilizers being maximal in in that case. See doubly transitive group for further discussion.\n\nIf is a function, then for any subset of we can form the image and for any subset of we can form the inverse image Then and form a monotone Galois connection between the power set of and the power set of , both ordered by inclusion ⊆. There is a further adjoint pair in this situation: for a subset of , define Then and form a monotone Galois connection between the power set of and the power set of . In the first Galois connection, is the upper adjoint, while in the second Galois connection it serves as the lower adjoint.\n\nIn the case of a quotient map between algebraic objects (such as groups), this connection is called the lattice theorem: subgroups of connect to subgroups of , and the closure operator on subgroups of is given by .\n\nPick some mathematical object that has an underlying set, for instance a group, ring, vector space, etc. For any subset of , let be the smallest subobject of that contains , i.e. the subgroup, subring or subspace generated by . For any subobject of , let be the underlying set of . (We can even take to be a topological space, let the closure of , and take as \"subobjects of \" the closed subsets of .) Now and form a monotone Galois connection between subsets of and subobjects of , if both are ordered by inclusion. is the lower adjoint.\n\nA very general comment of William Lawvere is that \"syntax and semantics\" are adjoint: take to be the set of all logical theories (axiomatizations), and the power set of the set of all mathematical structures. For a theory , let be the set of all structures that satisfy the axioms ; for a set of mathematical structures , let be the minimum of the axiomatizations which approximate . We can then say that is a subset of if and only if logically implies : the \"semantics functor\" and the \"syntax functor\" form a monotone Galois connection, with semantics being the lower adjoint.\n\nThe motivating example comes from Galois theory: suppose is a field extension. Let be the set of all subfields of that contain , ordered by inclusion ⊆. If is such a subfield, write for the group of field automorphisms of that hold fixed. Let be the set of subgroups of , ordered by inclusion ⊆. For such a subgroup , define to be the field consisting of all elements of that are held fixed by all elements of . Then the maps and form an antitone Galois connection.\n\nAnalogously, given a path-connected topological space , there is an antitone Galois connection between subgroups of the fundamental group and path-connected covering spaces of . In particular, if is semi-locally simply connected, then for every subgroup of , there is a covering space with as its fundamental group.\n\nGiven an inner product space , we can form the orthogonal complement of any subspace of . This yields an antitone Galois connection between the set of subspaces of and itself, ordered by inclusion; both polarities are equal to .\n\nGiven a vector space and a subset of we can define its annihilator , consisting of all elements of the dual space of that vanish on . Similarly, given a subset of , we define its annihilator This gives an antitone Galois connection between the subsets of and the subsets of .\n\nIn algebraic geometry, the relation between sets of polynomials and their zero sets is an antitone Galois connection.\n\nFix a natural number and a field and let be the set of all subsets of the polynomial ring ordered by inclusion ⊆, and let be the set of all subsets of ordered by inclusion ⊆. If is a set of polynomials, define the variety of zeros as\n\nthe set of common zeros of the polynomials in . If is a subset of , define as an ideal of polynomials vanishing on , that is\n\nThen and \"I\" form an antitone Galois connection.\n\nThe closure on is the closure in the Zariski topology, and if the field is algebraically closed, then the closure on the polynomial ring is the radical of ideal generated by .\n\nMore generally, given a commutative ring (not necessarily a polynomial ring), there is an antitone Galois connection between radical ideals in the ring and subvarieties of the affine variety .\n\nMore generally, there is an antitone Galois connection between ideals in the ring and subschemes of the corresponding affine variety.\n\nSuppose and are arbitrary sets and a binary relation over and is given. For any subset of , we define Similarly, for any subset of , define Then and yield an antitone Galois connection between the power sets of and , both ordered by inclusion ⊆. \n\nUp to isomorphism \"all\" antitone Galois connections between power sets arise in this way. This follows from the \"Basic Theorem on Concept Lattices\" . Theory and applications of Galois connections arising from binary relations are studied in formal concept analysis. That field uses Galois connections for mathematical data analysis. Many algorithms for Galois connections can be found in the respective literature, e.g., in .\n\nIn the following, we consider a (monotone) Galois connection , where is the lower adjoint as introduced above. Some helpful and instructive basic properties can be obtained immediately. By the defining property of Galois connections, is equivalent to , for all in . By a similar reasoning (or just by applying the duality principle for order theory), one finds that , for all in . These properties can be described by saying the composite is \"deflationary\", while is \"inflationary\" (or \"extensive\").\n\nNow consider such that , then using the above one obtains . Applying the basic property of Galois connections, one can now conclude that . But this just shows that preserves the order of any two elements, i.e. it is monotone. Again, a similar reasoning yields monotonicity of . Thus monotonicity does not have to be included in the definition explicitly. However, mentioning monotonicity helps to avoid confusion about the two alternative notions of Galois connections.\n\nAnother basic property of Galois connections is the fact that , for all in . Clearly we find that\n\nbecause is inflationary as shown above. On the other hand, since is deflationary, while is monotonic, one finds that\n\nThis shows the desired equality. Furthermore, we can use this property to conclude that\n\nand\n\ni.e., and are idempotent.\n\nIt can be shown (see Blyth or Erné for proofs) that a function is a lower (resp. upper) adjoint if and only if is a residuated mapping (resp. residual mapping). Therefore, the notion of residuated mapping and monotone Galois connection are essentially the same.\n\nThe above findings can be summarized as follows: for a Galois connection, the composite is monotone (being the composite of monotone functions), inflationary, and idempotent. This states that is in fact a closure operator on . Dually, is monotone, deflationary, and idempotent. Such mappings are sometimes called kernel operators. In the context of frames and locales, the composite is called the nucleus induced by . Nuclei induce frame homomorphisms; a subset of a locale is called a sublocale if it is given by a nucleus.\n\nConversely, any closure operator on some poset gives rise to the Galois connection with lower adjoint being just the corestriction of to the image of (i.e. as a surjective mapping the closure system ). The upper adjoint is then given by the inclusion of into , that maps each closed element to itself, considered as an element of . In this way, closure operators and Galois connections are seen to be closely related, each specifying an instance of the other. Similar conclusions hold true for kernel operators.\n\nThe above considerations also show that closed elements of (elements with ) are mapped to elements within the range of the kernel operator , and vice versa.\n\nAnother important property of Galois connections is that lower adjoints preserve all suprema that exist within their domain. Dually, upper adjoints preserve all existing infima. From these properties, one can also conclude monotonicity of the adjoints immediately. The adjoint functor theorem for order theory states that the converse implication is also valid in certain cases: especially, any mapping between complete lattices that preserves all suprema is the lower adjoint of a Galois connection.\n\nIn this situation, an important feature of Galois connections is that one adjoint uniquely determines the other. Hence one can strengthen the above statement to guarantee that any supremum-preserving map between complete lattices is the lower adjoint of a unique Galois connection. The main property to derive this uniqueness is the following: For every in , is the least element of such that . Dually, for every in , is the greatest in such that . The existence of a certain Galois connection now implies the existence of the respective least or greatest elements, no matter whether the corresponding posets satisfy any completeness properties. Thus, when one upper adjoint of a Galois connection is given, the other upper adjoint can be defined via this same property.\n\nOn the other hand, some monotone function is a lower adjoint if and only if each set of the form for in , contains a greatest element. Again, this can be dualized for the upper adjoint.\n\nGalois connections also provide an interesting class of mappings between posets which can be used to obtain categories of posets. Especially, it is possible to compose Galois connections: given Galois connections between posets and and between and , the composite is also a Galois connection. When considering categories of complete lattices, this can be simplified to considering just mappings preserving all suprema (or, alternatively, infima). Mapping complete lattices to their duals, this categories display auto duality, that are quite fundamental for obtaining other duality theorems. More special kinds of morphisms that induce adjoint mappings in the other direction are the morphisms usually considered for frames (or locales).\n\nEvery partially ordered set can be viewed as a category in a natural way: there is a unique morphism from \"x\" to \"y\" if and only if . A monotone Galois connection is then nothing but a pair of adjoint functors between two categories that arise from partially ordered sets. In this context, the upper adjoint is the \"right adjoint\" while the lower adjoint is the \"left adjoint\". However, this terminology is avoided for Galois connections, since there was a time when posets were transformed into categories in a dual fashion, i.e. with arrows pointing in the opposite direction. This led to a complementary notation concerning left and right adjoints, which today is ambiguous.\n\nGalois connections may be used to describe many forms of abstraction in the theory of abstract interpretation of programming languages.\n\n\"The following books and survey articles include Galois connections using the monotone definition:\"\n\n\"Some publications using the original (antitone) definition:\"\n"}
{"id": "4788155", "url": "https://en.wikipedia.org/wiki?curid=4788155", "title": "GapP", "text": "GapP\n\nGapP is a counting complexity class, consisting of all of the functions \"f\" such that there exists a polynomial-time non-deterministic Turing machine \"M\" where, for any input \"x\", \"f(x)\" is equal to the number of accepting paths of \"M\" minus the number of rejecting paths of \"M\". GapP is exactly the closure of #P under subtraction. It also has all the other nice closure properties of #P, such as addition, multiplication, and binomial coefficients.\n\nThe counting class AWPP is defined in terms of GapP functions.\n\n"}
{"id": "22411300", "url": "https://en.wikipedia.org/wiki?curid=22411300", "title": "HHpred / HHsearch", "text": "HHpred / HHsearch\n\nHHsearch is an open-source software program for protein sequence searching that is part of the free HH-suite software package. HHpred is a free protein function and protein structure prediction server that is based on HHsearch and HHblits, another program in the HH-suite package.\nHHpred and HHsearch are among the most popular methods for protein structure prediction and the detection of remotely related sequences, each having been cited over 500 times.\n\nSequence searches are frequently performed by biologists to infer the function of an unknown protein from its sequence. For this purpose, the protein's sequence is compared to the sequences of other proteins in public databases and its function is deduced from those of the most similar sequences. Often, no sequences with annotated functions can be found in such a search. In this case, more sensitive methods are required to identify more remotely related proteins or protein families. From these relationships, hypotheses about the protein's functions, structure, and domain composition can be inferred. HHsearch performs searches with a protein sequence through databases. The HHpred server and the HH-suite software package offer many popular, regularly updated databases, such as the Protein Data Bank, as well as the InterPro, Pfam, COG, and SCOP databases.\n\nHHpred and HHsearch belong to the class of profile-profile comparison tools, which includes the most sensitive sequence search methods to date.\nThey represent both the query sequence and the database sequences by \"sequence profiles\", also called \"position-specific scoring matrices\" (PSSMs). Profiles are calculated from a multiple sequence alignment of related sequences which are collected, for example, using the PSI-BLAST program or the HHblits program from the HH-suite package. A profile is a matrix containing for each position in the query sequence the similarity score for the 20 amino acids. These scores are calculated from the frequencies of the amino acids at the corresponding positions in the multiple sequence alignment. Because profiles contain much more information than a single sequence (e.g. the position-specific degree of conservation), profile-profile comparison methods are much more powerful than sequence-sequence comparison methods like BLAST or profile-sequence comparison methods like PSI-BLAST.\n\nHHpred and HHsearch represent query and database proteins by profile hidden Markov models (HMMs), an extension of sequence profiles which also record position-specific amino acid insertion and deletion frequencies. HHsearch searches a database of HMMs with a query HMM. Before starting the search through the actual database of HMMs, HHsearch/HHpred builds a multiple sequence alignment of related sequences using the HHblits program from the HH-suite package. From this alignment, a profile HMM is calculated. The databases contain HMMs that are precalculated in the same fashion using PSI-BLAST. The output of HHpred and HHsearch is a ranked list of database matches (including E-values and probabilities for a true relationship) and the pairwise query-database sequence alignments. A search through the PDB database of proteins with solved 3D structure takes a few minutes. If a significant match with a protein of known structure (a \"template\") is found in the PDB database, HHpred allows the user to build a homology model using the MODELLER software, starting from the pairwise query-template alignment.\n\nApplications of HHpred and HHsearch include protein structure prediction, complex structure prediction, function prediction, domain prediction, domain boundary prediction, and evolutionary classification of proteins.\n\nHHpred servers have been ranked among the best servers during CASP7, 8, and 9, for blind protein structure prediction experiments. In CASP9, HHpredA, B, and C were ranked 1st, 2nd, and 3rd out of 81 participating automatic structure prediction servers in template-based modeling and 6th, 7th, 8th on all 147 targets, while being much faster than the best 20 servers. In CASP8, HHpred was ranked 7th on all targets and 2nd on the subset of single domain proteins, while still being more than 50 times faster than the top-ranked servers.\n\n\n"}
{"id": "2316192", "url": "https://en.wikipedia.org/wiki?curid=2316192", "title": "Hosohedron", "text": "Hosohedron\n\nIn geometry, an \"n\"-gonal hosohedron is a tessellation of lunes on a spherical surface, such that each lune shares the same two polar opposite vertices. \n\nA regular \"n\"-gonal hosohedron has Schläfli symbol {2, \"n\"}, with each spherical lune having internal angle radians ( degrees).\n\nFor a regular polyhedron whose Schläfli symbol is {\"m\", \"n\"}, the number of polygonal faces may be found by:\n\nThe Platonic solids known to antiquity are the only integer solutions for \"m\" ≥ 3 and \"n\" ≥ 3. The restriction \"m\" ≥ 3 enforces that the polygonal faces must have at least three sides.\n\nWhen considering polyhedra as a spherical tiling, this restriction may be relaxed, since digons (2-gons) can be represented as spherical lunes, having non-zero area. Allowing \"m\" = 2 admits a new infinite class of regular polyhedra, which are the hosohedra. On a spherical surface, the polyhedron {2, \"n\"} is represented as \"n\" abutting lunes, with interior angles of . All these lunes share two common vertices.\n\nThe digonal (lune) faces of a 2\"n\"-hosohedron, {2,2\"n\"}, represents the fundamental domains of dihedral symmetry in three dimensions: C, [\"n\"], (*\"nn\"), order 2\"n\". The reflection domains can be shown as alternately colored lunes as mirror images. Bisecting the lunes into two spherical triangles creates bipyramids and define dihedral symmetry D, order 4\"n\".\n\nThe tetragonal hosohedron is topologically equivalent to the bicylinder Steinmetz solid, the intersection of two cylinders at right-angles.\n\nThe dual of the n-gonal hosohedron {2, \"n\"} is the \"n\"-gonal dihedron, {\"n\", 2}. The polyhedron {2,2} is self-dual, and is both a hosohedron and a dihedron. \n\nA hosohedron may be modified in the same manner as the other polyhedra to produce a truncated variation. The truncated \"n\"-gonal hosohedron is the n-gonal prism.\n\nIn the limit the hosohedron becomes an apeirogonal hosohedron as a 2-dimensional tessellation:\n\nMultidimensional analogues in general are called hosotopes. A regular hosotope with Schläfli symbol {2,\"p\"...,\"q\"} has two vertices, each with a vertex figure {\"p\"...,\"q\"}.\n\nThe two-dimensional hosotope, {2}, is a digon.\n\nThe term “hosohedron” was coined by H.S.M. Coxeter, and possibly derives from the Greek ὅσος (\"hosos\") “as many”, the idea being that a hosohedron can have “as many faces as desired”.\n\n\n"}
{"id": "39783", "url": "https://en.wikipedia.org/wiki?curid=39783", "title": "Hypercube", "text": "Hypercube\n\nIn geometry, a hypercube is an \"n\"-dimensional analogue of a square () and a cube (). It is a closed, compact, convex figure whose 1-skeleton consists of groups of opposite parallel line segments aligned in each of the space's dimensions, perpendicular to each other and of the same length. A unit hypercube's longest diagonal in \"n\" dimensions is equal to formula_1.\n\nAn \"n\"-dimensional hypercube is also called an n\"-cube or an n\"-dimensional cube. The term \"measure polytope\" is also used, notably in the work of H. S. M. Coxeter (originally from Elte, 1912), but it has now been superseded.\n\nThe hypercube is the special case of a hyperrectangle (also called an \"n-orthotope\").\n\nA \"unit hypercube\" is a hypercube whose side has length one unit. Often, the hypercube whose corners (or \"vertices\") are the 2 points in R with each coordinate equal to 0 or 1 is called \"the\" unit hypercube.\n\nA hypercube can be defined by increasing the numbers of dimensions of a shape:\n\nThis can be generalized to any number of dimensions. This process of sweeping out volumes can be formalized mathematically as a Minkowski sum: the \"d\"-dimensional hypercube is the Minkowski sum of \"d\" mutually perpendicular unit-length line segments, and is therefore an example of a zonotope.\n\nThe 1-skeleton of a hypercube is a hypercube graph.\n\nA unit hypercube of \"n\" dimensions is the convex hull of the points given by all sign permutations of the Cartesian coordinates formula_2. It has an edge length of 1 and an \"n\"-dimensional volume of 1.\n\nAn \"n\"-dimensional hypercube is also often regarded as the convex hull of all sign permutations of the coordinates formula_3. This form is often chosen due to ease of writing out the coordinates. Its edge length is 2, and its \"n\"-dimensional volume is 2.\n\nEvery \"n\"-cube of n > 0 is composed of elements, or \"n\"-cubes of a lower dimension, on the (\"n\"−1)-dimensional surface on the parent hypercube. A side is any element of (\"n\"−1)-dimension of the parent hypercube. A hypercube of dimension \"n\" has 2\"n\" sides (a 1-dimensional line has 2 endpoints; a 2-dimensional square has 4 sides or edges; a 3-dimensional cube has 6 2-dimensional faces; a 4-dimensional tesseract has 8 cells). The number of vertices (points) of a hypercube is formula_4 (a cube has formula_5 vertices, for instance).\n\nThe number of \"m\"-dimensional hypercubes (just referred to as \"m\"-cube from here on) on the boundary of an \"n\"-cube is\n\nFor example, the boundary of a 4-cube (n=4) contains 8 cubes (3-cubes), 24 squares (2-cubes), 32 lines (1-cubes) and 16 vertices (0-cubes).\n\nThis identity can be proved by combinatorial arguments; each of the formula_8 vertices defines a vertex in\na \"m\"-dimensional boundary. There are formula_9 ways of choosing which lines (\"sides\") that defines the subspace that the boundary is in. But, each side is counted formula_10 times since it has that many vertices, we need to divide by this number.\n\nThis identity can also be used to generate the formula for the \"n\"-dimensional cube surface area. The surface area of a hypercube is: formula_11.\n\nThese numbers can also be generated by the linear recurrence relation\n\nFor example, extending a square via its 4 vertices adds one extra line (edge) per vertex, and also adds the final second square, to form a cube, giving formula_18 = 12 lines in total.\nAn \"n\"-cube can be projected inside a regular 2\"n\"-gonal polygon by a skew orthogonal projection, shown here from the line segment to the 15-cube.\n\nThe hypercubes are one of the few families of regular polytopes that are represented in any number of dimensions.\n\nThe hypercube (offset) family is one of three regular polytope families, labeled by Coxeter as \"γ\". The other two are the hypercube dual family, the cross-polytopes, labeled as \"β\" and the simplices, labeled as \"α\". A fourth family, the infinite tessellations of hypercubes, he labeled as \"δ\".\n\nAnother related family of semiregular and uniform polytopes is the demihypercubes, which are constructed from hypercubes with alternate vertices deleted and simplex facets added in the gaps, labeled as \"hγ\".\n\n\"n\"-cubes can be combined with their duals (the cross-polytopes) to form compound polytopes:\n\n\nThe graph of the \"n\"-hypercube's edges is isomorphic to the Hasse diagram of the (\"n\"-1)-simplex's face lattice. This can be seen by orienting the \"n\"-hypercube so that two opposite vertices lie vertically, corresponding to the (\"n\"-1)-simplex itself and the null polytope, respectively. Each vertex connected to the top vertex then uniquely maps to one of the (\"n\"-1)-simplex's facets (\"n\"-2 faces), and each vertex connected to those vertices maps to one of the simplex's \"n\"-3 faces, and so forth, and the vertices connected to the bottom vertex map to the simplex's vertices.\n\nThis relation may be used to generate the face lattice of an (\"n-1\")-simplex efficiently, since face lattice enumeration algorithms applicable to general polytopes are more computationally expensive.\n\nRegular complex polytopes can be defined in complex Hilbert space called \"generalized hypercubes\", γ = {4}{3}...{3}, or ... Real solutions exist with \"p\"=2, i.e. γ = γ = {4}{3}...{3} = {4,3..,3}. For \"p\">2, they exist in formula_19. The facets are generalized (\"n\"-1)-cube and the vertex figure are regular simplexes.\n\nThe regular polygon perimeter seen in these orthogonal projections is called a petrie polygon. The generalized squares (n=2) are shown with edges outlined as red and blue alternating color \"p\"-edges, while the higher n-cubes are drawn with black outlined \"p\"-edges.\n\nThe number of \"m\"-face elements in a \"p\"-generalized \"n\"-cube are: formula_20. This is \"p\" vertices and \"pn\" facets.\n\n\n\n"}
{"id": "13985992", "url": "https://en.wikipedia.org/wiki?curid=13985992", "title": "Jacobi–Anger expansion", "text": "Jacobi–Anger expansion\n\nIn mathematics, the Jacobi–Anger expansion (or Jacobi–Anger identity) is an expansion of exponentials of trigonometric functions in the basis of their harmonics. It is useful in physics (for example, to convert between plane waves and cylindrical waves), and in signal processing (to describe FM signals). This identity is named after the 19th-century mathematicians Carl Jacobi and Carl Theodor Anger.\n\nThe most general identity is given by:\n\nwhere formula_2 is the formula_3-th Bessel function of the first kind and formula_4 is the imaginary unit, formula_5 \nSubstituting formula_6 by formula_7, we also get:\n\nUsing the relation formula_9 valid for integer formula_3, the expansion becomes:\n\nThe following real-valued variations are often useful as well:\n\n\n"}
{"id": "9169420", "url": "https://en.wikipedia.org/wiki?curid=9169420", "title": "Jacques Dixmier", "text": "Jacques Dixmier\n\nJacques Dixmier (born 1924) is a French mathematician. He worked on operator algebras, especially C*-algebras, and wrote several of the standard reference books on them, and introduced the Dixmier trace and the Dixmier mapping. \n\nDixmier received his Ph.D. in 1949 from the University of Paris, and his students include Alain Connes.\n\nIn 1949 upon the initiative of Jean-Pierre Serre and Pierre Samuel, Dixmier became a member of Bourbaki, in which he made essential contributions to the Bourbaki volume on Lie algebras. After retiring as professor emeritus from the University of Paris VI, he spent five years at the Institut des Hautes Études Scientifiques.\n\nOften there is made the erroneous claim that Dixmier originated the name \"von Neumann algebra\" for the operator algebras introduced by John von Neumann, but Dixmier said in an interview that the name originated from a proposal by Jean Dieudonné.\nDixmier was an invited speaker at the International Congress of Mathematicians in 1966 at Moscow with talk \"Espace dual d'une algèbre, ou d'un groupe localement compact\" and again in 1978 at Helsinki with talk \"Algèbres enveloppantes\".\n\n"}
{"id": "2712157", "url": "https://en.wikipedia.org/wiki?curid=2712157", "title": "Jerry McNerney", "text": "Jerry McNerney\n\nGerald Mark McNerney (born June 18, 1951) is an American businessman, politician, and the U.S. Representative for , serving in Congress since 2007. He is a member of the Democratic Party. The district, numbered as the 11th District until 2013, is based in Stockton and includes parts of San Joaquin County, East Contra Costa County, and southern Sacramento County. McNerney holds a Ph.D in mathematics.\n\nMcNerney was born in Albuquerque, New Mexico, the son of Rosemary (née Tischhauser) and Col. John E. McNerney. He is of Swiss and Irish descent. He attended St. Joseph's Military Academy in Hays, Kansas, and, for two years, the United States Military Academy at West Point. After leaving West Point in 1971 in protest of U.S. involvement in the Vietnam War, he enrolled at the University of New Mexico in Albuquerque, where he received bachelor's and master's degrees and, in 1981, a Ph.D. in Mathematics, with a doctoral dissertation in differential geometry focusing on a generalization of the Laplace–Beltrami operator.\n\nMcNerney served several years as a contractor to Sandia National Laboratories at Kirtland Air Force Base on national security programs. In 1985, he accepted a senior engineering position with U.S. Windpower (Kenetech). In 1994, he began working as an energy consultant for PG&E, FloWind, The Electric Power Research Institute, and other utility companies. Before being elected to Congress, Jerry served as the CEO of a 2004 start-up company manufacturing wind turbines, named HAWT Power (Horizontal Axis Wind Turbine Power). A 1992 article that he co-authored in an IEEE journal is a good example of his writings during this period.\n\nMcNerney first ran for Congress against Richard Pombo in California's 11th congressional district in the 2004 House elections. He entered the race two weeks before the primary election as a write-in candidate. He qualified to be a write-in candidate for the March 2004 primary by one signature. Having no primary opponent, he won the primary and qualified for the November general election ballot as the Democratic nominee. He lost the general election, 61%-39%. \n\n\nMcNerney launched his 2006 campaign early in the fall of 2005. In June 2006 he won the Democratic primary with 52.8% of the vote, defeating Steve Filson, who had been endorsed by the DCCC, and Stevan Thomas. \nIn late July, Republicans Pete McCloskey and Tom Benigno (both of whom ran in the Republican primary against Pombo) endorsed McNerney. In September, analysis of the campaign was changed from \"Republican safe\" to \"Republican favored\" due to the emergence of McNerney's campaign. The report noted \"a [GOP] party spokesman says it's because they want to win decisively but others speculate that internal polling has delivered bad news for the incumbent.\" On October 3, 2006, a poll commissioned by Defenders of Wildlife Action Fund was released with McNerney leading Pombo 48 percent to 46 percent. Based on these events, in early October, CQPolitics.com changed their rating of this race from \"Republican Favored\" to \"Leans Republican\"\n\nOn November 7, 2006, McNerney defeated Pombo 53–47%. \n\n\nMcNerney won re-election 55% to 45% over Republican nominee Dean Andal.\n\n\nMcNerney won re-election 48–47%, defeating Republican nominee David Harmer.\n\n\nFor his first three terms, McNerney represented a district that encompassed eastern Alameda County, most of San Joaquin County and a small portion of Santa Clara County. After redistricting, his district was renumbered as the 9th District. It lost its portion of Alameda County, including McNerney's home in Pleasanton, while picking up part of Sacramento County. After the new map was announced, McNerney announced he would move to San Joaquin County in the new 9th. Although the new district is somewhat more Democratic than its predecessor, it is influenced by the Central Valley. He eventually bought a home in Stockton. He won re-election 56–44%, defeating Republican nominee Ricky Gill.\n\nIn 2010, President Obama signed into law a bill written by McNerney which improves care of returning service members with traumatic brain injuries (TBI) by establishing an evaluation panel to assess the Veteran's Administration treatments for TBI and recommend improvements. He also wrote a bill in 2013 that allowed veterans to keep receiving their benefits during the government shutdown.\n\nMcNerney was one of the first lawmakers to call for the resignation of VA Secretary Eric Shinseki following revelations in the news media about delays in care at VA health care facilities.\n\nMcNerney is a proponent of renewable energy. He voted to reduce carbon emissions. He has voted for tax incentives for renewable energy and for allowing states to impose stricter emissions standards. He opposes drilling in the Outer Continental Shelf.\n\nMcNerney co-sponsored the bill To require the Secretary of Energy to prepare a report on the impact of thermal insulation on both energy and water use for potable hot water (H.R. 4801; 113th Congress), would require the United States Secretary of Energy to prepare a report on the effects that thermal insulation has on both energy consumption and systems for providing potable water in . McNerney said that \"it is important for us to look for ways to save taxpayer money and ensure the federal government is doing its part to preserve our natural resources.\" McNerney argued it would be a good way to collect data so that \"we can use the findings from this study and make sure we are doing everything we can in both federal and private buildings to maximize energy and water efficiency.\"\n\nIn 2007, McNerney voted against legislation that would have prevented the DEA from enforcing prohibition in the twelve states (including California) which allow the use of marijuana for medical purposes.\n\nIn 2013, McNerney introduced the Methamphetamine Education, Treatment and Hope (METH) Act to modernize and expand programs that combat methamphetamine abuse by expanding treatment for addicts, particularly mothers or pregnant women, and provide grants to provide substance abuse and mental health services in rural areas.\n\nMcNerney voted in favor of legislation allowing employees to form unions by signing cards authorizing union representation. In 2009, McNerney voted for the Patient Protection and Affordable Care Act. He has opposed free trade agreements, voting against CAFTA, GATT, and the U.S.-Peru free trade agreement.\n\nIn April 2018, McNerney, together with Jared Huffman, Jamie Raskin, and Dan Kildee, launched the Congressional Freethought Caucus. Its stated goals include \"pushing public policy formed on the basis of reason, science, and moral values\", promoting the \"separation of church and state,\" opposing discrimination against \"atheists, agnostics, humanists, seekers, religious and nonreligious persons\", among others. Huffman and Raskin will act as co-chairs.\n\n\n\nMcNerney resides in Stockton, California. He and his wife, Mary, have three children. \n\n"}
{"id": "61632", "url": "https://en.wikipedia.org/wiki?curid=61632", "title": "John Milnor", "text": "John Milnor\n\nJohn Willard Milnor (born February 20, 1931) is an American mathematician known for his work in differential topology, K-theory and dynamical systems. Milnor is a distinguished professor at Stony Brook University and one of the four mathematicians to have won the Fields Medal, the Wolf Prize, and the Abel Prize (along with Pierre Deligne, Jean-Pierre Serre, and John G. Thompson).\n\nMilnor was born on February 20, 1931 in Orange, New Jersey. His father was J. Willard Milnor and his mother was Emily Cox Milnor. As an undergraduate at Princeton University he was named a Putnam Fellow in 1949 and 1950 and also proved the Fary–Milnor theorem. He continued on to graduate school at Princeton under the direction of Ralph Fox and submitted his dissertation, entitled \"Isotopy of Links\", which concerned link groups (a generalization of the classical knot group) and their associated link structure, in 1954. Upon completing his doctorate he went on to work at Princeton. He was a professor at the Institute for Advanced Study from 1970 to 1990.\n\nHis students have included Tadatoshi Akiba, Jon Folkman, John Mather, Laurent C. Siebenmann, and Michael Spivak. His wife, Dusa McDuff, is a professor of mathematics at Barnard College.\n\nOne of his published works is his proof in 1956 of the existence of 7-dimensional spheres with nonstandard differential structure. Later, with Michel Kervaire, he showed that the 7-sphere has 15 differentiable structures (28 if one considers orientation).An \"n\"-sphere with nonstandard differential structure is called an exotic sphere, a term coined by Milnor. Egbert Brieskorn found simple algebraic equations for 28 complex hypersurfaces in complex 5-space such that their intersection with a small sphere of dimension 9 around a singular point is diffeomorphic to these exotic spheres. Subsequently Milnor worked on the topology of isolated singular points of complex hypersurfaces in general, developing the theory of the Milnor fibration whose fiber has the homotopy type of a bouquet of μ spheres where μ is known as the Milnor number. Milnor's 1968 book on his theory inspired the growth of a huge and rich research area which continues to mature to this day.\n\nIn 1961 Milnor disproved the Hauptvermutung by illustrating two simplicial complexes which are homeomorphic but combinatorially distinct.\n\nIn 1984 Milnor introduced a definition of attractor . The objects generalize standard attractors, include so-called unstable attractors and are now known as Milnor attractors.\n\nMilnor's current interest is dynamics, especially holomorphic dynamics. His work in dynamics is summarized by Peter Makienko in his review of \"Topological Methods in Modern Mathematics\":\n\nIt is evident now that low-dimensional dynamics, to a large extent initiated by Milnor's work, is a fundamental part of general dynamical systems theory. Milnor cast his eye on dynamical systems theory in the mid-1970s. By that time the Smale program in dynamics had been completed. Milnor's approach was to start over from the very beginning, looking at the simplest nontrivial families of maps. The first choice, one-dimensional dynamics, became the subject of his joint paper with Thurston. Even the case of a unimodal map, that is, one with a single critical point, turns out to be extremely rich. This work may be compared with Poincaré's work on circle diffeomorphisms, which 100 years before had inaugurated the qualitative theory of dynamical systems. Milnor's work has opened several new directions in this field, and has given us many basic concepts, challenging problems and nice theorems. \n\nHe was an editor of the \"Annals of Mathematics\" for a number of years after 1962. He has written a number of books.\n\nIn 1962 Milnor was awarded the Fields Medal for his work in differential topology. He later went on to win the National Medal of Science (1967), the Lester R. Ford Award in 1970 and again in 1984, the Leroy P Steele Prize for \"Seminal Contribution to Research\" (1982), the Wolf Prize in Mathematics (1989), the Leroy P Steele Prize for Mathematical Exposition (2004), and the Leroy P Steele Prize for Lifetime Achievement (2011) \"... for a paper of fundamental and lasting importance, On manifolds homeomorphic to the 7-sphere, Annals of Mathematics 64 (1956), 399–405\". In 1991 a symposium was held at Stony Brook University in celebration of his 60th birthday.\n\nMilnor was awarded the 2011 Abel Prize, for his \"pioneering discoveries in topology, geometry and algebra.\" Reacting to the award, Milnor told the \"New Scientist\" \"It feels very good,\" adding that \"[o]ne is always surprised by a call at 6 o'clock in the morning.\"\nIn 2013 he became a fellow of the American Mathematical Society, for \"contributions to differential topology, geometric topology, algebraic topology, algebra, and dynamical systems\".\n\n\n\n\n"}
{"id": "267444", "url": "https://en.wikipedia.org/wiki?curid=267444", "title": "Jordan curve theorem", "text": "Jordan curve theorem\n\nIn topology, a Jordan curve, sometimes called a \"plane simple closed curve\", is a non-self-intersecting continuous loop in the plane. The Jordan curve theorem asserts that every Jordan curve divides the plane into an \"interior\" region bounded by the curve and an \"exterior\" region containing all of the nearby and far away exterior points, so that every continuous path connecting a point of one region to a point of the other intersects with that loop somewhere. While the statement of this theorem seems to be intuitively obvious, it takes some ingenuity to prove it by elementary means. More transparent proofs rely on the mathematical machinery of algebraic topology, and these lead to generalizations to higher-dimensional spaces.\n\nThe Jordan curve theorem is named after the mathematician Camille Jordan (1838–1922), who found its first proof. For decades, mathematicians generally thought that this proof was flawed and that the first rigorous proof was carried out by Oswald Veblen. However, this notion has been challenged by Thomas C. Hales and others.\n\nA \"Jordan curve\" or a \"simple closed curve\" in the plane R is the image \"C\" of an injective continuous map of a circle into the plane, \"φ\": \"S\" → R. A Jordan arc in the plane is the image of an injective continuous map of a closed interval into the plane.\n\nAlternatively, a Jordan curve is the image of a continuous map \"φ\": [0,1] → R such that \"φ\"(0) = \"φ\"(1) and the restriction of \"φ\" to [0,1) is injective. The first two conditions say that \"C\" is a continuous loop, whereas the last condition stipulates that \"C\" has no self-intersection points.\n\nWith these definitions, the Jordan curve theorem can be stated as follows:\nLet \"C\" be a Jordan curve in the plane R. Then its complement, R \\ \"C\", consists of exactly two connected components. One of these components is bounded (the interior) and the other is unbounded (the exterior), and the curve \"C\" is the boundary of each component.\nFurthermore, the complement of a Jordan arc in the plane is connected.\n\nThe Jordan curve theorem was independently generalized to higher dimensions by H. Lebesgue and L.E.J. Brouwer in 1911, resulting in the Jordan–Brouwer separation theorem.\nLet \"X\" be a \"topological sphere\" in the (\"n\"+1)-dimensional Euclidean space R (\"n\" > 0), i.e. the image of an injective continuous mapping of the \"n\"-sphere \"S\" into R. Then the complement \"Y\" of \"X\" in R consists of exactly two connected components. One of these components is bounded (the interior) and the other is unbounded (the exterior). The set \"X\" is their common boundary.\nThe proof uses homology theory. It is first established that, more generally, if \"X\" is homeomorphic to the \"k\"-sphere, then the reduced integral homology groups of \"Y\" = R \\ \"X\" are as follows:\n\nThis is proved by induction in \"k\" using the Mayer–Vietoris sequence. When \"n\" = \"k\", the zeroth reduced homology of \"Y\" has rank 1, which means that \"Y\" has 2 connected components (which are, moreover, path connected), and with a bit of extra work, one shows that their common boundary is \"X\". A further generalization was found by J. W. Alexander, who established the Alexander duality between the reduced homology of a compact subset \"X\" of R and the reduced cohomology of its complement. If \"X\" is an \"n\"-dimensional compact connected submanifold of R (or S) without boundary, its complement has 2 connected components.\n\nThere is a strengthening of the Jordan curve theorem, called the Jordan–Schönflies theorem, which states that the interior and the exterior planar regions determined by a Jordan curve in R are homeomorphic to the interior and exterior of the unit disk. In particular, for any point \"P\" in the interior region and a point \"A\" on the Jordan curve, there exists a Jordan arc connecting \"P\" with \"A\" and, with the exception of the endpoint \"A\", completely lying in the interior region. An alternative and equivalent formulation of the Jordan–Schönflies theorem asserts that any Jordan curve \"φ\": \"S\" → R, where \"S\" is viewed as the unit circle in the plane, can be extended to a homeomorphism \"ψ\": R → R of the plane. Unlike Lebesgues' and Brouwer's generalization of the Jordan curve theorem, this statement becomes \"false\" in higher dimensions: while the exterior of the unit ball in R is simply connected, because it retracts onto the unit sphere, the Alexander horned sphere is a subset of R homeomorphic to a sphere, but so twisted in space that the unbounded component of its complement in R is not simply connected, and hence not homeomorphic to the exterior of the unit ball.\n\nThe statement of the Jordan curve theorem may seem obvious at first, but it is a rather difficult theorem to prove. Bernard Bolzano was the first to formulate a precise conjecture, observing that it was not a self-evident statement, but that it required a proof. It is easy to establish this result for polygons, but the problem came in generalizing it to all kinds of badly behaved curves, which include nowhere differentiable curves, such as the Koch snowflake and other fractal curves, or even a Jordan curve of positive area constructed by .\n\nThe first proof of this theorem was given by Camille Jordan in his lectures on real analysis, and was published in his book \"Cours d'analyse de l'École Polytechnique\". There is some controversy about whether Jordan's proof was complete: the majority of commenters on it have claimed that the first complete proof was given later by Oswald Veblen, who said the following about Jordan's proof:\n\nHowever, Thomas C. Hales wrote:\n\nHales also pointed out that the special case of simple polygons is not only an easy exercise, but was not really used by Jordan anyway, and quoted Michael Reeken as saying:\n\nEarlier, Jordan's proof and another early proof by de la Vallée Poussin had already been critically analyzed and completed by Schoenflies (1924).\n\nDue to the importance of the Jordan curve theorem in low-dimensional topology and complex analysis, it received much attention from prominent mathematicians of the first half of the 20th century. Various proofs of the theorem and its generalizations were constructed by J. W. Alexander, Louis Antoine, Bieberbach, Luitzen Brouwer, Denjoy, Hartogs, Béla Kerékjártó, Alfred Pringsheim, and Arthur Moritz Schoenflies.\n\nNew elementary proofs of the Jordan curve theorem, as well as simplifications of the earlier proofs, continue to be carried out.\n\n\nThe first formal proof of the Jordan curve theorem was created by in the HOL Light system, in January 2005, and contained about 60,000 lines. Another rigorous 6,500-line formal proof was produced in 2005 by an international team of mathematicians using the Mizar system. Both the Mizar and the HOL Light proof rely on libraries of previously proved theorems, so these two sizes are not comparable. showed that the Jordan curve theorem is equivalent in proof-theoretic strength to the weak König's lemma.\n\n\n\n"}
{"id": "12120792", "url": "https://en.wikipedia.org/wiki?curid=12120792", "title": "Legendre wavelet", "text": "Legendre wavelet\n\nIn functional analysis, compactly supported wavelets derived from Legendre polynomials are termed Legendre wavelets or spherical harmonic wavelets. Legendre functions have widespread applications in which spherical coordinate system is appropriate.\n\nWavelets associated to FIR filters are commonly preferred in most applications. An extra appealing feature is that the Legendre filters are \"linear phase\" FIR (i.e. multiresolution analysis associated with linear phase filters). These wavelets have been implemented on MATLAB (wavelet toolbox). Although being compactly supported wavelet, legdN are not orthogonal (but for \"N\" = 1).\n\nAssociated Legendre polynomials are the colatitudinal part of the spherical harmonics which are common to all separations of Laplace's equation in spherical polar coordinates. The radial part of the solution varies from one potential to another, but the harmonics are always the same and are a consequence of spherical symmetry. Spherical harmonics formula_1 are solutions of the Legendre formula_2-order differential equation, \"n\" integer:\n\nformula_4 polynomials can be used to define the smoothing filter formula_5 of a multiresolution analysis (MRA). Since the appropriate boundary conditions for an MRA are formula_6 and formula_7, the smoothing filter of an MRA can be defined so that the magnitude of the low-pass formula_8 can be associated to Legendre polynomials according to: formula_9\n\nIllustrative examples of filter transfer functions for a Legendre MRA are shown in figure 1, for formula_11 A low-pass behaviour is exhibited for the filter \"H\", as expected. The number of zeroes within formula_12 is equal to the degree of the Legendre polynomial. Therefore, the roll-off of side-lobes with frequency is easily controlled by the parameter formula_13.\n\nThe low-pass filter transfer function is given by\n\nThe transfer function of the high-pass analysing filter formula_15 is chosen according to Quadrature mirror filter condition, yielding:\n\nIndeed, formula_17 and formula_18, as expected.\n\nA suitable phase assignment is done so as to properly adjust the transfer function formula_19 to the form\n\nThe filter coefficients formula_21 are given by:\n\nfrom which the symmetry: \n\nfollows. There are just formula_24 non-zero filter coefficients on formula_25, so that the Legendre wavelets have compact support for every odd integer formula_13.\n\nLegendre wavelets can be easily loaded into the MATLAB wavelet toolbox—The m-files to allow the computation of Legendre wavelet transform, details and filter are (freeware) available. The finite support width Legendre family is denoted by legd (short name). Wavelets: 'legdN'. The parameter \"N\" in the legdN family is found according to formula_29 (length of the MRA filters). \n\nLegendre wavelets can be derived from the low-pass reconstruction filter by an iterative procedure (the cascade algorithm). The wavelet has compact support and finite impulse response AMR filters (FIR) are used (table 1). The first wavelet of the Legendre's family is exactly the well-known Haar wavelet. Figure 2 shows an emerging pattern that progressively looks like the wavelet's shape. \n\nThe Legendre wavelet shape can be visualised using the wavemenu command of MATLAB. Figure 3 shows legd8 wavelet displayed using MATLAB. Legendre Polynomials are also associated with windows families.\n\nWavelet packets (WP) systems derived from Legendre wavelets can also be easily accomplished. Figure 5 illustrates the WP functions derived from legd2. \n"}
{"id": "55569888", "url": "https://en.wikipedia.org/wiki?curid=55569888", "title": "List of quantum processors", "text": "List of quantum processors\n\nThis list contains quantum processors, also known as quantum processing units (QPUs).\n\nThese QPUs are based on the quantum circuit and quantum logic gate-based model of computing.\nThese QPUs are based on quantum annealing.\n"}
{"id": "37956942", "url": "https://en.wikipedia.org/wiki?curid=37956942", "title": "List of things named after Sophus Lie", "text": "List of things named after Sophus Lie\n\nThis is a list of things named after Sophus Lie. Sophus Lie (1842 – 1899), a mathematician, is the eponym of all of the things (and topics) listed below.\n\n\n"}
{"id": "58720309", "url": "https://en.wikipedia.org/wiki?curid=58720309", "title": "Mary Bradburn", "text": "Mary Bradburn\n\nMary Bradburn (1918–2000) was a British mathematics educator who became president of the Mathematical Association for the 1994–1995 term.\n\nBradborn was born on 17 March 1918 in Normanby in North Yorkshire, the daughter of a marine engineer and a Scotswoman. She attended a school that didn't approve of girls studying mathematics, but allowed her to progress through the mathematics curriculum at her own rate, several years ahead of the other students.\n\nShe earned a state scholarship, but at 17, she was below the required age for Oxford and Cambridge, so she ended up going to Royal Holloway College. She was a student there beginning in 1935 and, despite multiple extracurricular activities, earned first class honours in mathematics in 1938, and completed a master's degree there in 1940. \n\nWith another scholarship from the University of London, she went to the University of Edinburgh for graduate study with Max Born, beginning in 1941; her dissertation was \"The Statistical Thermodynamics of Crystal Lattices\".\nShe taught briefly at Edinburgh and the University of Dundee before\nreturning to Royal Holloway as an instructor in 1945. She remained at Royal Holloway through its 1965 transition from a women's college to a coeducational one (a change that she supported), until her retirement in 1980.\n\nShe became a Fellow of the Royal Astronomical Society in 1955.\nThe mathematics department of Royal Holloway offers an annual prize to undergraduates, the Mary Bradburn Prize, named in her memory.\nThe British Federation of Women Graduates also offers a Mary Bradburn Prize,\nfrom a bequest left by Bradburn.\n"}
{"id": "12809158", "url": "https://en.wikipedia.org/wiki?curid=12809158", "title": "Mathematical Contest in Modeling", "text": "Mathematical Contest in Modeling\n\nThe Mathematical Contest in Modeling (MCM) is a multi-day mathematical modelling competition held annually in USA, during the first or second weekend in February, since 1985 by COMAP and sponsored by SIAM, the NSA, and INFORMS. It is distinguished from other major mathematical competitions such as Putnam by its strong focus on research, originality, teamwork, communication and justification of results. It runs concurrently with the Interdisciplinary Contest in Modeling (ICM).\n\nAt the beginning of the contest, teams have a choice between two problems. Problem A involves a system that requires the use of continuous mathematics, and thus often involves concepts from geometry, physics, or engineering. Problem B involves a system that requires the use of discrete mathematics. In 2016, a \"data insights\" problem was added, where teams are given access to database files and tasked with using them to answer a question. This problem was designated as Problem C, though previously, Problem C referred to an ICM problem. These problems tend to be open-ended, and are drawn from all fields of science, business, and public policy. Past problems include\nTeams have 96 hours to research and submit their solutions in the form of a research paper. During this time, they may consult any available references, but may not discuss their problem with anyone outside their teams. Several guides containing advice and recommendations for teams and/or advisors have been published online or in print.\n\nThousands of international teams of three undergraduates compete to produce original mathematical papers in response to one of two modeling problems. Initially, participation was largely from the United States, however in recent years international participation has grown significantly, particularly from the People's Republic of China, so that in 2007 teams from the United States comprised only 24% of total participation. In 2014, the percentage of teams from the People's Republic of China reached a record high of 92.9%.\n\nAfter the competition, all papers are judged and placed into the following categories:\n\nUntil 2009, Outstanding Winner papers were published in The UMAP Journal.\n\n\n\n\n\n\n\n"}
{"id": "3910789", "url": "https://en.wikipedia.org/wiki?curid=3910789", "title": "Modified internal rate of return", "text": "Modified internal rate of return\n\nThe modified internal rate of return (MIRR) is a financial measure of an investment's attractiveness. It is used in capital budgeting to rank alternative investments of equal size. As the name implies, MIRR is a modification of the internal rate of return (IRR) and as such aims to resolve some problems with the IRR.\n\nWhile there are several problems with the IRR, MIRR resolves two of them.\n\nFirstly, IRR is sometimes misapplied, under an assumption that interim positive cash flows are reinvested at the same rate of return as that of the project that generated them. This is usually an unrealistic scenario and a more likely situation is that the funds will be reinvested at a rate closer to the firm's cost of capital. The IRR therefore often gives an unduly optimistic picture of the projects under study. Generally for comparing projects more fairly, the weighted average cost of capital should be used for reinvesting the interim cash flows.\n\nSecondly, more than one IRR can be found for projects with alternating positive and negative cash flows, which leads to confusion and ambiguity. MIRR finds only one value.\n\nMIRR is calculated as follows:\n\nformula_1, \n\nwhere \"n\" is the number of equal periods at the end of which the cash flows occur (not the number of cash flows), \"PV\" is present value (at the beginning of the first period), \"FV\" is future value (at the end of the last period).\n\nThe formula adds up the negative cash flows after discounting them to time zero using the external cost of capital, adds up the positive cash flows including the proceeds of reinvestment at the external reinvestment rate to the final period, and then works out what rate of return would cause the magnitude of the discounted negative cash flows at time zero to be equivalent to the future value of the positive cash flows at the final time period.\n\nSpreadsheet applications, such as Microsoft Excel, have inbuilt functions to calculate the MIRR. In Microsoft Excel this function is \"=MIRR(...)\".\n\nIf an investment project is described by the sequence of cash flows:\n\nthen the IRR formula_2 is given by\n\nformula_3.\n\nIn this case, the answer is 25.48% (with this conventional pattern of cash flows, the project has a unique IRR).\n\nTo calculate the MIRR, we will assume a finance rate of 10% and a reinvestment rate of 12%. First, we calculate the present value of the negative cash flows (discounted at the finance rate):\n\nformula_4.\n\nSecond, we calculate the future value of the positive cash flows (reinvested at the reinvestment rate):\n\nformula_5.\n\nThird, we find the MIRR:\n\nformula_6.\n\nThe calculated MIRR (17.91%) is significantly different from the IRR (25.48%).\n\nLike the internal rate of return, the modified internal rate of return is not valid for ranking projects of different sizes, because a larger project with a smaller modified internal rate of return may have a higher net present value. However, there exist variants of the modified internal rate of return which can be used for such comparisons.\n"}
{"id": "3366476", "url": "https://en.wikipedia.org/wiki?curid=3366476", "title": "Moore method", "text": "Moore method\n\nThe Moore method is a deductive manner of instruction used in advanced mathematics courses. It is named after Robert Lee Moore, a famous topologist who first used a stronger version of the method at the University of Pennsylvania when he began teaching there in 1911.\n\nThe way the course is conducted varies from instructor to instructor, but the content of the course is usually presented in whole or in part by the students themselves. Instead of using a textbook, the students are given a list of definitions and theorems which they are to prove and present in class, leading them through the subject material. The Moore method typically limits the amount of material that a class is able to cover, but its advocates claim that it induces a depth of understanding that listening to lectures cannot give.\n\nF. Burton Jones, a student of Moore and a practitioner of his method, described it as follows:\n\nThe students were forbidden to read any book or article about the subject. They were even forbidden to talk about it outside of class. Hersh and John-Steiner claim that, \"this method is reminiscent of a well-known, old method of teaching swimming called 'sink or swim' \".\n\nAfter Moore became an associate-professor at University of Texas at Austin in 1920, the Moore method began to gain popularity. Today, the University of Texas at Austin remains a strong advocate of the method and uses it in various courses within their mathematics department, including:\n\nIn addition:\n\n\n\n"}
{"id": "1835001", "url": "https://en.wikipedia.org/wiki?curid=1835001", "title": "Nakayama's lemma", "text": "Nakayama's lemma\n\nIn mathematics, more specifically modern algebra and commutative algebra, Nakayama's lemma — also known as the Krull–Azumaya theorem — governs the interaction between the Jacobson radical of a ring (typically a commutative ring) and its finitely generated modules. Informally, the lemma immediately gives a precise sense in which finitely generated modules over a commutative ring behave like vector spaces over a field. It is an important tool in algebraic geometry, because it allows local data on algebraic varieties, in the form of modules over local rings, to be studied pointwise as vector spaces over the residue field of the ring.\n\nThe lemma is named after the Japanese mathematician Tadashi Nakayama and introduced in its present form in , although it was first discovered in the special case of ideals in a commutative ring by Wolfgang Krull and then in general by Goro Azumaya (1951). In the commutative case, the lemma is a simple consequence of a generalized form of the Cayley–Hamilton theorem, an observation made by Michael Atiyah (1969). The special case of the noncommutative version of the lemma for right ideals appears in Nathan Jacobson (1945), and so the noncommutative Nakayama lemma is sometimes known as the Jacobson–Azumaya theorem. The latter has various applications in the theory of Jacobson radicals.\n\nLet \"R\" be a commutative ring with identity 1. The following is Nakayama's lemma, as stated in :\n\nStatement 1: Let \"I\" be an ideal in \"R\", and \"M\" a finitely-generated module over \"R\". If \"IM\" = \"M\", then there exists an \"r\" ∈ \"R\" with \"r\" ≡ 1 (mod \"I\"), such that \"rM\" = 0.\n\nThis is proven below.\n\nThe following corollary is also known as Nakayama's lemma, and it is in this form that it most often appears.\n\nStatement 2: If \"M\" is a finitely-generated module over \"R\", J(\"R\") is the Jacobson radical of \"R\", and J(\"R\")\"M\" = \"M\", then \"M\" = 0.\n\nMore generally, one has\n\nStatement 3: If \"M\" is a finitely-generated module over \"R\", \"N\" is a submodule of \"M\", and \"M\" = \"N\" + J(\"R\")\"M\", then \"M\" = \"N\".\n\nThe following result manifests Nakayama's lemma in terms of generators\n\nStatement 4: If \"M\" is a finitely-generated module over \"R\" and the images of elements \"m\"...,\"m\" of \"M\" in \"M\" / \"J\"(\"R\")\"M\" generate \"M\" / \"J\"(\"R\")\"M\" as an \"R\"-module, then \"m\"...,\"m\" also generate \"M\" as an \"R\"-module.\n\nIf one assumes instead that \"R\" is complete and \"M\" is separated with respect to the \"I\"-adic topology for an ideal \"I\" in \"R\", this last statement holds with \"I\" in place of \"J\"(\"R\") and without assuming in advance that \"M\" is finitely generated. Here separatedness means that the \"I\"-adic topology satisfies the \"T\" separation axiom, and is equivalent to formula_1\n\nIn the special case of a finitely generated module \"M\" over a local ring \"R\" with maximal ideal \"m\", the quotient \"M\"/\"mM\" is a vector space over the field \"R\"/\"m\". Statement 4 then implies that a basis of \"M\"/\"mM\" lifts to a minimal set of generators of \"M\". Conversely, every minimal set of generators of \"M\" is obtained in this way, and any two such sets of generators are related by an invertible matrix with entries in the ring.\n\nIn this form, Nakayama's lemma takes on concrete geometrical significance. Local rings arise in geometry as the germs of functions at a point. Finitely generated modules over local rings arise quite often as germs of sections of vector bundles. Working at the level of germs rather than points, the notion of finite-dimensional vector bundle gives way to that of a coherent sheaf. Informally, Nakayama's lemma says that one can still regard a coherent sheaf as coming from a vector bundle in some sense. More precisely, let \"F\" be a coherent sheaf of O-modules over an arbitrary scheme \"X\". The stalk of \"F\" at a point \"p\" ∈ \"X\", denoted by \"F\", is a module over the local ring O. The fibre of \"F\" at \"p\" is the vector space \"F\"(\"p\") = \"F\"/\"mF\" where \"m\" is the maximal ideal of O. Nakayama's lemma implies that a basis of the fibre \"F\"(\"p\") lifts to a minimal set of generators of \"F\". That is:\n\nThe going up theorem is essentially a corollary of Nakayama's lemma. It asserts:\n\n\nNakayama's lemma makes precise one sense in which finitely generated modules over a commutative ring are like vector spaces over a field. The following consequence of Nakayama's lemma gives another way in which this is true:\n\n\nOver a local ring, one can say more about module epimorphisms:\n\n\nNakayama's lemma also has several versions in homological algebra. The above statement about epimorphisms can be used to show:\n\nA geometrical and global counterpart to this is the Serre–Swan theorem, relating projective modules and coherent sheaves.\n\nMore generally, one has\n\nA standard proof of the Nakayama lemma uses the following technique due to .\n\nThis assertion is precisely a generalized version of the Cayley–Hamilton theorem, and the proof proceeds along the same lines. On the generators \"x\" of \"M\", one has a relation of the form\nwhere \"a\" ∈ \"I\". Thus\nThe required result follows by multiplying by the adjugate of the matrix (φδ − \"a\") and invoking Cramer's rule. One finds then det(φδ − \"a\") = 0, so the required polynomial is\n\nTo prove Nakayama's lemma from the Cayley–Hamilton theorem, assume that \"IM\" = \"M\" and take φ to be the identity on \"M\". Then define a polynomial \"p\"(\"x\") as above. Then\nhas the required property.\n\nA version of the lemma holds for right modules over non-commutative unital rings \"R\". The resulting theorem is sometimes known as the Jacobson–Azumaya theorem.\n\nLet J(\"R\") be the Jacobson radical of \"R\". If \"U\" is a right module over a ring, \"R\", and \"I\" is a right ideal in \"R\", then define \"U·I\" to be the set of all (finite) sums of elements of the form \"u·i\", where · is simply the action of \"R\" on \"U\". Necessarily, \"U·I\" is a submodule of \"U\".\n\nIf \"V\" is a maximal submodule of \"U\", then \"U\"/\"V\" is simple. So \"U·J(\"R\") is necessarily a subset of \"V\", by the definition of J(\"R\") and the fact that \"U\"/\"V\" is simple. Thus, if \"U\" contains at least one (proper) maximal submodule, \"U·J(\"R\") is a proper submodule of \"U\". However, this need not hold for arbitrary modules \"U\" over \"R\", for \"U\" need not contain any maximal submodules. Naturally, if \"U\" is a Noetherian module, this holds. If \"R\" is Noetherian, and \"U\" is finitely generated, then \"U\" is a Noetherian module over \"R\", and the conclusion is satisfied. Somewhat remarkable is that the weaker assumption, namely that \"U\" is finitely generated as an \"R\"-module (and no finiteness assumption on \"R\"), is sufficient to guarantee the conclusion. This is essentially the statement of Nakayama's lemma.\n\nPrecisely, one has:\n\nLet \"X\" be a finite subset of \"U\", minimal with respect to the property that it generates \"U\". Since \"U\" is non-zero, this set \"X\" is nonempty. Denote every element of \"X\" by \"x\", for formula_9. Since \"X\" generates \"U\",\n\nSuppose, formula_11, to obtain a contradiction. Since, formula_12, we conclude,\n\nBy associativity,\n\nSince \"J\"(\"R\") is a (two-sided) ideal in \"R\", we have formula_17 for every \"i\", and thus this becomes\n\nApplying distributivity,\n\nSince formula_19, it is quasiregular and thus, formula_22, for all \"i\", where \"U\"(\"R\") denotes the group of units in \"R\". Choose some \"j\" and write,\n\nTherefore,\n\nThus \"x\" is a linear combination of the elements of \"X\" distinct from \"x\". This contradicts the minimality of \"X\" and establishes the result.\n\nThere is also a graded version of Nakayama's lemma. Let \"R\" be a ring that is graded by the ordered semi group of non-negative integers, and let formula_25 denote the ideal generated by positively graded elements. Then if \"M\" is a graded module over \"R\" for which formula_26 for \"i\" sufficiently negative (in particular, if \"M\" is finitely generated and \"R\" does not contain elements of negative degree) such that formula_27, then formula_28. Of particular importance is the case that \"R\" is a polynomial ring with the standard grading, and \"M\" is a finitely generated module.\n\nThe proof is much easier than in the ungraded case: taking \"i\" to be the least integer such that formula_29, we see that formula_30 does not appear in formula_31, so either formula_32, or such an \"i\" does not exist, i.e., formula_28.\n\n\n"}
{"id": "50506958", "url": "https://en.wikipedia.org/wiki?curid=50506958", "title": "Nora Calderwood", "text": "Nora Calderwood\n\nNora Isobel Calderwood (14 March 1896 – April 1985) was a Scottish professor and mathematician.\n\nCalderwood was born in 1896 in Blairgowrie, Perthshire, in Scotland. Her father Daniel Scott Calderwood was the headmaster of the Blairgowrie Public School. Her family then moved to Edinburgh when she was still young, after her father was appointed as the headmaster of the Church of Scotland Normal School.\n\nCalderwood started at James Gillespie's School in 1901, at the age of five, staying for six years. On receiving a bursary from the Edinburgh Burgh Committee on Secondary Education, she studied at Edinburgh Ladies' College from 1907 to 1914. In 1910, at the age of 14, she passed Higher Piano, and in 1913 was named the dux of the music classes at Edinburgh Ladies' College. She was also awarded the prize to the best Science scholar and best Arithmetician, both of which she resigned, and the Costorphine Prize for the best mathematician.\n\nCalderwood studied at the University of Edinburgh from 1914 to 1920, earning a Bachelor of Science (Pure) in 1919 and a Master of Arts in Political Economy in 1920. Courses taken include Mathematics, Latin, Natural Philosophy, and Chemistry. She joined the faculty of Birmingham University the following year, lecturing in mathematics. Soon, however, she returned to Edinburgh to continue her studies under mathematician Alexander Aitken, earning a PhD in mathematics from the University in 1931 with a thesis on \"Researches in the Theory of Matrices.\"\nCalderwood was a member of Edinburgh Mathematical Society joined in 1919 as an undergraduate, and the London Mathematical Society, joined in 1922.\n\nShe is fondly listed by a student, Margaret Lee née Ireland (1962 BSc Mathematics) as one of her favourite memories: \"Dr Nora Calderwood - what a woman who loved us so much, she could barely keep exam questions secret\".\n\nHers is the namesake for the Calderwood Prize, an academic award at the University of Birmingham.\n\nCalderwood never married. She was very proficient at piano, and gave recitals at Birmingham.\n"}
{"id": "632539", "url": "https://en.wikipedia.org/wiki?curid=632539", "title": "Numerical Recipes", "text": "Numerical Recipes\n\nNumerical Recipes is the generic title of a series of books on algorithms and numerical analysis by William H. Press, Saul A. Teukolsky, William T. Vetterling and Brian P. Flannery. In various editions, the books have been in print since 1986. The most recent edition was published in 2007. In 2015 Numerical Recipes sold its historic two-letter domain name nr.com and became codice_1 instead.\n\nThe \"Numerical Recipes\" books cover a range of topics that include both classical numerical analysis (interpolation, integration, linear algebra, differential equations, and so on), signal processing (Fourier methods, filtering), statistical treatment of data, and a few topics in machine learning (hidden Markov model, support vector machines). The writing style is accessible and has an informal tone. The emphasis is on understanding the underlying basics of techniques, not on the refinements that may, in practice, be needed to achieve optimal performance and reliability. Few results are proved with any degree of rigor, although the ideas behind proofs are often sketched, and references are given. Importantly, virtually all methods that are discussed are also implemented in a programming language, with the code printed in the book. Each version is keyed to a specific language.\n\nAccording to the publisher, Cambridge University Press, the \"Numerical Recipes\" books are historically the all-time best-selling books on scientific programming methods. In recent years, \"Numerical Recipes\" books have been cited in the scientific literature more than 3000 times per year according to ISI Web of Knowledge (e.g., 3962 times in the year 2008). And as of the end of 2017, the book had over 44000 citation on Google Scholar.\n\nThe first publication was in 1986 with the title,”Numerical Recipes, The Art of Scientific Computing,” containing code in both Fortran and Pascal; an accompanying book, “Numerical Recipes Example Book (Pascal) was first published in 1985. (A preface note in “Examples\" mentions that the main book was also published in 1985, but the official note in that book says 1986.) Supplemental editions followed with code in Pascal, BASIC, and C. \"Numerical Recipes\" took, from the start, an opinionated editorial position at odds with the conventional wisdom of the numerical analysis community:\n\nHowever, as it turned out, the 1980s were fertile years for the \"black box\" side, yielding important libraries such as BLAS and LAPACK, and integrated environments like MATLAB and Mathematica. By the early 1990s, when Second Edition versions of \"Numerical Recipes\" (with code in C, Fortran-77, and Fortran-90) were published, it was clear that the constituency for \"Numerical Recipes\" was by no means the majority of scientists doing computation, but only that slice that lived \"between\" the more mathematical numerical analysts and the larger community using integrated environments. The Second Edition versions occupied a stable role in this niche environment.\n\nBy the mid-2000s, the practice of scientific computing had been radically altered by the mature Internet and Web. Recognizing that their \"Numerical Recipes\" books were increasingly valued more for their explanatory text than for their code examples, the authors significantly expanded the scope of the book, and significantly rewrote a large part of the text. They continued to include code, still printed in the book, now in C++, for every method discussed. The Third Edition was also released as an electronic book, eventually made available on the Web for free (with limited page views) or by paid or institutional subscription (with unlimited page views).\n\nThe code listings are copyrighted and commercially licensed by the \"Numerical Recipes\" authors. One early motivation for the GNU Scientific Library was that a free library was needed as a substitute for \"Numerical Recipes\".\n\nAnother line of criticism centers on the coding style of the books, which strike some modern readers as \"Fortran-ish\", though written in contemporary, object-oriented C++. The authors have defended their very terse coding style as necessary to the format of the book because of space limitations and for readability.\n\nThe books differ by edition (1st, 2nd, and 3rd) and by the computer language in which the code is given.\n\n\nThe books are published by Cambridge University Press.\n\n"}
{"id": "41031824", "url": "https://en.wikipedia.org/wiki?curid=41031824", "title": "Numerical methods in fluid mechanics", "text": "Numerical methods in fluid mechanics\n\nFluid motion is governed by the Navier–Stokes equations, a set of coupled and nonlinear\npartial differential equations derived from the basic laws of conservation of mass, momentum\nand energy. The unknowns are usually the flow velocity, the pressure and density and temperature. The analytical solution of this equation is impossible hence scientists resort to laboratory experiments in such situations. The answers delivered are, however, usually qualitatively different since dynamical and geometric similitude are difficult to enforce simultaneously between the lab experiment and the prototype. Furthermore, the design and construction of these experiments can be difficult (and costly), particularly for stratified rotating flows. Computational fluid dynamics (CFD) is an additional tool in the arsenal of scientists. In its early days CFD was often controversial, as it involved additional approximation to the governing equations and raised additional (legitimate) issues. Nowadays CFD is an established discipline alongside theoretical and experimental methods. This position is in large part due to the exponential growth of computer power which has allowed us to tackle ever larger and more complex problems.\n\nThe central process in CFD is the process of discretization, i.e. the process of taking differential equations with an infinite number of degrees of freedom, and reducing it to a system of finite degrees of freedom. Hence, instead of determining the solution everywhere and for all times, we will be satisfied with its calculation at a finite number of locations and at specified time intervals. The partial differential equations are then reduced to a system of algebraic equations that can be solved on a computer. Errors creep in during the discretization process. The nature and characteristics of the errors must be controlled in order to ensure that:\nOnce these two criteria are established, the power of computing machines can be leveraged to solve the problem in a numerically reliable fashion. Various discretization schemes have been developed to cope with a variety of issues. The most notable for our purposes are: finite difference methods, finite volume methods, finite element methods, and spectral methods.\n\nFinite difference replace the infinitesimal limiting process of derivative calculation:\n\nwith a finite limiting process,i.e.\n\nThe term formula_3 gives an indication of the magnitude of the error as a function of the mesh spacing. In this instance, the error is halfed if the grid spacing, _x is halved, and we say that this is a first order method. Most FDM used in practice are at least second order accurate except in very special circumstances. Finite Difference method is still the most popular numerical method for solution of PDEs because of their simplicity, efficiency and low computational cost. Their major drawback is in their geometric inflexibility which complicates their applications to general complex domains. These can be alleviated by the use of either mapping techniques and/or masking to fit the computational mesh to the computational domain.\n\nThe finite element method was designed to deal with problem with complicated computational regions. The PDE is first recast into a variational form which essentially forces the mean error to be small everywhere. The discretization step proceeds by dividing the computational domain into elements of triangular or rectangular shape. The solution within each element is interpolated with a polynomial of usually low order. Again, the unknowns are the solution at the collocation points. The CFD community adopted the FEM in the 1980s when reliable methods for dealing with advection dominated problems were devised.\n\nBoth finite element and finite difference methods are low order methods, usually of 2nd − 4th order, and have local approximation property. By local we mean that a particular collocation point is affected by a limited number of points around it. In contrast, spectral method have global approximation property. The interpolation functions, either polynomials or trigonomic functions are global in nature. Their main benefits is in the rate of convergence which depends on the smoothness of the solution (i.e. how many continuous derivatives does it admit). For infinitely smooth solution, the error decreases exponentially, i.e. faster than algebraic. Spectral methods are mostly used in the computations of homogeneous turbulence, and require relatively simple geometries. Atmospheric model have also adopted spectral methods because of their convergence properties and the regular spherical shape of their computational domain.\n\nFinite volume methods are primarily used in aerodynamics applications where strong shocks and discontinuities in the solution occur. Finite volume method solves an integral form of the governing equations so that local continuity property do not have to hold.\n\nThe CPU time to solve the system of equations differs substantially from method to method. Finite differences are usually the cheapest on a per grid point basis followed by the finite element method and spectral method. However, a per grid point basis comparison is a little like comparing apple and oranges. Spectral methods deliver more accuracy on a per grid point basis than either FEM or FDM. The comparison is more meaningful if the question is recast as ”what is the computational cost to achieve a given error tolerance?”. The problem becomes one of defining the error measure which is a complicated task in general situations.\n\nEquation is an explicit approximation to the original differential equation since no information about the unknown function at the future time (\"n\" + 1) has been used on the right hand side of the equation. In order to derive the error committed in the approximation we rely again on Taylor series.\n\nThis is an example of an implicit method since the unknown \"u\"(\"n\" + 1) has been used in evaluating the slope of the solution on the right hand side; this is not a problem to solve for \"u\"(\"n\" + 1) in this scalar and linear case. For more complicated situations like a nonlinear right hand side or a system of equations, a nonlinear system of equations may have to be inverted.\n\n"}
{"id": "37864", "url": "https://en.wikipedia.org/wiki?curid=37864", "title": "Nyquist–Shannon sampling theorem", "text": "Nyquist–Shannon sampling theorem\n\nIn the field of digital signal processing, the sampling theorem is a fundamental bridge between continuous-time signals (often called \"analog signals\") and discrete-time signals (often called \"digital signals\"). It establishes a sufficient condition for a sample rate that permits a discrete sequence of \"samples\" to capture all the information from a continuous-time signal of finite bandwidth.\n\nStrictly speaking, the theorem only applies to a class of mathematical functions having a Fourier transform that is zero outside of a finite region of frequencies. Intuitively we expect that when one reduces a continuous function to a discrete sequence and interpolates back to a continuous function, the fidelity of the result depends on the density (or sample rate) of the original samples. The sampling theorem introduces the concept of a sample rate that is sufficient for perfect fidelity for the class of functions that are bandlimited to a given bandwidth, such that no actual information is lost in the sampling process. It expresses the sufficient sample rate in terms of the bandwidth for the class of functions. The theorem also leads to a formula for perfectly reconstructing the original continuous-time function from the samples.\n\nPerfect reconstruction may still be possible when the sample-rate criterion is not satisfied, provided other constraints on the signal are known. (See below and compressed sensing.) In some cases (when the sample-rate criterion is not satisfied), utilizing additional constraints allows for approximate reconstructions. The fidelity of these reconstructions can be verified and quantified utilizing Bochner's theorem.\n\nThe name \"Nyquist–Shannon sampling theorem\" honors Harry Nyquist and Claude Shannon. The theorem was also discovered independently by E. T. Whittaker, by Vladimir Kotelnikov, and by others. It is thus also known by the names \"Nyquist–Shannon–Kotelnikov\", \"Whittaker–Shannon–Kotelnikov\", \"Whittaker–Nyquist–Kotelnikov–Shannon\", and \"cardinal theorem of interpolation\".\n\nSampling is a process of converting a signal (for example, a function of continuous time and/or space) into a numeric sequence (a function of discrete time and/or space). Shannon's version of the theorem states:\n\nIf a function formula_1 contains no frequencies higher than \"B\" hertz, it is completely determined by giving its ordinates at a series of points spaced formula_2 seconds apart.\n\nA sufficient sample-rate is therefore anything larger than formula_3 samples per second. Equivalently, for a given sample rate formula_4, perfect reconstruction is guaranteed possible for a bandlimit formula_5.\n\nWhen the bandlimit is too high (or there is no bandlimit), the reconstruction exhibits imperfections known as aliasing. Modern statements of the theorem are sometimes careful to explicitly state that formula_1 must contain no sinusoidal component at exactly frequency \"B\", or that \"B\" must be strictly less than ½ the sample rate. The threshold formula_3 is called the Nyquist rate and is an attribute of the continuous-time input formula_1 to be sampled. The sample rate must exceed the Nyquist rate for the samples to suffice to represent \"x\"(\"t\"). The threshold \"f\"/2 is called the Nyquist frequency and is an attribute of the sampling equipment. All meaningful frequency components of the properly sampled \"x\"(\"t\") exist below the Nyquist frequency. The condition described by these inequalities is called the Nyquist criterion, or sometimes the \"Raabe condition\". The theorem is also applicable to functions of other domains, such as \"space,\" in the case of a digitized image. The only change, in the case of other domains, is the units of measure applied to \"t\", \"f\", and \"B\".\n\nThe is customarily used to represent the interval between samples and is called the sample period or sampling interval. And the samples of function \"x\"(\"t\") are commonly denoted by (alternatively \"x\" in older signal processing literature), for all integer values of \"n\". A mathematically ideal way to interpolate the sequence involves the use of sinc functions. Each sample in the sequence is replaced by a sinc function, centered on the time axis at the original location of the sample, \"nT\", with the amplitude of the sinc function scaled to the sample value, \"x\"[\"n\"]. Subsequently, the sinc functions are summed into a continuous function. A mathematically equivalent method is to convolve one sinc function with a series of Dirac delta pulses, weighted by the sample values. Neither method is numerically practical. Instead, some type of approximation of the sinc functions, finite in length, is used. The imperfections attributable to the approximation are known as \"interpolation error\".\n\nPractical digital-to-analog converters produce neither scaled and delayed sinc functions, nor ideal Dirac pulses. Instead they produce a piecewise-constant sequence of scaled and delayed rectangular pulses (the zero-order hold), usually followed by an \"anti-aliasing filter\" to clean up spurious high-frequency content.\n\nWhen formula_1 is a function with a Fourier transform formula_10:\n\nthe Poisson summation formula indicates that the samples, formula_12, of formula_1 are sufficient to create a periodic summation of formula_10. The result is:\n\nwhich is a periodic function and its equivalent representation as a Fourier series, whose coefficients are \"T\"•\"x\"(\"nT\"). This function is also known as the discrete-time Fourier transform (DTFT) of the sequence \"T\"•\"x\"(\"nT\"), for integers n.\n\nAs depicted, copies of \"X\"(\"f\") are shifted by multiples of \"f\" and combined by addition. For a band-limited function  (\"X\"(\"f\") = \"0\" for all |\"f\"| ≥ \"B\"),  and sufficiently large \"f\", it is possible for the copies to remain distinct from each other. But if the Nyquist criterion is not satisfied, adjacent copies overlap, and it is not possible in general to discern an unambiguous \"X\"(\"f\"). Any frequency component above \"f\"/2 is indistinguishable from a lower-frequency component, called an \"alias\", associated with one of the copies. In such cases, the customary interpolation techniques produce the alias, rather than the original component. When the sample-rate is pre-determined by other considerations (such as an industry standard), \"x\"(\"t\") is usually filtered to reduce its high frequencies to acceptable levels before it is sampled. The type of filter required is a lowpass filter, and in this application it is called an anti-aliasing filter.\n\nWhen there is no overlap of the copies (also known as \"images\") of formula_10, the formula_2 term of formula_17 can be recovered by the product:\n\nAt this point, the sampling theorem is proved, since formula_10 uniquely determines formula_1.\n\nAll that remains is to derive the formula for reconstruction. formula_22 need not be precisely defined in the region formula_23 because \"X\"(\"f\") is zero in that region. However, the worst case is when \"B\" = \"f\"/2, the Nyquist frequency. A function that is sufficient for that and all less severe cases is:\n\nwhere rect(•) is the rectangular function.  Therefore:\n\nThe inverse transform of both sides produces the Whittaker–Shannon interpolation formula:\n\nwhich shows how the samples, \"x\"(\"nT\"), can be combined to reconstruct \"x\"(\"t\").\n\n\nPoisson shows that the Fourier series in produces the periodic summation of formula_10, regardless of formula_4 and formula_32. Shannon, however, only derives the series coefficients for the case formula_33. Virtually quoting Shannon's original paper:\n\nShannon's proof of the theorem is complete at that point, but he goes on to discuss reconstruction via sinc functions, what we now call the Whittaker–Shannon interpolation formula as discussed above. He does not derive or prove the properties of the sinc function, but these would have been familiar to engineers reading his works at the time, since the Fourier pair relationship between rect (the rectangular function) and sinc was well known.\n\nAs in the other proof, the existence of the Fourier transform of the original signal is assumed, so the proof does not say whether the sampling theorem extends to bandlimited stationary random processes.\n\nThe sampling theorem is usually formulated for functions of a single variable. Consequently, the theorem is directly applicable to time-dependent signals and is normally formulated in that context. However, the sampling theorem can be extended in a straightforward way to functions of arbitrarily many variables. Grayscale images, for example, are often represented as two-dimensional arrays (or matrices) of real numbers representing the relative intensities of pixels (picture elements) located at the intersections of row and column sample locations. As a result, images require two independent variables, or indices, to specify each pixel uniquely—one for the row, and one for the column.\n\nColor images typically consist of a composite of three separate grayscale images, one to represent each of the three primary colors—red, green, and blue, or \"RGB\" for short. Other colorspaces using 3-vectors for colors include HSV, CIELAB, XYZ, etc. Some colorspaces such as cyan, magenta, yellow, and black (CMYK) may represent color by four dimensions. All of these are treated as vector-valued functions over a two-dimensional sampled domain.\n\nSimilar to one-dimensional discrete-time signals, images can also suffer from aliasing if the sampling resolution, or pixel density, is inadequate. For example, a digital photograph of a striped shirt with high frequencies (in other words, the distance between the stripes is small), can cause aliasing of the shirt when it is sampled by the camera's image sensor. The aliasing appears as a moiré pattern. The \"solution\" to higher sampling in the spatial domain for this case would be to move closer to the shirt, use a higher resolution sensor, or to optically blur the image before acquiring it with the sensor.\n\nAnother example is shown to the right in the brick patterns. The top image shows the effects when the sampling theorem's condition is not satisfied. When software rescales an image (the same process that creates the thumbnail shown in the lower image) it, in effect, runs the image through a low-pass filter first and then downsamples the image to result in a smaller image that does not exhibit the moiré pattern. The top image is what happens when the image is downsampled without low-pass filtering: aliasing results.\n\nThe sampling theorem applies to camera systems, where the scene and lens constitute an analog spatial signal source, and the image sensor is a spatial sampling device. Each of these components is characterized by a modulation transfer function (MTF), representing the precise resolution (spatial bandwidth) available in that component. Effects of aliasing or blurring can occur when the lens MTF and sensor MTF are mismatched. When the optical image which is sampled by the sensor device contains higher spatial frequencies than the sensor, the under sampling acts as a low-pass filter to reduce or eliminate aliasing. When the area of the sampling spot (the size of the pixel sensor) is not large enough to provide sufficient spatial anti-aliasing, a separate anti-aliasing filter (optical low-pass filter) may be included in a camera system to reduce the MTF of the optical image. Instead of requiring an optical filter, the graphics processing unit of smartphone cameras performs digital signal processing to remove aliasing with a digital filter. Digital filters also apply sharpening to amplify the contrast from the lens at high spatial frequencies, which otherwise falls off rapidly at diffraction limits.\n\nThe sampling theorem also applies to post-processing digital images, such as to up or down sampling. Effects of aliasing, blurring, and sharpening may be adjusted with digital filtering implemented in software, which necessarily follows the theoretical principles.\n\nTo illustrate the necessity of formula_55, consider the family of sinusoids generated by different values of formula_56 in this formula:\n\nWith formula_33 or equivalently formula_59, the samples are given by:\n\nregardless of the value of formula_56. That sort of ambiguity is the reason for the \"strict\" inequality of the sampling theorem's condition.\n\nAs discussed by Shannon:\n\nThat is, a sufficient no-loss condition for sampling signals that do not have baseband components exists that involves the \"width\" of the non-zero frequency interval as opposed to its highest frequency component. See \"Sampling (signal processing)\" for more details and examples.\n\nFor example, in order to sample the FM radio signals in the frequency range of 100–102 MHz, it is not necessary to sample at 204 MHz (twice the upper frequency), but rather it is sufficient to sample at 4 MHz (twice the width of the frequency interval).\n\nA bandpass condition is that \"X\"(\"f\") = 0, for all nonnegative \"f\" outside the open band of frequencies:\n\nfor some nonnegative integer \"N\". This formulation includes the normal baseband condition as the case \"N\"=0.\n\nThe corresponding interpolation function is the impulse response of an ideal brick-wall bandpass filter (as opposed to the ideal brick-wall lowpass filter used above) with cutoffs at the upper and lower edges of the specified band, which is the difference between a pair of lowpass impulse responses:\n\nOther generalizations, for example to signals occupying multiple non-contiguous bands, are possible as well. Even the most generalized form of the sampling theorem does not have a provably true converse. That is, one cannot conclude that information is necessarily lost just because the conditions of the sampling theorem are not satisfied; from an engineering perspective, however, it is generally safe to assume that if the sampling theorem is not satisfied then information will most likely be lost.\n\nThe sampling theory of Shannon can be generalized for the case of nonuniform sampling, that is, samples not taken equally spaced in time. The Shannon sampling theory for non-uniform sampling states that a band-limited signal can be perfectly reconstructed from its samples if the average sampling rate satisfies the Nyquist condition. Therefore, although uniformly spaced samples may result in easier reconstruction algorithms, it is not a necessary condition for perfect reconstruction.\n\nThe general theory for non-baseband and nonuniform samples was developed in 1967 by Henry Landau. He proved that the average sampling rate (uniform or otherwise) must be twice the \"occupied\" bandwidth of the signal, assuming it is \"a priori\" known what portion of the spectrum was occupied.\nIn the late 1990s, this work was partially extended to cover signals of when the amount of occupied bandwidth was known, but the actual occupied portion of the spectrum was unknown. In the 2000s, a complete theory was developed\n(see the section Beyond Nyquist below) using compressed sensing. In particular, the theory, using signal processing language, is described in this 2009 paper. They show, among other things, that if the frequency locations are unknown, then it is necessary to sample at least at twice the Nyquist criteria; in other words, you must pay at least a factor of 2 for not knowing the location of the spectrum. Note that minimum sampling requirements do not necessarily guarantee stability.\n\nThe Nyquist–Shannon sampling theorem provides a sufficient condition for the sampling and reconstruction of a band-limited signal. When reconstruction is done via the Whittaker–Shannon interpolation formula, the Nyquist criterion is also a necessary condition to avoid aliasing, in the sense that if samples are taken at a slower rate than twice the band limit, then there are some signals that will not be correctly reconstructed. However, if further restrictions are imposed on the signal, then the Nyquist criterion may no longer be a necessary condition.\n\nA non-trivial example of exploiting extra assumptions about the signal is given by the recent field of compressed sensing, which allows for full reconstruction with a sub-Nyquist sampling rate. Specifically, this applies to signals that are sparse (or compressible) in some domain. As an example, compressed sensing deals with signals that may have a low over-all bandwidth (say, the \"effective\" bandwidth \"EB\"), but the frequency locations are unknown, rather than all together in a single band, so that the passband technique does not apply. In other words, the frequency spectrum is sparse. Traditionally, the necessary sampling rate is thus 2\"B\". Using compressed sensing techniques, the signal could be perfectly reconstructed if it is sampled at a rate slightly lower than 2\"EB\". With this approach, reconstruction is no longer given by a formula, but instead by the solution to a linear optimization program.\n\nAnother example where sub-Nyquist sampling is optimal arises under the additional constraint that the samples are quantized in an optimal manner, as in a combined system of sampling and optimal lossy compression. This setting is relevant in cases where the joint effect of sampling and quantization is to be considered, and can provide a lower bound for the minimal reconstruction error that can be attained in sampling and quantizing a random signal. For stationary Gaussian random signals, this lower bound is usually attained at a sub-Nyquist sampling rate, indicating that sub-Nyquist sampling is optimal for this signal model under optimal quantization.\n\nThe sampling theorem was implied by the work of Harry Nyquist in 1928, in which he showed that up to 2\"B\" independent pulse samples could be sent through a system of bandwidth \"B\"; but he did not explicitly consider the problem of sampling and reconstruction of continuous signals. About the same time, Karl Küpfmüller showed a similar result and discussed the sinc-function impulse response of a band-limiting filter, via its integral, the step-response sine integral; this bandlimiting and reconstruction filter that is so central to the sampling theorem is sometimes referred to as a \"Küpfmüller filter\" (but seldom so in English).\n\nThe sampling theorem, essentially a dual of Nyquist's result, was proved by Claude E. Shannon. V. A. Kotelnikov published similar results in 1933, as did the mathematician\nE. T. Whittaker in 1915, J. M. Whittaker in 1935, and Gabor in 1946 (\"Theory of communication\"). In 1999, the Eduard Rhein Foundation awarded Kotelnikov their Basic Research Award \"for the first theoretically exact formulation of the sampling theorem\".\n\nIn 1948 and 1949, Claude E. Shannon published the two revolutionary articles in which he founded the information theory. In Shannon 1948 the sampling theorem is formulated as “Theorem 13”: Let \"f\"(\"t\") contain no frequencies over W. Then\nIt was not until these articles were published that the theorem known as “Shannon’s sampling theorem” became common property among communication engineers, although Shannon himself writes that this is a fact which is common knowledge in the communication art. A few lines further on, however, he adds: \"but in spite of its evident importance, [it] seems not to have appeared explicitly in the literature of communication theory\".\n\nOthers who have independently discovered or played roles in the development of the sampling theorem have been discussed in several historical articles, for example, by Jerri and by Lüke. For example, Lüke points out that H. Raabe, an assistant to Küpfmüller, proved the theorem in his 1939 Ph.D. dissertation; the term \"Raabe condition\" came to be associated with the criterion for unambiguous representation (sampling rate greater than twice the bandwidth). Meijering mentions several other discoverers and names in a paragraph and pair of footnotes:\n\nAs pointed out by Higgins [135], the sampling theorem should really be considered in two parts, as done above: the first stating the fact that a bandlimited function is completely determined by its samples, the second describing how to reconstruct the function using its samples. Both parts of the sampling theorem were given in a somewhat different form by J. M. Whittaker [350, 351, 353] and before him also by Ogura [241, 242]. They were probably not aware of the fact that the first part of the theorem had been stated as early as 1897 by Borel [25]. As we have seen, Borel also used around that time what became known as the cardinal series. However, he appears not to have made the link [135]. In later years it became known that the sampling theorem had been presented before Shannon to the Russian communication community by Kotel'nikov [173]. In more implicit, verbal form, it had also been described in the German literature by Raabe [257]. Several authors [33, 205] have mentioned that Someya [296] introduced the theorem in the Japanese literature parallel to Shannon. In the English literature, Weston [347] introduced it independently of Shannon around the same time.\n\nExactly how, when, or why Harry Nyquist had his name attached to the sampling theorem remains obscure. The term \"Nyquist Sampling Theorem\" (capitalized thus) appeared as early as 1959 in a book from his former employer, Bell Labs, and appeared again in 1963, and not capitalized in 1965. It had been called the \"Shannon Sampling Theorem\" as early as 1954, but also just \"the sampling theorem\" by several other books in the early 1950s.\n\nIn 1958, Blackman and Tukey cited Nyquist's 1928 article as a reference for \"the sampling theorem of information theory\", even though that article does not treat sampling and reconstruction of continuous signals as others did. Their glossary of terms includes these entries:\n\nExactly what \"Nyquist's result\" they are referring to remains mysterious.\n\nWhen Shannon stated and proved the sampling theorem in his 1949 article, according to Meijering, \"he referred to the critical sampling interval formula_67 as the \"Nyquist interval\" corresponding to the band \"W\", in recognition of Nyquist’s discovery of the fundamental importance of this interval in connection with telegraphy\". This explains Nyquist's name on the critical interval, but not on the theorem.\n\nSimilarly, Nyquist's name was attached to \"Nyquist rate\" in 1953 by Harold S. Black:\n\nAccording to the OED, this may be the origin of the term \"Nyquist rate\". In Black's usage, it is not a sampling rate, but a signaling rate.\n\n\n\n"}
{"id": "38707008", "url": "https://en.wikipedia.org/wiki?curid=38707008", "title": "Order-4 heptagonal tiling", "text": "Order-4 heptagonal tiling\n\nIn geometry, the order-4 heptagonal tiling is a regular tiling of the hyperbolic plane. It has Schläfli symbol of {7,4}.\n\nThis tiling represents a hyperbolic kaleidoscope of 7 mirrors meeting as edges of a regular heptagon. This symmetry by orbifold notation is called *2222222 with 7 order-2 mirror intersections. In Coxeter notation can be represented as [1,7,1,4], removing two of three mirrors (passing through the heptagon center) in the [7,4] symmetry.\n\nThe kaleidoscopic domains can be seen as bicolored heptagons, representing mirror images of the fundamental domain. This coloring represents the uniform tiling t{7,7} and as a quasiregular tiling is called a \"heptaheptagonal tiling\".\n\nThis tiling is topologically related as a part of sequence of regular tilings with heptagonal faces, starting with the heptagonal tiling, with Schläfli symbol {6,n}, and Coxeter diagram , progressing to infinity.\nThis tiling is also topologically related as a part of sequence of regular polyhedra and tilings with four faces per vertex, starting with the octahedron, with Schläfli symbol {n,4}, and Coxeter diagram , with n progressing to infinity.\n\n\n"}
{"id": "5719307", "url": "https://en.wikipedia.org/wiki?curid=5719307", "title": "Paley graph", "text": "Paley graph\n\nIn mathematics, Paley graphs are dense undirected graphs constructed from the members of a suitable finite field by connecting pairs of elements that differ by a quadratic residue. The Paley graphs form an infinite family of conference graphs, which yield an infinite family of symmetric conference matrices. Paley graphs allow graph-theoretic tools to be applied to the number theory of quadratic residues, and have interesting properties that make them useful in graph theory more generally.\n\nPaley graphs are named after Raymond Paley. They are closely related to the Paley construction for constructing Hadamard matrices from quadratic residues .\nThey were introduced as graphs independently by and . Sachs was interested in them for their self-complementarity properties, while Erdős and Rényi studied their symmetries.\n\nPaley digraphs are directed analogs of Paley graphs that yield antisymmetric conference matrices. They were introduced by (independently of Sachs, Erdős, and Rényi) as a way of constructing tournaments with a property previously known to be held only by random tournaments: in a Paley digraph, every small subset of vertices is dominated by some other vertex.\n\nLet \"q\" be a prime power such that \"q\" = 1 (mod 4). That is, \"q\" should either be an arbitrary power of a Pythagorean prime (a prime congruent to 1 mod 4) or an even power of an odd non-Pythagorean prime. This choice of \"q\" implies that in the unique finite field F of order \"q\", the element  −1 has a square root.\n\nNow let \"V\" = F and let\n\nIf a pair {\"a\",\"b\"} is included in \"E\", it is included under either ordering of its two elements. For, \"a\" − \"b\" = −(\"b\" − \"a\"), and  −1 is a square, from which it follows that \"a\" − \"b\" is a square if and only if \"b\" − \"a\" is a square.\n\nBy definition \"G\" = (\"V\", \"E\") is the Paley graph of order \"q\".\n\nFor \"q\" = 13, the field F is just integer arithmetic modulo 13. The numbers with square roots mod 13 are:\nThus, in the Paley graph, we form a vertex for each of the integers in the range [0,12], and connect each such integer \"x\" to six neighbors: \"x\" ± 1 (mod 13), \"x\" ± 3 (mod 13), and \"x\" ± 4 (mod 13).\n\n\n\n\nLet \"q\" be a prime power such that \"q\" = 3 (mod 4). Thus, the finite field of order \"q\", F, has no square root of −1. Consequently, for each pair (\"a\",\"b\") of distinct elements of F, either \"a\" − \"b\" or \"b\" − \"a\", but not both, is a square. The Paley digraph is the directed graph with vertex set \"V\" = F and arc set \n\nThe Paley digraph is a tournament because each pair of distinct vertices is linked by an arc in one and only one direction.\n\nThe Paley digraph leads to the construction of some antisymmetric conference matrices and biplane geometries.\n\nThe six neighbors of each vertex in the Paley graph of order 13 are connected in a cycle; that is, the graph is locally cyclic. Therefore, this graph can be embedded as a Whitney triangulation of a torus, in which every face is a triangle and every triangle is a face. More generally, if any Paley graph of order \"q\" could be embedded so that all its faces are triangles, we could calculate the genus of the resulting surface via the Euler characteristic as formula_8. conjectures that the minimum genus of a surface into which a Paley graph can be embedded is near this bound in the case that \"q\" is a square, and questions whether such a bound might hold more generally. Specifically, Mohar conjectures that the Paley graphs of square order can be embedded into surfaces with genus\nwhere the o(1) term can be any function of \"q\" that goes to zero in the limit as \"q\" goes to infinity.\n\n\n"}
{"id": "1168486", "url": "https://en.wikipedia.org/wiki?curid=1168486", "title": "Plücker coordinates", "text": "Plücker coordinates\n\nIn geometry, Plücker coordinates, introduced by Julius Plücker in the 19th century, are a way to assign six homogeneous coordinates to each line in projective 3-space, P. Because they satisfy a quadratic constraint, they establish a one-to-one correspondence between the 4-dimensional space of lines in P and points on a quadric in P (projective 5-space). A predecessor and special case of Grassmann coordinates (which describe \"k\"-dimensional linear subspaces, or \"flats\", in an \"n\"-dimensional Euclidean space), Plücker coordinates arise naturally in geometric algebra. They have proved useful for computer graphics, and also can be extended to coordinates for the screws and wrenches in the theory of kinematics used for robot control.\n\nA line \"L\" in 3-dimensional Euclidean space is determined by two distinct points that it contains, or by two distinct planes that contain it. Consider the first case, with points x = (\"x\",\"x\",\"x\") and y = (\"y\",\"y\",\"y\"). The vector displacement from x to y is nonzero because the points are distinct, and represents the \"direction\" of the line. That is, every displacement between points on \"L\" is a scalar multiple of d = y − x. If a physical particle of unit mass were to move from x to y, it would have a moment about the origin. The geometric equivalent is a vector whose direction is perpendicular to the plane containing \"L\" and the origin, and whose length equals twice the area of the triangle formed by the displacement and the origin. Treating the points as displacements from the origin, the moment is m = x×y, where \"×\" denotes the vector cross product. For a fixed line, \"L\", the area of the triangle is proportional to the length of the segment between x and y, considered as the base of the triangle; it is not changed by sliding the base along the line, parallel to itself. By definition the moment vector is perpendicular to every displacement along the line, so d•m = 0, where \"•\" denotes the vector dot product.\n\nAlthough neither d nor m alone is sufficient to determine \"L\", together the pair does so uniquely, up to a common (nonzero) scalar multiple which depends on the distance between x and y. That is, the coordinates\n\nmay be considered homogeneous coordinates for \"L\", in the sense that all pairs (\"λd:\"λm), for \"λ\" ≠ 0, can be produced by points on \"L\" and only \"L\", and any such pair determines a unique line so long as d is not zero and d•m = 0. Furthermore, this approach extends to include points, lines, and a plane \"at infinity\", in the sense of projective geometry.\n\nAlternatively, let the equations for points x of two distinct planes containing \"L\" be\n\nThen their respective planes are perpendicular to vectors a and b, and the direction of \"L\" must be perpendicular to both. Hence we may set d = a×b, which is nonzero because a and b are neither zero nor parallel (the planes being distinct and intersecting). If point x satisfies both plane equations, then it also satisfies the linear combination\n\nThat is, m = \"a\" b − \"b\" a is a vector perpendicular to displacements to points on \"L\" from the origin; it is, in fact, a moment consistent with the d previously defined from a and b.\n\nproof：need to show m = \"a\" b − \"b\" a = r×d = r× a×b, let a•a = b•b = 1\n\npoint B is the origin. the line is pass point D and orthogonal to the plane, the 2 plane pass CD and DE both orthogonal to the plane, BD is diameter\n\nangle BCE = BDE = BGF, so points D,G,E,H on a circle, and angle GHG is right angle, FG orthogonal to BD, so 4 points C, D, H, F on a circle, and \n\nwhen ||r|| = 0, the line is the one pass origin with direction d; if ||r|| > 0, the line is with direction d, the plane including the origin and the line has normal vector m, the line is tangent to a circle on the plane centered origin and with radius ||\"r\"|| at point r.\n\nAlthough the usual algebraic definition tends to obscure the relationship, (d:m) are the Plücker coordinates of \"L\".\n\nIn a 3-dimensional projective space P, let \"L\" be a line through distinct points x and y with homogeneous coordinates (\"x\":\"x\":\"x\":\"x\") and (\"y\":\"y\":\"y\":\"y\").\nThe Plücker coordinates \"p\" are defined as follows:\nThis implies \"p\" = 0 and \"p\" = −\"p\", reducing the possibilities to only six (4 choose 2) independent quantities. The sixtuple\n\nis uniquely determined by \"L\" up to a common nonzero scale factor. Furthermore, not all six components can be zero.\nThus the Plücker coordinates of \"L\" may be considered as homogeneous coordinates of a point in a 5-dimensional projective space, as suggested by the colon notation.\n\nTo see these facts, let \"M\" be the 4×2 matrix with the point coordinates as columns.\n\nThe Plücker coordinate \"p\" is the determinant of rows \"i\" and \"j\" of \"M\".\nBecause x and y are distinct points, the columns of \"M\" are linearly independent; \"M\" has rank 2. Let \"M′\" be a second matrix, with columns x′ and y′ a different pair of distinct points on \"L\". Then the columns of \"M′\" are linear combinations of the columns of \"M\"; so for some 2×2 nonsingular matrix Λ,\n\nIn particular, rows \"i\" and \"j\" of \"M′\" and \"M\" are related by\n\nTherefore, the determinant of the left side 2×2 matrix equals the product of the determinants of the right side 2×2 matrices, the latter of which is a fixed scalar, det Λ. Furthermore, all six 2×2 subdeterminants in \"M\" cannot be zero because the rank of \"M\" is 2.\n\nDenote the set of all lines (linear images of P) in P by \"G\". We thus have a map:\nwhere\n\nAlternatively, a line can be described as the intersection of two planes. Let \"L\" \nbe a line contained in distinct planes a and b with homogeneous coefficients (\"a\":\"a\":\"a\":\"a\") and (\"b\":\"b\":\"b\":\"b\"), respectively. (The first plane equation is ∑ \"a\"\"x\"=0, for example.) The dual Plücker coordinate \"p\" is\n\nDual coordinates are convenient in some computations, and they are equivalent to primary coordinates:\nHere, equality between the two vectors in homogeneous coordinates means that the numbers on the right side are equal to the numbers on the left side up to some common scaling factor formula_8. Specifically, let (\"i\",\"j\",\"k\",\"ℓ\") be an even permutation of (0,1,2,3); then\n\nTo relate back to the geometric intuition, take \"x\" = 0 as the plane at infinity; thus the coordinates of points \"not\" at infinity can be normalized so that \"x\" = 1. Then \"M\" becomes\n\nand setting x = (\"x\",\"x\",\"x\") and y = (\"y\",\"y\",\"y\"), we have d = (\"p\",\"p\",\"p\") and m = (\"p\",\"p\",\"p\").\n\nDually, we have d = (\"p\",\"p\",\"p\") and m = (\"p\",\"p\",\"p\").\n\nIf the point z = (\"z\":\"z\":\"z\":\"z\") lies on \"L\", then the columns of\n\nare linearly dependent, so that the rank of this larger matrix is still 2. This implies that all 3×3 submatrices have determinant zero, generating four (4 choose 3) plane equations, such as\n\nThe four possible planes obtained are as follows.\n\nUsing dual coordinates, and letting (\"a\":\"a\":\"a\":\"a\") be the line coefficients, each of these is simply \"a\" = \"p\", or\n\nEach Plücker coordinate appears in two of the four equations, each time multiplying a different variable; and as at least one of the coordinates is nonzero, we are guaranteed non-vacuous equations for two distinct planes intersecting in \"L\". Thus the Plücker coordinates of a line determine that line uniquely, and the map α is an injection.\n\nThe image of α is not the complete set of points in P; the Plücker coordinates of a line \"L\" satisfy the quadratic Plücker relation\n\nFor proof, write this homogeneous polynomial as determinants and use Laplace expansion (in reverse).\n\nSince both 3×3 determinants have duplicate columns, the right hand side is identically zero.\n\nAnother proof may be done like this:\nSince vector\n\nis perpendicular to vector\n\n(see above), the scalar product of \"d\" and \"m\" must be zero! q.e.d.\n\nLetting (\"x\":\"x\":\"x\":\"x\") be the point coordinates, four possible points on a line each have coordinates \"x\" = \"p\", for \"j\" = 0…3. Some of these possible points may be inadmissible because all coordinates are zero, but since at least one Plücker coordinate is nonzero, at least two distinct points are guaranteed.\n\nIf (\"q\":\"q\":\"q\":\"q\":\"q\":\"q\") are the homogeneous coordinates of a point in P, without loss of generality assume that \"q\" is nonzero. Then the matrix\n\nhas rank 2, and so its columns are distinct points defining a line \"L\". When the P coordinates, \"q\", satisfy the quadratic Plücker relation, they are the Plücker coordinates of \"L\". To see this, first normalize \"q\" to 1. Then we immediately have that for the Plücker coordinates computed from \"M\", \"p\" = \"q\", except for\n\nBut if the \"q\" satisfy the Plücker relation \"q\"+\"q\"\"q\"+\"q\"\"q\" = 0, then \"p\" = \"q\", completing the set of identities.\n\nConsequently, α is a surjection onto the algebraic variety consisting of the set of zeros of the quadratic polynomial\n\nAnd since α is also an injection, the lines in P are thus in bijective correspondence with the points of this quadric in P, called the Plücker quadric or Klein quadric.\n\nPlücker coordinates allow concise solutions to problems of line geometry in 3-dimensional space, especially those involving incidence.\n\nTwo lines in P are either skew or coplanar, and in the latter case they are either coincident or intersect in a unique point. If \"p\" and \"p\"′ are the Plücker coordinates of two lines, then they are coplanar precisely when d⋅m′+m⋅d′ = 0, as shown by\n\nWhen the lines are skew, the sign of the result indicates the sense of crossing: positive if a right-handed screw takes \"L\" into \"L\"′, else negative.\n\nThe quadratic Plücker relation essentially states that a line is coplanar with itself.\n\nIn the event that two lines are coplanar but not parallel, their common plane has equation\n\nwhere x = (\"x\",\"x\",\"x\").\n\nThe slightest perturbation will destroy the existence of a common plane, and near-parallelism of the lines will cause numeric difficulties in finding such a plane even if it does exist.\n\nDually, two coplanar lines, neither of which contains the origin, have common point\n\nTo handle lines not meeting this restriction, see the references.\n\nGiven a plane with equation\n\nor more concisely 0 = \"a\"\"x\"+a•x; and given a line not in it with Plücker coordinates (d:m), then their point of intersection is\n\nThe point coordinates, (\"x\":\"x\":\"x\":\"x\"), can also be expressed in terms of Plücker coordinates as\n\nDually, given a point (\"y\":y) and a line not containing it, their common plane has equation\n\nThe plane coordinates, (\"a\":\"a\":\"a\":\"a\"), can also be expressed in terms of dual Plücker coordinates as\n\nBecause the Klein quadric is in P, it contains linear subspaces of dimensions one and two (but no higher). These correspond to one- and two-parameter families of lines in P.\n\nFor example, suppose \"L\" and \"L\"′ are distinct lines in P determined by points x, y and x′, y′, respectively. Linear combinations of their determining points give linear combinations of their Plücker coordinates, generating a one-parameter family of lines containing \"L\" and \"L\"′. This corresponds to a one-dimensional linear subspace belonging to the Klein quadric.\n\nIf three distinct and non-parallel lines are coplanar; their linear combinations generate a two-parameter family of lines, all the lines in the plane. This corresponds to a two-dimensional linear subspace belonging to the Klein quadric.\n\nIf three distinct and non-coplanar lines intersect in a point, their linear combinations generate a two-parameter family of lines, all the lines through the point. This also corresponds to a two-dimensional linear subspace belonging to the Klein quadric.\n\nA ruled surface is a family of lines that is not necessarily linear. It corresponds to a curve on the Klein quadric. For example, a hyperboloid of one sheet is a quadric surface in P ruled by two different families of lines, one line of each passing through each point of the surface; each family corresponds under the Plücker map to a conic section within the Klein quadric in P.\n\nDuring the nineteenth century, \"line geometry\" was studied intensively. In terms of the bijection given above, this is a description of the intrinsic geometry of the Klein quadric.\n\nLine geometry is extensively used in ray tracing application where the geometry and intersections of rays need to be calculated in 3D. An implementation is described in\nIntroduction to Plücker Coordinates written for the Ray Tracing forum by Thouis Jones.\n\n\n"}
{"id": "373065", "url": "https://en.wikipedia.org/wiki?curid=373065", "title": "Polynomial ring", "text": "Polynomial ring\n\nIn mathematics, especially in the field of algebra, a polynomial ring or polynomial algebra is a ring (which is also a commutative algebra) formed from the set of polynomials in one or more indeterminates (traditionally also called variables) with coefficients in another ring, often a field. \n\nPolynomial rings occur in many parts of mathematics, and the study of their properties was among the main motivations for the development of commutative algebra and ring theory. Polynomial rings and their ideals are fundamental in algebraic geometry. Many classes of rings, such as unique factorization domains, regular rings, group rings, rings of formal power series, Ore polynomials, graded rings, are generalizations of polynomial rings.\n\nA closely related notion is that of the ring of polynomial functions on a vector space, and, more generally, ring of regular functions on an algebraic variety.\n\nThe polynomial ring, \"K\"[\"X\"], in \"X\" over a field \"K\" is defined as the set of expressions, called polynomials in \"X\", of the form\n\nwhere \"p\", \"p\", ..., \"p\", the coefficients of \"p\", are elements of \"K\", and \"X\", \"X\", are symbols, which are considered as \"powers of \"X\"\", and, by convention, follow the usual rules of exponentiation: , , and formula_2 for any nonnegative integers \"k\" and \"l\". The symbol \"X\" is called an indeterminate or variable.\n\nTwo polynomials are defined to be equal when the corresponding coefficient of each \"X\" is equal.\n\nThis terminology is suggested by real or complex \"polynomial functions\". However, in general, \"X\" and its powers, \"X\", are treated as formal symbols, not as elements of the field \"K\" or functions over it. One can think of the ring \"K\"[\"X\"] as arising from \"K\" by adding one new element \"X\" that is external to \"K\" and requiring that \"X\" commute with all elements of \"K\".\n\nThe polynomial ring in \"X\" over \"K\" is equipped with an addition, a multiplication and a scalar multiplication that make it a commutative algebra. These operations are defined according to the ordinary rules for manipulating algebraic expressions. Specifically, if \nand\nthen\nand\nwhere \"k\" = max(\"m\", \"n\"), \"l\" = \"m\" + \"n\",\nand\n\nIf necessary, the polynomials \"p\" and \"q\" are extended by adding \"dummy terms\" with zero coefficients, so that the expressions for \"r\" and \"s\" are always defined. Specifically, if , then \"p\" = 0 for .\n\nThe scalar multiplication is the special case of the multiplication where \"p\" = \"p\" is reduced to its term which is independent of \"X\", that is\n\nIt is easy to verify that these three operations satisfy the axioms of a commutative algebra. Therefore, polynomial rings are also called \"polynomial algebras\".\n\nAnother equivalent definition is often preferred, although less intuitive, because it is easier to make it completely rigorous, which consists in defining a polynomial as an infinite sequence of elements of \"K\", having the property that only a finite number of the elements are nonzero, or equivalently, a sequence for which there is some \"m\" so that for . In this case, the expression \nis considered an alternate notation for the sequence .\n\nMore generally, the field \"K\" can be replaced by any commutative ring \"R\" when taking the same construction as above, giving rise to the polynomial ring over \"R\", which is denoted \"R\"[\"X\"].\n\nThe degree of a polynomial \"p\", written deg(\"p\") is the largest \"k\" such that the coefficient of \"X\" is not zero. In this case the coefficient \"p\" is called the leading coefficient. In the special case of zero polynomial, all of whose coefficients are zero, the degree has been variously left undefined, defined to be −1, or defined to be a special symbol −∞.\n\nIf \"K\" is a field, or more generally an integral domain, then from the definition of multiplication,\nIt follows immediately that if \"K\" is an integral domain then so is \"K\"[\"X\"].\n\nThe next property of the polynomial ring is much deeper. Already Euclid noted that every positive integer can be uniquely factored into a product of primes — this statement is now called the fundamental theorem of arithmetic. The proof is based on Euclid's algorithm for finding the greatest common divisor of natural numbers. At each step of this algorithm, a pair , , of natural numbers is replaced by a new pair , where \"r\" is the remainder from the division of \"a\" by \"b\", and the new numbers are \"smaller\". Gauss remarked that the procedure of division with the remainder can also be defined for polynomials: given two polynomials \"p\" and \"q\", where , one can write\n\nwhere the quotient \"u\" and the remainder \"r\" are polynomials, the degree of \"r\" is less than the degree of \"q\", and a decomposition with these properties is unique. The quotient and the remainder are found using the polynomial long division. The degree of the polynomial now plays a role similar to the absolute value of an integer: it is strictly less in the remainder \"r\" than it is in \"q\", and when repeating this step such decrease cannot go on indefinitely. Therefore, eventually some division will be exact, at which point the last non-zero remainder is the greatest common divisor of the initial two polynomials. Using the existence of greatest common divisors, Gauss was able to simultaneously rigorously prove the fundamental theorem of arithmetic for integers and its generalization to polynomials. In fact there exist other commutative rings than Z and \"K\"[\"X\"] that similarly admit an analogue of the Euclidean algorithm; all such rings are called Euclidean rings. Rings for which there exists unique (in an appropriate sense) factorization of nonzero elements into irreducible factors are called \"unique factorization domains\" or \"factorial rings\"; the given construction shows that all Euclidean rings, and in particular Z and \"K\"[\"X\"], are unique factorization domains.\n\nAlthough Euclidean algorithm allows proving the unique factorization property, it does not provide any algorithm for computing the factorization. For integers, there are factorization algorithms. However, even with the fastest computers, there are unable to factorize large integers with few large prime factors. This is the basis idea of the RSA cryptosystem, that is widely used for secure internet communications. For polynomials over the integers, over the rational numbers, or over a finite field there are efficient algorithms that are implemented in computer algebra systems (see Factorization of polynomials). On the other hand, there is an example of a field \"F\" such that there exist algorithms for the operations of \"F\", but there cannot exist any algorithm for deciding whether a polynomial of the form formula_13 is irreducible or is a product of polynomials of lower degree.\n\nAnother corollary of the polynomial division with the remainder is the fact that every proper ideal \"I\" of \"K\"[\"X\"] is principal, i.e. \"I\" consists of the multiples of a single polynomial \"f\". Thus the polynomial ring \"K\"[\"X\"] is a principal ideal domain, and for the same reason every Euclidean domain is a principal ideal domain. Also every principal ideal domain is a unique-factorization domain. These deductions make essential use of the fact that the polynomial coefficients lie in a field, namely in the polynomial division step, which requires the leading coefficient of \"q\", which is only known to be non-zero, to have an inverse. If \"R\" is an integral domain that is not a field then \"R\"[\"X\"] is neither a Euclidean domain nor a principal ideal domain; however it could still be a unique factorization domain (and will be so if and only if \"R\" itself is a unique factorization domain, for instance if it is Z or another polynomial ring).\n\nThe ring \"K\"[\"X\"] of polynomials over \"K\" is obtained from \"K\" by adjoining one element, \"X\". It turns out that any commutative ring \"L\" containing \"K\" and generated as a ring by a single element in addition to \"K\" can be described using \"K\"[\"X\"]. In particular, this applies to finite field extensions of \"K\".\n\nLet us consider an element in a commutative ring \"L\" that contains \"K\". There is a unique ring homomorphism from \"K\"[\"X\"] into \"L\" that maps \"X\" to and does not affect the elements of \"K\" itself (it is the identity map on \"K\"). This homomorphism is unique since it must maps each power of \"X\" to the same power of and any linear combination of powers of \"X\" with coefficients in \"K\" to the same linear combination of powers of \"X\". It consists thus of \"replacing \"X\" with \" in every polynomial. \n\nIf \"L\" is generated as a ring by adding to \"K\", any element of \"L\" appears as the right hand side of the last expression for suitable \"m\" and elements \"a\", ..., \"a\" of \"K\". Therefore, is surjective and \"L\" is a homomorphic image of \"K\"[\"X\"]. More formally, let be the kernel of . It is an ideal of \"K\"[\"X\"] and by the first isomorphism theorem for rings, \"L\" is isomorphic to the quotient of the polynomial ring \"K\"[\"X\"] by the ideal . Since the polynomial ring is a principal ideal domain, this ideal is principal: there exists a polynomial such that\n\nwhere formula_16 denotes the ideal generated by formula_17\n\nA particularly important application is to the case when the larger ring \"L\" is a field. Then the polynomial \"p\" must be irreducible. Conversely, the primitive element theorem states that any finite separable field extension \"L\"/\"K\" can be generated by a single element and the preceding theory then gives a concrete description of the field \"L\" as the quotient of the polynomial ring \"K\"[\"X\"] by a principal ideal generated by an irreducible polynomial \"p\". As an illustration, the field C of complex numbers is an extension of the field R of real numbers generated by a single element \"i\" such that \"i\" + 1 = 0. Accordingly, the polynomial \"X\" + 1 is irreducible over R and\n\nMore generally, given a (not necessarily commutative) ring \"A\" containing \"K\" and an element \"a\" of \"A\" that commutes with all elements of \"K\", there is a unique ring homomorphism from the polynomial ring \"K\"[X] to \"A\" that maps \"X\" to \"a\":\n\nThis homomorphism is given by the same formula as before, but it is not surjective in general. The existence and uniqueness of such a homomorphism expresses a certain universal property of the ring of polynomials in one variable and explains the ubiquity of polynomial rings in various questions and constructions of ring theory and commutative algebra.\n\nThe structure theorem for finitely generated modules over a principal ideal domain applies to\n\"K\"[\"X\"]. This means that every finitely generated module over \"K\"[\"X\"] may be decomposed into a direct sum of a free module and finitely many modules of the form formula_20, where \"P\" is an irreducible polynomial over \"K\" and \"k\" a positive integer.\n\nLet \"K\" be a field or, more generally, a commutative ring, and \"R\" a ring containing \"K\". For any polynomial \"P\" in \"K\"[X] and any element \"a\" in \"R\", the substitution of \"X\" by \"a\" in \"P\" defines an element of \"R\", which is denoted \"P\"(\"a\"). This element is obtained by, after the substitution, carrying on, in \"R\", the operations indicated by the expression of the polynomial. This computation is called the \"evaluation\" of \"P\" at \"a\". For example, if we have \nwe have \n(in the first example \"R\" = \"K\", and in the second one \"R\" = \"K\"[\"X\"]). Substituting \"X\" by itself results in \nexplaining why the sentences \"\"Let P be a polynomial\" and \"Let P\"(\"X\") \"be a polynomial\"\" are equivalent.\n\nFor every \"a\" in \"R\", the map formula_25 defines a ring homomorphism from \"K\"[\"X\"] into \"R\".\n\nThe \"polynomial function\" defined by a polynomial \"P\" is the function from \"K\" into \"K\" that is defined by formula_26 If \"K\" is an infinite field, two different polynomials define different polynomial functions, but this property is false for finite fields. For example, if \"K\" is a field with \"q\" elements, then the polynomials 0 and \"X\"-\"X\" both define the zero function.\n\nA polynomial in \"n\" variables \"X\", …, \"X\" with coefficients in a field \"K\" is defined analogously to a polynomial in one variable, but the notation is more cumbersome. For any multi-index \"α\" = (\"α\", …, \"α\"), where each \"α\" is a non-negative integer, let\n\nThe product \"X\" is called the monomial of multidegree \"α\". A polynomial is a finite linear combination of monomials with coefficients in \"K\"\n\nwhere\nformula_29\nand only finitely many coefficients \"p\" are different from 0. The degree of a monomial \"X\", frequently denoted |\"α\"|, is defined as\n\nand the degree of a polynomial \"p\" is the largest degree of a monomial occurring with non-zero coefficient in the expansion of \"p\".\n\nPolynomials in \"n\" variables with coefficients in \"K\" form a commutative ring denoted \n\"K\"[\"X\", ..., \"X\"], or sometimes \"K\"[\"X\"], where \"X\" is a symbol representing the full set of variables, \"X\" = (\"X\", ..., \"X\"), and called the polynomial ring in \"n\" variables. The polynomial ring in \"n\" variables can be obtained by repeated application of \"K\"[\"X\"] (the order by which is irrelevant). For example, \"K\"[\"X\", \"X\"] is isomorphic to \"K\"[\"X\"][\"X\"]. \n\nPolynomials in several variables play fundamental role in algebraic geometry. Many results in commutative and homological algebra originated in the study of ideals and modules over polynomial rings.\n\nPolynomial rings may also be referred to as free commutative algebras, since they are the free objects in the category of commutative algebras. Similarly, a polynomial ring with coefficients in the integers is the free commutative ring over its set of variables.\n\nA group of fundamental results concerning the relation between ideals of the polynomial ring \"K\"[\"X\",…, \"X\"] and algebraic subsets of \"K\" originating with David Hilbert is known under the name Nullstellensatz (literally: \"zero-locus theorem\").\n\n\n\nOne of the basic techniques in commutative algebra is to relate properties of a ring with properties of its subrings. The notation \"R\" ⊂ \"S\" indicates that a ring \"R\" is a subring of a ring \"S\". In this case \"S\" is called an \"overring\" of \"R\" and one speaks of a ring extension. This works particularly well for polynomial rings and allows one to establish many important properties of the ring of polynomials in several variables over a field, \"K\"[\"X\",…, \"X\"], by induction in \"n\".\n\nIn the following properties, \"R\" is a commutative ring and \"S\" = \"R\"[\"X\",…, \"X\"] is the ring of polynomials in \"n\" variables over \"R\". The ring extension \"R\" ⊂ \"S\" can be built from \"R\" in \"n\" steps, by successively adjoining \"X\",…, \"X\". Thus to establish each of the properties below, it is sufficient to consider the case \"n\" = 1.\n\n\nPolynomial rings can be generalized in a great many ways, including polynomial rings with generalized exponents, power series rings, noncommutative polynomial rings, and skew-polynomial rings.\n\nOne slight generalization of polynomial rings is to allow for infinitely many indeterminates. Each monomial still involves only a finite number of indeterminates (so that its degree remains finite), and each polynomial is a still a (finite) linear combination of monomials. Thus, any individual polynomial involves only finitely many indeterminates, and any finite computation involving polynomials remains inside some subring of polynomials in finitely many indeterminates.\n\nIn the case of infinitely many indeterminates, one can consider a ring strictly larger than the polynomial ring but smaller than the power series ring, by taking the subring of the latter formed by power series whose monomials have a bounded degree. Its elements still have a finite degree and are therefore somewhat like polynomials, but it is possible for instance to take the sum of all indeterminates, which is not a polynomial. A ring of this kind plays a role in constructing the ring of symmetric functions.\n\nA simple generalization only changes the set from which the exponents on the variable are drawn. The formulas for addition and multiplication make sense as long as one can add exponents: \"X\" · \"X\" = \"X\". A set for which addition makes sense (is closed and associative) is called a monoid. The set of functions from a monoid \"N\" to a ring \"R\" which are nonzero at only finitely many places can be given the structure of a ring known as \"R\"[\"N\"], the monoid ring of \"N\" with coefficients in \"R\". The addition is defined component-wise, so that if , then for every \"n\" in \"N\". The multiplication is defined as the Cauchy product, so that if , then for each \"n\" in \"N\", \"c\" is the sum of all \"a\"\"b\" where \"i\", \"j\" range over all pairs of elements of \"N\" which sum to \"n\".\n\nWhen \"N\" is commutative, it is convenient to denote the function \"a\" in \"R\"[\"N\"] as the formal sum:\nand then the formulas for addition and multiplication are the familiar:\nand\nwhere the latter sum is taken over all \"i\", \"j\" in \"N\" that sum to \"n\".\n\nSome authors such as go so far as to take this monoid definition as the starting point, and regular single variable polynomials are the special case where \"N\" is the monoid of non-negative integers. Polynomials in several variables simply take \"N\" to be the direct product of several copies of the monoid of non-negative integers. \n\nSeveral interesting examples of rings and groups are formed by taking \"N\" to be the additive monoid of non-negative rational numbers, . See also Puiseux series.\n\nPower series generalize the choice of exponent in a different direction by allowing infinitely many nonzero terms. This requires various hypotheses on the monoid \"N\" used for the exponents, to ensure that the sums in the Cauchy product are finite sums. Alternatively, a topology can be placed on the ring, and then one restricts to convergent infinite sums. For the standard choice of \"N\", the non-negative integers, there is no trouble, and the ring of formal power series is defined as the set of functions from \"N\" to a ring \"R\" with addition component-wise, and multiplication given by the Cauchy product. The ring of power series can be seen as the completion of the polynomial ring.\n\nFor polynomial rings of more than one variable, the products \"X\"·\"Y\" and \"Y\"·\"X\" are simply defined to be equal. A more general notion of polynomial ring is obtained when the distinction between these two formal products is maintained. Formally, the polynomial ring in \"n\" noncommuting variables with coefficients in the ring \"R\" is the monoid ring \"R\"[\"N\"], where the monoid \"N\" is the free monoid on \"n\" letters, also known as the set of all strings over an alphabet of \"n\" symbols, with multiplication given by concatenation. Neither the coefficients nor the variables need commute amongst themselves, but the coefficients and variables commute with each other.\n\nJust as the polynomial ring in \"n\" variables with coefficients in the commutative ring \"R\" is the free commutative \"R\"-algebra of rank \"n\", the noncommutative polynomial ring in \"n\" variables with coefficients in the commutative ring \"R\" is the free associative, unital \"R\"-algebra on \"n\" generators, which is noncommutative when \"n\" > 1.\n\nOther generalizations of polynomials are differential and skew-polynomial rings.\n\nA differential polynomial ring is a ring of differential operators formed from a ring \"R\" and a derivation \"δ\" of \"R\" into \"R\". This derivation operates on \"R\", and will be denoted \"X\", when viewed as an operator. The elements of \"R\" also operate on \"R\" by multiplication. The composition of operators is denoted as the usual multiplication. It follows that the relation \"δ\"(\"ab\") = \"aδ\"(\"b\") + \"δ\"(\"a\")\"b\" may be rewritten\nas\n\nThis relation may be extended to define a skew multiplication between two polynomials in \"X\" with coefficients in \"R\", which make them a non-commutative ring.\n\nThe standard example, called a Weyl algebra, takes \"R\" to be a (usual) polynomial ring \"k\"[\"Y\"], and \"δ\" to be the standard polynomial derivative formula_39. Taking \"a\" =\"Y\" in the above relation, one gets the canonical commutation relation, \"X\"·\"Y\" − \"Y\"·\"X\" = 1. Extending this relation by associativity and distributivity allows explicitly constructing the Weyl algebra..\n\nThe skew-polynomial ring is defined similarly for a ring \"R\" and a ring endomorphism \"f\" of \"R\", by extending the multiplication from the relation \"X\"·\"r\" = \"f\"(\"r\")·\"X\" to produce an associative multiplication that distributes over the standard addition. More generally, given a homomorphism \"F\" from the monoid N of the positive integers into the endomorphism ring of \"R\", the formula \"X\"·\"r\" = \"F\"(\"n\")(\"r\")·\"X\" allows constructing a skew-polynomial ring. Skew polynomial rings are closely related to crossed product algebras.\n\n\n"}
{"id": "1242592", "url": "https://en.wikipedia.org/wiki?curid=1242592", "title": "Projective differential geometry", "text": "Projective differential geometry\n\nIn mathematics, projective differential geometry is the study of differential geometry, from the point of view of properties of mathematical objects such as functions, diffeomorphisms, and submanifolds, that are invariant under transformations of the projective group. This is a mixture of the approaches from Riemannian geometry of studying invariances, and of the Erlangen program of characterizing geometries according to their group symmetries. \n\nThe area was much studied by mathematicians from around 1890 for a generation (by J. G. Darboux, George Henri Halphen, Ernest Julius Wilczynski, E. Bompiani, G. Fubini, Eduard Čech, amongst others), without a comprehensive theory of differential invariants emerging. Élie Cartan formulated the idea of a general projective connection, as part of his method of moving frames; abstractly speaking, this is the level of generality at which the Erlangen program can be reconciled with differential geometry, while it also develops the oldest part of the theory (for the projective line), namely the Schwarzian derivative, the simplest projective differential invariant.\n\nFurther work from the 1930s onwards was carried out by J. Kanitani, Shiing-Shen Chern, A. P. Norden, G. Bol, S. P. Finikov and G. F. Laptev. Even the basic results on osculation of curves, a manifestly projective-invariant topic, lack any comprehensive theory. The ideas of projective differential geometry recur in mathematics and its applications, but the formulations given are still rooted in the language of the early twentieth century.\n\n\n\n"}
{"id": "19358248", "url": "https://en.wikipedia.org/wiki?curid=19358248", "title": "Quaternionic vector space", "text": "Quaternionic vector space\n\nIn mathematics, a left (or right) quaternionic vector space is a left (or right) H-module where H denotes the noncommutative ring of the quaternions.\n\nThe space H of \"n\"-tuples of quaternions is both a left and right H-module using the componentwise left and right multiplication:\nfor quaternions \"q\" and \"q\", \"q\", ... \"q\".\n\nSince H is a division algebra, every finitely generated (left or right) H-module has a basis, and hence is isomorphic to H for some \"n\".\n\n"}
{"id": "1493236", "url": "https://en.wikipedia.org/wiki?curid=1493236", "title": "Random permutation", "text": "Random permutation\n\nA random permutation is a random ordering of a set of objects, that is, a permutation-valued random variable. The use of random permutations is often fundamental to fields that use randomized algorithms such as coding theory, cryptography, and simulation. A good example of a random permutation is the shuffling of a deck of cards: this is ideally a random permutation of the 52 cards.\n\nOne method of generating a random permutation of a set of length \"n\" uniformly at random (i.e., each of the \"n\"! permutations is equally likely to appear) is to generate a sequence by taking a random number between 1 and \"n\" sequentially, ensuring that there is no repetition, and interpreting this sequence (\"x\", ..., \"x\") as the permutation\n\nshown here in two-line notation.\n\nThis brute-force method will require occasional retries whenever the random number picked is a repeat of a number already selected. This can be avoided if, on the \"i\"th step (when \"x\", ..., \"x\" have already been chosen), one chooses a number \"j\" at random between 1 and \"n\" − \"i\" + 1 and sets \"x\" equal to the \"j\"th largest of the unchosen numbers.\n\nA simple algorithm to generate a permutation of \"n\" items uniformly at random without retries, known as the Knuth shuffle, is to start with any permutation (for example, the identity permutation), and then go through the positions 0 through \"n\" − 2 (we use a convention where the first element has index 0, and the last element has index \"n\" − 1), and for each position \"i\" swap the element currently there with a randomly chosen element from positions \"i\" through \"n\" − 1 (the end), inclusive. It's easy to verify that any permutation of \"n\" elements will be produced by this algorithm with probability exactly 1/\"n\"!, thus yielding a uniform distribution over all such permutations.\n\nNote that the codice_1 function can \"not\" be implemented simply as codice_2 unless a bias in the results is acceptable.\n\nThe probability distribution of the number of fixed points of a uniformly distributed random permutation approaches a Poisson distribution with expected value 1 as \"n\" grows. In particular, it is an elegant application of the inclusion–exclusion principle to show that the probability that there are no fixed points approaches 1/\"e\". The first \"n\" moments of this distribution are exactly those of the Poisson distribution.\n\nAs with all random processes, the quality of the resulting distribution of an implementation of a randomized algorithm such as the Knuth shuffle (i.e., how close it is to the desired uniform distribution) depends on the quality of the underlying source of randomness, such as a pseudorandom number generator. There are many possible randomness tests for random permutations, such as some of the Diehard tests. A typical example of such a test is to take some permutation statistic for which the distribution is known and test whether the distribution of this statistic on a set of randomly generated permutations closely approximates the true distribution.\n\n\n"}
{"id": "2029423", "url": "https://en.wikipedia.org/wiki?curid=2029423", "title": "Richard Watson (bishop of Llandaff)", "text": "Richard Watson (bishop of Llandaff)\n\nRichard Watson (1737–1816) was an Anglican bishop and academic, who served as the Bishop of Llandaff from 1782 to 1816. He wrote some notable political pamphlets. In theology, he belonged to an influential group of followers of Edmund Law that included also John Hey and William Paley.\n\nWatson was born in Heversham, Westmorland (now Cumbria), and educated at Heversham Grammar School and Trinity College, Cambridge, on a scholarship endowed by Edward Wilson of Nether Levens (1557–1653). In 1759 he graduated as Second Wrangler after having challenged Massey for the position of Senior Wrangler. This challenge, in part, prompted the University Proctor, William Farish, to introduce the practice of assigning specific marks to individual questions in University tests and, in so doing, replaced the practice of 'judgement' at Cambridge with 'marking'. Marking subsequently emerged as the predominant method to determine rank order in meritocratic systems. In 1760 he became a fellow of Trinity and in 1762 received his MA degree. He became a professor of chemistry in 1764 and was elected a fellow of the Royal Society in 1769 after publishing a paper on the solution of salts in \"Philosophical Transactions\".\n\nWatson's theological career began when he became the Cambridge Regius Professor of Divinity in 1771. In 1773, he married Dorothy Wilson, daughter of Edward Wilson of Dallam Tower and a descendant of the eponymous benefactor who had endowed Watson's scholarship. In 1774, he took up the position of prebendary of Ely Cathedral. He became archdeacon of Ely and rector of Northwold in 1779, leaving the Northwold post two years later to become rector of Knaptoft. In 1782, he left all his previous appointments to take up the post of Bishop of Llandaff, which he held until his death in 1816. In 1788, he purchased the Calgarth estate in Troutbeck Bridge, Windermere, Westmorland. The same year he was elected a Foreign Honorary Member of the American Academy of Arts and Sciences.\n\nWatson was buried at St Martin's Church in Bowness-on-Windermere.\n\nWatson contributed to the Revolution controversy, with \"A treatise upon the authenticity of the Scriptures, and the truth of the Christian religion\" (1792) and most notably in 1796 when he delivered his counterblast to Thomas Paine's \"The Age of Reason\" in \"An Apology for the Bible\" which he had \"reason to believe, was of singular service in stopping that torrent of irreligion which had been excited by [Paine's] writings\". In 1798 he published \"An Address to the People of Great Britain\", which argued for national taxes to be raised to pay for the war against France and to reduce the national debt. Gilbert Wakefield, a Unitarian minister who taught at Warrington Academy, responded with \"A Reply to Some Parts of the Bishop Llandaff's Address to the People of Great Britain\", attacking the privileged position of the wealthy.\n\nAn autobiography, \"Anecdotes of the life of Richard Watson, Bishop of Landaff\", was finished in 1814 and published posthumously in 1817.\n\nIn the 19th century, it was rumoured that Watson had been the first to propose the electric telegraph, but this is incorrect. At the time William Watson (1715–1787) made researches in electricity, but even he was not involved in the telegraph.\n\n"}
{"id": "31424241", "url": "https://en.wikipedia.org/wiki?curid=31424241", "title": "Self-propelled particles", "text": "Self-propelled particles\n\nSelf-propelled particles (SPP), also referred to as self-driven particles, are terms used by physicists to describe autonomous agents, which convert energy from the environment into directed or persistent motion. Natural systems which have inspired the study and design of these particles include walking, swimming or flying animals. Other biological systems include bacteria, cells, algae and other micro-organisms. Generally, self-propelled particles often refer to artificial systems such as robots or specifically designed particles such as swimming Janus colloids, nanomotors and walking grains. In the case of directed propulsion, which is driven by a chemical gradient, this is referred to as chemotaxis, observed in biological systems, e.g. bacteria quorum sensing and ant pheromone detection, and in synthetic systems, e.g. bimetallic nanorods and enzyme molecule chemotaxis. \n\nSelf-propelled particles interact with each other, which can lead to the emergence of collective behaviours. These collective behaviours mimic the self-organization observed with the flocking of birds, the swarming of bugs, the formation of sheep herds, etc.\n\nTo understand the ubiquity of such phenomena, physicists have developed a number of self-propelled particles models. These models predict that self-propelled particles share certain properties at the group level, regardless of the type of animals (or artificial particles) in the swarm. It has become a challenge in theoretical physics to find minimal statistical models that capture these behaviours.\n\nMost animals can be seen as SPP: they find energy in their food and exhibit various locomotion strategies, from flying to crawling. The most prominent examples of collective behaviours in these systems are fish schools, birds flocks, sheep herds, human crowds. At a smaller scale, cells and bacteria can also be treated as SPP. These biological systems can propel themselves based on the presence of chemoattractants. At even smaller scale, molecular motors transform ATP energy into directional motion. Recent work has shown that enzyme molecules will also propel themselves. Further, it has been shown that they will preferentially move towards a region of higher substrate concentration, a phenomenon that has been developed into a purification technique to isolate live enzymes. Additionally, microparticles can become self-propelled when they are functionalized with enzymes. The catalytic reactions of the enzymes direct the particles based on corresponding substrate gradients.\n\nThere is a distinction between wet and dry systems. In the first case the particles \"swim\" in a surrounding fluid; in the second case the particles \"walk\" on a substrate. \n\nActive colloidal particles, dubbed nanomotors, are the prototypical example of wet SPP. Janus particles are colloidal particles with two different sides, having different physical or chemical properties. This symmetry breaking allows, by properly tuning the environment (typically the surrounding solution), for the motion of the Janus particle. For instance, the two sides of the Janus particle can induce a local gradient of, temperature, electric field, or concentration of chemical species. This induces motion of the Janus particle along the gradient through, respectively, thermophoresis, electrophoresis or diffusiophoresis. Because the Janus particles consume energy from their environment (catalysis of chemical reactions, light absorption, etc.), the resulting motion constitutes an irreversible process and the particles are out of equilibrium. \n\nWalking grains are a typical realization of dry SPP: The grains are milli-metric disks sitting on a vertically vibrating plate, which serves as the source of energy and momentum. The disks have two different contacts (\"feet\") with the plate, a hard needle-like foot in the front and a large soft rubber foot in the back. When shaken, the disks move in a preferential direction defined by the polar (head-tail) symmetry of the contacts. This together with the vibrational noise result in a persistent random walk.\n\nTypical collective motion generally includes the formation of self-assembled structures, such as clusters and organized assemblies. \n\nThe prominent and most spectacular emergent large scale behaviour observed in assemblies of SPP is directed collective motion. In that case all particles move in the same direction. On top of that, spatial structures can emerge such as bands, vortices, asters, moving clusters.\n\nAnother class of large scale behaviour, which does \"not\" imply directed motion is either the spontaneous formation of clusters or the separation in a gas-like and a liquid-like phase, an unexpected phenomenon when the SPP have purely repulsive interaction. This phase separation has been called Motility Induced Phase Separation (MIPS).\n\nThe modeling of SPP was introduced in 1995 by Tamás Vicsek \"et al.\" as a special case of the Boids model introduced in 1986 by Reynolds. In that case the SPP are point particles, which move with a constant speed. and adopt (at each time increment) the average direction of motion of the other particles in their local neighborhood up to some added noise.\nSimulations demonstrate that a suitable \"nearest neighbour rule\" eventually results in all the particles swarming together, or moving in the same direction. This emerges, even though there is no centralised coordination, and even though the neighbours for each particle constantly change over time (see the interactive simulation in the box on the right).\n\nSince then a number of models have been proposed, ranging from the simples so called Active Brownian Particle to highly elaborated and specialized models aiming at describing specific systems and situations. Among the important ingredients in these models, one can list \nOne can also include effective influences of the surrounding; for instance the nominal velocity of the SPP can be set to depend on the local density, in order to take into account crowding effects.\n\nYoung desert locusts are solitary and wingless nymphs. If food is short they can gather together and start occupying neighbouring areas, recruiting more locusts. Eventually they can become a marching army extending over many kilometres. This can be the prelude to the development of the vast flying adult locust swarms which devastate vegetation on a continental scale.\n\nOne of the key predictions of the SPP model is that as the population density of a group increases, an abrupt transition occurs from individuals moving in relatively disordered and independent ways within the group to the group moving as a highly aligned whole. Thus, in the case of young desert locusts, a trigger point should occur which turns disorganised and dispersed locusts into a coordinated marching army. When the critical population density is reached, the insects should start marching together in a stable way and in the same direction.\n\nIn 2006, a group of researchers examined how this model held up in the laboratory. Locusts were placed in a circular arena, and their movements were tracked with computer software. At low densities, below 18 locusts per square metre, the locusts mill about in a disordered way. At intermediate densities, they start falling into line and marching together, punctuated by abrupt but coordinated changes in direction. However, when densities reached a critical value at about 74 locusts/m, the locusts ceased making rapid and spontaneous changes in direction, and instead marched steadily in the same direction for the full eight hours of the experiment (see video on the left). This confirmed the behaviour predicted by the SPP models.\n\nIn the field, according to the Food and Agriculture Organization of the United Nations, the average density of marching bands is 50 locusts/m (50 million locusts/km), with a typical range from 20 to 120 locusts/m. The research findings discussed above demonstrate the dynamic instability that is present at the lower locust densities typical in the field, where marching groups randomly switch direction without any external perturbation. Understanding this phenomenon, together with the switch to fully coordinated marching at higher densities, is essential if the swarming of desert locusts is to be controlled.\n\nSwarming animals, such as ants, bees, fish and birds, are often observed suddenly switching from one state to another. For example, birds abruptly switch from a flying state to a landing state. Or fish switch from schooling in one direction to schooling in another direction. Such state switches can occur with astonishing speed and synchronicity, as though all the members in the group made a unanimous decision at the same moment. Phenomena like these have long puzzled researchers.\n\nIn 2010, Bhattacharya and Vicsek used an SPP model to analyse what is happening here. As a paradigm, they considered how flying birds arrive at a collective decision to make a sudden and synchronised change to land. The birds, such as the starlings in the image on the right, have no decision-making leader, yet the flock know exactly how to land in a unified way. The need for the group to land overrides deviating intentions by individual birds. The particle model found that the collective shift to landing depends on perturbations that apply to the individual birds, such as where the birds are in the flock. It is behaviour that can be compared with the way that sand avalanches, if it is piled up, before the point at which symmetric and carefully placed grains would avalanche, because the fluctuations become increasingly non-linear.\n\n\"Our main motivation was to better understand something which is puzzling and out there in nature, especially in cases involving the stopping or starting of a collective behavioural pattern in a group of people or animals ... We propose a simple model for a system whose members have the tendency to follow the others both in space and in their state of mind concerning a decision about stopping an activity. This is a very general model, which can be applied to similar situations.\" The model could also be applied to a swarm of unmanned drones, to initiating a desired motion in a crowd of people, or to interpreting group patterns when stock market shares are bought or sold.\n\nSPP models have been applied in many other areas, such as schooling fish, robotic swarms, molecular motors, the development of human stampedes and the evolution of human trails in urban green spaces. SPP in Stokes flow, such as Janus particles, are often modeled by the squirmer model.\n\n\n\n"}
{"id": "42836571", "url": "https://en.wikipedia.org/wiki?curid=42836571", "title": "Similarity (network science)", "text": "Similarity (network science)\n\nSimilarity in network analysis occurs when two nodes (or other more elaborate structures) fall in the same equivalence class. \n\nThere are three fundamental approaches to constructing measures of network similarity: structural equivalence, automorphic equivalence, and regular equivalence. There is a hierarchy of the three equivalence concepts: any set of structural equivalences are also automorphic and regular equivalences. Any set of automorphic equivalences are also regular equivalences. Not all regular equivalences are necessarily automorphic or structural; and not all automorphic equivalences are necessarily structural.\n\nAgglomerative Hierarchical clustering of nodes on the basis of the similarity of their profiles of ties to other nodes provides a joining tree or Dendrogram that visualizes the degree of similarity among cases - and can be used to find approximate equivalence classes.\n\nUsually our goal in equivalence analysis is to identify and visualize \"classes\" or clusters of cases. In using cluster analysis, we are implicitly assuming that the similarity or distance among cases reflects as single underlying dimension. It is possible, however, that there are multiple \"aspects\" or \"dimensions\" underlying the observed similarities of cases. Factor or components analysis could be applied to correlations or covariances among cases. Alternatively, multi-dimensional scaling could be used (non-metric for data that are inherently nominal or ordinal; metric for valued).\n\nMDS represents the patterns of similarity or dissimilarity in the tie profiles among the actors (when applied to adjacency or distances) as a \"map\" in multi-dimensional space. This map lets us see how \"close\" actors are, whether they \"cluster\" in multi-dimensional space, and how much variation there is along each dimension.\n\nTwo vertices of a network are structurally equivalent if they share many of the same neighbors. \n\nThere is no actor who has exactly the same set of ties as actor A, so actor A is in a class by itself. The same is true for actors B, C, D and G. Each of these nodes has a unique set of edges to other nodes. E and F, however, fall in the same structural equivalence class. Each has only one edge; and that tie is to B. Since E and F have exactly the same pattern of edges with all the vertices, they are structurally equivalent. The same is true in the case of H and I.\n\nStructural equivalence is the strongest form of similarity. In many real networks exact equivalence may be rare, and it could be useful to ease the criteria and measure approximate equivalence. \n\nA closely related concept is \"institutional equivalence\": two actors (e.g., firms) are institutionally equivalent if they operate in the same set of institutional fields. While structurally equivalent actors have identical relational patterns or network positions, institutional equivalence captures the similarity of institutional influences that actors experience from being in the same fields, regardless of how similar their network positions are. For example, two banks in Chicago might have very different patterns of ties (e.g., one may be a central node, and the other may be in a peripheral position) such that they are not structural equivalents, but because they both operate in the field of finance and banking and in the same geographically defined field (Chicago), they will be subject to some of the same institutional influences.\n\nA simple count of common neighbors for two vertices is not on its own a very good measure. One should know the degree of the vertices or how many common neighbors other pairs of vertices has. Cosine similarity takes into account these regards and also allow for the varying degrees of vertices. Salton proposed that we regard the i-th and j-th rows/columns of the adjacency matrix as two vectors and use the cosine of the angle between them as a similarity measure. The cosine similarity of i and j is the number of common neighbors divided by the geometric mean of their degrees. \n\nIts value lies in the range from 0 to 1. The value of 1 indicates that the two vertices have exactly the same neighbors while the value of zero means that they do not have any common neighbors. Cosine similarity is technically undefined if one or both of the nodes has zero degree, but according to the convention we say that cosine similarity is 0 in these cases.\n\nPearson product-moment correlation coefficient is an alternative method to normalize the count of common neighbors. This method compares the number of common neighbors with the expected value that count would take in a network where vertices are connected randomly. This quantity lies strictly in the range from -1 to 1.\n\nEuclidean distance is equal to the number of neighbors that differ between two vertices. It is rather a dissimilarity measure, since it is larger for vertices which differ more. It could be normalized by dividing by its maximum value. The maximum means that there are no common neighbors, in which case the distance is equal to the sum of the degrees of the vertices.\n\nFormally \"Two vertices are automorphically equivalent if all the vertices can be re-labeled to form an isomorphic graph with the labels of u and v interchanged. Two automorphically equivalent vertices share exactly the same label-independent properties.\"\n\nMore intuitively, actors are automorphically equivalent if we can permute the graph in such a way that exchanging the two actors has no effect on the distances among all actors in the graph.\n\nSuppose the graph describes the organizational structure of a company. Actor A is the central headquarter, actors B, C, and D are managers. Actors E, F and H, I are workers at smaller stores; G is the lone worker at an other store.\n\nEven though actor B and actor D are not structurally equivalent (they do have the same boss, but not the same workers), they do seem to be \"equivalent\" in a different sense. Both manager B and D has a boss (in this case, the same boss), and each has two workers. If we swapped them, and also swapped the four workers, all of the distances among all the actors in the network would be exactly identical. \n\nThere are actually five automorphic equivalence classes: {A}, {B, D}, {C}, {E, F, H, I}, and {G}. Note that the less strict definition of \"equivalence\" has reduced the number of classes.\n\nFormally, \"Two actors are regularly equivalent if they are equally related to equivalent others.\" In other words, regularly equivalent vertices are vertices that, while they do not necessarily share neighbors, have neighbors who are themselves similar.\n\nTwo mothers, for example, are equivalent, because each has a similar pattern of connections with a husband, children, etc. The two mothers do not have ties to the same husband or the same children, so they are not structurally equivalent. Because different mothers may have different numbers of husbands and children, they will not be automorphically equivalent. But they are similar because they have the same relationships with some member or members of another set of actors (who are themselves regarded as equivalent because of the similarity of their ties to a member of the set \"mother\").\n\nIn the graph there are three regular equivalence classes. The first is actor A; the second is composed of the three actors B, C, and D; the third consists of the remaining five actors E, F, G, H, and I.\n\nThe easiest class to see is the five actors across the bottom of the diagram (E, F, G, H, and I). These actors are regularly equivalent to one another because:\n\n\nEach of the five actors, then, has an identical pattern of ties with actors in the other classes.\n\nActors B, C, and D form a class similarly. B and D actually have ties with two members of the third class, whereas actor C has a tie to only one member of the third class, but this doesn't matter, as there is a tie to some member of the third class.\n\nActor A is in a class by itself, defined by:\n\n"}
{"id": "23537962", "url": "https://en.wikipedia.org/wiki?curid=23537962", "title": "Small control property", "text": "Small control property\n\nIn nonlinear control theory, a non-linear system of the form formula_1 is said to satisfy the small control property if for every formula_2 there exists a formula_3 so that for all formula_4 there exists a formula_5 so that the time derivative of the system's Lyapunov function is negative definite at that point.\n\nIn other words, even if the control input is arbitrarily small, a starting configuration close enough to the origin of the system can be found that is asymptotically stabilizable by such an input.\n"}
{"id": "23444306", "url": "https://en.wikipedia.org/wiki?curid=23444306", "title": "Specht's theorem", "text": "Specht's theorem\n\nIn mathematics, Specht's theorem gives a necessary and sufficient condition for two matrices to be unitarily equivalent. It is named after Wilhelm Specht, who proved the theorem in 1940.\n\nTwo matrices \"A\" and \"B\" are said to be \"unitarily equivalent\" if there exists a unitary matrix \"U\" such that \"B\" = \"U\" *\"AU\". Two matrices which are unitarily equivalent are also similar. Two similar matrices represent the same linear map, but with respect to a different basis; unitary equivalence corresponds to a change from an orthonormal basis to another orthonormal basis. \n\nIf \"A\" and \"B\" are unitarily equivalent, then tr \"AA\"* = tr \"BB\"*, where tr denotes the trace (in other words, the Frobenius norm is a unitary invariant). This follows from the cyclic invariance of the trace: if \"B\" = \"U\" *\"AU\", then tr \"BB\"* = tr \"U\" *\"AUU\" *\"A\"*\"U\" = tr \"AUU\" *\"A\"*\"UU\" * = tr \"AA\"*, where the second equality is cyclic invariance. \n\nThus, tr \"AA\"* = tr \"BB\"* is a necessary condition for unitary equivalence, but it is not sufficient. Specht's theorem gives infinitely many necessary conditions which together are also sufficient. The formulation of the theorem uses the following definition. A word in two variables, say \"x\" and \"y\", is an expression of the form\n\nwhere \"m\", \"n\", \"m\", \"n\", …, \"m\" are non-negative integers. The \"degree\" of this word is\n\nSpecht's theorem: Two matrices \"A\" and \"B\" are unitarily equivalent if and only if tr \"W\"(\"A\", \"A\"*) = tr \"W\"(\"B\", \"B\"*) for all words \"W\". \n\nThe theorem gives an infinite number of trace identities, but it can be reduced to a finite subset. Let \"n\" denote the size of the matrices \"A\" and \"B\". For the case \"n\" = 2, the following three conditions are sufficient: \n\nFor \"n\" = 3, the following seven conditions are sufficient:\n\nFor general \"n\", it suffices to show that tr \"W\"(\"A\", \"A\"*) = tr \"W\"(\"B\", \"B\"*) for all words of degree at most \n\nIt has been conjectured that this can be reduced to an expression linear in \"n\".\n\n"}
{"id": "748730", "url": "https://en.wikipedia.org/wiki?curid=748730", "title": "Sperner family", "text": "Sperner family\n\nIn combinatorics, a Sperner family (or Sperner system), named in honor of Emanuel Sperner, is a family of sets (F, \"E\") in which none of the sets is contained in another. Equivalently, a Sperner family is an antichain in the inclusion lattice over the power set of \"E\". A Sperner family is also sometimes called an independent system or a clutter.\n\nSperner families are counted by the Dedekind numbers, and their size is bounded by Sperner's theorem and the Lubell–Yamamoto–Meshalkin inequality. They may also be described in the language of hypergraphs rather than set families, where they are called clutters.\n\nThe number of different Sperner families on a set of \"n\" elements is counted by the Dedekind numbers, the first few of which are\nAlthough accurate asymptotic estimates are known for larger values of \"n\", it is unknown whether there exists an exact formula that can be used to compute these numbers efficiently.\n\nThe \"k\"-element subsets of an \"n\"-element set form a Sperner family, the size of which is maximized when \"k\" = \"n\"/2 (or the nearest integer to it).\nSperner's theorem states that these families are the largest possible Sperner families over an \"n\"-element set. Formally, the theorem states that, for every Sperner family \"S\" over an \"n\"-element set,\n\nThe Lubell–Yamamoto–Meshalkin inequality provides another bound on the size of a Sperner family, and can be used to prove Sperner's theorem.\nIt states that, if \"a\" denotes the number of sets of size \"k\" in a Sperner family over a set of \"n\" elements, then\n\nA clutter \"H\" is a hypergraph formula_3, with the added property that formula_4 whenever formula_5 and formula_6 (i.e. no edge properly contains another). That is, the sets of vertices represented by the hyperedges form a Sperner family. Clutters are an important structure in the study of combinatorial optimization. An opposite notion to a clutter is an abstract simplicial complex, where every subset of an edge is contained in the hypergraph (this is an order ideal in the poset of subsets of \"E\").\n\nIf formula_7 is a clutter, then the blocker of \"H\", denoted formula_8, is the clutter with vertex set \"V\" and edge set consisting of all minimal sets formula_9 so that formula_10 for every formula_11. It can be shown that formula_12 , so blockers give us a type of duality. We define formula_13 to be the size of the largest collection of disjoint edges in \"H\" and formula_14 to be the size of the smallest edge in formula_8. It is easy to see that formula_16.\n\n\nThere is a minor relation on clutters which is similar to the minor relation on graphs. If formula_7 is a clutter and formula_32, then we may delete \"v\" to get the clutter formula_33 with vertex set formula_34 and edge set consisting of all formula_11 which do not contain \"v\". We contract \"v\" to get the clutter formula_36. These two operations commute, and if \"J\" is another clutter, we say that \"J\" is a minor of \"H\" if a clutter isomorphic to \"J\" may be obtained from \"H\" by a sequence of deletions and contractions.\n\n"}
{"id": "59113779", "url": "https://en.wikipedia.org/wiki?curid=59113779", "title": "Tannery's theorem", "text": "Tannery's theorem\n\nIn mathematical analysis, Tannery's theorem gives sufficient conditions for the interchanging of the limit and infinite summation operations. It is named after Jules Tannery.\n\nLet formula_1 and suppose that formula_2. If formula_3 and formula_4, then formula_5.\n\nTannery's theorem follows directly from Lebesgue's dominated convergence theorem applied to the sequence space ℓ.\n\nAn elementary proof can also be given.\n\nTannery theorem can be used to prove that the binomial limit and the infinite series characterizations of the exponential formula_6are equivalent. Note that \n\nDefine formula_8. We have that formula_9, so Tannery's theorem can be applied and\n\nGeneralizations of Tannery's theorem\n"}
{"id": "3741564", "url": "https://en.wikipedia.org/wiki?curid=3741564", "title": "Type (model theory)", "text": "Type (model theory)\n\nIn model theory and related areas of mathematics, a type is an object that, loosely speaking, describes how a (real or possible) element or elements in a mathematical structure might behave. More precisely, it is a set of first-order formulas in a language \"L\" with free variables \"x\", \"x\",…, \"x\" that are true of a sequence of elements of an \"L\"-structure formula_1. Depending on the context, types can be complete or partial and they may use a fixed set of constants, \"A\", from the structure formula_1. The question of which types represent actual elements of formula_1 leads to the ideas of saturated models and omitting types.\n\nConsider a structure formula_1 for a language \"L\". Let \"M\" be the universe of the structure. For every \"A\" ⊆ \"M\", let \"L\"(\"A\") be the language obtained from \"L\" by adding a constant \"c\" for every \"a\" ∈ \"A\". In other words,\n\nA 1-type (of formula_1) over \"A\" is a set \"p\"(\"x\") of formulas in \"L\"(\"A\") with at most one free variable \"x\" (therefore 1-type) such that for every finite subset \"p\"(\"x\") ⊆ \"p\"(\"x\") there is some \"b\" ∈ \"M\", depending on \"p\"(\"x\"), with formula_7 (i.e. all formulas in \"p\"(\"x\") are true in formula_1 when \"x\" is replaced by \"b\").\n\nSimilarly an n\"-type (of formula_1) over \"A\" is defined to be a set \"p\"(\"x\",…,\"x\") = \"p\"(x) of formulas in \"L\"(\"A\"), each having its free variables occurring only among the given \"n\" free variables \"x\",…,\"x\", such that for every finite subset \"p\"(x) ⊆ \"p\"(x\") there are some elements \"b\",…,\"b\" ∈ \"M\" with formula_10.\n\nComplete type refers to those types that are maximal with respect to inclusion, i.e. if \"p\"(x) is a complete type, then for every formula_11 either formula_12 or formula_13. Any non-complete type is called a partial type. \nSo, the word type in general refers to any \"n\"-type, partial or complete, over any chosen set of parameters (possibly the empty set).\n\nAn \"n\"-type \"p\"(x) is said to be realized in formula_1 if there is an element b ∈ \"M\" such that formula_15. The existence of such a realization is guaranteed for any type by the Compactness theorem, although the realization might take place in some elementary extension of formula_1, rather than in formula_1 itself. \nIf a complete type is realized by b in formula_1, then the type is typically denoted formula_19 and referred to as the complete type of \"b\" over \"A\".\n\nA type \"p\"(x) is said to be isolated by \"φ, for formula_20, if formula_21. Since finite subsets of a type are always realized in formula_1, there is always an element b ∈ \"M\" such that \"φ\"(b) is true in formula_1; i.e. formula_24, thus b\" realizes the entire isolated type. So isolated types will be realized in every elementary substructure or extension. Because of this, isolated types can never be omitted (see below).\n\nA model that realizes the maximum possible variety of types is called a saturated model, and the ultrapower construction provides one way of producing saturated models.\n\nConsider the language with one binary connective, which we denote as formula_25. Let formula_1 be the structure formula_27 for this language, which is the ordinal formula_28 with its standard well-ordering. Let formula_29 denote the theory of formula_1.\n\nConsider the set of formulas formula_31. First, we claim this is a type. Let formula_32 be a finite subset of formula_33. We need to find an formula_34 that satisfies all the formulas in formula_35. Well, we can just take the successor of the largest ordinal mentioned in the set of formulas formula_36. Then this will clearly contain all the ordinals mentioned in formula_36. Thus we have that formula_33 is a type. \nNext, note that formula_33 is not realized in formula_1. For, if it were there would be some formula_41 that contains every element of formula_28. \nIf we wanted to realize the type, we might be tempted to consider the model formula_43, which is indeed a supermodel of formula_1 that realizes the type. Unfortunately, this extension is not elementary, that is this model does not have to satisfy formula_29. In particular, the sentence formula_46 is satisfied by this model and not by formula_1.\n\nSo, we wish to realize the type in an elementary extension. We can do this by defining a new structure in this language, which we will denote formula_48. The domain of the structure will be formula_49 where formula_50 is the set of integers adorned in such a way that formula_51. Let formula_52 denote the usual order of formula_50. We interpret the symbol formula_25 in our new structure by formula_55. The idea being that we are adding a \"Z-chain\", or copy of the integers, above all the finite ordinals. Clearly any element of formula_50 realizes the type formula_33. Moreover, one can verify that this extension is elementary.\n\nAnother example: the complete type of the number 2 over the emptyset, considered as a member of the natural numbers, would be the set of all first-order statements describing a variable \"x\" that are true for \"x\" = 2. This set would include formulas such as formula_58, formula_59, and formula_60. This is an example of an isolated type, since the formula formula_61 implies all other formulas that are true about the number 2.\n\nFor example, the statements\n\nand\n\ndescribing the square root of 2 are consistent with the axioms of ordered fields, and can be extended to a complete type. This type is not realized in the ordered field of rational numbers, but is realized in the ordered field of reals. Similarly, the infinite set of formulas (over the emptyset) {x>1, x>1+1, x>1+1+1, ...} is not realized in the ordered field of real numbers, but is realized in the ordered field of hyperreals. If we allow more parameters, for instance all of the reals, we can specify a type formula_64 that is realized by an infinitesimal hyperreal that violates the Archimedean property.\n\nThe reason it is useful to restrict the parameters to a certain subset of the model is that it helps to distinguish the types that can be satisfied from those that cannot. For example, using the entire set of real numbers as parameters one could generate an uncountably infinite set of formulas like formula_65, formula_66, ... that would explicitly rule out every possible real value for \"x\", and therefore could never be realized within the real numbers.\n\nIt is useful to consider the set of complete \"n\"-types over \"A\" as a topological space. Consider the following equivalence relation on formulae in the free variables \"x\",…, \"x\" with parameters in \"M\":\nOne can show that formula_68 iff they are contained in exactly the same complete types.\n\nThe set of formulae in free variables \"x\",…,\"x\" over \"A\" up to this equivalence relation is a Boolean algebra (and is canonically isomorphic to the set of \"A\"-definable subsets of \"M\"). The complete \"n\"-types correspond to ultrafilters of this boolean algebra. The set of complete \"n\"-types can be made into a topological space by taking the sets of types containing a given formula as basic open sets. This constructs the Stone space, which is compact, Hausdorff, and totally disconnected.\n\nExample. The complete theory of algebraically closed fields of characteristic 0 has quantifier elimination, which allows one to show that the possible complete 1-types correspond to:\nIn other words, the 1-types correspond exactly to the prime ideals of the polynomial ring Q[\"x\"] over the rationals Q: if \"r\" is an element of the model of type \"p\", then the ideal corresponding to \"p\" is the set of polynomials with \"r\" as a root. More generally, the complete \"n\"-types correspond to the prime ideals of the polynomial ring Q[\"x\"...,\"x\"], in other words to the points of the prime spectrum of this ring. (The Stone space topology can in fact be viewed as the Zariski topology of a Boolean ring induced in a natural way from the lattice structure of the Boolean Algebra; while the Zariski topology is not in general Hausdorff, it is in the case of Boolean rings.) For example, if \"q\"(\"x\",\"y\") is an irreducible polynomial in 2 variables, there is a 2-type whose realizations are (informally) pairs (\"x\",\"y\") of transcendental elements with \"q\"(\"x\",\"y\")=0.\n\nGiven a complete \"n\"-type \"p\" one can ask if there is a model of the theory that omits \"p\", in other words there is no \"n\"-tuple in the model that realizes \"p\". \nIf \"p\" is an isolated point in the Stone space, i.e. if {\"p\"} is an open set, it is easy to see that every model realizes \"p\" (at least if the theory is complete). The omitting types theorem says that conversely if \"p\" is not isolated then there is a countable model omitting \"p\" (provided that the language is countable).\n\nExample: In the theory of algebraically closed fields of characteristic 0, there is a 1-type represented by elements that are transcendental over the prime field. This is a non-isolated point of the Stone space (in fact, the only non-isolated point). The field of algebraic numbers is a model omitting this type, and the algebraic closure of any \ntranscendental extension of the rationals is a model realizing this type.\n\nAll the other types are \"algebraic numbers\" (more precisely, they are the sets of first order statements satisfied by some given algebraic number), and all such types are realized in all algebraically closed fields of characteristic 0.\n\n"}
{"id": "58892481", "url": "https://en.wikipedia.org/wiki?curid=58892481", "title": "Ulam–Warburton automaton", "text": "Ulam–Warburton automaton\n\nThe Ulam–Warburton cellular automaton is a 2-dimensional fractal pattern that grows on a regular square grid.\n\nStarting with one square initially ON and all others OFF successive iterations are generated by turning ON all squares that share precisely one edge with an ON square.\n\nThe automaton is named after the Polish-American scientist and mathematician Stanislaw Ulam and the Scottish engineer, inventor and amateur mathematician Mike Warburton. .\n\nThe sequence of the number of squares turned ON in each iteration starts 1, 4, 4, 12, 4, 12, 12, 36, 4, … This is referenced as A147582 in the online On-Line Encyclopedia of Integer Sequences. If we let formula_1 be the number of squares turned ON in each iteration we have an explicit formula: formula_2 and for formula_3\n\nwhere formula_5 is the binary Hamming weight function. \n\nThe sequence of the total number of squares turned ON is denoted formula_6 and starts 1, 5, 9, 21, 25, 37, 49, 85, 89, ... OEIS reference A147562\n\nformula_6 has fractal-like behavior with a sharp upper bound for formula_9\n\nThe upper bound only contacts formula_6 at 'high-water' points when formula_12.\nThese are also the generations at which the Ulam–Warburton cellular automaton, based on squares, the Hex–Ulam–Warburton cellular automaton, based on hexagons and the Sierpinski triangle return to their base shape.\n\nRelated to the UW-cellular automaton is the toothpick sequence and various toothpick-like sequences which can be explored here.\nThe classic game of Nim and a related impartial combinatorial ruleset called LIM are also connected with the Ulam–Warburton cellular automaton in interesting and surprising ways.\n"}
{"id": "32892599", "url": "https://en.wikipedia.org/wiki?curid=32892599", "title": "Yakov Geronimus", "text": "Yakov Geronimus\n\nYakov Lazarevich Geronimus, sometimes spelled J. Geronimus (; February 6, 1898, Rostov – July 17, 1984, Kharkov) was a Russian mathematician known for contributions to theoretical mechanics and the study of orthogonal polynomials. The Geronimus polynomials are named after him.\n\n"}
